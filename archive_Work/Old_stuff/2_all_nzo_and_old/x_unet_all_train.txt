/shared/home/matevz.vidovic/Diplomska/Prototip/Delo/model_wrapper.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model = torch.load(self.prev_model_path, map_location=torch.device(device))
unet_original_main.py do_log: True
Log file name: log_09_17-35-19_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(ptd='./Data/vein_and_sclera_data', sd='unet_all_train', mti=200, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_original_all.yaml', ntibp=None, ptp=None, map=None)
YAML: {'batch_size': 2, 'learning_rate': 1e-05, 'num_of_dataloader_workers': 7, 'train_epoch_size_limit': 400, 'num_epochs_per_training_iteration': 1, 'cleanup_k': 3, 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': True, 'loss_fn_name': 'MCDL', 'alphas': [], 'dataset_option': 'aug_tf', 'zero_out_non_sclera': True, 'add_sclera_to_img': False, 'add_bcosfire_to_img': True, 'add_coye_to_img': True, 'model': '64_2_6', 'input_width': 2048, 'input_height': 1024, 'input_channels': 5, 'output_channels': 2, 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq'}
Validation phase: False
Namespace(ptd='./Data/vein_and_sclera_data', sd='unet_all_train', mti=200, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_original_all.yaml', ntibp=None, ptp=None, map=None)
Device: cuda
dataset_aug_tf.py do_log: False
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 88
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 88
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 44
val dataloader num of batches: 14
test dataloader num of batches: 6
Loaded model path:  ./unet_all_train/saved_model_wrapper/models/UNet_65_.pth
per-ex loss: 0.505590  [    2/   88]
per-ex loss: 0.514727  [    4/   88]
per-ex loss: 0.433007  [    6/   88]
per-ex loss: 0.516227  [    8/   88]
per-ex loss: 0.499653  [   10/   88]
per-ex loss: 0.389109  [   12/   88]
per-ex loss: 0.470120  [   14/   88]
per-ex loss: 0.627975  [   16/   88]
per-ex loss: 0.492173  [   18/   88]
per-ex loss: 0.489252  [   20/   88]
per-ex loss: 0.447792  [   22/   88]
per-ex loss: 0.364653  [   24/   88]
per-ex loss: 0.448456  [   26/   88]
per-ex loss: 0.594093  [   28/   88]
per-ex loss: 0.628312  [   30/   88]
per-ex loss: 0.535849  [   32/   88]
per-ex loss: 0.599871  [   34/   88]
per-ex loss: 0.403126  [   36/   88]
per-ex loss: 0.462003  [   38/   88]
per-ex loss: 0.455669  [   40/   88]
per-ex loss: 0.487001  [   42/   88]
per-ex loss: 0.567767  [   44/   88]
per-ex loss: 0.417602  [   46/   88]
per-ex loss: 0.422973  [   48/   88]
per-ex loss: 0.439201  [   50/   88]
per-ex loss: 0.660991  [   52/   88]
per-ex loss: 0.426151  [   54/   88]
per-ex loss: 0.395243  [   56/   88]
per-ex loss: 0.548251  [   58/   88]
per-ex loss: 0.439801  [   60/   88]
per-ex loss: 0.471642  [   62/   88]
per-ex loss: 0.472495  [   64/   88]
per-ex loss: 0.509710  [   66/   88]
per-ex loss: 0.585498  [   68/   88]
per-ex loss: 0.448966  [   70/   88]
per-ex loss: 0.431093  [   72/   88]
per-ex loss: 0.453632  [   74/   88]
per-ex loss: 0.455611  [   76/   88]
per-ex loss: 0.487408  [   78/   88]
per-ex loss: 0.452045  [   80/   88]
per-ex loss: 0.508874  [   82/   88]
per-ex loss: 0.401644  [   84/   88]
per-ex loss: 0.387610  [   86/   88]
per-ex loss: 0.396951  [   88/   88]
Train Error: Avg loss: 0.48058674
validation Error: 
 Avg loss: 0.52729688 
 F1: 0.494913 
 Precision: 0.627266 
 Recall: 0.408681
 IoU: 0.328827

test Error: 
 Avg loss: 0.49230024 
 F1: 0.544153 
 Precision: 0.707685 
 Recall: 0.442012
 IoU: 0.373770

We have finished training iteration 66
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_43_.pth
per-ex loss: 0.373529  [    2/   88]
per-ex loss: 0.510546  [    4/   88]
per-ex loss: 0.366699  [    6/   88]
per-ex loss: 0.430917  [    8/   88]
per-ex loss: 0.377212  [   10/   88]
per-ex loss: 0.373403  [   12/   88]
per-ex loss: 0.415068  [   14/   88]
per-ex loss: 0.521927  [   16/   88]
per-ex loss: 0.442437  [   18/   88]
per-ex loss: 0.394829  [   20/   88]
per-ex loss: 0.493109  [   22/   88]
per-ex loss: 0.511994  [   24/   88]
per-ex loss: 0.386841  [   26/   88]
per-ex loss: 0.501323  [   28/   88]
per-ex loss: 0.455823  [   30/   88]
per-ex loss: 0.574074  [   32/   88]
per-ex loss: 0.405572  [   34/   88]
per-ex loss: 0.441827  [   36/   88]
per-ex loss: 0.605539  [   38/   88]
per-ex loss: 0.431803  [   40/   88]
per-ex loss: 0.536175  [   42/   88]
per-ex loss: 0.515392  [   44/   88]
per-ex loss: 0.425036  [   46/   88]
per-ex loss: 0.480440  [   48/   88]
per-ex loss: 0.565939  [   50/   88]
per-ex loss: 0.459580  [   52/   88]
per-ex loss: 0.391177  [   54/   88]
per-ex loss: 0.487760  [   56/   88]
per-ex loss: 0.482769  [   58/   88]
per-ex loss: 0.569997  [   60/   88]
per-ex loss: 0.622538  [   62/   88]
per-ex loss: 0.593885  [   64/   88]
per-ex loss: 0.541879  [   66/   88]
per-ex loss: 0.437954  [   68/   88]
per-ex loss: 0.616186  [   70/   88]
per-ex loss: 0.666472  [   72/   88]
per-ex loss: 0.554039  [   74/   88]
per-ex loss: 0.384534  [   76/   88]
per-ex loss: 0.573618  [   78/   88]
per-ex loss: 0.387047  [   80/   88]
per-ex loss: 0.409756  [   82/   88]
per-ex loss: 0.503001  [   84/   88]
per-ex loss: 0.590189  [   86/   88]
per-ex loss: 0.408114  [   88/   88]
Train Error: Avg loss: 0.48222607
validation Error: 
 Avg loss: 0.51368666 
 F1: 0.501538 
 Precision: 0.637247 
 Recall: 0.413483
 IoU: 0.334702

test Error: 
 Avg loss: 0.48481920 
 F1: 0.550298 
 Precision: 0.706002 
 Recall: 0.450863
 IoU: 0.379594

We have finished training iteration 67
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_64_.pth
per-ex loss: 0.469592  [    2/   88]
per-ex loss: 0.409962  [    4/   88]
per-ex loss: 0.512610  [    6/   88]
per-ex loss: 0.481212  [    8/   88]
per-ex loss: 0.386208  [   10/   88]
per-ex loss: 0.529196  [   12/   88]
per-ex loss: 0.384050  [   14/   88]
per-ex loss: 0.555236  [   16/   88]
per-ex loss: 0.492644  [   18/   88]
per-ex loss: 0.501913  [   20/   88]
per-ex loss: 0.455918  [   22/   88]
per-ex loss: 0.516380  [   24/   88]
per-ex loss: 0.407053  [   26/   88]
per-ex loss: 0.397267  [   28/   88]
per-ex loss: 0.403613  [   30/   88]
per-ex loss: 0.493197  [   32/   88]
per-ex loss: 0.477266  [   34/   88]
per-ex loss: 0.479647  [   36/   88]
per-ex loss: 0.616005  [   38/   88]
per-ex loss: 0.471770  [   40/   88]
per-ex loss: 0.466834  [   42/   88]
per-ex loss: 0.501649  [   44/   88]
per-ex loss: 0.457069  [   46/   88]
per-ex loss: 0.629014  [   48/   88]
per-ex loss: 0.419841  [   50/   88]
per-ex loss: 0.537758  [   52/   88]
per-ex loss: 0.424915  [   54/   88]
per-ex loss: 0.404671  [   56/   88]
per-ex loss: 0.396208  [   58/   88]
per-ex loss: 0.417316  [   60/   88]
per-ex loss: 0.586554  [   62/   88]
per-ex loss: 0.478999  [   64/   88]
per-ex loss: 0.423955  [   66/   88]
per-ex loss: 0.654760  [   68/   88]
per-ex loss: 0.407081  [   70/   88]
per-ex loss: 0.399472  [   72/   88]
per-ex loss: 0.607613  [   74/   88]
per-ex loss: 0.617365  [   76/   88]
per-ex loss: 0.444987  [   78/   88]
per-ex loss: 0.509657  [   80/   88]
per-ex loss: 0.444364  [   82/   88]
per-ex loss: 0.379283  [   84/   88]
per-ex loss: 0.578116  [   86/   88]
per-ex loss: 0.434576  [   88/   88]
Train Error: Avg loss: 0.47869994
validation Error: 
 Avg loss: 0.52804857 
 F1: 0.489734 
 Precision: 0.490010 
 Recall: 0.489459
 IoU: 0.324270

test Error: 
 Avg loss: 0.47747600 
 F1: 0.558277 
 Precision: 0.582472 
 Recall: 0.536012
 IoU: 0.387229

We have finished training iteration 68
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_66_.pth
per-ex loss: 0.430756  [    2/   88]
per-ex loss: 0.457698  [    4/   88]
per-ex loss: 0.490904  [    6/   88]
per-ex loss: 0.374187  [    8/   88]
per-ex loss: 0.479285  [   10/   88]
per-ex loss: 0.419416  [   12/   88]
per-ex loss: 0.470053  [   14/   88]
per-ex loss: 0.429998  [   16/   88]
per-ex loss: 0.383535  [   18/   88]
per-ex loss: 0.470612  [   20/   88]
per-ex loss: 0.376940  [   22/   88]
per-ex loss: 0.431480  [   24/   88]
per-ex loss: 0.484630  [   26/   88]
per-ex loss: 0.500559  [   28/   88]
per-ex loss: 0.450708  [   30/   88]
per-ex loss: 0.576704  [   32/   88]
per-ex loss: 0.435250  [   34/   88]
per-ex loss: 0.468689  [   36/   88]
per-ex loss: 0.477987  [   38/   88]
per-ex loss: 0.445632  [   40/   88]
per-ex loss: 0.571067  [   42/   88]
per-ex loss: 0.484198  [   44/   88]
per-ex loss: 0.524870  [   46/   88]
per-ex loss: 0.554171  [   48/   88]
per-ex loss: 0.417774  [   50/   88]
per-ex loss: 0.501206  [   52/   88]
per-ex loss: 0.613987  [   54/   88]
per-ex loss: 0.430828  [   56/   88]
per-ex loss: 0.601834  [   58/   88]
per-ex loss: 0.415638  [   60/   88]
per-ex loss: 0.561695  [   62/   88]
per-ex loss: 0.457214  [   64/   88]
per-ex loss: 0.553759  [   66/   88]
per-ex loss: 0.449822  [   68/   88]
per-ex loss: 0.464075  [   70/   88]
per-ex loss: 0.383286  [   72/   88]
per-ex loss: 0.552100  [   74/   88]
per-ex loss: 0.501411  [   76/   88]
per-ex loss: 0.425990  [   78/   88]
per-ex loss: 0.391395  [   80/   88]
per-ex loss: 0.397832  [   82/   88]
per-ex loss: 0.655188  [   84/   88]
per-ex loss: 0.649366  [   86/   88]
per-ex loss: 0.509880  [   88/   88]
Train Error: Avg loss: 0.48008204
validation Error: 
 Avg loss: 0.52862429 
 F1: 0.496504 
 Precision: 0.636046 
 Recall: 0.407174
 IoU: 0.330233

test Error: 
 Avg loss: 0.48099797 
 F1: 0.559072 
 Precision: 0.693762 
 Recall: 0.468177
 IoU: 0.387994

We have finished training iteration 69
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_67_.pth
per-ex loss: 0.365119  [    2/   88]
per-ex loss: 0.546342  [    4/   88]
per-ex loss: 0.369666  [    6/   88]
per-ex loss: 0.506086  [    8/   88]
per-ex loss: 0.465313  [   10/   88]
per-ex loss: 0.470761  [   12/   88]
per-ex loss: 0.470865  [   14/   88]
per-ex loss: 0.501537  [   16/   88]
per-ex loss: 0.413122  [   18/   88]
per-ex loss: 0.429480  [   20/   88]
per-ex loss: 0.488695  [   22/   88]
per-ex loss: 0.564274  [   24/   88]
per-ex loss: 0.377548  [   26/   88]
per-ex loss: 0.392106  [   28/   88]
per-ex loss: 0.552951  [   30/   88]
per-ex loss: 0.603206  [   32/   88]
per-ex loss: 0.668057  [   34/   88]
per-ex loss: 0.626225  [   36/   88]
per-ex loss: 0.382045  [   38/   88]
per-ex loss: 0.455269  [   40/   88]
per-ex loss: 0.458434  [   42/   88]
per-ex loss: 0.428684  [   44/   88]
per-ex loss: 0.420093  [   46/   88]
per-ex loss: 0.568758  [   48/   88]
per-ex loss: 0.419593  [   50/   88]
per-ex loss: 0.460924  [   52/   88]
per-ex loss: 0.533640  [   54/   88]
per-ex loss: 0.658461  [   56/   88]
per-ex loss: 0.459301  [   58/   88]
per-ex loss: 0.410830  [   60/   88]
per-ex loss: 0.429334  [   62/   88]
per-ex loss: 0.560333  [   64/   88]
per-ex loss: 0.420903  [   66/   88]
per-ex loss: 0.512799  [   68/   88]
per-ex loss: 0.619028  [   70/   88]
per-ex loss: 0.408453  [   72/   88]
per-ex loss: 0.472395  [   74/   88]
per-ex loss: 0.401198  [   76/   88]
per-ex loss: 0.551036  [   78/   88]
per-ex loss: 0.426093  [   80/   88]
per-ex loss: 0.527840  [   82/   88]
per-ex loss: 0.411030  [   84/   88]
per-ex loss: 0.541166  [   86/   88]
per-ex loss: 0.662464  [   88/   88]
Train Error: Avg loss: 0.48662389
validation Error: 
 Avg loss: 0.53490349 
 F1: 0.487212 
 Precision: 0.485646 
 Recall: 0.488788
 IoU: 0.322062

test Error: 
 Avg loss: 0.46833419 
 F1: 0.565655 
 Precision: 0.583536 
 Recall: 0.548837
 IoU: 0.394364

We have finished training iteration 70
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_68_.pth
per-ex loss: 0.532656  [    2/   88]
per-ex loss: 0.548708  [    4/   88]
per-ex loss: 0.359461  [    6/   88]
per-ex loss: 0.473458  [    8/   88]
per-ex loss: 0.394184  [   10/   88]
per-ex loss: 0.481492  [   12/   88]
per-ex loss: 0.477654  [   14/   88]
per-ex loss: 0.485644  [   16/   88]
per-ex loss: 0.427313  [   18/   88]
per-ex loss: 0.505045  [   20/   88]
per-ex loss: 0.443071  [   22/   88]
per-ex loss: 0.474823  [   24/   88]
per-ex loss: 0.523128  [   26/   88]
per-ex loss: 0.398296  [   28/   88]
per-ex loss: 0.462547  [   30/   88]
per-ex loss: 0.466028  [   32/   88]
per-ex loss: 0.432675  [   34/   88]
per-ex loss: 0.435900  [   36/   88]
per-ex loss: 0.631460  [   38/   88]
per-ex loss: 0.381908  [   40/   88]
per-ex loss: 0.389290  [   42/   88]
per-ex loss: 0.424155  [   44/   88]
per-ex loss: 0.542494  [   46/   88]
per-ex loss: 0.491810  [   48/   88]
per-ex loss: 0.478900  [   50/   88]
per-ex loss: 0.417960  [   52/   88]
per-ex loss: 0.553213  [   54/   88]
per-ex loss: 0.662223  [   56/   88]
per-ex loss: 0.509038  [   58/   88]
per-ex loss: 0.626440  [   60/   88]
per-ex loss: 0.586587  [   62/   88]
per-ex loss: 0.454062  [   64/   88]
per-ex loss: 0.373805  [   66/   88]
per-ex loss: 0.439853  [   68/   88]
per-ex loss: 0.512505  [   70/   88]
per-ex loss: 0.358369  [   72/   88]
per-ex loss: 0.478209  [   74/   88]
per-ex loss: 0.437549  [   76/   88]
per-ex loss: 0.448752  [   78/   88]
per-ex loss: 0.428919  [   80/   88]
per-ex loss: 0.445585  [   82/   88]
per-ex loss: 0.418597  [   84/   88]
per-ex loss: 0.467830  [   86/   88]
per-ex loss: 0.393867  [   88/   88]
Train Error: Avg loss: 0.46989688
validation Error: 
 Avg loss: 0.52103980 
 F1: 0.491274 
 Precision: 0.585619 
 Recall: 0.423110
 IoU: 0.325622

test Error: 
 Avg loss: 0.48012280 
 F1: 0.559782 
 Precision: 0.687345 
 Recall: 0.472155
 IoU: 0.388679

We have finished training iteration 71
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_69_.pth
per-ex loss: 0.424364  [    2/   88]
per-ex loss: 0.457615  [    4/   88]
per-ex loss: 0.635614  [    6/   88]
per-ex loss: 0.412346  [    8/   88]
per-ex loss: 0.439481  [   10/   88]
per-ex loss: 0.466265  [   12/   88]
per-ex loss: 0.605631  [   14/   88]
per-ex loss: 0.437399  [   16/   88]
per-ex loss: 0.506386  [   18/   88]
per-ex loss: 0.547749  [   20/   88]
per-ex loss: 0.526915  [   22/   88]
per-ex loss: 0.554686  [   24/   88]
per-ex loss: 0.651990  [   26/   88]
per-ex loss: 0.400608  [   28/   88]
per-ex loss: 0.469418  [   30/   88]
per-ex loss: 0.519278  [   32/   88]
per-ex loss: 0.372169  [   34/   88]
per-ex loss: 0.433220  [   36/   88]
per-ex loss: 0.425079  [   38/   88]
per-ex loss: 0.394557  [   40/   88]
per-ex loss: 0.392794  [   42/   88]
per-ex loss: 0.610243  [   44/   88]
per-ex loss: 0.394384  [   46/   88]
per-ex loss: 0.392535  [   48/   88]
per-ex loss: 0.505369  [   50/   88]
per-ex loss: 0.449791  [   52/   88]
per-ex loss: 0.377339  [   54/   88]
per-ex loss: 0.530114  [   56/   88]
per-ex loss: 0.557854  [   58/   88]
per-ex loss: 0.394501  [   60/   88]
per-ex loss: 0.497667  [   62/   88]
per-ex loss: 0.413794  [   64/   88]
per-ex loss: 0.344580  [   66/   88]
per-ex loss: 0.440608  [   68/   88]
per-ex loss: 0.431903  [   70/   88]
per-ex loss: 0.603746  [   72/   88]
per-ex loss: 0.465932  [   74/   88]
per-ex loss: 0.563675  [   76/   88]
per-ex loss: 0.498666  [   78/   88]
per-ex loss: 0.554509  [   80/   88]
per-ex loss: 0.451876  [   82/   88]
per-ex loss: 0.617241  [   84/   88]
per-ex loss: 0.375417  [   86/   88]
per-ex loss: 0.617608  [   88/   88]
Train Error: Avg loss: 0.48097539
validation Error: 
 Avg loss: 0.51803008 
 F1: 0.508012 
 Precision: 0.569664 
 Recall: 0.458401
 IoU: 0.340493

test Error: 
 Avg loss: 0.47214055 
 F1: 0.565918 
 Precision: 0.631265 
 Recall: 0.512830
 IoU: 0.394620

We have finished training iteration 72
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_70_.pth
per-ex loss: 0.539555  [    2/   88]
per-ex loss: 0.429754  [    4/   88]
per-ex loss: 0.445615  [    6/   88]
per-ex loss: 0.390261  [    8/   88]
per-ex loss: 0.500735  [   10/   88]
per-ex loss: 0.562790  [   12/   88]
per-ex loss: 0.495027  [   14/   88]
per-ex loss: 0.446765  [   16/   88]
per-ex loss: 0.584464  [   18/   88]
per-ex loss: 0.428065  [   20/   88]
per-ex loss: 0.535127  [   22/   88]
per-ex loss: 0.579165  [   24/   88]
per-ex loss: 0.428331  [   26/   88]
per-ex loss: 0.378065  [   28/   88]
per-ex loss: 0.459022  [   30/   88]
per-ex loss: 0.469357  [   32/   88]
per-ex loss: 0.673496  [   34/   88]
per-ex loss: 0.449442  [   36/   88]
per-ex loss: 0.569388  [   38/   88]
per-ex loss: 0.427936  [   40/   88]
per-ex loss: 0.418458  [   42/   88]
per-ex loss: 0.393261  [   44/   88]
per-ex loss: 0.363612  [   46/   88]
per-ex loss: 0.526015  [   48/   88]
per-ex loss: 0.569077  [   50/   88]
per-ex loss: 0.383581  [   52/   88]
per-ex loss: 0.532721  [   54/   88]
per-ex loss: 0.417309  [   56/   88]
per-ex loss: 0.532316  [   58/   88]
per-ex loss: 0.495940  [   60/   88]
per-ex loss: 0.556202  [   62/   88]
per-ex loss: 0.604981  [   64/   88]
per-ex loss: 0.444499  [   66/   88]
per-ex loss: 0.456119  [   68/   88]
per-ex loss: 0.410181  [   70/   88]
per-ex loss: 0.414681  [   72/   88]
per-ex loss: 0.409111  [   74/   88]
per-ex loss: 0.413893  [   76/   88]
per-ex loss: 0.493113  [   78/   88]
per-ex loss: 0.512692  [   80/   88]
per-ex loss: 0.467422  [   82/   88]
per-ex loss: 0.432509  [   84/   88]
per-ex loss: 0.571311  [   86/   88]
per-ex loss: 0.451705  [   88/   88]
Train Error: Avg loss: 0.47870616
validation Error: 
 Avg loss: 0.55363484 
 F1: 0.477352 
 Precision: 0.544004 
 Recall: 0.425249
 IoU: 0.313501

test Error: 
 Avg loss: 0.48920270 
 F1: 0.546271 
 Precision: 0.665484 
 Recall: 0.463280
 IoU: 0.375772

We have finished training iteration 73
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_71_.pth
per-ex loss: 0.525870  [    2/   88]
per-ex loss: 0.465867  [    4/   88]
per-ex loss: 0.435125  [    6/   88]
per-ex loss: 0.550621  [    8/   88]
per-ex loss: 0.571174  [   10/   88]
per-ex loss: 0.464671  [   12/   88]
per-ex loss: 0.382646  [   14/   88]
per-ex loss: 0.474821  [   16/   88]
per-ex loss: 0.618756  [   18/   88]
per-ex loss: 0.418079  [   20/   88]
per-ex loss: 0.511497  [   22/   88]
per-ex loss: 0.432309  [   24/   88]
per-ex loss: 0.497310  [   26/   88]
per-ex loss: 0.628150  [   28/   88]
per-ex loss: 0.527282  [   30/   88]
per-ex loss: 0.496406  [   32/   88]
per-ex loss: 0.394866  [   34/   88]
per-ex loss: 0.441755  [   36/   88]
per-ex loss: 0.512589  [   38/   88]
per-ex loss: 0.370440  [   40/   88]
per-ex loss: 0.483259  [   42/   88]
per-ex loss: 0.431786  [   44/   88]
per-ex loss: 0.401540  [   46/   88]
per-ex loss: 0.385271  [   48/   88]
per-ex loss: 0.346563  [   50/   88]
per-ex loss: 0.408682  [   52/   88]
per-ex loss: 0.482690  [   54/   88]
per-ex loss: 0.411921  [   56/   88]
per-ex loss: 0.388516  [   58/   88]
per-ex loss: 0.497197  [   60/   88]
per-ex loss: 0.435064  [   62/   88]
per-ex loss: 0.455987  [   64/   88]
per-ex loss: 0.426365  [   66/   88]
per-ex loss: 0.585196  [   68/   88]
per-ex loss: 0.421352  [   70/   88]
per-ex loss: 0.430314  [   72/   88]
per-ex loss: 0.350890  [   74/   88]
per-ex loss: 0.481577  [   76/   88]
per-ex loss: 0.414026  [   78/   88]
per-ex loss: 0.524075  [   80/   88]
per-ex loss: 0.484660  [   82/   88]
per-ex loss: 0.658334  [   84/   88]
per-ex loss: 0.568646  [   86/   88]
per-ex loss: 0.460666  [   88/   88]
Train Error: Avg loss: 0.46942749
validation Error: 
 Avg loss: 0.53445346 
 F1: 0.488323 
 Precision: 0.492249 
 Recall: 0.484459
 IoU: 0.323034

test Error: 
 Avg loss: 0.47019396 
 F1: 0.567458 
 Precision: 0.593591 
 Recall: 0.543529
 IoU: 0.396120

We have finished training iteration 74
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_72_.pth
per-ex loss: 0.422137  [    2/   88]
per-ex loss: 0.402516  [    4/   88]
per-ex loss: 0.379356  [    6/   88]
per-ex loss: 0.406399  [    8/   88]
per-ex loss: 0.504880  [   10/   88]
per-ex loss: 0.355102  [   12/   88]
per-ex loss: 0.465776  [   14/   88]
per-ex loss: 0.498658  [   16/   88]
per-ex loss: 0.421305  [   18/   88]
per-ex loss: 0.454354  [   20/   88]
per-ex loss: 0.425463  [   22/   88]
per-ex loss: 0.571317  [   24/   88]
per-ex loss: 0.494490  [   26/   88]
per-ex loss: 0.414077  [   28/   88]
per-ex loss: 0.499210  [   30/   88]
per-ex loss: 0.342571  [   32/   88]
per-ex loss: 0.477155  [   34/   88]
per-ex loss: 0.424942  [   36/   88]
per-ex loss: 0.428335  [   38/   88]
per-ex loss: 0.658010  [   40/   88]
per-ex loss: 0.504419  [   42/   88]
per-ex loss: 0.400628  [   44/   88]
per-ex loss: 0.485380  [   46/   88]
per-ex loss: 0.380864  [   48/   88]
per-ex loss: 0.394326  [   50/   88]
per-ex loss: 0.627352  [   52/   88]
per-ex loss: 0.562544  [   54/   88]
per-ex loss: 0.419892  [   56/   88]
per-ex loss: 0.555096  [   58/   88]
per-ex loss: 0.414372  [   60/   88]
per-ex loss: 0.475954  [   62/   88]
per-ex loss: 0.477812  [   64/   88]
per-ex loss: 0.499984  [   66/   88]
per-ex loss: 0.404506  [   68/   88]
per-ex loss: 0.491523  [   70/   88]
per-ex loss: 0.540217  [   72/   88]
per-ex loss: 0.494140  [   74/   88]
per-ex loss: 0.479832  [   76/   88]
per-ex loss: 0.398488  [   78/   88]
per-ex loss: 0.576102  [   80/   88]
per-ex loss: 0.492873  [   82/   88]
per-ex loss: 0.583250  [   84/   88]
per-ex loss: 0.411154  [   86/   88]
per-ex loss: 0.474727  [   88/   88]
Train Error: Avg loss: 0.46798835
validation Error: 
 Avg loss: 0.51720167 
 F1: 0.507968 
 Precision: 0.572656 
 Recall: 0.456410
 IoU: 0.340454

test Error: 
 Avg loss: 0.46715570 
 F1: 0.569310 
 Precision: 0.648678 
 Recall: 0.507246
 IoU: 0.397927

We have finished training iteration 75
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_73_.pth
per-ex loss: 0.425838  [    2/   88]
per-ex loss: 0.410506  [    4/   88]
per-ex loss: 0.481466  [    6/   88]
per-ex loss: 0.629827  [    8/   88]
per-ex loss: 0.383790  [   10/   88]
per-ex loss: 0.381943  [   12/   88]
per-ex loss: 0.479156  [   14/   88]
per-ex loss: 0.400577  [   16/   88]
per-ex loss: 0.382901  [   18/   88]
per-ex loss: 0.583323  [   20/   88]
per-ex loss: 0.582716  [   22/   88]
per-ex loss: 0.514915  [   24/   88]
per-ex loss: 0.375051  [   26/   88]
per-ex loss: 0.456274  [   28/   88]
per-ex loss: 0.540276  [   30/   88]
per-ex loss: 0.394804  [   32/   88]
per-ex loss: 0.415036  [   34/   88]
per-ex loss: 0.429123  [   36/   88]
per-ex loss: 0.403335  [   38/   88]
per-ex loss: 0.493546  [   40/   88]
per-ex loss: 0.453164  [   42/   88]
per-ex loss: 0.501736  [   44/   88]
per-ex loss: 0.372520  [   46/   88]
per-ex loss: 0.551354  [   48/   88]
per-ex loss: 0.613948  [   50/   88]
per-ex loss: 0.604124  [   52/   88]
per-ex loss: 0.494739  [   54/   88]
per-ex loss: 0.523965  [   56/   88]
per-ex loss: 0.390744  [   58/   88]
per-ex loss: 0.612696  [   60/   88]
per-ex loss: 0.511109  [   62/   88]
per-ex loss: 0.395803  [   64/   88]
per-ex loss: 0.588418  [   66/   88]
per-ex loss: 0.452690  [   68/   88]
per-ex loss: 0.505923  [   70/   88]
per-ex loss: 0.431299  [   72/   88]
per-ex loss: 0.522536  [   74/   88]
per-ex loss: 0.493795  [   76/   88]
per-ex loss: 0.531551  [   78/   88]
per-ex loss: 0.583887  [   80/   88]
per-ex loss: 0.421668  [   82/   88]
per-ex loss: 0.578302  [   84/   88]
per-ex loss: 0.464323  [   86/   88]
per-ex loss: 0.372905  [   88/   88]
Train Error: Avg loss: 0.48040003
validation Error: 
 Avg loss: 0.52761948 
 F1: 0.492082 
 Precision: 0.485421 
 Recall: 0.498928
 IoU: 0.326332

test Error: 
 Avg loss: 0.46896446 
 F1: 0.563491 
 Precision: 0.564075 
 Recall: 0.562909
 IoU: 0.392264

We have finished training iteration 76
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_74_.pth
per-ex loss: 0.431105  [    2/   88]
per-ex loss: 0.675022  [    4/   88]
per-ex loss: 0.604723  [    6/   88]
per-ex loss: 0.562300  [    8/   88]
per-ex loss: 0.483212  [   10/   88]
per-ex loss: 0.402999  [   12/   88]
per-ex loss: 0.544784  [   14/   88]
per-ex loss: 0.571760  [   16/   88]
per-ex loss: 0.390709  [   18/   88]
per-ex loss: 0.368766  [   20/   88]
per-ex loss: 0.369875  [   22/   88]
per-ex loss: 0.593941  [   24/   88]
per-ex loss: 0.424200  [   26/   88]
per-ex loss: 0.366570  [   28/   88]
per-ex loss: 0.488032  [   30/   88]
per-ex loss: 0.505700  [   32/   88]
per-ex loss: 0.414709  [   34/   88]
per-ex loss: 0.414618  [   36/   88]
per-ex loss: 0.475338  [   38/   88]
per-ex loss: 0.422802  [   40/   88]
per-ex loss: 0.344068  [   42/   88]
per-ex loss: 0.442890  [   44/   88]
per-ex loss: 0.376683  [   46/   88]
per-ex loss: 0.541339  [   48/   88]
per-ex loss: 0.578061  [   50/   88]
per-ex loss: 0.643970  [   52/   88]
per-ex loss: 0.488666  [   54/   88]
per-ex loss: 0.415802  [   56/   88]
per-ex loss: 0.472152  [   58/   88]
per-ex loss: 0.387426  [   60/   88]
per-ex loss: 0.426344  [   62/   88]
per-ex loss: 0.542531  [   64/   88]
per-ex loss: 0.445482  [   66/   88]
per-ex loss: 0.480792  [   68/   88]
per-ex loss: 0.513629  [   70/   88]
per-ex loss: 0.573705  [   72/   88]
per-ex loss: 0.379456  [   74/   88]
per-ex loss: 0.513055  [   76/   88]
per-ex loss: 0.392355  [   78/   88]
per-ex loss: 0.500050  [   80/   88]
per-ex loss: 0.513723  [   82/   88]
per-ex loss: 0.629551  [   84/   88]
per-ex loss: 0.398486  [   86/   88]
per-ex loss: 0.462214  [   88/   88]
Train Error: Avg loss: 0.47667255
validation Error: 
 Avg loss: 0.54178403 
 F1: 0.476350 
 Precision: 0.656823 
 Recall: 0.373676
 IoU: 0.312637

test Error: 
 Avg loss: 0.51400568 
 F1: 0.522039 
 Precision: 0.736938 
 Recall: 0.404176
 IoU: 0.353215

We have finished training iteration 77
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_75_.pth
per-ex loss: 0.532782  [    2/   88]
per-ex loss: 0.407701  [    4/   88]
per-ex loss: 0.357857  [    6/   88]
per-ex loss: 0.419016  [    8/   88]
per-ex loss: 0.589303  [   10/   88]
per-ex loss: 0.414995  [   12/   88]
per-ex loss: 0.438360  [   14/   88]
per-ex loss: 0.427298  [   16/   88]
per-ex loss: 0.498715  [   18/   88]
per-ex loss: 0.366932  [   20/   88]
per-ex loss: 0.479178  [   22/   88]
per-ex loss: 0.542460  [   24/   88]
per-ex loss: 0.419343  [   26/   88]
per-ex loss: 0.456152  [   28/   88]
per-ex loss: 0.355377  [   30/   88]
per-ex loss: 0.580290  [   32/   88]
per-ex loss: 0.444259  [   34/   88]
per-ex loss: 0.582808  [   36/   88]
per-ex loss: 0.569539  [   38/   88]
per-ex loss: 0.517322  [   40/   88]
per-ex loss: 0.433940  [   42/   88]
per-ex loss: 0.481292  [   44/   88]
per-ex loss: 0.444057  [   46/   88]
per-ex loss: 0.410846  [   48/   88]
per-ex loss: 0.408406  [   50/   88]
per-ex loss: 0.394808  [   52/   88]
per-ex loss: 0.607055  [   54/   88]
per-ex loss: 0.589955  [   56/   88]
per-ex loss: 0.459234  [   58/   88]
per-ex loss: 0.556032  [   60/   88]
per-ex loss: 0.639045  [   62/   88]
per-ex loss: 0.582212  [   64/   88]
per-ex loss: 0.462427  [   66/   88]
per-ex loss: 0.588420  [   68/   88]
per-ex loss: 0.370057  [   70/   88]
per-ex loss: 0.532698  [   72/   88]
per-ex loss: 0.395574  [   74/   88]
per-ex loss: 0.430784  [   76/   88]
per-ex loss: 0.507059  [   78/   88]
per-ex loss: 0.442996  [   80/   88]
per-ex loss: 0.584357  [   82/   88]
per-ex loss: 0.405707  [   84/   88]
per-ex loss: 0.380317  [   86/   88]
per-ex loss: 0.582278  [   88/   88]
Train Error: Avg loss: 0.47930094
validation Error: 
 Avg loss: 0.51442302 
 F1: 0.509090 
 Precision: 0.535473 
 Recall: 0.485184
 IoU: 0.341462

test Error: 
 Avg loss: 0.46315886 
 F1: 0.573955 
 Precision: 0.607497 
 Recall: 0.543923
 IoU: 0.402480

We have finished training iteration 78
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_76_.pth
per-ex loss: 0.402108  [    2/   88]
per-ex loss: 0.398969  [    4/   88]
per-ex loss: 0.427502  [    6/   88]
per-ex loss: 0.462053  [    8/   88]
per-ex loss: 0.373010  [   10/   88]
per-ex loss: 0.603697  [   12/   88]
per-ex loss: 0.408326  [   14/   88]
per-ex loss: 0.403030  [   16/   88]
per-ex loss: 0.516650  [   18/   88]
per-ex loss: 0.403844  [   20/   88]
per-ex loss: 0.375101  [   22/   88]
per-ex loss: 0.696883  [   24/   88]
per-ex loss: 0.423169  [   26/   88]
per-ex loss: 0.529594  [   28/   88]
per-ex loss: 0.509252  [   30/   88]
per-ex loss: 0.536106  [   32/   88]
per-ex loss: 0.519029  [   34/   88]
per-ex loss: 0.361915  [   36/   88]
per-ex loss: 0.636401  [   38/   88]
per-ex loss: 0.645762  [   40/   88]
per-ex loss: 0.399776  [   42/   88]
per-ex loss: 0.484885  [   44/   88]
per-ex loss: 0.387334  [   46/   88]
per-ex loss: 0.424130  [   48/   88]
per-ex loss: 0.472198  [   50/   88]
per-ex loss: 0.405628  [   52/   88]
per-ex loss: 0.553860  [   54/   88]
per-ex loss: 0.366875  [   56/   88]
per-ex loss: 0.504986  [   58/   88]
per-ex loss: 0.399235  [   60/   88]
per-ex loss: 0.477175  [   62/   88]
per-ex loss: 0.606699  [   64/   88]
per-ex loss: 0.506368  [   66/   88]
per-ex loss: 0.424121  [   68/   88]
per-ex loss: 0.429817  [   70/   88]
per-ex loss: 0.589475  [   72/   88]
per-ex loss: 0.561069  [   74/   88]
per-ex loss: 0.556654  [   76/   88]
per-ex loss: 0.375898  [   78/   88]
per-ex loss: 0.566722  [   80/   88]
per-ex loss: 0.404341  [   82/   88]
per-ex loss: 0.422078  [   84/   88]
per-ex loss: 0.401860  [   86/   88]
per-ex loss: 0.591010  [   88/   88]
Train Error: Avg loss: 0.47601352
validation Error: 
 Avg loss: 0.50217613 
 F1: 0.512495 
 Precision: 0.535516 
 Recall: 0.491371
 IoU: 0.344533

test Error: 
 Avg loss: 0.45940231 
 F1: 0.574729 
 Precision: 0.592492 
 Recall: 0.557999
 IoU: 0.403242

We have finished training iteration 79
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_77_.pth
per-ex loss: 0.429205  [    2/   88]
per-ex loss: 0.541163  [    4/   88]
per-ex loss: 0.396725  [    6/   88]
per-ex loss: 0.428565  [    8/   88]
per-ex loss: 0.615164  [   10/   88]
per-ex loss: 0.609775  [   12/   88]
per-ex loss: 0.591856  [   14/   88]
per-ex loss: 0.599455  [   16/   88]
per-ex loss: 0.568755  [   18/   88]
per-ex loss: 0.475026  [   20/   88]
per-ex loss: 0.506061  [   22/   88]
per-ex loss: 0.549623  [   24/   88]
per-ex loss: 0.500628  [   26/   88]
per-ex loss: 0.432553  [   28/   88]
per-ex loss: 0.439162  [   30/   88]
per-ex loss: 0.428679  [   32/   88]
per-ex loss: 0.394435  [   34/   88]
per-ex loss: 0.403707  [   36/   88]
per-ex loss: 0.465512  [   38/   88]
per-ex loss: 0.520882  [   40/   88]
per-ex loss: 0.426456  [   42/   88]
per-ex loss: 0.453019  [   44/   88]
per-ex loss: 0.350911  [   46/   88]
per-ex loss: 0.387807  [   48/   88]
per-ex loss: 0.456800  [   50/   88]
per-ex loss: 0.436637  [   52/   88]
per-ex loss: 0.443109  [   54/   88]
per-ex loss: 0.485782  [   56/   88]
per-ex loss: 0.346534  [   58/   88]
per-ex loss: 0.373324  [   60/   88]
per-ex loss: 0.418857  [   62/   88]
per-ex loss: 0.424711  [   64/   88]
per-ex loss: 0.555547  [   66/   88]
per-ex loss: 0.509250  [   68/   88]
per-ex loss: 0.647274  [   70/   88]
per-ex loss: 0.461228  [   72/   88]
per-ex loss: 0.466515  [   74/   88]
per-ex loss: 0.429076  [   76/   88]
per-ex loss: 0.429994  [   78/   88]
per-ex loss: 0.467725  [   80/   88]
per-ex loss: 0.611928  [   82/   88]
per-ex loss: 0.529851  [   84/   88]
per-ex loss: 0.551473  [   86/   88]
per-ex loss: 0.551170  [   88/   88]
Train Error: Avg loss: 0.47981615
validation Error: 
 Avg loss: 0.51671026 
 F1: 0.496835 
 Precision: 0.503739 
 Recall: 0.490117
 IoU: 0.330526

test Error: 
 Avg loss: 0.46550958 
 F1: 0.572572 
 Precision: 0.583281 
 Recall: 0.562249
 IoU: 0.401121

We have finished training iteration 80
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_78_.pth
per-ex loss: 0.455150  [    2/   88]
per-ex loss: 0.547717  [    4/   88]
per-ex loss: 0.419710  [    6/   88]
per-ex loss: 0.374302  [    8/   88]
per-ex loss: 0.366647  [   10/   88]
per-ex loss: 0.486265  [   12/   88]
per-ex loss: 0.452420  [   14/   88]
per-ex loss: 0.614436  [   16/   88]
per-ex loss: 0.373579  [   18/   88]
per-ex loss: 0.495815  [   20/   88]
per-ex loss: 0.440668  [   22/   88]
per-ex loss: 0.517943  [   24/   88]
per-ex loss: 0.348346  [   26/   88]
per-ex loss: 0.424259  [   28/   88]
per-ex loss: 0.502612  [   30/   88]
per-ex loss: 0.441723  [   32/   88]
per-ex loss: 0.484861  [   34/   88]
per-ex loss: 0.398171  [   36/   88]
per-ex loss: 0.573676  [   38/   88]
per-ex loss: 0.540662  [   40/   88]
per-ex loss: 0.505477  [   42/   88]
per-ex loss: 0.511279  [   44/   88]
per-ex loss: 0.619857  [   46/   88]
per-ex loss: 0.447400  [   48/   88]
per-ex loss: 0.529076  [   50/   88]
per-ex loss: 0.397904  [   52/   88]
per-ex loss: 0.440856  [   54/   88]
per-ex loss: 0.431447  [   56/   88]
per-ex loss: 0.453888  [   58/   88]
per-ex loss: 0.473333  [   60/   88]
per-ex loss: 0.546782  [   62/   88]
per-ex loss: 0.578224  [   64/   88]
per-ex loss: 0.370047  [   66/   88]
per-ex loss: 0.382912  [   68/   88]
per-ex loss: 0.515818  [   70/   88]
per-ex loss: 0.563169  [   72/   88]
per-ex loss: 0.538743  [   74/   88]
per-ex loss: 0.449178  [   76/   88]
per-ex loss: 0.370507  [   78/   88]
per-ex loss: 0.440608  [   80/   88]
per-ex loss: 0.409652  [   82/   88]
per-ex loss: 0.432320  [   84/   88]
per-ex loss: 0.561482  [   86/   88]
per-ex loss: 0.422056  [   88/   88]
Train Error: Avg loss: 0.46934040
validation Error: 
 Avg loss: 0.52647022 
 F1: 0.506366 
 Precision: 0.548887 
 Recall: 0.469959
 IoU: 0.339016

test Error: 
 Avg loss: 0.46575377 
 F1: 0.571923 
 Precision: 0.635867 
 Recall: 0.519665
 IoU: 0.400485

We have finished training iteration 81
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_57_.pth
per-ex loss: 0.395546  [    2/   88]
per-ex loss: 0.559038  [    4/   88]
per-ex loss: 0.391546  [    6/   88]
per-ex loss: 0.485300  [    8/   88]
per-ex loss: 0.629558  [   10/   88]
per-ex loss: 0.364467  [   12/   88]
per-ex loss: 0.430992  [   14/   88]
per-ex loss: 0.556505  [   16/   88]
per-ex loss: 0.521974  [   18/   88]
per-ex loss: 0.573789  [   20/   88]
per-ex loss: 0.536583  [   22/   88]
per-ex loss: 0.378895  [   24/   88]
per-ex loss: 0.428467  [   26/   88]
per-ex loss: 0.484844  [   28/   88]
per-ex loss: 0.542308  [   30/   88]
per-ex loss: 0.504103  [   32/   88]
per-ex loss: 0.453287  [   34/   88]
per-ex loss: 0.352315  [   36/   88]
per-ex loss: 0.576688  [   38/   88]
per-ex loss: 0.581668  [   40/   88]
per-ex loss: 0.428168  [   42/   88]
per-ex loss: 0.437483  [   44/   88]
per-ex loss: 0.486267  [   46/   88]
per-ex loss: 0.431597  [   48/   88]
per-ex loss: 0.397778  [   50/   88]
per-ex loss: 0.471807  [   52/   88]
per-ex loss: 0.402069  [   54/   88]
per-ex loss: 0.615943  [   56/   88]
per-ex loss: 0.487873  [   58/   88]
per-ex loss: 0.414711  [   60/   88]
per-ex loss: 0.649237  [   62/   88]
per-ex loss: 0.466555  [   64/   88]
per-ex loss: 0.424266  [   66/   88]
per-ex loss: 0.399276  [   68/   88]
per-ex loss: 0.485973  [   70/   88]
per-ex loss: 0.421853  [   72/   88]
per-ex loss: 0.369506  [   74/   88]
per-ex loss: 0.437050  [   76/   88]
per-ex loss: 0.419366  [   78/   88]
per-ex loss: 0.404957  [   80/   88]
per-ex loss: 0.571204  [   82/   88]
per-ex loss: 0.452439  [   84/   88]
per-ex loss: 0.435655  [   86/   88]
per-ex loss: 0.606581  [   88/   88]
Train Error: Avg loss: 0.47421560
validation Error: 
 Avg loss: 0.55419284 
 F1: 0.478139 
 Precision: 0.624363 
 Recall: 0.387409
 IoU: 0.314181

test Error: 
 Avg loss: 0.50759915 
 F1: 0.527855 
 Precision: 0.722694 
 Recall: 0.415764
 IoU: 0.358562

We have finished training iteration 82
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_80_.pth
per-ex loss: 0.561576  [    2/   88]
per-ex loss: 0.394690  [    4/   88]
per-ex loss: 0.458250  [    6/   88]
per-ex loss: 0.532587  [    8/   88]
per-ex loss: 0.587346  [   10/   88]
per-ex loss: 0.410532  [   12/   88]
per-ex loss: 0.410214  [   14/   88]
per-ex loss: 0.508709  [   16/   88]
per-ex loss: 0.640243  [   18/   88]
per-ex loss: 0.565820  [   20/   88]
per-ex loss: 0.639682  [   22/   88]
per-ex loss: 0.466072  [   24/   88]
per-ex loss: 0.559722  [   26/   88]
per-ex loss: 0.503350  [   28/   88]
per-ex loss: 0.398228  [   30/   88]
per-ex loss: 0.502738  [   32/   88]
per-ex loss: 0.440765  [   34/   88]
per-ex loss: 0.399671  [   36/   88]
per-ex loss: 0.494182  [   38/   88]
per-ex loss: 0.486506  [   40/   88]
per-ex loss: 0.575557  [   42/   88]
per-ex loss: 0.461517  [   44/   88]
per-ex loss: 0.562986  [   46/   88]
per-ex loss: 0.431791  [   48/   88]
per-ex loss: 0.444219  [   50/   88]
per-ex loss: 0.474478  [   52/   88]
per-ex loss: 0.412278  [   54/   88]
per-ex loss: 0.475244  [   56/   88]
per-ex loss: 0.555135  [   58/   88]
per-ex loss: 0.393789  [   60/   88]
per-ex loss: 0.415775  [   62/   88]
per-ex loss: 0.386425  [   64/   88]
per-ex loss: 0.362574  [   66/   88]
per-ex loss: 0.493655  [   68/   88]
per-ex loss: 0.475860  [   70/   88]
per-ex loss: 0.475636  [   72/   88]
per-ex loss: 0.626659  [   74/   88]
per-ex loss: 0.482016  [   76/   88]
per-ex loss: 0.451491  [   78/   88]
per-ex loss: 0.432731  [   80/   88]
per-ex loss: 0.408057  [   82/   88]
per-ex loss: 0.534939  [   84/   88]
per-ex loss: 0.478585  [   86/   88]
per-ex loss: 0.425801  [   88/   88]
Train Error: Avg loss: 0.48177451
validation Error: 
 Avg loss: 0.50983277 
 F1: 0.516174 
 Precision: 0.600872 
 Recall: 0.452404
 IoU: 0.347867

test Error: 
 Avg loss: 0.47732154 
 F1: 0.558938 
 Precision: 0.642600 
 Recall: 0.494551
 IoU: 0.387865

We have finished training iteration 83
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_81_.pth
per-ex loss: 0.552926  [    2/   88]
per-ex loss: 0.620298  [    4/   88]
per-ex loss: 0.395712  [    6/   88]
per-ex loss: 0.614505  [    8/   88]
per-ex loss: 0.410399  [   10/   88]
per-ex loss: 0.414205  [   12/   88]
per-ex loss: 0.576652  [   14/   88]
per-ex loss: 0.382871  [   16/   88]
per-ex loss: 0.502152  [   18/   88]
per-ex loss: 0.453621  [   20/   88]
per-ex loss: 0.409185  [   22/   88]
per-ex loss: 0.471630  [   24/   88]
per-ex loss: 0.482333  [   26/   88]
per-ex loss: 0.531159  [   28/   88]
per-ex loss: 0.485329  [   30/   88]
per-ex loss: 0.416406  [   32/   88]
per-ex loss: 0.476611  [   34/   88]
per-ex loss: 0.439654  [   36/   88]
per-ex loss: 0.479607  [   38/   88]
per-ex loss: 0.391582  [   40/   88]
per-ex loss: 0.507448  [   42/   88]
per-ex loss: 0.400426  [   44/   88]
per-ex loss: 0.605219  [   46/   88]
per-ex loss: 0.433755  [   48/   88]
per-ex loss: 0.412051  [   50/   88]
per-ex loss: 0.496405  [   52/   88]
per-ex loss: 0.405671  [   54/   88]
per-ex loss: 0.407861  [   56/   88]
per-ex loss: 0.378481  [   58/   88]
per-ex loss: 0.541075  [   60/   88]
per-ex loss: 0.647725  [   62/   88]
per-ex loss: 0.436809  [   64/   88]
per-ex loss: 0.384489  [   66/   88]
per-ex loss: 0.382909  [   68/   88]
per-ex loss: 0.468131  [   70/   88]
per-ex loss: 0.463858  [   72/   88]
per-ex loss: 0.360379  [   74/   88]
per-ex loss: 0.551828  [   76/   88]
per-ex loss: 0.412014  [   78/   88]
per-ex loss: 0.414183  [   80/   88]
per-ex loss: 0.681834  [   82/   88]
per-ex loss: 0.453149  [   84/   88]
per-ex loss: 0.486694  [   86/   88]
per-ex loss: 0.422053  [   88/   88]
Train Error: Avg loss: 0.46957463
validation Error: 
 Avg loss: 0.51557174 
 F1: 0.505112 
 Precision: 0.648605 
 Recall: 0.413608
 IoU: 0.337893

test Error: 
 Avg loss: 0.47663610 
 F1: 0.561609 
 Precision: 0.710357 
 Recall: 0.464370
 IoU: 0.390442

We have finished training iteration 84
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_82_.pth
per-ex loss: 0.388699  [    2/   88]
per-ex loss: 0.427730  [    4/   88]
per-ex loss: 0.518611  [    6/   88]
per-ex loss: 0.398440  [    8/   88]
per-ex loss: 0.587297  [   10/   88]
per-ex loss: 0.617428  [   12/   88]
per-ex loss: 0.414280  [   14/   88]
per-ex loss: 0.430446  [   16/   88]
per-ex loss: 0.476002  [   18/   88]
per-ex loss: 0.409204  [   20/   88]
per-ex loss: 0.454131  [   22/   88]
per-ex loss: 0.553162  [   24/   88]
per-ex loss: 0.439536  [   26/   88]
per-ex loss: 0.492530  [   28/   88]
per-ex loss: 0.446854  [   30/   88]
per-ex loss: 0.545267  [   32/   88]
per-ex loss: 0.398723  [   34/   88]
per-ex loss: 0.364594  [   36/   88]
per-ex loss: 0.494865  [   38/   88]
per-ex loss: 0.365324  [   40/   88]
per-ex loss: 0.424887  [   42/   88]
per-ex loss: 0.438617  [   44/   88]
per-ex loss: 0.506637  [   46/   88]
per-ex loss: 0.401672  [   48/   88]
per-ex loss: 0.413884  [   50/   88]
per-ex loss: 0.487652  [   52/   88]
per-ex loss: 0.572493  [   54/   88]
per-ex loss: 0.480864  [   56/   88]
per-ex loss: 0.425048  [   58/   88]
per-ex loss: 0.519173  [   60/   88]
per-ex loss: 0.606395  [   62/   88]
per-ex loss: 0.434984  [   64/   88]
per-ex loss: 0.529148  [   66/   88]
per-ex loss: 0.496749  [   68/   88]
per-ex loss: 0.434066  [   70/   88]
per-ex loss: 0.484093  [   72/   88]
per-ex loss: 0.472937  [   74/   88]
per-ex loss: 0.358225  [   76/   88]
per-ex loss: 0.592038  [   78/   88]
per-ex loss: 0.364113  [   80/   88]
per-ex loss: 0.587942  [   82/   88]
per-ex loss: 0.531129  [   84/   88]
per-ex loss: 0.414748  [   86/   88]
per-ex loss: 0.554738  [   88/   88]
Train Error: Avg loss: 0.47171265
validation Error: 
 Avg loss: 0.52296806 
 F1: 0.510310 
 Precision: 0.564396 
 Recall: 0.465684
 IoU: 0.342562

test Error: 
 Avg loss: 0.46153378 
 F1: 0.575388 
 Precision: 0.641932 
 Recall: 0.521345
 IoU: 0.403891

We have finished training iteration 85
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_83_.pth
per-ex loss: 0.360754  [    2/   88]
per-ex loss: 0.390900  [    4/   88]
per-ex loss: 0.373794  [    6/   88]
per-ex loss: 0.594748  [    8/   88]
per-ex loss: 0.430858  [   10/   88]
per-ex loss: 0.591653  [   12/   88]
per-ex loss: 0.489557  [   14/   88]
per-ex loss: 0.566435  [   16/   88]
per-ex loss: 0.549617  [   18/   88]
per-ex loss: 0.620744  [   20/   88]
per-ex loss: 0.628655  [   22/   88]
per-ex loss: 0.413288  [   24/   88]
per-ex loss: 0.393981  [   26/   88]
per-ex loss: 0.466784  [   28/   88]
per-ex loss: 0.581307  [   30/   88]
per-ex loss: 0.587406  [   32/   88]
per-ex loss: 0.440447  [   34/   88]
per-ex loss: 0.389644  [   36/   88]
per-ex loss: 0.517017  [   38/   88]
per-ex loss: 0.480990  [   40/   88]
per-ex loss: 0.394654  [   42/   88]
per-ex loss: 0.420654  [   44/   88]
per-ex loss: 0.342683  [   46/   88]
per-ex loss: 0.657649  [   48/   88]
per-ex loss: 0.431886  [   50/   88]
per-ex loss: 0.428706  [   52/   88]
per-ex loss: 0.429296  [   54/   88]
per-ex loss: 0.694280  [   56/   88]
per-ex loss: 0.484412  [   58/   88]
per-ex loss: 0.390619  [   60/   88]
per-ex loss: 0.412881  [   62/   88]
per-ex loss: 0.386468  [   64/   88]
per-ex loss: 0.471451  [   66/   88]
per-ex loss: 0.613199  [   68/   88]
per-ex loss: 0.587321  [   70/   88]
per-ex loss: 0.438760  [   72/   88]
per-ex loss: 0.445161  [   74/   88]
per-ex loss: 0.460963  [   76/   88]
per-ex loss: 0.498264  [   78/   88]
per-ex loss: 0.452845  [   80/   88]
per-ex loss: 0.447522  [   82/   88]
per-ex loss: 0.378386  [   84/   88]
per-ex loss: 0.541490  [   86/   88]
per-ex loss: 0.374982  [   88/   88]
Train Error: Avg loss: 0.47847983
validation Error: 
 Avg loss: 0.51713652 
 F1: 0.506781 
 Precision: 0.497021 
 Recall: 0.516932
 IoU: 0.339388

test Error: 
 Avg loss: 0.46351236 
 F1: 0.568626 
 Precision: 0.568839 
 Recall: 0.568414
 IoU: 0.397259

We have finished training iteration 86
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_84_.pth
per-ex loss: 0.568845  [    2/   88]
per-ex loss: 0.425520  [    4/   88]
per-ex loss: 0.453354  [    6/   88]
per-ex loss: 0.399421  [    8/   88]
per-ex loss: 0.518663  [   10/   88]
per-ex loss: 0.383410  [   12/   88]
per-ex loss: 0.377610  [   14/   88]
per-ex loss: 0.360507  [   16/   88]
per-ex loss: 0.465355  [   18/   88]
per-ex loss: 0.496356  [   20/   88]
per-ex loss: 0.406396  [   22/   88]
per-ex loss: 0.597312  [   24/   88]
per-ex loss: 0.389744  [   26/   88]
per-ex loss: 0.545123  [   28/   88]
per-ex loss: 0.647191  [   30/   88]
per-ex loss: 0.448013  [   32/   88]
per-ex loss: 0.466341  [   34/   88]
per-ex loss: 0.550265  [   36/   88]
per-ex loss: 0.474223  [   38/   88]
per-ex loss: 0.460656  [   40/   88]
per-ex loss: 0.492230  [   42/   88]
per-ex loss: 0.398693  [   44/   88]
per-ex loss: 0.441711  [   46/   88]
per-ex loss: 0.453912  [   48/   88]
per-ex loss: 0.480605  [   50/   88]
per-ex loss: 0.429281  [   52/   88]
per-ex loss: 0.505153  [   54/   88]
per-ex loss: 0.428016  [   56/   88]
per-ex loss: 0.433136  [   58/   88]
per-ex loss: 0.451412  [   60/   88]
per-ex loss: 0.370014  [   62/   88]
per-ex loss: 0.500562  [   64/   88]
per-ex loss: 0.403392  [   66/   88]
per-ex loss: 0.506806  [   68/   88]
per-ex loss: 0.526134  [   70/   88]
per-ex loss: 0.456144  [   72/   88]
per-ex loss: 0.453371  [   74/   88]
per-ex loss: 0.555644  [   76/   88]
per-ex loss: 0.474600  [   78/   88]
per-ex loss: 0.407278  [   80/   88]
per-ex loss: 0.417243  [   82/   88]
per-ex loss: 0.596910  [   84/   88]
per-ex loss: 0.441271  [   86/   88]
per-ex loss: 0.344975  [   88/   88]
Train Error: Avg loss: 0.46369999
validation Error: 
 Avg loss: 0.51482360 
 F1: 0.515864 
 Precision: 0.568642 
 Recall: 0.472051
 IoU: 0.347585

test Error: 
 Avg loss: 0.46814593 
 F1: 0.564853 
 Precision: 0.625837 
 Recall: 0.514698
 IoU: 0.393585

We have finished training iteration 87
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_85_.pth
per-ex loss: 0.588368  [    2/   88]
per-ex loss: 0.392542  [    4/   88]
per-ex loss: 0.584725  [    6/   88]
per-ex loss: 0.444368  [    8/   88]
per-ex loss: 0.426128  [   10/   88]
per-ex loss: 0.385723  [   12/   88]
per-ex loss: 0.539705  [   14/   88]
per-ex loss: 0.408003  [   16/   88]
per-ex loss: 0.485584  [   18/   88]
per-ex loss: 0.430639  [   20/   88]
per-ex loss: 0.460007  [   22/   88]
per-ex loss: 0.603244  [   24/   88]
per-ex loss: 0.462714  [   26/   88]
per-ex loss: 0.445041  [   28/   88]
per-ex loss: 0.641819  [   30/   88]
per-ex loss: 0.490497  [   32/   88]
per-ex loss: 0.403919  [   34/   88]
per-ex loss: 0.405220  [   36/   88]
per-ex loss: 0.378804  [   38/   88]
per-ex loss: 0.437440  [   40/   88]
per-ex loss: 0.420041  [   42/   88]
per-ex loss: 0.379217  [   44/   88]
per-ex loss: 0.465571  [   46/   88]
per-ex loss: 0.505257  [   48/   88]
per-ex loss: 0.350855  [   50/   88]
per-ex loss: 0.564332  [   52/   88]
per-ex loss: 0.618946  [   54/   88]
per-ex loss: 0.401803  [   56/   88]
per-ex loss: 0.501340  [   58/   88]
per-ex loss: 0.524062  [   60/   88]
per-ex loss: 0.506427  [   62/   88]
per-ex loss: 0.423404  [   64/   88]
per-ex loss: 0.378527  [   66/   88]
per-ex loss: 0.504717  [   68/   88]
per-ex loss: 0.454858  [   70/   88]
per-ex loss: 0.447032  [   72/   88]
per-ex loss: 0.413379  [   74/   88]
per-ex loss: 0.359005  [   76/   88]
per-ex loss: 0.403938  [   78/   88]
per-ex loss: 0.474276  [   80/   88]
per-ex loss: 0.511873  [   82/   88]
per-ex loss: 0.525705  [   84/   88]
per-ex loss: 0.626774  [   86/   88]
per-ex loss: 0.391645  [   88/   88]
Train Error: Avg loss: 0.46744257
validation Error: 
 Avg loss: 0.50630181 
 F1: 0.516964 
 Precision: 0.579515 
 Recall: 0.466601
 IoU: 0.348585

test Error: 
 Avg loss: 0.46062860 
 F1: 0.576161 
 Precision: 0.634790 
 Recall: 0.527447
 IoU: 0.404654

We have finished training iteration 88
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_86_.pth
per-ex loss: 0.521208  [    2/   88]
per-ex loss: 0.583974  [    4/   88]
per-ex loss: 0.414098  [    6/   88]
per-ex loss: 0.374016  [    8/   88]
per-ex loss: 0.476869  [   10/   88]
per-ex loss: 0.355544  [   12/   88]
per-ex loss: 0.417711  [   14/   88]
per-ex loss: 0.608698  [   16/   88]
per-ex loss: 0.562187  [   18/   88]
per-ex loss: 0.445859  [   20/   88]
per-ex loss: 0.429816  [   22/   88]
per-ex loss: 0.361537  [   24/   88]
per-ex loss: 0.359159  [   26/   88]
per-ex loss: 0.352265  [   28/   88]
per-ex loss: 0.439211  [   30/   88]
per-ex loss: 0.428280  [   32/   88]
per-ex loss: 0.447211  [   34/   88]
per-ex loss: 0.394076  [   36/   88]
per-ex loss: 0.394294  [   38/   88]
per-ex loss: 0.658270  [   40/   88]
per-ex loss: 0.392812  [   42/   88]
per-ex loss: 0.581659  [   44/   88]
per-ex loss: 0.641761  [   46/   88]
per-ex loss: 0.417356  [   48/   88]
per-ex loss: 0.506282  [   50/   88]
per-ex loss: 0.449200  [   52/   88]
per-ex loss: 0.440193  [   54/   88]
per-ex loss: 0.479599  [   56/   88]
per-ex loss: 0.489978  [   58/   88]
per-ex loss: 0.465298  [   60/   88]
per-ex loss: 0.505179  [   62/   88]
per-ex loss: 0.549662  [   64/   88]
per-ex loss: 0.499260  [   66/   88]
per-ex loss: 0.657781  [   68/   88]
per-ex loss: 0.409339  [   70/   88]
per-ex loss: 0.496090  [   72/   88]
per-ex loss: 0.420788  [   74/   88]
per-ex loss: 0.473364  [   76/   88]
per-ex loss: 0.560778  [   78/   88]
per-ex loss: 0.503196  [   80/   88]
per-ex loss: 0.382620  [   82/   88]
per-ex loss: 0.453779  [   84/   88]
per-ex loss: 0.412183  [   86/   88]
per-ex loss: 0.409536  [   88/   88]
Train Error: Avg loss: 0.46868124
validation Error: 
 Avg loss: 0.50889911 
 F1: 0.504192 
 Precision: 0.679194 
 Recall: 0.400896
 IoU: 0.337070

test Error: 
 Avg loss: 0.49079197 
 F1: 0.544509 
 Precision: 0.732671 
 Recall: 0.433245
 IoU: 0.374107

We have finished training iteration 89
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_87_.pth
per-ex loss: 0.401393  [    2/   88]
per-ex loss: 0.406190  [    4/   88]
per-ex loss: 0.555047  [    6/   88]
per-ex loss: 0.468982  [    8/   88]
per-ex loss: 0.400512  [   10/   88]
per-ex loss: 0.624715  [   12/   88]
per-ex loss: 0.632084  [   14/   88]
per-ex loss: 0.372662  [   16/   88]
per-ex loss: 0.500502  [   18/   88]
per-ex loss: 0.487650  [   20/   88]
per-ex loss: 0.588943  [   22/   88]
per-ex loss: 0.365984  [   24/   88]
per-ex loss: 0.508602  [   26/   88]
per-ex loss: 0.402781  [   28/   88]
per-ex loss: 0.432310  [   30/   88]
per-ex loss: 0.355201  [   32/   88]
per-ex loss: 0.368437  [   34/   88]
per-ex loss: 0.537135  [   36/   88]
per-ex loss: 0.429033  [   38/   88]
per-ex loss: 0.558268  [   40/   88]
per-ex loss: 0.380129  [   42/   88]
per-ex loss: 0.391251  [   44/   88]
per-ex loss: 0.506181  [   46/   88]
per-ex loss: 0.417629  [   48/   88]
per-ex loss: 0.496231  [   50/   88]
per-ex loss: 0.500164  [   52/   88]
per-ex loss: 0.360395  [   54/   88]
per-ex loss: 0.521258  [   56/   88]
per-ex loss: 0.648658  [   58/   88]
per-ex loss: 0.472853  [   60/   88]
per-ex loss: 0.500590  [   62/   88]
per-ex loss: 0.489867  [   64/   88]
per-ex loss: 0.484839  [   66/   88]
per-ex loss: 0.448026  [   68/   88]
per-ex loss: 0.399956  [   70/   88]
per-ex loss: 0.502966  [   72/   88]
per-ex loss: 0.497602  [   74/   88]
per-ex loss: 0.425078  [   76/   88]
per-ex loss: 0.545894  [   78/   88]
per-ex loss: 0.465548  [   80/   88]
per-ex loss: 0.603986  [   82/   88]
per-ex loss: 0.432633  [   84/   88]
per-ex loss: 0.516709  [   86/   88]
per-ex loss: 0.439090  [   88/   88]
Train Error: Avg loss: 0.47372650
validation Error: 
 Avg loss: 0.52553271 
 F1: 0.485607 
 Precision: 0.445847 
 Recall: 0.533153
 IoU: 0.320661

test Error: 
 Avg loss: 0.46989692 
 F1: 0.560826 
 Precision: 0.520582 
 Recall: 0.607814
 IoU: 0.389686

We have finished training iteration 90
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_88_.pth
per-ex loss: 0.564378  [    2/   88]
per-ex loss: 0.481872  [    4/   88]
per-ex loss: 0.435174  [    6/   88]
per-ex loss: 0.532812  [    8/   88]
per-ex loss: 0.501931  [   10/   88]
per-ex loss: 0.557515  [   12/   88]
per-ex loss: 0.508739  [   14/   88]
per-ex loss: 0.596503  [   16/   88]
per-ex loss: 0.460743  [   18/   88]
per-ex loss: 0.465124  [   20/   88]
per-ex loss: 0.445500  [   22/   88]
per-ex loss: 0.478517  [   24/   88]
per-ex loss: 0.397201  [   26/   88]
per-ex loss: 0.450993  [   28/   88]
per-ex loss: 0.446888  [   30/   88]
per-ex loss: 0.620479  [   32/   88]
per-ex loss: 0.664042  [   34/   88]
per-ex loss: 0.527796  [   36/   88]
per-ex loss: 0.579076  [   38/   88]
per-ex loss: 0.483649  [   40/   88]
per-ex loss: 0.565432  [   42/   88]
per-ex loss: 0.394907  [   44/   88]
per-ex loss: 0.376736  [   46/   88]
per-ex loss: 0.437934  [   48/   88]
per-ex loss: 0.426695  [   50/   88]
per-ex loss: 0.423620  [   52/   88]
per-ex loss: 0.350085  [   54/   88]
per-ex loss: 0.576682  [   56/   88]
per-ex loss: 0.461201  [   58/   88]
per-ex loss: 0.342438  [   60/   88]
per-ex loss: 0.576731  [   62/   88]
per-ex loss: 0.460678  [   64/   88]
per-ex loss: 0.451773  [   66/   88]
per-ex loss: 0.363957  [   68/   88]
per-ex loss: 0.463955  [   70/   88]
per-ex loss: 0.446014  [   72/   88]
per-ex loss: 0.445650  [   74/   88]
per-ex loss: 0.361327  [   76/   88]
per-ex loss: 0.422989  [   78/   88]
per-ex loss: 0.362964  [   80/   88]
per-ex loss: 0.520454  [   82/   88]
per-ex loss: 0.365624  [   84/   88]
per-ex loss: 0.493378  [   86/   88]
per-ex loss: 0.385783  [   88/   88]
Train Error: Avg loss: 0.46990773
validation Error: 
 Avg loss: 0.51331282 
 F1: 0.510796 
 Precision: 0.632577 
 Recall: 0.428334
 IoU: 0.342999

test Error: 
 Avg loss: 0.47140876 
 F1: 0.566476 
 Precision: 0.689956 
 Recall: 0.480485
 IoU: 0.395164

We have finished training iteration 91
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_89_.pth
per-ex loss: 0.390015  [    2/   88]
per-ex loss: 0.483545  [    4/   88]
per-ex loss: 0.352118  [    6/   88]
per-ex loss: 0.392463  [    8/   88]
per-ex loss: 0.365880  [   10/   88]
per-ex loss: 0.485741  [   12/   88]
per-ex loss: 0.607332  [   14/   88]
per-ex loss: 0.429096  [   16/   88]
per-ex loss: 0.416111  [   18/   88]
per-ex loss: 0.550756  [   20/   88]
per-ex loss: 0.400912  [   22/   88]
per-ex loss: 0.417998  [   24/   88]
per-ex loss: 0.399583  [   26/   88]
per-ex loss: 0.357133  [   28/   88]
per-ex loss: 0.368878  [   30/   88]
per-ex loss: 0.518772  [   32/   88]
per-ex loss: 0.568405  [   34/   88]
per-ex loss: 0.373334  [   36/   88]
per-ex loss: 0.548865  [   38/   88]
per-ex loss: 0.599507  [   40/   88]
per-ex loss: 0.464401  [   42/   88]
per-ex loss: 0.576087  [   44/   88]
per-ex loss: 0.474535  [   46/   88]
per-ex loss: 0.454257  [   48/   88]
per-ex loss: 0.539242  [   50/   88]
per-ex loss: 0.464490  [   52/   88]
per-ex loss: 0.410492  [   54/   88]
per-ex loss: 0.470299  [   56/   88]
per-ex loss: 0.431179  [   58/   88]
per-ex loss: 0.500116  [   60/   88]
per-ex loss: 0.530023  [   62/   88]
per-ex loss: 0.384245  [   64/   88]
per-ex loss: 0.434417  [   66/   88]
per-ex loss: 0.471652  [   68/   88]
per-ex loss: 0.480731  [   70/   88]
per-ex loss: 0.423438  [   72/   88]
per-ex loss: 0.480518  [   74/   88]
per-ex loss: 0.387847  [   76/   88]
per-ex loss: 0.488544  [   78/   88]
per-ex loss: 0.420256  [   80/   88]
per-ex loss: 0.453452  [   82/   88]
per-ex loss: 0.417190  [   84/   88]
per-ex loss: 0.388242  [   86/   88]
per-ex loss: 0.541224  [   88/   88]
Train Error: Avg loss: 0.45712094
validation Error: 
 Avg loss: 0.52788566 
 F1: 0.489003 
 Precision: 0.537355 
 Recall: 0.448634
 IoU: 0.323629

test Error: 
 Avg loss: 0.47597593 
 F1: 0.558573 
 Precision: 0.644428 
 Recall: 0.492905
 IoU: 0.387514

We have finished training iteration 92
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_90_.pth
per-ex loss: 0.648885  [    2/   88]
per-ex loss: 0.581933  [    4/   88]
per-ex loss: 0.428288  [    6/   88]
per-ex loss: 0.419091  [    8/   88]
per-ex loss: 0.440382  [   10/   88]
per-ex loss: 0.475804  [   12/   88]
per-ex loss: 0.395788  [   14/   88]
per-ex loss: 0.437030  [   16/   88]
per-ex loss: 0.467282  [   18/   88]
per-ex loss: 0.564611  [   20/   88]
per-ex loss: 0.409888  [   22/   88]
per-ex loss: 0.522551  [   24/   88]
per-ex loss: 0.366878  [   26/   88]
per-ex loss: 0.409721  [   28/   88]
per-ex loss: 0.538051  [   30/   88]
per-ex loss: 0.446259  [   32/   88]
per-ex loss: 0.480691  [   34/   88]
per-ex loss: 0.428436  [   36/   88]
per-ex loss: 0.375799  [   38/   88]
per-ex loss: 0.410496  [   40/   88]
per-ex loss: 0.342495  [   42/   88]
per-ex loss: 0.369644  [   44/   88]
per-ex loss: 0.458779  [   46/   88]
per-ex loss: 0.518578  [   48/   88]
per-ex loss: 0.427487  [   50/   88]
per-ex loss: 0.462313  [   52/   88]
per-ex loss: 0.620434  [   54/   88]
per-ex loss: 0.512787  [   56/   88]
per-ex loss: 0.389930  [   58/   88]
per-ex loss: 0.370134  [   60/   88]
per-ex loss: 0.559659  [   62/   88]
per-ex loss: 0.479324  [   64/   88]
per-ex loss: 0.536731  [   66/   88]
per-ex loss: 0.377552  [   68/   88]
per-ex loss: 0.410172  [   70/   88]
per-ex loss: 0.452091  [   72/   88]
per-ex loss: 0.388203  [   74/   88]
per-ex loss: 0.467875  [   76/   88]
per-ex loss: 0.602668  [   78/   88]
per-ex loss: 0.523762  [   80/   88]
per-ex loss: 0.575725  [   82/   88]
per-ex loss: 0.406771  [   84/   88]
per-ex loss: 0.426246  [   86/   88]
per-ex loss: 0.401352  [   88/   88]
Train Error: Avg loss: 0.46201311
validation Error: 
 Avg loss: 0.52065241 
 F1: 0.511665 
 Precision: 0.620972 
 Recall: 0.435080
 IoU: 0.343784

test Error: 
 Avg loss: 0.47087163 
 F1: 0.565815 
 Precision: 0.682322 
 Recall: 0.483293
 IoU: 0.394520

We have finished training iteration 93
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_91_.pth
per-ex loss: 0.443108  [    2/   88]
per-ex loss: 0.405753  [    4/   88]
per-ex loss: 0.467326  [    6/   88]
per-ex loss: 0.615865  [    8/   88]
per-ex loss: 0.422637  [   10/   88]
per-ex loss: 0.417930  [   12/   88]
per-ex loss: 0.471071  [   14/   88]
per-ex loss: 0.511181  [   16/   88]
per-ex loss: 0.460969  [   18/   88]
per-ex loss: 0.452994  [   20/   88]
per-ex loss: 0.444062  [   22/   88]
per-ex loss: 0.403303  [   24/   88]
per-ex loss: 0.545128  [   26/   88]
per-ex loss: 0.448304  [   28/   88]
per-ex loss: 0.546518  [   30/   88]
per-ex loss: 0.387381  [   32/   88]
per-ex loss: 0.423572  [   34/   88]
per-ex loss: 0.387357  [   36/   88]
per-ex loss: 0.590174  [   38/   88]
per-ex loss: 0.408664  [   40/   88]
per-ex loss: 0.608888  [   42/   88]
per-ex loss: 0.492569  [   44/   88]
per-ex loss: 0.456042  [   46/   88]
per-ex loss: 0.403903  [   48/   88]
per-ex loss: 0.429630  [   50/   88]
per-ex loss: 0.364854  [   52/   88]
per-ex loss: 0.389761  [   54/   88]
per-ex loss: 0.501507  [   56/   88]
per-ex loss: 0.419440  [   58/   88]
per-ex loss: 0.554728  [   60/   88]
per-ex loss: 0.566262  [   62/   88]
per-ex loss: 0.415812  [   64/   88]
per-ex loss: 0.380433  [   66/   88]
per-ex loss: 0.630919  [   68/   88]
per-ex loss: 0.389668  [   70/   88]
per-ex loss: 0.610589  [   72/   88]
per-ex loss: 0.617865  [   74/   88]
per-ex loss: 0.380628  [   76/   88]
per-ex loss: 0.461374  [   78/   88]
per-ex loss: 0.382634  [   80/   88]
per-ex loss: 0.483700  [   82/   88]
per-ex loss: 0.463723  [   84/   88]
per-ex loss: 0.388552  [   86/   88]
per-ex loss: 0.502672  [   88/   88]
Train Error: Avg loss: 0.46703299
validation Error: 
 Avg loss: 0.52347081 
 F1: 0.504901 
 Precision: 0.645921 
 Recall: 0.414423
 IoU: 0.337704

test Error: 
 Avg loss: 0.48883127 
 F1: 0.546420 
 Precision: 0.710836 
 Recall: 0.443776
 IoU: 0.375914

We have finished training iteration 94
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_92_.pth
per-ex loss: 0.430511  [    2/   88]
per-ex loss: 0.445941  [    4/   88]
per-ex loss: 0.490976  [    6/   88]
per-ex loss: 0.435432  [    8/   88]
per-ex loss: 0.411933  [   10/   88]
per-ex loss: 0.378500  [   12/   88]
per-ex loss: 0.374286  [   14/   88]
per-ex loss: 0.449955  [   16/   88]
per-ex loss: 0.547021  [   18/   88]
per-ex loss: 0.345346  [   20/   88]
per-ex loss: 0.478426  [   22/   88]
per-ex loss: 0.477379  [   24/   88]
per-ex loss: 0.473209  [   26/   88]
per-ex loss: 0.510373  [   28/   88]
per-ex loss: 0.636482  [   30/   88]
per-ex loss: 0.374718  [   32/   88]
per-ex loss: 0.424882  [   34/   88]
per-ex loss: 0.437518  [   36/   88]
per-ex loss: 0.433019  [   38/   88]
per-ex loss: 0.401995  [   40/   88]
per-ex loss: 0.502294  [   42/   88]
per-ex loss: 0.551615  [   44/   88]
per-ex loss: 0.429463  [   46/   88]
per-ex loss: 0.393898  [   48/   88]
per-ex loss: 0.640757  [   50/   88]
per-ex loss: 0.646742  [   52/   88]
per-ex loss: 0.502860  [   54/   88]
per-ex loss: 0.432100  [   56/   88]
per-ex loss: 0.416782  [   58/   88]
per-ex loss: 0.548586  [   60/   88]
per-ex loss: 0.485126  [   62/   88]
per-ex loss: 0.495831  [   64/   88]
per-ex loss: 0.425934  [   66/   88]
per-ex loss: 0.435888  [   68/   88]
per-ex loss: 0.425694  [   70/   88]
per-ex loss: 0.577885  [   72/   88]
per-ex loss: 0.455061  [   74/   88]
per-ex loss: 0.494463  [   76/   88]
per-ex loss: 0.374117  [   78/   88]
per-ex loss: 0.390994  [   80/   88]
per-ex loss: 0.409360  [   82/   88]
per-ex loss: 0.510905  [   84/   88]
per-ex loss: 0.527244  [   86/   88]
per-ex loss: 0.512804  [   88/   88]
Train Error: Avg loss: 0.46691611
validation Error: 
 Avg loss: 0.55145010 
 F1: 0.467612 
 Precision: 0.509667 
 Recall: 0.431969
 IoU: 0.305153

test Error: 
 Avg loss: 0.48830527 
 F1: 0.545729 
 Precision: 0.656154 
 Recall: 0.467118
 IoU: 0.375260

We have finished training iteration 95
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_93_.pth
per-ex loss: 0.438628  [    2/   88]
per-ex loss: 0.473503  [    4/   88]
per-ex loss: 0.621625  [    6/   88]
per-ex loss: 0.500240  [    8/   88]
per-ex loss: 0.523135  [   10/   88]
per-ex loss: 0.645403  [   12/   88]
per-ex loss: 0.446085  [   14/   88]
per-ex loss: 0.410105  [   16/   88]
per-ex loss: 0.395647  [   18/   88]
per-ex loss: 0.596393  [   20/   88]
per-ex loss: 0.414666  [   22/   88]
per-ex loss: 0.473272  [   24/   88]
per-ex loss: 0.389900  [   26/   88]
per-ex loss: 0.403143  [   28/   88]
per-ex loss: 0.464813  [   30/   88]
per-ex loss: 0.537180  [   32/   88]
per-ex loss: 0.633577  [   34/   88]
per-ex loss: 0.374392  [   36/   88]
per-ex loss: 0.441570  [   38/   88]
per-ex loss: 0.464697  [   40/   88]
per-ex loss: 0.494666  [   42/   88]
per-ex loss: 0.354063  [   44/   88]
per-ex loss: 0.474647  [   46/   88]
per-ex loss: 0.337934  [   48/   88]
per-ex loss: 0.646260  [   50/   88]
per-ex loss: 0.441419  [   52/   88]
per-ex loss: 0.412074  [   54/   88]
per-ex loss: 0.591310  [   56/   88]
per-ex loss: 0.416296  [   58/   88]
per-ex loss: 0.507311  [   60/   88]
per-ex loss: 0.365403  [   62/   88]
per-ex loss: 0.447889  [   64/   88]
per-ex loss: 0.491640  [   66/   88]
per-ex loss: 0.560442  [   68/   88]
per-ex loss: 0.460100  [   70/   88]
per-ex loss: 0.570250  [   72/   88]
per-ex loss: 0.439443  [   74/   88]
per-ex loss: 0.388311  [   76/   88]
per-ex loss: 0.466421  [   78/   88]
per-ex loss: 0.499071  [   80/   88]
per-ex loss: 0.511186  [   82/   88]
per-ex loss: 0.489719  [   84/   88]
per-ex loss: 0.443813  [   86/   88]
per-ex loss: 0.506619  [   88/   88]
Train Error: Avg loss: 0.47646054
validation Error: 
 Avg loss: 0.53687899 
 F1: 0.497447 
 Precision: 0.500063 
 Recall: 0.494858
 IoU: 0.331068

test Error: 
 Avg loss: 0.47782263 
 F1: 0.556009 
 Precision: 0.579738 
 Recall: 0.534146
 IoU: 0.385050

We have finished training iteration 96
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_94_.pth
per-ex loss: 0.556883  [    2/   88]
per-ex loss: 0.401278  [    4/   88]
per-ex loss: 0.435639  [    6/   88]
per-ex loss: 0.551495  [    8/   88]
per-ex loss: 0.373329  [   10/   88]
per-ex loss: 0.399415  [   12/   88]
per-ex loss: 0.522289  [   14/   88]
per-ex loss: 0.633229  [   16/   88]
per-ex loss: 0.584198  [   18/   88]
per-ex loss: 0.436126  [   20/   88]
per-ex loss: 0.342991  [   22/   88]
per-ex loss: 0.410993  [   24/   88]
per-ex loss: 0.503912  [   26/   88]
per-ex loss: 0.615548  [   28/   88]
per-ex loss: 0.420424  [   30/   88]
per-ex loss: 0.370068  [   32/   88]
per-ex loss: 0.480640  [   34/   88]
per-ex loss: 0.391205  [   36/   88]
per-ex loss: 0.459167  [   38/   88]
per-ex loss: 0.542702  [   40/   88]
per-ex loss: 0.477469  [   42/   88]
per-ex loss: 0.557815  [   44/   88]
per-ex loss: 0.529785  [   46/   88]
per-ex loss: 0.443600  [   48/   88]
per-ex loss: 0.473702  [   50/   88]
per-ex loss: 0.562544  [   52/   88]
per-ex loss: 0.607342  [   54/   88]
per-ex loss: 0.383180  [   56/   88]
per-ex loss: 0.462710  [   58/   88]
per-ex loss: 0.535388  [   60/   88]
per-ex loss: 0.417039  [   62/   88]
per-ex loss: 0.384048  [   64/   88]
per-ex loss: 0.416007  [   66/   88]
per-ex loss: 0.408004  [   68/   88]
per-ex loss: 0.468168  [   70/   88]
per-ex loss: 0.449663  [   72/   88]
per-ex loss: 0.409406  [   74/   88]
per-ex loss: 0.459050  [   76/   88]
per-ex loss: 0.471996  [   78/   88]
per-ex loss: 0.399644  [   80/   88]
per-ex loss: 0.506728  [   82/   88]
per-ex loss: 0.390331  [   84/   88]
per-ex loss: 0.671599  [   86/   88]
per-ex loss: 0.342555  [   88/   88]
Train Error: Avg loss: 0.46952958
validation Error: 
 Avg loss: 0.51371298 
 F1: 0.511093 
 Precision: 0.567799 
 Recall: 0.464686
 IoU: 0.343268

test Error: 
 Avg loss: 0.46337298 
 F1: 0.570317 
 Precision: 0.638288 
 Recall: 0.515429
 IoU: 0.398912

We have finished training iteration 97
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_95_.pth
per-ex loss: 0.410534  [    2/   88]
per-ex loss: 0.464627  [    4/   88]
per-ex loss: 0.571664  [    6/   88]
per-ex loss: 0.516443  [    8/   88]
per-ex loss: 0.381081  [   10/   88]
per-ex loss: 0.432931  [   12/   88]
per-ex loss: 0.430055  [   14/   88]
per-ex loss: 0.607020  [   16/   88]
per-ex loss: 0.360345  [   18/   88]
per-ex loss: 0.354670  [   20/   88]
per-ex loss: 0.489005  [   22/   88]
per-ex loss: 0.428870  [   24/   88]
per-ex loss: 0.510195  [   26/   88]
per-ex loss: 0.466628  [   28/   88]
per-ex loss: 0.582533  [   30/   88]
per-ex loss: 0.496739  [   32/   88]
per-ex loss: 0.513505  [   34/   88]
per-ex loss: 0.350780  [   36/   88]
per-ex loss: 0.427437  [   38/   88]
per-ex loss: 0.454229  [   40/   88]
per-ex loss: 0.423850  [   42/   88]
per-ex loss: 0.470509  [   44/   88]
per-ex loss: 0.445495  [   46/   88]
per-ex loss: 0.488528  [   48/   88]
per-ex loss: 0.590982  [   50/   88]
per-ex loss: 0.610788  [   52/   88]
per-ex loss: 0.458404  [   54/   88]
per-ex loss: 0.410856  [   56/   88]
per-ex loss: 0.601016  [   58/   88]
per-ex loss: 0.473637  [   60/   88]
per-ex loss: 0.405144  [   62/   88]
per-ex loss: 0.376413  [   64/   88]
per-ex loss: 0.381507  [   66/   88]
per-ex loss: 0.432507  [   68/   88]
per-ex loss: 0.502193  [   70/   88]
per-ex loss: 0.389854  [   72/   88]
per-ex loss: 0.654425  [   74/   88]
per-ex loss: 0.413507  [   76/   88]
per-ex loss: 0.419237  [   78/   88]
per-ex loss: 0.389567  [   80/   88]
per-ex loss: 0.412130  [   82/   88]
per-ex loss: 0.526124  [   84/   88]
per-ex loss: 0.416148  [   86/   88]
per-ex loss: 0.448885  [   88/   88]
Train Error: Avg loss: 0.46343174
validation Error: 
 Avg loss: 0.51484539 
 F1: 0.508046 
 Precision: 0.573061 
 Recall: 0.456281
 IoU: 0.340524

test Error: 
 Avg loss: 0.46903562 
 F1: 0.567873 
 Precision: 0.647557 
 Recall: 0.505651
 IoU: 0.396524

We have finished training iteration 98
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_96_.pth
per-ex loss: 0.417676  [    2/   88]
per-ex loss: 0.503028  [    4/   88]
per-ex loss: 0.586043  [    6/   88]
per-ex loss: 0.446976  [    8/   88]
per-ex loss: 0.420990  [   10/   88]
per-ex loss: 0.469315  [   12/   88]
per-ex loss: 0.415653  [   14/   88]
per-ex loss: 0.381740  [   16/   88]
per-ex loss: 0.388722  [   18/   88]
per-ex loss: 0.488857  [   20/   88]
per-ex loss: 0.368119  [   22/   88]
per-ex loss: 0.410655  [   24/   88]
per-ex loss: 0.545007  [   26/   88]
per-ex loss: 0.383277  [   28/   88]
per-ex loss: 0.347556  [   30/   88]
per-ex loss: 0.384841  [   32/   88]
per-ex loss: 0.510032  [   34/   88]
per-ex loss: 0.506495  [   36/   88]
per-ex loss: 0.437685  [   38/   88]
per-ex loss: 0.651409  [   40/   88]
per-ex loss: 0.490405  [   42/   88]
per-ex loss: 0.497220  [   44/   88]
per-ex loss: 0.455127  [   46/   88]
per-ex loss: 0.350161  [   48/   88]
per-ex loss: 0.378074  [   50/   88]
per-ex loss: 0.357518  [   52/   88]
per-ex loss: 0.356675  [   54/   88]
per-ex loss: 0.481929  [   56/   88]
per-ex loss: 0.429172  [   58/   88]
per-ex loss: 0.609861  [   60/   88]
per-ex loss: 0.378367  [   62/   88]
per-ex loss: 0.505911  [   64/   88]
per-ex loss: 0.438189  [   66/   88]
per-ex loss: 0.466394  [   68/   88]
per-ex loss: 0.422824  [   70/   88]
per-ex loss: 0.459899  [   72/   88]
per-ex loss: 0.463949  [   74/   88]
per-ex loss: 0.538174  [   76/   88]
per-ex loss: 0.471116  [   78/   88]
per-ex loss: 0.544049  [   80/   88]
per-ex loss: 0.555480  [   82/   88]
per-ex loss: 0.408952  [   84/   88]
per-ex loss: 0.444107  [   86/   88]
per-ex loss: 0.670332  [   88/   88]
Train Error: Avg loss: 0.45995357
validation Error: 
 Avg loss: 0.54711705 
 F1: 0.470952 
 Precision: 0.430847 
 Recall: 0.519288
 IoU: 0.308003

test Error: 
 Avg loss: 0.48835874 
 F1: 0.544454 
 Precision: 0.521724 
 Recall: 0.569255
 IoU: 0.374055

We have finished training iteration 99
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_97_.pth
per-ex loss: 0.551620  [    2/   88]
per-ex loss: 0.465645  [    4/   88]
per-ex loss: 0.649104  [    6/   88]
per-ex loss: 0.504972  [    8/   88]
per-ex loss: 0.466333  [   10/   88]
per-ex loss: 0.407968  [   12/   88]
per-ex loss: 0.481294  [   14/   88]
per-ex loss: 0.435589  [   16/   88]
per-ex loss: 0.434100  [   18/   88]
per-ex loss: 0.356593  [   20/   88]
per-ex loss: 0.520783  [   22/   88]
per-ex loss: 0.462971  [   24/   88]
per-ex loss: 0.419724  [   26/   88]
per-ex loss: 0.486281  [   28/   88]
per-ex loss: 0.386879  [   30/   88]
per-ex loss: 0.447963  [   32/   88]
per-ex loss: 0.485843  [   34/   88]
per-ex loss: 0.339408  [   36/   88]
per-ex loss: 0.595291  [   38/   88]
per-ex loss: 0.514315  [   40/   88]
per-ex loss: 0.493689  [   42/   88]
per-ex loss: 0.437371  [   44/   88]
per-ex loss: 0.570705  [   46/   88]
per-ex loss: 0.364680  [   48/   88]
per-ex loss: 0.359961  [   50/   88]
per-ex loss: 0.410734  [   52/   88]
per-ex loss: 0.434183  [   54/   88]
per-ex loss: 0.629247  [   56/   88]
per-ex loss: 0.417595  [   58/   88]
per-ex loss: 0.483437  [   60/   88]
per-ex loss: 0.572212  [   62/   88]
per-ex loss: 0.422737  [   64/   88]
per-ex loss: 0.444502  [   66/   88]
per-ex loss: 0.532777  [   68/   88]
per-ex loss: 0.495876  [   70/   88]
per-ex loss: 0.348157  [   72/   88]
per-ex loss: 0.625081  [   74/   88]
per-ex loss: 0.422292  [   76/   88]
per-ex loss: 0.424516  [   78/   88]
per-ex loss: 0.438311  [   80/   88]
per-ex loss: 0.407832  [   82/   88]
per-ex loss: 0.365254  [   84/   88]
per-ex loss: 0.390329  [   86/   88]
per-ex loss: 0.386763  [   88/   88]
Train Error: Avg loss: 0.46115721
validation Error: 
 Avg loss: 0.53239784 
 F1: 0.484260 
 Precision: 0.586049 
 Recall: 0.412597
 IoU: 0.319488

test Error: 
 Avg loss: 0.49663572 
 F1: 0.538465 
 Precision: 0.696052 
 Recall: 0.439061
 IoU: 0.368424

We have finished training iteration 100
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_98_.pth
per-ex loss: 0.409346  [    2/   88]
per-ex loss: 0.382451  [    4/   88]
per-ex loss: 0.596235  [    6/   88]
per-ex loss: 0.355405  [    8/   88]
per-ex loss: 0.431540  [   10/   88]
per-ex loss: 0.363834  [   12/   88]
per-ex loss: 0.366532  [   14/   88]
per-ex loss: 0.507924  [   16/   88]
per-ex loss: 0.399664  [   18/   88]
per-ex loss: 0.484030  [   20/   88]
per-ex loss: 0.463629  [   22/   88]
per-ex loss: 0.385176  [   24/   88]
per-ex loss: 0.426940  [   26/   88]
per-ex loss: 0.432770  [   28/   88]
per-ex loss: 0.516501  [   30/   88]
per-ex loss: 0.525644  [   32/   88]
per-ex loss: 0.396744  [   34/   88]
per-ex loss: 0.385332  [   36/   88]
per-ex loss: 0.408572  [   38/   88]
per-ex loss: 0.474088  [   40/   88]
per-ex loss: 0.576882  [   42/   88]
per-ex loss: 0.434540  [   44/   88]
per-ex loss: 0.612959  [   46/   88]
per-ex loss: 0.476326  [   48/   88]
per-ex loss: 0.606260  [   50/   88]
per-ex loss: 0.464324  [   52/   88]
per-ex loss: 0.485569  [   54/   88]
per-ex loss: 0.508456  [   56/   88]
per-ex loss: 0.443480  [   58/   88]
per-ex loss: 0.451552  [   60/   88]
per-ex loss: 0.521855  [   62/   88]
per-ex loss: 0.408569  [   64/   88]
per-ex loss: 0.382307  [   66/   88]
per-ex loss: 0.598010  [   68/   88]
per-ex loss: 0.430654  [   70/   88]
per-ex loss: 0.414743  [   72/   88]
per-ex loss: 0.400116  [   74/   88]
per-ex loss: 0.447598  [   76/   88]
per-ex loss: 0.365328  [   78/   88]
per-ex loss: 0.485045  [   80/   88]
per-ex loss: 0.606658  [   82/   88]
per-ex loss: 0.413593  [   84/   88]
per-ex loss: 0.450913  [   86/   88]
per-ex loss: 0.681723  [   88/   88]
Train Error: Avg loss: 0.46317764
validation Error: 
 Avg loss: 0.51487247 
 F1: 0.514474 
 Precision: 0.589561 
 Recall: 0.456352
 IoU: 0.346324

test Error: 
 Avg loss: 0.46577055 
 F1: 0.570507 
 Precision: 0.645461 
 Recall: 0.511150
 IoU: 0.399097

We have finished training iteration 101
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_99_.pth
per-ex loss: 0.451786  [    2/   88]
per-ex loss: 0.664757  [    4/   88]
per-ex loss: 0.452614  [    6/   88]
per-ex loss: 0.538995  [    8/   88]
per-ex loss: 0.529804  [   10/   88]
per-ex loss: 0.406637  [   12/   88]
per-ex loss: 0.407942  [   14/   88]
per-ex loss: 0.580820  [   16/   88]
per-ex loss: 0.426047  [   18/   88]
per-ex loss: 0.634992  [   20/   88]
per-ex loss: 0.397860  [   22/   88]
per-ex loss: 0.526567  [   24/   88]
per-ex loss: 0.468500  [   26/   88]
per-ex loss: 0.398233  [   28/   88]
per-ex loss: 0.519050  [   30/   88]
per-ex loss: 0.590207  [   32/   88]
per-ex loss: 0.365158  [   34/   88]
per-ex loss: 0.392724  [   36/   88]
per-ex loss: 0.420730  [   38/   88]
per-ex loss: 0.380589  [   40/   88]
per-ex loss: 0.382839  [   42/   88]
per-ex loss: 0.417947  [   44/   88]
per-ex loss: 0.653097  [   46/   88]
per-ex loss: 0.374303  [   48/   88]
per-ex loss: 0.566186  [   50/   88]
per-ex loss: 0.408844  [   52/   88]
per-ex loss: 0.475910  [   54/   88]
per-ex loss: 0.515667  [   56/   88]
per-ex loss: 0.422359  [   58/   88]
per-ex loss: 0.379065  [   60/   88]
per-ex loss: 0.423634  [   62/   88]
per-ex loss: 0.640161  [   64/   88]
per-ex loss: 0.400567  [   66/   88]
per-ex loss: 0.546967  [   68/   88]
per-ex loss: 0.368726  [   70/   88]
per-ex loss: 0.552206  [   72/   88]
per-ex loss: 0.407632  [   74/   88]
per-ex loss: 0.463569  [   76/   88]
per-ex loss: 0.421489  [   78/   88]
per-ex loss: 0.361648  [   80/   88]
per-ex loss: 0.443051  [   82/   88]
per-ex loss: 0.371552  [   84/   88]
per-ex loss: 0.597663  [   86/   88]
per-ex loss: 0.448809  [   88/   88]
Train Error: Avg loss: 0.46813414
validation Error: 
 Avg loss: 0.51789899 
 F1: 0.507824 
 Precision: 0.539109 
 Recall: 0.479971
 IoU: 0.340325

test Error: 
 Avg loss: 0.47372367 
 F1: 0.559963 
 Precision: 0.607370 
 Recall: 0.519421
 IoU: 0.388853

We have finished training iteration 102
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_100_.pth
per-ex loss: 0.475844  [    2/   88]
per-ex loss: 0.510927  [    4/   88]
per-ex loss: 0.449694  [    6/   88]
per-ex loss: 0.457245  [    8/   88]
per-ex loss: 0.366055  [   10/   88]
per-ex loss: 0.441088  [   12/   88]
per-ex loss: 0.351937  [   14/   88]
per-ex loss: 0.548707  [   16/   88]
per-ex loss: 0.511969  [   18/   88]
per-ex loss: 0.407561  [   20/   88]
per-ex loss: 0.368216  [   22/   88]
per-ex loss: 0.410620  [   24/   88]
per-ex loss: 0.554791  [   26/   88]
per-ex loss: 0.628400  [   28/   88]
per-ex loss: 0.467752  [   30/   88]
per-ex loss: 0.612620  [   32/   88]
per-ex loss: 0.484447  [   34/   88]
per-ex loss: 0.467398  [   36/   88]
per-ex loss: 0.418832  [   38/   88]
per-ex loss: 0.527768  [   40/   88]
per-ex loss: 0.460425  [   42/   88]
per-ex loss: 0.408445  [   44/   88]
per-ex loss: 0.449870  [   46/   88]
per-ex loss: 0.507172  [   48/   88]
per-ex loss: 0.456256  [   50/   88]
per-ex loss: 0.501544  [   52/   88]
per-ex loss: 0.439772  [   54/   88]
per-ex loss: 0.607604  [   56/   88]
per-ex loss: 0.467203  [   58/   88]
per-ex loss: 0.465847  [   60/   88]
per-ex loss: 0.359170  [   62/   88]
per-ex loss: 0.366173  [   64/   88]
per-ex loss: 0.550239  [   66/   88]
per-ex loss: 0.390178  [   68/   88]
per-ex loss: 0.411065  [   70/   88]
per-ex loss: 0.530837  [   72/   88]
per-ex loss: 0.391487  [   74/   88]
per-ex loss: 0.450135  [   76/   88]
per-ex loss: 0.492450  [   78/   88]
per-ex loss: 0.575205  [   80/   88]
per-ex loss: 0.420936  [   82/   88]
per-ex loss: 0.459605  [   84/   88]
per-ex loss: 0.364811  [   86/   88]
per-ex loss: 0.366941  [   88/   88]
Train Error: Avg loss: 0.46261913
validation Error: 
 Avg loss: 0.53067354 
 F1: 0.485598 
 Precision: 0.528168 
 Recall: 0.449379
 IoU: 0.320653

test Error: 
 Avg loss: 0.48547186 
 F1: 0.549081 
 Precision: 0.621851 
 Recall: 0.491558
 IoU: 0.378437

We have finished training iteration 103
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_101_.pth
per-ex loss: 0.449961  [    2/   88]
per-ex loss: 0.640399  [    4/   88]
per-ex loss: 0.456959  [    6/   88]
per-ex loss: 0.562882  [    8/   88]
per-ex loss: 0.354966  [   10/   88]
per-ex loss: 0.523127  [   12/   88]
per-ex loss: 0.417121  [   14/   88]
per-ex loss: 0.498164  [   16/   88]
per-ex loss: 0.455902  [   18/   88]
per-ex loss: 0.449106  [   20/   88]
per-ex loss: 0.351924  [   22/   88]
per-ex loss: 0.458307  [   24/   88]
per-ex loss: 0.455625  [   26/   88]
per-ex loss: 0.611156  [   28/   88]
per-ex loss: 0.382763  [   30/   88]
per-ex loss: 0.519812  [   32/   88]
per-ex loss: 0.476748  [   34/   88]
per-ex loss: 0.480710  [   36/   88]
per-ex loss: 0.368049  [   38/   88]
per-ex loss: 0.523261  [   40/   88]
per-ex loss: 0.521777  [   42/   88]
per-ex loss: 0.372611  [   44/   88]
per-ex loss: 0.460894  [   46/   88]
per-ex loss: 0.454997  [   48/   88]
per-ex loss: 0.451816  [   50/   88]
per-ex loss: 0.454242  [   52/   88]
per-ex loss: 0.434143  [   54/   88]
per-ex loss: 0.425825  [   56/   88]
per-ex loss: 0.647975  [   58/   88]
per-ex loss: 0.415329  [   60/   88]
per-ex loss: 0.502526  [   62/   88]
per-ex loss: 0.456029  [   64/   88]
per-ex loss: 0.364180  [   66/   88]
per-ex loss: 0.410890  [   68/   88]
per-ex loss: 0.391151  [   70/   88]
per-ex loss: 0.415991  [   72/   88]
per-ex loss: 0.484300  [   74/   88]
per-ex loss: 0.414586  [   76/   88]
per-ex loss: 0.381198  [   78/   88]
per-ex loss: 0.477734  [   80/   88]
per-ex loss: 0.528261  [   82/   88]
per-ex loss: 0.385223  [   84/   88]
per-ex loss: 0.552291  [   86/   88]
per-ex loss: 0.681851  [   88/   88]
Train Error: Avg loss: 0.46642640
validation Error: 
 Avg loss: 0.53602179 
 F1: 0.494600 
 Precision: 0.601404 
 Recall: 0.420010
 IoU: 0.328551

test Error: 
 Avg loss: 0.48571117 
 F1: 0.550229 
 Precision: 0.701682 
 Recall: 0.452550
 IoU: 0.379528

We have finished training iteration 104
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_102_.pth
per-ex loss: 0.437150  [    2/   88]
per-ex loss: 0.576776  [    4/   88]
per-ex loss: 0.523205  [    6/   88]
per-ex loss: 0.457889  [    8/   88]
per-ex loss: 0.520425  [   10/   88]
per-ex loss: 0.525174  [   12/   88]
per-ex loss: 0.438525  [   14/   88]
per-ex loss: 0.432261  [   16/   88]
per-ex loss: 0.366848  [   18/   88]
per-ex loss: 0.639731  [   20/   88]
per-ex loss: 0.395607  [   22/   88]
per-ex loss: 0.555379  [   24/   88]
per-ex loss: 0.647930  [   26/   88]
per-ex loss: 0.543592  [   28/   88]
per-ex loss: 0.504985  [   30/   88]
per-ex loss: 0.460577  [   32/   88]
per-ex loss: 0.421334  [   34/   88]
per-ex loss: 0.413889  [   36/   88]
per-ex loss: 0.581604  [   38/   88]
per-ex loss: 0.339504  [   40/   88]
per-ex loss: 0.518294  [   42/   88]
per-ex loss: 0.554096  [   44/   88]
per-ex loss: 0.519002  [   46/   88]
per-ex loss: 0.369843  [   48/   88]
per-ex loss: 0.503732  [   50/   88]
per-ex loss: 0.613494  [   52/   88]
per-ex loss: 0.425582  [   54/   88]
per-ex loss: 0.518062  [   56/   88]
per-ex loss: 0.480927  [   58/   88]
per-ex loss: 0.437055  [   60/   88]
per-ex loss: 0.429701  [   62/   88]
per-ex loss: 0.395105  [   64/   88]
per-ex loss: 0.362749  [   66/   88]
per-ex loss: 0.368489  [   68/   88]
per-ex loss: 0.470340  [   70/   88]
per-ex loss: 0.334459  [   72/   88]
per-ex loss: 0.361662  [   74/   88]
per-ex loss: 0.438776  [   76/   88]
per-ex loss: 0.495966  [   78/   88]
per-ex loss: 0.360438  [   80/   88]
per-ex loss: 0.400956  [   82/   88]
per-ex loss: 0.590619  [   84/   88]
per-ex loss: 0.685323  [   86/   88]
per-ex loss: 0.578191  [   88/   88]
Train Error: Avg loss: 0.47716466
validation Error: 
 Avg loss: 0.53397593 
 F1: 0.498968 
 Precision: 0.591505 
 Recall: 0.431468
 IoU: 0.332417

test Error: 
 Avg loss: 0.47600350 
 F1: 0.561894 
 Precision: 0.674213 
 Recall: 0.481654
 IoU: 0.390718

We have finished training iteration 105
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_103_.pth
per-ex loss: 0.354359  [    2/   88]
per-ex loss: 0.556385  [    4/   88]
per-ex loss: 0.433289  [    6/   88]
per-ex loss: 0.374186  [    8/   88]
per-ex loss: 0.571803  [   10/   88]
per-ex loss: 0.465095  [   12/   88]
per-ex loss: 0.417214  [   14/   88]
per-ex loss: 0.591977  [   16/   88]
per-ex loss: 0.412745  [   18/   88]
per-ex loss: 0.501653  [   20/   88]
per-ex loss: 0.437659  [   22/   88]
per-ex loss: 0.465612  [   24/   88]
per-ex loss: 0.522595  [   26/   88]
per-ex loss: 0.480134  [   28/   88]
per-ex loss: 0.384264  [   30/   88]
per-ex loss: 0.517336  [   32/   88]
per-ex loss: 0.502400  [   34/   88]
per-ex loss: 0.508523  [   36/   88]
per-ex loss: 0.453275  [   38/   88]
per-ex loss: 0.371998  [   40/   88]
per-ex loss: 0.466239  [   42/   88]
per-ex loss: 0.438456  [   44/   88]
per-ex loss: 0.478952  [   46/   88]
per-ex loss: 0.459799  [   48/   88]
per-ex loss: 0.395941  [   50/   88]
per-ex loss: 0.458881  [   52/   88]
per-ex loss: 0.482588  [   54/   88]
per-ex loss: 0.407286  [   56/   88]
per-ex loss: 0.396245  [   58/   88]
per-ex loss: 0.648294  [   60/   88]
per-ex loss: 0.465558  [   62/   88]
per-ex loss: 0.541196  [   64/   88]
per-ex loss: 0.362951  [   66/   88]
per-ex loss: 0.430408  [   68/   88]
per-ex loss: 0.506998  [   70/   88]
per-ex loss: 0.486239  [   72/   88]
per-ex loss: 0.398084  [   74/   88]
per-ex loss: 0.433499  [   76/   88]
per-ex loss: 0.391413  [   78/   88]
per-ex loss: 0.676932  [   80/   88]
per-ex loss: 0.446210  [   82/   88]
per-ex loss: 0.335674  [   84/   88]
per-ex loss: 0.496833  [   86/   88]
per-ex loss: 0.370161  [   88/   88]
Train Error: Avg loss: 0.46130319
validation Error: 
 Avg loss: 0.53346594 
 F1: 0.487994 
 Precision: 0.527310 
 Recall: 0.454134
 IoU: 0.322746

test Error: 
 Avg loss: 0.48764519 
 F1: 0.546539 
 Precision: 0.608499 
 Recall: 0.496030
 IoU: 0.376026

We have finished training iteration 106
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_104_.pth
per-ex loss: 0.575819  [    2/   88]
per-ex loss: 0.538591  [    4/   88]
per-ex loss: 0.598376  [    6/   88]
per-ex loss: 0.406662  [    8/   88]
per-ex loss: 0.436859  [   10/   88]
per-ex loss: 0.574577  [   12/   88]
per-ex loss: 0.435374  [   14/   88]
per-ex loss: 0.522529  [   16/   88]
per-ex loss: 0.465563  [   18/   88]
per-ex loss: 0.443471  [   20/   88]
per-ex loss: 0.510712  [   22/   88]
per-ex loss: 0.385477  [   24/   88]
per-ex loss: 0.356059  [   26/   88]
per-ex loss: 0.491718  [   28/   88]
per-ex loss: 0.350468  [   30/   88]
per-ex loss: 0.465277  [   32/   88]
per-ex loss: 0.443032  [   34/   88]
per-ex loss: 0.557480  [   36/   88]
per-ex loss: 0.382125  [   38/   88]
per-ex loss: 0.338310  [   40/   88]
per-ex loss: 0.428216  [   42/   88]
per-ex loss: 0.396376  [   44/   88]
per-ex loss: 0.483574  [   46/   88]
per-ex loss: 0.520426  [   48/   88]
per-ex loss: 0.353731  [   50/   88]
per-ex loss: 0.483084  [   52/   88]
per-ex loss: 0.376103  [   54/   88]
per-ex loss: 0.636516  [   56/   88]
per-ex loss: 0.384173  [   58/   88]
per-ex loss: 0.592173  [   60/   88]
per-ex loss: 0.447679  [   62/   88]
per-ex loss: 0.392851  [   64/   88]
per-ex loss: 0.355818  [   66/   88]
per-ex loss: 0.527763  [   68/   88]
per-ex loss: 0.463461  [   70/   88]
per-ex loss: 0.475571  [   72/   88]
per-ex loss: 0.584818  [   74/   88]
per-ex loss: 0.365422  [   76/   88]
per-ex loss: 0.426949  [   78/   88]
per-ex loss: 0.609931  [   80/   88]
per-ex loss: 0.434643  [   82/   88]
per-ex loss: 0.652934  [   84/   88]
per-ex loss: 0.400333  [   86/   88]
per-ex loss: 0.371625  [   88/   88]
Train Error: Avg loss: 0.46460560
validation Error: 
 Avg loss: 0.53803575 
 F1: 0.504056 
 Precision: 0.511906 
 Recall: 0.496442
 IoU: 0.336948

test Error: 
 Avg loss: 0.47217868 
 F1: 0.561654 
 Precision: 0.598500 
 Recall: 0.529081
 IoU: 0.390486

We have finished training iteration 107
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_105_.pth
per-ex loss: 0.467778  [    2/   88]
per-ex loss: 0.476209  [    4/   88]
per-ex loss: 0.411137  [    6/   88]
per-ex loss: 0.370240  [    8/   88]
per-ex loss: 0.412904  [   10/   88]
per-ex loss: 0.371805  [   12/   88]
per-ex loss: 0.461887  [   14/   88]
per-ex loss: 0.418288  [   16/   88]
per-ex loss: 0.494825  [   18/   88]
per-ex loss: 0.393752  [   20/   88]
per-ex loss: 0.458027  [   22/   88]
per-ex loss: 0.373157  [   24/   88]
per-ex loss: 0.661297  [   26/   88]
per-ex loss: 0.350034  [   28/   88]
per-ex loss: 0.422860  [   30/   88]
per-ex loss: 0.584764  [   32/   88]
per-ex loss: 0.393555  [   34/   88]
per-ex loss: 0.565769  [   36/   88]
per-ex loss: 0.564692  [   38/   88]
per-ex loss: 0.636166  [   40/   88]
per-ex loss: 0.376280  [   42/   88]
per-ex loss: 0.415355  [   44/   88]
per-ex loss: 0.422214  [   46/   88]
per-ex loss: 0.446507  [   48/   88]
per-ex loss: 0.509289  [   50/   88]
per-ex loss: 0.578221  [   52/   88]
per-ex loss: 0.397207  [   54/   88]
per-ex loss: 0.411704  [   56/   88]
per-ex loss: 0.472502  [   58/   88]
per-ex loss: 0.373247  [   60/   88]
per-ex loss: 0.585034  [   62/   88]
per-ex loss: 0.455810  [   64/   88]
per-ex loss: 0.515949  [   66/   88]
per-ex loss: 0.370014  [   68/   88]
per-ex loss: 0.434684  [   70/   88]
per-ex loss: 0.489032  [   72/   88]
per-ex loss: 0.440413  [   74/   88]
per-ex loss: 0.425586  [   76/   88]
per-ex loss: 0.534029  [   78/   88]
per-ex loss: 0.432876  [   80/   88]
per-ex loss: 0.441034  [   82/   88]
per-ex loss: 0.350435  [   84/   88]
per-ex loss: 0.632463  [   86/   88]
per-ex loss: 0.331785  [   88/   88]
Train Error: Avg loss: 0.45751855
validation Error: 
 Avg loss: 0.51565581 
 F1: 0.502653 
 Precision: 0.555432 
 Recall: 0.459035
 IoU: 0.335696

test Error: 
 Avg loss: 0.47075415 
 F1: 0.566956 
 Precision: 0.642072 
 Recall: 0.507574
 IoU: 0.395630

We have finished training iteration 108
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_106_.pth
per-ex loss: 0.479914  [    2/   88]
per-ex loss: 0.457952  [    4/   88]
per-ex loss: 0.345730  [    6/   88]
per-ex loss: 0.526317  [    8/   88]
per-ex loss: 0.482976  [   10/   88]
per-ex loss: 0.469725  [   12/   88]
per-ex loss: 0.417367  [   14/   88]
per-ex loss: 0.373711  [   16/   88]
per-ex loss: 0.475701  [   18/   88]
per-ex loss: 0.570321  [   20/   88]
per-ex loss: 0.511594  [   22/   88]
per-ex loss: 0.452492  [   24/   88]
per-ex loss: 0.407478  [   26/   88]
per-ex loss: 0.456077  [   28/   88]
per-ex loss: 0.390600  [   30/   88]
per-ex loss: 0.626006  [   32/   88]
per-ex loss: 0.399507  [   34/   88]
per-ex loss: 0.385983  [   36/   88]
per-ex loss: 0.375808  [   38/   88]
per-ex loss: 0.537840  [   40/   88]
per-ex loss: 0.646475  [   42/   88]
per-ex loss: 0.390344  [   44/   88]
per-ex loss: 0.362695  [   46/   88]
per-ex loss: 0.412907  [   48/   88]
per-ex loss: 0.454987  [   50/   88]
per-ex loss: 0.470011  [   52/   88]
per-ex loss: 0.491998  [   54/   88]
per-ex loss: 0.354570  [   56/   88]
per-ex loss: 0.390377  [   58/   88]
per-ex loss: 0.378732  [   60/   88]
per-ex loss: 0.495878  [   62/   88]
per-ex loss: 0.484916  [   64/   88]
per-ex loss: 0.369701  [   66/   88]
per-ex loss: 0.415936  [   68/   88]
per-ex loss: 0.382661  [   70/   88]
per-ex loss: 0.369011  [   72/   88]
per-ex loss: 0.518551  [   74/   88]
per-ex loss: 0.545316  [   76/   88]
per-ex loss: 0.487012  [   78/   88]
per-ex loss: 0.591482  [   80/   88]
per-ex loss: 0.505592  [   82/   88]
per-ex loss: 0.378535  [   84/   88]
per-ex loss: 0.393791  [   86/   88]
per-ex loss: 0.677265  [   88/   88]
Train Error: Avg loss: 0.45708732
validation Error: 
 Avg loss: 0.52234436 
 F1: 0.504712 
 Precision: 0.591830 
 Recall: 0.439951
 IoU: 0.337535

test Error: 
 Avg loss: 0.48196714 
 F1: 0.555399 
 Precision: 0.665366 
 Recall: 0.476626
 IoU: 0.384465

We have finished training iteration 109
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_107_.pth
per-ex loss: 0.481168  [    2/   88]
per-ex loss: 0.498686  [    4/   88]
per-ex loss: 0.559160  [    6/   88]
per-ex loss: 0.408674  [    8/   88]
per-ex loss: 0.457247  [   10/   88]
per-ex loss: 0.447780  [   12/   88]
per-ex loss: 0.429369  [   14/   88]
per-ex loss: 0.503390  [   16/   88]
per-ex loss: 0.479953  [   18/   88]
per-ex loss: 0.389308  [   20/   88]
per-ex loss: 0.375690  [   22/   88]
per-ex loss: 0.416220  [   24/   88]
per-ex loss: 0.489866  [   26/   88]
per-ex loss: 0.354997  [   28/   88]
per-ex loss: 0.550066  [   30/   88]
per-ex loss: 0.566759  [   32/   88]
per-ex loss: 0.421842  [   34/   88]
per-ex loss: 0.469078  [   36/   88]
per-ex loss: 0.638076  [   38/   88]
per-ex loss: 0.388216  [   40/   88]
per-ex loss: 0.430345  [   42/   88]
per-ex loss: 0.375751  [   44/   88]
per-ex loss: 0.465592  [   46/   88]
per-ex loss: 0.347653  [   48/   88]
per-ex loss: 0.519009  [   50/   88]
per-ex loss: 0.487000  [   52/   88]
per-ex loss: 0.516400  [   54/   88]
per-ex loss: 0.407277  [   56/   88]
per-ex loss: 0.373450  [   58/   88]
per-ex loss: 0.449896  [   60/   88]
per-ex loss: 0.438564  [   62/   88]
per-ex loss: 0.475311  [   64/   88]
per-ex loss: 0.602020  [   66/   88]
per-ex loss: 0.468107  [   68/   88]
per-ex loss: 0.475781  [   70/   88]
per-ex loss: 0.404807  [   72/   88]
per-ex loss: 0.419975  [   74/   88]
per-ex loss: 0.491694  [   76/   88]
per-ex loss: 0.411317  [   78/   88]
per-ex loss: 0.428726  [   80/   88]
per-ex loss: 0.401052  [   82/   88]
per-ex loss: 0.392431  [   84/   88]
per-ex loss: 0.558325  [   86/   88]
per-ex loss: 0.454699  [   88/   88]
Train Error: Avg loss: 0.45728931
validation Error: 
 Avg loss: 0.49792667 
 F1: 0.520545 
 Precision: 0.586952 
 Recall: 0.467638
 IoU: 0.351850

test Error: 
 Avg loss: 0.45838732 
 F1: 0.577580 
 Precision: 0.626400 
 Recall: 0.535820
 IoU: 0.406055

We have finished training iteration 110
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_108_.pth
per-ex loss: 0.527989  [    2/   88]
per-ex loss: 0.369576  [    4/   88]
per-ex loss: 0.405200  [    6/   88]
per-ex loss: 0.528759  [    8/   88]
per-ex loss: 0.480326  [   10/   88]
per-ex loss: 0.396102  [   12/   88]
per-ex loss: 0.632769  [   14/   88]
per-ex loss: 0.650701  [   16/   88]
per-ex loss: 0.468360  [   18/   88]
per-ex loss: 0.439499  [   20/   88]
per-ex loss: 0.564689  [   22/   88]
per-ex loss: 0.336357  [   24/   88]
per-ex loss: 0.481261  [   26/   88]
per-ex loss: 0.535299  [   28/   88]
per-ex loss: 0.480739  [   30/   88]
per-ex loss: 0.445134  [   32/   88]
per-ex loss: 0.629447  [   34/   88]
per-ex loss: 0.562483  [   36/   88]
per-ex loss: 0.481376  [   38/   88]
per-ex loss: 0.483032  [   40/   88]
per-ex loss: 0.511139  [   42/   88]
per-ex loss: 0.433007  [   44/   88]
per-ex loss: 0.593814  [   46/   88]
per-ex loss: 0.446752  [   48/   88]
per-ex loss: 0.385713  [   50/   88]
per-ex loss: 0.589574  [   52/   88]
per-ex loss: 0.391821  [   54/   88]
per-ex loss: 0.417332  [   56/   88]
per-ex loss: 0.451078  [   58/   88]
per-ex loss: 0.422055  [   60/   88]
per-ex loss: 0.562255  [   62/   88]
per-ex loss: 0.384401  [   64/   88]
per-ex loss: 0.416794  [   66/   88]
per-ex loss: 0.361479  [   68/   88]
per-ex loss: 0.430082  [   70/   88]
per-ex loss: 0.503353  [   72/   88]
per-ex loss: 0.367244  [   74/   88]
per-ex loss: 0.395050  [   76/   88]
per-ex loss: 0.691125  [   78/   88]
per-ex loss: 0.412092  [   80/   88]
per-ex loss: 0.434284  [   82/   88]
per-ex loss: 0.362305  [   84/   88]
per-ex loss: 0.484612  [   86/   88]
per-ex loss: 0.357840  [   88/   88]
Train Error: Avg loss: 0.47055224
validation Error: 
 Avg loss: 0.49887696 
 F1: 0.520891 
 Precision: 0.532629 
 Recall: 0.509658
 IoU: 0.352165

test Error: 
 Avg loss: 0.45736088 
 F1: 0.577486 
 Precision: 0.579853 
 Recall: 0.575138
 IoU: 0.405961

We have finished training iteration 111
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_109_.pth
per-ex loss: 0.484792  [    2/   88]
per-ex loss: 0.448828  [    4/   88]
per-ex loss: 0.593443  [    6/   88]
per-ex loss: 0.362402  [    8/   88]
per-ex loss: 0.566737  [   10/   88]
per-ex loss: 0.473595  [   12/   88]
per-ex loss: 0.519137  [   14/   88]
per-ex loss: 0.623849  [   16/   88]
per-ex loss: 0.470431  [   18/   88]
per-ex loss: 0.405485  [   20/   88]
per-ex loss: 0.473510  [   22/   88]
per-ex loss: 0.453016  [   24/   88]
per-ex loss: 0.400975  [   26/   88]
per-ex loss: 0.329087  [   28/   88]
per-ex loss: 0.369940  [   30/   88]
per-ex loss: 0.397775  [   32/   88]
per-ex loss: 0.406509  [   34/   88]
per-ex loss: 0.447516  [   36/   88]
per-ex loss: 0.472577  [   38/   88]
per-ex loss: 0.584607  [   40/   88]
per-ex loss: 0.376347  [   42/   88]
per-ex loss: 0.646520  [   44/   88]
per-ex loss: 0.435092  [   46/   88]
per-ex loss: 0.406867  [   48/   88]
per-ex loss: 0.582479  [   50/   88]
per-ex loss: 0.382305  [   52/   88]
per-ex loss: 0.426221  [   54/   88]
per-ex loss: 0.617640  [   56/   88]
per-ex loss: 0.352702  [   58/   88]
per-ex loss: 0.371983  [   60/   88]
per-ex loss: 0.378575  [   62/   88]
per-ex loss: 0.443338  [   64/   88]
per-ex loss: 0.436894  [   66/   88]
per-ex loss: 0.472528  [   68/   88]
per-ex loss: 0.379432  [   70/   88]
per-ex loss: 0.496012  [   72/   88]
per-ex loss: 0.467659  [   74/   88]
per-ex loss: 0.446319  [   76/   88]
per-ex loss: 0.367357  [   78/   88]
per-ex loss: 0.463855  [   80/   88]
per-ex loss: 0.517324  [   82/   88]
per-ex loss: 0.409531  [   84/   88]
per-ex loss: 0.508425  [   86/   88]
per-ex loss: 0.518057  [   88/   88]
Train Error: Avg loss: 0.45881078
validation Error: 
 Avg loss: 0.48889150 
 F1: 0.526519 
 Precision: 0.560060 
 Recall: 0.496768
 IoU: 0.357330

test Error: 
 Avg loss: 0.46114923 
 F1: 0.573398 
 Precision: 0.588111 
 Recall: 0.559404
 IoU: 0.401933

We have finished training iteration 112
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_65_.pth
per-ex loss: 0.483163  [    2/   88]
per-ex loss: 0.391406  [    4/   88]
per-ex loss: 0.579214  [    6/   88]
per-ex loss: 0.469933  [    8/   88]
per-ex loss: 0.522158  [   10/   88]
per-ex loss: 0.396836  [   12/   88]
per-ex loss: 0.435959  [   14/   88]
per-ex loss: 0.584533  [   16/   88]
per-ex loss: 0.347026  [   18/   88]
per-ex loss: 0.411686  [   20/   88]
per-ex loss: 0.443466  [   22/   88]
per-ex loss: 0.355743  [   24/   88]
per-ex loss: 0.413407  [   26/   88]
per-ex loss: 0.416846  [   28/   88]
per-ex loss: 0.600244  [   30/   88]
per-ex loss: 0.492346  [   32/   88]
per-ex loss: 0.404484  [   34/   88]
per-ex loss: 0.562595  [   36/   88]
per-ex loss: 0.437656  [   38/   88]
per-ex loss: 0.448459  [   40/   88]
per-ex loss: 0.431890  [   42/   88]
per-ex loss: 0.581141  [   44/   88]
per-ex loss: 0.410667  [   46/   88]
per-ex loss: 0.611071  [   48/   88]
per-ex loss: 0.497607  [   50/   88]
per-ex loss: 0.504231  [   52/   88]
per-ex loss: 0.425223  [   54/   88]
per-ex loss: 0.525867  [   56/   88]
per-ex loss: 0.570265  [   58/   88]
per-ex loss: 0.415693  [   60/   88]
per-ex loss: 0.396637  [   62/   88]
per-ex loss: 0.440988  [   64/   88]
per-ex loss: 0.419560  [   66/   88]
per-ex loss: 0.420667  [   68/   88]
per-ex loss: 0.473895  [   70/   88]
per-ex loss: 0.399814  [   72/   88]
per-ex loss: 0.432408  [   74/   88]
per-ex loss: 0.452220  [   76/   88]
per-ex loss: 0.345809  [   78/   88]
per-ex loss: 0.422702  [   80/   88]
per-ex loss: 0.481020  [   82/   88]
per-ex loss: 0.587502  [   84/   88]
per-ex loss: 0.483616  [   86/   88]
per-ex loss: 0.441695  [   88/   88]
Train Error: Avg loss: 0.46293972
validation Error: 
 Avg loss: 0.51959977 
 F1: 0.499282 
 Precision: 0.598195 
 Recall: 0.428439
 IoU: 0.332696

test Error: 
 Avg loss: 0.47834918 
 F1: 0.557920 
 Precision: 0.678309 
 Recall: 0.473823
 IoU: 0.386885

We have finished training iteration 113
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_79_.pth
per-ex loss: 0.490362  [    2/   88]
per-ex loss: 0.451711  [    4/   88]
per-ex loss: 0.402101  [    6/   88]
per-ex loss: 0.421809  [    8/   88]
per-ex loss: 0.521799  [   10/   88]
per-ex loss: 0.596366  [   12/   88]
per-ex loss: 0.378945  [   14/   88]
per-ex loss: 0.458613  [   16/   88]
per-ex loss: 0.371336  [   18/   88]
per-ex loss: 0.353329  [   20/   88]
per-ex loss: 0.468368  [   22/   88]
per-ex loss: 0.399928  [   24/   88]
per-ex loss: 0.568398  [   26/   88]
per-ex loss: 0.411082  [   28/   88]
per-ex loss: 0.414177  [   30/   88]
per-ex loss: 0.441826  [   32/   88]
per-ex loss: 0.468657  [   34/   88]
per-ex loss: 0.384108  [   36/   88]
per-ex loss: 0.411129  [   38/   88]
per-ex loss: 0.393335  [   40/   88]
per-ex loss: 0.485951  [   42/   88]
per-ex loss: 0.418036  [   44/   88]
per-ex loss: 0.385912  [   46/   88]
per-ex loss: 0.446959  [   48/   88]
per-ex loss: 0.468795  [   50/   88]
per-ex loss: 0.411279  [   52/   88]
per-ex loss: 0.370836  [   54/   88]
per-ex loss: 0.445047  [   56/   88]
per-ex loss: 0.435092  [   58/   88]
per-ex loss: 0.594275  [   60/   88]
per-ex loss: 0.585614  [   62/   88]
per-ex loss: 0.477833  [   64/   88]
per-ex loss: 0.476291  [   66/   88]
per-ex loss: 0.421420  [   68/   88]
per-ex loss: 0.549560  [   70/   88]
per-ex loss: 0.673884  [   72/   88]
per-ex loss: 0.594132  [   74/   88]
per-ex loss: 0.514602  [   76/   88]
per-ex loss: 0.398893  [   78/   88]
per-ex loss: 0.423075  [   80/   88]
per-ex loss: 0.559991  [   82/   88]
per-ex loss: 0.394323  [   84/   88]
per-ex loss: 0.408529  [   86/   88]
per-ex loss: 0.415476  [   88/   88]
Train Error: Avg loss: 0.45825413
validation Error: 
 Avg loss: 0.52403492 
 F1: 0.496718 
 Precision: 0.543110 
 Recall: 0.457627
 IoU: 0.330422

test Error: 
 Avg loss: 0.48233904 
 F1: 0.549070 
 Precision: 0.616067 
 Recall: 0.495215
 IoU: 0.378426

We have finished training iteration 114
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_111_.pth
per-ex loss: 0.472008  [    2/   88]
per-ex loss: 0.450833  [    4/   88]
per-ex loss: 0.503994  [    6/   88]
per-ex loss: 0.339392  [    8/   88]
per-ex loss: 0.448442  [   10/   88]
per-ex loss: 0.359579  [   12/   88]
per-ex loss: 0.360124  [   14/   88]
per-ex loss: 0.526401  [   16/   88]
per-ex loss: 0.445345  [   18/   88]
per-ex loss: 0.408229  [   20/   88]
per-ex loss: 0.483138  [   22/   88]
per-ex loss: 0.437643  [   24/   88]
per-ex loss: 0.457456  [   26/   88]
per-ex loss: 0.587336  [   28/   88]
per-ex loss: 0.484816  [   30/   88]
per-ex loss: 0.415546  [   32/   88]
per-ex loss: 0.500289  [   34/   88]
per-ex loss: 0.592633  [   36/   88]
per-ex loss: 0.484395  [   38/   88]
per-ex loss: 0.434097  [   40/   88]
per-ex loss: 0.393609  [   42/   88]
per-ex loss: 0.363715  [   44/   88]
per-ex loss: 0.447696  [   46/   88]
per-ex loss: 0.562879  [   48/   88]
per-ex loss: 0.453374  [   50/   88]
per-ex loss: 0.549592  [   52/   88]
per-ex loss: 0.425619  [   54/   88]
per-ex loss: 0.467665  [   56/   88]
per-ex loss: 0.378941  [   58/   88]
per-ex loss: 0.388988  [   60/   88]
per-ex loss: 0.355262  [   62/   88]
per-ex loss: 0.368941  [   64/   88]
per-ex loss: 0.494574  [   66/   88]
per-ex loss: 0.422638  [   68/   88]
per-ex loss: 0.478916  [   70/   88]
per-ex loss: 0.668411  [   72/   88]
per-ex loss: 0.399688  [   74/   88]
per-ex loss: 0.382882  [   76/   88]
per-ex loss: 0.471246  [   78/   88]
per-ex loss: 0.444527  [   80/   88]
per-ex loss: 0.569223  [   82/   88]
per-ex loss: 0.574994  [   84/   88]
per-ex loss: 0.433408  [   86/   88]
per-ex loss: 0.391138  [   88/   88]
Train Error: Avg loss: 0.45635506
validation Error: 
 Avg loss: 0.51501066 
 F1: 0.518890 
 Precision: 0.515927 
 Recall: 0.521886
 IoU: 0.350338

test Error: 
 Avg loss: 0.47590933 
 F1: 0.555411 
 Precision: 0.545224 
 Recall: 0.565985
 IoU: 0.384477

We have finished training iteration 115
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_113_.pth
per-ex loss: 0.438728  [    2/   88]
per-ex loss: 0.435797  [    4/   88]
per-ex loss: 0.559746  [    6/   88]
per-ex loss: 0.491164  [    8/   88]
per-ex loss: 0.340276  [   10/   88]
per-ex loss: 0.442775  [   12/   88]
per-ex loss: 0.439745  [   14/   88]
per-ex loss: 0.360561  [   16/   88]
per-ex loss: 0.603847  [   18/   88]
per-ex loss: 0.529960  [   20/   88]
per-ex loss: 0.635146  [   22/   88]
per-ex loss: 0.501598  [   24/   88]
per-ex loss: 0.459546  [   26/   88]
per-ex loss: 0.464058  [   28/   88]
per-ex loss: 0.390527  [   30/   88]
per-ex loss: 0.376054  [   32/   88]
per-ex loss: 0.422207  [   34/   88]
per-ex loss: 0.423841  [   36/   88]
per-ex loss: 0.413877  [   38/   88]
per-ex loss: 0.509846  [   40/   88]
per-ex loss: 0.423585  [   42/   88]
per-ex loss: 0.363056  [   44/   88]
per-ex loss: 0.370827  [   46/   88]
per-ex loss: 0.468065  [   48/   88]
per-ex loss: 0.556055  [   50/   88]
per-ex loss: 0.450614  [   52/   88]
per-ex loss: 0.465491  [   54/   88]
per-ex loss: 0.419662  [   56/   88]
per-ex loss: 0.482104  [   58/   88]
per-ex loss: 0.407619  [   60/   88]
per-ex loss: 0.478435  [   62/   88]
per-ex loss: 0.440425  [   64/   88]
per-ex loss: 0.413923  [   66/   88]
per-ex loss: 0.536949  [   68/   88]
per-ex loss: 0.480838  [   70/   88]
per-ex loss: 0.436001  [   72/   88]
per-ex loss: 0.370808  [   74/   88]
per-ex loss: 0.481557  [   76/   88]
per-ex loss: 0.479754  [   78/   88]
per-ex loss: 0.386147  [   80/   88]
per-ex loss: 0.572477  [   82/   88]
per-ex loss: 0.599494  [   84/   88]
per-ex loss: 0.429768  [   86/   88]
per-ex loss: 0.376982  [   88/   88]
Train Error: Avg loss: 0.45749859
validation Error: 
 Avg loss: 0.50566738 
 F1: 0.518111 
 Precision: 0.638332 
 Recall: 0.435996
 IoU: 0.349628

test Error: 
 Avg loss: 0.48630636 
 F1: 0.546632 
 Precision: 0.661740 
 Recall: 0.465636
 IoU: 0.376114

We have finished training iteration 116
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_114_.pth
per-ex loss: 0.421979  [    2/   88]
per-ex loss: 0.457211  [    4/   88]
per-ex loss: 0.481899  [    6/   88]
per-ex loss: 0.348896  [    8/   88]
per-ex loss: 0.510804  [   10/   88]
per-ex loss: 0.479407  [   12/   88]
per-ex loss: 0.454176  [   14/   88]
per-ex loss: 0.403445  [   16/   88]
per-ex loss: 0.430692  [   18/   88]
per-ex loss: 0.448567  [   20/   88]
per-ex loss: 0.362271  [   22/   88]
per-ex loss: 0.571133  [   24/   88]
per-ex loss: 0.422734  [   26/   88]
per-ex loss: 0.346321  [   28/   88]
per-ex loss: 0.363301  [   30/   88]
per-ex loss: 0.386066  [   32/   88]
per-ex loss: 0.495471  [   34/   88]
per-ex loss: 0.409786  [   36/   88]
per-ex loss: 0.381267  [   38/   88]
per-ex loss: 0.453419  [   40/   88]
per-ex loss: 0.341741  [   42/   88]
per-ex loss: 0.409178  [   44/   88]
per-ex loss: 0.456122  [   46/   88]
per-ex loss: 0.483931  [   48/   88]
per-ex loss: 0.509920  [   50/   88]
per-ex loss: 0.576338  [   52/   88]
per-ex loss: 0.567311  [   54/   88]
per-ex loss: 0.485850  [   56/   88]
per-ex loss: 0.401458  [   58/   88]
per-ex loss: 0.587864  [   60/   88]
per-ex loss: 0.433558  [   62/   88]
per-ex loss: 0.417595  [   64/   88]
per-ex loss: 0.457208  [   66/   88]
per-ex loss: 0.474921  [   68/   88]
per-ex loss: 0.571976  [   70/   88]
per-ex loss: 0.444095  [   72/   88]
per-ex loss: 0.492307  [   74/   88]
per-ex loss: 0.387674  [   76/   88]
per-ex loss: 0.381100  [   78/   88]
per-ex loss: 0.457730  [   80/   88]
per-ex loss: 0.576075  [   82/   88]
per-ex loss: 0.389920  [   84/   88]
per-ex loss: 0.610379  [   86/   88]
per-ex loss: 0.389908  [   88/   88]
Train Error: Avg loss: 0.45302280
validation Error: 
 Avg loss: 0.52968029 
 F1: 0.497925 
 Precision: 0.683885 
 Recall: 0.391476
 IoU: 0.331492

test Error: 
 Avg loss: 0.49747344 
 F1: 0.534874 
 Precision: 0.724654 
 Recall: 0.423867
 IoU: 0.365070

We have finished training iteration 117
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_115_.pth
per-ex loss: 0.446242  [    2/   88]
per-ex loss: 0.486441  [    4/   88]
per-ex loss: 0.557459  [    6/   88]
per-ex loss: 0.582997  [    8/   88]
per-ex loss: 0.378646  [   10/   88]
per-ex loss: 0.524387  [   12/   88]
per-ex loss: 0.470806  [   14/   88]
per-ex loss: 0.353633  [   16/   88]
per-ex loss: 0.385835  [   18/   88]
per-ex loss: 0.475522  [   20/   88]
per-ex loss: 0.481311  [   22/   88]
per-ex loss: 0.480047  [   24/   88]
per-ex loss: 0.491614  [   26/   88]
per-ex loss: 0.420008  [   28/   88]
per-ex loss: 0.468404  [   30/   88]
per-ex loss: 0.370541  [   32/   88]
per-ex loss: 0.542667  [   34/   88]
per-ex loss: 0.501359  [   36/   88]
per-ex loss: 0.439711  [   38/   88]
per-ex loss: 0.456076  [   40/   88]
per-ex loss: 0.488464  [   42/   88]
per-ex loss: 0.479205  [   44/   88]
per-ex loss: 0.374575  [   46/   88]
per-ex loss: 0.408161  [   48/   88]
per-ex loss: 0.531910  [   50/   88]
per-ex loss: 0.420224  [   52/   88]
per-ex loss: 0.390309  [   54/   88]
per-ex loss: 0.384283  [   56/   88]
per-ex loss: 0.424716  [   58/   88]
per-ex loss: 0.554076  [   60/   88]
per-ex loss: 0.462838  [   62/   88]
per-ex loss: 0.434386  [   64/   88]
per-ex loss: 0.611905  [   66/   88]
per-ex loss: 0.504075  [   68/   88]
per-ex loss: 0.490367  [   70/   88]
per-ex loss: 0.610979  [   72/   88]
per-ex loss: 0.437960  [   74/   88]
per-ex loss: 0.457222  [   76/   88]
per-ex loss: 0.469243  [   78/   88]
per-ex loss: 0.463818  [   80/   88]
per-ex loss: 0.360780  [   82/   88]
per-ex loss: 0.399532  [   84/   88]
per-ex loss: 0.377319  [   86/   88]
per-ex loss: 0.434520  [   88/   88]
Train Error: Avg loss: 0.46101297
validation Error: 
 Avg loss: 0.50003429 
 F1: 0.519526 
 Precision: 0.540011 
 Recall: 0.500539
 IoU: 0.350919

test Error: 
 Avg loss: 0.46664236 
 F1: 0.567005 
 Precision: 0.597955 
 Recall: 0.539101
 IoU: 0.395678

We have finished training iteration 118
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_116_.pth
per-ex loss: 0.354899  [    2/   88]
per-ex loss: 0.351200  [    4/   88]
per-ex loss: 0.412555  [    6/   88]
per-ex loss: 0.445059  [    8/   88]
per-ex loss: 0.607820  [   10/   88]
per-ex loss: 0.472817  [   12/   88]
per-ex loss: 0.508821  [   14/   88]
per-ex loss: 0.451542  [   16/   88]
per-ex loss: 0.454060  [   18/   88]
per-ex loss: 0.330484  [   20/   88]
per-ex loss: 0.561033  [   22/   88]
per-ex loss: 0.394014  [   24/   88]
per-ex loss: 0.632513  [   26/   88]
per-ex loss: 0.377834  [   28/   88]
per-ex loss: 0.430899  [   30/   88]
per-ex loss: 0.352806  [   32/   88]
per-ex loss: 0.590043  [   34/   88]
per-ex loss: 0.501775  [   36/   88]
per-ex loss: 0.486346  [   38/   88]
per-ex loss: 0.595412  [   40/   88]
per-ex loss: 0.464278  [   42/   88]
per-ex loss: 0.418100  [   44/   88]
per-ex loss: 0.408371  [   46/   88]
per-ex loss: 0.389266  [   48/   88]
per-ex loss: 0.390257  [   50/   88]
per-ex loss: 0.472449  [   52/   88]
per-ex loss: 0.439701  [   54/   88]
per-ex loss: 0.598527  [   56/   88]
per-ex loss: 0.469542  [   58/   88]
per-ex loss: 0.512839  [   60/   88]
per-ex loss: 0.356069  [   62/   88]
per-ex loss: 0.370619  [   64/   88]
per-ex loss: 0.637800  [   66/   88]
per-ex loss: 0.581547  [   68/   88]
per-ex loss: 0.442458  [   70/   88]
per-ex loss: 0.396425  [   72/   88]
per-ex loss: 0.601784  [   74/   88]
per-ex loss: 0.356074  [   76/   88]
per-ex loss: 0.428189  [   78/   88]
per-ex loss: 0.438088  [   80/   88]
per-ex loss: 0.433836  [   82/   88]
per-ex loss: 0.475830  [   84/   88]
per-ex loss: 0.536652  [   86/   88]
per-ex loss: 0.387897  [   88/   88]
Train Error: Avg loss: 0.46178475
validation Error: 
 Avg loss: 0.52834228 
 F1: 0.502463 
 Precision: 0.579715 
 Recall: 0.443379
 IoU: 0.335526

test Error: 
 Avg loss: 0.48458902 
 F1: 0.549667 
 Precision: 0.675474 
 Recall: 0.463365
 IoU: 0.378993

We have finished training iteration 119
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_117_.pth
per-ex loss: 0.617610  [    2/   88]
per-ex loss: 0.474878  [    4/   88]
per-ex loss: 0.471651  [    6/   88]
per-ex loss: 0.540629  [    8/   88]
per-ex loss: 0.402397  [   10/   88]
per-ex loss: 0.455924  [   12/   88]
per-ex loss: 0.358061  [   14/   88]
per-ex loss: 0.655592  [   16/   88]
per-ex loss: 0.402392  [   18/   88]
per-ex loss: 0.355539  [   20/   88]
per-ex loss: 0.426212  [   22/   88]
per-ex loss: 0.420356  [   24/   88]
per-ex loss: 0.498790  [   26/   88]
per-ex loss: 0.560317  [   28/   88]
per-ex loss: 0.562210  [   30/   88]
per-ex loss: 0.495852  [   32/   88]
per-ex loss: 0.394997  [   34/   88]
per-ex loss: 0.389513  [   36/   88]
per-ex loss: 0.373402  [   38/   88]
per-ex loss: 0.446357  [   40/   88]
per-ex loss: 0.477652  [   42/   88]
per-ex loss: 0.364410  [   44/   88]
per-ex loss: 0.387689  [   46/   88]
per-ex loss: 0.422925  [   48/   88]
per-ex loss: 0.393831  [   50/   88]
per-ex loss: 0.576116  [   52/   88]
per-ex loss: 0.545995  [   54/   88]
per-ex loss: 0.450112  [   56/   88]
per-ex loss: 0.492464  [   58/   88]
per-ex loss: 0.589393  [   60/   88]
per-ex loss: 0.397246  [   62/   88]
per-ex loss: 0.414072  [   64/   88]
per-ex loss: 0.367042  [   66/   88]
per-ex loss: 0.513041  [   68/   88]
per-ex loss: 0.358537  [   70/   88]
per-ex loss: 0.398469  [   72/   88]
per-ex loss: 0.409388  [   74/   88]
per-ex loss: 0.394046  [   76/   88]
per-ex loss: 0.404781  [   78/   88]
per-ex loss: 0.472470  [   80/   88]
per-ex loss: 0.423922  [   82/   88]
per-ex loss: 0.450154  [   84/   88]
per-ex loss: 0.447773  [   86/   88]
per-ex loss: 0.494329  [   88/   88]
Train Error: Avg loss: 0.45337575
validation Error: 
 Avg loss: 0.51597336 
 F1: 0.523434 
 Precision: 0.648373 
 Recall: 0.438866
 IoU: 0.354494

test Error: 
 Avg loss: 0.47530576 
 F1: 0.559734 
 Precision: 0.679346 
 Recall: 0.475936
 IoU: 0.388632

We have finished training iteration 120
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_118_.pth
per-ex loss: 0.406102  [    2/   88]
per-ex loss: 0.427079  [    4/   88]
per-ex loss: 0.514480  [    6/   88]
per-ex loss: 0.424000  [    8/   88]
per-ex loss: 0.433630  [   10/   88]
per-ex loss: 0.497137  [   12/   88]
per-ex loss: 0.399223  [   14/   88]
per-ex loss: 0.479934  [   16/   88]
per-ex loss: 0.442796  [   18/   88]
per-ex loss: 0.381401  [   20/   88]
per-ex loss: 0.337846  [   22/   88]
per-ex loss: 0.373846  [   24/   88]
per-ex loss: 0.434234  [   26/   88]
per-ex loss: 0.377914  [   28/   88]
per-ex loss: 0.433181  [   30/   88]
per-ex loss: 0.397249  [   32/   88]
per-ex loss: 0.450631  [   34/   88]
per-ex loss: 0.586721  [   36/   88]
per-ex loss: 0.388219  [   38/   88]
per-ex loss: 0.428572  [   40/   88]
per-ex loss: 0.513567  [   42/   88]
per-ex loss: 0.400705  [   44/   88]
per-ex loss: 0.502881  [   46/   88]
per-ex loss: 0.406557  [   48/   88]
per-ex loss: 0.414899  [   50/   88]
per-ex loss: 0.519909  [   52/   88]
per-ex loss: 0.504431  [   54/   88]
per-ex loss: 0.562386  [   56/   88]
per-ex loss: 0.409343  [   58/   88]
per-ex loss: 0.355076  [   60/   88]
per-ex loss: 0.547917  [   62/   88]
per-ex loss: 0.401599  [   64/   88]
per-ex loss: 0.444096  [   66/   88]
per-ex loss: 0.357620  [   68/   88]
per-ex loss: 0.382689  [   70/   88]
per-ex loss: 0.417082  [   72/   88]
per-ex loss: 0.561019  [   74/   88]
per-ex loss: 0.570507  [   76/   88]
per-ex loss: 0.471880  [   78/   88]
per-ex loss: 0.416577  [   80/   88]
per-ex loss: 0.522346  [   82/   88]
per-ex loss: 0.412369  [   84/   88]
per-ex loss: 0.405051  [   86/   88]
per-ex loss: 0.495027  [   88/   88]
Train Error: Avg loss: 0.44567557
validation Error: 
 Avg loss: 0.50680796 
 F1: 0.517567 
 Precision: 0.573951 
 Recall: 0.471271
 IoU: 0.349134

test Error: 
 Avg loss: 0.46301783 
 F1: 0.571319 
 Precision: 0.619961 
 Recall: 0.529755
 IoU: 0.399893

We have finished training iteration 121
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_119_.pth
per-ex loss: 0.384029  [    2/   88]
per-ex loss: 0.430417  [    4/   88]
per-ex loss: 0.411747  [    6/   88]
per-ex loss: 0.664512  [    8/   88]
per-ex loss: 0.382385  [   10/   88]
per-ex loss: 0.461768  [   12/   88]
per-ex loss: 0.480112  [   14/   88]
per-ex loss: 0.420896  [   16/   88]
per-ex loss: 0.528927  [   18/   88]
per-ex loss: 0.393332  [   20/   88]
per-ex loss: 0.459084  [   22/   88]
per-ex loss: 0.423613  [   24/   88]
per-ex loss: 0.486644  [   26/   88]
per-ex loss: 0.531489  [   28/   88]
per-ex loss: 0.392018  [   30/   88]
per-ex loss: 0.469352  [   32/   88]
per-ex loss: 0.562123  [   34/   88]
per-ex loss: 0.412159  [   36/   88]
per-ex loss: 0.382520  [   38/   88]
per-ex loss: 0.365905  [   40/   88]
per-ex loss: 0.462390  [   42/   88]
per-ex loss: 0.351517  [   44/   88]
per-ex loss: 0.452809  [   46/   88]
per-ex loss: 0.399550  [   48/   88]
per-ex loss: 0.480206  [   50/   88]
per-ex loss: 0.490057  [   52/   88]
per-ex loss: 0.409065  [   54/   88]
per-ex loss: 0.478200  [   56/   88]
per-ex loss: 0.506558  [   58/   88]
per-ex loss: 0.467256  [   60/   88]
per-ex loss: 0.400277  [   62/   88]
per-ex loss: 0.474932  [   64/   88]
per-ex loss: 0.391512  [   66/   88]
per-ex loss: 0.381749  [   68/   88]
per-ex loss: 0.568134  [   70/   88]
per-ex loss: 0.487758  [   72/   88]
per-ex loss: 0.478885  [   74/   88]
per-ex loss: 0.529393  [   76/   88]
per-ex loss: 0.508745  [   78/   88]
per-ex loss: 0.466015  [   80/   88]
per-ex loss: 0.400276  [   82/   88]
per-ex loss: 0.602738  [   84/   88]
per-ex loss: 0.405155  [   86/   88]
per-ex loss: 0.368526  [   88/   88]
Train Error: Avg loss: 0.45465303
validation Error: 
 Avg loss: 0.50971950 
 F1: 0.516755 
 Precision: 0.613720 
 Recall: 0.446249
 IoU: 0.348395

test Error: 
 Avg loss: 0.47018132 
 F1: 0.565166 
 Precision: 0.665388 
 Recall: 0.491184
 IoU: 0.393890

We have finished training iteration 122
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_120_.pth
per-ex loss: 0.467020  [    2/   88]
per-ex loss: 0.386404  [    4/   88]
per-ex loss: 0.412031  [    6/   88]
per-ex loss: 0.436681  [    8/   88]
per-ex loss: 0.596790  [   10/   88]
per-ex loss: 0.551376  [   12/   88]
per-ex loss: 0.413018  [   14/   88]
per-ex loss: 0.476271  [   16/   88]
per-ex loss: 0.523829  [   18/   88]
per-ex loss: 0.426174  [   20/   88]
per-ex loss: 0.406304  [   22/   88]
per-ex loss: 0.481376  [   24/   88]
per-ex loss: 0.604064  [   26/   88]
per-ex loss: 0.350578  [   28/   88]
per-ex loss: 0.353189  [   30/   88]
per-ex loss: 0.532127  [   32/   88]
per-ex loss: 0.545282  [   34/   88]
per-ex loss: 0.410033  [   36/   88]
per-ex loss: 0.359407  [   38/   88]
per-ex loss: 0.407674  [   40/   88]
per-ex loss: 0.443610  [   42/   88]
per-ex loss: 0.478792  [   44/   88]
per-ex loss: 0.366775  [   46/   88]
per-ex loss: 0.614421  [   48/   88]
per-ex loss: 0.562864  [   50/   88]
per-ex loss: 0.437106  [   52/   88]
per-ex loss: 0.506365  [   54/   88]
per-ex loss: 0.491553  [   56/   88]
per-ex loss: 0.550373  [   58/   88]
per-ex loss: 0.395526  [   60/   88]
per-ex loss: 0.380357  [   62/   88]
per-ex loss: 0.538360  [   64/   88]
per-ex loss: 0.393953  [   66/   88]
per-ex loss: 0.391325  [   68/   88]
per-ex loss: 0.370211  [   70/   88]
per-ex loss: 0.439339  [   72/   88]
per-ex loss: 0.489205  [   74/   88]
per-ex loss: 0.349904  [   76/   88]
per-ex loss: 0.462579  [   78/   88]
per-ex loss: 0.411830  [   80/   88]
per-ex loss: 0.499309  [   82/   88]
per-ex loss: 0.374558  [   84/   88]
per-ex loss: 0.525868  [   86/   88]
per-ex loss: 0.476953  [   88/   88]
Train Error: Avg loss: 0.45660831
validation Error: 
 Avg loss: 0.51856624 
 F1: 0.514420 
 Precision: 0.616530 
 Recall: 0.441327
 IoU: 0.346275

test Error: 
 Avg loss: 0.47565480 
 F1: 0.560400 
 Precision: 0.696460 
 Recall: 0.468812
 IoU: 0.389274

We have finished training iteration 123
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_121_.pth
per-ex loss: 0.545637  [    2/   88]
per-ex loss: 0.370515  [    4/   88]
per-ex loss: 0.362682  [    6/   88]
per-ex loss: 0.410900  [    8/   88]
per-ex loss: 0.493503  [   10/   88]
per-ex loss: 0.490685  [   12/   88]
per-ex loss: 0.436624  [   14/   88]
per-ex loss: 0.635307  [   16/   88]
per-ex loss: 0.513111  [   18/   88]
per-ex loss: 0.432087  [   20/   88]
per-ex loss: 0.425282  [   22/   88]
per-ex loss: 0.581966  [   24/   88]
per-ex loss: 0.422706  [   26/   88]
per-ex loss: 0.447053  [   28/   88]
per-ex loss: 0.520019  [   30/   88]
per-ex loss: 0.396393  [   32/   88]
per-ex loss: 0.469579  [   34/   88]
per-ex loss: 0.575345  [   36/   88]
per-ex loss: 0.345377  [   38/   88]
per-ex loss: 0.539536  [   40/   88]
per-ex loss: 0.398308  [   42/   88]
per-ex loss: 0.412966  [   44/   88]
per-ex loss: 0.353491  [   46/   88]
per-ex loss: 0.416193  [   48/   88]
per-ex loss: 0.422700  [   50/   88]
per-ex loss: 0.414253  [   52/   88]
per-ex loss: 0.423647  [   54/   88]
per-ex loss: 0.415427  [   56/   88]
per-ex loss: 0.439629  [   58/   88]
per-ex loss: 0.465534  [   60/   88]
per-ex loss: 0.390610  [   62/   88]
per-ex loss: 0.362975  [   64/   88]
per-ex loss: 0.584135  [   66/   88]
per-ex loss: 0.428990  [   68/   88]
per-ex loss: 0.411714  [   70/   88]
per-ex loss: 0.463626  [   72/   88]
per-ex loss: 0.675722  [   74/   88]
per-ex loss: 0.432389  [   76/   88]
per-ex loss: 0.387419  [   78/   88]
per-ex loss: 0.397026  [   80/   88]
per-ex loss: 0.429701  [   82/   88]
per-ex loss: 0.479665  [   84/   88]
per-ex loss: 0.393163  [   86/   88]
per-ex loss: 0.381328  [   88/   88]
Train Error: Avg loss: 0.44988454
validation Error: 
 Avg loss: 0.49902792 
 F1: 0.522372 
 Precision: 0.544183 
 Recall: 0.502242
 IoU: 0.353521

test Error: 
 Avg loss: 0.47367375 
 F1: 0.557357 
 Precision: 0.598127 
 Recall: 0.521791
 IoU: 0.386345

We have finished training iteration 124
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_122_.pth
per-ex loss: 0.505695  [    2/   88]
per-ex loss: 0.460985  [    4/   88]
per-ex loss: 0.510751  [    6/   88]
per-ex loss: 0.481920  [    8/   88]
per-ex loss: 0.383497  [   10/   88]
per-ex loss: 0.483383  [   12/   88]
per-ex loss: 0.387509  [   14/   88]
per-ex loss: 0.376772  [   16/   88]
per-ex loss: 0.441064  [   18/   88]
per-ex loss: 0.479502  [   20/   88]
per-ex loss: 0.547695  [   22/   88]
per-ex loss: 0.361431  [   24/   88]
per-ex loss: 0.455163  [   26/   88]
per-ex loss: 0.448928  [   28/   88]
per-ex loss: 0.577618  [   30/   88]
per-ex loss: 0.336892  [   32/   88]
per-ex loss: 0.373132  [   34/   88]
per-ex loss: 0.508997  [   36/   88]
per-ex loss: 0.366554  [   38/   88]
per-ex loss: 0.426723  [   40/   88]
per-ex loss: 0.390394  [   42/   88]
per-ex loss: 0.427906  [   44/   88]
per-ex loss: 0.535417  [   46/   88]
per-ex loss: 0.461564  [   48/   88]
per-ex loss: 0.608004  [   50/   88]
per-ex loss: 0.389877  [   52/   88]
per-ex loss: 0.465818  [   54/   88]
per-ex loss: 0.515840  [   56/   88]
per-ex loss: 0.488626  [   58/   88]
per-ex loss: 0.367459  [   60/   88]
per-ex loss: 0.452857  [   62/   88]
per-ex loss: 0.416220  [   64/   88]
per-ex loss: 0.382592  [   66/   88]
per-ex loss: 0.378287  [   68/   88]
per-ex loss: 0.564119  [   70/   88]
per-ex loss: 0.627176  [   72/   88]
per-ex loss: 0.497985  [   74/   88]
per-ex loss: 0.595800  [   76/   88]
per-ex loss: 0.400652  [   78/   88]
per-ex loss: 0.387915  [   80/   88]
per-ex loss: 0.408068  [   82/   88]
per-ex loss: 0.377570  [   84/   88]
per-ex loss: 0.461080  [   86/   88]
per-ex loss: 0.342654  [   88/   88]
Train Error: Avg loss: 0.45132024
validation Error: 
 Avg loss: 0.51047412 
 F1: 0.516709 
 Precision: 0.664173 
 Recall: 0.422830
 IoU: 0.348353

test Error: 
 Avg loss: 0.46902050 
 F1: 0.567037 
 Precision: 0.689745 
 Recall: 0.481395
 IoU: 0.395709

We have finished training iteration 125
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_123_.pth
per-ex loss: 0.568385  [    2/   88]
per-ex loss: 0.472864  [    4/   88]
per-ex loss: 0.601808  [    6/   88]
per-ex loss: 0.602905  [    8/   88]
per-ex loss: 0.466510  [   10/   88]
per-ex loss: 0.355336  [   12/   88]
per-ex loss: 0.382331  [   14/   88]
per-ex loss: 0.584594  [   16/   88]
per-ex loss: 0.460639  [   18/   88]
per-ex loss: 0.660390  [   20/   88]
per-ex loss: 0.440183  [   22/   88]
per-ex loss: 0.423058  [   24/   88]
per-ex loss: 0.374028  [   26/   88]
per-ex loss: 0.594791  [   28/   88]
per-ex loss: 0.448500  [   30/   88]
per-ex loss: 0.478691  [   32/   88]
per-ex loss: 0.409997  [   34/   88]
per-ex loss: 0.385776  [   36/   88]
per-ex loss: 0.343958  [   38/   88]
per-ex loss: 0.363975  [   40/   88]
per-ex loss: 0.344990  [   42/   88]
per-ex loss: 0.387328  [   44/   88]
per-ex loss: 0.380458  [   46/   88]
per-ex loss: 0.382861  [   48/   88]
per-ex loss: 0.470579  [   50/   88]
per-ex loss: 0.508275  [   52/   88]
per-ex loss: 0.468624  [   54/   88]
per-ex loss: 0.602906  [   56/   88]
per-ex loss: 0.547906  [   58/   88]
per-ex loss: 0.432365  [   60/   88]
per-ex loss: 0.425776  [   62/   88]
per-ex loss: 0.557178  [   64/   88]
per-ex loss: 0.559291  [   66/   88]
per-ex loss: 0.436656  [   68/   88]
per-ex loss: 0.532362  [   70/   88]
per-ex loss: 0.435497  [   72/   88]
per-ex loss: 0.382237  [   74/   88]
per-ex loss: 0.382287  [   76/   88]
per-ex loss: 0.407166  [   78/   88]
per-ex loss: 0.427698  [   80/   88]
per-ex loss: 0.542378  [   82/   88]
per-ex loss: 0.363393  [   84/   88]
per-ex loss: 0.431574  [   86/   88]
per-ex loss: 0.490439  [   88/   88]
Train Error: Avg loss: 0.46179422
validation Error: 
 Avg loss: 0.52055423 
 F1: 0.511974 
 Precision: 0.504741 
 Recall: 0.519417
 IoU: 0.344063

test Error: 
 Avg loss: 0.47173849 
 F1: 0.556901 
 Precision: 0.547338 
 Recall: 0.566804
 IoU: 0.385906

We have finished training iteration 126
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_124_.pth
per-ex loss: 0.415532  [    2/   88]
per-ex loss: 0.478366  [    4/   88]
per-ex loss: 0.628715  [    6/   88]
per-ex loss: 0.421690  [    8/   88]
per-ex loss: 0.614912  [   10/   88]
per-ex loss: 0.366521  [   12/   88]
per-ex loss: 0.573719  [   14/   88]
per-ex loss: 0.455881  [   16/   88]
per-ex loss: 0.434104  [   18/   88]
per-ex loss: 0.406826  [   20/   88]
per-ex loss: 0.511558  [   22/   88]
per-ex loss: 0.491431  [   24/   88]
per-ex loss: 0.576991  [   26/   88]
per-ex loss: 0.604991  [   28/   88]
per-ex loss: 0.554433  [   30/   88]
per-ex loss: 0.488662  [   32/   88]
per-ex loss: 0.410672  [   34/   88]
per-ex loss: 0.438844  [   36/   88]
per-ex loss: 0.368419  [   38/   88]
per-ex loss: 0.410146  [   40/   88]
per-ex loss: 0.643866  [   42/   88]
per-ex loss: 0.454498  [   44/   88]
per-ex loss: 0.564696  [   46/   88]
per-ex loss: 0.445548  [   48/   88]
per-ex loss: 0.391629  [   50/   88]
per-ex loss: 0.398029  [   52/   88]
per-ex loss: 0.582813  [   54/   88]
per-ex loss: 0.483816  [   56/   88]
per-ex loss: 0.329688  [   58/   88]
per-ex loss: 0.439834  [   60/   88]
per-ex loss: 0.410432  [   62/   88]
per-ex loss: 0.372125  [   64/   88]
per-ex loss: 0.416339  [   66/   88]
per-ex loss: 0.541019  [   68/   88]
per-ex loss: 0.420548  [   70/   88]
per-ex loss: 0.543906  [   72/   88]
per-ex loss: 0.391767  [   74/   88]
per-ex loss: 0.415778  [   76/   88]
per-ex loss: 0.434989  [   78/   88]
per-ex loss: 0.434252  [   80/   88]
per-ex loss: 0.396124  [   82/   88]
per-ex loss: 0.394207  [   84/   88]
per-ex loss: 0.355219  [   86/   88]
per-ex loss: 0.399680  [   88/   88]
Train Error: Avg loss: 0.46166399
validation Error: 
 Avg loss: 0.52255548 
 F1: 0.502231 
 Precision: 0.572473 
 Recall: 0.447342
 IoU: 0.335319

test Error: 
 Avg loss: 0.46919734 
 F1: 0.569511 
 Precision: 0.660035 
 Recall: 0.500824
 IoU: 0.398124

We have finished training iteration 127
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_125_.pth
per-ex loss: 0.450737  [    2/   88]
per-ex loss: 0.539976  [    4/   88]
per-ex loss: 0.636331  [    6/   88]
per-ex loss: 0.531005  [    8/   88]
per-ex loss: 0.385111  [   10/   88]
per-ex loss: 0.391637  [   12/   88]
per-ex loss: 0.432484  [   14/   88]
per-ex loss: 0.609031  [   16/   88]
per-ex loss: 0.369865  [   18/   88]
per-ex loss: 0.469102  [   20/   88]
per-ex loss: 0.430207  [   22/   88]
per-ex loss: 0.382092  [   24/   88]
per-ex loss: 0.428525  [   26/   88]
per-ex loss: 0.499832  [   28/   88]
per-ex loss: 0.477703  [   30/   88]
per-ex loss: 0.504456  [   32/   88]
per-ex loss: 0.484145  [   34/   88]
per-ex loss: 0.388518  [   36/   88]
per-ex loss: 0.509069  [   38/   88]
per-ex loss: 0.426399  [   40/   88]
per-ex loss: 0.507928  [   42/   88]
per-ex loss: 0.381563  [   44/   88]
per-ex loss: 0.430916  [   46/   88]
per-ex loss: 0.525689  [   48/   88]
per-ex loss: 0.445104  [   50/   88]
per-ex loss: 0.444767  [   52/   88]
per-ex loss: 0.409488  [   54/   88]
per-ex loss: 0.604965  [   56/   88]
per-ex loss: 0.357383  [   58/   88]
per-ex loss: 0.355157  [   60/   88]
per-ex loss: 0.349401  [   62/   88]
per-ex loss: 0.465033  [   64/   88]
per-ex loss: 0.471591  [   66/   88]
per-ex loss: 0.348229  [   68/   88]
per-ex loss: 0.354294  [   70/   88]
per-ex loss: 0.480557  [   72/   88]
per-ex loss: 0.435744  [   74/   88]
per-ex loss: 0.478303  [   76/   88]
per-ex loss: 0.533261  [   78/   88]
per-ex loss: 0.443316  [   80/   88]
per-ex loss: 0.423697  [   82/   88]
per-ex loss: 0.450263  [   84/   88]
per-ex loss: 0.508521  [   86/   88]
per-ex loss: 0.403088  [   88/   88]
Train Error: Avg loss: 0.45351102
validation Error: 
 Avg loss: 0.51158838 
 F1: 0.507258 
 Precision: 0.538310 
 Recall: 0.479592
 IoU: 0.339816

test Error: 
 Avg loss: 0.46992837 
 F1: 0.562920 
 Precision: 0.635634 
 Recall: 0.505134
 IoU: 0.391711

We have finished training iteration 128
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_126_.pth
per-ex loss: 0.573751  [    2/   88]
per-ex loss: 0.431792  [    4/   88]
per-ex loss: 0.398568  [    6/   88]
per-ex loss: 0.449679  [    8/   88]
per-ex loss: 0.452423  [   10/   88]
per-ex loss: 0.374899  [   12/   88]
per-ex loss: 0.608235  [   14/   88]
per-ex loss: 0.549999  [   16/   88]
per-ex loss: 0.614409  [   18/   88]
per-ex loss: 0.389625  [   20/   88]
per-ex loss: 0.424321  [   22/   88]
per-ex loss: 0.327601  [   24/   88]
per-ex loss: 0.425063  [   26/   88]
per-ex loss: 0.461379  [   28/   88]
per-ex loss: 0.537374  [   30/   88]
per-ex loss: 0.393554  [   32/   88]
per-ex loss: 0.385914  [   34/   88]
per-ex loss: 0.441850  [   36/   88]
per-ex loss: 0.372622  [   38/   88]
per-ex loss: 0.328475  [   40/   88]
per-ex loss: 0.361818  [   42/   88]
per-ex loss: 0.532048  [   44/   88]
per-ex loss: 0.426886  [   46/   88]
per-ex loss: 0.562634  [   48/   88]
per-ex loss: 0.529171  [   50/   88]
per-ex loss: 0.400474  [   52/   88]
per-ex loss: 0.433401  [   54/   88]
per-ex loss: 0.646052  [   56/   88]
per-ex loss: 0.470690  [   58/   88]
per-ex loss: 0.381239  [   60/   88]
per-ex loss: 0.444225  [   62/   88]
per-ex loss: 0.414694  [   64/   88]
per-ex loss: 0.416551  [   66/   88]
per-ex loss: 0.482567  [   68/   88]
per-ex loss: 0.411752  [   70/   88]
per-ex loss: 0.426726  [   72/   88]
per-ex loss: 0.333708  [   74/   88]
per-ex loss: 0.435462  [   76/   88]
per-ex loss: 0.443625  [   78/   88]
per-ex loss: 0.444018  [   80/   88]
per-ex loss: 0.461680  [   82/   88]
per-ex loss: 0.623750  [   84/   88]
per-ex loss: 0.383445  [   86/   88]
per-ex loss: 0.492820  [   88/   88]
Train Error: Avg loss: 0.45229479
validation Error: 
 Avg loss: 0.52796913 
 F1: 0.503318 
 Precision: 0.578195 
 Recall: 0.445610
 IoU: 0.336289

test Error: 
 Avg loss: 0.48137523 
 F1: 0.553891 
 Precision: 0.652765 
 Recall: 0.481029
 IoU: 0.383021

We have finished training iteration 129
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_127_.pth
per-ex loss: 0.435352  [    2/   88]
per-ex loss: 0.444118  [    4/   88]
per-ex loss: 0.562184  [    6/   88]
per-ex loss: 0.470300  [    8/   88]
per-ex loss: 0.572488  [   10/   88]
per-ex loss: 0.346644  [   12/   88]
per-ex loss: 0.338207  [   14/   88]
per-ex loss: 0.479722  [   16/   88]
per-ex loss: 0.575652  [   18/   88]
per-ex loss: 0.595051  [   20/   88]
per-ex loss: 0.388976  [   22/   88]
per-ex loss: 0.481289  [   24/   88]
per-ex loss: 0.385184  [   26/   88]
per-ex loss: 0.412944  [   28/   88]
per-ex loss: 0.391580  [   30/   88]
per-ex loss: 0.456763  [   32/   88]
per-ex loss: 0.368302  [   34/   88]
per-ex loss: 0.421061  [   36/   88]
per-ex loss: 0.512055  [   38/   88]
per-ex loss: 0.379636  [   40/   88]
per-ex loss: 0.367315  [   42/   88]
per-ex loss: 0.452092  [   44/   88]
per-ex loss: 0.411969  [   46/   88]
per-ex loss: 0.430475  [   48/   88]
per-ex loss: 0.467147  [   50/   88]
per-ex loss: 0.650485  [   52/   88]
per-ex loss: 0.488979  [   54/   88]
per-ex loss: 0.329938  [   56/   88]
per-ex loss: 0.366105  [   58/   88]
per-ex loss: 0.469230  [   60/   88]
per-ex loss: 0.641829  [   62/   88]
per-ex loss: 0.385191  [   64/   88]
per-ex loss: 0.386572  [   66/   88]
per-ex loss: 0.387695  [   68/   88]
per-ex loss: 0.353065  [   70/   88]
per-ex loss: 0.421197  [   72/   88]
per-ex loss: 0.398301  [   74/   88]
per-ex loss: 0.587099  [   76/   88]
per-ex loss: 0.638045  [   78/   88]
per-ex loss: 0.381063  [   80/   88]
per-ex loss: 0.495624  [   82/   88]
per-ex loss: 0.460670  [   84/   88]
per-ex loss: 0.454575  [   86/   88]
per-ex loss: 0.666050  [   88/   88]
Train Error: Avg loss: 0.45700500
validation Error: 
 Avg loss: 0.50530658 
 F1: 0.518273 
 Precision: 0.587820 
 Recall: 0.463442
 IoU: 0.349777

test Error: 
 Avg loss: 0.46705030 
 F1: 0.569070 
 Precision: 0.639457 
 Recall: 0.512642
 IoU: 0.397693

We have finished training iteration 130
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_128_.pth
per-ex loss: 0.526230  [    2/   88]
per-ex loss: 0.372333  [    4/   88]
per-ex loss: 0.449738  [    6/   88]
per-ex loss: 0.344798  [    8/   88]
per-ex loss: 0.425068  [   10/   88]
per-ex loss: 0.640299  [   12/   88]
per-ex loss: 0.431374  [   14/   88]
per-ex loss: 0.462071  [   16/   88]
per-ex loss: 0.472264  [   18/   88]
per-ex loss: 0.446808  [   20/   88]
per-ex loss: 0.488910  [   22/   88]
per-ex loss: 0.447300  [   24/   88]
per-ex loss: 0.405560  [   26/   88]
per-ex loss: 0.537222  [   28/   88]
per-ex loss: 0.352387  [   30/   88]
per-ex loss: 0.381445  [   32/   88]
per-ex loss: 0.573882  [   34/   88]
per-ex loss: 0.470686  [   36/   88]
per-ex loss: 0.514291  [   38/   88]
per-ex loss: 0.346809  [   40/   88]
per-ex loss: 0.470114  [   42/   88]
per-ex loss: 0.397746  [   44/   88]
per-ex loss: 0.497318  [   46/   88]
per-ex loss: 0.434041  [   48/   88]
per-ex loss: 0.532060  [   50/   88]
per-ex loss: 0.382767  [   52/   88]
per-ex loss: 0.355859  [   54/   88]
per-ex loss: 0.373458  [   56/   88]
per-ex loss: 0.607185  [   58/   88]
per-ex loss: 0.575558  [   60/   88]
per-ex loss: 0.369569  [   62/   88]
per-ex loss: 0.464188  [   64/   88]
per-ex loss: 0.562050  [   66/   88]
per-ex loss: 0.523363  [   68/   88]
per-ex loss: 0.389863  [   70/   88]
per-ex loss: 0.423288  [   72/   88]
per-ex loss: 0.464798  [   74/   88]
per-ex loss: 0.465938  [   76/   88]
per-ex loss: 0.352344  [   78/   88]
per-ex loss: 0.347363  [   80/   88]
per-ex loss: 0.428986  [   82/   88]
per-ex loss: 0.385570  [   84/   88]
per-ex loss: 0.393808  [   86/   88]
per-ex loss: 0.530380  [   88/   88]
Train Error: Avg loss: 0.45038840
validation Error: 
 Avg loss: 0.50907445 
 F1: 0.519409 
 Precision: 0.595530 
 Recall: 0.460543
 IoU: 0.350812

test Error: 
 Avg loss: 0.46888798 
 F1: 0.566184 
 Precision: 0.650212 
 Recall: 0.501390
 IoU: 0.394880

We have finished training iteration 131
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_129_.pth
per-ex loss: 0.377446  [    2/   88]
per-ex loss: 0.342709  [    4/   88]
per-ex loss: 0.466243  [    6/   88]
per-ex loss: 0.468488  [    8/   88]
per-ex loss: 0.594507  [   10/   88]
per-ex loss: 0.373271  [   12/   88]
per-ex loss: 0.390367  [   14/   88]
per-ex loss: 0.396655  [   16/   88]
per-ex loss: 0.427461  [   18/   88]
per-ex loss: 0.358874  [   20/   88]
per-ex loss: 0.559097  [   22/   88]
per-ex loss: 0.463210  [   24/   88]
per-ex loss: 0.507664  [   26/   88]
per-ex loss: 0.541276  [   28/   88]
per-ex loss: 0.455254  [   30/   88]
per-ex loss: 0.445683  [   32/   88]
per-ex loss: 0.497700  [   34/   88]
per-ex loss: 0.612042  [   36/   88]
per-ex loss: 0.376061  [   38/   88]
per-ex loss: 0.354717  [   40/   88]
per-ex loss: 0.589985  [   42/   88]
per-ex loss: 0.379258  [   44/   88]
per-ex loss: 0.356721  [   46/   88]
per-ex loss: 0.481649  [   48/   88]
per-ex loss: 0.402834  [   50/   88]
per-ex loss: 0.393973  [   52/   88]
per-ex loss: 0.551473  [   54/   88]
per-ex loss: 0.409475  [   56/   88]
per-ex loss: 0.551426  [   58/   88]
per-ex loss: 0.562052  [   60/   88]
per-ex loss: 0.505809  [   62/   88]
per-ex loss: 0.616159  [   64/   88]
per-ex loss: 0.414604  [   66/   88]
per-ex loss: 0.386820  [   68/   88]
per-ex loss: 0.405130  [   70/   88]
per-ex loss: 0.448539  [   72/   88]
per-ex loss: 0.357662  [   74/   88]
per-ex loss: 0.419073  [   76/   88]
per-ex loss: 0.412484  [   78/   88]
per-ex loss: 0.378554  [   80/   88]
per-ex loss: 0.372844  [   82/   88]
per-ex loss: 0.405897  [   84/   88]
per-ex loss: 0.428138  [   86/   88]
per-ex loss: 0.378848  [   88/   88]
Train Error: Avg loss: 0.44586664
validation Error: 
 Avg loss: 0.49441187 
 F1: 0.523239 
 Precision: 0.568329 
 Recall: 0.484777
 IoU: 0.354315

test Error: 
 Avg loss: 0.46943357 
 F1: 0.565598 
 Precision: 0.626066 
 Recall: 0.515781
 IoU: 0.394309

We have finished training iteration 132
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_130_.pth
per-ex loss: 0.435273  [    2/   88]
per-ex loss: 0.535619  [    4/   88]
per-ex loss: 0.402729  [    6/   88]
per-ex loss: 0.393533  [    8/   88]
per-ex loss: 0.353384  [   10/   88]
per-ex loss: 0.462312  [   12/   88]
per-ex loss: 0.363511  [   14/   88]
per-ex loss: 0.510677  [   16/   88]
per-ex loss: 0.437946  [   18/   88]
per-ex loss: 0.520116  [   20/   88]
per-ex loss: 0.468593  [   22/   88]
per-ex loss: 0.549791  [   24/   88]
per-ex loss: 0.502218  [   26/   88]
per-ex loss: 0.414129  [   28/   88]
per-ex loss: 0.414715  [   30/   88]
per-ex loss: 0.423991  [   32/   88]
per-ex loss: 0.463505  [   34/   88]
per-ex loss: 0.413810  [   36/   88]
per-ex loss: 0.413489  [   38/   88]
per-ex loss: 0.432821  [   40/   88]
per-ex loss: 0.350266  [   42/   88]
per-ex loss: 0.441057  [   44/   88]
per-ex loss: 0.336229  [   46/   88]
per-ex loss: 0.430076  [   48/   88]
per-ex loss: 0.481199  [   50/   88]
per-ex loss: 0.542050  [   52/   88]
per-ex loss: 0.432687  [   54/   88]
per-ex loss: 0.458031  [   56/   88]
per-ex loss: 0.493410  [   58/   88]
per-ex loss: 0.375431  [   60/   88]
per-ex loss: 0.561100  [   62/   88]
per-ex loss: 0.425725  [   64/   88]
per-ex loss: 0.394551  [   66/   88]
per-ex loss: 0.455330  [   68/   88]
per-ex loss: 0.453984  [   70/   88]
per-ex loss: 0.480767  [   72/   88]
per-ex loss: 0.355317  [   74/   88]
per-ex loss: 0.359710  [   76/   88]
per-ex loss: 0.376194  [   78/   88]
per-ex loss: 0.477015  [   80/   88]
per-ex loss: 0.467470  [   82/   88]
per-ex loss: 0.393093  [   84/   88]
per-ex loss: 0.490606  [   86/   88]
per-ex loss: 0.487483  [   88/   88]
Train Error: Avg loss: 0.44161230
validation Error: 
 Avg loss: 0.55414551 
 F1: 0.473527 
 Precision: 0.435411 
 Recall: 0.518957
 IoU: 0.310210

test Error: 
 Avg loss: 0.48194745 
 F1: 0.548730 
 Precision: 0.534747 
 Recall: 0.563463
 IoU: 0.378103

We have finished training iteration 133
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_131_.pth
per-ex loss: 0.392119  [    2/   88]
per-ex loss: 0.395284  [    4/   88]
per-ex loss: 0.403722  [    6/   88]
per-ex loss: 0.411290  [    8/   88]
per-ex loss: 0.452685  [   10/   88]
per-ex loss: 0.559738  [   12/   88]
per-ex loss: 0.591581  [   14/   88]
per-ex loss: 0.404711  [   16/   88]
per-ex loss: 0.410227  [   18/   88]
per-ex loss: 0.362432  [   20/   88]
per-ex loss: 0.477880  [   22/   88]
per-ex loss: 0.376618  [   24/   88]
per-ex loss: 0.361578  [   26/   88]
per-ex loss: 0.373682  [   28/   88]
per-ex loss: 0.506961  [   30/   88]
per-ex loss: 0.573560  [   32/   88]
per-ex loss: 0.379398  [   34/   88]
per-ex loss: 0.458875  [   36/   88]
per-ex loss: 0.362350  [   38/   88]
per-ex loss: 0.349652  [   40/   88]
per-ex loss: 0.533412  [   42/   88]
per-ex loss: 0.516478  [   44/   88]
per-ex loss: 0.431871  [   46/   88]
per-ex loss: 0.339698  [   48/   88]
per-ex loss: 0.631567  [   50/   88]
per-ex loss: 0.455262  [   52/   88]
per-ex loss: 0.567397  [   54/   88]
per-ex loss: 0.509583  [   56/   88]
per-ex loss: 0.466336  [   58/   88]
per-ex loss: 0.359281  [   60/   88]
per-ex loss: 0.685886  [   62/   88]
per-ex loss: 0.516498  [   64/   88]
per-ex loss: 0.540345  [   66/   88]
per-ex loss: 0.408831  [   68/   88]
per-ex loss: 0.448397  [   70/   88]
per-ex loss: 0.655559  [   72/   88]
per-ex loss: 0.383997  [   74/   88]
per-ex loss: 0.389528  [   76/   88]
per-ex loss: 0.545073  [   78/   88]
per-ex loss: 0.444492  [   80/   88]
per-ex loss: 0.425832  [   82/   88]
per-ex loss: 0.418351  [   84/   88]
per-ex loss: 0.485035  [   86/   88]
per-ex loss: 0.469638  [   88/   88]
Train Error: Avg loss: 0.45983389
validation Error: 
 Avg loss: 0.50133864 
 F1: 0.526327 
 Precision: 0.577967 
 Recall: 0.483158
 IoU: 0.357153

test Error: 
 Avg loss: 0.46242694 
 F1: 0.569060 
 Precision: 0.603484 
 Recall: 0.538352
 IoU: 0.397683

We have finished training iteration 134
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_110_.pth
per-ex loss: 0.518295  [    2/   88]
per-ex loss: 0.495413  [    4/   88]
per-ex loss: 0.451552  [    6/   88]
per-ex loss: 0.427454  [    8/   88]
per-ex loss: 0.368619  [   10/   88]
per-ex loss: 0.515403  [   12/   88]
per-ex loss: 0.550544  [   14/   88]
per-ex loss: 0.466399  [   16/   88]
per-ex loss: 0.580967  [   18/   88]
per-ex loss: 0.451477  [   20/   88]
per-ex loss: 0.354847  [   22/   88]
per-ex loss: 0.585772  [   24/   88]
per-ex loss: 0.342559  [   26/   88]
per-ex loss: 0.480755  [   28/   88]
per-ex loss: 0.381010  [   30/   88]
per-ex loss: 0.373649  [   32/   88]
per-ex loss: 0.381902  [   34/   88]
per-ex loss: 0.546757  [   36/   88]
per-ex loss: 0.424051  [   38/   88]
per-ex loss: 0.534061  [   40/   88]
per-ex loss: 0.419790  [   42/   88]
per-ex loss: 0.565791  [   44/   88]
per-ex loss: 0.491620  [   46/   88]
per-ex loss: 0.360093  [   48/   88]
per-ex loss: 0.623595  [   50/   88]
per-ex loss: 0.605626  [   52/   88]
per-ex loss: 0.415560  [   54/   88]
per-ex loss: 0.398044  [   56/   88]
per-ex loss: 0.405838  [   58/   88]
per-ex loss: 0.440281  [   60/   88]
per-ex loss: 0.363752  [   62/   88]
per-ex loss: 0.443680  [   64/   88]
per-ex loss: 0.491606  [   66/   88]
per-ex loss: 0.531755  [   68/   88]
per-ex loss: 0.345514  [   70/   88]
per-ex loss: 0.416178  [   72/   88]
per-ex loss: 0.431628  [   74/   88]
per-ex loss: 0.421857  [   76/   88]
per-ex loss: 0.435800  [   78/   88]
per-ex loss: 0.624737  [   80/   88]
per-ex loss: 0.373796  [   82/   88]
per-ex loss: 0.407664  [   84/   88]
per-ex loss: 0.360229  [   86/   88]
per-ex loss: 0.468191  [   88/   88]
Train Error: Avg loss: 0.45622985
validation Error: 
 Avg loss: 0.50998149 
 F1: 0.511453 
 Precision: 0.534138 
 Recall: 0.490617
 IoU: 0.343592

test Error: 
 Avg loss: 0.47115777 
 F1: 0.562196 
 Precision: 0.584001 
 Recall: 0.541961
 IoU: 0.391010

We have finished training iteration 135
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_133_.pth
per-ex loss: 0.460704  [    2/   88]
per-ex loss: 0.413255  [    4/   88]
per-ex loss: 0.445972  [    6/   88]
per-ex loss: 0.391879  [    8/   88]
per-ex loss: 0.449108  [   10/   88]
per-ex loss: 0.673188  [   12/   88]
per-ex loss: 0.547650  [   14/   88]
per-ex loss: 0.418021  [   16/   88]
per-ex loss: 0.369219  [   18/   88]
per-ex loss: 0.459145  [   20/   88]
per-ex loss: 0.389268  [   22/   88]
per-ex loss: 0.583494  [   24/   88]
per-ex loss: 0.626948  [   26/   88]
per-ex loss: 0.500461  [   28/   88]
per-ex loss: 0.419131  [   30/   88]
per-ex loss: 0.386643  [   32/   88]
per-ex loss: 0.434697  [   34/   88]
per-ex loss: 0.421324  [   36/   88]
per-ex loss: 0.501656  [   38/   88]
per-ex loss: 0.388907  [   40/   88]
per-ex loss: 0.410279  [   42/   88]
per-ex loss: 0.440986  [   44/   88]
per-ex loss: 0.424321  [   46/   88]
per-ex loss: 0.590166  [   48/   88]
per-ex loss: 0.557078  [   50/   88]
per-ex loss: 0.441597  [   52/   88]
per-ex loss: 0.498423  [   54/   88]
per-ex loss: 0.425995  [   56/   88]
per-ex loss: 0.357868  [   58/   88]
per-ex loss: 0.483761  [   60/   88]
per-ex loss: 0.508103  [   62/   88]
per-ex loss: 0.435907  [   64/   88]
per-ex loss: 0.427450  [   66/   88]
per-ex loss: 0.420179  [   68/   88]
per-ex loss: 0.410872  [   70/   88]
per-ex loss: 0.357792  [   72/   88]
per-ex loss: 0.372187  [   74/   88]
per-ex loss: 0.427790  [   76/   88]
per-ex loss: 0.422262  [   78/   88]
per-ex loss: 0.487306  [   80/   88]
per-ex loss: 0.363383  [   82/   88]
per-ex loss: 0.458055  [   84/   88]
per-ex loss: 0.562077  [   86/   88]
per-ex loss: 0.362248  [   88/   88]
Train Error: Avg loss: 0.45288078
validation Error: 
 Avg loss: 0.53201032 
 F1: 0.486246 
 Precision: 0.485829 
 Recall: 0.486663
 IoU: 0.321219

test Error: 
 Avg loss: 0.47878709 
 F1: 0.556751 
 Precision: 0.602522 
 Recall: 0.517443
 IoU: 0.385762

We have finished training iteration 136
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_134_.pth
per-ex loss: 0.425929  [    2/   88]
per-ex loss: 0.462873  [    4/   88]
per-ex loss: 0.371996  [    6/   88]
per-ex loss: 0.426998  [    8/   88]
per-ex loss: 0.376546  [   10/   88]
per-ex loss: 0.592137  [   12/   88]
per-ex loss: 0.478368  [   14/   88]
per-ex loss: 0.406666  [   16/   88]
per-ex loss: 0.443141  [   18/   88]
per-ex loss: 0.457951  [   20/   88]
per-ex loss: 0.567637  [   22/   88]
per-ex loss: 0.484631  [   24/   88]
per-ex loss: 0.660451  [   26/   88]
per-ex loss: 0.468516  [   28/   88]
per-ex loss: 0.476027  [   30/   88]
per-ex loss: 0.385880  [   32/   88]
per-ex loss: 0.412374  [   34/   88]
per-ex loss: 0.404129  [   36/   88]
per-ex loss: 0.453348  [   38/   88]
per-ex loss: 0.496260  [   40/   88]
per-ex loss: 0.503789  [   42/   88]
per-ex loss: 0.436644  [   44/   88]
per-ex loss: 0.353137  [   46/   88]
per-ex loss: 0.362484  [   48/   88]
per-ex loss: 0.485830  [   50/   88]
per-ex loss: 0.408137  [   52/   88]
per-ex loss: 0.527989  [   54/   88]
per-ex loss: 0.494087  [   56/   88]
per-ex loss: 0.538031  [   58/   88]
per-ex loss: 0.345104  [   60/   88]
per-ex loss: 0.534331  [   62/   88]
per-ex loss: 0.565790  [   64/   88]
per-ex loss: 0.422818  [   66/   88]
per-ex loss: 0.566266  [   68/   88]
per-ex loss: 0.360007  [   70/   88]
per-ex loss: 0.423375  [   72/   88]
per-ex loss: 0.418651  [   74/   88]
per-ex loss: 0.358053  [   76/   88]
per-ex loss: 0.505905  [   78/   88]
per-ex loss: 0.378951  [   80/   88]
per-ex loss: 0.449137  [   82/   88]
per-ex loss: 0.375544  [   84/   88]
per-ex loss: 0.427103  [   86/   88]
per-ex loss: 0.351208  [   88/   88]
Train Error: Avg loss: 0.45100523
validation Error: 
 Avg loss: 0.49414012 
 F1: 0.524245 
 Precision: 0.624152 
 Recall: 0.451909
 IoU: 0.355239

test Error: 
 Avg loss: 0.47878009 
 F1: 0.555705 
 Precision: 0.662689 
 Recall: 0.478462
 IoU: 0.384758

We have finished training iteration 137
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_135_.pth
per-ex loss: 0.377249  [    2/   88]
per-ex loss: 0.414801  [    4/   88]
per-ex loss: 0.619257  [    6/   88]
per-ex loss: 0.471223  [    8/   88]
per-ex loss: 0.414156  [   10/   88]
per-ex loss: 0.429807  [   12/   88]
per-ex loss: 0.374077  [   14/   88]
per-ex loss: 0.328406  [   16/   88]
per-ex loss: 0.389058  [   18/   88]
per-ex loss: 0.472957  [   20/   88]
per-ex loss: 0.530187  [   22/   88]
per-ex loss: 0.565362  [   24/   88]
per-ex loss: 0.526013  [   26/   88]
per-ex loss: 0.395562  [   28/   88]
per-ex loss: 0.415506  [   30/   88]
per-ex loss: 0.398788  [   32/   88]
per-ex loss: 0.350579  [   34/   88]
per-ex loss: 0.494349  [   36/   88]
per-ex loss: 0.379979  [   38/   88]
per-ex loss: 0.523773  [   40/   88]
per-ex loss: 0.410231  [   42/   88]
per-ex loss: 0.420833  [   44/   88]
per-ex loss: 0.481937  [   46/   88]
per-ex loss: 0.373524  [   48/   88]
per-ex loss: 0.452561  [   50/   88]
per-ex loss: 0.348531  [   52/   88]
per-ex loss: 0.446098  [   54/   88]
per-ex loss: 0.465015  [   56/   88]
per-ex loss: 0.545102  [   58/   88]
per-ex loss: 0.619232  [   60/   88]
per-ex loss: 0.455858  [   62/   88]
per-ex loss: 0.418527  [   64/   88]
per-ex loss: 0.468000  [   66/   88]
per-ex loss: 0.350192  [   68/   88]
per-ex loss: 0.343213  [   70/   88]
per-ex loss: 0.464770  [   72/   88]
per-ex loss: 0.414780  [   74/   88]
per-ex loss: 0.596727  [   76/   88]
per-ex loss: 0.591257  [   78/   88]
per-ex loss: 0.379276  [   80/   88]
per-ex loss: 0.445979  [   82/   88]
per-ex loss: 0.381033  [   84/   88]
per-ex loss: 0.505846  [   86/   88]
per-ex loss: 0.383149  [   88/   88]
Train Error: Avg loss: 0.44619904
validation Error: 
 Avg loss: 0.50816202 
 F1: 0.526771 
 Precision: 0.563736 
 Recall: 0.494355
 IoU: 0.357562

test Error: 
 Avg loss: 0.46153736 
 F1: 0.572502 
 Precision: 0.598232 
 Recall: 0.548894
 IoU: 0.401053

We have finished training iteration 138
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_136_.pth
per-ex loss: 0.404916  [    2/   88]
per-ex loss: 0.369035  [    4/   88]
per-ex loss: 0.539699  [    6/   88]
per-ex loss: 0.443046  [    8/   88]
per-ex loss: 0.489825  [   10/   88]
per-ex loss: 0.474689  [   12/   88]
per-ex loss: 0.413720  [   14/   88]
per-ex loss: 0.484494  [   16/   88]
per-ex loss: 0.372053  [   18/   88]
per-ex loss: 0.536682  [   20/   88]
per-ex loss: 0.376039  [   22/   88]
per-ex loss: 0.503054  [   24/   88]
per-ex loss: 0.498037  [   26/   88]
per-ex loss: 0.381249  [   28/   88]
per-ex loss: 0.350106  [   30/   88]
per-ex loss: 0.413185  [   32/   88]
per-ex loss: 0.400056  [   34/   88]
per-ex loss: 0.291433  [   36/   88]
per-ex loss: 0.472903  [   38/   88]
per-ex loss: 0.441463  [   40/   88]
per-ex loss: 0.531058  [   42/   88]
per-ex loss: 0.462268  [   44/   88]
per-ex loss: 0.376458  [   46/   88]
per-ex loss: 0.568909  [   48/   88]
per-ex loss: 0.378271  [   50/   88]
per-ex loss: 0.448163  [   52/   88]
per-ex loss: 0.388733  [   54/   88]
per-ex loss: 0.470035  [   56/   88]
per-ex loss: 0.332526  [   58/   88]
per-ex loss: 0.573578  [   60/   88]
per-ex loss: 0.443401  [   62/   88]
per-ex loss: 0.438299  [   64/   88]
per-ex loss: 0.388586  [   66/   88]
per-ex loss: 0.355275  [   68/   88]
per-ex loss: 0.553090  [   70/   88]
per-ex loss: 0.426846  [   72/   88]
per-ex loss: 0.505344  [   74/   88]
per-ex loss: 0.414950  [   76/   88]
per-ex loss: 0.553087  [   78/   88]
per-ex loss: 0.511008  [   80/   88]
per-ex loss: 0.421770  [   82/   88]
per-ex loss: 0.574343  [   84/   88]
per-ex loss: 0.579599  [   86/   88]
per-ex loss: 0.486670  [   88/   88]
Train Error: Avg loss: 0.45086247
validation Error: 
 Avg loss: 0.50550326 
 F1: 0.516912 
 Precision: 0.658617 
 Recall: 0.425388
 IoU: 0.348538

test Error: 
 Avg loss: 0.49613951 
 F1: 0.532916 
 Precision: 0.699181 
 Recall: 0.430535
 IoU: 0.363248

We have finished training iteration 139
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_132_.pth
per-ex loss: 0.496590  [    2/   88]
per-ex loss: 0.404428  [    4/   88]
per-ex loss: 0.468857  [    6/   88]
per-ex loss: 0.361083  [    8/   88]
per-ex loss: 0.414678  [   10/   88]
per-ex loss: 0.402776  [   12/   88]
per-ex loss: 0.373342  [   14/   88]
per-ex loss: 0.587630  [   16/   88]
per-ex loss: 0.397639  [   18/   88]
per-ex loss: 0.466580  [   20/   88]
per-ex loss: 0.389952  [   22/   88]
per-ex loss: 0.404615  [   24/   88]
per-ex loss: 0.450778  [   26/   88]
per-ex loss: 0.361098  [   28/   88]
per-ex loss: 0.627686  [   30/   88]
per-ex loss: 0.382955  [   32/   88]
per-ex loss: 0.552450  [   34/   88]
per-ex loss: 0.431127  [   36/   88]
per-ex loss: 0.357454  [   38/   88]
per-ex loss: 0.383984  [   40/   88]
per-ex loss: 0.365073  [   42/   88]
per-ex loss: 0.338390  [   44/   88]
per-ex loss: 0.414969  [   46/   88]
per-ex loss: 0.451236  [   48/   88]
per-ex loss: 0.519498  [   50/   88]
per-ex loss: 0.612270  [   52/   88]
per-ex loss: 0.371777  [   54/   88]
per-ex loss: 0.324160  [   56/   88]
per-ex loss: 0.431295  [   58/   88]
per-ex loss: 0.404816  [   60/   88]
per-ex loss: 0.574334  [   62/   88]
per-ex loss: 0.335020  [   64/   88]
per-ex loss: 0.357655  [   66/   88]
per-ex loss: 0.522225  [   68/   88]
per-ex loss: 0.446981  [   70/   88]
per-ex loss: 0.543014  [   72/   88]
per-ex loss: 0.426592  [   74/   88]
per-ex loss: 0.427962  [   76/   88]
per-ex loss: 0.401074  [   78/   88]
per-ex loss: 0.361645  [   80/   88]
per-ex loss: 0.582736  [   82/   88]
per-ex loss: 0.510663  [   84/   88]
per-ex loss: 0.646017  [   86/   88]
per-ex loss: 0.515898  [   88/   88]
Train Error: Avg loss: 0.44547728
validation Error: 
 Avg loss: 0.53128251 
 F1: 0.507049 
 Precision: 0.446994 
 Recall: 0.585746
 IoU: 0.339629

test Error: 
 Avg loss: 0.48473620 
 F1: 0.539121 
 Precision: 0.471375 
 Recall: 0.629607
 IoU: 0.369039

We have finished training iteration 140
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_138_.pth
per-ex loss: 0.367715  [    2/   88]
per-ex loss: 0.426859  [    4/   88]
per-ex loss: 0.419187  [    6/   88]
per-ex loss: 0.401556  [    8/   88]
per-ex loss: 0.433635  [   10/   88]
per-ex loss: 0.570738  [   12/   88]
per-ex loss: 0.569101  [   14/   88]
per-ex loss: 0.422521  [   16/   88]
per-ex loss: 0.486074  [   18/   88]
per-ex loss: 0.415983  [   20/   88]
per-ex loss: 0.434748  [   22/   88]
per-ex loss: 0.365526  [   24/   88]
per-ex loss: 0.490083  [   26/   88]
per-ex loss: 0.394601  [   28/   88]
per-ex loss: 0.401161  [   30/   88]
per-ex loss: 0.548360  [   32/   88]
per-ex loss: 0.346324  [   34/   88]
per-ex loss: 0.324068  [   36/   88]
per-ex loss: 0.566324  [   38/   88]
per-ex loss: 0.448118  [   40/   88]
per-ex loss: 0.604824  [   42/   88]
per-ex loss: 0.425974  [   44/   88]
per-ex loss: 0.398087  [   46/   88]
per-ex loss: 0.548585  [   48/   88]
per-ex loss: 0.366355  [   50/   88]
per-ex loss: 0.628641  [   52/   88]
per-ex loss: 0.576447  [   54/   88]
per-ex loss: 0.508365  [   56/   88]
per-ex loss: 0.526679  [   58/   88]
per-ex loss: 0.457569  [   60/   88]
per-ex loss: 0.404568  [   62/   88]
per-ex loss: 0.407397  [   64/   88]
per-ex loss: 0.359142  [   66/   88]
per-ex loss: 0.504676  [   68/   88]
per-ex loss: 0.451373  [   70/   88]
per-ex loss: 0.377954  [   72/   88]
per-ex loss: 0.397952  [   74/   88]
per-ex loss: 0.365223  [   76/   88]
per-ex loss: 0.421671  [   78/   88]
per-ex loss: 0.451728  [   80/   88]
per-ex loss: 0.415869  [   82/   88]
per-ex loss: 0.381701  [   84/   88]
per-ex loss: 0.475581  [   86/   88]
per-ex loss: 0.426717  [   88/   88]
Train Error: Avg loss: 0.44808551
validation Error: 
 Avg loss: 0.50453468 
 F1: 0.521118 
 Precision: 0.672238 
 Recall: 0.425471
 IoU: 0.352373

test Error: 
 Avg loss: 0.47837009 
 F1: 0.557452 
 Precision: 0.685372 
 Recall: 0.469772
 IoU: 0.386435

We have finished training iteration 141
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_139_.pth
per-ex loss: 0.383095  [    2/   88]
per-ex loss: 0.414231  [    4/   88]
per-ex loss: 0.621085  [    6/   88]
per-ex loss: 0.410067  [    8/   88]
per-ex loss: 0.439806  [   10/   88]
per-ex loss: 0.526376  [   12/   88]
per-ex loss: 0.456051  [   14/   88]
per-ex loss: 0.582656  [   16/   88]
per-ex loss: 0.406964  [   18/   88]
per-ex loss: 0.447829  [   20/   88]
per-ex loss: 0.602485  [   22/   88]
per-ex loss: 0.395548  [   24/   88]
per-ex loss: 0.461546  [   26/   88]
per-ex loss: 0.341161  [   28/   88]
per-ex loss: 0.346884  [   30/   88]
per-ex loss: 0.506010  [   32/   88]
per-ex loss: 0.350377  [   34/   88]
per-ex loss: 0.319211  [   36/   88]
per-ex loss: 0.446564  [   38/   88]
per-ex loss: 0.401068  [   40/   88]
per-ex loss: 0.521019  [   42/   88]
per-ex loss: 0.587273  [   44/   88]
per-ex loss: 0.482387  [   46/   88]
per-ex loss: 0.323187  [   48/   88]
per-ex loss: 0.444049  [   50/   88]
per-ex loss: 0.343025  [   52/   88]
per-ex loss: 0.392822  [   54/   88]
per-ex loss: 0.588945  [   56/   88]
per-ex loss: 0.412472  [   58/   88]
per-ex loss: 0.449814  [   60/   88]
per-ex loss: 0.492022  [   62/   88]
per-ex loss: 0.399398  [   64/   88]
per-ex loss: 0.415982  [   66/   88]
per-ex loss: 0.512440  [   68/   88]
per-ex loss: 0.528556  [   70/   88]
per-ex loss: 0.399564  [   72/   88]
per-ex loss: 0.346532  [   74/   88]
per-ex loss: 0.464672  [   76/   88]
per-ex loss: 0.461174  [   78/   88]
per-ex loss: 0.395024  [   80/   88]
per-ex loss: 0.503979  [   82/   88]
per-ex loss: 0.467647  [   84/   88]
per-ex loss: 0.339379  [   86/   88]
per-ex loss: 0.682033  [   88/   88]
Train Error: Avg loss: 0.45028195
validation Error: 
 Avg loss: 0.49832760 
 F1: 0.527956 
 Precision: 0.592141 
 Recall: 0.476325
 IoU: 0.358655

test Error: 
 Avg loss: 0.48945878 
 F1: 0.542394 
 Precision: 0.599274 
 Recall: 0.495375
 IoU: 0.372113

We have finished training iteration 142
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_140_.pth
per-ex loss: 0.345424  [    2/   88]
per-ex loss: 0.453251  [    4/   88]
per-ex loss: 0.318823  [    6/   88]
per-ex loss: 0.483840  [    8/   88]
per-ex loss: 0.510587  [   10/   88]
per-ex loss: 0.468341  [   12/   88]
per-ex loss: 0.379004  [   14/   88]
per-ex loss: 0.418262  [   16/   88]
per-ex loss: 0.546595  [   18/   88]
per-ex loss: 0.502298  [   20/   88]
per-ex loss: 0.367825  [   22/   88]
per-ex loss: 0.365599  [   24/   88]
per-ex loss: 0.494191  [   26/   88]
per-ex loss: 0.362968  [   28/   88]
per-ex loss: 0.383107  [   30/   88]
per-ex loss: 0.464660  [   32/   88]
per-ex loss: 0.464598  [   34/   88]
per-ex loss: 0.393191  [   36/   88]
per-ex loss: 0.482825  [   38/   88]
per-ex loss: 0.543798  [   40/   88]
per-ex loss: 0.352653  [   42/   88]
per-ex loss: 0.396441  [   44/   88]
per-ex loss: 0.457172  [   46/   88]
per-ex loss: 0.399778  [   48/   88]
per-ex loss: 0.673862  [   50/   88]
per-ex loss: 0.537201  [   52/   88]
per-ex loss: 0.604771  [   54/   88]
per-ex loss: 0.545370  [   56/   88]
per-ex loss: 0.452848  [   58/   88]
per-ex loss: 0.492685  [   60/   88]
per-ex loss: 0.433840  [   62/   88]
per-ex loss: 0.445219  [   64/   88]
per-ex loss: 0.416591  [   66/   88]
per-ex loss: 0.548466  [   68/   88]
per-ex loss: 0.405635  [   70/   88]
per-ex loss: 0.454485  [   72/   88]
per-ex loss: 0.384555  [   74/   88]
per-ex loss: 0.352208  [   76/   88]
per-ex loss: 0.426891  [   78/   88]
per-ex loss: 0.418458  [   80/   88]
per-ex loss: 0.503192  [   82/   88]
per-ex loss: 0.518717  [   84/   88]
per-ex loss: 0.461204  [   86/   88]
per-ex loss: 0.400058  [   88/   88]
Train Error: Avg loss: 0.45071564
validation Error: 
 Avg loss: 0.50167403 
 F1: 0.524425 
 Precision: 0.582556 
 Recall: 0.476843
 IoU: 0.355404

test Error: 
 Avg loss: 0.46432894 
 F1: 0.568372 
 Precision: 0.643320 
 Recall: 0.509065
 IoU: 0.397011

We have finished training iteration 143
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_141_.pth
per-ex loss: 0.407748  [    2/   88]
per-ex loss: 0.482418  [    4/   88]
per-ex loss: 0.400570  [    6/   88]
per-ex loss: 0.370068  [    8/   88]
per-ex loss: 0.415645  [   10/   88]
per-ex loss: 0.430595  [   12/   88]
per-ex loss: 0.366457  [   14/   88]
per-ex loss: 0.462191  [   16/   88]
per-ex loss: 0.426979  [   18/   88]
per-ex loss: 0.617342  [   20/   88]
per-ex loss: 0.431585  [   22/   88]
per-ex loss: 0.385692  [   24/   88]
per-ex loss: 0.507096  [   26/   88]
per-ex loss: 0.424078  [   28/   88]
per-ex loss: 0.476481  [   30/   88]
per-ex loss: 0.419187  [   32/   88]
per-ex loss: 0.413482  [   34/   88]
per-ex loss: 0.392853  [   36/   88]
per-ex loss: 0.588297  [   38/   88]
per-ex loss: 0.419848  [   40/   88]
per-ex loss: 0.441867  [   42/   88]
per-ex loss: 0.413448  [   44/   88]
per-ex loss: 0.404247  [   46/   88]
per-ex loss: 0.502033  [   48/   88]
per-ex loss: 0.327930  [   50/   88]
per-ex loss: 0.338355  [   52/   88]
per-ex loss: 0.577067  [   54/   88]
per-ex loss: 0.410339  [   56/   88]
per-ex loss: 0.608344  [   58/   88]
per-ex loss: 0.438432  [   60/   88]
per-ex loss: 0.571543  [   62/   88]
per-ex loss: 0.517068  [   64/   88]
per-ex loss: 0.471351  [   66/   88]
per-ex loss: 0.395208  [   68/   88]
per-ex loss: 0.372005  [   70/   88]
per-ex loss: 0.425234  [   72/   88]
per-ex loss: 0.431020  [   74/   88]
per-ex loss: 0.412996  [   76/   88]
per-ex loss: 0.362176  [   78/   88]
per-ex loss: 0.466410  [   80/   88]
per-ex loss: 0.356968  [   82/   88]
per-ex loss: 0.611220  [   84/   88]
per-ex loss: 0.441528  [   86/   88]
per-ex loss: 0.485458  [   88/   88]
Train Error: Avg loss: 0.44592862
validation Error: 
 Avg loss: 0.53571508 
 F1: 0.495497 
 Precision: 0.527455 
 Recall: 0.467190
 IoU: 0.329343

test Error: 
 Avg loss: 0.47711107 
 F1: 0.559347 
 Precision: 0.610701 
 Recall: 0.515959
 IoU: 0.388259

We have finished training iteration 144
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_142_.pth
per-ex loss: 0.423964  [    2/   88]
per-ex loss: 0.376070  [    4/   88]
per-ex loss: 0.598554  [    6/   88]
per-ex loss: 0.502111  [    8/   88]
per-ex loss: 0.473258  [   10/   88]
per-ex loss: 0.598992  [   12/   88]
per-ex loss: 0.396583  [   14/   88]
per-ex loss: 0.533705  [   16/   88]
per-ex loss: 0.653743  [   18/   88]
per-ex loss: 0.471478  [   20/   88]
per-ex loss: 0.376076  [   22/   88]
per-ex loss: 0.366555  [   24/   88]
per-ex loss: 0.556594  [   26/   88]
per-ex loss: 0.387418  [   28/   88]
per-ex loss: 0.406449  [   30/   88]
per-ex loss: 0.395461  [   32/   88]
per-ex loss: 0.421236  [   34/   88]
per-ex loss: 0.365354  [   36/   88]
per-ex loss: 0.431078  [   38/   88]
per-ex loss: 0.490299  [   40/   88]
per-ex loss: 0.628091  [   42/   88]
per-ex loss: 0.509930  [   44/   88]
per-ex loss: 0.403564  [   46/   88]
per-ex loss: 0.386957  [   48/   88]
per-ex loss: 0.355605  [   50/   88]
per-ex loss: 0.598154  [   52/   88]
per-ex loss: 0.458597  [   54/   88]
per-ex loss: 0.463248  [   56/   88]
per-ex loss: 0.340211  [   58/   88]
per-ex loss: 0.350657  [   60/   88]
per-ex loss: 0.404902  [   62/   88]
per-ex loss: 0.425230  [   64/   88]
per-ex loss: 0.411262  [   66/   88]
per-ex loss: 0.465989  [   68/   88]
per-ex loss: 0.529704  [   70/   88]
per-ex loss: 0.332782  [   72/   88]
per-ex loss: 0.395113  [   74/   88]
per-ex loss: 0.551773  [   76/   88]
per-ex loss: 0.394365  [   78/   88]
per-ex loss: 0.522348  [   80/   88]
per-ex loss: 0.407502  [   82/   88]
per-ex loss: 0.386697  [   84/   88]
per-ex loss: 0.367381  [   86/   88]
per-ex loss: 0.381959  [   88/   88]
Train Error: Avg loss: 0.44765902
validation Error: 
 Avg loss: 0.50242200 
 F1: 0.520706 
 Precision: 0.561920 
 Recall: 0.485124
 IoU: 0.351996

test Error: 
 Avg loss: 0.46617762 
 F1: 0.566163 
 Precision: 0.609593 
 Recall: 0.528509
 IoU: 0.394859

We have finished training iteration 145
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_143_.pth
per-ex loss: 0.388759  [    2/   88]
per-ex loss: 0.451073  [    4/   88]
per-ex loss: 0.403050  [    6/   88]
per-ex loss: 0.548505  [    8/   88]
per-ex loss: 0.343265  [   10/   88]
per-ex loss: 0.486048  [   12/   88]
per-ex loss: 0.358761  [   14/   88]
per-ex loss: 0.476971  [   16/   88]
per-ex loss: 0.348365  [   18/   88]
per-ex loss: 0.550024  [   20/   88]
per-ex loss: 0.454619  [   22/   88]
per-ex loss: 0.410883  [   24/   88]
per-ex loss: 0.599329  [   26/   88]
per-ex loss: 0.437185  [   28/   88]
per-ex loss: 0.343394  [   30/   88]
per-ex loss: 0.356512  [   32/   88]
per-ex loss: 0.491120  [   34/   88]
per-ex loss: 0.636787  [   36/   88]
per-ex loss: 0.396450  [   38/   88]
per-ex loss: 0.505674  [   40/   88]
per-ex loss: 0.458180  [   42/   88]
per-ex loss: 0.479339  [   44/   88]
per-ex loss: 0.495438  [   46/   88]
per-ex loss: 0.405021  [   48/   88]
per-ex loss: 0.392964  [   50/   88]
per-ex loss: 0.605631  [   52/   88]
per-ex loss: 0.476168  [   54/   88]
per-ex loss: 0.519910  [   56/   88]
per-ex loss: 0.513152  [   58/   88]
per-ex loss: 0.355541  [   60/   88]
per-ex loss: 0.348191  [   62/   88]
per-ex loss: 0.356971  [   64/   88]
per-ex loss: 0.447761  [   66/   88]
per-ex loss: 0.550152  [   68/   88]
per-ex loss: 0.422741  [   70/   88]
per-ex loss: 0.457429  [   72/   88]
per-ex loss: 0.418323  [   74/   88]
per-ex loss: 0.353021  [   76/   88]
per-ex loss: 0.342408  [   78/   88]
per-ex loss: 0.403057  [   80/   88]
per-ex loss: 0.443286  [   82/   88]
per-ex loss: 0.499289  [   84/   88]
per-ex loss: 0.505723  [   86/   88]
per-ex loss: 0.440773  [   88/   88]
Train Error: Avg loss: 0.44721010
validation Error: 
 Avg loss: 0.49977863 
 F1: 0.528975 
 Precision: 0.577384 
 Recall: 0.488055
 IoU: 0.359596

test Error: 
 Avg loss: 0.45809441 
 F1: 0.574631 
 Precision: 0.592901 
 Recall: 0.557452
 IoU: 0.403145

We have finished training iteration 146
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_144_.pth
per-ex loss: 0.341366  [    2/   88]
per-ex loss: 0.420811  [    4/   88]
per-ex loss: 0.432068  [    6/   88]
per-ex loss: 0.437105  [    8/   88]
per-ex loss: 0.513604  [   10/   88]
per-ex loss: 0.581737  [   12/   88]
per-ex loss: 0.405497  [   14/   88]
per-ex loss: 0.388402  [   16/   88]
per-ex loss: 0.452137  [   18/   88]
per-ex loss: 0.414392  [   20/   88]
per-ex loss: 0.603912  [   22/   88]
per-ex loss: 0.436087  [   24/   88]
per-ex loss: 0.359333  [   26/   88]
per-ex loss: 0.395853  [   28/   88]
per-ex loss: 0.445783  [   30/   88]
per-ex loss: 0.327360  [   32/   88]
per-ex loss: 0.419875  [   34/   88]
per-ex loss: 0.493319  [   36/   88]
per-ex loss: 0.365363  [   38/   88]
per-ex loss: 0.600159  [   40/   88]
per-ex loss: 0.491979  [   42/   88]
per-ex loss: 0.499680  [   44/   88]
per-ex loss: 0.490320  [   46/   88]
per-ex loss: 0.414935  [   48/   88]
per-ex loss: 0.430095  [   50/   88]
per-ex loss: 0.450781  [   52/   88]
per-ex loss: 0.353919  [   54/   88]
per-ex loss: 0.378771  [   56/   88]
per-ex loss: 0.562220  [   58/   88]
per-ex loss: 0.415978  [   60/   88]
per-ex loss: 0.457937  [   62/   88]
per-ex loss: 0.421268  [   64/   88]
per-ex loss: 0.560466  [   66/   88]
per-ex loss: 0.502010  [   68/   88]
per-ex loss: 0.524108  [   70/   88]
per-ex loss: 0.373646  [   72/   88]
per-ex loss: 0.621866  [   74/   88]
per-ex loss: 0.452031  [   76/   88]
per-ex loss: 0.414758  [   78/   88]
per-ex loss: 0.467877  [   80/   88]
per-ex loss: 0.514819  [   82/   88]
per-ex loss: 0.364882  [   84/   88]
per-ex loss: 0.450345  [   86/   88]
per-ex loss: 0.434399  [   88/   88]
Train Error: Avg loss: 0.45189211
validation Error: 
 Avg loss: 0.51391629 
 F1: 0.512014 
 Precision: 0.625814 
 Recall: 0.433233
 IoU: 0.344099

test Error: 
 Avg loss: 0.47261650 
 F1: 0.562085 
 Precision: 0.678305 
 Recall: 0.479866
 IoU: 0.390903

We have finished training iteration 147
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_145_.pth
per-ex loss: 0.373700  [    2/   88]
per-ex loss: 0.475542  [    4/   88]
per-ex loss: 0.488893  [    6/   88]
per-ex loss: 0.645365  [    8/   88]
per-ex loss: 0.501467  [   10/   88]
per-ex loss: 0.433644  [   12/   88]
per-ex loss: 0.393356  [   14/   88]
per-ex loss: 0.398973  [   16/   88]
per-ex loss: 0.410800  [   18/   88]
per-ex loss: 0.590971  [   20/   88]
per-ex loss: 0.461177  [   22/   88]
per-ex loss: 0.371804  [   24/   88]
per-ex loss: 0.350130  [   26/   88]
per-ex loss: 0.446218  [   28/   88]
per-ex loss: 0.396185  [   30/   88]
per-ex loss: 0.478790  [   32/   88]
per-ex loss: 0.459988  [   34/   88]
per-ex loss: 0.481637  [   36/   88]
per-ex loss: 0.515566  [   38/   88]
per-ex loss: 0.376629  [   40/   88]
per-ex loss: 0.578535  [   42/   88]
per-ex loss: 0.410717  [   44/   88]
per-ex loss: 0.644984  [   46/   88]
per-ex loss: 0.403301  [   48/   88]
per-ex loss: 0.570771  [   50/   88]
per-ex loss: 0.476340  [   52/   88]
per-ex loss: 0.408152  [   54/   88]
per-ex loss: 0.388013  [   56/   88]
per-ex loss: 0.385694  [   58/   88]
per-ex loss: 0.502116  [   60/   88]
per-ex loss: 0.576256  [   62/   88]
per-ex loss: 0.494816  [   64/   88]
per-ex loss: 0.503230  [   66/   88]
per-ex loss: 0.466343  [   68/   88]
per-ex loss: 0.366250  [   70/   88]
per-ex loss: 0.451681  [   72/   88]
per-ex loss: 0.364446  [   74/   88]
per-ex loss: 0.348056  [   76/   88]
per-ex loss: 0.386812  [   78/   88]
per-ex loss: 0.504568  [   80/   88]
per-ex loss: 0.462606  [   82/   88]
per-ex loss: 0.340617  [   84/   88]
per-ex loss: 0.397353  [   86/   88]
per-ex loss: 0.455005  [   88/   88]
Train Error: Avg loss: 0.45312488
validation Error: 
 Avg loss: 0.49947274 
 F1: 0.519276 
 Precision: 0.605227 
 Recall: 0.454701
 IoU: 0.350690

test Error: 
 Avg loss: 0.47727242 
 F1: 0.552939 
 Precision: 0.625218 
 Recall: 0.495640
 IoU: 0.382111

We have finished training iteration 148
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_146_.pth
per-ex loss: 0.432348  [    2/   88]
per-ex loss: 0.408610  [    4/   88]
per-ex loss: 0.502442  [    6/   88]
per-ex loss: 0.370368  [    8/   88]
per-ex loss: 0.598122  [   10/   88]
per-ex loss: 0.433838  [   12/   88]
per-ex loss: 0.363981  [   14/   88]
per-ex loss: 0.503375  [   16/   88]
per-ex loss: 0.415975  [   18/   88]
per-ex loss: 0.525460  [   20/   88]
per-ex loss: 0.598273  [   22/   88]
per-ex loss: 0.434765  [   24/   88]
per-ex loss: 0.618907  [   26/   88]
per-ex loss: 0.389126  [   28/   88]
per-ex loss: 0.561917  [   30/   88]
per-ex loss: 0.545724  [   32/   88]
per-ex loss: 0.372072  [   34/   88]
per-ex loss: 0.388539  [   36/   88]
per-ex loss: 0.373388  [   38/   88]
per-ex loss: 0.558777  [   40/   88]
per-ex loss: 0.381269  [   42/   88]
per-ex loss: 0.366368  [   44/   88]
per-ex loss: 0.589379  [   46/   88]
per-ex loss: 0.410189  [   48/   88]
per-ex loss: 0.486288  [   50/   88]
per-ex loss: 0.484916  [   52/   88]
per-ex loss: 0.459802  [   54/   88]
per-ex loss: 0.435639  [   56/   88]
per-ex loss: 0.356893  [   58/   88]
per-ex loss: 0.413366  [   60/   88]
per-ex loss: 0.437752  [   62/   88]
per-ex loss: 0.469313  [   64/   88]
per-ex loss: 0.458467  [   66/   88]
per-ex loss: 0.405417  [   68/   88]
per-ex loss: 0.514763  [   70/   88]
per-ex loss: 0.320459  [   72/   88]
per-ex loss: 0.485872  [   74/   88]
per-ex loss: 0.363323  [   76/   88]
per-ex loss: 0.384244  [   78/   88]
per-ex loss: 0.372016  [   80/   88]
per-ex loss: 0.368520  [   82/   88]
per-ex loss: 0.356275  [   84/   88]
per-ex loss: 0.394721  [   86/   88]
per-ex loss: 0.381416  [   88/   88]
Train Error: Avg loss: 0.44301534
validation Error: 
 Avg loss: 0.50775413 
 F1: 0.517458 
 Precision: 0.614010 
 Recall: 0.447146
 IoU: 0.349035

test Error: 
 Avg loss: 0.46994447 
 F1: 0.565059 
 Precision: 0.663142 
 Recall: 0.492252
 IoU: 0.393785

We have finished training iteration 149
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_147_.pth
per-ex loss: 0.388036  [    2/   88]
per-ex loss: 0.369048  [    4/   88]
per-ex loss: 0.413920  [    6/   88]
per-ex loss: 0.529134  [    8/   88]
per-ex loss: 0.392653  [   10/   88]
per-ex loss: 0.627049  [   12/   88]
per-ex loss: 0.406578  [   14/   88]
per-ex loss: 0.399852  [   16/   88]
per-ex loss: 0.577327  [   18/   88]
per-ex loss: 0.384136  [   20/   88]
per-ex loss: 0.405425  [   22/   88]
per-ex loss: 0.422117  [   24/   88]
per-ex loss: 0.348409  [   26/   88]
per-ex loss: 0.419460  [   28/   88]
per-ex loss: 0.516739  [   30/   88]
per-ex loss: 0.525354  [   32/   88]
per-ex loss: 0.465729  [   34/   88]
per-ex loss: 0.461068  [   36/   88]
per-ex loss: 0.453730  [   38/   88]
per-ex loss: 0.556378  [   40/   88]
per-ex loss: 0.457993  [   42/   88]
per-ex loss: 0.324988  [   44/   88]
per-ex loss: 0.399281  [   46/   88]
per-ex loss: 0.379099  [   48/   88]
per-ex loss: 0.436123  [   50/   88]
per-ex loss: 0.568499  [   52/   88]
per-ex loss: 0.372684  [   54/   88]
per-ex loss: 0.509738  [   56/   88]
per-ex loss: 0.559670  [   58/   88]
per-ex loss: 0.597763  [   60/   88]
per-ex loss: 0.382360  [   62/   88]
per-ex loss: 0.396742  [   64/   88]
per-ex loss: 0.411826  [   66/   88]
per-ex loss: 0.397982  [   68/   88]
per-ex loss: 0.343361  [   70/   88]
per-ex loss: 0.423650  [   72/   88]
per-ex loss: 0.558516  [   74/   88]
per-ex loss: 0.406194  [   76/   88]
per-ex loss: 0.401450  [   78/   88]
per-ex loss: 0.385724  [   80/   88]
per-ex loss: 0.442898  [   82/   88]
per-ex loss: 0.496487  [   84/   88]
per-ex loss: 0.375856  [   86/   88]
per-ex loss: 0.426955  [   88/   88]
Train Error: Avg loss: 0.44359048
validation Error: 
 Avg loss: 0.50506328 
 F1: 0.516593 
 Precision: 0.548936 
 Recall: 0.487849
 IoU: 0.348248

test Error: 
 Avg loss: 0.48043838 
 F1: 0.552804 
 Precision: 0.594576 
 Recall: 0.516517
 IoU: 0.381983

We have finished training iteration 150
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_148_.pth
per-ex loss: 0.458737  [    2/   88]
per-ex loss: 0.476768  [    4/   88]
per-ex loss: 0.415683  [    6/   88]
per-ex loss: 0.387991  [    8/   88]
per-ex loss: 0.361405  [   10/   88]
per-ex loss: 0.428718  [   12/   88]
per-ex loss: 0.338472  [   14/   88]
per-ex loss: 0.564147  [   16/   88]
per-ex loss: 0.419882  [   18/   88]
per-ex loss: 0.384401  [   20/   88]
per-ex loss: 0.341169  [   22/   88]
per-ex loss: 0.324762  [   24/   88]
per-ex loss: 0.455122  [   26/   88]
per-ex loss: 0.550970  [   28/   88]
per-ex loss: 0.426018  [   30/   88]
per-ex loss: 0.404647  [   32/   88]
per-ex loss: 0.562560  [   34/   88]
per-ex loss: 0.345734  [   36/   88]
per-ex loss: 0.423005  [   38/   88]
per-ex loss: 0.402371  [   40/   88]
per-ex loss: 0.590417  [   42/   88]
per-ex loss: 0.441601  [   44/   88]
per-ex loss: 0.422611  [   46/   88]
per-ex loss: 0.387932  [   48/   88]
per-ex loss: 0.459747  [   50/   88]
per-ex loss: 0.507154  [   52/   88]
per-ex loss: 0.468670  [   54/   88]
per-ex loss: 0.461337  [   56/   88]
per-ex loss: 0.445001  [   58/   88]
per-ex loss: 0.354630  [   60/   88]
per-ex loss: 0.459066  [   62/   88]
per-ex loss: 0.427937  [   64/   88]
per-ex loss: 0.383126  [   66/   88]
per-ex loss: 0.397906  [   68/   88]
per-ex loss: 0.413578  [   70/   88]
per-ex loss: 0.413150  [   72/   88]
per-ex loss: 0.384463  [   74/   88]
per-ex loss: 0.578961  [   76/   88]
per-ex loss: 0.414673  [   78/   88]
per-ex loss: 0.450601  [   80/   88]
per-ex loss: 0.529440  [   82/   88]
per-ex loss: 0.485850  [   84/   88]
per-ex loss: 0.584347  [   86/   88]
per-ex loss: 0.577870  [   88/   88]
Train Error: Avg loss: 0.44346886
validation Error: 
 Avg loss: 0.50189883 
 F1: 0.525696 
 Precision: 0.588288 
 Recall: 0.475142
 IoU: 0.356572

test Error: 
 Avg loss: 0.46857797 
 F1: 0.565305 
 Precision: 0.628369 
 Recall: 0.513744
 IoU: 0.394024

We have finished training iteration 151
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_149_.pth
per-ex loss: 0.517877  [    2/   88]
per-ex loss: 0.477885  [    4/   88]
per-ex loss: 0.542155  [    6/   88]
per-ex loss: 0.432976  [    8/   88]
per-ex loss: 0.383609  [   10/   88]
per-ex loss: 0.423586  [   12/   88]
per-ex loss: 0.466218  [   14/   88]
per-ex loss: 0.406822  [   16/   88]
per-ex loss: 0.551333  [   18/   88]
per-ex loss: 0.348022  [   20/   88]
per-ex loss: 0.493131  [   22/   88]
per-ex loss: 0.592857  [   24/   88]
per-ex loss: 0.588229  [   26/   88]
per-ex loss: 0.606281  [   28/   88]
per-ex loss: 0.385987  [   30/   88]
per-ex loss: 0.406625  [   32/   88]
per-ex loss: 0.439429  [   34/   88]
per-ex loss: 0.552679  [   36/   88]
per-ex loss: 0.540101  [   38/   88]
per-ex loss: 0.420690  [   40/   88]
per-ex loss: 0.389435  [   42/   88]
per-ex loss: 0.457971  [   44/   88]
per-ex loss: 0.327866  [   46/   88]
per-ex loss: 0.419302  [   48/   88]
per-ex loss: 0.444178  [   50/   88]
per-ex loss: 0.369542  [   52/   88]
per-ex loss: 0.408502  [   54/   88]
per-ex loss: 0.571280  [   56/   88]
per-ex loss: 0.524361  [   58/   88]
per-ex loss: 0.394752  [   60/   88]
per-ex loss: 0.340875  [   62/   88]
per-ex loss: 0.533615  [   64/   88]
per-ex loss: 0.408033  [   66/   88]
per-ex loss: 0.385616  [   68/   88]
per-ex loss: 0.364501  [   70/   88]
per-ex loss: 0.349091  [   72/   88]
per-ex loss: 0.427581  [   74/   88]
per-ex loss: 0.453937  [   76/   88]
per-ex loss: 0.406931  [   78/   88]
per-ex loss: 0.615691  [   80/   88]
per-ex loss: 0.528374  [   82/   88]
per-ex loss: 0.418974  [   84/   88]
per-ex loss: 0.381606  [   86/   88]
per-ex loss: 0.394276  [   88/   88]
Train Error: Avg loss: 0.45210872
validation Error: 
 Avg loss: 0.51151870 
 F1: 0.519987 
 Precision: 0.630750 
 Recall: 0.442314
 IoU: 0.351340

test Error: 
 Avg loss: 0.47687977 
 F1: 0.553086 
 Precision: 0.660522 
 Recall: 0.475710
 IoU: 0.382252

We have finished training iteration 152
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_150_.pth
per-ex loss: 0.482028  [    2/   88]
per-ex loss: 0.511849  [    4/   88]
per-ex loss: 0.403049  [    6/   88]
per-ex loss: 0.375166  [    8/   88]
per-ex loss: 0.565600  [   10/   88]
per-ex loss: 0.339848  [   12/   88]
per-ex loss: 0.492427  [   14/   88]
per-ex loss: 0.375465  [   16/   88]
per-ex loss: 0.474965  [   18/   88]
per-ex loss: 0.419646  [   20/   88]
per-ex loss: 0.405595  [   22/   88]
per-ex loss: 0.406936  [   24/   88]
per-ex loss: 0.433651  [   26/   88]
per-ex loss: 0.518625  [   28/   88]
per-ex loss: 0.644203  [   30/   88]
per-ex loss: 0.356026  [   32/   88]
per-ex loss: 0.467678  [   34/   88]
per-ex loss: 0.407375  [   36/   88]
per-ex loss: 0.429253  [   38/   88]
per-ex loss: 0.336038  [   40/   88]
per-ex loss: 0.374907  [   42/   88]
per-ex loss: 0.583556  [   44/   88]
per-ex loss: 0.496051  [   46/   88]
per-ex loss: 0.518754  [   48/   88]
per-ex loss: 0.453971  [   50/   88]
per-ex loss: 0.451501  [   52/   88]
per-ex loss: 0.410785  [   54/   88]
per-ex loss: 0.411895  [   56/   88]
per-ex loss: 0.367718  [   58/   88]
per-ex loss: 0.456817  [   60/   88]
per-ex loss: 0.552782  [   62/   88]
per-ex loss: 0.370167  [   64/   88]
per-ex loss: 0.378990  [   66/   88]
per-ex loss: 0.359848  [   68/   88]
per-ex loss: 0.457338  [   70/   88]
per-ex loss: 0.446603  [   72/   88]
per-ex loss: 0.606634  [   74/   88]
per-ex loss: 0.487391  [   76/   88]
per-ex loss: 0.364207  [   78/   88]
per-ex loss: 0.373552  [   80/   88]
per-ex loss: 0.568558  [   82/   88]
per-ex loss: 0.439424  [   84/   88]
per-ex loss: 0.375323  [   86/   88]
per-ex loss: 0.497684  [   88/   88]
Train Error: Avg loss: 0.44658811
validation Error: 
 Avg loss: 0.49964578 
 F1: 0.521486 
 Precision: 0.646312 
 Recall: 0.437071
 IoU: 0.352709

test Error: 
 Avg loss: 0.46910928 
 F1: 0.565510 
 Precision: 0.681970 
 Recall: 0.483024
 IoU: 0.394224

We have finished training iteration 153
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_151_.pth
per-ex loss: 0.489605  [    2/   88]
per-ex loss: 0.450816  [    4/   88]
per-ex loss: 0.464794  [    6/   88]
per-ex loss: 0.431706  [    8/   88]
per-ex loss: 0.480245  [   10/   88]
per-ex loss: 0.491908  [   12/   88]
per-ex loss: 0.450795  [   14/   88]
per-ex loss: 0.591064  [   16/   88]
per-ex loss: 0.471097  [   18/   88]
per-ex loss: 0.359551  [   20/   88]
per-ex loss: 0.385316  [   22/   88]
per-ex loss: 0.368650  [   24/   88]
per-ex loss: 0.445754  [   26/   88]
per-ex loss: 0.349146  [   28/   88]
per-ex loss: 0.351550  [   30/   88]
per-ex loss: 0.576769  [   32/   88]
per-ex loss: 0.468456  [   34/   88]
per-ex loss: 0.526074  [   36/   88]
per-ex loss: 0.449913  [   38/   88]
per-ex loss: 0.520584  [   40/   88]
per-ex loss: 0.345192  [   42/   88]
per-ex loss: 0.640905  [   44/   88]
per-ex loss: 0.391295  [   46/   88]
per-ex loss: 0.377106  [   48/   88]
per-ex loss: 0.473318  [   50/   88]
per-ex loss: 0.418725  [   52/   88]
per-ex loss: 0.366883  [   54/   88]
per-ex loss: 0.403473  [   56/   88]
per-ex loss: 0.417345  [   58/   88]
per-ex loss: 0.372645  [   60/   88]
per-ex loss: 0.417813  [   62/   88]
per-ex loss: 0.402950  [   64/   88]
per-ex loss: 0.482600  [   66/   88]
per-ex loss: 0.412048  [   68/   88]
per-ex loss: 0.464957  [   70/   88]
per-ex loss: 0.490334  [   72/   88]
per-ex loss: 0.411920  [   74/   88]
per-ex loss: 0.603289  [   76/   88]
per-ex loss: 0.386232  [   78/   88]
per-ex loss: 0.463656  [   80/   88]
per-ex loss: 0.594921  [   82/   88]
per-ex loss: 0.374885  [   84/   88]
per-ex loss: 0.523978  [   86/   88]
per-ex loss: 0.436868  [   88/   88]
Train Error: Avg loss: 0.44993475
validation Error: 
 Avg loss: 0.50224558 
 F1: 0.521964 
 Precision: 0.540220 
 Recall: 0.504902
 IoU: 0.353147

test Error: 
 Avg loss: 0.45770856 
 F1: 0.575373 
 Precision: 0.591161 
 Recall: 0.560406
 IoU: 0.403876

We have finished training iteration 154
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_152_.pth
per-ex loss: 0.348967  [    2/   88]
per-ex loss: 0.490531  [    4/   88]
per-ex loss: 0.546048  [    6/   88]
per-ex loss: 0.497179  [    8/   88]
per-ex loss: 0.500075  [   10/   88]
per-ex loss: 0.496508  [   12/   88]
per-ex loss: 0.540730  [   14/   88]
per-ex loss: 0.466860  [   16/   88]
per-ex loss: 0.558290  [   18/   88]
per-ex loss: 0.431359  [   20/   88]
per-ex loss: 0.408329  [   22/   88]
per-ex loss: 0.442959  [   24/   88]
per-ex loss: 0.364014  [   26/   88]
per-ex loss: 0.414715  [   28/   88]
per-ex loss: 0.435185  [   30/   88]
per-ex loss: 0.357424  [   32/   88]
per-ex loss: 0.373408  [   34/   88]
per-ex loss: 0.430158  [   36/   88]
per-ex loss: 0.453782  [   38/   88]
per-ex loss: 0.364624  [   40/   88]
per-ex loss: 0.524160  [   42/   88]
per-ex loss: 0.500948  [   44/   88]
per-ex loss: 0.341222  [   46/   88]
per-ex loss: 0.563242  [   48/   88]
per-ex loss: 0.392298  [   50/   88]
per-ex loss: 0.513833  [   52/   88]
per-ex loss: 0.372035  [   54/   88]
per-ex loss: 0.385083  [   56/   88]
per-ex loss: 0.485362  [   58/   88]
per-ex loss: 0.425039  [   60/   88]
per-ex loss: 0.354926  [   62/   88]
per-ex loss: 0.478409  [   64/   88]
per-ex loss: 0.439582  [   66/   88]
per-ex loss: 0.530589  [   68/   88]
per-ex loss: 0.409943  [   70/   88]
per-ex loss: 0.363545  [   72/   88]
per-ex loss: 0.364127  [   74/   88]
per-ex loss: 0.531493  [   76/   88]
per-ex loss: 0.351524  [   78/   88]
per-ex loss: 0.644468  [   80/   88]
per-ex loss: 0.399138  [   82/   88]
per-ex loss: 0.343166  [   84/   88]
per-ex loss: 0.611326  [   86/   88]
per-ex loss: 0.357400  [   88/   88]
Train Error: Avg loss: 0.44554548
validation Error: 
 Avg loss: 0.51454159 
 F1: 0.504989 
 Precision: 0.673372 
 Recall: 0.403972
 IoU: 0.337783

test Error: 
 Avg loss: 0.49520735 
 F1: 0.538129 
 Precision: 0.717728 
 Recall: 0.430423
 IoU: 0.368110

We have finished training iteration 155
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_153_.pth
per-ex loss: 0.417866  [    2/   88]
per-ex loss: 0.389748  [    4/   88]
per-ex loss: 0.422265  [    6/   88]
per-ex loss: 0.564220  [    8/   88]
per-ex loss: 0.412714  [   10/   88]
per-ex loss: 0.437654  [   12/   88]
per-ex loss: 0.348204  [   14/   88]
per-ex loss: 0.413663  [   16/   88]
per-ex loss: 0.546016  [   18/   88]
per-ex loss: 0.498420  [   20/   88]
per-ex loss: 0.384739  [   22/   88]
per-ex loss: 0.469298  [   24/   88]
per-ex loss: 0.399154  [   26/   88]
per-ex loss: 0.370507  [   28/   88]
per-ex loss: 0.417352  [   30/   88]
per-ex loss: 0.355803  [   32/   88]
per-ex loss: 0.370739  [   34/   88]
per-ex loss: 0.642945  [   36/   88]
per-ex loss: 0.520676  [   38/   88]
per-ex loss: 0.336817  [   40/   88]
per-ex loss: 0.384246  [   42/   88]
per-ex loss: 0.543603  [   44/   88]
per-ex loss: 0.417199  [   46/   88]
per-ex loss: 0.448764  [   48/   88]
per-ex loss: 0.465946  [   50/   88]
per-ex loss: 0.611724  [   52/   88]
per-ex loss: 0.485701  [   54/   88]
per-ex loss: 0.474009  [   56/   88]
per-ex loss: 0.424138  [   58/   88]
per-ex loss: 0.565453  [   60/   88]
per-ex loss: 0.385146  [   62/   88]
per-ex loss: 0.341927  [   64/   88]
per-ex loss: 0.397425  [   66/   88]
per-ex loss: 0.363380  [   68/   88]
per-ex loss: 0.352311  [   70/   88]
per-ex loss: 0.391753  [   72/   88]
per-ex loss: 0.386549  [   74/   88]
per-ex loss: 0.435148  [   76/   88]
per-ex loss: 0.481953  [   78/   88]
per-ex loss: 0.602026  [   80/   88]
per-ex loss: 0.393391  [   82/   88]
per-ex loss: 0.362710  [   84/   88]
per-ex loss: 0.483971  [   86/   88]
per-ex loss: 0.451943  [   88/   88]
Train Error: Avg loss: 0.44020944
validation Error: 
 Avg loss: 0.51227521 
 F1: 0.515168 
 Precision: 0.638179 
 Recall: 0.431915
 IoU: 0.346954

test Error: 
 Avg loss: 0.48398749 
 F1: 0.550706 
 Precision: 0.678054 
 Recall: 0.463630
 IoU: 0.379983

We have finished training iteration 156
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_154_.pth
per-ex loss: 0.337048  [    2/   88]
per-ex loss: 0.445937  [    4/   88]
per-ex loss: 0.326967  [    6/   88]
per-ex loss: 0.366921  [    8/   88]
per-ex loss: 0.536325  [   10/   88]
per-ex loss: 0.440120  [   12/   88]
per-ex loss: 0.419656  [   14/   88]
per-ex loss: 0.517679  [   16/   88]
per-ex loss: 0.524598  [   18/   88]
per-ex loss: 0.386011  [   20/   88]
per-ex loss: 0.427074  [   22/   88]
per-ex loss: 0.355816  [   24/   88]
per-ex loss: 0.412743  [   26/   88]
per-ex loss: 0.583332  [   28/   88]
per-ex loss: 0.507341  [   30/   88]
per-ex loss: 0.491737  [   32/   88]
per-ex loss: 0.399538  [   34/   88]
per-ex loss: 0.392815  [   36/   88]
per-ex loss: 0.457102  [   38/   88]
per-ex loss: 0.371091  [   40/   88]
per-ex loss: 0.482690  [   42/   88]
per-ex loss: 0.383851  [   44/   88]
per-ex loss: 0.565908  [   46/   88]
per-ex loss: 0.472277  [   48/   88]
per-ex loss: 0.400881  [   50/   88]
per-ex loss: 0.416283  [   52/   88]
per-ex loss: 0.595643  [   54/   88]
per-ex loss: 0.479052  [   56/   88]
per-ex loss: 0.375513  [   58/   88]
per-ex loss: 0.486844  [   60/   88]
per-ex loss: 0.377670  [   62/   88]
per-ex loss: 0.439905  [   64/   88]
per-ex loss: 0.477023  [   66/   88]
per-ex loss: 0.420062  [   68/   88]
per-ex loss: 0.414324  [   70/   88]
per-ex loss: 0.429106  [   72/   88]
per-ex loss: 0.534683  [   74/   88]
per-ex loss: 0.482311  [   76/   88]
per-ex loss: 0.346116  [   78/   88]
per-ex loss: 0.403390  [   80/   88]
per-ex loss: 0.359137  [   82/   88]
per-ex loss: 0.577694  [   84/   88]
per-ex loss: 0.346771  [   86/   88]
per-ex loss: 0.415538  [   88/   88]
Train Error: Avg loss: 0.44051186
validation Error: 
 Avg loss: 0.50359978 
 F1: 0.512014 
 Precision: 0.571890 
 Recall: 0.463487
 IoU: 0.344099

test Error: 
 Avg loss: 0.46790924 
 F1: 0.567351 
 Precision: 0.650550 
 Recall: 0.503019
 IoU: 0.396015

We have finished training iteration 157
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_155_.pth
per-ex loss: 0.374705  [    2/   88]
per-ex loss: 0.332204  [    4/   88]
per-ex loss: 0.424921  [    6/   88]
per-ex loss: 0.455253  [    8/   88]
per-ex loss: 0.441683  [   10/   88]
per-ex loss: 0.312450  [   12/   88]
per-ex loss: 0.441514  [   14/   88]
per-ex loss: 0.514843  [   16/   88]
per-ex loss: 0.480780  [   18/   88]
per-ex loss: 0.409709  [   20/   88]
per-ex loss: 0.372928  [   22/   88]
per-ex loss: 0.468134  [   24/   88]
per-ex loss: 0.342093  [   26/   88]
per-ex loss: 0.372138  [   28/   88]
per-ex loss: 0.385776  [   30/   88]
per-ex loss: 0.441644  [   32/   88]
per-ex loss: 0.353154  [   34/   88]
per-ex loss: 0.356796  [   36/   88]
per-ex loss: 0.412404  [   38/   88]
per-ex loss: 0.375912  [   40/   88]
per-ex loss: 0.351072  [   42/   88]
per-ex loss: 0.531082  [   44/   88]
per-ex loss: 0.395336  [   46/   88]
per-ex loss: 0.570960  [   48/   88]
per-ex loss: 0.419600  [   50/   88]
per-ex loss: 0.594571  [   52/   88]
per-ex loss: 0.320213  [   54/   88]
per-ex loss: 0.383851  [   56/   88]
per-ex loss: 0.396039  [   58/   88]
per-ex loss: 0.452056  [   60/   88]
per-ex loss: 0.573128  [   62/   88]
per-ex loss: 0.406261  [   64/   88]
per-ex loss: 0.458361  [   66/   88]
per-ex loss: 0.418182  [   68/   88]
per-ex loss: 0.455043  [   70/   88]
per-ex loss: 0.463494  [   72/   88]
per-ex loss: 0.488412  [   74/   88]
per-ex loss: 0.316505  [   76/   88]
per-ex loss: 0.516057  [   78/   88]
per-ex loss: 0.583314  [   80/   88]
per-ex loss: 0.461159  [   82/   88]
per-ex loss: 0.423011  [   84/   88]
per-ex loss: 0.387908  [   86/   88]
per-ex loss: 0.398065  [   88/   88]
Train Error: Avg loss: 0.42801636
validation Error: 
 Avg loss: 0.50602760 
 F1: 0.522894 
 Precision: 0.506659 
 Recall: 0.540205
 IoU: 0.353999

test Error: 
 Avg loss: 0.47724118 
 F1: 0.551262 
 Precision: 0.508623 
 Recall: 0.601703
 IoU: 0.380511

We have finished training iteration 158
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_156_.pth
per-ex loss: 0.460123  [    2/   88]
per-ex loss: 0.461326  [    4/   88]
per-ex loss: 0.453893  [    6/   88]
per-ex loss: 0.434499  [    8/   88]
per-ex loss: 0.393182  [   10/   88]
per-ex loss: 0.341912  [   12/   88]
per-ex loss: 0.446645  [   14/   88]
per-ex loss: 0.325106  [   16/   88]
per-ex loss: 0.331109  [   18/   88]
per-ex loss: 0.488752  [   20/   88]
per-ex loss: 0.476847  [   22/   88]
per-ex loss: 0.551004  [   24/   88]
per-ex loss: 0.366382  [   26/   88]
per-ex loss: 0.385740  [   28/   88]
per-ex loss: 0.450046  [   30/   88]
per-ex loss: 0.566818  [   32/   88]
per-ex loss: 0.583895  [   34/   88]
per-ex loss: 0.588604  [   36/   88]
per-ex loss: 0.371867  [   38/   88]
per-ex loss: 0.408852  [   40/   88]
per-ex loss: 0.543314  [   42/   88]
per-ex loss: 0.357287  [   44/   88]
per-ex loss: 0.443232  [   46/   88]
per-ex loss: 0.534430  [   48/   88]
per-ex loss: 0.348588  [   50/   88]
per-ex loss: 0.458344  [   52/   88]
per-ex loss: 0.429259  [   54/   88]
per-ex loss: 0.389376  [   56/   88]
per-ex loss: 0.489994  [   58/   88]
per-ex loss: 0.352987  [   60/   88]
per-ex loss: 0.398667  [   62/   88]
per-ex loss: 0.483390  [   64/   88]
per-ex loss: 0.406278  [   66/   88]
per-ex loss: 0.440609  [   68/   88]
per-ex loss: 0.474815  [   70/   88]
per-ex loss: 0.337311  [   72/   88]
per-ex loss: 0.501893  [   74/   88]
per-ex loss: 0.375076  [   76/   88]
per-ex loss: 0.418205  [   78/   88]
per-ex loss: 0.520059  [   80/   88]
per-ex loss: 0.475012  [   82/   88]
per-ex loss: 0.601093  [   84/   88]
per-ex loss: 0.381865  [   86/   88]
per-ex loss: 0.384876  [   88/   88]
Train Error: Avg loss: 0.44164916
validation Error: 
 Avg loss: 0.51462466 
 F1: 0.521064 
 Precision: 0.616213 
 Recall: 0.451369
 IoU: 0.352324

test Error: 
 Avg loss: 0.47428247 
 F1: 0.562908 
 Precision: 0.678647 
 Recall: 0.480894
 IoU: 0.391699

We have finished training iteration 159
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_157_.pth
per-ex loss: 0.403271  [    2/   88]
per-ex loss: 0.361258  [    4/   88]
per-ex loss: 0.475912  [    6/   88]
per-ex loss: 0.545144  [    8/   88]
per-ex loss: 0.546352  [   10/   88]
per-ex loss: 0.545364  [   12/   88]
per-ex loss: 0.324339  [   14/   88]
per-ex loss: 0.398344  [   16/   88]
per-ex loss: 0.413134  [   18/   88]
per-ex loss: 0.436759  [   20/   88]
per-ex loss: 0.362518  [   22/   88]
per-ex loss: 0.477362  [   24/   88]
per-ex loss: 0.422576  [   26/   88]
per-ex loss: 0.522256  [   28/   88]
per-ex loss: 0.385177  [   30/   88]
per-ex loss: 0.394763  [   32/   88]
per-ex loss: 0.364037  [   34/   88]
per-ex loss: 0.601970  [   36/   88]
per-ex loss: 0.601202  [   38/   88]
per-ex loss: 0.350281  [   40/   88]
per-ex loss: 0.483774  [   42/   88]
per-ex loss: 0.390518  [   44/   88]
per-ex loss: 0.459499  [   46/   88]
per-ex loss: 0.404208  [   48/   88]
per-ex loss: 0.383827  [   50/   88]
per-ex loss: 0.371962  [   52/   88]
per-ex loss: 0.401621  [   54/   88]
per-ex loss: 0.475818  [   56/   88]
per-ex loss: 0.474856  [   58/   88]
per-ex loss: 0.364893  [   60/   88]
per-ex loss: 0.353510  [   62/   88]
per-ex loss: 0.382698  [   64/   88]
per-ex loss: 0.349535  [   66/   88]
per-ex loss: 0.648240  [   68/   88]
per-ex loss: 0.426935  [   70/   88]
per-ex loss: 0.422479  [   72/   88]
per-ex loss: 0.498865  [   74/   88]
per-ex loss: 0.371217  [   76/   88]
per-ex loss: 0.402997  [   78/   88]
per-ex loss: 0.401901  [   80/   88]
per-ex loss: 0.550581  [   82/   88]
per-ex loss: 0.442098  [   84/   88]
per-ex loss: 0.598252  [   86/   88]
per-ex loss: 0.436285  [   88/   88]
Train Error: Avg loss: 0.44155874
validation Error: 
 Avg loss: 0.50649806 
 F1: 0.516258 
 Precision: 0.491002 
 Recall: 0.544253
 IoU: 0.347943

test Error: 
 Avg loss: 0.46892938 
 F1: 0.561285 
 Precision: 0.536188 
 Recall: 0.588846
 IoU: 0.390129

We have finished training iteration 160
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_158_.pth
per-ex loss: 0.392593  [    2/   88]
per-ex loss: 0.533071  [    4/   88]
per-ex loss: 0.376694  [    6/   88]
per-ex loss: 0.391059  [    8/   88]
per-ex loss: 0.465762  [   10/   88]
per-ex loss: 0.383841  [   12/   88]
per-ex loss: 0.579863  [   14/   88]
per-ex loss: 0.355762  [   16/   88]
per-ex loss: 0.414176  [   18/   88]
per-ex loss: 0.360223  [   20/   88]
per-ex loss: 0.363946  [   22/   88]
per-ex loss: 0.390202  [   24/   88]
per-ex loss: 0.356080  [   26/   88]
per-ex loss: 0.515869  [   28/   88]
per-ex loss: 0.580356  [   30/   88]
per-ex loss: 0.511949  [   32/   88]
per-ex loss: 0.446382  [   34/   88]
per-ex loss: 0.431142  [   36/   88]
per-ex loss: 0.514982  [   38/   88]
per-ex loss: 0.521111  [   40/   88]
per-ex loss: 0.361174  [   42/   88]
per-ex loss: 0.355360  [   44/   88]
per-ex loss: 0.414010  [   46/   88]
per-ex loss: 0.602228  [   48/   88]
per-ex loss: 0.440259  [   50/   88]
per-ex loss: 0.549338  [   52/   88]
per-ex loss: 0.396101  [   54/   88]
per-ex loss: 0.427857  [   56/   88]
per-ex loss: 0.459164  [   58/   88]
per-ex loss: 0.454495  [   60/   88]
per-ex loss: 0.426773  [   62/   88]
per-ex loss: 0.469413  [   64/   88]
per-ex loss: 0.387444  [   66/   88]
per-ex loss: 0.333833  [   68/   88]
per-ex loss: 0.540771  [   70/   88]
per-ex loss: 0.413072  [   72/   88]
per-ex loss: 0.480371  [   74/   88]
per-ex loss: 0.409630  [   76/   88]
per-ex loss: 0.401134  [   78/   88]
per-ex loss: 0.513518  [   80/   88]
per-ex loss: 0.585970  [   82/   88]
per-ex loss: 0.338153  [   84/   88]
per-ex loss: 0.395475  [   86/   88]
per-ex loss: 0.488601  [   88/   88]
Train Error: Avg loss: 0.44384560
validation Error: 
 Avg loss: 0.52954005 
 F1: 0.492684 
 Precision: 0.669460 
 Recall: 0.389764
 IoU: 0.326862

test Error: 
 Avg loss: 0.52096671 
 F1: 0.507250 
 Precision: 0.699660 
 Recall: 0.397841
 IoU: 0.339809

We have finished training iteration 161
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_159_.pth
per-ex loss: 0.411647  [    2/   88]
per-ex loss: 0.344981  [    4/   88]
per-ex loss: 0.370687  [    6/   88]
per-ex loss: 0.448945  [    8/   88]
per-ex loss: 0.572100  [   10/   88]
per-ex loss: 0.364777  [   12/   88]
per-ex loss: 0.437686  [   14/   88]
per-ex loss: 0.412016  [   16/   88]
per-ex loss: 0.566798  [   18/   88]
per-ex loss: 0.427658  [   20/   88]
per-ex loss: 0.347782  [   22/   88]
per-ex loss: 0.545696  [   24/   88]
per-ex loss: 0.485126  [   26/   88]
per-ex loss: 0.426015  [   28/   88]
per-ex loss: 0.432477  [   30/   88]
per-ex loss: 0.535218  [   32/   88]
per-ex loss: 0.382402  [   34/   88]
per-ex loss: 0.377721  [   36/   88]
per-ex loss: 0.371471  [   38/   88]
per-ex loss: 0.544051  [   40/   88]
per-ex loss: 0.348078  [   42/   88]
per-ex loss: 0.392148  [   44/   88]
per-ex loss: 0.472589  [   46/   88]
per-ex loss: 0.443296  [   48/   88]
per-ex loss: 0.471905  [   50/   88]
per-ex loss: 0.414922  [   52/   88]
per-ex loss: 0.416811  [   54/   88]
per-ex loss: 0.342320  [   56/   88]
per-ex loss: 0.438436  [   58/   88]
per-ex loss: 0.447387  [   60/   88]
per-ex loss: 0.454718  [   62/   88]
per-ex loss: 0.447524  [   64/   88]
per-ex loss: 0.423636  [   66/   88]
per-ex loss: 0.473359  [   68/   88]
per-ex loss: 0.461660  [   70/   88]
per-ex loss: 0.349160  [   72/   88]
per-ex loss: 0.424674  [   74/   88]
per-ex loss: 0.402459  [   76/   88]
per-ex loss: 0.460980  [   78/   88]
per-ex loss: 0.376161  [   80/   88]
per-ex loss: 0.398920  [   82/   88]
per-ex loss: 0.525896  [   84/   88]
per-ex loss: 0.432921  [   86/   88]
per-ex loss: 0.523550  [   88/   88]
Train Error: Avg loss: 0.43519919
validation Error: 
 Avg loss: 0.50122320 
 F1: 0.526270 
 Precision: 0.612496 
 Recall: 0.461325
 IoU: 0.357101

test Error: 
 Avg loss: 0.47139718 
 F1: 0.557849 
 Precision: 0.619869 
 Recall: 0.507110
 IoU: 0.386817

We have finished training iteration 162
Deleting model ./unet_all_train/saved_model_wrapper/models/UNet_160_.pth
per-ex loss: 0.423434  [    2/   88]
per-ex loss: 0.532732  [    4/   88]
per-ex loss: 0.523765  [    6/   88]
per-ex loss: 0.438669  [    8/   88]
per-ex loss: 0.455889  [   10/   88]
per-ex loss: 0.475643  [   12/   88]
per-ex loss: 0.518193  [   14/   88]
per-ex loss: 0.520639  [   16/   88]
per-ex loss: 0.418891  [   18/   88]
per-ex loss: 0.431586  [   20/   88]
per-ex loss: 0.384154  [   22/   88]
per-ex loss: 0.541699  [   24/   88]
per-ex loss: 0.354363  [   26/   88]
per-ex loss: 0.422034  [   28/   88]
per-ex loss: 0.546703  [   30/   88]
per-ex loss: 0.382364  [   32/   88]
per-ex loss: 0.449730  [   34/   88]
per-ex loss: 0.451170  [   36/   88]
per-ex loss: 0.411711  [   38/   88]
per-ex loss: 0.394504  [   40/   88]
per-ex loss: 0.353528  [   42/   88]
per-ex loss: 0.376462  [   44/   88]
per-ex loss: 0.412095  [   46/   88]
per-ex loss: 0.551150  [   48/   88]
per-ex loss: 0.646867  [   50/   88]
per-ex loss: 0.498707  [   52/   88]
per-ex loss: 0.503433  [   54/   88]
per-ex loss: 0.455146  [   56/   88]
per-ex loss: 0.332991  [   58/   88]
per-ex loss: 0.419256  [   60/   88]
per-ex loss: 0.518692  [   62/   88]
per-ex loss: 0.410932  [   64/   88]
per-ex loss: 0.354153  [   66/   88]
per-ex loss: 0.413793  [   68/   88]
per-ex loss: 0.355661  [   70/   88]
per-ex loss: 0.411844  [   72/   88]
per-ex loss: 0.452789  [   74/   88]
per-ex loss: 0.316743  [   76/   88]
per-ex loss: 0.495820  [   78/   88]
per-ex loss: 0.379120  [   80/   88]
per-ex loss: 0.436282  [   82/   88]
per-ex loss: 0.520586  [   84/   88]
per-ex loss: 0.366580  [   86/   88]
per-ex loss: 0.334021  [   88/   88]
Train Error: Avg loss: 0.44078465
slurmstepd: error: *** STEP 16728.0 ON aga1 CANCELLED AT 2025-01-09T20:43:00 ***
