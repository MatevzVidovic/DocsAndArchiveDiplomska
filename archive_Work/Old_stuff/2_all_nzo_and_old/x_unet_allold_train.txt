unet_original_main.py do_log: True
Log file name: log_09_15-58-44_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(ptd='./Data/vein_and_sclera_data', sd='unet_allold_train', mti=200, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_original_allold.yaml', ntibp=None, ptp=None, map=None)
YAML: {'batch_size': 2, 'learning_rate': 1e-05, 'num_of_dataloader_workers': 7, 'train_epoch_size_limit': 400, 'num_epochs_per_training_iteration': 1, 'cleanup_k': 3, 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': True, 'loss_fn_name': 'MCDL', 'alphas': [], 'dataset_option': 'aug_old', 'zero_out_non_sclera': True, 'add_sclera_to_img': False, 'add_bcosfire_to_img': True, 'add_coye_to_img': True, 'model': '64_2_6', 'input_width': 2048, 'input_height': 1024, 'input_channels': 5, 'output_channels': 2, 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq'}
Validation phase: False
Namespace(ptd='./Data/vein_and_sclera_data', sd='unet_allold_train', mti=200, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_original_allold.yaml', ntibp=None, ptp=None, map=None)
Device: cuda
dataset_aug_old.py do_log: False
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 88
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 88
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 44
val dataloader num of batches: 14
test dataloader num of batches: 6
Created new model instance.
[((((0,), 0), 0), 1), ((((0,), 0), 0), 4), ((((((0,), 1), 0), 1), 0), 1), ((((((0,), 1), 0), 1), 0), 4), ((((((0,), 1), 1), 1), 0), 1), ((((((0,), 1), 1), 1), 0), 4), ((((((0,), 1), 2), 1), 0), 1), ((((((0,), 1), 2), 1), 0), 4), ((((((0,), 1), 3), 1), 0), 1), ((((((0,), 1), 3), 1), 0), 4), ((((((0,), 1), 4), 1), 0), 1), ((((((0,), 1), 4), 1), 0), 4), ((((((0,), 1), 5), 1), 0), 1), ((((((0,), 1), 5), 1), 0), 4), ((((((0,), 2), 0), 1), 0), 1), ((((((0,), 2), 0), 1), 0), 4), ((((((0,), 2), 1), 1), 0), 1), ((((((0,), 2), 1), 1), 0), 4), ((((((0,), 2), 2), 1), 0), 1), ((((((0,), 2), 2), 1), 0), 4), ((((((0,), 2), 3), 1), 0), 1), ((((((0,), 2), 3), 1), 0), 4), ((((((0,), 2), 4), 1), 0), 1), ((((((0,), 2), 4), 1), 0), 4), ((((((0,), 2), 5), 1), 0), 1), ((((((0,), 2), 5), 1), 0), 4), ((((0,), 2), 0), 0), ((((0,), 2), 1), 0), ((((0,), 2), 2), 0), ((((0,), 2), 3), 0), ((((0,), 2), 4), 0), ((((0,), 2), 5), 0)]
per-ex loss: 0.898917  [    2/   88]
per-ex loss: 0.865964  [    4/   88]
per-ex loss: 0.876999  [    6/   88]
per-ex loss: 0.887923  [    8/   88]
per-ex loss: 0.902975  [   10/   88]
per-ex loss: 0.933801  [   12/   88]
per-ex loss: 0.798218  [   14/   88]
per-ex loss: 0.839147  [   16/   88]
per-ex loss: 0.852014  [   18/   88]
per-ex loss: 0.905756  [   20/   88]
per-ex loss: 0.883334  [   22/   88]
per-ex loss: 0.681429  [   24/   88]
per-ex loss: 0.759109  [   26/   88]
per-ex loss: 0.826175  [   28/   88]
per-ex loss: 0.879246  [   30/   88]
per-ex loss: 0.817243  [   32/   88]
per-ex loss: 0.762572  [   34/   88]
per-ex loss: 0.919759  [   36/   88]
per-ex loss: 0.770573  [   38/   88]
per-ex loss: 0.805192  [   40/   88]
per-ex loss: 0.777984  [   42/   88]
per-ex loss: 0.870580  [   44/   88]
per-ex loss: 0.856587  [   46/   88]
per-ex loss: 0.912325  [   48/   88]
per-ex loss: 0.842329  [   50/   88]
per-ex loss: 0.795306  [   52/   88]
per-ex loss: 0.793931  [   54/   88]
per-ex loss: 0.805572  [   56/   88]
per-ex loss: 0.818741  [   58/   88]
per-ex loss: 0.682141  [   60/   88]
per-ex loss: 0.809599  [   62/   88]
per-ex loss: 0.744570  [   64/   88]
per-ex loss: 0.821274  [   66/   88]
per-ex loss: 0.764207  [   68/   88]
per-ex loss: 0.854002  [   70/   88]
per-ex loss: 0.873725  [   72/   88]
per-ex loss: 0.659292  [   74/   88]
per-ex loss: 0.666017  [   76/   88]
per-ex loss: 0.763888  [   78/   88]
per-ex loss: 0.818355  [   80/   88]
per-ex loss: 0.753676  [   82/   88]
per-ex loss: 0.777226  [   84/   88]
per-ex loss: 0.868637  [   86/   88]
per-ex loss: 0.901988  [   88/   88]
Train Error: Avg loss: 0.82041585
validation Error: 
 Avg loss: 0.88360118 
 F1: 0.106605 
 Precision: 0.067617 
 Recall: 0.251781
 IoU: 0.056304

test Error: 
 Avg loss: 0.88263256 
 F1: 0.074854 
 Precision: 0.044888 
 Recall: 0.225163
 IoU: 0.038882

We have finished training iteration 1
per-ex loss: 0.735632  [    2/   88]
per-ex loss: 0.794244  [    4/   88]
per-ex loss: 0.859178  [    6/   88]
per-ex loss: 0.674575  [    8/   88]
per-ex loss: 0.849543  [   10/   88]
per-ex loss: 0.817310  [   12/   88]
per-ex loss: 0.900232  [   14/   88]
per-ex loss: 0.692830  [   16/   88]
per-ex loss: 0.675777  [   18/   88]
per-ex loss: 0.649717  [   20/   88]
per-ex loss: 0.565320  [   22/   88]
per-ex loss: 0.855992  [   24/   88]
per-ex loss: 0.761914  [   26/   88]
per-ex loss: 0.727668  [   28/   88]
per-ex loss: 0.716919  [   30/   88]
per-ex loss: 0.746420  [   32/   88]
per-ex loss: 0.725537  [   34/   88]
per-ex loss: 0.752146  [   36/   88]
per-ex loss: 0.766583  [   38/   88]
per-ex loss: 0.677860  [   40/   88]
per-ex loss: 0.702129  [   42/   88]
per-ex loss: 0.798682  [   44/   88]
per-ex loss: 0.787165  [   46/   88]
per-ex loss: 0.797492  [   48/   88]
per-ex loss: 0.653249  [   50/   88]
per-ex loss: 0.780066  [   52/   88]
per-ex loss: 0.592984  [   54/   88]
per-ex loss: 0.628293  [   56/   88]
per-ex loss: 0.750805  [   58/   88]
per-ex loss: 0.639772  [   60/   88]
per-ex loss: 0.709027  [   62/   88]
per-ex loss: 0.557780  [   64/   88]
per-ex loss: 0.728969  [   66/   88]
per-ex loss: 0.758845  [   68/   88]
per-ex loss: 0.629361  [   70/   88]
per-ex loss: 0.645856  [   72/   88]
per-ex loss: 0.448047  [   74/   88]
per-ex loss: 0.695640  [   76/   88]
per-ex loss: 0.590027  [   78/   88]
per-ex loss: 0.675814  [   80/   88]
per-ex loss: 0.657562  [   82/   88]
per-ex loss: 0.628522  [   84/   88]
per-ex loss: 0.609070  [   86/   88]
per-ex loss: 0.707152  [   88/   88]
Train Error: Avg loss: 0.70722060
validation Error: 
 Avg loss: 0.65881158 
 F1: 0.418084 
 Precision: 0.461997 
 Recall: 0.381794
 IoU: 0.264290

test Error: 
 Avg loss: 0.63346729 
 F1: 0.471670 
 Precision: 0.432510 
 Recall: 0.518627
 IoU: 0.308618

We have finished training iteration 2
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.582382  [    2/   88]
per-ex loss: 0.597894  [    4/   88]
per-ex loss: 0.548992  [    6/   88]
per-ex loss: 0.607276  [    8/   88]
per-ex loss: 0.813703  [   10/   88]
per-ex loss: 0.753929  [   12/   88]
per-ex loss: 0.658636  [   14/   88]
per-ex loss: 0.724186  [   16/   88]
per-ex loss: 0.762920  [   18/   88]
per-ex loss: 0.717311  [   20/   88]
per-ex loss: 0.492152  [   22/   88]
per-ex loss: 0.761628  [   24/   88]
per-ex loss: 0.603170  [   26/   88]
per-ex loss: 0.600002  [   28/   88]
per-ex loss: 0.657512  [   30/   88]
per-ex loss: 0.815908  [   32/   88]
per-ex loss: 0.634226  [   34/   88]
per-ex loss: 0.546628  [   36/   88]
per-ex loss: 0.662399  [   38/   88]
per-ex loss: 0.545678  [   40/   88]
per-ex loss: 0.707782  [   42/   88]
per-ex loss: 0.497203  [   44/   88]
per-ex loss: 0.711956  [   46/   88]
per-ex loss: 0.591001  [   48/   88]
per-ex loss: 0.645459  [   50/   88]
per-ex loss: 0.617706  [   52/   88]
per-ex loss: 0.579468  [   54/   88]
per-ex loss: 0.536323  [   56/   88]
per-ex loss: 0.564050  [   58/   88]
per-ex loss: 0.682203  [   60/   88]
per-ex loss: 0.581552  [   62/   88]
per-ex loss: 0.658425  [   64/   88]
per-ex loss: 0.725842  [   66/   88]
per-ex loss: 0.832596  [   68/   88]
per-ex loss: 0.688735  [   70/   88]
per-ex loss: 0.527989  [   72/   88]
per-ex loss: 0.739948  [   74/   88]
per-ex loss: 0.585081  [   76/   88]
per-ex loss: 0.573518  [   78/   88]
per-ex loss: 0.520523  [   80/   88]
per-ex loss: 0.563484  [   82/   88]
per-ex loss: 0.505555  [   84/   88]
per-ex loss: 0.698424  [   86/   88]
per-ex loss: 0.716498  [   88/   88]
Train Error: Avg loss: 0.63949659
validation Error: 
 Avg loss: 0.63272598 
 F1: 0.446341 
 Precision: 0.521779 
 Recall: 0.389962
 IoU: 0.287284

test Error: 
 Avg loss: 0.60830842 
 F1: 0.501830 
 Precision: 0.547868 
 Recall: 0.462930
 IoU: 0.334962

We have finished training iteration 3
per-ex loss: 0.521349  [    2/   88]
per-ex loss: 0.710664  [    4/   88]
per-ex loss: 0.506853  [    6/   88]
per-ex loss: 0.684909  [    8/   88]
per-ex loss: 0.723166  [   10/   88]
per-ex loss: 0.516236  [   12/   88]
per-ex loss: 0.774121  [   14/   88]
per-ex loss: 0.640901  [   16/   88]
per-ex loss: 0.481913  [   18/   88]
per-ex loss: 0.625891  [   20/   88]
per-ex loss: 0.663690  [   22/   88]
per-ex loss: 0.582099  [   24/   88]
per-ex loss: 0.475263  [   26/   88]
per-ex loss: 0.600877  [   28/   88]
per-ex loss: 0.529765  [   30/   88]
per-ex loss: 0.596970  [   32/   88]
per-ex loss: 0.797398  [   34/   88]
per-ex loss: 0.624772  [   36/   88]
per-ex loss: 0.529947  [   38/   88]
per-ex loss: 0.566770  [   40/   88]
per-ex loss: 0.580897  [   42/   88]
per-ex loss: 0.512239  [   44/   88]
per-ex loss: 0.647713  [   46/   88]
per-ex loss: 0.538807  [   48/   88]
per-ex loss: 0.560106  [   50/   88]
per-ex loss: 0.517785  [   52/   88]
per-ex loss: 0.721426  [   54/   88]
per-ex loss: 0.736436  [   56/   88]
per-ex loss: 0.584410  [   58/   88]
per-ex loss: 0.544556  [   60/   88]
per-ex loss: 0.641028  [   62/   88]
per-ex loss: 0.459952  [   64/   88]
per-ex loss: 0.689333  [   66/   88]
per-ex loss: 0.523160  [   68/   88]
per-ex loss: 0.782998  [   70/   88]
per-ex loss: 0.626805  [   72/   88]
per-ex loss: 0.532080  [   74/   88]
per-ex loss: 0.453458  [   76/   88]
per-ex loss: 0.678259  [   78/   88]
per-ex loss: 0.453037  [   80/   88]
per-ex loss: 0.500543  [   82/   88]
per-ex loss: 0.473598  [   84/   88]
per-ex loss: 0.595630  [   86/   88]
per-ex loss: 0.674301  [   88/   88]
Train Error: Avg loss: 0.59504798
validation Error: 
 Avg loss: 0.61271880 
 F1: 0.455121 
 Precision: 0.516667 
 Recall: 0.406677
 IoU: 0.294600

test Error: 
 Avg loss: 0.58067141 
 F1: 0.512684 
 Precision: 0.533781 
 Recall: 0.493191
 IoU: 0.344704

We have finished training iteration 4
per-ex loss: 0.654215  [    2/   88]
per-ex loss: 0.626597  [    4/   88]
per-ex loss: 0.721821  [    6/   88]
per-ex loss: 0.733176  [    8/   88]
per-ex loss: 0.580848  [   10/   88]
per-ex loss: 0.686194  [   12/   88]
per-ex loss: 0.711669  [   14/   88]
per-ex loss: 0.446631  [   16/   88]
per-ex loss: 0.776358  [   18/   88]
per-ex loss: 0.480603  [   20/   88]
per-ex loss: 0.504651  [   22/   88]
per-ex loss: 0.738086  [   24/   88]
per-ex loss: 0.492807  [   26/   88]
per-ex loss: 0.635931  [   28/   88]
per-ex loss: 0.481623  [   30/   88]
per-ex loss: 0.634376  [   32/   88]
per-ex loss: 0.633745  [   34/   88]
per-ex loss: 0.603566  [   36/   88]
per-ex loss: 0.567939  [   38/   88]
per-ex loss: 0.463404  [   40/   88]
per-ex loss: 0.503280  [   42/   88]
per-ex loss: 0.546738  [   44/   88]
per-ex loss: 0.565978  [   46/   88]
per-ex loss: 0.549612  [   48/   88]
per-ex loss: 0.524789  [   50/   88]
per-ex loss: 0.469581  [   52/   88]
per-ex loss: 0.743431  [   54/   88]
per-ex loss: 0.529941  [   56/   88]
per-ex loss: 0.481237  [   58/   88]
per-ex loss: 0.531356  [   60/   88]
per-ex loss: 0.541291  [   62/   88]
per-ex loss: 0.576872  [   64/   88]
per-ex loss: 0.570978  [   66/   88]
per-ex loss: 0.480968  [   68/   88]
per-ex loss: 0.626958  [   70/   88]
per-ex loss: 0.564254  [   72/   88]
per-ex loss: 0.497163  [   74/   88]
per-ex loss: 0.686986  [   76/   88]
per-ex loss: 0.566413  [   78/   88]
per-ex loss: 0.623007  [   80/   88]
per-ex loss: 0.754577  [   82/   88]
per-ex loss: 0.477563  [   84/   88]
per-ex loss: 0.459114  [   86/   88]
per-ex loss: 0.544038  [   88/   88]
Train Error: Avg loss: 0.58159925
validation Error: 
 Avg loss: 0.58772288 
 F1: 0.459114 
 Precision: 0.547598 
 Recall: 0.395247
 IoU: 0.297954

test Error: 
 Avg loss: 0.55672703 
 F1: 0.512820 
 Precision: 0.581052 
 Recall: 0.458928
 IoU: 0.344827

We have finished training iteration 5
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_1_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.747674  [    2/   88]
per-ex loss: 0.498985  [    4/   88]
per-ex loss: 0.564506  [    6/   88]
per-ex loss: 0.765613  [    8/   88]
per-ex loss: 0.710813  [   10/   88]
per-ex loss: 0.737126  [   12/   88]
per-ex loss: 0.543371  [   14/   88]
per-ex loss: 0.562693  [   16/   88]
per-ex loss: 0.729281  [   18/   88]
per-ex loss: 0.627615  [   20/   88]
per-ex loss: 0.425061  [   22/   88]
per-ex loss: 0.568840  [   24/   88]
per-ex loss: 0.524820  [   26/   88]
per-ex loss: 0.558546  [   28/   88]
per-ex loss: 0.650454  [   30/   88]
per-ex loss: 0.648088  [   32/   88]
per-ex loss: 0.535076  [   34/   88]
per-ex loss: 0.578444  [   36/   88]
per-ex loss: 0.467357  [   38/   88]
per-ex loss: 0.497389  [   40/   88]
per-ex loss: 0.631384  [   42/   88]
per-ex loss: 0.449286  [   44/   88]
per-ex loss: 0.604054  [   46/   88]
per-ex loss: 0.676109  [   48/   88]
per-ex loss: 0.684072  [   50/   88]
per-ex loss: 0.555892  [   52/   88]
per-ex loss: 0.505075  [   54/   88]
per-ex loss: 0.675218  [   56/   88]
per-ex loss: 0.546925  [   58/   88]
per-ex loss: 0.490658  [   60/   88]
per-ex loss: 0.443536  [   62/   88]
per-ex loss: 0.623263  [   64/   88]
per-ex loss: 0.529822  [   66/   88]
per-ex loss: 0.478604  [   68/   88]
per-ex loss: 0.563041  [   70/   88]
per-ex loss: 0.469600  [   72/   88]
per-ex loss: 0.440444  [   74/   88]
per-ex loss: 0.686669  [   76/   88]
per-ex loss: 0.638839  [   78/   88]
per-ex loss: 0.595348  [   80/   88]
per-ex loss: 0.498553  [   82/   88]
per-ex loss: 0.508536  [   84/   88]
per-ex loss: 0.505286  [   86/   88]
per-ex loss: 0.526345  [   88/   88]
Train Error: Avg loss: 0.57427974
validation Error: 
 Avg loss: 0.60983079 
 F1: 0.449054 
 Precision: 0.621019 
 Recall: 0.351673
 IoU: 0.289535

test Error: 
 Avg loss: 0.58267305 
 F1: 0.492145 
 Precision: 0.647399 
 Recall: 0.396951
 IoU: 0.326387

We have finished training iteration 6
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_2_.pth
per-ex loss: 0.476974  [    2/   88]
per-ex loss: 0.664383  [    4/   88]
per-ex loss: 0.522576  [    6/   88]
per-ex loss: 0.625467  [    8/   88]
per-ex loss: 0.638864  [   10/   88]
per-ex loss: 0.630174  [   12/   88]
per-ex loss: 0.602431  [   14/   88]
per-ex loss: 0.446632  [   16/   88]
per-ex loss: 0.577453  [   18/   88]
per-ex loss: 0.508645  [   20/   88]
per-ex loss: 0.526600  [   22/   88]
per-ex loss: 0.589445  [   24/   88]
per-ex loss: 0.399057  [   26/   88]
per-ex loss: 0.484540  [   28/   88]
per-ex loss: 0.678931  [   30/   88]
per-ex loss: 0.540675  [   32/   88]
per-ex loss: 0.552292  [   34/   88]
per-ex loss: 0.665501  [   36/   88]
per-ex loss: 0.642060  [   38/   88]
per-ex loss: 0.671608  [   40/   88]
per-ex loss: 0.428065  [   42/   88]
per-ex loss: 0.627909  [   44/   88]
per-ex loss: 0.677286  [   46/   88]
per-ex loss: 0.619222  [   48/   88]
per-ex loss: 0.468164  [   50/   88]
per-ex loss: 0.440942  [   52/   88]
per-ex loss: 0.699156  [   54/   88]
per-ex loss: 0.546450  [   56/   88]
per-ex loss: 0.551286  [   58/   88]
per-ex loss: 0.484990  [   60/   88]
per-ex loss: 0.458254  [   62/   88]
per-ex loss: 0.451652  [   64/   88]
per-ex loss: 0.464678  [   66/   88]
per-ex loss: 0.742995  [   68/   88]
per-ex loss: 0.416656  [   70/   88]
per-ex loss: 0.482827  [   72/   88]
per-ex loss: 0.410651  [   74/   88]
per-ex loss: 0.532587  [   76/   88]
per-ex loss: 0.406989  [   78/   88]
per-ex loss: 0.685900  [   80/   88]
per-ex loss: 0.766333  [   82/   88]
per-ex loss: 0.462647  [   84/   88]
per-ex loss: 0.579908  [   86/   88]
per-ex loss: 0.561267  [   88/   88]
Train Error: Avg loss: 0.55479825
validation Error: 
 Avg loss: 0.58708343 
 F1: 0.451532 
 Precision: 0.568014 
 Recall: 0.374694
 IoU: 0.291599

test Error: 
 Avg loss: 0.55453326 
 F1: 0.495233 
 Precision: 0.645396 
 Recall: 0.401757
 IoU: 0.329109

We have finished training iteration 7
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_3_.pth
per-ex loss: 0.544261  [    2/   88]
per-ex loss: 0.483517  [    4/   88]
per-ex loss: 0.484298  [    6/   88]
per-ex loss: 0.540568  [    8/   88]
per-ex loss: 0.764182  [   10/   88]
per-ex loss: 0.441667  [   12/   88]
per-ex loss: 0.471234  [   14/   88]
per-ex loss: 0.499913  [   16/   88]
per-ex loss: 0.477727  [   18/   88]
per-ex loss: 0.455722  [   20/   88]
per-ex loss: 0.697637  [   22/   88]
per-ex loss: 0.488214  [   24/   88]
per-ex loss: 0.784755  [   26/   88]
per-ex loss: 0.533267  [   28/   88]
per-ex loss: 0.459855  [   30/   88]
per-ex loss: 0.690113  [   32/   88]
per-ex loss: 0.491956  [   34/   88]
per-ex loss: 0.592395  [   36/   88]
per-ex loss: 0.464635  [   38/   88]
per-ex loss: 0.644956  [   40/   88]
per-ex loss: 0.644765  [   42/   88]
per-ex loss: 0.589195  [   44/   88]
per-ex loss: 0.530872  [   46/   88]
per-ex loss: 0.399599  [   48/   88]
per-ex loss: 0.611562  [   50/   88]
per-ex loss: 0.638571  [   52/   88]
per-ex loss: 0.658790  [   54/   88]
per-ex loss: 0.639095  [   56/   88]
per-ex loss: 0.600068  [   58/   88]
per-ex loss: 0.542419  [   60/   88]
per-ex loss: 0.452385  [   62/   88]
per-ex loss: 0.571346  [   64/   88]
per-ex loss: 0.604115  [   66/   88]
per-ex loss: 0.490256  [   68/   88]
per-ex loss: 0.481772  [   70/   88]
per-ex loss: 0.497308  [   72/   88]
per-ex loss: 0.537736  [   74/   88]
per-ex loss: 0.634190  [   76/   88]
per-ex loss: 0.723603  [   78/   88]
per-ex loss: 0.650600  [   80/   88]
per-ex loss: 0.444201  [   82/   88]
per-ex loss: 0.451497  [   84/   88]
per-ex loss: 0.458251  [   86/   88]
per-ex loss: 0.565927  [   88/   88]
Train Error: Avg loss: 0.55520450
validation Error: 
 Avg loss: 0.59516663 
 F1: 0.464694 
 Precision: 0.595141 
 Recall: 0.381151
 IoU: 0.302672

test Error: 
 Avg loss: 0.56213677 
 F1: 0.508807 
 Precision: 0.617454 
 Recall: 0.432674
 IoU: 0.341208

We have finished training iteration 8
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_4_.pth
per-ex loss: 0.491965  [    2/   88]
per-ex loss: 0.580303  [    4/   88]
per-ex loss: 0.513551  [    6/   88]
per-ex loss: 0.657408  [    8/   88]
per-ex loss: 0.453294  [   10/   88]
per-ex loss: 0.662632  [   12/   88]
per-ex loss: 0.609491  [   14/   88]
per-ex loss: 0.574610  [   16/   88]
per-ex loss: 0.620264  [   18/   88]
per-ex loss: 0.594976  [   20/   88]
per-ex loss: 0.631230  [   22/   88]
per-ex loss: 0.546631  [   24/   88]
per-ex loss: 0.583382  [   26/   88]
per-ex loss: 0.464269  [   28/   88]
per-ex loss: 0.419326  [   30/   88]
per-ex loss: 0.454702  [   32/   88]
per-ex loss: 0.648099  [   34/   88]
per-ex loss: 0.468635  [   36/   88]
per-ex loss: 0.421139  [   38/   88]
per-ex loss: 0.471161  [   40/   88]
per-ex loss: 0.427843  [   42/   88]
per-ex loss: 0.480543  [   44/   88]
per-ex loss: 0.460786  [   46/   88]
per-ex loss: 0.705905  [   48/   88]
per-ex loss: 0.699128  [   50/   88]
per-ex loss: 0.498308  [   52/   88]
per-ex loss: 0.725865  [   54/   88]
per-ex loss: 0.445602  [   56/   88]
per-ex loss: 0.586487  [   58/   88]
per-ex loss: 0.482097  [   60/   88]
per-ex loss: 0.535417  [   62/   88]
per-ex loss: 0.620812  [   64/   88]
per-ex loss: 0.591894  [   66/   88]
per-ex loss: 0.571815  [   68/   88]
per-ex loss: 0.504481  [   70/   88]
per-ex loss: 0.593313  [   72/   88]
per-ex loss: 0.443003  [   74/   88]
per-ex loss: 0.533946  [   76/   88]
per-ex loss: 0.509513  [   78/   88]
per-ex loss: 0.607313  [   80/   88]
per-ex loss: 0.620791  [   82/   88]
per-ex loss: 0.569203  [   84/   88]
per-ex loss: 0.553791  [   86/   88]
per-ex loss: 0.554590  [   88/   88]
Train Error: Avg loss: 0.54976167
validation Error: 
 Avg loss: 0.56687793 
 F1: 0.471970 
 Precision: 0.586468 
 Recall: 0.394877
 IoU: 0.308875

test Error: 
 Avg loss: 0.54024372 
 F1: 0.517644 
 Precision: 0.626811 
 Recall: 0.440863
 IoU: 0.349204

We have finished training iteration 9
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_6_.pth
per-ex loss: 0.615426  [    2/   88]
per-ex loss: 0.540558  [    4/   88]
per-ex loss: 0.446872  [    6/   88]
per-ex loss: 0.562017  [    8/   88]
per-ex loss: 0.501026  [   10/   88]
per-ex loss: 0.442249  [   12/   88]
per-ex loss: 0.445673  [   14/   88]
per-ex loss: 0.431261  [   16/   88]
per-ex loss: 0.611657  [   18/   88]
per-ex loss: 0.527030  [   20/   88]
per-ex loss: 0.486822  [   22/   88]
per-ex loss: 0.530888  [   24/   88]
per-ex loss: 0.455554  [   26/   88]
per-ex loss: 0.473524  [   28/   88]
per-ex loss: 0.522961  [   30/   88]
per-ex loss: 0.775921  [   32/   88]
per-ex loss: 0.598049  [   34/   88]
per-ex loss: 0.518039  [   36/   88]
per-ex loss: 0.425861  [   38/   88]
per-ex loss: 0.493911  [   40/   88]
per-ex loss: 0.529921  [   42/   88]
per-ex loss: 0.502740  [   44/   88]
per-ex loss: 0.593944  [   46/   88]
per-ex loss: 0.472314  [   48/   88]
per-ex loss: 0.541523  [   50/   88]
per-ex loss: 0.492910  [   52/   88]
per-ex loss: 0.586804  [   54/   88]
per-ex loss: 0.456223  [   56/   88]
per-ex loss: 0.697567  [   58/   88]
per-ex loss: 0.543302  [   60/   88]
per-ex loss: 0.597112  [   62/   88]
per-ex loss: 0.588227  [   64/   88]
per-ex loss: 0.596554  [   66/   88]
per-ex loss: 0.697510  [   68/   88]
per-ex loss: 0.632588  [   70/   88]
per-ex loss: 0.438069  [   72/   88]
per-ex loss: 0.595449  [   74/   88]
per-ex loss: 0.563043  [   76/   88]
per-ex loss: 0.475636  [   78/   88]
per-ex loss: 0.460145  [   80/   88]
per-ex loss: 0.627073  [   82/   88]
per-ex loss: 0.519120  [   84/   88]
per-ex loss: 0.478755  [   86/   88]
per-ex loss: 0.652167  [   88/   88]
Train Error: Avg loss: 0.53963624
validation Error: 
 Avg loss: 0.58782781 
 F1: 0.456574 
 Precision: 0.525174 
 Recall: 0.403825
 IoU: 0.295818

test Error: 
 Avg loss: 0.54747130 
 F1: 0.514196 
 Precision: 0.599284 
 Recall: 0.450266
 IoU: 0.346073

We have finished training iteration 10
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_8_.pth
per-ex loss: 0.552526  [    2/   88]
per-ex loss: 0.484533  [    4/   88]
per-ex loss: 0.473794  [    6/   88]
per-ex loss: 0.642073  [    8/   88]
per-ex loss: 0.568978  [   10/   88]
per-ex loss: 0.615934  [   12/   88]
per-ex loss: 0.649822  [   14/   88]
per-ex loss: 0.680613  [   16/   88]
per-ex loss: 0.518664  [   18/   88]
per-ex loss: 0.562329  [   20/   88]
per-ex loss: 0.468051  [   22/   88]
per-ex loss: 0.524085  [   24/   88]
per-ex loss: 0.734591  [   26/   88]
per-ex loss: 0.497686  [   28/   88]
per-ex loss: 0.528826  [   30/   88]
per-ex loss: 0.497786  [   32/   88]
per-ex loss: 0.466733  [   34/   88]
per-ex loss: 0.468403  [   36/   88]
per-ex loss: 0.612060  [   38/   88]
per-ex loss: 0.507633  [   40/   88]
per-ex loss: 0.412948  [   42/   88]
per-ex loss: 0.598783  [   44/   88]
per-ex loss: 0.437650  [   46/   88]
per-ex loss: 0.589539  [   48/   88]
per-ex loss: 0.548937  [   50/   88]
per-ex loss: 0.453715  [   52/   88]
per-ex loss: 0.479518  [   54/   88]
per-ex loss: 0.470035  [   56/   88]
per-ex loss: 0.576365  [   58/   88]
per-ex loss: 0.602664  [   60/   88]
per-ex loss: 0.588713  [   62/   88]
per-ex loss: 0.511047  [   64/   88]
per-ex loss: 0.540574  [   66/   88]
per-ex loss: 0.566881  [   68/   88]
per-ex loss: 0.449349  [   70/   88]
per-ex loss: 0.517720  [   72/   88]
per-ex loss: 0.434974  [   74/   88]
per-ex loss: 0.511911  [   76/   88]
per-ex loss: 0.479709  [   78/   88]
per-ex loss: 0.641649  [   80/   88]
per-ex loss: 0.441102  [   82/   88]
per-ex loss: 0.695287  [   84/   88]
per-ex loss: 0.458324  [   86/   88]
per-ex loss: 0.585207  [   88/   88]
Train Error: Avg loss: 0.53744817
validation Error: 
 Avg loss: 0.57752500 
 F1: 0.452400 
 Precision: 0.690131 
 Recall: 0.336489
 IoU: 0.292324

test Error: 
 Avg loss: 0.55612586 
 F1: 0.494386 
 Precision: 0.737862 
 Recall: 0.371726
 IoU: 0.328362

We have finished training iteration 11
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_5_.pth
per-ex loss: 0.425456  [    2/   88]
per-ex loss: 0.592417  [    4/   88]
per-ex loss: 0.649766  [    6/   88]
per-ex loss: 0.662596  [    8/   88]
per-ex loss: 0.560450  [   10/   88]
per-ex loss: 0.470067  [   12/   88]
per-ex loss: 0.575154  [   14/   88]
per-ex loss: 0.594537  [   16/   88]
per-ex loss: 0.472032  [   18/   88]
per-ex loss: 0.458403  [   20/   88]
per-ex loss: 0.425957  [   22/   88]
per-ex loss: 0.450629  [   24/   88]
per-ex loss: 0.547711  [   26/   88]
per-ex loss: 0.523222  [   28/   88]
per-ex loss: 0.519203  [   30/   88]
per-ex loss: 0.509585  [   32/   88]
per-ex loss: 0.453586  [   34/   88]
per-ex loss: 0.634740  [   36/   88]
per-ex loss: 0.645555  [   38/   88]
per-ex loss: 0.485378  [   40/   88]
per-ex loss: 0.529575  [   42/   88]
per-ex loss: 0.427860  [   44/   88]
per-ex loss: 0.437912  [   46/   88]
per-ex loss: 0.466096  [   48/   88]
per-ex loss: 0.478666  [   50/   88]
per-ex loss: 0.705315  [   52/   88]
per-ex loss: 0.397574  [   54/   88]
per-ex loss: 0.629897  [   56/   88]
per-ex loss: 0.484258  [   58/   88]
per-ex loss: 0.610464  [   60/   88]
per-ex loss: 0.484873  [   62/   88]
per-ex loss: 0.613477  [   64/   88]
per-ex loss: 0.625311  [   66/   88]
per-ex loss: 0.515068  [   68/   88]
per-ex loss: 0.398520  [   70/   88]
per-ex loss: 0.459636  [   72/   88]
per-ex loss: 0.517118  [   74/   88]
per-ex loss: 0.708788  [   76/   88]
per-ex loss: 0.659471  [   78/   88]
per-ex loss: 0.687642  [   80/   88]
per-ex loss: 0.610106  [   82/   88]
per-ex loss: 0.467551  [   84/   88]
per-ex loss: 0.489453  [   86/   88]
per-ex loss: 0.577750  [   88/   88]
Train Error: Avg loss: 0.53724599
validation Error: 
 Avg loss: 0.56654316 
 F1: 0.469623 
 Precision: 0.489194 
 Recall: 0.451558
 IoU: 0.306867

test Error: 
 Avg loss: 0.51417843 
 F1: 0.538708 
 Precision: 0.574599 
 Recall: 0.507037
 IoU: 0.368652

We have finished training iteration 12
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_10_.pth
per-ex loss: 0.677535  [    2/   88]
per-ex loss: 0.450850  [    4/   88]
per-ex loss: 0.513715  [    6/   88]
per-ex loss: 0.494025  [    8/   88]
per-ex loss: 0.445601  [   10/   88]
per-ex loss: 0.493009  [   12/   88]
per-ex loss: 0.575695  [   14/   88]
per-ex loss: 0.632505  [   16/   88]
per-ex loss: 0.496617  [   18/   88]
per-ex loss: 0.458210  [   20/   88]
per-ex loss: 0.655245  [   22/   88]
per-ex loss: 0.576109  [   24/   88]
per-ex loss: 0.488482  [   26/   88]
per-ex loss: 0.636138  [   28/   88]
per-ex loss: 0.711191  [   30/   88]
per-ex loss: 0.452159  [   32/   88]
per-ex loss: 0.490389  [   34/   88]
per-ex loss: 0.433691  [   36/   88]
per-ex loss: 0.461174  [   38/   88]
per-ex loss: 0.503118  [   40/   88]
per-ex loss: 0.575304  [   42/   88]
per-ex loss: 0.678889  [   44/   88]
per-ex loss: 0.548142  [   46/   88]
per-ex loss: 0.613081  [   48/   88]
per-ex loss: 0.562787  [   50/   88]
per-ex loss: 0.628689  [   52/   88]
per-ex loss: 0.563237  [   54/   88]
per-ex loss: 0.447960  [   56/   88]
per-ex loss: 0.437544  [   58/   88]
per-ex loss: 0.513715  [   60/   88]
per-ex loss: 0.509586  [   62/   88]
per-ex loss: 0.562744  [   64/   88]
per-ex loss: 0.430272  [   66/   88]
per-ex loss: 0.397486  [   68/   88]
per-ex loss: 0.584101  [   70/   88]
per-ex loss: 0.600564  [   72/   88]
per-ex loss: 0.601096  [   74/   88]
per-ex loss: 0.423259  [   76/   88]
per-ex loss: 0.544677  [   78/   88]
per-ex loss: 0.641748  [   80/   88]
per-ex loss: 0.675976  [   82/   88]
per-ex loss: 0.401955  [   84/   88]
per-ex loss: 0.519549  [   86/   88]
per-ex loss: 0.442251  [   88/   88]
Train Error: Avg loss: 0.53522883
validation Error: 
 Avg loss: 0.55419132 
 F1: 0.469915 
 Precision: 0.469932 
 Recall: 0.469898
 IoU: 0.307117

test Error: 
 Avg loss: 0.52532123 
 F1: 0.518968 
 Precision: 0.526362 
 Recall: 0.511779
 IoU: 0.350410

We have finished training iteration 13
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_7_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.524456  [    2/   88]
per-ex loss: 0.555018  [    4/   88]
per-ex loss: 0.511237  [    6/   88]
per-ex loss: 0.447079  [    8/   88]
per-ex loss: 0.457781  [   10/   88]
per-ex loss: 0.593341  [   12/   88]
per-ex loss: 0.448701  [   14/   88]
per-ex loss: 0.454374  [   16/   88]
per-ex loss: 0.442117  [   18/   88]
per-ex loss: 0.486483  [   20/   88]
per-ex loss: 0.426043  [   22/   88]
per-ex loss: 0.480390  [   24/   88]
per-ex loss: 0.570151  [   26/   88]
per-ex loss: 0.593287  [   28/   88]
per-ex loss: 0.502445  [   30/   88]
per-ex loss: 0.479575  [   32/   88]
per-ex loss: 0.466095  [   34/   88]
per-ex loss: 0.619922  [   36/   88]
per-ex loss: 0.549671  [   38/   88]
per-ex loss: 0.616421  [   40/   88]
per-ex loss: 0.515024  [   42/   88]
per-ex loss: 0.664344  [   44/   88]
per-ex loss: 0.689039  [   46/   88]
per-ex loss: 0.581958  [   48/   88]
per-ex loss: 0.454484  [   50/   88]
per-ex loss: 0.472082  [   52/   88]
per-ex loss: 0.533027  [   54/   88]
per-ex loss: 0.436209  [   56/   88]
per-ex loss: 0.663880  [   58/   88]
per-ex loss: 0.664233  [   60/   88]
per-ex loss: 0.474292  [   62/   88]
per-ex loss: 0.474385  [   64/   88]
per-ex loss: 0.436719  [   66/   88]
per-ex loss: 0.433338  [   68/   88]
per-ex loss: 0.664813  [   70/   88]
per-ex loss: 0.467253  [   72/   88]
per-ex loss: 0.669011  [   74/   88]
per-ex loss: 0.675571  [   76/   88]
per-ex loss: 0.467422  [   78/   88]
per-ex loss: 0.486799  [   80/   88]
per-ex loss: 0.452088  [   82/   88]
per-ex loss: 0.635058  [   84/   88]
per-ex loss: 0.580775  [   86/   88]
per-ex loss: 0.642003  [   88/   88]
Train Error: Avg loss: 0.53314534
validation Error: 
 Avg loss: 0.57386920 
 F1: 0.463628 
 Precision: 0.599738 
 Recall: 0.377871
 IoU: 0.301768

test Error: 
 Avg loss: 0.54154294 
 F1: 0.510196 
 Precision: 0.641225 
 Recall: 0.423631
 IoU: 0.342459

We have finished training iteration 14
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_11_.pth
per-ex loss: 0.475704  [    2/   88]
per-ex loss: 0.508312  [    4/   88]
per-ex loss: 0.647379  [    6/   88]
per-ex loss: 0.565550  [    8/   88]
per-ex loss: 0.462005  [   10/   88]
per-ex loss: 0.688051  [   12/   88]
per-ex loss: 0.547402  [   14/   88]
per-ex loss: 0.478171  [   16/   88]
per-ex loss: 0.557217  [   18/   88]
per-ex loss: 0.518759  [   20/   88]
per-ex loss: 0.390961  [   22/   88]
per-ex loss: 0.623792  [   24/   88]
per-ex loss: 0.510133  [   26/   88]
per-ex loss: 0.472748  [   28/   88]
per-ex loss: 0.453297  [   30/   88]
per-ex loss: 0.493680  [   32/   88]
per-ex loss: 0.617370  [   34/   88]
per-ex loss: 0.618634  [   36/   88]
per-ex loss: 0.583027  [   38/   88]
per-ex loss: 0.436216  [   40/   88]
per-ex loss: 0.546080  [   42/   88]
per-ex loss: 0.663159  [   44/   88]
per-ex loss: 0.589882  [   46/   88]
per-ex loss: 0.424247  [   48/   88]
per-ex loss: 0.529299  [   50/   88]
per-ex loss: 0.479724  [   52/   88]
per-ex loss: 0.601824  [   54/   88]
per-ex loss: 0.429533  [   56/   88]
per-ex loss: 0.379570  [   58/   88]
per-ex loss: 0.534290  [   60/   88]
per-ex loss: 0.519784  [   62/   88]
per-ex loss: 0.628336  [   64/   88]
per-ex loss: 0.635392  [   66/   88]
per-ex loss: 0.740043  [   68/   88]
per-ex loss: 0.494594  [   70/   88]
per-ex loss: 0.658899  [   72/   88]
per-ex loss: 0.581896  [   74/   88]
per-ex loss: 0.665883  [   76/   88]
per-ex loss: 0.501921  [   78/   88]
per-ex loss: 0.423004  [   80/   88]
per-ex loss: 0.435139  [   82/   88]
per-ex loss: 0.462703  [   84/   88]
per-ex loss: 0.558119  [   86/   88]
per-ex loss: 0.479095  [   88/   88]
Train Error: Avg loss: 0.53660967
validation Error: 
 Avg loss: 0.56203394 
 F1: 0.481885 
 Precision: 0.578438 
 Recall: 0.412954
 IoU: 0.317423

test Error: 
 Avg loss: 0.51684407 
 F1: 0.533247 
 Precision: 0.625530 
 Recall: 0.464692
 IoU: 0.363556

We have finished training iteration 15
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_9_.pth
per-ex loss: 0.476295  [    2/   88]
per-ex loss: 0.501484  [    4/   88]
per-ex loss: 0.440208  [    6/   88]
per-ex loss: 0.537282  [    8/   88]
per-ex loss: 0.608649  [   10/   88]
per-ex loss: 0.598874  [   12/   88]
per-ex loss: 0.428184  [   14/   88]
per-ex loss: 0.722907  [   16/   88]
per-ex loss: 0.532928  [   18/   88]
per-ex loss: 0.594590  [   20/   88]
per-ex loss: 0.685082  [   22/   88]
per-ex loss: 0.579724  [   24/   88]
per-ex loss: 0.466165  [   26/   88]
per-ex loss: 0.494178  [   28/   88]
per-ex loss: 0.498809  [   30/   88]
per-ex loss: 0.457421  [   32/   88]
per-ex loss: 0.433393  [   34/   88]
per-ex loss: 0.433672  [   36/   88]
per-ex loss: 0.482383  [   38/   88]
per-ex loss: 0.507763  [   40/   88]
per-ex loss: 0.440197  [   42/   88]
per-ex loss: 0.391683  [   44/   88]
per-ex loss: 0.554637  [   46/   88]
per-ex loss: 0.520773  [   48/   88]
per-ex loss: 0.504681  [   50/   88]
per-ex loss: 0.454048  [   52/   88]
per-ex loss: 0.497082  [   54/   88]
per-ex loss: 0.444351  [   56/   88]
per-ex loss: 0.574359  [   58/   88]
per-ex loss: 0.536804  [   60/   88]
per-ex loss: 0.564690  [   62/   88]
per-ex loss: 0.444430  [   64/   88]
per-ex loss: 0.430980  [   66/   88]
per-ex loss: 0.511902  [   68/   88]
per-ex loss: 0.545468  [   70/   88]
per-ex loss: 0.645047  [   72/   88]
per-ex loss: 0.572897  [   74/   88]
per-ex loss: 0.608977  [   76/   88]
per-ex loss: 0.549010  [   78/   88]
per-ex loss: 0.718562  [   80/   88]
per-ex loss: 0.435083  [   82/   88]
per-ex loss: 0.501409  [   84/   88]
per-ex loss: 0.543629  [   86/   88]
per-ex loss: 0.516661  [   88/   88]
Train Error: Avg loss: 0.52243984
validation Error: 
 Avg loss: 0.57331083 
 F1: 0.461425 
 Precision: 0.482595 
 Recall: 0.442036
 IoU: 0.299905

test Error: 
 Avg loss: 0.52247035 
 F1: 0.521693 
 Precision: 0.578429 
 Recall: 0.475093
 IoU: 0.352899

We have finished training iteration 16
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_14_.pth
per-ex loss: 0.607431  [    2/   88]
per-ex loss: 0.735623  [    4/   88]
per-ex loss: 0.554417  [    6/   88]
per-ex loss: 0.492585  [    8/   88]
per-ex loss: 0.514442  [   10/   88]
per-ex loss: 0.607874  [   12/   88]
per-ex loss: 0.489177  [   14/   88]
per-ex loss: 0.516584  [   16/   88]
per-ex loss: 0.456109  [   18/   88]
per-ex loss: 0.477077  [   20/   88]
per-ex loss: 0.626127  [   22/   88]
per-ex loss: 0.435008  [   24/   88]
per-ex loss: 0.420934  [   26/   88]
per-ex loss: 0.546428  [   28/   88]
per-ex loss: 0.625629  [   30/   88]
per-ex loss: 0.564202  [   32/   88]
per-ex loss: 0.470787  [   34/   88]
per-ex loss: 0.417732  [   36/   88]
per-ex loss: 0.556166  [   38/   88]
per-ex loss: 0.640630  [   40/   88]
per-ex loss: 0.684608  [   42/   88]
per-ex loss: 0.483199  [   44/   88]
per-ex loss: 0.492370  [   46/   88]
per-ex loss: 0.428869  [   48/   88]
per-ex loss: 0.420903  [   50/   88]
per-ex loss: 0.447950  [   52/   88]
per-ex loss: 0.563523  [   54/   88]
per-ex loss: 0.375997  [   56/   88]
per-ex loss: 0.495522  [   58/   88]
per-ex loss: 0.705612  [   60/   88]
per-ex loss: 0.474621  [   62/   88]
per-ex loss: 0.649572  [   64/   88]
per-ex loss: 0.702995  [   66/   88]
per-ex loss: 0.700443  [   68/   88]
per-ex loss: 0.503912  [   70/   88]
per-ex loss: 0.405519  [   72/   88]
per-ex loss: 0.471797  [   74/   88]
per-ex loss: 0.411631  [   76/   88]
per-ex loss: 0.525953  [   78/   88]
per-ex loss: 0.660040  [   80/   88]
per-ex loss: 0.553217  [   82/   88]
per-ex loss: 0.512217  [   84/   88]
per-ex loss: 0.734951  [   86/   88]
per-ex loss: 0.453526  [   88/   88]
Train Error: Avg loss: 0.53667972
validation Error: 
 Avg loss: 0.54642130 
 F1: 0.483354 
 Precision: 0.603665 
 Recall: 0.403030
 IoU: 0.318699

test Error: 
 Avg loss: 0.52026396 
 F1: 0.531525 
 Precision: 0.662041 
 Recall: 0.443995
 IoU: 0.361957

We have finished training iteration 17
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_12_.pth
per-ex loss: 0.482071  [    2/   88]
per-ex loss: 0.414111  [    4/   88]
per-ex loss: 0.465859  [    6/   88]
per-ex loss: 0.495943  [    8/   88]
per-ex loss: 0.582904  [   10/   88]
per-ex loss: 0.647472  [   12/   88]
per-ex loss: 0.548485  [   14/   88]
per-ex loss: 0.540947  [   16/   88]
per-ex loss: 0.517076  [   18/   88]
per-ex loss: 0.639450  [   20/   88]
per-ex loss: 0.431519  [   22/   88]
per-ex loss: 0.609292  [   24/   88]
per-ex loss: 0.446603  [   26/   88]
per-ex loss: 0.401593  [   28/   88]
per-ex loss: 0.541636  [   30/   88]
per-ex loss: 0.542679  [   32/   88]
per-ex loss: 0.593116  [   34/   88]
per-ex loss: 0.540654  [   36/   88]
per-ex loss: 0.459894  [   38/   88]
per-ex loss: 0.397862  [   40/   88]
per-ex loss: 0.506343  [   42/   88]
per-ex loss: 0.646216  [   44/   88]
per-ex loss: 0.613053  [   46/   88]
per-ex loss: 0.502220  [   48/   88]
per-ex loss: 0.549192  [   50/   88]
per-ex loss: 0.616254  [   52/   88]
per-ex loss: 0.408476  [   54/   88]
per-ex loss: 0.469198  [   56/   88]
per-ex loss: 0.563106  [   58/   88]
per-ex loss: 0.423265  [   60/   88]
per-ex loss: 0.527071  [   62/   88]
per-ex loss: 0.612420  [   64/   88]
per-ex loss: 0.705733  [   66/   88]
per-ex loss: 0.604661  [   68/   88]
per-ex loss: 0.541043  [   70/   88]
per-ex loss: 0.465417  [   72/   88]
per-ex loss: 0.452048  [   74/   88]
per-ex loss: 0.646662  [   76/   88]
per-ex loss: 0.404071  [   78/   88]
per-ex loss: 0.496751  [   80/   88]
per-ex loss: 0.443820  [   82/   88]
per-ex loss: 0.626878  [   84/   88]
per-ex loss: 0.409253  [   86/   88]
per-ex loss: 0.700445  [   88/   88]
Train Error: Avg loss: 0.52801726
validation Error: 
 Avg loss: 0.58629216 
 F1: 0.460025 
 Precision: 0.515127 
 Recall: 0.415572
 IoU: 0.298722

test Error: 
 Avg loss: 0.52550494 
 F1: 0.522015 
 Precision: 0.594483 
 Recall: 0.465294
 IoU: 0.353193

We have finished training iteration 18
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_16_.pth
per-ex loss: 0.505299  [    2/   88]
per-ex loss: 0.402667  [    4/   88]
per-ex loss: 0.607435  [    6/   88]
per-ex loss: 0.411516  [    8/   88]
per-ex loss: 0.485386  [   10/   88]
per-ex loss: 0.420794  [   12/   88]
per-ex loss: 0.677817  [   14/   88]
per-ex loss: 0.695081  [   16/   88]
per-ex loss: 0.510397  [   18/   88]
per-ex loss: 0.516476  [   20/   88]
per-ex loss: 0.468519  [   22/   88]
per-ex loss: 0.441975  [   24/   88]
per-ex loss: 0.621973  [   26/   88]
per-ex loss: 0.462987  [   28/   88]
per-ex loss: 0.485295  [   30/   88]
per-ex loss: 0.596588  [   32/   88]
per-ex loss: 0.423338  [   34/   88]
per-ex loss: 0.582620  [   36/   88]
per-ex loss: 0.500063  [   38/   88]
per-ex loss: 0.463180  [   40/   88]
per-ex loss: 0.685767  [   42/   88]
per-ex loss: 0.644617  [   44/   88]
per-ex loss: 0.470011  [   46/   88]
per-ex loss: 0.491072  [   48/   88]
per-ex loss: 0.438154  [   50/   88]
per-ex loss: 0.566755  [   52/   88]
per-ex loss: 0.364149  [   54/   88]
per-ex loss: 0.718436  [   56/   88]
per-ex loss: 0.628512  [   58/   88]
per-ex loss: 0.445607  [   60/   88]
per-ex loss: 0.440929  [   62/   88]
per-ex loss: 0.472713  [   64/   88]
per-ex loss: 0.433647  [   66/   88]
per-ex loss: 0.416369  [   68/   88]
per-ex loss: 0.592672  [   70/   88]
per-ex loss: 0.516766  [   72/   88]
per-ex loss: 0.408382  [   74/   88]
per-ex loss: 0.613164  [   76/   88]
per-ex loss: 0.470262  [   78/   88]
per-ex loss: 0.640954  [   80/   88]
per-ex loss: 0.534460  [   82/   88]
per-ex loss: 0.488173  [   84/   88]
per-ex loss: 0.470439  [   86/   88]
per-ex loss: 0.491897  [   88/   88]
Train Error: Avg loss: 0.51643890
validation Error: 
 Avg loss: 0.56594918 
 F1: 0.472952 
 Precision: 0.557742 
 Recall: 0.410541
 IoU: 0.309717

test Error: 
 Avg loss: 0.51737609 
 F1: 0.525067 
 Precision: 0.662504 
 Recall: 0.434856
 IoU: 0.355994

We have finished training iteration 19
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_15_.pth
per-ex loss: 0.484785  [    2/   88]
per-ex loss: 0.430282  [    4/   88]
per-ex loss: 0.441630  [    6/   88]
per-ex loss: 0.612020  [    8/   88]
per-ex loss: 0.429926  [   10/   88]
per-ex loss: 0.425633  [   12/   88]
per-ex loss: 0.566573  [   14/   88]
per-ex loss: 0.478698  [   16/   88]
per-ex loss: 0.591798  [   18/   88]
per-ex loss: 0.585089  [   20/   88]
per-ex loss: 0.567195  [   22/   88]
per-ex loss: 0.509984  [   24/   88]
per-ex loss: 0.617277  [   26/   88]
per-ex loss: 0.585756  [   28/   88]
per-ex loss: 0.459814  [   30/   88]
per-ex loss: 0.731531  [   32/   88]
per-ex loss: 0.564998  [   34/   88]
per-ex loss: 0.517928  [   36/   88]
per-ex loss: 0.493624  [   38/   88]
per-ex loss: 0.682792  [   40/   88]
per-ex loss: 0.522762  [   42/   88]
per-ex loss: 0.474380  [   44/   88]
per-ex loss: 0.440192  [   46/   88]
per-ex loss: 0.487376  [   48/   88]
per-ex loss: 0.452828  [   50/   88]
per-ex loss: 0.451916  [   52/   88]
per-ex loss: 0.481465  [   54/   88]
per-ex loss: 0.604426  [   56/   88]
per-ex loss: 0.545789  [   58/   88]
per-ex loss: 0.602832  [   60/   88]
per-ex loss: 0.398803  [   62/   88]
per-ex loss: 0.416985  [   64/   88]
per-ex loss: 0.564972  [   66/   88]
per-ex loss: 0.552889  [   68/   88]
per-ex loss: 0.439180  [   70/   88]
per-ex loss: 0.414862  [   72/   88]
per-ex loss: 0.539736  [   74/   88]
per-ex loss: 0.486684  [   76/   88]
per-ex loss: 0.414777  [   78/   88]
per-ex loss: 0.517673  [   80/   88]
per-ex loss: 0.644177  [   82/   88]
per-ex loss: 0.600575  [   84/   88]
per-ex loss: 0.570240  [   86/   88]
per-ex loss: 0.648483  [   88/   88]
Train Error: Avg loss: 0.52389388
validation Error: 
 Avg loss: 0.56439511 
 F1: 0.463825 
 Precision: 0.623790 
 Recall: 0.369158
 IoU: 0.301935

test Error: 
 Avg loss: 0.53473734 
 F1: 0.510557 
 Precision: 0.686774 
 Recall: 0.406305
 IoU: 0.342784

We have finished training iteration 20
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_18_.pth
per-ex loss: 0.486317  [    2/   88]
per-ex loss: 0.514286  [    4/   88]
per-ex loss: 0.445528  [    6/   88]
per-ex loss: 0.571173  [    8/   88]
per-ex loss: 0.467784  [   10/   88]
per-ex loss: 0.481050  [   12/   88]
per-ex loss: 0.605883  [   14/   88]
per-ex loss: 0.519072  [   16/   88]
per-ex loss: 0.500193  [   18/   88]
per-ex loss: 0.462405  [   20/   88]
per-ex loss: 0.446520  [   22/   88]
per-ex loss: 0.442060  [   24/   88]
per-ex loss: 0.437110  [   26/   88]
per-ex loss: 0.453801  [   28/   88]
per-ex loss: 0.579601  [   30/   88]
per-ex loss: 0.531757  [   32/   88]
per-ex loss: 0.370782  [   34/   88]
per-ex loss: 0.636686  [   36/   88]
per-ex loss: 0.467744  [   38/   88]
per-ex loss: 0.710066  [   40/   88]
per-ex loss: 0.659119  [   42/   88]
per-ex loss: 0.459556  [   44/   88]
per-ex loss: 0.533124  [   46/   88]
per-ex loss: 0.682801  [   48/   88]
per-ex loss: 0.475453  [   50/   88]
per-ex loss: 0.494040  [   52/   88]
per-ex loss: 0.520796  [   54/   88]
per-ex loss: 0.424681  [   56/   88]
per-ex loss: 0.399375  [   58/   88]
per-ex loss: 0.574125  [   60/   88]
per-ex loss: 0.499453  [   62/   88]
per-ex loss: 0.585273  [   64/   88]
per-ex loss: 0.491831  [   66/   88]
per-ex loss: 0.551417  [   68/   88]
per-ex loss: 0.605914  [   70/   88]
per-ex loss: 0.570727  [   72/   88]
per-ex loss: 0.519160  [   74/   88]
per-ex loss: 0.613627  [   76/   88]
per-ex loss: 0.453527  [   78/   88]
per-ex loss: 0.468736  [   80/   88]
per-ex loss: 0.489503  [   82/   88]
per-ex loss: 0.516403  [   84/   88]
per-ex loss: 0.544253  [   86/   88]
per-ex loss: 0.453618  [   88/   88]
Train Error: Avg loss: 0.51628021
validation Error: 
 Avg loss: 0.55200002 
 F1: 0.474091 
 Precision: 0.487607 
 Recall: 0.461305
 IoU: 0.310695

test Error: 
 Avg loss: 0.50953545 
 F1: 0.533139 
 Precision: 0.578390 
 Recall: 0.494455
 IoU: 0.363456

We have finished training iteration 21
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_19_.pth
per-ex loss: 0.543505  [    2/   88]
per-ex loss: 0.587352  [    4/   88]
per-ex loss: 0.543475  [    6/   88]
per-ex loss: 0.543773  [    8/   88]
per-ex loss: 0.549790  [   10/   88]
per-ex loss: 0.646912  [   12/   88]
per-ex loss: 0.452409  [   14/   88]
per-ex loss: 0.626599  [   16/   88]
per-ex loss: 0.409882  [   18/   88]
per-ex loss: 0.466712  [   20/   88]
per-ex loss: 0.429903  [   22/   88]
per-ex loss: 0.375243  [   24/   88]
per-ex loss: 0.449896  [   26/   88]
per-ex loss: 0.583497  [   28/   88]
per-ex loss: 0.529215  [   30/   88]
per-ex loss: 0.400125  [   32/   88]
per-ex loss: 0.564943  [   34/   88]
per-ex loss: 0.480323  [   36/   88]
per-ex loss: 0.470771  [   38/   88]
per-ex loss: 0.695240  [   40/   88]
per-ex loss: 0.628339  [   42/   88]
per-ex loss: 0.401235  [   44/   88]
per-ex loss: 0.468944  [   46/   88]
per-ex loss: 0.640066  [   48/   88]
per-ex loss: 0.442588  [   50/   88]
per-ex loss: 0.402397  [   52/   88]
per-ex loss: 0.576403  [   54/   88]
per-ex loss: 0.486976  [   56/   88]
per-ex loss: 0.482334  [   58/   88]
per-ex loss: 0.432386  [   60/   88]
per-ex loss: 0.418315  [   62/   88]
per-ex loss: 0.465444  [   64/   88]
per-ex loss: 0.582349  [   66/   88]
per-ex loss: 0.539455  [   68/   88]
per-ex loss: 0.421282  [   70/   88]
per-ex loss: 0.597889  [   72/   88]
per-ex loss: 0.505825  [   74/   88]
per-ex loss: 0.452863  [   76/   88]
per-ex loss: 0.458561  [   78/   88]
per-ex loss: 0.443039  [   80/   88]
per-ex loss: 0.443155  [   82/   88]
per-ex loss: 0.409587  [   84/   88]
per-ex loss: 0.614720  [   86/   88]
per-ex loss: 0.624381  [   88/   88]
Train Error: Avg loss: 0.50654771
validation Error: 
 Avg loss: 0.60723458 
 F1: 0.419929 
 Precision: 0.506916 
 Recall: 0.358423
 IoU: 0.265766

test Error: 
 Avg loss: 0.56276212 
 F1: 0.469206 
 Precision: 0.656515 
 Recall: 0.365053
 IoU: 0.306512

We have finished training iteration 22
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_20_.pth
per-ex loss: 0.590859  [    2/   88]
per-ex loss: 0.414228  [    4/   88]
per-ex loss: 0.743606  [    6/   88]
per-ex loss: 0.474928  [    8/   88]
per-ex loss: 0.592916  [   10/   88]
per-ex loss: 0.444057  [   12/   88]
per-ex loss: 0.493546  [   14/   88]
per-ex loss: 0.410419  [   16/   88]
per-ex loss: 0.570798  [   18/   88]
per-ex loss: 0.506614  [   20/   88]
per-ex loss: 0.526206  [   22/   88]
per-ex loss: 0.586184  [   24/   88]
per-ex loss: 0.576560  [   26/   88]
per-ex loss: 0.652824  [   28/   88]
per-ex loss: 0.510042  [   30/   88]
per-ex loss: 0.460523  [   32/   88]
per-ex loss: 0.451229  [   34/   88]
per-ex loss: 0.682915  [   36/   88]
per-ex loss: 0.655105  [   38/   88]
per-ex loss: 0.526491  [   40/   88]
per-ex loss: 0.395912  [   42/   88]
per-ex loss: 0.454650  [   44/   88]
per-ex loss: 0.495921  [   46/   88]
per-ex loss: 0.415669  [   48/   88]
per-ex loss: 0.499268  [   50/   88]
per-ex loss: 0.629653  [   52/   88]
per-ex loss: 0.503283  [   54/   88]
per-ex loss: 0.407868  [   56/   88]
per-ex loss: 0.452307  [   58/   88]
per-ex loss: 0.430459  [   60/   88]
per-ex loss: 0.586903  [   62/   88]
per-ex loss: 0.683560  [   64/   88]
per-ex loss: 0.670929  [   66/   88]
per-ex loss: 0.466584  [   68/   88]
per-ex loss: 0.544311  [   70/   88]
per-ex loss: 0.720784  [   72/   88]
per-ex loss: 0.587867  [   74/   88]
per-ex loss: 0.449694  [   76/   88]
per-ex loss: 0.473803  [   78/   88]
per-ex loss: 0.472075  [   80/   88]
per-ex loss: 0.461823  [   82/   88]
per-ex loss: 0.439516  [   84/   88]
per-ex loss: 0.424522  [   86/   88]
per-ex loss: 0.410565  [   88/   88]
Train Error: Avg loss: 0.52154486
validation Error: 
 Avg loss: 0.55647924 
 F1: 0.464917 
 Precision: 0.539986 
 Recall: 0.408173
 IoU: 0.302861

test Error: 
 Avg loss: 0.50604077 
 F1: 0.537246 
 Precision: 0.631296 
 Recall: 0.467585
 IoU: 0.367284

We have finished training iteration 23
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_13_.pth
per-ex loss: 0.724928  [    2/   88]
per-ex loss: 0.490701  [    4/   88]
per-ex loss: 0.444761  [    6/   88]
per-ex loss: 0.540482  [    8/   88]
per-ex loss: 0.513364  [   10/   88]
per-ex loss: 0.549034  [   12/   88]
per-ex loss: 0.466536  [   14/   88]
per-ex loss: 0.513416  [   16/   88]
per-ex loss: 0.483853  [   18/   88]
per-ex loss: 0.467673  [   20/   88]
per-ex loss: 0.490593  [   22/   88]
per-ex loss: 0.589367  [   24/   88]
per-ex loss: 0.431319  [   26/   88]
per-ex loss: 0.426865  [   28/   88]
per-ex loss: 0.589535  [   30/   88]
per-ex loss: 0.453624  [   32/   88]
per-ex loss: 0.403076  [   34/   88]
per-ex loss: 0.460258  [   36/   88]
per-ex loss: 0.651378  [   38/   88]
per-ex loss: 0.430042  [   40/   88]
per-ex loss: 0.520009  [   42/   88]
per-ex loss: 0.714233  [   44/   88]
per-ex loss: 0.624950  [   46/   88]
per-ex loss: 0.545933  [   48/   88]
per-ex loss: 0.531635  [   50/   88]
per-ex loss: 0.531335  [   52/   88]
per-ex loss: 0.545460  [   54/   88]
per-ex loss: 0.589972  [   56/   88]
per-ex loss: 0.472511  [   58/   88]
per-ex loss: 0.404877  [   60/   88]
per-ex loss: 0.502704  [   62/   88]
per-ex loss: 0.521802  [   64/   88]
per-ex loss: 0.445382  [   66/   88]
per-ex loss: 0.469434  [   68/   88]
per-ex loss: 0.457054  [   70/   88]
per-ex loss: 0.425029  [   72/   88]
per-ex loss: 0.570635  [   74/   88]
per-ex loss: 0.557689  [   76/   88]
per-ex loss: 0.524878  [   78/   88]
per-ex loss: 0.440483  [   80/   88]
per-ex loss: 0.563632  [   82/   88]
per-ex loss: 0.496430  [   84/   88]
per-ex loss: 0.543516  [   86/   88]
per-ex loss: 0.399941  [   88/   88]
Train Error: Avg loss: 0.51182566
validation Error: 
 Avg loss: 0.55330791 
 F1: 0.467468 
 Precision: 0.546845 
 Recall: 0.408214
 IoU: 0.305030

test Error: 
 Avg loss: 0.51469452 
 F1: 0.533146 
 Precision: 0.631196 
 Recall: 0.461462
 IoU: 0.363462

We have finished training iteration 24
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_22_.pth
per-ex loss: 0.532671  [    2/   88]
per-ex loss: 0.676978  [    4/   88]
per-ex loss: 0.570353  [    6/   88]
per-ex loss: 0.547842  [    8/   88]
per-ex loss: 0.551692  [   10/   88]
per-ex loss: 0.481087  [   12/   88]
per-ex loss: 0.584636  [   14/   88]
per-ex loss: 0.598493  [   16/   88]
per-ex loss: 0.577666  [   18/   88]
per-ex loss: 0.389223  [   20/   88]
per-ex loss: 0.509316  [   22/   88]
per-ex loss: 0.438218  [   24/   88]
per-ex loss: 0.415306  [   26/   88]
per-ex loss: 0.450454  [   28/   88]
per-ex loss: 0.474278  [   30/   88]
per-ex loss: 0.442465  [   32/   88]
per-ex loss: 0.549118  [   34/   88]
per-ex loss: 0.508543  [   36/   88]
per-ex loss: 0.441813  [   38/   88]
per-ex loss: 0.534976  [   40/   88]
per-ex loss: 0.452572  [   42/   88]
per-ex loss: 0.380289  [   44/   88]
per-ex loss: 0.472564  [   46/   88]
per-ex loss: 0.642169  [   48/   88]
per-ex loss: 0.431763  [   50/   88]
per-ex loss: 0.574837  [   52/   88]
per-ex loss: 0.419132  [   54/   88]
per-ex loss: 0.383410  [   56/   88]
per-ex loss: 0.668988  [   58/   88]
per-ex loss: 0.602598  [   60/   88]
per-ex loss: 0.556057  [   62/   88]
per-ex loss: 0.471961  [   64/   88]
per-ex loss: 0.432069  [   66/   88]
per-ex loss: 0.603272  [   68/   88]
per-ex loss: 0.493708  [   70/   88]
per-ex loss: 0.493262  [   72/   88]
per-ex loss: 0.613759  [   74/   88]
per-ex loss: 0.551422  [   76/   88]
per-ex loss: 0.416902  [   78/   88]
per-ex loss: 0.418718  [   80/   88]
per-ex loss: 0.606018  [   82/   88]
per-ex loss: 0.532770  [   84/   88]
per-ex loss: 0.684454  [   86/   88]
per-ex loss: 0.400389  [   88/   88]
Train Error: Avg loss: 0.51314113
validation Error: 
 Avg loss: 0.55349450 
 F1: 0.482521 
 Precision: 0.529201 
 Recall: 0.443409
 IoU: 0.317976

test Error: 
 Avg loss: 0.50504839 
 F1: 0.540052 
 Precision: 0.608050 
 Recall: 0.485733
 IoU: 0.369912

We have finished training iteration 25
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_23_.pth
per-ex loss: 0.530188  [    2/   88]
per-ex loss: 0.484448  [    4/   88]
per-ex loss: 0.450038  [    6/   88]
per-ex loss: 0.548369  [    8/   88]
per-ex loss: 0.456325  [   10/   88]
per-ex loss: 0.375432  [   12/   88]
per-ex loss: 0.424394  [   14/   88]
per-ex loss: 0.534045  [   16/   88]
per-ex loss: 0.546291  [   18/   88]
per-ex loss: 0.478882  [   20/   88]
per-ex loss: 0.561527  [   22/   88]
per-ex loss: 0.676721  [   24/   88]
per-ex loss: 0.618548  [   26/   88]
per-ex loss: 0.470691  [   28/   88]
per-ex loss: 0.592291  [   30/   88]
per-ex loss: 0.537204  [   32/   88]
per-ex loss: 0.440807  [   34/   88]
per-ex loss: 0.480600  [   36/   88]
per-ex loss: 0.404591  [   38/   88]
per-ex loss: 0.517334  [   40/   88]
per-ex loss: 0.483352  [   42/   88]
per-ex loss: 0.407612  [   44/   88]
per-ex loss: 0.488796  [   46/   88]
per-ex loss: 0.397776  [   48/   88]
per-ex loss: 0.439543  [   50/   88]
per-ex loss: 0.543350  [   52/   88]
per-ex loss: 0.669068  [   54/   88]
per-ex loss: 0.429251  [   56/   88]
per-ex loss: 0.658635  [   58/   88]
per-ex loss: 0.435367  [   60/   88]
per-ex loss: 0.444774  [   62/   88]
per-ex loss: 0.376023  [   64/   88]
per-ex loss: 0.552922  [   66/   88]
per-ex loss: 0.430993  [   68/   88]
per-ex loss: 0.484247  [   70/   88]
per-ex loss: 0.699788  [   72/   88]
per-ex loss: 0.560098  [   74/   88]
per-ex loss: 0.514384  [   76/   88]
per-ex loss: 0.457088  [   78/   88]
per-ex loss: 0.559709  [   80/   88]
per-ex loss: 0.537166  [   82/   88]
per-ex loss: 0.488897  [   84/   88]
per-ex loss: 0.488973  [   86/   88]
per-ex loss: 0.523258  [   88/   88]
Train Error: Avg loss: 0.50454082
validation Error: 
 Avg loss: 0.53878539 
 F1: 0.487822 
 Precision: 0.556676 
 Recall: 0.434125
 IoU: 0.322595

test Error: 
 Avg loss: 0.49824978 
 F1: 0.551162 
 Precision: 0.628800 
 Recall: 0.490589
 IoU: 0.380416

We have finished training iteration 26
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_24_.pth
per-ex loss: 0.424263  [    2/   88]
per-ex loss: 0.479419  [    4/   88]
per-ex loss: 0.441622  [    6/   88]
per-ex loss: 0.629014  [    8/   88]
per-ex loss: 0.586052  [   10/   88]
per-ex loss: 0.473276  [   12/   88]
per-ex loss: 0.553698  [   14/   88]
per-ex loss: 0.565038  [   16/   88]
per-ex loss: 0.387186  [   18/   88]
per-ex loss: 0.421913  [   20/   88]
per-ex loss: 0.622731  [   22/   88]
per-ex loss: 0.614503  [   24/   88]
per-ex loss: 0.512540  [   26/   88]
per-ex loss: 0.408925  [   28/   88]
per-ex loss: 0.510945  [   30/   88]
per-ex loss: 0.427146  [   32/   88]
per-ex loss: 0.434126  [   34/   88]
per-ex loss: 0.628253  [   36/   88]
per-ex loss: 0.518233  [   38/   88]
per-ex loss: 0.368466  [   40/   88]
per-ex loss: 0.486342  [   42/   88]
per-ex loss: 0.522766  [   44/   88]
per-ex loss: 0.505587  [   46/   88]
per-ex loss: 0.437126  [   48/   88]
per-ex loss: 0.647628  [   50/   88]
per-ex loss: 0.467898  [   52/   88]
per-ex loss: 0.710923  [   54/   88]
per-ex loss: 0.413926  [   56/   88]
per-ex loss: 0.495812  [   58/   88]
per-ex loss: 0.400540  [   60/   88]
per-ex loss: 0.405829  [   62/   88]
per-ex loss: 0.422941  [   64/   88]
per-ex loss: 0.462527  [   66/   88]
per-ex loss: 0.512786  [   68/   88]
per-ex loss: 0.699319  [   70/   88]
per-ex loss: 0.443609  [   72/   88]
per-ex loss: 0.466506  [   74/   88]
per-ex loss: 0.415072  [   76/   88]
per-ex loss: 0.535183  [   78/   88]
per-ex loss: 0.451397  [   80/   88]
per-ex loss: 0.689118  [   82/   88]
per-ex loss: 0.482422  [   84/   88]
per-ex loss: 0.563814  [   86/   88]
per-ex loss: 0.618906  [   88/   88]
Train Error: Avg loss: 0.50603020
validation Error: 
 Avg loss: 0.55513298 
 F1: 0.481755 
 Precision: 0.615407 
 Recall: 0.395797
 IoU: 0.317311

test Error: 
 Avg loss: 0.51648119 
 F1: 0.529555 
 Precision: 0.682607 
 Recall: 0.432567
 IoU: 0.360133

We have finished training iteration 27
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_25_.pth
per-ex loss: 0.463384  [    2/   88]
per-ex loss: 0.376288  [    4/   88]
per-ex loss: 0.613809  [    6/   88]
per-ex loss: 0.499693  [    8/   88]
per-ex loss: 0.593712  [   10/   88]
per-ex loss: 0.625502  [   12/   88]
per-ex loss: 0.386827  [   14/   88]
per-ex loss: 0.389686  [   16/   88]
per-ex loss: 0.546910  [   18/   88]
per-ex loss: 0.510261  [   20/   88]
per-ex loss: 0.503674  [   22/   88]
per-ex loss: 0.422531  [   24/   88]
per-ex loss: 0.566718  [   26/   88]
per-ex loss: 0.425453  [   28/   88]
per-ex loss: 0.495393  [   30/   88]
per-ex loss: 0.417731  [   32/   88]
per-ex loss: 0.528467  [   34/   88]
per-ex loss: 0.415785  [   36/   88]
per-ex loss: 0.421922  [   38/   88]
per-ex loss: 0.556869  [   40/   88]
per-ex loss: 0.447912  [   42/   88]
per-ex loss: 0.447572  [   44/   88]
per-ex loss: 0.511620  [   46/   88]
per-ex loss: 0.432220  [   48/   88]
per-ex loss: 0.548584  [   50/   88]
per-ex loss: 0.398980  [   52/   88]
per-ex loss: 0.616329  [   54/   88]
per-ex loss: 0.535155  [   56/   88]
per-ex loss: 0.590293  [   58/   88]
per-ex loss: 0.462210  [   60/   88]
per-ex loss: 0.417640  [   62/   88]
per-ex loss: 0.435096  [   64/   88]
per-ex loss: 0.436356  [   66/   88]
per-ex loss: 0.592785  [   68/   88]
per-ex loss: 0.441010  [   70/   88]
per-ex loss: 0.623771  [   72/   88]
per-ex loss: 0.588855  [   74/   88]
per-ex loss: 0.557927  [   76/   88]
per-ex loss: 0.492311  [   78/   88]
per-ex loss: 0.450261  [   80/   88]
per-ex loss: 0.485479  [   82/   88]
per-ex loss: 0.515967  [   84/   88]
per-ex loss: 0.676174  [   86/   88]
per-ex loss: 0.534942  [   88/   88]
Train Error: Avg loss: 0.50000138
validation Error: 
 Avg loss: 0.57393584 
 F1: 0.453359 
 Precision: 0.471520 
 Recall: 0.436545
 IoU: 0.293125

test Error: 
 Avg loss: 0.51431674 
 F1: 0.529532 
 Precision: 0.585569 
 Recall: 0.483284
 IoU: 0.360111

We have finished training iteration 28
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_21_.pth
per-ex loss: 0.506946  [    2/   88]
per-ex loss: 0.606802  [    4/   88]
per-ex loss: 0.588762  [    6/   88]
per-ex loss: 0.660405  [    8/   88]
per-ex loss: 0.465688  [   10/   88]
per-ex loss: 0.418329  [   12/   88]
per-ex loss: 0.500226  [   14/   88]
per-ex loss: 0.505168  [   16/   88]
per-ex loss: 0.485179  [   18/   88]
per-ex loss: 0.665369  [   20/   88]
per-ex loss: 0.374160  [   22/   88]
per-ex loss: 0.435039  [   24/   88]
per-ex loss: 0.430111  [   26/   88]
per-ex loss: 0.636170  [   28/   88]
per-ex loss: 0.418530  [   30/   88]
per-ex loss: 0.392372  [   32/   88]
per-ex loss: 0.640623  [   34/   88]
per-ex loss: 0.381350  [   36/   88]
per-ex loss: 0.576493  [   38/   88]
per-ex loss: 0.413950  [   40/   88]
per-ex loss: 0.387273  [   42/   88]
per-ex loss: 0.390953  [   44/   88]
per-ex loss: 0.559831  [   46/   88]
per-ex loss: 0.461090  [   48/   88]
per-ex loss: 0.450848  [   50/   88]
per-ex loss: 0.448257  [   52/   88]
per-ex loss: 0.427618  [   54/   88]
per-ex loss: 0.725419  [   56/   88]
per-ex loss: 0.680028  [   58/   88]
per-ex loss: 0.564391  [   60/   88]
per-ex loss: 0.658160  [   62/   88]
per-ex loss: 0.439128  [   64/   88]
per-ex loss: 0.557892  [   66/   88]
per-ex loss: 0.457203  [   68/   88]
per-ex loss: 0.457355  [   70/   88]
per-ex loss: 0.449432  [   72/   88]
per-ex loss: 0.595701  [   74/   88]
per-ex loss: 0.668255  [   76/   88]
per-ex loss: 0.442577  [   78/   88]
per-ex loss: 0.528040  [   80/   88]
per-ex loss: 0.473647  [   82/   88]
per-ex loss: 0.403623  [   84/   88]
per-ex loss: 0.587430  [   86/   88]
per-ex loss: 0.479980  [   88/   88]
Train Error: Avg loss: 0.50899557
validation Error: 
 Avg loss: 0.54507950 
 F1: 0.488930 
 Precision: 0.555771 
 Recall: 0.436440
 IoU: 0.323565

test Error: 
 Avg loss: 0.50105342 
 F1: 0.544671 
 Precision: 0.624926 
 Recall: 0.482683
 IoU: 0.374260

We have finished training iteration 29
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_27_.pth
per-ex loss: 0.416132  [    2/   88]
per-ex loss: 0.545330  [    4/   88]
per-ex loss: 0.497840  [    6/   88]
per-ex loss: 0.435703  [    8/   88]
per-ex loss: 0.400354  [   10/   88]
per-ex loss: 0.505677  [   12/   88]
per-ex loss: 0.509971  [   14/   88]
per-ex loss: 0.469444  [   16/   88]
per-ex loss: 0.725384  [   18/   88]
per-ex loss: 0.677077  [   20/   88]
per-ex loss: 0.552344  [   22/   88]
per-ex loss: 0.444026  [   24/   88]
per-ex loss: 0.421202  [   26/   88]
per-ex loss: 0.599321  [   28/   88]
per-ex loss: 0.413776  [   30/   88]
per-ex loss: 0.413389  [   32/   88]
per-ex loss: 0.404187  [   34/   88]
per-ex loss: 0.427909  [   36/   88]
per-ex loss: 0.608266  [   38/   88]
per-ex loss: 0.402717  [   40/   88]
per-ex loss: 0.458219  [   42/   88]
per-ex loss: 0.627600  [   44/   88]
per-ex loss: 0.484855  [   46/   88]
per-ex loss: 0.399051  [   48/   88]
per-ex loss: 0.443111  [   50/   88]
per-ex loss: 0.380296  [   52/   88]
per-ex loss: 0.512884  [   54/   88]
per-ex loss: 0.568127  [   56/   88]
per-ex loss: 0.417796  [   58/   88]
per-ex loss: 0.528755  [   60/   88]
per-ex loss: 0.523866  [   62/   88]
per-ex loss: 0.510685  [   64/   88]
per-ex loss: 0.420059  [   66/   88]
per-ex loss: 0.430724  [   68/   88]
per-ex loss: 0.518194  [   70/   88]
per-ex loss: 0.426832  [   72/   88]
per-ex loss: 0.530802  [   74/   88]
per-ex loss: 0.490631  [   76/   88]
per-ex loss: 0.560116  [   78/   88]
per-ex loss: 0.586799  [   80/   88]
per-ex loss: 0.636993  [   82/   88]
per-ex loss: 0.565462  [   84/   88]
per-ex loss: 0.417982  [   86/   88]
per-ex loss: 0.665218  [   88/   88]
Train Error: Avg loss: 0.49943423
validation Error: 
 Avg loss: 0.54569038 
 F1: 0.483656 
 Precision: 0.628442 
 Recall: 0.393092
 IoU: 0.318962

test Error: 
 Avg loss: 0.51293218 
 F1: 0.532368 
 Precision: 0.694827 
 Recall: 0.431482
 IoU: 0.362739

We have finished training iteration 30
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_28_.pth
per-ex loss: 0.390311  [    2/   88]
per-ex loss: 0.372974  [    4/   88]
per-ex loss: 0.447810  [    6/   88]
per-ex loss: 0.633736  [    8/   88]
per-ex loss: 0.594208  [   10/   88]
per-ex loss: 0.512506  [   12/   88]
per-ex loss: 0.469581  [   14/   88]
per-ex loss: 0.423945  [   16/   88]
per-ex loss: 0.441064  [   18/   88]
per-ex loss: 0.509987  [   20/   88]
per-ex loss: 0.437883  [   22/   88]
per-ex loss: 0.399719  [   24/   88]
per-ex loss: 0.485649  [   26/   88]
per-ex loss: 0.383361  [   28/   88]
per-ex loss: 0.449396  [   30/   88]
per-ex loss: 0.553329  [   32/   88]
per-ex loss: 0.486306  [   34/   88]
per-ex loss: 0.574762  [   36/   88]
per-ex loss: 0.368884  [   38/   88]
per-ex loss: 0.386325  [   40/   88]
per-ex loss: 0.471780  [   42/   88]
per-ex loss: 0.410419  [   44/   88]
per-ex loss: 0.611438  [   46/   88]
per-ex loss: 0.464922  [   48/   88]
per-ex loss: 0.689510  [   50/   88]
per-ex loss: 0.421145  [   52/   88]
per-ex loss: 0.602403  [   54/   88]
per-ex loss: 0.421360  [   56/   88]
per-ex loss: 0.519518  [   58/   88]
per-ex loss: 0.368419  [   60/   88]
per-ex loss: 0.559953  [   62/   88]
per-ex loss: 0.411670  [   64/   88]
per-ex loss: 0.636214  [   66/   88]
per-ex loss: 0.514231  [   68/   88]
per-ex loss: 0.520466  [   70/   88]
per-ex loss: 0.460242  [   72/   88]
per-ex loss: 0.406445  [   74/   88]
per-ex loss: 0.530895  [   76/   88]
per-ex loss: 0.444321  [   78/   88]
per-ex loss: 0.597717  [   80/   88]
per-ex loss: 0.505201  [   82/   88]
per-ex loss: 0.505626  [   84/   88]
per-ex loss: 0.442218  [   86/   88]
per-ex loss: 0.456164  [   88/   88]
Train Error: Avg loss: 0.48395488
validation Error: 
 Avg loss: 0.56694745 
 F1: 0.462243 
 Precision: 0.448195 
 Recall: 0.477200
 IoU: 0.300596

test Error: 
 Avg loss: 0.50047426 
 F1: 0.543492 
 Precision: 0.565977 
 Recall: 0.522726
 IoU: 0.373148

We have finished training iteration 31
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_17_.pth
per-ex loss: 0.643586  [    2/   88]
per-ex loss: 0.484046  [    4/   88]
per-ex loss: 0.475933  [    6/   88]
per-ex loss: 0.656472  [    8/   88]
per-ex loss: 0.506653  [   10/   88]
per-ex loss: 0.562858  [   12/   88]
per-ex loss: 0.612135  [   14/   88]
per-ex loss: 0.516585  [   16/   88]
per-ex loss: 0.427113  [   18/   88]
per-ex loss: 0.427836  [   20/   88]
per-ex loss: 0.588929  [   22/   88]
per-ex loss: 0.436273  [   24/   88]
per-ex loss: 0.472643  [   26/   88]
per-ex loss: 0.601997  [   28/   88]
per-ex loss: 0.542760  [   30/   88]
per-ex loss: 0.401562  [   32/   88]
per-ex loss: 0.404458  [   34/   88]
per-ex loss: 0.502014  [   36/   88]
per-ex loss: 0.452245  [   38/   88]
per-ex loss: 0.610092  [   40/   88]
per-ex loss: 0.504902  [   42/   88]
per-ex loss: 0.441968  [   44/   88]
per-ex loss: 0.372304  [   46/   88]
per-ex loss: 0.406682  [   48/   88]
per-ex loss: 0.477634  [   50/   88]
per-ex loss: 0.580543  [   52/   88]
per-ex loss: 0.583790  [   54/   88]
per-ex loss: 0.465720  [   56/   88]
per-ex loss: 0.559222  [   58/   88]
per-ex loss: 0.477776  [   60/   88]
per-ex loss: 0.450873  [   62/   88]
per-ex loss: 0.415161  [   64/   88]
per-ex loss: 0.477360  [   66/   88]
per-ex loss: 0.408280  [   68/   88]
per-ex loss: 0.389957  [   70/   88]
per-ex loss: 0.369749  [   72/   88]
per-ex loss: 0.432410  [   74/   88]
per-ex loss: 0.617550  [   76/   88]
per-ex loss: 0.566068  [   78/   88]
per-ex loss: 0.435741  [   80/   88]
per-ex loss: 0.475873  [   82/   88]
per-ex loss: 0.384792  [   84/   88]
per-ex loss: 0.654395  [   86/   88]
per-ex loss: 0.400617  [   88/   88]
Train Error: Avg loss: 0.49262634
validation Error: 
 Avg loss: 0.54539643 
 F1: 0.490153 
 Precision: 0.561927 
 Recall: 0.434637
 IoU: 0.324637

test Error: 
 Avg loss: 0.49524764 
 F1: 0.552039 
 Precision: 0.633522 
 Recall: 0.489127
 IoU: 0.381252

We have finished training iteration 32
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_30_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.669972  [    2/   88]
per-ex loss: 0.578482  [    4/   88]
per-ex loss: 0.609334  [    6/   88]
per-ex loss: 0.623405  [    8/   88]
per-ex loss: 0.413137  [   10/   88]
per-ex loss: 0.480385  [   12/   88]
per-ex loss: 0.503058  [   14/   88]
per-ex loss: 0.466860  [   16/   88]
per-ex loss: 0.683843  [   18/   88]
per-ex loss: 0.471452  [   20/   88]
per-ex loss: 0.636405  [   22/   88]
per-ex loss: 0.403068  [   24/   88]
per-ex loss: 0.494449  [   26/   88]
per-ex loss: 0.507222  [   28/   88]
per-ex loss: 0.469995  [   30/   88]
per-ex loss: 0.486327  [   32/   88]
per-ex loss: 0.686562  [   34/   88]
per-ex loss: 0.425146  [   36/   88]
per-ex loss: 0.533925  [   38/   88]
per-ex loss: 0.482757  [   40/   88]
per-ex loss: 0.670079  [   42/   88]
per-ex loss: 0.433203  [   44/   88]
per-ex loss: 0.396640  [   46/   88]
per-ex loss: 0.433846  [   48/   88]
per-ex loss: 0.483859  [   50/   88]
per-ex loss: 0.421366  [   52/   88]
per-ex loss: 0.390284  [   54/   88]
per-ex loss: 0.589432  [   56/   88]
per-ex loss: 0.443590  [   58/   88]
per-ex loss: 0.500575  [   60/   88]
per-ex loss: 0.367612  [   62/   88]
per-ex loss: 0.440475  [   64/   88]
per-ex loss: 0.616638  [   66/   88]
per-ex loss: 0.362736  [   68/   88]
per-ex loss: 0.461514  [   70/   88]
per-ex loss: 0.473219  [   72/   88]
per-ex loss: 0.518002  [   74/   88]
per-ex loss: 0.573801  [   76/   88]
per-ex loss: 0.380410  [   78/   88]
per-ex loss: 0.625728  [   80/   88]
per-ex loss: 0.453976  [   82/   88]
per-ex loss: 0.414332  [   84/   88]
per-ex loss: 0.652002  [   86/   88]
per-ex loss: 0.492629  [   88/   88]
Train Error: Avg loss: 0.50503940
validation Error: 
 Avg loss: 0.53627261 
 F1: 0.487385 
 Precision: 0.527783 
 Recall: 0.452731
 IoU: 0.322213

test Error: 
 Avg loss: 0.49145413 
 F1: 0.552196 
 Precision: 0.619347 
 Recall: 0.498182
 IoU: 0.381403

We have finished training iteration 33
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_31_.pth
per-ex loss: 0.398829  [    2/   88]
per-ex loss: 0.525583  [    4/   88]
per-ex loss: 0.620859  [    6/   88]
per-ex loss: 0.520596  [    8/   88]
per-ex loss: 0.484030  [   10/   88]
per-ex loss: 0.583875  [   12/   88]
per-ex loss: 0.478569  [   14/   88]
per-ex loss: 0.492665  [   16/   88]
per-ex loss: 0.743526  [   18/   88]
per-ex loss: 0.543309  [   20/   88]
per-ex loss: 0.434405  [   22/   88]
per-ex loss: 0.535224  [   24/   88]
per-ex loss: 0.534306  [   26/   88]
per-ex loss: 0.464054  [   28/   88]
per-ex loss: 0.464014  [   30/   88]
per-ex loss: 0.464303  [   32/   88]
per-ex loss: 0.492470  [   34/   88]
per-ex loss: 0.591681  [   36/   88]
per-ex loss: 0.407335  [   38/   88]
per-ex loss: 0.510652  [   40/   88]
per-ex loss: 0.435441  [   42/   88]
per-ex loss: 0.674305  [   44/   88]
per-ex loss: 0.499254  [   46/   88]
per-ex loss: 0.442235  [   48/   88]
per-ex loss: 0.418108  [   50/   88]
per-ex loss: 0.385848  [   52/   88]
per-ex loss: 0.482211  [   54/   88]
per-ex loss: 0.531490  [   56/   88]
per-ex loss: 0.648994  [   58/   88]
per-ex loss: 0.423733  [   60/   88]
per-ex loss: 0.505865  [   62/   88]
per-ex loss: 0.521372  [   64/   88]
per-ex loss: 0.577591  [   66/   88]
per-ex loss: 0.538346  [   68/   88]
per-ex loss: 0.388990  [   70/   88]
per-ex loss: 0.411849  [   72/   88]
per-ex loss: 0.617833  [   74/   88]
per-ex loss: 0.371608  [   76/   88]
per-ex loss: 0.413922  [   78/   88]
per-ex loss: 0.521753  [   80/   88]
per-ex loss: 0.613722  [   82/   88]
per-ex loss: 0.630626  [   84/   88]
per-ex loss: 0.481285  [   86/   88]
per-ex loss: 0.430712  [   88/   88]
Train Error: Avg loss: 0.50584947
validation Error: 
 Avg loss: 0.55771326 
 F1: 0.477879 
 Precision: 0.617507 
 Recall: 0.389750
 IoU: 0.313956

test Error: 
 Avg loss: 0.51480893 
 F1: 0.529084 
 Precision: 0.694207 
 Recall: 0.427419
 IoU: 0.359697

We have finished training iteration 34
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_32_.pth
per-ex loss: 0.521404  [    2/   88]
per-ex loss: 0.565959  [    4/   88]
per-ex loss: 0.643280  [    6/   88]
per-ex loss: 0.499469  [    8/   88]
per-ex loss: 0.409725  [   10/   88]
per-ex loss: 0.439999  [   12/   88]
per-ex loss: 0.520678  [   14/   88]
per-ex loss: 0.632600  [   16/   88]
per-ex loss: 0.449774  [   18/   88]
per-ex loss: 0.512027  [   20/   88]
per-ex loss: 0.415615  [   22/   88]
per-ex loss: 0.389676  [   24/   88]
per-ex loss: 0.657808  [   26/   88]
per-ex loss: 0.576604  [   28/   88]
per-ex loss: 0.509692  [   30/   88]
per-ex loss: 0.569657  [   32/   88]
per-ex loss: 0.407218  [   34/   88]
per-ex loss: 0.463096  [   36/   88]
per-ex loss: 0.400491  [   38/   88]
per-ex loss: 0.474543  [   40/   88]
per-ex loss: 0.404846  [   42/   88]
per-ex loss: 0.519215  [   44/   88]
per-ex loss: 0.641917  [   46/   88]
per-ex loss: 0.376917  [   48/   88]
per-ex loss: 0.680250  [   50/   88]
per-ex loss: 0.571584  [   52/   88]
per-ex loss: 0.391912  [   54/   88]
per-ex loss: 0.456789  [   56/   88]
per-ex loss: 0.564719  [   58/   88]
per-ex loss: 0.385581  [   60/   88]
per-ex loss: 0.604024  [   62/   88]
per-ex loss: 0.451842  [   64/   88]
per-ex loss: 0.407649  [   66/   88]
per-ex loss: 0.674729  [   68/   88]
per-ex loss: 0.421319  [   70/   88]
per-ex loss: 0.607375  [   72/   88]
per-ex loss: 0.523422  [   74/   88]
per-ex loss: 0.414061  [   76/   88]
per-ex loss: 0.574999  [   78/   88]
per-ex loss: 0.482114  [   80/   88]
per-ex loss: 0.568804  [   82/   88]
per-ex loss: 0.539521  [   84/   88]
per-ex loss: 0.387426  [   86/   88]
per-ex loss: 0.531114  [   88/   88]
Train Error: Avg loss: 0.50548730
validation Error: 
 Avg loss: 0.56604555 
 F1: 0.478065 
 Precision: 0.572843 
 Recall: 0.410197
 IoU: 0.314117

test Error: 
 Avg loss: 0.51286355 
 F1: 0.534781 
 Precision: 0.652510 
 Recall: 0.453041
 IoU: 0.364984

We have finished training iteration 35
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_29_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.474614  [    2/   88]
per-ex loss: 0.453814  [    4/   88]
per-ex loss: 0.420388  [    6/   88]
per-ex loss: 0.443056  [    8/   88]
per-ex loss: 0.423879  [   10/   88]
per-ex loss: 0.541338  [   12/   88]
per-ex loss: 0.436499  [   14/   88]
per-ex loss: 0.409949  [   16/   88]
per-ex loss: 0.717360  [   18/   88]
per-ex loss: 0.455202  [   20/   88]
per-ex loss: 0.512005  [   22/   88]
per-ex loss: 0.423216  [   24/   88]
per-ex loss: 0.497073  [   26/   88]
per-ex loss: 0.611551  [   28/   88]
per-ex loss: 0.550967  [   30/   88]
per-ex loss: 0.393342  [   32/   88]
per-ex loss: 0.648513  [   34/   88]
per-ex loss: 0.560148  [   36/   88]
per-ex loss: 0.541053  [   38/   88]
per-ex loss: 0.474933  [   40/   88]
per-ex loss: 0.644373  [   42/   88]
per-ex loss: 0.476023  [   44/   88]
per-ex loss: 0.439570  [   46/   88]
per-ex loss: 0.393275  [   48/   88]
per-ex loss: 0.582889  [   50/   88]
per-ex loss: 0.652651  [   52/   88]
per-ex loss: 0.423388  [   54/   88]
per-ex loss: 0.392888  [   56/   88]
per-ex loss: 0.632107  [   58/   88]
per-ex loss: 0.517068  [   60/   88]
per-ex loss: 0.525117  [   62/   88]
per-ex loss: 0.432924  [   64/   88]
per-ex loss: 0.475719  [   66/   88]
per-ex loss: 0.526707  [   68/   88]
per-ex loss: 0.517621  [   70/   88]
per-ex loss: 0.578020  [   72/   88]
per-ex loss: 0.379012  [   74/   88]
per-ex loss: 0.470203  [   76/   88]
per-ex loss: 0.379743  [   78/   88]
per-ex loss: 0.496917  [   80/   88]
per-ex loss: 0.523994  [   82/   88]
per-ex loss: 0.398407  [   84/   88]
per-ex loss: 0.590577  [   86/   88]
per-ex loss: 0.581411  [   88/   88]
Train Error: Avg loss: 0.50044333
validation Error: 
 Avg loss: 0.54019993 
 F1: 0.490119 
 Precision: 0.568162 
 Recall: 0.430927
 IoU: 0.324608

test Error: 
 Avg loss: 0.49828155 
 F1: 0.546056 
 Precision: 0.651920 
 Recall: 0.469771
 IoU: 0.375569

We have finished training iteration 36
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_34_.pth
per-ex loss: 0.470636  [    2/   88]
per-ex loss: 0.502561  [    4/   88]
per-ex loss: 0.479845  [    6/   88]
per-ex loss: 0.476212  [    8/   88]
per-ex loss: 0.456425  [   10/   88]
per-ex loss: 0.427561  [   12/   88]
per-ex loss: 0.482363  [   14/   88]
per-ex loss: 0.423367  [   16/   88]
per-ex loss: 0.479594  [   18/   88]
per-ex loss: 0.616987  [   20/   88]
per-ex loss: 0.418937  [   22/   88]
per-ex loss: 0.393126  [   24/   88]
per-ex loss: 0.429142  [   26/   88]
per-ex loss: 0.616348  [   28/   88]
per-ex loss: 0.373826  [   30/   88]
per-ex loss: 0.478068  [   32/   88]
per-ex loss: 0.517269  [   34/   88]
per-ex loss: 0.467014  [   36/   88]
per-ex loss: 0.576919  [   38/   88]
per-ex loss: 0.587162  [   40/   88]
per-ex loss: 0.394112  [   42/   88]
per-ex loss: 0.416913  [   44/   88]
per-ex loss: 0.422449  [   46/   88]
per-ex loss: 0.492615  [   48/   88]
per-ex loss: 0.521975  [   50/   88]
per-ex loss: 0.508552  [   52/   88]
per-ex loss: 0.633547  [   54/   88]
per-ex loss: 0.461517  [   56/   88]
per-ex loss: 0.566061  [   58/   88]
per-ex loss: 0.385950  [   60/   88]
per-ex loss: 0.408134  [   62/   88]
per-ex loss: 0.557637  [   64/   88]
per-ex loss: 0.458360  [   66/   88]
per-ex loss: 0.487953  [   68/   88]
per-ex loss: 0.525382  [   70/   88]
per-ex loss: 0.443684  [   72/   88]
per-ex loss: 0.416961  [   74/   88]
per-ex loss: 0.678628  [   76/   88]
per-ex loss: 0.537304  [   78/   88]
per-ex loss: 0.534308  [   80/   88]
per-ex loss: 0.389936  [   82/   88]
per-ex loss: 0.404277  [   84/   88]
per-ex loss: 0.526459  [   86/   88]
per-ex loss: 0.389502  [   88/   88]
Train Error: Avg loss: 0.48262680
validation Error: 
 Avg loss: 0.54743515 
 F1: 0.478757 
 Precision: 0.526294 
 Recall: 0.439096
 IoU: 0.314714

test Error: 
 Avg loss: 0.50250726 
 F1: 0.538822 
 Precision: 0.605470 
 Recall: 0.485391
 IoU: 0.368758

We have finished training iteration 37
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_35_.pth
per-ex loss: 0.464082  [    2/   88]
per-ex loss: 0.373359  [    4/   88]
per-ex loss: 0.424750  [    6/   88]
per-ex loss: 0.417878  [    8/   88]
per-ex loss: 0.705137  [   10/   88]
per-ex loss: 0.529678  [   12/   88]
per-ex loss: 0.486963  [   14/   88]
per-ex loss: 0.517057  [   16/   88]
per-ex loss: 0.562362  [   18/   88]
per-ex loss: 0.482832  [   20/   88]
per-ex loss: 0.420457  [   22/   88]
per-ex loss: 0.397947  [   24/   88]
per-ex loss: 0.540688  [   26/   88]
per-ex loss: 0.593039  [   28/   88]
per-ex loss: 0.698761  [   30/   88]
per-ex loss: 0.475500  [   32/   88]
per-ex loss: 0.447225  [   34/   88]
per-ex loss: 0.367911  [   36/   88]
per-ex loss: 0.584556  [   38/   88]
per-ex loss: 0.404460  [   40/   88]
per-ex loss: 0.431889  [   42/   88]
per-ex loss: 0.643795  [   44/   88]
per-ex loss: 0.421738  [   46/   88]
per-ex loss: 0.492092  [   48/   88]
per-ex loss: 0.422616  [   50/   88]
per-ex loss: 0.374915  [   52/   88]
per-ex loss: 0.430372  [   54/   88]
per-ex loss: 0.582449  [   56/   88]
per-ex loss: 0.484839  [   58/   88]
per-ex loss: 0.416400  [   60/   88]
per-ex loss: 0.488285  [   62/   88]
per-ex loss: 0.465430  [   64/   88]
per-ex loss: 0.581048  [   66/   88]
per-ex loss: 0.406925  [   68/   88]
per-ex loss: 0.454396  [   70/   88]
per-ex loss: 0.454849  [   72/   88]
per-ex loss: 0.425210  [   74/   88]
per-ex loss: 0.447479  [   76/   88]
per-ex loss: 0.404387  [   78/   88]
per-ex loss: 0.505101  [   80/   88]
per-ex loss: 0.630913  [   82/   88]
per-ex loss: 0.558442  [   84/   88]
per-ex loss: 0.454007  [   86/   88]
per-ex loss: 0.433631  [   88/   88]
Train Error: Avg loss: 0.48422389
validation Error: 
 Avg loss: 0.54055731 
 F1: 0.482295 
 Precision: 0.553161 
 Recall: 0.427524
 IoU: 0.317779

test Error: 
 Avg loss: 0.50023591 
 F1: 0.541036 
 Precision: 0.605527 
 Recall: 0.488960
 IoU: 0.370836

We have finished training iteration 38
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_36_.pth
per-ex loss: 0.659897  [    2/   88]
per-ex loss: 0.374271  [    4/   88]
per-ex loss: 0.454083  [    6/   88]
per-ex loss: 0.361707  [    8/   88]
per-ex loss: 0.463543  [   10/   88]
per-ex loss: 0.674080  [   12/   88]
per-ex loss: 0.433402  [   14/   88]
per-ex loss: 0.491188  [   16/   88]
per-ex loss: 0.482286  [   18/   88]
per-ex loss: 0.474918  [   20/   88]
per-ex loss: 0.541129  [   22/   88]
per-ex loss: 0.490004  [   24/   88]
per-ex loss: 0.408150  [   26/   88]
per-ex loss: 0.381857  [   28/   88]
per-ex loss: 0.479345  [   30/   88]
per-ex loss: 0.494216  [   32/   88]
per-ex loss: 0.555161  [   34/   88]
per-ex loss: 0.648579  [   36/   88]
per-ex loss: 0.569631  [   38/   88]
per-ex loss: 0.479649  [   40/   88]
per-ex loss: 0.400049  [   42/   88]
per-ex loss: 0.539473  [   44/   88]
per-ex loss: 0.498392  [   46/   88]
per-ex loss: 0.565962  [   48/   88]
per-ex loss: 0.392138  [   50/   88]
per-ex loss: 0.573014  [   52/   88]
per-ex loss: 0.598393  [   54/   88]
per-ex loss: 0.395374  [   56/   88]
per-ex loss: 0.489291  [   58/   88]
per-ex loss: 0.653590  [   60/   88]
per-ex loss: 0.607904  [   62/   88]
per-ex loss: 0.441503  [   64/   88]
per-ex loss: 0.405453  [   66/   88]
per-ex loss: 0.457234  [   68/   88]
per-ex loss: 0.568919  [   70/   88]
per-ex loss: 0.553872  [   72/   88]
per-ex loss: 0.505304  [   74/   88]
per-ex loss: 0.395936  [   76/   88]
per-ex loss: 0.370198  [   78/   88]
per-ex loss: 0.434557  [   80/   88]
per-ex loss: 0.664009  [   82/   88]
per-ex loss: 0.392234  [   84/   88]
per-ex loss: 0.419796  [   86/   88]
per-ex loss: 0.584534  [   88/   88]
Train Error: Avg loss: 0.49600517
validation Error: 
 Avg loss: 0.54149128 
 F1: 0.490524 
 Precision: 0.621224 
 Recall: 0.405261
 IoU: 0.324963

test Error: 
 Avg loss: 0.50126253 
 F1: 0.546836 
 Precision: 0.682096 
 Recall: 0.456343
 IoU: 0.376307

We have finished training iteration 39
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_37_.pth
per-ex loss: 0.552468  [    2/   88]
per-ex loss: 0.534516  [    4/   88]
per-ex loss: 0.411593  [    6/   88]
per-ex loss: 0.598636  [    8/   88]
per-ex loss: 0.486951  [   10/   88]
per-ex loss: 0.437526  [   12/   88]
per-ex loss: 0.499841  [   14/   88]
per-ex loss: 0.498374  [   16/   88]
per-ex loss: 0.491362  [   18/   88]
per-ex loss: 0.601921  [   20/   88]
per-ex loss: 0.544919  [   22/   88]
per-ex loss: 0.522872  [   24/   88]
per-ex loss: 0.432412  [   26/   88]
per-ex loss: 0.469485  [   28/   88]
per-ex loss: 0.573358  [   30/   88]
per-ex loss: 0.416950  [   32/   88]
per-ex loss: 0.594008  [   34/   88]
per-ex loss: 0.372718  [   36/   88]
per-ex loss: 0.612633  [   38/   88]
per-ex loss: 0.520933  [   40/   88]
per-ex loss: 0.446425  [   42/   88]
per-ex loss: 0.415154  [   44/   88]
per-ex loss: 0.487913  [   46/   88]
per-ex loss: 0.390247  [   48/   88]
per-ex loss: 0.420517  [   50/   88]
per-ex loss: 0.444411  [   52/   88]
per-ex loss: 0.630378  [   54/   88]
per-ex loss: 0.394251  [   56/   88]
per-ex loss: 0.569723  [   58/   88]
per-ex loss: 0.353663  [   60/   88]
per-ex loss: 0.384361  [   62/   88]
per-ex loss: 0.389635  [   64/   88]
per-ex loss: 0.545466  [   66/   88]
per-ex loss: 0.508962  [   68/   88]
per-ex loss: 0.461986  [   70/   88]
per-ex loss: 0.538273  [   72/   88]
per-ex loss: 0.425702  [   74/   88]
per-ex loss: 0.436434  [   76/   88]
per-ex loss: 0.422929  [   78/   88]
per-ex loss: 0.533542  [   80/   88]
per-ex loss: 0.539994  [   82/   88]
per-ex loss: 0.578262  [   84/   88]
per-ex loss: 0.660921  [   86/   88]
per-ex loss: 0.470198  [   88/   88]
Train Error: Avg loss: 0.49142782
validation Error: 
 Avg loss: 0.54383090 
 F1: 0.487035 
 Precision: 0.552349 
 Recall: 0.435534
 IoU: 0.321907

test Error: 
 Avg loss: 0.49877873 
 F1: 0.543840 
 Precision: 0.619343 
 Recall: 0.484745
 IoU: 0.373475

We have finished training iteration 40
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_38_.pth
per-ex loss: 0.599349  [    2/   88]
per-ex loss: 0.405915  [    4/   88]
per-ex loss: 0.618075  [    6/   88]
per-ex loss: 0.590414  [    8/   88]
per-ex loss: 0.411882  [   10/   88]
per-ex loss: 0.583307  [   12/   88]
per-ex loss: 0.614636  [   14/   88]
per-ex loss: 0.393171  [   16/   88]
per-ex loss: 0.421980  [   18/   88]
per-ex loss: 0.453638  [   20/   88]
per-ex loss: 0.436799  [   22/   88]
per-ex loss: 0.456455  [   24/   88]
per-ex loss: 0.405300  [   26/   88]
per-ex loss: 0.499899  [   28/   88]
per-ex loss: 0.557158  [   30/   88]
per-ex loss: 0.500661  [   32/   88]
per-ex loss: 0.607645  [   34/   88]
per-ex loss: 0.473222  [   36/   88]
per-ex loss: 0.397961  [   38/   88]
per-ex loss: 0.433136  [   40/   88]
per-ex loss: 0.500427  [   42/   88]
per-ex loss: 0.469559  [   44/   88]
per-ex loss: 0.491011  [   46/   88]
per-ex loss: 0.428066  [   48/   88]
per-ex loss: 0.409847  [   50/   88]
per-ex loss: 0.426432  [   52/   88]
per-ex loss: 0.657670  [   54/   88]
per-ex loss: 0.420393  [   56/   88]
per-ex loss: 0.406326  [   58/   88]
per-ex loss: 0.489467  [   60/   88]
per-ex loss: 0.563519  [   62/   88]
per-ex loss: 0.582504  [   64/   88]
per-ex loss: 0.366630  [   66/   88]
per-ex loss: 0.464029  [   68/   88]
per-ex loss: 0.503429  [   70/   88]
per-ex loss: 0.403144  [   72/   88]
per-ex loss: 0.648364  [   74/   88]
per-ex loss: 0.418706  [   76/   88]
per-ex loss: 0.586046  [   78/   88]
per-ex loss: 0.646848  [   80/   88]
per-ex loss: 0.372364  [   82/   88]
per-ex loss: 0.453352  [   84/   88]
per-ex loss: 0.606852  [   86/   88]
per-ex loss: 0.423149  [   88/   88]
Train Error: Avg loss: 0.49088038
validation Error: 
 Avg loss: 0.55245646 
 F1: 0.478561 
 Precision: 0.594504 
 Recall: 0.400461
 IoU: 0.314545

test Error: 
 Avg loss: 0.51209314 
 F1: 0.525566 
 Precision: 0.691470 
 Recall: 0.423868
 IoU: 0.356453

We have finished training iteration 41
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_39_.pth
per-ex loss: 0.524012  [    2/   88]
per-ex loss: 0.401248  [    4/   88]
per-ex loss: 0.601591  [    6/   88]
per-ex loss: 0.352513  [    8/   88]
per-ex loss: 0.483932  [   10/   88]
per-ex loss: 0.471103  [   12/   88]
per-ex loss: 0.431800  [   14/   88]
per-ex loss: 0.443555  [   16/   88]
per-ex loss: 0.423117  [   18/   88]
per-ex loss: 0.493391  [   20/   88]
per-ex loss: 0.450343  [   22/   88]
per-ex loss: 0.536618  [   24/   88]
per-ex loss: 0.413767  [   26/   88]
per-ex loss: 0.387751  [   28/   88]
per-ex loss: 0.515681  [   30/   88]
per-ex loss: 0.432671  [   32/   88]
per-ex loss: 0.513426  [   34/   88]
per-ex loss: 0.358270  [   36/   88]
per-ex loss: 0.573733  [   38/   88]
per-ex loss: 0.378526  [   40/   88]
per-ex loss: 0.428361  [   42/   88]
per-ex loss: 0.572527  [   44/   88]
per-ex loss: 0.462385  [   46/   88]
per-ex loss: 0.591640  [   48/   88]
per-ex loss: 0.422834  [   50/   88]
per-ex loss: 0.437650  [   52/   88]
per-ex loss: 0.631814  [   54/   88]
per-ex loss: 0.418602  [   56/   88]
per-ex loss: 0.498692  [   58/   88]
per-ex loss: 0.490021  [   60/   88]
per-ex loss: 0.386114  [   62/   88]
per-ex loss: 0.481570  [   64/   88]
per-ex loss: 0.508882  [   66/   88]
per-ex loss: 0.370923  [   68/   88]
per-ex loss: 0.499829  [   70/   88]
per-ex loss: 0.563838  [   72/   88]
per-ex loss: 0.682602  [   74/   88]
per-ex loss: 0.635747  [   76/   88]
per-ex loss: 0.374087  [   78/   88]
per-ex loss: 0.448730  [   80/   88]
per-ex loss: 0.583827  [   82/   88]
per-ex loss: 0.429420  [   84/   88]
per-ex loss: 0.399760  [   86/   88]
per-ex loss: 0.597818  [   88/   88]
Train Error: Avg loss: 0.47965272
validation Error: 
 Avg loss: 0.53236522 
 F1: 0.490230 
 Precision: 0.491740 
 Recall: 0.488730
 IoU: 0.324705

test Error: 
 Avg loss: 0.48591818 
 F1: 0.554581 
 Precision: 0.559756 
 Recall: 0.549502
 IoU: 0.383682

We have finished training iteration 42
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_40_.pth
per-ex loss: 0.398399  [    2/   88]
per-ex loss: 0.498065  [    4/   88]
per-ex loss: 0.507168  [    6/   88]
per-ex loss: 0.436520  [    8/   88]
per-ex loss: 0.588410  [   10/   88]
per-ex loss: 0.396882  [   12/   88]
per-ex loss: 0.473598  [   14/   88]
per-ex loss: 0.399545  [   16/   88]
per-ex loss: 0.550752  [   18/   88]
per-ex loss: 0.510963  [   20/   88]
per-ex loss: 0.368931  [   22/   88]
per-ex loss: 0.537582  [   24/   88]
per-ex loss: 0.573382  [   26/   88]
per-ex loss: 0.504525  [   28/   88]
per-ex loss: 0.427872  [   30/   88]
per-ex loss: 0.431732  [   32/   88]
per-ex loss: 0.437368  [   34/   88]
per-ex loss: 0.603512  [   36/   88]
per-ex loss: 0.480112  [   38/   88]
per-ex loss: 0.377856  [   40/   88]
per-ex loss: 0.442117  [   42/   88]
per-ex loss: 0.555998  [   44/   88]
per-ex loss: 0.468784  [   46/   88]
per-ex loss: 0.502164  [   48/   88]
per-ex loss: 0.400345  [   50/   88]
per-ex loss: 0.554487  [   52/   88]
per-ex loss: 0.500754  [   54/   88]
per-ex loss: 0.394891  [   56/   88]
per-ex loss: 0.388707  [   58/   88]
per-ex loss: 0.403597  [   60/   88]
per-ex loss: 0.381189  [   62/   88]
per-ex loss: 0.474419  [   64/   88]
per-ex loss: 0.611655  [   66/   88]
per-ex loss: 0.384959  [   68/   88]
per-ex loss: 0.457986  [   70/   88]
per-ex loss: 0.453612  [   72/   88]
per-ex loss: 0.655379  [   74/   88]
per-ex loss: 0.417273  [   76/   88]
per-ex loss: 0.601248  [   78/   88]
per-ex loss: 0.563562  [   80/   88]
per-ex loss: 0.571455  [   82/   88]
per-ex loss: 0.583619  [   84/   88]
per-ex loss: 0.463516  [   86/   88]
per-ex loss: 0.443547  [   88/   88]
Train Error: Avg loss: 0.48132806
validation Error: 
 Avg loss: 0.54626797 
 F1: 0.487397 
 Precision: 0.603629 
 Recall: 0.408699
 IoU: 0.322224

test Error: 
 Avg loss: 0.49522138 
 F1: 0.548941 
 Precision: 0.695247 
 Recall: 0.453507
 IoU: 0.378304

We have finished training iteration 43
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_41_.pth
per-ex loss: 0.426684  [    2/   88]
per-ex loss: 0.425484  [    4/   88]
per-ex loss: 0.450825  [    6/   88]
per-ex loss: 0.595689  [    8/   88]
per-ex loss: 0.572802  [   10/   88]
per-ex loss: 0.582644  [   12/   88]
per-ex loss: 0.499316  [   14/   88]
per-ex loss: 0.446274  [   16/   88]
per-ex loss: 0.457427  [   18/   88]
per-ex loss: 0.538361  [   20/   88]
per-ex loss: 0.538133  [   22/   88]
per-ex loss: 0.448359  [   24/   88]
per-ex loss: 0.455881  [   26/   88]
per-ex loss: 0.493686  [   28/   88]
per-ex loss: 0.583412  [   30/   88]
per-ex loss: 0.369412  [   32/   88]
per-ex loss: 0.391832  [   34/   88]
per-ex loss: 0.677846  [   36/   88]
per-ex loss: 0.576530  [   38/   88]
per-ex loss: 0.428085  [   40/   88]
per-ex loss: 0.682469  [   42/   88]
per-ex loss: 0.413659  [   44/   88]
per-ex loss: 0.577104  [   46/   88]
per-ex loss: 0.489748  [   48/   88]
per-ex loss: 0.348900  [   50/   88]
per-ex loss: 0.357263  [   52/   88]
per-ex loss: 0.423479  [   54/   88]
per-ex loss: 0.418130  [   56/   88]
per-ex loss: 0.456085  [   58/   88]
per-ex loss: 0.449188  [   60/   88]
per-ex loss: 0.635418  [   62/   88]
per-ex loss: 0.592710  [   64/   88]
per-ex loss: 0.491480  [   66/   88]
per-ex loss: 0.442260  [   68/   88]
per-ex loss: 0.396690  [   70/   88]
per-ex loss: 0.366596  [   72/   88]
per-ex loss: 0.532733  [   74/   88]
per-ex loss: 0.554756  [   76/   88]
per-ex loss: 0.487989  [   78/   88]
per-ex loss: 0.382488  [   80/   88]
per-ex loss: 0.425372  [   82/   88]
per-ex loss: 0.430597  [   84/   88]
per-ex loss: 0.388229  [   86/   88]
per-ex loss: 0.564092  [   88/   88]
Train Error: Avg loss: 0.48332083
validation Error: 
 Avg loss: 0.53421829 
 F1: 0.494530 
 Precision: 0.669468 
 Recall: 0.392077
 IoU: 0.328489

test Error: 
 Avg loss: 0.50295224 
 F1: 0.542577 
 Precision: 0.702247 
 Recall: 0.442065
 IoU: 0.372286

We have finished training iteration 44
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_26_.pth
per-ex loss: 0.478432  [    2/   88]
per-ex loss: 0.419344  [    4/   88]
per-ex loss: 0.504151  [    6/   88]
per-ex loss: 0.466899  [    8/   88]
per-ex loss: 0.353302  [   10/   88]
per-ex loss: 0.391951  [   12/   88]
per-ex loss: 0.607360  [   14/   88]
per-ex loss: 0.528604  [   16/   88]
per-ex loss: 0.694967  [   18/   88]
per-ex loss: 0.626725  [   20/   88]
per-ex loss: 0.536042  [   22/   88]
per-ex loss: 0.452622  [   24/   88]
per-ex loss: 0.412394  [   26/   88]
per-ex loss: 0.449785  [   28/   88]
per-ex loss: 0.552117  [   30/   88]
per-ex loss: 0.512225  [   32/   88]
per-ex loss: 0.572633  [   34/   88]
per-ex loss: 0.508399  [   36/   88]
per-ex loss: 0.511060  [   38/   88]
per-ex loss: 0.444563  [   40/   88]
per-ex loss: 0.447729  [   42/   88]
per-ex loss: 0.514827  [   44/   88]
per-ex loss: 0.505576  [   46/   88]
per-ex loss: 0.378290  [   48/   88]
per-ex loss: 0.607886  [   50/   88]
per-ex loss: 0.565715  [   52/   88]
per-ex loss: 0.581190  [   54/   88]
per-ex loss: 0.512069  [   56/   88]
per-ex loss: 0.520847  [   58/   88]
per-ex loss: 0.465738  [   60/   88]
per-ex loss: 0.391416  [   62/   88]
per-ex loss: 0.350297  [   64/   88]
per-ex loss: 0.534233  [   66/   88]
per-ex loss: 0.393880  [   68/   88]
per-ex loss: 0.407390  [   70/   88]
per-ex loss: 0.426682  [   72/   88]
per-ex loss: 0.635235  [   74/   88]
per-ex loss: 0.630289  [   76/   88]
per-ex loss: 0.452250  [   78/   88]
per-ex loss: 0.455778  [   80/   88]
per-ex loss: 0.392116  [   82/   88]
per-ex loss: 0.470604  [   84/   88]
per-ex loss: 0.450248  [   86/   88]
per-ex loss: 0.388939  [   88/   88]
Train Error: Avg loss: 0.48869991
validation Error: 
 Avg loss: 0.54612749 
 F1: 0.481693 
 Precision: 0.537840 
 Recall: 0.436160
 IoU: 0.317257

test Error: 
 Avg loss: 0.49436142 
 F1: 0.547689 
 Precision: 0.634313 
 Recall: 0.481882
 IoU: 0.377116

We have finished training iteration 45
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_43_.pth
per-ex loss: 0.689517  [    2/   88]
per-ex loss: 0.611897  [    4/   88]
per-ex loss: 0.623712  [    6/   88]
per-ex loss: 0.590138  [    8/   88]
per-ex loss: 0.463723  [   10/   88]
per-ex loss: 0.446729  [   12/   88]
per-ex loss: 0.501752  [   14/   88]
per-ex loss: 0.487700  [   16/   88]
per-ex loss: 0.459716  [   18/   88]
per-ex loss: 0.451827  [   20/   88]
per-ex loss: 0.413418  [   22/   88]
per-ex loss: 0.496726  [   24/   88]
per-ex loss: 0.420504  [   26/   88]
per-ex loss: 0.509275  [   28/   88]
per-ex loss: 0.616432  [   30/   88]
per-ex loss: 0.385696  [   32/   88]
per-ex loss: 0.456110  [   34/   88]
per-ex loss: 0.400866  [   36/   88]
per-ex loss: 0.545257  [   38/   88]
per-ex loss: 0.584460  [   40/   88]
per-ex loss: 0.388934  [   42/   88]
per-ex loss: 0.505683  [   44/   88]
per-ex loss: 0.506630  [   46/   88]
per-ex loss: 0.412384  [   48/   88]
per-ex loss: 0.640670  [   50/   88]
per-ex loss: 0.422414  [   52/   88]
per-ex loss: 0.481013  [   54/   88]
per-ex loss: 0.345210  [   56/   88]
per-ex loss: 0.375697  [   58/   88]
per-ex loss: 0.561418  [   60/   88]
per-ex loss: 0.404552  [   62/   88]
per-ex loss: 0.442520  [   64/   88]
per-ex loss: 0.477624  [   66/   88]
per-ex loss: 0.384426  [   68/   88]
per-ex loss: 0.481059  [   70/   88]
per-ex loss: 0.418472  [   72/   88]
per-ex loss: 0.517325  [   74/   88]
per-ex loss: 0.467304  [   76/   88]
per-ex loss: 0.608567  [   78/   88]
per-ex loss: 0.589766  [   80/   88]
per-ex loss: 0.437109  [   82/   88]
per-ex loss: 0.415501  [   84/   88]
per-ex loss: 0.471755  [   86/   88]
per-ex loss: 0.432429  [   88/   88]
Train Error: Avg loss: 0.48508892
validation Error: 
 Avg loss: 0.52376762 
 F1: 0.492625 
 Precision: 0.533049 
 Recall: 0.457899
 IoU: 0.326809

test Error: 
 Avg loss: 0.48499089 
 F1: 0.554572 
 Precision: 0.598388 
 Recall: 0.516735
 IoU: 0.383673

We have finished training iteration 46
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_33_.pth
per-ex loss: 0.383027  [    2/   88]
per-ex loss: 0.507330  [    4/   88]
per-ex loss: 0.611142  [    6/   88]
per-ex loss: 0.668406  [    8/   88]
per-ex loss: 0.550843  [   10/   88]
per-ex loss: 0.432242  [   12/   88]
per-ex loss: 0.486959  [   14/   88]
per-ex loss: 0.428873  [   16/   88]
per-ex loss: 0.510151  [   18/   88]
per-ex loss: 0.428997  [   20/   88]
per-ex loss: 0.563371  [   22/   88]
per-ex loss: 0.468697  [   24/   88]
per-ex loss: 0.481902  [   26/   88]
per-ex loss: 0.359441  [   28/   88]
per-ex loss: 0.627683  [   30/   88]
per-ex loss: 0.469335  [   32/   88]
per-ex loss: 0.396844  [   34/   88]
per-ex loss: 0.458455  [   36/   88]
per-ex loss: 0.444672  [   38/   88]
per-ex loss: 0.422056  [   40/   88]
per-ex loss: 0.509533  [   42/   88]
per-ex loss: 0.514572  [   44/   88]
per-ex loss: 0.434204  [   46/   88]
per-ex loss: 0.540876  [   48/   88]
per-ex loss: 0.615780  [   50/   88]
per-ex loss: 0.378951  [   52/   88]
per-ex loss: 0.393323  [   54/   88]
per-ex loss: 0.490210  [   56/   88]
per-ex loss: 0.501988  [   58/   88]
per-ex loss: 0.400708  [   60/   88]
per-ex loss: 0.612178  [   62/   88]
per-ex loss: 0.371241  [   64/   88]
per-ex loss: 0.504897  [   66/   88]
per-ex loss: 0.476106  [   68/   88]
per-ex loss: 0.527328  [   70/   88]
per-ex loss: 0.473676  [   72/   88]
per-ex loss: 0.392488  [   74/   88]
per-ex loss: 0.404475  [   76/   88]
per-ex loss: 0.468969  [   78/   88]
per-ex loss: 0.421758  [   80/   88]
per-ex loss: 0.469291  [   82/   88]
per-ex loss: 0.443701  [   84/   88]
per-ex loss: 0.621591  [   86/   88]
per-ex loss: 0.468413  [   88/   88]
Train Error: Avg loss: 0.48037915
validation Error: 
 Avg loss: 0.56016627 
 F1: 0.452274 
 Precision: 0.477223 
 Recall: 0.429804
 IoU: 0.292218

test Error: 
 Avg loss: 0.50712733 
 F1: 0.530952 
 Precision: 0.596766 
 Recall: 0.478213
 IoU: 0.361426

We have finished training iteration 47
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_45_.pth
per-ex loss: 0.465585  [    2/   88]
per-ex loss: 0.385594  [    4/   88]
per-ex loss: 0.512359  [    6/   88]
per-ex loss: 0.405589  [    8/   88]
per-ex loss: 0.568713  [   10/   88]
per-ex loss: 0.410849  [   12/   88]
per-ex loss: 0.636946  [   14/   88]
per-ex loss: 0.454861  [   16/   88]
per-ex loss: 0.691433  [   18/   88]
per-ex loss: 0.494106  [   20/   88]
per-ex loss: 0.532971  [   22/   88]
per-ex loss: 0.497408  [   24/   88]
per-ex loss: 0.542477  [   26/   88]
per-ex loss: 0.463913  [   28/   88]
per-ex loss: 0.466857  [   30/   88]
per-ex loss: 0.478185  [   32/   88]
per-ex loss: 0.563001  [   34/   88]
per-ex loss: 0.393561  [   36/   88]
per-ex loss: 0.469747  [   38/   88]
per-ex loss: 0.459746  [   40/   88]
per-ex loss: 0.553324  [   42/   88]
per-ex loss: 0.476115  [   44/   88]
per-ex loss: 0.561115  [   46/   88]
per-ex loss: 0.400812  [   48/   88]
per-ex loss: 0.419371  [   50/   88]
per-ex loss: 0.663878  [   52/   88]
per-ex loss: 0.398953  [   54/   88]
per-ex loss: 0.461836  [   56/   88]
per-ex loss: 0.388820  [   58/   88]
per-ex loss: 0.386872  [   60/   88]
per-ex loss: 0.519536  [   62/   88]
per-ex loss: 0.432887  [   64/   88]
per-ex loss: 0.403557  [   66/   88]
per-ex loss: 0.458216  [   68/   88]
per-ex loss: 0.427520  [   70/   88]
per-ex loss: 0.462248  [   72/   88]
per-ex loss: 0.384989  [   74/   88]
per-ex loss: 0.423675  [   76/   88]
per-ex loss: 0.607218  [   78/   88]
per-ex loss: 0.380149  [   80/   88]
per-ex loss: 0.467574  [   82/   88]
per-ex loss: 0.382189  [   84/   88]
per-ex loss: 0.415919  [   86/   88]
per-ex loss: 0.519510  [   88/   88]
Train Error: Avg loss: 0.47477691
validation Error: 
 Avg loss: 0.55297027 
 F1: 0.471399 
 Precision: 0.681213 
 Recall: 0.360397
 IoU: 0.308386

test Error: 
 Avg loss: 0.53496249 
 F1: 0.505865 
 Precision: 0.745443 
 Recall: 0.382828
 IoU: 0.338567

We have finished training iteration 48
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_44_.pth
per-ex loss: 0.565683  [    2/   88]
per-ex loss: 0.341714  [    4/   88]
per-ex loss: 0.420463  [    6/   88]
per-ex loss: 0.469103  [    8/   88]
per-ex loss: 0.632675  [   10/   88]
per-ex loss: 0.426051  [   12/   88]
per-ex loss: 0.543759  [   14/   88]
per-ex loss: 0.374235  [   16/   88]
per-ex loss: 0.443172  [   18/   88]
per-ex loss: 0.445639  [   20/   88]
per-ex loss: 0.462457  [   22/   88]
per-ex loss: 0.385717  [   24/   88]
per-ex loss: 0.394096  [   26/   88]
per-ex loss: 0.632125  [   28/   88]
per-ex loss: 0.576812  [   30/   88]
per-ex loss: 0.477426  [   32/   88]
per-ex loss: 0.416051  [   34/   88]
per-ex loss: 0.570270  [   36/   88]
per-ex loss: 0.622010  [   38/   88]
per-ex loss: 0.546229  [   40/   88]
per-ex loss: 0.436650  [   42/   88]
per-ex loss: 0.458593  [   44/   88]
per-ex loss: 0.513094  [   46/   88]
per-ex loss: 0.446205  [   48/   88]
per-ex loss: 0.531030  [   50/   88]
per-ex loss: 0.426510  [   52/   88]
per-ex loss: 0.369553  [   54/   88]
per-ex loss: 0.439163  [   56/   88]
per-ex loss: 0.465242  [   58/   88]
per-ex loss: 0.518797  [   60/   88]
per-ex loss: 0.376024  [   62/   88]
per-ex loss: 0.429091  [   64/   88]
per-ex loss: 0.421439  [   66/   88]
per-ex loss: 0.510821  [   68/   88]
per-ex loss: 0.522601  [   70/   88]
per-ex loss: 0.580205  [   72/   88]
per-ex loss: 0.468228  [   74/   88]
per-ex loss: 0.401864  [   76/   88]
per-ex loss: 0.630062  [   78/   88]
per-ex loss: 0.423670  [   80/   88]
per-ex loss: 0.591104  [   82/   88]
per-ex loss: 0.579514  [   84/   88]
per-ex loss: 0.435696  [   86/   88]
per-ex loss: 0.608300  [   88/   88]
Train Error: Avg loss: 0.48475321
validation Error: 
 Avg loss: 0.58631711 
 F1: 0.440086 
 Precision: 0.384654 
 Recall: 0.514186
 IoU: 0.282122

test Error: 
 Avg loss: 0.50504272 
 F1: 0.531527 
 Precision: 0.507395 
 Recall: 0.558068
 IoU: 0.361959

We have finished training iteration 49
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_47_.pth
per-ex loss: 0.422690  [    2/   88]
per-ex loss: 0.391184  [    4/   88]
per-ex loss: 0.411588  [    6/   88]
per-ex loss: 0.432642  [    8/   88]
per-ex loss: 0.475422  [   10/   88]
per-ex loss: 0.514087  [   12/   88]
per-ex loss: 0.400772  [   14/   88]
per-ex loss: 0.523858  [   16/   88]
per-ex loss: 0.372653  [   18/   88]
per-ex loss: 0.456021  [   20/   88]
per-ex loss: 0.517406  [   22/   88]
per-ex loss: 0.486128  [   24/   88]
per-ex loss: 0.403933  [   26/   88]
per-ex loss: 0.572616  [   28/   88]
per-ex loss: 0.577700  [   30/   88]
per-ex loss: 0.448407  [   32/   88]
per-ex loss: 0.468391  [   34/   88]
per-ex loss: 0.426287  [   36/   88]
per-ex loss: 0.601003  [   38/   88]
per-ex loss: 0.394028  [   40/   88]
per-ex loss: 0.409530  [   42/   88]
per-ex loss: 0.450644  [   44/   88]
per-ex loss: 0.400449  [   46/   88]
per-ex loss: 0.412483  [   48/   88]
per-ex loss: 0.559036  [   50/   88]
per-ex loss: 0.571305  [   52/   88]
per-ex loss: 0.406610  [   54/   88]
per-ex loss: 0.431359  [   56/   88]
per-ex loss: 0.414277  [   58/   88]
per-ex loss: 0.578939  [   60/   88]
per-ex loss: 0.541125  [   62/   88]
per-ex loss: 0.617231  [   64/   88]
per-ex loss: 0.477848  [   66/   88]
per-ex loss: 0.459575  [   68/   88]
per-ex loss: 0.678224  [   70/   88]
per-ex loss: 0.487230  [   72/   88]
per-ex loss: 0.545523  [   74/   88]
per-ex loss: 0.446886  [   76/   88]
per-ex loss: 0.476130  [   78/   88]
per-ex loss: 0.473662  [   80/   88]
per-ex loss: 0.450410  [   82/   88]
per-ex loss: 0.670392  [   84/   88]
per-ex loss: 0.369348  [   86/   88]
per-ex loss: 0.622193  [   88/   88]
Train Error: Avg loss: 0.48289148
validation Error: 
 Avg loss: 0.53237756 
 F1: 0.492817 
 Precision: 0.630416 
 Recall: 0.404523
 IoU: 0.326979

test Error: 
 Avg loss: 0.49652684 
 F1: 0.545496 
 Precision: 0.694543 
 Recall: 0.449117
 IoU: 0.375039

We have finished training iteration 50
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_48_.pth
per-ex loss: 0.362060  [    2/   88]
per-ex loss: 0.466898  [    4/   88]
per-ex loss: 0.439684  [    6/   88]
per-ex loss: 0.368493  [    8/   88]
per-ex loss: 0.515367  [   10/   88]
per-ex loss: 0.409427  [   12/   88]
per-ex loss: 0.373797  [   14/   88]
per-ex loss: 0.379022  [   16/   88]
per-ex loss: 0.397830  [   18/   88]
per-ex loss: 0.447482  [   20/   88]
per-ex loss: 0.430927  [   22/   88]
per-ex loss: 0.429945  [   24/   88]
per-ex loss: 0.348523  [   26/   88]
per-ex loss: 0.405552  [   28/   88]
per-ex loss: 0.413141  [   30/   88]
per-ex loss: 0.599688  [   32/   88]
per-ex loss: 0.435776  [   34/   88]
per-ex loss: 0.625824  [   36/   88]
per-ex loss: 0.487275  [   38/   88]
per-ex loss: 0.453495  [   40/   88]
per-ex loss: 0.524363  [   42/   88]
per-ex loss: 0.442880  [   44/   88]
per-ex loss: 0.537021  [   46/   88]
per-ex loss: 0.579559  [   48/   88]
per-ex loss: 0.488118  [   50/   88]
per-ex loss: 0.375445  [   52/   88]
per-ex loss: 0.529973  [   54/   88]
per-ex loss: 0.475624  [   56/   88]
per-ex loss: 0.475911  [   58/   88]
per-ex loss: 0.456037  [   60/   88]
per-ex loss: 0.540566  [   62/   88]
per-ex loss: 0.462776  [   64/   88]
per-ex loss: 0.595929  [   66/   88]
per-ex loss: 0.530686  [   68/   88]
per-ex loss: 0.572079  [   70/   88]
per-ex loss: 0.700437  [   72/   88]
per-ex loss: 0.381624  [   74/   88]
per-ex loss: 0.448037  [   76/   88]
per-ex loss: 0.502599  [   78/   88]
per-ex loss: 0.565894  [   80/   88]
per-ex loss: 0.474308  [   82/   88]
per-ex loss: 0.438497  [   84/   88]
per-ex loss: 0.507062  [   86/   88]
per-ex loss: 0.597239  [   88/   88]
Train Error: Avg loss: 0.47711074
validation Error: 
 Avg loss: 0.53663075 
 F1: 0.495716 
 Precision: 0.601616 
 Recall: 0.421518
 IoU: 0.329536

test Error: 
 Avg loss: 0.48628150 
 F1: 0.553321 
 Precision: 0.665486 
 Recall: 0.473512
 IoU: 0.382477

We have finished training iteration 51
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_49_.pth
per-ex loss: 0.438774  [    2/   88]
per-ex loss: 0.426595  [    4/   88]
per-ex loss: 0.394939  [    6/   88]
per-ex loss: 0.438484  [    8/   88]
per-ex loss: 0.539387  [   10/   88]
per-ex loss: 0.416885  [   12/   88]
per-ex loss: 0.455606  [   14/   88]
per-ex loss: 0.381184  [   16/   88]
per-ex loss: 0.399054  [   18/   88]
per-ex loss: 0.431569  [   20/   88]
per-ex loss: 0.485199  [   22/   88]
per-ex loss: 0.390658  [   24/   88]
per-ex loss: 0.580123  [   26/   88]
per-ex loss: 0.426890  [   28/   88]
per-ex loss: 0.391922  [   30/   88]
per-ex loss: 0.483482  [   32/   88]
per-ex loss: 0.516234  [   34/   88]
per-ex loss: 0.609140  [   36/   88]
per-ex loss: 0.403976  [   38/   88]
per-ex loss: 0.495634  [   40/   88]
per-ex loss: 0.485167  [   42/   88]
per-ex loss: 0.386263  [   44/   88]
per-ex loss: 0.372543  [   46/   88]
per-ex loss: 0.583982  [   48/   88]
per-ex loss: 0.427121  [   50/   88]
per-ex loss: 0.415667  [   52/   88]
per-ex loss: 0.436488  [   54/   88]
per-ex loss: 0.504042  [   56/   88]
per-ex loss: 0.425274  [   58/   88]
per-ex loss: 0.514838  [   60/   88]
per-ex loss: 0.383488  [   62/   88]
per-ex loss: 0.614162  [   64/   88]
per-ex loss: 0.482277  [   66/   88]
per-ex loss: 0.596924  [   68/   88]
per-ex loss: 0.394367  [   70/   88]
per-ex loss: 0.544596  [   72/   88]
per-ex loss: 0.394649  [   74/   88]
per-ex loss: 0.574781  [   76/   88]
per-ex loss: 0.557648  [   78/   88]
per-ex loss: 0.562781  [   80/   88]
per-ex loss: 0.611345  [   82/   88]
per-ex loss: 0.452178  [   84/   88]
per-ex loss: 0.548751  [   86/   88]
per-ex loss: 0.602014  [   88/   88]
Train Error: Avg loss: 0.47675192
validation Error: 
 Avg loss: 0.52422105 
 F1: 0.501699 
 Precision: 0.545558 
 Recall: 0.464367
 IoU: 0.334845

test Error: 
 Avg loss: 0.47712277 
 F1: 0.563170 
 Precision: 0.613292 
 Recall: 0.520621
 IoU: 0.391953

We have finished training iteration 52
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_50_.pth
per-ex loss: 0.593980  [    2/   88]
per-ex loss: 0.400702  [    4/   88]
per-ex loss: 0.466877  [    6/   88]
per-ex loss: 0.436518  [    8/   88]
per-ex loss: 0.423079  [   10/   88]
per-ex loss: 0.463246  [   12/   88]
per-ex loss: 0.471743  [   14/   88]
per-ex loss: 0.510515  [   16/   88]
per-ex loss: 0.433463  [   18/   88]
per-ex loss: 0.409558  [   20/   88]
per-ex loss: 0.509567  [   22/   88]
per-ex loss: 0.481796  [   24/   88]
per-ex loss: 0.608588  [   26/   88]
per-ex loss: 0.488003  [   28/   88]
per-ex loss: 0.484684  [   30/   88]
per-ex loss: 0.509485  [   32/   88]
per-ex loss: 0.535934  [   34/   88]
per-ex loss: 0.440166  [   36/   88]
per-ex loss: 0.457827  [   38/   88]
per-ex loss: 0.489347  [   40/   88]
per-ex loss: 0.568325  [   42/   88]
per-ex loss: 0.551482  [   44/   88]
per-ex loss: 0.411287  [   46/   88]
per-ex loss: 0.365451  [   48/   88]
per-ex loss: 0.426336  [   50/   88]
per-ex loss: 0.461677  [   52/   88]
per-ex loss: 0.408917  [   54/   88]
per-ex loss: 0.445603  [   56/   88]
per-ex loss: 0.424484  [   58/   88]
per-ex loss: 0.417173  [   60/   88]
per-ex loss: 0.651472  [   62/   88]
per-ex loss: 0.508814  [   64/   88]
per-ex loss: 0.469158  [   66/   88]
per-ex loss: 0.555513  [   68/   88]
per-ex loss: 0.647507  [   70/   88]
per-ex loss: 0.506566  [   72/   88]
per-ex loss: 0.511003  [   74/   88]
per-ex loss: 0.398572  [   76/   88]
per-ex loss: 0.397784  [   78/   88]
per-ex loss: 0.696180  [   80/   88]
per-ex loss: 0.428994  [   82/   88]
per-ex loss: 0.466714  [   84/   88]
per-ex loss: 0.514617  [   86/   88]
per-ex loss: 0.370155  [   88/   88]
Train Error: Avg loss: 0.48224693
validation Error: 
 Avg loss: 0.52093033 
 F1: 0.490550 
 Precision: 0.588015 
 Recall: 0.420801
 IoU: 0.324986

test Error: 
 Avg loss: 0.48956405 
 F1: 0.550756 
 Precision: 0.683045 
 Recall: 0.461395
 IoU: 0.380030

We have finished training iteration 53
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_51_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.462530  [    2/   88]
per-ex loss: 0.482231  [    4/   88]
per-ex loss: 0.358974  [    6/   88]
per-ex loss: 0.453801  [    8/   88]
per-ex loss: 0.474150  [   10/   88]
per-ex loss: 0.442349  [   12/   88]
per-ex loss: 0.515372  [   14/   88]
per-ex loss: 0.421598  [   16/   88]
per-ex loss: 0.389308  [   18/   88]
per-ex loss: 0.362471  [   20/   88]
per-ex loss: 0.407854  [   22/   88]
per-ex loss: 0.462971  [   24/   88]
per-ex loss: 0.446456  [   26/   88]
per-ex loss: 0.393780  [   28/   88]
per-ex loss: 0.399293  [   30/   88]
per-ex loss: 0.463022  [   32/   88]
per-ex loss: 0.425354  [   34/   88]
per-ex loss: 0.433681  [   36/   88]
per-ex loss: 0.541213  [   38/   88]
per-ex loss: 0.525599  [   40/   88]
per-ex loss: 0.562239  [   42/   88]
per-ex loss: 0.627994  [   44/   88]
per-ex loss: 0.527158  [   46/   88]
per-ex loss: 0.601115  [   48/   88]
per-ex loss: 0.450241  [   50/   88]
per-ex loss: 0.398720  [   52/   88]
per-ex loss: 0.619291  [   54/   88]
per-ex loss: 0.361400  [   56/   88]
per-ex loss: 0.395939  [   58/   88]
per-ex loss: 0.387675  [   60/   88]
per-ex loss: 0.540516  [   62/   88]
per-ex loss: 0.459161  [   64/   88]
per-ex loss: 0.417598  [   66/   88]
per-ex loss: 0.592242  [   68/   88]
per-ex loss: 0.423908  [   70/   88]
per-ex loss: 0.494178  [   72/   88]
per-ex loss: 0.433352  [   74/   88]
per-ex loss: 0.423256  [   76/   88]
per-ex loss: 0.621273  [   78/   88]
per-ex loss: 0.458963  [   80/   88]
per-ex loss: 0.409832  [   82/   88]
per-ex loss: 0.582407  [   84/   88]
per-ex loss: 0.436801  [   86/   88]
per-ex loss: 0.413875  [   88/   88]
Train Error: Avg loss: 0.46593506
validation Error: 
 Avg loss: 0.52125024 
 F1: 0.506896 
 Precision: 0.579267 
 Recall: 0.450600
 IoU: 0.339492

test Error: 
 Avg loss: 0.48074450 
 F1: 0.560106 
 Precision: 0.628315 
 Recall: 0.505256
 IoU: 0.388991

We have finished training iteration 54
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_42_.pth
per-ex loss: 0.488134  [    2/   88]
per-ex loss: 0.570490  [    4/   88]
per-ex loss: 0.603106  [    6/   88]
per-ex loss: 0.597966  [    8/   88]
per-ex loss: 0.631660  [   10/   88]
per-ex loss: 0.529596  [   12/   88]
per-ex loss: 0.547545  [   14/   88]
per-ex loss: 0.421562  [   16/   88]
per-ex loss: 0.387863  [   18/   88]
per-ex loss: 0.408459  [   20/   88]
per-ex loss: 0.500361  [   22/   88]
per-ex loss: 0.361921  [   24/   88]
per-ex loss: 0.377350  [   26/   88]
per-ex loss: 0.483379  [   28/   88]
per-ex loss: 0.456833  [   30/   88]
per-ex loss: 0.517500  [   32/   88]
per-ex loss: 0.433626  [   34/   88]
per-ex loss: 0.339805  [   36/   88]
per-ex loss: 0.433818  [   38/   88]
per-ex loss: 0.391271  [   40/   88]
per-ex loss: 0.342722  [   42/   88]
per-ex loss: 0.414634  [   44/   88]
per-ex loss: 0.577634  [   46/   88]
per-ex loss: 0.431195  [   48/   88]
per-ex loss: 0.413986  [   50/   88]
per-ex loss: 0.479249  [   52/   88]
per-ex loss: 0.441312  [   54/   88]
per-ex loss: 0.455423  [   56/   88]
per-ex loss: 0.409736  [   58/   88]
per-ex loss: 0.490976  [   60/   88]
per-ex loss: 0.454590  [   62/   88]
per-ex loss: 0.474721  [   64/   88]
per-ex loss: 0.656591  [   66/   88]
per-ex loss: 0.393974  [   68/   88]
per-ex loss: 0.449056  [   70/   88]
per-ex loss: 0.539332  [   72/   88]
per-ex loss: 0.398702  [   74/   88]
per-ex loss: 0.401290  [   76/   88]
per-ex loss: 0.503660  [   78/   88]
per-ex loss: 0.496782  [   80/   88]
per-ex loss: 0.637439  [   82/   88]
per-ex loss: 0.579511  [   84/   88]
per-ex loss: 0.457599  [   86/   88]
per-ex loss: 0.562825  [   88/   88]
Train Error: Avg loss: 0.47602693
validation Error: 
 Avg loss: 0.53223828 
 F1: 0.498522 
 Precision: 0.578149 
 Recall: 0.438174
 IoU: 0.332021

test Error: 
 Avg loss: 0.49041250 
 F1: 0.549780 
 Precision: 0.638344 
 Recall: 0.482797
 IoU: 0.379101

We have finished training iteration 55
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_52_.pth
per-ex loss: 0.496298  [    2/   88]
per-ex loss: 0.471490  [    4/   88]
per-ex loss: 0.485514  [    6/   88]
per-ex loss: 0.560966  [    8/   88]
per-ex loss: 0.396728  [   10/   88]
per-ex loss: 0.365548  [   12/   88]
per-ex loss: 0.568172  [   14/   88]
per-ex loss: 0.450021  [   16/   88]
per-ex loss: 0.415506  [   18/   88]
per-ex loss: 0.488706  [   20/   88]
per-ex loss: 0.392644  [   22/   88]
per-ex loss: 0.438982  [   24/   88]
per-ex loss: 0.505582  [   26/   88]
per-ex loss: 0.552464  [   28/   88]
per-ex loss: 0.488083  [   30/   88]
per-ex loss: 0.362050  [   32/   88]
per-ex loss: 0.436214  [   34/   88]
per-ex loss: 0.527505  [   36/   88]
per-ex loss: 0.598389  [   38/   88]
per-ex loss: 0.434198  [   40/   88]
per-ex loss: 0.556009  [   42/   88]
per-ex loss: 0.362744  [   44/   88]
per-ex loss: 0.435951  [   46/   88]
per-ex loss: 0.381475  [   48/   88]
per-ex loss: 0.570244  [   50/   88]
per-ex loss: 0.511011  [   52/   88]
per-ex loss: 0.538424  [   54/   88]
per-ex loss: 0.473890  [   56/   88]
per-ex loss: 0.434401  [   58/   88]
per-ex loss: 0.456174  [   60/   88]
per-ex loss: 0.360454  [   62/   88]
per-ex loss: 0.451094  [   64/   88]
per-ex loss: 0.587203  [   66/   88]
per-ex loss: 0.557732  [   68/   88]
per-ex loss: 0.488671  [   70/   88]
per-ex loss: 0.356406  [   72/   88]
per-ex loss: 0.437747  [   74/   88]
per-ex loss: 0.457545  [   76/   88]
per-ex loss: 0.434592  [   78/   88]
per-ex loss: 0.518229  [   80/   88]
per-ex loss: 0.578576  [   82/   88]
per-ex loss: 0.433622  [   84/   88]
per-ex loss: 0.396372  [   86/   88]
per-ex loss: 0.499269  [   88/   88]
Train Error: Avg loss: 0.47074762
validation Error: 
 Avg loss: 0.52038643 
 F1: 0.496854 
 Precision: 0.614574 
 Recall: 0.416981
 IoU: 0.330542

test Error: 
 Avg loss: 0.49311726 
 F1: 0.547386 
 Precision: 0.682336 
 Recall: 0.457001
 IoU: 0.376828

We have finished training iteration 56
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_46_.pth
per-ex loss: 0.385252  [    2/   88]
per-ex loss: 0.364021  [    4/   88]
per-ex loss: 0.463449  [    6/   88]
per-ex loss: 0.556564  [    8/   88]
per-ex loss: 0.378507  [   10/   88]
per-ex loss: 0.476862  [   12/   88]
per-ex loss: 0.470461  [   14/   88]
per-ex loss: 0.490531  [   16/   88]
per-ex loss: 0.569571  [   18/   88]
per-ex loss: 0.448471  [   20/   88]
per-ex loss: 0.422345  [   22/   88]
per-ex loss: 0.576912  [   24/   88]
per-ex loss: 0.447429  [   26/   88]
per-ex loss: 0.465839  [   28/   88]
per-ex loss: 0.402173  [   30/   88]
per-ex loss: 0.465243  [   32/   88]
per-ex loss: 0.386362  [   34/   88]
per-ex loss: 0.593210  [   36/   88]
per-ex loss: 0.474763  [   38/   88]
per-ex loss: 0.516289  [   40/   88]
per-ex loss: 0.508042  [   42/   88]
per-ex loss: 0.438424  [   44/   88]
per-ex loss: 0.495616  [   46/   88]
per-ex loss: 0.471011  [   48/   88]
per-ex loss: 0.556249  [   50/   88]
per-ex loss: 0.387132  [   52/   88]
per-ex loss: 0.449684  [   54/   88]
per-ex loss: 0.375086  [   56/   88]
per-ex loss: 0.359970  [   58/   88]
per-ex loss: 0.394837  [   60/   88]
per-ex loss: 0.450661  [   62/   88]
per-ex loss: 0.592375  [   64/   88]
per-ex loss: 0.560057  [   66/   88]
per-ex loss: 0.395969  [   68/   88]
per-ex loss: 0.375199  [   70/   88]
per-ex loss: 0.578398  [   72/   88]
per-ex loss: 0.497406  [   74/   88]
per-ex loss: 0.567414  [   76/   88]
per-ex loss: 0.509000  [   78/   88]
per-ex loss: 0.396806  [   80/   88]
per-ex loss: 0.443525  [   82/   88]
per-ex loss: 0.478143  [   84/   88]
per-ex loss: 0.415641  [   86/   88]
per-ex loss: 0.530363  [   88/   88]
Train Error: Avg loss: 0.46775588
validation Error: 
 Avg loss: 0.52702528 
 F1: 0.503006 
 Precision: 0.558145 
 Recall: 0.457783
 IoU: 0.336011

test Error: 
 Avg loss: 0.47923465 
 F1: 0.560089 
 Precision: 0.623405 
 Recall: 0.508449
 IoU: 0.388975

We have finished training iteration 57
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_55_.pth
per-ex loss: 0.497486  [    2/   88]
per-ex loss: 0.367109  [    4/   88]
per-ex loss: 0.383782  [    6/   88]
per-ex loss: 0.446024  [    8/   88]
per-ex loss: 0.615872  [   10/   88]
per-ex loss: 0.504298  [   12/   88]
per-ex loss: 0.393174  [   14/   88]
per-ex loss: 0.405912  [   16/   88]
per-ex loss: 0.473745  [   18/   88]
per-ex loss: 0.362028  [   20/   88]
per-ex loss: 0.354281  [   22/   88]
per-ex loss: 0.359626  [   24/   88]
per-ex loss: 0.380427  [   26/   88]
per-ex loss: 0.374020  [   28/   88]
per-ex loss: 0.610830  [   30/   88]
per-ex loss: 0.578727  [   32/   88]
per-ex loss: 0.474846  [   34/   88]
per-ex loss: 0.461638  [   36/   88]
per-ex loss: 0.411444  [   38/   88]
per-ex loss: 0.477373  [   40/   88]
per-ex loss: 0.535413  [   42/   88]
per-ex loss: 0.558596  [   44/   88]
per-ex loss: 0.429713  [   46/   88]
per-ex loss: 0.525417  [   48/   88]
per-ex loss: 0.418289  [   50/   88]
per-ex loss: 0.649359  [   52/   88]
per-ex loss: 0.604988  [   54/   88]
per-ex loss: 0.527953  [   56/   88]
per-ex loss: 0.438229  [   58/   88]
per-ex loss: 0.658086  [   60/   88]
per-ex loss: 0.408751  [   62/   88]
per-ex loss: 0.464564  [   64/   88]
per-ex loss: 0.477003  [   66/   88]
per-ex loss: 0.561236  [   68/   88]
per-ex loss: 0.459600  [   70/   88]
per-ex loss: 0.633558  [   72/   88]
per-ex loss: 0.564096  [   74/   88]
per-ex loss: 0.601834  [   76/   88]
per-ex loss: 0.438181  [   78/   88]
per-ex loss: 0.409418  [   80/   88]
per-ex loss: 0.403403  [   82/   88]
per-ex loss: 0.482390  [   84/   88]
per-ex loss: 0.544747  [   86/   88]
per-ex loss: 0.533099  [   88/   88]
Train Error: Avg loss: 0.48319460
validation Error: 
 Avg loss: 0.55364935 
 F1: 0.480532 
 Precision: 0.647465 
 Recall: 0.382034
 IoU: 0.316250

test Error: 
 Avg loss: 0.50826696 
 F1: 0.532133 
 Precision: 0.711848 
 Recall: 0.424870
 IoU: 0.362521

We have finished training iteration 58
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_54_.pth
per-ex loss: 0.448489  [    2/   88]
per-ex loss: 0.410500  [    4/   88]
per-ex loss: 0.572667  [    6/   88]
per-ex loss: 0.452989  [    8/   88]
per-ex loss: 0.410200  [   10/   88]
per-ex loss: 0.473738  [   12/   88]
per-ex loss: 0.399291  [   14/   88]
per-ex loss: 0.451553  [   16/   88]
per-ex loss: 0.530638  [   18/   88]
per-ex loss: 0.406596  [   20/   88]
per-ex loss: 0.602914  [   22/   88]
per-ex loss: 0.386593  [   24/   88]
per-ex loss: 0.452325  [   26/   88]
per-ex loss: 0.439586  [   28/   88]
per-ex loss: 0.390911  [   30/   88]
per-ex loss: 0.518223  [   32/   88]
per-ex loss: 0.455089  [   34/   88]
per-ex loss: 0.574158  [   36/   88]
per-ex loss: 0.396548  [   38/   88]
per-ex loss: 0.388793  [   40/   88]
per-ex loss: 0.485816  [   42/   88]
per-ex loss: 0.597093  [   44/   88]
per-ex loss: 0.499895  [   46/   88]
per-ex loss: 0.403765  [   48/   88]
per-ex loss: 0.701657  [   50/   88]
per-ex loss: 0.452692  [   52/   88]
per-ex loss: 0.516803  [   54/   88]
per-ex loss: 0.352432  [   56/   88]
per-ex loss: 0.455768  [   58/   88]
per-ex loss: 0.415305  [   60/   88]
per-ex loss: 0.371241  [   62/   88]
per-ex loss: 0.504874  [   64/   88]
per-ex loss: 0.567530  [   66/   88]
per-ex loss: 0.439777  [   68/   88]
per-ex loss: 0.412799  [   70/   88]
per-ex loss: 0.611618  [   72/   88]
per-ex loss: 0.413666  [   74/   88]
per-ex loss: 0.447335  [   76/   88]
per-ex loss: 0.530852  [   78/   88]
per-ex loss: 0.594168  [   80/   88]
per-ex loss: 0.542972  [   82/   88]
per-ex loss: 0.420690  [   84/   88]
per-ex loss: 0.594862  [   86/   88]
per-ex loss: 0.442531  [   88/   88]
Train Error: Avg loss: 0.47586232
validation Error: 
 Avg loss: 0.53525567 
 F1: 0.494086 
 Precision: 0.481932 
 Recall: 0.506870
 IoU: 0.328098

test Error: 
 Avg loss: 0.48627448 
 F1: 0.548454 
 Precision: 0.539041 
 Recall: 0.558201
 IoU: 0.377841

We have finished training iteration 59
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_57_.pth
per-ex loss: 0.507723  [    2/   88]
per-ex loss: 0.429673  [    4/   88]
per-ex loss: 0.456521  [    6/   88]
per-ex loss: 0.545029  [    8/   88]
per-ex loss: 0.608341  [   10/   88]
per-ex loss: 0.441601  [   12/   88]
per-ex loss: 0.413020  [   14/   88]
per-ex loss: 0.624658  [   16/   88]
per-ex loss: 0.459139  [   18/   88]
per-ex loss: 0.424560  [   20/   88]
per-ex loss: 0.472288  [   22/   88]
per-ex loss: 0.596403  [   24/   88]
per-ex loss: 0.614450  [   26/   88]
per-ex loss: 0.623697  [   28/   88]
per-ex loss: 0.405501  [   30/   88]
per-ex loss: 0.430641  [   32/   88]
per-ex loss: 0.360894  [   34/   88]
per-ex loss: 0.457021  [   36/   88]
per-ex loss: 0.428998  [   38/   88]
per-ex loss: 0.457170  [   40/   88]
per-ex loss: 0.625335  [   42/   88]
per-ex loss: 0.424936  [   44/   88]
per-ex loss: 0.343908  [   46/   88]
per-ex loss: 0.442012  [   48/   88]
per-ex loss: 0.465720  [   50/   88]
per-ex loss: 0.375290  [   52/   88]
per-ex loss: 0.603720  [   54/   88]
per-ex loss: 0.627490  [   56/   88]
per-ex loss: 0.364320  [   58/   88]
per-ex loss: 0.582517  [   60/   88]
per-ex loss: 0.418076  [   62/   88]
per-ex loss: 0.385284  [   64/   88]
per-ex loss: 0.489692  [   66/   88]
per-ex loss: 0.540330  [   68/   88]
per-ex loss: 0.357551  [   70/   88]
per-ex loss: 0.459213  [   72/   88]
per-ex loss: 0.416026  [   74/   88]
per-ex loss: 0.521783  [   76/   88]
per-ex loss: 0.389976  [   78/   88]
per-ex loss: 0.614859  [   80/   88]
per-ex loss: 0.484094  [   82/   88]
per-ex loss: 0.369244  [   84/   88]
per-ex loss: 0.401040  [   86/   88]
per-ex loss: 0.468492  [   88/   88]
Train Error: Avg loss: 0.47564174
validation Error: 
 Avg loss: 0.53523629 
 F1: 0.484338 
 Precision: 0.620462 
 Recall: 0.397196
 IoU: 0.319556

test Error: 
 Avg loss: 0.50069126 
 F1: 0.538412 
 Precision: 0.701005 
 Recall: 0.437044
 IoU: 0.368375

We have finished training iteration 60
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_58_.pth
per-ex loss: 0.426260  [    2/   88]
per-ex loss: 0.453294  [    4/   88]
per-ex loss: 0.521280  [    6/   88]
per-ex loss: 0.372495  [    8/   88]
per-ex loss: 0.640722  [   10/   88]
per-ex loss: 0.524876  [   12/   88]
per-ex loss: 0.485337  [   14/   88]
per-ex loss: 0.402722  [   16/   88]
per-ex loss: 0.392838  [   18/   88]
per-ex loss: 0.463747  [   20/   88]
per-ex loss: 0.571947  [   22/   88]
per-ex loss: 0.468951  [   24/   88]
per-ex loss: 0.365399  [   26/   88]
per-ex loss: 0.400616  [   28/   88]
per-ex loss: 0.416096  [   30/   88]
per-ex loss: 0.408743  [   32/   88]
per-ex loss: 0.456545  [   34/   88]
per-ex loss: 0.342518  [   36/   88]
per-ex loss: 0.481740  [   38/   88]
per-ex loss: 0.501571  [   40/   88]
per-ex loss: 0.567564  [   42/   88]
per-ex loss: 0.393999  [   44/   88]
per-ex loss: 0.363783  [   46/   88]
per-ex loss: 0.369064  [   48/   88]
per-ex loss: 0.422386  [   50/   88]
per-ex loss: 0.494367  [   52/   88]
per-ex loss: 0.422233  [   54/   88]
per-ex loss: 0.414913  [   56/   88]
per-ex loss: 0.532016  [   58/   88]
per-ex loss: 0.525118  [   60/   88]
per-ex loss: 0.583666  [   62/   88]
per-ex loss: 0.391816  [   64/   88]
per-ex loss: 0.462687  [   66/   88]
per-ex loss: 0.553410  [   68/   88]
per-ex loss: 0.420483  [   70/   88]
per-ex loss: 0.506500  [   72/   88]
per-ex loss: 0.441160  [   74/   88]
per-ex loss: 0.567617  [   76/   88]
per-ex loss: 0.404676  [   78/   88]
per-ex loss: 0.563934  [   80/   88]
per-ex loss: 0.488281  [   82/   88]
per-ex loss: 0.612751  [   84/   88]
per-ex loss: 0.575492  [   86/   88]
per-ex loss: 0.465984  [   88/   88]
Train Error: Avg loss: 0.46912720
validation Error: 
 Avg loss: 0.52706813 
 F1: 0.497330 
 Precision: 0.543940 
 Recall: 0.458078
 IoU: 0.330964

test Error: 
 Avg loss: 0.47972070 
 F1: 0.563597 
 Precision: 0.612081 
 Recall: 0.522231
 IoU: 0.392367

We have finished training iteration 61
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_59_.pth
per-ex loss: 0.396659  [    2/   88]
per-ex loss: 0.667531  [    4/   88]
per-ex loss: 0.425423  [    6/   88]
per-ex loss: 0.452973  [    8/   88]
per-ex loss: 0.551321  [   10/   88]
per-ex loss: 0.340716  [   12/   88]
per-ex loss: 0.430088  [   14/   88]
per-ex loss: 0.639602  [   16/   88]
per-ex loss: 0.406938  [   18/   88]
per-ex loss: 0.437695  [   20/   88]
per-ex loss: 0.452235  [   22/   88]
per-ex loss: 0.525932  [   24/   88]
per-ex loss: 0.656762  [   26/   88]
per-ex loss: 0.364403  [   28/   88]
per-ex loss: 0.583641  [   30/   88]
per-ex loss: 0.351626  [   32/   88]
per-ex loss: 0.509107  [   34/   88]
per-ex loss: 0.567224  [   36/   88]
per-ex loss: 0.365631  [   38/   88]
per-ex loss: 0.563582  [   40/   88]
per-ex loss: 0.414042  [   42/   88]
per-ex loss: 0.396563  [   44/   88]
per-ex loss: 0.493770  [   46/   88]
per-ex loss: 0.438023  [   48/   88]
per-ex loss: 0.662010  [   50/   88]
per-ex loss: 0.434058  [   52/   88]
per-ex loss: 0.500606  [   54/   88]
per-ex loss: 0.586999  [   56/   88]
per-ex loss: 0.513580  [   58/   88]
per-ex loss: 0.614716  [   60/   88]
per-ex loss: 0.567842  [   62/   88]
per-ex loss: 0.488055  [   64/   88]
per-ex loss: 0.411068  [   66/   88]
per-ex loss: 0.428931  [   68/   88]
per-ex loss: 0.469317  [   70/   88]
per-ex loss: 0.529524  [   72/   88]
per-ex loss: 0.419493  [   74/   88]
per-ex loss: 0.423407  [   76/   88]
per-ex loss: 0.414448  [   78/   88]
per-ex loss: 0.440529  [   80/   88]
per-ex loss: 0.548771  [   82/   88]
per-ex loss: 0.428254  [   84/   88]
per-ex loss: 0.371455  [   86/   88]
per-ex loss: 0.453042  [   88/   88]
Train Error: Avg loss: 0.48039983
validation Error: 
 Avg loss: 0.54366879 
 F1: 0.477631 
 Precision: 0.648144 
 Recall: 0.378148
 IoU: 0.313742

test Error: 
 Avg loss: 0.51144254 
 F1: 0.525778 
 Precision: 0.701884 
 Recall: 0.420318
 IoU: 0.356647

We have finished training iteration 62
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_60_.pth
per-ex loss: 0.511801  [    2/   88]
per-ex loss: 0.586173  [    4/   88]
per-ex loss: 0.455243  [    6/   88]
per-ex loss: 0.434360  [    8/   88]
per-ex loss: 0.461006  [   10/   88]
per-ex loss: 0.408332  [   12/   88]
per-ex loss: 0.420511  [   14/   88]
per-ex loss: 0.625023  [   16/   88]
per-ex loss: 0.541226  [   18/   88]
per-ex loss: 0.450946  [   20/   88]
per-ex loss: 0.471129  [   22/   88]
per-ex loss: 0.546431  [   24/   88]
per-ex loss: 0.420309  [   26/   88]
per-ex loss: 0.422471  [   28/   88]
per-ex loss: 0.412658  [   30/   88]
per-ex loss: 0.457752  [   32/   88]
per-ex loss: 0.413016  [   34/   88]
per-ex loss: 0.556422  [   36/   88]
per-ex loss: 0.499607  [   38/   88]
per-ex loss: 0.603033  [   40/   88]
per-ex loss: 0.417739  [   42/   88]
per-ex loss: 0.414543  [   44/   88]
per-ex loss: 0.501044  [   46/   88]
per-ex loss: 0.427461  [   48/   88]
per-ex loss: 0.384343  [   50/   88]
per-ex loss: 0.432811  [   52/   88]
per-ex loss: 0.529980  [   54/   88]
per-ex loss: 0.385606  [   56/   88]
per-ex loss: 0.420596  [   58/   88]
per-ex loss: 0.595700  [   60/   88]
per-ex loss: 0.468311  [   62/   88]
per-ex loss: 0.460356  [   64/   88]
per-ex loss: 0.528952  [   66/   88]
per-ex loss: 0.467110  [   68/   88]
per-ex loss: 0.461307  [   70/   88]
per-ex loss: 0.330437  [   72/   88]
per-ex loss: 0.475521  [   74/   88]
per-ex loss: 0.394776  [   76/   88]
per-ex loss: 0.379844  [   78/   88]
per-ex loss: 0.389581  [   80/   88]
per-ex loss: 0.408566  [   82/   88]
per-ex loss: 0.380493  [   84/   88]
per-ex loss: 0.593590  [   86/   88]
per-ex loss: 0.444765  [   88/   88]
Train Error: Avg loss: 0.46342901
validation Error: 
 Avg loss: 0.51900662 
 F1: 0.497223 
 Precision: 0.590930 
 Recall: 0.429167
 IoU: 0.330869

test Error: 
 Avg loss: 0.49001954 
 F1: 0.550125 
 Precision: 0.650980 
 Recall: 0.476328
 IoU: 0.379429

We have finished training iteration 63
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_61_.pth
per-ex loss: 0.358756  [    2/   88]
per-ex loss: 0.446073  [    4/   88]
per-ex loss: 0.446089  [    6/   88]
per-ex loss: 0.379825  [    8/   88]
per-ex loss: 0.603460  [   10/   88]
per-ex loss: 0.406078  [   12/   88]
per-ex loss: 0.593264  [   14/   88]
per-ex loss: 0.361834  [   16/   88]
per-ex loss: 0.402786  [   18/   88]
per-ex loss: 0.569567  [   20/   88]
per-ex loss: 0.445929  [   22/   88]
per-ex loss: 0.448749  [   24/   88]
per-ex loss: 0.477824  [   26/   88]
per-ex loss: 0.569509  [   28/   88]
per-ex loss: 0.554564  [   30/   88]
per-ex loss: 0.382183  [   32/   88]
per-ex loss: 0.392804  [   34/   88]
per-ex loss: 0.654773  [   36/   88]
per-ex loss: 0.412303  [   38/   88]
per-ex loss: 0.518364  [   40/   88]
per-ex loss: 0.486215  [   42/   88]
per-ex loss: 0.351638  [   44/   88]
per-ex loss: 0.558739  [   46/   88]
per-ex loss: 0.425072  [   48/   88]
per-ex loss: 0.473054  [   50/   88]
per-ex loss: 0.376703  [   52/   88]
per-ex loss: 0.485360  [   54/   88]
per-ex loss: 0.510921  [   56/   88]
per-ex loss: 0.422984  [   58/   88]
per-ex loss: 0.490252  [   60/   88]
per-ex loss: 0.531461  [   62/   88]
per-ex loss: 0.398543  [   64/   88]
per-ex loss: 0.359407  [   66/   88]
per-ex loss: 0.542954  [   68/   88]
per-ex loss: 0.521935  [   70/   88]
per-ex loss: 0.349069  [   72/   88]
per-ex loss: 0.687573  [   74/   88]
per-ex loss: 0.381863  [   76/   88]
per-ex loss: 0.518365  [   78/   88]
per-ex loss: 0.482620  [   80/   88]
per-ex loss: 0.429373  [   82/   88]
per-ex loss: 0.477571  [   84/   88]
per-ex loss: 0.510707  [   86/   88]
per-ex loss: 0.428089  [   88/   88]
Train Error: Avg loss: 0.46875462
validation Error: 
 Avg loss: 0.52691312 
 F1: 0.497410 
 Precision: 0.557689 
 Recall: 0.448891
 IoU: 0.331035

test Error: 
 Avg loss: 0.47373623 
 F1: 0.565684 
 Precision: 0.649104 
 Recall: 0.501264
 IoU: 0.394393

We have finished training iteration 64
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_62_.pth
per-ex loss: 0.425188  [    2/   88]
per-ex loss: 0.447430  [    4/   88]
per-ex loss: 0.412727  [    6/   88]
per-ex loss: 0.380999  [    8/   88]
per-ex loss: 0.447741  [   10/   88]
per-ex loss: 0.450707  [   12/   88]
per-ex loss: 0.598371  [   14/   88]
per-ex loss: 0.412118  [   16/   88]
per-ex loss: 0.463496  [   18/   88]
per-ex loss: 0.402475  [   20/   88]
per-ex loss: 0.596424  [   22/   88]
per-ex loss: 0.463001  [   24/   88]
per-ex loss: 0.533800  [   26/   88]
per-ex loss: 0.480723  [   28/   88]
per-ex loss: 0.409556  [   30/   88]
per-ex loss: 0.376846  [   32/   88]
per-ex loss: 0.528749  [   34/   88]
per-ex loss: 0.409596  [   36/   88]
per-ex loss: 0.415071  [   38/   88]
per-ex loss: 0.569013  [   40/   88]
per-ex loss: 0.451877  [   42/   88]
per-ex loss: 0.508112  [   44/   88]
per-ex loss: 0.454422  [   46/   88]
per-ex loss: 0.465214  [   48/   88]
per-ex loss: 0.534188  [   50/   88]
per-ex loss: 0.468770  [   52/   88]
per-ex loss: 0.386180  [   54/   88]
per-ex loss: 0.598313  [   56/   88]
per-ex loss: 0.387641  [   58/   88]
per-ex loss: 0.363997  [   60/   88]
per-ex loss: 0.468067  [   62/   88]
per-ex loss: 0.566325  [   64/   88]
per-ex loss: 0.446650  [   66/   88]
per-ex loss: 0.386862  [   68/   88]
per-ex loss: 0.648429  [   70/   88]
per-ex loss: 0.425523  [   72/   88]
per-ex loss: 0.645007  [   74/   88]
per-ex loss: 0.627423  [   76/   88]
per-ex loss: 0.451315  [   78/   88]
per-ex loss: 0.514313  [   80/   88]
per-ex loss: 0.441887  [   82/   88]
per-ex loss: 0.415884  [   84/   88]
per-ex loss: 0.431234  [   86/   88]
per-ex loss: 0.502708  [   88/   88]
Train Error: Avg loss: 0.47305383
validation Error: 
 Avg loss: 0.52337995 
 F1: 0.502523 
 Precision: 0.566531 
 Recall: 0.451510
 IoU: 0.335580

test Error: 
 Avg loss: 0.47785138 
 F1: 0.565112 
 Precision: 0.631860 
 Recall: 0.511119
 IoU: 0.393837

We have finished training iteration 65
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_53_.pth
per-ex loss: 0.402448  [    2/   88]
per-ex loss: 0.470736  [    4/   88]
per-ex loss: 0.529925  [    6/   88]
per-ex loss: 0.522882  [    8/   88]
per-ex loss: 0.588371  [   10/   88]
per-ex loss: 0.381582  [   12/   88]
per-ex loss: 0.620135  [   14/   88]
per-ex loss: 0.440971  [   16/   88]
per-ex loss: 0.392582  [   18/   88]
per-ex loss: 0.411344  [   20/   88]
per-ex loss: 0.421134  [   22/   88]
per-ex loss: 0.419589  [   24/   88]
per-ex loss: 0.601739  [   26/   88]
per-ex loss: 0.515676  [   28/   88]
per-ex loss: 0.595634  [   30/   88]
per-ex loss: 0.582422  [   32/   88]
per-ex loss: 0.428581  [   34/   88]
per-ex loss: 0.416500  [   36/   88]
per-ex loss: 0.410371  [   38/   88]
per-ex loss: 0.421612  [   40/   88]
per-ex loss: 0.496504  [   42/   88]
per-ex loss: 0.394863  [   44/   88]
per-ex loss: 0.640105  [   46/   88]
per-ex loss: 0.424651  [   48/   88]
per-ex loss: 0.600375  [   50/   88]
per-ex loss: 0.612915  [   52/   88]
per-ex loss: 0.529479  [   54/   88]
per-ex loss: 0.399418  [   56/   88]
per-ex loss: 0.363659  [   58/   88]
per-ex loss: 0.394545  [   60/   88]
per-ex loss: 0.359833  [   62/   88]
per-ex loss: 0.454618  [   64/   88]
per-ex loss: 0.418139  [   66/   88]
per-ex loss: 0.503212  [   68/   88]
per-ex loss: 0.417036  [   70/   88]
per-ex loss: 0.463752  [   72/   88]
per-ex loss: 0.525985  [   74/   88]
per-ex loss: 0.514291  [   76/   88]
per-ex loss: 0.413306  [   78/   88]
per-ex loss: 0.468553  [   80/   88]
per-ex loss: 0.552895  [   82/   88]
per-ex loss: 0.411549  [   84/   88]
per-ex loss: 0.365704  [   86/   88]
per-ex loss: 0.472307  [   88/   88]
Train Error: Avg loss: 0.47208925
validation Error: 
 Avg loss: 0.54652755 
 F1: 0.495504 
 Precision: 0.573744 
 Recall: 0.436041
 IoU: 0.329349

test Error: 
 Avg loss: 0.48919178 
 F1: 0.545521 
 Precision: 0.645519 
 Recall: 0.472349
 IoU: 0.375063

We have finished training iteration 66
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_64_.pth
per-ex loss: 0.572911  [    2/   88]
per-ex loss: 0.404866  [    4/   88]
per-ex loss: 0.466130  [    6/   88]
per-ex loss: 0.528663  [    8/   88]
per-ex loss: 0.521210  [   10/   88]
per-ex loss: 0.386223  [   12/   88]
per-ex loss: 0.560037  [   14/   88]
per-ex loss: 0.515599  [   16/   88]
per-ex loss: 0.400331  [   18/   88]
per-ex loss: 0.423257  [   20/   88]
per-ex loss: 0.420366  [   22/   88]
per-ex loss: 0.564565  [   24/   88]
per-ex loss: 0.698775  [   26/   88]
per-ex loss: 0.459323  [   28/   88]
per-ex loss: 0.391327  [   30/   88]
per-ex loss: 0.649506  [   32/   88]
per-ex loss: 0.452564  [   34/   88]
per-ex loss: 0.467725  [   36/   88]
per-ex loss: 0.448802  [   38/   88]
per-ex loss: 0.513653  [   40/   88]
per-ex loss: 0.382148  [   42/   88]
per-ex loss: 0.437604  [   44/   88]
per-ex loss: 0.661511  [   46/   88]
per-ex loss: 0.401920  [   48/   88]
per-ex loss: 0.438552  [   50/   88]
per-ex loss: 0.410344  [   52/   88]
per-ex loss: 0.403487  [   54/   88]
per-ex loss: 0.403841  [   56/   88]
per-ex loss: 0.454257  [   58/   88]
per-ex loss: 0.577573  [   60/   88]
per-ex loss: 0.419034  [   62/   88]
per-ex loss: 0.393057  [   64/   88]
per-ex loss: 0.682506  [   66/   88]
per-ex loss: 0.427510  [   68/   88]
per-ex loss: 0.377502  [   70/   88]
per-ex loss: 0.462945  [   72/   88]
per-ex loss: 0.389347  [   74/   88]
per-ex loss: 0.397294  [   76/   88]
per-ex loss: 0.373274  [   78/   88]
per-ex loss: 0.377851  [   80/   88]
per-ex loss: 0.412819  [   82/   88]
per-ex loss: 0.387001  [   84/   88]
per-ex loss: 0.457275  [   86/   88]
per-ex loss: 0.421799  [   88/   88]
Train Error: Avg loss: 0.46355185
validation Error: 
 Avg loss: 0.55834352 
 F1: 0.474114 
 Precision: 0.720711 
 Recall: 0.353247
 IoU: 0.310714

test Error: 
 Avg loss: 0.53524103 
 F1: 0.503867 
 Precision: 0.773814 
 Recall: 0.373553
 IoU: 0.336780

We have finished training iteration 67
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_65_.pth
per-ex loss: 0.358856  [    2/   88]
per-ex loss: 0.399803  [    4/   88]
per-ex loss: 0.398676  [    6/   88]
per-ex loss: 0.333600  [    8/   88]
per-ex loss: 0.578321  [   10/   88]
per-ex loss: 0.681576  [   12/   88]
per-ex loss: 0.451123  [   14/   88]
per-ex loss: 0.489669  [   16/   88]
per-ex loss: 0.541578  [   18/   88]
per-ex loss: 0.453208  [   20/   88]
per-ex loss: 0.394534  [   22/   88]
per-ex loss: 0.389396  [   24/   88]
per-ex loss: 0.380347  [   26/   88]
per-ex loss: 0.457922  [   28/   88]
per-ex loss: 0.473887  [   30/   88]
per-ex loss: 0.558321  [   32/   88]
per-ex loss: 0.583565  [   34/   88]
per-ex loss: 0.403385  [   36/   88]
per-ex loss: 0.378853  [   38/   88]
per-ex loss: 0.382199  [   40/   88]
per-ex loss: 0.386973  [   42/   88]
per-ex loss: 0.387704  [   44/   88]
per-ex loss: 0.482206  [   46/   88]
per-ex loss: 0.429574  [   48/   88]
per-ex loss: 0.474299  [   50/   88]
per-ex loss: 0.476339  [   52/   88]
per-ex loss: 0.585104  [   54/   88]
per-ex loss: 0.525408  [   56/   88]
per-ex loss: 0.580998  [   58/   88]
per-ex loss: 0.348408  [   60/   88]
per-ex loss: 0.443081  [   62/   88]
per-ex loss: 0.396264  [   64/   88]
per-ex loss: 0.521301  [   66/   88]
per-ex loss: 0.487248  [   68/   88]
per-ex loss: 0.510391  [   70/   88]
per-ex loss: 0.590009  [   72/   88]
per-ex loss: 0.567060  [   74/   88]
per-ex loss: 0.630787  [   76/   88]
per-ex loss: 0.410915  [   78/   88]
per-ex loss: 0.450436  [   80/   88]
per-ex loss: 0.432333  [   82/   88]
per-ex loss: 0.421083  [   84/   88]
per-ex loss: 0.427615  [   86/   88]
per-ex loss: 0.459584  [   88/   88]
Train Error: Avg loss: 0.46622585
validation Error: 
 Avg loss: 0.52339922 
 F1: 0.498976 
 Precision: 0.540796 
 Recall: 0.463159
 IoU: 0.332423

test Error: 
 Avg loss: 0.48584019 
 F1: 0.558966 
 Precision: 0.597736 
 Recall: 0.524918
 IoU: 0.387892

We have finished training iteration 68
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_66_.pth
per-ex loss: 0.425553  [    2/   88]
per-ex loss: 0.544305  [    4/   88]
per-ex loss: 0.448316  [    6/   88]
per-ex loss: 0.526185  [    8/   88]
per-ex loss: 0.403324  [   10/   88]
per-ex loss: 0.469700  [   12/   88]
per-ex loss: 0.380004  [   14/   88]
per-ex loss: 0.381220  [   16/   88]
per-ex loss: 0.426287  [   18/   88]
per-ex loss: 0.534475  [   20/   88]
per-ex loss: 0.374307  [   22/   88]
per-ex loss: 0.565214  [   24/   88]
per-ex loss: 0.639177  [   26/   88]
per-ex loss: 0.358479  [   28/   88]
per-ex loss: 0.595854  [   30/   88]
per-ex loss: 0.500577  [   32/   88]
per-ex loss: 0.399557  [   34/   88]
per-ex loss: 0.371086  [   36/   88]
per-ex loss: 0.461915  [   38/   88]
per-ex loss: 0.408811  [   40/   88]
per-ex loss: 0.420321  [   42/   88]
per-ex loss: 0.614480  [   44/   88]
per-ex loss: 0.580719  [   46/   88]
per-ex loss: 0.428606  [   48/   88]
per-ex loss: 0.447895  [   50/   88]
per-ex loss: 0.484205  [   52/   88]
per-ex loss: 0.404879  [   54/   88]
per-ex loss: 0.371894  [   56/   88]
per-ex loss: 0.368953  [   58/   88]
per-ex loss: 0.489961  [   60/   88]
per-ex loss: 0.438060  [   62/   88]
per-ex loss: 0.422287  [   64/   88]
per-ex loss: 0.450775  [   66/   88]
per-ex loss: 0.568749  [   68/   88]
per-ex loss: 0.665408  [   70/   88]
per-ex loss: 0.440353  [   72/   88]
per-ex loss: 0.474312  [   74/   88]
per-ex loss: 0.567934  [   76/   88]
per-ex loss: 0.435625  [   78/   88]
per-ex loss: 0.359099  [   80/   88]
per-ex loss: 0.448869  [   82/   88]
per-ex loss: 0.422827  [   84/   88]
per-ex loss: 0.597006  [   86/   88]
per-ex loss: 0.426005  [   88/   88]
Train Error: Avg loss: 0.46689929
validation Error: 
 Avg loss: 0.52624309 
 F1: 0.506492 
 Precision: 0.624695 
 Recall: 0.425903
 IoU: 0.339129

test Error: 
 Avg loss: 0.48354775 
 F1: 0.553593 
 Precision: 0.650188 
 Recall: 0.481986
 IoU: 0.382736

We have finished training iteration 69
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_67_.pth
per-ex loss: 0.391609  [    2/   88]
per-ex loss: 0.405984  [    4/   88]
per-ex loss: 0.437449  [    6/   88]
per-ex loss: 0.367400  [    8/   88]
per-ex loss: 0.382079  [   10/   88]
per-ex loss: 0.578342  [   12/   88]
per-ex loss: 0.436784  [   14/   88]
per-ex loss: 0.357564  [   16/   88]
per-ex loss: 0.396432  [   18/   88]
per-ex loss: 0.524167  [   20/   88]
per-ex loss: 0.456433  [   22/   88]
per-ex loss: 0.522689  [   24/   88]
per-ex loss: 0.520208  [   26/   88]
per-ex loss: 0.654742  [   28/   88]
per-ex loss: 0.460323  [   30/   88]
per-ex loss: 0.457225  [   32/   88]
per-ex loss: 0.640824  [   34/   88]
per-ex loss: 0.370636  [   36/   88]
per-ex loss: 0.389081  [   38/   88]
per-ex loss: 0.384489  [   40/   88]
per-ex loss: 0.394850  [   42/   88]
per-ex loss: 0.414794  [   44/   88]
per-ex loss: 0.448904  [   46/   88]
per-ex loss: 0.455802  [   48/   88]
per-ex loss: 0.530205  [   50/   88]
per-ex loss: 0.403348  [   52/   88]
per-ex loss: 0.401189  [   54/   88]
per-ex loss: 0.594619  [   56/   88]
per-ex loss: 0.364929  [   58/   88]
per-ex loss: 0.343607  [   60/   88]
per-ex loss: 0.462757  [   62/   88]
per-ex loss: 0.378093  [   64/   88]
per-ex loss: 0.415753  [   66/   88]
per-ex loss: 0.432090  [   68/   88]
per-ex loss: 0.496490  [   70/   88]
per-ex loss: 0.535966  [   72/   88]
per-ex loss: 0.424494  [   74/   88]
per-ex loss: 0.486799  [   76/   88]
per-ex loss: 0.499871  [   78/   88]
per-ex loss: 0.590658  [   80/   88]
per-ex loss: 0.519113  [   82/   88]
per-ex loss: 0.410119  [   84/   88]
per-ex loss: 0.501199  [   86/   88]
per-ex loss: 0.489454  [   88/   88]
Train Error: Avg loss: 0.45749005
validation Error: 
 Avg loss: 0.54228824 
 F1: 0.470451 
 Precision: 0.428811 
 Recall: 0.521048
 IoU: 0.307575

test Error: 
 Avg loss: 0.48592921 
 F1: 0.547793 
 Precision: 0.507911 
 Recall: 0.594471
 IoU: 0.377214

We have finished training iteration 70
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_68_.pth
per-ex loss: 0.447107  [    2/   88]
per-ex loss: 0.487194  [    4/   88]
per-ex loss: 0.600576  [    6/   88]
per-ex loss: 0.485068  [    8/   88]
per-ex loss: 0.636830  [   10/   88]
per-ex loss: 0.605809  [   12/   88]
per-ex loss: 0.431011  [   14/   88]
per-ex loss: 0.398447  [   16/   88]
per-ex loss: 0.399802  [   18/   88]
per-ex loss: 0.374381  [   20/   88]
per-ex loss: 0.608999  [   22/   88]
per-ex loss: 0.613272  [   24/   88]
per-ex loss: 0.355240  [   26/   88]
per-ex loss: 0.605998  [   28/   88]
per-ex loss: 0.448856  [   30/   88]
per-ex loss: 0.361389  [   32/   88]
per-ex loss: 0.349488  [   34/   88]
per-ex loss: 0.482456  [   36/   88]
per-ex loss: 0.492983  [   38/   88]
per-ex loss: 0.427876  [   40/   88]
per-ex loss: 0.500425  [   42/   88]
per-ex loss: 0.498633  [   44/   88]
per-ex loss: 0.663180  [   46/   88]
per-ex loss: 0.484159  [   48/   88]
per-ex loss: 0.335621  [   50/   88]
per-ex loss: 0.625404  [   52/   88]
per-ex loss: 0.418999  [   54/   88]
per-ex loss: 0.403877  [   56/   88]
per-ex loss: 0.455700  [   58/   88]
per-ex loss: 0.422846  [   60/   88]
per-ex loss: 0.433281  [   62/   88]
per-ex loss: 0.478173  [   64/   88]
per-ex loss: 0.457990  [   66/   88]
per-ex loss: 0.622427  [   68/   88]
per-ex loss: 0.343506  [   70/   88]
per-ex loss: 0.423625  [   72/   88]
per-ex loss: 0.487291  [   74/   88]
per-ex loss: 0.518954  [   76/   88]
per-ex loss: 0.396646  [   78/   88]
per-ex loss: 0.439964  [   80/   88]
per-ex loss: 0.361471  [   82/   88]
per-ex loss: 0.475144  [   84/   88]
per-ex loss: 0.554401  [   86/   88]
per-ex loss: 0.418382  [   88/   88]
Train Error: Avg loss: 0.47347453
validation Error: 
 Avg loss: 0.52846580 
 F1: 0.492773 
 Precision: 0.521095 
 Recall: 0.467370
 IoU: 0.326940

test Error: 
 Avg loss: 0.48516487 
 F1: 0.552796 
 Precision: 0.594391 
 Recall: 0.516641
 IoU: 0.381975

We have finished training iteration 71
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_69_.pth
per-ex loss: 0.419521  [    2/   88]
per-ex loss: 0.426750  [    4/   88]
per-ex loss: 0.405786  [    6/   88]
per-ex loss: 0.611798  [    8/   88]
per-ex loss: 0.461841  [   10/   88]
per-ex loss: 0.354300  [   12/   88]
per-ex loss: 0.457885  [   14/   88]
per-ex loss: 0.424788  [   16/   88]
per-ex loss: 0.560119  [   18/   88]
per-ex loss: 0.445822  [   20/   88]
per-ex loss: 0.520052  [   22/   88]
per-ex loss: 0.360760  [   24/   88]
per-ex loss: 0.618781  [   26/   88]
per-ex loss: 0.543729  [   28/   88]
per-ex loss: 0.564537  [   30/   88]
per-ex loss: 0.425131  [   32/   88]
per-ex loss: 0.350377  [   34/   88]
per-ex loss: 0.524535  [   36/   88]
per-ex loss: 0.397570  [   38/   88]
per-ex loss: 0.469991  [   40/   88]
per-ex loss: 0.614044  [   42/   88]
per-ex loss: 0.445102  [   44/   88]
per-ex loss: 0.420926  [   46/   88]
per-ex loss: 0.374753  [   48/   88]
per-ex loss: 0.400677  [   50/   88]
per-ex loss: 0.459047  [   52/   88]
per-ex loss: 0.394778  [   54/   88]
per-ex loss: 0.385424  [   56/   88]
per-ex loss: 0.613141  [   58/   88]
per-ex loss: 0.548288  [   60/   88]
per-ex loss: 0.385031  [   62/   88]
per-ex loss: 0.442873  [   64/   88]
per-ex loss: 0.607444  [   66/   88]
per-ex loss: 0.631806  [   68/   88]
per-ex loss: 0.547731  [   70/   88]
per-ex loss: 0.618558  [   72/   88]
per-ex loss: 0.395392  [   74/   88]
per-ex loss: 0.468244  [   76/   88]
per-ex loss: 0.470393  [   78/   88]
per-ex loss: 0.547173  [   80/   88]
per-ex loss: 0.433518  [   82/   88]
per-ex loss: 0.364808  [   84/   88]
per-ex loss: 0.379072  [   86/   88]
per-ex loss: 0.615206  [   88/   88]
Train Error: Avg loss: 0.47517043
validation Error: 
 Avg loss: 0.50843742 
 F1: 0.505497 
 Precision: 0.547402 
 Recall: 0.469552
 IoU: 0.338237

test Error: 
 Avg loss: 0.47854825 
 F1: 0.556685 
 Precision: 0.609248 
 Recall: 0.512471
 IoU: 0.385698

We have finished training iteration 72
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_70_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.587927  [    2/   88]
per-ex loss: 0.363059  [    4/   88]
per-ex loss: 0.383199  [    6/   88]
per-ex loss: 0.479326  [    8/   88]
per-ex loss: 0.584555  [   10/   88]
per-ex loss: 0.475398  [   12/   88]
per-ex loss: 0.473757  [   14/   88]
per-ex loss: 0.419866  [   16/   88]
per-ex loss: 0.388905  [   18/   88]
per-ex loss: 0.452705  [   20/   88]
per-ex loss: 0.458141  [   22/   88]
per-ex loss: 0.564548  [   24/   88]
per-ex loss: 0.373930  [   26/   88]
per-ex loss: 0.368614  [   28/   88]
per-ex loss: 0.439542  [   30/   88]
per-ex loss: 0.640093  [   32/   88]
per-ex loss: 0.359339  [   34/   88]
per-ex loss: 0.489550  [   36/   88]
per-ex loss: 0.475279  [   38/   88]
per-ex loss: 0.488908  [   40/   88]
per-ex loss: 0.417629  [   42/   88]
per-ex loss: 0.557835  [   44/   88]
per-ex loss: 0.585759  [   46/   88]
per-ex loss: 0.487566  [   48/   88]
per-ex loss: 0.477391  [   50/   88]
per-ex loss: 0.362467  [   52/   88]
per-ex loss: 0.388192  [   54/   88]
per-ex loss: 0.537797  [   56/   88]
per-ex loss: 0.500830  [   58/   88]
per-ex loss: 0.453428  [   60/   88]
per-ex loss: 0.455216  [   62/   88]
per-ex loss: 0.418433  [   64/   88]
per-ex loss: 0.470534  [   66/   88]
per-ex loss: 0.418095  [   68/   88]
per-ex loss: 0.393072  [   70/   88]
per-ex loss: 0.523310  [   72/   88]
per-ex loss: 0.449720  [   74/   88]
per-ex loss: 0.387033  [   76/   88]
per-ex loss: 0.366467  [   78/   88]
per-ex loss: 0.544882  [   80/   88]
per-ex loss: 0.534514  [   82/   88]
per-ex loss: 0.503477  [   84/   88]
per-ex loss: 0.459427  [   86/   88]
per-ex loss: 0.453427  [   88/   88]
Train Error: Avg loss: 0.46393511
validation Error: 
 Avg loss: 0.53719128 
 F1: 0.490550 
 Precision: 0.590466 
 Recall: 0.419556
 IoU: 0.324986

test Error: 
 Avg loss: 0.50605128 
 F1: 0.530623 
 Precision: 0.637763 
 Recall: 0.454304
 IoU: 0.361121

We have finished training iteration 73
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_71_.pth
per-ex loss: 0.418325  [    2/   88]
per-ex loss: 0.396386  [    4/   88]
per-ex loss: 0.516053  [    6/   88]
per-ex loss: 0.607889  [    8/   88]
per-ex loss: 0.410928  [   10/   88]
per-ex loss: 0.419605  [   12/   88]
per-ex loss: 0.486763  [   14/   88]
per-ex loss: 0.439083  [   16/   88]
per-ex loss: 0.536147  [   18/   88]
per-ex loss: 0.345804  [   20/   88]
per-ex loss: 0.352269  [   22/   88]
per-ex loss: 0.476284  [   24/   88]
per-ex loss: 0.500275  [   26/   88]
per-ex loss: 0.660363  [   28/   88]
per-ex loss: 0.463236  [   30/   88]
per-ex loss: 0.417093  [   32/   88]
per-ex loss: 0.405044  [   34/   88]
per-ex loss: 0.432171  [   36/   88]
per-ex loss: 0.483963  [   38/   88]
per-ex loss: 0.431378  [   40/   88]
per-ex loss: 0.422838  [   42/   88]
per-ex loss: 0.393564  [   44/   88]
per-ex loss: 0.394323  [   46/   88]
per-ex loss: 0.446618  [   48/   88]
per-ex loss: 0.510361  [   50/   88]
per-ex loss: 0.568134  [   52/   88]
per-ex loss: 0.471212  [   54/   88]
per-ex loss: 0.364468  [   56/   88]
per-ex loss: 0.480889  [   58/   88]
per-ex loss: 0.430492  [   60/   88]
per-ex loss: 0.362269  [   62/   88]
per-ex loss: 0.502566  [   64/   88]
per-ex loss: 0.580558  [   66/   88]
per-ex loss: 0.671175  [   68/   88]
per-ex loss: 0.611156  [   70/   88]
per-ex loss: 0.432909  [   72/   88]
per-ex loss: 0.464627  [   74/   88]
per-ex loss: 0.462479  [   76/   88]
per-ex loss: 0.624799  [   78/   88]
per-ex loss: 0.601785  [   80/   88]
per-ex loss: 0.408503  [   82/   88]
per-ex loss: 0.442628  [   84/   88]
per-ex loss: 0.375558  [   86/   88]
per-ex loss: 0.356712  [   88/   88]
Train Error: Avg loss: 0.46772005
validation Error: 
 Avg loss: 0.52975748 
 F1: 0.503672 
 Precision: 0.600546 
 Recall: 0.433711
 IoU: 0.336606

test Error: 
 Avg loss: 0.49170981 
 F1: 0.546670 
 Precision: 0.647888 
 Recall: 0.472805
 IoU: 0.376150

We have finished training iteration 74
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_56_.pth
per-ex loss: 0.463404  [    2/   88]
per-ex loss: 0.392617  [    4/   88]
per-ex loss: 0.412117  [    6/   88]
per-ex loss: 0.600292  [    8/   88]
per-ex loss: 0.389989  [   10/   88]
per-ex loss: 0.588208  [   12/   88]
per-ex loss: 0.495615  [   14/   88]
per-ex loss: 0.326636  [   16/   88]
per-ex loss: 0.669373  [   18/   88]
per-ex loss: 0.605016  [   20/   88]
per-ex loss: 0.447423  [   22/   88]
per-ex loss: 0.446430  [   24/   88]
per-ex loss: 0.513106  [   26/   88]
per-ex loss: 0.455072  [   28/   88]
per-ex loss: 0.393960  [   30/   88]
per-ex loss: 0.587503  [   32/   88]
per-ex loss: 0.369533  [   34/   88]
per-ex loss: 0.483406  [   36/   88]
per-ex loss: 0.389537  [   38/   88]
per-ex loss: 0.389468  [   40/   88]
per-ex loss: 0.447399  [   42/   88]
per-ex loss: 0.347379  [   44/   88]
per-ex loss: 0.379747  [   46/   88]
per-ex loss: 0.483691  [   48/   88]
per-ex loss: 0.601355  [   50/   88]
per-ex loss: 0.587793  [   52/   88]
per-ex loss: 0.440657  [   54/   88]
per-ex loss: 0.572992  [   56/   88]
per-ex loss: 0.379356  [   58/   88]
per-ex loss: 0.382328  [   60/   88]
per-ex loss: 0.446438  [   62/   88]
per-ex loss: 0.436719  [   64/   88]
per-ex loss: 0.361398  [   66/   88]
per-ex loss: 0.495952  [   68/   88]
per-ex loss: 0.607255  [   70/   88]
per-ex loss: 0.548817  [   72/   88]
per-ex loss: 0.363803  [   74/   88]
per-ex loss: 0.445817  [   76/   88]
per-ex loss: 0.398912  [   78/   88]
per-ex loss: 0.465661  [   80/   88]
per-ex loss: 0.387287  [   82/   88]
per-ex loss: 0.588773  [   84/   88]
per-ex loss: 0.385232  [   86/   88]
per-ex loss: 0.478660  [   88/   88]
Train Error: Avg loss: 0.46482100
validation Error: 
 Avg loss: 0.52288121 
 F1: 0.492776 
 Precision: 0.590447 
 Recall: 0.422832
 IoU: 0.326943

test Error: 
 Avg loss: 0.49408471 
 F1: 0.545891 
 Precision: 0.668853 
 Recall: 0.461118
 IoU: 0.375412

We have finished training iteration 75
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_73_.pth
per-ex loss: 0.514523  [    2/   88]
per-ex loss: 0.418016  [    4/   88]
per-ex loss: 0.389517  [    6/   88]
per-ex loss: 0.423781  [    8/   88]
per-ex loss: 0.467567  [   10/   88]
per-ex loss: 0.325150  [   12/   88]
per-ex loss: 0.424510  [   14/   88]
per-ex loss: 0.587838  [   16/   88]
per-ex loss: 0.491661  [   18/   88]
per-ex loss: 0.449745  [   20/   88]
per-ex loss: 0.343747  [   22/   88]
per-ex loss: 0.358703  [   24/   88]
per-ex loss: 0.603926  [   26/   88]
per-ex loss: 0.429329  [   28/   88]
per-ex loss: 0.369028  [   30/   88]
per-ex loss: 0.380971  [   32/   88]
per-ex loss: 0.519659  [   34/   88]
per-ex loss: 0.580110  [   36/   88]
per-ex loss: 0.451778  [   38/   88]
per-ex loss: 0.487790  [   40/   88]
per-ex loss: 0.615994  [   42/   88]
per-ex loss: 0.529392  [   44/   88]
per-ex loss: 0.401802  [   46/   88]
per-ex loss: 0.714514  [   48/   88]
per-ex loss: 0.441767  [   50/   88]
per-ex loss: 0.395627  [   52/   88]
per-ex loss: 0.465786  [   54/   88]
per-ex loss: 0.446741  [   56/   88]
per-ex loss: 0.399971  [   58/   88]
per-ex loss: 0.532367  [   60/   88]
per-ex loss: 0.501995  [   62/   88]
per-ex loss: 0.519885  [   64/   88]
per-ex loss: 0.384944  [   66/   88]
per-ex loss: 0.404887  [   68/   88]
per-ex loss: 0.490222  [   70/   88]
per-ex loss: 0.464981  [   72/   88]
per-ex loss: 0.467075  [   74/   88]
per-ex loss: 0.449226  [   76/   88]
per-ex loss: 0.557169  [   78/   88]
per-ex loss: 0.469550  [   80/   88]
per-ex loss: 0.342086  [   82/   88]
per-ex loss: 0.358471  [   84/   88]
per-ex loss: 0.413325  [   86/   88]
per-ex loss: 0.513238  [   88/   88]
Train Error: Avg loss: 0.46132643
validation Error: 
 Avg loss: 0.51606633 
 F1: 0.503842 
 Precision: 0.622754 
 Recall: 0.423060
 IoU: 0.336757

test Error: 
 Avg loss: 0.48740774 
 F1: 0.549367 
 Precision: 0.661324 
 Recall: 0.469829
 IoU: 0.378709

We have finished training iteration 76
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_74_.pth
per-ex loss: 0.457784  [    2/   88]
per-ex loss: 0.454371  [    4/   88]
per-ex loss: 0.500976  [    6/   88]
per-ex loss: 0.452018  [    8/   88]
per-ex loss: 0.506242  [   10/   88]
per-ex loss: 0.345913  [   12/   88]
per-ex loss: 0.425726  [   14/   88]
per-ex loss: 0.410675  [   16/   88]
per-ex loss: 0.341196  [   18/   88]
per-ex loss: 0.577864  [   20/   88]
per-ex loss: 0.355870  [   22/   88]
per-ex loss: 0.489731  [   24/   88]
per-ex loss: 0.557371  [   26/   88]
per-ex loss: 0.551321  [   28/   88]
per-ex loss: 0.419718  [   30/   88]
per-ex loss: 0.370851  [   32/   88]
per-ex loss: 0.433781  [   34/   88]
per-ex loss: 0.413337  [   36/   88]
per-ex loss: 0.394869  [   38/   88]
per-ex loss: 0.342419  [   40/   88]
per-ex loss: 0.600827  [   42/   88]
per-ex loss: 0.542515  [   44/   88]
per-ex loss: 0.410744  [   46/   88]
per-ex loss: 0.487319  [   48/   88]
per-ex loss: 0.478151  [   50/   88]
per-ex loss: 0.506646  [   52/   88]
per-ex loss: 0.455222  [   54/   88]
per-ex loss: 0.413836  [   56/   88]
per-ex loss: 0.600750  [   58/   88]
per-ex loss: 0.457544  [   60/   88]
per-ex loss: 0.417367  [   62/   88]
per-ex loss: 0.428472  [   64/   88]
per-ex loss: 0.454014  [   66/   88]
per-ex loss: 0.582170  [   68/   88]
per-ex loss: 0.473630  [   70/   88]
per-ex loss: 0.599962  [   72/   88]
per-ex loss: 0.406349  [   74/   88]
per-ex loss: 0.502988  [   76/   88]
per-ex loss: 0.463857  [   78/   88]
per-ex loss: 0.402166  [   80/   88]
per-ex loss: 0.589237  [   82/   88]
per-ex loss: 0.483585  [   84/   88]
per-ex loss: 0.390025  [   86/   88]
per-ex loss: 0.380288  [   88/   88]
Train Error: Avg loss: 0.46203849
validation Error: 
 Avg loss: 0.53053402 
 F1: 0.481160 
 Precision: 0.497713 
 Recall: 0.465673
 IoU: 0.316795

test Error: 
 Avg loss: 0.48063504 
 F1: 0.555741 
 Precision: 0.597845 
 Recall: 0.519178
 IoU: 0.384794

We have finished training iteration 77
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_75_.pth
per-ex loss: 0.400988  [    2/   88]
per-ex loss: 0.596574  [    4/   88]
per-ex loss: 0.385185  [    6/   88]
per-ex loss: 0.414351  [    8/   88]
per-ex loss: 0.472609  [   10/   88]
per-ex loss: 0.337626  [   12/   88]
per-ex loss: 0.486109  [   14/   88]
per-ex loss: 0.501952  [   16/   88]
per-ex loss: 0.544792  [   18/   88]
per-ex loss: 0.405400  [   20/   88]
per-ex loss: 0.475011  [   22/   88]
per-ex loss: 0.363409  [   24/   88]
per-ex loss: 0.607911  [   26/   88]
per-ex loss: 0.388807  [   28/   88]
per-ex loss: 0.473265  [   30/   88]
per-ex loss: 0.456906  [   32/   88]
per-ex loss: 0.572340  [   34/   88]
per-ex loss: 0.447278  [   36/   88]
per-ex loss: 0.417443  [   38/   88]
per-ex loss: 0.426972  [   40/   88]
per-ex loss: 0.575387  [   42/   88]
per-ex loss: 0.467890  [   44/   88]
per-ex loss: 0.459285  [   46/   88]
per-ex loss: 0.414208  [   48/   88]
per-ex loss: 0.469765  [   50/   88]
per-ex loss: 0.403932  [   52/   88]
per-ex loss: 0.427866  [   54/   88]
per-ex loss: 0.390313  [   56/   88]
per-ex loss: 0.541395  [   58/   88]
per-ex loss: 0.503923  [   60/   88]
per-ex loss: 0.473530  [   62/   88]
per-ex loss: 0.441077  [   64/   88]
per-ex loss: 0.370187  [   66/   88]
per-ex loss: 0.522669  [   68/   88]
per-ex loss: 0.377794  [   70/   88]
per-ex loss: 0.410582  [   72/   88]
per-ex loss: 0.342647  [   74/   88]
per-ex loss: 0.411105  [   76/   88]
per-ex loss: 0.417273  [   78/   88]
per-ex loss: 0.462461  [   80/   88]
per-ex loss: 0.425007  [   82/   88]
per-ex loss: 0.585337  [   84/   88]
per-ex loss: 0.467991  [   86/   88]
per-ex loss: 0.558102  [   88/   88]
Train Error: Avg loss: 0.45669666
validation Error: 
 Avg loss: 0.54027460 
 F1: 0.501213 
 Precision: 0.603859 
 Recall: 0.428393
 IoU: 0.334412

test Error: 
 Avg loss: 0.49102441 
 F1: 0.547367 
 Precision: 0.652645 
 Recall: 0.471336
 IoU: 0.376811

We have finished training iteration 78
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_63_.pth
per-ex loss: 0.437060  [    2/   88]
per-ex loss: 0.397628  [    4/   88]
per-ex loss: 0.429292  [    6/   88]
per-ex loss: 0.432125  [    8/   88]
per-ex loss: 0.389319  [   10/   88]
per-ex loss: 0.423186  [   12/   88]
per-ex loss: 0.457706  [   14/   88]
per-ex loss: 0.603602  [   16/   88]
per-ex loss: 0.396651  [   18/   88]
per-ex loss: 0.372618  [   20/   88]
per-ex loss: 0.514312  [   22/   88]
per-ex loss: 0.381431  [   24/   88]
per-ex loss: 0.644472  [   26/   88]
per-ex loss: 0.569042  [   28/   88]
per-ex loss: 0.358279  [   30/   88]
per-ex loss: 0.410519  [   32/   88]
per-ex loss: 0.558494  [   34/   88]
per-ex loss: 0.416514  [   36/   88]
per-ex loss: 0.373224  [   38/   88]
per-ex loss: 0.456576  [   40/   88]
per-ex loss: 0.420969  [   42/   88]
per-ex loss: 0.371295  [   44/   88]
per-ex loss: 0.384794  [   46/   88]
per-ex loss: 0.454863  [   48/   88]
per-ex loss: 0.601857  [   50/   88]
per-ex loss: 0.478459  [   52/   88]
per-ex loss: 0.499389  [   54/   88]
per-ex loss: 0.367054  [   56/   88]
per-ex loss: 0.611752  [   58/   88]
per-ex loss: 0.422187  [   60/   88]
per-ex loss: 0.488484  [   62/   88]
per-ex loss: 0.406227  [   64/   88]
per-ex loss: 0.351241  [   66/   88]
per-ex loss: 0.540872  [   68/   88]
per-ex loss: 0.396546  [   70/   88]
per-ex loss: 0.447772  [   72/   88]
per-ex loss: 0.421582  [   74/   88]
per-ex loss: 0.436679  [   76/   88]
per-ex loss: 0.465609  [   78/   88]
per-ex loss: 0.677544  [   80/   88]
per-ex loss: 0.581189  [   82/   88]
per-ex loss: 0.344628  [   84/   88]
per-ex loss: 0.586819  [   86/   88]
per-ex loss: 0.434763  [   88/   88]
Train Error: Avg loss: 0.45942335
validation Error: 
 Avg loss: 0.51937800 
 F1: 0.505005 
 Precision: 0.565879 
 Recall: 0.455957
 IoU: 0.337797

test Error: 
 Avg loss: 0.47007013 
 F1: 0.568199 
 Precision: 0.626404 
 Recall: 0.519891
 IoU: 0.396842

We have finished training iteration 79
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_77_.pth
per-ex loss: 0.589621  [    2/   88]
per-ex loss: 0.459132  [    4/   88]
per-ex loss: 0.535635  [    6/   88]
per-ex loss: 0.500421  [    8/   88]
per-ex loss: 0.516341  [   10/   88]
per-ex loss: 0.491613  [   12/   88]
per-ex loss: 0.467758  [   14/   88]
per-ex loss: 0.609116  [   16/   88]
per-ex loss: 0.447309  [   18/   88]
per-ex loss: 0.375122  [   20/   88]
per-ex loss: 0.402763  [   22/   88]
per-ex loss: 0.539921  [   24/   88]
per-ex loss: 0.471963  [   26/   88]
per-ex loss: 0.533941  [   28/   88]
per-ex loss: 0.411651  [   30/   88]
per-ex loss: 0.341883  [   32/   88]
per-ex loss: 0.357874  [   34/   88]
per-ex loss: 0.420603  [   36/   88]
per-ex loss: 0.516675  [   38/   88]
per-ex loss: 0.354339  [   40/   88]
per-ex loss: 0.344195  [   42/   88]
per-ex loss: 0.510577  [   44/   88]
per-ex loss: 0.498650  [   46/   88]
per-ex loss: 0.361265  [   48/   88]
per-ex loss: 0.568911  [   50/   88]
per-ex loss: 0.554018  [   52/   88]
per-ex loss: 0.411232  [   54/   88]
per-ex loss: 0.477146  [   56/   88]
per-ex loss: 0.555794  [   58/   88]
per-ex loss: 0.390379  [   60/   88]
per-ex loss: 0.521589  [   62/   88]
per-ex loss: 0.352074  [   64/   88]
per-ex loss: 0.484767  [   66/   88]
per-ex loss: 0.548181  [   68/   88]
per-ex loss: 0.449372  [   70/   88]
per-ex loss: 0.500737  [   72/   88]
per-ex loss: 0.571931  [   74/   88]
per-ex loss: 0.438487  [   76/   88]
per-ex loss: 0.515128  [   78/   88]
per-ex loss: 0.492635  [   80/   88]
per-ex loss: 0.552437  [   82/   88]
per-ex loss: 0.616327  [   84/   88]
per-ex loss: 0.646903  [   86/   88]
per-ex loss: 0.414568  [   88/   88]
Train Error: Avg loss: 0.48002236
validation Error: 
 Avg loss: 0.58936941 
 F1: 0.428330 
 Precision: 0.334560 
 Recall: 0.595135
 IoU: 0.272532

test Error: 
 Avg loss: 0.52330789 
 F1: 0.505432 
 Precision: 0.408227 
 Recall: 0.663396
 IoU: 0.338179

We have finished training iteration 80
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_78_.pth
per-ex loss: 0.436181  [    2/   88]
per-ex loss: 0.383744  [    4/   88]
per-ex loss: 0.475322  [    6/   88]
per-ex loss: 0.645239  [    8/   88]
per-ex loss: 0.469363  [   10/   88]
per-ex loss: 0.551620  [   12/   88]
per-ex loss: 0.435936  [   14/   88]
per-ex loss: 0.528736  [   16/   88]
per-ex loss: 0.637262  [   18/   88]
per-ex loss: 0.463339  [   20/   88]
per-ex loss: 0.484826  [   22/   88]
per-ex loss: 0.607926  [   24/   88]
per-ex loss: 0.399370  [   26/   88]
per-ex loss: 0.600941  [   28/   88]
per-ex loss: 0.486838  [   30/   88]
per-ex loss: 0.423693  [   32/   88]
per-ex loss: 0.454212  [   34/   88]
per-ex loss: 0.336241  [   36/   88]
per-ex loss: 0.518249  [   38/   88]
per-ex loss: 0.413170  [   40/   88]
per-ex loss: 0.445911  [   42/   88]
per-ex loss: 0.448031  [   44/   88]
per-ex loss: 0.495156  [   46/   88]
per-ex loss: 0.364452  [   48/   88]
per-ex loss: 0.500525  [   50/   88]
per-ex loss: 0.665760  [   52/   88]
per-ex loss: 0.453876  [   54/   88]
per-ex loss: 0.393558  [   56/   88]
per-ex loss: 0.446508  [   58/   88]
per-ex loss: 0.453779  [   60/   88]
per-ex loss: 0.426405  [   62/   88]
per-ex loss: 0.365867  [   64/   88]
per-ex loss: 0.353414  [   66/   88]
per-ex loss: 0.449573  [   68/   88]
per-ex loss: 0.434592  [   70/   88]
per-ex loss: 0.565648  [   72/   88]
per-ex loss: 0.354981  [   74/   88]
per-ex loss: 0.546942  [   76/   88]
per-ex loss: 0.475287  [   78/   88]
per-ex loss: 0.594913  [   80/   88]
per-ex loss: 0.390041  [   82/   88]
per-ex loss: 0.464230  [   84/   88]
per-ex loss: 0.366585  [   86/   88]
per-ex loss: 0.407178  [   88/   88]
Train Error: Avg loss: 0.46853224
validation Error: 
 Avg loss: 0.51437208 
 F1: 0.506455 
 Precision: 0.584924 
 Recall: 0.446550
 IoU: 0.339096

test Error: 
 Avg loss: 0.47672968 
 F1: 0.560066 
 Precision: 0.635371 
 Recall: 0.500720
 IoU: 0.388953

We have finished training iteration 81
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_79_.pth
per-ex loss: 0.515520  [    2/   88]
per-ex loss: 0.420586  [    4/   88]
per-ex loss: 0.529626  [    6/   88]
per-ex loss: 0.405716  [    8/   88]
per-ex loss: 0.409131  [   10/   88]
per-ex loss: 0.357705  [   12/   88]
per-ex loss: 0.436942  [   14/   88]
per-ex loss: 0.507530  [   16/   88]
per-ex loss: 0.456194  [   18/   88]
per-ex loss: 0.371454  [   20/   88]
per-ex loss: 0.420128  [   22/   88]
per-ex loss: 0.427044  [   24/   88]
per-ex loss: 0.490394  [   26/   88]
per-ex loss: 0.356819  [   28/   88]
per-ex loss: 0.584463  [   30/   88]
per-ex loss: 0.368643  [   32/   88]
per-ex loss: 0.647634  [   34/   88]
per-ex loss: 0.378476  [   36/   88]
per-ex loss: 0.537141  [   38/   88]
per-ex loss: 0.493128  [   40/   88]
per-ex loss: 0.590323  [   42/   88]
per-ex loss: 0.405843  [   44/   88]
per-ex loss: 0.488111  [   46/   88]
per-ex loss: 0.445406  [   48/   88]
per-ex loss: 0.637350  [   50/   88]
per-ex loss: 0.440697  [   52/   88]
per-ex loss: 0.380632  [   54/   88]
per-ex loss: 0.566111  [   56/   88]
per-ex loss: 0.347931  [   58/   88]
per-ex loss: 0.511032  [   60/   88]
per-ex loss: 0.514871  [   62/   88]
per-ex loss: 0.386905  [   64/   88]
per-ex loss: 0.413679  [   66/   88]
per-ex loss: 0.382480  [   68/   88]
per-ex loss: 0.591018  [   70/   88]
per-ex loss: 0.511621  [   72/   88]
per-ex loss: 0.517892  [   74/   88]
per-ex loss: 0.518953  [   76/   88]
per-ex loss: 0.407206  [   78/   88]
per-ex loss: 0.572653  [   80/   88]
per-ex loss: 0.384684  [   82/   88]
per-ex loss: 0.484451  [   84/   88]
per-ex loss: 0.442945  [   86/   88]
per-ex loss: 0.397278  [   88/   88]
Train Error: Avg loss: 0.46487142
validation Error: 
 Avg loss: 0.53306716 
 F1: 0.505434 
 Precision: 0.550883 
 Recall: 0.466913
 IoU: 0.338181

test Error: 
 Avg loss: 0.47410831 
 F1: 0.563373 
 Precision: 0.608115 
 Recall: 0.524763
 IoU: 0.392150

We have finished training iteration 82
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_80_.pth
per-ex loss: 0.496647  [    2/   88]
per-ex loss: 0.491131  [    4/   88]
per-ex loss: 0.409289  [    6/   88]
per-ex loss: 0.452634  [    8/   88]
per-ex loss: 0.525808  [   10/   88]
per-ex loss: 0.479001  [   12/   88]
per-ex loss: 0.663860  [   14/   88]
per-ex loss: 0.569256  [   16/   88]
per-ex loss: 0.425182  [   18/   88]
per-ex loss: 0.422390  [   20/   88]
per-ex loss: 0.604561  [   22/   88]
per-ex loss: 0.394618  [   24/   88]
per-ex loss: 0.524434  [   26/   88]
per-ex loss: 0.388950  [   28/   88]
per-ex loss: 0.573959  [   30/   88]
per-ex loss: 0.403455  [   32/   88]
per-ex loss: 0.609589  [   34/   88]
per-ex loss: 0.421897  [   36/   88]
per-ex loss: 0.443575  [   38/   88]
per-ex loss: 0.657054  [   40/   88]
per-ex loss: 0.446908  [   42/   88]
per-ex loss: 0.367529  [   44/   88]
per-ex loss: 0.379632  [   46/   88]
per-ex loss: 0.440864  [   48/   88]
per-ex loss: 0.452649  [   50/   88]
per-ex loss: 0.473616  [   52/   88]
per-ex loss: 0.404815  [   54/   88]
per-ex loss: 0.353482  [   56/   88]
per-ex loss: 0.450086  [   58/   88]
per-ex loss: 0.409695  [   60/   88]
per-ex loss: 0.477437  [   62/   88]
per-ex loss: 0.389900  [   64/   88]
per-ex loss: 0.384561  [   66/   88]
per-ex loss: 0.435341  [   68/   88]
per-ex loss: 0.510052  [   70/   88]
per-ex loss: 0.487915  [   72/   88]
per-ex loss: 0.561392  [   74/   88]
per-ex loss: 0.401020  [   76/   88]
per-ex loss: 0.349657  [   78/   88]
per-ex loss: 0.372911  [   80/   88]
per-ex loss: 0.438708  [   82/   88]
per-ex loss: 0.431501  [   84/   88]
per-ex loss: 0.410281  [   86/   88]
per-ex loss: 0.345751  [   88/   88]
Train Error: Avg loss: 0.45756796
validation Error: 
 Avg loss: 0.53626099 
 F1: 0.477544 
 Precision: 0.619114 
 Recall: 0.388668
 IoU: 0.313667

test Error: 
 Avg loss: 0.50533137 
 F1: 0.531736 
 Precision: 0.730517 
 Recall: 0.417995
 IoU: 0.362153

We have finished training iteration 83
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_76_.pth
per-ex loss: 0.430592  [    2/   88]
per-ex loss: 0.602839  [    4/   88]
per-ex loss: 0.525944  [    6/   88]
per-ex loss: 0.398415  [    8/   88]
per-ex loss: 0.409745  [   10/   88]
per-ex loss: 0.501722  [   12/   88]
per-ex loss: 0.412885  [   14/   88]
per-ex loss: 0.370003  [   16/   88]
per-ex loss: 0.502435  [   18/   88]
per-ex loss: 0.492849  [   20/   88]
per-ex loss: 0.431291  [   22/   88]
per-ex loss: 0.387019  [   24/   88]
per-ex loss: 0.402171  [   26/   88]
per-ex loss: 0.392939  [   28/   88]
per-ex loss: 0.436566  [   30/   88]
per-ex loss: 0.382941  [   32/   88]
per-ex loss: 0.418648  [   34/   88]
per-ex loss: 0.424219  [   36/   88]
per-ex loss: 0.637192  [   38/   88]
per-ex loss: 0.522537  [   40/   88]
per-ex loss: 0.452315  [   42/   88]
per-ex loss: 0.457903  [   44/   88]
per-ex loss: 0.458143  [   46/   88]
per-ex loss: 0.381141  [   48/   88]
per-ex loss: 0.398731  [   50/   88]
per-ex loss: 0.437903  [   52/   88]
per-ex loss: 0.504798  [   54/   88]
per-ex loss: 0.461018  [   56/   88]
per-ex loss: 0.497747  [   58/   88]
per-ex loss: 0.370220  [   60/   88]
per-ex loss: 0.596480  [   62/   88]
per-ex loss: 0.377440  [   64/   88]
per-ex loss: 0.556769  [   66/   88]
per-ex loss: 0.516716  [   68/   88]
per-ex loss: 0.605303  [   70/   88]
per-ex loss: 0.371328  [   72/   88]
per-ex loss: 0.356434  [   74/   88]
per-ex loss: 0.400502  [   76/   88]
per-ex loss: 0.420257  [   78/   88]
per-ex loss: 0.503064  [   80/   88]
per-ex loss: 0.423184  [   82/   88]
per-ex loss: 0.370865  [   84/   88]
per-ex loss: 0.476942  [   86/   88]
per-ex loss: 0.495832  [   88/   88]
Train Error: Avg loss: 0.45395425
validation Error: 
 Avg loss: 0.54407397 
 F1: 0.488580 
 Precision: 0.510614 
 Recall: 0.468369
 IoU: 0.323259

test Error: 
 Avg loss: 0.48677511 
 F1: 0.553379 
 Precision: 0.621643 
 Recall: 0.498624
 IoU: 0.382532

We have finished training iteration 84
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_82_.pth
per-ex loss: 0.391349  [    2/   88]
per-ex loss: 0.367112  [    4/   88]
per-ex loss: 0.462858  [    6/   88]
per-ex loss: 0.386592  [    8/   88]
per-ex loss: 0.435738  [   10/   88]
per-ex loss: 0.398515  [   12/   88]
per-ex loss: 0.623230  [   14/   88]
per-ex loss: 0.362128  [   16/   88]
per-ex loss: 0.473783  [   18/   88]
per-ex loss: 0.347839  [   20/   88]
per-ex loss: 0.399660  [   22/   88]
per-ex loss: 0.641953  [   24/   88]
per-ex loss: 0.575319  [   26/   88]
per-ex loss: 0.511474  [   28/   88]
per-ex loss: 0.495322  [   30/   88]
per-ex loss: 0.529484  [   32/   88]
per-ex loss: 0.531635  [   34/   88]
per-ex loss: 0.600780  [   36/   88]
per-ex loss: 0.506052  [   38/   88]
per-ex loss: 0.426572  [   40/   88]
per-ex loss: 0.409690  [   42/   88]
per-ex loss: 0.407191  [   44/   88]
per-ex loss: 0.432669  [   46/   88]
per-ex loss: 0.467244  [   48/   88]
per-ex loss: 0.367612  [   50/   88]
per-ex loss: 0.553171  [   52/   88]
per-ex loss: 0.388281  [   54/   88]
per-ex loss: 0.359559  [   56/   88]
per-ex loss: 0.425043  [   58/   88]
per-ex loss: 0.576113  [   60/   88]
per-ex loss: 0.487181  [   62/   88]
per-ex loss: 0.514975  [   64/   88]
per-ex loss: 0.589065  [   66/   88]
per-ex loss: 0.452733  [   68/   88]
per-ex loss: 0.377366  [   70/   88]
per-ex loss: 0.453794  [   72/   88]
per-ex loss: 0.539335  [   74/   88]
per-ex loss: 0.392961  [   76/   88]
per-ex loss: 0.606129  [   78/   88]
per-ex loss: 0.432265  [   80/   88]
per-ex loss: 0.416004  [   82/   88]
per-ex loss: 0.355025  [   84/   88]
per-ex loss: 0.398471  [   86/   88]
per-ex loss: 0.434818  [   88/   88]
Train Error: Avg loss: 0.46145657
validation Error: 
 Avg loss: 0.52168044 
 F1: 0.499026 
 Precision: 0.640364 
 Recall: 0.408798
 IoU: 0.332468

test Error: 
 Avg loss: 0.49285278 
 F1: 0.546787 
 Precision: 0.697018 
 Recall: 0.449833
 IoU: 0.376261

We have finished training iteration 85
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_83_.pth
per-ex loss: 0.359379  [    2/   88]
per-ex loss: 0.472819  [    4/   88]
per-ex loss: 0.425446  [    6/   88]
per-ex loss: 0.421063  [    8/   88]
per-ex loss: 0.480579  [   10/   88]
per-ex loss: 0.380159  [   12/   88]
per-ex loss: 0.447985  [   14/   88]
per-ex loss: 0.387594  [   16/   88]
per-ex loss: 0.382211  [   18/   88]
per-ex loss: 0.328725  [   20/   88]
per-ex loss: 0.578320  [   22/   88]
per-ex loss: 0.491975  [   24/   88]
per-ex loss: 0.492771  [   26/   88]
per-ex loss: 0.354868  [   28/   88]
per-ex loss: 0.498608  [   30/   88]
per-ex loss: 0.543155  [   32/   88]
per-ex loss: 0.457998  [   34/   88]
per-ex loss: 0.538568  [   36/   88]
per-ex loss: 0.556413  [   38/   88]
per-ex loss: 0.480540  [   40/   88]
per-ex loss: 0.454728  [   42/   88]
per-ex loss: 0.456378  [   44/   88]
per-ex loss: 0.418013  [   46/   88]
per-ex loss: 0.494834  [   48/   88]
per-ex loss: 0.419076  [   50/   88]
per-ex loss: 0.549763  [   52/   88]
per-ex loss: 0.600078  [   54/   88]
per-ex loss: 0.526907  [   56/   88]
per-ex loss: 0.604461  [   58/   88]
per-ex loss: 0.383432  [   60/   88]
per-ex loss: 0.567403  [   62/   88]
per-ex loss: 0.376078  [   64/   88]
per-ex loss: 0.605861  [   66/   88]
per-ex loss: 0.459640  [   68/   88]
per-ex loss: 0.404107  [   70/   88]
per-ex loss: 0.389490  [   72/   88]
per-ex loss: 0.573196  [   74/   88]
per-ex loss: 0.447086  [   76/   88]
per-ex loss: 0.485280  [   78/   88]
per-ex loss: 0.454342  [   80/   88]
per-ex loss: 0.436234  [   82/   88]
per-ex loss: 0.409026  [   84/   88]
per-ex loss: 0.352235  [   86/   88]
per-ex loss: 0.393175  [   88/   88]
Train Error: Avg loss: 0.46227266
validation Error: 
 Avg loss: 0.51000980 
 F1: 0.511433 
 Precision: 0.612199 
 Recall: 0.439150
 IoU: 0.343574

test Error: 
 Avg loss: 0.47368856 
 F1: 0.567732 
 Precision: 0.661770 
 Recall: 0.497095
 IoU: 0.396387

We have finished training iteration 86
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_84_.pth
per-ex loss: 0.481376  [    2/   88]
per-ex loss: 0.381692  [    4/   88]
per-ex loss: 0.452175  [    6/   88]
per-ex loss: 0.450525  [    8/   88]
per-ex loss: 0.521798  [   10/   88]
per-ex loss: 0.448819  [   12/   88]
per-ex loss: 0.344752  [   14/   88]
per-ex loss: 0.355767  [   16/   88]
per-ex loss: 0.491667  [   18/   88]
per-ex loss: 0.500553  [   20/   88]
per-ex loss: 0.661741  [   22/   88]
per-ex loss: 0.354766  [   24/   88]
per-ex loss: 0.514203  [   26/   88]
per-ex loss: 0.545788  [   28/   88]
per-ex loss: 0.357307  [   30/   88]
per-ex loss: 0.602038  [   32/   88]
per-ex loss: 0.467520  [   34/   88]
per-ex loss: 0.542293  [   36/   88]
per-ex loss: 0.566277  [   38/   88]
per-ex loss: 0.384934  [   40/   88]
per-ex loss: 0.619296  [   42/   88]
per-ex loss: 0.432932  [   44/   88]
per-ex loss: 0.455502  [   46/   88]
per-ex loss: 0.451000  [   48/   88]
per-ex loss: 0.400705  [   50/   88]
per-ex loss: 0.399566  [   52/   88]
per-ex loss: 0.484953  [   54/   88]
per-ex loss: 0.439870  [   56/   88]
per-ex loss: 0.343147  [   58/   88]
per-ex loss: 0.389199  [   60/   88]
per-ex loss: 0.355343  [   62/   88]
per-ex loss: 0.395519  [   64/   88]
per-ex loss: 0.439729  [   66/   88]
per-ex loss: 0.550517  [   68/   88]
per-ex loss: 0.487200  [   70/   88]
per-ex loss: 0.616294  [   72/   88]
per-ex loss: 0.461386  [   74/   88]
per-ex loss: 0.511180  [   76/   88]
per-ex loss: 0.483112  [   78/   88]
per-ex loss: 0.364223  [   80/   88]
per-ex loss: 0.504934  [   82/   88]
per-ex loss: 0.415091  [   84/   88]
per-ex loss: 0.446951  [   86/   88]
per-ex loss: 0.455756  [   88/   88]
Train Error: Avg loss: 0.46203169
validation Error: 
 Avg loss: 0.52530614 
 F1: 0.499590 
 Precision: 0.534790 
 Recall: 0.468737
 IoU: 0.332969

test Error: 
 Avg loss: 0.47810282 
 F1: 0.560738 
 Precision: 0.623125 
 Recall: 0.509706
 IoU: 0.389601

We have finished training iteration 87
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_85_.pth
per-ex loss: 0.513993  [    2/   88]
per-ex loss: 0.654947  [    4/   88]
per-ex loss: 0.362939  [    6/   88]
per-ex loss: 0.436929  [    8/   88]
per-ex loss: 0.482551  [   10/   88]
per-ex loss: 0.438482  [   12/   88]
per-ex loss: 0.436403  [   14/   88]
per-ex loss: 0.368891  [   16/   88]
per-ex loss: 0.606011  [   18/   88]
per-ex loss: 0.522900  [   20/   88]
per-ex loss: 0.474101  [   22/   88]
per-ex loss: 0.371973  [   24/   88]
per-ex loss: 0.585783  [   26/   88]
per-ex loss: 0.586197  [   28/   88]
per-ex loss: 0.338297  [   30/   88]
per-ex loss: 0.584123  [   32/   88]
per-ex loss: 0.386460  [   34/   88]
per-ex loss: 0.440402  [   36/   88]
per-ex loss: 0.552387  [   38/   88]
per-ex loss: 0.413110  [   40/   88]
per-ex loss: 0.356458  [   42/   88]
per-ex loss: 0.369122  [   44/   88]
per-ex loss: 0.570352  [   46/   88]
per-ex loss: 0.615559  [   48/   88]
per-ex loss: 0.538678  [   50/   88]
per-ex loss: 0.415665  [   52/   88]
per-ex loss: 0.515580  [   54/   88]
per-ex loss: 0.438202  [   56/   88]
per-ex loss: 0.523416  [   58/   88]
per-ex loss: 0.411704  [   60/   88]
per-ex loss: 0.424105  [   62/   88]
per-ex loss: 0.464647  [   64/   88]
per-ex loss: 0.412611  [   66/   88]
per-ex loss: 0.492309  [   68/   88]
per-ex loss: 0.576724  [   70/   88]
per-ex loss: 0.496021  [   72/   88]
per-ex loss: 0.383696  [   74/   88]
per-ex loss: 0.590246  [   76/   88]
per-ex loss: 0.392164  [   78/   88]
per-ex loss: 0.384087  [   80/   88]
per-ex loss: 0.509941  [   82/   88]
per-ex loss: 0.355137  [   84/   88]
per-ex loss: 0.458363  [   86/   88]
per-ex loss: 0.378619  [   88/   88]
Train Error: Avg loss: 0.46887019
validation Error: 
 Avg loss: 0.53262029 
 F1: 0.478603 
 Precision: 0.537459 
 Recall: 0.431365
 IoU: 0.314581

test Error: 
 Avg loss: 0.49296390 
 F1: 0.546230 
 Precision: 0.648501 
 Recall: 0.471822
 IoU: 0.375733

We have finished training iteration 88
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_81_.pth
per-ex loss: 0.437656  [    2/   88]
per-ex loss: 0.544932  [    4/   88]
per-ex loss: 0.419192  [    6/   88]
per-ex loss: 0.456831  [    8/   88]
per-ex loss: 0.387708  [   10/   88]
per-ex loss: 0.494101  [   12/   88]
per-ex loss: 0.581728  [   14/   88]
per-ex loss: 0.403466  [   16/   88]
per-ex loss: 0.570198  [   18/   88]
per-ex loss: 0.447262  [   20/   88]
per-ex loss: 0.479848  [   22/   88]
per-ex loss: 0.524177  [   24/   88]
per-ex loss: 0.534230  [   26/   88]
per-ex loss: 0.494615  [   28/   88]
per-ex loss: 0.416828  [   30/   88]
per-ex loss: 0.557402  [   32/   88]
per-ex loss: 0.411619  [   34/   88]
per-ex loss: 0.406190  [   36/   88]
per-ex loss: 0.526380  [   38/   88]
per-ex loss: 0.370895  [   40/   88]
per-ex loss: 0.417591  [   42/   88]
per-ex loss: 0.437144  [   44/   88]
per-ex loss: 0.566990  [   46/   88]
per-ex loss: 0.447798  [   48/   88]
per-ex loss: 0.396421  [   50/   88]
per-ex loss: 0.495292  [   52/   88]
per-ex loss: 0.605642  [   54/   88]
per-ex loss: 0.378213  [   56/   88]
per-ex loss: 0.553040  [   58/   88]
per-ex loss: 0.416770  [   60/   88]
per-ex loss: 0.439481  [   62/   88]
per-ex loss: 0.347988  [   64/   88]
per-ex loss: 0.343545  [   66/   88]
per-ex loss: 0.410380  [   68/   88]
per-ex loss: 0.526243  [   70/   88]
per-ex loss: 0.355916  [   72/   88]
per-ex loss: 0.434719  [   74/   88]
per-ex loss: 0.487153  [   76/   88]
per-ex loss: 0.363521  [   78/   88]
per-ex loss: 0.387943  [   80/   88]
per-ex loss: 0.385860  [   82/   88]
per-ex loss: 0.461467  [   84/   88]
per-ex loss: 0.363372  [   86/   88]
per-ex loss: 0.474730  [   88/   88]
Train Error: Avg loss: 0.45369263
validation Error: 
 Avg loss: 0.53634902 
 F1: 0.490565 
 Precision: 0.620954 
 Recall: 0.405432
 IoU: 0.324999

test Error: 
 Avg loss: 0.49206841 
 F1: 0.547129 
 Precision: 0.698155 
 Recall: 0.449823
 IoU: 0.376585

We have finished training iteration 89
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_87_.pth
per-ex loss: 0.430142  [    2/   88]
per-ex loss: 0.428980  [    4/   88]
per-ex loss: 0.391712  [    6/   88]
per-ex loss: 0.388288  [    8/   88]
per-ex loss: 0.454041  [   10/   88]
per-ex loss: 0.492223  [   12/   88]
per-ex loss: 0.386223  [   14/   88]
per-ex loss: 0.383853  [   16/   88]
per-ex loss: 0.411383  [   18/   88]
per-ex loss: 0.502200  [   20/   88]
per-ex loss: 0.551869  [   22/   88]
per-ex loss: 0.437901  [   24/   88]
per-ex loss: 0.399998  [   26/   88]
per-ex loss: 0.400335  [   28/   88]
per-ex loss: 0.485832  [   30/   88]
per-ex loss: 0.408648  [   32/   88]
per-ex loss: 0.406577  [   34/   88]
per-ex loss: 0.603439  [   36/   88]
per-ex loss: 0.476603  [   38/   88]
per-ex loss: 0.431126  [   40/   88]
per-ex loss: 0.439503  [   42/   88]
per-ex loss: 0.430117  [   44/   88]
per-ex loss: 0.475883  [   46/   88]
per-ex loss: 0.370716  [   48/   88]
per-ex loss: 0.438018  [   50/   88]
per-ex loss: 0.476970  [   52/   88]
per-ex loss: 0.480402  [   54/   88]
per-ex loss: 0.435200  [   56/   88]
per-ex loss: 0.573425  [   58/   88]
per-ex loss: 0.565261  [   60/   88]
per-ex loss: 0.421323  [   62/   88]
per-ex loss: 0.405793  [   64/   88]
per-ex loss: 0.455793  [   66/   88]
per-ex loss: 0.359339  [   68/   88]
per-ex loss: 0.508884  [   70/   88]
per-ex loss: 0.352621  [   72/   88]
per-ex loss: 0.413397  [   74/   88]
per-ex loss: 0.539753  [   76/   88]
per-ex loss: 0.380886  [   78/   88]
per-ex loss: 0.587799  [   80/   88]
per-ex loss: 0.374485  [   82/   88]
per-ex loss: 0.356289  [   84/   88]
per-ex loss: 0.375596  [   86/   88]
per-ex loss: 0.495775  [   88/   88]
Train Error: Avg loss: 0.44510457
validation Error: 
 Avg loss: 0.51974403 
 F1: 0.492623 
 Precision: 0.613348 
 Recall: 0.411606
 IoU: 0.326808

test Error: 
 Avg loss: 0.49496211 
 F1: 0.544309 
 Precision: 0.703099 
 Recall: 0.444028
 IoU: 0.373918

We have finished training iteration 90
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_88_.pth
per-ex loss: 0.478122  [    2/   88]
per-ex loss: 0.471820  [    4/   88]
per-ex loss: 0.568081  [    6/   88]
per-ex loss: 0.458194  [    8/   88]
per-ex loss: 0.343820  [   10/   88]
per-ex loss: 0.418299  [   12/   88]
per-ex loss: 0.415449  [   14/   88]
per-ex loss: 0.552276  [   16/   88]
per-ex loss: 0.380974  [   18/   88]
per-ex loss: 0.396633  [   20/   88]
per-ex loss: 0.526549  [   22/   88]
per-ex loss: 0.410156  [   24/   88]
per-ex loss: 0.481543  [   26/   88]
per-ex loss: 0.525663  [   28/   88]
per-ex loss: 0.423208  [   30/   88]
per-ex loss: 0.378191  [   32/   88]
per-ex loss: 0.610143  [   34/   88]
per-ex loss: 0.420440  [   36/   88]
per-ex loss: 0.444122  [   38/   88]
per-ex loss: 0.591407  [   40/   88]
per-ex loss: 0.384491  [   42/   88]
per-ex loss: 0.486498  [   44/   88]
per-ex loss: 0.453762  [   46/   88]
per-ex loss: 0.395942  [   48/   88]
per-ex loss: 0.606364  [   50/   88]
per-ex loss: 0.444768  [   52/   88]
per-ex loss: 0.417935  [   54/   88]
per-ex loss: 0.409586  [   56/   88]
per-ex loss: 0.498331  [   58/   88]
per-ex loss: 0.438778  [   60/   88]
per-ex loss: 0.493077  [   62/   88]
per-ex loss: 0.511601  [   64/   88]
per-ex loss: 0.519932  [   66/   88]
per-ex loss: 0.421071  [   68/   88]
per-ex loss: 0.414766  [   70/   88]
per-ex loss: 0.477005  [   72/   88]
per-ex loss: 0.359430  [   74/   88]
per-ex loss: 0.378271  [   76/   88]
per-ex loss: 0.479236  [   78/   88]
per-ex loss: 0.379302  [   80/   88]
per-ex loss: 0.422709  [   82/   88]
per-ex loss: 0.578056  [   84/   88]
per-ex loss: 0.434307  [   86/   88]
per-ex loss: 0.433697  [   88/   88]
Train Error: Avg loss: 0.45759101
validation Error: 
 Avg loss: 0.54473157 
 F1: 0.480140 
 Precision: 0.676869 
 Recall: 0.372015
 IoU: 0.315911

test Error: 
 Avg loss: 0.52231383 
 F1: 0.512094 
 Precision: 0.734871 
 Recall: 0.392966
 IoU: 0.344171

We have finished training iteration 91
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_89_.pth
per-ex loss: 0.641484  [    2/   88]
per-ex loss: 0.440326  [    4/   88]
per-ex loss: 0.583403  [    6/   88]
per-ex loss: 0.492223  [    8/   88]
per-ex loss: 0.378778  [   10/   88]
per-ex loss: 0.384848  [   12/   88]
per-ex loss: 0.397262  [   14/   88]
per-ex loss: 0.376308  [   16/   88]
per-ex loss: 0.531258  [   18/   88]
per-ex loss: 0.599646  [   20/   88]
per-ex loss: 0.589411  [   22/   88]
per-ex loss: 0.402133  [   24/   88]
per-ex loss: 0.503580  [   26/   88]
per-ex loss: 0.500388  [   28/   88]
per-ex loss: 0.377377  [   30/   88]
per-ex loss: 0.386220  [   32/   88]
per-ex loss: 0.383215  [   34/   88]
per-ex loss: 0.611307  [   36/   88]
per-ex loss: 0.366713  [   38/   88]
per-ex loss: 0.462489  [   40/   88]
per-ex loss: 0.431138  [   42/   88]
per-ex loss: 0.391903  [   44/   88]
per-ex loss: 0.472351  [   46/   88]
per-ex loss: 0.551019  [   48/   88]
per-ex loss: 0.519417  [   50/   88]
per-ex loss: 0.438180  [   52/   88]
per-ex loss: 0.445841  [   54/   88]
per-ex loss: 0.361040  [   56/   88]
per-ex loss: 0.450218  [   58/   88]
per-ex loss: 0.442373  [   60/   88]
per-ex loss: 0.402211  [   62/   88]
per-ex loss: 0.536338  [   64/   88]
per-ex loss: 0.428118  [   66/   88]
per-ex loss: 0.412970  [   68/   88]
per-ex loss: 0.381631  [   70/   88]
per-ex loss: 0.511611  [   72/   88]
per-ex loss: 0.480290  [   74/   88]
per-ex loss: 0.435969  [   76/   88]
per-ex loss: 0.338006  [   78/   88]
per-ex loss: 0.400813  [   80/   88]
per-ex loss: 0.356238  [   82/   88]
per-ex loss: 0.473738  [   84/   88]
per-ex loss: 0.551041  [   86/   88]
per-ex loss: 0.336282  [   88/   88]
Train Error: Avg loss: 0.45357053
validation Error: 
 Avg loss: 0.53040632 
 F1: 0.497028 
 Precision: 0.627602 
 Recall: 0.411429
 IoU: 0.330697

test Error: 
 Avg loss: 0.49032740 
 F1: 0.550562 
 Precision: 0.699798 
 Recall: 0.453789
 IoU: 0.379845

We have finished training iteration 92
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_90_.pth
per-ex loss: 0.462598  [    2/   88]
per-ex loss: 0.340397  [    4/   88]
per-ex loss: 0.351689  [    6/   88]
per-ex loss: 0.366564  [    8/   88]
per-ex loss: 0.539118  [   10/   88]
per-ex loss: 0.639087  [   12/   88]
per-ex loss: 0.396224  [   14/   88]
per-ex loss: 0.396197  [   16/   88]
per-ex loss: 0.431760  [   18/   88]
per-ex loss: 0.373589  [   20/   88]
per-ex loss: 0.469075  [   22/   88]
per-ex loss: 0.378113  [   24/   88]
per-ex loss: 0.507519  [   26/   88]
per-ex loss: 0.510522  [   28/   88]
per-ex loss: 0.579906  [   30/   88]
per-ex loss: 0.431092  [   32/   88]
per-ex loss: 0.517123  [   34/   88]
per-ex loss: 0.423419  [   36/   88]
per-ex loss: 0.514380  [   38/   88]
per-ex loss: 0.498421  [   40/   88]
per-ex loss: 0.596522  [   42/   88]
per-ex loss: 0.538858  [   44/   88]
per-ex loss: 0.548888  [   46/   88]
per-ex loss: 0.501647  [   48/   88]
per-ex loss: 0.468464  [   50/   88]
per-ex loss: 0.360539  [   52/   88]
per-ex loss: 0.437977  [   54/   88]
per-ex loss: 0.510250  [   56/   88]
per-ex loss: 0.443036  [   58/   88]
per-ex loss: 0.529299  [   60/   88]
per-ex loss: 0.472440  [   62/   88]
per-ex loss: 0.366059  [   64/   88]
per-ex loss: 0.399037  [   66/   88]
per-ex loss: 0.445702  [   68/   88]
per-ex loss: 0.476838  [   70/   88]
per-ex loss: 0.324385  [   72/   88]
per-ex loss: 0.424827  [   74/   88]
per-ex loss: 0.489200  [   76/   88]
per-ex loss: 0.453140  [   78/   88]
per-ex loss: 0.398425  [   80/   88]
per-ex loss: 0.639955  [   82/   88]
per-ex loss: 0.555065  [   84/   88]
per-ex loss: 0.437309  [   86/   88]
per-ex loss: 0.384480  [   88/   88]
Train Error: Avg loss: 0.46202579
validation Error: 
 Avg loss: 0.50623631 
 F1: 0.512276 
 Precision: 0.590863 
 Recall: 0.452140
 IoU: 0.344335

test Error: 
 Avg loss: 0.47456122 
 F1: 0.561422 
 Precision: 0.623523 
 Recall: 0.510570
 IoU: 0.390261

We have finished training iteration 93
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_91_.pth
per-ex loss: 0.358043  [    2/   88]
per-ex loss: 0.593589  [    4/   88]
per-ex loss: 0.447087  [    6/   88]
per-ex loss: 0.349965  [    8/   88]
per-ex loss: 0.411912  [   10/   88]
per-ex loss: 0.523881  [   12/   88]
per-ex loss: 0.443639  [   14/   88]
per-ex loss: 0.659073  [   16/   88]
per-ex loss: 0.574080  [   18/   88]
per-ex loss: 0.355654  [   20/   88]
per-ex loss: 0.402994  [   22/   88]
per-ex loss: 0.406997  [   24/   88]
per-ex loss: 0.414913  [   26/   88]
per-ex loss: 0.432961  [   28/   88]
per-ex loss: 0.385391  [   30/   88]
per-ex loss: 0.348394  [   32/   88]
per-ex loss: 0.420240  [   34/   88]
per-ex loss: 0.394093  [   36/   88]
per-ex loss: 0.392243  [   38/   88]
per-ex loss: 0.471118  [   40/   88]
per-ex loss: 0.502696  [   42/   88]
per-ex loss: 0.387912  [   44/   88]
per-ex loss: 0.378886  [   46/   88]
per-ex loss: 0.444767  [   48/   88]
per-ex loss: 0.532183  [   50/   88]
per-ex loss: 0.540829  [   52/   88]
per-ex loss: 0.346453  [   54/   88]
per-ex loss: 0.421236  [   56/   88]
per-ex loss: 0.393958  [   58/   88]
per-ex loss: 0.444164  [   60/   88]
per-ex loss: 0.497991  [   62/   88]
per-ex loss: 0.513418  [   64/   88]
per-ex loss: 0.366628  [   66/   88]
per-ex loss: 0.562279  [   68/   88]
per-ex loss: 0.638773  [   70/   88]
per-ex loss: 0.410972  [   72/   88]
per-ex loss: 0.421005  [   74/   88]
per-ex loss: 0.616043  [   76/   88]
per-ex loss: 0.570528  [   78/   88]
per-ex loss: 0.341972  [   80/   88]
per-ex loss: 0.473985  [   82/   88]
per-ex loss: 0.471762  [   84/   88]
per-ex loss: 0.547553  [   86/   88]
per-ex loss: 0.578260  [   88/   88]
Train Error: Avg loss: 0.45887542
validation Error: 
 Avg loss: 0.50201774 
 F1: 0.513015 
 Precision: 0.564636 
 Recall: 0.470042
 IoU: 0.345004

test Error: 
 Avg loss: 0.46763858 
 F1: 0.571113 
 Precision: 0.624729 
 Recall: 0.525973
 IoU: 0.399691

We have finished training iteration 94
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_92_.pth
per-ex loss: 0.659069  [    2/   88]
per-ex loss: 0.388953  [    4/   88]
per-ex loss: 0.417641  [    6/   88]
per-ex loss: 0.511559  [    8/   88]
per-ex loss: 0.441126  [   10/   88]
per-ex loss: 0.378262  [   12/   88]
per-ex loss: 0.383379  [   14/   88]
per-ex loss: 0.470375  [   16/   88]
per-ex loss: 0.388862  [   18/   88]
per-ex loss: 0.407453  [   20/   88]
per-ex loss: 0.367998  [   22/   88]
per-ex loss: 0.459012  [   24/   88]
per-ex loss: 0.582001  [   26/   88]
per-ex loss: 0.352923  [   28/   88]
per-ex loss: 0.461463  [   30/   88]
per-ex loss: 0.354358  [   32/   88]
per-ex loss: 0.461609  [   34/   88]
per-ex loss: 0.405299  [   36/   88]
per-ex loss: 0.489005  [   38/   88]
per-ex loss: 0.352089  [   40/   88]
per-ex loss: 0.373718  [   42/   88]
per-ex loss: 0.356550  [   44/   88]
per-ex loss: 0.448304  [   46/   88]
per-ex loss: 0.506178  [   48/   88]
per-ex loss: 0.584022  [   50/   88]
per-ex loss: 0.513439  [   52/   88]
per-ex loss: 0.451540  [   54/   88]
per-ex loss: 0.515588  [   56/   88]
per-ex loss: 0.381052  [   58/   88]
per-ex loss: 0.636555  [   60/   88]
per-ex loss: 0.453379  [   62/   88]
per-ex loss: 0.492758  [   64/   88]
per-ex loss: 0.395239  [   66/   88]
per-ex loss: 0.481830  [   68/   88]
per-ex loss: 0.439434  [   70/   88]
per-ex loss: 0.435619  [   72/   88]
per-ex loss: 0.472112  [   74/   88]
per-ex loss: 0.541756  [   76/   88]
per-ex loss: 0.393092  [   78/   88]
per-ex loss: 0.535734  [   80/   88]
per-ex loss: 0.529886  [   82/   88]
per-ex loss: 0.412396  [   84/   88]
per-ex loss: 0.598527  [   86/   88]
per-ex loss: 0.537478  [   88/   88]
Train Error: Avg loss: 0.45951415
validation Error: 
 Avg loss: 0.51347073 
 F1: 0.501475 
 Precision: 0.579774 
 Recall: 0.441809
 IoU: 0.334646

test Error: 
 Avg loss: 0.48528928 
 F1: 0.551220 
 Precision: 0.643700 
 Recall: 0.481975
 IoU: 0.380472

We have finished training iteration 95
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_86_.pth
per-ex loss: 0.478152  [    2/   88]
per-ex loss: 0.576799  [    4/   88]
per-ex loss: 0.474799  [    6/   88]
per-ex loss: 0.477517  [    8/   88]
per-ex loss: 0.372048  [   10/   88]
per-ex loss: 0.345541  [   12/   88]
per-ex loss: 0.541783  [   14/   88]
per-ex loss: 0.470079  [   16/   88]
per-ex loss: 0.353367  [   18/   88]
per-ex loss: 0.505802  [   20/   88]
per-ex loss: 0.487700  [   22/   88]
per-ex loss: 0.346498  [   24/   88]
per-ex loss: 0.531499  [   26/   88]
per-ex loss: 0.380907  [   28/   88]
per-ex loss: 0.404570  [   30/   88]
per-ex loss: 0.343110  [   32/   88]
per-ex loss: 0.403428  [   34/   88]
per-ex loss: 0.526384  [   36/   88]
per-ex loss: 0.448491  [   38/   88]
per-ex loss: 0.387473  [   40/   88]
per-ex loss: 0.559031  [   42/   88]
per-ex loss: 0.479231  [   44/   88]
per-ex loss: 0.498839  [   46/   88]
per-ex loss: 0.415467  [   48/   88]
per-ex loss: 0.437479  [   50/   88]
per-ex loss: 0.344778  [   52/   88]
per-ex loss: 0.439440  [   54/   88]
per-ex loss: 0.414918  [   56/   88]
per-ex loss: 0.457255  [   58/   88]
per-ex loss: 0.606672  [   60/   88]
per-ex loss: 0.370825  [   62/   88]
per-ex loss: 0.491275  [   64/   88]
per-ex loss: 0.448401  [   66/   88]
per-ex loss: 0.475463  [   68/   88]
per-ex loss: 0.391058  [   70/   88]
per-ex loss: 0.586783  [   72/   88]
per-ex loss: 0.610947  [   74/   88]
per-ex loss: 0.378808  [   76/   88]
per-ex loss: 0.389909  [   78/   88]
per-ex loss: 0.522358  [   80/   88]
per-ex loss: 0.596824  [   82/   88]
per-ex loss: 0.405445  [   84/   88]
per-ex loss: 0.433288  [   86/   88]
per-ex loss: 0.370088  [   88/   88]
Train Error: Avg loss: 0.45410298
validation Error: 
 Avg loss: 0.53072490 
 F1: 0.496454 
 Precision: 0.577337 
 Recall: 0.435449
 IoU: 0.330189

test Error: 
 Avg loss: 0.47813494 
 F1: 0.561780 
 Precision: 0.653279 
 Recall: 0.492763
 IoU: 0.390608

We have finished training iteration 96
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_72_.pth
per-ex loss: 0.349218  [    2/   88]
per-ex loss: 0.569462  [    4/   88]
per-ex loss: 0.597284  [    6/   88]
per-ex loss: 0.475242  [    8/   88]
per-ex loss: 0.381649  [   10/   88]
per-ex loss: 0.505122  [   12/   88]
per-ex loss: 0.484450  [   14/   88]
per-ex loss: 0.385267  [   16/   88]
per-ex loss: 0.538779  [   18/   88]
per-ex loss: 0.597584  [   20/   88]
per-ex loss: 0.617885  [   22/   88]
per-ex loss: 0.588593  [   24/   88]
per-ex loss: 0.341859  [   26/   88]
per-ex loss: 0.371368  [   28/   88]
per-ex loss: 0.492882  [   30/   88]
per-ex loss: 0.439269  [   32/   88]
per-ex loss: 0.611913  [   34/   88]
per-ex loss: 0.530272  [   36/   88]
per-ex loss: 0.502626  [   38/   88]
per-ex loss: 0.485735  [   40/   88]
per-ex loss: 0.555180  [   42/   88]
per-ex loss: 0.612284  [   44/   88]
per-ex loss: 0.448691  [   46/   88]
per-ex loss: 0.415222  [   48/   88]
per-ex loss: 0.380650  [   50/   88]
per-ex loss: 0.460109  [   52/   88]
per-ex loss: 0.427368  [   54/   88]
per-ex loss: 0.584800  [   56/   88]
per-ex loss: 0.409267  [   58/   88]
per-ex loss: 0.601637  [   60/   88]
per-ex loss: 0.570253  [   62/   88]
per-ex loss: 0.423072  [   64/   88]
per-ex loss: 0.449461  [   66/   88]
per-ex loss: 0.411547  [   68/   88]
per-ex loss: 0.488480  [   70/   88]
per-ex loss: 0.395714  [   72/   88]
per-ex loss: 0.410857  [   74/   88]
per-ex loss: 0.411347  [   76/   88]
per-ex loss: 0.424577  [   78/   88]
per-ex loss: 0.371442  [   80/   88]
per-ex loss: 0.364798  [   82/   88]
per-ex loss: 0.355413  [   84/   88]
per-ex loss: 0.374703  [   86/   88]
per-ex loss: 0.385791  [   88/   88]
Train Error: Avg loss: 0.46816189
validation Error: 
 Avg loss: 0.50362456 
 F1: 0.508610 
 Precision: 0.606671 
 Recall: 0.437839
 IoU: 0.341031

test Error: 
 Avg loss: 0.47819448 
 F1: 0.560654 
 Precision: 0.655597 
 Recall: 0.489732
 IoU: 0.389520

We have finished training iteration 97
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_95_.pth
per-ex loss: 0.345711  [    2/   88]
per-ex loss: 0.391152  [    4/   88]
per-ex loss: 0.407599  [    6/   88]
per-ex loss: 0.416392  [    8/   88]
per-ex loss: 0.526114  [   10/   88]
per-ex loss: 0.528964  [   12/   88]
per-ex loss: 0.558476  [   14/   88]
per-ex loss: 0.356893  [   16/   88]
per-ex loss: 0.529129  [   18/   88]
per-ex loss: 0.425030  [   20/   88]
per-ex loss: 0.649517  [   22/   88]
per-ex loss: 0.450796  [   24/   88]
per-ex loss: 0.395231  [   26/   88]
per-ex loss: 0.385826  [   28/   88]
per-ex loss: 0.532551  [   30/   88]
per-ex loss: 0.413393  [   32/   88]
per-ex loss: 0.451557  [   34/   88]
per-ex loss: 0.582023  [   36/   88]
per-ex loss: 0.411148  [   38/   88]
per-ex loss: 0.447107  [   40/   88]
per-ex loss: 0.607488  [   42/   88]
per-ex loss: 0.368245  [   44/   88]
per-ex loss: 0.500949  [   46/   88]
per-ex loss: 0.419002  [   48/   88]
per-ex loss: 0.431566  [   50/   88]
per-ex loss: 0.413755  [   52/   88]
per-ex loss: 0.454715  [   54/   88]
per-ex loss: 0.316045  [   56/   88]
per-ex loss: 0.614197  [   58/   88]
per-ex loss: 0.457309  [   60/   88]
per-ex loss: 0.409508  [   62/   88]
per-ex loss: 0.350138  [   64/   88]
per-ex loss: 0.511278  [   66/   88]
per-ex loss: 0.423487  [   68/   88]
per-ex loss: 0.367144  [   70/   88]
per-ex loss: 0.424731  [   72/   88]
per-ex loss: 0.457583  [   74/   88]
per-ex loss: 0.344082  [   76/   88]
per-ex loss: 0.555881  [   78/   88]
per-ex loss: 0.481723  [   80/   88]
per-ex loss: 0.505494  [   82/   88]
per-ex loss: 0.351441  [   84/   88]
per-ex loss: 0.375591  [   86/   88]
per-ex loss: 0.425743  [   88/   88]
Train Error: Avg loss: 0.44935690
validation Error: 
 Avg loss: 0.53480515 
 F1: 0.493369 
 Precision: 0.662417 
 Recall: 0.393061
 IoU: 0.327465

test Error: 
 Avg loss: 0.50260482 
 F1: 0.534105 
 Precision: 0.704735 
 Recall: 0.429994
 IoU: 0.364354

We have finished training iteration 98
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_96_.pth
per-ex loss: 0.406335  [    2/   88]
per-ex loss: 0.545891  [    4/   88]
per-ex loss: 0.414282  [    6/   88]
per-ex loss: 0.350107  [    8/   88]
per-ex loss: 0.438726  [   10/   88]
per-ex loss: 0.401704  [   12/   88]
per-ex loss: 0.500434  [   14/   88]
per-ex loss: 0.349558  [   16/   88]
per-ex loss: 0.572420  [   18/   88]
per-ex loss: 0.422776  [   20/   88]
per-ex loss: 0.516238  [   22/   88]
per-ex loss: 0.473757  [   24/   88]
per-ex loss: 0.499963  [   26/   88]
per-ex loss: 0.401752  [   28/   88]
per-ex loss: 0.578171  [   30/   88]
per-ex loss: 0.359776  [   32/   88]
per-ex loss: 0.629865  [   34/   88]
per-ex loss: 0.559301  [   36/   88]
per-ex loss: 0.507608  [   38/   88]
per-ex loss: 0.384463  [   40/   88]
per-ex loss: 0.484271  [   42/   88]
per-ex loss: 0.437326  [   44/   88]
per-ex loss: 0.572568  [   46/   88]
per-ex loss: 0.388420  [   48/   88]
per-ex loss: 0.348408  [   50/   88]
per-ex loss: 0.502562  [   52/   88]
per-ex loss: 0.525507  [   54/   88]
per-ex loss: 0.402488  [   56/   88]
per-ex loss: 0.343334  [   58/   88]
per-ex loss: 0.596007  [   60/   88]
per-ex loss: 0.455653  [   62/   88]
per-ex loss: 0.528507  [   64/   88]
per-ex loss: 0.405392  [   66/   88]
per-ex loss: 0.425617  [   68/   88]
per-ex loss: 0.382688  [   70/   88]
per-ex loss: 0.405924  [   72/   88]
per-ex loss: 0.361712  [   74/   88]
per-ex loss: 0.471392  [   76/   88]
per-ex loss: 0.354254  [   78/   88]
per-ex loss: 0.423662  [   80/   88]
per-ex loss: 0.591272  [   82/   88]
per-ex loss: 0.401258  [   84/   88]
per-ex loss: 0.398092  [   86/   88]
per-ex loss: 0.439742  [   88/   88]
Train Error: Avg loss: 0.45361777
validation Error: 
 Avg loss: 0.50601364 
 F1: 0.516793 
 Precision: 0.623543 
 Recall: 0.441252
 IoU: 0.348430

test Error: 
 Avg loss: 0.47954188 
 F1: 0.559926 
 Precision: 0.651347 
 Recall: 0.491009
 IoU: 0.388817

We have finished training iteration 99
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_93_.pth
per-ex loss: 0.439664  [    2/   88]
per-ex loss: 0.375584  [    4/   88]
per-ex loss: 0.347773  [    6/   88]
per-ex loss: 0.454871  [    8/   88]
per-ex loss: 0.641847  [   10/   88]
per-ex loss: 0.444341  [   12/   88]
per-ex loss: 0.455954  [   14/   88]
per-ex loss: 0.374359  [   16/   88]
per-ex loss: 0.464221  [   18/   88]
per-ex loss: 0.463069  [   20/   88]
per-ex loss: 0.581943  [   22/   88]
per-ex loss: 0.348354  [   24/   88]
per-ex loss: 0.449379  [   26/   88]
per-ex loss: 0.471594  [   28/   88]
per-ex loss: 0.570933  [   30/   88]
per-ex loss: 0.358439  [   32/   88]
per-ex loss: 0.383141  [   34/   88]
per-ex loss: 0.398172  [   36/   88]
per-ex loss: 0.517217  [   38/   88]
per-ex loss: 0.540562  [   40/   88]
per-ex loss: 0.390466  [   42/   88]
per-ex loss: 0.418435  [   44/   88]
per-ex loss: 0.494296  [   46/   88]
per-ex loss: 0.366727  [   48/   88]
per-ex loss: 0.313040  [   50/   88]
per-ex loss: 0.326162  [   52/   88]
per-ex loss: 0.472268  [   54/   88]
per-ex loss: 0.414201  [   56/   88]
per-ex loss: 0.386760  [   58/   88]
per-ex loss: 0.362237  [   60/   88]
per-ex loss: 0.511900  [   62/   88]
per-ex loss: 0.372429  [   64/   88]
per-ex loss: 0.557492  [   66/   88]
per-ex loss: 0.391892  [   68/   88]
per-ex loss: 0.382471  [   70/   88]
per-ex loss: 0.570900  [   72/   88]
per-ex loss: 0.522426  [   74/   88]
per-ex loss: 0.477128  [   76/   88]
per-ex loss: 0.358610  [   78/   88]
per-ex loss: 0.542048  [   80/   88]
per-ex loss: 0.381964  [   82/   88]
per-ex loss: 0.516742  [   84/   88]
per-ex loss: 0.644121  [   86/   88]
per-ex loss: 0.515751  [   88/   88]
Train Error: Avg loss: 0.44936098
validation Error: 
 Avg loss: 0.54442237 
 F1: 0.489473 
 Precision: 0.505175 
 Recall: 0.474718
 IoU: 0.324041

test Error: 
 Avg loss: 0.48438397 
 F1: 0.554417 
 Precision: 0.594273 
 Recall: 0.519571
 IoU: 0.383525

We have finished training iteration 100
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_98_.pth
per-ex loss: 0.463285  [    2/   88]
per-ex loss: 0.453222  [    4/   88]
per-ex loss: 0.376505  [    6/   88]
per-ex loss: 0.382197  [    8/   88]
per-ex loss: 0.388260  [   10/   88]
per-ex loss: 0.454006  [   12/   88]
per-ex loss: 0.517243  [   14/   88]
per-ex loss: 0.377264  [   16/   88]
per-ex loss: 0.547155  [   18/   88]
per-ex loss: 0.418660  [   20/   88]
per-ex loss: 0.419900  [   22/   88]
per-ex loss: 0.385311  [   24/   88]
per-ex loss: 0.455263  [   26/   88]
per-ex loss: 0.610839  [   28/   88]
per-ex loss: 0.464669  [   30/   88]
per-ex loss: 0.348074  [   32/   88]
per-ex loss: 0.470821  [   34/   88]
per-ex loss: 0.433612  [   36/   88]
per-ex loss: 0.461794  [   38/   88]
per-ex loss: 0.452826  [   40/   88]
per-ex loss: 0.388608  [   42/   88]
per-ex loss: 0.482250  [   44/   88]
per-ex loss: 0.360925  [   46/   88]
per-ex loss: 0.463008  [   48/   88]
per-ex loss: 0.407842  [   50/   88]
per-ex loss: 0.549162  [   52/   88]
per-ex loss: 0.429285  [   54/   88]
per-ex loss: 0.425483  [   56/   88]
per-ex loss: 0.444133  [   58/   88]
per-ex loss: 0.403097  [   60/   88]
per-ex loss: 0.709977  [   62/   88]
per-ex loss: 0.393855  [   64/   88]
per-ex loss: 0.442249  [   66/   88]
per-ex loss: 0.445614  [   68/   88]
per-ex loss: 0.342643  [   70/   88]
per-ex loss: 0.565564  [   72/   88]
per-ex loss: 0.358316  [   74/   88]
per-ex loss: 0.377430  [   76/   88]
per-ex loss: 0.549554  [   78/   88]
per-ex loss: 0.387662  [   80/   88]
per-ex loss: 0.398721  [   82/   88]
per-ex loss: 0.624278  [   84/   88]
per-ex loss: 0.373080  [   86/   88]
per-ex loss: 0.359100  [   88/   88]
Train Error: Avg loss: 0.44460779
validation Error: 
 Avg loss: 0.52136057 
 F1: 0.501119 
 Precision: 0.556638 
 Recall: 0.455671
 IoU: 0.334329

test Error: 
 Avg loss: 0.48530853 
 F1: 0.550772 
 Precision: 0.630330 
 Recall: 0.489047
 IoU: 0.380045

We have finished training iteration 101
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_99_.pth
per-ex loss: 0.331730  [    2/   88]
per-ex loss: 0.424173  [    4/   88]
per-ex loss: 0.437583  [    6/   88]
per-ex loss: 0.418280  [    8/   88]
per-ex loss: 0.382738  [   10/   88]
per-ex loss: 0.371732  [   12/   88]
per-ex loss: 0.476483  [   14/   88]
per-ex loss: 0.360366  [   16/   88]
per-ex loss: 0.448539  [   18/   88]
per-ex loss: 0.447963  [   20/   88]
per-ex loss: 0.379200  [   22/   88]
per-ex loss: 0.354659  [   24/   88]
per-ex loss: 0.545159  [   26/   88]
per-ex loss: 0.455570  [   28/   88]
per-ex loss: 0.409953  [   30/   88]
per-ex loss: 0.486908  [   32/   88]
per-ex loss: 0.430618  [   34/   88]
per-ex loss: 0.628572  [   36/   88]
per-ex loss: 0.455086  [   38/   88]
per-ex loss: 0.371146  [   40/   88]
per-ex loss: 0.433619  [   42/   88]
per-ex loss: 0.398623  [   44/   88]
per-ex loss: 0.584166  [   46/   88]
per-ex loss: 0.552764  [   48/   88]
per-ex loss: 0.387915  [   50/   88]
per-ex loss: 0.558657  [   52/   88]
per-ex loss: 0.424491  [   54/   88]
per-ex loss: 0.440904  [   56/   88]
per-ex loss: 0.434180  [   58/   88]
per-ex loss: 0.479065  [   60/   88]
per-ex loss: 0.370227  [   62/   88]
per-ex loss: 0.601033  [   64/   88]
per-ex loss: 0.434877  [   66/   88]
per-ex loss: 0.503925  [   68/   88]
per-ex loss: 0.455336  [   70/   88]
per-ex loss: 0.392851  [   72/   88]
per-ex loss: 0.492096  [   74/   88]
per-ex loss: 0.512659  [   76/   88]
per-ex loss: 0.321911  [   78/   88]
per-ex loss: 0.388618  [   80/   88]
per-ex loss: 0.442214  [   82/   88]
per-ex loss: 0.548526  [   84/   88]
per-ex loss: 0.548698  [   86/   88]
per-ex loss: 0.520424  [   88/   88]
Train Error: Avg loss: 0.45100533
validation Error: 
 Avg loss: 0.51590906 
 F1: 0.504762 
 Precision: 0.634767 
 Recall: 0.418957
 IoU: 0.337580

test Error: 
 Avg loss: 0.48100519 
 F1: 0.556298 
 Precision: 0.671909 
 Recall: 0.474632
 IoU: 0.385328

We have finished training iteration 102
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_100_.pth
per-ex loss: 0.372275  [    2/   88]
per-ex loss: 0.378885  [    4/   88]
per-ex loss: 0.464886  [    6/   88]
per-ex loss: 0.365063  [    8/   88]
per-ex loss: 0.480037  [   10/   88]
per-ex loss: 0.390008  [   12/   88]
per-ex loss: 0.581981  [   14/   88]
per-ex loss: 0.340517  [   16/   88]
per-ex loss: 0.351027  [   18/   88]
per-ex loss: 0.473082  [   20/   88]
per-ex loss: 0.350311  [   22/   88]
per-ex loss: 0.447769  [   24/   88]
per-ex loss: 0.466491  [   26/   88]
per-ex loss: 0.418976  [   28/   88]
per-ex loss: 0.437053  [   30/   88]
per-ex loss: 0.452697  [   32/   88]
per-ex loss: 0.361916  [   34/   88]
per-ex loss: 0.365469  [   36/   88]
per-ex loss: 0.366192  [   38/   88]
per-ex loss: 0.463857  [   40/   88]
per-ex loss: 0.485956  [   42/   88]
per-ex loss: 0.448366  [   44/   88]
per-ex loss: 0.462363  [   46/   88]
per-ex loss: 0.333480  [   48/   88]
per-ex loss: 0.398713  [   50/   88]
per-ex loss: 0.397501  [   52/   88]
per-ex loss: 0.326402  [   54/   88]
per-ex loss: 0.533020  [   56/   88]
per-ex loss: 0.623861  [   58/   88]
per-ex loss: 0.446352  [   60/   88]
per-ex loss: 0.390947  [   62/   88]
per-ex loss: 0.644297  [   64/   88]
per-ex loss: 0.486031  [   66/   88]
per-ex loss: 0.367992  [   68/   88]
per-ex loss: 0.499306  [   70/   88]
per-ex loss: 0.440911  [   72/   88]
per-ex loss: 0.495299  [   74/   88]
per-ex loss: 0.409808  [   76/   88]
per-ex loss: 0.597974  [   78/   88]
per-ex loss: 0.509336  [   80/   88]
per-ex loss: 0.571219  [   82/   88]
per-ex loss: 0.347775  [   84/   88]
per-ex loss: 0.533159  [   86/   88]
per-ex loss: 0.370503  [   88/   88]
Train Error: Avg loss: 0.44202416
validation Error: 
 Avg loss: 0.51011203 
 F1: 0.517032 
 Precision: 0.564835 
 Recall: 0.476690
 IoU: 0.348647

test Error: 
 Avg loss: 0.46829341 
 F1: 0.569673 
 Precision: 0.615190 
 Recall: 0.530427
 IoU: 0.398281

We have finished training iteration 103
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_101_.pth
per-ex loss: 0.485528  [    2/   88]
per-ex loss: 0.402028  [    4/   88]
per-ex loss: 0.517280  [    6/   88]
per-ex loss: 0.514025  [    8/   88]
per-ex loss: 0.567732  [   10/   88]
per-ex loss: 0.353649  [   12/   88]
per-ex loss: 0.354488  [   14/   88]
per-ex loss: 0.374820  [   16/   88]
per-ex loss: 0.560931  [   18/   88]
per-ex loss: 0.409003  [   20/   88]
per-ex loss: 0.577917  [   22/   88]
per-ex loss: 0.368948  [   24/   88]
per-ex loss: 0.319367  [   26/   88]
per-ex loss: 0.459941  [   28/   88]
per-ex loss: 0.488399  [   30/   88]
per-ex loss: 0.440971  [   32/   88]
per-ex loss: 0.373830  [   34/   88]
per-ex loss: 0.534758  [   36/   88]
per-ex loss: 0.407653  [   38/   88]
per-ex loss: 0.354998  [   40/   88]
per-ex loss: 0.444187  [   42/   88]
per-ex loss: 0.431433  [   44/   88]
per-ex loss: 0.418603  [   46/   88]
per-ex loss: 0.334029  [   48/   88]
per-ex loss: 0.319394  [   50/   88]
per-ex loss: 0.517892  [   52/   88]
per-ex loss: 0.368962  [   54/   88]
per-ex loss: 0.453528  [   56/   88]
per-ex loss: 0.468446  [   58/   88]
per-ex loss: 0.372445  [   60/   88]
per-ex loss: 0.601486  [   62/   88]
per-ex loss: 0.414786  [   64/   88]
per-ex loss: 0.424981  [   66/   88]
per-ex loss: 0.405530  [   68/   88]
per-ex loss: 0.377447  [   70/   88]
per-ex loss: 0.485461  [   72/   88]
per-ex loss: 0.381688  [   74/   88]
per-ex loss: 0.505815  [   76/   88]
per-ex loss: 0.369792  [   78/   88]
per-ex loss: 0.559845  [   80/   88]
per-ex loss: 0.371124  [   82/   88]
per-ex loss: 0.553881  [   84/   88]
per-ex loss: 0.466591  [   86/   88]
per-ex loss: 0.465189  [   88/   88]
Train Error: Avg loss: 0.44042725
validation Error: 
 Avg loss: 0.53596615 
 F1: 0.493487 
 Precision: 0.607059 
 Recall: 0.415713
 IoU: 0.327569

test Error: 
 Avg loss: 0.48703530 
 F1: 0.547708 
 Precision: 0.668114 
 Recall: 0.464074
 IoU: 0.377134

We have finished training iteration 104
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_102_.pth
per-ex loss: 0.414600  [    2/   88]
per-ex loss: 0.379820  [    4/   88]
per-ex loss: 0.585034  [    6/   88]
per-ex loss: 0.501984  [    8/   88]
per-ex loss: 0.324693  [   10/   88]
per-ex loss: 0.428021  [   12/   88]
per-ex loss: 0.484822  [   14/   88]
per-ex loss: 0.468310  [   16/   88]
per-ex loss: 0.399400  [   18/   88]
per-ex loss: 0.354489  [   20/   88]
per-ex loss: 0.535379  [   22/   88]
per-ex loss: 0.399976  [   24/   88]
per-ex loss: 0.448624  [   26/   88]
per-ex loss: 0.324707  [   28/   88]
per-ex loss: 0.406717  [   30/   88]
per-ex loss: 0.393410  [   32/   88]
per-ex loss: 0.542545  [   34/   88]
per-ex loss: 0.494612  [   36/   88]
per-ex loss: 0.399711  [   38/   88]
per-ex loss: 0.415545  [   40/   88]
per-ex loss: 0.487833  [   42/   88]
per-ex loss: 0.448239  [   44/   88]
per-ex loss: 0.504561  [   46/   88]
per-ex loss: 0.355913  [   48/   88]
per-ex loss: 0.471286  [   50/   88]
per-ex loss: 0.364089  [   52/   88]
per-ex loss: 0.399553  [   54/   88]
per-ex loss: 0.473505  [   56/   88]
per-ex loss: 0.484870  [   58/   88]
per-ex loss: 0.400351  [   60/   88]
per-ex loss: 0.385797  [   62/   88]
per-ex loss: 0.466967  [   64/   88]
per-ex loss: 0.375195  [   66/   88]
per-ex loss: 0.387326  [   68/   88]
per-ex loss: 0.640744  [   70/   88]
per-ex loss: 0.439385  [   72/   88]
per-ex loss: 0.421663  [   74/   88]
per-ex loss: 0.439182  [   76/   88]
per-ex loss: 0.496185  [   78/   88]
per-ex loss: 0.504397  [   80/   88]
per-ex loss: 0.394913  [   82/   88]
per-ex loss: 0.555238  [   84/   88]
per-ex loss: 0.509275  [   86/   88]
per-ex loss: 0.505300  [   88/   88]
Train Error: Avg loss: 0.44577650
validation Error: 
 Avg loss: 0.52587168 
 F1: 0.505886 
 Precision: 0.481869 
 Recall: 0.532422
 IoU: 0.338586

test Error: 
 Avg loss: 0.49878115 
 F1: 0.534457 
 Precision: 0.482103 
 Recall: 0.599566
 IoU: 0.364681

We have finished training iteration 105
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_103_.pth
per-ex loss: 0.431336  [    2/   88]
per-ex loss: 0.420049  [    4/   88]
per-ex loss: 0.421434  [    6/   88]
per-ex loss: 0.391778  [    8/   88]
per-ex loss: 0.449403  [   10/   88]
per-ex loss: 0.559930  [   12/   88]
per-ex loss: 0.340450  [   14/   88]
per-ex loss: 0.448805  [   16/   88]
per-ex loss: 0.429069  [   18/   88]
per-ex loss: 0.566773  [   20/   88]
per-ex loss: 0.336764  [   22/   88]
per-ex loss: 0.351550  [   24/   88]
per-ex loss: 0.411758  [   26/   88]
per-ex loss: 0.533697  [   28/   88]
per-ex loss: 0.404955  [   30/   88]
per-ex loss: 0.426975  [   32/   88]
per-ex loss: 0.609000  [   34/   88]
per-ex loss: 0.365494  [   36/   88]
per-ex loss: 0.361009  [   38/   88]
per-ex loss: 0.608948  [   40/   88]
per-ex loss: 0.581461  [   42/   88]
per-ex loss: 0.593624  [   44/   88]
per-ex loss: 0.578255  [   46/   88]
per-ex loss: 0.440788  [   48/   88]
per-ex loss: 0.484516  [   50/   88]
per-ex loss: 0.394290  [   52/   88]
per-ex loss: 0.475172  [   54/   88]
per-ex loss: 0.393261  [   56/   88]
per-ex loss: 0.502976  [   58/   88]
per-ex loss: 0.661906  [   60/   88]
per-ex loss: 0.536975  [   62/   88]
per-ex loss: 0.366804  [   64/   88]
per-ex loss: 0.355782  [   66/   88]
per-ex loss: 0.422003  [   68/   88]
per-ex loss: 0.419801  [   70/   88]
per-ex loss: 0.415821  [   72/   88]
per-ex loss: 0.370174  [   74/   88]
per-ex loss: 0.480023  [   76/   88]
per-ex loss: 0.352813  [   78/   88]
per-ex loss: 0.335114  [   80/   88]
per-ex loss: 0.479807  [   82/   88]
per-ex loss: 0.372808  [   84/   88]
per-ex loss: 0.491432  [   86/   88]
per-ex loss: 0.457902  [   88/   88]
Train Error: Avg loss: 0.45074287
validation Error: 
 Avg loss: 0.52732850 
 F1: 0.500810 
 Precision: 0.604306 
 Recall: 0.427580
 IoU: 0.334053

test Error: 
 Avg loss: 0.48751510 
 F1: 0.548257 
 Precision: 0.677094 
 Recall: 0.460612
 IoU: 0.377654

We have finished training iteration 106
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_104_.pth
per-ex loss: 0.580342  [    2/   88]
per-ex loss: 0.423175  [    4/   88]
per-ex loss: 0.525476  [    6/   88]
per-ex loss: 0.336379  [    8/   88]
per-ex loss: 0.540065  [   10/   88]
per-ex loss: 0.416118  [   12/   88]
per-ex loss: 0.383698  [   14/   88]
per-ex loss: 0.362702  [   16/   88]
per-ex loss: 0.342247  [   18/   88]
per-ex loss: 0.609988  [   20/   88]
per-ex loss: 0.399791  [   22/   88]
per-ex loss: 0.413844  [   24/   88]
per-ex loss: 0.594777  [   26/   88]
per-ex loss: 0.370604  [   28/   88]
per-ex loss: 0.431814  [   30/   88]
per-ex loss: 0.445850  [   32/   88]
per-ex loss: 0.454172  [   34/   88]
per-ex loss: 0.339067  [   36/   88]
per-ex loss: 0.348543  [   38/   88]
per-ex loss: 0.549503  [   40/   88]
per-ex loss: 0.454286  [   42/   88]
per-ex loss: 0.370717  [   44/   88]
per-ex loss: 0.442771  [   46/   88]
per-ex loss: 0.436818  [   48/   88]
per-ex loss: 0.517336  [   50/   88]
per-ex loss: 0.415518  [   52/   88]
per-ex loss: 0.478863  [   54/   88]
per-ex loss: 0.473782  [   56/   88]
per-ex loss: 0.519163  [   58/   88]
per-ex loss: 0.475095  [   60/   88]
per-ex loss: 0.489251  [   62/   88]
per-ex loss: 0.408703  [   64/   88]
per-ex loss: 0.519450  [   66/   88]
per-ex loss: 0.402459  [   68/   88]
per-ex loss: 0.517177  [   70/   88]
per-ex loss: 0.355392  [   72/   88]
per-ex loss: 0.472660  [   74/   88]
per-ex loss: 0.563551  [   76/   88]
per-ex loss: 0.344324  [   78/   88]
per-ex loss: 0.354808  [   80/   88]
per-ex loss: 0.511311  [   82/   88]
per-ex loss: 0.571872  [   84/   88]
per-ex loss: 0.380494  [   86/   88]
per-ex loss: 0.354476  [   88/   88]
Train Error: Avg loss: 0.44769165
validation Error: 
 Avg loss: 0.52912921 
 F1: 0.496317 
 Precision: 0.643720 
 Recall: 0.403843
 IoU: 0.330068

test Error: 
 Avg loss: 0.49370206 
 F1: 0.543958 
 Precision: 0.692794 
 Recall: 0.447763
 IoU: 0.373587

We have finished training iteration 107
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_105_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.494494  [    2/   88]
per-ex loss: 0.422901  [    4/   88]
per-ex loss: 0.473826  [    6/   88]
per-ex loss: 0.382014  [    8/   88]
per-ex loss: 0.582706  [   10/   88]
per-ex loss: 0.375205  [   12/   88]
per-ex loss: 0.378774  [   14/   88]
per-ex loss: 0.349921  [   16/   88]
per-ex loss: 0.400048  [   18/   88]
per-ex loss: 0.376189  [   20/   88]
per-ex loss: 0.530665  [   22/   88]
per-ex loss: 0.416602  [   24/   88]
per-ex loss: 0.388802  [   26/   88]
per-ex loss: 0.339294  [   28/   88]
per-ex loss: 0.491657  [   30/   88]
per-ex loss: 0.366651  [   32/   88]
per-ex loss: 0.442043  [   34/   88]
per-ex loss: 0.460018  [   36/   88]
per-ex loss: 0.482993  [   38/   88]
per-ex loss: 0.481460  [   40/   88]
per-ex loss: 0.526117  [   42/   88]
per-ex loss: 0.363059  [   44/   88]
per-ex loss: 0.447997  [   46/   88]
per-ex loss: 0.373190  [   48/   88]
per-ex loss: 0.508174  [   50/   88]
per-ex loss: 0.437420  [   52/   88]
per-ex loss: 0.531634  [   54/   88]
per-ex loss: 0.441961  [   56/   88]
per-ex loss: 0.433557  [   58/   88]
per-ex loss: 0.500468  [   60/   88]
per-ex loss: 0.378995  [   62/   88]
per-ex loss: 0.615031  [   64/   88]
per-ex loss: 0.378786  [   66/   88]
per-ex loss: 0.338767  [   68/   88]
per-ex loss: 0.569543  [   70/   88]
per-ex loss: 0.321571  [   72/   88]
per-ex loss: 0.401456  [   74/   88]
per-ex loss: 0.381550  [   76/   88]
per-ex loss: 0.374643  [   78/   88]
per-ex loss: 0.403415  [   80/   88]
per-ex loss: 0.649960  [   82/   88]
per-ex loss: 0.596704  [   84/   88]
per-ex loss: 0.382853  [   86/   88]
per-ex loss: 0.575162  [   88/   88]
Train Error: Avg loss: 0.44473356
validation Error: 
 Avg loss: 0.52152519 
 F1: 0.506087 
 Precision: 0.624674 
 Recall: 0.425341
 IoU: 0.338766

test Error: 
 Avg loss: 0.48768605 
 F1: 0.550000 
 Precision: 0.653511 
 Recall: 0.474796
 IoU: 0.379311

We have finished training iteration 108
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_106_.pth
per-ex loss: 0.384135  [    2/   88]
per-ex loss: 0.553805  [    4/   88]
per-ex loss: 0.435743  [    6/   88]
per-ex loss: 0.484715  [    8/   88]
per-ex loss: 0.489731  [   10/   88]
per-ex loss: 0.409467  [   12/   88]
per-ex loss: 0.375274  [   14/   88]
per-ex loss: 0.335077  [   16/   88]
per-ex loss: 0.556965  [   18/   88]
per-ex loss: 0.430438  [   20/   88]
per-ex loss: 0.545764  [   22/   88]
per-ex loss: 0.355919  [   24/   88]
per-ex loss: 0.452144  [   26/   88]
per-ex loss: 0.370457  [   28/   88]
per-ex loss: 0.641641  [   30/   88]
per-ex loss: 0.452969  [   32/   88]
per-ex loss: 0.326560  [   34/   88]
per-ex loss: 0.353081  [   36/   88]
per-ex loss: 0.398540  [   38/   88]
per-ex loss: 0.510637  [   40/   88]
per-ex loss: 0.421899  [   42/   88]
per-ex loss: 0.383641  [   44/   88]
per-ex loss: 0.504306  [   46/   88]
per-ex loss: 0.413852  [   48/   88]
per-ex loss: 0.432583  [   50/   88]
per-ex loss: 0.462309  [   52/   88]
per-ex loss: 0.497575  [   54/   88]
per-ex loss: 0.444185  [   56/   88]
per-ex loss: 0.527025  [   58/   88]
per-ex loss: 0.439772  [   60/   88]
per-ex loss: 0.416990  [   62/   88]
per-ex loss: 0.467538  [   64/   88]
per-ex loss: 0.409103  [   66/   88]
per-ex loss: 0.334556  [   68/   88]
per-ex loss: 0.551143  [   70/   88]
per-ex loss: 0.584406  [   72/   88]
per-ex loss: 0.416901  [   74/   88]
per-ex loss: 0.375494  [   76/   88]
per-ex loss: 0.401585  [   78/   88]
per-ex loss: 0.439064  [   80/   88]
per-ex loss: 0.469688  [   82/   88]
per-ex loss: 0.383552  [   84/   88]
per-ex loss: 0.405551  [   86/   88]
per-ex loss: 0.350499  [   88/   88]
Train Error: Avg loss: 0.44082451
validation Error: 
 Avg loss: 0.53000953 
 F1: 0.507958 
 Precision: 0.584749 
 Recall: 0.448994
 IoU: 0.340444

test Error: 
 Avg loss: 0.48586395 
 F1: 0.549604 
 Precision: 0.611089 
 Recall: 0.499360
 IoU: 0.378933

We have finished training iteration 109
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_107_.pth
per-ex loss: 0.348162  [    2/   88]
per-ex loss: 0.538580  [    4/   88]
per-ex loss: 0.442557  [    6/   88]
per-ex loss: 0.382153  [    8/   88]
per-ex loss: 0.329792  [   10/   88]
per-ex loss: 0.543004  [   12/   88]
per-ex loss: 0.361419  [   14/   88]
per-ex loss: 0.534407  [   16/   88]
per-ex loss: 0.434545  [   18/   88]
per-ex loss: 0.355761  [   20/   88]
per-ex loss: 0.361909  [   22/   88]
per-ex loss: 0.403900  [   24/   88]
per-ex loss: 0.365864  [   26/   88]
per-ex loss: 0.361754  [   28/   88]
per-ex loss: 0.609120  [   30/   88]
per-ex loss: 0.436569  [   32/   88]
per-ex loss: 0.445295  [   34/   88]
per-ex loss: 0.458476  [   36/   88]
per-ex loss: 0.422977  [   38/   88]
per-ex loss: 0.383276  [   40/   88]
per-ex loss: 0.557736  [   42/   88]
per-ex loss: 0.443378  [   44/   88]
per-ex loss: 0.419354  [   46/   88]
per-ex loss: 0.417875  [   48/   88]
per-ex loss: 0.409301  [   50/   88]
per-ex loss: 0.363749  [   52/   88]
per-ex loss: 0.418745  [   54/   88]
per-ex loss: 0.462711  [   56/   88]
per-ex loss: 0.451665  [   58/   88]
per-ex loss: 0.555508  [   60/   88]
per-ex loss: 0.396642  [   62/   88]
per-ex loss: 0.374789  [   64/   88]
per-ex loss: 0.492279  [   66/   88]
per-ex loss: 0.364830  [   68/   88]
per-ex loss: 0.464957  [   70/   88]
per-ex loss: 0.513391  [   72/   88]
per-ex loss: 0.483198  [   74/   88]
per-ex loss: 0.573553  [   76/   88]
per-ex loss: 0.398125  [   78/   88]
per-ex loss: 0.491970  [   80/   88]
per-ex loss: 0.388570  [   82/   88]
per-ex loss: 0.436657  [   84/   88]
per-ex loss: 0.498312  [   86/   88]
per-ex loss: 0.445570  [   88/   88]
Train Error: Avg loss: 0.43959971
validation Error: 
 Avg loss: 0.51473142 
 F1: 0.515325 
 Precision: 0.515650 
 Recall: 0.514999
 IoU: 0.347096

test Error: 
 Avg loss: 0.48339561 
 F1: 0.548369 
 Precision: 0.537449 
 Recall: 0.559741
 IoU: 0.377760

We have finished training iteration 110
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_108_.pth
per-ex loss: 0.412510  [    2/   88]
per-ex loss: 0.486579  [    4/   88]
per-ex loss: 0.368484  [    6/   88]
per-ex loss: 0.502823  [    8/   88]
per-ex loss: 0.587872  [   10/   88]
per-ex loss: 0.423825  [   12/   88]
per-ex loss: 0.417000  [   14/   88]
per-ex loss: 0.399401  [   16/   88]
per-ex loss: 0.388536  [   18/   88]
per-ex loss: 0.376011  [   20/   88]
per-ex loss: 0.378463  [   22/   88]
per-ex loss: 0.456894  [   24/   88]
per-ex loss: 0.439340  [   26/   88]
per-ex loss: 0.642402  [   28/   88]
per-ex loss: 0.390693  [   30/   88]
per-ex loss: 0.455077  [   32/   88]
per-ex loss: 0.399320  [   34/   88]
per-ex loss: 0.503604  [   36/   88]
per-ex loss: 0.534187  [   38/   88]
per-ex loss: 0.357677  [   40/   88]
per-ex loss: 0.434225  [   42/   88]
per-ex loss: 0.423583  [   44/   88]
per-ex loss: 0.393863  [   46/   88]
per-ex loss: 0.358253  [   48/   88]
per-ex loss: 0.565104  [   50/   88]
per-ex loss: 0.356199  [   52/   88]
per-ex loss: 0.353181  [   54/   88]
per-ex loss: 0.475572  [   56/   88]
per-ex loss: 0.384263  [   58/   88]
per-ex loss: 0.351806  [   60/   88]
per-ex loss: 0.395971  [   62/   88]
per-ex loss: 0.370687  [   64/   88]
per-ex loss: 0.459397  [   66/   88]
per-ex loss: 0.512673  [   68/   88]
per-ex loss: 0.548447  [   70/   88]
per-ex loss: 0.359763  [   72/   88]
per-ex loss: 0.442381  [   74/   88]
per-ex loss: 0.497130  [   76/   88]
per-ex loss: 0.552479  [   78/   88]
per-ex loss: 0.391299  [   80/   88]
per-ex loss: 0.546576  [   82/   88]
per-ex loss: 0.432221  [   84/   88]
per-ex loss: 0.403648  [   86/   88]
per-ex loss: 0.577929  [   88/   88]
Train Error: Avg loss: 0.44334888
validation Error: 
 Avg loss: 0.51740792 
 F1: 0.504404 
 Precision: 0.509561 
 Recall: 0.499350
 IoU: 0.337259

test Error: 
 Avg loss: 0.48596353 
 F1: 0.552104 
 Precision: 0.555598 
 Recall: 0.548653
 IoU: 0.381314

We have finished training iteration 111
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_109_.pth
per-ex loss: 0.326718  [    2/   88]
per-ex loss: 0.416244  [    4/   88]
per-ex loss: 0.464136  [    6/   88]
per-ex loss: 0.512698  [    8/   88]
per-ex loss: 0.567891  [   10/   88]
per-ex loss: 0.445092  [   12/   88]
per-ex loss: 0.518254  [   14/   88]
per-ex loss: 0.380613  [   16/   88]
per-ex loss: 0.316897  [   18/   88]
per-ex loss: 0.391776  [   20/   88]
per-ex loss: 0.409837  [   22/   88]
per-ex loss: 0.436843  [   24/   88]
per-ex loss: 0.499923  [   26/   88]
per-ex loss: 0.573097  [   28/   88]
per-ex loss: 0.366436  [   30/   88]
per-ex loss: 0.489142  [   32/   88]
per-ex loss: 0.313579  [   34/   88]
per-ex loss: 0.356912  [   36/   88]
per-ex loss: 0.411476  [   38/   88]
per-ex loss: 0.524229  [   40/   88]
per-ex loss: 0.395694  [   42/   88]
per-ex loss: 0.371930  [   44/   88]
per-ex loss: 0.418955  [   46/   88]
per-ex loss: 0.608985  [   48/   88]
per-ex loss: 0.331140  [   50/   88]
per-ex loss: 0.435875  [   52/   88]
per-ex loss: 0.431492  [   54/   88]
per-ex loss: 0.360639  [   56/   88]
per-ex loss: 0.433959  [   58/   88]
per-ex loss: 0.482498  [   60/   88]
per-ex loss: 0.370859  [   62/   88]
per-ex loss: 0.459297  [   64/   88]
per-ex loss: 0.389972  [   66/   88]
per-ex loss: 0.547616  [   68/   88]
per-ex loss: 0.408384  [   70/   88]
per-ex loss: 0.531638  [   72/   88]
per-ex loss: 0.382830  [   74/   88]
per-ex loss: 0.397476  [   76/   88]
per-ex loss: 0.532018  [   78/   88]
per-ex loss: 0.563391  [   80/   88]
per-ex loss: 0.448196  [   82/   88]
per-ex loss: 0.418798  [   84/   88]
per-ex loss: 0.361376  [   86/   88]
per-ex loss: 0.584139  [   88/   88]
Train Error: Avg loss: 0.44065800
validation Error: 
 Avg loss: 0.51748295 
 F1: 0.503101 
 Precision: 0.526996 
 Recall: 0.481279
 IoU: 0.336096

test Error: 
 Avg loss: 0.47755054 
 F1: 0.556649 
 Precision: 0.595655 
 Recall: 0.522437
 IoU: 0.385664

We have finished training iteration 112
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_110_.pth
per-ex loss: 0.388489  [    2/   88]
per-ex loss: 0.327683  [    4/   88]
per-ex loss: 0.413666  [    6/   88]
per-ex loss: 0.521940  [    8/   88]
per-ex loss: 0.350289  [   10/   88]
per-ex loss: 0.423234  [   12/   88]
per-ex loss: 0.386673  [   14/   88]
per-ex loss: 0.310024  [   16/   88]
per-ex loss: 0.389047  [   18/   88]
per-ex loss: 0.587716  [   20/   88]
per-ex loss: 0.445608  [   22/   88]
per-ex loss: 0.533270  [   24/   88]
per-ex loss: 0.407262  [   26/   88]
per-ex loss: 0.593653  [   28/   88]
per-ex loss: 0.388203  [   30/   88]
per-ex loss: 0.388597  [   32/   88]
per-ex loss: 0.495174  [   34/   88]
per-ex loss: 0.475444  [   36/   88]
per-ex loss: 0.448542  [   38/   88]
per-ex loss: 0.488475  [   40/   88]
per-ex loss: 0.386276  [   42/   88]
per-ex loss: 0.409091  [   44/   88]
per-ex loss: 0.377270  [   46/   88]
per-ex loss: 0.580849  [   48/   88]
per-ex loss: 0.534553  [   50/   88]
per-ex loss: 0.434977  [   52/   88]
per-ex loss: 0.381220  [   54/   88]
per-ex loss: 0.379457  [   56/   88]
per-ex loss: 0.558180  [   58/   88]
per-ex loss: 0.577685  [   60/   88]
per-ex loss: 0.391233  [   62/   88]
per-ex loss: 0.416056  [   64/   88]
per-ex loss: 0.537629  [   66/   88]
per-ex loss: 0.515479  [   68/   88]
per-ex loss: 0.323556  [   70/   88]
per-ex loss: 0.510847  [   72/   88]
per-ex loss: 0.322360  [   74/   88]
per-ex loss: 0.548671  [   76/   88]
per-ex loss: 0.426192  [   78/   88]
per-ex loss: 0.487453  [   80/   88]
per-ex loss: 0.456554  [   82/   88]
per-ex loss: 0.456708  [   84/   88]
per-ex loss: 0.471590  [   86/   88]
per-ex loss: 0.424603  [   88/   88]
Train Error: Avg loss: 0.44707906
validation Error: 
 Avg loss: 0.51189274 
 F1: 0.507303 
 Precision: 0.566966 
 Recall: 0.459002
 IoU: 0.339857

test Error: 
 Avg loss: 0.48042424 
 F1: 0.556610 
 Precision: 0.640482 
 Recall: 0.492160
 IoU: 0.385627

We have finished training iteration 113
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_111_.pth
per-ex loss: 0.436087  [    2/   88]
per-ex loss: 0.398991  [    4/   88]
per-ex loss: 0.391127  [    6/   88]
per-ex loss: 0.360581  [    8/   88]
per-ex loss: 0.344289  [   10/   88]
per-ex loss: 0.565580  [   12/   88]
per-ex loss: 0.429357  [   14/   88]
per-ex loss: 0.417218  [   16/   88]
per-ex loss: 0.373430  [   18/   88]
per-ex loss: 0.461592  [   20/   88]
per-ex loss: 0.330672  [   22/   88]
per-ex loss: 0.571103  [   24/   88]
per-ex loss: 0.414621  [   26/   88]
per-ex loss: 0.414762  [   28/   88]
per-ex loss: 0.385642  [   30/   88]
per-ex loss: 0.448923  [   32/   88]
per-ex loss: 0.412067  [   34/   88]
per-ex loss: 0.545289  [   36/   88]
per-ex loss: 0.371649  [   38/   88]
per-ex loss: 0.461089  [   40/   88]
per-ex loss: 0.425262  [   42/   88]
per-ex loss: 0.527311  [   44/   88]
per-ex loss: 0.490572  [   46/   88]
per-ex loss: 0.449655  [   48/   88]
per-ex loss: 0.376455  [   50/   88]
per-ex loss: 0.356069  [   52/   88]
per-ex loss: 0.485442  [   54/   88]
per-ex loss: 0.539017  [   56/   88]
per-ex loss: 0.339368  [   58/   88]
per-ex loss: 0.375670  [   60/   88]
per-ex loss: 0.548864  [   62/   88]
per-ex loss: 0.529411  [   64/   88]
per-ex loss: 0.383110  [   66/   88]
per-ex loss: 0.447345  [   68/   88]
per-ex loss: 0.415326  [   70/   88]
per-ex loss: 0.507586  [   72/   88]
per-ex loss: 0.480093  [   74/   88]
per-ex loss: 0.380533  [   76/   88]
per-ex loss: 0.400265  [   78/   88]
per-ex loss: 0.435440  [   80/   88]
per-ex loss: 0.358082  [   82/   88]
per-ex loss: 0.390380  [   84/   88]
per-ex loss: 0.618854  [   86/   88]
per-ex loss: 0.599847  [   88/   88]
Train Error: Avg loss: 0.44077333
validation Error: 
 Avg loss: 0.51214496 
 F1: 0.506668 
 Precision: 0.654005 
 Recall: 0.413510
 IoU: 0.339287

test Error: 
 Avg loss: 0.48414605 
 F1: 0.552402 
 Precision: 0.709037 
 Recall: 0.452450
 IoU: 0.381599

We have finished training iteration 114
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_112_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.523921  [    2/   88]
per-ex loss: 0.442019  [    4/   88]
per-ex loss: 0.373139  [    6/   88]
per-ex loss: 0.400406  [    8/   88]
per-ex loss: 0.581400  [   10/   88]
per-ex loss: 0.549482  [   12/   88]
per-ex loss: 0.342488  [   14/   88]
per-ex loss: 0.381301  [   16/   88]
per-ex loss: 0.515634  [   18/   88]
per-ex loss: 0.556147  [   20/   88]
per-ex loss: 0.430348  [   22/   88]
per-ex loss: 0.422105  [   24/   88]
per-ex loss: 0.431984  [   26/   88]
per-ex loss: 0.463906  [   28/   88]
per-ex loss: 0.479515  [   30/   88]
per-ex loss: 0.389075  [   32/   88]
per-ex loss: 0.414895  [   34/   88]
per-ex loss: 0.372985  [   36/   88]
per-ex loss: 0.473825  [   38/   88]
per-ex loss: 0.383207  [   40/   88]
per-ex loss: 0.387758  [   42/   88]
per-ex loss: 0.397651  [   44/   88]
per-ex loss: 0.672015  [   46/   88]
per-ex loss: 0.352350  [   48/   88]
per-ex loss: 0.497101  [   50/   88]
per-ex loss: 0.418921  [   52/   88]
per-ex loss: 0.516737  [   54/   88]
per-ex loss: 0.419967  [   56/   88]
per-ex loss: 0.331621  [   58/   88]
per-ex loss: 0.352182  [   60/   88]
per-ex loss: 0.339218  [   62/   88]
per-ex loss: 0.385528  [   64/   88]
per-ex loss: 0.459700  [   66/   88]
per-ex loss: 0.384253  [   68/   88]
per-ex loss: 0.399130  [   70/   88]
per-ex loss: 0.574720  [   72/   88]
per-ex loss: 0.378387  [   74/   88]
per-ex loss: 0.533871  [   76/   88]
per-ex loss: 0.488281  [   78/   88]
per-ex loss: 0.430078  [   80/   88]
per-ex loss: 0.519254  [   82/   88]
per-ex loss: 0.293213  [   84/   88]
per-ex loss: 0.364871  [   86/   88]
per-ex loss: 0.470849  [   88/   88]
Train Error: Avg loss: 0.43853267
validation Error: 
 Avg loss: 0.52156411 
 F1: 0.502493 
 Precision: 0.592141 
 Recall: 0.436420
 IoU: 0.335553

test Error: 
 Avg loss: 0.49237681 
 F1: 0.540267 
 Precision: 0.644334 
 Recall: 0.465141
 IoU: 0.370113

We have finished training iteration 115
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_113_.pth
per-ex loss: 0.384005  [    2/   88]
per-ex loss: 0.538797  [    4/   88]
per-ex loss: 0.540121  [    6/   88]
per-ex loss: 0.519428  [    8/   88]
per-ex loss: 0.371501  [   10/   88]
per-ex loss: 0.450099  [   12/   88]
per-ex loss: 0.423392  [   14/   88]
per-ex loss: 0.405329  [   16/   88]
per-ex loss: 0.383891  [   18/   88]
per-ex loss: 0.328247  [   20/   88]
per-ex loss: 0.357544  [   22/   88]
per-ex loss: 0.479045  [   24/   88]
per-ex loss: 0.658337  [   26/   88]
per-ex loss: 0.392065  [   28/   88]
per-ex loss: 0.370114  [   30/   88]
per-ex loss: 0.475421  [   32/   88]
per-ex loss: 0.368126  [   34/   88]
per-ex loss: 0.593031  [   36/   88]
per-ex loss: 0.337929  [   38/   88]
per-ex loss: 0.450502  [   40/   88]
per-ex loss: 0.422675  [   42/   88]
per-ex loss: 0.364869  [   44/   88]
per-ex loss: 0.469876  [   46/   88]
per-ex loss: 0.456694  [   48/   88]
per-ex loss: 0.332402  [   50/   88]
per-ex loss: 0.557890  [   52/   88]
per-ex loss: 0.395313  [   54/   88]
per-ex loss: 0.389251  [   56/   88]
per-ex loss: 0.405602  [   58/   88]
per-ex loss: 0.331256  [   60/   88]
per-ex loss: 0.324933  [   62/   88]
per-ex loss: 0.557107  [   64/   88]
per-ex loss: 0.453281  [   66/   88]
per-ex loss: 0.522292  [   68/   88]
per-ex loss: 0.374510  [   70/   88]
per-ex loss: 0.388051  [   72/   88]
per-ex loss: 0.442693  [   74/   88]
per-ex loss: 0.456372  [   76/   88]
per-ex loss: 0.542068  [   78/   88]
per-ex loss: 0.560360  [   80/   88]
per-ex loss: 0.474885  [   82/   88]
per-ex loss: 0.619034  [   84/   88]
per-ex loss: 0.421815  [   86/   88]
per-ex loss: 0.635412  [   88/   88]
Train Error: Avg loss: 0.44830830
validation Error: 
 Avg loss: 0.54170456 
 F1: 0.473630 
 Precision: 0.433028 
 Recall: 0.522633
 IoU: 0.310298

test Error: 
 Avg loss: 0.50212651 
 F1: 0.530873 
 Precision: 0.477783 
 Recall: 0.597237
 IoU: 0.361353

We have finished training iteration 116
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_114_.pth
per-ex loss: 0.549239  [    2/   88]
per-ex loss: 0.548294  [    4/   88]
per-ex loss: 0.378579  [    6/   88]
per-ex loss: 0.610555  [    8/   88]
per-ex loss: 0.556983  [   10/   88]
per-ex loss: 0.356917  [   12/   88]
per-ex loss: 0.392364  [   14/   88]
per-ex loss: 0.525825  [   16/   88]
per-ex loss: 0.382996  [   18/   88]
per-ex loss: 0.377135  [   20/   88]
per-ex loss: 0.637536  [   22/   88]
per-ex loss: 0.375266  [   24/   88]
per-ex loss: 0.573486  [   26/   88]
per-ex loss: 0.476310  [   28/   88]
per-ex loss: 0.348325  [   30/   88]
per-ex loss: 0.513487  [   32/   88]
per-ex loss: 0.574743  [   34/   88]
per-ex loss: 0.474744  [   36/   88]
per-ex loss: 0.367905  [   38/   88]
per-ex loss: 0.369894  [   40/   88]
per-ex loss: 0.516820  [   42/   88]
per-ex loss: 0.381699  [   44/   88]
per-ex loss: 0.465658  [   46/   88]
per-ex loss: 0.458867  [   48/   88]
per-ex loss: 0.524822  [   50/   88]
per-ex loss: 0.355687  [   52/   88]
per-ex loss: 0.607523  [   54/   88]
per-ex loss: 0.463992  [   56/   88]
per-ex loss: 0.395943  [   58/   88]
per-ex loss: 0.511559  [   60/   88]
per-ex loss: 0.386083  [   62/   88]
per-ex loss: 0.352863  [   64/   88]
per-ex loss: 0.370294  [   66/   88]
per-ex loss: 0.394679  [   68/   88]
per-ex loss: 0.405348  [   70/   88]
per-ex loss: 0.391149  [   72/   88]
per-ex loss: 0.445174  [   74/   88]
per-ex loss: 0.347515  [   76/   88]
per-ex loss: 0.619134  [   78/   88]
per-ex loss: 0.474752  [   80/   88]
per-ex loss: 0.488013  [   82/   88]
per-ex loss: 0.386048  [   84/   88]
per-ex loss: 0.389657  [   86/   88]
per-ex loss: 0.352905  [   88/   88]
Train Error: Avg loss: 0.45174471
validation Error: 
 Avg loss: 0.50459876 
 F1: 0.523474 
 Precision: 0.615829 
 Recall: 0.455206
 IoU: 0.354530

test Error: 
 Avg loss: 0.47521676 
 F1: 0.562568 
 Precision: 0.631458 
 Recall: 0.507231
 IoU: 0.391370

We have finished training iteration 117
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_115_.pth
per-ex loss: 0.420048  [    2/   88]
per-ex loss: 0.397924  [    4/   88]
per-ex loss: 0.372381  [    6/   88]
per-ex loss: 0.367273  [    8/   88]
per-ex loss: 0.499262  [   10/   88]
per-ex loss: 0.428480  [   12/   88]
per-ex loss: 0.427822  [   14/   88]
per-ex loss: 0.453081  [   16/   88]
per-ex loss: 0.520351  [   18/   88]
per-ex loss: 0.427478  [   20/   88]
per-ex loss: 0.406818  [   22/   88]
per-ex loss: 0.407181  [   24/   88]
per-ex loss: 0.377203  [   26/   88]
per-ex loss: 0.431259  [   28/   88]
per-ex loss: 0.440334  [   30/   88]
per-ex loss: 0.359767  [   32/   88]
per-ex loss: 0.435632  [   34/   88]
per-ex loss: 0.523194  [   36/   88]
per-ex loss: 0.383801  [   38/   88]
per-ex loss: 0.449423  [   40/   88]
per-ex loss: 0.461224  [   42/   88]
per-ex loss: 0.390226  [   44/   88]
per-ex loss: 0.398802  [   46/   88]
per-ex loss: 0.518555  [   48/   88]
per-ex loss: 0.421395  [   50/   88]
per-ex loss: 0.337074  [   52/   88]
per-ex loss: 0.367128  [   54/   88]
per-ex loss: 0.492938  [   56/   88]
per-ex loss: 0.454677  [   58/   88]
per-ex loss: 0.334241  [   60/   88]
per-ex loss: 0.603677  [   62/   88]
per-ex loss: 0.476608  [   64/   88]
per-ex loss: 0.438690  [   66/   88]
per-ex loss: 0.454649  [   68/   88]
per-ex loss: 0.430833  [   70/   88]
per-ex loss: 0.462301  [   72/   88]
per-ex loss: 0.363749  [   74/   88]
per-ex loss: 0.444659  [   76/   88]
per-ex loss: 0.486722  [   78/   88]
per-ex loss: 0.346556  [   80/   88]
per-ex loss: 0.440975  [   82/   88]
per-ex loss: 0.341257  [   84/   88]
per-ex loss: 0.400379  [   86/   88]
per-ex loss: 0.374558  [   88/   88]
Train Error: Avg loss: 0.42660419
validation Error: 
 Avg loss: 0.52763949 
 F1: 0.493719 
 Precision: 0.596536 
 Recall: 0.421134
 IoU: 0.327774

test Error: 
 Avg loss: 0.48536561 
 F1: 0.552852 
 Precision: 0.675201 
 Recall: 0.468041
 IoU: 0.382029

We have finished training iteration 118
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_116_.pth
per-ex loss: 0.393006  [    2/   88]
per-ex loss: 0.583374  [    4/   88]
per-ex loss: 0.535310  [    6/   88]
per-ex loss: 0.406921  [    8/   88]
per-ex loss: 0.436714  [   10/   88]
per-ex loss: 0.438464  [   12/   88]
per-ex loss: 0.406508  [   14/   88]
per-ex loss: 0.476562  [   16/   88]
per-ex loss: 0.386502  [   18/   88]
per-ex loss: 0.414253  [   20/   88]
per-ex loss: 0.371391  [   22/   88]
per-ex loss: 0.552123  [   24/   88]
per-ex loss: 0.385039  [   26/   88]
per-ex loss: 0.354073  [   28/   88]
per-ex loss: 0.468826  [   30/   88]
per-ex loss: 0.517744  [   32/   88]
per-ex loss: 0.398352  [   34/   88]
per-ex loss: 0.458223  [   36/   88]
per-ex loss: 0.396065  [   38/   88]
per-ex loss: 0.439240  [   40/   88]
per-ex loss: 0.404832  [   42/   88]
per-ex loss: 0.347192  [   44/   88]
per-ex loss: 0.340719  [   46/   88]
per-ex loss: 0.552068  [   48/   88]
per-ex loss: 0.513400  [   50/   88]
per-ex loss: 0.606459  [   52/   88]
per-ex loss: 0.329011  [   54/   88]
per-ex loss: 0.430653  [   56/   88]
per-ex loss: 0.372167  [   58/   88]
per-ex loss: 0.636204  [   60/   88]
per-ex loss: 0.326597  [   62/   88]
per-ex loss: 0.482757  [   64/   88]
per-ex loss: 0.378977  [   66/   88]
per-ex loss: 0.507008  [   68/   88]
per-ex loss: 0.364237  [   70/   88]
per-ex loss: 0.383074  [   72/   88]
per-ex loss: 0.330913  [   74/   88]
per-ex loss: 0.371105  [   76/   88]
per-ex loss: 0.422135  [   78/   88]
per-ex loss: 0.396970  [   80/   88]
per-ex loss: 0.369700  [   82/   88]
per-ex loss: 0.442111  [   84/   88]
per-ex loss: 0.478576  [   86/   88]
per-ex loss: 0.491916  [   88/   88]
Train Error: Avg loss: 0.43403340
validation Error: 
 Avg loss: 0.52104230 
 F1: 0.497959 
 Precision: 0.570457 
 Recall: 0.441810
 IoU: 0.331521

test Error: 
 Avg loss: 0.48602725 
 F1: 0.550430 
 Precision: 0.665386 
 Recall: 0.469344
 IoU: 0.379720

We have finished training iteration 119
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_117_.pth
per-ex loss: 0.363272  [    2/   88]
per-ex loss: 0.314489  [    4/   88]
per-ex loss: 0.377594  [    6/   88]
per-ex loss: 0.446482  [    8/   88]
per-ex loss: 0.327993  [   10/   88]
per-ex loss: 0.442081  [   12/   88]
per-ex loss: 0.433243  [   14/   88]
per-ex loss: 0.542648  [   16/   88]
per-ex loss: 0.368843  [   18/   88]
per-ex loss: 0.508219  [   20/   88]
per-ex loss: 0.451789  [   22/   88]
per-ex loss: 0.494479  [   24/   88]
per-ex loss: 0.434735  [   26/   88]
per-ex loss: 0.543765  [   28/   88]
per-ex loss: 0.436409  [   30/   88]
per-ex loss: 0.318830  [   32/   88]
per-ex loss: 0.434272  [   34/   88]
per-ex loss: 0.502098  [   36/   88]
per-ex loss: 0.419062  [   38/   88]
per-ex loss: 0.524076  [   40/   88]
per-ex loss: 0.506551  [   42/   88]
per-ex loss: 0.354964  [   44/   88]
per-ex loss: 0.384568  [   46/   88]
per-ex loss: 0.539603  [   48/   88]
per-ex loss: 0.435728  [   50/   88]
per-ex loss: 0.382680  [   52/   88]
per-ex loss: 0.510986  [   54/   88]
per-ex loss: 0.561868  [   56/   88]
per-ex loss: 0.383262  [   58/   88]
per-ex loss: 0.418119  [   60/   88]
per-ex loss: 0.442843  [   62/   88]
per-ex loss: 0.382551  [   64/   88]
per-ex loss: 0.630182  [   66/   88]
per-ex loss: 0.447047  [   68/   88]
per-ex loss: 0.373276  [   70/   88]
per-ex loss: 0.608147  [   72/   88]
per-ex loss: 0.348530  [   74/   88]
per-ex loss: 0.426371  [   76/   88]
per-ex loss: 0.504921  [   78/   88]
per-ex loss: 0.344032  [   80/   88]
per-ex loss: 0.377350  [   82/   88]
per-ex loss: 0.483669  [   84/   88]
per-ex loss: 0.559187  [   86/   88]
per-ex loss: 0.505093  [   88/   88]
Train Error: Avg loss: 0.44536158
validation Error: 
 Avg loss: 0.51462714 
 F1: 0.517670 
 Precision: 0.589977 
 Recall: 0.461151
 IoU: 0.349227

test Error: 
 Avg loss: 0.46631644 
 F1: 0.567916 
 Precision: 0.635017 
 Recall: 0.513640
 IoU: 0.396566

We have finished training iteration 120
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_118_.pth
per-ex loss: 0.419367  [    2/   88]
per-ex loss: 0.422974  [    4/   88]
per-ex loss: 0.528482  [    6/   88]
per-ex loss: 0.610889  [    8/   88]
per-ex loss: 0.361212  [   10/   88]
per-ex loss: 0.427698  [   12/   88]
per-ex loss: 0.435708  [   14/   88]
per-ex loss: 0.369026  [   16/   88]
per-ex loss: 0.448779  [   18/   88]
per-ex loss: 0.578653  [   20/   88]
per-ex loss: 0.491563  [   22/   88]
per-ex loss: 0.422099  [   24/   88]
per-ex loss: 0.462268  [   26/   88]
per-ex loss: 0.509840  [   28/   88]
per-ex loss: 0.399807  [   30/   88]
per-ex loss: 0.340077  [   32/   88]
per-ex loss: 0.432136  [   34/   88]
per-ex loss: 0.349479  [   36/   88]
per-ex loss: 0.411772  [   38/   88]
per-ex loss: 0.311984  [   40/   88]
per-ex loss: 0.482097  [   42/   88]
per-ex loss: 0.346377  [   44/   88]
per-ex loss: 0.494889  [   46/   88]
per-ex loss: 0.347017  [   48/   88]
per-ex loss: 0.395538  [   50/   88]
per-ex loss: 0.431896  [   52/   88]
per-ex loss: 0.353634  [   54/   88]
per-ex loss: 0.366190  [   56/   88]
per-ex loss: 0.649839  [   58/   88]
per-ex loss: 0.403769  [   60/   88]
per-ex loss: 0.334034  [   62/   88]
per-ex loss: 0.437483  [   64/   88]
per-ex loss: 0.558976  [   66/   88]
per-ex loss: 0.440471  [   68/   88]
per-ex loss: 0.387598  [   70/   88]
per-ex loss: 0.520271  [   72/   88]
per-ex loss: 0.366709  [   74/   88]
per-ex loss: 0.343650  [   76/   88]
per-ex loss: 0.510865  [   78/   88]
per-ex loss: 0.434711  [   80/   88]
per-ex loss: 0.383776  [   82/   88]
per-ex loss: 0.518433  [   84/   88]
per-ex loss: 0.499750  [   86/   88]
per-ex loss: 0.456178  [   88/   88]
Train Error: Avg loss: 0.43631736
validation Error: 
 Avg loss: 0.52560389 
 F1: 0.501109 
 Precision: 0.493728 
 Recall: 0.508714
 IoU: 0.334320

test Error: 
 Avg loss: 0.48876085 
 F1: 0.544582 
 Precision: 0.547243 
 Recall: 0.541947
 IoU: 0.374176

We have finished training iteration 121
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_119_.pth
per-ex loss: 0.531170  [    2/   88]
per-ex loss: 0.403603  [    4/   88]
per-ex loss: 0.373863  [    6/   88]
per-ex loss: 0.424760  [    8/   88]
per-ex loss: 0.374875  [   10/   88]
per-ex loss: 0.366507  [   12/   88]
per-ex loss: 0.379778  [   14/   88]
per-ex loss: 0.513197  [   16/   88]
per-ex loss: 0.647118  [   18/   88]
per-ex loss: 0.470847  [   20/   88]
per-ex loss: 0.432237  [   22/   88]
per-ex loss: 0.372800  [   24/   88]
per-ex loss: 0.413121  [   26/   88]
per-ex loss: 0.554216  [   28/   88]
per-ex loss: 0.360332  [   30/   88]
per-ex loss: 0.409791  [   32/   88]
per-ex loss: 0.534943  [   34/   88]
per-ex loss: 0.348247  [   36/   88]
per-ex loss: 0.370808  [   38/   88]
per-ex loss: 0.457193  [   40/   88]
per-ex loss: 0.391407  [   42/   88]
per-ex loss: 0.443152  [   44/   88]
per-ex loss: 0.347219  [   46/   88]
per-ex loss: 0.326731  [   48/   88]
per-ex loss: 0.465069  [   50/   88]
per-ex loss: 0.507421  [   52/   88]
per-ex loss: 0.387882  [   54/   88]
per-ex loss: 0.406426  [   56/   88]
per-ex loss: 0.596917  [   58/   88]
per-ex loss: 0.583078  [   60/   88]
per-ex loss: 0.494546  [   62/   88]
per-ex loss: 0.593490  [   64/   88]
per-ex loss: 0.452599  [   66/   88]
per-ex loss: 0.369574  [   68/   88]
per-ex loss: 0.491267  [   70/   88]
per-ex loss: 0.361698  [   72/   88]
per-ex loss: 0.430239  [   74/   88]
per-ex loss: 0.395262  [   76/   88]
per-ex loss: 0.404355  [   78/   88]
per-ex loss: 0.487345  [   80/   88]
per-ex loss: 0.581604  [   82/   88]
per-ex loss: 0.455305  [   84/   88]
per-ex loss: 0.313556  [   86/   88]
per-ex loss: 0.353146  [   88/   88]
Train Error: Avg loss: 0.44042491
validation Error: 
 Avg loss: 0.51523570 
 F1: 0.515288 
 Precision: 0.623969 
 Recall: 0.438850
 IoU: 0.347062

test Error: 
 Avg loss: 0.48862667 
 F1: 0.546274 
 Precision: 0.664196 
 Recall: 0.463911
 IoU: 0.375775

We have finished training iteration 122
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_120_.pth
per-ex loss: 0.375201  [    2/   88]
per-ex loss: 0.502477  [    4/   88]
per-ex loss: 0.378503  [    6/   88]
per-ex loss: 0.406731  [    8/   88]
per-ex loss: 0.568333  [   10/   88]
per-ex loss: 0.477199  [   12/   88]
per-ex loss: 0.427647  [   14/   88]
per-ex loss: 0.451789  [   16/   88]
per-ex loss: 0.484275  [   18/   88]
per-ex loss: 0.352697  [   20/   88]
per-ex loss: 0.440715  [   22/   88]
per-ex loss: 0.510481  [   24/   88]
per-ex loss: 0.486086  [   26/   88]
per-ex loss: 0.582826  [   28/   88]
per-ex loss: 0.416203  [   30/   88]
per-ex loss: 0.433236  [   32/   88]
per-ex loss: 0.475110  [   34/   88]
per-ex loss: 0.364109  [   36/   88]
per-ex loss: 0.444401  [   38/   88]
per-ex loss: 0.319554  [   40/   88]
per-ex loss: 0.400649  [   42/   88]
per-ex loss: 0.607141  [   44/   88]
per-ex loss: 0.421072  [   46/   88]
per-ex loss: 0.337738  [   48/   88]
per-ex loss: 0.403718  [   50/   88]
per-ex loss: 0.398984  [   52/   88]
per-ex loss: 0.298582  [   54/   88]
per-ex loss: 0.426248  [   56/   88]
per-ex loss: 0.429780  [   58/   88]
per-ex loss: 0.579432  [   60/   88]
per-ex loss: 0.520238  [   62/   88]
per-ex loss: 0.401764  [   64/   88]
per-ex loss: 0.458841  [   66/   88]
per-ex loss: 0.343946  [   68/   88]
per-ex loss: 0.419695  [   70/   88]
per-ex loss: 0.371823  [   72/   88]
per-ex loss: 0.376019  [   74/   88]
per-ex loss: 0.541991  [   76/   88]
per-ex loss: 0.452145  [   78/   88]
per-ex loss: 0.522004  [   80/   88]
per-ex loss: 0.356309  [   82/   88]
per-ex loss: 0.353307  [   84/   88]
per-ex loss: 0.406332  [   86/   88]
per-ex loss: 0.534591  [   88/   88]
Train Error: Avg loss: 0.43772546
validation Error: 
 Avg loss: 0.52604384 
 F1: 0.497125 
 Precision: 0.680135 
 Recall: 0.391721
 IoU: 0.330783

test Error: 
 Avg loss: 0.49869174 
 F1: 0.536681 
 Precision: 0.717947 
 Recall: 0.428495
 IoU: 0.366756

We have finished training iteration 123
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_121_.pth
per-ex loss: 0.421272  [    2/   88]
per-ex loss: 0.343276  [    4/   88]
per-ex loss: 0.396400  [    6/   88]
per-ex loss: 0.429526  [    8/   88]
per-ex loss: 0.336260  [   10/   88]
per-ex loss: 0.480763  [   12/   88]
per-ex loss: 0.390491  [   14/   88]
per-ex loss: 0.334557  [   16/   88]
per-ex loss: 0.486015  [   18/   88]
per-ex loss: 0.516701  [   20/   88]
per-ex loss: 0.513937  [   22/   88]
per-ex loss: 0.515565  [   24/   88]
per-ex loss: 0.412538  [   26/   88]
per-ex loss: 0.439929  [   28/   88]
per-ex loss: 0.501990  [   30/   88]
per-ex loss: 0.510153  [   32/   88]
per-ex loss: 0.625844  [   34/   88]
per-ex loss: 0.410236  [   36/   88]
per-ex loss: 0.332784  [   38/   88]
per-ex loss: 0.503251  [   40/   88]
per-ex loss: 0.362563  [   42/   88]
per-ex loss: 0.501386  [   44/   88]
per-ex loss: 0.484039  [   46/   88]
per-ex loss: 0.551678  [   48/   88]
per-ex loss: 0.412094  [   50/   88]
per-ex loss: 0.382338  [   52/   88]
per-ex loss: 0.415271  [   54/   88]
per-ex loss: 0.369658  [   56/   88]
per-ex loss: 0.324416  [   58/   88]
per-ex loss: 0.553271  [   60/   88]
per-ex loss: 0.480103  [   62/   88]
per-ex loss: 0.390498  [   64/   88]
per-ex loss: 0.328215  [   66/   88]
per-ex loss: 0.367171  [   68/   88]
per-ex loss: 0.492379  [   70/   88]
per-ex loss: 0.368586  [   72/   88]
per-ex loss: 0.589234  [   74/   88]
per-ex loss: 0.612543  [   76/   88]
per-ex loss: 0.375650  [   78/   88]
per-ex loss: 0.421342  [   80/   88]
per-ex loss: 0.369253  [   82/   88]
per-ex loss: 0.520576  [   84/   88]
per-ex loss: 0.373367  [   86/   88]
per-ex loss: 0.367572  [   88/   88]
Train Error: Avg loss: 0.43897026
validation Error: 
 Avg loss: 0.51640193 
 F1: 0.514327 
 Precision: 0.609030 
 Recall: 0.445114
 IoU: 0.346192

test Error: 
 Avg loss: 0.47451840 
 F1: 0.560157 
 Precision: 0.637610 
 Recall: 0.499483
 IoU: 0.389040

We have finished training iteration 124
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_122_.pth
per-ex loss: 0.378375  [    2/   88]
per-ex loss: 0.373950  [    4/   88]
per-ex loss: 0.472055  [    6/   88]
per-ex loss: 0.409098  [    8/   88]
per-ex loss: 0.481246  [   10/   88]
per-ex loss: 0.385956  [   12/   88]
per-ex loss: 0.320294  [   14/   88]
per-ex loss: 0.335784  [   16/   88]
per-ex loss: 0.336132  [   18/   88]
per-ex loss: 0.528665  [   20/   88]
per-ex loss: 0.633051  [   22/   88]
per-ex loss: 0.431023  [   24/   88]
per-ex loss: 0.465589  [   26/   88]
per-ex loss: 0.375918  [   28/   88]
per-ex loss: 0.356004  [   30/   88]
per-ex loss: 0.378929  [   32/   88]
per-ex loss: 0.393550  [   34/   88]
per-ex loss: 0.427340  [   36/   88]
per-ex loss: 0.386480  [   38/   88]
per-ex loss: 0.533337  [   40/   88]
per-ex loss: 0.410213  [   42/   88]
per-ex loss: 0.406313  [   44/   88]
per-ex loss: 0.423271  [   46/   88]
per-ex loss: 0.346391  [   48/   88]
per-ex loss: 0.403280  [   50/   88]
per-ex loss: 0.457221  [   52/   88]
per-ex loss: 0.355473  [   54/   88]
per-ex loss: 0.330622  [   56/   88]
per-ex loss: 0.403143  [   58/   88]
per-ex loss: 0.489739  [   60/   88]
per-ex loss: 0.338673  [   62/   88]
per-ex loss: 0.557381  [   64/   88]
per-ex loss: 0.552709  [   66/   88]
per-ex loss: 0.485077  [   68/   88]
per-ex loss: 0.459543  [   70/   88]
per-ex loss: 0.345378  [   72/   88]
per-ex loss: 0.511496  [   74/   88]
per-ex loss: 0.466518  [   76/   88]
per-ex loss: 0.520362  [   78/   88]
per-ex loss: 0.457067  [   80/   88]
per-ex loss: 0.436222  [   82/   88]
per-ex loss: 0.445087  [   84/   88]
per-ex loss: 0.435288  [   86/   88]
per-ex loss: 0.625251  [   88/   88]
Train Error: Avg loss: 0.43328398
validation Error: 
 Avg loss: 0.51801860 
 F1: 0.504518 
 Precision: 0.483508 
 Recall: 0.527436
 IoU: 0.337361

test Error: 
 Avg loss: 0.48853922 
 F1: 0.538182 
 Precision: 0.511260 
 Recall: 0.568097
 IoU: 0.368159

We have finished training iteration 125
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_123_.pth
per-ex loss: 0.372372  [    2/   88]
per-ex loss: 0.378061  [    4/   88]
per-ex loss: 0.428248  [    6/   88]
per-ex loss: 0.379341  [    8/   88]
per-ex loss: 0.372983  [   10/   88]
per-ex loss: 0.556827  [   12/   88]
per-ex loss: 0.540185  [   14/   88]
per-ex loss: 0.381804  [   16/   88]
per-ex loss: 0.367018  [   18/   88]
per-ex loss: 0.457585  [   20/   88]
per-ex loss: 0.483158  [   22/   88]
per-ex loss: 0.633320  [   24/   88]
per-ex loss: 0.407005  [   26/   88]
per-ex loss: 0.427119  [   28/   88]
per-ex loss: 0.446039  [   30/   88]
per-ex loss: 0.406394  [   32/   88]
per-ex loss: 0.440496  [   34/   88]
per-ex loss: 0.463567  [   36/   88]
per-ex loss: 0.501720  [   38/   88]
per-ex loss: 0.365166  [   40/   88]
per-ex loss: 0.566645  [   42/   88]
per-ex loss: 0.488134  [   44/   88]
per-ex loss: 0.421800  [   46/   88]
per-ex loss: 0.485147  [   48/   88]
per-ex loss: 0.395004  [   50/   88]
per-ex loss: 0.403029  [   52/   88]
per-ex loss: 0.339679  [   54/   88]
per-ex loss: 0.385181  [   56/   88]
per-ex loss: 0.503967  [   58/   88]
per-ex loss: 0.313110  [   60/   88]
per-ex loss: 0.415781  [   62/   88]
per-ex loss: 0.577962  [   64/   88]
per-ex loss: 0.390142  [   66/   88]
per-ex loss: 0.374213  [   68/   88]
per-ex loss: 0.466980  [   70/   88]
per-ex loss: 0.321816  [   72/   88]
per-ex loss: 0.342347  [   74/   88]
per-ex loss: 0.441328  [   76/   88]
per-ex loss: 0.347162  [   78/   88]
per-ex loss: 0.434742  [   80/   88]
per-ex loss: 0.391385  [   82/   88]
per-ex loss: 0.348944  [   84/   88]
per-ex loss: 0.506260  [   86/   88]
per-ex loss: 0.445620  [   88/   88]
Train Error: Avg loss: 0.42988149
validation Error: 
 Avg loss: 0.52318446 
 F1: 0.505956 
 Precision: 0.517777 
 Recall: 0.494663
 IoU: 0.338649

test Error: 
 Avg loss: 0.46817301 
 F1: 0.569201 
 Precision: 0.592249 
 Recall: 0.547879
 IoU: 0.397820

We have finished training iteration 126
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_124_.pth
per-ex loss: 0.385874  [    2/   88]
per-ex loss: 0.496785  [    4/   88]
per-ex loss: 0.409793  [    6/   88]
per-ex loss: 0.437998  [    8/   88]
per-ex loss: 0.387354  [   10/   88]
per-ex loss: 0.514337  [   12/   88]
per-ex loss: 0.539946  [   14/   88]
per-ex loss: 0.327930  [   16/   88]
per-ex loss: 0.376705  [   18/   88]
per-ex loss: 0.474451  [   20/   88]
per-ex loss: 0.503374  [   22/   88]
per-ex loss: 0.575953  [   24/   88]
per-ex loss: 0.538881  [   26/   88]
per-ex loss: 0.360279  [   28/   88]
per-ex loss: 0.373746  [   30/   88]
per-ex loss: 0.394605  [   32/   88]
per-ex loss: 0.399558  [   34/   88]
per-ex loss: 0.401398  [   36/   88]
per-ex loss: 0.352143  [   38/   88]
per-ex loss: 0.444366  [   40/   88]
per-ex loss: 0.600878  [   42/   88]
per-ex loss: 0.413480  [   44/   88]
per-ex loss: 0.480186  [   46/   88]
per-ex loss: 0.448199  [   48/   88]
per-ex loss: 0.412970  [   50/   88]
per-ex loss: 0.445658  [   52/   88]
per-ex loss: 0.484537  [   54/   88]
per-ex loss: 0.360447  [   56/   88]
per-ex loss: 0.369223  [   58/   88]
per-ex loss: 0.552755  [   60/   88]
per-ex loss: 0.432354  [   62/   88]
per-ex loss: 0.592164  [   64/   88]
per-ex loss: 0.401307  [   66/   88]
per-ex loss: 0.313299  [   68/   88]
per-ex loss: 0.406057  [   70/   88]
per-ex loss: 0.372218  [   72/   88]
per-ex loss: 0.348677  [   74/   88]
per-ex loss: 0.370814  [   76/   88]
per-ex loss: 0.483424  [   78/   88]
per-ex loss: 0.556613  [   80/   88]
per-ex loss: 0.344134  [   82/   88]
per-ex loss: 0.361242  [   84/   88]
per-ex loss: 0.375812  [   86/   88]
per-ex loss: 0.562621  [   88/   88]
Train Error: Avg loss: 0.43601235
validation Error: 
 Avg loss: 0.51430896 
 F1: 0.497544 
 Precision: 0.621891 
 Recall: 0.414638
 IoU: 0.331154

test Error: 
 Avg loss: 0.48854553 
 F1: 0.550776 
 Precision: 0.700048 
 Recall: 0.453975
 IoU: 0.380049

We have finished training iteration 127
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_125_.pth
per-ex loss: 0.432832  [    2/   88]
per-ex loss: 0.479190  [    4/   88]
per-ex loss: 0.387814  [    6/   88]
per-ex loss: 0.655706  [    8/   88]
per-ex loss: 0.341219  [   10/   88]
per-ex loss: 0.546093  [   12/   88]
per-ex loss: 0.427896  [   14/   88]
per-ex loss: 0.392076  [   16/   88]
per-ex loss: 0.459603  [   18/   88]
per-ex loss: 0.334004  [   20/   88]
per-ex loss: 0.392767  [   22/   88]
per-ex loss: 0.558783  [   24/   88]
per-ex loss: 0.465038  [   26/   88]
per-ex loss: 0.470600  [   28/   88]
per-ex loss: 0.570977  [   30/   88]
per-ex loss: 0.309617  [   32/   88]
per-ex loss: 0.526544  [   34/   88]
per-ex loss: 0.459209  [   36/   88]
per-ex loss: 0.383563  [   38/   88]
per-ex loss: 0.422162  [   40/   88]
per-ex loss: 0.496276  [   42/   88]
per-ex loss: 0.348171  [   44/   88]
per-ex loss: 0.580910  [   46/   88]
per-ex loss: 0.406634  [   48/   88]
per-ex loss: 0.370551  [   50/   88]
per-ex loss: 0.393230  [   52/   88]
per-ex loss: 0.332200  [   54/   88]
per-ex loss: 0.372553  [   56/   88]
per-ex loss: 0.325915  [   58/   88]
per-ex loss: 0.481908  [   60/   88]
per-ex loss: 0.500932  [   62/   88]
per-ex loss: 0.362226  [   64/   88]
per-ex loss: 0.483041  [   66/   88]
per-ex loss: 0.584652  [   68/   88]
per-ex loss: 0.393187  [   70/   88]
per-ex loss: 0.498781  [   72/   88]
per-ex loss: 0.389617  [   74/   88]
per-ex loss: 0.382075  [   76/   88]
per-ex loss: 0.519737  [   78/   88]
per-ex loss: 0.554088  [   80/   88]
per-ex loss: 0.501011  [   82/   88]
per-ex loss: 0.389665  [   84/   88]
per-ex loss: 0.416474  [   86/   88]
per-ex loss: 0.405957  [   88/   88]
Train Error: Avg loss: 0.44330644
validation Error: 
 Avg loss: 0.51540119 
 F1: 0.510074 
 Precision: 0.574451 
 Recall: 0.458672
 IoU: 0.342349

test Error: 
 Avg loss: 0.47389191 
 F1: 0.565305 
 Precision: 0.633579 
 Recall: 0.510314
 IoU: 0.394025

We have finished training iteration 128
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_126_.pth
per-ex loss: 0.560552  [    2/   88]
per-ex loss: 0.456340  [    4/   88]
per-ex loss: 0.343067  [    6/   88]
per-ex loss: 0.652064  [    8/   88]
per-ex loss: 0.395661  [   10/   88]
per-ex loss: 0.477111  [   12/   88]
per-ex loss: 0.368667  [   14/   88]
per-ex loss: 0.341894  [   16/   88]
per-ex loss: 0.371371  [   18/   88]
per-ex loss: 0.319144  [   20/   88]
per-ex loss: 0.492606  [   22/   88]
per-ex loss: 0.384237  [   24/   88]
per-ex loss: 0.489608  [   26/   88]
per-ex loss: 0.369412  [   28/   88]
per-ex loss: 0.537253  [   30/   88]
per-ex loss: 0.420958  [   32/   88]
per-ex loss: 0.415390  [   34/   88]
per-ex loss: 0.546038  [   36/   88]
per-ex loss: 0.433200  [   38/   88]
per-ex loss: 0.345413  [   40/   88]
per-ex loss: 0.389359  [   42/   88]
per-ex loss: 0.436574  [   44/   88]
per-ex loss: 0.382095  [   46/   88]
per-ex loss: 0.468978  [   48/   88]
per-ex loss: 0.539430  [   50/   88]
per-ex loss: 0.399137  [   52/   88]
per-ex loss: 0.357120  [   54/   88]
per-ex loss: 0.546769  [   56/   88]
per-ex loss: 0.520802  [   58/   88]
per-ex loss: 0.555023  [   60/   88]
per-ex loss: 0.387445  [   62/   88]
per-ex loss: 0.476674  [   64/   88]
per-ex loss: 0.569886  [   66/   88]
per-ex loss: 0.334330  [   68/   88]
per-ex loss: 0.481316  [   70/   88]
per-ex loss: 0.344243  [   72/   88]
per-ex loss: 0.434147  [   74/   88]
per-ex loss: 0.394657  [   76/   88]
per-ex loss: 0.409928  [   78/   88]
per-ex loss: 0.353545  [   80/   88]
per-ex loss: 0.378141  [   82/   88]
per-ex loss: 0.409940  [   84/   88]
per-ex loss: 0.389295  [   86/   88]
per-ex loss: 0.416989  [   88/   88]
Train Error: Avg loss: 0.43399563
validation Error: 
 Avg loss: 0.51638460 
 F1: 0.506882 
 Precision: 0.525636 
 Recall: 0.489420
 IoU: 0.339479

test Error: 
 Avg loss: 0.46893391 
 F1: 0.566740 
 Precision: 0.596564 
 Recall: 0.539755
 IoU: 0.395420

We have finished training iteration 129
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_127_.pth
per-ex loss: 0.638591  [    2/   88]
per-ex loss: 0.405488  [    4/   88]
per-ex loss: 0.401478  [    6/   88]
per-ex loss: 0.368593  [    8/   88]
per-ex loss: 0.346889  [   10/   88]
per-ex loss: 0.446456  [   12/   88]
per-ex loss: 0.384249  [   14/   88]
per-ex loss: 0.289411  [   16/   88]
per-ex loss: 0.427302  [   18/   88]
per-ex loss: 0.600598  [   20/   88]
per-ex loss: 0.598998  [   22/   88]
per-ex loss: 0.368812  [   24/   88]
per-ex loss: 0.460905  [   26/   88]
per-ex loss: 0.418185  [   28/   88]
per-ex loss: 0.377106  [   30/   88]
per-ex loss: 0.330519  [   32/   88]
per-ex loss: 0.423142  [   34/   88]
per-ex loss: 0.373148  [   36/   88]
per-ex loss: 0.506978  [   38/   88]
per-ex loss: 0.616975  [   40/   88]
per-ex loss: 0.453389  [   42/   88]
per-ex loss: 0.324255  [   44/   88]
per-ex loss: 0.362475  [   46/   88]
per-ex loss: 0.411811  [   48/   88]
per-ex loss: 0.422720  [   50/   88]
per-ex loss: 0.569827  [   52/   88]
per-ex loss: 0.331932  [   54/   88]
per-ex loss: 0.335761  [   56/   88]
per-ex loss: 0.344498  [   58/   88]
per-ex loss: 0.425745  [   60/   88]
per-ex loss: 0.466903  [   62/   88]
per-ex loss: 0.416672  [   64/   88]
per-ex loss: 0.453700  [   66/   88]
per-ex loss: 0.440809  [   68/   88]
per-ex loss: 0.465457  [   70/   88]
per-ex loss: 0.390957  [   72/   88]
per-ex loss: 0.419049  [   74/   88]
per-ex loss: 0.386454  [   76/   88]
per-ex loss: 0.400531  [   78/   88]
per-ex loss: 0.578820  [   80/   88]
per-ex loss: 0.514074  [   82/   88]
per-ex loss: 0.401779  [   84/   88]
per-ex loss: 0.367136  [   86/   88]
per-ex loss: 0.460529  [   88/   88]
Train Error: Avg loss: 0.43020702
validation Error: 
 Avg loss: 0.52616367 
 F1: 0.508532 
 Precision: 0.492548 
 Recall: 0.525588
 IoU: 0.340961

test Error: 
 Avg loss: 0.47582476 
 F1: 0.554725 
 Precision: 0.536527 
 Recall: 0.574201
 IoU: 0.383820

We have finished training iteration 130
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_128_.pth
per-ex loss: 0.471125  [    2/   88]
per-ex loss: 0.371201  [    4/   88]
per-ex loss: 0.489541  [    6/   88]
per-ex loss: 0.630959  [    8/   88]
per-ex loss: 0.379989  [   10/   88]
per-ex loss: 0.494383  [   12/   88]
per-ex loss: 0.537608  [   14/   88]
per-ex loss: 0.540034  [   16/   88]
per-ex loss: 0.445909  [   18/   88]
per-ex loss: 0.393767  [   20/   88]
per-ex loss: 0.367921  [   22/   88]
per-ex loss: 0.553558  [   24/   88]
per-ex loss: 0.387269  [   26/   88]
per-ex loss: 0.361387  [   28/   88]
per-ex loss: 0.328820  [   30/   88]
per-ex loss: 0.370299  [   32/   88]
per-ex loss: 0.376370  [   34/   88]
per-ex loss: 0.525462  [   36/   88]
per-ex loss: 0.405357  [   38/   88]
per-ex loss: 0.401141  [   40/   88]
per-ex loss: 0.353359  [   42/   88]
per-ex loss: 0.341028  [   44/   88]
per-ex loss: 0.335294  [   46/   88]
per-ex loss: 0.422753  [   48/   88]
per-ex loss: 0.343574  [   50/   88]
per-ex loss: 0.349876  [   52/   88]
per-ex loss: 0.507889  [   54/   88]
per-ex loss: 0.358353  [   56/   88]
per-ex loss: 0.412630  [   58/   88]
per-ex loss: 0.539134  [   60/   88]
per-ex loss: 0.495353  [   62/   88]
per-ex loss: 0.394489  [   64/   88]
per-ex loss: 0.353127  [   66/   88]
per-ex loss: 0.379434  [   68/   88]
per-ex loss: 0.460779  [   70/   88]
per-ex loss: 0.528318  [   72/   88]
per-ex loss: 0.399451  [   74/   88]
per-ex loss: 0.431838  [   76/   88]
per-ex loss: 0.355797  [   78/   88]
per-ex loss: 0.417103  [   80/   88]
per-ex loss: 0.539328  [   82/   88]
per-ex loss: 0.370702  [   84/   88]
per-ex loss: 0.336044  [   86/   88]
per-ex loss: 0.579128  [   88/   88]
Train Error: Avg loss: 0.42811095
validation Error: 
 Avg loss: 0.53050792 
 F1: 0.489962 
 Precision: 0.657100 
 Recall: 0.390607
 IoU: 0.324470

test Error: 
 Avg loss: 0.50632232 
 F1: 0.526553 
 Precision: 0.739628 
 Recall: 0.408788
 IoU: 0.357362

We have finished training iteration 131
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_129_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.429996  [    2/   88]
per-ex loss: 0.407014  [    4/   88]
per-ex loss: 0.546120  [    6/   88]
per-ex loss: 0.401380  [    8/   88]
per-ex loss: 0.377128  [   10/   88]
per-ex loss: 0.468843  [   12/   88]
per-ex loss: 0.427449  [   14/   88]
per-ex loss: 0.374381  [   16/   88]
per-ex loss: 0.367138  [   18/   88]
per-ex loss: 0.536499  [   20/   88]
per-ex loss: 0.423006  [   22/   88]
per-ex loss: 0.547961  [   24/   88]
per-ex loss: 0.411894  [   26/   88]
per-ex loss: 0.477074  [   28/   88]
per-ex loss: 0.357122  [   30/   88]
per-ex loss: 0.365067  [   32/   88]
per-ex loss: 0.358527  [   34/   88]
per-ex loss: 0.414926  [   36/   88]
per-ex loss: 0.323758  [   38/   88]
per-ex loss: 0.526417  [   40/   88]
per-ex loss: 0.335661  [   42/   88]
per-ex loss: 0.471442  [   44/   88]
per-ex loss: 0.614553  [   46/   88]
per-ex loss: 0.332596  [   48/   88]
per-ex loss: 0.416260  [   50/   88]
per-ex loss: 0.446241  [   52/   88]
per-ex loss: 0.348996  [   54/   88]
per-ex loss: 0.374398  [   56/   88]
per-ex loss: 0.371935  [   58/   88]
per-ex loss: 0.549973  [   60/   88]
per-ex loss: 0.390166  [   62/   88]
per-ex loss: 0.389885  [   64/   88]
per-ex loss: 0.581052  [   66/   88]
per-ex loss: 0.480458  [   68/   88]
per-ex loss: 0.388478  [   70/   88]
per-ex loss: 0.432874  [   72/   88]
per-ex loss: 0.521698  [   74/   88]
per-ex loss: 0.387157  [   76/   88]
per-ex loss: 0.508358  [   78/   88]
per-ex loss: 0.607597  [   80/   88]
per-ex loss: 0.391943  [   82/   88]
per-ex loss: 0.412331  [   84/   88]
per-ex loss: 0.487295  [   86/   88]
per-ex loss: 0.362004  [   88/   88]
Train Error: Avg loss: 0.43511477
validation Error: 
 Avg loss: 0.52441129 
 F1: 0.506062 
 Precision: 0.569611 
 Recall: 0.455270
 IoU: 0.338744

test Error: 
 Avg loss: 0.48431399 
 F1: 0.549545 
 Precision: 0.603074 
 Recall: 0.504744
 IoU: 0.378878

We have finished training iteration 132
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_130_.pth
per-ex loss: 0.426149  [    2/   88]
per-ex loss: 0.424263  [    4/   88]
per-ex loss: 0.413233  [    6/   88]
per-ex loss: 0.436923  [    8/   88]
per-ex loss: 0.448906  [   10/   88]
per-ex loss: 0.372942  [   12/   88]
per-ex loss: 0.364610  [   14/   88]
per-ex loss: 0.545712  [   16/   88]
per-ex loss: 0.382558  [   18/   88]
per-ex loss: 0.374030  [   20/   88]
per-ex loss: 0.383699  [   22/   88]
per-ex loss: 0.410536  [   24/   88]
per-ex loss: 0.427955  [   26/   88]
per-ex loss: 0.450447  [   28/   88]
per-ex loss: 0.516218  [   30/   88]
per-ex loss: 0.335007  [   32/   88]
per-ex loss: 0.546507  [   34/   88]
per-ex loss: 0.412514  [   36/   88]
per-ex loss: 0.345640  [   38/   88]
per-ex loss: 0.524504  [   40/   88]
per-ex loss: 0.394816  [   42/   88]
per-ex loss: 0.441942  [   44/   88]
per-ex loss: 0.322862  [   46/   88]
per-ex loss: 0.441792  [   48/   88]
per-ex loss: 0.469785  [   50/   88]
per-ex loss: 0.354905  [   52/   88]
per-ex loss: 0.395743  [   54/   88]
per-ex loss: 0.446280  [   56/   88]
per-ex loss: 0.462974  [   58/   88]
per-ex loss: 0.488018  [   60/   88]
per-ex loss: 0.335544  [   62/   88]
per-ex loss: 0.564204  [   64/   88]
per-ex loss: 0.508006  [   66/   88]
per-ex loss: 0.398909  [   68/   88]
per-ex loss: 0.635694  [   70/   88]
per-ex loss: 0.453843  [   72/   88]
per-ex loss: 0.416049  [   74/   88]
per-ex loss: 0.419463  [   76/   88]
per-ex loss: 0.605009  [   78/   88]
per-ex loss: 0.368594  [   80/   88]
per-ex loss: 0.445086  [   82/   88]
per-ex loss: 0.483475  [   84/   88]
per-ex loss: 0.380522  [   86/   88]
per-ex loss: 0.394157  [   88/   88]
Train Error: Avg loss: 0.43568239
validation Error: 
 Avg loss: 0.51906094 
 F1: 0.511634 
 Precision: 0.661290 
 Recall: 0.417214
 IoU: 0.343755

test Error: 
 Avg loss: 0.48405701 
 F1: 0.552257 
 Precision: 0.698657 
 Recall: 0.456583
 IoU: 0.381461

We have finished training iteration 133
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_131_.pth
per-ex loss: 0.356657  [    2/   88]
per-ex loss: 0.392104  [    4/   88]
per-ex loss: 0.358954  [    6/   88]
per-ex loss: 0.328999  [    8/   88]
per-ex loss: 0.534083  [   10/   88]
per-ex loss: 0.384850  [   12/   88]
per-ex loss: 0.544471  [   14/   88]
per-ex loss: 0.550470  [   16/   88]
per-ex loss: 0.421853  [   18/   88]
per-ex loss: 0.647106  [   20/   88]
per-ex loss: 0.430054  [   22/   88]
per-ex loss: 0.429325  [   24/   88]
per-ex loss: 0.367894  [   26/   88]
per-ex loss: 0.510358  [   28/   88]
per-ex loss: 0.392589  [   30/   88]
per-ex loss: 0.367468  [   32/   88]
per-ex loss: 0.594636  [   34/   88]
per-ex loss: 0.373828  [   36/   88]
per-ex loss: 0.357140  [   38/   88]
per-ex loss: 0.360190  [   40/   88]
per-ex loss: 0.330929  [   42/   88]
per-ex loss: 0.517027  [   44/   88]
per-ex loss: 0.444689  [   46/   88]
per-ex loss: 0.390917  [   48/   88]
per-ex loss: 0.547092  [   50/   88]
per-ex loss: 0.411663  [   52/   88]
per-ex loss: 0.480373  [   54/   88]
per-ex loss: 0.416370  [   56/   88]
per-ex loss: 0.324613  [   58/   88]
per-ex loss: 0.443314  [   60/   88]
per-ex loss: 0.348505  [   62/   88]
per-ex loss: 0.418547  [   64/   88]
per-ex loss: 0.527434  [   66/   88]
per-ex loss: 0.492182  [   68/   88]
per-ex loss: 0.361987  [   70/   88]
per-ex loss: 0.357193  [   72/   88]
per-ex loss: 0.336393  [   74/   88]
per-ex loss: 0.409026  [   76/   88]
per-ex loss: 0.428580  [   78/   88]
per-ex loss: 0.526637  [   80/   88]
per-ex loss: 0.379446  [   82/   88]
per-ex loss: 0.537395  [   84/   88]
per-ex loss: 0.406836  [   86/   88]
per-ex loss: 0.420626  [   88/   88]
Train Error: Avg loss: 0.43092729
validation Error: 
 Avg loss: 0.50032233 
 F1: 0.517854 
 Precision: 0.607663 
 Recall: 0.451173
 IoU: 0.349394

test Error: 
 Avg loss: 0.46484651 
 F1: 0.573752 
 Precision: 0.659409 
 Recall: 0.507791
 IoU: 0.402281

We have finished training iteration 134
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_132_.pth
per-ex loss: 0.514133  [    2/   88]
per-ex loss: 0.397493  [    4/   88]
per-ex loss: 0.370584  [    6/   88]
per-ex loss: 0.399353  [    8/   88]
per-ex loss: 0.408361  [   10/   88]
per-ex loss: 0.349800  [   12/   88]
per-ex loss: 0.415326  [   14/   88]
per-ex loss: 0.416163  [   16/   88]
per-ex loss: 0.351858  [   18/   88]
per-ex loss: 0.313217  [   20/   88]
per-ex loss: 0.414080  [   22/   88]
per-ex loss: 0.319844  [   24/   88]
per-ex loss: 0.315259  [   26/   88]
per-ex loss: 0.395607  [   28/   88]
per-ex loss: 0.448254  [   30/   88]
per-ex loss: 0.530437  [   32/   88]
per-ex loss: 0.415654  [   34/   88]
per-ex loss: 0.312548  [   36/   88]
per-ex loss: 0.338194  [   38/   88]
per-ex loss: 0.338446  [   40/   88]
per-ex loss: 0.572720  [   42/   88]
per-ex loss: 0.521748  [   44/   88]
per-ex loss: 0.453598  [   46/   88]
per-ex loss: 0.583561  [   48/   88]
per-ex loss: 0.452833  [   50/   88]
per-ex loss: 0.548162  [   52/   88]
per-ex loss: 0.526529  [   54/   88]
per-ex loss: 0.444512  [   56/   88]
per-ex loss: 0.600059  [   58/   88]
per-ex loss: 0.373080  [   60/   88]
per-ex loss: 0.414472  [   62/   88]
per-ex loss: 0.427829  [   64/   88]
per-ex loss: 0.394967  [   66/   88]
per-ex loss: 0.485450  [   68/   88]
per-ex loss: 0.405203  [   70/   88]
per-ex loss: 0.337918  [   72/   88]
per-ex loss: 0.564423  [   74/   88]
per-ex loss: 0.438174  [   76/   88]
per-ex loss: 0.568788  [   78/   88]
per-ex loss: 0.343930  [   80/   88]
per-ex loss: 0.365101  [   82/   88]
per-ex loss: 0.441277  [   84/   88]
per-ex loss: 0.371728  [   86/   88]
per-ex loss: 0.579211  [   88/   88]
Train Error: Avg loss: 0.43136095
validation Error: 
 Avg loss: 0.52228969 
 F1: 0.505740 
 Precision: 0.684677 
 Recall: 0.400953
 IoU: 0.338455

test Error: 
 Avg loss: 0.49286285 
 F1: 0.543224 
 Precision: 0.704227 
 Recall: 0.442140
 IoU: 0.372895

We have finished training iteration 135
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_133_.pth
per-ex loss: 0.364598  [    2/   88]
per-ex loss: 0.521515  [    4/   88]
per-ex loss: 0.447568  [    6/   88]
per-ex loss: 0.329310  [    8/   88]
per-ex loss: 0.392625  [   10/   88]
per-ex loss: 0.531077  [   12/   88]
per-ex loss: 0.341115  [   14/   88]
per-ex loss: 0.524985  [   16/   88]
per-ex loss: 0.478010  [   18/   88]
per-ex loss: 0.345177  [   20/   88]
per-ex loss: 0.415232  [   22/   88]
per-ex loss: 0.360036  [   24/   88]
per-ex loss: 0.371520  [   26/   88]
per-ex loss: 0.488687  [   28/   88]
per-ex loss: 0.426737  [   30/   88]
per-ex loss: 0.351611  [   32/   88]
per-ex loss: 0.440166  [   34/   88]
per-ex loss: 0.527591  [   36/   88]
per-ex loss: 0.378228  [   38/   88]
per-ex loss: 0.367314  [   40/   88]
per-ex loss: 0.413703  [   42/   88]
per-ex loss: 0.691767  [   44/   88]
per-ex loss: 0.463624  [   46/   88]
per-ex loss: 0.392292  [   48/   88]
per-ex loss: 0.335750  [   50/   88]
per-ex loss: 0.386793  [   52/   88]
per-ex loss: 0.450066  [   54/   88]
per-ex loss: 0.353630  [   56/   88]
per-ex loss: 0.418050  [   58/   88]
per-ex loss: 0.528166  [   60/   88]
per-ex loss: 0.364897  [   62/   88]
per-ex loss: 0.358113  [   64/   88]
per-ex loss: 0.436856  [   66/   88]
per-ex loss: 0.475570  [   68/   88]
per-ex loss: 0.380436  [   70/   88]
per-ex loss: 0.385524  [   72/   88]
per-ex loss: 0.379052  [   74/   88]
per-ex loss: 0.479396  [   76/   88]
per-ex loss: 0.394990  [   78/   88]
per-ex loss: 0.508794  [   80/   88]
per-ex loss: 0.361087  [   82/   88]
per-ex loss: 0.317813  [   84/   88]
per-ex loss: 0.656999  [   86/   88]
per-ex loss: 0.475854  [   88/   88]
Train Error: Avg loss: 0.42755272
validation Error: 
 Avg loss: 0.52407920 
 F1: 0.498525 
 Precision: 0.527446 
 Recall: 0.472611
 IoU: 0.332023

test Error: 
 Avg loss: 0.47816413 
 F1: 0.556647 
 Precision: 0.607477 
 Recall: 0.513666
 IoU: 0.385662

We have finished training iteration 136
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_97_.pth
per-ex loss: 0.379179  [    2/   88]
per-ex loss: 0.358891  [    4/   88]
per-ex loss: 0.477367  [    6/   88]
per-ex loss: 0.444181  [    8/   88]
per-ex loss: 0.469756  [   10/   88]
per-ex loss: 0.492502  [   12/   88]
per-ex loss: 0.443522  [   14/   88]
per-ex loss: 0.463695  [   16/   88]
per-ex loss: 0.510371  [   18/   88]
per-ex loss: 0.414268  [   20/   88]
per-ex loss: 0.393799  [   22/   88]
per-ex loss: 0.379143  [   24/   88]
per-ex loss: 0.449894  [   26/   88]
per-ex loss: 0.355347  [   28/   88]
per-ex loss: 0.587944  [   30/   88]
per-ex loss: 0.363319  [   32/   88]
per-ex loss: 0.380812  [   34/   88]
per-ex loss: 0.481446  [   36/   88]
per-ex loss: 0.374498  [   38/   88]
per-ex loss: 0.356931  [   40/   88]
per-ex loss: 0.553163  [   42/   88]
per-ex loss: 0.524086  [   44/   88]
per-ex loss: 0.538786  [   46/   88]
per-ex loss: 0.423980  [   48/   88]
per-ex loss: 0.421217  [   50/   88]
per-ex loss: 0.336643  [   52/   88]
per-ex loss: 0.429634  [   54/   88]
per-ex loss: 0.447934  [   56/   88]
per-ex loss: 0.626702  [   58/   88]
per-ex loss: 0.357143  [   60/   88]
per-ex loss: 0.384672  [   62/   88]
per-ex loss: 0.301406  [   64/   88]
per-ex loss: 0.417654  [   66/   88]
per-ex loss: 0.550478  [   68/   88]
per-ex loss: 0.385800  [   70/   88]
per-ex loss: 0.414453  [   72/   88]
per-ex loss: 0.350047  [   74/   88]
per-ex loss: 0.479998  [   76/   88]
per-ex loss: 0.542648  [   78/   88]
per-ex loss: 0.335297  [   80/   88]
per-ex loss: 0.375831  [   82/   88]
per-ex loss: 0.549447  [   84/   88]
per-ex loss: 0.345960  [   86/   88]
per-ex loss: 0.438034  [   88/   88]
Train Error: Avg loss: 0.43426991
validation Error: 
 Avg loss: 0.52512937 
 F1: 0.507474 
 Precision: 0.649611 
 Recall: 0.416371
 IoU: 0.340010

test Error: 
 Avg loss: 0.50128287 
 F1: 0.529979 
 Precision: 0.688864 
 Recall: 0.430650
 IoU: 0.360525

We have finished training iteration 137
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_135_.pth
per-ex loss: 0.346676  [    2/   88]
per-ex loss: 0.415158  [    4/   88]
per-ex loss: 0.472587  [    6/   88]
per-ex loss: 0.373415  [    8/   88]
per-ex loss: 0.366408  [   10/   88]
per-ex loss: 0.365137  [   12/   88]
per-ex loss: 0.409822  [   14/   88]
per-ex loss: 0.446412  [   16/   88]
per-ex loss: 0.515505  [   18/   88]
per-ex loss: 0.488307  [   20/   88]
per-ex loss: 0.313040  [   22/   88]
per-ex loss: 0.359078  [   24/   88]
per-ex loss: 0.429553  [   26/   88]
per-ex loss: 0.338217  [   28/   88]
per-ex loss: 0.407243  [   30/   88]
per-ex loss: 0.356533  [   32/   88]
per-ex loss: 0.312257  [   34/   88]
per-ex loss: 0.526349  [   36/   88]
per-ex loss: 0.370251  [   38/   88]
per-ex loss: 0.597320  [   40/   88]
per-ex loss: 0.356645  [   42/   88]
per-ex loss: 0.402636  [   44/   88]
per-ex loss: 0.358932  [   46/   88]
per-ex loss: 0.436765  [   48/   88]
per-ex loss: 0.460635  [   50/   88]
per-ex loss: 0.385708  [   52/   88]
per-ex loss: 0.540552  [   54/   88]
per-ex loss: 0.482190  [   56/   88]
per-ex loss: 0.331278  [   58/   88]
per-ex loss: 0.472163  [   60/   88]
per-ex loss: 0.509146  [   62/   88]
per-ex loss: 0.339160  [   64/   88]
per-ex loss: 0.379222  [   66/   88]
per-ex loss: 0.436157  [   68/   88]
per-ex loss: 0.414289  [   70/   88]
per-ex loss: 0.427699  [   72/   88]
per-ex loss: 0.464818  [   74/   88]
per-ex loss: 0.356534  [   76/   88]
per-ex loss: 0.359615  [   78/   88]
per-ex loss: 0.456028  [   80/   88]
per-ex loss: 0.405169  [   82/   88]
per-ex loss: 0.319659  [   84/   88]
per-ex loss: 0.593879  [   86/   88]
per-ex loss: 0.554671  [   88/   88]
Train Error: Avg loss: 0.41938219
validation Error: 
 Avg loss: 0.50945676 
 F1: 0.514905 
 Precision: 0.633197 
 Recall: 0.433853
 IoU: 0.346715

test Error: 
 Avg loss: 0.47957827 
 F1: 0.556637 
 Precision: 0.676075 
 Recall: 0.473063
 IoU: 0.385653

We have finished training iteration 138
Deleting model ./unet_allold_train/saved_model_wrapper/models/UNet_136_.pth
per-ex loss: 0.418559  [    2/   88]
per-ex loss: 0.370061  [    4/   88]
per-ex loss: 0.447373  [    6/   88]
per-ex loss: 0.433005  [    8/   88]
per-ex loss: 0.446892  [   10/   88]
per-ex loss: 0.414916  [   12/   88]
per-ex loss: 0.353760  [   14/   88]
per-ex loss: 0.425306  [   16/   88]
per-ex loss: 0.372257  [   18/   88]
per-ex loss: 0.389279  [   20/   88]
per-ex loss: 0.474254  [   22/   88]
per-ex loss: 0.392724  [   24/   88]
per-ex loss: 0.534010  [   26/   88]
per-ex loss: 0.382133  [   28/   88]
per-ex loss: 0.325321  [   30/   88]
per-ex loss: 0.627561  [   32/   88]
per-ex loss: 0.400238  [   34/   88]
per-ex loss: 0.339115  [   36/   88]
per-ex loss: 0.566063  [   38/   88]
per-ex loss: 0.404495  [   40/   88]
per-ex loss: 0.405542  [   42/   88]
per-ex loss: 0.396761  [   44/   88]
per-ex loss: 0.402876  [   46/   88]
per-ex loss: 0.381642  [   48/   88]
per-ex loss: 0.416580  [   50/   88]
per-ex loss: 0.349004  [   52/   88]
per-ex loss: 0.551369  [   54/   88]
per-ex loss: 0.340249  [   56/   88]
per-ex loss: 0.448347  [   58/   88]
per-ex loss: 0.402662  [   60/   88]
per-ex loss: 0.361107  [   62/   88]
per-ex loss: 0.477851  [   64/   88]
per-ex loss: 0.422077  [   66/   88]
per-ex loss: 0.497583  [   68/   88]
per-ex loss: 0.455281  [   70/   88]
per-ex loss: 0.502819  [   72/   88]
per-ex loss: 0.367041  [   74/   88]
per-ex loss: 0.426795  [   76/   88]
per-ex loss: 0.387880  [   78/   88]
per-ex loss: 0.564568  [   80/   88]
per-ex loss: 0.479059  [   82/   88]
per-ex loss: 0.372328  [   84/   88]
per-ex loss: 0.505081  [   86/   88]
per-ex loss: 0.403478  [   88/   88]
Train Error: Avg loss: 0.42807503
validation Error: 
 Avg loss: 0.52692457 
 F1: 0.498320 
 Precision: 0.495038 
 Recall: 0.501646
 IoU: 0.331842

slurmstepd: error: *** STEP 16718.0 ON aga2 CANCELLED AT 2025-01-09T17:58:49 DUE TO TIME LIMIT ***
