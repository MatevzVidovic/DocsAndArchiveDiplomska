unet_att_main_just_train.py do_log: False
Log file name: log_14_16-12-18_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(sd='unet_att_res_nzo_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_att_res_nzo.yaml', yo=None, ntibp=None, ptp=None, map=None)
YAML: {'path_to_data': './Data/vein_and_sclera_data', 'batch_size': 1, 'learning_rate': 1e-05, 'num_of_dataloader_workers': 7, 'train_epoch_size_limit': 400, 'num_epochs_per_training_iteration': 1, 'cleanup_k': 3, 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': True, 'loss_fn_name': 'MCDL', 'alphas': [], 'dataset_option': 'aug_tf', 'zero_out_non_sclera': False, 'add_sclera_to_img': True, 'add_bcosfire_to_img': True, 'add_coye_to_img': True, 'model_type': 'res_att', 'model': '64_2_6', 'input_width': 2048, 'input_height': 1024, 'input_channels': 6, 'output_channels': 2, 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq'}
Validation phase: False
Namespace(sd='unet_att_res_nzo_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_att_res_nzo.yaml', yo=None, ntibp=None, ptp=None, map=None)
Device: cuda
dataset_aug_tf.py do_log: False
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 88
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 88
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 88
val dataloader num of batches: 27
test dataloader num of batches: 12
Created new model instance.
per-ex loss: 0.851436  [    1/   88]
per-ex loss: 0.851122  [    2/   88]
per-ex loss: 0.907208  [    3/   88]
per-ex loss: 0.865201  [    4/   88]
per-ex loss: 0.851402  [    5/   88]
per-ex loss: 0.868472  [    6/   88]
per-ex loss: 0.910146  [    7/   88]
per-ex loss: 0.852340  [    8/   88]
per-ex loss: 0.807621  [    9/   88]
per-ex loss: 0.859141  [   10/   88]
per-ex loss: 0.950423  [   11/   88]
per-ex loss: 0.784476  [   12/   88]
per-ex loss: 0.814815  [   13/   88]
per-ex loss: 0.915987  [   14/   88]
per-ex loss: 0.764206  [   15/   88]
per-ex loss: 0.849578  [   16/   88]
per-ex loss: 0.838361  [   17/   88]
per-ex loss: 0.828125  [   18/   88]
per-ex loss: 0.945069  [   19/   88]
per-ex loss: 0.871528  [   20/   88]
per-ex loss: 0.823137  [   21/   88]
per-ex loss: 0.801944  [   22/   88]
per-ex loss: 0.856470  [   23/   88]
per-ex loss: 0.881273  [   24/   88]
per-ex loss: 0.810058  [   25/   88]
per-ex loss: 0.875892  [   26/   88]
per-ex loss: 0.920589  [   27/   88]
per-ex loss: 0.734693  [   28/   88]
per-ex loss: 0.874105  [   29/   88]
per-ex loss: 0.743429  [   30/   88]
per-ex loss: 0.617852  [   31/   88]
per-ex loss: 0.758591  [   32/   88]
per-ex loss: 0.605677  [   33/   88]
per-ex loss: 0.578865  [   34/   88]
per-ex loss: 0.858655  [   35/   88]
per-ex loss: 0.826508  [   36/   88]
per-ex loss: 0.801370  [   37/   88]
per-ex loss: 0.814161  [   38/   88]
per-ex loss: 0.877260  [   39/   88]
per-ex loss: 0.635946  [   40/   88]
per-ex loss: 0.816884  [   41/   88]
per-ex loss: 0.807567  [   42/   88]
per-ex loss: 0.852215  [   43/   88]
per-ex loss: 0.837291  [   44/   88]
per-ex loss: 0.587627  [   45/   88]
per-ex loss: 0.634860  [   46/   88]
per-ex loss: 0.806440  [   47/   88]
per-ex loss: 0.821323  [   48/   88]
per-ex loss: 0.660249  [   49/   88]
per-ex loss: 0.690470  [   50/   88]
per-ex loss: 0.686070  [   51/   88]
per-ex loss: 0.680377  [   52/   88]
per-ex loss: 0.595828  [   53/   88]
per-ex loss: 0.670114  [   54/   88]
per-ex loss: 0.630783  [   55/   88]
per-ex loss: 0.789314  [   56/   88]
per-ex loss: 0.696488  [   57/   88]
per-ex loss: 0.814283  [   58/   88]
per-ex loss: 0.832006  [   59/   88]
per-ex loss: 0.768730  [   60/   88]
per-ex loss: 0.804372  [   61/   88]
per-ex loss: 0.798435  [   62/   88]
per-ex loss: 0.495069  [   63/   88]
per-ex loss: 0.598451  [   64/   88]
per-ex loss: 0.829986  [   65/   88]
per-ex loss: 0.867153  [   66/   88]
per-ex loss: 0.610515  [   67/   88]
per-ex loss: 0.639226  [   68/   88]
per-ex loss: 0.479689  [   69/   88]
per-ex loss: 0.820477  [   70/   88]
per-ex loss: 0.827621  [   71/   88]
per-ex loss: 0.586600  [   72/   88]
per-ex loss: 0.619383  [   73/   88]
per-ex loss: 0.712204  [   74/   88]
per-ex loss: 0.474179  [   75/   88]
per-ex loss: 0.595129  [   76/   88]
per-ex loss: 0.603083  [   77/   88]
per-ex loss: 0.494726  [   78/   88]
per-ex loss: 0.683037  [   79/   88]
per-ex loss: 0.732312  [   80/   88]
per-ex loss: 0.773297  [   81/   88]
per-ex loss: 0.623802  [   82/   88]
per-ex loss: 0.495582  [   83/   88]
per-ex loss: 0.692389  [   84/   88]
per-ex loss: 0.773383  [   85/   88]
per-ex loss: 0.755505  [   86/   88]
per-ex loss: 0.697607  [   87/   88]
per-ex loss: 0.656608  [   88/   88]
Train Error: Avg loss: 0.75343035
validation Error: 
 Avg loss: 0.65039074 
 F1: 0.398589 
 Precision: 0.624361 
 Recall: 0.292735
 IoU: 0.248899

test Error: 
 Avg loss: 0.62275475 
 F1: 0.463358 
 Precision: 0.590910 
 Recall: 0.381096
 IoU: 0.301539

We have finished training iteration 1
per-ex loss: 0.707647  [    1/   88]
per-ex loss: 0.765667  [    2/   88]
per-ex loss: 0.824548  [    3/   88]
per-ex loss: 0.443859  [    4/   88]
per-ex loss: 0.492991  [    5/   88]
per-ex loss: 0.694111  [    6/   88]
per-ex loss: 0.724492  [    7/   88]
per-ex loss: 0.742173  [    8/   88]
per-ex loss: 0.769045  [    9/   88]
per-ex loss: 0.717342  [   10/   88]
per-ex loss: 0.737419  [   11/   88]
per-ex loss: 0.690539  [   12/   88]
per-ex loss: 0.733784  [   13/   88]
per-ex loss: 0.683483  [   14/   88]
per-ex loss: 0.630194  [   15/   88]
per-ex loss: 0.583738  [   16/   88]
per-ex loss: 0.570990  [   17/   88]
per-ex loss: 0.459668  [   18/   88]
per-ex loss: 0.599252  [   19/   88]
per-ex loss: 0.714430  [   20/   88]
per-ex loss: 0.573428  [   21/   88]
per-ex loss: 0.714203  [   22/   88]
per-ex loss: 0.596345  [   23/   88]
per-ex loss: 0.543054  [   24/   88]
per-ex loss: 0.808317  [   25/   88]
per-ex loss: 0.581223  [   26/   88]
per-ex loss: 0.684955  [   27/   88]
per-ex loss: 0.586585  [   28/   88]
per-ex loss: 0.631557  [   29/   88]
per-ex loss: 0.719995  [   30/   88]
per-ex loss: 0.650342  [   31/   88]
per-ex loss: 0.460249  [   32/   88]
per-ex loss: 0.477376  [   33/   88]
per-ex loss: 0.553054  [   34/   88]
per-ex loss: 0.726043  [   35/   88]
per-ex loss: 0.673693  [   36/   88]
per-ex loss: 0.560175  [   37/   88]
per-ex loss: 0.783279  [   38/   88]
per-ex loss: 0.622873  [   39/   88]
per-ex loss: 0.800617  [   40/   88]
per-ex loss: 0.728334  [   41/   88]
per-ex loss: 0.737242  [   42/   88]
per-ex loss: 0.536547  [   43/   88]
per-ex loss: 0.664830  [   44/   88]
per-ex loss: 0.642264  [   45/   88]
per-ex loss: 0.751400  [   46/   88]
per-ex loss: 0.547372  [   47/   88]
per-ex loss: 0.516798  [   48/   88]
per-ex loss: 0.528861  [   49/   88]
per-ex loss: 0.489393  [   50/   88]
per-ex loss: 0.567148  [   51/   88]
per-ex loss: 0.526623  [   52/   88]
per-ex loss: 0.708899  [   53/   88]
per-ex loss: 0.500941  [   54/   88]
per-ex loss: 0.681098  [   55/   88]
per-ex loss: 0.458519  [   56/   88]
per-ex loss: 0.709063  [   57/   88]
per-ex loss: 0.480886  [   58/   88]
per-ex loss: 0.806566  [   59/   88]
per-ex loss: 0.676024  [   60/   88]
per-ex loss: 0.849936  [   61/   88]
per-ex loss: 0.401676  [   62/   88]
per-ex loss: 0.554516  [   63/   88]
per-ex loss: 0.549253  [   64/   88]
per-ex loss: 0.742443  [   65/   88]
per-ex loss: 0.485480  [   66/   88]
per-ex loss: 0.686150  [   67/   88]
per-ex loss: 0.500495  [   68/   88]
per-ex loss: 0.772269  [   69/   88]
per-ex loss: 0.515015  [   70/   88]
per-ex loss: 0.472830  [   71/   88]
per-ex loss: 0.666214  [   72/   88]
per-ex loss: 0.532468  [   73/   88]
per-ex loss: 0.677161  [   74/   88]
per-ex loss: 0.540554  [   75/   88]
per-ex loss: 0.656838  [   76/   88]
per-ex loss: 0.512540  [   77/   88]
per-ex loss: 0.612306  [   78/   88]
per-ex loss: 0.496999  [   79/   88]
per-ex loss: 0.453097  [   80/   88]
per-ex loss: 0.618174  [   81/   88]
per-ex loss: 0.744697  [   82/   88]
per-ex loss: 0.711034  [   83/   88]
per-ex loss: 0.704524  [   84/   88]
per-ex loss: 0.467345  [   85/   88]
per-ex loss: 0.653552  [   86/   88]
per-ex loss: 0.523631  [   87/   88]
per-ex loss: 0.495142  [   88/   88]
Train Error: Avg loss: 0.62372594
validation Error: 
 Avg loss: 0.61768023 
 F1: 0.412727 
 Precision: 0.509705 
 Recall: 0.346753
 IoU: 0.260023

test Error: 
 Avg loss: 0.57347562 
 F1: 0.489333 
 Precision: 0.518134 
 Recall: 0.463565
 IoU: 0.323918

We have finished training iteration 2
per-ex loss: 0.652922  [    1/   88]
per-ex loss: 0.496414  [    2/   88]
per-ex loss: 0.578005  [    3/   88]
per-ex loss: 0.467854  [    4/   88]
per-ex loss: 0.733816  [    5/   88]
per-ex loss: 0.689812  [    6/   88]
per-ex loss: 0.672655  [    7/   88]
per-ex loss: 0.648507  [    8/   88]
per-ex loss: 0.494419  [    9/   88]
per-ex loss: 0.483506  [   10/   88]
per-ex loss: 0.473436  [   11/   88]
per-ex loss: 0.658920  [   12/   88]
per-ex loss: 0.738910  [   13/   88]
per-ex loss: 0.685157  [   14/   88]
per-ex loss: 0.434725  [   15/   88]
per-ex loss: 0.636475  [   16/   88]
per-ex loss: 0.443699  [   17/   88]
per-ex loss: 0.657514  [   18/   88]
per-ex loss: 0.730713  [   19/   88]
per-ex loss: 0.681343  [   20/   88]
per-ex loss: 0.525305  [   21/   88]
per-ex loss: 0.651455  [   22/   88]
per-ex loss: 0.501677  [   23/   88]
per-ex loss: 0.720050  [   24/   88]
per-ex loss: 0.775252  [   25/   88]
per-ex loss: 0.409482  [   26/   88]
per-ex loss: 0.496419  [   27/   88]
per-ex loss: 0.445382  [   28/   88]
per-ex loss: 0.729944  [   29/   88]
per-ex loss: 0.497557  [   30/   88]
per-ex loss: 0.759936  [   31/   88]
per-ex loss: 0.628813  [   32/   88]
per-ex loss: 0.700684  [   33/   88]
per-ex loss: 0.531878  [   34/   88]
per-ex loss: 0.536969  [   35/   88]
per-ex loss: 0.398679  [   36/   88]
per-ex loss: 0.605537  [   37/   88]
per-ex loss: 0.552994  [   38/   88]
per-ex loss: 0.434988  [   39/   88]
per-ex loss: 0.665800  [   40/   88]
per-ex loss: 0.524076  [   41/   88]
per-ex loss: 0.688659  [   42/   88]
per-ex loss: 0.532080  [   43/   88]
per-ex loss: 0.677535  [   44/   88]
per-ex loss: 0.619847  [   45/   88]
per-ex loss: 0.652694  [   46/   88]
per-ex loss: 0.462489  [   47/   88]
per-ex loss: 0.620780  [   48/   88]
per-ex loss: 0.500041  [   49/   88]
per-ex loss: 0.505521  [   50/   88]
per-ex loss: 0.464283  [   51/   88]
per-ex loss: 0.551448  [   52/   88]
per-ex loss: 0.796391  [   53/   88]
per-ex loss: 0.556556  [   54/   88]
per-ex loss: 0.519313  [   55/   88]
per-ex loss: 0.509439  [   56/   88]
per-ex loss: 0.554988  [   57/   88]
per-ex loss: 0.518703  [   58/   88]
per-ex loss: 0.659591  [   59/   88]
per-ex loss: 0.445135  [   60/   88]
per-ex loss: 0.443309  [   61/   88]
per-ex loss: 0.492872  [   62/   88]
per-ex loss: 0.718821  [   63/   88]
per-ex loss: 0.494362  [   64/   88]
per-ex loss: 0.509996  [   65/   88]
per-ex loss: 0.769380  [   66/   88]
per-ex loss: 0.675223  [   67/   88]
per-ex loss: 0.690110  [   68/   88]
per-ex loss: 0.658030  [   69/   88]
per-ex loss: 0.701053  [   70/   88]
per-ex loss: 0.651531  [   71/   88]
per-ex loss: 0.606629  [   72/   88]
per-ex loss: 0.468268  [   73/   88]
per-ex loss: 0.477231  [   74/   88]
per-ex loss: 0.487599  [   75/   88]
per-ex loss: 0.478506  [   76/   88]
per-ex loss: 0.663429  [   77/   88]
per-ex loss: 0.444311  [   78/   88]
per-ex loss: 0.497937  [   79/   88]
per-ex loss: 0.472892  [   80/   88]
per-ex loss: 0.766464  [   81/   88]
per-ex loss: 0.707877  [   82/   88]
per-ex loss: 0.772400  [   83/   88]
per-ex loss: 0.639935  [   84/   88]
per-ex loss: 0.554296  [   85/   88]
per-ex loss: 0.535775  [   86/   88]
per-ex loss: 0.600459  [   87/   88]
per-ex loss: 0.720900  [   88/   88]
Train Error: Avg loss: 0.58734964
validation Error: 
 Avg loss: 0.59949268 
 F1: 0.428573 
 Precision: 0.478139 
 Recall: 0.388318
 IoU: 0.272728

test Error: 
 Avg loss: 0.55664788 
 F1: 0.503166 
 Precision: 0.492414 
 Recall: 0.514398
 IoU: 0.336154

We have finished training iteration 3
per-ex loss: 0.738945  [    1/   88]
per-ex loss: 0.499633  [    2/   88]
per-ex loss: 0.666637  [    3/   88]
per-ex loss: 0.729330  [    4/   88]
per-ex loss: 0.659317  [    5/   88]
per-ex loss: 0.534182  [    6/   88]
per-ex loss: 0.653738  [    7/   88]
per-ex loss: 0.457765  [    8/   88]
per-ex loss: 0.684004  [    9/   88]
per-ex loss: 0.577933  [   10/   88]
per-ex loss: 0.526922  [   11/   88]
per-ex loss: 0.441808  [   12/   88]
per-ex loss: 0.635131  [   13/   88]
per-ex loss: 0.484188  [   14/   88]
per-ex loss: 0.524991  [   15/   88]
per-ex loss: 0.509555  [   16/   88]
per-ex loss: 0.405896  [   17/   88]
per-ex loss: 0.445328  [   18/   88]
per-ex loss: 0.456359  [   19/   88]
per-ex loss: 0.685827  [   20/   88]
per-ex loss: 0.498887  [   21/   88]
per-ex loss: 0.566003  [   22/   88]
per-ex loss: 0.472177  [   23/   88]
per-ex loss: 0.697055  [   24/   88]
per-ex loss: 0.733820  [   25/   88]
per-ex loss: 0.421690  [   26/   88]
per-ex loss: 0.655641  [   27/   88]
per-ex loss: 0.389364  [   28/   88]
per-ex loss: 0.674072  [   29/   88]
per-ex loss: 0.508105  [   30/   88]
per-ex loss: 0.496404  [   31/   88]
per-ex loss: 0.367815  [   32/   88]
per-ex loss: 0.460456  [   33/   88]
per-ex loss: 0.621889  [   34/   88]
per-ex loss: 0.654876  [   35/   88]
per-ex loss: 0.773246  [   36/   88]
per-ex loss: 0.499304  [   37/   88]
per-ex loss: 0.511230  [   38/   88]
per-ex loss: 0.473925  [   39/   88]
per-ex loss: 0.611879  [   40/   88]
per-ex loss: 0.601336  [   41/   88]
per-ex loss: 0.620638  [   42/   88]
per-ex loss: 0.563508  [   43/   88]
per-ex loss: 0.668764  [   44/   88]
per-ex loss: 0.611173  [   45/   88]
per-ex loss: 0.442743  [   46/   88]
per-ex loss: 0.724695  [   47/   88]
per-ex loss: 0.675727  [   48/   88]
per-ex loss: 0.404703  [   49/   88]
per-ex loss: 0.756052  [   50/   88]
per-ex loss: 0.468907  [   51/   88]
per-ex loss: 0.476020  [   52/   88]
per-ex loss: 0.705013  [   53/   88]
per-ex loss: 0.637378  [   54/   88]
per-ex loss: 0.593698  [   55/   88]
per-ex loss: 0.623756  [   56/   88]
per-ex loss: 0.550232  [   57/   88]
per-ex loss: 0.528723  [   58/   88]
per-ex loss: 0.444300  [   59/   88]
per-ex loss: 0.619184  [   60/   88]
per-ex loss: 0.642208  [   61/   88]
per-ex loss: 0.433013  [   62/   88]
per-ex loss: 0.511306  [   63/   88]
per-ex loss: 0.677576  [   64/   88]
per-ex loss: 0.385200  [   65/   88]
per-ex loss: 0.733865  [   66/   88]
per-ex loss: 0.579608  [   67/   88]
per-ex loss: 0.509012  [   68/   88]
per-ex loss: 0.520665  [   69/   88]
per-ex loss: 0.678811  [   70/   88]
per-ex loss: 0.491137  [   71/   88]
per-ex loss: 0.743634  [   72/   88]
per-ex loss: 0.707548  [   73/   88]
per-ex loss: 0.701774  [   74/   88]
per-ex loss: 0.700119  [   75/   88]
per-ex loss: 0.470563  [   76/   88]
per-ex loss: 0.548367  [   77/   88]
per-ex loss: 0.464079  [   78/   88]
per-ex loss: 0.621359  [   79/   88]
per-ex loss: 0.518487  [   80/   88]
per-ex loss: 0.608171  [   81/   88]
per-ex loss: 0.439445  [   82/   88]
per-ex loss: 0.635456  [   83/   88]
per-ex loss: 0.554972  [   84/   88]
per-ex loss: 0.573302  [   85/   88]
per-ex loss: 0.707712  [   86/   88]
per-ex loss: 0.705155  [   87/   88]
per-ex loss: 0.576932  [   88/   88]
Train Error: Avg loss: 0.57456086
validation Error: 
 Avg loss: 0.60451336 
 F1: 0.420435 
 Precision: 0.608412 
 Recall: 0.321197
 IoU: 0.266171

test Error: 
 Avg loss: 0.55561107 
 F1: 0.502785 
 Precision: 0.625182 
 Recall: 0.420467
 IoU: 0.335814

We have finished training iteration 4
per-ex loss: 0.625112  [    1/   88]
per-ex loss: 0.432240  [    2/   88]
per-ex loss: 0.732806  [    3/   88]
per-ex loss: 0.653561  [    4/   88]
per-ex loss: 0.681606  [    5/   88]
per-ex loss: 0.439547  [    6/   88]
per-ex loss: 0.397231  [    7/   88]
per-ex loss: 0.527710  [    8/   88]
per-ex loss: 0.632156  [    9/   88]
per-ex loss: 0.727190  [   10/   88]
per-ex loss: 0.710452  [   11/   88]
per-ex loss: 0.508495  [   12/   88]
per-ex loss: 0.605328  [   13/   88]
per-ex loss: 0.699891  [   14/   88]
per-ex loss: 0.472041  [   15/   88]
per-ex loss: 0.570877  [   16/   88]
per-ex loss: 0.510707  [   17/   88]
per-ex loss: 0.447463  [   18/   88]
per-ex loss: 0.464823  [   19/   88]
per-ex loss: 0.634484  [   20/   88]
per-ex loss: 0.450032  [   21/   88]
per-ex loss: 0.477137  [   22/   88]
per-ex loss: 0.587038  [   23/   88]
per-ex loss: 0.431533  [   24/   88]
per-ex loss: 0.602854  [   25/   88]
per-ex loss: 0.422288  [   26/   88]
per-ex loss: 0.463842  [   27/   88]
per-ex loss: 0.431306  [   28/   88]
per-ex loss: 0.450435  [   29/   88]
per-ex loss: 0.559973  [   30/   88]
per-ex loss: 0.668475  [   31/   88]
per-ex loss: 0.543886  [   32/   88]
per-ex loss: 0.506352  [   33/   88]
per-ex loss: 0.730379  [   34/   88]
per-ex loss: 0.512753  [   35/   88]
per-ex loss: 0.647017  [   36/   88]
per-ex loss: 0.709523  [   37/   88]
per-ex loss: 0.656314  [   38/   88]
per-ex loss: 0.476986  [   39/   88]
per-ex loss: 0.448184  [   40/   88]
per-ex loss: 0.521013  [   41/   88]
per-ex loss: 0.636752  [   42/   88]
per-ex loss: 0.453749  [   43/   88]
per-ex loss: 0.529448  [   44/   88]
per-ex loss: 0.778518  [   45/   88]
per-ex loss: 0.518127  [   46/   88]
per-ex loss: 0.706435  [   47/   88]
per-ex loss: 0.616205  [   48/   88]
per-ex loss: 0.675650  [   49/   88]
per-ex loss: 0.485078  [   50/   88]
per-ex loss: 0.441405  [   51/   88]
per-ex loss: 0.734092  [   52/   88]
per-ex loss: 0.428316  [   53/   88]
per-ex loss: 0.641406  [   54/   88]
per-ex loss: 0.687698  [   55/   88]
per-ex loss: 0.438031  [   56/   88]
per-ex loss: 0.625681  [   57/   88]
per-ex loss: 0.539453  [   58/   88]
per-ex loss: 0.644116  [   59/   88]
per-ex loss: 0.527776  [   60/   88]
per-ex loss: 0.496067  [   61/   88]
per-ex loss: 0.472163  [   62/   88]
per-ex loss: 0.633358  [   63/   88]
per-ex loss: 0.503682  [   64/   88]
per-ex loss: 0.723819  [   65/   88]
per-ex loss: 0.542136  [   66/   88]
per-ex loss: 0.464795  [   67/   88]
per-ex loss: 0.405158  [   68/   88]
per-ex loss: 0.561260  [   69/   88]
per-ex loss: 0.732221  [   70/   88]
per-ex loss: 0.381923  [   71/   88]
per-ex loss: 0.616952  [   72/   88]
per-ex loss: 0.603185  [   73/   88]
per-ex loss: 0.676385  [   74/   88]
per-ex loss: 0.583360  [   75/   88]
per-ex loss: 0.686534  [   76/   88]
per-ex loss: 0.517231  [   77/   88]
per-ex loss: 0.563546  [   78/   88]
per-ex loss: 0.474473  [   79/   88]
per-ex loss: 0.530167  [   80/   88]
per-ex loss: 0.634667  [   81/   88]
per-ex loss: 0.638668  [   82/   88]
per-ex loss: 0.710759  [   83/   88]
per-ex loss: 0.612560  [   84/   88]
per-ex loss: 0.514702  [   85/   88]
per-ex loss: 0.378272  [   86/   88]
per-ex loss: 0.631059  [   87/   88]
per-ex loss: 0.542380  [   88/   88]
Train Error: Avg loss: 0.56489157
validation Error: 
 Avg loss: 0.61116378 
 F1: 0.415027 
 Precision: 0.634937 
 Recall: 0.308261
 IoU: 0.261851

test Error: 
 Avg loss: 0.55969453 
 F1: 0.491876 
 Precision: 0.652928 
 Recall: 0.394554
 IoU: 0.326151

We have finished training iteration 5
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_1_.pth
per-ex loss: 0.703940  [    1/   88]
per-ex loss: 0.756241  [    2/   88]
per-ex loss: 0.439060  [    3/   88]
per-ex loss: 0.673675  [    4/   88]
per-ex loss: 0.565819  [    5/   88]
per-ex loss: 0.596946  [    6/   88]
per-ex loss: 0.589238  [    7/   88]
per-ex loss: 0.440923  [    8/   88]
per-ex loss: 0.431440  [    9/   88]
per-ex loss: 0.558934  [   10/   88]
per-ex loss: 0.611116  [   11/   88]
per-ex loss: 0.433354  [   12/   88]
per-ex loss: 0.437001  [   13/   88]
per-ex loss: 0.696733  [   14/   88]
per-ex loss: 0.651988  [   15/   88]
per-ex loss: 0.548712  [   16/   88]
per-ex loss: 0.462197  [   17/   88]
per-ex loss: 0.663491  [   18/   88]
per-ex loss: 0.639530  [   19/   88]
per-ex loss: 0.571207  [   20/   88]
per-ex loss: 0.516438  [   21/   88]
per-ex loss: 0.392051  [   22/   88]
per-ex loss: 0.609310  [   23/   88]
per-ex loss: 0.441041  [   24/   88]
per-ex loss: 0.641201  [   25/   88]
per-ex loss: 0.501890  [   26/   88]
per-ex loss: 0.530768  [   27/   88]
per-ex loss: 0.642844  [   28/   88]
per-ex loss: 0.409029  [   29/   88]
per-ex loss: 0.728461  [   30/   88]
per-ex loss: 0.611228  [   31/   88]
per-ex loss: 0.442275  [   32/   88]
per-ex loss: 0.495528  [   33/   88]
per-ex loss: 0.701047  [   34/   88]
per-ex loss: 0.483293  [   35/   88]
per-ex loss: 0.509665  [   36/   88]
per-ex loss: 0.629195  [   37/   88]
per-ex loss: 0.741459  [   38/   88]
per-ex loss: 0.617382  [   39/   88]
per-ex loss: 0.469990  [   40/   88]
per-ex loss: 0.516257  [   41/   88]
per-ex loss: 0.628474  [   42/   88]
per-ex loss: 0.709847  [   43/   88]
per-ex loss: 0.465105  [   44/   88]
per-ex loss: 0.471709  [   45/   88]
per-ex loss: 0.673077  [   46/   88]
per-ex loss: 0.384781  [   47/   88]
per-ex loss: 0.546096  [   48/   88]
per-ex loss: 0.538756  [   49/   88]
per-ex loss: 0.689069  [   50/   88]
per-ex loss: 0.745670  [   51/   88]
per-ex loss: 0.492696  [   52/   88]
per-ex loss: 0.545229  [   53/   88]
per-ex loss: 0.524633  [   54/   88]
per-ex loss: 0.500756  [   55/   88]
per-ex loss: 0.606450  [   56/   88]
per-ex loss: 0.576901  [   57/   88]
per-ex loss: 0.743112  [   58/   88]
per-ex loss: 0.384367  [   59/   88]
per-ex loss: 0.494152  [   60/   88]
per-ex loss: 0.653772  [   61/   88]
per-ex loss: 0.628558  [   62/   88]
per-ex loss: 0.640998  [   63/   88]
per-ex loss: 0.484152  [   64/   88]
per-ex loss: 0.540127  [   65/   88]
per-ex loss: 0.744349  [   66/   88]
per-ex loss: 0.452997  [   67/   88]
per-ex loss: 0.733979  [   68/   88]
per-ex loss: 0.469640  [   69/   88]
per-ex loss: 0.667666  [   70/   88]
per-ex loss: 0.435032  [   71/   88]
per-ex loss: 0.564350  [   72/   88]
per-ex loss: 0.477544  [   73/   88]
per-ex loss: 0.722681  [   74/   88]
per-ex loss: 0.587358  [   75/   88]
per-ex loss: 0.666426  [   76/   88]
per-ex loss: 0.514065  [   77/   88]
per-ex loss: 0.449943  [   78/   88]
per-ex loss: 0.505399  [   79/   88]
per-ex loss: 0.411124  [   80/   88]
per-ex loss: 0.422018  [   81/   88]
per-ex loss: 0.424403  [   82/   88]
per-ex loss: 0.582734  [   83/   88]
per-ex loss: 0.455691  [   84/   88]
per-ex loss: 0.500609  [   85/   88]
per-ex loss: 0.389411  [   86/   88]
per-ex loss: 0.464890  [   87/   88]
per-ex loss: 0.442948  [   88/   88]
Train Error: Avg loss: 0.55517771
validation Error: 
 Avg loss: 0.61079691 
 F1: 0.414312 
 Precision: 0.682548 
 Recall: 0.297425
 IoU: 0.261282

test Error: 
 Avg loss: 0.56170704 
 F1: 0.491760 
 Precision: 0.687477 
 Recall: 0.382786
 IoU: 0.326049

We have finished training iteration 6
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_2_.pth
per-ex loss: 0.470413  [    1/   88]
per-ex loss: 0.579667  [    2/   88]
per-ex loss: 0.646750  [    3/   88]
per-ex loss: 0.512735  [    4/   88]
per-ex loss: 0.538397  [    5/   88]
per-ex loss: 0.393886  [    6/   88]
per-ex loss: 0.474457  [    7/   88]
per-ex loss: 0.697061  [    8/   88]
per-ex loss: 0.416623  [    9/   88]
per-ex loss: 0.584853  [   10/   88]
per-ex loss: 0.594459  [   11/   88]
per-ex loss: 0.525685  [   12/   88]
per-ex loss: 0.636518  [   13/   88]
per-ex loss: 0.514552  [   14/   88]
per-ex loss: 0.473395  [   15/   88]
per-ex loss: 0.597004  [   16/   88]
per-ex loss: 0.706889  [   17/   88]
per-ex loss: 0.431093  [   18/   88]
per-ex loss: 0.494979  [   19/   88]
per-ex loss: 0.498163  [   20/   88]
per-ex loss: 0.677392  [   21/   88]
per-ex loss: 0.601954  [   22/   88]
per-ex loss: 0.442882  [   23/   88]
per-ex loss: 0.507912  [   24/   88]
per-ex loss: 0.739494  [   25/   88]
per-ex loss: 0.663490  [   26/   88]
per-ex loss: 0.658711  [   27/   88]
per-ex loss: 0.573300  [   28/   88]
per-ex loss: 0.555601  [   29/   88]
per-ex loss: 0.425406  [   30/   88]
per-ex loss: 0.400667  [   31/   88]
per-ex loss: 0.646701  [   32/   88]
per-ex loss: 0.501614  [   33/   88]
per-ex loss: 0.456557  [   34/   88]
per-ex loss: 0.383449  [   35/   88]
per-ex loss: 0.503618  [   36/   88]
per-ex loss: 0.616187  [   37/   88]
per-ex loss: 0.494035  [   38/   88]
per-ex loss: 0.503502  [   39/   88]
per-ex loss: 0.776918  [   40/   88]
per-ex loss: 0.475806  [   41/   88]
per-ex loss: 0.519809  [   42/   88]
per-ex loss: 0.391286  [   43/   88]
per-ex loss: 0.390768  [   44/   88]
per-ex loss: 0.488997  [   45/   88]
per-ex loss: 0.615477  [   46/   88]
per-ex loss: 0.430803  [   47/   88]
per-ex loss: 0.647792  [   48/   88]
per-ex loss: 0.509622  [   49/   88]
per-ex loss: 0.518971  [   50/   88]
per-ex loss: 0.634362  [   51/   88]
per-ex loss: 0.434531  [   52/   88]
per-ex loss: 0.735425  [   53/   88]
per-ex loss: 0.461898  [   54/   88]
per-ex loss: 0.648254  [   55/   88]
per-ex loss: 0.467696  [   56/   88]
per-ex loss: 0.510250  [   57/   88]
per-ex loss: 0.663477  [   58/   88]
per-ex loss: 0.713828  [   59/   88]
per-ex loss: 0.365751  [   60/   88]
per-ex loss: 0.707700  [   61/   88]
per-ex loss: 0.611127  [   62/   88]
per-ex loss: 0.729850  [   63/   88]
per-ex loss: 0.409732  [   64/   88]
per-ex loss: 0.714712  [   65/   88]
per-ex loss: 0.566969  [   66/   88]
per-ex loss: 0.664510  [   67/   88]
per-ex loss: 0.546044  [   68/   88]
per-ex loss: 0.490460  [   69/   88]
per-ex loss: 0.482540  [   70/   88]
per-ex loss: 0.624208  [   71/   88]
per-ex loss: 0.457782  [   72/   88]
per-ex loss: 0.506529  [   73/   88]
per-ex loss: 0.423792  [   74/   88]
per-ex loss: 0.593144  [   75/   88]
per-ex loss: 0.679046  [   76/   88]
per-ex loss: 0.495389  [   77/   88]
per-ex loss: 0.453380  [   78/   88]
per-ex loss: 0.548105  [   79/   88]
per-ex loss: 0.388756  [   80/   88]
per-ex loss: 0.689464  [   81/   88]
per-ex loss: 0.729344  [   82/   88]
per-ex loss: 0.736141  [   83/   88]
per-ex loss: 0.621880  [   84/   88]
per-ex loss: 0.673908  [   85/   88]
per-ex loss: 0.455876  [   86/   88]
per-ex loss: 0.591065  [   87/   88]
per-ex loss: 0.755228  [   88/   88]
Train Error: Avg loss: 0.55555062
validation Error: 
 Avg loss: 0.60115593 
 F1: 0.418439 
 Precision: 0.597090 
 Recall: 0.322073
 IoU: 0.264573

test Error: 
 Avg loss: 0.54848894 
 F1: 0.503519 
 Precision: 0.618669 
 Recall: 0.424507
 IoU: 0.336469

We have finished training iteration 7
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_5_.pth
per-ex loss: 0.754784  [    1/   88]
per-ex loss: 0.602996  [    2/   88]
per-ex loss: 0.483228  [    3/   88]
per-ex loss: 0.437477  [    4/   88]
per-ex loss: 0.663850  [    5/   88]
per-ex loss: 0.423188  [    6/   88]
per-ex loss: 0.613204  [    7/   88]
per-ex loss: 0.441474  [    8/   88]
per-ex loss: 0.596015  [    9/   88]
per-ex loss: 0.492799  [   10/   88]
per-ex loss: 0.494141  [   11/   88]
per-ex loss: 0.485714  [   12/   88]
per-ex loss: 0.727360  [   13/   88]
per-ex loss: 0.447982  [   14/   88]
per-ex loss: 0.716942  [   15/   88]
per-ex loss: 0.472625  [   16/   88]
per-ex loss: 0.725816  [   17/   88]
per-ex loss: 0.659561  [   18/   88]
per-ex loss: 0.482515  [   19/   88]
per-ex loss: 0.692159  [   20/   88]
per-ex loss: 0.642080  [   21/   88]
per-ex loss: 0.414132  [   22/   88]
per-ex loss: 0.600667  [   23/   88]
per-ex loss: 0.485159  [   24/   88]
per-ex loss: 0.525340  [   25/   88]
per-ex loss: 0.647758  [   26/   88]
per-ex loss: 0.402779  [   27/   88]
per-ex loss: 0.579060  [   28/   88]
per-ex loss: 0.441533  [   29/   88]
per-ex loss: 0.458544  [   30/   88]
per-ex loss: 0.703512  [   31/   88]
per-ex loss: 0.517596  [   32/   88]
per-ex loss: 0.614684  [   33/   88]
per-ex loss: 0.675997  [   34/   88]
per-ex loss: 0.704869  [   35/   88]
per-ex loss: 0.676584  [   36/   88]
per-ex loss: 0.434602  [   37/   88]
per-ex loss: 0.737586  [   38/   88]
per-ex loss: 0.716552  [   39/   88]
per-ex loss: 0.420083  [   40/   88]
per-ex loss: 0.733137  [   41/   88]
per-ex loss: 0.467315  [   42/   88]
per-ex loss: 0.569299  [   43/   88]
per-ex loss: 0.509776  [   44/   88]
per-ex loss: 0.507005  [   45/   88]
per-ex loss: 0.413765  [   46/   88]
per-ex loss: 0.522765  [   47/   88]
per-ex loss: 0.402382  [   48/   88]
per-ex loss: 0.524797  [   49/   88]
per-ex loss: 0.461079  [   50/   88]
per-ex loss: 0.494694  [   51/   88]
per-ex loss: 0.383099  [   52/   88]
per-ex loss: 0.676125  [   53/   88]
per-ex loss: 0.552246  [   54/   88]
per-ex loss: 0.605240  [   55/   88]
per-ex loss: 0.458504  [   56/   88]
per-ex loss: 0.616916  [   57/   88]
per-ex loss: 0.580400  [   58/   88]
per-ex loss: 0.433942  [   59/   88]
per-ex loss: 0.388573  [   60/   88]
per-ex loss: 0.471489  [   61/   88]
per-ex loss: 0.487599  [   62/   88]
per-ex loss: 0.480802  [   63/   88]
per-ex loss: 0.584000  [   64/   88]
per-ex loss: 0.571748  [   65/   88]
per-ex loss: 0.671250  [   66/   88]
per-ex loss: 0.697257  [   67/   88]
per-ex loss: 0.674131  [   68/   88]
per-ex loss: 0.423799  [   69/   88]
per-ex loss: 0.402923  [   70/   88]
per-ex loss: 0.467072  [   71/   88]
per-ex loss: 0.576912  [   72/   88]
per-ex loss: 0.430377  [   73/   88]
per-ex loss: 0.584803  [   74/   88]
per-ex loss: 0.627600  [   75/   88]
per-ex loss: 0.696161  [   76/   88]
per-ex loss: 0.364552  [   77/   88]
per-ex loss: 0.456244  [   78/   88]
per-ex loss: 0.622998  [   79/   88]
per-ex loss: 0.408274  [   80/   88]
per-ex loss: 0.641730  [   81/   88]
per-ex loss: 0.527185  [   82/   88]
per-ex loss: 0.748849  [   83/   88]
per-ex loss: 0.624817  [   84/   88]
per-ex loss: 0.542641  [   85/   88]
per-ex loss: 0.611170  [   86/   88]
per-ex loss: 0.486706  [   87/   88]
per-ex loss: 0.526622  [   88/   88]
Train Error: Avg loss: 0.55140582
validation Error: 
 Avg loss: 0.57182550 
 F1: 0.454362 
 Precision: 0.612939 
 Recall: 0.360973
 IoU: 0.293964

test Error: 
 Avg loss: 0.52065555 
 F1: 0.533766 
 Precision: 0.636038 
 Recall: 0.459828
 IoU: 0.364039

We have finished training iteration 8
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_6_.pth
per-ex loss: 0.418201  [    1/   88]
per-ex loss: 0.643784  [    2/   88]
per-ex loss: 0.489396  [    3/   88]
per-ex loss: 0.373179  [    4/   88]
per-ex loss: 0.599979  [    5/   88]
per-ex loss: 0.388320  [    6/   88]
per-ex loss: 0.513933  [    7/   88]
per-ex loss: 0.641332  [    8/   88]
per-ex loss: 0.454836  [    9/   88]
per-ex loss: 0.397235  [   10/   88]
per-ex loss: 0.544238  [   11/   88]
per-ex loss: 0.594805  [   12/   88]
per-ex loss: 0.466434  [   13/   88]
per-ex loss: 0.501074  [   14/   88]
per-ex loss: 0.478184  [   15/   88]
per-ex loss: 0.695662  [   16/   88]
per-ex loss: 0.407365  [   17/   88]
per-ex loss: 0.404267  [   18/   88]
per-ex loss: 0.688212  [   19/   88]
per-ex loss: 0.708393  [   20/   88]
per-ex loss: 0.408256  [   21/   88]
per-ex loss: 0.606825  [   22/   88]
per-ex loss: 0.606295  [   23/   88]
per-ex loss: 0.748127  [   24/   88]
per-ex loss: 0.611153  [   25/   88]
per-ex loss: 0.469895  [   26/   88]
per-ex loss: 0.624529  [   27/   88]
per-ex loss: 0.605660  [   28/   88]
per-ex loss: 0.598145  [   29/   88]
per-ex loss: 0.608030  [   30/   88]
per-ex loss: 0.510155  [   31/   88]
per-ex loss: 0.468840  [   32/   88]
per-ex loss: 0.428463  [   33/   88]
per-ex loss: 0.609692  [   34/   88]
per-ex loss: 0.683175  [   35/   88]
per-ex loss: 0.650833  [   36/   88]
per-ex loss: 0.551938  [   37/   88]
per-ex loss: 0.539725  [   38/   88]
per-ex loss: 0.512853  [   39/   88]
per-ex loss: 0.526398  [   40/   88]
per-ex loss: 0.509681  [   41/   88]
per-ex loss: 0.649359  [   42/   88]
per-ex loss: 0.603579  [   43/   88]
per-ex loss: 0.695146  [   44/   88]
per-ex loss: 0.697234  [   45/   88]
per-ex loss: 0.697918  [   46/   88]
per-ex loss: 0.717113  [   47/   88]
per-ex loss: 0.494491  [   48/   88]
per-ex loss: 0.749076  [   49/   88]
per-ex loss: 0.587020  [   50/   88]
per-ex loss: 0.695007  [   51/   88]
per-ex loss: 0.502926  [   52/   88]
per-ex loss: 0.508610  [   53/   88]
per-ex loss: 0.588578  [   54/   88]
per-ex loss: 0.693041  [   55/   88]
per-ex loss: 0.615825  [   56/   88]
per-ex loss: 0.649528  [   57/   88]
per-ex loss: 0.422784  [   58/   88]
per-ex loss: 0.410404  [   59/   88]
per-ex loss: 0.545253  [   60/   88]
per-ex loss: 0.581706  [   61/   88]
per-ex loss: 0.390747  [   62/   88]
per-ex loss: 0.674521  [   63/   88]
per-ex loss: 0.440256  [   64/   88]
per-ex loss: 0.383457  [   65/   88]
per-ex loss: 0.459351  [   66/   88]
per-ex loss: 0.431196  [   67/   88]
per-ex loss: 0.748954  [   68/   88]
per-ex loss: 0.585650  [   69/   88]
per-ex loss: 0.487468  [   70/   88]
per-ex loss: 0.456305  [   71/   88]
per-ex loss: 0.501052  [   72/   88]
per-ex loss: 0.709600  [   73/   88]
per-ex loss: 0.421180  [   74/   88]
per-ex loss: 0.464766  [   75/   88]
per-ex loss: 0.510097  [   76/   88]
per-ex loss: 0.399823  [   77/   88]
per-ex loss: 0.631925  [   78/   88]
per-ex loss: 0.556832  [   79/   88]
per-ex loss: 0.424300  [   80/   88]
per-ex loss: 0.464004  [   81/   88]
per-ex loss: 0.429411  [   82/   88]
per-ex loss: 0.470848  [   83/   88]
per-ex loss: 0.456088  [   84/   88]
per-ex loss: 0.370304  [   85/   88]
per-ex loss: 0.683055  [   86/   88]
per-ex loss: 0.403436  [   87/   88]
per-ex loss: 0.528488  [   88/   88]
Train Error: Avg loss: 0.54403646
validation Error: 
 Avg loss: 0.59324665 
 F1: 0.429773 
 Precision: 0.656957 
 Recall: 0.319341
 IoU: 0.273701

test Error: 
 Avg loss: 0.54195824 
 F1: 0.510380 
 Precision: 0.680098 
 Recall: 0.408452
 IoU: 0.342625

We have finished training iteration 9
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_4_.pth
per-ex loss: 0.646271  [    1/   88]
per-ex loss: 0.441052  [    2/   88]
per-ex loss: 0.449327  [    3/   88]
per-ex loss: 0.551650  [    4/   88]
per-ex loss: 0.452797  [    5/   88]
per-ex loss: 0.399029  [    6/   88]
per-ex loss: 0.409364  [    7/   88]
per-ex loss: 0.374169  [    8/   88]
per-ex loss: 0.658491  [    9/   88]
per-ex loss: 0.486191  [   10/   88]
per-ex loss: 0.379608  [   11/   88]
per-ex loss: 0.588317  [   12/   88]
per-ex loss: 0.443809  [   13/   88]
per-ex loss: 0.638685  [   14/   88]
per-ex loss: 0.672374  [   15/   88]
per-ex loss: 0.430159  [   16/   88]
per-ex loss: 0.614429  [   17/   88]
per-ex loss: 0.476201  [   18/   88]
per-ex loss: 0.428618  [   19/   88]
per-ex loss: 0.663329  [   20/   88]
per-ex loss: 0.600834  [   21/   88]
per-ex loss: 0.497008  [   22/   88]
per-ex loss: 0.431929  [   23/   88]
per-ex loss: 0.416468  [   24/   88]
per-ex loss: 0.592894  [   25/   88]
per-ex loss: 0.415909  [   26/   88]
per-ex loss: 0.440683  [   27/   88]
per-ex loss: 0.425913  [   28/   88]
per-ex loss: 0.500285  [   29/   88]
per-ex loss: 0.598508  [   30/   88]
per-ex loss: 0.667605  [   31/   88]
per-ex loss: 0.670159  [   32/   88]
per-ex loss: 0.397375  [   33/   88]
per-ex loss: 0.706299  [   34/   88]
per-ex loss: 0.587965  [   35/   88]
per-ex loss: 0.440202  [   36/   88]
per-ex loss: 0.549312  [   37/   88]
per-ex loss: 0.632617  [   38/   88]
per-ex loss: 0.754948  [   39/   88]
per-ex loss: 0.481262  [   40/   88]
per-ex loss: 0.668737  [   41/   88]
per-ex loss: 0.490334  [   42/   88]
per-ex loss: 0.470258  [   43/   88]
per-ex loss: 0.464792  [   44/   88]
per-ex loss: 0.492858  [   45/   88]
per-ex loss: 0.599573  [   46/   88]
per-ex loss: 0.492972  [   47/   88]
per-ex loss: 0.770757  [   48/   88]
per-ex loss: 0.646448  [   49/   88]
per-ex loss: 0.709941  [   50/   88]
per-ex loss: 0.457770  [   51/   88]
per-ex loss: 0.680200  [   52/   88]
per-ex loss: 0.525784  [   53/   88]
per-ex loss: 0.477443  [   54/   88]
per-ex loss: 0.718914  [   55/   88]
per-ex loss: 0.578541  [   56/   88]
per-ex loss: 0.571028  [   57/   88]
per-ex loss: 0.579965  [   58/   88]
per-ex loss: 0.401515  [   59/   88]
per-ex loss: 0.707974  [   60/   88]
per-ex loss: 0.433655  [   61/   88]
per-ex loss: 0.497742  [   62/   88]
per-ex loss: 0.592233  [   63/   88]
per-ex loss: 0.492442  [   64/   88]
per-ex loss: 0.614243  [   65/   88]
per-ex loss: 0.402499  [   66/   88]
per-ex loss: 0.372082  [   67/   88]
per-ex loss: 0.512414  [   68/   88]
per-ex loss: 0.522541  [   69/   88]
per-ex loss: 0.466522  [   70/   88]
per-ex loss: 0.732017  [   71/   88]
per-ex loss: 0.662230  [   72/   88]
per-ex loss: 0.647536  [   73/   88]
per-ex loss: 0.607571  [   74/   88]
per-ex loss: 0.613985  [   75/   88]
per-ex loss: 0.527235  [   76/   88]
per-ex loss: 0.453606  [   77/   88]
per-ex loss: 0.463243  [   78/   88]
per-ex loss: 0.361457  [   79/   88]
per-ex loss: 0.514534  [   80/   88]
per-ex loss: 0.649385  [   81/   88]
per-ex loss: 0.708615  [   82/   88]
per-ex loss: 0.630910  [   83/   88]
per-ex loss: 0.445462  [   84/   88]
per-ex loss: 0.680318  [   85/   88]
per-ex loss: 0.456851  [   86/   88]
per-ex loss: 0.654406  [   87/   88]
per-ex loss: 0.393322  [   88/   88]
Train Error: Avg loss: 0.54121481
validation Error: 
 Avg loss: 0.57977057 
 F1: 0.446287 
 Precision: 0.629969 
 Recall: 0.345537
 IoU: 0.287239

test Error: 
 Avg loss: 0.53263140 
 F1: 0.522591 
 Precision: 0.638148 
 Recall: 0.442468
 IoU: 0.353721

We have finished training iteration 10
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_7_.pth
per-ex loss: 0.418437  [    1/   88]
per-ex loss: 0.464668  [    2/   88]
per-ex loss: 0.490820  [    3/   88]
per-ex loss: 0.422575  [    4/   88]
per-ex loss: 0.371399  [    5/   88]
per-ex loss: 0.575683  [    6/   88]
per-ex loss: 0.710529  [    7/   88]
per-ex loss: 0.482998  [    8/   88]
per-ex loss: 0.471157  [    9/   88]
per-ex loss: 0.571496  [   10/   88]
per-ex loss: 0.465039  [   11/   88]
per-ex loss: 0.540023  [   12/   88]
per-ex loss: 0.645836  [   13/   88]
per-ex loss: 0.683172  [   14/   88]
per-ex loss: 0.459327  [   15/   88]
per-ex loss: 0.456733  [   16/   88]
per-ex loss: 0.390028  [   17/   88]
per-ex loss: 0.610892  [   18/   88]
per-ex loss: 0.415776  [   19/   88]
per-ex loss: 0.647106  [   20/   88]
per-ex loss: 0.579850  [   21/   88]
per-ex loss: 0.755679  [   22/   88]
per-ex loss: 0.633924  [   23/   88]
per-ex loss: 0.620597  [   24/   88]
per-ex loss: 0.513634  [   25/   88]
per-ex loss: 0.507968  [   26/   88]
per-ex loss: 0.597800  [   27/   88]
per-ex loss: 0.419635  [   28/   88]
per-ex loss: 0.512718  [   29/   88]
per-ex loss: 0.381001  [   30/   88]
per-ex loss: 0.459160  [   31/   88]
per-ex loss: 0.474368  [   32/   88]
per-ex loss: 0.688194  [   33/   88]
per-ex loss: 0.619798  [   34/   88]
per-ex loss: 0.641503  [   35/   88]
per-ex loss: 0.481957  [   36/   88]
per-ex loss: 0.642710  [   37/   88]
per-ex loss: 0.658054  [   38/   88]
per-ex loss: 0.341312  [   39/   88]
per-ex loss: 0.505104  [   40/   88]
per-ex loss: 0.590975  [   41/   88]
per-ex loss: 0.480230  [   42/   88]
per-ex loss: 0.500144  [   43/   88]
per-ex loss: 0.691789  [   44/   88]
per-ex loss: 0.495138  [   45/   88]
per-ex loss: 0.393044  [   46/   88]
per-ex loss: 0.715002  [   47/   88]
per-ex loss: 0.414847  [   48/   88]
per-ex loss: 0.454943  [   49/   88]
per-ex loss: 0.418078  [   50/   88]
per-ex loss: 0.575828  [   51/   88]
per-ex loss: 0.632875  [   52/   88]
per-ex loss: 0.590306  [   53/   88]
per-ex loss: 0.676498  [   54/   88]
per-ex loss: 0.585609  [   55/   88]
per-ex loss: 0.391780  [   56/   88]
per-ex loss: 0.435035  [   57/   88]
per-ex loss: 0.639884  [   58/   88]
per-ex loss: 0.693051  [   59/   88]
per-ex loss: 0.691101  [   60/   88]
per-ex loss: 0.579019  [   61/   88]
per-ex loss: 0.459274  [   62/   88]
per-ex loss: 0.464541  [   63/   88]
per-ex loss: 0.423379  [   64/   88]
per-ex loss: 0.625969  [   65/   88]
per-ex loss: 0.383236  [   66/   88]
per-ex loss: 0.463705  [   67/   88]
per-ex loss: 0.599674  [   68/   88]
per-ex loss: 0.604704  [   69/   88]
per-ex loss: 0.614581  [   70/   88]
per-ex loss: 0.396065  [   71/   88]
per-ex loss: 0.510538  [   72/   88]
per-ex loss: 0.634270  [   73/   88]
per-ex loss: 0.636891  [   74/   88]
per-ex loss: 0.622837  [   75/   88]
per-ex loss: 0.587129  [   76/   88]
per-ex loss: 0.397984  [   77/   88]
per-ex loss: 0.465142  [   78/   88]
per-ex loss: 0.521153  [   79/   88]
per-ex loss: 0.566464  [   80/   88]
per-ex loss: 0.384926  [   81/   88]
per-ex loss: 0.506729  [   82/   88]
per-ex loss: 0.709973  [   83/   88]
per-ex loss: 0.416810  [   84/   88]
per-ex loss: 0.473001  [   85/   88]
per-ex loss: 0.633215  [   86/   88]
per-ex loss: 0.505546  [   87/   88]
per-ex loss: 0.628845  [   88/   88]
Train Error: Avg loss: 0.53648201
validation Error: 
 Avg loss: 0.57193583 
 F1: 0.455769 
 Precision: 0.498090 
 Recall: 0.420076
 IoU: 0.295143

test Error: 
 Avg loss: 0.52126387 
 F1: 0.536250 
 Precision: 0.531978 
 Recall: 0.540591
 IoU: 0.366353

We have finished training iteration 11
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_3_.pth
per-ex loss: 0.580880  [    1/   88]
per-ex loss: 0.351383  [    2/   88]
per-ex loss: 0.501559  [    3/   88]
per-ex loss: 0.583933  [    4/   88]
per-ex loss: 0.404094  [    5/   88]
per-ex loss: 0.621282  [    6/   88]
per-ex loss: 0.435533  [    7/   88]
per-ex loss: 0.570040  [    8/   88]
per-ex loss: 0.432760  [    9/   88]
per-ex loss: 0.622450  [   10/   88]
per-ex loss: 0.454948  [   11/   88]
per-ex loss: 0.497126  [   12/   88]
per-ex loss: 0.621855  [   13/   88]
per-ex loss: 0.403233  [   14/   88]
per-ex loss: 0.641480  [   15/   88]
per-ex loss: 0.422325  [   16/   88]
per-ex loss: 0.384794  [   17/   88]
per-ex loss: 0.467548  [   18/   88]
per-ex loss: 0.675609  [   19/   88]
per-ex loss: 0.433283  [   20/   88]
per-ex loss: 0.434584  [   21/   88]
per-ex loss: 0.494075  [   22/   88]
per-ex loss: 0.424847  [   23/   88]
per-ex loss: 0.713116  [   24/   88]
per-ex loss: 0.508945  [   25/   88]
per-ex loss: 0.572747  [   26/   88]
per-ex loss: 0.675231  [   27/   88]
per-ex loss: 0.648320  [   28/   88]
per-ex loss: 0.454839  [   29/   88]
per-ex loss: 0.393299  [   30/   88]
per-ex loss: 0.489726  [   31/   88]
per-ex loss: 0.611104  [   32/   88]
per-ex loss: 0.698731  [   33/   88]
per-ex loss: 0.664177  [   34/   88]
per-ex loss: 0.742801  [   35/   88]
per-ex loss: 0.597301  [   36/   88]
per-ex loss: 0.437127  [   37/   88]
per-ex loss: 0.531026  [   38/   88]
per-ex loss: 0.524267  [   39/   88]
per-ex loss: 0.474972  [   40/   88]
per-ex loss: 0.449140  [   41/   88]
per-ex loss: 0.467430  [   42/   88]
per-ex loss: 0.346087  [   43/   88]
per-ex loss: 0.410001  [   44/   88]
per-ex loss: 0.454432  [   45/   88]
per-ex loss: 0.720994  [   46/   88]
per-ex loss: 0.428211  [   47/   88]
per-ex loss: 0.490762  [   48/   88]
per-ex loss: 0.702152  [   49/   88]
per-ex loss: 0.672615  [   50/   88]
per-ex loss: 0.649307  [   51/   88]
per-ex loss: 0.605152  [   52/   88]
per-ex loss: 0.729802  [   53/   88]
per-ex loss: 0.463514  [   54/   88]
per-ex loss: 0.383532  [   55/   88]
per-ex loss: 0.408816  [   56/   88]
per-ex loss: 0.593956  [   57/   88]
per-ex loss: 0.412097  [   58/   88]
per-ex loss: 0.441910  [   59/   88]
per-ex loss: 0.363484  [   60/   88]
per-ex loss: 0.407742  [   61/   88]
per-ex loss: 0.588172  [   62/   88]
per-ex loss: 0.581643  [   63/   88]
per-ex loss: 0.502739  [   64/   88]
per-ex loss: 0.737416  [   65/   88]
per-ex loss: 0.598068  [   66/   88]
per-ex loss: 0.656471  [   67/   88]
per-ex loss: 0.485694  [   68/   88]
per-ex loss: 0.584819  [   69/   88]
per-ex loss: 0.491535  [   70/   88]
per-ex loss: 0.641729  [   71/   88]
per-ex loss: 0.690485  [   72/   88]
per-ex loss: 0.563064  [   73/   88]
per-ex loss: 0.654901  [   74/   88]
per-ex loss: 0.601429  [   75/   88]
per-ex loss: 0.503782  [   76/   88]
per-ex loss: 0.602737  [   77/   88]
per-ex loss: 0.645964  [   78/   88]
per-ex loss: 0.602974  [   79/   88]
per-ex loss: 0.479640  [   80/   88]
per-ex loss: 0.565707  [   81/   88]
per-ex loss: 0.573453  [   82/   88]
per-ex loss: 0.597129  [   83/   88]
per-ex loss: 0.423611  [   84/   88]
per-ex loss: 0.483478  [   85/   88]
per-ex loss: 0.700248  [   86/   88]
per-ex loss: 0.450440  [   87/   88]
per-ex loss: 0.468015  [   88/   88]
Train Error: Avg loss: 0.53720258
validation Error: 
 Avg loss: 0.57426166 
 F1: 0.451279 
 Precision: 0.596854 
 Recall: 0.362793
 IoU: 0.291388

test Error: 
 Avg loss: 0.52738683 
 F1: 0.524587 
 Precision: 0.635752 
 Recall: 0.446511
 IoU: 0.355552

We have finished training iteration 12
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_9_.pth
per-ex loss: 0.588283  [    1/   88]
per-ex loss: 0.589439  [    2/   88]
per-ex loss: 0.369491  [    3/   88]
per-ex loss: 0.419908  [    4/   88]
per-ex loss: 0.659495  [    5/   88]
per-ex loss: 0.484031  [    6/   88]
per-ex loss: 0.612494  [    7/   88]
per-ex loss: 0.379625  [    8/   88]
per-ex loss: 0.353517  [    9/   88]
per-ex loss: 0.486291  [   10/   88]
per-ex loss: 0.420282  [   11/   88]
per-ex loss: 0.530850  [   12/   88]
per-ex loss: 0.607109  [   13/   88]
per-ex loss: 0.478042  [   14/   88]
per-ex loss: 0.619046  [   15/   88]
per-ex loss: 0.479123  [   16/   88]
per-ex loss: 0.606309  [   17/   88]
per-ex loss: 0.620372  [   18/   88]
per-ex loss: 0.668654  [   19/   88]
per-ex loss: 0.691683  [   20/   88]
per-ex loss: 0.623165  [   21/   88]
per-ex loss: 0.421098  [   22/   88]
per-ex loss: 0.463933  [   23/   88]
per-ex loss: 0.424195  [   24/   88]
per-ex loss: 0.566107  [   25/   88]
per-ex loss: 0.700853  [   26/   88]
per-ex loss: 0.410455  [   27/   88]
per-ex loss: 0.442526  [   28/   88]
per-ex loss: 0.362211  [   29/   88]
per-ex loss: 0.541077  [   30/   88]
per-ex loss: 0.664759  [   31/   88]
per-ex loss: 0.444115  [   32/   88]
per-ex loss: 0.692289  [   33/   88]
per-ex loss: 0.565949  [   34/   88]
per-ex loss: 0.433881  [   35/   88]
per-ex loss: 0.434298  [   36/   88]
per-ex loss: 0.354195  [   37/   88]
per-ex loss: 0.464013  [   38/   88]
per-ex loss: 0.505045  [   39/   88]
per-ex loss: 0.711773  [   40/   88]
per-ex loss: 0.710107  [   41/   88]
per-ex loss: 0.737382  [   42/   88]
per-ex loss: 0.482742  [   43/   88]
per-ex loss: 0.462995  [   44/   88]
per-ex loss: 0.590020  [   45/   88]
per-ex loss: 0.399050  [   46/   88]
per-ex loss: 0.568885  [   47/   88]
per-ex loss: 0.480479  [   48/   88]
per-ex loss: 0.607769  [   49/   88]
per-ex loss: 0.506354  [   50/   88]
per-ex loss: 0.668170  [   51/   88]
per-ex loss: 0.439986  [   52/   88]
per-ex loss: 0.463569  [   53/   88]
per-ex loss: 0.727466  [   54/   88]
per-ex loss: 0.405086  [   55/   88]
per-ex loss: 0.630013  [   56/   88]
per-ex loss: 0.444153  [   57/   88]
per-ex loss: 0.469518  [   58/   88]
per-ex loss: 0.585980  [   59/   88]
per-ex loss: 0.654793  [   60/   88]
per-ex loss: 0.479311  [   61/   88]
per-ex loss: 0.449773  [   62/   88]
per-ex loss: 0.661520  [   63/   88]
per-ex loss: 0.465859  [   64/   88]
per-ex loss: 0.455150  [   65/   88]
per-ex loss: 0.608894  [   66/   88]
per-ex loss: 0.509680  [   67/   88]
per-ex loss: 0.677210  [   68/   88]
per-ex loss: 0.524963  [   69/   88]
per-ex loss: 0.481902  [   70/   88]
per-ex loss: 0.407420  [   71/   88]
per-ex loss: 0.479893  [   72/   88]
per-ex loss: 0.683608  [   73/   88]
per-ex loss: 0.400047  [   74/   88]
per-ex loss: 0.704472  [   75/   88]
per-ex loss: 0.565747  [   76/   88]
per-ex loss: 0.683855  [   77/   88]
per-ex loss: 0.618870  [   78/   88]
per-ex loss: 0.704699  [   79/   88]
per-ex loss: 0.477046  [   80/   88]
per-ex loss: 0.604922  [   81/   88]
per-ex loss: 0.631617  [   82/   88]
per-ex loss: 0.410571  [   83/   88]
per-ex loss: 0.502540  [   84/   88]
per-ex loss: 0.485867  [   85/   88]
per-ex loss: 0.463378  [   86/   88]
per-ex loss: 0.616022  [   87/   88]
per-ex loss: 0.394664  [   88/   88]
Train Error: Avg loss: 0.53493186
validation Error: 
 Avg loss: 0.56190747 
 F1: 0.467967 
 Precision: 0.587135 
 Recall: 0.389012
 IoU: 0.305455

test Error: 
 Avg loss: 0.52141717 
 F1: 0.532331 
 Precision: 0.610362 
 Recall: 0.471990
 IoU: 0.362705

We have finished training iteration 13
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_10_.pth
per-ex loss: 0.588489  [    1/   88]
per-ex loss: 0.447993  [    2/   88]
per-ex loss: 0.402815  [    3/   88]
per-ex loss: 0.405185  [    4/   88]
per-ex loss: 0.442604  [    5/   88]
per-ex loss: 0.450341  [    6/   88]
per-ex loss: 0.463464  [    7/   88]
per-ex loss: 0.480575  [    8/   88]
per-ex loss: 0.466313  [    9/   88]
per-ex loss: 0.456824  [   10/   88]
per-ex loss: 0.536497  [   11/   88]
per-ex loss: 0.498337  [   12/   88]
per-ex loss: 0.638275  [   13/   88]
per-ex loss: 0.640534  [   14/   88]
per-ex loss: 0.386350  [   15/   88]
per-ex loss: 0.476074  [   16/   88]
per-ex loss: 0.440849  [   17/   88]
per-ex loss: 0.586290  [   18/   88]
per-ex loss: 0.450928  [   19/   88]
per-ex loss: 0.683111  [   20/   88]
per-ex loss: 0.666151  [   21/   88]
per-ex loss: 0.605074  [   22/   88]
per-ex loss: 0.402053  [   23/   88]
per-ex loss: 0.572884  [   24/   88]
per-ex loss: 0.482926  [   25/   88]
per-ex loss: 0.602261  [   26/   88]
per-ex loss: 0.394780  [   27/   88]
per-ex loss: 0.570533  [   28/   88]
per-ex loss: 0.414790  [   29/   88]
per-ex loss: 0.583628  [   30/   88]
per-ex loss: 0.582642  [   31/   88]
per-ex loss: 0.415182  [   32/   88]
per-ex loss: 0.405702  [   33/   88]
per-ex loss: 0.482243  [   34/   88]
per-ex loss: 0.612092  [   35/   88]
per-ex loss: 0.628802  [   36/   88]
per-ex loss: 0.397451  [   37/   88]
per-ex loss: 0.386136  [   38/   88]
per-ex loss: 0.475483  [   39/   88]
per-ex loss: 0.670386  [   40/   88]
per-ex loss: 0.374128  [   41/   88]
per-ex loss: 0.630353  [   42/   88]
per-ex loss: 0.545432  [   43/   88]
per-ex loss: 0.623420  [   44/   88]
per-ex loss: 0.415413  [   45/   88]
per-ex loss: 0.711664  [   46/   88]
per-ex loss: 0.536588  [   47/   88]
per-ex loss: 0.448861  [   48/   88]
per-ex loss: 0.477349  [   49/   88]
per-ex loss: 0.470268  [   50/   88]
per-ex loss: 0.485690  [   51/   88]
per-ex loss: 0.433071  [   52/   88]
per-ex loss: 0.571689  [   53/   88]
per-ex loss: 0.561546  [   54/   88]
per-ex loss: 0.468361  [   55/   88]
per-ex loss: 0.658230  [   56/   88]
per-ex loss: 0.638144  [   57/   88]
per-ex loss: 0.668352  [   58/   88]
per-ex loss: 0.487711  [   59/   88]
per-ex loss: 0.645938  [   60/   88]
per-ex loss: 0.672099  [   61/   88]
per-ex loss: 0.607423  [   62/   88]
per-ex loss: 0.443588  [   63/   88]
per-ex loss: 0.385110  [   64/   88]
per-ex loss: 0.497295  [   65/   88]
per-ex loss: 0.598148  [   66/   88]
per-ex loss: 0.624306  [   67/   88]
per-ex loss: 0.633063  [   68/   88]
per-ex loss: 0.659920  [   69/   88]
per-ex loss: 0.640442  [   70/   88]
per-ex loss: 0.408432  [   71/   88]
per-ex loss: 0.432910  [   72/   88]
per-ex loss: 0.415770  [   73/   88]
per-ex loss: 0.683851  [   74/   88]
per-ex loss: 0.710598  [   75/   88]
per-ex loss: 0.516566  [   76/   88]
per-ex loss: 0.471412  [   77/   88]
per-ex loss: 0.711903  [   78/   88]
per-ex loss: 0.356467  [   79/   88]
per-ex loss: 0.659437  [   80/   88]
per-ex loss: 0.395483  [   81/   88]
per-ex loss: 0.713353  [   82/   88]
per-ex loss: 0.443163  [   83/   88]
per-ex loss: 0.540917  [   84/   88]
per-ex loss: 0.632953  [   85/   88]
per-ex loss: 0.721793  [   86/   88]
per-ex loss: 0.396465  [   87/   88]
per-ex loss: 0.651178  [   88/   88]
Train Error: Avg loss: 0.53146930
validation Error: 
 Avg loss: 0.59610936 
 F1: 0.422134 
 Precision: 0.519443 
 Recall: 0.355532
 IoU: 0.267535

test Error: 
 Avg loss: 0.52488396 
 F1: 0.522513 
 Precision: 0.611074 
 Recall: 0.456373
 IoU: 0.353650

We have finished training iteration 14
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_12_.pth
per-ex loss: 0.390965  [    1/   88]
per-ex loss: 0.453647  [    2/   88]
per-ex loss: 0.549689  [    3/   88]
per-ex loss: 0.377650  [    4/   88]
per-ex loss: 0.609812  [    5/   88]
per-ex loss: 0.530839  [    6/   88]
per-ex loss: 0.595662  [    7/   88]
per-ex loss: 0.445749  [    8/   88]
per-ex loss: 0.668098  [    9/   88]
per-ex loss: 0.409850  [   10/   88]
per-ex loss: 0.484296  [   11/   88]
per-ex loss: 0.423115  [   12/   88]
per-ex loss: 0.678066  [   13/   88]
per-ex loss: 0.626044  [   14/   88]
per-ex loss: 0.430996  [   15/   88]
per-ex loss: 0.664870  [   16/   88]
per-ex loss: 0.449660  [   17/   88]
per-ex loss: 0.457008  [   18/   88]
per-ex loss: 0.595420  [   19/   88]
per-ex loss: 0.674920  [   20/   88]
per-ex loss: 0.491666  [   21/   88]
per-ex loss: 0.632098  [   22/   88]
per-ex loss: 0.700970  [   23/   88]
per-ex loss: 0.672490  [   24/   88]
per-ex loss: 0.480769  [   25/   88]
per-ex loss: 0.478522  [   26/   88]
per-ex loss: 0.497993  [   27/   88]
per-ex loss: 0.576688  [   28/   88]
per-ex loss: 0.408029  [   29/   88]
per-ex loss: 0.573430  [   30/   88]
per-ex loss: 0.436599  [   31/   88]
per-ex loss: 0.650389  [   32/   88]
per-ex loss: 0.452258  [   33/   88]
per-ex loss: 0.391040  [   34/   88]
per-ex loss: 0.431501  [   35/   88]
per-ex loss: 0.432882  [   36/   88]
per-ex loss: 0.547826  [   37/   88]
per-ex loss: 0.548192  [   38/   88]
per-ex loss: 0.360146  [   39/   88]
per-ex loss: 0.453821  [   40/   88]
per-ex loss: 0.468591  [   41/   88]
per-ex loss: 0.608072  [   42/   88]
per-ex loss: 0.423312  [   43/   88]
per-ex loss: 0.639684  [   44/   88]
per-ex loss: 0.619650  [   45/   88]
per-ex loss: 0.378047  [   46/   88]
per-ex loss: 0.504144  [   47/   88]
per-ex loss: 0.470201  [   48/   88]
per-ex loss: 0.405096  [   49/   88]
per-ex loss: 0.582438  [   50/   88]
per-ex loss: 0.642934  [   51/   88]
per-ex loss: 0.428512  [   52/   88]
per-ex loss: 0.452359  [   53/   88]
per-ex loss: 0.603942  [   54/   88]
per-ex loss: 0.477370  [   55/   88]
per-ex loss: 0.473117  [   56/   88]
per-ex loss: 0.666297  [   57/   88]
per-ex loss: 0.462117  [   58/   88]
per-ex loss: 0.475472  [   59/   88]
per-ex loss: 0.428624  [   60/   88]
per-ex loss: 0.417885  [   61/   88]
per-ex loss: 0.384319  [   62/   88]
per-ex loss: 0.630401  [   63/   88]
per-ex loss: 0.474675  [   64/   88]
per-ex loss: 0.371000  [   65/   88]
per-ex loss: 0.487585  [   66/   88]
per-ex loss: 0.585512  [   67/   88]
per-ex loss: 0.734959  [   68/   88]
per-ex loss: 0.634034  [   69/   88]
per-ex loss: 0.400757  [   70/   88]
per-ex loss: 0.411712  [   71/   88]
per-ex loss: 0.626663  [   72/   88]
per-ex loss: 0.703058  [   73/   88]
per-ex loss: 0.460849  [   74/   88]
per-ex loss: 0.558743  [   75/   88]
per-ex loss: 0.651784  [   76/   88]
per-ex loss: 0.551054  [   77/   88]
per-ex loss: 0.413322  [   78/   88]
per-ex loss: 0.611963  [   79/   88]
per-ex loss: 0.359562  [   80/   88]
per-ex loss: 0.705567  [   81/   88]
per-ex loss: 0.689303  [   82/   88]
per-ex loss: 0.491705  [   83/   88]
per-ex loss: 0.582174  [   84/   88]
per-ex loss: 0.690090  [   85/   88]
per-ex loss: 0.715375  [   86/   88]
per-ex loss: 0.506301  [   87/   88]
per-ex loss: 0.577850  [   88/   88]
Train Error: Avg loss: 0.52695278
validation Error: 
 Avg loss: 0.55557439 
 F1: 0.471324 
 Precision: 0.563959 
 Recall: 0.404828
 IoU: 0.308322

test Error: 
 Avg loss: 0.50885728 
 F1: 0.548006 
 Precision: 0.589368 
 Recall: 0.512069
 IoU: 0.377416

We have finished training iteration 15
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_11_.pth
per-ex loss: 0.447471  [    1/   88]
per-ex loss: 0.676879  [    2/   88]
per-ex loss: 0.416135  [    3/   88]
per-ex loss: 0.638269  [    4/   88]
per-ex loss: 0.716275  [    5/   88]
per-ex loss: 0.491295  [    6/   88]
per-ex loss: 0.460681  [    7/   88]
per-ex loss: 0.662771  [    8/   88]
per-ex loss: 0.475985  [    9/   88]
per-ex loss: 0.612313  [   10/   88]
per-ex loss: 0.441038  [   11/   88]
per-ex loss: 0.494280  [   12/   88]
per-ex loss: 0.493010  [   13/   88]
per-ex loss: 0.346518  [   14/   88]
per-ex loss: 0.553111  [   15/   88]
per-ex loss: 0.517925  [   16/   88]
per-ex loss: 0.524350  [   17/   88]
per-ex loss: 0.567799  [   18/   88]
per-ex loss: 0.438468  [   19/   88]
per-ex loss: 0.439225  [   20/   88]
per-ex loss: 0.623331  [   21/   88]
per-ex loss: 0.663180  [   22/   88]
per-ex loss: 0.601364  [   23/   88]
per-ex loss: 0.379388  [   24/   88]
per-ex loss: 0.406334  [   25/   88]
per-ex loss: 0.445195  [   26/   88]
per-ex loss: 0.369326  [   27/   88]
per-ex loss: 0.476924  [   28/   88]
per-ex loss: 0.531493  [   29/   88]
per-ex loss: 0.739198  [   30/   88]
per-ex loss: 0.618652  [   31/   88]
per-ex loss: 0.457245  [   32/   88]
per-ex loss: 0.621680  [   33/   88]
per-ex loss: 0.433061  [   34/   88]
per-ex loss: 0.434689  [   35/   88]
per-ex loss: 0.364702  [   36/   88]
per-ex loss: 0.610402  [   37/   88]
per-ex loss: 0.610288  [   38/   88]
per-ex loss: 0.572768  [   39/   88]
per-ex loss: 0.569579  [   40/   88]
per-ex loss: 0.483737  [   41/   88]
per-ex loss: 0.684303  [   42/   88]
per-ex loss: 0.552201  [   43/   88]
per-ex loss: 0.657064  [   44/   88]
per-ex loss: 0.699691  [   45/   88]
per-ex loss: 0.470369  [   46/   88]
per-ex loss: 0.690136  [   47/   88]
per-ex loss: 0.457272  [   48/   88]
per-ex loss: 0.388894  [   49/   88]
per-ex loss: 0.628394  [   50/   88]
per-ex loss: 0.508022  [   51/   88]
per-ex loss: 0.589719  [   52/   88]
per-ex loss: 0.412428  [   53/   88]
per-ex loss: 0.458434  [   54/   88]
per-ex loss: 0.631550  [   55/   88]
per-ex loss: 0.669183  [   56/   88]
per-ex loss: 0.627193  [   57/   88]
per-ex loss: 0.584260  [   58/   88]
per-ex loss: 0.722526  [   59/   88]
per-ex loss: 0.622203  [   60/   88]
per-ex loss: 0.383570  [   61/   88]
per-ex loss: 0.510294  [   62/   88]
per-ex loss: 0.427831  [   63/   88]
per-ex loss: 0.676961  [   64/   88]
per-ex loss: 0.455432  [   65/   88]
per-ex loss: 0.624265  [   66/   88]
per-ex loss: 0.514138  [   67/   88]
per-ex loss: 0.514267  [   68/   88]
per-ex loss: 0.413049  [   69/   88]
per-ex loss: 0.628340  [   70/   88]
per-ex loss: 0.395779  [   71/   88]
per-ex loss: 0.438633  [   72/   88]
per-ex loss: 0.454188  [   73/   88]
per-ex loss: 0.375432  [   74/   88]
per-ex loss: 0.430796  [   75/   88]
per-ex loss: 0.398071  [   76/   88]
per-ex loss: 0.584644  [   77/   88]
per-ex loss: 0.456724  [   78/   88]
per-ex loss: 0.445217  [   79/   88]
per-ex loss: 0.398731  [   80/   88]
per-ex loss: 0.666841  [   81/   88]
per-ex loss: 0.731780  [   82/   88]
per-ex loss: 0.432432  [   83/   88]
per-ex loss: 0.670149  [   84/   88]
per-ex loss: 0.607261  [   85/   88]
per-ex loss: 0.491240  [   86/   88]
per-ex loss: 0.565540  [   87/   88]
per-ex loss: 0.475173  [   88/   88]
Train Error: Avg loss: 0.53007907
validation Error: 
 Avg loss: 0.56856744 
 F1: 0.461213 
 Precision: 0.489005 
 Recall: 0.436411
 IoU: 0.299725

test Error: 
 Avg loss: 0.51595806 
 F1: 0.540003 
 Precision: 0.541265 
 Recall: 0.538748
 IoU: 0.369866

We have finished training iteration 16
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_14_.pth
per-ex loss: 0.661153  [    1/   88]
per-ex loss: 0.672704  [    2/   88]
per-ex loss: 0.581192  [    3/   88]
per-ex loss: 0.443044  [    4/   88]
per-ex loss: 0.417273  [    5/   88]
per-ex loss: 0.367539  [    6/   88]
per-ex loss: 0.410489  [    7/   88]
per-ex loss: 0.447897  [    8/   88]
per-ex loss: 0.471692  [    9/   88]
per-ex loss: 0.539976  [   10/   88]
per-ex loss: 0.344086  [   11/   88]
per-ex loss: 0.427179  [   12/   88]
per-ex loss: 0.474957  [   13/   88]
per-ex loss: 0.551630  [   14/   88]
per-ex loss: 0.702811  [   15/   88]
per-ex loss: 0.705347  [   16/   88]
per-ex loss: 0.617623  [   17/   88]
per-ex loss: 0.650888  [   18/   88]
per-ex loss: 0.431578  [   19/   88]
per-ex loss: 0.589890  [   20/   88]
per-ex loss: 0.469747  [   21/   88]
per-ex loss: 0.614705  [   22/   88]
per-ex loss: 0.454887  [   23/   88]
per-ex loss: 0.746232  [   24/   88]
per-ex loss: 0.599359  [   25/   88]
per-ex loss: 0.476220  [   26/   88]
per-ex loss: 0.439271  [   27/   88]
per-ex loss: 0.497236  [   28/   88]
per-ex loss: 0.641609  [   29/   88]
per-ex loss: 0.407213  [   30/   88]
per-ex loss: 0.560309  [   31/   88]
per-ex loss: 0.484902  [   32/   88]
per-ex loss: 0.604457  [   33/   88]
per-ex loss: 0.465315  [   34/   88]
per-ex loss: 0.431614  [   35/   88]
per-ex loss: 0.344190  [   36/   88]
per-ex loss: 0.641228  [   37/   88]
per-ex loss: 0.661113  [   38/   88]
per-ex loss: 0.396407  [   39/   88]
per-ex loss: 0.428338  [   40/   88]
per-ex loss: 0.395871  [   41/   88]
per-ex loss: 0.600222  [   42/   88]
per-ex loss: 0.575284  [   43/   88]
per-ex loss: 0.470440  [   44/   88]
per-ex loss: 0.645113  [   45/   88]
per-ex loss: 0.483933  [   46/   88]
per-ex loss: 0.414118  [   47/   88]
per-ex loss: 0.452674  [   48/   88]
per-ex loss: 0.698022  [   49/   88]
per-ex loss: 0.622000  [   50/   88]
per-ex loss: 0.398437  [   51/   88]
per-ex loss: 0.391999  [   52/   88]
per-ex loss: 0.420290  [   53/   88]
per-ex loss: 0.550229  [   54/   88]
per-ex loss: 0.480779  [   55/   88]
per-ex loss: 0.630654  [   56/   88]
per-ex loss: 0.688022  [   57/   88]
per-ex loss: 0.392198  [   58/   88]
per-ex loss: 0.691793  [   59/   88]
per-ex loss: 0.651545  [   60/   88]
per-ex loss: 0.436394  [   61/   88]
per-ex loss: 0.597985  [   62/   88]
per-ex loss: 0.409987  [   63/   88]
per-ex loss: 0.382118  [   64/   88]
per-ex loss: 0.468780  [   65/   88]
per-ex loss: 0.586724  [   66/   88]
per-ex loss: 0.589872  [   67/   88]
per-ex loss: 0.472706  [   68/   88]
per-ex loss: 0.623146  [   69/   88]
per-ex loss: 0.575765  [   70/   88]
per-ex loss: 0.460249  [   71/   88]
per-ex loss: 0.395161  [   72/   88]
per-ex loss: 0.372171  [   73/   88]
per-ex loss: 0.596458  [   74/   88]
per-ex loss: 0.647712  [   75/   88]
per-ex loss: 0.624285  [   76/   88]
per-ex loss: 0.478395  [   77/   88]
per-ex loss: 0.483177  [   78/   88]
per-ex loss: 0.464351  [   79/   88]
per-ex loss: 0.436354  [   80/   88]
per-ex loss: 0.669921  [   81/   88]
per-ex loss: 0.617350  [   82/   88]
per-ex loss: 0.515780  [   83/   88]
per-ex loss: 0.580131  [   84/   88]
per-ex loss: 0.591051  [   85/   88]
per-ex loss: 0.409057  [   86/   88]
per-ex loss: 0.435730  [   87/   88]
per-ex loss: 0.555747  [   88/   88]
Train Error: Avg loss: 0.52276681
validation Error: 
 Avg loss: 0.54731482 
 F1: 0.483458 
 Precision: 0.529226 
 Recall: 0.444977
 IoU: 0.318790

test Error: 
 Avg loss: 0.50381208 
 F1: 0.549349 
 Precision: 0.558546 
 Recall: 0.540451
 IoU: 0.378692

We have finished training iteration 17
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_8_.pth
per-ex loss: 0.692499  [    1/   88]
per-ex loss: 0.452136  [    2/   88]
per-ex loss: 0.457775  [    3/   88]
per-ex loss: 0.561870  [    4/   88]
per-ex loss: 0.467117  [    5/   88]
per-ex loss: 0.578567  [    6/   88]
per-ex loss: 0.471176  [    7/   88]
per-ex loss: 0.620289  [    8/   88]
per-ex loss: 0.574506  [    9/   88]
per-ex loss: 0.735578  [   10/   88]
per-ex loss: 0.689527  [   11/   88]
per-ex loss: 0.599987  [   12/   88]
per-ex loss: 0.419322  [   13/   88]
per-ex loss: 0.619495  [   14/   88]
per-ex loss: 0.507139  [   15/   88]
per-ex loss: 0.561481  [   16/   88]
per-ex loss: 0.544475  [   17/   88]
per-ex loss: 0.662249  [   18/   88]
per-ex loss: 0.403085  [   19/   88]
per-ex loss: 0.473345  [   20/   88]
per-ex loss: 0.413101  [   21/   88]
per-ex loss: 0.585260  [   22/   88]
per-ex loss: 0.572562  [   23/   88]
per-ex loss: 0.582895  [   24/   88]
per-ex loss: 0.407204  [   25/   88]
per-ex loss: 0.612360  [   26/   88]
per-ex loss: 0.435656  [   27/   88]
per-ex loss: 0.544614  [   28/   88]
per-ex loss: 0.385571  [   29/   88]
per-ex loss: 0.401163  [   30/   88]
per-ex loss: 0.481753  [   31/   88]
per-ex loss: 0.678242  [   32/   88]
per-ex loss: 0.377211  [   33/   88]
per-ex loss: 0.371385  [   34/   88]
per-ex loss: 0.636154  [   35/   88]
per-ex loss: 0.500440  [   36/   88]
per-ex loss: 0.611013  [   37/   88]
per-ex loss: 0.467251  [   38/   88]
per-ex loss: 0.396003  [   39/   88]
per-ex loss: 0.392523  [   40/   88]
per-ex loss: 0.557439  [   41/   88]
per-ex loss: 0.391429  [   42/   88]
per-ex loss: 0.308567  [   43/   88]
per-ex loss: 0.669426  [   44/   88]
per-ex loss: 0.654416  [   45/   88]
per-ex loss: 0.565600  [   46/   88]
per-ex loss: 0.475213  [   47/   88]
per-ex loss: 0.644925  [   48/   88]
per-ex loss: 0.511243  [   49/   88]
per-ex loss: 0.451528  [   50/   88]
per-ex loss: 0.613672  [   51/   88]
per-ex loss: 0.644323  [   52/   88]
per-ex loss: 0.406151  [   53/   88]
per-ex loss: 0.456789  [   54/   88]
per-ex loss: 0.449308  [   55/   88]
per-ex loss: 0.665604  [   56/   88]
per-ex loss: 0.463676  [   57/   88]
per-ex loss: 0.699222  [   58/   88]
per-ex loss: 0.360378  [   59/   88]
per-ex loss: 0.327237  [   60/   88]
per-ex loss: 0.699393  [   61/   88]
per-ex loss: 0.611850  [   62/   88]
per-ex loss: 0.445934  [   63/   88]
per-ex loss: 0.443239  [   64/   88]
per-ex loss: 0.540197  [   65/   88]
per-ex loss: 0.598595  [   66/   88]
per-ex loss: 0.486172  [   67/   88]
per-ex loss: 0.704234  [   68/   88]
per-ex loss: 0.370634  [   69/   88]
per-ex loss: 0.586435  [   70/   88]
per-ex loss: 0.472386  [   71/   88]
per-ex loss: 0.433427  [   72/   88]
per-ex loss: 0.424409  [   73/   88]
per-ex loss: 0.445849  [   74/   88]
per-ex loss: 0.384933  [   75/   88]
per-ex loss: 0.579423  [   76/   88]
per-ex loss: 0.410391  [   77/   88]
per-ex loss: 0.542835  [   78/   88]
per-ex loss: 0.684703  [   79/   88]
per-ex loss: 0.604946  [   80/   88]
per-ex loss: 0.636768  [   81/   88]
per-ex loss: 0.468455  [   82/   88]
per-ex loss: 0.402805  [   83/   88]
per-ex loss: 0.417320  [   84/   88]
per-ex loss: 0.441351  [   85/   88]
per-ex loss: 0.574508  [   86/   88]
per-ex loss: 0.482953  [   87/   88]
per-ex loss: 0.502922  [   88/   88]
Train Error: Avg loss: 0.51883172
validation Error: 
 Avg loss: 0.56548757 
 F1: 0.466844 
 Precision: 0.492862 
 Recall: 0.443435
 IoU: 0.304499

test Error: 
 Avg loss: 0.52064650 
 F1: 0.530067 
 Precision: 0.539432 
 Recall: 0.521021
 IoU: 0.360606

We have finished training iteration 18
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_16_.pth
per-ex loss: 0.388243  [    1/   88]
per-ex loss: 0.566179  [    2/   88]
per-ex loss: 0.369307  [    3/   88]
per-ex loss: 0.725711  [    4/   88]
per-ex loss: 0.473478  [    5/   88]
per-ex loss: 0.402562  [    6/   88]
per-ex loss: 0.383177  [    7/   88]
per-ex loss: 0.427288  [    8/   88]
per-ex loss: 0.452703  [    9/   88]
per-ex loss: 0.412245  [   10/   88]
per-ex loss: 0.452399  [   11/   88]
per-ex loss: 0.478722  [   12/   88]
per-ex loss: 0.437620  [   13/   88]
per-ex loss: 0.359894  [   14/   88]
per-ex loss: 0.398751  [   15/   88]
per-ex loss: 0.482249  [   16/   88]
per-ex loss: 0.546605  [   17/   88]
per-ex loss: 0.430088  [   18/   88]
per-ex loss: 0.729815  [   19/   88]
per-ex loss: 0.567759  [   20/   88]
per-ex loss: 0.607963  [   21/   88]
per-ex loss: 0.568682  [   22/   88]
per-ex loss: 0.612518  [   23/   88]
per-ex loss: 0.421731  [   24/   88]
per-ex loss: 0.599692  [   25/   88]
per-ex loss: 0.456107  [   26/   88]
per-ex loss: 0.395677  [   27/   88]
per-ex loss: 0.689025  [   28/   88]
per-ex loss: 0.398313  [   29/   88]
per-ex loss: 0.730137  [   30/   88]
per-ex loss: 0.551743  [   31/   88]
per-ex loss: 0.595237  [   32/   88]
per-ex loss: 0.419548  [   33/   88]
per-ex loss: 0.449929  [   34/   88]
per-ex loss: 0.672422  [   35/   88]
per-ex loss: 0.596169  [   36/   88]
per-ex loss: 0.563570  [   37/   88]
per-ex loss: 0.535229  [   38/   88]
per-ex loss: 0.466505  [   39/   88]
per-ex loss: 0.681719  [   40/   88]
per-ex loss: 0.650482  [   41/   88]
per-ex loss: 0.407558  [   42/   88]
per-ex loss: 0.507712  [   43/   88]
per-ex loss: 0.695223  [   44/   88]
per-ex loss: 0.372910  [   45/   88]
per-ex loss: 0.407419  [   46/   88]
per-ex loss: 0.455113  [   47/   88]
per-ex loss: 0.570991  [   48/   88]
per-ex loss: 0.484977  [   49/   88]
per-ex loss: 0.652259  [   50/   88]
per-ex loss: 0.488308  [   51/   88]
per-ex loss: 0.600705  [   52/   88]
per-ex loss: 0.399371  [   53/   88]
per-ex loss: 0.520195  [   54/   88]
per-ex loss: 0.419691  [   55/   88]
per-ex loss: 0.371691  [   56/   88]
per-ex loss: 0.629969  [   57/   88]
per-ex loss: 0.482680  [   58/   88]
per-ex loss: 0.481128  [   59/   88]
per-ex loss: 0.692175  [   60/   88]
per-ex loss: 0.653254  [   61/   88]
per-ex loss: 0.634597  [   62/   88]
per-ex loss: 0.531903  [   63/   88]
per-ex loss: 0.591338  [   64/   88]
per-ex loss: 0.455131  [   65/   88]
per-ex loss: 0.523899  [   66/   88]
per-ex loss: 0.623684  [   67/   88]
per-ex loss: 0.463565  [   68/   88]
per-ex loss: 0.588480  [   69/   88]
per-ex loss: 0.578753  [   70/   88]
per-ex loss: 0.504070  [   71/   88]
per-ex loss: 0.598052  [   72/   88]
per-ex loss: 0.386781  [   73/   88]
per-ex loss: 0.608405  [   74/   88]
per-ex loss: 0.432849  [   75/   88]
per-ex loss: 0.712449  [   76/   88]
per-ex loss: 0.695041  [   77/   88]
per-ex loss: 0.621255  [   78/   88]
per-ex loss: 0.524299  [   79/   88]
per-ex loss: 0.560844  [   80/   88]
per-ex loss: 0.469372  [   81/   88]
per-ex loss: 0.384546  [   82/   88]
per-ex loss: 0.595942  [   83/   88]
per-ex loss: 0.537944  [   84/   88]
per-ex loss: 0.352735  [   85/   88]
per-ex loss: 0.452195  [   86/   88]
per-ex loss: 0.413147  [   87/   88]
per-ex loss: 0.603477  [   88/   88]
Train Error: Avg loss: 0.52142354
validation Error: 
 Avg loss: 0.54709592 
 F1: 0.482447 
 Precision: 0.587816 
 Recall: 0.409112
 IoU: 0.317911

test Error: 
 Avg loss: 0.50991383 
 F1: 0.545970 
 Precision: 0.622796 
 Recall: 0.486017
 IoU: 0.375488

We have finished training iteration 19
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_13_.pth
per-ex loss: 0.591326  [    1/   88]
per-ex loss: 0.555756  [    2/   88]
per-ex loss: 0.411553  [    3/   88]
per-ex loss: 0.438581  [    4/   88]
per-ex loss: 0.524584  [    5/   88]
per-ex loss: 0.386277  [    6/   88]
per-ex loss: 0.581351  [    7/   88]
per-ex loss: 0.411727  [    8/   88]
per-ex loss: 0.572373  [    9/   88]
per-ex loss: 0.452649  [   10/   88]
per-ex loss: 0.417936  [   11/   88]
per-ex loss: 0.439171  [   12/   88]
per-ex loss: 0.440530  [   13/   88]
per-ex loss: 0.565591  [   14/   88]
per-ex loss: 0.605231  [   15/   88]
per-ex loss: 0.603808  [   16/   88]
per-ex loss: 0.573461  [   17/   88]
per-ex loss: 0.636462  [   18/   88]
per-ex loss: 0.420160  [   19/   88]
per-ex loss: 0.396017  [   20/   88]
per-ex loss: 0.440516  [   21/   88]
per-ex loss: 0.625061  [   22/   88]
per-ex loss: 0.459145  [   23/   88]
per-ex loss: 0.652542  [   24/   88]
per-ex loss: 0.665908  [   25/   88]
per-ex loss: 0.389904  [   26/   88]
per-ex loss: 0.596280  [   27/   88]
per-ex loss: 0.457952  [   28/   88]
per-ex loss: 0.610108  [   29/   88]
per-ex loss: 0.440126  [   30/   88]
per-ex loss: 0.473004  [   31/   88]
per-ex loss: 0.395409  [   32/   88]
per-ex loss: 0.636514  [   33/   88]
per-ex loss: 0.544214  [   34/   88]
per-ex loss: 0.358549  [   35/   88]
per-ex loss: 0.665557  [   36/   88]
per-ex loss: 0.406128  [   37/   88]
per-ex loss: 0.454917  [   38/   88]
per-ex loss: 0.459202  [   39/   88]
per-ex loss: 0.399496  [   40/   88]
per-ex loss: 0.669166  [   41/   88]
per-ex loss: 0.738479  [   42/   88]
per-ex loss: 0.442725  [   43/   88]
per-ex loss: 0.386851  [   44/   88]
per-ex loss: 0.344014  [   45/   88]
per-ex loss: 0.691698  [   46/   88]
per-ex loss: 0.637491  [   47/   88]
per-ex loss: 0.633786  [   48/   88]
per-ex loss: 0.474061  [   49/   88]
per-ex loss: 0.563184  [   50/   88]
per-ex loss: 0.458265  [   51/   88]
per-ex loss: 0.451665  [   52/   88]
per-ex loss: 0.386390  [   53/   88]
per-ex loss: 0.357196  [   54/   88]
per-ex loss: 0.704066  [   55/   88]
per-ex loss: 0.572120  [   56/   88]
per-ex loss: 0.592484  [   57/   88]
per-ex loss: 0.547457  [   58/   88]
per-ex loss: 0.431033  [   59/   88]
per-ex loss: 0.461004  [   60/   88]
per-ex loss: 0.577676  [   61/   88]
per-ex loss: 0.392015  [   62/   88]
per-ex loss: 0.569776  [   63/   88]
per-ex loss: 0.549066  [   64/   88]
per-ex loss: 0.593821  [   65/   88]
per-ex loss: 0.472909  [   66/   88]
per-ex loss: 0.696864  [   67/   88]
per-ex loss: 0.365363  [   68/   88]
per-ex loss: 0.382810  [   69/   88]
per-ex loss: 0.588219  [   70/   88]
per-ex loss: 0.397742  [   71/   88]
per-ex loss: 0.599997  [   72/   88]
per-ex loss: 0.392013  [   73/   88]
per-ex loss: 0.679315  [   74/   88]
per-ex loss: 0.516082  [   75/   88]
per-ex loss: 0.435996  [   76/   88]
per-ex loss: 0.450136  [   77/   88]
per-ex loss: 0.415971  [   78/   88]
per-ex loss: 0.445149  [   79/   88]
per-ex loss: 0.667292  [   80/   88]
per-ex loss: 0.509944  [   81/   88]
per-ex loss: 0.418325  [   82/   88]
per-ex loss: 0.460413  [   83/   88]
per-ex loss: 0.702602  [   84/   88]
per-ex loss: 0.545699  [   85/   88]
per-ex loss: 0.584194  [   86/   88]
per-ex loss: 0.390136  [   87/   88]
per-ex loss: 0.717519  [   88/   88]
Train Error: Avg loss: 0.51380973
validation Error: 
 Avg loss: 0.58497068 
 F1: 0.432781 
 Precision: 0.423836 
 Recall: 0.442113
 IoU: 0.276146

test Error: 
 Avg loss: 0.51864370 
 F1: 0.532260 
 Precision: 0.517707 
 Recall: 0.547654
 IoU: 0.362639

We have finished training iteration 20
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_18_.pth
per-ex loss: 0.373854  [    1/   88]
per-ex loss: 0.441188  [    2/   88]
per-ex loss: 0.437981  [    3/   88]
per-ex loss: 0.517668  [    4/   88]
per-ex loss: 0.566587  [    5/   88]
per-ex loss: 0.472265  [    6/   88]
per-ex loss: 0.432196  [    7/   88]
per-ex loss: 0.335074  [    8/   88]
per-ex loss: 0.370226  [    9/   88]
per-ex loss: 0.563251  [   10/   88]
per-ex loss: 0.413687  [   11/   88]
per-ex loss: 0.468001  [   12/   88]
per-ex loss: 0.548658  [   13/   88]
per-ex loss: 0.610973  [   14/   88]
per-ex loss: 0.475195  [   15/   88]
per-ex loss: 0.399555  [   16/   88]
per-ex loss: 0.575851  [   17/   88]
per-ex loss: 0.602634  [   18/   88]
per-ex loss: 0.663335  [   19/   88]
per-ex loss: 0.456677  [   20/   88]
per-ex loss: 0.688925  [   21/   88]
per-ex loss: 0.474996  [   22/   88]
per-ex loss: 0.410673  [   23/   88]
per-ex loss: 0.501389  [   24/   88]
per-ex loss: 0.398740  [   25/   88]
per-ex loss: 0.563692  [   26/   88]
per-ex loss: 0.630444  [   27/   88]
per-ex loss: 0.646015  [   28/   88]
per-ex loss: 0.411189  [   29/   88]
per-ex loss: 0.394346  [   30/   88]
per-ex loss: 0.438674  [   31/   88]
per-ex loss: 0.538119  [   32/   88]
per-ex loss: 0.638271  [   33/   88]
per-ex loss: 0.349031  [   34/   88]
per-ex loss: 0.556861  [   35/   88]
per-ex loss: 0.486461  [   36/   88]
per-ex loss: 0.446115  [   37/   88]
per-ex loss: 0.636571  [   38/   88]
per-ex loss: 0.414604  [   39/   88]
per-ex loss: 0.758064  [   40/   88]
per-ex loss: 0.396300  [   41/   88]
per-ex loss: 0.656531  [   42/   88]
per-ex loss: 0.451103  [   43/   88]
per-ex loss: 0.402748  [   44/   88]
per-ex loss: 0.611642  [   45/   88]
per-ex loss: 0.423728  [   46/   88]
per-ex loss: 0.609233  [   47/   88]
per-ex loss: 0.446459  [   48/   88]
per-ex loss: 0.703236  [   49/   88]
per-ex loss: 0.494659  [   50/   88]
per-ex loss: 0.418939  [   51/   88]
per-ex loss: 0.568729  [   52/   88]
per-ex loss: 0.558487  [   53/   88]
per-ex loss: 0.672693  [   54/   88]
per-ex loss: 0.362132  [   55/   88]
per-ex loss: 0.402974  [   56/   88]
per-ex loss: 0.612156  [   57/   88]
per-ex loss: 0.562776  [   58/   88]
per-ex loss: 0.484560  [   59/   88]
per-ex loss: 0.374261  [   60/   88]
per-ex loss: 0.409608  [   61/   88]
per-ex loss: 0.401322  [   62/   88]
per-ex loss: 0.431721  [   63/   88]
per-ex loss: 0.646834  [   64/   88]
per-ex loss: 0.552574  [   65/   88]
per-ex loss: 0.616832  [   66/   88]
per-ex loss: 0.477115  [   67/   88]
per-ex loss: 0.488122  [   68/   88]
per-ex loss: 0.426774  [   69/   88]
per-ex loss: 0.692457  [   70/   88]
per-ex loss: 0.359856  [   71/   88]
per-ex loss: 0.674799  [   72/   88]
per-ex loss: 0.656077  [   73/   88]
per-ex loss: 0.505962  [   74/   88]
per-ex loss: 0.431957  [   75/   88]
per-ex loss: 0.720915  [   76/   88]
per-ex loss: 0.698673  [   77/   88]
per-ex loss: 0.714833  [   78/   88]
per-ex loss: 0.603059  [   79/   88]
per-ex loss: 0.553700  [   80/   88]
per-ex loss: 0.581805  [   81/   88]
per-ex loss: 0.494348  [   82/   88]
per-ex loss: 0.477443  [   83/   88]
per-ex loss: 0.546725  [   84/   88]
per-ex loss: 0.465156  [   85/   88]
per-ex loss: 0.789529  [   86/   88]
per-ex loss: 0.671074  [   87/   88]
per-ex loss: 0.392038  [   88/   88]
Train Error: Avg loss: 0.52048514
validation Error: 
 Avg loss: 0.55116476 
 F1: 0.475557 
 Precision: 0.475212 
 Recall: 0.475902
 IoU: 0.311954

test Error: 
 Avg loss: 0.50874515 
 F1: 0.544345 
 Precision: 0.516099 
 Recall: 0.575861
 IoU: 0.373952

We have finished training iteration 21
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_15_.pth
per-ex loss: 0.408751  [    1/   88]
per-ex loss: 0.392679  [    2/   88]
per-ex loss: 0.494133  [    3/   88]
per-ex loss: 0.421222  [    4/   88]
per-ex loss: 0.536810  [    5/   88]
per-ex loss: 0.508286  [    6/   88]
per-ex loss: 0.711342  [    7/   88]
per-ex loss: 0.379378  [    8/   88]
per-ex loss: 0.425536  [    9/   88]
per-ex loss: 0.678526  [   10/   88]
per-ex loss: 0.458344  [   11/   88]
per-ex loss: 0.430089  [   12/   88]
per-ex loss: 0.590359  [   13/   88]
per-ex loss: 0.612641  [   14/   88]
per-ex loss: 0.682387  [   15/   88]
per-ex loss: 0.364414  [   16/   88]
per-ex loss: 0.355355  [   17/   88]
per-ex loss: 0.431175  [   18/   88]
per-ex loss: 0.605783  [   19/   88]
per-ex loss: 0.563282  [   20/   88]
per-ex loss: 0.716611  [   21/   88]
per-ex loss: 0.406692  [   22/   88]
per-ex loss: 0.448171  [   23/   88]
per-ex loss: 0.631599  [   24/   88]
per-ex loss: 0.584941  [   25/   88]
per-ex loss: 0.758595  [   26/   88]
per-ex loss: 0.367927  [   27/   88]
per-ex loss: 0.571698  [   28/   88]
per-ex loss: 0.640358  [   29/   88]
per-ex loss: 0.402127  [   30/   88]
per-ex loss: 0.618182  [   31/   88]
per-ex loss: 0.568311  [   32/   88]
per-ex loss: 0.459048  [   33/   88]
per-ex loss: 0.610280  [   34/   88]
per-ex loss: 0.547089  [   35/   88]
per-ex loss: 0.600287  [   36/   88]
per-ex loss: 0.468649  [   37/   88]
per-ex loss: 0.640510  [   38/   88]
per-ex loss: 0.552910  [   39/   88]
per-ex loss: 0.684224  [   40/   88]
per-ex loss: 0.424515  [   41/   88]
per-ex loss: 0.577298  [   42/   88]
per-ex loss: 0.457766  [   43/   88]
per-ex loss: 0.414972  [   44/   88]
per-ex loss: 0.499167  [   45/   88]
per-ex loss: 0.512810  [   46/   88]
per-ex loss: 0.563253  [   47/   88]
per-ex loss: 0.462943  [   48/   88]
per-ex loss: 0.477665  [   49/   88]
per-ex loss: 0.520419  [   50/   88]
per-ex loss: 0.398508  [   51/   88]
per-ex loss: 0.397307  [   52/   88]
per-ex loss: 0.558291  [   53/   88]
per-ex loss: 0.581145  [   54/   88]
per-ex loss: 0.464905  [   55/   88]
per-ex loss: 0.385770  [   56/   88]
per-ex loss: 0.468961  [   57/   88]
per-ex loss: 0.438886  [   58/   88]
per-ex loss: 0.356152  [   59/   88]
per-ex loss: 0.438916  [   60/   88]
per-ex loss: 0.671039  [   61/   88]
per-ex loss: 0.553025  [   62/   88]
per-ex loss: 0.430367  [   63/   88]
per-ex loss: 0.625049  [   64/   88]
per-ex loss: 0.669493  [   65/   88]
per-ex loss: 0.700305  [   66/   88]
per-ex loss: 0.429702  [   67/   88]
per-ex loss: 0.644789  [   68/   88]
per-ex loss: 0.362941  [   69/   88]
per-ex loss: 0.553946  [   70/   88]
per-ex loss: 0.596928  [   71/   88]
per-ex loss: 0.704988  [   72/   88]
per-ex loss: 0.422079  [   73/   88]
per-ex loss: 0.627857  [   74/   88]
per-ex loss: 0.529194  [   75/   88]
per-ex loss: 0.461525  [   76/   88]
per-ex loss: 0.440293  [   77/   88]
per-ex loss: 0.401713  [   78/   88]
per-ex loss: 0.448551  [   79/   88]
per-ex loss: 0.596330  [   80/   88]
per-ex loss: 0.427155  [   81/   88]
per-ex loss: 0.606504  [   82/   88]
per-ex loss: 0.463427  [   83/   88]
per-ex loss: 0.407546  [   84/   88]
per-ex loss: 0.407592  [   85/   88]
per-ex loss: 0.460563  [   86/   88]
per-ex loss: 0.498440  [   87/   88]
per-ex loss: 0.672677  [   88/   88]
Train Error: Avg loss: 0.51752690
validation Error: 
 Avg loss: 0.55147876 
 F1: 0.479389 
 Precision: 0.625063 
 Recall: 0.388782
 IoU: 0.315261

test Error: 
 Avg loss: 0.51361287 
 F1: 0.542537 
 Precision: 0.643545 
 Recall: 0.468936
 IoU: 0.372248

We have finished training iteration 22
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_20_.pth
per-ex loss: 0.547839  [    1/   88]
per-ex loss: 0.398106  [    2/   88]
per-ex loss: 0.440873  [    3/   88]
per-ex loss: 0.387623  [    4/   88]
per-ex loss: 0.432546  [    5/   88]
per-ex loss: 0.530747  [    6/   88]
per-ex loss: 0.734327  [    7/   88]
per-ex loss: 0.658418  [    8/   88]
per-ex loss: 0.673623  [    9/   88]
per-ex loss: 0.421454  [   10/   88]
per-ex loss: 0.392512  [   11/   88]
per-ex loss: 0.603060  [   12/   88]
per-ex loss: 0.651745  [   13/   88]
per-ex loss: 0.459888  [   14/   88]
per-ex loss: 0.341140  [   15/   88]
per-ex loss: 0.584915  [   16/   88]
per-ex loss: 0.439663  [   17/   88]
per-ex loss: 0.443613  [   18/   88]
per-ex loss: 0.422920  [   19/   88]
per-ex loss: 0.617378  [   20/   88]
per-ex loss: 0.566184  [   21/   88]
per-ex loss: 0.547258  [   22/   88]
per-ex loss: 0.362869  [   23/   88]
per-ex loss: 0.427009  [   24/   88]
per-ex loss: 0.620013  [   25/   88]
per-ex loss: 0.662901  [   26/   88]
per-ex loss: 0.676992  [   27/   88]
per-ex loss: 0.473128  [   28/   88]
per-ex loss: 0.478120  [   29/   88]
per-ex loss: 0.442676  [   30/   88]
per-ex loss: 0.336951  [   31/   88]
per-ex loss: 0.412986  [   32/   88]
per-ex loss: 0.667263  [   33/   88]
per-ex loss: 0.618681  [   34/   88]
per-ex loss: 0.440832  [   35/   88]
per-ex loss: 0.632628  [   36/   88]
per-ex loss: 0.586801  [   37/   88]
per-ex loss: 0.601814  [   38/   88]
per-ex loss: 0.469781  [   39/   88]
per-ex loss: 0.334604  [   40/   88]
per-ex loss: 0.548577  [   41/   88]
per-ex loss: 0.565420  [   42/   88]
per-ex loss: 0.431630  [   43/   88]
per-ex loss: 0.537647  [   44/   88]
per-ex loss: 0.684886  [   45/   88]
per-ex loss: 0.365251  [   46/   88]
per-ex loss: 0.415459  [   47/   88]
per-ex loss: 0.691250  [   48/   88]
per-ex loss: 0.609859  [   49/   88]
per-ex loss: 0.686293  [   50/   88]
per-ex loss: 0.491286  [   51/   88]
per-ex loss: 0.451389  [   52/   88]
per-ex loss: 0.407820  [   53/   88]
per-ex loss: 0.458796  [   54/   88]
per-ex loss: 0.403677  [   55/   88]
per-ex loss: 0.413791  [   56/   88]
per-ex loss: 0.559832  [   57/   88]
per-ex loss: 0.405386  [   58/   88]
per-ex loss: 0.454719  [   59/   88]
per-ex loss: 0.458826  [   60/   88]
per-ex loss: 0.465048  [   61/   88]
per-ex loss: 0.642668  [   62/   88]
per-ex loss: 0.570592  [   63/   88]
per-ex loss: 0.660901  [   64/   88]
per-ex loss: 0.396176  [   65/   88]
per-ex loss: 0.361288  [   66/   88]
per-ex loss: 0.492982  [   67/   88]
per-ex loss: 0.399624  [   68/   88]
per-ex loss: 0.473776  [   69/   88]
per-ex loss: 0.593956  [   70/   88]
per-ex loss: 0.595047  [   71/   88]
per-ex loss: 0.597677  [   72/   88]
per-ex loss: 0.744727  [   73/   88]
per-ex loss: 0.592646  [   74/   88]
per-ex loss: 0.692253  [   75/   88]
per-ex loss: 0.398656  [   76/   88]
per-ex loss: 0.406519  [   77/   88]
per-ex loss: 0.577727  [   78/   88]
per-ex loss: 0.405987  [   79/   88]
per-ex loss: 0.386831  [   80/   88]
per-ex loss: 0.649574  [   81/   88]
per-ex loss: 0.547869  [   82/   88]
per-ex loss: 0.467586  [   83/   88]
per-ex loss: 0.468314  [   84/   88]
per-ex loss: 0.540656  [   85/   88]
per-ex loss: 0.530081  [   86/   88]
per-ex loss: 0.472469  [   87/   88]
per-ex loss: 0.451999  [   88/   88]
Train Error: Avg loss: 0.51326478
validation Error: 
 Avg loss: 0.54821297 
 F1: 0.479544 
 Precision: 0.519009 
 Recall: 0.445656
 IoU: 0.315394

test Error: 
 Avg loss: 0.50346129 
 F1: 0.551443 
 Precision: 0.568694 
 Recall: 0.535207
 IoU: 0.380684

We have finished training iteration 23
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_21_.pth
per-ex loss: 0.693585  [    1/   88]
per-ex loss: 0.434840  [    2/   88]
per-ex loss: 0.693916  [    3/   88]
per-ex loss: 0.466308  [    4/   88]
per-ex loss: 0.401769  [    5/   88]
per-ex loss: 0.647150  [    6/   88]
per-ex loss: 0.462213  [    7/   88]
per-ex loss: 0.593693  [    8/   88]
per-ex loss: 0.592778  [    9/   88]
per-ex loss: 0.404932  [   10/   88]
per-ex loss: 0.442677  [   11/   88]
per-ex loss: 0.441778  [   12/   88]
per-ex loss: 0.680603  [   13/   88]
per-ex loss: 0.526097  [   14/   88]
per-ex loss: 0.414980  [   15/   88]
per-ex loss: 0.450220  [   16/   88]
per-ex loss: 0.407490  [   17/   88]
per-ex loss: 0.564139  [   18/   88]
per-ex loss: 0.454282  [   19/   88]
per-ex loss: 0.409196  [   20/   88]
per-ex loss: 0.459958  [   21/   88]
per-ex loss: 0.360833  [   22/   88]
per-ex loss: 0.378068  [   23/   88]
per-ex loss: 0.639185  [   24/   88]
per-ex loss: 0.603392  [   25/   88]
per-ex loss: 0.628422  [   26/   88]
per-ex loss: 0.422038  [   27/   88]
per-ex loss: 0.549199  [   28/   88]
per-ex loss: 0.336959  [   29/   88]
per-ex loss: 0.374133  [   30/   88]
per-ex loss: 0.423787  [   31/   88]
per-ex loss: 0.614760  [   32/   88]
per-ex loss: 0.397132  [   33/   88]
per-ex loss: 0.597957  [   34/   88]
per-ex loss: 0.547876  [   35/   88]
per-ex loss: 0.617164  [   36/   88]
per-ex loss: 0.477935  [   37/   88]
per-ex loss: 0.392996  [   38/   88]
per-ex loss: 0.409343  [   39/   88]
per-ex loss: 0.579892  [   40/   88]
per-ex loss: 0.385125  [   41/   88]
per-ex loss: 0.559265  [   42/   88]
per-ex loss: 0.559548  [   43/   88]
per-ex loss: 0.660234  [   44/   88]
per-ex loss: 0.486581  [   45/   88]
per-ex loss: 0.364528  [   46/   88]
per-ex loss: 0.617905  [   47/   88]
per-ex loss: 0.436293  [   48/   88]
per-ex loss: 0.480616  [   49/   88]
per-ex loss: 0.683582  [   50/   88]
per-ex loss: 0.433788  [   51/   88]
per-ex loss: 0.435415  [   52/   88]
per-ex loss: 0.359451  [   53/   88]
per-ex loss: 0.412796  [   54/   88]
per-ex loss: 0.458240  [   55/   88]
per-ex loss: 0.461608  [   56/   88]
per-ex loss: 0.418636  [   57/   88]
per-ex loss: 0.325462  [   58/   88]
per-ex loss: 0.560270  [   59/   88]
per-ex loss: 0.387527  [   60/   88]
per-ex loss: 0.386228  [   61/   88]
per-ex loss: 0.549518  [   62/   88]
per-ex loss: 0.595907  [   63/   88]
per-ex loss: 0.533351  [   64/   88]
per-ex loss: 0.602252  [   65/   88]
per-ex loss: 0.607730  [   66/   88]
per-ex loss: 0.352213  [   67/   88]
per-ex loss: 0.571113  [   68/   88]
per-ex loss: 0.529216  [   69/   88]
per-ex loss: 0.596158  [   70/   88]
per-ex loss: 0.575139  [   71/   88]
per-ex loss: 0.558346  [   72/   88]
per-ex loss: 0.603329  [   73/   88]
per-ex loss: 0.662506  [   74/   88]
per-ex loss: 0.616332  [   75/   88]
per-ex loss: 0.355626  [   76/   88]
per-ex loss: 0.478708  [   77/   88]
per-ex loss: 0.451231  [   78/   88]
per-ex loss: 0.744925  [   79/   88]
per-ex loss: 0.445436  [   80/   88]
per-ex loss: 0.636604  [   81/   88]
per-ex loss: 0.413174  [   82/   88]
per-ex loss: 0.527139  [   83/   88]
per-ex loss: 0.424801  [   84/   88]
per-ex loss: 0.555324  [   85/   88]
per-ex loss: 0.433628  [   86/   88]
per-ex loss: 0.721593  [   87/   88]
per-ex loss: 0.633533  [   88/   88]
Train Error: Avg loss: 0.50729092
validation Error: 
 Avg loss: 0.54227290 
 F1: 0.482362 
 Precision: 0.576439 
 Recall: 0.414684
 IoU: 0.317837

test Error: 
 Avg loss: 0.50651093 
 F1: 0.547504 
 Precision: 0.618687 
 Recall: 0.491011
 IoU: 0.376940

We have finished training iteration 24
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_22_.pth
per-ex loss: 0.447250  [    1/   88]
per-ex loss: 0.366234  [    2/   88]
per-ex loss: 0.593227  [    3/   88]
per-ex loss: 0.659312  [    4/   88]
per-ex loss: 0.406778  [    5/   88]
per-ex loss: 0.425966  [    6/   88]
per-ex loss: 0.394705  [    7/   88]
per-ex loss: 0.554199  [    8/   88]
per-ex loss: 0.586555  [    9/   88]
per-ex loss: 0.609616  [   10/   88]
per-ex loss: 0.683372  [   11/   88]
per-ex loss: 0.676795  [   12/   88]
per-ex loss: 0.714478  [   13/   88]
per-ex loss: 0.582099  [   14/   88]
per-ex loss: 0.607391  [   15/   88]
per-ex loss: 0.418899  [   16/   88]
per-ex loss: 0.430661  [   17/   88]
per-ex loss: 0.462138  [   18/   88]
per-ex loss: 0.426269  [   19/   88]
per-ex loss: 0.679503  [   20/   88]
per-ex loss: 0.661004  [   21/   88]
per-ex loss: 0.460658  [   22/   88]
per-ex loss: 0.514991  [   23/   88]
per-ex loss: 0.386168  [   24/   88]
per-ex loss: 0.456449  [   25/   88]
per-ex loss: 0.582878  [   26/   88]
per-ex loss: 0.558066  [   27/   88]
per-ex loss: 0.414278  [   28/   88]
per-ex loss: 0.578928  [   29/   88]
per-ex loss: 0.424229  [   30/   88]
per-ex loss: 0.365241  [   31/   88]
per-ex loss: 0.437685  [   32/   88]
per-ex loss: 0.645197  [   33/   88]
per-ex loss: 0.381009  [   34/   88]
per-ex loss: 0.454960  [   35/   88]
per-ex loss: 0.594880  [   36/   88]
per-ex loss: 0.555326  [   37/   88]
per-ex loss: 0.405040  [   38/   88]
per-ex loss: 0.591836  [   39/   88]
per-ex loss: 0.500279  [   40/   88]
per-ex loss: 0.484729  [   41/   88]
per-ex loss: 0.637177  [   42/   88]
per-ex loss: 0.359616  [   43/   88]
per-ex loss: 0.481199  [   44/   88]
per-ex loss: 0.419241  [   45/   88]
per-ex loss: 0.452455  [   46/   88]
per-ex loss: 0.439763  [   47/   88]
per-ex loss: 0.553541  [   48/   88]
per-ex loss: 0.617210  [   49/   88]
per-ex loss: 0.390286  [   50/   88]
per-ex loss: 0.430143  [   51/   88]
per-ex loss: 0.608341  [   52/   88]
per-ex loss: 0.650286  [   53/   88]
per-ex loss: 0.337991  [   54/   88]
per-ex loss: 0.620914  [   55/   88]
per-ex loss: 0.583625  [   56/   88]
per-ex loss: 0.705240  [   57/   88]
per-ex loss: 0.315232  [   58/   88]
per-ex loss: 0.504207  [   59/   88]
per-ex loss: 0.378951  [   60/   88]
per-ex loss: 0.472964  [   61/   88]
per-ex loss: 0.467898  [   62/   88]
per-ex loss: 0.376428  [   63/   88]
per-ex loss: 0.487890  [   64/   88]
per-ex loss: 0.412278  [   65/   88]
per-ex loss: 0.478188  [   66/   88]
per-ex loss: 0.669651  [   67/   88]
per-ex loss: 0.439176  [   68/   88]
per-ex loss: 0.410209  [   69/   88]
per-ex loss: 0.432332  [   70/   88]
per-ex loss: 0.415067  [   71/   88]
per-ex loss: 0.609850  [   72/   88]
per-ex loss: 0.657734  [   73/   88]
per-ex loss: 0.365923  [   74/   88]
per-ex loss: 0.457582  [   75/   88]
per-ex loss: 0.407932  [   76/   88]
per-ex loss: 0.581763  [   77/   88]
per-ex loss: 0.712821  [   78/   88]
per-ex loss: 0.388440  [   79/   88]
per-ex loss: 0.574949  [   80/   88]
per-ex loss: 0.570328  [   81/   88]
per-ex loss: 0.443182  [   82/   88]
per-ex loss: 0.551632  [   83/   88]
per-ex loss: 0.446956  [   84/   88]
per-ex loss: 0.559270  [   85/   88]
per-ex loss: 0.432744  [   86/   88]
per-ex loss: 0.573535  [   87/   88]
per-ex loss: 0.534778  [   88/   88]
Train Error: Avg loss: 0.50677493
validation Error: 
 Avg loss: 0.55075406 
 F1: 0.478899 
 Precision: 0.477626 
 Recall: 0.480178
 IoU: 0.314837

test Error: 
 Avg loss: 0.49420502 
 F1: 0.554544 
 Precision: 0.555179 
 Recall: 0.553911
 IoU: 0.383647

We have finished training iteration 25
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_23_.pth
per-ex loss: 0.594122  [    1/   88]
per-ex loss: 0.355502  [    2/   88]
per-ex loss: 0.466051  [    3/   88]
per-ex loss: 0.451896  [    4/   88]
per-ex loss: 0.544916  [    5/   88]
per-ex loss: 0.532685  [    6/   88]
per-ex loss: 0.597331  [    7/   88]
per-ex loss: 0.542880  [    8/   88]
per-ex loss: 0.416125  [    9/   88]
per-ex loss: 0.421480  [   10/   88]
per-ex loss: 0.441585  [   11/   88]
per-ex loss: 0.629083  [   12/   88]
per-ex loss: 0.593713  [   13/   88]
per-ex loss: 0.456704  [   14/   88]
per-ex loss: 0.370235  [   15/   88]
per-ex loss: 0.558334  [   16/   88]
per-ex loss: 0.538724  [   17/   88]
per-ex loss: 0.643141  [   18/   88]
per-ex loss: 0.436444  [   19/   88]
per-ex loss: 0.684890  [   20/   88]
per-ex loss: 0.579744  [   21/   88]
per-ex loss: 0.401320  [   22/   88]
per-ex loss: 0.510022  [   23/   88]
per-ex loss: 0.555516  [   24/   88]
per-ex loss: 0.386387  [   25/   88]
per-ex loss: 0.683190  [   26/   88]
per-ex loss: 0.389231  [   27/   88]
per-ex loss: 0.425247  [   28/   88]
per-ex loss: 0.470060  [   29/   88]
per-ex loss: 0.462908  [   30/   88]
per-ex loss: 0.574063  [   31/   88]
per-ex loss: 0.551683  [   32/   88]
per-ex loss: 0.378617  [   33/   88]
per-ex loss: 0.616091  [   34/   88]
per-ex loss: 0.648170  [   35/   88]
per-ex loss: 0.428771  [   36/   88]
per-ex loss: 0.444758  [   37/   88]
per-ex loss: 0.429378  [   38/   88]
per-ex loss: 0.460659  [   39/   88]
per-ex loss: 0.601016  [   40/   88]
per-ex loss: 0.383397  [   41/   88]
per-ex loss: 0.408977  [   42/   88]
per-ex loss: 0.450304  [   43/   88]
per-ex loss: 0.360840  [   44/   88]
per-ex loss: 0.604727  [   45/   88]
per-ex loss: 0.594113  [   46/   88]
per-ex loss: 0.458541  [   47/   88]
per-ex loss: 0.360505  [   48/   88]
per-ex loss: 0.390557  [   49/   88]
per-ex loss: 0.636619  [   50/   88]
per-ex loss: 0.471035  [   51/   88]
per-ex loss: 0.434867  [   52/   88]
per-ex loss: 0.532079  [   53/   88]
per-ex loss: 0.428547  [   54/   88]
per-ex loss: 0.678362  [   55/   88]
per-ex loss: 0.606969  [   56/   88]
per-ex loss: 0.503403  [   57/   88]
per-ex loss: 0.395659  [   58/   88]
per-ex loss: 0.343423  [   59/   88]
per-ex loss: 0.410616  [   60/   88]
per-ex loss: 0.587910  [   61/   88]
per-ex loss: 0.411301  [   62/   88]
per-ex loss: 0.425582  [   63/   88]
per-ex loss: 0.593420  [   64/   88]
per-ex loss: 0.442279  [   65/   88]
per-ex loss: 0.491576  [   66/   88]
per-ex loss: 0.561763  [   67/   88]
per-ex loss: 0.675022  [   68/   88]
per-ex loss: 0.580863  [   69/   88]
per-ex loss: 0.521987  [   70/   88]
per-ex loss: 0.652063  [   71/   88]
per-ex loss: 0.354368  [   72/   88]
per-ex loss: 0.409758  [   73/   88]
per-ex loss: 0.553861  [   74/   88]
per-ex loss: 0.356438  [   75/   88]
per-ex loss: 0.566189  [   76/   88]
per-ex loss: 0.625859  [   77/   88]
per-ex loss: 0.371383  [   78/   88]
per-ex loss: 0.615445  [   79/   88]
per-ex loss: 0.420007  [   80/   88]
per-ex loss: 0.425956  [   81/   88]
per-ex loss: 0.456820  [   82/   88]
per-ex loss: 0.403221  [   83/   88]
per-ex loss: 0.619681  [   84/   88]
per-ex loss: 0.410056  [   85/   88]
per-ex loss: 0.731350  [   86/   88]
per-ex loss: 0.719331  [   87/   88]
per-ex loss: 0.414700  [   88/   88]
Train Error: Avg loss: 0.50141367
validation Error: 
 Avg loss: 0.54317019 
 F1: 0.485712 
 Precision: 0.607950 
 Recall: 0.404401
 IoU: 0.320753

test Error: 
 Avg loss: 0.50447679 
 F1: 0.548078 
 Precision: 0.640521 
 Recall: 0.478954
 IoU: 0.377485

We have finished training iteration 26
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_17_.pth
per-ex loss: 0.452587  [    1/   88]
per-ex loss: 0.546032  [    2/   88]
per-ex loss: 0.688364  [    3/   88]
per-ex loss: 0.395914  [    4/   88]
per-ex loss: 0.414852  [    5/   88]
per-ex loss: 0.467362  [    6/   88]
per-ex loss: 0.564232  [    7/   88]
per-ex loss: 0.386689  [    8/   88]
per-ex loss: 0.435582  [    9/   88]
per-ex loss: 0.584844  [   10/   88]
per-ex loss: 0.594039  [   11/   88]
per-ex loss: 0.362721  [   12/   88]
per-ex loss: 0.310418  [   13/   88]
per-ex loss: 0.599448  [   14/   88]
per-ex loss: 0.400157  [   15/   88]
per-ex loss: 0.554866  [   16/   88]
per-ex loss: 0.575805  [   17/   88]
per-ex loss: 0.663284  [   18/   88]
per-ex loss: 0.359497  [   19/   88]
per-ex loss: 0.705767  [   20/   88]
per-ex loss: 0.647274  [   21/   88]
per-ex loss: 0.626740  [   22/   88]
per-ex loss: 0.599751  [   23/   88]
per-ex loss: 0.456775  [   24/   88]
per-ex loss: 0.539473  [   25/   88]
per-ex loss: 0.433771  [   26/   88]
per-ex loss: 0.523632  [   27/   88]
per-ex loss: 0.610149  [   28/   88]
per-ex loss: 0.452327  [   29/   88]
per-ex loss: 0.470556  [   30/   88]
per-ex loss: 0.483461  [   31/   88]
per-ex loss: 0.532812  [   32/   88]
per-ex loss: 0.410338  [   33/   88]
per-ex loss: 0.389597  [   34/   88]
per-ex loss: 0.576182  [   35/   88]
per-ex loss: 0.713488  [   36/   88]
per-ex loss: 0.603445  [   37/   88]
per-ex loss: 0.494374  [   38/   88]
per-ex loss: 0.529398  [   39/   88]
per-ex loss: 0.470776  [   40/   88]
per-ex loss: 0.623093  [   41/   88]
per-ex loss: 0.442694  [   42/   88]
per-ex loss: 0.596415  [   43/   88]
per-ex loss: 0.361659  [   44/   88]
per-ex loss: 0.663970  [   45/   88]
per-ex loss: 0.441898  [   46/   88]
per-ex loss: 0.434511  [   47/   88]
per-ex loss: 0.456630  [   48/   88]
per-ex loss: 0.718084  [   49/   88]
per-ex loss: 0.398009  [   50/   88]
per-ex loss: 0.596023  [   51/   88]
per-ex loss: 0.412474  [   52/   88]
per-ex loss: 0.409752  [   53/   88]
per-ex loss: 0.593072  [   54/   88]
per-ex loss: 0.567830  [   55/   88]
per-ex loss: 0.410751  [   56/   88]
per-ex loss: 0.635938  [   57/   88]
per-ex loss: 0.415380  [   58/   88]
per-ex loss: 0.557824  [   59/   88]
per-ex loss: 0.390098  [   60/   88]
per-ex loss: 0.665325  [   61/   88]
per-ex loss: 0.568425  [   62/   88]
per-ex loss: 0.465568  [   63/   88]
per-ex loss: 0.541017  [   64/   88]
per-ex loss: 0.421875  [   65/   88]
per-ex loss: 0.433797  [   66/   88]
per-ex loss: 0.536770  [   67/   88]
per-ex loss: 0.577264  [   68/   88]
per-ex loss: 0.623314  [   69/   88]
per-ex loss: 0.376070  [   70/   88]
per-ex loss: 0.426828  [   71/   88]
per-ex loss: 0.492894  [   72/   88]
per-ex loss: 0.450844  [   73/   88]
per-ex loss: 0.657298  [   74/   88]
per-ex loss: 0.429778  [   75/   88]
per-ex loss: 0.467260  [   76/   88]
per-ex loss: 0.615199  [   77/   88]
per-ex loss: 0.551432  [   78/   88]
per-ex loss: 0.418926  [   79/   88]
per-ex loss: 0.425996  [   80/   88]
per-ex loss: 0.624266  [   81/   88]
per-ex loss: 0.402764  [   82/   88]
per-ex loss: 0.418588  [   83/   88]
per-ex loss: 0.428929  [   84/   88]
per-ex loss: 0.459865  [   85/   88]
per-ex loss: 0.412454  [   86/   88]
per-ex loss: 0.330856  [   87/   88]
per-ex loss: 0.639112  [   88/   88]
Train Error: Avg loss: 0.50699541
validation Error: 
 Avg loss: 0.54364220 
 F1: 0.486176 
 Precision: 0.554689 
 Recall: 0.432727
 IoU: 0.321158

test Error: 
 Avg loss: 0.50313282 
 F1: 0.548971 
 Precision: 0.607950 
 Recall: 0.500423
 IoU: 0.378332

We have finished training iteration 27
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_25_.pth
per-ex loss: 0.625128  [    1/   88]
per-ex loss: 0.539174  [    2/   88]
per-ex loss: 0.601253  [    3/   88]
per-ex loss: 0.486248  [    4/   88]
per-ex loss: 0.541686  [    5/   88]
per-ex loss: 0.591615  [    6/   88]
per-ex loss: 0.563070  [    7/   88]
per-ex loss: 0.416317  [    8/   88]
per-ex loss: 0.423832  [    9/   88]
per-ex loss: 0.603684  [   10/   88]
per-ex loss: 0.486243  [   11/   88]
per-ex loss: 0.434400  [   12/   88]
per-ex loss: 0.576998  [   13/   88]
per-ex loss: 0.382165  [   14/   88]
per-ex loss: 0.306491  [   15/   88]
per-ex loss: 0.708926  [   16/   88]
per-ex loss: 0.437933  [   17/   88]
per-ex loss: 0.564817  [   18/   88]
per-ex loss: 0.420388  [   19/   88]
per-ex loss: 0.595946  [   20/   88]
per-ex loss: 0.490540  [   21/   88]
per-ex loss: 0.642157  [   22/   88]
per-ex loss: 0.406765  [   23/   88]
per-ex loss: 0.392433  [   24/   88]
per-ex loss: 0.581573  [   25/   88]
per-ex loss: 0.542732  [   26/   88]
per-ex loss: 0.428788  [   27/   88]
per-ex loss: 0.379574  [   28/   88]
per-ex loss: 0.373324  [   29/   88]
per-ex loss: 0.688020  [   30/   88]
per-ex loss: 0.710394  [   31/   88]
per-ex loss: 0.377729  [   32/   88]
per-ex loss: 0.343993  [   33/   88]
per-ex loss: 0.426288  [   34/   88]
per-ex loss: 0.740643  [   35/   88]
per-ex loss: 0.471972  [   36/   88]
per-ex loss: 0.446794  [   37/   88]
per-ex loss: 0.475699  [   38/   88]
per-ex loss: 0.405771  [   39/   88]
per-ex loss: 0.456238  [   40/   88]
per-ex loss: 0.400791  [   41/   88]
per-ex loss: 0.624887  [   42/   88]
per-ex loss: 0.550279  [   43/   88]
per-ex loss: 0.392260  [   44/   88]
per-ex loss: 0.411861  [   45/   88]
per-ex loss: 0.480976  [   46/   88]
per-ex loss: 0.605843  [   47/   88]
per-ex loss: 0.593518  [   48/   88]
per-ex loss: 0.587036  [   49/   88]
per-ex loss: 0.345488  [   50/   88]
per-ex loss: 0.416590  [   51/   88]
per-ex loss: 0.422531  [   52/   88]
per-ex loss: 0.384240  [   53/   88]
per-ex loss: 0.627051  [   54/   88]
per-ex loss: 0.559469  [   55/   88]
per-ex loss: 0.671248  [   56/   88]
per-ex loss: 0.544461  [   57/   88]
per-ex loss: 0.640867  [   58/   88]
per-ex loss: 0.546327  [   59/   88]
per-ex loss: 0.554473  [   60/   88]
per-ex loss: 0.456337  [   61/   88]
per-ex loss: 0.418258  [   62/   88]
per-ex loss: 0.641921  [   63/   88]
per-ex loss: 0.354407  [   64/   88]
per-ex loss: 0.468335  [   65/   88]
per-ex loss: 0.408653  [   66/   88]
per-ex loss: 0.653432  [   67/   88]
per-ex loss: 0.407263  [   68/   88]
per-ex loss: 0.379233  [   69/   88]
per-ex loss: 0.396599  [   70/   88]
per-ex loss: 0.360130  [   71/   88]
per-ex loss: 0.556112  [   72/   88]
per-ex loss: 0.644060  [   73/   88]
per-ex loss: 0.526015  [   74/   88]
per-ex loss: 0.464653  [   75/   88]
per-ex loss: 0.422732  [   76/   88]
per-ex loss: 0.605188  [   77/   88]
per-ex loss: 0.594882  [   78/   88]
per-ex loss: 0.608505  [   79/   88]
per-ex loss: 0.435693  [   80/   88]
per-ex loss: 0.362031  [   81/   88]
per-ex loss: 0.589753  [   82/   88]
per-ex loss: 0.494369  [   83/   88]
per-ex loss: 0.340657  [   84/   88]
per-ex loss: 0.603185  [   85/   88]
per-ex loss: 0.693158  [   86/   88]
per-ex loss: 0.426996  [   87/   88]
per-ex loss: 0.429254  [   88/   88]
Train Error: Avg loss: 0.50215619
validation Error: 
 Avg loss: 0.57036612 
 F1: 0.451952 
 Precision: 0.444597 
 Recall: 0.459554
 IoU: 0.291950

test Error: 
 Avg loss: 0.51337976 
 F1: 0.535737 
 Precision: 0.528554 
 Recall: 0.543117
 IoU: 0.365875

We have finished training iteration 28
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_19_.pth
per-ex loss: 0.512769  [    1/   88]
per-ex loss: 0.562017  [    2/   88]
per-ex loss: 0.431421  [    3/   88]
per-ex loss: 0.377424  [    4/   88]
per-ex loss: 0.683162  [    5/   88]
per-ex loss: 0.436186  [    6/   88]
per-ex loss: 0.390221  [    7/   88]
per-ex loss: 0.592435  [    8/   88]
per-ex loss: 0.312478  [    9/   88]
per-ex loss: 0.396845  [   10/   88]
per-ex loss: 0.616732  [   11/   88]
per-ex loss: 0.493764  [   12/   88]
per-ex loss: 0.410178  [   13/   88]
per-ex loss: 0.706033  [   14/   88]
per-ex loss: 0.443667  [   15/   88]
per-ex loss: 0.400439  [   16/   88]
per-ex loss: 0.419474  [   17/   88]
per-ex loss: 0.361353  [   18/   88]
per-ex loss: 0.570606  [   19/   88]
per-ex loss: 0.580009  [   20/   88]
per-ex loss: 0.396873  [   21/   88]
per-ex loss: 0.550429  [   22/   88]
per-ex loss: 0.621186  [   23/   88]
per-ex loss: 0.433398  [   24/   88]
per-ex loss: 0.402572  [   25/   88]
per-ex loss: 0.620352  [   26/   88]
per-ex loss: 0.397317  [   27/   88]
per-ex loss: 0.425251  [   28/   88]
per-ex loss: 0.400305  [   29/   88]
per-ex loss: 0.676762  [   30/   88]
per-ex loss: 0.491632  [   31/   88]
per-ex loss: 0.543439  [   32/   88]
per-ex loss: 0.401167  [   33/   88]
per-ex loss: 0.422991  [   34/   88]
per-ex loss: 0.608505  [   35/   88]
per-ex loss: 0.715087  [   36/   88]
per-ex loss: 0.659422  [   37/   88]
per-ex loss: 0.425035  [   38/   88]
per-ex loss: 0.497115  [   39/   88]
per-ex loss: 0.471614  [   40/   88]
per-ex loss: 0.530858  [   41/   88]
per-ex loss: 0.616422  [   42/   88]
per-ex loss: 0.455897  [   43/   88]
per-ex loss: 0.593626  [   44/   88]
per-ex loss: 0.325749  [   45/   88]
per-ex loss: 0.328778  [   46/   88]
per-ex loss: 0.419433  [   47/   88]
per-ex loss: 0.450314  [   48/   88]
per-ex loss: 0.658527  [   49/   88]
per-ex loss: 0.416525  [   50/   88]
per-ex loss: 0.389274  [   51/   88]
per-ex loss: 0.357041  [   52/   88]
per-ex loss: 0.430460  [   53/   88]
per-ex loss: 0.417059  [   54/   88]
per-ex loss: 0.419398  [   55/   88]
per-ex loss: 0.418985  [   56/   88]
per-ex loss: 0.569077  [   57/   88]
per-ex loss: 0.566140  [   58/   88]
per-ex loss: 0.648287  [   59/   88]
per-ex loss: 0.398929  [   60/   88]
per-ex loss: 0.353328  [   61/   88]
per-ex loss: 0.536636  [   62/   88]
per-ex loss: 0.356475  [   63/   88]
per-ex loss: 0.675584  [   64/   88]
per-ex loss: 0.630246  [   65/   88]
per-ex loss: 0.552441  [   66/   88]
per-ex loss: 0.362994  [   67/   88]
per-ex loss: 0.359949  [   68/   88]
per-ex loss: 0.579580  [   69/   88]
per-ex loss: 0.466553  [   70/   88]
per-ex loss: 0.690547  [   71/   88]
per-ex loss: 0.468523  [   72/   88]
per-ex loss: 0.612842  [   73/   88]
per-ex loss: 0.645275  [   74/   88]
per-ex loss: 0.469287  [   75/   88]
per-ex loss: 0.581661  [   76/   88]
per-ex loss: 0.549576  [   77/   88]
per-ex loss: 0.489703  [   78/   88]
per-ex loss: 0.549357  [   79/   88]
per-ex loss: 0.482009  [   80/   88]
per-ex loss: 0.583221  [   81/   88]
per-ex loss: 0.425234  [   82/   88]
per-ex loss: 0.536215  [   83/   88]
per-ex loss: 0.668800  [   84/   88]
per-ex loss: 0.564490  [   85/   88]
per-ex loss: 0.506181  [   86/   88]
per-ex loss: 0.472132  [   87/   88]
per-ex loss: 0.578665  [   88/   88]
Train Error: Avg loss: 0.50018123
validation Error: 
 Avg loss: 0.54499004 
 F1: 0.480779 
 Precision: 0.509882 
 Recall: 0.454819
 IoU: 0.316464

test Error: 
 Avg loss: 0.49974818 
 F1: 0.551176 
 Precision: 0.571647 
 Recall: 0.532121
 IoU: 0.380430

We have finished training iteration 29
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_27_.pth
per-ex loss: 0.566632  [    1/   88]
per-ex loss: 0.413637  [    2/   88]
per-ex loss: 0.573549  [    3/   88]
per-ex loss: 0.622784  [    4/   88]
per-ex loss: 0.393707  [    5/   88]
per-ex loss: 0.334167  [    6/   88]
per-ex loss: 0.608911  [    7/   88]
per-ex loss: 0.680396  [    8/   88]
per-ex loss: 0.437065  [    9/   88]
per-ex loss: 0.386180  [   10/   88]
per-ex loss: 0.387818  [   11/   88]
per-ex loss: 0.446292  [   12/   88]
per-ex loss: 0.420877  [   13/   88]
per-ex loss: 0.427803  [   14/   88]
per-ex loss: 0.436499  [   15/   88]
per-ex loss: 0.747528  [   16/   88]
per-ex loss: 0.540188  [   17/   88]
per-ex loss: 0.377284  [   18/   88]
per-ex loss: 0.636642  [   19/   88]
per-ex loss: 0.397710  [   20/   88]
per-ex loss: 0.485330  [   21/   88]
per-ex loss: 0.541593  [   22/   88]
per-ex loss: 0.479915  [   23/   88]
per-ex loss: 0.461706  [   24/   88]
per-ex loss: 0.672912  [   25/   88]
per-ex loss: 0.540726  [   26/   88]
per-ex loss: 0.556104  [   27/   88]
per-ex loss: 0.563853  [   28/   88]
per-ex loss: 0.446760  [   29/   88]
per-ex loss: 0.486260  [   30/   88]
per-ex loss: 0.681023  [   31/   88]
per-ex loss: 0.366886  [   32/   88]
per-ex loss: 0.387642  [   33/   88]
per-ex loss: 0.558619  [   34/   88]
per-ex loss: 0.436073  [   35/   88]
per-ex loss: 0.516036  [   36/   88]
per-ex loss: 0.602717  [   37/   88]
per-ex loss: 0.591093  [   38/   88]
per-ex loss: 0.365485  [   39/   88]
per-ex loss: 0.428569  [   40/   88]
per-ex loss: 0.636259  [   41/   88]
per-ex loss: 0.577964  [   42/   88]
per-ex loss: 0.367694  [   43/   88]
per-ex loss: 0.569183  [   44/   88]
per-ex loss: 0.464697  [   45/   88]
per-ex loss: 0.396872  [   46/   88]
per-ex loss: 0.376379  [   47/   88]
per-ex loss: 0.515587  [   48/   88]
per-ex loss: 0.614249  [   49/   88]
per-ex loss: 0.350534  [   50/   88]
per-ex loss: 0.632025  [   51/   88]
per-ex loss: 0.473480  [   52/   88]
per-ex loss: 0.444105  [   53/   88]
per-ex loss: 0.438606  [   54/   88]
per-ex loss: 0.380202  [   55/   88]
per-ex loss: 0.552335  [   56/   88]
per-ex loss: 0.427995  [   57/   88]
per-ex loss: 0.589502  [   58/   88]
per-ex loss: 0.683326  [   59/   88]
per-ex loss: 0.378432  [   60/   88]
per-ex loss: 0.618666  [   61/   88]
per-ex loss: 0.593918  [   62/   88]
per-ex loss: 0.441630  [   63/   88]
per-ex loss: 0.662349  [   64/   88]
per-ex loss: 0.561840  [   65/   88]
per-ex loss: 0.355844  [   66/   88]
per-ex loss: 0.418167  [   67/   88]
per-ex loss: 0.294428  [   68/   88]
per-ex loss: 0.677512  [   69/   88]
per-ex loss: 0.561714  [   70/   88]
per-ex loss: 0.626690  [   71/   88]
per-ex loss: 0.662769  [   72/   88]
per-ex loss: 0.539901  [   73/   88]
per-ex loss: 0.434060  [   74/   88]
per-ex loss: 0.408720  [   75/   88]
per-ex loss: 0.441875  [   76/   88]
per-ex loss: 0.554309  [   77/   88]
per-ex loss: 0.560042  [   78/   88]
per-ex loss: 0.471011  [   79/   88]
per-ex loss: 0.600337  [   80/   88]
per-ex loss: 0.422377  [   81/   88]
per-ex loss: 0.394724  [   82/   88]
per-ex loss: 0.383350  [   83/   88]
per-ex loss: 0.404899  [   84/   88]
per-ex loss: 0.362148  [   85/   88]
per-ex loss: 0.695948  [   86/   88]
per-ex loss: 0.402720  [   87/   88]
per-ex loss: 0.413092  [   88/   88]
Train Error: Avg loss: 0.49817544
validation Error: 
 Avg loss: 0.55491734 
 F1: 0.469628 
 Precision: 0.645391 
 Recall: 0.369107
 IoU: 0.306872

test Error: 
 Avg loss: 0.51339239 
 F1: 0.534658 
 Precision: 0.677571 
 Recall: 0.441531
 IoU: 0.364869

We have finished training iteration 30
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_28_.pth
per-ex loss: 0.441000  [    1/   88]
per-ex loss: 0.426156  [    2/   88]
per-ex loss: 0.442112  [    3/   88]
per-ex loss: 0.334553  [    4/   88]
per-ex loss: 0.618883  [    5/   88]
per-ex loss: 0.511351  [    6/   88]
per-ex loss: 0.435658  [    7/   88]
per-ex loss: 0.633849  [    8/   88]
per-ex loss: 0.368750  [    9/   88]
per-ex loss: 0.408516  [   10/   88]
per-ex loss: 0.431769  [   11/   88]
per-ex loss: 0.519758  [   12/   88]
per-ex loss: 0.661477  [   13/   88]
per-ex loss: 0.591488  [   14/   88]
per-ex loss: 0.429503  [   15/   88]
per-ex loss: 0.349633  [   16/   88]
per-ex loss: 0.391764  [   17/   88]
per-ex loss: 0.466392  [   18/   88]
per-ex loss: 0.469069  [   19/   88]
per-ex loss: 0.556062  [   20/   88]
per-ex loss: 0.564432  [   21/   88]
per-ex loss: 0.435362  [   22/   88]
per-ex loss: 0.410856  [   23/   88]
per-ex loss: 0.560084  [   24/   88]
per-ex loss: 0.696950  [   25/   88]
per-ex loss: 0.417034  [   26/   88]
per-ex loss: 0.401638  [   27/   88]
per-ex loss: 0.534978  [   28/   88]
per-ex loss: 0.466223  [   29/   88]
per-ex loss: 0.642233  [   30/   88]
per-ex loss: 0.618886  [   31/   88]
per-ex loss: 0.650777  [   32/   88]
per-ex loss: 0.414121  [   33/   88]
per-ex loss: 0.557724  [   34/   88]
per-ex loss: 0.421119  [   35/   88]
per-ex loss: 0.638330  [   36/   88]
per-ex loss: 0.409468  [   37/   88]
per-ex loss: 0.523951  [   38/   88]
per-ex loss: 0.572452  [   39/   88]
per-ex loss: 0.645978  [   40/   88]
per-ex loss: 0.396652  [   41/   88]
per-ex loss: 0.614906  [   42/   88]
per-ex loss: 0.417335  [   43/   88]
per-ex loss: 0.355174  [   44/   88]
per-ex loss: 0.593932  [   45/   88]
per-ex loss: 0.725971  [   46/   88]
per-ex loss: 0.447546  [   47/   88]
per-ex loss: 0.425737  [   48/   88]
per-ex loss: 0.502136  [   49/   88]
per-ex loss: 0.540753  [   50/   88]
per-ex loss: 0.423940  [   51/   88]
per-ex loss: 0.553618  [   52/   88]
per-ex loss: 0.571581  [   53/   88]
per-ex loss: 0.459600  [   54/   88]
per-ex loss: 0.459845  [   55/   88]
per-ex loss: 0.598319  [   56/   88]
per-ex loss: 0.477183  [   57/   88]
per-ex loss: 0.410471  [   58/   88]
per-ex loss: 0.434543  [   59/   88]
per-ex loss: 0.418943  [   60/   88]
per-ex loss: 0.686284  [   61/   88]
per-ex loss: 0.385602  [   62/   88]
per-ex loss: 0.410540  [   63/   88]
per-ex loss: 0.349059  [   64/   88]
per-ex loss: 0.629344  [   65/   88]
per-ex loss: 0.625484  [   66/   88]
per-ex loss: 0.396798  [   67/   88]
per-ex loss: 0.401584  [   68/   88]
per-ex loss: 0.390278  [   69/   88]
per-ex loss: 0.546992  [   70/   88]
per-ex loss: 0.363309  [   71/   88]
per-ex loss: 0.417166  [   72/   88]
per-ex loss: 0.367208  [   73/   88]
per-ex loss: 0.610330  [   74/   88]
per-ex loss: 0.595922  [   75/   88]
per-ex loss: 0.600358  [   76/   88]
per-ex loss: 0.553975  [   77/   88]
per-ex loss: 0.520554  [   78/   88]
per-ex loss: 0.648434  [   79/   88]
per-ex loss: 0.499949  [   80/   88]
per-ex loss: 0.672534  [   81/   88]
per-ex loss: 0.389232  [   82/   88]
per-ex loss: 0.588620  [   83/   88]
per-ex loss: 0.543785  [   84/   88]
per-ex loss: 0.579247  [   85/   88]
per-ex loss: 0.388539  [   86/   88]
per-ex loss: 0.674056  [   87/   88]
per-ex loss: 0.374192  [   88/   88]
Train Error: Avg loss: 0.50122611
validation Error: 
 Avg loss: 0.55155685 
 F1: 0.476506 
 Precision: 0.589855 
 Recall: 0.399699
 IoU: 0.312772

test Error: 
 Avg loss: 0.50723006 
 F1: 0.541359 
 Precision: 0.651460 
 Recall: 0.463094
 IoU: 0.371140

We have finished training iteration 31
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_29_.pth
per-ex loss: 0.391420  [    1/   88]
per-ex loss: 0.484793  [    2/   88]
per-ex loss: 0.373050  [    3/   88]
per-ex loss: 0.609839  [    4/   88]
per-ex loss: 0.403821  [    5/   88]
per-ex loss: 0.350783  [    6/   88]
per-ex loss: 0.609586  [    7/   88]
per-ex loss: 0.419883  [    8/   88]
per-ex loss: 0.432776  [    9/   88]
per-ex loss: 0.530415  [   10/   88]
per-ex loss: 0.574893  [   11/   88]
per-ex loss: 0.461667  [   12/   88]
per-ex loss: 0.390840  [   13/   88]
per-ex loss: 0.512013  [   14/   88]
per-ex loss: 0.439007  [   15/   88]
per-ex loss: 0.536345  [   16/   88]
per-ex loss: 0.402051  [   17/   88]
per-ex loss: 0.382058  [   18/   88]
per-ex loss: 0.464718  [   19/   88]
per-ex loss: 0.440779  [   20/   88]
per-ex loss: 0.613340  [   21/   88]
per-ex loss: 0.394047  [   22/   88]
per-ex loss: 0.668190  [   23/   88]
per-ex loss: 0.594610  [   24/   88]
per-ex loss: 0.612494  [   25/   88]
per-ex loss: 0.657789  [   26/   88]
per-ex loss: 0.667626  [   27/   88]
per-ex loss: 0.448980  [   28/   88]
per-ex loss: 0.561115  [   29/   88]
per-ex loss: 0.408731  [   30/   88]
per-ex loss: 0.418134  [   31/   88]
per-ex loss: 0.440006  [   32/   88]
per-ex loss: 0.365687  [   33/   88]
per-ex loss: 0.705953  [   34/   88]
per-ex loss: 0.410614  [   35/   88]
per-ex loss: 0.383234  [   36/   88]
per-ex loss: 0.478699  [   37/   88]
per-ex loss: 0.536000  [   38/   88]
per-ex loss: 0.489474  [   39/   88]
per-ex loss: 0.469879  [   40/   88]
per-ex loss: 0.335224  [   41/   88]
per-ex loss: 0.643319  [   42/   88]
per-ex loss: 0.582818  [   43/   88]
per-ex loss: 0.429363  [   44/   88]
per-ex loss: 0.418073  [   45/   88]
per-ex loss: 0.527149  [   46/   88]
per-ex loss: 0.550880  [   47/   88]
per-ex loss: 0.566516  [   48/   88]
per-ex loss: 0.604766  [   49/   88]
per-ex loss: 0.697383  [   50/   88]
per-ex loss: 0.403472  [   51/   88]
per-ex loss: 0.584512  [   52/   88]
per-ex loss: 0.416165  [   53/   88]
per-ex loss: 0.679107  [   54/   88]
per-ex loss: 0.694265  [   55/   88]
per-ex loss: 0.624109  [   56/   88]
per-ex loss: 0.418596  [   57/   88]
per-ex loss: 0.335846  [   58/   88]
per-ex loss: 0.469537  [   59/   88]
per-ex loss: 0.542676  [   60/   88]
per-ex loss: 0.535713  [   61/   88]
per-ex loss: 0.430471  [   62/   88]
per-ex loss: 0.425039  [   63/   88]
per-ex loss: 0.635787  [   64/   88]
per-ex loss: 0.645378  [   65/   88]
per-ex loss: 0.426541  [   66/   88]
per-ex loss: 0.560232  [   67/   88]
per-ex loss: 0.382028  [   68/   88]
per-ex loss: 0.422903  [   69/   88]
per-ex loss: 0.541404  [   70/   88]
per-ex loss: 0.408428  [   71/   88]
per-ex loss: 0.589220  [   72/   88]
per-ex loss: 0.448996  [   73/   88]
per-ex loss: 0.439074  [   74/   88]
per-ex loss: 0.398768  [   75/   88]
per-ex loss: 0.565008  [   76/   88]
per-ex loss: 0.683674  [   77/   88]
per-ex loss: 0.355150  [   78/   88]
per-ex loss: 0.539140  [   79/   88]
per-ex loss: 0.369931  [   80/   88]
per-ex loss: 0.451157  [   81/   88]
per-ex loss: 0.411667  [   82/   88]
per-ex loss: 0.578380  [   83/   88]
per-ex loss: 0.672506  [   84/   88]
per-ex loss: 0.581975  [   85/   88]
per-ex loss: 0.621234  [   86/   88]
per-ex loss: 0.466278  [   87/   88]
per-ex loss: 0.363358  [   88/   88]
Train Error: Avg loss: 0.50009719
validation Error: 
 Avg loss: 0.54139703 
 F1: 0.484555 
 Precision: 0.484978 
 Recall: 0.484133
 IoU: 0.319744

test Error: 
 Avg loss: 0.49496605 
 F1: 0.557482 
 Precision: 0.538518 
 Recall: 0.577830
 IoU: 0.386464

We have finished training iteration 32
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_30_.pth
per-ex loss: 0.671189  [    1/   88]
per-ex loss: 0.594922  [    2/   88]
per-ex loss: 0.564100  [    3/   88]
per-ex loss: 0.588625  [    4/   88]
per-ex loss: 0.568732  [    5/   88]
per-ex loss: 0.678838  [    6/   88]
per-ex loss: 0.401573  [    7/   88]
per-ex loss: 0.525191  [    8/   88]
per-ex loss: 0.398728  [    9/   88]
per-ex loss: 0.440910  [   10/   88]
per-ex loss: 0.390854  [   11/   88]
per-ex loss: 0.649208  [   12/   88]
per-ex loss: 0.661892  [   13/   88]
per-ex loss: 0.431277  [   14/   88]
per-ex loss: 0.397238  [   15/   88]
per-ex loss: 0.479954  [   16/   88]
per-ex loss: 0.540640  [   17/   88]
per-ex loss: 0.412286  [   18/   88]
per-ex loss: 0.347055  [   19/   88]
per-ex loss: 0.470106  [   20/   88]
per-ex loss: 0.502717  [   21/   88]
per-ex loss: 0.344981  [   22/   88]
per-ex loss: 0.322389  [   23/   88]
per-ex loss: 0.444181  [   24/   88]
per-ex loss: 0.412923  [   25/   88]
per-ex loss: 0.455587  [   26/   88]
per-ex loss: 0.379111  [   27/   88]
per-ex loss: 0.723954  [   28/   88]
per-ex loss: 0.570651  [   29/   88]
per-ex loss: 0.604684  [   30/   88]
per-ex loss: 0.718506  [   31/   88]
per-ex loss: 0.639999  [   32/   88]
per-ex loss: 0.577302  [   33/   88]
per-ex loss: 0.553029  [   34/   88]
per-ex loss: 0.384754  [   35/   88]
per-ex loss: 0.382805  [   36/   88]
per-ex loss: 0.456632  [   37/   88]
per-ex loss: 0.553361  [   38/   88]
per-ex loss: 0.448883  [   39/   88]
per-ex loss: 0.520610  [   40/   88]
per-ex loss: 0.518324  [   41/   88]
per-ex loss: 0.391029  [   42/   88]
per-ex loss: 0.627342  [   43/   88]
per-ex loss: 0.582504  [   44/   88]
per-ex loss: 0.408766  [   45/   88]
per-ex loss: 0.422565  [   46/   88]
per-ex loss: 0.471238  [   47/   88]
per-ex loss: 0.407447  [   48/   88]
per-ex loss: 0.475608  [   49/   88]
per-ex loss: 0.401165  [   50/   88]
per-ex loss: 0.381336  [   51/   88]
per-ex loss: 0.621268  [   52/   88]
per-ex loss: 0.448053  [   53/   88]
per-ex loss: 0.396723  [   54/   88]
per-ex loss: 0.634360  [   55/   88]
per-ex loss: 0.376970  [   56/   88]
per-ex loss: 0.684218  [   57/   88]
per-ex loss: 0.658765  [   58/   88]
per-ex loss: 0.416367  [   59/   88]
per-ex loss: 0.559078  [   60/   88]
per-ex loss: 0.417854  [   61/   88]
per-ex loss: 0.357690  [   62/   88]
per-ex loss: 0.536641  [   63/   88]
per-ex loss: 0.611864  [   64/   88]
per-ex loss: 0.453520  [   65/   88]
per-ex loss: 0.558366  [   66/   88]
per-ex loss: 0.410364  [   67/   88]
per-ex loss: 0.463922  [   68/   88]
per-ex loss: 0.310787  [   69/   88]
per-ex loss: 0.542950  [   70/   88]
per-ex loss: 0.398851  [   71/   88]
per-ex loss: 0.494248  [   72/   88]
per-ex loss: 0.604323  [   73/   88]
per-ex loss: 0.431886  [   74/   88]
per-ex loss: 0.564692  [   75/   88]
per-ex loss: 0.533900  [   76/   88]
per-ex loss: 0.684997  [   77/   88]
per-ex loss: 0.607003  [   78/   88]
per-ex loss: 0.485934  [   79/   88]
per-ex loss: 0.445580  [   80/   88]
per-ex loss: 0.570650  [   81/   88]
per-ex loss: 0.481155  [   82/   88]
per-ex loss: 0.570160  [   83/   88]
per-ex loss: 0.426462  [   84/   88]
per-ex loss: 0.389619  [   85/   88]
per-ex loss: 0.418491  [   86/   88]
per-ex loss: 0.572917  [   87/   88]
per-ex loss: 0.593576  [   88/   88]
Train Error: Avg loss: 0.50036257
validation Error: 
 Avg loss: 0.54829067 
 F1: 0.479668 
 Precision: 0.602924 
 Recall: 0.398253
 IoU: 0.315502

test Error: 
 Avg loss: 0.50327209 
 F1: 0.549581 
 Precision: 0.638047 
 Recall: 0.482660
 IoU: 0.378912

We have finished training iteration 33
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_31_.pth
per-ex loss: 0.398592  [    1/   88]
per-ex loss: 0.388959  [    2/   88]
per-ex loss: 0.667215  [    3/   88]
per-ex loss: 0.517694  [    4/   88]
per-ex loss: 0.597669  [    5/   88]
per-ex loss: 0.348199  [    6/   88]
per-ex loss: 0.324973  [    7/   88]
per-ex loss: 0.364142  [    8/   88]
per-ex loss: 0.465505  [    9/   88]
per-ex loss: 0.606506  [   10/   88]
per-ex loss: 0.454423  [   11/   88]
per-ex loss: 0.512494  [   12/   88]
per-ex loss: 0.386540  [   13/   88]
per-ex loss: 0.637917  [   14/   88]
per-ex loss: 0.347224  [   15/   88]
per-ex loss: 0.411065  [   16/   88]
per-ex loss: 0.451049  [   17/   88]
per-ex loss: 0.549755  [   18/   88]
per-ex loss: 0.545927  [   19/   88]
per-ex loss: 0.595032  [   20/   88]
per-ex loss: 0.645250  [   21/   88]
per-ex loss: 0.409958  [   22/   88]
per-ex loss: 0.548658  [   23/   88]
per-ex loss: 0.619452  [   24/   88]
per-ex loss: 0.429976  [   25/   88]
per-ex loss: 0.574539  [   26/   88]
per-ex loss: 0.403732  [   27/   88]
per-ex loss: 0.394593  [   28/   88]
per-ex loss: 0.660253  [   29/   88]
per-ex loss: 0.378483  [   30/   88]
per-ex loss: 0.677147  [   31/   88]
per-ex loss: 0.428499  [   32/   88]
per-ex loss: 0.410282  [   33/   88]
per-ex loss: 0.533619  [   34/   88]
per-ex loss: 0.373706  [   35/   88]
per-ex loss: 0.434818  [   36/   88]
per-ex loss: 0.360664  [   37/   88]
per-ex loss: 0.712569  [   38/   88]
per-ex loss: 0.574837  [   39/   88]
per-ex loss: 0.449030  [   40/   88]
per-ex loss: 0.335857  [   41/   88]
per-ex loss: 0.554515  [   42/   88]
per-ex loss: 0.439147  [   43/   88]
per-ex loss: 0.426433  [   44/   88]
per-ex loss: 0.345014  [   45/   88]
per-ex loss: 0.374685  [   46/   88]
per-ex loss: 0.573721  [   47/   88]
per-ex loss: 0.505454  [   48/   88]
per-ex loss: 0.381535  [   49/   88]
per-ex loss: 0.352186  [   50/   88]
per-ex loss: 0.432686  [   51/   88]
per-ex loss: 0.350568  [   52/   88]
per-ex loss: 0.627696  [   53/   88]
per-ex loss: 0.439517  [   54/   88]
per-ex loss: 0.410395  [   55/   88]
per-ex loss: 0.521398  [   56/   88]
per-ex loss: 0.386809  [   57/   88]
per-ex loss: 0.523331  [   58/   88]
per-ex loss: 0.628719  [   59/   88]
per-ex loss: 0.583428  [   60/   88]
per-ex loss: 0.401076  [   61/   88]
per-ex loss: 0.653801  [   62/   88]
per-ex loss: 0.640385  [   63/   88]
per-ex loss: 0.445882  [   64/   88]
per-ex loss: 0.411282  [   65/   88]
per-ex loss: 0.664461  [   66/   88]
per-ex loss: 0.444337  [   67/   88]
per-ex loss: 0.419341  [   68/   88]
per-ex loss: 0.474529  [   69/   88]
per-ex loss: 0.441515  [   70/   88]
per-ex loss: 0.607309  [   71/   88]
per-ex loss: 0.544696  [   72/   88]
per-ex loss: 0.620522  [   73/   88]
per-ex loss: 0.482397  [   74/   88]
per-ex loss: 0.412191  [   75/   88]
per-ex loss: 0.385372  [   76/   88]
per-ex loss: 0.540014  [   77/   88]
per-ex loss: 0.521753  [   78/   88]
per-ex loss: 0.576382  [   79/   88]
per-ex loss: 0.407526  [   80/   88]
per-ex loss: 0.453702  [   81/   88]
per-ex loss: 0.384791  [   82/   88]
per-ex loss: 0.715205  [   83/   88]
per-ex loss: 0.581466  [   84/   88]
per-ex loss: 0.669397  [   85/   88]
per-ex loss: 0.633120  [   86/   88]
per-ex loss: 0.556579  [   87/   88]
per-ex loss: 0.604240  [   88/   88]
Train Error: Avg loss: 0.49406032
validation Error: 
 Avg loss: 0.53932980 
 F1: 0.489953 
 Precision: 0.612777 
 Recall: 0.408145
 IoU: 0.324462

test Error: 
 Avg loss: 0.50186632 
 F1: 0.548446 
 Precision: 0.658725 
 Recall: 0.469796
 IoU: 0.377834

We have finished training iteration 34
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_26_.pth
per-ex loss: 0.400840  [    1/   88]
per-ex loss: 0.612520  [    2/   88]
per-ex loss: 0.624278  [    3/   88]
per-ex loss: 0.575572  [    4/   88]
per-ex loss: 0.556382  [    5/   88]
per-ex loss: 0.591391  [    6/   88]
per-ex loss: 0.342283  [    7/   88]
per-ex loss: 0.454975  [    8/   88]
per-ex loss: 0.384634  [    9/   88]
per-ex loss: 0.424907  [   10/   88]
per-ex loss: 0.463204  [   11/   88]
per-ex loss: 0.363183  [   12/   88]
per-ex loss: 0.683778  [   13/   88]
per-ex loss: 0.446137  [   14/   88]
per-ex loss: 0.370286  [   15/   88]
per-ex loss: 0.414935  [   16/   88]
per-ex loss: 0.402466  [   17/   88]
per-ex loss: 0.730546  [   18/   88]
per-ex loss: 0.440724  [   19/   88]
per-ex loss: 0.477384  [   20/   88]
per-ex loss: 0.424098  [   21/   88]
per-ex loss: 0.394902  [   22/   88]
per-ex loss: 0.375243  [   23/   88]
per-ex loss: 0.419998  [   24/   88]
per-ex loss: 0.440627  [   25/   88]
per-ex loss: 0.543017  [   26/   88]
per-ex loss: 0.627654  [   27/   88]
per-ex loss: 0.379243  [   28/   88]
per-ex loss: 0.629272  [   29/   88]
per-ex loss: 0.514153  [   30/   88]
per-ex loss: 0.445092  [   31/   88]
per-ex loss: 0.380471  [   32/   88]
per-ex loss: 0.618398  [   33/   88]
per-ex loss: 0.442426  [   34/   88]
per-ex loss: 0.573329  [   35/   88]
per-ex loss: 0.663060  [   36/   88]
per-ex loss: 0.678866  [   37/   88]
per-ex loss: 0.435081  [   38/   88]
per-ex loss: 0.597222  [   39/   88]
per-ex loss: 0.513948  [   40/   88]
per-ex loss: 0.468121  [   41/   88]
per-ex loss: 0.677527  [   42/   88]
per-ex loss: 0.538253  [   43/   88]
per-ex loss: 0.483663  [   44/   88]
per-ex loss: 0.598212  [   45/   88]
per-ex loss: 0.404432  [   46/   88]
per-ex loss: 0.633774  [   47/   88]
per-ex loss: 0.630903  [   48/   88]
per-ex loss: 0.392928  [   49/   88]
per-ex loss: 0.438159  [   50/   88]
per-ex loss: 0.542718  [   51/   88]
per-ex loss: 0.458452  [   52/   88]
per-ex loss: 0.381502  [   53/   88]
per-ex loss: 0.548772  [   54/   88]
per-ex loss: 0.358478  [   55/   88]
per-ex loss: 0.331264  [   56/   88]
per-ex loss: 0.397914  [   57/   88]
per-ex loss: 0.626353  [   58/   88]
per-ex loss: 0.556799  [   59/   88]
per-ex loss: 0.442416  [   60/   88]
per-ex loss: 0.363240  [   61/   88]
per-ex loss: 0.690872  [   62/   88]
per-ex loss: 0.424020  [   63/   88]
per-ex loss: 0.325398  [   64/   88]
per-ex loss: 0.337449  [   65/   88]
per-ex loss: 0.370888  [   66/   88]
per-ex loss: 0.344495  [   67/   88]
per-ex loss: 0.510943  [   68/   88]
per-ex loss: 0.359610  [   69/   88]
per-ex loss: 0.633276  [   70/   88]
per-ex loss: 0.527132  [   71/   88]
per-ex loss: 0.385343  [   72/   88]
per-ex loss: 0.585325  [   73/   88]
per-ex loss: 0.391803  [   74/   88]
per-ex loss: 0.531936  [   75/   88]
per-ex loss: 0.414498  [   76/   88]
per-ex loss: 0.565473  [   77/   88]
per-ex loss: 0.552568  [   78/   88]
per-ex loss: 0.528599  [   79/   88]
per-ex loss: 0.691060  [   80/   88]
per-ex loss: 0.498633  [   81/   88]
per-ex loss: 0.500822  [   82/   88]
per-ex loss: 0.542120  [   83/   88]
per-ex loss: 0.608577  [   84/   88]
per-ex loss: 0.543798  [   85/   88]
per-ex loss: 0.535292  [   86/   88]
per-ex loss: 0.653926  [   87/   88]
per-ex loss: 0.413247  [   88/   88]
Train Error: Avg loss: 0.49542619
validation Error: 
 Avg loss: 0.53197574 
 F1: 0.496067 
 Precision: 0.536224 
 Recall: 0.461505
 IoU: 0.329846

test Error: 
 Avg loss: 0.49432900 
 F1: 0.556995 
 Precision: 0.574557 
 Recall: 0.540475
 IoU: 0.385996

We have finished training iteration 35
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_33_.pth
per-ex loss: 0.585809  [    1/   88]
per-ex loss: 0.507854  [    2/   88]
per-ex loss: 0.445800  [    3/   88]
per-ex loss: 0.527818  [    4/   88]
per-ex loss: 0.346009  [    5/   88]
per-ex loss: 0.580078  [    6/   88]
per-ex loss: 0.444218  [    7/   88]
per-ex loss: 0.371620  [    8/   88]
per-ex loss: 0.412362  [    9/   88]
per-ex loss: 0.440965  [   10/   88]
per-ex loss: 0.625904  [   11/   88]
per-ex loss: 0.606517  [   12/   88]
per-ex loss: 0.502952  [   13/   88]
per-ex loss: 0.411648  [   14/   88]
per-ex loss: 0.445359  [   15/   88]
per-ex loss: 0.589545  [   16/   88]
per-ex loss: 0.683977  [   17/   88]
per-ex loss: 0.391888  [   18/   88]
per-ex loss: 0.465434  [   19/   88]
per-ex loss: 0.399921  [   20/   88]
per-ex loss: 0.400486  [   21/   88]
per-ex loss: 0.650160  [   22/   88]
per-ex loss: 0.422165  [   23/   88]
per-ex loss: 0.678390  [   24/   88]
per-ex loss: 0.410915  [   25/   88]
per-ex loss: 0.568116  [   26/   88]
per-ex loss: 0.410533  [   27/   88]
per-ex loss: 0.417561  [   28/   88]
per-ex loss: 0.418367  [   29/   88]
per-ex loss: 0.576480  [   30/   88]
per-ex loss: 0.696313  [   31/   88]
per-ex loss: 0.533207  [   32/   88]
per-ex loss: 0.386705  [   33/   88]
per-ex loss: 0.522853  [   34/   88]
per-ex loss: 0.378374  [   35/   88]
per-ex loss: 0.480444  [   36/   88]
per-ex loss: 0.606804  [   37/   88]
per-ex loss: 0.372726  [   38/   88]
per-ex loss: 0.435182  [   39/   88]
per-ex loss: 0.538253  [   40/   88]
per-ex loss: 0.405963  [   41/   88]
per-ex loss: 0.649033  [   42/   88]
per-ex loss: 0.394666  [   43/   88]
per-ex loss: 0.412530  [   44/   88]
per-ex loss: 0.613782  [   45/   88]
per-ex loss: 0.539041  [   46/   88]
per-ex loss: 0.631935  [   47/   88]
per-ex loss: 0.440838  [   48/   88]
per-ex loss: 0.551149  [   49/   88]
per-ex loss: 0.418017  [   50/   88]
per-ex loss: 0.359459  [   51/   88]
per-ex loss: 0.385547  [   52/   88]
per-ex loss: 0.626220  [   53/   88]
per-ex loss: 0.420614  [   54/   88]
per-ex loss: 0.705337  [   55/   88]
per-ex loss: 0.579137  [   56/   88]
per-ex loss: 0.351298  [   57/   88]
per-ex loss: 0.360088  [   58/   88]
per-ex loss: 0.411430  [   59/   88]
per-ex loss: 0.564486  [   60/   88]
per-ex loss: 0.562564  [   61/   88]
per-ex loss: 0.376423  [   62/   88]
per-ex loss: 0.378228  [   63/   88]
per-ex loss: 0.535994  [   64/   88]
per-ex loss: 0.442138  [   65/   88]
per-ex loss: 0.429388  [   66/   88]
per-ex loss: 0.409820  [   67/   88]
per-ex loss: 0.614646  [   68/   88]
per-ex loss: 0.521095  [   69/   88]
per-ex loss: 0.610020  [   70/   88]
per-ex loss: 0.537273  [   71/   88]
per-ex loss: 0.354728  [   72/   88]
per-ex loss: 0.582127  [   73/   88]
per-ex loss: 0.426822  [   74/   88]
per-ex loss: 0.597969  [   75/   88]
per-ex loss: 0.363524  [   76/   88]
per-ex loss: 0.630995  [   77/   88]
per-ex loss: 0.600456  [   78/   88]
per-ex loss: 0.393741  [   79/   88]
per-ex loss: 0.546951  [   80/   88]
per-ex loss: 0.433466  [   81/   88]
per-ex loss: 0.670749  [   82/   88]
per-ex loss: 0.448531  [   83/   88]
per-ex loss: 0.590482  [   84/   88]
per-ex loss: 0.420187  [   85/   88]
per-ex loss: 0.314671  [   86/   88]
per-ex loss: 0.604539  [   87/   88]
per-ex loss: 0.401116  [   88/   88]
Train Error: Avg loss: 0.49214684
validation Error: 
 Avg loss: 0.53333123 
 F1: 0.495516 
 Precision: 0.533809 
 Recall: 0.462350
 IoU: 0.329360

test Error: 
 Avg loss: 0.48728543 
 F1: 0.566307 
 Precision: 0.587627 
 Recall: 0.546480
 IoU: 0.394999

We have finished training iteration 36
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_24_.pth
per-ex loss: 0.287857  [    1/   88]
per-ex loss: 0.517548  [    2/   88]
per-ex loss: 0.505205  [    3/   88]
per-ex loss: 0.466163  [    4/   88]
per-ex loss: 0.549961  [    5/   88]
per-ex loss: 0.409872  [    6/   88]
per-ex loss: 0.619371  [    7/   88]
per-ex loss: 0.671153  [    8/   88]
per-ex loss: 0.674007  [    9/   88]
per-ex loss: 0.632098  [   10/   88]
per-ex loss: 0.557497  [   11/   88]
per-ex loss: 0.707659  [   12/   88]
per-ex loss: 0.392811  [   13/   88]
per-ex loss: 0.593948  [   14/   88]
per-ex loss: 0.537738  [   15/   88]
per-ex loss: 0.481213  [   16/   88]
per-ex loss: 0.369421  [   17/   88]
per-ex loss: 0.642044  [   18/   88]
per-ex loss: 0.413970  [   19/   88]
per-ex loss: 0.385118  [   20/   88]
per-ex loss: 0.557346  [   21/   88]
per-ex loss: 0.423130  [   22/   88]
per-ex loss: 0.464714  [   23/   88]
per-ex loss: 0.429494  [   24/   88]
per-ex loss: 0.417209  [   25/   88]
per-ex loss: 0.402112  [   26/   88]
per-ex loss: 0.319640  [   27/   88]
per-ex loss: 0.660770  [   28/   88]
per-ex loss: 0.347262  [   29/   88]
per-ex loss: 0.394466  [   30/   88]
per-ex loss: 0.396524  [   31/   88]
per-ex loss: 0.443295  [   32/   88]
per-ex loss: 0.509987  [   33/   88]
per-ex loss: 0.586852  [   34/   88]
per-ex loss: 0.569033  [   35/   88]
per-ex loss: 0.523726  [   36/   88]
per-ex loss: 0.404510  [   37/   88]
per-ex loss: 0.538030  [   38/   88]
per-ex loss: 0.568583  [   39/   88]
per-ex loss: 0.496538  [   40/   88]
per-ex loss: 0.342199  [   41/   88]
per-ex loss: 0.595326  [   42/   88]
per-ex loss: 0.432488  [   43/   88]
per-ex loss: 0.545153  [   44/   88]
per-ex loss: 0.656888  [   45/   88]
per-ex loss: 0.388031  [   46/   88]
per-ex loss: 0.365631  [   47/   88]
per-ex loss: 0.700385  [   48/   88]
per-ex loss: 0.410331  [   49/   88]
per-ex loss: 0.647006  [   50/   88]
per-ex loss: 0.332200  [   51/   88]
per-ex loss: 0.388410  [   52/   88]
per-ex loss: 0.401412  [   53/   88]
per-ex loss: 0.406946  [   54/   88]
per-ex loss: 0.412259  [   55/   88]
per-ex loss: 0.597456  [   56/   88]
per-ex loss: 0.429411  [   57/   88]
per-ex loss: 0.679935  [   58/   88]
per-ex loss: 0.590991  [   59/   88]
per-ex loss: 0.379642  [   60/   88]
per-ex loss: 0.440321  [   61/   88]
per-ex loss: 0.458452  [   62/   88]
per-ex loss: 0.367702  [   63/   88]
per-ex loss: 0.534750  [   64/   88]
per-ex loss: 0.396707  [   65/   88]
per-ex loss: 0.529193  [   66/   88]
per-ex loss: 0.357468  [   67/   88]
per-ex loss: 0.397912  [   68/   88]
per-ex loss: 0.607882  [   69/   88]
per-ex loss: 0.565428  [   70/   88]
per-ex loss: 0.372704  [   71/   88]
per-ex loss: 0.447216  [   72/   88]
per-ex loss: 0.616780  [   73/   88]
per-ex loss: 0.604434  [   74/   88]
per-ex loss: 0.414049  [   75/   88]
per-ex loss: 0.432460  [   76/   88]
per-ex loss: 0.402491  [   77/   88]
per-ex loss: 0.560578  [   78/   88]
per-ex loss: 0.435142  [   79/   88]
per-ex loss: 0.669948  [   80/   88]
per-ex loss: 0.416351  [   81/   88]
per-ex loss: 0.670319  [   82/   88]
per-ex loss: 0.621500  [   83/   88]
per-ex loss: 0.602051  [   84/   88]
per-ex loss: 0.419569  [   85/   88]
per-ex loss: 0.585508  [   86/   88]
per-ex loss: 0.445002  [   87/   88]
per-ex loss: 0.437102  [   88/   88]
Train Error: Avg loss: 0.49294315
validation Error: 
 Avg loss: 0.54078785 
 F1: 0.485568 
 Precision: 0.558474 
 Recall: 0.429498
 IoU: 0.320627

test Error: 
 Avg loss: 0.50178039 
 F1: 0.548972 
 Precision: 0.612929 
 Recall: 0.497100
 IoU: 0.378333

We have finished training iteration 37
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_32_.pth
per-ex loss: 0.622072  [    1/   88]
per-ex loss: 0.601381  [    2/   88]
per-ex loss: 0.402346  [    3/   88]
per-ex loss: 0.635427  [    4/   88]
per-ex loss: 0.381522  [    5/   88]
per-ex loss: 0.644001  [    6/   88]
per-ex loss: 0.411054  [    7/   88]
per-ex loss: 0.387182  [    8/   88]
per-ex loss: 0.478678  [    9/   88]
per-ex loss: 0.392946  [   10/   88]
per-ex loss: 0.506997  [   11/   88]
per-ex loss: 0.351011  [   12/   88]
per-ex loss: 0.514512  [   13/   88]
per-ex loss: 0.469522  [   14/   88]
per-ex loss: 0.669342  [   15/   88]
per-ex loss: 0.390081  [   16/   88]
per-ex loss: 0.400413  [   17/   88]
per-ex loss: 0.592521  [   18/   88]
per-ex loss: 0.552867  [   19/   88]
per-ex loss: 0.446544  [   20/   88]
per-ex loss: 0.376131  [   21/   88]
per-ex loss: 0.389176  [   22/   88]
per-ex loss: 0.623117  [   23/   88]
per-ex loss: 0.635739  [   24/   88]
per-ex loss: 0.528828  [   25/   88]
per-ex loss: 0.402657  [   26/   88]
per-ex loss: 0.311749  [   27/   88]
per-ex loss: 0.441473  [   28/   88]
per-ex loss: 0.349304  [   29/   88]
per-ex loss: 0.553746  [   30/   88]
per-ex loss: 0.365387  [   31/   88]
per-ex loss: 0.419615  [   32/   88]
per-ex loss: 0.636842  [   33/   88]
per-ex loss: 0.647437  [   34/   88]
per-ex loss: 0.436816  [   35/   88]
per-ex loss: 0.406269  [   36/   88]
per-ex loss: 0.615665  [   37/   88]
per-ex loss: 0.420664  [   38/   88]
per-ex loss: 0.461481  [   39/   88]
per-ex loss: 0.438586  [   40/   88]
per-ex loss: 0.532571  [   41/   88]
per-ex loss: 0.533488  [   42/   88]
per-ex loss: 0.516752  [   43/   88]
per-ex loss: 0.412617  [   44/   88]
per-ex loss: 0.361412  [   45/   88]
per-ex loss: 0.604863  [   46/   88]
per-ex loss: 0.272983  [   47/   88]
per-ex loss: 0.395772  [   48/   88]
per-ex loss: 0.551769  [   49/   88]
per-ex loss: 0.423651  [   50/   88]
per-ex loss: 0.415251  [   51/   88]
per-ex loss: 0.543304  [   52/   88]
per-ex loss: 0.420776  [   53/   88]
per-ex loss: 0.571446  [   54/   88]
per-ex loss: 0.412611  [   55/   88]
per-ex loss: 0.561654  [   56/   88]
per-ex loss: 0.470687  [   57/   88]
per-ex loss: 0.557443  [   58/   88]
per-ex loss: 0.433458  [   59/   88]
per-ex loss: 0.573983  [   60/   88]
per-ex loss: 0.672283  [   61/   88]
per-ex loss: 0.568092  [   62/   88]
per-ex loss: 0.712345  [   63/   88]
per-ex loss: 0.522519  [   64/   88]
per-ex loss: 0.555463  [   65/   88]
per-ex loss: 0.380365  [   66/   88]
per-ex loss: 0.595589  [   67/   88]
per-ex loss: 0.455640  [   68/   88]
per-ex loss: 0.472576  [   69/   88]
per-ex loss: 0.672274  [   70/   88]
per-ex loss: 0.380249  [   71/   88]
per-ex loss: 0.492343  [   72/   88]
per-ex loss: 0.371751  [   73/   88]
per-ex loss: 0.601896  [   74/   88]
per-ex loss: 0.429722  [   75/   88]
per-ex loss: 0.685499  [   76/   88]
per-ex loss: 0.337992  [   77/   88]
per-ex loss: 0.359381  [   78/   88]
per-ex loss: 0.531386  [   79/   88]
per-ex loss: 0.579874  [   80/   88]
per-ex loss: 0.439797  [   81/   88]
per-ex loss: 0.356712  [   82/   88]
per-ex loss: 0.602989  [   83/   88]
per-ex loss: 0.404748  [   84/   88]
per-ex loss: 0.447374  [   85/   88]
per-ex loss: 0.593534  [   86/   88]
per-ex loss: 0.391291  [   87/   88]
per-ex loss: 0.680618  [   88/   88]
Train Error: Avg loss: 0.49058968
validation Error: 
 Avg loss: 0.54736836 
 F1: 0.479775 
 Precision: 0.563868 
 Recall: 0.417509
 IoU: 0.315594

test Error: 
 Avg loss: 0.50962661 
 F1: 0.540618 
 Precision: 0.594681 
 Recall: 0.495565
 IoU: 0.370443

We have finished training iteration 38
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_34_.pth
per-ex loss: 0.395135  [    1/   88]
per-ex loss: 0.648407  [    2/   88]
per-ex loss: 0.374054  [    3/   88]
per-ex loss: 0.551169  [    4/   88]
per-ex loss: 0.361313  [    5/   88]
per-ex loss: 0.607392  [    6/   88]
per-ex loss: 0.408779  [    7/   88]
per-ex loss: 0.404480  [    8/   88]
per-ex loss: 0.565262  [    9/   88]
per-ex loss: 0.473451  [   10/   88]
per-ex loss: 0.541186  [   11/   88]
per-ex loss: 0.674599  [   12/   88]
per-ex loss: 0.429703  [   13/   88]
per-ex loss: 0.449281  [   14/   88]
per-ex loss: 0.546321  [   15/   88]
per-ex loss: 0.700658  [   16/   88]
per-ex loss: 0.627028  [   17/   88]
per-ex loss: 0.460929  [   18/   88]
per-ex loss: 0.424869  [   19/   88]
per-ex loss: 0.550439  [   20/   88]
per-ex loss: 0.377511  [   21/   88]
per-ex loss: 0.617183  [   22/   88]
per-ex loss: 0.454793  [   23/   88]
per-ex loss: 0.722264  [   24/   88]
per-ex loss: 0.384469  [   25/   88]
per-ex loss: 0.580728  [   26/   88]
per-ex loss: 0.418163  [   27/   88]
per-ex loss: 0.368037  [   28/   88]
per-ex loss: 0.655794  [   29/   88]
per-ex loss: 0.553566  [   30/   88]
per-ex loss: 0.378605  [   31/   88]
per-ex loss: 0.461169  [   32/   88]
per-ex loss: 0.589440  [   33/   88]
per-ex loss: 0.627070  [   34/   88]
per-ex loss: 0.407004  [   35/   88]
per-ex loss: 0.419185  [   36/   88]
per-ex loss: 0.441495  [   37/   88]
per-ex loss: 0.460893  [   38/   88]
per-ex loss: 0.553143  [   39/   88]
per-ex loss: 0.372107  [   40/   88]
per-ex loss: 0.602836  [   41/   88]
per-ex loss: 0.388185  [   42/   88]
per-ex loss: 0.505281  [   43/   88]
per-ex loss: 0.434568  [   44/   88]
per-ex loss: 0.443540  [   45/   88]
per-ex loss: 0.616247  [   46/   88]
per-ex loss: 0.414001  [   47/   88]
per-ex loss: 0.415726  [   48/   88]
per-ex loss: 0.331800  [   49/   88]
per-ex loss: 0.556131  [   50/   88]
per-ex loss: 0.358262  [   51/   88]
per-ex loss: 0.563965  [   52/   88]
per-ex loss: 0.463575  [   53/   88]
per-ex loss: 0.493233  [   54/   88]
per-ex loss: 0.306686  [   55/   88]
per-ex loss: 0.572254  [   56/   88]
per-ex loss: 0.418373  [   57/   88]
per-ex loss: 0.483657  [   58/   88]
per-ex loss: 0.432741  [   59/   88]
per-ex loss: 0.586107  [   60/   88]
per-ex loss: 0.357057  [   61/   88]
per-ex loss: 0.663998  [   62/   88]
per-ex loss: 0.550777  [   63/   88]
per-ex loss: 0.498710  [   64/   88]
per-ex loss: 0.540155  [   65/   88]
per-ex loss: 0.642453  [   66/   88]
per-ex loss: 0.674999  [   67/   88]
per-ex loss: 0.431682  [   68/   88]
per-ex loss: 0.418988  [   69/   88]
per-ex loss: 0.427223  [   70/   88]
per-ex loss: 0.336011  [   71/   88]
per-ex loss: 0.431098  [   72/   88]
per-ex loss: 0.386048  [   73/   88]
per-ex loss: 0.410409  [   74/   88]
per-ex loss: 0.656021  [   75/   88]
per-ex loss: 0.381816  [   76/   88]
per-ex loss: 0.596950  [   77/   88]
per-ex loss: 0.524316  [   78/   88]
per-ex loss: 0.644511  [   79/   88]
per-ex loss: 0.553286  [   80/   88]
per-ex loss: 0.361266  [   81/   88]
per-ex loss: 0.401863  [   82/   88]
per-ex loss: 0.533872  [   83/   88]
per-ex loss: 0.522695  [   84/   88]
per-ex loss: 0.470723  [   85/   88]
per-ex loss: 0.548230  [   86/   88]
per-ex loss: 0.565072  [   87/   88]
per-ex loss: 0.411857  [   88/   88]
Train Error: Avg loss: 0.49279923
validation Error: 
 Avg loss: 0.57465872 
 F1: 0.453757 
 Precision: 0.694775 
 Recall: 0.336890
 IoU: 0.293458

test Error: 
 Avg loss: 0.54451908 
 F1: 0.495521 
 Precision: 0.754127 
 Recall: 0.368987
 IoU: 0.329364

We have finished training iteration 39
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_37_.pth
per-ex loss: 0.536185  [    1/   88]
per-ex loss: 0.773302  [    2/   88]
per-ex loss: 0.417480  [    3/   88]
per-ex loss: 0.588763  [    4/   88]
per-ex loss: 0.370501  [    5/   88]
per-ex loss: 0.387632  [    6/   88]
per-ex loss: 0.518202  [    7/   88]
per-ex loss: 0.338108  [    8/   88]
per-ex loss: 0.565342  [    9/   88]
per-ex loss: 0.414845  [   10/   88]
per-ex loss: 0.556960  [   11/   88]
per-ex loss: 0.336345  [   12/   88]
per-ex loss: 0.350087  [   13/   88]
per-ex loss: 0.391769  [   14/   88]
per-ex loss: 0.447160  [   15/   88]
per-ex loss: 0.533394  [   16/   88]
per-ex loss: 0.588253  [   17/   88]
per-ex loss: 0.354944  [   18/   88]
per-ex loss: 0.553451  [   19/   88]
per-ex loss: 0.473341  [   20/   88]
per-ex loss: 0.483501  [   21/   88]
per-ex loss: 0.382017  [   22/   88]
per-ex loss: 0.455253  [   23/   88]
per-ex loss: 0.381940  [   24/   88]
per-ex loss: 0.421959  [   25/   88]
per-ex loss: 0.287421  [   26/   88]
per-ex loss: 0.329494  [   27/   88]
per-ex loss: 0.641732  [   28/   88]
per-ex loss: 0.437512  [   29/   88]
per-ex loss: 0.411138  [   30/   88]
per-ex loss: 0.638382  [   31/   88]
per-ex loss: 0.653579  [   32/   88]
per-ex loss: 0.531869  [   33/   88]
per-ex loss: 0.425887  [   34/   88]
per-ex loss: 0.463309  [   35/   88]
per-ex loss: 0.390254  [   36/   88]
per-ex loss: 0.326461  [   37/   88]
per-ex loss: 0.663943  [   38/   88]
per-ex loss: 0.350638  [   39/   88]
per-ex loss: 0.628034  [   40/   88]
per-ex loss: 0.519928  [   41/   88]
per-ex loss: 0.629083  [   42/   88]
per-ex loss: 0.490297  [   43/   88]
per-ex loss: 0.700279  [   44/   88]
per-ex loss: 0.652682  [   45/   88]
per-ex loss: 0.380880  [   46/   88]
per-ex loss: 0.512621  [   47/   88]
per-ex loss: 0.457488  [   48/   88]
per-ex loss: 0.373699  [   49/   88]
per-ex loss: 0.603970  [   50/   88]
per-ex loss: 0.463370  [   51/   88]
per-ex loss: 0.548175  [   52/   88]
per-ex loss: 0.473710  [   53/   88]
per-ex loss: 0.671040  [   54/   88]
per-ex loss: 0.383982  [   55/   88]
per-ex loss: 0.386342  [   56/   88]
per-ex loss: 0.494801  [   57/   88]
per-ex loss: 0.423265  [   58/   88]
per-ex loss: 0.605845  [   59/   88]
per-ex loss: 0.549100  [   60/   88]
per-ex loss: 0.400047  [   61/   88]
per-ex loss: 0.392488  [   62/   88]
per-ex loss: 0.364780  [   63/   88]
per-ex loss: 0.450008  [   64/   88]
per-ex loss: 0.412274  [   65/   88]
per-ex loss: 0.401355  [   66/   88]
per-ex loss: 0.441537  [   67/   88]
per-ex loss: 0.415733  [   68/   88]
per-ex loss: 0.590449  [   69/   88]
per-ex loss: 0.565850  [   70/   88]
per-ex loss: 0.528069  [   71/   88]
per-ex loss: 0.629379  [   72/   88]
per-ex loss: 0.390182  [   73/   88]
per-ex loss: 0.425989  [   74/   88]
per-ex loss: 0.702773  [   75/   88]
per-ex loss: 0.521448  [   76/   88]
per-ex loss: 0.434118  [   77/   88]
per-ex loss: 0.528963  [   78/   88]
per-ex loss: 0.607840  [   79/   88]
per-ex loss: 0.471736  [   80/   88]
per-ex loss: 0.428793  [   81/   88]
per-ex loss: 0.591434  [   82/   88]
per-ex loss: 0.466079  [   83/   88]
per-ex loss: 0.694954  [   84/   88]
per-ex loss: 0.662289  [   85/   88]
per-ex loss: 0.564032  [   86/   88]
per-ex loss: 0.585430  [   87/   88]
per-ex loss: 0.351814  [   88/   88]
Train Error: Avg loss: 0.49023616
validation Error: 
 Avg loss: 0.52934342 
 F1: 0.501079 
 Precision: 0.528385 
 Recall: 0.476456
 IoU: 0.334293

test Error: 
 Avg loss: 0.49519912 
 F1: 0.555575 
 Precision: 0.554476 
 Recall: 0.556678
 IoU: 0.384634

We have finished training iteration 40
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_38_.pth
per-ex loss: 0.537725  [    1/   88]
per-ex loss: 0.471000  [    2/   88]
per-ex loss: 0.393482  [    3/   88]
per-ex loss: 0.368768  [    4/   88]
per-ex loss: 0.381075  [    5/   88]
per-ex loss: 0.547735  [    6/   88]
per-ex loss: 0.434854  [    7/   88]
per-ex loss: 0.376559  [    8/   88]
per-ex loss: 0.539719  [    9/   88]
per-ex loss: 0.568129  [   10/   88]
per-ex loss: 0.534431  [   11/   88]
per-ex loss: 0.381820  [   12/   88]
per-ex loss: 0.660957  [   13/   88]
per-ex loss: 0.408089  [   14/   88]
per-ex loss: 0.703375  [   15/   88]
per-ex loss: 0.556943  [   16/   88]
per-ex loss: 0.581597  [   17/   88]
per-ex loss: 0.482733  [   18/   88]
per-ex loss: 0.330612  [   19/   88]
per-ex loss: 0.375191  [   20/   88]
per-ex loss: 0.392888  [   21/   88]
per-ex loss: 0.659674  [   22/   88]
per-ex loss: 0.378083  [   23/   88]
per-ex loss: 0.674242  [   24/   88]
per-ex loss: 0.402918  [   25/   88]
per-ex loss: 0.409011  [   26/   88]
per-ex loss: 0.413688  [   27/   88]
per-ex loss: 0.573036  [   28/   88]
per-ex loss: 0.386938  [   29/   88]
per-ex loss: 0.690867  [   30/   88]
per-ex loss: 0.519264  [   31/   88]
per-ex loss: 0.373032  [   32/   88]
per-ex loss: 0.502543  [   33/   88]
per-ex loss: 0.608526  [   34/   88]
per-ex loss: 0.333188  [   35/   88]
per-ex loss: 0.396644  [   36/   88]
per-ex loss: 0.442442  [   37/   88]
per-ex loss: 0.567552  [   38/   88]
per-ex loss: 0.424582  [   39/   88]
per-ex loss: 0.423560  [   40/   88]
per-ex loss: 0.422671  [   41/   88]
per-ex loss: 0.449633  [   42/   88]
per-ex loss: 0.374929  [   43/   88]
per-ex loss: 0.597602  [   44/   88]
per-ex loss: 0.605137  [   45/   88]
per-ex loss: 0.360305  [   46/   88]
per-ex loss: 0.685742  [   47/   88]
per-ex loss: 0.641098  [   48/   88]
per-ex loss: 0.387506  [   49/   88]
per-ex loss: 0.343819  [   50/   88]
per-ex loss: 0.415267  [   51/   88]
per-ex loss: 0.409446  [   52/   88]
per-ex loss: 0.528813  [   53/   88]
per-ex loss: 0.612257  [   54/   88]
per-ex loss: 0.600337  [   55/   88]
per-ex loss: 0.362963  [   56/   88]
per-ex loss: 0.598183  [   57/   88]
per-ex loss: 0.551742  [   58/   88]
per-ex loss: 0.413925  [   59/   88]
per-ex loss: 0.608819  [   60/   88]
per-ex loss: 0.419962  [   61/   88]
per-ex loss: 0.600307  [   62/   88]
per-ex loss: 0.396583  [   63/   88]
per-ex loss: 0.552572  [   64/   88]
per-ex loss: 0.443557  [   65/   88]
per-ex loss: 0.456314  [   66/   88]
per-ex loss: 0.392423  [   67/   88]
per-ex loss: 0.420467  [   68/   88]
per-ex loss: 0.570501  [   69/   88]
per-ex loss: 0.579869  [   70/   88]
per-ex loss: 0.546409  [   71/   88]
per-ex loss: 0.622500  [   72/   88]
per-ex loss: 0.404518  [   73/   88]
per-ex loss: 0.507770  [   74/   88]
per-ex loss: 0.544803  [   75/   88]
per-ex loss: 0.462766  [   76/   88]
per-ex loss: 0.434588  [   77/   88]
per-ex loss: 0.602923  [   78/   88]
per-ex loss: 0.378209  [   79/   88]
per-ex loss: 0.581131  [   80/   88]
per-ex loss: 0.480619  [   81/   88]
per-ex loss: 0.378228  [   82/   88]
per-ex loss: 0.430311  [   83/   88]
per-ex loss: 0.397270  [   84/   88]
per-ex loss: 0.380589  [   85/   88]
per-ex loss: 0.613347  [   86/   88]
per-ex loss: 0.346456  [   87/   88]
per-ex loss: 0.635450  [   88/   88]
Train Error: Avg loss: 0.48588763
validation Error: 
 Avg loss: 0.53921707 
 F1: 0.485770 
 Precision: 0.603407 
 Recall: 0.406518
 IoU: 0.320803

test Error: 
 Avg loss: 0.50260685 
 F1: 0.547349 
 Precision: 0.642635 
 Recall: 0.476672
 IoU: 0.376794

We have finished training iteration 41
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_39_.pth
per-ex loss: 0.639659  [    1/   88]
per-ex loss: 0.445638  [    2/   88]
per-ex loss: 0.384074  [    3/   88]
per-ex loss: 0.578749  [    4/   88]
per-ex loss: 0.576236  [    5/   88]
per-ex loss: 0.600806  [    6/   88]
per-ex loss: 0.476839  [    7/   88]
per-ex loss: 0.560335  [    8/   88]
per-ex loss: 0.381083  [    9/   88]
per-ex loss: 0.293419  [   10/   88]
per-ex loss: 0.363889  [   11/   88]
per-ex loss: 0.439503  [   12/   88]
per-ex loss: 0.624277  [   13/   88]
per-ex loss: 0.397534  [   14/   88]
per-ex loss: 0.672994  [   15/   88]
per-ex loss: 0.311270  [   16/   88]
per-ex loss: 0.334571  [   17/   88]
per-ex loss: 0.519713  [   18/   88]
per-ex loss: 0.407305  [   19/   88]
per-ex loss: 0.544204  [   20/   88]
per-ex loss: 0.396965  [   21/   88]
per-ex loss: 0.435875  [   22/   88]
per-ex loss: 0.598233  [   23/   88]
per-ex loss: 0.680535  [   24/   88]
per-ex loss: 0.420509  [   25/   88]
per-ex loss: 0.353468  [   26/   88]
per-ex loss: 0.416428  [   27/   88]
per-ex loss: 0.371819  [   28/   88]
per-ex loss: 0.440755  [   29/   88]
per-ex loss: 0.617189  [   30/   88]
per-ex loss: 0.586995  [   31/   88]
per-ex loss: 0.442792  [   32/   88]
per-ex loss: 0.642138  [   33/   88]
per-ex loss: 0.489857  [   34/   88]
per-ex loss: 0.391516  [   35/   88]
per-ex loss: 0.720547  [   36/   88]
per-ex loss: 0.554091  [   37/   88]
per-ex loss: 0.378321  [   38/   88]
per-ex loss: 0.435795  [   39/   88]
per-ex loss: 0.354441  [   40/   88]
per-ex loss: 0.549159  [   41/   88]
per-ex loss: 0.399602  [   42/   88]
per-ex loss: 0.594787  [   43/   88]
per-ex loss: 0.444924  [   44/   88]
per-ex loss: 0.474584  [   45/   88]
per-ex loss: 0.527030  [   46/   88]
per-ex loss: 0.461222  [   47/   88]
per-ex loss: 0.419716  [   48/   88]
per-ex loss: 0.350462  [   49/   88]
per-ex loss: 0.568498  [   50/   88]
per-ex loss: 0.591101  [   51/   88]
per-ex loss: 0.492834  [   52/   88]
per-ex loss: 0.376780  [   53/   88]
per-ex loss: 0.555813  [   54/   88]
per-ex loss: 0.457794  [   55/   88]
per-ex loss: 0.699636  [   56/   88]
per-ex loss: 0.436180  [   57/   88]
per-ex loss: 0.462262  [   58/   88]
per-ex loss: 0.649600  [   59/   88]
per-ex loss: 0.370269  [   60/   88]
per-ex loss: 0.530809  [   61/   88]
per-ex loss: 0.349066  [   62/   88]
per-ex loss: 0.396769  [   63/   88]
per-ex loss: 0.374535  [   64/   88]
per-ex loss: 0.607283  [   65/   88]
per-ex loss: 0.675490  [   66/   88]
per-ex loss: 0.437522  [   67/   88]
per-ex loss: 0.547640  [   68/   88]
per-ex loss: 0.486297  [   69/   88]
per-ex loss: 0.535080  [   70/   88]
per-ex loss: 0.349890  [   71/   88]
per-ex loss: 0.635434  [   72/   88]
per-ex loss: 0.440179  [   73/   88]
per-ex loss: 0.396398  [   74/   88]
per-ex loss: 0.449917  [   75/   88]
per-ex loss: 0.408254  [   76/   88]
per-ex loss: 0.568952  [   77/   88]
per-ex loss: 0.628028  [   78/   88]
per-ex loss: 0.576603  [   79/   88]
per-ex loss: 0.519228  [   80/   88]
per-ex loss: 0.580771  [   81/   88]
per-ex loss: 0.418374  [   82/   88]
per-ex loss: 0.404204  [   83/   88]
per-ex loss: 0.411120  [   84/   88]
per-ex loss: 0.579232  [   85/   88]
per-ex loss: 0.613159  [   86/   88]
per-ex loss: 0.389607  [   87/   88]
per-ex loss: 0.405120  [   88/   88]
Train Error: Avg loss: 0.48724524
validation Error: 
 Avg loss: 0.55365806 
 F1: 0.474077 
 Precision: 0.595504 
 Recall: 0.393782
 IoU: 0.310682

test Error: 
 Avg loss: 0.50988637 
 F1: 0.536122 
 Precision: 0.663401 
 Recall: 0.449821
 IoU: 0.366234

We have finished training iteration 42
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_36_.pth
per-ex loss: 0.465566  [    1/   88]
per-ex loss: 0.424963  [    2/   88]
per-ex loss: 0.343853  [    3/   88]
per-ex loss: 0.534900  [    4/   88]
per-ex loss: 0.433254  [    5/   88]
per-ex loss: 0.403053  [    6/   88]
per-ex loss: 0.724669  [    7/   88]
per-ex loss: 0.375338  [    8/   88]
per-ex loss: 0.590918  [    9/   88]
per-ex loss: 0.466913  [   10/   88]
per-ex loss: 0.437707  [   11/   88]
per-ex loss: 0.534106  [   12/   88]
per-ex loss: 0.538430  [   13/   88]
per-ex loss: 0.324876  [   14/   88]
per-ex loss: 0.555792  [   15/   88]
per-ex loss: 0.561149  [   16/   88]
per-ex loss: 0.529033  [   17/   88]
per-ex loss: 0.380433  [   18/   88]
per-ex loss: 0.344584  [   19/   88]
per-ex loss: 0.594910  [   20/   88]
per-ex loss: 0.524426  [   21/   88]
per-ex loss: 0.422307  [   22/   88]
per-ex loss: 0.670264  [   23/   88]
per-ex loss: 0.443956  [   24/   88]
per-ex loss: 0.416470  [   25/   88]
per-ex loss: 0.421910  [   26/   88]
per-ex loss: 0.425404  [   27/   88]
per-ex loss: 0.541319  [   28/   88]
per-ex loss: 0.404645  [   29/   88]
per-ex loss: 0.543267  [   30/   88]
per-ex loss: 0.579256  [   31/   88]
per-ex loss: 0.339321  [   32/   88]
per-ex loss: 0.294669  [   33/   88]
per-ex loss: 0.441314  [   34/   88]
per-ex loss: 0.510427  [   35/   88]
per-ex loss: 0.377225  [   36/   88]
per-ex loss: 0.621171  [   37/   88]
per-ex loss: 0.556107  [   38/   88]
per-ex loss: 0.374352  [   39/   88]
per-ex loss: 0.403964  [   40/   88]
per-ex loss: 0.460203  [   41/   88]
per-ex loss: 0.499959  [   42/   88]
per-ex loss: 0.670277  [   43/   88]
per-ex loss: 0.668394  [   44/   88]
per-ex loss: 0.622491  [   45/   88]
per-ex loss: 0.375803  [   46/   88]
per-ex loss: 0.387474  [   47/   88]
per-ex loss: 0.593171  [   48/   88]
per-ex loss: 0.427319  [   49/   88]
per-ex loss: 0.566183  [   50/   88]
per-ex loss: 0.669391  [   51/   88]
per-ex loss: 0.582004  [   52/   88]
per-ex loss: 0.644593  [   53/   88]
per-ex loss: 0.535013  [   54/   88]
per-ex loss: 0.388378  [   55/   88]
per-ex loss: 0.417852  [   56/   88]
per-ex loss: 0.419224  [   57/   88]
per-ex loss: 0.520199  [   58/   88]
per-ex loss: 0.517289  [   59/   88]
per-ex loss: 0.642681  [   60/   88]
per-ex loss: 0.372205  [   61/   88]
per-ex loss: 0.580411  [   62/   88]
per-ex loss: 0.526208  [   63/   88]
per-ex loss: 0.329650  [   64/   88]
per-ex loss: 0.438065  [   65/   88]
per-ex loss: 0.443458  [   66/   88]
per-ex loss: 0.594602  [   67/   88]
per-ex loss: 0.369930  [   68/   88]
per-ex loss: 0.639002  [   69/   88]
per-ex loss: 0.348664  [   70/   88]
per-ex loss: 0.368362  [   71/   88]
per-ex loss: 0.424750  [   72/   88]
per-ex loss: 0.419802  [   73/   88]
per-ex loss: 0.600013  [   74/   88]
per-ex loss: 0.581087  [   75/   88]
per-ex loss: 0.600067  [   76/   88]
per-ex loss: 0.606869  [   77/   88]
per-ex loss: 0.389822  [   78/   88]
per-ex loss: 0.546197  [   79/   88]
per-ex loss: 0.391771  [   80/   88]
per-ex loss: 0.576902  [   81/   88]
per-ex loss: 0.422915  [   82/   88]
per-ex loss: 0.438986  [   83/   88]
per-ex loss: 0.564578  [   84/   88]
per-ex loss: 0.404759  [   85/   88]
per-ex loss: 0.359894  [   86/   88]
per-ex loss: 0.357168  [   87/   88]
per-ex loss: 0.386165  [   88/   88]
Train Error: Avg loss: 0.48370933
validation Error: 
 Avg loss: 0.55330893 
 F1: 0.472216 
 Precision: 0.678017 
 Recall: 0.362259
 IoU: 0.309086

test Error: 
 Avg loss: 0.51395511 
 F1: 0.532820 
 Precision: 0.695181 
 Recall: 0.431940
 IoU: 0.363160

We have finished training iteration 43
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_41_.pth
per-ex loss: 0.635768  [    1/   88]
per-ex loss: 0.411014  [    2/   88]
per-ex loss: 0.531071  [    3/   88]
per-ex loss: 0.597903  [    4/   88]
per-ex loss: 0.439670  [    5/   88]
per-ex loss: 0.695264  [    6/   88]
per-ex loss: 0.627209  [    7/   88]
per-ex loss: 0.544626  [    8/   88]
per-ex loss: 0.618566  [    9/   88]
per-ex loss: 0.595700  [   10/   88]
per-ex loss: 0.387791  [   11/   88]
per-ex loss: 0.349856  [   12/   88]
per-ex loss: 0.374646  [   13/   88]
per-ex loss: 0.590028  [   14/   88]
per-ex loss: 0.387743  [   15/   88]
per-ex loss: 0.451396  [   16/   88]
per-ex loss: 0.420899  [   17/   88]
per-ex loss: 0.417823  [   18/   88]
per-ex loss: 0.414206  [   19/   88]
per-ex loss: 0.398801  [   20/   88]
per-ex loss: 0.470968  [   21/   88]
per-ex loss: 0.576795  [   22/   88]
per-ex loss: 0.651857  [   23/   88]
per-ex loss: 0.329322  [   24/   88]
per-ex loss: 0.569520  [   25/   88]
per-ex loss: 0.540417  [   26/   88]
per-ex loss: 0.395168  [   27/   88]
per-ex loss: 0.398555  [   28/   88]
per-ex loss: 0.385559  [   29/   88]
per-ex loss: 0.536733  [   30/   88]
per-ex loss: 0.651248  [   31/   88]
per-ex loss: 0.519169  [   32/   88]
per-ex loss: 0.602724  [   33/   88]
per-ex loss: 0.537935  [   34/   88]
per-ex loss: 0.653684  [   35/   88]
per-ex loss: 0.528309  [   36/   88]
per-ex loss: 0.465992  [   37/   88]
per-ex loss: 0.533711  [   38/   88]
per-ex loss: 0.419947  [   39/   88]
per-ex loss: 0.468684  [   40/   88]
per-ex loss: 0.420601  [   41/   88]
per-ex loss: 0.428881  [   42/   88]
per-ex loss: 0.509316  [   43/   88]
per-ex loss: 0.584691  [   44/   88]
per-ex loss: 0.553997  [   45/   88]
per-ex loss: 0.403188  [   46/   88]
per-ex loss: 0.599268  [   47/   88]
per-ex loss: 0.439825  [   48/   88]
per-ex loss: 0.381645  [   49/   88]
per-ex loss: 0.438800  [   50/   88]
per-ex loss: 0.479691  [   51/   88]
per-ex loss: 0.369074  [   52/   88]
per-ex loss: 0.310692  [   53/   88]
per-ex loss: 0.668847  [   54/   88]
per-ex loss: 0.645718  [   55/   88]
per-ex loss: 0.406803  [   56/   88]
per-ex loss: 0.487212  [   57/   88]
per-ex loss: 0.342784  [   58/   88]
per-ex loss: 0.655727  [   59/   88]
per-ex loss: 0.441176  [   60/   88]
per-ex loss: 0.384254  [   61/   88]
per-ex loss: 0.408006  [   62/   88]
per-ex loss: 0.405089  [   63/   88]
per-ex loss: 0.579886  [   64/   88]
per-ex loss: 0.318899  [   65/   88]
per-ex loss: 0.383893  [   66/   88]
per-ex loss: 0.493069  [   67/   88]
per-ex loss: 0.429010  [   68/   88]
per-ex loss: 0.346258  [   69/   88]
per-ex loss: 0.343518  [   70/   88]
per-ex loss: 0.424294  [   71/   88]
per-ex loss: 0.333960  [   72/   88]
per-ex loss: 0.431438  [   73/   88]
per-ex loss: 0.728412  [   74/   88]
per-ex loss: 0.406716  [   75/   88]
per-ex loss: 0.556465  [   76/   88]
per-ex loss: 0.344940  [   77/   88]
per-ex loss: 0.674707  [   78/   88]
per-ex loss: 0.418678  [   79/   88]
per-ex loss: 0.569357  [   80/   88]
per-ex loss: 0.540906  [   81/   88]
per-ex loss: 0.594272  [   82/   88]
per-ex loss: 0.384603  [   83/   88]
per-ex loss: 0.417179  [   84/   88]
per-ex loss: 0.537601  [   85/   88]
per-ex loss: 0.434119  [   86/   88]
per-ex loss: 0.577089  [   87/   88]
per-ex loss: 0.653536  [   88/   88]
Train Error: Avg loss: 0.48652689
validation Error: 
 Avg loss: 0.53949311 
 F1: 0.489829 
 Precision: 0.521557 
 Recall: 0.461740
 IoU: 0.324353

test Error: 
 Avg loss: 0.48380725 
 F1: 0.564917 
 Precision: 0.608196 
 Recall: 0.527388
 IoU: 0.393647

We have finished training iteration 44
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_42_.pth
per-ex loss: 0.462372  [    1/   88]
per-ex loss: 0.623651  [    2/   88]
per-ex loss: 0.576687  [    3/   88]
per-ex loss: 0.531038  [    4/   88]
per-ex loss: 0.658243  [    5/   88]
per-ex loss: 0.554840  [    6/   88]
per-ex loss: 0.365915  [    7/   88]
per-ex loss: 0.558723  [    8/   88]
per-ex loss: 0.477617  [    9/   88]
per-ex loss: 0.712254  [   10/   88]
per-ex loss: 0.683169  [   11/   88]
per-ex loss: 0.596797  [   12/   88]
per-ex loss: 0.460870  [   13/   88]
per-ex loss: 0.536162  [   14/   88]
per-ex loss: 0.558204  [   15/   88]
per-ex loss: 0.396146  [   16/   88]
per-ex loss: 0.404542  [   17/   88]
per-ex loss: 0.349791  [   18/   88]
per-ex loss: 0.698891  [   19/   88]
per-ex loss: 0.347282  [   20/   88]
per-ex loss: 0.437893  [   21/   88]
per-ex loss: 0.378430  [   22/   88]
per-ex loss: 0.625231  [   23/   88]
per-ex loss: 0.350696  [   24/   88]
per-ex loss: 0.575867  [   25/   88]
per-ex loss: 0.388869  [   26/   88]
per-ex loss: 0.427431  [   27/   88]
per-ex loss: 0.399216  [   28/   88]
per-ex loss: 0.444869  [   29/   88]
per-ex loss: 0.566899  [   30/   88]
per-ex loss: 0.505354  [   31/   88]
per-ex loss: 0.524957  [   32/   88]
per-ex loss: 0.473477  [   33/   88]
per-ex loss: 0.548388  [   34/   88]
per-ex loss: 0.408193  [   35/   88]
per-ex loss: 0.341266  [   36/   88]
per-ex loss: 0.538915  [   37/   88]
per-ex loss: 0.547552  [   38/   88]
per-ex loss: 0.349227  [   39/   88]
per-ex loss: 0.584191  [   40/   88]
per-ex loss: 0.590900  [   41/   88]
per-ex loss: 0.391232  [   42/   88]
per-ex loss: 0.537083  [   43/   88]
per-ex loss: 0.435492  [   44/   88]
per-ex loss: 0.412867  [   45/   88]
per-ex loss: 0.474755  [   46/   88]
per-ex loss: 0.414958  [   47/   88]
per-ex loss: 0.568383  [   48/   88]
per-ex loss: 0.590863  [   49/   88]
per-ex loss: 0.340320  [   50/   88]
per-ex loss: 0.610219  [   51/   88]
per-ex loss: 0.554032  [   52/   88]
per-ex loss: 0.535382  [   53/   88]
per-ex loss: 0.432310  [   54/   88]
per-ex loss: 0.658624  [   55/   88]
per-ex loss: 0.421999  [   56/   88]
per-ex loss: 0.368311  [   57/   88]
per-ex loss: 0.453217  [   58/   88]
per-ex loss: 0.642181  [   59/   88]
per-ex loss: 0.503878  [   60/   88]
per-ex loss: 0.345847  [   61/   88]
per-ex loss: 0.537790  [   62/   88]
per-ex loss: 0.664654  [   63/   88]
per-ex loss: 0.444976  [   64/   88]
per-ex loss: 0.374200  [   65/   88]
per-ex loss: 0.482280  [   66/   88]
per-ex loss: 0.610391  [   67/   88]
per-ex loss: 0.401053  [   68/   88]
per-ex loss: 0.474588  [   69/   88]
per-ex loss: 0.591937  [   70/   88]
per-ex loss: 0.421929  [   71/   88]
per-ex loss: 0.679664  [   72/   88]
per-ex loss: 0.415659  [   73/   88]
per-ex loss: 0.422473  [   74/   88]
per-ex loss: 0.437146  [   75/   88]
per-ex loss: 0.330813  [   76/   88]
per-ex loss: 0.319632  [   77/   88]
per-ex loss: 0.439573  [   78/   88]
per-ex loss: 0.344240  [   79/   88]
per-ex loss: 0.396173  [   80/   88]
per-ex loss: 0.400222  [   81/   88]
per-ex loss: 0.696134  [   82/   88]
per-ex loss: 0.304478  [   83/   88]
per-ex loss: 0.570675  [   84/   88]
per-ex loss: 0.381146  [   85/   88]
per-ex loss: 0.391297  [   86/   88]
per-ex loss: 0.541124  [   87/   88]
per-ex loss: 0.375846  [   88/   88]
Train Error: Avg loss: 0.48526203
validation Error: 
 Avg loss: 0.53651715 
 F1: 0.491369 
 Precision: 0.609521 
 Recall: 0.411586
 IoU: 0.325705

test Error: 
 Avg loss: 0.50212181 
 F1: 0.550704 
 Precision: 0.640429 
 Recall: 0.483031
 IoU: 0.379981

We have finished training iteration 45
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_43_.pth
per-ex loss: 0.383326  [    1/   88]
per-ex loss: 0.413217  [    2/   88]
per-ex loss: 0.387559  [    3/   88]
per-ex loss: 0.670264  [    4/   88]
per-ex loss: 0.449038  [    5/   88]
per-ex loss: 0.594175  [    6/   88]
per-ex loss: 0.357708  [    7/   88]
per-ex loss: 0.364025  [    8/   88]
per-ex loss: 0.672564  [    9/   88]
per-ex loss: 0.629727  [   10/   88]
per-ex loss: 0.518776  [   11/   88]
per-ex loss: 0.537759  [   12/   88]
per-ex loss: 0.451015  [   13/   88]
per-ex loss: 0.512648  [   14/   88]
per-ex loss: 0.552869  [   15/   88]
per-ex loss: 0.405054  [   16/   88]
per-ex loss: 0.694874  [   17/   88]
per-ex loss: 0.567396  [   18/   88]
per-ex loss: 0.415026  [   19/   88]
per-ex loss: 0.544229  [   20/   88]
per-ex loss: 0.553800  [   21/   88]
per-ex loss: 0.359983  [   22/   88]
per-ex loss: 0.434320  [   23/   88]
per-ex loss: 0.420688  [   24/   88]
per-ex loss: 0.409672  [   25/   88]
per-ex loss: 0.422577  [   26/   88]
per-ex loss: 0.624104  [   27/   88]
per-ex loss: 0.504944  [   28/   88]
per-ex loss: 0.380817  [   29/   88]
per-ex loss: 0.587963  [   30/   88]
per-ex loss: 0.595991  [   31/   88]
per-ex loss: 0.661116  [   32/   88]
per-ex loss: 0.372018  [   33/   88]
per-ex loss: 0.615561  [   34/   88]
per-ex loss: 0.341835  [   35/   88]
per-ex loss: 0.365451  [   36/   88]
per-ex loss: 0.367968  [   37/   88]
per-ex loss: 0.444433  [   38/   88]
per-ex loss: 0.443459  [   39/   88]
per-ex loss: 0.489598  [   40/   88]
per-ex loss: 0.624713  [   41/   88]
per-ex loss: 0.351789  [   42/   88]
per-ex loss: 0.368785  [   43/   88]
per-ex loss: 0.539066  [   44/   88]
per-ex loss: 0.338737  [   45/   88]
per-ex loss: 0.550125  [   46/   88]
per-ex loss: 0.412343  [   47/   88]
per-ex loss: 0.502821  [   48/   88]
per-ex loss: 0.586243  [   49/   88]
per-ex loss: 0.593887  [   50/   88]
per-ex loss: 0.302968  [   51/   88]
per-ex loss: 0.618350  [   52/   88]
per-ex loss: 0.387616  [   53/   88]
per-ex loss: 0.451120  [   54/   88]
per-ex loss: 0.581259  [   55/   88]
per-ex loss: 0.408144  [   56/   88]
per-ex loss: 0.393820  [   57/   88]
per-ex loss: 0.463418  [   58/   88]
per-ex loss: 0.404219  [   59/   88]
per-ex loss: 0.405216  [   60/   88]
per-ex loss: 0.673198  [   61/   88]
per-ex loss: 0.688645  [   62/   88]
per-ex loss: 0.387681  [   63/   88]
per-ex loss: 0.419157  [   64/   88]
per-ex loss: 0.630317  [   65/   88]
per-ex loss: 0.384680  [   66/   88]
per-ex loss: 0.336225  [   67/   88]
per-ex loss: 0.558119  [   68/   88]
per-ex loss: 0.561346  [   69/   88]
per-ex loss: 0.602062  [   70/   88]
per-ex loss: 0.693958  [   71/   88]
per-ex loss: 0.476587  [   72/   88]
per-ex loss: 0.411375  [   73/   88]
per-ex loss: 0.410358  [   74/   88]
per-ex loss: 0.474603  [   75/   88]
per-ex loss: 0.435495  [   76/   88]
per-ex loss: 0.506000  [   77/   88]
per-ex loss: 0.348439  [   78/   88]
per-ex loss: 0.444332  [   79/   88]
per-ex loss: 0.382800  [   80/   88]
per-ex loss: 0.319155  [   81/   88]
per-ex loss: 0.578185  [   82/   88]
per-ex loss: 0.587753  [   83/   88]
per-ex loss: 0.547840  [   84/   88]
per-ex loss: 0.345232  [   85/   88]
per-ex loss: 0.524845  [   86/   88]
per-ex loss: 0.605163  [   87/   88]
per-ex loss: 0.396366  [   88/   88]
Train Error: Avg loss: 0.48331933
validation Error: 
 Avg loss: 0.53442906 
 F1: 0.492358 
 Precision: 0.592880 
 Recall: 0.420981
 IoU: 0.326575

test Error: 
 Avg loss: 0.49893540 
 F1: 0.549747 
 Precision: 0.627472 
 Recall: 0.489154
 IoU: 0.379069

We have finished training iteration 46
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_44_.pth
per-ex loss: 0.376645  [    1/   88]
per-ex loss: 0.388348  [    2/   88]
per-ex loss: 0.622037  [    3/   88]
per-ex loss: 0.669817  [    4/   88]
per-ex loss: 0.565205  [    5/   88]
per-ex loss: 0.648345  [    6/   88]
per-ex loss: 0.526410  [    7/   88]
per-ex loss: 0.535636  [    8/   88]
per-ex loss: 0.392727  [    9/   88]
per-ex loss: 0.653048  [   10/   88]
per-ex loss: 0.382099  [   11/   88]
per-ex loss: 0.549541  [   12/   88]
per-ex loss: 0.449613  [   13/   88]
per-ex loss: 0.359592  [   14/   88]
per-ex loss: 0.362610  [   15/   88]
per-ex loss: 0.571720  [   16/   88]
per-ex loss: 0.426298  [   17/   88]
per-ex loss: 0.444545  [   18/   88]
per-ex loss: 0.627054  [   19/   88]
per-ex loss: 0.507745  [   20/   88]
per-ex loss: 0.410180  [   21/   88]
per-ex loss: 0.525632  [   22/   88]
per-ex loss: 0.431355  [   23/   88]
per-ex loss: 0.401475  [   24/   88]
per-ex loss: 0.598135  [   25/   88]
per-ex loss: 0.381949  [   26/   88]
per-ex loss: 0.576583  [   27/   88]
per-ex loss: 0.441573  [   28/   88]
per-ex loss: 0.382118  [   29/   88]
per-ex loss: 0.423265  [   30/   88]
per-ex loss: 0.665274  [   31/   88]
per-ex loss: 0.458967  [   32/   88]
per-ex loss: 0.550712  [   33/   88]
per-ex loss: 0.407779  [   34/   88]
per-ex loss: 0.598509  [   35/   88]
per-ex loss: 0.439502  [   36/   88]
per-ex loss: 0.553983  [   37/   88]
per-ex loss: 0.465528  [   38/   88]
per-ex loss: 0.446567  [   39/   88]
per-ex loss: 0.625022  [   40/   88]
per-ex loss: 0.402023  [   41/   88]
per-ex loss: 0.541163  [   42/   88]
per-ex loss: 0.655304  [   43/   88]
per-ex loss: 0.473128  [   44/   88]
per-ex loss: 0.620546  [   45/   88]
per-ex loss: 0.482417  [   46/   88]
per-ex loss: 0.543126  [   47/   88]
per-ex loss: 0.585132  [   48/   88]
per-ex loss: 0.420245  [   49/   88]
per-ex loss: 0.426991  [   50/   88]
per-ex loss: 0.654785  [   51/   88]
per-ex loss: 0.429892  [   52/   88]
per-ex loss: 0.305454  [   53/   88]
per-ex loss: 0.680133  [   54/   88]
per-ex loss: 0.651801  [   55/   88]
per-ex loss: 0.604685  [   56/   88]
per-ex loss: 0.421121  [   57/   88]
per-ex loss: 0.618179  [   58/   88]
per-ex loss: 0.338552  [   59/   88]
per-ex loss: 0.384535  [   60/   88]
per-ex loss: 0.391201  [   61/   88]
per-ex loss: 0.594679  [   62/   88]
per-ex loss: 0.563454  [   63/   88]
per-ex loss: 0.337927  [   64/   88]
per-ex loss: 0.565115  [   65/   88]
per-ex loss: 0.324889  [   66/   88]
per-ex loss: 0.536103  [   67/   88]
per-ex loss: 0.559868  [   68/   88]
per-ex loss: 0.340595  [   69/   88]
per-ex loss: 0.381257  [   70/   88]
per-ex loss: 0.469946  [   71/   88]
per-ex loss: 0.350057  [   72/   88]
per-ex loss: 0.379346  [   73/   88]
per-ex loss: 0.548722  [   74/   88]
per-ex loss: 0.362832  [   75/   88]
per-ex loss: 0.516154  [   76/   88]
per-ex loss: 0.402465  [   77/   88]
per-ex loss: 0.342328  [   78/   88]
per-ex loss: 0.602438  [   79/   88]
per-ex loss: 0.387195  [   80/   88]
per-ex loss: 0.595227  [   81/   88]
per-ex loss: 0.466218  [   82/   88]
per-ex loss: 0.353037  [   83/   88]
per-ex loss: 0.391739  [   84/   88]
per-ex loss: 0.366935  [   85/   88]
per-ex loss: 0.503322  [   86/   88]
per-ex loss: 0.470624  [   87/   88]
per-ex loss: 0.389113  [   88/   88]
Train Error: Avg loss: 0.48380843
validation Error: 
 Avg loss: 0.53149653 
 F1: 0.495769 
 Precision: 0.596711 
 Recall: 0.424036
 IoU: 0.329583

test Error: 
 Avg loss: 0.49366598 
 F1: 0.556916 
 Precision: 0.644468 
 Recall: 0.490307
 IoU: 0.385920

We have finished training iteration 47
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_45_.pth
per-ex loss: 0.689419  [    1/   88]
per-ex loss: 0.337744  [    2/   88]
per-ex loss: 0.394952  [    3/   88]
per-ex loss: 0.466006  [    4/   88]
per-ex loss: 0.495733  [    5/   88]
per-ex loss: 0.434283  [    6/   88]
per-ex loss: 0.420866  [    7/   88]
per-ex loss: 0.507163  [    8/   88]
per-ex loss: 0.535810  [    9/   88]
per-ex loss: 0.555966  [   10/   88]
per-ex loss: 0.427653  [   11/   88]
per-ex loss: 0.343704  [   12/   88]
per-ex loss: 0.536858  [   13/   88]
per-ex loss: 0.487004  [   14/   88]
per-ex loss: 0.634241  [   15/   88]
per-ex loss: 0.427797  [   16/   88]
per-ex loss: 0.422246  [   17/   88]
per-ex loss: 0.473816  [   18/   88]
per-ex loss: 0.397238  [   19/   88]
per-ex loss: 0.409578  [   20/   88]
per-ex loss: 0.486248  [   21/   88]
per-ex loss: 0.374515  [   22/   88]
per-ex loss: 0.586708  [   23/   88]
per-ex loss: 0.361046  [   24/   88]
per-ex loss: 0.578181  [   25/   88]
per-ex loss: 0.409807  [   26/   88]
per-ex loss: 0.421564  [   27/   88]
per-ex loss: 0.547377  [   28/   88]
per-ex loss: 0.439166  [   29/   88]
per-ex loss: 0.640881  [   30/   88]
per-ex loss: 0.669932  [   31/   88]
per-ex loss: 0.549478  [   32/   88]
per-ex loss: 0.645418  [   33/   88]
per-ex loss: 0.491792  [   34/   88]
per-ex loss: 0.549667  [   35/   88]
per-ex loss: 0.529842  [   36/   88]
per-ex loss: 0.587249  [   37/   88]
per-ex loss: 0.391851  [   38/   88]
per-ex loss: 0.647076  [   39/   88]
per-ex loss: 0.620660  [   40/   88]
per-ex loss: 0.525167  [   41/   88]
per-ex loss: 0.431542  [   42/   88]
per-ex loss: 0.439628  [   43/   88]
per-ex loss: 0.375311  [   44/   88]
per-ex loss: 0.363790  [   45/   88]
per-ex loss: 0.670462  [   46/   88]
per-ex loss: 0.576089  [   47/   88]
per-ex loss: 0.547343  [   48/   88]
per-ex loss: 0.394477  [   49/   88]
per-ex loss: 0.364184  [   50/   88]
per-ex loss: 0.379469  [   51/   88]
per-ex loss: 0.511537  [   52/   88]
per-ex loss: 0.594937  [   53/   88]
per-ex loss: 0.345178  [   54/   88]
per-ex loss: 0.363566  [   55/   88]
per-ex loss: 0.574325  [   56/   88]
per-ex loss: 0.585385  [   57/   88]
per-ex loss: 0.307186  [   58/   88]
per-ex loss: 0.571967  [   59/   88]
per-ex loss: 0.474846  [   60/   88]
per-ex loss: 0.665823  [   61/   88]
per-ex loss: 0.373841  [   62/   88]
per-ex loss: 0.409983  [   63/   88]
per-ex loss: 0.555710  [   64/   88]
per-ex loss: 0.638096  [   65/   88]
per-ex loss: 0.571381  [   66/   88]
per-ex loss: 0.653285  [   67/   88]
per-ex loss: 0.622205  [   68/   88]
per-ex loss: 0.387965  [   69/   88]
per-ex loss: 0.567055  [   70/   88]
per-ex loss: 0.372254  [   71/   88]
per-ex loss: 0.340427  [   72/   88]
per-ex loss: 0.661671  [   73/   88]
per-ex loss: 0.413150  [   74/   88]
per-ex loss: 0.405210  [   75/   88]
per-ex loss: 0.300875  [   76/   88]
per-ex loss: 0.346284  [   77/   88]
per-ex loss: 0.431465  [   78/   88]
per-ex loss: 0.528790  [   79/   88]
per-ex loss: 0.444902  [   80/   88]
per-ex loss: 0.310429  [   81/   88]
per-ex loss: 0.610198  [   82/   88]
per-ex loss: 0.547426  [   83/   88]
per-ex loss: 0.381381  [   84/   88]
per-ex loss: 0.389428  [   85/   88]
per-ex loss: 0.354690  [   86/   88]
per-ex loss: 0.358949  [   87/   88]
per-ex loss: 0.352331  [   88/   88]
Train Error: Avg loss: 0.48088782
validation Error: 
 Avg loss: 0.53659330 
 F1: 0.492267 
 Precision: 0.612686 
 Recall: 0.411408
 IoU: 0.326495

test Error: 
 Avg loss: 0.50633296 
 F1: 0.546776 
 Precision: 0.663784 
 Recall: 0.464837
 IoU: 0.376250

We have finished training iteration 48
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_46_.pth
per-ex loss: 0.321549  [    1/   88]
per-ex loss: 0.378089  [    2/   88]
per-ex loss: 0.438331  [    3/   88]
per-ex loss: 0.548971  [    4/   88]
per-ex loss: 0.583277  [    5/   88]
per-ex loss: 0.591333  [    6/   88]
per-ex loss: 0.598777  [    7/   88]
per-ex loss: 0.470429  [    8/   88]
per-ex loss: 0.416391  [    9/   88]
per-ex loss: 0.552061  [   10/   88]
per-ex loss: 0.622441  [   11/   88]
per-ex loss: 0.326720  [   12/   88]
per-ex loss: 0.439452  [   13/   88]
per-ex loss: 0.406139  [   14/   88]
per-ex loss: 0.554954  [   15/   88]
per-ex loss: 0.393469  [   16/   88]
per-ex loss: 0.470996  [   17/   88]
per-ex loss: 0.412135  [   18/   88]
per-ex loss: 0.420360  [   19/   88]
per-ex loss: 0.383386  [   20/   88]
per-ex loss: 0.353021  [   21/   88]
per-ex loss: 0.585133  [   22/   88]
per-ex loss: 0.386519  [   23/   88]
per-ex loss: 0.538814  [   24/   88]
per-ex loss: 0.594771  [   25/   88]
per-ex loss: 0.384778  [   26/   88]
per-ex loss: 0.462541  [   27/   88]
per-ex loss: 0.700186  [   28/   88]
per-ex loss: 0.419582  [   29/   88]
per-ex loss: 0.435310  [   30/   88]
per-ex loss: 0.489765  [   31/   88]
per-ex loss: 0.590731  [   32/   88]
per-ex loss: 0.441645  [   33/   88]
per-ex loss: 0.631355  [   34/   88]
per-ex loss: 0.366635  [   35/   88]
per-ex loss: 0.484443  [   36/   88]
per-ex loss: 0.497660  [   37/   88]
per-ex loss: 0.474190  [   38/   88]
per-ex loss: 0.536789  [   39/   88]
per-ex loss: 0.372316  [   40/   88]
per-ex loss: 0.393999  [   41/   88]
per-ex loss: 0.539688  [   42/   88]
per-ex loss: 0.480385  [   43/   88]
per-ex loss: 0.474447  [   44/   88]
per-ex loss: 0.609999  [   45/   88]
per-ex loss: 0.421881  [   46/   88]
per-ex loss: 0.411338  [   47/   88]
per-ex loss: 0.369999  [   48/   88]
per-ex loss: 0.392172  [   49/   88]
per-ex loss: 0.403237  [   50/   88]
per-ex loss: 0.411550  [   51/   88]
per-ex loss: 0.680673  [   52/   88]
per-ex loss: 0.429957  [   53/   88]
per-ex loss: 0.360169  [   54/   88]
per-ex loss: 0.513897  [   55/   88]
per-ex loss: 0.529727  [   56/   88]
per-ex loss: 0.688631  [   57/   88]
per-ex loss: 0.604542  [   58/   88]
per-ex loss: 0.583993  [   59/   88]
per-ex loss: 0.264064  [   60/   88]
per-ex loss: 0.312578  [   61/   88]
per-ex loss: 0.561673  [   62/   88]
per-ex loss: 0.553955  [   63/   88]
per-ex loss: 0.597242  [   64/   88]
per-ex loss: 0.664096  [   65/   88]
per-ex loss: 0.624741  [   66/   88]
per-ex loss: 0.494245  [   67/   88]
per-ex loss: 0.395123  [   68/   88]
per-ex loss: 0.374072  [   69/   88]
per-ex loss: 0.562102  [   70/   88]
per-ex loss: 0.572499  [   71/   88]
per-ex loss: 0.363621  [   72/   88]
per-ex loss: 0.379044  [   73/   88]
per-ex loss: 0.340327  [   74/   88]
per-ex loss: 0.623409  [   75/   88]
per-ex loss: 0.615471  [   76/   88]
per-ex loss: 0.366571  [   77/   88]
per-ex loss: 0.632207  [   78/   88]
per-ex loss: 0.625608  [   79/   88]
per-ex loss: 0.404604  [   80/   88]
per-ex loss: 0.432176  [   81/   88]
per-ex loss: 0.412762  [   82/   88]
per-ex loss: 0.502180  [   83/   88]
per-ex loss: 0.460337  [   84/   88]
per-ex loss: 0.633862  [   85/   88]
per-ex loss: 0.374014  [   86/   88]
per-ex loss: 0.514956  [   87/   88]
per-ex loss: 0.448098  [   88/   88]
Train Error: Avg loss: 0.48274280
validation Error: 
 Avg loss: 0.53684872 
 F1: 0.486423 
 Precision: 0.633875 
 Recall: 0.394625
 IoU: 0.321373

test Error: 
 Avg loss: 0.50058683 
 F1: 0.550165 
 Precision: 0.656993 
 Recall: 0.473219
 IoU: 0.379467

We have finished training iteration 49
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_35_.pth
per-ex loss: 0.379401  [    1/   88]
per-ex loss: 0.425459  [    2/   88]
per-ex loss: 0.678118  [    3/   88]
per-ex loss: 0.618026  [    4/   88]
per-ex loss: 0.561074  [    5/   88]
per-ex loss: 0.368596  [    6/   88]
per-ex loss: 0.515428  [    7/   88]
per-ex loss: 0.500214  [    8/   88]
per-ex loss: 0.536814  [    9/   88]
per-ex loss: 0.408560  [   10/   88]
per-ex loss: 0.546829  [   11/   88]
per-ex loss: 0.371062  [   12/   88]
per-ex loss: 0.451445  [   13/   88]
per-ex loss: 0.372196  [   14/   88]
per-ex loss: 0.460550  [   15/   88]
per-ex loss: 0.564493  [   16/   88]
per-ex loss: 0.530630  [   17/   88]
per-ex loss: 0.561592  [   18/   88]
per-ex loss: 0.730835  [   19/   88]
per-ex loss: 0.621505  [   20/   88]
per-ex loss: 0.612141  [   21/   88]
per-ex loss: 0.385336  [   22/   88]
per-ex loss: 0.359710  [   23/   88]
per-ex loss: 0.378450  [   24/   88]
per-ex loss: 0.362973  [   25/   88]
per-ex loss: 0.612698  [   26/   88]
per-ex loss: 0.573380  [   27/   88]
per-ex loss: 0.354892  [   28/   88]
per-ex loss: 0.681726  [   29/   88]
per-ex loss: 0.606514  [   30/   88]
per-ex loss: 0.514394  [   31/   88]
per-ex loss: 0.504771  [   32/   88]
per-ex loss: 0.545948  [   33/   88]
per-ex loss: 0.537892  [   34/   88]
per-ex loss: 0.442773  [   35/   88]
per-ex loss: 0.397019  [   36/   88]
per-ex loss: 0.549989  [   37/   88]
per-ex loss: 0.347208  [   38/   88]
per-ex loss: 0.400587  [   39/   88]
per-ex loss: 0.360330  [   40/   88]
per-ex loss: 0.369897  [   41/   88]
per-ex loss: 0.334959  [   42/   88]
per-ex loss: 0.530148  [   43/   88]
per-ex loss: 0.401426  [   44/   88]
per-ex loss: 0.379992  [   45/   88]
per-ex loss: 0.586096  [   46/   88]
per-ex loss: 0.345704  [   47/   88]
per-ex loss: 0.357728  [   48/   88]
per-ex loss: 0.421558  [   49/   88]
per-ex loss: 0.382890  [   50/   88]
per-ex loss: 0.418627  [   51/   88]
per-ex loss: 0.394939  [   52/   88]
per-ex loss: 0.596397  [   53/   88]
per-ex loss: 0.367885  [   54/   88]
per-ex loss: 0.691330  [   55/   88]
per-ex loss: 0.380006  [   56/   88]
per-ex loss: 0.457246  [   57/   88]
per-ex loss: 0.608596  [   58/   88]
per-ex loss: 0.420258  [   59/   88]
per-ex loss: 0.407441  [   60/   88]
per-ex loss: 0.588161  [   61/   88]
per-ex loss: 0.587499  [   62/   88]
per-ex loss: 0.623558  [   63/   88]
per-ex loss: 0.379790  [   64/   88]
per-ex loss: 0.348620  [   65/   88]
per-ex loss: 0.448067  [   66/   88]
per-ex loss: 0.518554  [   67/   88]
per-ex loss: 0.482639  [   68/   88]
per-ex loss: 0.587901  [   69/   88]
per-ex loss: 0.516763  [   70/   88]
per-ex loss: 0.311212  [   71/   88]
per-ex loss: 0.367819  [   72/   88]
per-ex loss: 0.662493  [   73/   88]
per-ex loss: 0.646421  [   74/   88]
per-ex loss: 0.419859  [   75/   88]
per-ex loss: 0.523854  [   76/   88]
per-ex loss: 0.557811  [   77/   88]
per-ex loss: 0.404281  [   78/   88]
per-ex loss: 0.402891  [   79/   88]
per-ex loss: 0.446739  [   80/   88]
per-ex loss: 0.553640  [   81/   88]
per-ex loss: 0.433665  [   82/   88]
per-ex loss: 0.407153  [   83/   88]
per-ex loss: 0.414515  [   84/   88]
per-ex loss: 0.318334  [   85/   88]
per-ex loss: 0.431930  [   86/   88]
per-ex loss: 0.514881  [   87/   88]
per-ex loss: 0.690430  [   88/   88]
Train Error: Avg loss: 0.47925180
validation Error: 
 Avg loss: 0.53272886 
 F1: 0.493932 
 Precision: 0.585698 
 Recall: 0.427027
 IoU: 0.327962

test Error: 
 Avg loss: 0.50359417 
 F1: 0.547320 
 Precision: 0.618432 
 Recall: 0.490876
 IoU: 0.376766

We have finished training iteration 50
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_48_.pth
per-ex loss: 0.431748  [    1/   88]
per-ex loss: 0.595405  [    2/   88]
per-ex loss: 0.367465  [    3/   88]
per-ex loss: 0.563021  [    4/   88]
per-ex loss: 0.637970  [    5/   88]
per-ex loss: 0.469171  [    6/   88]
per-ex loss: 0.349844  [    7/   88]
per-ex loss: 0.334694  [    8/   88]
per-ex loss: 0.598568  [    9/   88]
per-ex loss: 0.394442  [   10/   88]
per-ex loss: 0.554689  [   11/   88]
per-ex loss: 0.408800  [   12/   88]
per-ex loss: 0.407930  [   13/   88]
per-ex loss: 0.403818  [   14/   88]
per-ex loss: 0.389902  [   15/   88]
per-ex loss: 0.473651  [   16/   88]
per-ex loss: 0.372669  [   17/   88]
per-ex loss: 0.439542  [   18/   88]
per-ex loss: 0.388780  [   19/   88]
per-ex loss: 0.613061  [   20/   88]
per-ex loss: 0.555683  [   21/   88]
per-ex loss: 0.576171  [   22/   88]
per-ex loss: 0.620048  [   23/   88]
per-ex loss: 0.490003  [   24/   88]
per-ex loss: 0.653579  [   25/   88]
per-ex loss: 0.545370  [   26/   88]
per-ex loss: 0.556972  [   27/   88]
per-ex loss: 0.535411  [   28/   88]
per-ex loss: 0.521525  [   29/   88]
per-ex loss: 0.471619  [   30/   88]
per-ex loss: 0.446326  [   31/   88]
per-ex loss: 0.346355  [   32/   88]
per-ex loss: 0.657261  [   33/   88]
per-ex loss: 0.547337  [   34/   88]
per-ex loss: 0.383960  [   35/   88]
per-ex loss: 0.388731  [   36/   88]
per-ex loss: 0.599500  [   37/   88]
per-ex loss: 0.585742  [   38/   88]
per-ex loss: 0.415231  [   39/   88]
per-ex loss: 0.392415  [   40/   88]
per-ex loss: 0.389828  [   41/   88]
per-ex loss: 0.598007  [   42/   88]
per-ex loss: 0.408392  [   43/   88]
per-ex loss: 0.375955  [   44/   88]
per-ex loss: 0.409580  [   45/   88]
per-ex loss: 0.580395  [   46/   88]
per-ex loss: 0.313193  [   47/   88]
per-ex loss: 0.348939  [   48/   88]
per-ex loss: 0.405235  [   49/   88]
per-ex loss: 0.581339  [   50/   88]
per-ex loss: 0.511587  [   51/   88]
per-ex loss: 0.439144  [   52/   88]
per-ex loss: 0.655194  [   53/   88]
per-ex loss: 0.568728  [   54/   88]
per-ex loss: 0.557556  [   55/   88]
per-ex loss: 0.509396  [   56/   88]
per-ex loss: 0.366841  [   57/   88]
per-ex loss: 0.360514  [   58/   88]
per-ex loss: 0.572193  [   59/   88]
per-ex loss: 0.452956  [   60/   88]
per-ex loss: 0.540496  [   61/   88]
per-ex loss: 0.560596  [   62/   88]
per-ex loss: 0.374322  [   63/   88]
per-ex loss: 0.404233  [   64/   88]
per-ex loss: 0.396842  [   65/   88]
per-ex loss: 0.695948  [   66/   88]
per-ex loss: 0.419592  [   67/   88]
per-ex loss: 0.663065  [   68/   88]
per-ex loss: 0.674325  [   69/   88]
per-ex loss: 0.474539  [   70/   88]
per-ex loss: 0.558622  [   71/   88]
per-ex loss: 0.432181  [   72/   88]
per-ex loss: 0.627691  [   73/   88]
per-ex loss: 0.463504  [   74/   88]
per-ex loss: 0.278659  [   75/   88]
per-ex loss: 0.344390  [   76/   88]
per-ex loss: 0.390425  [   77/   88]
per-ex loss: 0.394058  [   78/   88]
per-ex loss: 0.410049  [   79/   88]
per-ex loss: 0.313308  [   80/   88]
per-ex loss: 0.562070  [   81/   88]
per-ex loss: 0.344775  [   82/   88]
per-ex loss: 0.619902  [   83/   88]
per-ex loss: 0.452394  [   84/   88]
per-ex loss: 0.674827  [   85/   88]
per-ex loss: 0.634024  [   86/   88]
per-ex loss: 0.364376  [   87/   88]
per-ex loss: 0.504731  [   88/   88]
Train Error: Avg loss: 0.48253780
validation Error: 
 Avg loss: 0.54203184 
 F1: 0.486887 
 Precision: 0.490763 
 Recall: 0.483071
 IoU: 0.321778

test Error: 
 Avg loss: 0.50289053 
 F1: 0.544503 
 Precision: 0.545488 
 Recall: 0.543522
 IoU: 0.374101

We have finished training iteration 51
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_49_.pth
per-ex loss: 0.396418  [    1/   88]
per-ex loss: 0.361836  [    2/   88]
per-ex loss: 0.409170  [    3/   88]
per-ex loss: 0.394193  [    4/   88]
per-ex loss: 0.548078  [    5/   88]
per-ex loss: 0.414354  [    6/   88]
per-ex loss: 0.393252  [    7/   88]
per-ex loss: 0.574645  [    8/   88]
per-ex loss: 0.385795  [    9/   88]
per-ex loss: 0.325118  [   10/   88]
per-ex loss: 0.565356  [   11/   88]
per-ex loss: 0.336172  [   12/   88]
per-ex loss: 0.505397  [   13/   88]
per-ex loss: 0.521608  [   14/   88]
per-ex loss: 0.414218  [   15/   88]
per-ex loss: 0.519853  [   16/   88]
per-ex loss: 0.635475  [   17/   88]
per-ex loss: 0.426762  [   18/   88]
per-ex loss: 0.384087  [   19/   88]
per-ex loss: 0.592109  [   20/   88]
per-ex loss: 0.419078  [   21/   88]
per-ex loss: 0.585413  [   22/   88]
per-ex loss: 0.543455  [   23/   88]
per-ex loss: 0.592338  [   24/   88]
per-ex loss: 0.662692  [   25/   88]
per-ex loss: 0.628774  [   26/   88]
per-ex loss: 0.660745  [   27/   88]
per-ex loss: 0.562313  [   28/   88]
per-ex loss: 0.528558  [   29/   88]
per-ex loss: 0.387795  [   30/   88]
per-ex loss: 0.609517  [   31/   88]
per-ex loss: 0.599011  [   32/   88]
per-ex loss: 0.406737  [   33/   88]
per-ex loss: 0.372349  [   34/   88]
per-ex loss: 0.408848  [   35/   88]
per-ex loss: 0.429685  [   36/   88]
per-ex loss: 0.409408  [   37/   88]
per-ex loss: 0.509570  [   38/   88]
per-ex loss: 0.610245  [   39/   88]
per-ex loss: 0.425005  [   40/   88]
per-ex loss: 0.427696  [   41/   88]
per-ex loss: 0.512618  [   42/   88]
per-ex loss: 0.322921  [   43/   88]
per-ex loss: 0.419039  [   44/   88]
per-ex loss: 0.591052  [   45/   88]
per-ex loss: 0.457588  [   46/   88]
per-ex loss: 0.570629  [   47/   88]
per-ex loss: 0.636098  [   48/   88]
per-ex loss: 0.529209  [   49/   88]
per-ex loss: 0.346061  [   50/   88]
per-ex loss: 0.550042  [   51/   88]
per-ex loss: 0.686803  [   52/   88]
per-ex loss: 0.400571  [   53/   88]
per-ex loss: 0.378766  [   54/   88]
per-ex loss: 0.401492  [   55/   88]
per-ex loss: 0.385725  [   56/   88]
per-ex loss: 0.441380  [   57/   88]
per-ex loss: 0.457489  [   58/   88]
per-ex loss: 0.401759  [   59/   88]
per-ex loss: 0.448662  [   60/   88]
per-ex loss: 0.378173  [   61/   88]
per-ex loss: 0.531462  [   62/   88]
per-ex loss: 0.595666  [   63/   88]
per-ex loss: 0.624661  [   64/   88]
per-ex loss: 0.343801  [   65/   88]
per-ex loss: 0.608487  [   66/   88]
per-ex loss: 0.375689  [   67/   88]
per-ex loss: 0.348619  [   68/   88]
per-ex loss: 0.672724  [   69/   88]
per-ex loss: 0.566354  [   70/   88]
per-ex loss: 0.410402  [   71/   88]
per-ex loss: 0.436375  [   72/   88]
per-ex loss: 0.448695  [   73/   88]
per-ex loss: 0.680528  [   74/   88]
per-ex loss: 0.344735  [   75/   88]
per-ex loss: 0.413962  [   76/   88]
per-ex loss: 0.555121  [   77/   88]
per-ex loss: 0.441114  [   78/   88]
per-ex loss: 0.373398  [   79/   88]
per-ex loss: 0.420326  [   80/   88]
per-ex loss: 0.666498  [   81/   88]
per-ex loss: 0.591161  [   82/   88]
per-ex loss: 0.437587  [   83/   88]
per-ex loss: 0.548437  [   84/   88]
per-ex loss: 0.508463  [   85/   88]
per-ex loss: 0.510282  [   86/   88]
per-ex loss: 0.466858  [   87/   88]
per-ex loss: 0.358991  [   88/   88]
Train Error: Avg loss: 0.48272272
validation Error: 
 Avg loss: 0.53365827 
 F1: 0.498437 
 Precision: 0.517025 
 Recall: 0.481139
 IoU: 0.331945

test Error: 
 Avg loss: 0.48386993 
 F1: 0.567962 
 Precision: 0.575497 
 Recall: 0.560622
 IoU: 0.396611

We have finished training iteration 52
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_50_.pth
per-ex loss: 0.431327  [    1/   88]
per-ex loss: 0.387283  [    2/   88]
per-ex loss: 0.479318  [    3/   88]
per-ex loss: 0.344860  [    4/   88]
per-ex loss: 0.338183  [    5/   88]
per-ex loss: 0.653875  [    6/   88]
per-ex loss: 0.599308  [    7/   88]
per-ex loss: 0.518558  [    8/   88]
per-ex loss: 0.427166  [    9/   88]
per-ex loss: 0.539228  [   10/   88]
per-ex loss: 0.382581  [   11/   88]
per-ex loss: 0.419080  [   12/   88]
per-ex loss: 0.423612  [   13/   88]
per-ex loss: 0.391100  [   14/   88]
per-ex loss: 0.657573  [   15/   88]
per-ex loss: 0.374674  [   16/   88]
per-ex loss: 0.394833  [   17/   88]
per-ex loss: 0.572833  [   18/   88]
per-ex loss: 0.412876  [   19/   88]
per-ex loss: 0.563369  [   20/   88]
per-ex loss: 0.393489  [   21/   88]
per-ex loss: 0.627393  [   22/   88]
per-ex loss: 0.535201  [   23/   88]
per-ex loss: 0.349131  [   24/   88]
per-ex loss: 0.359847  [   25/   88]
per-ex loss: 0.415352  [   26/   88]
per-ex loss: 0.382494  [   27/   88]
per-ex loss: 0.413285  [   28/   88]
per-ex loss: 0.308478  [   29/   88]
per-ex loss: 0.550199  [   30/   88]
per-ex loss: 0.390129  [   31/   88]
per-ex loss: 0.268415  [   32/   88]
per-ex loss: 0.423227  [   33/   88]
per-ex loss: 0.400321  [   34/   88]
per-ex loss: 0.597018  [   35/   88]
per-ex loss: 0.528378  [   36/   88]
per-ex loss: 0.356171  [   37/   88]
per-ex loss: 0.389294  [   38/   88]
per-ex loss: 0.496071  [   39/   88]
per-ex loss: 0.389755  [   40/   88]
per-ex loss: 0.412814  [   41/   88]
per-ex loss: 0.441298  [   42/   88]
per-ex loss: 0.584690  [   43/   88]
per-ex loss: 0.663492  [   44/   88]
per-ex loss: 0.340695  [   45/   88]
per-ex loss: 0.351358  [   46/   88]
per-ex loss: 0.644254  [   47/   88]
per-ex loss: 0.372677  [   48/   88]
per-ex loss: 0.564056  [   49/   88]
per-ex loss: 0.394011  [   50/   88]
per-ex loss: 0.470952  [   51/   88]
per-ex loss: 0.540923  [   52/   88]
per-ex loss: 0.427287  [   53/   88]
per-ex loss: 0.507723  [   54/   88]
per-ex loss: 0.354154  [   55/   88]
per-ex loss: 0.614341  [   56/   88]
per-ex loss: 0.408149  [   57/   88]
per-ex loss: 0.332038  [   58/   88]
per-ex loss: 0.616665  [   59/   88]
per-ex loss: 0.402363  [   60/   88]
per-ex loss: 0.434847  [   61/   88]
per-ex loss: 0.511863  [   62/   88]
per-ex loss: 0.401294  [   63/   88]
per-ex loss: 0.681845  [   64/   88]
per-ex loss: 0.700051  [   65/   88]
per-ex loss: 0.532479  [   66/   88]
per-ex loss: 0.407770  [   67/   88]
per-ex loss: 0.545477  [   68/   88]
per-ex loss: 0.538948  [   69/   88]
per-ex loss: 0.587362  [   70/   88]
per-ex loss: 0.376469  [   71/   88]
per-ex loss: 0.676996  [   72/   88]
per-ex loss: 0.514906  [   73/   88]
per-ex loss: 0.585196  [   74/   88]
per-ex loss: 0.559276  [   75/   88]
per-ex loss: 0.581647  [   76/   88]
per-ex loss: 0.542483  [   77/   88]
per-ex loss: 0.564701  [   78/   88]
per-ex loss: 0.524000  [   79/   88]
per-ex loss: 0.396921  [   80/   88]
per-ex loss: 0.510986  [   81/   88]
per-ex loss: 0.371925  [   82/   88]
per-ex loss: 0.493927  [   83/   88]
per-ex loss: 0.396155  [   84/   88]
per-ex loss: 0.438348  [   85/   88]
per-ex loss: 0.676015  [   86/   88]
per-ex loss: 0.637061  [   87/   88]
per-ex loss: 0.532629  [   88/   88]
Train Error: Avg loss: 0.47785006
validation Error: 
 Avg loss: 0.54022585 
 F1: 0.487809 
 Precision: 0.588067 
 Recall: 0.416757
 IoU: 0.322584

test Error: 
 Avg loss: 0.50158782 
 F1: 0.548252 
 Precision: 0.605809 
 Recall: 0.500683
 IoU: 0.377650

We have finished training iteration 53
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_51_.pth
per-ex loss: 0.558986  [    1/   88]
per-ex loss: 0.534272  [    2/   88]
per-ex loss: 0.387592  [    3/   88]
per-ex loss: 0.585763  [    4/   88]
per-ex loss: 0.401154  [    5/   88]
per-ex loss: 0.721086  [    6/   88]
per-ex loss: 0.634826  [    7/   88]
per-ex loss: 0.406257  [    8/   88]
per-ex loss: 0.597996  [    9/   88]
per-ex loss: 0.384246  [   10/   88]
per-ex loss: 0.349721  [   11/   88]
per-ex loss: 0.418229  [   12/   88]
per-ex loss: 0.545788  [   13/   88]
per-ex loss: 0.432142  [   14/   88]
per-ex loss: 0.545915  [   15/   88]
per-ex loss: 0.454666  [   16/   88]
per-ex loss: 0.585197  [   17/   88]
per-ex loss: 0.361573  [   18/   88]
per-ex loss: 0.589339  [   19/   88]
per-ex loss: 0.526147  [   20/   88]
per-ex loss: 0.465702  [   21/   88]
per-ex loss: 0.434834  [   22/   88]
per-ex loss: 0.371675  [   23/   88]
per-ex loss: 0.568548  [   24/   88]
per-ex loss: 0.432685  [   25/   88]
per-ex loss: 0.423528  [   26/   88]
per-ex loss: 0.381916  [   27/   88]
per-ex loss: 0.303343  [   28/   88]
per-ex loss: 0.502459  [   29/   88]
per-ex loss: 0.546327  [   30/   88]
per-ex loss: 0.629098  [   31/   88]
per-ex loss: 0.486925  [   32/   88]
per-ex loss: 0.413863  [   33/   88]
per-ex loss: 0.490726  [   34/   88]
per-ex loss: 0.505988  [   35/   88]
per-ex loss: 0.407726  [   36/   88]
per-ex loss: 0.558223  [   37/   88]
per-ex loss: 0.342604  [   38/   88]
per-ex loss: 0.678044  [   39/   88]
per-ex loss: 0.321067  [   40/   88]
per-ex loss: 0.426286  [   41/   88]
per-ex loss: 0.357028  [   42/   88]
per-ex loss: 0.369120  [   43/   88]
per-ex loss: 0.369619  [   44/   88]
per-ex loss: 0.454767  [   45/   88]
per-ex loss: 0.559513  [   46/   88]
per-ex loss: 0.636286  [   47/   88]
per-ex loss: 0.664098  [   48/   88]
per-ex loss: 0.529865  [   49/   88]
per-ex loss: 0.518887  [   50/   88]
per-ex loss: 0.511488  [   51/   88]
per-ex loss: 0.348427  [   52/   88]
per-ex loss: 0.600443  [   53/   88]
per-ex loss: 0.656747  [   54/   88]
per-ex loss: 0.662418  [   55/   88]
per-ex loss: 0.378102  [   56/   88]
per-ex loss: 0.584964  [   57/   88]
per-ex loss: 0.366418  [   58/   88]
per-ex loss: 0.350228  [   59/   88]
per-ex loss: 0.367527  [   60/   88]
per-ex loss: 0.461908  [   61/   88]
per-ex loss: 0.391138  [   62/   88]
per-ex loss: 0.565765  [   63/   88]
per-ex loss: 0.589690  [   64/   88]
per-ex loss: 0.521749  [   65/   88]
per-ex loss: 0.354716  [   66/   88]
per-ex loss: 0.333605  [   67/   88]
per-ex loss: 0.606706  [   68/   88]
per-ex loss: 0.366883  [   69/   88]
per-ex loss: 0.647785  [   70/   88]
per-ex loss: 0.482810  [   71/   88]
per-ex loss: 0.607629  [   72/   88]
per-ex loss: 0.366354  [   73/   88]
per-ex loss: 0.578599  [   74/   88]
per-ex loss: 0.309328  [   75/   88]
per-ex loss: 0.569175  [   76/   88]
per-ex loss: 0.401643  [   77/   88]
per-ex loss: 0.438081  [   78/   88]
per-ex loss: 0.513181  [   79/   88]
per-ex loss: 0.384674  [   80/   88]
per-ex loss: 0.444696  [   81/   88]
per-ex loss: 0.464771  [   82/   88]
per-ex loss: 0.586018  [   83/   88]
per-ex loss: 0.418240  [   84/   88]
per-ex loss: 0.443022  [   85/   88]
per-ex loss: 0.391629  [   86/   88]
per-ex loss: 0.428365  [   87/   88]
per-ex loss: 0.592362  [   88/   88]
Train Error: Avg loss: 0.48021512
validation Error: 
 Avg loss: 0.52866561 
 F1: 0.495455 
 Precision: 0.626923 
 Recall: 0.409567
 IoU: 0.329305

test Error: 
 Avg loss: 0.48955236 
 F1: 0.561856 
 Precision: 0.649320 
 Recall: 0.495159
 IoU: 0.390682

We have finished training iteration 54
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_52_.pth
per-ex loss: 0.342860  [    1/   88]
per-ex loss: 0.399731  [    2/   88]
per-ex loss: 0.547714  [    3/   88]
per-ex loss: 0.354949  [    4/   88]
per-ex loss: 0.587424  [    5/   88]
per-ex loss: 0.442904  [    6/   88]
per-ex loss: 0.584215  [    7/   88]
per-ex loss: 0.594496  [    8/   88]
per-ex loss: 0.612055  [    9/   88]
per-ex loss: 0.350570  [   10/   88]
per-ex loss: 0.287460  [   11/   88]
per-ex loss: 0.374261  [   12/   88]
per-ex loss: 0.534333  [   13/   88]
per-ex loss: 0.589725  [   14/   88]
per-ex loss: 0.396211  [   15/   88]
per-ex loss: 0.628178  [   16/   88]
per-ex loss: 0.345185  [   17/   88]
per-ex loss: 0.557478  [   18/   88]
per-ex loss: 0.545555  [   19/   88]
per-ex loss: 0.540138  [   20/   88]
per-ex loss: 0.369602  [   21/   88]
per-ex loss: 0.361895  [   22/   88]
per-ex loss: 0.669051  [   23/   88]
per-ex loss: 0.615612  [   24/   88]
per-ex loss: 0.437224  [   25/   88]
per-ex loss: 0.663241  [   26/   88]
per-ex loss: 0.662607  [   27/   88]
per-ex loss: 0.487146  [   28/   88]
per-ex loss: 0.391573  [   29/   88]
per-ex loss: 0.593946  [   30/   88]
per-ex loss: 0.440648  [   31/   88]
per-ex loss: 0.455587  [   32/   88]
per-ex loss: 0.700870  [   33/   88]
per-ex loss: 0.576315  [   34/   88]
per-ex loss: 0.398740  [   35/   88]
per-ex loss: 0.446917  [   36/   88]
per-ex loss: 0.513385  [   37/   88]
per-ex loss: 0.437546  [   38/   88]
per-ex loss: 0.391640  [   39/   88]
per-ex loss: 0.398954  [   40/   88]
per-ex loss: 0.402266  [   41/   88]
per-ex loss: 0.381629  [   42/   88]
per-ex loss: 0.344643  [   43/   88]
per-ex loss: 0.488472  [   44/   88]
per-ex loss: 0.399573  [   45/   88]
per-ex loss: 0.618841  [   46/   88]
per-ex loss: 0.586922  [   47/   88]
per-ex loss: 0.577551  [   48/   88]
per-ex loss: 0.338840  [   49/   88]
per-ex loss: 0.387861  [   50/   88]
per-ex loss: 0.673969  [   51/   88]
per-ex loss: 0.333794  [   52/   88]
per-ex loss: 0.425117  [   53/   88]
per-ex loss: 0.435149  [   54/   88]
per-ex loss: 0.539742  [   55/   88]
per-ex loss: 0.434559  [   56/   88]
per-ex loss: 0.436834  [   57/   88]
per-ex loss: 0.451904  [   58/   88]
per-ex loss: 0.356607  [   59/   88]
per-ex loss: 0.499070  [   60/   88]
per-ex loss: 0.406473  [   61/   88]
per-ex loss: 0.326374  [   62/   88]
per-ex loss: 0.406322  [   63/   88]
per-ex loss: 0.612180  [   64/   88]
per-ex loss: 0.305402  [   65/   88]
per-ex loss: 0.627990  [   66/   88]
per-ex loss: 0.469354  [   67/   88]
per-ex loss: 0.576400  [   68/   88]
per-ex loss: 0.346661  [   69/   88]
per-ex loss: 0.418280  [   70/   88]
per-ex loss: 0.581415  [   71/   88]
per-ex loss: 0.531755  [   72/   88]
per-ex loss: 0.380763  [   73/   88]
per-ex loss: 0.492833  [   74/   88]
per-ex loss: 0.384475  [   75/   88]
per-ex loss: 0.504921  [   76/   88]
per-ex loss: 0.677792  [   77/   88]
per-ex loss: 0.403658  [   78/   88]
per-ex loss: 0.607032  [   79/   88]
per-ex loss: 0.455126  [   80/   88]
per-ex loss: 0.589136  [   81/   88]
per-ex loss: 0.524813  [   82/   88]
per-ex loss: 0.362242  [   83/   88]
per-ex loss: 0.384327  [   84/   88]
per-ex loss: 0.532885  [   85/   88]
per-ex loss: 0.403206  [   86/   88]
per-ex loss: 0.527650  [   87/   88]
per-ex loss: 0.552613  [   88/   88]
Train Error: Avg loss: 0.47878821
validation Error: 
 Avg loss: 0.52596398 
 F1: 0.504329 
 Precision: 0.592799 
 Recall: 0.438836
 IoU: 0.337192

test Error: 
 Avg loss: 0.48502125 
 F1: 0.565731 
 Precision: 0.637146 
 Recall: 0.508711
 IoU: 0.394438

We have finished training iteration 55
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_53_.pth
per-ex loss: 0.435768  [    1/   88]
per-ex loss: 0.553515  [    2/   88]
per-ex loss: 0.394325  [    3/   88]
per-ex loss: 0.465281  [    4/   88]
per-ex loss: 0.522858  [    5/   88]
per-ex loss: 0.372944  [    6/   88]
per-ex loss: 0.590858  [    7/   88]
per-ex loss: 0.381103  [    8/   88]
per-ex loss: 0.653606  [    9/   88]
per-ex loss: 0.424184  [   10/   88]
per-ex loss: 0.593876  [   11/   88]
per-ex loss: 0.412375  [   12/   88]
per-ex loss: 0.383299  [   13/   88]
per-ex loss: 0.340362  [   14/   88]
per-ex loss: 0.371962  [   15/   88]
per-ex loss: 0.615398  [   16/   88]
per-ex loss: 0.536631  [   17/   88]
per-ex loss: 0.572772  [   18/   88]
per-ex loss: 0.405744  [   19/   88]
per-ex loss: 0.337746  [   20/   88]
per-ex loss: 0.505903  [   21/   88]
per-ex loss: 0.433630  [   22/   88]
per-ex loss: 0.594214  [   23/   88]
per-ex loss: 0.471576  [   24/   88]
per-ex loss: 0.605744  [   25/   88]
per-ex loss: 0.418218  [   26/   88]
per-ex loss: 0.353248  [   27/   88]
per-ex loss: 0.314847  [   28/   88]
per-ex loss: 0.381202  [   29/   88]
per-ex loss: 0.340623  [   30/   88]
per-ex loss: 0.397361  [   31/   88]
per-ex loss: 0.684236  [   32/   88]
per-ex loss: 0.343381  [   33/   88]
per-ex loss: 0.373514  [   34/   88]
per-ex loss: 0.433463  [   35/   88]
per-ex loss: 0.413908  [   36/   88]
per-ex loss: 0.434461  [   37/   88]
per-ex loss: 0.390700  [   38/   88]
per-ex loss: 0.514792  [   39/   88]
per-ex loss: 0.498860  [   40/   88]
per-ex loss: 0.369708  [   41/   88]
per-ex loss: 0.568980  [   42/   88]
per-ex loss: 0.365835  [   43/   88]
per-ex loss: 0.439322  [   44/   88]
per-ex loss: 0.525229  [   45/   88]
per-ex loss: 0.431982  [   46/   88]
per-ex loss: 0.463114  [   47/   88]
per-ex loss: 0.443882  [   48/   88]
per-ex loss: 0.565300  [   49/   88]
per-ex loss: 0.429865  [   50/   88]
per-ex loss: 0.582106  [   51/   88]
per-ex loss: 0.295802  [   52/   88]
per-ex loss: 0.540810  [   53/   88]
per-ex loss: 0.371044  [   54/   88]
per-ex loss: 0.667738  [   55/   88]
per-ex loss: 0.577559  [   56/   88]
per-ex loss: 0.398249  [   57/   88]
per-ex loss: 0.555363  [   58/   88]
per-ex loss: 0.380319  [   59/   88]
per-ex loss: 0.666586  [   60/   88]
per-ex loss: 0.366299  [   61/   88]
per-ex loss: 0.498345  [   62/   88]
per-ex loss: 0.570387  [   63/   88]
per-ex loss: 0.330493  [   64/   88]
per-ex loss: 0.635206  [   65/   88]
per-ex loss: 0.553556  [   66/   88]
per-ex loss: 0.406477  [   67/   88]
per-ex loss: 0.394383  [   68/   88]
per-ex loss: 0.625164  [   69/   88]
per-ex loss: 0.610051  [   70/   88]
per-ex loss: 0.512129  [   71/   88]
per-ex loss: 0.559329  [   72/   88]
per-ex loss: 0.388040  [   73/   88]
per-ex loss: 0.585551  [   74/   88]
per-ex loss: 0.591230  [   75/   88]
per-ex loss: 0.621746  [   76/   88]
per-ex loss: 0.405064  [   77/   88]
per-ex loss: 0.391408  [   78/   88]
per-ex loss: 0.605844  [   79/   88]
per-ex loss: 0.416675  [   80/   88]
per-ex loss: 0.507291  [   81/   88]
per-ex loss: 0.499513  [   82/   88]
per-ex loss: 0.417189  [   83/   88]
per-ex loss: 0.359077  [   84/   88]
per-ex loss: 0.570440  [   85/   88]
per-ex loss: 0.394262  [   86/   88]
per-ex loss: 0.675212  [   87/   88]
per-ex loss: 0.373477  [   88/   88]
Train Error: Avg loss: 0.47467179
validation Error: 
 Avg loss: 0.52705517 
 F1: 0.496273 
 Precision: 0.577143 
 Recall: 0.435280
 IoU: 0.330028

test Error: 
 Avg loss: 0.48927705 
 F1: 0.556679 
 Precision: 0.610314 
 Recall: 0.511709
 IoU: 0.385693

We have finished training iteration 56
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_47_.pth
per-ex loss: 0.436405  [    1/   88]
per-ex loss: 0.382891  [    2/   88]
per-ex loss: 0.537161  [    3/   88]
per-ex loss: 0.531149  [    4/   88]
per-ex loss: 0.343276  [    5/   88]
per-ex loss: 0.545480  [    6/   88]
per-ex loss: 0.425614  [    7/   88]
per-ex loss: 0.576686  [    8/   88]
per-ex loss: 0.499316  [    9/   88]
per-ex loss: 0.360552  [   10/   88]
per-ex loss: 0.339214  [   11/   88]
per-ex loss: 0.449134  [   12/   88]
per-ex loss: 0.547254  [   13/   88]
per-ex loss: 0.500844  [   14/   88]
per-ex loss: 0.581955  [   15/   88]
per-ex loss: 0.351742  [   16/   88]
per-ex loss: 0.340303  [   17/   88]
per-ex loss: 0.410037  [   18/   88]
per-ex loss: 0.401294  [   19/   88]
per-ex loss: 0.575757  [   20/   88]
per-ex loss: 0.278667  [   21/   88]
per-ex loss: 0.589419  [   22/   88]
per-ex loss: 0.586082  [   23/   88]
per-ex loss: 0.327061  [   24/   88]
per-ex loss: 0.557761  [   25/   88]
per-ex loss: 0.652183  [   26/   88]
per-ex loss: 0.539108  [   27/   88]
per-ex loss: 0.420413  [   28/   88]
per-ex loss: 0.395292  [   29/   88]
per-ex loss: 0.573033  [   30/   88]
per-ex loss: 0.352204  [   31/   88]
per-ex loss: 0.388055  [   32/   88]
per-ex loss: 0.537044  [   33/   88]
per-ex loss: 0.693638  [   34/   88]
per-ex loss: 0.678107  [   35/   88]
per-ex loss: 0.425954  [   36/   88]
per-ex loss: 0.425714  [   37/   88]
per-ex loss: 0.340643  [   38/   88]
per-ex loss: 0.627680  [   39/   88]
per-ex loss: 0.403284  [   40/   88]
per-ex loss: 0.526180  [   41/   88]
per-ex loss: 0.382423  [   42/   88]
per-ex loss: 0.404000  [   43/   88]
per-ex loss: 0.379324  [   44/   88]
per-ex loss: 0.556123  [   45/   88]
per-ex loss: 0.545852  [   46/   88]
per-ex loss: 0.387382  [   47/   88]
per-ex loss: 0.423129  [   48/   88]
per-ex loss: 0.383262  [   49/   88]
per-ex loss: 0.542353  [   50/   88]
per-ex loss: 0.522171  [   51/   88]
per-ex loss: 0.362926  [   52/   88]
per-ex loss: 0.563453  [   53/   88]
per-ex loss: 0.644609  [   54/   88]
per-ex loss: 0.511846  [   55/   88]
per-ex loss: 0.583100  [   56/   88]
per-ex loss: 0.409892  [   57/   88]
per-ex loss: 0.398781  [   58/   88]
per-ex loss: 0.540618  [   59/   88]
per-ex loss: 0.448135  [   60/   88]
per-ex loss: 0.570287  [   61/   88]
per-ex loss: 0.383864  [   62/   88]
per-ex loss: 0.500379  [   63/   88]
per-ex loss: 0.399908  [   64/   88]
per-ex loss: 0.464731  [   65/   88]
per-ex loss: 0.538672  [   66/   88]
per-ex loss: 0.635933  [   67/   88]
per-ex loss: 0.593192  [   68/   88]
per-ex loss: 0.406177  [   69/   88]
per-ex loss: 0.598029  [   70/   88]
per-ex loss: 0.498786  [   71/   88]
per-ex loss: 0.352171  [   72/   88]
per-ex loss: 0.405548  [   73/   88]
per-ex loss: 0.425311  [   74/   88]
per-ex loss: 0.315518  [   75/   88]
per-ex loss: 0.365680  [   76/   88]
per-ex loss: 0.378326  [   77/   88]
per-ex loss: 0.346550  [   78/   88]
per-ex loss: 0.671951  [   79/   88]
per-ex loss: 0.633821  [   80/   88]
per-ex loss: 0.662287  [   81/   88]
per-ex loss: 0.495157  [   82/   88]
per-ex loss: 0.426607  [   83/   88]
per-ex loss: 0.560114  [   84/   88]
per-ex loss: 0.486110  [   85/   88]
per-ex loss: 0.438534  [   86/   88]
per-ex loss: 0.348645  [   87/   88]
per-ex loss: 0.476363  [   88/   88]
Train Error: Avg loss: 0.47524562
validation Error: 
 Avg loss: 0.52315740 
 F1: 0.503705 
 Precision: 0.571309 
 Recall: 0.450408
 IoU: 0.336635

test Error: 
 Avg loss: 0.48231218 
 F1: 0.566191 
 Precision: 0.607365 
 Recall: 0.530245
 IoU: 0.394886

We have finished training iteration 57
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_40_.pth
per-ex loss: 0.344294  [    1/   88]
per-ex loss: 0.637079  [    2/   88]
per-ex loss: 0.383298  [    3/   88]
per-ex loss: 0.310031  [    4/   88]
per-ex loss: 0.355412  [    5/   88]
per-ex loss: 0.670801  [    6/   88]
per-ex loss: 0.344432  [    7/   88]
per-ex loss: 0.474620  [    8/   88]
per-ex loss: 0.426090  [    9/   88]
per-ex loss: 0.646240  [   10/   88]
per-ex loss: 0.348443  [   11/   88]
per-ex loss: 0.373152  [   12/   88]
per-ex loss: 0.433775  [   13/   88]
per-ex loss: 0.509736  [   14/   88]
per-ex loss: 0.549577  [   15/   88]
per-ex loss: 0.393767  [   16/   88]
per-ex loss: 0.621266  [   17/   88]
per-ex loss: 0.568211  [   18/   88]
per-ex loss: 0.403600  [   19/   88]
per-ex loss: 0.550185  [   20/   88]
per-ex loss: 0.373173  [   21/   88]
per-ex loss: 0.407754  [   22/   88]
per-ex loss: 0.377599  [   23/   88]
per-ex loss: 0.412225  [   24/   88]
per-ex loss: 0.394311  [   25/   88]
per-ex loss: 0.341243  [   26/   88]
per-ex loss: 0.563334  [   27/   88]
per-ex loss: 0.508760  [   28/   88]
per-ex loss: 0.654258  [   29/   88]
per-ex loss: 0.451700  [   30/   88]
per-ex loss: 0.546218  [   31/   88]
per-ex loss: 0.574818  [   32/   88]
per-ex loss: 0.332830  [   33/   88]
per-ex loss: 0.345906  [   34/   88]
per-ex loss: 0.670750  [   35/   88]
per-ex loss: 0.609931  [   36/   88]
per-ex loss: 0.456409  [   37/   88]
per-ex loss: 0.410526  [   38/   88]
per-ex loss: 0.588602  [   39/   88]
per-ex loss: 0.415493  [   40/   88]
per-ex loss: 0.426800  [   41/   88]
per-ex loss: 0.409137  [   42/   88]
per-ex loss: 0.482658  [   43/   88]
per-ex loss: 0.390589  [   44/   88]
per-ex loss: 0.489656  [   45/   88]
per-ex loss: 0.374674  [   46/   88]
per-ex loss: 0.311827  [   47/   88]
per-ex loss: 0.431609  [   48/   88]
per-ex loss: 0.587781  [   49/   88]
per-ex loss: 0.555027  [   50/   88]
per-ex loss: 0.332831  [   51/   88]
per-ex loss: 0.483895  [   52/   88]
per-ex loss: 0.358082  [   53/   88]
per-ex loss: 0.308107  [   54/   88]
per-ex loss: 0.377515  [   55/   88]
per-ex loss: 0.374330  [   56/   88]
per-ex loss: 0.597725  [   57/   88]
per-ex loss: 0.362569  [   58/   88]
per-ex loss: 0.401774  [   59/   88]
per-ex loss: 0.329269  [   60/   88]
per-ex loss: 0.380060  [   61/   88]
per-ex loss: 0.501362  [   62/   88]
per-ex loss: 0.424680  [   63/   88]
per-ex loss: 0.589707  [   64/   88]
per-ex loss: 0.438967  [   65/   88]
per-ex loss: 0.679838  [   66/   88]
per-ex loss: 0.590501  [   67/   88]
per-ex loss: 0.573952  [   68/   88]
per-ex loss: 0.507334  [   69/   88]
per-ex loss: 0.574740  [   70/   88]
per-ex loss: 0.460328  [   71/   88]
per-ex loss: 0.641265  [   72/   88]
per-ex loss: 0.584058  [   73/   88]
per-ex loss: 0.529630  [   74/   88]
per-ex loss: 0.538814  [   75/   88]
per-ex loss: 0.418264  [   76/   88]
per-ex loss: 0.455754  [   77/   88]
per-ex loss: 0.433594  [   78/   88]
per-ex loss: 0.665694  [   79/   88]
per-ex loss: 0.544266  [   80/   88]
per-ex loss: 0.466004  [   81/   88]
per-ex loss: 0.543117  [   82/   88]
per-ex loss: 0.531346  [   83/   88]
per-ex loss: 0.363454  [   84/   88]
per-ex loss: 0.610566  [   85/   88]
per-ex loss: 0.525062  [   86/   88]
per-ex loss: 0.338121  [   87/   88]
per-ex loss: 0.560746  [   88/   88]
Train Error: Avg loss: 0.47314692
validation Error: 
 Avg loss: 0.52788239 
 F1: 0.500414 
 Precision: 0.535013 
 Recall: 0.470018
 IoU: 0.333702

test Error: 
 Avg loss: 0.48840368 
 F1: 0.562501 
 Precision: 0.564932 
 Recall: 0.560092
 IoU: 0.391306

We have finished training iteration 58
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_54_.pth
per-ex loss: 0.540164  [    1/   88]
per-ex loss: 0.496717  [    2/   88]
per-ex loss: 0.539872  [    3/   88]
per-ex loss: 0.634508  [    4/   88]
per-ex loss: 0.571064  [    5/   88]
per-ex loss: 0.392027  [    6/   88]
per-ex loss: 0.523272  [    7/   88]
per-ex loss: 0.372097  [    8/   88]
per-ex loss: 0.587264  [    9/   88]
per-ex loss: 0.365334  [   10/   88]
per-ex loss: 0.393847  [   11/   88]
per-ex loss: 0.532627  [   12/   88]
per-ex loss: 0.544868  [   13/   88]
per-ex loss: 0.353592  [   14/   88]
per-ex loss: 0.604740  [   15/   88]
per-ex loss: 0.305939  [   16/   88]
per-ex loss: 0.625285  [   17/   88]
per-ex loss: 0.396369  [   18/   88]
per-ex loss: 0.483001  [   19/   88]
per-ex loss: 0.470483  [   20/   88]
per-ex loss: 0.485348  [   21/   88]
per-ex loss: 0.362347  [   22/   88]
per-ex loss: 0.344945  [   23/   88]
per-ex loss: 0.621639  [   24/   88]
per-ex loss: 0.632709  [   25/   88]
per-ex loss: 0.565082  [   26/   88]
per-ex loss: 0.647633  [   27/   88]
per-ex loss: 0.382048  [   28/   88]
per-ex loss: 0.364117  [   29/   88]
per-ex loss: 0.550818  [   30/   88]
per-ex loss: 0.531768  [   31/   88]
per-ex loss: 0.415513  [   32/   88]
per-ex loss: 0.343969  [   33/   88]
per-ex loss: 0.594155  [   34/   88]
per-ex loss: 0.391583  [   35/   88]
per-ex loss: 0.380510  [   36/   88]
per-ex loss: 0.421479  [   37/   88]
per-ex loss: 0.352147  [   38/   88]
per-ex loss: 0.345005  [   39/   88]
per-ex loss: 0.389758  [   40/   88]
per-ex loss: 0.371854  [   41/   88]
per-ex loss: 0.416299  [   42/   88]
per-ex loss: 0.535087  [   43/   88]
per-ex loss: 0.576694  [   44/   88]
per-ex loss: 0.585055  [   45/   88]
per-ex loss: 0.388282  [   46/   88]
per-ex loss: 0.413614  [   47/   88]
per-ex loss: 0.455485  [   48/   88]
per-ex loss: 0.516334  [   49/   88]
per-ex loss: 0.416878  [   50/   88]
per-ex loss: 0.360618  [   51/   88]
per-ex loss: 0.566661  [   52/   88]
per-ex loss: 0.413872  [   53/   88]
per-ex loss: 0.487178  [   54/   88]
per-ex loss: 0.355154  [   55/   88]
per-ex loss: 0.464761  [   56/   88]
per-ex loss: 0.416415  [   57/   88]
per-ex loss: 0.686650  [   58/   88]
per-ex loss: 0.529316  [   59/   88]
per-ex loss: 0.464122  [   60/   88]
per-ex loss: 0.399807  [   61/   88]
per-ex loss: 0.376238  [   62/   88]
per-ex loss: 0.545480  [   63/   88]
per-ex loss: 0.601951  [   64/   88]
per-ex loss: 0.368822  [   65/   88]
per-ex loss: 0.617124  [   66/   88]
per-ex loss: 0.563014  [   67/   88]
per-ex loss: 0.442225  [   68/   88]
per-ex loss: 0.334386  [   69/   88]
per-ex loss: 0.444317  [   70/   88]
per-ex loss: 0.420630  [   71/   88]
per-ex loss: 0.450814  [   72/   88]
per-ex loss: 0.385528  [   73/   88]
per-ex loss: 0.330612  [   74/   88]
per-ex loss: 0.402229  [   75/   88]
per-ex loss: 0.425343  [   76/   88]
per-ex loss: 0.340966  [   77/   88]
per-ex loss: 0.668297  [   78/   88]
per-ex loss: 0.587515  [   79/   88]
per-ex loss: 0.498652  [   80/   88]
per-ex loss: 0.350321  [   81/   88]
per-ex loss: 0.602272  [   82/   88]
per-ex loss: 0.538645  [   83/   88]
per-ex loss: 0.298972  [   84/   88]
per-ex loss: 0.668997  [   85/   88]
per-ex loss: 0.570451  [   86/   88]
per-ex loss: 0.619122  [   87/   88]
per-ex loss: 0.345307  [   88/   88]
Train Error: Avg loss: 0.47129557
validation Error: 
 Avg loss: 0.54780337 
 F1: 0.475884 
 Precision: 0.477449 
 Recall: 0.474329
 IoU: 0.312236

test Error: 
 Avg loss: 0.49978451 
 F1: 0.544323 
 Precision: 0.541106 
 Recall: 0.547578
 IoU: 0.373931

We have finished training iteration 59
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_56_.pth
per-ex loss: 0.656871  [    1/   88]
per-ex loss: 0.587909  [    2/   88]
per-ex loss: 0.391094  [    3/   88]
per-ex loss: 0.530005  [    4/   88]
per-ex loss: 0.398734  [    5/   88]
per-ex loss: 0.556678  [    6/   88]
per-ex loss: 0.455378  [    7/   88]
per-ex loss: 0.571687  [    8/   88]
per-ex loss: 0.408314  [    9/   88]
per-ex loss: 0.369660  [   10/   88]
per-ex loss: 0.408082  [   11/   88]
per-ex loss: 0.313351  [   12/   88]
per-ex loss: 0.370522  [   13/   88]
per-ex loss: 0.527711  [   14/   88]
per-ex loss: 0.338843  [   15/   88]
per-ex loss: 0.500544  [   16/   88]
per-ex loss: 0.506432  [   17/   88]
per-ex loss: 0.678364  [   18/   88]
per-ex loss: 0.416888  [   19/   88]
per-ex loss: 0.381423  [   20/   88]
per-ex loss: 0.411250  [   21/   88]
per-ex loss: 0.502663  [   22/   88]
per-ex loss: 0.552706  [   23/   88]
per-ex loss: 0.538748  [   24/   88]
per-ex loss: 0.548626  [   25/   88]
per-ex loss: 0.672521  [   26/   88]
per-ex loss: 0.495935  [   27/   88]
per-ex loss: 0.578245  [   28/   88]
per-ex loss: 0.435374  [   29/   88]
per-ex loss: 0.350081  [   30/   88]
per-ex loss: 0.444806  [   31/   88]
per-ex loss: 0.645594  [   32/   88]
per-ex loss: 0.384459  [   33/   88]
per-ex loss: 0.585127  [   34/   88]
per-ex loss: 0.720908  [   35/   88]
per-ex loss: 0.589788  [   36/   88]
per-ex loss: 0.601889  [   37/   88]
per-ex loss: 0.640431  [   38/   88]
per-ex loss: 0.495697  [   39/   88]
per-ex loss: 0.353271  [   40/   88]
per-ex loss: 0.358580  [   41/   88]
per-ex loss: 0.592452  [   42/   88]
per-ex loss: 0.440231  [   43/   88]
per-ex loss: 0.625479  [   44/   88]
per-ex loss: 0.507492  [   45/   88]
per-ex loss: 0.423943  [   46/   88]
per-ex loss: 0.360543  [   47/   88]
per-ex loss: 0.412515  [   48/   88]
per-ex loss: 0.509882  [   49/   88]
per-ex loss: 0.633118  [   50/   88]
per-ex loss: 0.573332  [   51/   88]
per-ex loss: 0.348201  [   52/   88]
per-ex loss: 0.542912  [   53/   88]
per-ex loss: 0.356116  [   54/   88]
per-ex loss: 0.394633  [   55/   88]
per-ex loss: 0.608225  [   56/   88]
per-ex loss: 0.537146  [   57/   88]
per-ex loss: 0.335652  [   58/   88]
per-ex loss: 0.599454  [   59/   88]
per-ex loss: 0.381945  [   60/   88]
per-ex loss: 0.453212  [   61/   88]
per-ex loss: 0.330123  [   62/   88]
per-ex loss: 0.559411  [   63/   88]
per-ex loss: 0.397850  [   64/   88]
per-ex loss: 0.438048  [   65/   88]
per-ex loss: 0.380665  [   66/   88]
per-ex loss: 0.384973  [   67/   88]
per-ex loss: 0.418431  [   68/   88]
per-ex loss: 0.396620  [   69/   88]
per-ex loss: 0.428742  [   70/   88]
per-ex loss: 0.343053  [   71/   88]
per-ex loss: 0.284509  [   72/   88]
per-ex loss: 0.424159  [   73/   88]
per-ex loss: 0.506890  [   74/   88]
per-ex loss: 0.612624  [   75/   88]
per-ex loss: 0.673233  [   76/   88]
per-ex loss: 0.339559  [   77/   88]
per-ex loss: 0.459227  [   78/   88]
per-ex loss: 0.606875  [   79/   88]
per-ex loss: 0.413280  [   80/   88]
per-ex loss: 0.545728  [   81/   88]
per-ex loss: 0.397290  [   82/   88]
per-ex loss: 0.426617  [   83/   88]
per-ex loss: 0.633095  [   84/   88]
per-ex loss: 0.379335  [   85/   88]
per-ex loss: 0.393690  [   86/   88]
per-ex loss: 0.425402  [   87/   88]
per-ex loss: 0.395590  [   88/   88]
Train Error: Avg loss: 0.47621243
validation Error: 
 Avg loss: 0.52215668 
 F1: 0.505923 
 Precision: 0.596162 
 Recall: 0.439411
 IoU: 0.338619

test Error: 
 Avg loss: 0.48446742 
 F1: 0.563151 
 Precision: 0.629050 
 Recall: 0.509750
 IoU: 0.391935

We have finished training iteration 60
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_58_.pth
per-ex loss: 0.502084  [    1/   88]
per-ex loss: 0.387618  [    2/   88]
per-ex loss: 0.649950  [    3/   88]
per-ex loss: 0.590516  [    4/   88]
per-ex loss: 0.545458  [    5/   88]
per-ex loss: 0.458736  [    6/   88]
per-ex loss: 0.509377  [    7/   88]
per-ex loss: 0.624502  [    8/   88]
per-ex loss: 0.542098  [    9/   88]
per-ex loss: 0.593209  [   10/   88]
per-ex loss: 0.385928  [   11/   88]
per-ex loss: 0.559677  [   12/   88]
per-ex loss: 0.345284  [   13/   88]
per-ex loss: 0.406416  [   14/   88]
per-ex loss: 0.556079  [   15/   88]
per-ex loss: 0.420388  [   16/   88]
per-ex loss: 0.495972  [   17/   88]
per-ex loss: 0.599397  [   18/   88]
per-ex loss: 0.417589  [   19/   88]
per-ex loss: 0.377089  [   20/   88]
per-ex loss: 0.366746  [   21/   88]
per-ex loss: 0.363860  [   22/   88]
per-ex loss: 0.591563  [   23/   88]
per-ex loss: 0.597640  [   24/   88]
per-ex loss: 0.507751  [   25/   88]
per-ex loss: 0.352303  [   26/   88]
per-ex loss: 0.385292  [   27/   88]
per-ex loss: 0.633683  [   28/   88]
per-ex loss: 0.433284  [   29/   88]
per-ex loss: 0.337527  [   30/   88]
per-ex loss: 0.521945  [   31/   88]
per-ex loss: 0.426693  [   32/   88]
per-ex loss: 0.459663  [   33/   88]
per-ex loss: 0.405778  [   34/   88]
per-ex loss: 0.333686  [   35/   88]
per-ex loss: 0.337783  [   36/   88]
per-ex loss: 0.421063  [   37/   88]
per-ex loss: 0.531572  [   38/   88]
per-ex loss: 0.494372  [   39/   88]
per-ex loss: 0.418751  [   40/   88]
per-ex loss: 0.425522  [   41/   88]
per-ex loss: 0.338058  [   42/   88]
per-ex loss: 0.572091  [   43/   88]
per-ex loss: 0.358630  [   44/   88]
per-ex loss: 0.463607  [   45/   88]
per-ex loss: 0.420959  [   46/   88]
per-ex loss: 0.379076  [   47/   88]
per-ex loss: 0.476834  [   48/   88]
per-ex loss: 0.380726  [   49/   88]
per-ex loss: 0.348086  [   50/   88]
per-ex loss: 0.674736  [   51/   88]
per-ex loss: 0.351947  [   52/   88]
per-ex loss: 0.669305  [   53/   88]
per-ex loss: 0.500476  [   54/   88]
per-ex loss: 0.592817  [   55/   88]
per-ex loss: 0.425474  [   56/   88]
per-ex loss: 0.377885  [   57/   88]
per-ex loss: 0.583064  [   58/   88]
per-ex loss: 0.399852  [   59/   88]
per-ex loss: 0.378679  [   60/   88]
per-ex loss: 0.381235  [   61/   88]
per-ex loss: 0.422268  [   62/   88]
per-ex loss: 0.624869  [   63/   88]
per-ex loss: 0.618901  [   64/   88]
per-ex loss: 0.521286  [   65/   88]
per-ex loss: 0.425466  [   66/   88]
per-ex loss: 0.553865  [   67/   88]
per-ex loss: 0.623052  [   68/   88]
per-ex loss: 0.339135  [   69/   88]
per-ex loss: 0.393927  [   70/   88]
per-ex loss: 0.383059  [   71/   88]
per-ex loss: 0.539509  [   72/   88]
per-ex loss: 0.406709  [   73/   88]
per-ex loss: 0.386813  [   74/   88]
per-ex loss: 0.322719  [   75/   88]
per-ex loss: 0.591623  [   76/   88]
per-ex loss: 0.434016  [   77/   88]
per-ex loss: 0.523372  [   78/   88]
per-ex loss: 0.564668  [   79/   88]
per-ex loss: 0.369166  [   80/   88]
per-ex loss: 0.694923  [   81/   88]
per-ex loss: 0.549491  [   82/   88]
per-ex loss: 0.587855  [   83/   88]
per-ex loss: 0.304022  [   84/   88]
per-ex loss: 0.672214  [   85/   88]
per-ex loss: 0.390411  [   86/   88]
per-ex loss: 0.661943  [   87/   88]
per-ex loss: 0.304818  [   88/   88]
Train Error: Avg loss: 0.47269859
validation Error: 
 Avg loss: 0.52313614 
 F1: 0.504083 
 Precision: 0.540069 
 Recall: 0.472594
 IoU: 0.336973

test Error: 
 Avg loss: 0.48628800 
 F1: 0.561399 
 Precision: 0.580204 
 Recall: 0.543775
 IoU: 0.390240

We have finished training iteration 61
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_59_.pth
per-ex loss: 0.488424  [    1/   88]
per-ex loss: 0.508818  [    2/   88]
per-ex loss: 0.416938  [    3/   88]
per-ex loss: 0.412813  [    4/   88]
per-ex loss: 0.541303  [    5/   88]
per-ex loss: 0.390457  [    6/   88]
per-ex loss: 0.536115  [    7/   88]
per-ex loss: 0.505943  [    8/   88]
per-ex loss: 0.363230  [    9/   88]
per-ex loss: 0.643115  [   10/   88]
per-ex loss: 0.611265  [   11/   88]
per-ex loss: 0.355921  [   12/   88]
per-ex loss: 0.339430  [   13/   88]
per-ex loss: 0.402131  [   14/   88]
per-ex loss: 0.581175  [   15/   88]
per-ex loss: 0.575977  [   16/   88]
per-ex loss: 0.543347  [   17/   88]
per-ex loss: 0.365089  [   18/   88]
per-ex loss: 0.541668  [   19/   88]
per-ex loss: 0.359122  [   20/   88]
per-ex loss: 0.597558  [   21/   88]
per-ex loss: 0.424484  [   22/   88]
per-ex loss: 0.329782  [   23/   88]
per-ex loss: 0.484383  [   24/   88]
per-ex loss: 0.425025  [   25/   88]
per-ex loss: 0.482986  [   26/   88]
per-ex loss: 0.453194  [   27/   88]
per-ex loss: 0.548900  [   28/   88]
per-ex loss: 0.634968  [   29/   88]
per-ex loss: 0.529668  [   30/   88]
per-ex loss: 0.584049  [   31/   88]
per-ex loss: 0.435068  [   32/   88]
per-ex loss: 0.446761  [   33/   88]
per-ex loss: 0.370421  [   34/   88]
per-ex loss: 0.337978  [   35/   88]
per-ex loss: 0.597987  [   36/   88]
per-ex loss: 0.364058  [   37/   88]
per-ex loss: 0.438256  [   38/   88]
per-ex loss: 0.341936  [   39/   88]
per-ex loss: 0.501973  [   40/   88]
per-ex loss: 0.580288  [   41/   88]
per-ex loss: 0.406437  [   42/   88]
per-ex loss: 0.406219  [   43/   88]
per-ex loss: 0.522915  [   44/   88]
per-ex loss: 0.494959  [   45/   88]
per-ex loss: 0.570875  [   46/   88]
per-ex loss: 0.393291  [   47/   88]
per-ex loss: 0.399357  [   48/   88]
per-ex loss: 0.590528  [   49/   88]
per-ex loss: 0.371950  [   50/   88]
per-ex loss: 0.627955  [   51/   88]
per-ex loss: 0.530754  [   52/   88]
per-ex loss: 0.349157  [   53/   88]
per-ex loss: 0.381958  [   54/   88]
per-ex loss: 0.577811  [   55/   88]
per-ex loss: 0.313987  [   56/   88]
per-ex loss: 0.340369  [   57/   88]
per-ex loss: 0.666891  [   58/   88]
per-ex loss: 0.607455  [   59/   88]
per-ex loss: 0.558473  [   60/   88]
per-ex loss: 0.329405  [   61/   88]
per-ex loss: 0.382841  [   62/   88]
per-ex loss: 0.649745  [   63/   88]
per-ex loss: 0.444921  [   64/   88]
per-ex loss: 0.482303  [   65/   88]
per-ex loss: 0.417773  [   66/   88]
per-ex loss: 0.614340  [   67/   88]
per-ex loss: 0.394180  [   68/   88]
per-ex loss: 0.657016  [   69/   88]
per-ex loss: 0.274836  [   70/   88]
per-ex loss: 0.357260  [   71/   88]
per-ex loss: 0.332482  [   72/   88]
per-ex loss: 0.490125  [   73/   88]
per-ex loss: 0.454375  [   74/   88]
per-ex loss: 0.671947  [   75/   88]
per-ex loss: 0.344364  [   76/   88]
per-ex loss: 0.346379  [   77/   88]
per-ex loss: 0.408004  [   78/   88]
per-ex loss: 0.540266  [   79/   88]
per-ex loss: 0.535987  [   80/   88]
per-ex loss: 0.382029  [   81/   88]
per-ex loss: 0.696538  [   82/   88]
per-ex loss: 0.445463  [   83/   88]
per-ex loss: 0.360172  [   84/   88]
per-ex loss: 0.427510  [   85/   88]
per-ex loss: 0.382652  [   86/   88]
per-ex loss: 0.576492  [   87/   88]
per-ex loss: 0.540169  [   88/   88]
Train Error: Avg loss: 0.47094225
validation Error: 
 Avg loss: 0.53891633 
 F1: 0.489396 
 Precision: 0.551145 
 Recall: 0.440090
 IoU: 0.323974

test Error: 
 Avg loss: 0.48691522 
 F1: 0.562892 
 Precision: 0.633335 
 Recall: 0.506551
 IoU: 0.391684

We have finished training iteration 62
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_55_.pth
per-ex loss: 0.373065  [    1/   88]
per-ex loss: 0.424397  [    2/   88]
per-ex loss: 0.516497  [    3/   88]
per-ex loss: 0.517364  [    4/   88]
per-ex loss: 0.420005  [    5/   88]
per-ex loss: 0.486371  [    6/   88]
per-ex loss: 0.358985  [    7/   88]
per-ex loss: 0.514454  [    8/   88]
per-ex loss: 0.554389  [    9/   88]
per-ex loss: 0.359225  [   10/   88]
per-ex loss: 0.439691  [   11/   88]
per-ex loss: 0.407444  [   12/   88]
per-ex loss: 0.365462  [   13/   88]
per-ex loss: 0.315189  [   14/   88]
per-ex loss: 0.366325  [   15/   88]
per-ex loss: 0.427012  [   16/   88]
per-ex loss: 0.592240  [   17/   88]
per-ex loss: 0.376745  [   18/   88]
per-ex loss: 0.536245  [   19/   88]
per-ex loss: 0.416597  [   20/   88]
per-ex loss: 0.686319  [   21/   88]
per-ex loss: 0.601557  [   22/   88]
per-ex loss: 0.264943  [   23/   88]
per-ex loss: 0.339039  [   24/   88]
per-ex loss: 0.548584  [   25/   88]
per-ex loss: 0.636148  [   26/   88]
per-ex loss: 0.382765  [   27/   88]
per-ex loss: 0.599964  [   28/   88]
per-ex loss: 0.374675  [   29/   88]
per-ex loss: 0.354486  [   30/   88]
per-ex loss: 0.634107  [   31/   88]
per-ex loss: 0.357229  [   32/   88]
per-ex loss: 0.398973  [   33/   88]
per-ex loss: 0.411786  [   34/   88]
per-ex loss: 0.389519  [   35/   88]
per-ex loss: 0.589557  [   36/   88]
per-ex loss: 0.380135  [   37/   88]
per-ex loss: 0.367633  [   38/   88]
per-ex loss: 0.561486  [   39/   88]
per-ex loss: 0.347168  [   40/   88]
per-ex loss: 0.484979  [   41/   88]
per-ex loss: 0.620279  [   42/   88]
per-ex loss: 0.537714  [   43/   88]
per-ex loss: 0.455758  [   44/   88]
per-ex loss: 0.459900  [   45/   88]
per-ex loss: 0.538491  [   46/   88]
per-ex loss: 0.402317  [   47/   88]
per-ex loss: 0.580465  [   48/   88]
per-ex loss: 0.535810  [   49/   88]
per-ex loss: 0.407320  [   50/   88]
per-ex loss: 0.581998  [   51/   88]
per-ex loss: 0.526595  [   52/   88]
per-ex loss: 0.584567  [   53/   88]
per-ex loss: 0.409155  [   54/   88]
per-ex loss: 0.386471  [   55/   88]
per-ex loss: 0.673332  [   56/   88]
per-ex loss: 0.439215  [   57/   88]
per-ex loss: 0.412834  [   58/   88]
per-ex loss: 0.303387  [   59/   88]
per-ex loss: 0.585100  [   60/   88]
per-ex loss: 0.370228  [   61/   88]
per-ex loss: 0.425497  [   62/   88]
per-ex loss: 0.377767  [   63/   88]
per-ex loss: 0.512046  [   64/   88]
per-ex loss: 0.581464  [   65/   88]
per-ex loss: 0.605366  [   66/   88]
per-ex loss: 0.327216  [   67/   88]
per-ex loss: 0.338947  [   68/   88]
per-ex loss: 0.439159  [   69/   88]
per-ex loss: 0.365925  [   70/   88]
per-ex loss: 0.432530  [   71/   88]
per-ex loss: 0.641482  [   72/   88]
per-ex loss: 0.552606  [   73/   88]
per-ex loss: 0.353047  [   74/   88]
per-ex loss: 0.508634  [   75/   88]
per-ex loss: 0.490275  [   76/   88]
per-ex loss: 0.563908  [   77/   88]
per-ex loss: 0.404716  [   78/   88]
per-ex loss: 0.674487  [   79/   88]
per-ex loss: 0.586313  [   80/   88]
per-ex loss: 0.363430  [   81/   88]
per-ex loss: 0.543312  [   82/   88]
per-ex loss: 0.454416  [   83/   88]
per-ex loss: 0.509434  [   84/   88]
per-ex loss: 0.438263  [   85/   88]
per-ex loss: 0.390226  [   86/   88]
per-ex loss: 0.526561  [   87/   88]
per-ex loss: 0.667406  [   88/   88]
Train Error: Avg loss: 0.47004680
validation Error: 
 Avg loss: 0.53107002 
 F1: 0.497249 
 Precision: 0.528064 
 Recall: 0.469832
 IoU: 0.330893

test Error: 
 Avg loss: 0.48625663 
 F1: 0.564273 
 Precision: 0.567432 
 Recall: 0.561148
 IoU: 0.393022

We have finished training iteration 63
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_57_.pth
per-ex loss: 0.496868  [    1/   88]
per-ex loss: 0.589100  [    2/   88]
per-ex loss: 0.555702  [    3/   88]
per-ex loss: 0.519919  [    4/   88]
per-ex loss: 0.581762  [    5/   88]
per-ex loss: 0.626999  [    6/   88]
per-ex loss: 0.645188  [    7/   88]
per-ex loss: 0.360978  [    8/   88]
per-ex loss: 0.690352  [    9/   88]
per-ex loss: 0.354807  [   10/   88]
per-ex loss: 0.399772  [   11/   88]
per-ex loss: 0.410465  [   12/   88]
per-ex loss: 0.522097  [   13/   88]
per-ex loss: 0.479600  [   14/   88]
per-ex loss: 0.571654  [   15/   88]
per-ex loss: 0.666972  [   16/   88]
per-ex loss: 0.472641  [   17/   88]
per-ex loss: 0.399734  [   18/   88]
per-ex loss: 0.588751  [   19/   88]
per-ex loss: 0.491452  [   20/   88]
per-ex loss: 0.605815  [   21/   88]
per-ex loss: 0.619576  [   22/   88]
per-ex loss: 0.349566  [   23/   88]
per-ex loss: 0.477640  [   24/   88]
per-ex loss: 0.388120  [   25/   88]
per-ex loss: 0.541680  [   26/   88]
per-ex loss: 0.350083  [   27/   88]
per-ex loss: 0.409505  [   28/   88]
per-ex loss: 0.491129  [   29/   88]
per-ex loss: 0.394469  [   30/   88]
per-ex loss: 0.564911  [   31/   88]
per-ex loss: 0.365434  [   32/   88]
per-ex loss: 0.439374  [   33/   88]
per-ex loss: 0.483114  [   34/   88]
per-ex loss: 0.385469  [   35/   88]
per-ex loss: 0.368064  [   36/   88]
per-ex loss: 0.405023  [   37/   88]
per-ex loss: 0.490680  [   38/   88]
per-ex loss: 0.551513  [   39/   88]
per-ex loss: 0.421653  [   40/   88]
per-ex loss: 0.322338  [   41/   88]
per-ex loss: 0.321057  [   42/   88]
per-ex loss: 0.483445  [   43/   88]
per-ex loss: 0.389901  [   44/   88]
per-ex loss: 0.368928  [   45/   88]
per-ex loss: 0.384899  [   46/   88]
per-ex loss: 0.599811  [   47/   88]
per-ex loss: 0.705609  [   48/   88]
per-ex loss: 0.369361  [   49/   88]
per-ex loss: 0.405147  [   50/   88]
per-ex loss: 0.422575  [   51/   88]
per-ex loss: 0.539492  [   52/   88]
per-ex loss: 0.502518  [   53/   88]
per-ex loss: 0.561674  [   54/   88]
per-ex loss: 0.332060  [   55/   88]
per-ex loss: 0.566118  [   56/   88]
per-ex loss: 0.607757  [   57/   88]
per-ex loss: 0.498807  [   58/   88]
per-ex loss: 0.355390  [   59/   88]
per-ex loss: 0.516107  [   60/   88]
per-ex loss: 0.374143  [   61/   88]
per-ex loss: 0.426796  [   62/   88]
per-ex loss: 0.625568  [   63/   88]
per-ex loss: 0.541474  [   64/   88]
per-ex loss: 0.405203  [   65/   88]
per-ex loss: 0.292199  [   66/   88]
per-ex loss: 0.664704  [   67/   88]
per-ex loss: 0.572144  [   68/   88]
per-ex loss: 0.399162  [   69/   88]
per-ex loss: 0.374435  [   70/   88]
per-ex loss: 0.307970  [   71/   88]
per-ex loss: 0.368004  [   72/   88]
per-ex loss: 0.569875  [   73/   88]
per-ex loss: 0.543886  [   74/   88]
per-ex loss: 0.361290  [   75/   88]
per-ex loss: 0.333001  [   76/   88]
per-ex loss: 0.583282  [   77/   88]
per-ex loss: 0.353016  [   78/   88]
per-ex loss: 0.435507  [   79/   88]
per-ex loss: 0.433414  [   80/   88]
per-ex loss: 0.580794  [   81/   88]
per-ex loss: 0.656419  [   82/   88]
per-ex loss: 0.411234  [   83/   88]
per-ex loss: 0.395034  [   84/   88]
per-ex loss: 0.445571  [   85/   88]
per-ex loss: 0.337447  [   86/   88]
per-ex loss: 0.432380  [   87/   88]
per-ex loss: 0.365979  [   88/   88]
Train Error: Avg loss: 0.47011995
validation Error: 
 Avg loss: 0.52428979 
 F1: 0.502399 
 Precision: 0.543755 
 Recall: 0.466890
 IoU: 0.335469

test Error: 
 Avg loss: 0.48636887 
 F1: 0.560970 
 Precision: 0.587939 
 Recall: 0.536366
 IoU: 0.389825

We have finished training iteration 64
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_62_.pth
per-ex loss: 0.572855  [    1/   88]
per-ex loss: 0.410496  [    2/   88]
per-ex loss: 0.429890  [    3/   88]
per-ex loss: 0.652534  [    4/   88]
per-ex loss: 0.341787  [    5/   88]
per-ex loss: 0.572972  [    6/   88]
per-ex loss: 0.501504  [    7/   88]
per-ex loss: 0.342053  [    8/   88]
per-ex loss: 0.367995  [    9/   88]
per-ex loss: 0.571472  [   10/   88]
per-ex loss: 0.425950  [   11/   88]
per-ex loss: 0.281051  [   12/   88]
per-ex loss: 0.396739  [   13/   88]
per-ex loss: 0.532694  [   14/   88]
per-ex loss: 0.578921  [   15/   88]
per-ex loss: 0.447722  [   16/   88]
per-ex loss: 0.505332  [   17/   88]
per-ex loss: 0.456998  [   18/   88]
per-ex loss: 0.508748  [   19/   88]
per-ex loss: 0.488922  [   20/   88]
per-ex loss: 0.451987  [   21/   88]
per-ex loss: 0.499793  [   22/   88]
per-ex loss: 0.492308  [   23/   88]
per-ex loss: 0.296881  [   24/   88]
per-ex loss: 0.532935  [   25/   88]
per-ex loss: 0.583182  [   26/   88]
per-ex loss: 0.443591  [   27/   88]
per-ex loss: 0.596912  [   28/   88]
per-ex loss: 0.567837  [   29/   88]
per-ex loss: 0.585155  [   30/   88]
per-ex loss: 0.655841  [   31/   88]
per-ex loss: 0.395191  [   32/   88]
per-ex loss: 0.480770  [   33/   88]
per-ex loss: 0.338868  [   34/   88]
per-ex loss: 0.438233  [   35/   88]
per-ex loss: 0.339922  [   36/   88]
per-ex loss: 0.542878  [   37/   88]
per-ex loss: 0.369411  [   38/   88]
per-ex loss: 0.511126  [   39/   88]
per-ex loss: 0.342319  [   40/   88]
per-ex loss: 0.385387  [   41/   88]
per-ex loss: 0.372308  [   42/   88]
per-ex loss: 0.573652  [   43/   88]
per-ex loss: 0.355552  [   44/   88]
per-ex loss: 0.656472  [   45/   88]
per-ex loss: 0.631415  [   46/   88]
per-ex loss: 0.610045  [   47/   88]
per-ex loss: 0.544414  [   48/   88]
per-ex loss: 0.330557  [   49/   88]
per-ex loss: 0.338597  [   50/   88]
per-ex loss: 0.429886  [   51/   88]
per-ex loss: 0.575889  [   52/   88]
per-ex loss: 0.644866  [   53/   88]
per-ex loss: 0.646070  [   54/   88]
per-ex loss: 0.407947  [   55/   88]
per-ex loss: 0.330433  [   56/   88]
per-ex loss: 0.584337  [   57/   88]
per-ex loss: 0.377349  [   58/   88]
per-ex loss: 0.432336  [   59/   88]
per-ex loss: 0.507177  [   60/   88]
per-ex loss: 0.420974  [   61/   88]
per-ex loss: 0.413905  [   62/   88]
per-ex loss: 0.406624  [   63/   88]
per-ex loss: 0.353734  [   64/   88]
per-ex loss: 0.403847  [   65/   88]
per-ex loss: 0.361216  [   66/   88]
per-ex loss: 0.374302  [   67/   88]
per-ex loss: 0.468660  [   68/   88]
per-ex loss: 0.369975  [   69/   88]
per-ex loss: 0.546031  [   70/   88]
per-ex loss: 0.659077  [   71/   88]
per-ex loss: 0.616770  [   72/   88]
per-ex loss: 0.390441  [   73/   88]
per-ex loss: 0.500687  [   74/   88]
per-ex loss: 0.532074  [   75/   88]
per-ex loss: 0.644541  [   76/   88]
per-ex loss: 0.536435  [   77/   88]
per-ex loss: 0.420761  [   78/   88]
per-ex loss: 0.454494  [   79/   88]
per-ex loss: 0.373082  [   80/   88]
per-ex loss: 0.444201  [   81/   88]
per-ex loss: 0.391445  [   82/   88]
per-ex loss: 0.364109  [   83/   88]
per-ex loss: 0.553351  [   84/   88]
per-ex loss: 0.396499  [   85/   88]
per-ex loss: 0.381547  [   86/   88]
per-ex loss: 0.371409  [   87/   88]
per-ex loss: 0.396000  [   88/   88]
Train Error: Avg loss: 0.46741652
validation Error: 
 Avg loss: 0.52715784 
 F1: 0.501718 
 Precision: 0.630308 
 Recall: 0.416705
 IoU: 0.334862

test Error: 
 Avg loss: 0.49864010 
 F1: 0.550504 
 Precision: 0.633564 
 Recall: 0.486698
 IoU: 0.379790

We have finished training iteration 65
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_63_.pth
per-ex loss: 0.513592  [    1/   88]
per-ex loss: 0.398598  [    2/   88]
per-ex loss: 0.567610  [    3/   88]
per-ex loss: 0.535920  [    4/   88]
per-ex loss: 0.593003  [    5/   88]
per-ex loss: 0.390458  [    6/   88]
per-ex loss: 0.555105  [    7/   88]
per-ex loss: 0.283091  [    8/   88]
per-ex loss: 0.407133  [    9/   88]
per-ex loss: 0.421461  [   10/   88]
per-ex loss: 0.379673  [   11/   88]
per-ex loss: 0.583059  [   12/   88]
per-ex loss: 0.591284  [   13/   88]
per-ex loss: 0.603964  [   14/   88]
per-ex loss: 0.614540  [   15/   88]
per-ex loss: 0.522480  [   16/   88]
per-ex loss: 0.659320  [   17/   88]
per-ex loss: 0.331193  [   18/   88]
per-ex loss: 0.347988  [   19/   88]
per-ex loss: 0.649643  [   20/   88]
per-ex loss: 0.306365  [   21/   88]
per-ex loss: 0.393594  [   22/   88]
per-ex loss: 0.374810  [   23/   88]
per-ex loss: 0.588302  [   24/   88]
per-ex loss: 0.351410  [   25/   88]
per-ex loss: 0.458405  [   26/   88]
per-ex loss: 0.585325  [   27/   88]
per-ex loss: 0.407003  [   28/   88]
per-ex loss: 0.528741  [   29/   88]
per-ex loss: 0.586390  [   30/   88]
per-ex loss: 0.482740  [   31/   88]
per-ex loss: 0.382999  [   32/   88]
per-ex loss: 0.589549  [   33/   88]
per-ex loss: 0.320045  [   34/   88]
per-ex loss: 0.430383  [   35/   88]
per-ex loss: 0.366135  [   36/   88]
per-ex loss: 0.535899  [   37/   88]
per-ex loss: 0.344496  [   38/   88]
per-ex loss: 0.403411  [   39/   88]
per-ex loss: 0.444315  [   40/   88]
per-ex loss: 0.538160  [   41/   88]
per-ex loss: 0.652664  [   42/   88]
per-ex loss: 0.382798  [   43/   88]
per-ex loss: 0.534938  [   44/   88]
per-ex loss: 0.668163  [   45/   88]
per-ex loss: 0.341130  [   46/   88]
per-ex loss: 0.418050  [   47/   88]
per-ex loss: 0.406200  [   48/   88]
per-ex loss: 0.522745  [   49/   88]
per-ex loss: 0.435968  [   50/   88]
per-ex loss: 0.557373  [   51/   88]
per-ex loss: 0.633057  [   52/   88]
per-ex loss: 0.378103  [   53/   88]
per-ex loss: 0.352772  [   54/   88]
per-ex loss: 0.465476  [   55/   88]
per-ex loss: 0.393602  [   56/   88]
per-ex loss: 0.574486  [   57/   88]
per-ex loss: 0.331607  [   58/   88]
per-ex loss: 0.619684  [   59/   88]
per-ex loss: 0.354894  [   60/   88]
per-ex loss: 0.388598  [   61/   88]
per-ex loss: 0.506076  [   62/   88]
per-ex loss: 0.431602  [   63/   88]
per-ex loss: 0.338985  [   64/   88]
per-ex loss: 0.417899  [   65/   88]
per-ex loss: 0.459088  [   66/   88]
per-ex loss: 0.547919  [   67/   88]
per-ex loss: 0.331653  [   68/   88]
per-ex loss: 0.384925  [   69/   88]
per-ex loss: 0.333935  [   70/   88]
per-ex loss: 0.337423  [   71/   88]
per-ex loss: 0.425802  [   72/   88]
per-ex loss: 0.564286  [   73/   88]
per-ex loss: 0.418054  [   74/   88]
per-ex loss: 0.551126  [   75/   88]
per-ex loss: 0.474033  [   76/   88]
per-ex loss: 0.349317  [   77/   88]
per-ex loss: 0.580051  [   78/   88]
per-ex loss: 0.428668  [   79/   88]
per-ex loss: 0.656601  [   80/   88]
per-ex loss: 0.649147  [   81/   88]
per-ex loss: 0.547559  [   82/   88]
per-ex loss: 0.377069  [   83/   88]
per-ex loss: 0.515554  [   84/   88]
per-ex loss: 0.345447  [   85/   88]
per-ex loss: 0.347111  [   86/   88]
per-ex loss: 0.439333  [   87/   88]
per-ex loss: 0.601346  [   88/   88]
Train Error: Avg loss: 0.46749896
validation Error: 
 Avg loss: 0.52322795 
 F1: 0.506791 
 Precision: 0.554700 
 Recall: 0.466500
 IoU: 0.339398

test Error: 
 Avg loss: 0.48355960 
 F1: 0.568029 
 Precision: 0.607383 
 Recall: 0.533465
 IoU: 0.396677

We have finished training iteration 66
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_64_.pth
per-ex loss: 0.537501  [    1/   88]
per-ex loss: 0.424267  [    2/   88]
per-ex loss: 0.453169  [    3/   88]
per-ex loss: 0.470012  [    4/   88]
per-ex loss: 0.463169  [    5/   88]
per-ex loss: 0.348689  [    6/   88]
per-ex loss: 0.543622  [    7/   88]
per-ex loss: 0.371679  [    8/   88]
per-ex loss: 0.534593  [    9/   88]
per-ex loss: 0.391933  [   10/   88]
per-ex loss: 0.585961  [   11/   88]
per-ex loss: 0.644463  [   12/   88]
per-ex loss: 0.522244  [   13/   88]
per-ex loss: 0.544381  [   14/   88]
per-ex loss: 0.444759  [   15/   88]
per-ex loss: 0.500298  [   16/   88]
per-ex loss: 0.553981  [   17/   88]
per-ex loss: 0.544200  [   18/   88]
per-ex loss: 0.353182  [   19/   88]
per-ex loss: 0.400554  [   20/   88]
per-ex loss: 0.374704  [   21/   88]
per-ex loss: 0.560273  [   22/   88]
per-ex loss: 0.395764  [   23/   88]
per-ex loss: 0.472335  [   24/   88]
per-ex loss: 0.619436  [   25/   88]
per-ex loss: 0.421873  [   26/   88]
per-ex loss: 0.417197  [   27/   88]
per-ex loss: 0.367134  [   28/   88]
per-ex loss: 0.571491  [   29/   88]
per-ex loss: 0.378051  [   30/   88]
per-ex loss: 0.485715  [   31/   88]
per-ex loss: 0.435377  [   32/   88]
per-ex loss: 0.348213  [   33/   88]
per-ex loss: 0.341205  [   34/   88]
per-ex loss: 0.337375  [   35/   88]
per-ex loss: 0.344916  [   36/   88]
per-ex loss: 0.591535  [   37/   88]
per-ex loss: 0.438806  [   38/   88]
per-ex loss: 0.509830  [   39/   88]
per-ex loss: 0.444703  [   40/   88]
per-ex loss: 0.388711  [   41/   88]
per-ex loss: 0.520220  [   42/   88]
per-ex loss: 0.381078  [   43/   88]
per-ex loss: 0.635380  [   44/   88]
per-ex loss: 0.420310  [   45/   88]
per-ex loss: 0.651611  [   46/   88]
per-ex loss: 0.371657  [   47/   88]
per-ex loss: 0.429039  [   48/   88]
per-ex loss: 0.492738  [   49/   88]
per-ex loss: 0.509047  [   50/   88]
per-ex loss: 0.680040  [   51/   88]
per-ex loss: 0.372240  [   52/   88]
per-ex loss: 0.306038  [   53/   88]
per-ex loss: 0.567566  [   54/   88]
per-ex loss: 0.613963  [   55/   88]
per-ex loss: 0.568412  [   56/   88]
per-ex loss: 0.331363  [   57/   88]
per-ex loss: 0.331480  [   58/   88]
per-ex loss: 0.565446  [   59/   88]
per-ex loss: 0.579968  [   60/   88]
per-ex loss: 0.414516  [   61/   88]
per-ex loss: 0.520071  [   62/   88]
per-ex loss: 0.355533  [   63/   88]
per-ex loss: 0.575146  [   64/   88]
per-ex loss: 0.407117  [   65/   88]
per-ex loss: 0.372351  [   66/   88]
per-ex loss: 0.387102  [   67/   88]
per-ex loss: 0.307091  [   68/   88]
per-ex loss: 0.505388  [   69/   88]
per-ex loss: 0.667699  [   70/   88]
per-ex loss: 0.400626  [   71/   88]
per-ex loss: 0.388481  [   72/   88]
per-ex loss: 0.495773  [   73/   88]
per-ex loss: 0.367987  [   74/   88]
per-ex loss: 0.648505  [   75/   88]
per-ex loss: 0.397667  [   76/   88]
per-ex loss: 0.399152  [   77/   88]
per-ex loss: 0.386366  [   78/   88]
per-ex loss: 0.366935  [   79/   88]
per-ex loss: 0.442262  [   80/   88]
per-ex loss: 0.327732  [   81/   88]
per-ex loss: 0.689421  [   82/   88]
per-ex loss: 0.464843  [   83/   88]
per-ex loss: 0.412669  [   84/   88]
per-ex loss: 0.474223  [   85/   88]
per-ex loss: 0.567108  [   86/   88]
per-ex loss: 0.313015  [   87/   88]
per-ex loss: 0.609230  [   88/   88]
Train Error: Avg loss: 0.46441940
validation Error: 
 Avg loss: 0.53349022 
 F1: 0.491242 
 Precision: 0.618282 
 Recall: 0.407510
 IoU: 0.325594

test Error: 
 Avg loss: 0.49789303 
 F1: 0.544816 
 Precision: 0.666003 
 Recall: 0.460942
 IoU: 0.374397

We have finished training iteration 67
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_65_.pth
per-ex loss: 0.615987  [    1/   88]
per-ex loss: 0.410521  [    2/   88]
per-ex loss: 0.528444  [    3/   88]
per-ex loss: 0.380843  [    4/   88]
per-ex loss: 0.376630  [    5/   88]
per-ex loss: 0.434759  [    6/   88]
per-ex loss: 0.370632  [    7/   88]
per-ex loss: 0.472327  [    8/   88]
per-ex loss: 0.637859  [    9/   88]
per-ex loss: 0.558437  [   10/   88]
per-ex loss: 0.597197  [   11/   88]
per-ex loss: 0.401828  [   12/   88]
per-ex loss: 0.391236  [   13/   88]
per-ex loss: 0.331459  [   14/   88]
per-ex loss: 0.402754  [   15/   88]
per-ex loss: 0.373613  [   16/   88]
per-ex loss: 0.649810  [   17/   88]
per-ex loss: 0.528428  [   18/   88]
per-ex loss: 0.533999  [   19/   88]
per-ex loss: 0.430601  [   20/   88]
per-ex loss: 0.341586  [   21/   88]
per-ex loss: 0.529339  [   22/   88]
per-ex loss: 0.358384  [   23/   88]
per-ex loss: 0.478831  [   24/   88]
per-ex loss: 0.531698  [   25/   88]
per-ex loss: 0.352684  [   26/   88]
per-ex loss: 0.578948  [   27/   88]
per-ex loss: 0.558074  [   28/   88]
per-ex loss: 0.470766  [   29/   88]
per-ex loss: 0.320691  [   30/   88]
per-ex loss: 0.515117  [   31/   88]
per-ex loss: 0.412690  [   32/   88]
per-ex loss: 0.481117  [   33/   88]
per-ex loss: 0.489275  [   34/   88]
per-ex loss: 0.613270  [   35/   88]
per-ex loss: 0.362583  [   36/   88]
per-ex loss: 0.416449  [   37/   88]
per-ex loss: 0.616498  [   38/   88]
per-ex loss: 0.577222  [   39/   88]
per-ex loss: 0.450445  [   40/   88]
per-ex loss: 0.394048  [   41/   88]
per-ex loss: 0.491901  [   42/   88]
per-ex loss: 0.544440  [   43/   88]
per-ex loss: 0.389072  [   44/   88]
per-ex loss: 0.644799  [   45/   88]
per-ex loss: 0.411812  [   46/   88]
per-ex loss: 0.630297  [   47/   88]
per-ex loss: 0.339670  [   48/   88]
per-ex loss: 0.307747  [   49/   88]
per-ex loss: 0.624353  [   50/   88]
per-ex loss: 0.337617  [   51/   88]
per-ex loss: 0.544007  [   52/   88]
per-ex loss: 0.449445  [   53/   88]
per-ex loss: 0.422685  [   54/   88]
per-ex loss: 0.568289  [   55/   88]
per-ex loss: 0.559101  [   56/   88]
per-ex loss: 0.492727  [   57/   88]
per-ex loss: 0.569208  [   58/   88]
per-ex loss: 0.384359  [   59/   88]
per-ex loss: 0.343962  [   60/   88]
per-ex loss: 0.410650  [   61/   88]
per-ex loss: 0.579369  [   62/   88]
per-ex loss: 0.590527  [   63/   88]
per-ex loss: 0.313934  [   64/   88]
per-ex loss: 0.430991  [   65/   88]
per-ex loss: 0.590039  [   66/   88]
per-ex loss: 0.469989  [   67/   88]
per-ex loss: 0.555893  [   68/   88]
per-ex loss: 0.371913  [   69/   88]
per-ex loss: 0.414764  [   70/   88]
per-ex loss: 0.420356  [   71/   88]
per-ex loss: 0.395537  [   72/   88]
per-ex loss: 0.689012  [   73/   88]
per-ex loss: 0.305739  [   74/   88]
per-ex loss: 0.343561  [   75/   88]
per-ex loss: 0.332414  [   76/   88]
per-ex loss: 0.404072  [   77/   88]
per-ex loss: 0.510998  [   78/   88]
per-ex loss: 0.393286  [   79/   88]
per-ex loss: 0.565051  [   80/   88]
per-ex loss: 0.329538  [   81/   88]
per-ex loss: 0.568316  [   82/   88]
per-ex loss: 0.498946  [   83/   88]
per-ex loss: 0.391020  [   84/   88]
per-ex loss: 0.361670  [   85/   88]
per-ex loss: 0.393188  [   86/   88]
per-ex loss: 0.410967  [   87/   88]
per-ex loss: 0.512987  [   88/   88]
Train Error: Avg loss: 0.46462842
validation Error: 
 Avg loss: 0.52709276 
 F1: 0.501776 
 Precision: 0.586141 
 Recall: 0.438642
 IoU: 0.334914

test Error: 
 Avg loss: 0.48868257 
 F1: 0.560211 
 Precision: 0.627463 
 Recall: 0.505979
 IoU: 0.389092

We have finished training iteration 68
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_66_.pth
per-ex loss: 0.343988  [    1/   88]
per-ex loss: 0.567753  [    2/   88]
per-ex loss: 0.394800  [    3/   88]
per-ex loss: 0.439255  [    4/   88]
per-ex loss: 0.546206  [    5/   88]
per-ex loss: 0.580565  [    6/   88]
per-ex loss: 0.426892  [    7/   88]
per-ex loss: 0.349142  [    8/   88]
per-ex loss: 0.374684  [    9/   88]
per-ex loss: 0.493310  [   10/   88]
per-ex loss: 0.536108  [   11/   88]
per-ex loss: 0.357857  [   12/   88]
per-ex loss: 0.414885  [   13/   88]
per-ex loss: 0.493578  [   14/   88]
per-ex loss: 0.557630  [   15/   88]
per-ex loss: 0.571106  [   16/   88]
per-ex loss: 0.479585  [   17/   88]
per-ex loss: 0.447208  [   18/   88]
per-ex loss: 0.377894  [   19/   88]
per-ex loss: 0.397545  [   20/   88]
per-ex loss: 0.593504  [   21/   88]
per-ex loss: 0.384314  [   22/   88]
per-ex loss: 0.379716  [   23/   88]
per-ex loss: 0.393125  [   24/   88]
per-ex loss: 0.474050  [   25/   88]
per-ex loss: 0.391741  [   26/   88]
per-ex loss: 0.360606  [   27/   88]
per-ex loss: 0.298179  [   28/   88]
per-ex loss: 0.429728  [   29/   88]
per-ex loss: 0.577546  [   30/   88]
per-ex loss: 0.314654  [   31/   88]
per-ex loss: 0.560998  [   32/   88]
per-ex loss: 0.531357  [   33/   88]
per-ex loss: 0.349283  [   34/   88]
per-ex loss: 0.407845  [   35/   88]
per-ex loss: 0.649989  [   36/   88]
per-ex loss: 0.563016  [   37/   88]
per-ex loss: 0.397863  [   38/   88]
per-ex loss: 0.567014  [   39/   88]
per-ex loss: 0.646481  [   40/   88]
per-ex loss: 0.626051  [   41/   88]
per-ex loss: 0.411424  [   42/   88]
per-ex loss: 0.375421  [   43/   88]
per-ex loss: 0.364946  [   44/   88]
per-ex loss: 0.389252  [   45/   88]
per-ex loss: 0.653280  [   46/   88]
per-ex loss: 0.242797  [   47/   88]
per-ex loss: 0.488734  [   48/   88]
per-ex loss: 0.419714  [   49/   88]
per-ex loss: 0.568672  [   50/   88]
per-ex loss: 0.611085  [   51/   88]
per-ex loss: 0.338450  [   52/   88]
per-ex loss: 0.356772  [   53/   88]
per-ex loss: 0.681360  [   54/   88]
per-ex loss: 0.397352  [   55/   88]
per-ex loss: 0.628365  [   56/   88]
per-ex loss: 0.499061  [   57/   88]
per-ex loss: 0.558345  [   58/   88]
per-ex loss: 0.537259  [   59/   88]
per-ex loss: 0.380451  [   60/   88]
per-ex loss: 0.565816  [   61/   88]
per-ex loss: 0.666055  [   62/   88]
per-ex loss: 0.550586  [   63/   88]
per-ex loss: 0.351818  [   64/   88]
per-ex loss: 0.460600  [   65/   88]
per-ex loss: 0.485789  [   66/   88]
per-ex loss: 0.433014  [   67/   88]
per-ex loss: 0.427742  [   68/   88]
per-ex loss: 0.367758  [   69/   88]
per-ex loss: 0.341368  [   70/   88]
per-ex loss: 0.608070  [   71/   88]
per-ex loss: 0.639074  [   72/   88]
per-ex loss: 0.348188  [   73/   88]
per-ex loss: 0.459007  [   74/   88]
per-ex loss: 0.415929  [   75/   88]
per-ex loss: 0.581164  [   76/   88]
per-ex loss: 0.483980  [   77/   88]
per-ex loss: 0.368866  [   78/   88]
per-ex loss: 0.422978  [   79/   88]
per-ex loss: 0.498712  [   80/   88]
per-ex loss: 0.360910  [   81/   88]
per-ex loss: 0.621499  [   82/   88]
per-ex loss: 0.602587  [   83/   88]
per-ex loss: 0.588651  [   84/   88]
per-ex loss: 0.329446  [   85/   88]
per-ex loss: 0.395564  [   86/   88]
per-ex loss: 0.365021  [   87/   88]
per-ex loss: 0.483004  [   88/   88]
Train Error: Avg loss: 0.46787482
validation Error: 
 Avg loss: 0.52194535 
 F1: 0.505531 
 Precision: 0.564340 
 Recall: 0.457823
 IoU: 0.338268

test Error: 
 Avg loss: 0.48384178 
 F1: 0.562461 
 Precision: 0.612709 
 Recall: 0.519830
 IoU: 0.391267

We have finished training iteration 69
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_67_.pth
per-ex loss: 0.651720  [    1/   88]
per-ex loss: 0.416432  [    2/   88]
per-ex loss: 0.393104  [    3/   88]
per-ex loss: 0.405389  [    4/   88]
per-ex loss: 0.455394  [    5/   88]
per-ex loss: 0.382518  [    6/   88]
per-ex loss: 0.359753  [    7/   88]
per-ex loss: 0.274499  [    8/   88]
per-ex loss: 0.337504  [    9/   88]
per-ex loss: 0.520764  [   10/   88]
per-ex loss: 0.488648  [   11/   88]
per-ex loss: 0.443231  [   12/   88]
per-ex loss: 0.463899  [   13/   88]
per-ex loss: 0.327073  [   14/   88]
per-ex loss: 0.386775  [   15/   88]
per-ex loss: 0.401053  [   16/   88]
per-ex loss: 0.586101  [   17/   88]
per-ex loss: 0.453737  [   18/   88]
per-ex loss: 0.459137  [   19/   88]
per-ex loss: 0.349420  [   20/   88]
per-ex loss: 0.332370  [   21/   88]
per-ex loss: 0.329101  [   22/   88]
per-ex loss: 0.388749  [   23/   88]
per-ex loss: 0.372814  [   24/   88]
per-ex loss: 0.397039  [   25/   88]
per-ex loss: 0.663786  [   26/   88]
per-ex loss: 0.354819  [   27/   88]
per-ex loss: 0.578857  [   28/   88]
per-ex loss: 0.566964  [   29/   88]
per-ex loss: 0.401789  [   30/   88]
per-ex loss: 0.486675  [   31/   88]
per-ex loss: 0.595540  [   32/   88]
per-ex loss: 0.653138  [   33/   88]
per-ex loss: 0.487360  [   34/   88]
per-ex loss: 0.613883  [   35/   88]
per-ex loss: 0.579490  [   36/   88]
per-ex loss: 0.628855  [   37/   88]
per-ex loss: 0.364323  [   38/   88]
per-ex loss: 0.665648  [   39/   88]
per-ex loss: 0.439401  [   40/   88]
per-ex loss: 0.416595  [   41/   88]
per-ex loss: 0.568146  [   42/   88]
per-ex loss: 0.371547  [   43/   88]
per-ex loss: 0.383990  [   44/   88]
per-ex loss: 0.547377  [   45/   88]
per-ex loss: 0.529586  [   46/   88]
per-ex loss: 0.399991  [   47/   88]
per-ex loss: 0.543483  [   48/   88]
per-ex loss: 0.567828  [   49/   88]
per-ex loss: 0.543035  [   50/   88]
per-ex loss: 0.509060  [   51/   88]
per-ex loss: 0.622835  [   52/   88]
per-ex loss: 0.579535  [   53/   88]
per-ex loss: 0.584097  [   54/   88]
per-ex loss: 0.306477  [   55/   88]
per-ex loss: 0.328571  [   56/   88]
per-ex loss: 0.325920  [   57/   88]
per-ex loss: 0.357870  [   58/   88]
per-ex loss: 0.399699  [   59/   88]
per-ex loss: 0.576245  [   60/   88]
per-ex loss: 0.580605  [   61/   88]
per-ex loss: 0.386285  [   62/   88]
per-ex loss: 0.526066  [   63/   88]
per-ex loss: 0.517356  [   64/   88]
per-ex loss: 0.399799  [   65/   88]
per-ex loss: 0.429023  [   66/   88]
per-ex loss: 0.506294  [   67/   88]
per-ex loss: 0.351095  [   68/   88]
per-ex loss: 0.383464  [   69/   88]
per-ex loss: 0.667536  [   70/   88]
per-ex loss: 0.334693  [   71/   88]
per-ex loss: 0.383800  [   72/   88]
per-ex loss: 0.374920  [   73/   88]
per-ex loss: 0.535027  [   74/   88]
per-ex loss: 0.366562  [   75/   88]
per-ex loss: 0.374383  [   76/   88]
per-ex loss: 0.396598  [   77/   88]
per-ex loss: 0.569906  [   78/   88]
per-ex loss: 0.423599  [   79/   88]
per-ex loss: 0.519563  [   80/   88]
per-ex loss: 0.467668  [   81/   88]
per-ex loss: 0.554981  [   82/   88]
per-ex loss: 0.362096  [   83/   88]
per-ex loss: 0.431309  [   84/   88]
per-ex loss: 0.457964  [   85/   88]
per-ex loss: 0.588560  [   86/   88]
per-ex loss: 0.378446  [   87/   88]
per-ex loss: 0.622419  [   88/   88]
Train Error: Avg loss: 0.46373471
validation Error: 
 Avg loss: 0.53801952 
 F1: 0.492123 
 Precision: 0.506970 
 Recall: 0.478121
 IoU: 0.326368

test Error: 
 Avg loss: 0.49100130 
 F1: 0.556396 
 Precision: 0.564452 
 Recall: 0.548567
 IoU: 0.385422

We have finished training iteration 70
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_68_.pth
per-ex loss: 0.514218  [    1/   88]
per-ex loss: 0.395136  [    2/   88]
per-ex loss: 0.349217  [    3/   88]
per-ex loss: 0.557999  [    4/   88]
per-ex loss: 0.377543  [    5/   88]
per-ex loss: 0.310849  [    6/   88]
per-ex loss: 0.384139  [    7/   88]
per-ex loss: 0.419807  [    8/   88]
per-ex loss: 0.423773  [    9/   88]
per-ex loss: 0.490223  [   10/   88]
per-ex loss: 0.517198  [   11/   88]
per-ex loss: 0.383762  [   12/   88]
per-ex loss: 0.644068  [   13/   88]
per-ex loss: 0.617944  [   14/   88]
per-ex loss: 0.480061  [   15/   88]
per-ex loss: 0.587241  [   16/   88]
per-ex loss: 0.576969  [   17/   88]
per-ex loss: 0.500611  [   18/   88]
per-ex loss: 0.611467  [   19/   88]
per-ex loss: 0.352506  [   20/   88]
per-ex loss: 0.529250  [   21/   88]
per-ex loss: 0.496498  [   22/   88]
per-ex loss: 0.577385  [   23/   88]
per-ex loss: 0.365436  [   24/   88]
per-ex loss: 0.532162  [   25/   88]
per-ex loss: 0.450533  [   26/   88]
per-ex loss: 0.410236  [   27/   88]
per-ex loss: 0.411225  [   28/   88]
per-ex loss: 0.544204  [   29/   88]
per-ex loss: 0.411271  [   30/   88]
per-ex loss: 0.640330  [   31/   88]
per-ex loss: 0.460309  [   32/   88]
per-ex loss: 0.522390  [   33/   88]
per-ex loss: 0.435550  [   34/   88]
per-ex loss: 0.548528  [   35/   88]
per-ex loss: 0.435129  [   36/   88]
per-ex loss: 0.421755  [   37/   88]
per-ex loss: 0.485543  [   38/   88]
per-ex loss: 0.372203  [   39/   88]
per-ex loss: 0.385874  [   40/   88]
per-ex loss: 0.473749  [   41/   88]
per-ex loss: 0.357305  [   42/   88]
per-ex loss: 0.411759  [   43/   88]
per-ex loss: 0.337411  [   44/   88]
per-ex loss: 0.427341  [   45/   88]
per-ex loss: 0.585353  [   46/   88]
per-ex loss: 0.496961  [   47/   88]
per-ex loss: 0.331881  [   48/   88]
per-ex loss: 0.469846  [   49/   88]
per-ex loss: 0.389588  [   50/   88]
per-ex loss: 0.643630  [   51/   88]
per-ex loss: 0.585642  [   52/   88]
per-ex loss: 0.390119  [   53/   88]
per-ex loss: 0.560875  [   54/   88]
per-ex loss: 0.365039  [   55/   88]
per-ex loss: 0.385102  [   56/   88]
per-ex loss: 0.330080  [   57/   88]
per-ex loss: 0.560028  [   58/   88]
per-ex loss: 0.328934  [   59/   88]
per-ex loss: 0.664658  [   60/   88]
per-ex loss: 0.606945  [   61/   88]
per-ex loss: 0.339698  [   62/   88]
per-ex loss: 0.349430  [   63/   88]
per-ex loss: 0.370661  [   64/   88]
per-ex loss: 0.582165  [   65/   88]
per-ex loss: 0.321534  [   66/   88]
per-ex loss: 0.409771  [   67/   88]
per-ex loss: 0.625465  [   68/   88]
per-ex loss: 0.433823  [   69/   88]
per-ex loss: 0.403385  [   70/   88]
per-ex loss: 0.659690  [   71/   88]
per-ex loss: 0.456215  [   72/   88]
per-ex loss: 0.507939  [   73/   88]
per-ex loss: 0.394612  [   74/   88]
per-ex loss: 0.407765  [   75/   88]
per-ex loss: 0.289223  [   76/   88]
per-ex loss: 0.325100  [   77/   88]
per-ex loss: 0.533099  [   78/   88]
per-ex loss: 0.651796  [   79/   88]
per-ex loss: 0.373089  [   80/   88]
per-ex loss: 0.572375  [   81/   88]
per-ex loss: 0.613451  [   82/   88]
per-ex loss: 0.332534  [   83/   88]
per-ex loss: 0.369790  [   84/   88]
per-ex loss: 0.477880  [   85/   88]
per-ex loss: 0.529079  [   86/   88]
per-ex loss: 0.367356  [   87/   88]
per-ex loss: 0.354558  [   88/   88]
Train Error: Avg loss: 0.46232123
validation Error: 
 Avg loss: 0.52360537 
 F1: 0.506878 
 Precision: 0.616532 
 Recall: 0.430339
 IoU: 0.339475

test Error: 
 Avg loss: 0.49164070 
 F1: 0.556363 
 Precision: 0.625398 
 Recall: 0.501055
 IoU: 0.385390

We have finished training iteration 71
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_61_.pth
per-ex loss: 0.584752  [    1/   88]
per-ex loss: 0.358716  [    2/   88]
per-ex loss: 0.544169  [    3/   88]
per-ex loss: 0.392391  [    4/   88]
per-ex loss: 0.570585  [    5/   88]
per-ex loss: 0.475672  [    6/   88]
per-ex loss: 0.399857  [    7/   88]
per-ex loss: 0.550977  [    8/   88]
per-ex loss: 0.569968  [    9/   88]
per-ex loss: 0.304029  [   10/   88]
per-ex loss: 0.393944  [   11/   88]
per-ex loss: 0.622076  [   12/   88]
per-ex loss: 0.321923  [   13/   88]
per-ex loss: 0.368376  [   14/   88]
per-ex loss: 0.395031  [   15/   88]
per-ex loss: 0.534862  [   16/   88]
per-ex loss: 0.343657  [   17/   88]
per-ex loss: 0.492811  [   18/   88]
per-ex loss: 0.369541  [   19/   88]
per-ex loss: 0.414862  [   20/   88]
per-ex loss: 0.384510  [   21/   88]
per-ex loss: 0.400789  [   22/   88]
per-ex loss: 0.557596  [   23/   88]
per-ex loss: 0.533762  [   24/   88]
per-ex loss: 0.328147  [   25/   88]
per-ex loss: 0.350328  [   26/   88]
per-ex loss: 0.532132  [   27/   88]
per-ex loss: 0.387375  [   28/   88]
per-ex loss: 0.451335  [   29/   88]
per-ex loss: 0.379590  [   30/   88]
per-ex loss: 0.521294  [   31/   88]
per-ex loss: 0.378565  [   32/   88]
per-ex loss: 0.396342  [   33/   88]
per-ex loss: 0.533670  [   34/   88]
per-ex loss: 0.343746  [   35/   88]
per-ex loss: 0.364639  [   36/   88]
per-ex loss: 0.532951  [   37/   88]
per-ex loss: 0.434856  [   38/   88]
per-ex loss: 0.503527  [   39/   88]
per-ex loss: 0.668652  [   40/   88]
per-ex loss: 0.467079  [   41/   88]
per-ex loss: 0.480407  [   42/   88]
per-ex loss: 0.585202  [   43/   88]
per-ex loss: 0.413238  [   44/   88]
per-ex loss: 0.538133  [   45/   88]
per-ex loss: 0.644089  [   46/   88]
per-ex loss: 0.453746  [   47/   88]
per-ex loss: 0.426716  [   48/   88]
per-ex loss: 0.607416  [   49/   88]
per-ex loss: 0.448562  [   50/   88]
per-ex loss: 0.602773  [   51/   88]
per-ex loss: 0.372171  [   52/   88]
per-ex loss: 0.541347  [   53/   88]
per-ex loss: 0.280727  [   54/   88]
per-ex loss: 0.379514  [   55/   88]
per-ex loss: 0.553623  [   56/   88]
per-ex loss: 0.335064  [   57/   88]
per-ex loss: 0.713995  [   58/   88]
per-ex loss: 0.596375  [   59/   88]
per-ex loss: 0.661444  [   60/   88]
per-ex loss: 0.355277  [   61/   88]
per-ex loss: 0.340182  [   62/   88]
per-ex loss: 0.414266  [   63/   88]
per-ex loss: 0.373139  [   64/   88]
per-ex loss: 0.353874  [   65/   88]
per-ex loss: 0.523685  [   66/   88]
per-ex loss: 0.515282  [   67/   88]
per-ex loss: 0.367619  [   68/   88]
per-ex loss: 0.430445  [   69/   88]
per-ex loss: 0.561818  [   70/   88]
per-ex loss: 0.418963  [   71/   88]
per-ex loss: 0.390637  [   72/   88]
per-ex loss: 0.458683  [   73/   88]
per-ex loss: 0.410229  [   74/   88]
per-ex loss: 0.417272  [   75/   88]
per-ex loss: 0.591784  [   76/   88]
per-ex loss: 0.353764  [   77/   88]
per-ex loss: 0.367819  [   78/   88]
per-ex loss: 0.583584  [   79/   88]
per-ex loss: 0.665924  [   80/   88]
per-ex loss: 0.323202  [   81/   88]
per-ex loss: 0.640752  [   82/   88]
per-ex loss: 0.300878  [   83/   88]
per-ex loss: 0.569241  [   84/   88]
per-ex loss: 0.599643  [   85/   88]
per-ex loss: 0.401213  [   86/   88]
per-ex loss: 0.496253  [   87/   88]
per-ex loss: 0.483465  [   88/   88]
Train Error: Avg loss: 0.46366498
validation Error: 
 Avg loss: 0.52615666 
 F1: 0.499311 
 Precision: 0.560879 
 Recall: 0.449922
 IoU: 0.332721

test Error: 
 Avg loss: 0.48870026 
 F1: 0.558504 
 Precision: 0.584722 
 Recall: 0.534537
 IoU: 0.387448

We have finished training iteration 72
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_70_.pth
per-ex loss: 0.397130  [    1/   88]
per-ex loss: 0.564884  [    2/   88]
per-ex loss: 0.459629  [    3/   88]
per-ex loss: 0.372545  [    4/   88]
per-ex loss: 0.577252  [    5/   88]
per-ex loss: 0.609423  [    6/   88]
per-ex loss: 0.310186  [    7/   88]
per-ex loss: 0.621860  [    8/   88]
per-ex loss: 0.346674  [    9/   88]
per-ex loss: 0.389680  [   10/   88]
per-ex loss: 0.555569  [   11/   88]
per-ex loss: 0.347549  [   12/   88]
per-ex loss: 0.583271  [   13/   88]
per-ex loss: 0.630264  [   14/   88]
per-ex loss: 0.498119  [   15/   88]
per-ex loss: 0.481347  [   16/   88]
per-ex loss: 0.516773  [   17/   88]
per-ex loss: 0.399212  [   18/   88]
per-ex loss: 0.382696  [   19/   88]
per-ex loss: 0.405230  [   20/   88]
per-ex loss: 0.364087  [   21/   88]
per-ex loss: 0.410794  [   22/   88]
per-ex loss: 0.525545  [   23/   88]
per-ex loss: 0.363055  [   24/   88]
per-ex loss: 0.476936  [   25/   88]
per-ex loss: 0.326240  [   26/   88]
per-ex loss: 0.316409  [   27/   88]
per-ex loss: 0.648254  [   28/   88]
per-ex loss: 0.316150  [   29/   88]
per-ex loss: 0.658955  [   30/   88]
per-ex loss: 0.554589  [   31/   88]
per-ex loss: 0.350602  [   32/   88]
per-ex loss: 0.579836  [   33/   88]
per-ex loss: 0.426079  [   34/   88]
per-ex loss: 0.374733  [   35/   88]
per-ex loss: 0.565465  [   36/   88]
per-ex loss: 0.412068  [   37/   88]
per-ex loss: 0.334725  [   38/   88]
per-ex loss: 0.575219  [   39/   88]
per-ex loss: 0.586710  [   40/   88]
per-ex loss: 0.377348  [   41/   88]
per-ex loss: 0.490863  [   42/   88]
per-ex loss: 0.457388  [   43/   88]
per-ex loss: 0.429436  [   44/   88]
per-ex loss: 0.493175  [   45/   88]
per-ex loss: 0.389819  [   46/   88]
per-ex loss: 0.469748  [   47/   88]
per-ex loss: 0.567722  [   48/   88]
per-ex loss: 0.245404  [   49/   88]
per-ex loss: 0.334733  [   50/   88]
per-ex loss: 0.664841  [   51/   88]
per-ex loss: 0.493207  [   52/   88]
per-ex loss: 0.417923  [   53/   88]
per-ex loss: 0.640492  [   54/   88]
per-ex loss: 0.510358  [   55/   88]
per-ex loss: 0.443649  [   56/   88]
per-ex loss: 0.375820  [   57/   88]
per-ex loss: 0.340403  [   58/   88]
per-ex loss: 0.411838  [   59/   88]
per-ex loss: 0.399305  [   60/   88]
per-ex loss: 0.405363  [   61/   88]
per-ex loss: 0.523841  [   62/   88]
per-ex loss: 0.508295  [   63/   88]
per-ex loss: 0.415528  [   64/   88]
per-ex loss: 0.304671  [   65/   88]
per-ex loss: 0.341631  [   66/   88]
per-ex loss: 0.401403  [   67/   88]
per-ex loss: 0.606633  [   68/   88]
per-ex loss: 0.595158  [   69/   88]
per-ex loss: 0.353729  [   70/   88]
per-ex loss: 0.424601  [   71/   88]
per-ex loss: 0.404830  [   72/   88]
per-ex loss: 0.389022  [   73/   88]
per-ex loss: 0.591648  [   74/   88]
per-ex loss: 0.454354  [   75/   88]
per-ex loss: 0.522311  [   76/   88]
per-ex loss: 0.516219  [   77/   88]
per-ex loss: 0.367908  [   78/   88]
per-ex loss: 0.323912  [   79/   88]
per-ex loss: 0.432779  [   80/   88]
per-ex loss: 0.559688  [   81/   88]
per-ex loss: 0.478653  [   82/   88]
per-ex loss: 0.383549  [   83/   88]
per-ex loss: 0.590007  [   84/   88]
per-ex loss: 0.595019  [   85/   88]
per-ex loss: 0.658212  [   86/   88]
per-ex loss: 0.510832  [   87/   88]
per-ex loss: 0.482387  [   88/   88]
Train Error: Avg loss: 0.46262953
validation Error: 
 Avg loss: 0.55795096 
 F1: 0.466029 
 Precision: 0.714968 
 Recall: 0.345672
 IoU: 0.303805

test Error: 
 Avg loss: 0.51774994 
 F1: 0.527448 
 Precision: 0.727726 
 Recall: 0.413616
 IoU: 0.358186

We have finished training iteration 73
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_71_.pth
per-ex loss: 0.499247  [    1/   88]
per-ex loss: 0.513910  [    2/   88]
per-ex loss: 0.328981  [    3/   88]
per-ex loss: 0.548193  [    4/   88]
per-ex loss: 0.655974  [    5/   88]
per-ex loss: 0.375584  [    6/   88]
per-ex loss: 0.612860  [    7/   88]
per-ex loss: 0.546876  [    8/   88]
per-ex loss: 0.400145  [    9/   88]
per-ex loss: 0.388513  [   10/   88]
per-ex loss: 0.616308  [   11/   88]
per-ex loss: 0.562232  [   12/   88]
per-ex loss: 0.427301  [   13/   88]
per-ex loss: 0.587364  [   14/   88]
per-ex loss: 0.437036  [   15/   88]
per-ex loss: 0.344405  [   16/   88]
per-ex loss: 0.510461  [   17/   88]
per-ex loss: 0.340445  [   18/   88]
per-ex loss: 0.393681  [   19/   88]
per-ex loss: 0.325642  [   20/   88]
per-ex loss: 0.320743  [   21/   88]
per-ex loss: 0.348984  [   22/   88]
per-ex loss: 0.660962  [   23/   88]
per-ex loss: 0.455383  [   24/   88]
per-ex loss: 0.677387  [   25/   88]
per-ex loss: 0.432678  [   26/   88]
per-ex loss: 0.405568  [   27/   88]
per-ex loss: 0.588915  [   28/   88]
per-ex loss: 0.368743  [   29/   88]
per-ex loss: 0.365291  [   30/   88]
per-ex loss: 0.530263  [   31/   88]
per-ex loss: 0.363110  [   32/   88]
per-ex loss: 0.620807  [   33/   88]
per-ex loss: 0.351638  [   34/   88]
per-ex loss: 0.564542  [   35/   88]
per-ex loss: 0.540193  [   36/   88]
per-ex loss: 0.377061  [   37/   88]
per-ex loss: 0.416361  [   38/   88]
per-ex loss: 0.437571  [   39/   88]
per-ex loss: 0.491140  [   40/   88]
per-ex loss: 0.566385  [   41/   88]
per-ex loss: 0.338068  [   42/   88]
per-ex loss: 0.480839  [   43/   88]
per-ex loss: 0.494123  [   44/   88]
per-ex loss: 0.397662  [   45/   88]
per-ex loss: 0.343102  [   46/   88]
per-ex loss: 0.537528  [   47/   88]
per-ex loss: 0.386373  [   48/   88]
per-ex loss: 0.451476  [   49/   88]
per-ex loss: 0.383627  [   50/   88]
per-ex loss: 0.609123  [   51/   88]
per-ex loss: 0.588304  [   52/   88]
per-ex loss: 0.375153  [   53/   88]
per-ex loss: 0.401084  [   54/   88]
per-ex loss: 0.387398  [   55/   88]
per-ex loss: 0.462665  [   56/   88]
per-ex loss: 0.569109  [   57/   88]
per-ex loss: 0.583939  [   58/   88]
per-ex loss: 0.365141  [   59/   88]
per-ex loss: 0.547256  [   60/   88]
per-ex loss: 0.650855  [   61/   88]
per-ex loss: 0.341835  [   62/   88]
per-ex loss: 0.527122  [   63/   88]
per-ex loss: 0.341713  [   64/   88]
per-ex loss: 0.538484  [   65/   88]
per-ex loss: 0.560181  [   66/   88]
per-ex loss: 0.420231  [   67/   88]
per-ex loss: 0.356869  [   68/   88]
per-ex loss: 0.667878  [   69/   88]
per-ex loss: 0.417099  [   70/   88]
per-ex loss: 0.429919  [   71/   88]
per-ex loss: 0.349373  [   72/   88]
per-ex loss: 0.447386  [   73/   88]
per-ex loss: 0.314022  [   74/   88]
per-ex loss: 0.307226  [   75/   88]
per-ex loss: 0.491006  [   76/   88]
per-ex loss: 0.486556  [   77/   88]
per-ex loss: 0.715160  [   78/   88]
per-ex loss: 0.543832  [   79/   88]
per-ex loss: 0.414276  [   80/   88]
per-ex loss: 0.375682  [   81/   88]
per-ex loss: 0.580046  [   82/   88]
per-ex loss: 0.422282  [   83/   88]
per-ex loss: 0.386613  [   84/   88]
per-ex loss: 0.437319  [   85/   88]
per-ex loss: 0.628084  [   86/   88]
per-ex loss: 0.581709  [   87/   88]
per-ex loss: 0.463072  [   88/   88]
Train Error: Avg loss: 0.46814412
validation Error: 
 Avg loss: 0.53036771 
 F1: 0.494802 
 Precision: 0.548736 
 Recall: 0.450522
 IoU: 0.328729

test Error: 
 Avg loss: 0.50101388 
 F1: 0.540476 
 Precision: 0.587799 
 Recall: 0.500206
 IoU: 0.370310

We have finished training iteration 74
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_72_.pth
per-ex loss: 0.368052  [    1/   88]
per-ex loss: 0.346349  [    2/   88]
per-ex loss: 0.415400  [    3/   88]
per-ex loss: 0.338643  [    4/   88]
per-ex loss: 0.351290  [    5/   88]
per-ex loss: 0.360340  [    6/   88]
per-ex loss: 0.393714  [    7/   88]
per-ex loss: 0.338531  [    8/   88]
per-ex loss: 0.378384  [    9/   88]
per-ex loss: 0.579232  [   10/   88]
per-ex loss: 0.516959  [   11/   88]
per-ex loss: 0.340668  [   12/   88]
per-ex loss: 0.540223  [   13/   88]
per-ex loss: 0.336940  [   14/   88]
per-ex loss: 0.513476  [   15/   88]
per-ex loss: 0.389405  [   16/   88]
per-ex loss: 0.569504  [   17/   88]
per-ex loss: 0.407244  [   18/   88]
per-ex loss: 0.349143  [   19/   88]
per-ex loss: 0.373035  [   20/   88]
per-ex loss: 0.339651  [   21/   88]
per-ex loss: 0.310037  [   22/   88]
per-ex loss: 0.586744  [   23/   88]
per-ex loss: 0.519022  [   24/   88]
per-ex loss: 0.473977  [   25/   88]
per-ex loss: 0.515251  [   26/   88]
per-ex loss: 0.595108  [   27/   88]
per-ex loss: 0.607208  [   28/   88]
per-ex loss: 0.399016  [   29/   88]
per-ex loss: 0.365046  [   30/   88]
per-ex loss: 0.425483  [   31/   88]
per-ex loss: 0.327150  [   32/   88]
per-ex loss: 0.456438  [   33/   88]
per-ex loss: 0.376122  [   34/   88]
per-ex loss: 0.397657  [   35/   88]
per-ex loss: 0.295163  [   36/   88]
per-ex loss: 0.489785  [   37/   88]
per-ex loss: 0.479728  [   38/   88]
per-ex loss: 0.416552  [   39/   88]
per-ex loss: 0.685212  [   40/   88]
per-ex loss: 0.495841  [   41/   88]
per-ex loss: 0.675504  [   42/   88]
per-ex loss: 0.640782  [   43/   88]
per-ex loss: 0.590518  [   44/   88]
per-ex loss: 0.527960  [   45/   88]
per-ex loss: 0.509422  [   46/   88]
per-ex loss: 0.629293  [   47/   88]
per-ex loss: 0.573270  [   48/   88]
per-ex loss: 0.387545  [   49/   88]
per-ex loss: 0.447075  [   50/   88]
per-ex loss: 0.528099  [   51/   88]
per-ex loss: 0.590050  [   52/   88]
per-ex loss: 0.426164  [   53/   88]
per-ex loss: 0.642475  [   54/   88]
per-ex loss: 0.648264  [   55/   88]
per-ex loss: 0.636618  [   56/   88]
per-ex loss: 0.336163  [   57/   88]
per-ex loss: 0.329955  [   58/   88]
per-ex loss: 0.339420  [   59/   88]
per-ex loss: 0.604141  [   60/   88]
per-ex loss: 0.393415  [   61/   88]
per-ex loss: 0.330142  [   62/   88]
per-ex loss: 0.386898  [   63/   88]
per-ex loss: 0.482781  [   64/   88]
per-ex loss: 0.498337  [   65/   88]
per-ex loss: 0.543414  [   66/   88]
per-ex loss: 0.397125  [   67/   88]
per-ex loss: 0.390357  [   68/   88]
per-ex loss: 0.352488  [   69/   88]
per-ex loss: 0.578060  [   70/   88]
per-ex loss: 0.489652  [   71/   88]
per-ex loss: 0.521067  [   72/   88]
per-ex loss: 0.554465  [   73/   88]
per-ex loss: 0.388419  [   74/   88]
per-ex loss: 0.471243  [   75/   88]
per-ex loss: 0.433727  [   76/   88]
per-ex loss: 0.406959  [   77/   88]
per-ex loss: 0.611134  [   78/   88]
per-ex loss: 0.561421  [   79/   88]
per-ex loss: 0.407917  [   80/   88]
per-ex loss: 0.386515  [   81/   88]
per-ex loss: 0.535277  [   82/   88]
per-ex loss: 0.414851  [   83/   88]
per-ex loss: 0.555508  [   84/   88]
per-ex loss: 0.402188  [   85/   88]
per-ex loss: 0.499480  [   86/   88]
per-ex loss: 0.488124  [   87/   88]
per-ex loss: 0.571815  [   88/   88]
Train Error: Avg loss: 0.46452523
validation Error: 
 Avg loss: 0.53273663 
 F1: 0.491867 
 Precision: 0.649509 
 Recall: 0.395802
 IoU: 0.326143

test Error: 
 Avg loss: 0.49147217 
 F1: 0.560905 
 Precision: 0.666223 
 Recall: 0.484339
 IoU: 0.389762

We have finished training iteration 75
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_73_.pth
per-ex loss: 0.304008  [    1/   88]
per-ex loss: 0.368569  [    2/   88]
per-ex loss: 0.342079  [    3/   88]
per-ex loss: 0.321500  [    4/   88]
per-ex loss: 0.549382  [    5/   88]
per-ex loss: 0.581035  [    6/   88]
per-ex loss: 0.436518  [    7/   88]
per-ex loss: 0.356965  [    8/   88]
per-ex loss: 0.625689  [    9/   88]
per-ex loss: 0.419308  [   10/   88]
per-ex loss: 0.511513  [   11/   88]
per-ex loss: 0.554360  [   12/   88]
per-ex loss: 0.404790  [   13/   88]
per-ex loss: 0.376338  [   14/   88]
per-ex loss: 0.379427  [   15/   88]
per-ex loss: 0.529125  [   16/   88]
per-ex loss: 0.661562  [   17/   88]
per-ex loss: 0.639680  [   18/   88]
per-ex loss: 0.514185  [   19/   88]
per-ex loss: 0.373097  [   20/   88]
per-ex loss: 0.428213  [   21/   88]
per-ex loss: 0.639002  [   22/   88]
per-ex loss: 0.382927  [   23/   88]
per-ex loss: 0.651231  [   24/   88]
per-ex loss: 0.440600  [   25/   88]
per-ex loss: 0.529519  [   26/   88]
per-ex loss: 0.469089  [   27/   88]
per-ex loss: 0.593537  [   28/   88]
per-ex loss: 0.435694  [   29/   88]
per-ex loss: 0.559653  [   30/   88]
per-ex loss: 0.362173  [   31/   88]
per-ex loss: 0.343174  [   32/   88]
per-ex loss: 0.372367  [   33/   88]
per-ex loss: 0.362630  [   34/   88]
per-ex loss: 0.480726  [   35/   88]
per-ex loss: 0.335014  [   36/   88]
per-ex loss: 0.548114  [   37/   88]
per-ex loss: 0.608742  [   38/   88]
per-ex loss: 0.416405  [   39/   88]
per-ex loss: 0.407764  [   40/   88]
per-ex loss: 0.663996  [   41/   88]
per-ex loss: 0.478824  [   42/   88]
per-ex loss: 0.403748  [   43/   88]
per-ex loss: 0.417854  [   44/   88]
per-ex loss: 0.561988  [   45/   88]
per-ex loss: 0.379207  [   46/   88]
per-ex loss: 0.618219  [   47/   88]
per-ex loss: 0.416445  [   48/   88]
per-ex loss: 0.324387  [   49/   88]
per-ex loss: 0.551594  [   50/   88]
per-ex loss: 0.388555  [   51/   88]
per-ex loss: 0.535089  [   52/   88]
per-ex loss: 0.570579  [   53/   88]
per-ex loss: 0.425377  [   54/   88]
per-ex loss: 0.568057  [   55/   88]
per-ex loss: 0.470437  [   56/   88]
per-ex loss: 0.476611  [   57/   88]
per-ex loss: 0.533904  [   58/   88]
per-ex loss: 0.586985  [   59/   88]
per-ex loss: 0.330697  [   60/   88]
per-ex loss: 0.541533  [   61/   88]
per-ex loss: 0.345245  [   62/   88]
per-ex loss: 0.506574  [   63/   88]
per-ex loss: 0.406414  [   64/   88]
per-ex loss: 0.365024  [   65/   88]
per-ex loss: 0.358120  [   66/   88]
per-ex loss: 0.280459  [   67/   88]
per-ex loss: 0.408767  [   68/   88]
per-ex loss: 0.332213  [   69/   88]
per-ex loss: 0.449554  [   70/   88]
per-ex loss: 0.482445  [   71/   88]
per-ex loss: 0.584486  [   72/   88]
per-ex loss: 0.362955  [   73/   88]
per-ex loss: 0.313372  [   74/   88]
per-ex loss: 0.565829  [   75/   88]
per-ex loss: 0.402705  [   76/   88]
per-ex loss: 0.618959  [   77/   88]
per-ex loss: 0.320200  [   78/   88]
per-ex loss: 0.404770  [   79/   88]
per-ex loss: 0.583932  [   80/   88]
per-ex loss: 0.498010  [   81/   88]
per-ex loss: 0.388398  [   82/   88]
per-ex loss: 0.340160  [   83/   88]
per-ex loss: 0.411879  [   84/   88]
per-ex loss: 0.314065  [   85/   88]
per-ex loss: 0.334824  [   86/   88]
per-ex loss: 0.651606  [   87/   88]
per-ex loss: 0.481207  [   88/   88]
Train Error: Avg loss: 0.45877231
validation Error: 
 Avg loss: 0.52017494 
 F1: 0.508452 
 Precision: 0.610889 
 Recall: 0.435436
 IoU: 0.340889

test Error: 
 Avg loss: 0.48832664 
 F1: 0.562555 
 Precision: 0.630281 
 Recall: 0.507972
 IoU: 0.391357

We have finished training iteration 76
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_74_.pth
per-ex loss: 0.396566  [    1/   88]
per-ex loss: 0.668802  [    2/   88]
per-ex loss: 0.579401  [    3/   88]
per-ex loss: 0.445801  [    4/   88]
per-ex loss: 0.366354  [    5/   88]
per-ex loss: 0.455861  [    6/   88]
per-ex loss: 0.572562  [    7/   88]
per-ex loss: 0.407032  [    8/   88]
per-ex loss: 0.574113  [    9/   88]
per-ex loss: 0.603168  [   10/   88]
per-ex loss: 0.390227  [   11/   88]
per-ex loss: 0.349515  [   12/   88]
per-ex loss: 0.486229  [   13/   88]
per-ex loss: 0.553686  [   14/   88]
per-ex loss: 0.408776  [   15/   88]
per-ex loss: 0.368730  [   16/   88]
per-ex loss: 0.569177  [   17/   88]
per-ex loss: 0.357364  [   18/   88]
per-ex loss: 0.451317  [   19/   88]
per-ex loss: 0.525945  [   20/   88]
per-ex loss: 0.409430  [   21/   88]
per-ex loss: 0.379787  [   22/   88]
per-ex loss: 0.470598  [   23/   88]
per-ex loss: 0.480141  [   24/   88]
per-ex loss: 0.422836  [   25/   88]
per-ex loss: 0.489300  [   26/   88]
per-ex loss: 0.531489  [   27/   88]
per-ex loss: 0.547597  [   28/   88]
per-ex loss: 0.533086  [   29/   88]
per-ex loss: 0.629650  [   30/   88]
per-ex loss: 0.574984  [   31/   88]
per-ex loss: 0.376383  [   32/   88]
per-ex loss: 0.437182  [   33/   88]
per-ex loss: 0.615147  [   34/   88]
per-ex loss: 0.434122  [   35/   88]
per-ex loss: 0.442387  [   36/   88]
per-ex loss: 0.567355  [   37/   88]
per-ex loss: 0.649297  [   38/   88]
per-ex loss: 0.305168  [   39/   88]
per-ex loss: 0.654432  [   40/   88]
per-ex loss: 0.351799  [   41/   88]
per-ex loss: 0.356812  [   42/   88]
per-ex loss: 0.398010  [   43/   88]
per-ex loss: 0.446747  [   44/   88]
per-ex loss: 0.504045  [   45/   88]
per-ex loss: 0.586394  [   46/   88]
per-ex loss: 0.398597  [   47/   88]
per-ex loss: 0.382563  [   48/   88]
per-ex loss: 0.484422  [   49/   88]
per-ex loss: 0.370447  [   50/   88]
per-ex loss: 0.345815  [   51/   88]
per-ex loss: 0.521858  [   52/   88]
per-ex loss: 0.363452  [   53/   88]
per-ex loss: 0.428655  [   54/   88]
per-ex loss: 0.530724  [   55/   88]
per-ex loss: 0.574528  [   56/   88]
per-ex loss: 0.374453  [   57/   88]
per-ex loss: 0.401152  [   58/   88]
per-ex loss: 0.342783  [   59/   88]
per-ex loss: 0.323123  [   60/   88]
per-ex loss: 0.397434  [   61/   88]
per-ex loss: 0.287789  [   62/   88]
per-ex loss: 0.675620  [   63/   88]
per-ex loss: 0.629234  [   64/   88]
per-ex loss: 0.383980  [   65/   88]
per-ex loss: 0.388867  [   66/   88]
per-ex loss: 0.339861  [   67/   88]
per-ex loss: 0.383492  [   68/   88]
per-ex loss: 0.356528  [   69/   88]
per-ex loss: 0.580116  [   70/   88]
per-ex loss: 0.584300  [   71/   88]
per-ex loss: 0.325339  [   72/   88]
per-ex loss: 0.318287  [   73/   88]
per-ex loss: 0.474570  [   74/   88]
per-ex loss: 0.354823  [   75/   88]
per-ex loss: 0.630769  [   76/   88]
per-ex loss: 0.551200  [   77/   88]
per-ex loss: 0.429542  [   78/   88]
per-ex loss: 0.619846  [   79/   88]
per-ex loss: 0.466353  [   80/   88]
per-ex loss: 0.540236  [   81/   88]
per-ex loss: 0.567099  [   82/   88]
per-ex loss: 0.374252  [   83/   88]
per-ex loss: 0.396266  [   84/   88]
per-ex loss: 0.476270  [   85/   88]
per-ex loss: 0.322495  [   86/   88]
per-ex loss: 0.480669  [   87/   88]
per-ex loss: 0.371053  [   88/   88]
Train Error: Avg loss: 0.46220075
validation Error: 
 Avg loss: 0.52366580 
 F1: 0.503167 
 Precision: 0.544730 
 Recall: 0.467498
 IoU: 0.336155

test Error: 
 Avg loss: 0.48763552 
 F1: 0.554994 
 Precision: 0.559126 
 Recall: 0.550922
 IoU: 0.384077

We have finished training iteration 77
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_75_.pth
per-ex loss: 0.324725  [    1/   88]
per-ex loss: 0.374231  [    2/   88]
per-ex loss: 0.416247  [    3/   88]
per-ex loss: 0.334526  [    4/   88]
per-ex loss: 0.566069  [    5/   88]
per-ex loss: 0.589965  [    6/   88]
per-ex loss: 0.526959  [    7/   88]
per-ex loss: 0.573748  [    8/   88]
per-ex loss: 0.424329  [    9/   88]
per-ex loss: 0.570745  [   10/   88]
per-ex loss: 0.390401  [   11/   88]
per-ex loss: 0.364516  [   12/   88]
per-ex loss: 0.415155  [   13/   88]
per-ex loss: 0.615258  [   14/   88]
per-ex loss: 0.378551  [   15/   88]
per-ex loss: 0.469153  [   16/   88]
per-ex loss: 0.542386  [   17/   88]
per-ex loss: 0.551951  [   18/   88]
per-ex loss: 0.558990  [   19/   88]
per-ex loss: 0.340859  [   20/   88]
per-ex loss: 0.480842  [   21/   88]
per-ex loss: 0.396251  [   22/   88]
per-ex loss: 0.338695  [   23/   88]
per-ex loss: 0.511492  [   24/   88]
per-ex loss: 0.524498  [   25/   88]
per-ex loss: 0.521201  [   26/   88]
per-ex loss: 0.401595  [   27/   88]
per-ex loss: 0.665284  [   28/   88]
per-ex loss: 0.649339  [   29/   88]
per-ex loss: 0.479347  [   30/   88]
per-ex loss: 0.634817  [   31/   88]
per-ex loss: 0.348253  [   32/   88]
per-ex loss: 0.334787  [   33/   88]
per-ex loss: 0.638100  [   34/   88]
per-ex loss: 0.374526  [   35/   88]
per-ex loss: 0.411485  [   36/   88]
per-ex loss: 0.570939  [   37/   88]
per-ex loss: 0.435916  [   38/   88]
per-ex loss: 0.329276  [   39/   88]
per-ex loss: 0.560818  [   40/   88]
per-ex loss: 0.532895  [   41/   88]
per-ex loss: 0.399533  [   42/   88]
per-ex loss: 0.400605  [   43/   88]
per-ex loss: 0.417471  [   44/   88]
per-ex loss: 0.386286  [   45/   88]
per-ex loss: 0.324310  [   46/   88]
per-ex loss: 0.579176  [   47/   88]
per-ex loss: 0.398125  [   48/   88]
per-ex loss: 0.628510  [   49/   88]
per-ex loss: 0.645709  [   50/   88]
per-ex loss: 0.545946  [   51/   88]
per-ex loss: 0.434561  [   52/   88]
per-ex loss: 0.538568  [   53/   88]
per-ex loss: 0.438236  [   54/   88]
per-ex loss: 0.429833  [   55/   88]
per-ex loss: 0.367013  [   56/   88]
per-ex loss: 0.615277  [   57/   88]
per-ex loss: 0.516192  [   58/   88]
per-ex loss: 0.353357  [   59/   88]
per-ex loss: 0.346787  [   60/   88]
per-ex loss: 0.420466  [   61/   88]
per-ex loss: 0.382715  [   62/   88]
per-ex loss: 0.431577  [   63/   88]
per-ex loss: 0.374630  [   64/   88]
per-ex loss: 0.342226  [   65/   88]
per-ex loss: 0.316215  [   66/   88]
per-ex loss: 0.378199  [   67/   88]
per-ex loss: 0.468731  [   68/   88]
per-ex loss: 0.520345  [   69/   88]
per-ex loss: 0.657240  [   70/   88]
per-ex loss: 0.536570  [   71/   88]
per-ex loss: 0.403350  [   72/   88]
per-ex loss: 0.333083  [   73/   88]
per-ex loss: 0.543726  [   74/   88]
per-ex loss: 0.371643  [   75/   88]
per-ex loss: 0.440873  [   76/   88]
per-ex loss: 0.387414  [   77/   88]
per-ex loss: 0.570540  [   78/   88]
per-ex loss: 0.405792  [   79/   88]
per-ex loss: 0.500820  [   80/   88]
per-ex loss: 0.598820  [   81/   88]
per-ex loss: 0.578768  [   82/   88]
per-ex loss: 0.289192  [   83/   88]
per-ex loss: 0.494410  [   84/   88]
per-ex loss: 0.549716  [   85/   88]
per-ex loss: 0.335142  [   86/   88]
per-ex loss: 0.380367  [   87/   88]
per-ex loss: 0.483672  [   88/   88]
Train Error: Avg loss: 0.46285063
validation Error: 
 Avg loss: 0.53364135 
 F1: 0.496166 
 Precision: 0.612526 
 Recall: 0.416957
 IoU: 0.329934

test Error: 
 Avg loss: 0.48641381 
 F1: 0.563385 
 Precision: 0.669787 
 Recall: 0.486155
 IoU: 0.392162

We have finished training iteration 78
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_60_.pth
per-ex loss: 0.398772  [    1/   88]
per-ex loss: 0.330682  [    2/   88]
per-ex loss: 0.331346  [    3/   88]
per-ex loss: 0.365902  [    4/   88]
per-ex loss: 0.343599  [    5/   88]
per-ex loss: 0.426543  [    6/   88]
per-ex loss: 0.379138  [    7/   88]
per-ex loss: 0.565377  [    8/   88]
per-ex loss: 0.492310  [    9/   88]
per-ex loss: 0.521798  [   10/   88]
per-ex loss: 0.558890  [   11/   88]
per-ex loss: 0.359022  [   12/   88]
per-ex loss: 0.343481  [   13/   88]
per-ex loss: 0.564847  [   14/   88]
per-ex loss: 0.567946  [   15/   88]
per-ex loss: 0.338978  [   16/   88]
per-ex loss: 0.379413  [   17/   88]
per-ex loss: 0.426846  [   18/   88]
per-ex loss: 0.566898  [   19/   88]
per-ex loss: 0.565234  [   20/   88]
per-ex loss: 0.623914  [   21/   88]
per-ex loss: 0.393338  [   22/   88]
per-ex loss: 0.405809  [   23/   88]
per-ex loss: 0.545059  [   24/   88]
per-ex loss: 0.385837  [   25/   88]
per-ex loss: 0.572163  [   26/   88]
per-ex loss: 0.347026  [   27/   88]
per-ex loss: 0.357542  [   28/   88]
per-ex loss: 0.576092  [   29/   88]
per-ex loss: 0.512597  [   30/   88]
per-ex loss: 0.414218  [   31/   88]
per-ex loss: 0.486876  [   32/   88]
per-ex loss: 0.325662  [   33/   88]
per-ex loss: 0.311861  [   34/   88]
per-ex loss: 0.595050  [   35/   88]
per-ex loss: 0.343548  [   36/   88]
per-ex loss: 0.608919  [   37/   88]
per-ex loss: 0.559508  [   38/   88]
per-ex loss: 0.368267  [   39/   88]
per-ex loss: 0.620313  [   40/   88]
per-ex loss: 0.568936  [   41/   88]
per-ex loss: 0.467865  [   42/   88]
per-ex loss: 0.389880  [   43/   88]
per-ex loss: 0.403986  [   44/   88]
per-ex loss: 0.361557  [   45/   88]
per-ex loss: 0.677279  [   46/   88]
per-ex loss: 0.563427  [   47/   88]
per-ex loss: 0.621702  [   48/   88]
per-ex loss: 0.332613  [   49/   88]
per-ex loss: 0.395802  [   50/   88]
per-ex loss: 0.261875  [   51/   88]
per-ex loss: 0.560010  [   52/   88]
per-ex loss: 0.410601  [   53/   88]
per-ex loss: 0.427639  [   54/   88]
per-ex loss: 0.410950  [   55/   88]
per-ex loss: 0.458916  [   56/   88]
per-ex loss: 0.331863  [   57/   88]
per-ex loss: 0.475176  [   58/   88]
per-ex loss: 0.542138  [   59/   88]
per-ex loss: 0.576661  [   60/   88]
per-ex loss: 0.554619  [   61/   88]
per-ex loss: 0.512061  [   62/   88]
per-ex loss: 0.400799  [   63/   88]
per-ex loss: 0.392380  [   64/   88]
per-ex loss: 0.465118  [   65/   88]
per-ex loss: 0.419344  [   66/   88]
per-ex loss: 0.323742  [   67/   88]
per-ex loss: 0.501964  [   68/   88]
per-ex loss: 0.379863  [   69/   88]
per-ex loss: 0.344985  [   70/   88]
per-ex loss: 0.348799  [   71/   88]
per-ex loss: 0.609010  [   72/   88]
per-ex loss: 0.389910  [   73/   88]
per-ex loss: 0.654185  [   74/   88]
per-ex loss: 0.514691  [   75/   88]
per-ex loss: 0.653723  [   76/   88]
per-ex loss: 0.386406  [   77/   88]
per-ex loss: 0.454068  [   78/   88]
per-ex loss: 0.487855  [   79/   88]
per-ex loss: 0.433412  [   80/   88]
per-ex loss: 0.498633  [   81/   88]
per-ex loss: 0.641444  [   82/   88]
per-ex loss: 0.437315  [   83/   88]
per-ex loss: 0.528819  [   84/   88]
per-ex loss: 0.459698  [   85/   88]
per-ex loss: 0.439211  [   86/   88]
per-ex loss: 0.385559  [   87/   88]
per-ex loss: 0.323205  [   88/   88]
Train Error: Avg loss: 0.45863990
validation Error: 
 Avg loss: 0.51892664 
 F1: 0.508362 
 Precision: 0.550477 
 Recall: 0.472232
 IoU: 0.340807

test Error: 
 Avg loss: 0.48146928 
 F1: 0.567691 
 Precision: 0.581342 
 Recall: 0.554666
 IoU: 0.396347

We have finished training iteration 79
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_77_.pth
per-ex loss: 0.349437  [    1/   88]
per-ex loss: 0.335410  [    2/   88]
per-ex loss: 0.396329  [    3/   88]
per-ex loss: 0.475121  [    4/   88]
per-ex loss: 0.300796  [    5/   88]
per-ex loss: 0.387612  [    6/   88]
per-ex loss: 0.413904  [    7/   88]
per-ex loss: 0.420524  [    8/   88]
per-ex loss: 0.536081  [    9/   88]
per-ex loss: 0.535055  [   10/   88]
per-ex loss: 0.345781  [   11/   88]
per-ex loss: 0.580574  [   12/   88]
per-ex loss: 0.340950  [   13/   88]
per-ex loss: 0.652003  [   14/   88]
per-ex loss: 0.615996  [   15/   88]
per-ex loss: 0.356052  [   16/   88]
per-ex loss: 0.574289  [   17/   88]
per-ex loss: 0.515233  [   18/   88]
per-ex loss: 0.630487  [   19/   88]
per-ex loss: 0.569679  [   20/   88]
per-ex loss: 0.333662  [   21/   88]
per-ex loss: 0.420880  [   22/   88]
per-ex loss: 0.523348  [   23/   88]
per-ex loss: 0.296456  [   24/   88]
per-ex loss: 0.515478  [   25/   88]
per-ex loss: 0.387657  [   26/   88]
per-ex loss: 0.552505  [   27/   88]
per-ex loss: 0.380417  [   28/   88]
per-ex loss: 0.332031  [   29/   88]
per-ex loss: 0.570792  [   30/   88]
per-ex loss: 0.607102  [   31/   88]
per-ex loss: 0.434679  [   32/   88]
per-ex loss: 0.373147  [   33/   88]
per-ex loss: 0.389353  [   34/   88]
per-ex loss: 0.584151  [   35/   88]
per-ex loss: 0.416234  [   36/   88]
per-ex loss: 0.590927  [   37/   88]
per-ex loss: 0.540953  [   38/   88]
per-ex loss: 0.523810  [   39/   88]
per-ex loss: 0.541246  [   40/   88]
per-ex loss: 0.637778  [   41/   88]
per-ex loss: 0.348184  [   42/   88]
per-ex loss: 0.451543  [   43/   88]
per-ex loss: 0.500366  [   44/   88]
per-ex loss: 0.530816  [   45/   88]
per-ex loss: 0.443664  [   46/   88]
per-ex loss: 0.522280  [   47/   88]
per-ex loss: 0.341913  [   48/   88]
per-ex loss: 0.381723  [   49/   88]
per-ex loss: 0.412405  [   50/   88]
per-ex loss: 0.349856  [   51/   88]
per-ex loss: 0.646900  [   52/   88]
per-ex loss: 0.456628  [   53/   88]
per-ex loss: 0.457302  [   54/   88]
per-ex loss: 0.388484  [   55/   88]
per-ex loss: 0.421100  [   56/   88]
per-ex loss: 0.366495  [   57/   88]
per-ex loss: 0.523185  [   58/   88]
per-ex loss: 0.667430  [   59/   88]
per-ex loss: 0.367366  [   60/   88]
per-ex loss: 0.481122  [   61/   88]
per-ex loss: 0.486914  [   62/   88]
per-ex loss: 0.406763  [   63/   88]
per-ex loss: 0.268908  [   64/   88]
per-ex loss: 0.356091  [   65/   88]
per-ex loss: 0.373806  [   66/   88]
per-ex loss: 0.390001  [   67/   88]
per-ex loss: 0.358317  [   68/   88]
per-ex loss: 0.378262  [   69/   88]
per-ex loss: 0.421660  [   70/   88]
per-ex loss: 0.328202  [   71/   88]
per-ex loss: 0.397626  [   72/   88]
per-ex loss: 0.362824  [   73/   88]
per-ex loss: 0.659722  [   74/   88]
per-ex loss: 0.470817  [   75/   88]
per-ex loss: 0.538293  [   76/   88]
per-ex loss: 0.516393  [   77/   88]
per-ex loss: 0.391497  [   78/   88]
per-ex loss: 0.568532  [   79/   88]
per-ex loss: 0.631528  [   80/   88]
per-ex loss: 0.577322  [   81/   88]
per-ex loss: 0.475492  [   82/   88]
per-ex loss: 0.405822  [   83/   88]
per-ex loss: 0.392190  [   84/   88]
per-ex loss: 0.400046  [   85/   88]
per-ex loss: 0.349961  [   86/   88]
per-ex loss: 0.547850  [   87/   88]
per-ex loss: 0.580920  [   88/   88]
Train Error: Avg loss: 0.45884590
validation Error: 
 Avg loss: 0.52573557 
 F1: 0.506443 
 Precision: 0.583331 
 Recall: 0.447463
 IoU: 0.339085

test Error: 
 Avg loss: 0.48477248 
 F1: 0.558977 
 Precision: 0.632944 
 Recall: 0.500488
 IoU: 0.387902

We have finished training iteration 80
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_78_.pth
per-ex loss: 0.543742  [    1/   88]
per-ex loss: 0.465869  [    2/   88]
per-ex loss: 0.587175  [    3/   88]
per-ex loss: 0.398126  [    4/   88]
per-ex loss: 0.556642  [    5/   88]
per-ex loss: 0.307547  [    6/   88]
per-ex loss: 0.343124  [    7/   88]
per-ex loss: 0.495231  [    8/   88]
per-ex loss: 0.416548  [    9/   88]
per-ex loss: 0.504635  [   10/   88]
per-ex loss: 0.588959  [   11/   88]
per-ex loss: 0.393047  [   12/   88]
per-ex loss: 0.532560  [   13/   88]
per-ex loss: 0.367748  [   14/   88]
per-ex loss: 0.368306  [   15/   88]
per-ex loss: 0.369802  [   16/   88]
per-ex loss: 0.466149  [   17/   88]
per-ex loss: 0.399358  [   18/   88]
per-ex loss: 0.407092  [   19/   88]
per-ex loss: 0.344621  [   20/   88]
per-ex loss: 0.353135  [   21/   88]
per-ex loss: 0.401007  [   22/   88]
per-ex loss: 0.488605  [   23/   88]
per-ex loss: 0.365992  [   24/   88]
per-ex loss: 0.654446  [   25/   88]
per-ex loss: 0.330706  [   26/   88]
per-ex loss: 0.536577  [   27/   88]
per-ex loss: 0.365827  [   28/   88]
per-ex loss: 0.354326  [   29/   88]
per-ex loss: 0.427138  [   30/   88]
per-ex loss: 0.626562  [   31/   88]
per-ex loss: 0.570314  [   32/   88]
per-ex loss: 0.415042  [   33/   88]
per-ex loss: 0.470084  [   34/   88]
per-ex loss: 0.614532  [   35/   88]
per-ex loss: 0.537639  [   36/   88]
per-ex loss: 0.450457  [   37/   88]
per-ex loss: 0.428090  [   38/   88]
per-ex loss: 0.344608  [   39/   88]
per-ex loss: 0.337381  [   40/   88]
per-ex loss: 0.403604  [   41/   88]
per-ex loss: 0.610819  [   42/   88]
per-ex loss: 0.330902  [   43/   88]
per-ex loss: 0.297332  [   44/   88]
per-ex loss: 0.538439  [   45/   88]
per-ex loss: 0.555835  [   46/   88]
per-ex loss: 0.364817  [   47/   88]
per-ex loss: 0.466941  [   48/   88]
per-ex loss: 0.620861  [   49/   88]
per-ex loss: 0.554562  [   50/   88]
per-ex loss: 0.419726  [   51/   88]
per-ex loss: 0.265226  [   52/   88]
per-ex loss: 0.568609  [   53/   88]
per-ex loss: 0.563961  [   54/   88]
per-ex loss: 0.460656  [   55/   88]
per-ex loss: 0.605797  [   56/   88]
per-ex loss: 0.647737  [   57/   88]
per-ex loss: 0.385491  [   58/   88]
per-ex loss: 0.453916  [   59/   88]
per-ex loss: 0.531532  [   60/   88]
per-ex loss: 0.561449  [   61/   88]
per-ex loss: 0.489543  [   62/   88]
per-ex loss: 0.337415  [   63/   88]
per-ex loss: 0.318553  [   64/   88]
per-ex loss: 0.385408  [   65/   88]
per-ex loss: 0.496687  [   66/   88]
per-ex loss: 0.509310  [   67/   88]
per-ex loss: 0.362446  [   68/   88]
per-ex loss: 0.338304  [   69/   88]
per-ex loss: 0.345669  [   70/   88]
per-ex loss: 0.341831  [   71/   88]
per-ex loss: 0.405677  [   72/   88]
per-ex loss: 0.604576  [   73/   88]
per-ex loss: 0.396957  [   74/   88]
per-ex loss: 0.472743  [   75/   88]
per-ex loss: 0.365490  [   76/   88]
per-ex loss: 0.486881  [   77/   88]
per-ex loss: 0.345602  [   78/   88]
per-ex loss: 0.405734  [   79/   88]
per-ex loss: 0.594323  [   80/   88]
per-ex loss: 0.680430  [   81/   88]
per-ex loss: 0.362778  [   82/   88]
per-ex loss: 0.556873  [   83/   88]
per-ex loss: 0.386164  [   84/   88]
per-ex loss: 0.360249  [   85/   88]
per-ex loss: 0.560808  [   86/   88]
per-ex loss: 0.436737  [   87/   88]
per-ex loss: 0.575845  [   88/   88]
Train Error: Avg loss: 0.45522723
validation Error: 
 Avg loss: 0.52667507 
 F1: 0.499215 
 Precision: 0.618401 
 Recall: 0.418548
 IoU: 0.332636

test Error: 
 Avg loss: 0.49321072 
 F1: 0.560349 
 Precision: 0.636803 
 Recall: 0.500285
 IoU: 0.389225

We have finished training iteration 81
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_69_.pth
per-ex loss: 0.553417  [    1/   88]
per-ex loss: 0.395440  [    2/   88]
per-ex loss: 0.466887  [    3/   88]
per-ex loss: 0.519032  [    4/   88]
per-ex loss: 0.403667  [    5/   88]
per-ex loss: 0.438661  [    6/   88]
per-ex loss: 0.356710  [    7/   88]
per-ex loss: 0.425805  [    8/   88]
per-ex loss: 0.408624  [    9/   88]
per-ex loss: 0.369860  [   10/   88]
per-ex loss: 0.381869  [   11/   88]
per-ex loss: 0.546875  [   12/   88]
per-ex loss: 0.578738  [   13/   88]
per-ex loss: 0.607036  [   14/   88]
per-ex loss: 0.563732  [   15/   88]
per-ex loss: 0.655415  [   16/   88]
per-ex loss: 0.562636  [   17/   88]
per-ex loss: 0.543258  [   18/   88]
per-ex loss: 0.413000  [   19/   88]
per-ex loss: 0.590631  [   20/   88]
per-ex loss: 0.348748  [   21/   88]
per-ex loss: 0.579002  [   22/   88]
per-ex loss: 0.421563  [   23/   88]
per-ex loss: 0.377256  [   24/   88]
per-ex loss: 0.375055  [   25/   88]
per-ex loss: 0.450331  [   26/   88]
per-ex loss: 0.301087  [   27/   88]
per-ex loss: 0.510667  [   28/   88]
per-ex loss: 0.643175  [   29/   88]
per-ex loss: 0.523964  [   30/   88]
per-ex loss: 0.381486  [   31/   88]
per-ex loss: 0.240551  [   32/   88]
per-ex loss: 0.572486  [   33/   88]
per-ex loss: 0.497424  [   34/   88]
per-ex loss: 0.391543  [   35/   88]
per-ex loss: 0.430870  [   36/   88]
per-ex loss: 0.442359  [   37/   88]
per-ex loss: 0.450611  [   38/   88]
per-ex loss: 0.580532  [   39/   88]
per-ex loss: 0.376449  [   40/   88]
per-ex loss: 0.364111  [   41/   88]
per-ex loss: 0.504944  [   42/   88]
per-ex loss: 0.514615  [   43/   88]
per-ex loss: 0.587829  [   44/   88]
per-ex loss: 0.491246  [   45/   88]
per-ex loss: 0.394327  [   46/   88]
per-ex loss: 0.359084  [   47/   88]
per-ex loss: 0.381256  [   48/   88]
per-ex loss: 0.484909  [   49/   88]
per-ex loss: 0.653769  [   50/   88]
per-ex loss: 0.403861  [   51/   88]
per-ex loss: 0.361648  [   52/   88]
per-ex loss: 0.466028  [   53/   88]
per-ex loss: 0.611682  [   54/   88]
per-ex loss: 0.391490  [   55/   88]
per-ex loss: 0.578440  [   56/   88]
per-ex loss: 0.541772  [   57/   88]
per-ex loss: 0.466589  [   58/   88]
per-ex loss: 0.336436  [   59/   88]
per-ex loss: 0.295812  [   60/   88]
per-ex loss: 0.383678  [   61/   88]
per-ex loss: 0.358584  [   62/   88]
per-ex loss: 0.533968  [   63/   88]
per-ex loss: 0.340100  [   64/   88]
per-ex loss: 0.636982  [   65/   88]
per-ex loss: 0.350354  [   66/   88]
per-ex loss: 0.348891  [   67/   88]
per-ex loss: 0.462365  [   68/   88]
per-ex loss: 0.358926  [   69/   88]
per-ex loss: 0.546796  [   70/   88]
per-ex loss: 0.389553  [   71/   88]
per-ex loss: 0.531893  [   72/   88]
per-ex loss: 0.323814  [   73/   88]
per-ex loss: 0.411375  [   74/   88]
per-ex loss: 0.385996  [   75/   88]
per-ex loss: 0.388584  [   76/   88]
per-ex loss: 0.357388  [   77/   88]
per-ex loss: 0.419592  [   78/   88]
per-ex loss: 0.551388  [   79/   88]
per-ex loss: 0.603105  [   80/   88]
per-ex loss: 0.547463  [   81/   88]
per-ex loss: 0.480397  [   82/   88]
per-ex loss: 0.625832  [   83/   88]
per-ex loss: 0.470372  [   84/   88]
per-ex loss: 0.338628  [   85/   88]
per-ex loss: 0.357951  [   86/   88]
per-ex loss: 0.420116  [   87/   88]
per-ex loss: 0.317821  [   88/   88]
Train Error: Avg loss: 0.45577513
validation Error: 
 Avg loss: 0.52082547 
 F1: 0.509162 
 Precision: 0.560928 
 Recall: 0.466142
 IoU: 0.341527

test Error: 
 Avg loss: 0.47601002 
 F1: 0.572431 
 Precision: 0.596694 
 Recall: 0.550065
 IoU: 0.400984

We have finished training iteration 82
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_80_.pth
per-ex loss: 0.457348  [    1/   88]
per-ex loss: 0.464417  [    2/   88]
per-ex loss: 0.557071  [    3/   88]
per-ex loss: 0.430093  [    4/   88]
per-ex loss: 0.340390  [    5/   88]
per-ex loss: 0.363099  [    6/   88]
per-ex loss: 0.465625  [    7/   88]
per-ex loss: 0.561844  [    8/   88]
per-ex loss: 0.372094  [    9/   88]
per-ex loss: 0.372862  [   10/   88]
per-ex loss: 0.336672  [   11/   88]
per-ex loss: 0.613836  [   12/   88]
per-ex loss: 0.386515  [   13/   88]
per-ex loss: 0.399467  [   14/   88]
per-ex loss: 0.563215  [   15/   88]
per-ex loss: 0.410577  [   16/   88]
per-ex loss: 0.386233  [   17/   88]
per-ex loss: 0.347264  [   18/   88]
per-ex loss: 0.407694  [   19/   88]
per-ex loss: 0.633632  [   20/   88]
per-ex loss: 0.403497  [   21/   88]
per-ex loss: 0.324606  [   22/   88]
per-ex loss: 0.397959  [   23/   88]
per-ex loss: 0.525904  [   24/   88]
per-ex loss: 0.398030  [   25/   88]
per-ex loss: 0.324748  [   26/   88]
per-ex loss: 0.578086  [   27/   88]
per-ex loss: 0.526422  [   28/   88]
per-ex loss: 0.354293  [   29/   88]
per-ex loss: 0.348514  [   30/   88]
per-ex loss: 0.660849  [   31/   88]
per-ex loss: 0.466470  [   32/   88]
per-ex loss: 0.337853  [   33/   88]
per-ex loss: 0.468282  [   34/   88]
per-ex loss: 0.380681  [   35/   88]
per-ex loss: 0.370835  [   36/   88]
per-ex loss: 0.300772  [   37/   88]
per-ex loss: 0.537805  [   38/   88]
per-ex loss: 0.667118  [   39/   88]
per-ex loss: 0.385327  [   40/   88]
per-ex loss: 0.353056  [   41/   88]
per-ex loss: 0.422707  [   42/   88]
per-ex loss: 0.558994  [   43/   88]
per-ex loss: 0.320360  [   44/   88]
per-ex loss: 0.328540  [   45/   88]
per-ex loss: 0.433354  [   46/   88]
per-ex loss: 0.407269  [   47/   88]
per-ex loss: 0.389720  [   48/   88]
per-ex loss: 0.642986  [   49/   88]
per-ex loss: 0.407697  [   50/   88]
per-ex loss: 0.581465  [   51/   88]
per-ex loss: 0.534337  [   52/   88]
per-ex loss: 0.433545  [   53/   88]
per-ex loss: 0.319265  [   54/   88]
per-ex loss: 0.616962  [   55/   88]
per-ex loss: 0.598983  [   56/   88]
per-ex loss: 0.535239  [   57/   88]
per-ex loss: 0.632708  [   58/   88]
per-ex loss: 0.467158  [   59/   88]
per-ex loss: 0.462673  [   60/   88]
per-ex loss: 0.388951  [   61/   88]
per-ex loss: 0.309012  [   62/   88]
per-ex loss: 0.575053  [   63/   88]
per-ex loss: 0.405239  [   64/   88]
per-ex loss: 0.324997  [   65/   88]
per-ex loss: 0.367463  [   66/   88]
per-ex loss: 0.500215  [   67/   88]
per-ex loss: 0.353407  [   68/   88]
per-ex loss: 0.520148  [   69/   88]
per-ex loss: 0.545838  [   70/   88]
per-ex loss: 0.227175  [   71/   88]
per-ex loss: 0.580892  [   72/   88]
per-ex loss: 0.378270  [   73/   88]
per-ex loss: 0.368354  [   74/   88]
per-ex loss: 0.558235  [   75/   88]
per-ex loss: 0.584457  [   76/   88]
per-ex loss: 0.544311  [   77/   88]
per-ex loss: 0.479112  [   78/   88]
per-ex loss: 0.494497  [   79/   88]
per-ex loss: 0.593114  [   80/   88]
per-ex loss: 0.647758  [   81/   88]
per-ex loss: 0.509460  [   82/   88]
per-ex loss: 0.396737  [   83/   88]
per-ex loss: 0.548787  [   84/   88]
per-ex loss: 0.352713  [   85/   88]
per-ex loss: 0.441258  [   86/   88]
per-ex loss: 0.502443  [   87/   88]
per-ex loss: 0.380669  [   88/   88]
Train Error: Avg loss: 0.45401796
validation Error: 
 Avg loss: 0.52267461 
 F1: 0.505101 
 Precision: 0.605123 
 Recall: 0.433455
 IoU: 0.337883

test Error: 
 Avg loss: 0.49217404 
 F1: 0.556202 
 Precision: 0.633287 
 Recall: 0.495847
 IoU: 0.385235

We have finished training iteration 83
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_81_.pth
per-ex loss: 0.364493  [    1/   88]
per-ex loss: 0.576510  [    2/   88]
per-ex loss: 0.325709  [    3/   88]
per-ex loss: 0.549648  [    4/   88]
per-ex loss: 0.468247  [    5/   88]
per-ex loss: 0.626804  [    6/   88]
per-ex loss: 0.607456  [    7/   88]
per-ex loss: 0.369514  [    8/   88]
per-ex loss: 0.420149  [    9/   88]
per-ex loss: 0.439027  [   10/   88]
per-ex loss: 0.446290  [   11/   88]
per-ex loss: 0.307648  [   12/   88]
per-ex loss: 0.467940  [   13/   88]
per-ex loss: 0.368749  [   14/   88]
per-ex loss: 0.387774  [   15/   88]
per-ex loss: 0.630437  [   16/   88]
per-ex loss: 0.522925  [   17/   88]
per-ex loss: 0.576418  [   18/   88]
per-ex loss: 0.505921  [   19/   88]
per-ex loss: 0.582795  [   20/   88]
per-ex loss: 0.530472  [   21/   88]
per-ex loss: 0.485189  [   22/   88]
per-ex loss: 0.636212  [   23/   88]
per-ex loss: 0.441980  [   24/   88]
per-ex loss: 0.412450  [   25/   88]
per-ex loss: 0.388585  [   26/   88]
per-ex loss: 0.333916  [   27/   88]
per-ex loss: 0.416044  [   28/   88]
per-ex loss: 0.626609  [   29/   88]
per-ex loss: 0.329310  [   30/   88]
per-ex loss: 0.385835  [   31/   88]
per-ex loss: 0.355296  [   32/   88]
per-ex loss: 0.339473  [   33/   88]
per-ex loss: 0.448183  [   34/   88]
per-ex loss: 0.567877  [   35/   88]
per-ex loss: 0.414923  [   36/   88]
per-ex loss: 0.521567  [   37/   88]
per-ex loss: 0.356856  [   38/   88]
per-ex loss: 0.238615  [   39/   88]
per-ex loss: 0.532456  [   40/   88]
per-ex loss: 0.639282  [   41/   88]
per-ex loss: 0.393409  [   42/   88]
per-ex loss: 0.317607  [   43/   88]
per-ex loss: 0.520745  [   44/   88]
per-ex loss: 0.577421  [   45/   88]
per-ex loss: 0.395155  [   46/   88]
per-ex loss: 0.554443  [   47/   88]
per-ex loss: 0.584411  [   48/   88]
per-ex loss: 0.425373  [   49/   88]
per-ex loss: 0.334824  [   50/   88]
per-ex loss: 0.456402  [   51/   88]
per-ex loss: 0.583142  [   52/   88]
per-ex loss: 0.614232  [   53/   88]
per-ex loss: 0.347515  [   54/   88]
per-ex loss: 0.524628  [   55/   88]
per-ex loss: 0.429462  [   56/   88]
per-ex loss: 0.336371  [   57/   88]
per-ex loss: 0.519974  [   58/   88]
per-ex loss: 0.550084  [   59/   88]
per-ex loss: 0.574425  [   60/   88]
per-ex loss: 0.365444  [   61/   88]
per-ex loss: 0.382070  [   62/   88]
per-ex loss: 0.384529  [   63/   88]
per-ex loss: 0.463526  [   64/   88]
per-ex loss: 0.605543  [   65/   88]
per-ex loss: 0.553864  [   66/   88]
per-ex loss: 0.350217  [   67/   88]
per-ex loss: 0.366683  [   68/   88]
per-ex loss: 0.403500  [   69/   88]
per-ex loss: 0.368318  [   70/   88]
per-ex loss: 0.391784  [   71/   88]
per-ex loss: 0.385749  [   72/   88]
per-ex loss: 0.431397  [   73/   88]
per-ex loss: 0.423157  [   74/   88]
per-ex loss: 0.338952  [   75/   88]
per-ex loss: 0.385264  [   76/   88]
per-ex loss: 0.308677  [   77/   88]
per-ex loss: 0.565311  [   78/   88]
per-ex loss: 0.491019  [   79/   88]
per-ex loss: 0.549247  [   80/   88]
per-ex loss: 0.519199  [   81/   88]
per-ex loss: 0.331923  [   82/   88]
per-ex loss: 0.672681  [   83/   88]
per-ex loss: 0.503162  [   84/   88]
per-ex loss: 0.320257  [   85/   88]
per-ex loss: 0.399910  [   86/   88]
per-ex loss: 0.357165  [   87/   88]
per-ex loss: 0.468780  [   88/   88]
Train Error: Avg loss: 0.45536967
validation Error: 
 Avg loss: 0.54485967 
 F1: 0.480184 
 Precision: 0.684226 
 Recall: 0.369882
 IoU: 0.315949

test Error: 
 Avg loss: 0.51193721 
 F1: 0.531799 
 Precision: 0.705013 
 Recall: 0.426912
 IoU: 0.362212

We have finished training iteration 84
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_82_.pth
per-ex loss: 0.420303  [    1/   88]
per-ex loss: 0.346489  [    2/   88]
per-ex loss: 0.572381  [    3/   88]
per-ex loss: 0.403443  [    4/   88]
per-ex loss: 0.468079  [    5/   88]
per-ex loss: 0.408194  [    6/   88]
per-ex loss: 0.359909  [    7/   88]
per-ex loss: 0.432292  [    8/   88]
per-ex loss: 0.558121  [    9/   88]
per-ex loss: 0.373861  [   10/   88]
per-ex loss: 0.507681  [   11/   88]
per-ex loss: 0.335605  [   12/   88]
per-ex loss: 0.415472  [   13/   88]
per-ex loss: 0.465595  [   14/   88]
per-ex loss: 0.368526  [   15/   88]
per-ex loss: 0.368215  [   16/   88]
per-ex loss: 0.592000  [   17/   88]
per-ex loss: 0.393902  [   18/   88]
per-ex loss: 0.587485  [   19/   88]
per-ex loss: 0.647606  [   20/   88]
per-ex loss: 0.585753  [   21/   88]
per-ex loss: 0.332709  [   22/   88]
per-ex loss: 0.402393  [   23/   88]
per-ex loss: 0.522522  [   24/   88]
per-ex loss: 0.436376  [   25/   88]
per-ex loss: 0.490650  [   26/   88]
per-ex loss: 0.429118  [   27/   88]
per-ex loss: 0.421757  [   28/   88]
per-ex loss: 0.575996  [   29/   88]
per-ex loss: 0.331269  [   30/   88]
per-ex loss: 0.538971  [   31/   88]
per-ex loss: 0.557625  [   32/   88]
per-ex loss: 0.364629  [   33/   88]
per-ex loss: 0.303045  [   34/   88]
per-ex loss: 0.385614  [   35/   88]
per-ex loss: 0.540750  [   36/   88]
per-ex loss: 0.636145  [   37/   88]
per-ex loss: 0.525196  [   38/   88]
per-ex loss: 0.331871  [   39/   88]
per-ex loss: 0.490407  [   40/   88]
per-ex loss: 0.350592  [   41/   88]
per-ex loss: 0.582452  [   42/   88]
per-ex loss: 0.371878  [   43/   88]
per-ex loss: 0.550931  [   44/   88]
per-ex loss: 0.344325  [   45/   88]
per-ex loss: 0.350917  [   46/   88]
per-ex loss: 0.279375  [   47/   88]
per-ex loss: 0.648395  [   48/   88]
per-ex loss: 0.495119  [   49/   88]
per-ex loss: 0.467817  [   50/   88]
per-ex loss: 0.377773  [   51/   88]
per-ex loss: 0.462667  [   52/   88]
per-ex loss: 0.543249  [   53/   88]
per-ex loss: 0.588367  [   54/   88]
per-ex loss: 0.341909  [   55/   88]
per-ex loss: 0.417382  [   56/   88]
per-ex loss: 0.532934  [   57/   88]
per-ex loss: 0.390933  [   58/   88]
per-ex loss: 0.370216  [   59/   88]
per-ex loss: 0.636680  [   60/   88]
per-ex loss: 0.365729  [   61/   88]
per-ex loss: 0.607259  [   62/   88]
per-ex loss: 0.412730  [   63/   88]
per-ex loss: 0.439117  [   64/   88]
per-ex loss: 0.426004  [   65/   88]
per-ex loss: 0.322445  [   66/   88]
per-ex loss: 0.321720  [   67/   88]
per-ex loss: 0.370689  [   68/   88]
per-ex loss: 0.658343  [   69/   88]
per-ex loss: 0.454381  [   70/   88]
per-ex loss: 0.320058  [   71/   88]
per-ex loss: 0.333605  [   72/   88]
per-ex loss: 0.450540  [   73/   88]
per-ex loss: 0.508667  [   74/   88]
per-ex loss: 0.458443  [   75/   88]
per-ex loss: 0.513171  [   76/   88]
per-ex loss: 0.395216  [   77/   88]
per-ex loss: 0.391025  [   78/   88]
per-ex loss: 0.312392  [   79/   88]
per-ex loss: 0.626379  [   80/   88]
per-ex loss: 0.317210  [   81/   88]
per-ex loss: 0.563152  [   82/   88]
per-ex loss: 0.527520  [   83/   88]
per-ex loss: 0.390455  [   84/   88]
per-ex loss: 0.349855  [   85/   88]
per-ex loss: 0.432391  [   86/   88]
per-ex loss: 0.571078  [   87/   88]
per-ex loss: 0.524517  [   88/   88]
Train Error: Avg loss: 0.45109036
validation Error: 
 Avg loss: 0.53222611 
 F1: 0.493822 
 Precision: 0.644573 
 Recall: 0.400219
 IoU: 0.327864

test Error: 
 Avg loss: 0.49892353 
 F1: 0.553023 
 Precision: 0.646375 
 Recall: 0.483233
 IoU: 0.382192

We have finished training iteration 85
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_83_.pth
per-ex loss: 0.417347  [    1/   88]
per-ex loss: 0.549119  [    2/   88]
per-ex loss: 0.297803  [    3/   88]
per-ex loss: 0.496985  [    4/   88]
per-ex loss: 0.361530  [    5/   88]
per-ex loss: 0.414006  [    6/   88]
per-ex loss: 0.393704  [    7/   88]
per-ex loss: 0.524788  [    8/   88]
per-ex loss: 0.422496  [    9/   88]
per-ex loss: 0.591628  [   10/   88]
per-ex loss: 0.555351  [   11/   88]
per-ex loss: 0.449207  [   12/   88]
per-ex loss: 0.414397  [   13/   88]
per-ex loss: 0.383712  [   14/   88]
per-ex loss: 0.323120  [   15/   88]
per-ex loss: 0.330275  [   16/   88]
per-ex loss: 0.355798  [   17/   88]
per-ex loss: 0.528573  [   18/   88]
per-ex loss: 0.554231  [   19/   88]
per-ex loss: 0.575170  [   20/   88]
per-ex loss: 0.506504  [   21/   88]
per-ex loss: 0.478970  [   22/   88]
per-ex loss: 0.626779  [   23/   88]
per-ex loss: 0.399638  [   24/   88]
per-ex loss: 0.330789  [   25/   88]
per-ex loss: 0.405238  [   26/   88]
per-ex loss: 0.444247  [   27/   88]
per-ex loss: 0.323900  [   28/   88]
per-ex loss: 0.523644  [   29/   88]
per-ex loss: 0.469815  [   30/   88]
per-ex loss: 0.509931  [   31/   88]
per-ex loss: 0.604230  [   32/   88]
per-ex loss: 0.344543  [   33/   88]
per-ex loss: 0.441411  [   34/   88]
per-ex loss: 0.388236  [   35/   88]
per-ex loss: 0.538048  [   36/   88]
per-ex loss: 0.312547  [   37/   88]
per-ex loss: 0.572225  [   38/   88]
per-ex loss: 0.346648  [   39/   88]
per-ex loss: 0.574690  [   40/   88]
per-ex loss: 0.567900  [   41/   88]
per-ex loss: 0.440570  [   42/   88]
per-ex loss: 0.559082  [   43/   88]
per-ex loss: 0.444283  [   44/   88]
per-ex loss: 0.337626  [   45/   88]
per-ex loss: 0.345052  [   46/   88]
per-ex loss: 0.515890  [   47/   88]
per-ex loss: 0.370955  [   48/   88]
per-ex loss: 0.343042  [   49/   88]
per-ex loss: 0.398983  [   50/   88]
per-ex loss: 0.325215  [   51/   88]
per-ex loss: 0.389011  [   52/   88]
per-ex loss: 0.368063  [   53/   88]
per-ex loss: 0.587050  [   54/   88]
per-ex loss: 0.603240  [   55/   88]
per-ex loss: 0.464241  [   56/   88]
per-ex loss: 0.537913  [   57/   88]
per-ex loss: 0.638784  [   58/   88]
per-ex loss: 0.576328  [   59/   88]
per-ex loss: 0.505618  [   60/   88]
per-ex loss: 0.333108  [   61/   88]
per-ex loss: 0.441105  [   62/   88]
per-ex loss: 0.426621  [   63/   88]
per-ex loss: 0.409853  [   64/   88]
per-ex loss: 0.242809  [   65/   88]
per-ex loss: 0.395580  [   66/   88]
per-ex loss: 0.358076  [   67/   88]
per-ex loss: 0.469268  [   68/   88]
per-ex loss: 0.335482  [   69/   88]
per-ex loss: 0.384438  [   70/   88]
per-ex loss: 0.289863  [   71/   88]
per-ex loss: 0.539402  [   72/   88]
per-ex loss: 0.344272  [   73/   88]
per-ex loss: 0.417368  [   74/   88]
per-ex loss: 0.495769  [   75/   88]
per-ex loss: 0.363639  [   76/   88]
per-ex loss: 0.378191  [   77/   88]
per-ex loss: 0.621087  [   78/   88]
per-ex loss: 0.388975  [   79/   88]
per-ex loss: 0.678372  [   80/   88]
per-ex loss: 0.314141  [   81/   88]
per-ex loss: 0.660360  [   82/   88]
per-ex loss: 0.452318  [   83/   88]
per-ex loss: 0.356697  [   84/   88]
per-ex loss: 0.562894  [   85/   88]
per-ex loss: 0.648663  [   86/   88]
per-ex loss: 0.526309  [   87/   88]
per-ex loss: 0.402870  [   88/   88]
Train Error: Avg loss: 0.45042781
validation Error: 
 Avg loss: 0.53008056 
 F1: 0.499593 
 Precision: 0.496572 
 Recall: 0.502651
 IoU: 0.332972

test Error: 
 Avg loss: 0.48644816 
 F1: 0.553358 
 Precision: 0.537959 
 Recall: 0.569663
 IoU: 0.382512

We have finished training iteration 86
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_84_.pth
per-ex loss: 0.455672  [    1/   88]
per-ex loss: 0.519539  [    2/   88]
per-ex loss: 0.344559  [    3/   88]
per-ex loss: 0.352493  [    4/   88]
per-ex loss: 0.447434  [    5/   88]
per-ex loss: 0.547317  [    6/   88]
per-ex loss: 0.332915  [    7/   88]
per-ex loss: 0.554826  [    8/   88]
per-ex loss: 0.340175  [    9/   88]
per-ex loss: 0.327562  [   10/   88]
per-ex loss: 0.335360  [   11/   88]
per-ex loss: 0.387061  [   12/   88]
per-ex loss: 0.349826  [   13/   88]
per-ex loss: 0.455253  [   14/   88]
per-ex loss: 0.395947  [   15/   88]
per-ex loss: 0.596370  [   16/   88]
per-ex loss: 0.300499  [   17/   88]
per-ex loss: 0.370701  [   18/   88]
per-ex loss: 0.360571  [   19/   88]
per-ex loss: 0.659124  [   20/   88]
per-ex loss: 0.572087  [   21/   88]
per-ex loss: 0.569955  [   22/   88]
per-ex loss: 0.466077  [   23/   88]
per-ex loss: 0.556187  [   24/   88]
per-ex loss: 0.365407  [   25/   88]
per-ex loss: 0.523027  [   26/   88]
per-ex loss: 0.317168  [   27/   88]
per-ex loss: 0.404310  [   28/   88]
per-ex loss: 0.262943  [   29/   88]
per-ex loss: 0.571480  [   30/   88]
per-ex loss: 0.379181  [   31/   88]
per-ex loss: 0.340586  [   32/   88]
per-ex loss: 0.570901  [   33/   88]
per-ex loss: 0.443532  [   34/   88]
per-ex loss: 0.632020  [   35/   88]
per-ex loss: 0.602455  [   36/   88]
per-ex loss: 0.539619  [   37/   88]
per-ex loss: 0.388758  [   38/   88]
per-ex loss: 0.355819  [   39/   88]
per-ex loss: 0.466144  [   40/   88]
per-ex loss: 0.549940  [   41/   88]
per-ex loss: 0.469886  [   42/   88]
per-ex loss: 0.386101  [   43/   88]
per-ex loss: 0.500093  [   44/   88]
per-ex loss: 0.542565  [   45/   88]
per-ex loss: 0.492567  [   46/   88]
per-ex loss: 0.434162  [   47/   88]
per-ex loss: 0.557336  [   48/   88]
per-ex loss: 0.470866  [   49/   88]
per-ex loss: 0.390757  [   50/   88]
per-ex loss: 0.413984  [   51/   88]
per-ex loss: 0.400825  [   52/   88]
per-ex loss: 0.360390  [   53/   88]
per-ex loss: 0.358172  [   54/   88]
per-ex loss: 0.529291  [   55/   88]
per-ex loss: 0.385995  [   56/   88]
per-ex loss: 0.401017  [   57/   88]
per-ex loss: 0.479841  [   58/   88]
per-ex loss: 0.378083  [   59/   88]
per-ex loss: 0.361256  [   60/   88]
per-ex loss: 0.636853  [   61/   88]
per-ex loss: 0.352109  [   62/   88]
per-ex loss: 0.528445  [   63/   88]
per-ex loss: 0.654319  [   64/   88]
per-ex loss: 0.572165  [   65/   88]
per-ex loss: 0.341857  [   66/   88]
per-ex loss: 0.403391  [   67/   88]
per-ex loss: 0.366729  [   68/   88]
per-ex loss: 0.382181  [   69/   88]
per-ex loss: 0.472090  [   70/   88]
per-ex loss: 0.522565  [   71/   88]
per-ex loss: 0.305703  [   72/   88]
per-ex loss: 0.645039  [   73/   88]
per-ex loss: 0.413535  [   74/   88]
per-ex loss: 0.338575  [   75/   88]
per-ex loss: 0.382402  [   76/   88]
per-ex loss: 0.330239  [   77/   88]
per-ex loss: 0.484921  [   78/   88]
per-ex loss: 0.636601  [   79/   88]
per-ex loss: 0.658364  [   80/   88]
per-ex loss: 0.361550  [   81/   88]
per-ex loss: 0.566140  [   82/   88]
per-ex loss: 0.372054  [   83/   88]
per-ex loss: 0.466978  [   84/   88]
per-ex loss: 0.516396  [   85/   88]
per-ex loss: 0.363686  [   86/   88]
per-ex loss: 0.430576  [   87/   88]
per-ex loss: 0.435554  [   88/   88]
Train Error: Avg loss: 0.44960232
validation Error: 
 Avg loss: 0.52335521 
 F1: 0.504754 
 Precision: 0.626223 
 Recall: 0.422752
 IoU: 0.337572

test Error: 
 Avg loss: 0.48773148 
 F1: 0.560985 
 Precision: 0.647566 
 Recall: 0.494826
 IoU: 0.389840

We have finished training iteration 87
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_85_.pth
per-ex loss: 0.375148  [    1/   88]
per-ex loss: 0.427634  [    2/   88]
per-ex loss: 0.462728  [    3/   88]
per-ex loss: 0.572121  [    4/   88]
per-ex loss: 0.513238  [    5/   88]
per-ex loss: 0.479588  [    6/   88]
per-ex loss: 0.420210  [    7/   88]
per-ex loss: 0.465895  [    8/   88]
per-ex loss: 0.502249  [    9/   88]
per-ex loss: 0.551136  [   10/   88]
per-ex loss: 0.645383  [   11/   88]
per-ex loss: 0.383954  [   12/   88]
per-ex loss: 0.491951  [   13/   88]
per-ex loss: 0.630444  [   14/   88]
per-ex loss: 0.449828  [   15/   88]
per-ex loss: 0.499456  [   16/   88]
per-ex loss: 0.548203  [   17/   88]
per-ex loss: 0.409072  [   18/   88]
per-ex loss: 0.386858  [   19/   88]
per-ex loss: 0.529783  [   20/   88]
per-ex loss: 0.442748  [   21/   88]
per-ex loss: 0.335348  [   22/   88]
per-ex loss: 0.552414  [   23/   88]
per-ex loss: 0.341086  [   24/   88]
per-ex loss: 0.290623  [   25/   88]
per-ex loss: 0.360640  [   26/   88]
per-ex loss: 0.587075  [   27/   88]
per-ex loss: 0.402847  [   28/   88]
per-ex loss: 0.386783  [   29/   88]
per-ex loss: 0.382064  [   30/   88]
per-ex loss: 0.623648  [   31/   88]
per-ex loss: 0.543586  [   32/   88]
per-ex loss: 0.352418  [   33/   88]
per-ex loss: 0.385442  [   34/   88]
per-ex loss: 0.453728  [   35/   88]
per-ex loss: 0.346502  [   36/   88]
per-ex loss: 0.314265  [   37/   88]
per-ex loss: 0.407426  [   38/   88]
per-ex loss: 0.349983  [   39/   88]
per-ex loss: 0.535610  [   40/   88]
per-ex loss: 0.639711  [   41/   88]
per-ex loss: 0.355691  [   42/   88]
per-ex loss: 0.365592  [   43/   88]
per-ex loss: 0.637786  [   44/   88]
per-ex loss: 0.389129  [   45/   88]
per-ex loss: 0.377806  [   46/   88]
per-ex loss: 0.547603  [   47/   88]
per-ex loss: 0.395929  [   48/   88]
per-ex loss: 0.267931  [   49/   88]
per-ex loss: 0.393054  [   50/   88]
per-ex loss: 0.539897  [   51/   88]
per-ex loss: 0.359622  [   52/   88]
per-ex loss: 0.358455  [   53/   88]
per-ex loss: 0.435534  [   54/   88]
per-ex loss: 0.404086  [   55/   88]
per-ex loss: 0.536843  [   56/   88]
per-ex loss: 0.562615  [   57/   88]
per-ex loss: 0.614094  [   58/   88]
per-ex loss: 0.526056  [   59/   88]
per-ex loss: 0.450885  [   60/   88]
per-ex loss: 0.389801  [   61/   88]
per-ex loss: 0.560200  [   62/   88]
per-ex loss: 0.356418  [   63/   88]
per-ex loss: 0.584023  [   64/   88]
per-ex loss: 0.558331  [   65/   88]
per-ex loss: 0.420762  [   66/   88]
per-ex loss: 0.323047  [   67/   88]
per-ex loss: 0.333685  [   68/   88]
per-ex loss: 0.556972  [   69/   88]
per-ex loss: 0.458550  [   70/   88]
per-ex loss: 0.450383  [   71/   88]
per-ex loss: 0.319356  [   72/   88]
per-ex loss: 0.478388  [   73/   88]
per-ex loss: 0.562656  [   74/   88]
per-ex loss: 0.393186  [   75/   88]
per-ex loss: 0.628630  [   76/   88]
per-ex loss: 0.334672  [   77/   88]
per-ex loss: 0.608350  [   78/   88]
per-ex loss: 0.357298  [   79/   88]
per-ex loss: 0.340561  [   80/   88]
per-ex loss: 0.526374  [   81/   88]
per-ex loss: 0.299976  [   82/   88]
per-ex loss: 0.323816  [   83/   88]
per-ex loss: 0.334844  [   84/   88]
per-ex loss: 0.332706  [   85/   88]
per-ex loss: 0.334887  [   86/   88]
per-ex loss: 0.528845  [   87/   88]
per-ex loss: 0.527232  [   88/   88]
Train Error: Avg loss: 0.44910660
validation Error: 
 Avg loss: 0.53500644 
 F1: 0.495193 
 Precision: 0.634829 
 Recall: 0.405910
 IoU: 0.329074

test Error: 
 Avg loss: 0.49423696 
 F1: 0.555014 
 Precision: 0.655618 
 Recall: 0.481179
 IoU: 0.384097

We have finished training iteration 88
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_86_.pth
per-ex loss: 0.523849  [    1/   88]
per-ex loss: 0.382068  [    2/   88]
per-ex loss: 0.298839  [    3/   88]
per-ex loss: 0.445555  [    4/   88]
per-ex loss: 0.465145  [    5/   88]
per-ex loss: 0.396270  [    6/   88]
per-ex loss: 0.337870  [    7/   88]
per-ex loss: 0.412645  [    8/   88]
per-ex loss: 0.627021  [    9/   88]
per-ex loss: 0.304905  [   10/   88]
per-ex loss: 0.299030  [   11/   88]
per-ex loss: 0.484877  [   12/   88]
per-ex loss: 0.525732  [   13/   88]
per-ex loss: 0.341551  [   14/   88]
per-ex loss: 0.250662  [   15/   88]
per-ex loss: 0.371427  [   16/   88]
per-ex loss: 0.376819  [   17/   88]
per-ex loss: 0.475338  [   18/   88]
per-ex loss: 0.560984  [   19/   88]
per-ex loss: 0.547623  [   20/   88]
per-ex loss: 0.543451  [   21/   88]
per-ex loss: 0.541342  [   22/   88]
per-ex loss: 0.462980  [   23/   88]
per-ex loss: 0.614062  [   24/   88]
per-ex loss: 0.416768  [   25/   88]
per-ex loss: 0.410599  [   26/   88]
per-ex loss: 0.408289  [   27/   88]
per-ex loss: 0.442198  [   28/   88]
per-ex loss: 0.392824  [   29/   88]
per-ex loss: 0.325783  [   30/   88]
per-ex loss: 0.615179  [   31/   88]
per-ex loss: 0.392524  [   32/   88]
per-ex loss: 0.597981  [   33/   88]
per-ex loss: 0.593859  [   34/   88]
per-ex loss: 0.335335  [   35/   88]
per-ex loss: 0.323878  [   36/   88]
per-ex loss: 0.568603  [   37/   88]
per-ex loss: 0.550921  [   38/   88]
per-ex loss: 0.374624  [   39/   88]
per-ex loss: 0.468833  [   40/   88]
per-ex loss: 0.437980  [   41/   88]
per-ex loss: 0.458647  [   42/   88]
per-ex loss: 0.406898  [   43/   88]
per-ex loss: 0.564240  [   44/   88]
per-ex loss: 0.492717  [   45/   88]
per-ex loss: 0.455283  [   46/   88]
per-ex loss: 0.349022  [   47/   88]
per-ex loss: 0.360433  [   48/   88]
per-ex loss: 0.627169  [   49/   88]
per-ex loss: 0.368739  [   50/   88]
per-ex loss: 0.307940  [   51/   88]
per-ex loss: 0.371969  [   52/   88]
per-ex loss: 0.365570  [   53/   88]
per-ex loss: 0.375705  [   54/   88]
per-ex loss: 0.653388  [   55/   88]
per-ex loss: 0.315834  [   56/   88]
per-ex loss: 0.578916  [   57/   88]
per-ex loss: 0.325953  [   58/   88]
per-ex loss: 0.389891  [   59/   88]
per-ex loss: 0.379800  [   60/   88]
per-ex loss: 0.416933  [   61/   88]
per-ex loss: 0.333068  [   62/   88]
per-ex loss: 0.384146  [   63/   88]
per-ex loss: 0.409833  [   64/   88]
per-ex loss: 0.575147  [   65/   88]
per-ex loss: 0.625417  [   66/   88]
per-ex loss: 0.593892  [   67/   88]
per-ex loss: 0.316772  [   68/   88]
per-ex loss: 0.601991  [   69/   88]
per-ex loss: 0.504429  [   70/   88]
per-ex loss: 0.627160  [   71/   88]
per-ex loss: 0.365346  [   72/   88]
per-ex loss: 0.364986  [   73/   88]
per-ex loss: 0.529910  [   74/   88]
per-ex loss: 0.457567  [   75/   88]
per-ex loss: 0.554217  [   76/   88]
per-ex loss: 0.518826  [   77/   88]
per-ex loss: 0.385617  [   78/   88]
per-ex loss: 0.437566  [   79/   88]
per-ex loss: 0.400006  [   80/   88]
per-ex loss: 0.398124  [   81/   88]
per-ex loss: 0.478651  [   82/   88]
per-ex loss: 0.647014  [   83/   88]
per-ex loss: 0.643416  [   84/   88]
per-ex loss: 0.359654  [   85/   88]
per-ex loss: 0.568566  [   86/   88]
per-ex loss: 0.394591  [   87/   88]
per-ex loss: 0.515157  [   88/   88]
Train Error: Avg loss: 0.45229930
validation Error: 
 Avg loss: 0.51815316 
 F1: 0.512202 
 Precision: 0.540069 
 Recall: 0.487069
 IoU: 0.344268

test Error: 
 Avg loss: 0.48126293 
 F1: 0.563116 
 Precision: 0.562593 
 Recall: 0.563639
 IoU: 0.391900

We have finished training iteration 89
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_87_.pth
per-ex loss: 0.452015  [    1/   88]
per-ex loss: 0.478268  [    2/   88]
per-ex loss: 0.571903  [    3/   88]
per-ex loss: 0.382734  [    4/   88]
per-ex loss: 0.522370  [    5/   88]
per-ex loss: 0.389546  [    6/   88]
per-ex loss: 0.348078  [    7/   88]
per-ex loss: 0.550744  [    8/   88]
per-ex loss: 0.338883  [    9/   88]
per-ex loss: 0.577092  [   10/   88]
per-ex loss: 0.605410  [   11/   88]
per-ex loss: 0.381739  [   12/   88]
per-ex loss: 0.383086  [   13/   88]
per-ex loss: 0.477980  [   14/   88]
per-ex loss: 0.376116  [   15/   88]
per-ex loss: 0.620778  [   16/   88]
per-ex loss: 0.416110  [   17/   88]
per-ex loss: 0.564643  [   18/   88]
per-ex loss: 0.390859  [   19/   88]
per-ex loss: 0.498754  [   20/   88]
per-ex loss: 0.644949  [   21/   88]
per-ex loss: 0.538585  [   22/   88]
per-ex loss: 0.495133  [   23/   88]
per-ex loss: 0.368026  [   24/   88]
per-ex loss: 0.564471  [   25/   88]
per-ex loss: 0.628397  [   26/   88]
per-ex loss: 0.330079  [   27/   88]
per-ex loss: 0.560875  [   28/   88]
per-ex loss: 0.338654  [   29/   88]
per-ex loss: 0.345274  [   30/   88]
per-ex loss: 0.405153  [   31/   88]
per-ex loss: 0.389654  [   32/   88]
per-ex loss: 0.524185  [   33/   88]
per-ex loss: 0.443847  [   34/   88]
per-ex loss: 0.337528  [   35/   88]
per-ex loss: 0.361815  [   36/   88]
per-ex loss: 0.514411  [   37/   88]
per-ex loss: 0.330941  [   38/   88]
per-ex loss: 0.524320  [   39/   88]
per-ex loss: 0.393208  [   40/   88]
per-ex loss: 0.603691  [   41/   88]
per-ex loss: 0.685432  [   42/   88]
per-ex loss: 0.339990  [   43/   88]
per-ex loss: 0.403023  [   44/   88]
per-ex loss: 0.556354  [   45/   88]
per-ex loss: 0.534390  [   46/   88]
per-ex loss: 0.457721  [   47/   88]
per-ex loss: 0.349969  [   48/   88]
per-ex loss: 0.305241  [   49/   88]
per-ex loss: 0.584550  [   50/   88]
per-ex loss: 0.323927  [   51/   88]
per-ex loss: 0.378793  [   52/   88]
per-ex loss: 0.277342  [   53/   88]
per-ex loss: 0.374341  [   54/   88]
per-ex loss: 0.381537  [   55/   88]
per-ex loss: 0.569498  [   56/   88]
per-ex loss: 0.409226  [   57/   88]
per-ex loss: 0.532184  [   58/   88]
per-ex loss: 0.390276  [   59/   88]
per-ex loss: 0.298143  [   60/   88]
per-ex loss: 0.628989  [   61/   88]
per-ex loss: 0.725692  [   62/   88]
per-ex loss: 0.520904  [   63/   88]
per-ex loss: 0.481212  [   64/   88]
per-ex loss: 0.560814  [   65/   88]
per-ex loss: 0.620318  [   66/   88]
per-ex loss: 0.395451  [   67/   88]
per-ex loss: 0.340016  [   68/   88]
per-ex loss: 0.370246  [   69/   88]
per-ex loss: 0.330812  [   70/   88]
per-ex loss: 0.582420  [   71/   88]
per-ex loss: 0.415065  [   72/   88]
per-ex loss: 0.366970  [   73/   88]
per-ex loss: 0.493118  [   74/   88]
per-ex loss: 0.499406  [   75/   88]
per-ex loss: 0.438724  [   76/   88]
per-ex loss: 0.422919  [   77/   88]
per-ex loss: 0.360525  [   78/   88]
per-ex loss: 0.302179  [   79/   88]
per-ex loss: 0.448721  [   80/   88]
per-ex loss: 0.392325  [   81/   88]
per-ex loss: 0.348433  [   82/   88]
per-ex loss: 0.448923  [   83/   88]
per-ex loss: 0.325353  [   84/   88]
per-ex loss: 0.474714  [   85/   88]
per-ex loss: 0.421394  [   86/   88]
per-ex loss: 0.549909  [   87/   88]
per-ex loss: 0.393795  [   88/   88]
Train Error: Avg loss: 0.45206353
validation Error: 
 Avg loss: 0.51822191 
 F1: 0.513357 
 Precision: 0.591090 
 Recall: 0.453693
 IoU: 0.345313

test Error: 
 Avg loss: 0.47665222 
 F1: 0.568776 
 Precision: 0.622840 
 Recall: 0.523349
 IoU: 0.397406

We have finished training iteration 90
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_88_.pth
per-ex loss: 0.529248  [    1/   88]
per-ex loss: 0.382390  [    2/   88]
per-ex loss: 0.551157  [    3/   88]
per-ex loss: 0.353920  [    4/   88]
per-ex loss: 0.387189  [    5/   88]
per-ex loss: 0.344412  [    6/   88]
per-ex loss: 0.471504  [    7/   88]
per-ex loss: 0.523558  [    8/   88]
per-ex loss: 0.643654  [    9/   88]
per-ex loss: 0.640567  [   10/   88]
per-ex loss: 0.423933  [   11/   88]
per-ex loss: 0.321605  [   12/   88]
per-ex loss: 0.350394  [   13/   88]
per-ex loss: 0.297627  [   14/   88]
per-ex loss: 0.339371  [   15/   88]
per-ex loss: 0.467520  [   16/   88]
per-ex loss: 0.371698  [   17/   88]
per-ex loss: 0.353181  [   18/   88]
per-ex loss: 0.424165  [   19/   88]
per-ex loss: 0.377345  [   20/   88]
per-ex loss: 0.588204  [   21/   88]
per-ex loss: 0.527835  [   22/   88]
per-ex loss: 0.389129  [   23/   88]
per-ex loss: 0.308479  [   24/   88]
per-ex loss: 0.386708  [   25/   88]
per-ex loss: 0.474313  [   26/   88]
per-ex loss: 0.446669  [   27/   88]
per-ex loss: 0.507685  [   28/   88]
per-ex loss: 0.365960  [   29/   88]
per-ex loss: 0.388898  [   30/   88]
per-ex loss: 0.522735  [   31/   88]
per-ex loss: 0.534344  [   32/   88]
per-ex loss: 0.322370  [   33/   88]
per-ex loss: 0.405555  [   34/   88]
per-ex loss: 0.397330  [   35/   88]
per-ex loss: 0.476355  [   36/   88]
per-ex loss: 0.622682  [   37/   88]
per-ex loss: 0.631475  [   38/   88]
per-ex loss: 0.516736  [   39/   88]
per-ex loss: 0.403303  [   40/   88]
per-ex loss: 0.446962  [   41/   88]
per-ex loss: 0.439443  [   42/   88]
per-ex loss: 0.554468  [   43/   88]
per-ex loss: 0.583496  [   44/   88]
per-ex loss: 0.570699  [   45/   88]
per-ex loss: 0.406968  [   46/   88]
per-ex loss: 0.505125  [   47/   88]
per-ex loss: 0.605803  [   48/   88]
per-ex loss: 0.327271  [   49/   88]
per-ex loss: 0.466980  [   50/   88]
per-ex loss: 0.358934  [   51/   88]
per-ex loss: 0.557633  [   52/   88]
per-ex loss: 0.398752  [   53/   88]
per-ex loss: 0.559196  [   54/   88]
per-ex loss: 0.348247  [   55/   88]
per-ex loss: 0.676339  [   56/   88]
per-ex loss: 0.445160  [   57/   88]
per-ex loss: 0.334643  [   58/   88]
per-ex loss: 0.555821  [   59/   88]
per-ex loss: 0.537477  [   60/   88]
per-ex loss: 0.381250  [   61/   88]
per-ex loss: 0.553847  [   62/   88]
per-ex loss: 0.339562  [   63/   88]
per-ex loss: 0.332120  [   64/   88]
per-ex loss: 0.571015  [   65/   88]
per-ex loss: 0.345682  [   66/   88]
per-ex loss: 0.355635  [   67/   88]
per-ex loss: 0.590283  [   68/   88]
per-ex loss: 0.615765  [   69/   88]
per-ex loss: 0.505202  [   70/   88]
per-ex loss: 0.440571  [   71/   88]
per-ex loss: 0.651742  [   72/   88]
per-ex loss: 0.261225  [   73/   88]
per-ex loss: 0.457948  [   74/   88]
per-ex loss: 0.317710  [   75/   88]
per-ex loss: 0.424485  [   76/   88]
per-ex loss: 0.325621  [   77/   88]
per-ex loss: 0.331208  [   78/   88]
per-ex loss: 0.372454  [   79/   88]
per-ex loss: 0.414327  [   80/   88]
per-ex loss: 0.349787  [   81/   88]
per-ex loss: 0.363757  [   82/   88]
per-ex loss: 0.366283  [   83/   88]
per-ex loss: 0.570243  [   84/   88]
per-ex loss: 0.530003  [   85/   88]
per-ex loss: 0.458998  [   86/   88]
per-ex loss: 0.364768  [   87/   88]
per-ex loss: 0.355770  [   88/   88]
Train Error: Avg loss: 0.44770315
validation Error: 
 Avg loss: 0.53441522 
 F1: 0.494861 
 Precision: 0.626017 
 Recall: 0.409143
 IoU: 0.328781

test Error: 
 Avg loss: 0.48594045 
 F1: 0.562952 
 Precision: 0.678133 
 Recall: 0.481217
 IoU: 0.391742

We have finished training iteration 91
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_76_.pth
per-ex loss: 0.459355  [    1/   88]
per-ex loss: 0.417106  [    2/   88]
per-ex loss: 0.410999  [    3/   88]
per-ex loss: 0.305464  [    4/   88]
per-ex loss: 0.539931  [    5/   88]
per-ex loss: 0.549997  [    6/   88]
per-ex loss: 0.347648  [    7/   88]
per-ex loss: 0.344658  [    8/   88]
per-ex loss: 0.374837  [    9/   88]
per-ex loss: 0.413319  [   10/   88]
per-ex loss: 0.526594  [   11/   88]
per-ex loss: 0.516436  [   12/   88]
per-ex loss: 0.453598  [   13/   88]
per-ex loss: 0.542946  [   14/   88]
per-ex loss: 0.542918  [   15/   88]
per-ex loss: 0.519637  [   16/   88]
per-ex loss: 0.504167  [   17/   88]
per-ex loss: 0.341191  [   18/   88]
per-ex loss: 0.634892  [   19/   88]
per-ex loss: 0.469333  [   20/   88]
per-ex loss: 0.460261  [   21/   88]
per-ex loss: 0.257530  [   22/   88]
per-ex loss: 0.404979  [   23/   88]
per-ex loss: 0.367595  [   24/   88]
per-ex loss: 0.651647  [   25/   88]
per-ex loss: 0.606107  [   26/   88]
per-ex loss: 0.452870  [   27/   88]
per-ex loss: 0.331772  [   28/   88]
per-ex loss: 0.406312  [   29/   88]
per-ex loss: 0.436162  [   30/   88]
per-ex loss: 0.448589  [   31/   88]
per-ex loss: 0.442436  [   32/   88]
per-ex loss: 0.330185  [   33/   88]
per-ex loss: 0.409318  [   34/   88]
per-ex loss: 0.386820  [   35/   88]
per-ex loss: 0.390339  [   36/   88]
per-ex loss: 0.364231  [   37/   88]
per-ex loss: 0.553747  [   38/   88]
per-ex loss: 0.558404  [   39/   88]
per-ex loss: 0.296086  [   40/   88]
per-ex loss: 0.289421  [   41/   88]
per-ex loss: 0.354192  [   42/   88]
per-ex loss: 0.544217  [   43/   88]
per-ex loss: 0.559325  [   44/   88]
per-ex loss: 0.331179  [   45/   88]
per-ex loss: 0.332195  [   46/   88]
per-ex loss: 0.357289  [   47/   88]
per-ex loss: 0.521690  [   48/   88]
per-ex loss: 0.383119  [   49/   88]
per-ex loss: 0.353403  [   50/   88]
per-ex loss: 0.348318  [   51/   88]
per-ex loss: 0.616953  [   52/   88]
per-ex loss: 0.374711  [   53/   88]
per-ex loss: 0.436851  [   54/   88]
per-ex loss: 0.571877  [   55/   88]
per-ex loss: 0.338323  [   56/   88]
per-ex loss: 0.307126  [   57/   88]
per-ex loss: 0.437681  [   58/   88]
per-ex loss: 0.603069  [   59/   88]
per-ex loss: 0.486836  [   60/   88]
per-ex loss: 0.389295  [   61/   88]
per-ex loss: 0.404906  [   62/   88]
per-ex loss: 0.545914  [   63/   88]
per-ex loss: 0.527816  [   64/   88]
per-ex loss: 0.361221  [   65/   88]
per-ex loss: 0.492119  [   66/   88]
per-ex loss: 0.460652  [   67/   88]
per-ex loss: 0.391333  [   68/   88]
per-ex loss: 0.397450  [   69/   88]
per-ex loss: 0.364474  [   70/   88]
per-ex loss: 0.636454  [   71/   88]
per-ex loss: 0.633981  [   72/   88]
per-ex loss: 0.332136  [   73/   88]
per-ex loss: 0.402967  [   74/   88]
per-ex loss: 0.364409  [   75/   88]
per-ex loss: 0.404278  [   76/   88]
per-ex loss: 0.471901  [   77/   88]
per-ex loss: 0.314140  [   78/   88]
per-ex loss: 0.331689  [   79/   88]
per-ex loss: 0.464620  [   80/   88]
per-ex loss: 0.582565  [   81/   88]
per-ex loss: 0.555517  [   82/   88]
per-ex loss: 0.630487  [   83/   88]
per-ex loss: 0.362722  [   84/   88]
per-ex loss: 0.552649  [   85/   88]
per-ex loss: 0.558632  [   86/   88]
per-ex loss: 0.328955  [   87/   88]
per-ex loss: 0.518828  [   88/   88]
Train Error: Avg loss: 0.44432164
validation Error: 
 Avg loss: 0.53578862 
 F1: 0.492655 
 Precision: 0.495459 
 Recall: 0.489882
 IoU: 0.326836

test Error: 
 Avg loss: 0.49388738 
 F1: 0.548126 
 Precision: 0.527702 
 Recall: 0.570196
 IoU: 0.377530

We have finished training iteration 92
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_79_.pth
per-ex loss: 0.541176  [    1/   88]
per-ex loss: 0.529628  [    2/   88]
per-ex loss: 0.658525  [    3/   88]
per-ex loss: 0.590400  [    4/   88]
per-ex loss: 0.516633  [    5/   88]
per-ex loss: 0.301041  [    6/   88]
per-ex loss: 0.389303  [    7/   88]
per-ex loss: 0.421589  [    8/   88]
per-ex loss: 0.573161  [    9/   88]
per-ex loss: 0.518607  [   10/   88]
per-ex loss: 0.570058  [   11/   88]
per-ex loss: 0.513845  [   12/   88]
per-ex loss: 0.379124  [   13/   88]
per-ex loss: 0.379636  [   14/   88]
per-ex loss: 0.321126  [   15/   88]
per-ex loss: 0.472366  [   16/   88]
per-ex loss: 0.356663  [   17/   88]
per-ex loss: 0.257590  [   18/   88]
per-ex loss: 0.531822  [   19/   88]
per-ex loss: 0.613467  [   20/   88]
per-ex loss: 0.362842  [   21/   88]
per-ex loss: 0.368508  [   22/   88]
per-ex loss: 0.380690  [   23/   88]
per-ex loss: 0.541350  [   24/   88]
per-ex loss: 0.528072  [   25/   88]
per-ex loss: 0.364727  [   26/   88]
per-ex loss: 0.553661  [   27/   88]
per-ex loss: 0.423831  [   28/   88]
per-ex loss: 0.372357  [   29/   88]
per-ex loss: 0.557128  [   30/   88]
per-ex loss: 0.541514  [   31/   88]
per-ex loss: 0.324501  [   32/   88]
per-ex loss: 0.434961  [   33/   88]
per-ex loss: 0.608962  [   34/   88]
per-ex loss: 0.374655  [   35/   88]
per-ex loss: 0.432660  [   36/   88]
per-ex loss: 0.660029  [   37/   88]
per-ex loss: 0.405610  [   38/   88]
per-ex loss: 0.388590  [   39/   88]
per-ex loss: 0.533722  [   40/   88]
per-ex loss: 0.560227  [   41/   88]
per-ex loss: 0.426338  [   42/   88]
per-ex loss: 0.348532  [   43/   88]
per-ex loss: 0.402348  [   44/   88]
per-ex loss: 0.399741  [   45/   88]
per-ex loss: 0.574014  [   46/   88]
per-ex loss: 0.619467  [   47/   88]
per-ex loss: 0.352735  [   48/   88]
per-ex loss: 0.335619  [   49/   88]
per-ex loss: 0.310378  [   50/   88]
per-ex loss: 0.357318  [   51/   88]
per-ex loss: 0.339422  [   52/   88]
per-ex loss: 0.641037  [   53/   88]
per-ex loss: 0.439729  [   54/   88]
per-ex loss: 0.332968  [   55/   88]
per-ex loss: 0.342434  [   56/   88]
per-ex loss: 0.377647  [   57/   88]
per-ex loss: 0.518923  [   58/   88]
per-ex loss: 0.418768  [   59/   88]
per-ex loss: 0.394251  [   60/   88]
per-ex loss: 0.330698  [   61/   88]
per-ex loss: 0.294108  [   62/   88]
per-ex loss: 0.491638  [   63/   88]
per-ex loss: 0.450380  [   64/   88]
per-ex loss: 0.312748  [   65/   88]
per-ex loss: 0.370170  [   66/   88]
per-ex loss: 0.391896  [   67/   88]
per-ex loss: 0.476623  [   68/   88]
per-ex loss: 0.457646  [   69/   88]
per-ex loss: 0.396819  [   70/   88]
per-ex loss: 0.372621  [   71/   88]
per-ex loss: 0.564258  [   72/   88]
per-ex loss: 0.546381  [   73/   88]
per-ex loss: 0.340795  [   74/   88]
per-ex loss: 0.425174  [   75/   88]
per-ex loss: 0.327226  [   76/   88]
per-ex loss: 0.408753  [   77/   88]
per-ex loss: 0.593518  [   78/   88]
per-ex loss: 0.596288  [   79/   88]
per-ex loss: 0.670907  [   80/   88]
per-ex loss: 0.532699  [   81/   88]
per-ex loss: 0.451776  [   82/   88]
per-ex loss: 0.340793  [   83/   88]
per-ex loss: 0.631240  [   84/   88]
per-ex loss: 0.462829  [   85/   88]
per-ex loss: 0.452709  [   86/   88]
per-ex loss: 0.387877  [   87/   88]
per-ex loss: 0.581358  [   88/   88]
Train Error: Avg loss: 0.45165855
validation Error: 
 Avg loss: 0.52547520 
 F1: 0.505070 
 Precision: 0.523713 
 Recall: 0.487709
 IoU: 0.337856

test Error: 
 Avg loss: 0.48766013 
 F1: 0.554050 
 Precision: 0.558427 
 Recall: 0.549741
 IoU: 0.383174

We have finished training iteration 93
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_91_.pth
per-ex loss: 0.348891  [    1/   88]
per-ex loss: 0.484947  [    2/   88]
per-ex loss: 0.525557  [    3/   88]
per-ex loss: 0.400727  [    4/   88]
per-ex loss: 0.286128  [    5/   88]
per-ex loss: 0.296117  [    6/   88]
per-ex loss: 0.473078  [    7/   88]
per-ex loss: 0.536795  [    8/   88]
per-ex loss: 0.355365  [    9/   88]
per-ex loss: 0.594742  [   10/   88]
per-ex loss: 0.454099  [   11/   88]
per-ex loss: 0.322327  [   12/   88]
per-ex loss: 0.365938  [   13/   88]
per-ex loss: 0.399174  [   14/   88]
per-ex loss: 0.301089  [   15/   88]
per-ex loss: 0.523443  [   16/   88]
per-ex loss: 0.567306  [   17/   88]
per-ex loss: 0.535993  [   18/   88]
per-ex loss: 0.526741  [   19/   88]
per-ex loss: 0.558570  [   20/   88]
per-ex loss: 0.322426  [   21/   88]
per-ex loss: 0.443051  [   22/   88]
per-ex loss: 0.427335  [   23/   88]
per-ex loss: 0.475565  [   24/   88]
per-ex loss: 0.374860  [   25/   88]
per-ex loss: 0.451580  [   26/   88]
per-ex loss: 0.425117  [   27/   88]
per-ex loss: 0.373326  [   28/   88]
per-ex loss: 0.533327  [   29/   88]
per-ex loss: 0.497485  [   30/   88]
per-ex loss: 0.465743  [   31/   88]
per-ex loss: 0.568000  [   32/   88]
per-ex loss: 0.613042  [   33/   88]
per-ex loss: 0.394677  [   34/   88]
per-ex loss: 0.465152  [   35/   88]
per-ex loss: 0.396111  [   36/   88]
per-ex loss: 0.342514  [   37/   88]
per-ex loss: 0.600412  [   38/   88]
per-ex loss: 0.372993  [   39/   88]
per-ex loss: 0.330518  [   40/   88]
per-ex loss: 0.384140  [   41/   88]
per-ex loss: 0.347810  [   42/   88]
per-ex loss: 0.350744  [   43/   88]
per-ex loss: 0.460318  [   44/   88]
per-ex loss: 0.317273  [   45/   88]
per-ex loss: 0.335068  [   46/   88]
per-ex loss: 0.318735  [   47/   88]
per-ex loss: 0.371514  [   48/   88]
per-ex loss: 0.330654  [   49/   88]
per-ex loss: 0.313114  [   50/   88]
per-ex loss: 0.335448  [   51/   88]
per-ex loss: 0.327545  [   52/   88]
per-ex loss: 0.535605  [   53/   88]
per-ex loss: 0.412721  [   54/   88]
per-ex loss: 0.433017  [   55/   88]
per-ex loss: 0.546656  [   56/   88]
per-ex loss: 0.412260  [   57/   88]
per-ex loss: 0.398990  [   58/   88]
per-ex loss: 0.562425  [   59/   88]
per-ex loss: 0.495249  [   60/   88]
per-ex loss: 0.256208  [   61/   88]
per-ex loss: 0.365696  [   62/   88]
per-ex loss: 0.428308  [   63/   88]
per-ex loss: 0.510391  [   64/   88]
per-ex loss: 0.640034  [   65/   88]
per-ex loss: 0.385727  [   66/   88]
per-ex loss: 0.532392  [   67/   88]
per-ex loss: 0.640732  [   68/   88]
per-ex loss: 0.327823  [   69/   88]
per-ex loss: 0.513308  [   70/   88]
per-ex loss: 0.386026  [   71/   88]
per-ex loss: 0.347644  [   72/   88]
per-ex loss: 0.575786  [   73/   88]
per-ex loss: 0.438117  [   74/   88]
per-ex loss: 0.396851  [   75/   88]
per-ex loss: 0.508134  [   76/   88]
per-ex loss: 0.459389  [   77/   88]
per-ex loss: 0.609240  [   78/   88]
per-ex loss: 0.691769  [   79/   88]
per-ex loss: 0.496230  [   80/   88]
per-ex loss: 0.578786  [   81/   88]
per-ex loss: 0.549192  [   82/   88]
per-ex loss: 0.649961  [   83/   88]
per-ex loss: 0.587724  [   84/   88]
per-ex loss: 0.372285  [   85/   88]
per-ex loss: 0.391999  [   86/   88]
per-ex loss: 0.651257  [   87/   88]
per-ex loss: 0.418323  [   88/   88]
Train Error: Avg loss: 0.44805543
validation Error: 
 Avg loss: 0.53661733 
 F1: 0.495908 
 Precision: 0.537853 
 Recall: 0.460032
 IoU: 0.329706

test Error: 
 Avg loss: 0.49876539 
 F1: 0.546632 
 Precision: 0.569990 
 Recall: 0.525114
 IoU: 0.376114

We have finished training iteration 94
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_92_.pth
per-ex loss: 0.606728  [    1/   88]
per-ex loss: 0.344102  [    2/   88]
per-ex loss: 0.328218  [    3/   88]
per-ex loss: 0.320004  [    4/   88]
per-ex loss: 0.562034  [    5/   88]
per-ex loss: 0.334252  [    6/   88]
per-ex loss: 0.640986  [    7/   88]
per-ex loss: 0.507071  [    8/   88]
per-ex loss: 0.373312  [    9/   88]
per-ex loss: 0.518315  [   10/   88]
per-ex loss: 0.387428  [   11/   88]
per-ex loss: 0.292393  [   12/   88]
per-ex loss: 0.333113  [   13/   88]
per-ex loss: 0.469919  [   14/   88]
per-ex loss: 0.320558  [   15/   88]
per-ex loss: 0.415162  [   16/   88]
per-ex loss: 0.455078  [   17/   88]
per-ex loss: 0.470639  [   18/   88]
per-ex loss: 0.449625  [   19/   88]
per-ex loss: 0.378817  [   20/   88]
per-ex loss: 0.603134  [   21/   88]
per-ex loss: 0.412986  [   22/   88]
per-ex loss: 0.572302  [   23/   88]
per-ex loss: 0.383085  [   24/   88]
per-ex loss: 0.510804  [   25/   88]
per-ex loss: 0.487609  [   26/   88]
per-ex loss: 0.379247  [   27/   88]
per-ex loss: 0.366540  [   28/   88]
per-ex loss: 0.578975  [   29/   88]
per-ex loss: 0.327352  [   30/   88]
per-ex loss: 0.361684  [   31/   88]
per-ex loss: 0.572229  [   32/   88]
per-ex loss: 0.354163  [   33/   88]
per-ex loss: 0.643123  [   34/   88]
per-ex loss: 0.479989  [   35/   88]
per-ex loss: 0.343507  [   36/   88]
per-ex loss: 0.542237  [   37/   88]
per-ex loss: 0.412599  [   38/   88]
per-ex loss: 0.553319  [   39/   88]
per-ex loss: 0.388342  [   40/   88]
per-ex loss: 0.391703  [   41/   88]
per-ex loss: 0.595387  [   42/   88]
per-ex loss: 0.407130  [   43/   88]
per-ex loss: 0.312029  [   44/   88]
per-ex loss: 0.388840  [   45/   88]
per-ex loss: 0.456663  [   46/   88]
per-ex loss: 0.563417  [   47/   88]
per-ex loss: 0.359855  [   48/   88]
per-ex loss: 0.421757  [   49/   88]
per-ex loss: 0.475318  [   50/   88]
per-ex loss: 0.334227  [   51/   88]
per-ex loss: 0.385244  [   52/   88]
per-ex loss: 0.352390  [   53/   88]
per-ex loss: 0.520974  [   54/   88]
per-ex loss: 0.347555  [   55/   88]
per-ex loss: 0.536808  [   56/   88]
per-ex loss: 0.561917  [   57/   88]
per-ex loss: 0.387827  [   58/   88]
per-ex loss: 0.386449  [   59/   88]
per-ex loss: 0.519994  [   60/   88]
per-ex loss: 0.631808  [   61/   88]
per-ex loss: 0.607980  [   62/   88]
per-ex loss: 0.529499  [   63/   88]
per-ex loss: 0.543456  [   64/   88]
per-ex loss: 0.323880  [   65/   88]
per-ex loss: 0.443531  [   66/   88]
per-ex loss: 0.630621  [   67/   88]
per-ex loss: 0.531655  [   68/   88]
per-ex loss: 0.317742  [   69/   88]
per-ex loss: 0.446938  [   70/   88]
per-ex loss: 0.411603  [   71/   88]
per-ex loss: 0.329119  [   72/   88]
per-ex loss: 0.458140  [   73/   88]
per-ex loss: 0.590196  [   74/   88]
per-ex loss: 0.345638  [   75/   88]
per-ex loss: 0.318025  [   76/   88]
per-ex loss: 0.391832  [   77/   88]
per-ex loss: 0.388611  [   78/   88]
per-ex loss: 0.408373  [   79/   88]
per-ex loss: 0.571898  [   80/   88]
per-ex loss: 0.462023  [   81/   88]
per-ex loss: 0.365277  [   82/   88]
per-ex loss: 0.385883  [   83/   88]
per-ex loss: 0.415107  [   84/   88]
per-ex loss: 0.633673  [   85/   88]
per-ex loss: 0.548492  [   86/   88]
per-ex loss: 0.604811  [   87/   88]
per-ex loss: 0.600630  [   88/   88]
Train Error: Avg loss: 0.45139669
validation Error: 
 Avg loss: 0.52445247 
 F1: 0.505190 
 Precision: 0.512055 
 Recall: 0.498506
 IoU: 0.337962

test Error: 
 Avg loss: 0.47835730 
 F1: 0.563385 
 Precision: 0.553645 
 Recall: 0.573473
 IoU: 0.392161

We have finished training iteration 95
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_93_.pth
per-ex loss: 0.600176  [    1/   88]
per-ex loss: 0.366203  [    2/   88]
per-ex loss: 0.335718  [    3/   88]
per-ex loss: 0.317471  [    4/   88]
per-ex loss: 0.503803  [    5/   88]
per-ex loss: 0.424250  [    6/   88]
per-ex loss: 0.386806  [    7/   88]
per-ex loss: 0.536121  [    8/   88]
per-ex loss: 0.399213  [    9/   88]
per-ex loss: 0.582504  [   10/   88]
per-ex loss: 0.461786  [   11/   88]
per-ex loss: 0.347713  [   12/   88]
per-ex loss: 0.331950  [   13/   88]
per-ex loss: 0.618815  [   14/   88]
per-ex loss: 0.543622  [   15/   88]
per-ex loss: 0.491714  [   16/   88]
per-ex loss: 0.626902  [   17/   88]
per-ex loss: 0.441023  [   18/   88]
per-ex loss: 0.370183  [   19/   88]
per-ex loss: 0.252720  [   20/   88]
per-ex loss: 0.582899  [   21/   88]
per-ex loss: 0.370871  [   22/   88]
per-ex loss: 0.415366  [   23/   88]
per-ex loss: 0.532769  [   24/   88]
per-ex loss: 0.570101  [   25/   88]
per-ex loss: 0.517703  [   26/   88]
per-ex loss: 0.546635  [   27/   88]
per-ex loss: 0.558715  [   28/   88]
per-ex loss: 0.375778  [   29/   88]
per-ex loss: 0.343158  [   30/   88]
per-ex loss: 0.307351  [   31/   88]
per-ex loss: 0.393817  [   32/   88]
per-ex loss: 0.403249  [   33/   88]
per-ex loss: 0.554353  [   34/   88]
per-ex loss: 0.563777  [   35/   88]
per-ex loss: 0.326314  [   36/   88]
per-ex loss: 0.631879  [   37/   88]
per-ex loss: 0.513536  [   38/   88]
per-ex loss: 0.415974  [   39/   88]
per-ex loss: 0.318366  [   40/   88]
per-ex loss: 0.386530  [   41/   88]
per-ex loss: 0.422322  [   42/   88]
per-ex loss: 0.328698  [   43/   88]
per-ex loss: 0.535851  [   44/   88]
per-ex loss: 0.653931  [   45/   88]
per-ex loss: 0.418669  [   46/   88]
per-ex loss: 0.360691  [   47/   88]
per-ex loss: 0.332072  [   48/   88]
per-ex loss: 0.366744  [   49/   88]
per-ex loss: 0.419543  [   50/   88]
per-ex loss: 0.402934  [   51/   88]
per-ex loss: 0.549696  [   52/   88]
per-ex loss: 0.361590  [   53/   88]
per-ex loss: 0.429122  [   54/   88]
per-ex loss: 0.410399  [   55/   88]
per-ex loss: 0.393335  [   56/   88]
per-ex loss: 0.487249  [   57/   88]
per-ex loss: 0.518908  [   58/   88]
per-ex loss: 0.345812  [   59/   88]
per-ex loss: 0.541904  [   60/   88]
per-ex loss: 0.571624  [   61/   88]
per-ex loss: 0.543324  [   62/   88]
per-ex loss: 0.388087  [   63/   88]
per-ex loss: 0.377377  [   64/   88]
per-ex loss: 0.322608  [   65/   88]
per-ex loss: 0.460007  [   66/   88]
per-ex loss: 0.625299  [   67/   88]
per-ex loss: 0.644321  [   68/   88]
per-ex loss: 0.426338  [   69/   88]
per-ex loss: 0.399848  [   70/   88]
per-ex loss: 0.292479  [   71/   88]
per-ex loss: 0.521689  [   72/   88]
per-ex loss: 0.391068  [   73/   88]
per-ex loss: 0.518544  [   74/   88]
per-ex loss: 0.376198  [   75/   88]
per-ex loss: 0.299899  [   76/   88]
per-ex loss: 0.576676  [   77/   88]
per-ex loss: 0.640307  [   78/   88]
per-ex loss: 0.539281  [   79/   88]
per-ex loss: 0.532460  [   80/   88]
per-ex loss: 0.405978  [   81/   88]
per-ex loss: 0.379955  [   82/   88]
per-ex loss: 0.319009  [   83/   88]
per-ex loss: 0.363605  [   84/   88]
per-ex loss: 0.516224  [   85/   88]
per-ex loss: 0.317127  [   86/   88]
per-ex loss: 0.426601  [   87/   88]
per-ex loss: 0.350293  [   88/   88]
Train Error: Avg loss: 0.44742644
validation Error: 
 Avg loss: 0.54039068 
 F1: 0.485723 
 Precision: 0.569523 
 Recall: 0.423421
 IoU: 0.320763

test Error: 
 Avg loss: 0.51059001 
 F1: 0.531450 
 Precision: 0.589732 
 Recall: 0.483652
 IoU: 0.361888

We have finished training iteration 96
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_94_.pth
per-ex loss: 0.543761  [    1/   88]
per-ex loss: 0.479567  [    2/   88]
per-ex loss: 0.542232  [    3/   88]
per-ex loss: 0.587926  [    4/   88]
per-ex loss: 0.409104  [    5/   88]
per-ex loss: 0.405534  [    6/   88]
per-ex loss: 0.531055  [    7/   88]
per-ex loss: 0.298479  [    8/   88]
per-ex loss: 0.322780  [    9/   88]
per-ex loss: 0.545863  [   10/   88]
per-ex loss: 0.598575  [   11/   88]
per-ex loss: 0.421732  [   12/   88]
per-ex loss: 0.505947  [   13/   88]
per-ex loss: 0.320479  [   14/   88]
per-ex loss: 0.328488  [   15/   88]
per-ex loss: 0.335001  [   16/   88]
per-ex loss: 0.621287  [   17/   88]
per-ex loss: 0.565404  [   18/   88]
per-ex loss: 0.411166  [   19/   88]
per-ex loss: 0.291945  [   20/   88]
per-ex loss: 0.371939  [   21/   88]
per-ex loss: 0.359877  [   22/   88]
per-ex loss: 0.365815  [   23/   88]
per-ex loss: 0.408020  [   24/   88]
per-ex loss: 0.300997  [   25/   88]
per-ex loss: 0.303867  [   26/   88]
per-ex loss: 0.625628  [   27/   88]
per-ex loss: 0.310060  [   28/   88]
per-ex loss: 0.614969  [   29/   88]
per-ex loss: 0.579504  [   30/   88]
per-ex loss: 0.459910  [   31/   88]
per-ex loss: 0.400099  [   32/   88]
per-ex loss: 0.478409  [   33/   88]
per-ex loss: 0.415395  [   34/   88]
per-ex loss: 0.447458  [   35/   88]
per-ex loss: 0.418693  [   36/   88]
per-ex loss: 0.296852  [   37/   88]
per-ex loss: 0.556672  [   38/   88]
per-ex loss: 0.550479  [   39/   88]
per-ex loss: 0.384370  [   40/   88]
per-ex loss: 0.319601  [   41/   88]
per-ex loss: 0.412774  [   42/   88]
per-ex loss: 0.418198  [   43/   88]
per-ex loss: 0.551636  [   44/   88]
per-ex loss: 0.460189  [   45/   88]
per-ex loss: 0.470127  [   46/   88]
per-ex loss: 0.380127  [   47/   88]
per-ex loss: 0.581336  [   48/   88]
per-ex loss: 0.360140  [   49/   88]
per-ex loss: 0.365723  [   50/   88]
per-ex loss: 0.531382  [   51/   88]
per-ex loss: 0.536627  [   52/   88]
per-ex loss: 0.540533  [   53/   88]
per-ex loss: 0.341643  [   54/   88]
per-ex loss: 0.468036  [   55/   88]
per-ex loss: 0.644348  [   56/   88]
per-ex loss: 0.553139  [   57/   88]
per-ex loss: 0.453210  [   58/   88]
per-ex loss: 0.499960  [   59/   88]
per-ex loss: 0.344646  [   60/   88]
per-ex loss: 0.288515  [   61/   88]
per-ex loss: 0.558188  [   62/   88]
per-ex loss: 0.356148  [   63/   88]
per-ex loss: 0.545281  [   64/   88]
per-ex loss: 0.347058  [   65/   88]
per-ex loss: 0.357053  [   66/   88]
per-ex loss: 0.464115  [   67/   88]
per-ex loss: 0.408154  [   68/   88]
per-ex loss: 0.404816  [   69/   88]
per-ex loss: 0.577944  [   70/   88]
per-ex loss: 0.374461  [   71/   88]
per-ex loss: 0.391763  [   72/   88]
per-ex loss: 0.514529  [   73/   88]
per-ex loss: 0.439659  [   74/   88]
per-ex loss: 0.463424  [   75/   88]
per-ex loss: 0.391089  [   76/   88]
per-ex loss: 0.381034  [   77/   88]
per-ex loss: 0.416994  [   78/   88]
per-ex loss: 0.440279  [   79/   88]
per-ex loss: 0.380067  [   80/   88]
per-ex loss: 0.657879  [   81/   88]
per-ex loss: 0.325289  [   82/   88]
per-ex loss: 0.413563  [   83/   88]
per-ex loss: 0.650327  [   84/   88]
per-ex loss: 0.319680  [   85/   88]
per-ex loss: 0.347002  [   86/   88]
per-ex loss: 0.320726  [   87/   88]
per-ex loss: 0.668372  [   88/   88]
Train Error: Avg loss: 0.44491047
validation Error: 
 Avg loss: 0.52298637 
 F1: 0.503230 
 Precision: 0.592073 
 Recall: 0.437571
 IoU: 0.336211

test Error: 
 Avg loss: 0.49291437 
 F1: 0.554230 
 Precision: 0.605277 
 Recall: 0.511123
 IoU: 0.383346

We have finished training iteration 97
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_95_.pth
per-ex loss: 0.383155  [    1/   88]
per-ex loss: 0.352453  [    2/   88]
per-ex loss: 0.343563  [    3/   88]
per-ex loss: 0.329068  [    4/   88]
per-ex loss: 0.336672  [    5/   88]
per-ex loss: 0.550929  [    6/   88]
per-ex loss: 0.494400  [    7/   88]
per-ex loss: 0.558701  [    8/   88]
per-ex loss: 0.599630  [    9/   88]
per-ex loss: 0.482736  [   10/   88]
per-ex loss: 0.460669  [   11/   88]
per-ex loss: 0.499819  [   12/   88]
per-ex loss: 0.324371  [   13/   88]
per-ex loss: 0.312066  [   14/   88]
per-ex loss: 0.541213  [   15/   88]
per-ex loss: 0.558592  [   16/   88]
per-ex loss: 0.558939  [   17/   88]
per-ex loss: 0.443577  [   18/   88]
per-ex loss: 0.425709  [   19/   88]
per-ex loss: 0.509470  [   20/   88]
per-ex loss: 0.556364  [   21/   88]
per-ex loss: 0.355516  [   22/   88]
per-ex loss: 0.385739  [   23/   88]
per-ex loss: 0.386800  [   24/   88]
per-ex loss: 0.378666  [   25/   88]
per-ex loss: 0.392260  [   26/   88]
per-ex loss: 0.334566  [   27/   88]
per-ex loss: 0.296366  [   28/   88]
per-ex loss: 0.380063  [   29/   88]
per-ex loss: 0.533139  [   30/   88]
per-ex loss: 0.318445  [   31/   88]
per-ex loss: 0.293023  [   32/   88]
per-ex loss: 0.667911  [   33/   88]
per-ex loss: 0.409729  [   34/   88]
per-ex loss: 0.571231  [   35/   88]
per-ex loss: 0.582273  [   36/   88]
per-ex loss: 0.518677  [   37/   88]
per-ex loss: 0.365841  [   38/   88]
per-ex loss: 0.400554  [   39/   88]
per-ex loss: 0.335831  [   40/   88]
per-ex loss: 0.352921  [   41/   88]
per-ex loss: 0.426825  [   42/   88]
per-ex loss: 0.519061  [   43/   88]
per-ex loss: 0.487955  [   44/   88]
per-ex loss: 0.446034  [   45/   88]
per-ex loss: 0.400789  [   46/   88]
per-ex loss: 0.583668  [   47/   88]
per-ex loss: 0.301093  [   48/   88]
per-ex loss: 0.628625  [   49/   88]
per-ex loss: 0.426806  [   50/   88]
per-ex loss: 0.373136  [   51/   88]
per-ex loss: 0.466594  [   52/   88]
per-ex loss: 0.607250  [   53/   88]
per-ex loss: 0.381028  [   54/   88]
per-ex loss: 0.358027  [   55/   88]
per-ex loss: 0.404758  [   56/   88]
per-ex loss: 0.401036  [   57/   88]
per-ex loss: 0.463386  [   58/   88]
per-ex loss: 0.311551  [   59/   88]
per-ex loss: 0.316090  [   60/   88]
per-ex loss: 0.669147  [   61/   88]
per-ex loss: 0.570255  [   62/   88]
per-ex loss: 0.626708  [   63/   88]
per-ex loss: 0.419149  [   64/   88]
per-ex loss: 0.545004  [   65/   88]
per-ex loss: 0.379312  [   66/   88]
per-ex loss: 0.443351  [   67/   88]
per-ex loss: 0.584240  [   68/   88]
per-ex loss: 0.523094  [   69/   88]
per-ex loss: 0.482401  [   70/   88]
per-ex loss: 0.585819  [   71/   88]
per-ex loss: 0.352462  [   72/   88]
per-ex loss: 0.591320  [   73/   88]
per-ex loss: 0.350941  [   74/   88]
per-ex loss: 0.433157  [   75/   88]
per-ex loss: 0.371240  [   76/   88]
per-ex loss: 0.442001  [   77/   88]
per-ex loss: 0.621442  [   78/   88]
per-ex loss: 0.412324  [   79/   88]
per-ex loss: 0.339752  [   80/   88]
per-ex loss: 0.253483  [   81/   88]
per-ex loss: 0.361168  [   82/   88]
per-ex loss: 0.344218  [   83/   88]
per-ex loss: 0.346782  [   84/   88]
per-ex loss: 0.544702  [   85/   88]
per-ex loss: 0.504939  [   86/   88]
per-ex loss: 0.404224  [   87/   88]
per-ex loss: 0.521787  [   88/   88]
Train Error: Avg loss: 0.44556572
validation Error: 
 Avg loss: 0.51948689 
 F1: 0.505487 
 Precision: 0.590221 
 Recall: 0.442028
 IoU: 0.338229

test Error: 
 Avg loss: 0.48650914 
 F1: 0.559684 
 Precision: 0.598606 
 Recall: 0.525515
 IoU: 0.388585

We have finished training iteration 98
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_96_.pth
per-ex loss: 0.419256  [    1/   88]
per-ex loss: 0.361201  [    2/   88]
per-ex loss: 0.533778  [    3/   88]
per-ex loss: 0.385111  [    4/   88]
per-ex loss: 0.394361  [    5/   88]
per-ex loss: 0.404892  [    6/   88]
per-ex loss: 0.500886  [    7/   88]
per-ex loss: 0.443078  [    8/   88]
per-ex loss: 0.552991  [    9/   88]
per-ex loss: 0.539124  [   10/   88]
per-ex loss: 0.306890  [   11/   88]
per-ex loss: 0.612354  [   12/   88]
per-ex loss: 0.524857  [   13/   88]
per-ex loss: 0.549465  [   14/   88]
per-ex loss: 0.330704  [   15/   88]
per-ex loss: 0.394774  [   16/   88]
per-ex loss: 0.414774  [   17/   88]
per-ex loss: 0.240386  [   18/   88]
per-ex loss: 0.561307  [   19/   88]
per-ex loss: 0.360885  [   20/   88]
per-ex loss: 0.341244  [   21/   88]
per-ex loss: 0.528025  [   22/   88]
per-ex loss: 0.439899  [   23/   88]
per-ex loss: 0.347512  [   24/   88]
per-ex loss: 0.331769  [   25/   88]
per-ex loss: 0.350281  [   26/   88]
per-ex loss: 0.516997  [   27/   88]
per-ex loss: 0.332549  [   28/   88]
per-ex loss: 0.315605  [   29/   88]
per-ex loss: 0.566902  [   30/   88]
per-ex loss: 0.507338  [   31/   88]
per-ex loss: 0.588212  [   32/   88]
per-ex loss: 0.442589  [   33/   88]
per-ex loss: 0.384754  [   34/   88]
per-ex loss: 0.364376  [   35/   88]
per-ex loss: 0.510090  [   36/   88]
per-ex loss: 0.359609  [   37/   88]
per-ex loss: 0.374104  [   38/   88]
per-ex loss: 0.633974  [   39/   88]
per-ex loss: 0.432053  [   40/   88]
per-ex loss: 0.334747  [   41/   88]
per-ex loss: 0.406267  [   42/   88]
per-ex loss: 0.381334  [   43/   88]
per-ex loss: 0.303606  [   44/   88]
per-ex loss: 0.372711  [   45/   88]
per-ex loss: 0.581631  [   46/   88]
per-ex loss: 0.409044  [   47/   88]
per-ex loss: 0.336759  [   48/   88]
per-ex loss: 0.635677  [   49/   88]
per-ex loss: 0.382220  [   50/   88]
per-ex loss: 0.476335  [   51/   88]
per-ex loss: 0.477904  [   52/   88]
per-ex loss: 0.536256  [   53/   88]
per-ex loss: 0.519388  [   54/   88]
per-ex loss: 0.613744  [   55/   88]
per-ex loss: 0.426629  [   56/   88]
per-ex loss: 0.331376  [   57/   88]
per-ex loss: 0.542740  [   58/   88]
per-ex loss: 0.355153  [   59/   88]
per-ex loss: 0.334662  [   60/   88]
per-ex loss: 0.632392  [   61/   88]
per-ex loss: 0.398342  [   62/   88]
per-ex loss: 0.396181  [   63/   88]
per-ex loss: 0.469152  [   64/   88]
per-ex loss: 0.334689  [   65/   88]
per-ex loss: 0.666654  [   66/   88]
per-ex loss: 0.299777  [   67/   88]
per-ex loss: 0.360170  [   68/   88]
per-ex loss: 0.494836  [   69/   88]
per-ex loss: 0.590395  [   70/   88]
per-ex loss: 0.603091  [   71/   88]
per-ex loss: 0.555468  [   72/   88]
per-ex loss: 0.547969  [   73/   88]
per-ex loss: 0.348219  [   74/   88]
per-ex loss: 0.416906  [   75/   88]
per-ex loss: 0.506400  [   76/   88]
per-ex loss: 0.589679  [   77/   88]
per-ex loss: 0.425223  [   78/   88]
per-ex loss: 0.324587  [   79/   88]
per-ex loss: 0.390390  [   80/   88]
per-ex loss: 0.476680  [   81/   88]
per-ex loss: 0.540060  [   82/   88]
per-ex loss: 0.438096  [   83/   88]
per-ex loss: 0.641108  [   84/   88]
per-ex loss: 0.411153  [   85/   88]
per-ex loss: 0.347839  [   86/   88]
per-ex loss: 0.519823  [   87/   88]
per-ex loss: 0.355444  [   88/   88]
Train Error: Avg loss: 0.44702116
validation Error: 
 Avg loss: 0.52033319 
 F1: 0.505590 
 Precision: 0.566120 
 Recall: 0.456754
 IoU: 0.338321

test Error: 
 Avg loss: 0.48543199 
 F1: 0.562733 
 Precision: 0.595921 
 Recall: 0.533046
 IoU: 0.391530

We have finished training iteration 99
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_97_.pth
per-ex loss: 0.516063  [    1/   88]
per-ex loss: 0.440813  [    2/   88]
per-ex loss: 0.329461  [    3/   88]
per-ex loss: 0.263617  [    4/   88]
per-ex loss: 0.299597  [    5/   88]
per-ex loss: 0.371197  [    6/   88]
per-ex loss: 0.320736  [    7/   88]
per-ex loss: 0.317000  [    8/   88]
per-ex loss: 0.384477  [    9/   88]
per-ex loss: 0.456026  [   10/   88]
per-ex loss: 0.427885  [   11/   88]
per-ex loss: 0.543133  [   12/   88]
per-ex loss: 0.307423  [   13/   88]
per-ex loss: 0.347716  [   14/   88]
per-ex loss: 0.348716  [   15/   88]
per-ex loss: 0.385059  [   16/   88]
per-ex loss: 0.581569  [   17/   88]
per-ex loss: 0.395352  [   18/   88]
per-ex loss: 0.404405  [   19/   88]
per-ex loss: 0.585217  [   20/   88]
per-ex loss: 0.459424  [   21/   88]
per-ex loss: 0.482291  [   22/   88]
per-ex loss: 0.372672  [   23/   88]
per-ex loss: 0.375409  [   24/   88]
per-ex loss: 0.388355  [   25/   88]
per-ex loss: 0.392029  [   26/   88]
per-ex loss: 0.446964  [   27/   88]
per-ex loss: 0.357206  [   28/   88]
per-ex loss: 0.320842  [   29/   88]
per-ex loss: 0.529016  [   30/   88]
per-ex loss: 0.398874  [   31/   88]
per-ex loss: 0.334609  [   32/   88]
per-ex loss: 0.465923  [   33/   88]
per-ex loss: 0.546381  [   34/   88]
per-ex loss: 0.544780  [   35/   88]
per-ex loss: 0.339629  [   36/   88]
per-ex loss: 0.339929  [   37/   88]
per-ex loss: 0.450815  [   38/   88]
per-ex loss: 0.389130  [   39/   88]
per-ex loss: 0.490431  [   40/   88]
per-ex loss: 0.637004  [   41/   88]
per-ex loss: 0.303988  [   42/   88]
per-ex loss: 0.620963  [   43/   88]
per-ex loss: 0.532692  [   44/   88]
per-ex loss: 0.487614  [   45/   88]
per-ex loss: 0.371240  [   46/   88]
per-ex loss: 0.528197  [   47/   88]
per-ex loss: 0.460767  [   48/   88]
per-ex loss: 0.648259  [   49/   88]
per-ex loss: 0.421090  [   50/   88]
per-ex loss: 0.561959  [   51/   88]
per-ex loss: 0.353874  [   52/   88]
per-ex loss: 0.395411  [   53/   88]
per-ex loss: 0.318591  [   54/   88]
per-ex loss: 0.594587  [   55/   88]
per-ex loss: 0.381096  [   56/   88]
per-ex loss: 0.527133  [   57/   88]
per-ex loss: 0.634692  [   58/   88]
per-ex loss: 0.529457  [   59/   88]
per-ex loss: 0.560299  [   60/   88]
per-ex loss: 0.349586  [   61/   88]
per-ex loss: 0.368918  [   62/   88]
per-ex loss: 0.352396  [   63/   88]
per-ex loss: 0.634265  [   64/   88]
per-ex loss: 0.560521  [   65/   88]
per-ex loss: 0.571962  [   66/   88]
per-ex loss: 0.597602  [   67/   88]
per-ex loss: 0.329560  [   68/   88]
per-ex loss: 0.428052  [   69/   88]
per-ex loss: 0.344514  [   70/   88]
per-ex loss: 0.384598  [   71/   88]
per-ex loss: 0.314217  [   72/   88]
per-ex loss: 0.567505  [   73/   88]
per-ex loss: 0.515735  [   74/   88]
per-ex loss: 0.448764  [   75/   88]
per-ex loss: 0.537043  [   76/   88]
per-ex loss: 0.425772  [   77/   88]
per-ex loss: 0.324973  [   78/   88]
per-ex loss: 0.529068  [   79/   88]
per-ex loss: 0.364627  [   80/   88]
per-ex loss: 0.390334  [   81/   88]
per-ex loss: 0.458307  [   82/   88]
per-ex loss: 0.362845  [   83/   88]
per-ex loss: 0.573784  [   84/   88]
per-ex loss: 0.669977  [   85/   88]
per-ex loss: 0.330913  [   86/   88]
per-ex loss: 0.375688  [   87/   88]
per-ex loss: 0.625086  [   88/   88]
Train Error: Avg loss: 0.44381474
validation Error: 
 Avg loss: 0.52514009 
 F1: 0.503972 
 Precision: 0.568983 
 Recall: 0.452295
 IoU: 0.336874

test Error: 
 Avg loss: 0.48303930 
 F1: 0.560475 
 Precision: 0.607040 
 Recall: 0.520545
 IoU: 0.389347

We have finished training iteration 100
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_98_.pth
per-ex loss: 0.373057  [    1/   88]
per-ex loss: 0.536470  [    2/   88]
per-ex loss: 0.583336  [    3/   88]
per-ex loss: 0.619061  [    4/   88]
per-ex loss: 0.374988  [    5/   88]
per-ex loss: 0.387436  [    6/   88]
per-ex loss: 0.633828  [    7/   88]
per-ex loss: 0.646387  [    8/   88]
per-ex loss: 0.359033  [    9/   88]
per-ex loss: 0.345386  [   10/   88]
per-ex loss: 0.321209  [   11/   88]
per-ex loss: 0.539743  [   12/   88]
per-ex loss: 0.470666  [   13/   88]
per-ex loss: 0.517826  [   14/   88]
per-ex loss: 0.331698  [   15/   88]
per-ex loss: 0.388137  [   16/   88]
per-ex loss: 0.519941  [   17/   88]
per-ex loss: 0.439264  [   18/   88]
per-ex loss: 0.334828  [   19/   88]
per-ex loss: 0.391533  [   20/   88]
per-ex loss: 0.585039  [   21/   88]
per-ex loss: 0.377518  [   22/   88]
per-ex loss: 0.565778  [   23/   88]
per-ex loss: 0.421527  [   24/   88]
per-ex loss: 0.552117  [   25/   88]
per-ex loss: 0.339128  [   26/   88]
per-ex loss: 0.310130  [   27/   88]
per-ex loss: 0.251447  [   28/   88]
per-ex loss: 0.454581  [   29/   88]
per-ex loss: 0.322311  [   30/   88]
per-ex loss: 0.327964  [   31/   88]
per-ex loss: 0.530823  [   32/   88]
per-ex loss: 0.491710  [   33/   88]
per-ex loss: 0.423852  [   34/   88]
per-ex loss: 0.369399  [   35/   88]
per-ex loss: 0.636818  [   36/   88]
per-ex loss: 0.419234  [   37/   88]
per-ex loss: 0.522378  [   38/   88]
per-ex loss: 0.525281  [   39/   88]
per-ex loss: 0.342027  [   40/   88]
per-ex loss: 0.387440  [   41/   88]
per-ex loss: 0.400027  [   42/   88]
per-ex loss: 0.327822  [   43/   88]
per-ex loss: 0.307157  [   44/   88]
per-ex loss: 0.594276  [   45/   88]
per-ex loss: 0.533593  [   46/   88]
per-ex loss: 0.391607  [   47/   88]
per-ex loss: 0.395014  [   48/   88]
per-ex loss: 0.594819  [   49/   88]
per-ex loss: 0.622500  [   50/   88]
per-ex loss: 0.579876  [   51/   88]
per-ex loss: 0.367910  [   52/   88]
per-ex loss: 0.371084  [   53/   88]
per-ex loss: 0.480928  [   54/   88]
per-ex loss: 0.457576  [   55/   88]
per-ex loss: 0.423290  [   56/   88]
per-ex loss: 0.454479  [   57/   88]
per-ex loss: 0.455220  [   58/   88]
per-ex loss: 0.419587  [   59/   88]
per-ex loss: 0.546611  [   60/   88]
per-ex loss: 0.387168  [   61/   88]
per-ex loss: 0.415032  [   62/   88]
per-ex loss: 0.496709  [   63/   88]
per-ex loss: 0.313771  [   64/   88]
per-ex loss: 0.544732  [   65/   88]
per-ex loss: 0.495681  [   66/   88]
per-ex loss: 0.487473  [   67/   88]
per-ex loss: 0.366842  [   68/   88]
per-ex loss: 0.386595  [   69/   88]
per-ex loss: 0.503036  [   70/   88]
per-ex loss: 0.321991  [   71/   88]
per-ex loss: 0.302593  [   72/   88]
per-ex loss: 0.553822  [   73/   88]
per-ex loss: 0.381566  [   74/   88]
per-ex loss: 0.416432  [   75/   88]
per-ex loss: 0.312100  [   76/   88]
per-ex loss: 0.662809  [   77/   88]
per-ex loss: 0.305547  [   78/   88]
per-ex loss: 0.592750  [   79/   88]
per-ex loss: 0.516969  [   80/   88]
per-ex loss: 0.493986  [   81/   88]
per-ex loss: 0.456932  [   82/   88]
per-ex loss: 0.385347  [   83/   88]
per-ex loss: 0.403981  [   84/   88]
per-ex loss: 0.517424  [   85/   88]
per-ex loss: 0.600566  [   86/   88]
per-ex loss: 0.395632  [   87/   88]
per-ex loss: 0.362655  [   88/   88]
Train Error: Avg loss: 0.44743004
validation Error: 
 Avg loss: 0.52042985 
 F1: 0.507928 
 Precision: 0.532744 
 Recall: 0.485321
 IoU: 0.340418

test Error: 
 Avg loss: 0.48316569 
 F1: 0.561438 
 Precision: 0.558515 
 Recall: 0.564391
 IoU: 0.390277

We have finished training iteration 101
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_99_.pth
per-ex loss: 0.360465  [    1/   88]
per-ex loss: 0.327298  [    2/   88]
per-ex loss: 0.519987  [    3/   88]
per-ex loss: 0.412261  [    4/   88]
per-ex loss: 0.597101  [    5/   88]
per-ex loss: 0.572441  [    6/   88]
per-ex loss: 0.490980  [    7/   88]
per-ex loss: 0.363423  [    8/   88]
per-ex loss: 0.497921  [    9/   88]
per-ex loss: 0.537165  [   10/   88]
per-ex loss: 0.621710  [   11/   88]
per-ex loss: 0.345824  [   12/   88]
per-ex loss: 0.365746  [   13/   88]
per-ex loss: 0.329083  [   14/   88]
per-ex loss: 0.289630  [   15/   88]
per-ex loss: 0.646280  [   16/   88]
per-ex loss: 0.525299  [   17/   88]
per-ex loss: 0.327015  [   18/   88]
per-ex loss: 0.359571  [   19/   88]
per-ex loss: 0.294275  [   20/   88]
per-ex loss: 0.452759  [   21/   88]
per-ex loss: 0.366277  [   22/   88]
per-ex loss: 0.324215  [   23/   88]
per-ex loss: 0.309926  [   24/   88]
per-ex loss: 0.601238  [   25/   88]
per-ex loss: 0.563438  [   26/   88]
per-ex loss: 0.408508  [   27/   88]
per-ex loss: 0.625424  [   28/   88]
per-ex loss: 0.359947  [   29/   88]
per-ex loss: 0.402124  [   30/   88]
per-ex loss: 0.580774  [   31/   88]
per-ex loss: 0.305804  [   32/   88]
per-ex loss: 0.360042  [   33/   88]
per-ex loss: 0.626984  [   34/   88]
per-ex loss: 0.416432  [   35/   88]
per-ex loss: 0.518838  [   36/   88]
per-ex loss: 0.336076  [   37/   88]
per-ex loss: 0.420835  [   38/   88]
per-ex loss: 0.375971  [   39/   88]
per-ex loss: 0.495039  [   40/   88]
per-ex loss: 0.323823  [   41/   88]
per-ex loss: 0.339220  [   42/   88]
per-ex loss: 0.413527  [   43/   88]
per-ex loss: 0.383321  [   44/   88]
per-ex loss: 0.389839  [   45/   88]
per-ex loss: 0.435184  [   46/   88]
per-ex loss: 0.520103  [   47/   88]
per-ex loss: 0.619463  [   48/   88]
per-ex loss: 0.545978  [   49/   88]
per-ex loss: 0.399150  [   50/   88]
per-ex loss: 0.356277  [   51/   88]
per-ex loss: 0.337046  [   52/   88]
per-ex loss: 0.454176  [   53/   88]
per-ex loss: 0.334467  [   54/   88]
per-ex loss: 0.339514  [   55/   88]
per-ex loss: 0.548900  [   56/   88]
per-ex loss: 0.238514  [   57/   88]
per-ex loss: 0.551151  [   58/   88]
per-ex loss: 0.300696  [   59/   88]
per-ex loss: 0.395517  [   60/   88]
per-ex loss: 0.541872  [   61/   88]
per-ex loss: 0.536215  [   62/   88]
per-ex loss: 0.474205  [   63/   88]
per-ex loss: 0.430436  [   64/   88]
per-ex loss: 0.521651  [   65/   88]
per-ex loss: 0.284252  [   66/   88]
per-ex loss: 0.294201  [   67/   88]
per-ex loss: 0.573374  [   68/   88]
per-ex loss: 0.378260  [   69/   88]
per-ex loss: 0.579405  [   70/   88]
per-ex loss: 0.432466  [   71/   88]
per-ex loss: 0.362782  [   72/   88]
per-ex loss: 0.536970  [   73/   88]
per-ex loss: 0.378357  [   74/   88]
per-ex loss: 0.344541  [   75/   88]
per-ex loss: 0.390833  [   76/   88]
per-ex loss: 0.549256  [   77/   88]
per-ex loss: 0.344060  [   78/   88]
per-ex loss: 0.642298  [   79/   88]
per-ex loss: 0.556913  [   80/   88]
per-ex loss: 0.426745  [   81/   88]
per-ex loss: 0.541063  [   82/   88]
per-ex loss: 0.487249  [   83/   88]
per-ex loss: 0.397236  [   84/   88]
per-ex loss: 0.465401  [   85/   88]
per-ex loss: 0.470536  [   86/   88]
per-ex loss: 0.373297  [   87/   88]
per-ex loss: 0.441575  [   88/   88]
Train Error: Avg loss: 0.43881189
validation Error: 
 Avg loss: 0.52886650 
 F1: 0.501901 
 Precision: 0.515386 
 Recall: 0.489104
 IoU: 0.335026

test Error: 
 Avg loss: 0.48541245 
 F1: 0.556987 
 Precision: 0.565050 
 Recall: 0.549150
 IoU: 0.385989

We have finished training iteration 102
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_100_.pth
per-ex loss: 0.399831  [    1/   88]
per-ex loss: 0.347974  [    2/   88]
per-ex loss: 0.271003  [    3/   88]
per-ex loss: 0.321540  [    4/   88]
per-ex loss: 0.378455  [    5/   88]
per-ex loss: 0.310909  [    6/   88]
per-ex loss: 0.557290  [    7/   88]
per-ex loss: 0.543278  [    8/   88]
per-ex loss: 0.637931  [    9/   88]
per-ex loss: 0.441548  [   10/   88]
per-ex loss: 0.357971  [   11/   88]
per-ex loss: 0.334393  [   12/   88]
per-ex loss: 0.493988  [   13/   88]
per-ex loss: 0.417056  [   14/   88]
per-ex loss: 0.420961  [   15/   88]
per-ex loss: 0.510399  [   16/   88]
per-ex loss: 0.531788  [   17/   88]
per-ex loss: 0.446350  [   18/   88]
per-ex loss: 0.293670  [   19/   88]
per-ex loss: 0.336576  [   20/   88]
per-ex loss: 0.407814  [   21/   88]
per-ex loss: 0.589614  [   22/   88]
per-ex loss: 0.424796  [   23/   88]
per-ex loss: 0.340576  [   24/   88]
per-ex loss: 0.543417  [   25/   88]
per-ex loss: 0.423841  [   26/   88]
per-ex loss: 0.328240  [   27/   88]
per-ex loss: 0.550910  [   28/   88]
per-ex loss: 0.416246  [   29/   88]
per-ex loss: 0.485371  [   30/   88]
per-ex loss: 0.503648  [   31/   88]
per-ex loss: 0.364545  [   32/   88]
per-ex loss: 0.292213  [   33/   88]
per-ex loss: 0.565492  [   34/   88]
per-ex loss: 0.514950  [   35/   88]
per-ex loss: 0.634670  [   36/   88]
per-ex loss: 0.455707  [   37/   88]
per-ex loss: 0.345120  [   38/   88]
per-ex loss: 0.549843  [   39/   88]
per-ex loss: 0.381907  [   40/   88]
per-ex loss: 0.440491  [   41/   88]
per-ex loss: 0.637602  [   42/   88]
per-ex loss: 0.328266  [   43/   88]
per-ex loss: 0.533452  [   44/   88]
per-ex loss: 0.528313  [   45/   88]
per-ex loss: 0.453430  [   46/   88]
per-ex loss: 0.500409  [   47/   88]
per-ex loss: 0.568090  [   48/   88]
per-ex loss: 0.540019  [   49/   88]
per-ex loss: 0.365301  [   50/   88]
per-ex loss: 0.522981  [   51/   88]
per-ex loss: 0.623050  [   52/   88]
per-ex loss: 0.459454  [   53/   88]
per-ex loss: 0.569546  [   54/   88]
per-ex loss: 0.353368  [   55/   88]
per-ex loss: 0.470808  [   56/   88]
per-ex loss: 0.248219  [   57/   88]
per-ex loss: 0.533775  [   58/   88]
per-ex loss: 0.461420  [   59/   88]
per-ex loss: 0.383293  [   60/   88]
per-ex loss: 0.380577  [   61/   88]
per-ex loss: 0.367652  [   62/   88]
per-ex loss: 0.387198  [   63/   88]
per-ex loss: 0.365417  [   64/   88]
per-ex loss: 0.331095  [   65/   88]
per-ex loss: 0.595797  [   66/   88]
per-ex loss: 0.538310  [   67/   88]
per-ex loss: 0.626032  [   68/   88]
per-ex loss: 0.344179  [   69/   88]
per-ex loss: 0.545699  [   70/   88]
per-ex loss: 0.333535  [   71/   88]
per-ex loss: 0.396330  [   72/   88]
per-ex loss: 0.456932  [   73/   88]
per-ex loss: 0.386043  [   74/   88]
per-ex loss: 0.557062  [   75/   88]
per-ex loss: 0.400735  [   76/   88]
per-ex loss: 0.381613  [   77/   88]
per-ex loss: 0.365276  [   78/   88]
per-ex loss: 0.506712  [   79/   88]
per-ex loss: 0.463709  [   80/   88]
per-ex loss: 0.413671  [   81/   88]
per-ex loss: 0.326478  [   82/   88]
per-ex loss: 0.325703  [   83/   88]
per-ex loss: 0.322576  [   84/   88]
per-ex loss: 0.335171  [   85/   88]
per-ex loss: 0.294631  [   86/   88]
per-ex loss: 0.426906  [   87/   88]
per-ex loss: 0.337718  [   88/   88]
Train Error: Avg loss: 0.43756674
validation Error: 
 Avg loss: 0.52480365 
 F1: 0.506299 
 Precision: 0.619133 
 Recall: 0.428252
 IoU: 0.338956

test Error: 
 Avg loss: 0.48681111 
 F1: 0.561127 
 Precision: 0.648274 
 Recall: 0.494635
 IoU: 0.389977

We have finished training iteration 103
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_101_.pth
per-ex loss: 0.477573  [    1/   88]
per-ex loss: 0.386749  [    2/   88]
per-ex loss: 0.364111  [    3/   88]
per-ex loss: 0.382999  [    4/   88]
per-ex loss: 0.450472  [    5/   88]
per-ex loss: 0.381099  [    6/   88]
per-ex loss: 0.383018  [    7/   88]
per-ex loss: 0.454168  [    8/   88]
per-ex loss: 0.567215  [    9/   88]
per-ex loss: 0.611005  [   10/   88]
per-ex loss: 0.391012  [   11/   88]
per-ex loss: 0.532024  [   12/   88]
per-ex loss: 0.350563  [   13/   88]
per-ex loss: 0.412072  [   14/   88]
per-ex loss: 0.374273  [   15/   88]
per-ex loss: 0.526366  [   16/   88]
per-ex loss: 0.368740  [   17/   88]
per-ex loss: 0.208167  [   18/   88]
per-ex loss: 0.521796  [   19/   88]
per-ex loss: 0.647524  [   20/   88]
per-ex loss: 0.307626  [   21/   88]
per-ex loss: 0.599141  [   22/   88]
per-ex loss: 0.360631  [   23/   88]
per-ex loss: 0.359188  [   24/   88]
per-ex loss: 0.611430  [   25/   88]
per-ex loss: 0.482735  [   26/   88]
per-ex loss: 0.416075  [   27/   88]
per-ex loss: 0.429473  [   28/   88]
per-ex loss: 0.416653  [   29/   88]
per-ex loss: 0.424353  [   30/   88]
per-ex loss: 0.334601  [   31/   88]
per-ex loss: 0.416891  [   32/   88]
per-ex loss: 0.389834  [   33/   88]
per-ex loss: 0.487410  [   34/   88]
per-ex loss: 0.559932  [   35/   88]
per-ex loss: 0.572466  [   36/   88]
per-ex loss: 0.509534  [   37/   88]
per-ex loss: 0.359861  [   38/   88]
per-ex loss: 0.476995  [   39/   88]
per-ex loss: 0.380440  [   40/   88]
per-ex loss: 0.390505  [   41/   88]
per-ex loss: 0.499855  [   42/   88]
per-ex loss: 0.529755  [   43/   88]
per-ex loss: 0.605665  [   44/   88]
per-ex loss: 0.405066  [   45/   88]
per-ex loss: 0.552832  [   46/   88]
per-ex loss: 0.590940  [   47/   88]
per-ex loss: 0.407843  [   48/   88]
per-ex loss: 0.378126  [   49/   88]
per-ex loss: 0.635739  [   50/   88]
per-ex loss: 0.436518  [   51/   88]
per-ex loss: 0.333435  [   52/   88]
per-ex loss: 0.296228  [   53/   88]
per-ex loss: 0.327017  [   54/   88]
per-ex loss: 0.420500  [   55/   88]
per-ex loss: 0.548398  [   56/   88]
per-ex loss: 0.517245  [   57/   88]
per-ex loss: 0.516229  [   58/   88]
per-ex loss: 0.357901  [   59/   88]
per-ex loss: 0.548302  [   60/   88]
per-ex loss: 0.301019  [   61/   88]
per-ex loss: 0.556278  [   62/   88]
per-ex loss: 0.294802  [   63/   88]
per-ex loss: 0.457781  [   64/   88]
per-ex loss: 0.372051  [   65/   88]
per-ex loss: 0.361249  [   66/   88]
per-ex loss: 0.376165  [   67/   88]
per-ex loss: 0.319366  [   68/   88]
per-ex loss: 0.326670  [   69/   88]
per-ex loss: 0.571768  [   70/   88]
per-ex loss: 0.523265  [   71/   88]
per-ex loss: 0.531958  [   72/   88]
per-ex loss: 0.317884  [   73/   88]
per-ex loss: 0.521737  [   74/   88]
per-ex loss: 0.361665  [   75/   88]
per-ex loss: 0.563502  [   76/   88]
per-ex loss: 0.547881  [   77/   88]
per-ex loss: 0.310877  [   78/   88]
per-ex loss: 0.326460  [   79/   88]
per-ex loss: 0.329573  [   80/   88]
per-ex loss: 0.417330  [   81/   88]
per-ex loss: 0.356271  [   82/   88]
per-ex loss: 0.448186  [   83/   88]
per-ex loss: 0.540896  [   84/   88]
per-ex loss: 0.633206  [   85/   88]
per-ex loss: 0.331692  [   86/   88]
per-ex loss: 0.339151  [   87/   88]
per-ex loss: 0.423104  [   88/   88]
Train Error: Avg loss: 0.44029656
validation Error: 
 Avg loss: 0.51665615 
 F1: 0.513320 
 Precision: 0.594725 
 Recall: 0.451516
 IoU: 0.345279

test Error: 
 Avg loss: 0.47746336 
 F1: 0.571289 
 Precision: 0.612274 
 Recall: 0.535447
 IoU: 0.399863

We have finished training iteration 104
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_102_.pth
per-ex loss: 0.484051  [    1/   88]
per-ex loss: 0.330348  [    2/   88]
per-ex loss: 0.428931  [    3/   88]
per-ex loss: 0.384249  [    4/   88]
per-ex loss: 0.360333  [    5/   88]
per-ex loss: 0.522030  [    6/   88]
per-ex loss: 0.377758  [    7/   88]
per-ex loss: 0.567723  [    8/   88]
per-ex loss: 0.383567  [    9/   88]
per-ex loss: 0.293818  [   10/   88]
per-ex loss: 0.551867  [   11/   88]
per-ex loss: 0.314450  [   12/   88]
per-ex loss: 0.413277  [   13/   88]
per-ex loss: 0.382659  [   14/   88]
per-ex loss: 0.384480  [   15/   88]
per-ex loss: 0.335940  [   16/   88]
per-ex loss: 0.513476  [   17/   88]
per-ex loss: 0.343363  [   18/   88]
per-ex loss: 0.418614  [   19/   88]
per-ex loss: 0.307296  [   20/   88]
per-ex loss: 0.627863  [   21/   88]
per-ex loss: 0.399957  [   22/   88]
per-ex loss: 0.439554  [   23/   88]
per-ex loss: 0.348417  [   24/   88]
per-ex loss: 0.549732  [   25/   88]
per-ex loss: 0.519254  [   26/   88]
per-ex loss: 0.415943  [   27/   88]
per-ex loss: 0.638476  [   28/   88]
per-ex loss: 0.348781  [   29/   88]
per-ex loss: 0.602006  [   30/   88]
per-ex loss: 0.400930  [   31/   88]
per-ex loss: 0.408701  [   32/   88]
per-ex loss: 0.531868  [   33/   88]
per-ex loss: 0.486135  [   34/   88]
per-ex loss: 0.451431  [   35/   88]
per-ex loss: 0.380603  [   36/   88]
per-ex loss: 0.626716  [   37/   88]
per-ex loss: 0.414632  [   38/   88]
per-ex loss: 0.366591  [   39/   88]
per-ex loss: 0.340878  [   40/   88]
per-ex loss: 0.311729  [   41/   88]
per-ex loss: 0.530778  [   42/   88]
per-ex loss: 0.371132  [   43/   88]
per-ex loss: 0.611583  [   44/   88]
per-ex loss: 0.323429  [   45/   88]
per-ex loss: 0.619754  [   46/   88]
per-ex loss: 0.304491  [   47/   88]
per-ex loss: 0.300090  [   48/   88]
per-ex loss: 0.455935  [   49/   88]
per-ex loss: 0.335002  [   50/   88]
per-ex loss: 0.430364  [   51/   88]
per-ex loss: 0.579078  [   52/   88]
per-ex loss: 0.400011  [   53/   88]
per-ex loss: 0.382080  [   54/   88]
per-ex loss: 0.353708  [   55/   88]
per-ex loss: 0.516520  [   56/   88]
per-ex loss: 0.379558  [   57/   88]
per-ex loss: 0.358457  [   58/   88]
per-ex loss: 0.462149  [   59/   88]
per-ex loss: 0.374416  [   60/   88]
per-ex loss: 0.348614  [   61/   88]
per-ex loss: 0.525194  [   62/   88]
per-ex loss: 0.390638  [   63/   88]
per-ex loss: 0.567865  [   64/   88]
per-ex loss: 0.326697  [   65/   88]
per-ex loss: 0.512531  [   66/   88]
per-ex loss: 0.465688  [   67/   88]
per-ex loss: 0.415435  [   68/   88]
per-ex loss: 0.320083  [   69/   88]
per-ex loss: 0.626014  [   70/   88]
per-ex loss: 0.272829  [   71/   88]
per-ex loss: 0.559768  [   72/   88]
per-ex loss: 0.532783  [   73/   88]
per-ex loss: 0.489434  [   74/   88]
per-ex loss: 0.563012  [   75/   88]
per-ex loss: 0.311408  [   76/   88]
per-ex loss: 0.335276  [   77/   88]
per-ex loss: 0.443658  [   78/   88]
per-ex loss: 0.360834  [   79/   88]
per-ex loss: 0.350713  [   80/   88]
per-ex loss: 0.383851  [   81/   88]
per-ex loss: 0.552970  [   82/   88]
per-ex loss: 0.364933  [   83/   88]
per-ex loss: 0.528857  [   84/   88]
per-ex loss: 0.548411  [   85/   88]
per-ex loss: 0.491500  [   86/   88]
per-ex loss: 0.458126  [   87/   88]
per-ex loss: 0.550151  [   88/   88]
Train Error: Avg loss: 0.43629810
validation Error: 
 Avg loss: 0.52295151 
 F1: 0.509337 
 Precision: 0.568395 
 Recall: 0.461396
 IoU: 0.341684

test Error: 
 Avg loss: 0.47857634 
 F1: 0.566028 
 Precision: 0.599050 
 Recall: 0.536457
 IoU: 0.394728

We have finished training iteration 105
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_103_.pth
per-ex loss: 0.560290  [    1/   88]
per-ex loss: 0.557341  [    2/   88]
per-ex loss: 0.362497  [    3/   88]
per-ex loss: 0.427837  [    4/   88]
per-ex loss: 0.340039  [    5/   88]
per-ex loss: 0.493009  [    6/   88]
per-ex loss: 0.374156  [    7/   88]
per-ex loss: 0.644639  [    8/   88]
per-ex loss: 0.534644  [    9/   88]
per-ex loss: 0.348646  [   10/   88]
per-ex loss: 0.388203  [   11/   88]
per-ex loss: 0.410610  [   12/   88]
per-ex loss: 0.589966  [   13/   88]
per-ex loss: 0.539771  [   14/   88]
per-ex loss: 0.411907  [   15/   88]
per-ex loss: 0.617059  [   16/   88]
per-ex loss: 0.571638  [   17/   88]
per-ex loss: 0.578784  [   18/   88]
per-ex loss: 0.373889  [   19/   88]
per-ex loss: 0.613116  [   20/   88]
per-ex loss: 0.556739  [   21/   88]
per-ex loss: 0.436325  [   22/   88]
per-ex loss: 0.309757  [   23/   88]
per-ex loss: 0.474018  [   24/   88]
per-ex loss: 0.585023  [   25/   88]
per-ex loss: 0.387132  [   26/   88]
per-ex loss: 0.636428  [   27/   88]
per-ex loss: 0.615702  [   28/   88]
per-ex loss: 0.427921  [   29/   88]
per-ex loss: 0.454079  [   30/   88]
per-ex loss: 0.356672  [   31/   88]
per-ex loss: 0.380349  [   32/   88]
per-ex loss: 0.517653  [   33/   88]
per-ex loss: 0.307923  [   34/   88]
per-ex loss: 0.553467  [   35/   88]
per-ex loss: 0.402569  [   36/   88]
per-ex loss: 0.523350  [   37/   88]
per-ex loss: 0.482737  [   38/   88]
per-ex loss: 0.649154  [   39/   88]
per-ex loss: 0.444475  [   40/   88]
per-ex loss: 0.334717  [   41/   88]
per-ex loss: 0.524264  [   42/   88]
per-ex loss: 0.395950  [   43/   88]
per-ex loss: 0.423709  [   44/   88]
per-ex loss: 0.462248  [   45/   88]
per-ex loss: 0.327881  [   46/   88]
per-ex loss: 0.341533  [   47/   88]
per-ex loss: 0.397106  [   48/   88]
per-ex loss: 0.335942  [   49/   88]
per-ex loss: 0.389378  [   50/   88]
per-ex loss: 0.473168  [   51/   88]
per-ex loss: 0.535994  [   52/   88]
per-ex loss: 0.315513  [   53/   88]
per-ex loss: 0.322406  [   54/   88]
per-ex loss: 0.509626  [   55/   88]
per-ex loss: 0.342130  [   56/   88]
per-ex loss: 0.508909  [   57/   88]
per-ex loss: 0.360285  [   58/   88]
per-ex loss: 0.429323  [   59/   88]
per-ex loss: 0.327560  [   60/   88]
per-ex loss: 0.351875  [   61/   88]
per-ex loss: 0.372015  [   62/   88]
per-ex loss: 0.289027  [   63/   88]
per-ex loss: 0.525985  [   64/   88]
per-ex loss: 0.457802  [   65/   88]
per-ex loss: 0.336039  [   66/   88]
per-ex loss: 0.371703  [   67/   88]
per-ex loss: 0.419831  [   68/   88]
per-ex loss: 0.331763  [   69/   88]
per-ex loss: 0.469003  [   70/   88]
per-ex loss: 0.556558  [   71/   88]
per-ex loss: 0.295230  [   72/   88]
per-ex loss: 0.230172  [   73/   88]
per-ex loss: 0.403530  [   74/   88]
per-ex loss: 0.533724  [   75/   88]
per-ex loss: 0.490843  [   76/   88]
per-ex loss: 0.557575  [   77/   88]
per-ex loss: 0.341248  [   78/   88]
per-ex loss: 0.535339  [   79/   88]
per-ex loss: 0.319370  [   80/   88]
per-ex loss: 0.320142  [   81/   88]
per-ex loss: 0.455097  [   82/   88]
per-ex loss: 0.419346  [   83/   88]
per-ex loss: 0.364635  [   84/   88]
per-ex loss: 0.512463  [   85/   88]
per-ex loss: 0.390783  [   86/   88]
per-ex loss: 0.343865  [   87/   88]
per-ex loss: 0.444168  [   88/   88]
Train Error: Avg loss: 0.44018507
validation Error: 
 Avg loss: 0.52599334 
 F1: 0.502594 
 Precision: 0.580526 
 Recall: 0.443110
 IoU: 0.335643

test Error: 
 Avg loss: 0.48696418 
 F1: 0.555938 
 Precision: 0.612683 
 Recall: 0.508813
 IoU: 0.384982

We have finished training iteration 106
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_90_.pth
per-ex loss: 0.371355  [    1/   88]
per-ex loss: 0.483545  [    2/   88]
per-ex loss: 0.415114  [    3/   88]
per-ex loss: 0.563613  [    4/   88]
per-ex loss: 0.366506  [    5/   88]
per-ex loss: 0.560313  [    6/   88]
per-ex loss: 0.370786  [    7/   88]
per-ex loss: 0.613566  [    8/   88]
per-ex loss: 0.562134  [    9/   88]
per-ex loss: 0.576637  [   10/   88]
per-ex loss: 0.372295  [   11/   88]
per-ex loss: 0.391593  [   12/   88]
per-ex loss: 0.360405  [   13/   88]
per-ex loss: 0.593784  [   14/   88]
per-ex loss: 0.335605  [   15/   88]
per-ex loss: 0.430416  [   16/   88]
per-ex loss: 0.401016  [   17/   88]
per-ex loss: 0.303565  [   18/   88]
per-ex loss: 0.601520  [   19/   88]
per-ex loss: 0.437571  [   20/   88]
per-ex loss: 0.330407  [   21/   88]
per-ex loss: 0.423800  [   22/   88]
per-ex loss: 0.349591  [   23/   88]
per-ex loss: 0.330640  [   24/   88]
per-ex loss: 0.428672  [   25/   88]
per-ex loss: 0.367085  [   26/   88]
per-ex loss: 0.544963  [   27/   88]
per-ex loss: 0.353114  [   28/   88]
per-ex loss: 0.342072  [   29/   88]
per-ex loss: 0.433166  [   30/   88]
per-ex loss: 0.379972  [   31/   88]
per-ex loss: 0.393120  [   32/   88]
per-ex loss: 0.398817  [   33/   88]
per-ex loss: 0.280753  [   34/   88]
per-ex loss: 0.505537  [   35/   88]
per-ex loss: 0.375961  [   36/   88]
per-ex loss: 0.427475  [   37/   88]
per-ex loss: 0.357414  [   38/   88]
per-ex loss: 0.256335  [   39/   88]
per-ex loss: 0.350633  [   40/   88]
per-ex loss: 0.590699  [   41/   88]
per-ex loss: 0.639862  [   42/   88]
per-ex loss: 0.316242  [   43/   88]
per-ex loss: 0.364296  [   44/   88]
per-ex loss: 0.564603  [   45/   88]
per-ex loss: 0.522162  [   46/   88]
per-ex loss: 0.445876  [   47/   88]
per-ex loss: 0.515719  [   48/   88]
per-ex loss: 0.620727  [   49/   88]
per-ex loss: 0.424762  [   50/   88]
per-ex loss: 0.360523  [   51/   88]
per-ex loss: 0.459406  [   52/   88]
per-ex loss: 0.403170  [   53/   88]
per-ex loss: 0.355156  [   54/   88]
per-ex loss: 0.509238  [   55/   88]
per-ex loss: 0.577114  [   56/   88]
per-ex loss: 0.532767  [   57/   88]
per-ex loss: 0.540034  [   58/   88]
per-ex loss: 0.334211  [   59/   88]
per-ex loss: 0.478621  [   60/   88]
per-ex loss: 0.297254  [   61/   88]
per-ex loss: 0.325906  [   62/   88]
per-ex loss: 0.297215  [   63/   88]
per-ex loss: 0.364866  [   64/   88]
per-ex loss: 0.339100  [   65/   88]
per-ex loss: 0.387171  [   66/   88]
per-ex loss: 0.480216  [   67/   88]
per-ex loss: 0.461680  [   68/   88]
per-ex loss: 0.317410  [   69/   88]
per-ex loss: 0.575180  [   70/   88]
per-ex loss: 0.553264  [   71/   88]
per-ex loss: 0.647042  [   72/   88]
per-ex loss: 0.394963  [   73/   88]
per-ex loss: 0.321216  [   74/   88]
per-ex loss: 0.328219  [   75/   88]
per-ex loss: 0.320600  [   76/   88]
per-ex loss: 0.413921  [   77/   88]
per-ex loss: 0.334514  [   78/   88]
per-ex loss: 0.574391  [   79/   88]
per-ex loss: 0.532760  [   80/   88]
per-ex loss: 0.424668  [   81/   88]
per-ex loss: 0.547496  [   82/   88]
per-ex loss: 0.526991  [   83/   88]
per-ex loss: 0.554579  [   84/   88]
per-ex loss: 0.415849  [   85/   88]
per-ex loss: 0.384433  [   86/   88]
per-ex loss: 0.499199  [   87/   88]
per-ex loss: 0.628247  [   88/   88]
Train Error: Avg loss: 0.43800460
validation Error: 
 Avg loss: 0.52937704 
 F1: 0.497751 
 Precision: 0.643377 
 Recall: 0.405882
 IoU: 0.331338

test Error: 
 Avg loss: 0.48879964 
 F1: 0.555853 
 Precision: 0.668684 
 Recall: 0.475602
 IoU: 0.384901

We have finished training iteration 107
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_105_.pth
per-ex loss: 0.374258  [    1/   88]
per-ex loss: 0.419504  [    2/   88]
per-ex loss: 0.373401  [    3/   88]
per-ex loss: 0.544432  [    4/   88]
per-ex loss: 0.556437  [    5/   88]
per-ex loss: 0.430665  [    6/   88]
per-ex loss: 0.377485  [    7/   88]
per-ex loss: 0.356020  [    8/   88]
per-ex loss: 0.328545  [    9/   88]
per-ex loss: 0.567373  [   10/   88]
per-ex loss: 0.475787  [   11/   88]
per-ex loss: 0.487679  [   12/   88]
per-ex loss: 0.337796  [   13/   88]
per-ex loss: 0.530168  [   14/   88]
per-ex loss: 0.380481  [   15/   88]
per-ex loss: 0.601835  [   16/   88]
per-ex loss: 0.292900  [   17/   88]
per-ex loss: 0.454021  [   18/   88]
per-ex loss: 0.736269  [   19/   88]
per-ex loss: 0.510056  [   20/   88]
per-ex loss: 0.348838  [   21/   88]
per-ex loss: 0.564286  [   22/   88]
per-ex loss: 0.427718  [   23/   88]
per-ex loss: 0.346297  [   24/   88]
per-ex loss: 0.372627  [   25/   88]
per-ex loss: 0.222391  [   26/   88]
per-ex loss: 0.613467  [   27/   88]
per-ex loss: 0.610060  [   28/   88]
per-ex loss: 0.453086  [   29/   88]
per-ex loss: 0.512923  [   30/   88]
per-ex loss: 0.446381  [   31/   88]
per-ex loss: 0.330195  [   32/   88]
per-ex loss: 0.437274  [   33/   88]
per-ex loss: 0.620400  [   34/   88]
per-ex loss: 0.385870  [   35/   88]
per-ex loss: 0.569239  [   36/   88]
per-ex loss: 0.525707  [   37/   88]
per-ex loss: 0.489563  [   38/   88]
per-ex loss: 0.371783  [   39/   88]
per-ex loss: 0.406212  [   40/   88]
per-ex loss: 0.513358  [   41/   88]
per-ex loss: 0.300742  [   42/   88]
per-ex loss: 0.319440  [   43/   88]
per-ex loss: 0.520471  [   44/   88]
per-ex loss: 0.316207  [   45/   88]
per-ex loss: 0.545993  [   46/   88]
per-ex loss: 0.409087  [   47/   88]
per-ex loss: 0.348590  [   48/   88]
per-ex loss: 0.324139  [   49/   88]
per-ex loss: 0.435611  [   50/   88]
per-ex loss: 0.375434  [   51/   88]
per-ex loss: 0.596438  [   52/   88]
per-ex loss: 0.567100  [   53/   88]
per-ex loss: 0.484198  [   54/   88]
per-ex loss: 0.311640  [   55/   88]
per-ex loss: 0.294798  [   56/   88]
per-ex loss: 0.519385  [   57/   88]
per-ex loss: 0.523384  [   58/   88]
per-ex loss: 0.353018  [   59/   88]
per-ex loss: 0.569543  [   60/   88]
per-ex loss: 0.594899  [   61/   88]
per-ex loss: 0.519504  [   62/   88]
per-ex loss: 0.332652  [   63/   88]
per-ex loss: 0.416617  [   64/   88]
per-ex loss: 0.430669  [   65/   88]
per-ex loss: 0.617688  [   66/   88]
per-ex loss: 0.395310  [   67/   88]
per-ex loss: 0.353088  [   68/   88]
per-ex loss: 0.393451  [   69/   88]
per-ex loss: 0.636760  [   70/   88]
per-ex loss: 0.542697  [   71/   88]
per-ex loss: 0.420862  [   72/   88]
per-ex loss: 0.447821  [   73/   88]
per-ex loss: 0.350192  [   74/   88]
per-ex loss: 0.327154  [   75/   88]
per-ex loss: 0.337562  [   76/   88]
per-ex loss: 0.383535  [   77/   88]
per-ex loss: 0.333577  [   78/   88]
per-ex loss: 0.328879  [   79/   88]
per-ex loss: 0.566279  [   80/   88]
per-ex loss: 0.315416  [   81/   88]
per-ex loss: 0.377228  [   82/   88]
per-ex loss: 0.374838  [   83/   88]
per-ex loss: 0.404519  [   84/   88]
per-ex loss: 0.380269  [   85/   88]
per-ex loss: 0.371914  [   86/   88]
per-ex loss: 0.427777  [   87/   88]
per-ex loss: 0.419270  [   88/   88]
Train Error: Avg loss: 0.43884583
validation Error: 
 Avg loss: 0.51846906 
 F1: 0.508834 
 Precision: 0.609950 
 Recall: 0.436475
 IoU: 0.341232

test Error: 
 Avg loss: 0.48405753 
 F1: 0.564422 
 Precision: 0.641712 
 Recall: 0.503749
 IoU: 0.393167

We have finished training iteration 108
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_106_.pth
per-ex loss: 0.346312  [    1/   88]
per-ex loss: 0.533160  [    2/   88]
per-ex loss: 0.374208  [    3/   88]
per-ex loss: 0.290953  [    4/   88]
per-ex loss: 0.548937  [    5/   88]
per-ex loss: 0.491872  [    6/   88]
per-ex loss: 0.383793  [    7/   88]
per-ex loss: 0.447242  [    8/   88]
per-ex loss: 0.614784  [    9/   88]
per-ex loss: 0.418145  [   10/   88]
per-ex loss: 0.392381  [   11/   88]
per-ex loss: 0.342752  [   12/   88]
per-ex loss: 0.508759  [   13/   88]
per-ex loss: 0.360114  [   14/   88]
per-ex loss: 0.356386  [   15/   88]
per-ex loss: 0.635035  [   16/   88]
per-ex loss: 0.592578  [   17/   88]
per-ex loss: 0.323716  [   18/   88]
per-ex loss: 0.298473  [   19/   88]
per-ex loss: 0.408653  [   20/   88]
per-ex loss: 0.574960  [   21/   88]
per-ex loss: 0.521485  [   22/   88]
per-ex loss: 0.387677  [   23/   88]
per-ex loss: 0.453097  [   24/   88]
per-ex loss: 0.321209  [   25/   88]
per-ex loss: 0.637672  [   26/   88]
per-ex loss: 0.557141  [   27/   88]
per-ex loss: 0.315279  [   28/   88]
per-ex loss: 0.500935  [   29/   88]
per-ex loss: 0.519387  [   30/   88]
per-ex loss: 0.292818  [   31/   88]
per-ex loss: 0.571940  [   32/   88]
per-ex loss: 0.499723  [   33/   88]
per-ex loss: 0.298345  [   34/   88]
per-ex loss: 0.530207  [   35/   88]
per-ex loss: 0.304829  [   36/   88]
per-ex loss: 0.369780  [   37/   88]
per-ex loss: 0.328287  [   38/   88]
per-ex loss: 0.432582  [   39/   88]
per-ex loss: 0.452546  [   40/   88]
per-ex loss: 0.357814  [   41/   88]
per-ex loss: 0.608559  [   42/   88]
per-ex loss: 0.352535  [   43/   88]
per-ex loss: 0.445079  [   44/   88]
per-ex loss: 0.573317  [   45/   88]
per-ex loss: 0.564565  [   46/   88]
per-ex loss: 0.545788  [   47/   88]
per-ex loss: 0.316641  [   48/   88]
per-ex loss: 0.330357  [   49/   88]
per-ex loss: 0.499618  [   50/   88]
per-ex loss: 0.317061  [   51/   88]
per-ex loss: 0.538427  [   52/   88]
per-ex loss: 0.403395  [   53/   88]
per-ex loss: 0.555307  [   54/   88]
per-ex loss: 0.291082  [   55/   88]
per-ex loss: 0.369464  [   56/   88]
per-ex loss: 0.422095  [   57/   88]
per-ex loss: 0.396363  [   58/   88]
per-ex loss: 0.406390  [   59/   88]
per-ex loss: 0.409617  [   60/   88]
per-ex loss: 0.475624  [   61/   88]
per-ex loss: 0.368485  [   62/   88]
per-ex loss: 0.551479  [   63/   88]
per-ex loss: 0.386762  [   64/   88]
per-ex loss: 0.520426  [   65/   88]
per-ex loss: 0.471111  [   66/   88]
per-ex loss: 0.546846  [   67/   88]
per-ex loss: 0.379309  [   68/   88]
per-ex loss: 0.467405  [   69/   88]
per-ex loss: 0.621236  [   70/   88]
per-ex loss: 0.351730  [   71/   88]
per-ex loss: 0.359676  [   72/   88]
per-ex loss: 0.535736  [   73/   88]
per-ex loss: 0.322390  [   74/   88]
per-ex loss: 0.522437  [   75/   88]
per-ex loss: 0.326692  [   76/   88]
per-ex loss: 0.418189  [   77/   88]
per-ex loss: 0.612244  [   78/   88]
per-ex loss: 0.486630  [   79/   88]
per-ex loss: 0.333809  [   80/   88]
per-ex loss: 0.455076  [   81/   88]
per-ex loss: 0.419454  [   82/   88]
per-ex loss: 0.400538  [   83/   88]
per-ex loss: 0.511411  [   84/   88]
per-ex loss: 0.248435  [   85/   88]
per-ex loss: 0.411994  [   86/   88]
per-ex loss: 0.426461  [   87/   88]
per-ex loss: 0.334924  [   88/   88]
Train Error: Avg loss: 0.43759246
validation Error: 
 Avg loss: 0.51656788 
 F1: 0.512145 
 Precision: 0.592050 
 Recall: 0.451244
 IoU: 0.344217

test Error: 
 Avg loss: 0.47810251 
 F1: 0.567455 
 Precision: 0.609585 
 Recall: 0.530773
 IoU: 0.396117

We have finished training iteration 109
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_107_.pth
per-ex loss: 0.554423  [    1/   88]
per-ex loss: 0.322775  [    2/   88]
per-ex loss: 0.322505  [    3/   88]
per-ex loss: 0.360959  [    4/   88]
per-ex loss: 0.646313  [    5/   88]
per-ex loss: 0.659004  [    6/   88]
per-ex loss: 0.555400  [    7/   88]
per-ex loss: 0.566243  [    8/   88]
per-ex loss: 0.326365  [    9/   88]
per-ex loss: 0.409918  [   10/   88]
per-ex loss: 0.412945  [   11/   88]
per-ex loss: 0.328370  [   12/   88]
per-ex loss: 0.267815  [   13/   88]
per-ex loss: 0.308556  [   14/   88]
per-ex loss: 0.517836  [   15/   88]
per-ex loss: 0.334542  [   16/   88]
per-ex loss: 0.358278  [   17/   88]
per-ex loss: 0.650226  [   18/   88]
per-ex loss: 0.318677  [   19/   88]
per-ex loss: 0.344567  [   20/   88]
per-ex loss: 0.495635  [   21/   88]
per-ex loss: 0.332658  [   22/   88]
per-ex loss: 0.448907  [   23/   88]
per-ex loss: 0.389796  [   24/   88]
per-ex loss: 0.501708  [   25/   88]
per-ex loss: 0.352430  [   26/   88]
per-ex loss: 0.571704  [   27/   88]
per-ex loss: 0.632021  [   28/   88]
per-ex loss: 0.540722  [   29/   88]
per-ex loss: 0.504711  [   30/   88]
per-ex loss: 0.417108  [   31/   88]
per-ex loss: 0.350253  [   32/   88]
per-ex loss: 0.455259  [   33/   88]
per-ex loss: 0.616668  [   34/   88]
per-ex loss: 0.619155  [   35/   88]
per-ex loss: 0.468647  [   36/   88]
per-ex loss: 0.506187  [   37/   88]
per-ex loss: 0.592407  [   38/   88]
per-ex loss: 0.322637  [   39/   88]
per-ex loss: 0.556479  [   40/   88]
per-ex loss: 0.365235  [   41/   88]
per-ex loss: 0.387444  [   42/   88]
per-ex loss: 0.386867  [   43/   88]
per-ex loss: 0.379006  [   44/   88]
per-ex loss: 0.482431  [   45/   88]
per-ex loss: 0.440046  [   46/   88]
per-ex loss: 0.438491  [   47/   88]
per-ex loss: 0.291335  [   48/   88]
per-ex loss: 0.355984  [   49/   88]
per-ex loss: 0.286271  [   50/   88]
per-ex loss: 0.509218  [   51/   88]
per-ex loss: 0.396031  [   52/   88]
per-ex loss: 0.358691  [   53/   88]
per-ex loss: 0.553174  [   54/   88]
per-ex loss: 0.312569  [   55/   88]
per-ex loss: 0.616376  [   56/   88]
per-ex loss: 0.453004  [   57/   88]
per-ex loss: 0.515020  [   58/   88]
per-ex loss: 0.383673  [   59/   88]
per-ex loss: 0.328124  [   60/   88]
per-ex loss: 0.584166  [   61/   88]
per-ex loss: 0.314655  [   62/   88]
per-ex loss: 0.383255  [   63/   88]
per-ex loss: 0.611332  [   64/   88]
per-ex loss: 0.520986  [   65/   88]
per-ex loss: 0.553543  [   66/   88]
per-ex loss: 0.398748  [   67/   88]
per-ex loss: 0.410205  [   68/   88]
per-ex loss: 0.397558  [   69/   88]
per-ex loss: 0.378583  [   70/   88]
per-ex loss: 0.325824  [   71/   88]
per-ex loss: 0.435513  [   72/   88]
per-ex loss: 0.505942  [   73/   88]
per-ex loss: 0.458086  [   74/   88]
per-ex loss: 0.350020  [   75/   88]
per-ex loss: 0.391765  [   76/   88]
per-ex loss: 0.359232  [   77/   88]
per-ex loss: 0.414215  [   78/   88]
per-ex loss: 0.349213  [   79/   88]
per-ex loss: 0.650894  [   80/   88]
per-ex loss: 0.338854  [   81/   88]
per-ex loss: 0.469146  [   82/   88]
per-ex loss: 0.334479  [   83/   88]
per-ex loss: 0.530069  [   84/   88]
per-ex loss: 0.545994  [   85/   88]
per-ex loss: 0.537739  [   86/   88]
per-ex loss: 0.407683  [   87/   88]
per-ex loss: 0.424734  [   88/   88]
Train Error: Avg loss: 0.44161627
validation Error: 
 Avg loss: 0.51796078 
 F1: 0.512981 
 Precision: 0.552280 
 Recall: 0.478904
 IoU: 0.344973

test Error: 
 Avg loss: 0.49086530 
 F1: 0.547553 
 Precision: 0.557530 
 Recall: 0.537927
 IoU: 0.376987

We have finished training iteration 110
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_108_.pth
per-ex loss: 0.361722  [    1/   88]
per-ex loss: 0.331560  [    2/   88]
per-ex loss: 0.305062  [    3/   88]
per-ex loss: 0.513923  [    4/   88]
per-ex loss: 0.602889  [    5/   88]
per-ex loss: 0.324973  [    6/   88]
per-ex loss: 0.350337  [    7/   88]
per-ex loss: 0.318164  [    8/   88]
per-ex loss: 0.321702  [    9/   88]
per-ex loss: 0.579800  [   10/   88]
per-ex loss: 0.441136  [   11/   88]
per-ex loss: 0.377585  [   12/   88]
per-ex loss: 0.621830  [   13/   88]
per-ex loss: 0.350187  [   14/   88]
per-ex loss: 0.518743  [   15/   88]
per-ex loss: 0.398577  [   16/   88]
per-ex loss: 0.368063  [   17/   88]
per-ex loss: 0.386801  [   18/   88]
per-ex loss: 0.345392  [   19/   88]
per-ex loss: 0.513061  [   20/   88]
per-ex loss: 0.623187  [   21/   88]
per-ex loss: 0.361191  [   22/   88]
per-ex loss: 0.414462  [   23/   88]
per-ex loss: 0.477990  [   24/   88]
per-ex loss: 0.444125  [   25/   88]
per-ex loss: 0.576368  [   26/   88]
per-ex loss: 0.545550  [   27/   88]
per-ex loss: 0.369248  [   28/   88]
per-ex loss: 0.334719  [   29/   88]
per-ex loss: 0.224832  [   30/   88]
per-ex loss: 0.346501  [   31/   88]
per-ex loss: 0.388807  [   32/   88]
per-ex loss: 0.381132  [   33/   88]
per-ex loss: 0.553089  [   34/   88]
per-ex loss: 0.344281  [   35/   88]
per-ex loss: 0.316823  [   36/   88]
per-ex loss: 0.366637  [   37/   88]
per-ex loss: 0.440814  [   38/   88]
per-ex loss: 0.333782  [   39/   88]
per-ex loss: 0.441379  [   40/   88]
per-ex loss: 0.537825  [   41/   88]
per-ex loss: 0.586385  [   42/   88]
per-ex loss: 0.531725  [   43/   88]
per-ex loss: 0.494835  [   44/   88]
per-ex loss: 0.360666  [   45/   88]
per-ex loss: 0.553715  [   46/   88]
per-ex loss: 0.299426  [   47/   88]
per-ex loss: 0.436812  [   48/   88]
per-ex loss: 0.330922  [   49/   88]
per-ex loss: 0.615358  [   50/   88]
per-ex loss: 0.391011  [   51/   88]
per-ex loss: 0.292218  [   52/   88]
per-ex loss: 0.313010  [   53/   88]
per-ex loss: 0.537609  [   54/   88]
per-ex loss: 0.395065  [   55/   88]
per-ex loss: 0.383121  [   56/   88]
per-ex loss: 0.346775  [   57/   88]
per-ex loss: 0.611993  [   58/   88]
per-ex loss: 0.374052  [   59/   88]
per-ex loss: 0.290733  [   60/   88]
per-ex loss: 0.509088  [   61/   88]
per-ex loss: 0.539780  [   62/   88]
per-ex loss: 0.414164  [   63/   88]
per-ex loss: 0.338704  [   64/   88]
per-ex loss: 0.337947  [   65/   88]
per-ex loss: 0.560990  [   66/   88]
per-ex loss: 0.410120  [   67/   88]
per-ex loss: 0.395564  [   68/   88]
per-ex loss: 0.338825  [   69/   88]
per-ex loss: 0.438345  [   70/   88]
per-ex loss: 0.559685  [   71/   88]
per-ex loss: 0.478943  [   72/   88]
per-ex loss: 0.379217  [   73/   88]
per-ex loss: 0.418487  [   74/   88]
per-ex loss: 0.413305  [   75/   88]
per-ex loss: 0.499529  [   76/   88]
per-ex loss: 0.378021  [   77/   88]
per-ex loss: 0.299257  [   78/   88]
per-ex loss: 0.621271  [   79/   88]
per-ex loss: 0.554462  [   80/   88]
per-ex loss: 0.642196  [   81/   88]
per-ex loss: 0.553437  [   82/   88]
per-ex loss: 0.468250  [   83/   88]
per-ex loss: 0.295821  [   84/   88]
per-ex loss: 0.445987  [   85/   88]
per-ex loss: 0.482916  [   86/   88]
per-ex loss: 0.523704  [   87/   88]
per-ex loss: 0.563817  [   88/   88]
Train Error: Avg loss: 0.43365354
validation Error: 
 Avg loss: 0.51806831 
 F1: 0.510129 
 Precision: 0.607473 
 Recall: 0.439674
 IoU: 0.342398

test Error: 
 Avg loss: 0.47985828 
 F1: 0.565013 
 Precision: 0.624677 
 Recall: 0.515752
 IoU: 0.393740

We have finished training iteration 111
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_89_.pth
per-ex loss: 0.395167  [    1/   88]
per-ex loss: 0.610940  [    2/   88]
per-ex loss: 0.430463  [    3/   88]
per-ex loss: 0.332296  [    4/   88]
per-ex loss: 0.314338  [    5/   88]
per-ex loss: 0.550558  [    6/   88]
per-ex loss: 0.471414  [    7/   88]
per-ex loss: 0.512325  [    8/   88]
per-ex loss: 0.564158  [    9/   88]
per-ex loss: 0.379758  [   10/   88]
per-ex loss: 0.565442  [   11/   88]
per-ex loss: 0.397436  [   12/   88]
per-ex loss: 0.498006  [   13/   88]
per-ex loss: 0.325245  [   14/   88]
per-ex loss: 0.570902  [   15/   88]
per-ex loss: 0.447867  [   16/   88]
per-ex loss: 0.365188  [   17/   88]
per-ex loss: 0.366809  [   18/   88]
per-ex loss: 0.452815  [   19/   88]
per-ex loss: 0.389246  [   20/   88]
per-ex loss: 0.290791  [   21/   88]
per-ex loss: 0.223367  [   22/   88]
per-ex loss: 0.386522  [   23/   88]
per-ex loss: 0.528527  [   24/   88]
per-ex loss: 0.354634  [   25/   88]
per-ex loss: 0.343050  [   26/   88]
per-ex loss: 0.318348  [   27/   88]
per-ex loss: 0.630813  [   28/   88]
per-ex loss: 0.510060  [   29/   88]
per-ex loss: 0.476770  [   30/   88]
per-ex loss: 0.478422  [   31/   88]
per-ex loss: 0.445658  [   32/   88]
per-ex loss: 0.415122  [   33/   88]
per-ex loss: 0.420675  [   34/   88]
per-ex loss: 0.379368  [   35/   88]
per-ex loss: 0.610027  [   36/   88]
per-ex loss: 0.482974  [   37/   88]
per-ex loss: 0.388750  [   38/   88]
per-ex loss: 0.628294  [   39/   88]
per-ex loss: 0.427116  [   40/   88]
per-ex loss: 0.511614  [   41/   88]
per-ex loss: 0.327555  [   42/   88]
per-ex loss: 0.290080  [   43/   88]
per-ex loss: 0.412430  [   44/   88]
per-ex loss: 0.340872  [   45/   88]
per-ex loss: 0.385112  [   46/   88]
per-ex loss: 0.369197  [   47/   88]
per-ex loss: 0.406678  [   48/   88]
per-ex loss: 0.579585  [   49/   88]
per-ex loss: 0.548241  [   50/   88]
per-ex loss: 0.541912  [   51/   88]
per-ex loss: 0.298602  [   52/   88]
per-ex loss: 0.420682  [   53/   88]
per-ex loss: 0.613925  [   54/   88]
per-ex loss: 0.399727  [   55/   88]
per-ex loss: 0.378779  [   56/   88]
per-ex loss: 0.337045  [   57/   88]
per-ex loss: 0.396222  [   58/   88]
per-ex loss: 0.539333  [   59/   88]
per-ex loss: 0.328018  [   60/   88]
per-ex loss: 0.566488  [   61/   88]
per-ex loss: 0.327789  [   62/   88]
per-ex loss: 0.641914  [   63/   88]
per-ex loss: 0.365061  [   64/   88]
per-ex loss: 0.375407  [   65/   88]
per-ex loss: 0.312104  [   66/   88]
per-ex loss: 0.525159  [   67/   88]
per-ex loss: 0.556846  [   68/   88]
per-ex loss: 0.300642  [   69/   88]
per-ex loss: 0.578309  [   70/   88]
per-ex loss: 0.346400  [   71/   88]
per-ex loss: 0.509758  [   72/   88]
per-ex loss: 0.327624  [   73/   88]
per-ex loss: 0.341208  [   74/   88]
per-ex loss: 0.422926  [   75/   88]
per-ex loss: 0.613673  [   76/   88]
per-ex loss: 0.349268  [   77/   88]
per-ex loss: 0.446271  [   78/   88]
per-ex loss: 0.519828  [   79/   88]
per-ex loss: 0.545313  [   80/   88]
per-ex loss: 0.538777  [   81/   88]
per-ex loss: 0.565090  [   82/   88]
per-ex loss: 0.332610  [   83/   88]
per-ex loss: 0.476457  [   84/   88]
per-ex loss: 0.318395  [   85/   88]
per-ex loss: 0.445340  [   86/   88]
per-ex loss: 0.386527  [   87/   88]
per-ex loss: 0.358061  [   88/   88]
Train Error: Avg loss: 0.43748319
validation Error: 
 Avg loss: 0.51758395 
 F1: 0.510547 
 Precision: 0.596451 
 Recall: 0.446272
 IoU: 0.342775

test Error: 
 Avg loss: 0.48005459 
 F1: 0.567774 
 Precision: 0.617179 
 Recall: 0.525692
 IoU: 0.396427

We have finished training iteration 112
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_110_.pth
per-ex loss: 0.584149  [    1/   88]
per-ex loss: 0.373811  [    2/   88]
per-ex loss: 0.611450  [    3/   88]
per-ex loss: 0.483784  [    4/   88]
per-ex loss: 0.342342  [    5/   88]
per-ex loss: 0.381426  [    6/   88]
per-ex loss: 0.310475  [    7/   88]
per-ex loss: 0.290615  [    8/   88]
per-ex loss: 0.469566  [    9/   88]
per-ex loss: 0.411589  [   10/   88]
per-ex loss: 0.520658  [   11/   88]
per-ex loss: 0.388673  [   12/   88]
per-ex loss: 0.520623  [   13/   88]
per-ex loss: 0.393516  [   14/   88]
per-ex loss: 0.507231  [   15/   88]
per-ex loss: 0.514052  [   16/   88]
per-ex loss: 0.466809  [   17/   88]
per-ex loss: 0.436457  [   18/   88]
per-ex loss: 0.615002  [   19/   88]
per-ex loss: 0.425839  [   20/   88]
per-ex loss: 0.308286  [   21/   88]
per-ex loss: 0.322626  [   22/   88]
per-ex loss: 0.435560  [   23/   88]
per-ex loss: 0.340269  [   24/   88]
per-ex loss: 0.314024  [   25/   88]
per-ex loss: 0.440689  [   26/   88]
per-ex loss: 0.548029  [   27/   88]
per-ex loss: 0.612233  [   28/   88]
per-ex loss: 0.521384  [   29/   88]
per-ex loss: 0.380099  [   30/   88]
per-ex loss: 0.384648  [   31/   88]
per-ex loss: 0.452140  [   32/   88]
per-ex loss: 0.543983  [   33/   88]
per-ex loss: 0.530653  [   34/   88]
per-ex loss: 0.326857  [   35/   88]
per-ex loss: 0.333657  [   36/   88]
per-ex loss: 0.581309  [   37/   88]
per-ex loss: 0.300251  [   38/   88]
per-ex loss: 0.523621  [   39/   88]
per-ex loss: 0.443198  [   40/   88]
per-ex loss: 0.532912  [   41/   88]
per-ex loss: 0.378796  [   42/   88]
per-ex loss: 0.245666  [   43/   88]
per-ex loss: 0.544992  [   44/   88]
per-ex loss: 0.428272  [   45/   88]
per-ex loss: 0.305486  [   46/   88]
per-ex loss: 0.377665  [   47/   88]
per-ex loss: 0.376026  [   48/   88]
per-ex loss: 0.546719  [   49/   88]
per-ex loss: 0.331427  [   50/   88]
per-ex loss: 0.525494  [   51/   88]
per-ex loss: 0.287323  [   52/   88]
per-ex loss: 0.397821  [   53/   88]
per-ex loss: 0.347130  [   54/   88]
per-ex loss: 0.458247  [   55/   88]
per-ex loss: 0.333623  [   56/   88]
per-ex loss: 0.399678  [   57/   88]
per-ex loss: 0.445430  [   58/   88]
per-ex loss: 0.422432  [   59/   88]
per-ex loss: 0.436472  [   60/   88]
per-ex loss: 0.510597  [   61/   88]
per-ex loss: 0.532573  [   62/   88]
per-ex loss: 0.399403  [   63/   88]
per-ex loss: 0.382665  [   64/   88]
per-ex loss: 0.332004  [   65/   88]
per-ex loss: 0.285936  [   66/   88]
per-ex loss: 0.573795  [   67/   88]
per-ex loss: 0.601589  [   68/   88]
per-ex loss: 0.347450  [   69/   88]
per-ex loss: 0.373237  [   70/   88]
per-ex loss: 0.569453  [   71/   88]
per-ex loss: 0.358595  [   72/   88]
per-ex loss: 0.366254  [   73/   88]
per-ex loss: 0.331062  [   74/   88]
per-ex loss: 0.572236  [   75/   88]
per-ex loss: 0.394265  [   76/   88]
per-ex loss: 0.349076  [   77/   88]
per-ex loss: 0.467785  [   78/   88]
per-ex loss: 0.616370  [   79/   88]
per-ex loss: 0.294526  [   80/   88]
per-ex loss: 0.639761  [   81/   88]
per-ex loss: 0.373245  [   82/   88]
per-ex loss: 0.369236  [   83/   88]
per-ex loss: 0.369207  [   84/   88]
per-ex loss: 0.564940  [   85/   88]
per-ex loss: 0.404420  [   86/   88]
per-ex loss: 0.319436  [   87/   88]
per-ex loss: 0.624290  [   88/   88]
Train Error: Avg loss: 0.43337049
validation Error: 
 Avg loss: 0.52131552 
 F1: 0.508414 
 Precision: 0.551607 
 Recall: 0.471495
 IoU: 0.340855

test Error: 
 Avg loss: 0.48858064 
 F1: 0.554612 
 Precision: 0.577312 
 Recall: 0.533629
 IoU: 0.383711

We have finished training iteration 113
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_111_.pth
per-ex loss: 0.314025  [    1/   88]
per-ex loss: 0.428883  [    2/   88]
per-ex loss: 0.558383  [    3/   88]
per-ex loss: 0.333774  [    4/   88]
per-ex loss: 0.648797  [    5/   88]
per-ex loss: 0.424869  [    6/   88]
per-ex loss: 0.458494  [    7/   88]
per-ex loss: 0.381673  [    8/   88]
per-ex loss: 0.355801  [    9/   88]
per-ex loss: 0.428829  [   10/   88]
per-ex loss: 0.414209  [   11/   88]
per-ex loss: 0.381240  [   12/   88]
per-ex loss: 0.324860  [   13/   88]
per-ex loss: 0.339257  [   14/   88]
per-ex loss: 0.335555  [   15/   88]
per-ex loss: 0.330090  [   16/   88]
per-ex loss: 0.344207  [   17/   88]
per-ex loss: 0.527221  [   18/   88]
per-ex loss: 0.387847  [   19/   88]
per-ex loss: 0.289187  [   20/   88]
per-ex loss: 0.425952  [   21/   88]
per-ex loss: 0.404513  [   22/   88]
per-ex loss: 0.415171  [   23/   88]
per-ex loss: 0.395870  [   24/   88]
per-ex loss: 0.322919  [   25/   88]
per-ex loss: 0.571030  [   26/   88]
per-ex loss: 0.383933  [   27/   88]
per-ex loss: 0.668331  [   28/   88]
per-ex loss: 0.446420  [   29/   88]
per-ex loss: 0.516126  [   30/   88]
per-ex loss: 0.326184  [   31/   88]
per-ex loss: 0.358524  [   32/   88]
per-ex loss: 0.245097  [   33/   88]
per-ex loss: 0.519072  [   34/   88]
per-ex loss: 0.603053  [   35/   88]
per-ex loss: 0.412217  [   36/   88]
per-ex loss: 0.345437  [   37/   88]
per-ex loss: 0.522515  [   38/   88]
per-ex loss: 0.350109  [   39/   88]
per-ex loss: 0.290640  [   40/   88]
per-ex loss: 0.356505  [   41/   88]
per-ex loss: 0.379377  [   42/   88]
per-ex loss: 0.495153  [   43/   88]
per-ex loss: 0.337433  [   44/   88]
per-ex loss: 0.523041  [   45/   88]
per-ex loss: 0.326496  [   46/   88]
per-ex loss: 0.569673  [   47/   88]
per-ex loss: 0.554558  [   48/   88]
per-ex loss: 0.519552  [   49/   88]
per-ex loss: 0.319922  [   50/   88]
per-ex loss: 0.379540  [   51/   88]
per-ex loss: 0.611387  [   52/   88]
per-ex loss: 0.456842  [   53/   88]
per-ex loss: 0.615826  [   54/   88]
per-ex loss: 0.426540  [   55/   88]
per-ex loss: 0.713296  [   56/   88]
per-ex loss: 0.548095  [   57/   88]
per-ex loss: 0.448863  [   58/   88]
per-ex loss: 0.577782  [   59/   88]
per-ex loss: 0.433410  [   60/   88]
per-ex loss: 0.510171  [   61/   88]
per-ex loss: 0.321099  [   62/   88]
per-ex loss: 0.358709  [   63/   88]
per-ex loss: 0.370777  [   64/   88]
per-ex loss: 0.337298  [   65/   88]
per-ex loss: 0.331438  [   66/   88]
per-ex loss: 0.510872  [   67/   88]
per-ex loss: 0.559333  [   68/   88]
per-ex loss: 0.382840  [   69/   88]
per-ex loss: 0.574622  [   70/   88]
per-ex loss: 0.485877  [   71/   88]
per-ex loss: 0.555833  [   72/   88]
per-ex loss: 0.362397  [   73/   88]
per-ex loss: 0.322168  [   74/   88]
per-ex loss: 0.379478  [   75/   88]
per-ex loss: 0.435987  [   76/   88]
per-ex loss: 0.618018  [   77/   88]
per-ex loss: 0.511222  [   78/   88]
per-ex loss: 0.463779  [   79/   88]
per-ex loss: 0.519275  [   80/   88]
per-ex loss: 0.416125  [   81/   88]
per-ex loss: 0.535284  [   82/   88]
per-ex loss: 0.299491  [   83/   88]
per-ex loss: 0.503115  [   84/   88]
per-ex loss: 0.533841  [   85/   88]
per-ex loss: 0.389151  [   86/   88]
per-ex loss: 0.365872  [   87/   88]
per-ex loss: 0.314240  [   88/   88]
Train Error: Avg loss: 0.43627178
validation Error: 
 Avg loss: 0.51977043 
 F1: 0.509457 
 Precision: 0.573297 
 Recall: 0.458410
 IoU: 0.341793

test Error: 
 Avg loss: 0.48962841 
 F1: 0.550992 
 Precision: 0.591293 
 Recall: 0.515835
 IoU: 0.380255

We have finished training iteration 114
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_112_.pth
per-ex loss: 0.601317  [    1/   88]
per-ex loss: 0.348229  [    2/   88]
per-ex loss: 0.587255  [    3/   88]
per-ex loss: 0.359408  [    4/   88]
per-ex loss: 0.357660  [    5/   88]
per-ex loss: 0.338533  [    6/   88]
per-ex loss: 0.383838  [    7/   88]
per-ex loss: 0.507728  [    8/   88]
per-ex loss: 0.389927  [    9/   88]
per-ex loss: 0.440233  [   10/   88]
per-ex loss: 0.321241  [   11/   88]
per-ex loss: 0.559654  [   12/   88]
per-ex loss: 0.604331  [   13/   88]
per-ex loss: 0.422976  [   14/   88]
per-ex loss: 0.353691  [   15/   88]
per-ex loss: 0.314026  [   16/   88]
per-ex loss: 0.382559  [   17/   88]
per-ex loss: 0.478309  [   18/   88]
per-ex loss: 0.367282  [   19/   88]
per-ex loss: 0.548783  [   20/   88]
per-ex loss: 0.499634  [   21/   88]
per-ex loss: 0.359012  [   22/   88]
per-ex loss: 0.522054  [   23/   88]
per-ex loss: 0.534274  [   24/   88]
per-ex loss: 0.625055  [   25/   88]
per-ex loss: 0.482581  [   26/   88]
per-ex loss: 0.341699  [   27/   88]
per-ex loss: 0.538828  [   28/   88]
per-ex loss: 0.564101  [   29/   88]
per-ex loss: 0.638896  [   30/   88]
per-ex loss: 0.317768  [   31/   88]
per-ex loss: 0.567722  [   32/   88]
per-ex loss: 0.332375  [   33/   88]
per-ex loss: 0.329032  [   34/   88]
per-ex loss: 0.364818  [   35/   88]
per-ex loss: 0.450109  [   36/   88]
per-ex loss: 0.389539  [   37/   88]
per-ex loss: 0.296726  [   38/   88]
per-ex loss: 0.603813  [   39/   88]
per-ex loss: 0.348216  [   40/   88]
per-ex loss: 0.453435  [   41/   88]
per-ex loss: 0.514632  [   42/   88]
per-ex loss: 0.414310  [   43/   88]
per-ex loss: 0.630089  [   44/   88]
per-ex loss: 0.414013  [   45/   88]
per-ex loss: 0.391855  [   46/   88]
per-ex loss: 0.376362  [   47/   88]
per-ex loss: 0.450834  [   48/   88]
per-ex loss: 0.538496  [   49/   88]
per-ex loss: 0.280330  [   50/   88]
per-ex loss: 0.317614  [   51/   88]
per-ex loss: 0.346813  [   52/   88]
per-ex loss: 0.403535  [   53/   88]
per-ex loss: 0.606707  [   54/   88]
per-ex loss: 0.352286  [   55/   88]
per-ex loss: 0.547400  [   56/   88]
per-ex loss: 0.452215  [   57/   88]
per-ex loss: 0.387337  [   58/   88]
per-ex loss: 0.336475  [   59/   88]
per-ex loss: 0.421229  [   60/   88]
per-ex loss: 0.522932  [   61/   88]
per-ex loss: 0.403545  [   62/   88]
per-ex loss: 0.445095  [   63/   88]
per-ex loss: 0.438958  [   64/   88]
per-ex loss: 0.456682  [   65/   88]
per-ex loss: 0.507976  [   66/   88]
per-ex loss: 0.411535  [   67/   88]
per-ex loss: 0.372169  [   68/   88]
per-ex loss: 0.537147  [   69/   88]
per-ex loss: 0.374796  [   70/   88]
per-ex loss: 0.626304  [   71/   88]
per-ex loss: 0.340247  [   72/   88]
per-ex loss: 0.378464  [   73/   88]
per-ex loss: 0.401780  [   74/   88]
per-ex loss: 0.509723  [   75/   88]
per-ex loss: 0.473151  [   76/   88]
per-ex loss: 0.547055  [   77/   88]
per-ex loss: 0.275172  [   78/   88]
per-ex loss: 0.385892  [   79/   88]
per-ex loss: 0.243865  [   80/   88]
per-ex loss: 0.323143  [   81/   88]
per-ex loss: 0.596686  [   82/   88]
per-ex loss: 0.529408  [   83/   88]
per-ex loss: 0.290903  [   84/   88]
per-ex loss: 0.314627  [   85/   88]
per-ex loss: 0.518460  [   86/   88]
per-ex loss: 0.289957  [   87/   88]
per-ex loss: 0.546878  [   88/   88]
Train Error: Avg loss: 0.43717894
validation Error: 
 Avg loss: 0.52175450 
 F1: 0.509755 
 Precision: 0.603682 
 Recall: 0.441120
 IoU: 0.342061

test Error: 
 Avg loss: 0.47964271 
 F1: 0.565463 
 Precision: 0.630039 
 Recall: 0.512894
 IoU: 0.394178

We have finished training iteration 115
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_113_.pth
per-ex loss: 0.543024  [    1/   88]
per-ex loss: 0.378065  [    2/   88]
per-ex loss: 0.361511  [    3/   88]
per-ex loss: 0.439333  [    4/   88]
per-ex loss: 0.419167  [    5/   88]
per-ex loss: 0.494315  [    6/   88]
per-ex loss: 0.529659  [    7/   88]
per-ex loss: 0.309118  [    8/   88]
per-ex loss: 0.562912  [    9/   88]
per-ex loss: 0.404435  [   10/   88]
per-ex loss: 0.311484  [   11/   88]
per-ex loss: 0.631746  [   12/   88]
per-ex loss: 0.319152  [   13/   88]
per-ex loss: 0.599967  [   14/   88]
per-ex loss: 0.332923  [   15/   88]
per-ex loss: 0.532221  [   16/   88]
per-ex loss: 0.521792  [   17/   88]
per-ex loss: 0.349918  [   18/   88]
per-ex loss: 0.408552  [   19/   88]
per-ex loss: 0.401076  [   20/   88]
per-ex loss: 0.324509  [   21/   88]
per-ex loss: 0.357271  [   22/   88]
per-ex loss: 0.503932  [   23/   88]
per-ex loss: 0.356989  [   24/   88]
per-ex loss: 0.364817  [   25/   88]
per-ex loss: 0.566040  [   26/   88]
per-ex loss: 0.365134  [   27/   88]
per-ex loss: 0.335023  [   28/   88]
per-ex loss: 0.460859  [   29/   88]
per-ex loss: 0.289185  [   30/   88]
per-ex loss: 0.367821  [   31/   88]
per-ex loss: 0.437590  [   32/   88]
per-ex loss: 0.478597  [   33/   88]
per-ex loss: 0.394157  [   34/   88]
per-ex loss: 0.367644  [   35/   88]
per-ex loss: 0.311579  [   36/   88]
per-ex loss: 0.301445  [   37/   88]
per-ex loss: 0.541145  [   38/   88]
per-ex loss: 0.535453  [   39/   88]
per-ex loss: 0.371861  [   40/   88]
per-ex loss: 0.611624  [   41/   88]
per-ex loss: 0.531501  [   42/   88]
per-ex loss: 0.326309  [   43/   88]
per-ex loss: 0.472318  [   44/   88]
per-ex loss: 0.530954  [   45/   88]
per-ex loss: 0.418775  [   46/   88]
per-ex loss: 0.461259  [   47/   88]
per-ex loss: 0.514909  [   48/   88]
per-ex loss: 0.370051  [   49/   88]
per-ex loss: 0.618353  [   50/   88]
per-ex loss: 0.617356  [   51/   88]
per-ex loss: 0.381998  [   52/   88]
per-ex loss: 0.549605  [   53/   88]
per-ex loss: 0.360572  [   54/   88]
per-ex loss: 0.391943  [   55/   88]
per-ex loss: 0.398901  [   56/   88]
per-ex loss: 0.461029  [   57/   88]
per-ex loss: 0.254365  [   58/   88]
per-ex loss: 0.620717  [   59/   88]
per-ex loss: 0.361051  [   60/   88]
per-ex loss: 0.300909  [   61/   88]
per-ex loss: 0.617318  [   62/   88]
per-ex loss: 0.329698  [   63/   88]
per-ex loss: 0.367420  [   64/   88]
per-ex loss: 0.332098  [   65/   88]
per-ex loss: 0.296355  [   66/   88]
per-ex loss: 0.442654  [   67/   88]
per-ex loss: 0.530623  [   68/   88]
per-ex loss: 0.572729  [   69/   88]
per-ex loss: 0.420280  [   70/   88]
per-ex loss: 0.331018  [   71/   88]
per-ex loss: 0.494901  [   72/   88]
per-ex loss: 0.570765  [   73/   88]
per-ex loss: 0.388743  [   74/   88]
per-ex loss: 0.387142  [   75/   88]
per-ex loss: 0.449570  [   76/   88]
per-ex loss: 0.438143  [   77/   88]
per-ex loss: 0.333160  [   78/   88]
per-ex loss: 0.568005  [   79/   88]
per-ex loss: 0.584459  [   80/   88]
per-ex loss: 0.399843  [   81/   88]
per-ex loss: 0.294696  [   82/   88]
per-ex loss: 0.599685  [   83/   88]
per-ex loss: 0.472687  [   84/   88]
per-ex loss: 0.288271  [   85/   88]
per-ex loss: 0.341162  [   86/   88]
per-ex loss: 0.325189  [   87/   88]
per-ex loss: 0.516621  [   88/   88]
Train Error: Avg loss: 0.43328587
validation Error: 
 Avg loss: 0.52082763 
 F1: 0.509363 
 Precision: 0.567150 
 Recall: 0.462264
 IoU: 0.341709

test Error: 
 Avg loss: 0.47793610 
 F1: 0.566978 
 Precision: 0.609462 
 Recall: 0.530031
 IoU: 0.395652

We have finished training iteration 116
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_114_.pth
per-ex loss: 0.590410  [    1/   88]
per-ex loss: 0.379150  [    2/   88]
per-ex loss: 0.443455  [    3/   88]
per-ex loss: 0.534863  [    4/   88]
per-ex loss: 0.534412  [    5/   88]
per-ex loss: 0.308517  [    6/   88]
per-ex loss: 0.423382  [    7/   88]
per-ex loss: 0.555502  [    8/   88]
per-ex loss: 0.507931  [    9/   88]
per-ex loss: 0.532924  [   10/   88]
per-ex loss: 0.395621  [   11/   88]
per-ex loss: 0.374701  [   12/   88]
per-ex loss: 0.456237  [   13/   88]
per-ex loss: 0.336531  [   14/   88]
per-ex loss: 0.319405  [   15/   88]
per-ex loss: 0.285850  [   16/   88]
per-ex loss: 0.552090  [   17/   88]
per-ex loss: 0.400588  [   18/   88]
per-ex loss: 0.387872  [   19/   88]
per-ex loss: 0.603188  [   20/   88]
per-ex loss: 0.470177  [   21/   88]
per-ex loss: 0.414277  [   22/   88]
per-ex loss: 0.346592  [   23/   88]
per-ex loss: 0.506833  [   24/   88]
per-ex loss: 0.398833  [   25/   88]
per-ex loss: 0.552348  [   26/   88]
per-ex loss: 0.517734  [   27/   88]
per-ex loss: 0.606595  [   28/   88]
per-ex loss: 0.322266  [   29/   88]
per-ex loss: 0.327009  [   30/   88]
per-ex loss: 0.303678  [   31/   88]
per-ex loss: 0.498927  [   32/   88]
per-ex loss: 0.460235  [   33/   88]
per-ex loss: 0.640174  [   34/   88]
per-ex loss: 0.351319  [   35/   88]
per-ex loss: 0.561945  [   36/   88]
per-ex loss: 0.545621  [   37/   88]
per-ex loss: 0.293266  [   38/   88]
per-ex loss: 0.264008  [   39/   88]
per-ex loss: 0.322519  [   40/   88]
per-ex loss: 0.435296  [   41/   88]
per-ex loss: 0.380715  [   42/   88]
per-ex loss: 0.564189  [   43/   88]
per-ex loss: 0.294194  [   44/   88]
per-ex loss: 0.518762  [   45/   88]
per-ex loss: 0.364169  [   46/   88]
per-ex loss: 0.349748  [   47/   88]
per-ex loss: 0.449778  [   48/   88]
per-ex loss: 0.374794  [   49/   88]
per-ex loss: 0.496073  [   50/   88]
per-ex loss: 0.576133  [   51/   88]
per-ex loss: 0.441443  [   52/   88]
per-ex loss: 0.525408  [   53/   88]
per-ex loss: 0.368515  [   54/   88]
per-ex loss: 0.312546  [   55/   88]
per-ex loss: 0.333208  [   56/   88]
per-ex loss: 0.327327  [   57/   88]
per-ex loss: 0.633871  [   58/   88]
per-ex loss: 0.616053  [   59/   88]
per-ex loss: 0.436318  [   60/   88]
per-ex loss: 0.309530  [   61/   88]
per-ex loss: 0.398875  [   62/   88]
per-ex loss: 0.371356  [   63/   88]
per-ex loss: 0.365818  [   64/   88]
per-ex loss: 0.353343  [   65/   88]
per-ex loss: 0.633339  [   66/   88]
per-ex loss: 0.408601  [   67/   88]
per-ex loss: 0.518451  [   68/   88]
per-ex loss: 0.632045  [   69/   88]
per-ex loss: 0.367556  [   70/   88]
per-ex loss: 0.321868  [   71/   88]
per-ex loss: 0.422878  [   72/   88]
per-ex loss: 0.440511  [   73/   88]
per-ex loss: 0.387754  [   74/   88]
per-ex loss: 0.351501  [   75/   88]
per-ex loss: 0.581530  [   76/   88]
per-ex loss: 0.510839  [   77/   88]
per-ex loss: 0.541534  [   78/   88]
per-ex loss: 0.394475  [   79/   88]
per-ex loss: 0.368772  [   80/   88]
per-ex loss: 0.418880  [   81/   88]
per-ex loss: 0.384115  [   82/   88]
per-ex loss: 0.422257  [   83/   88]
per-ex loss: 0.535646  [   84/   88]
per-ex loss: 0.334844  [   85/   88]
per-ex loss: 0.349810  [   86/   88]
per-ex loss: 0.342482  [   87/   88]
per-ex loss: 0.446800  [   88/   88]
Train Error: Avg loss: 0.43569247
validation Error: 
 Avg loss: 0.52405890 
 F1: 0.504393 
 Precision: 0.520705 
 Recall: 0.489071
 IoU: 0.337249

test Error: 
 Avg loss: 0.48991437 
 F1: 0.553038 
 Precision: 0.548688 
 Recall: 0.557459
 IoU: 0.382207

We have finished training iteration 117
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_115_.pth
per-ex loss: 0.379898  [    1/   88]
per-ex loss: 0.632923  [    2/   88]
per-ex loss: 0.406326  [    3/   88]
per-ex loss: 0.378632  [    4/   88]
per-ex loss: 0.519968  [    5/   88]
per-ex loss: 0.359878  [    6/   88]
per-ex loss: 0.473830  [    7/   88]
per-ex loss: 0.271452  [    8/   88]
per-ex loss: 0.422413  [    9/   88]
per-ex loss: 0.323278  [   10/   88]
per-ex loss: 0.637059  [   11/   88]
per-ex loss: 0.310315  [   12/   88]
per-ex loss: 0.383152  [   13/   88]
per-ex loss: 0.397929  [   14/   88]
per-ex loss: 0.551575  [   15/   88]
per-ex loss: 0.461671  [   16/   88]
per-ex loss: 0.279432  [   17/   88]
per-ex loss: 0.611222  [   18/   88]
per-ex loss: 0.385829  [   19/   88]
per-ex loss: 0.370158  [   20/   88]
per-ex loss: 0.359569  [   21/   88]
per-ex loss: 0.333411  [   22/   88]
per-ex loss: 0.574927  [   23/   88]
per-ex loss: 0.391020  [   24/   88]
per-ex loss: 0.396295  [   25/   88]
per-ex loss: 0.319231  [   26/   88]
per-ex loss: 0.318153  [   27/   88]
per-ex loss: 0.308300  [   28/   88]
per-ex loss: 0.286555  [   29/   88]
per-ex loss: 0.372472  [   30/   88]
per-ex loss: 0.574541  [   31/   88]
per-ex loss: 0.552169  [   32/   88]
per-ex loss: 0.491670  [   33/   88]
per-ex loss: 0.518988  [   34/   88]
per-ex loss: 0.625903  [   35/   88]
per-ex loss: 0.356907  [   36/   88]
per-ex loss: 0.453096  [   37/   88]
per-ex loss: 0.572115  [   38/   88]
per-ex loss: 0.508150  [   39/   88]
per-ex loss: 0.359647  [   40/   88]
per-ex loss: 0.611083  [   41/   88]
per-ex loss: 0.401460  [   42/   88]
per-ex loss: 0.459951  [   43/   88]
per-ex loss: 0.289107  [   44/   88]
per-ex loss: 0.414419  [   45/   88]
per-ex loss: 0.479006  [   46/   88]
per-ex loss: 0.529742  [   47/   88]
per-ex loss: 0.470171  [   48/   88]
per-ex loss: 0.357770  [   49/   88]
per-ex loss: 0.383319  [   50/   88]
per-ex loss: 0.451069  [   51/   88]
per-ex loss: 0.349633  [   52/   88]
per-ex loss: 0.310962  [   53/   88]
per-ex loss: 0.510746  [   54/   88]
per-ex loss: 0.307034  [   55/   88]
per-ex loss: 0.504581  [   56/   88]
per-ex loss: 0.413674  [   57/   88]
per-ex loss: 0.325868  [   58/   88]
per-ex loss: 0.370358  [   59/   88]
per-ex loss: 0.337543  [   60/   88]
per-ex loss: 0.521587  [   61/   88]
per-ex loss: 0.405027  [   62/   88]
per-ex loss: 0.353067  [   63/   88]
per-ex loss: 0.474392  [   64/   88]
per-ex loss: 0.563444  [   65/   88]
per-ex loss: 0.364019  [   66/   88]
per-ex loss: 0.366872  [   67/   88]
per-ex loss: 0.548558  [   68/   88]
per-ex loss: 0.613518  [   69/   88]
per-ex loss: 0.540005  [   70/   88]
per-ex loss: 0.350397  [   71/   88]
per-ex loss: 0.433606  [   72/   88]
per-ex loss: 0.384550  [   73/   88]
per-ex loss: 0.365967  [   74/   88]
per-ex loss: 0.584368  [   75/   88]
per-ex loss: 0.386436  [   76/   88]
per-ex loss: 0.538973  [   77/   88]
per-ex loss: 0.331605  [   78/   88]
per-ex loss: 0.437770  [   79/   88]
per-ex loss: 0.427240  [   80/   88]
per-ex loss: 0.292905  [   81/   88]
per-ex loss: 0.521119  [   82/   88]
per-ex loss: 0.517361  [   83/   88]
per-ex loss: 0.481334  [   84/   88]
per-ex loss: 0.329929  [   85/   88]
per-ex loss: 0.326528  [   86/   88]
per-ex loss: 0.533405  [   87/   88]
per-ex loss: 0.547382  [   88/   88]
Train Error: Avg loss: 0.43237409
validation Error: 
 Avg loss: 0.53158596 
 F1: 0.499679 
 Precision: 0.540873 
 Recall: 0.464316
 IoU: 0.333048

test Error: 
 Avg loss: 0.49238109 
 F1: 0.549669 
 Precision: 0.577545 
 Recall: 0.524361
 IoU: 0.378996

We have finished training iteration 118
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_116_.pth
per-ex loss: 0.338688  [    1/   88]
per-ex loss: 0.341354  [    2/   88]
per-ex loss: 0.467411  [    3/   88]
per-ex loss: 0.540065  [    4/   88]
per-ex loss: 0.365755  [    5/   88]
per-ex loss: 0.327591  [    6/   88]
per-ex loss: 0.562582  [    7/   88]
per-ex loss: 0.410195  [    8/   88]
per-ex loss: 0.314240  [    9/   88]
per-ex loss: 0.374386  [   10/   88]
per-ex loss: 0.451333  [   11/   88]
per-ex loss: 0.455883  [   12/   88]
per-ex loss: 0.329312  [   13/   88]
per-ex loss: 0.362550  [   14/   88]
per-ex loss: 0.365204  [   15/   88]
per-ex loss: 0.388745  [   16/   88]
per-ex loss: 0.605927  [   17/   88]
per-ex loss: 0.297349  [   18/   88]
per-ex loss: 0.328037  [   19/   88]
per-ex loss: 0.361624  [   20/   88]
per-ex loss: 0.580935  [   21/   88]
per-ex loss: 0.357659  [   22/   88]
per-ex loss: 0.354900  [   23/   88]
per-ex loss: 0.362687  [   24/   88]
per-ex loss: 0.480095  [   25/   88]
per-ex loss: 0.499265  [   26/   88]
per-ex loss: 0.541665  [   27/   88]
per-ex loss: 0.584003  [   28/   88]
per-ex loss: 0.359275  [   29/   88]
per-ex loss: 0.327121  [   30/   88]
per-ex loss: 0.582092  [   31/   88]
per-ex loss: 0.510058  [   32/   88]
per-ex loss: 0.379430  [   33/   88]
per-ex loss: 0.340889  [   34/   88]
per-ex loss: 0.518696  [   35/   88]
per-ex loss: 0.322075  [   36/   88]
per-ex loss: 0.411526  [   37/   88]
per-ex loss: 0.529384  [   38/   88]
per-ex loss: 0.610631  [   39/   88]
per-ex loss: 0.356568  [   40/   88]
per-ex loss: 0.254214  [   41/   88]
per-ex loss: 0.392327  [   42/   88]
per-ex loss: 0.403672  [   43/   88]
per-ex loss: 0.608046  [   44/   88]
per-ex loss: 0.613856  [   45/   88]
per-ex loss: 0.529392  [   46/   88]
per-ex loss: 0.624160  [   47/   88]
per-ex loss: 0.294863  [   48/   88]
per-ex loss: 0.340966  [   49/   88]
per-ex loss: 0.547138  [   50/   88]
per-ex loss: 0.543079  [   51/   88]
per-ex loss: 0.331452  [   52/   88]
per-ex loss: 0.554517  [   53/   88]
per-ex loss: 0.462425  [   54/   88]
per-ex loss: 0.370123  [   55/   88]
per-ex loss: 0.450844  [   56/   88]
per-ex loss: 0.327058  [   57/   88]
per-ex loss: 0.424555  [   58/   88]
per-ex loss: 0.537197  [   59/   88]
per-ex loss: 0.462724  [   60/   88]
per-ex loss: 0.362498  [   61/   88]
per-ex loss: 0.520569  [   62/   88]
per-ex loss: 0.553836  [   63/   88]
per-ex loss: 0.324679  [   64/   88]
per-ex loss: 0.543392  [   65/   88]
per-ex loss: 0.412404  [   66/   88]
per-ex loss: 0.392934  [   67/   88]
per-ex loss: 0.314592  [   68/   88]
per-ex loss: 0.508048  [   69/   88]
per-ex loss: 0.370527  [   70/   88]
per-ex loss: 0.471293  [   71/   88]
per-ex loss: 0.386040  [   72/   88]
per-ex loss: 0.366429  [   73/   88]
per-ex loss: 0.426223  [   74/   88]
per-ex loss: 0.312602  [   75/   88]
per-ex loss: 0.421263  [   76/   88]
per-ex loss: 0.523095  [   77/   88]
per-ex loss: 0.476968  [   78/   88]
per-ex loss: 0.357664  [   79/   88]
per-ex loss: 0.569368  [   80/   88]
per-ex loss: 0.463677  [   81/   88]
per-ex loss: 0.618881  [   82/   88]
per-ex loss: 0.336012  [   83/   88]
per-ex loss: 0.516547  [   84/   88]
per-ex loss: 0.272924  [   85/   88]
per-ex loss: 0.303722  [   86/   88]
per-ex loss: 0.353746  [   87/   88]
per-ex loss: 0.559872  [   88/   88]
Train Error: Avg loss: 0.43306360
validation Error: 
 Avg loss: 0.52062696 
 F1: 0.508214 
 Precision: 0.547372 
 Recall: 0.474285
 IoU: 0.340675

test Error: 
 Avg loss: 0.48361262 
 F1: 0.562317 
 Precision: 0.569639 
 Recall: 0.555181
 IoU: 0.391127

We have finished training iteration 119
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_117_.pth
per-ex loss: 0.317511  [    1/   88]
per-ex loss: 0.399351  [    2/   88]
per-ex loss: 0.389134  [    3/   88]
per-ex loss: 0.453596  [    4/   88]
per-ex loss: 0.425713  [    5/   88]
per-ex loss: 0.554522  [    6/   88]
per-ex loss: 0.283368  [    7/   88]
per-ex loss: 0.386371  [    8/   88]
per-ex loss: 0.535393  [    9/   88]
per-ex loss: 0.361012  [   10/   88]
per-ex loss: 0.446413  [   11/   88]
per-ex loss: 0.433892  [   12/   88]
per-ex loss: 0.511574  [   13/   88]
per-ex loss: 0.533495  [   14/   88]
per-ex loss: 0.304442  [   15/   88]
per-ex loss: 0.635919  [   16/   88]
per-ex loss: 0.639420  [   17/   88]
per-ex loss: 0.438320  [   18/   88]
per-ex loss: 0.365705  [   19/   88]
per-ex loss: 0.346957  [   20/   88]
per-ex loss: 0.323434  [   21/   88]
per-ex loss: 0.505631  [   22/   88]
per-ex loss: 0.449468  [   23/   88]
per-ex loss: 0.290579  [   24/   88]
per-ex loss: 0.508443  [   25/   88]
per-ex loss: 0.450708  [   26/   88]
per-ex loss: 0.352408  [   27/   88]
per-ex loss: 0.509084  [   28/   88]
per-ex loss: 0.410119  [   29/   88]
per-ex loss: 0.362764  [   30/   88]
per-ex loss: 0.394904  [   31/   88]
per-ex loss: 0.497819  [   32/   88]
per-ex loss: 0.312973  [   33/   88]
per-ex loss: 0.411263  [   34/   88]
per-ex loss: 0.412021  [   35/   88]
per-ex loss: 0.440328  [   36/   88]
per-ex loss: 0.393861  [   37/   88]
per-ex loss: 0.549934  [   38/   88]
per-ex loss: 0.549469  [   39/   88]
per-ex loss: 0.601232  [   40/   88]
per-ex loss: 0.292321  [   41/   88]
per-ex loss: 0.461883  [   42/   88]
per-ex loss: 0.310005  [   43/   88]
per-ex loss: 0.325895  [   44/   88]
per-ex loss: 0.293026  [   45/   88]
per-ex loss: 0.347220  [   46/   88]
per-ex loss: 0.385145  [   47/   88]
per-ex loss: 0.594822  [   48/   88]
per-ex loss: 0.315558  [   49/   88]
per-ex loss: 0.519677  [   50/   88]
per-ex loss: 0.522316  [   51/   88]
per-ex loss: 0.287743  [   52/   88]
per-ex loss: 0.602749  [   53/   88]
per-ex loss: 0.496235  [   54/   88]
per-ex loss: 0.556142  [   55/   88]
per-ex loss: 0.366378  [   56/   88]
per-ex loss: 0.577778  [   57/   88]
per-ex loss: 0.360030  [   58/   88]
per-ex loss: 0.369051  [   59/   88]
per-ex loss: 0.432593  [   60/   88]
per-ex loss: 0.472701  [   61/   88]
per-ex loss: 0.402364  [   62/   88]
per-ex loss: 0.631165  [   63/   88]
per-ex loss: 0.345308  [   64/   88]
per-ex loss: 0.556842  [   65/   88]
per-ex loss: 0.349411  [   66/   88]
per-ex loss: 0.356183  [   67/   88]
per-ex loss: 0.401653  [   68/   88]
per-ex loss: 0.324031  [   69/   88]
per-ex loss: 0.548929  [   70/   88]
per-ex loss: 0.364209  [   71/   88]
per-ex loss: 0.411944  [   72/   88]
per-ex loss: 0.446506  [   73/   88]
per-ex loss: 0.500179  [   74/   88]
per-ex loss: 0.316196  [   75/   88]
per-ex loss: 0.395604  [   76/   88]
per-ex loss: 0.288483  [   77/   88]
per-ex loss: 0.356716  [   78/   88]
per-ex loss: 0.605790  [   79/   88]
per-ex loss: 0.253038  [   80/   88]
per-ex loss: 0.561836  [   81/   88]
per-ex loss: 0.614395  [   82/   88]
per-ex loss: 0.379612  [   83/   88]
per-ex loss: 0.365798  [   84/   88]
per-ex loss: 0.516534  [   85/   88]
per-ex loss: 0.585938  [   86/   88]
per-ex loss: 0.315052  [   87/   88]
per-ex loss: 0.320352  [   88/   88]
Train Error: Avg loss: 0.43058964
validation Error: 
 Avg loss: 0.52722200 
 F1: 0.499232 
 Precision: 0.630167 
 Recall: 0.413348
 IoU: 0.332651

test Error: 
 Avg loss: 0.48771763 
 F1: 0.560599 
 Precision: 0.648041 
 Recall: 0.493950
 IoU: 0.389467

We have finished training iteration 120
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_118_.pth
per-ex loss: 0.431364  [    1/   88]
per-ex loss: 0.523001  [    2/   88]
per-ex loss: 0.344134  [    3/   88]
per-ex loss: 0.342083  [    4/   88]
per-ex loss: 0.383213  [    5/   88]
per-ex loss: 0.287917  [    6/   88]
per-ex loss: 0.403899  [    7/   88]
per-ex loss: 0.464387  [    8/   88]
per-ex loss: 0.531305  [    9/   88]
per-ex loss: 0.576744  [   10/   88]
per-ex loss: 0.641155  [   11/   88]
per-ex loss: 0.511941  [   12/   88]
per-ex loss: 0.541837  [   13/   88]
per-ex loss: 0.308284  [   14/   88]
per-ex loss: 0.608147  [   15/   88]
per-ex loss: 0.409969  [   16/   88]
per-ex loss: 0.469158  [   17/   88]
per-ex loss: 0.455668  [   18/   88]
per-ex loss: 0.359177  [   19/   88]
per-ex loss: 0.575186  [   20/   88]
per-ex loss: 0.319699  [   21/   88]
per-ex loss: 0.405005  [   22/   88]
per-ex loss: 0.525315  [   23/   88]
per-ex loss: 0.549600  [   24/   88]
per-ex loss: 0.406049  [   25/   88]
per-ex loss: 0.340338  [   26/   88]
per-ex loss: 0.318921  [   27/   88]
per-ex loss: 0.412360  [   28/   88]
per-ex loss: 0.392968  [   29/   88]
per-ex loss: 0.372365  [   30/   88]
per-ex loss: 0.299503  [   31/   88]
per-ex loss: 0.514405  [   32/   88]
per-ex loss: 0.326095  [   33/   88]
per-ex loss: 0.427130  [   34/   88]
per-ex loss: 0.449915  [   35/   88]
per-ex loss: 0.394134  [   36/   88]
per-ex loss: 0.325502  [   37/   88]
per-ex loss: 0.292272  [   38/   88]
per-ex loss: 0.365600  [   39/   88]
per-ex loss: 0.585738  [   40/   88]
per-ex loss: 0.550432  [   41/   88]
per-ex loss: 0.345478  [   42/   88]
per-ex loss: 0.289892  [   43/   88]
per-ex loss: 0.425159  [   44/   88]
per-ex loss: 0.319829  [   45/   88]
per-ex loss: 0.393225  [   46/   88]
per-ex loss: 0.395506  [   47/   88]
per-ex loss: 0.446245  [   48/   88]
per-ex loss: 0.372540  [   49/   88]
per-ex loss: 0.455580  [   50/   88]
per-ex loss: 0.407835  [   51/   88]
per-ex loss: 0.624298  [   52/   88]
per-ex loss: 0.502932  [   53/   88]
per-ex loss: 0.350046  [   54/   88]
per-ex loss: 0.386705  [   55/   88]
per-ex loss: 0.402422  [   56/   88]
per-ex loss: 0.346780  [   57/   88]
per-ex loss: 0.364344  [   58/   88]
per-ex loss: 0.511860  [   59/   88]
per-ex loss: 0.321229  [   60/   88]
per-ex loss: 0.403008  [   61/   88]
per-ex loss: 0.527684  [   62/   88]
per-ex loss: 0.602785  [   63/   88]
per-ex loss: 0.322288  [   64/   88]
per-ex loss: 0.318363  [   65/   88]
per-ex loss: 0.584065  [   66/   88]
per-ex loss: 0.348889  [   67/   88]
per-ex loss: 0.583456  [   68/   88]
per-ex loss: 0.353306  [   69/   88]
per-ex loss: 0.320596  [   70/   88]
per-ex loss: 0.325455  [   71/   88]
per-ex loss: 0.384163  [   72/   88]
per-ex loss: 0.301533  [   73/   88]
per-ex loss: 0.439847  [   74/   88]
per-ex loss: 0.586624  [   75/   88]
per-ex loss: 0.561229  [   76/   88]
per-ex loss: 0.478647  [   77/   88]
per-ex loss: 0.518297  [   78/   88]
per-ex loss: 0.422101  [   79/   88]
per-ex loss: 0.480632  [   80/   88]
per-ex loss: 0.493433  [   81/   88]
per-ex loss: 0.544657  [   82/   88]
per-ex loss: 0.634385  [   83/   88]
per-ex loss: 0.224663  [   84/   88]
per-ex loss: 0.367039  [   85/   88]
per-ex loss: 0.548060  [   86/   88]
per-ex loss: 0.557300  [   87/   88]
per-ex loss: 0.319026  [   88/   88]
Train Error: Avg loss: 0.43131081
validation Error: 
 Avg loss: 0.54000571 
 F1: 0.489260 
 Precision: 0.475304 
 Recall: 0.504061
 IoU: 0.323855

test Error: 
 Avg loss: 0.48968435 
 F1: 0.549523 
 Precision: 0.517088 
 Recall: 0.586300
 IoU: 0.378857

We have finished training iteration 121
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_119_.pth
per-ex loss: 0.386841  [    1/   88]
per-ex loss: 0.490937  [    2/   88]
per-ex loss: 0.338356  [    3/   88]
per-ex loss: 0.454231  [    4/   88]
per-ex loss: 0.326962  [    5/   88]
per-ex loss: 0.490286  [    6/   88]
per-ex loss: 0.615123  [    7/   88]
per-ex loss: 0.294962  [    8/   88]
per-ex loss: 0.398399  [    9/   88]
per-ex loss: 0.438765  [   10/   88]
per-ex loss: 0.521370  [   11/   88]
per-ex loss: 0.311836  [   12/   88]
per-ex loss: 0.533864  [   13/   88]
per-ex loss: 0.572417  [   14/   88]
per-ex loss: 0.441989  [   15/   88]
per-ex loss: 0.353428  [   16/   88]
per-ex loss: 0.417183  [   17/   88]
per-ex loss: 0.328058  [   18/   88]
per-ex loss: 0.302292  [   19/   88]
per-ex loss: 0.559075  [   20/   88]
per-ex loss: 0.396657  [   21/   88]
per-ex loss: 0.534254  [   22/   88]
per-ex loss: 0.239546  [   23/   88]
per-ex loss: 0.388633  [   24/   88]
per-ex loss: 0.318880  [   25/   88]
per-ex loss: 0.555223  [   26/   88]
per-ex loss: 0.293384  [   27/   88]
per-ex loss: 0.499876  [   28/   88]
per-ex loss: 0.533107  [   29/   88]
per-ex loss: 0.394242  [   30/   88]
per-ex loss: 0.531689  [   31/   88]
per-ex loss: 0.305248  [   32/   88]
per-ex loss: 0.324857  [   33/   88]
per-ex loss: 0.426704  [   34/   88]
per-ex loss: 0.497614  [   35/   88]
per-ex loss: 0.459637  [   36/   88]
per-ex loss: 0.548503  [   37/   88]
per-ex loss: 0.457304  [   38/   88]
per-ex loss: 0.608293  [   39/   88]
per-ex loss: 0.375875  [   40/   88]
per-ex loss: 0.362960  [   41/   88]
per-ex loss: 0.575461  [   42/   88]
per-ex loss: 0.537631  [   43/   88]
per-ex loss: 0.288765  [   44/   88]
per-ex loss: 0.353920  [   45/   88]
per-ex loss: 0.312664  [   46/   88]
per-ex loss: 0.350767  [   47/   88]
per-ex loss: 0.316549  [   48/   88]
per-ex loss: 0.283129  [   49/   88]
per-ex loss: 0.339775  [   50/   88]
per-ex loss: 0.407180  [   51/   88]
per-ex loss: 0.453151  [   52/   88]
per-ex loss: 0.584581  [   53/   88]
per-ex loss: 0.363781  [   54/   88]
per-ex loss: 0.363153  [   55/   88]
per-ex loss: 0.511674  [   56/   88]
per-ex loss: 0.394590  [   57/   88]
per-ex loss: 0.333611  [   58/   88]
per-ex loss: 0.440654  [   59/   88]
per-ex loss: 0.396982  [   60/   88]
per-ex loss: 0.516083  [   61/   88]
per-ex loss: 0.319040  [   62/   88]
per-ex loss: 0.364325  [   63/   88]
per-ex loss: 0.412083  [   64/   88]
per-ex loss: 0.597308  [   65/   88]
per-ex loss: 0.528875  [   66/   88]
per-ex loss: 0.337574  [   67/   88]
per-ex loss: 0.362673  [   68/   88]
per-ex loss: 0.522210  [   69/   88]
per-ex loss: 0.532437  [   70/   88]
per-ex loss: 0.679636  [   71/   88]
per-ex loss: 0.416172  [   72/   88]
per-ex loss: 0.521256  [   73/   88]
per-ex loss: 0.440613  [   74/   88]
per-ex loss: 0.480455  [   75/   88]
per-ex loss: 0.391087  [   76/   88]
per-ex loss: 0.316299  [   77/   88]
per-ex loss: 0.631137  [   78/   88]
per-ex loss: 0.364598  [   79/   88]
per-ex loss: 0.382083  [   80/   88]
per-ex loss: 0.382651  [   81/   88]
per-ex loss: 0.410126  [   82/   88]
per-ex loss: 0.541964  [   83/   88]
per-ex loss: 0.328001  [   84/   88]
per-ex loss: 0.620462  [   85/   88]
per-ex loss: 0.429133  [   86/   88]
per-ex loss: 0.550922  [   87/   88]
per-ex loss: 0.389738  [   88/   88]
Train Error: Avg loss: 0.43188429
validation Error: 
 Avg loss: 0.52638447 
 F1: 0.501818 
 Precision: 0.530101 
 Recall: 0.476400
 IoU: 0.334951

test Error: 
 Avg loss: 0.48172475 
 F1: 0.563282 
 Precision: 0.557319 
 Recall: 0.569373
 IoU: 0.392062

We have finished training iteration 122
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_120_.pth
per-ex loss: 0.433382  [    1/   88]
per-ex loss: 0.451572  [    2/   88]
per-ex loss: 0.388177  [    3/   88]
per-ex loss: 0.447788  [    4/   88]
per-ex loss: 0.327439  [    5/   88]
per-ex loss: 0.536403  [    6/   88]
per-ex loss: 0.517798  [    7/   88]
per-ex loss: 0.412379  [    8/   88]
per-ex loss: 0.317075  [    9/   88]
per-ex loss: 0.448442  [   10/   88]
per-ex loss: 0.566710  [   11/   88]
per-ex loss: 0.658261  [   12/   88]
per-ex loss: 0.331151  [   13/   88]
per-ex loss: 0.321720  [   14/   88]
per-ex loss: 0.547139  [   15/   88]
per-ex loss: 0.403881  [   16/   88]
per-ex loss: 0.570720  [   17/   88]
per-ex loss: 0.618177  [   18/   88]
per-ex loss: 0.556604  [   19/   88]
per-ex loss: 0.326379  [   20/   88]
per-ex loss: 0.340538  [   21/   88]
per-ex loss: 0.393594  [   22/   88]
per-ex loss: 0.363982  [   23/   88]
per-ex loss: 0.565800  [   24/   88]
per-ex loss: 0.516509  [   25/   88]
per-ex loss: 0.288466  [   26/   88]
per-ex loss: 0.315979  [   27/   88]
per-ex loss: 0.403896  [   28/   88]
per-ex loss: 0.534892  [   29/   88]
per-ex loss: 0.321014  [   30/   88]
per-ex loss: 0.369849  [   31/   88]
per-ex loss: 0.328482  [   32/   88]
per-ex loss: 0.589619  [   33/   88]
per-ex loss: 0.411127  [   34/   88]
per-ex loss: 0.331439  [   35/   88]
per-ex loss: 0.499424  [   36/   88]
per-ex loss: 0.621295  [   37/   88]
per-ex loss: 0.292962  [   38/   88]
per-ex loss: 0.444006  [   39/   88]
per-ex loss: 0.615615  [   40/   88]
per-ex loss: 0.377874  [   41/   88]
per-ex loss: 0.422789  [   42/   88]
per-ex loss: 0.542017  [   43/   88]
per-ex loss: 0.385253  [   44/   88]
per-ex loss: 0.390957  [   45/   88]
per-ex loss: 0.374910  [   46/   88]
per-ex loss: 0.333308  [   47/   88]
per-ex loss: 0.541307  [   48/   88]
per-ex loss: 0.374732  [   49/   88]
per-ex loss: 0.512851  [   50/   88]
per-ex loss: 0.364654  [   51/   88]
per-ex loss: 0.481943  [   52/   88]
per-ex loss: 0.352360  [   53/   88]
per-ex loss: 0.609272  [   54/   88]
per-ex loss: 0.441103  [   55/   88]
per-ex loss: 0.495507  [   56/   88]
per-ex loss: 0.361302  [   57/   88]
per-ex loss: 0.613619  [   58/   88]
per-ex loss: 0.322242  [   59/   88]
per-ex loss: 0.498857  [   60/   88]
per-ex loss: 0.386767  [   61/   88]
per-ex loss: 0.383680  [   62/   88]
per-ex loss: 0.329960  [   63/   88]
per-ex loss: 0.346517  [   64/   88]
per-ex loss: 0.288422  [   65/   88]
per-ex loss: 0.357213  [   66/   88]
per-ex loss: 0.363033  [   67/   88]
per-ex loss: 0.440633  [   68/   88]
per-ex loss: 0.429002  [   69/   88]
per-ex loss: 0.612603  [   70/   88]
per-ex loss: 0.271948  [   71/   88]
per-ex loss: 0.521270  [   72/   88]
per-ex loss: 0.548982  [   73/   88]
per-ex loss: 0.334515  [   74/   88]
per-ex loss: 0.468063  [   75/   88]
per-ex loss: 0.415406  [   76/   88]
per-ex loss: 0.455865  [   77/   88]
per-ex loss: 0.288976  [   78/   88]
per-ex loss: 0.538302  [   79/   88]
per-ex loss: 0.412541  [   80/   88]
per-ex loss: 0.355420  [   81/   88]
per-ex loss: 0.468989  [   82/   88]
per-ex loss: 0.568302  [   83/   88]
per-ex loss: 0.222178  [   84/   88]
per-ex loss: 0.335775  [   85/   88]
per-ex loss: 0.442073  [   86/   88]
per-ex loss: 0.363778  [   87/   88]
per-ex loss: 0.536773  [   88/   88]
Train Error: Avg loss: 0.43197198
validation Error: 
 Avg loss: 0.51526681 
 F1: 0.513386 
 Precision: 0.595017 
 Recall: 0.451451
 IoU: 0.345339

test Error: 
 Avg loss: 0.47778674 
 F1: 0.570516 
 Precision: 0.615931 
 Recall: 0.531338
 IoU: 0.399106

We have finished training iteration 123
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_121_.pth
per-ex loss: 0.557737  [    1/   88]
per-ex loss: 0.523084  [    2/   88]
per-ex loss: 0.407828  [    3/   88]
per-ex loss: 0.383451  [    4/   88]
per-ex loss: 0.431491  [    5/   88]
per-ex loss: 0.472953  [    6/   88]
per-ex loss: 0.550701  [    7/   88]
per-ex loss: 0.346707  [    8/   88]
per-ex loss: 0.509951  [    9/   88]
per-ex loss: 0.278755  [   10/   88]
per-ex loss: 0.570817  [   11/   88]
per-ex loss: 0.522999  [   12/   88]
per-ex loss: 0.524049  [   13/   88]
per-ex loss: 0.385317  [   14/   88]
per-ex loss: 0.463619  [   15/   88]
per-ex loss: 0.464416  [   16/   88]
per-ex loss: 0.585217  [   17/   88]
per-ex loss: 0.394489  [   18/   88]
per-ex loss: 0.356883  [   19/   88]
per-ex loss: 0.426035  [   20/   88]
per-ex loss: 0.521932  [   21/   88]
per-ex loss: 0.498464  [   22/   88]
per-ex loss: 0.348143  [   23/   88]
per-ex loss: 0.435892  [   24/   88]
per-ex loss: 0.331541  [   25/   88]
per-ex loss: 0.338805  [   26/   88]
per-ex loss: 0.444436  [   27/   88]
per-ex loss: 0.294776  [   28/   88]
per-ex loss: 0.319938  [   29/   88]
per-ex loss: 0.308070  [   30/   88]
per-ex loss: 0.392266  [   31/   88]
per-ex loss: 0.317687  [   32/   88]
per-ex loss: 0.559819  [   33/   88]
per-ex loss: 0.477574  [   34/   88]
per-ex loss: 0.617960  [   35/   88]
per-ex loss: 0.342760  [   36/   88]
per-ex loss: 0.337683  [   37/   88]
per-ex loss: 0.565951  [   38/   88]
per-ex loss: 0.321748  [   39/   88]
per-ex loss: 0.298889  [   40/   88]
per-ex loss: 0.611690  [   41/   88]
per-ex loss: 0.535860  [   42/   88]
per-ex loss: 0.527223  [   43/   88]
per-ex loss: 0.388857  [   44/   88]
per-ex loss: 0.451034  [   45/   88]
per-ex loss: 0.521807  [   46/   88]
per-ex loss: 0.411656  [   47/   88]
per-ex loss: 0.464872  [   48/   88]
per-ex loss: 0.325639  [   49/   88]
per-ex loss: 0.406261  [   50/   88]
per-ex loss: 0.329294  [   51/   88]
per-ex loss: 0.401321  [   52/   88]
per-ex loss: 0.356048  [   53/   88]
per-ex loss: 0.364306  [   54/   88]
per-ex loss: 0.284342  [   55/   88]
per-ex loss: 0.334942  [   56/   88]
per-ex loss: 0.304397  [   57/   88]
per-ex loss: 0.321524  [   58/   88]
per-ex loss: 0.380881  [   59/   88]
per-ex loss: 0.534713  [   60/   88]
per-ex loss: 0.396104  [   61/   88]
per-ex loss: 0.351597  [   62/   88]
per-ex loss: 0.519526  [   63/   88]
per-ex loss: 0.323589  [   64/   88]
per-ex loss: 0.470367  [   65/   88]
per-ex loss: 0.585024  [   66/   88]
per-ex loss: 0.624133  [   67/   88]
per-ex loss: 0.310623  [   68/   88]
per-ex loss: 0.439828  [   69/   88]
per-ex loss: 0.531954  [   70/   88]
per-ex loss: 0.418747  [   71/   88]
per-ex loss: 0.356399  [   72/   88]
per-ex loss: 0.619853  [   73/   88]
per-ex loss: 0.360585  [   74/   88]
per-ex loss: 0.231312  [   75/   88]
per-ex loss: 0.348200  [   76/   88]
per-ex loss: 0.539354  [   77/   88]
per-ex loss: 0.430482  [   78/   88]
per-ex loss: 0.392992  [   79/   88]
per-ex loss: 0.386979  [   80/   88]
per-ex loss: 0.354404  [   81/   88]
per-ex loss: 0.318199  [   82/   88]
per-ex loss: 0.377847  [   83/   88]
per-ex loss: 0.340168  [   84/   88]
per-ex loss: 0.625312  [   85/   88]
per-ex loss: 0.510315  [   86/   88]
per-ex loss: 0.432339  [   87/   88]
per-ex loss: 0.507876  [   88/   88]
Train Error: Avg loss: 0.42717736
validation Error: 
 Avg loss: 0.52082041 
 F1: 0.507465 
 Precision: 0.592088 
 Recall: 0.444007
 IoU: 0.340002

test Error: 
 Avg loss: 0.49085720 
 F1: 0.549950 
 Precision: 0.613268 
 Recall: 0.498484
 IoU: 0.379263

We have finished training iteration 124
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_122_.pth
per-ex loss: 0.438574  [    1/   88]
per-ex loss: 0.530243  [    2/   88]
per-ex loss: 0.357820  [    3/   88]
per-ex loss: 0.382625  [    4/   88]
per-ex loss: 0.523034  [    5/   88]
per-ex loss: 0.327673  [    6/   88]
per-ex loss: 0.333171  [    7/   88]
per-ex loss: 0.331756  [    8/   88]
per-ex loss: 0.364681  [    9/   88]
per-ex loss: 0.422365  [   10/   88]
per-ex loss: 0.496318  [   11/   88]
per-ex loss: 0.437340  [   12/   88]
per-ex loss: 0.315728  [   13/   88]
per-ex loss: 0.295080  [   14/   88]
per-ex loss: 0.301423  [   15/   88]
per-ex loss: 0.363065  [   16/   88]
per-ex loss: 0.374462  [   17/   88]
per-ex loss: 0.539229  [   18/   88]
per-ex loss: 0.354142  [   19/   88]
per-ex loss: 0.286006  [   20/   88]
per-ex loss: 0.350142  [   21/   88]
per-ex loss: 0.615072  [   22/   88]
per-ex loss: 0.426592  [   23/   88]
per-ex loss: 0.533820  [   24/   88]
per-ex loss: 0.626222  [   25/   88]
per-ex loss: 0.402883  [   26/   88]
per-ex loss: 0.527603  [   27/   88]
per-ex loss: 0.553119  [   28/   88]
per-ex loss: 0.391322  [   29/   88]
per-ex loss: 0.443025  [   30/   88]
per-ex loss: 0.320504  [   31/   88]
per-ex loss: 0.363691  [   32/   88]
per-ex loss: 0.394480  [   33/   88]
per-ex loss: 0.511070  [   34/   88]
per-ex loss: 0.365564  [   35/   88]
per-ex loss: 0.344800  [   36/   88]
per-ex loss: 0.391628  [   37/   88]
per-ex loss: 0.320239  [   38/   88]
per-ex loss: 0.603630  [   39/   88]
per-ex loss: 0.284000  [   40/   88]
per-ex loss: 0.366893  [   41/   88]
per-ex loss: 0.513590  [   42/   88]
per-ex loss: 0.473575  [   43/   88]
per-ex loss: 0.506673  [   44/   88]
per-ex loss: 0.616362  [   45/   88]
per-ex loss: 0.361384  [   46/   88]
per-ex loss: 0.399084  [   47/   88]
per-ex loss: 0.533580  [   48/   88]
per-ex loss: 0.598773  [   49/   88]
per-ex loss: 0.522438  [   50/   88]
per-ex loss: 0.300773  [   51/   88]
per-ex loss: 0.298742  [   52/   88]
per-ex loss: 0.453413  [   53/   88]
per-ex loss: 0.357444  [   54/   88]
per-ex loss: 0.316094  [   55/   88]
per-ex loss: 0.466229  [   56/   88]
per-ex loss: 0.445397  [   57/   88]
per-ex loss: 0.545085  [   58/   88]
per-ex loss: 0.538140  [   59/   88]
per-ex loss: 0.547579  [   60/   88]
per-ex loss: 0.308169  [   61/   88]
per-ex loss: 0.340358  [   62/   88]
per-ex loss: 0.343642  [   63/   88]
per-ex loss: 0.371796  [   64/   88]
per-ex loss: 0.318638  [   65/   88]
per-ex loss: 0.640220  [   66/   88]
per-ex loss: 0.539548  [   67/   88]
per-ex loss: 0.336456  [   68/   88]
per-ex loss: 0.519196  [   69/   88]
per-ex loss: 0.417236  [   70/   88]
per-ex loss: 0.578559  [   71/   88]
per-ex loss: 0.485495  [   72/   88]
per-ex loss: 0.375140  [   73/   88]
per-ex loss: 0.501270  [   74/   88]
per-ex loss: 0.555930  [   75/   88]
per-ex loss: 0.321880  [   76/   88]
per-ex loss: 0.517448  [   77/   88]
per-ex loss: 0.363290  [   78/   88]
per-ex loss: 0.397093  [   79/   88]
per-ex loss: 0.467936  [   80/   88]
per-ex loss: 0.448139  [   81/   88]
per-ex loss: 0.566852  [   82/   88]
per-ex loss: 0.600820  [   83/   88]
per-ex loss: 0.365731  [   84/   88]
per-ex loss: 0.437305  [   85/   88]
per-ex loss: 0.368525  [   86/   88]
per-ex loss: 0.390765  [   87/   88]
per-ex loss: 0.334834  [   88/   88]
Train Error: Avg loss: 0.43088254
validation Error: 
 Avg loss: 0.51933448 
 F1: 0.509648 
 Precision: 0.598459 
 Recall: 0.443791
 IoU: 0.341965

test Error: 
 Avg loss: 0.47808972 
 F1: 0.568080 
 Precision: 0.632715 
 Recall: 0.515426
 IoU: 0.396726

We have finished training iteration 125
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_104_.pth
per-ex loss: 0.565179  [    1/   88]
per-ex loss: 0.475898  [    2/   88]
per-ex loss: 0.338441  [    3/   88]
per-ex loss: 0.313706  [    4/   88]
per-ex loss: 0.550727  [    5/   88]
per-ex loss: 0.517067  [    6/   88]
per-ex loss: 0.347787  [    7/   88]
per-ex loss: 0.618442  [    8/   88]
per-ex loss: 0.307056  [    9/   88]
per-ex loss: 0.531048  [   10/   88]
per-ex loss: 0.298099  [   11/   88]
per-ex loss: 0.618490  [   12/   88]
per-ex loss: 0.416134  [   13/   88]
per-ex loss: 0.377947  [   14/   88]
per-ex loss: 0.323591  [   15/   88]
per-ex loss: 0.369711  [   16/   88]
per-ex loss: 0.336764  [   17/   88]
per-ex loss: 0.369860  [   18/   88]
per-ex loss: 0.212273  [   19/   88]
per-ex loss: 0.329365  [   20/   88]
per-ex loss: 0.406785  [   21/   88]
per-ex loss: 0.601721  [   22/   88]
per-ex loss: 0.592538  [   23/   88]
per-ex loss: 0.360876  [   24/   88]
per-ex loss: 0.451283  [   25/   88]
per-ex loss: 0.400958  [   26/   88]
per-ex loss: 0.561672  [   27/   88]
per-ex loss: 0.541016  [   28/   88]
per-ex loss: 0.372695  [   29/   88]
per-ex loss: 0.507177  [   30/   88]
per-ex loss: 0.527984  [   31/   88]
per-ex loss: 0.513111  [   32/   88]
per-ex loss: 0.320573  [   33/   88]
per-ex loss: 0.571186  [   34/   88]
per-ex loss: 0.412069  [   35/   88]
per-ex loss: 0.335929  [   36/   88]
per-ex loss: 0.285457  [   37/   88]
per-ex loss: 0.314115  [   38/   88]
per-ex loss: 0.513380  [   39/   88]
per-ex loss: 0.502956  [   40/   88]
per-ex loss: 0.513743  [   41/   88]
per-ex loss: 0.366160  [   42/   88]
per-ex loss: 0.283408  [   43/   88]
per-ex loss: 0.512098  [   44/   88]
per-ex loss: 0.490051  [   45/   88]
per-ex loss: 0.412672  [   46/   88]
per-ex loss: 0.316838  [   47/   88]
per-ex loss: 0.399679  [   48/   88]
per-ex loss: 0.443085  [   49/   88]
per-ex loss: 0.483715  [   50/   88]
per-ex loss: 0.631894  [   51/   88]
per-ex loss: 0.372415  [   52/   88]
per-ex loss: 0.348488  [   53/   88]
per-ex loss: 0.350094  [   54/   88]
per-ex loss: 0.372339  [   55/   88]
per-ex loss: 0.408209  [   56/   88]
per-ex loss: 0.469999  [   57/   88]
per-ex loss: 0.321616  [   58/   88]
per-ex loss: 0.527680  [   59/   88]
per-ex loss: 0.433067  [   60/   88]
per-ex loss: 0.311452  [   61/   88]
per-ex loss: 0.358037  [   62/   88]
per-ex loss: 0.324859  [   63/   88]
per-ex loss: 0.309263  [   64/   88]
per-ex loss: 0.530594  [   65/   88]
per-ex loss: 0.528141  [   66/   88]
per-ex loss: 0.335339  [   67/   88]
per-ex loss: 0.343499  [   68/   88]
per-ex loss: 0.328134  [   69/   88]
per-ex loss: 0.411170  [   70/   88]
per-ex loss: 0.591760  [   71/   88]
per-ex loss: 0.425857  [   72/   88]
per-ex loss: 0.626338  [   73/   88]
per-ex loss: 0.481459  [   74/   88]
per-ex loss: 0.522403  [   75/   88]
per-ex loss: 0.293410  [   76/   88]
per-ex loss: 0.384443  [   77/   88]
per-ex loss: 0.348089  [   78/   88]
per-ex loss: 0.487931  [   79/   88]
per-ex loss: 0.474225  [   80/   88]
per-ex loss: 0.532389  [   81/   88]
per-ex loss: 0.553427  [   82/   88]
per-ex loss: 0.380801  [   83/   88]
per-ex loss: 0.358029  [   84/   88]
per-ex loss: 0.358202  [   85/   88]
per-ex loss: 0.310469  [   86/   88]
per-ex loss: 0.421919  [   87/   88]
per-ex loss: 0.413968  [   88/   88]
Train Error: Avg loss: 0.42629459
validation Error: 
 Avg loss: 0.52276918 
 F1: 0.504416 
 Precision: 0.543527 
 Recall: 0.470556
 IoU: 0.337271

test Error: 
 Avg loss: 0.48772380 
 F1: 0.552931 
 Precision: 0.571871 
 Recall: 0.535205
 IoU: 0.382104

We have finished training iteration 126
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_124_.pth
per-ex loss: 0.307549  [    1/   88]
per-ex loss: 0.434393  [    2/   88]
per-ex loss: 0.418290  [    3/   88]
per-ex loss: 0.433086  [    4/   88]
per-ex loss: 0.547297  [    5/   88]
per-ex loss: 0.541121  [    6/   88]
per-ex loss: 0.289287  [    7/   88]
per-ex loss: 0.557667  [    8/   88]
per-ex loss: 0.374081  [    9/   88]
per-ex loss: 0.484995  [   10/   88]
per-ex loss: 0.367985  [   11/   88]
per-ex loss: 0.353313  [   12/   88]
per-ex loss: 0.411987  [   13/   88]
per-ex loss: 0.521700  [   14/   88]
per-ex loss: 0.682979  [   15/   88]
per-ex loss: 0.513150  [   16/   88]
per-ex loss: 0.239234  [   17/   88]
per-ex loss: 0.376837  [   18/   88]
per-ex loss: 0.381916  [   19/   88]
per-ex loss: 0.563272  [   20/   88]
per-ex loss: 0.515178  [   21/   88]
per-ex loss: 0.353393  [   22/   88]
per-ex loss: 0.611872  [   23/   88]
per-ex loss: 0.494674  [   24/   88]
per-ex loss: 0.403117  [   25/   88]
per-ex loss: 0.370214  [   26/   88]
per-ex loss: 0.347943  [   27/   88]
per-ex loss: 0.385239  [   28/   88]
per-ex loss: 0.460991  [   29/   88]
per-ex loss: 0.528675  [   30/   88]
per-ex loss: 0.340170  [   31/   88]
per-ex loss: 0.473832  [   32/   88]
per-ex loss: 0.320358  [   33/   88]
per-ex loss: 0.423054  [   34/   88]
per-ex loss: 0.555631  [   35/   88]
per-ex loss: 0.299707  [   36/   88]
per-ex loss: 0.339753  [   37/   88]
per-ex loss: 0.554100  [   38/   88]
per-ex loss: 0.468453  [   39/   88]
per-ex loss: 0.414318  [   40/   88]
per-ex loss: 0.336557  [   41/   88]
per-ex loss: 0.501550  [   42/   88]
per-ex loss: 0.396892  [   43/   88]
per-ex loss: 0.342995  [   44/   88]
per-ex loss: 0.493759  [   45/   88]
per-ex loss: 0.502136  [   46/   88]
per-ex loss: 0.440648  [   47/   88]
per-ex loss: 0.351172  [   48/   88]
per-ex loss: 0.517895  [   49/   88]
per-ex loss: 0.602281  [   50/   88]
per-ex loss: 0.581873  [   51/   88]
per-ex loss: 0.283971  [   52/   88]
per-ex loss: 0.395739  [   53/   88]
per-ex loss: 0.515170  [   54/   88]
per-ex loss: 0.343350  [   55/   88]
per-ex loss: 0.359133  [   56/   88]
per-ex loss: 0.391019  [   57/   88]
per-ex loss: 0.439454  [   58/   88]
per-ex loss: 0.326129  [   59/   88]
per-ex loss: 0.338280  [   60/   88]
per-ex loss: 0.437797  [   61/   88]
per-ex loss: 0.386115  [   62/   88]
per-ex loss: 0.402726  [   63/   88]
per-ex loss: 0.382455  [   64/   88]
per-ex loss: 0.409421  [   65/   88]
per-ex loss: 0.474114  [   66/   88]
per-ex loss: 0.277358  [   67/   88]
per-ex loss: 0.280497  [   68/   88]
per-ex loss: 0.627136  [   69/   88]
per-ex loss: 0.532983  [   70/   88]
per-ex loss: 0.604879  [   71/   88]
per-ex loss: 0.312175  [   72/   88]
per-ex loss: 0.540305  [   73/   88]
per-ex loss: 0.533081  [   74/   88]
per-ex loss: 0.342211  [   75/   88]
per-ex loss: 0.315652  [   76/   88]
per-ex loss: 0.603065  [   77/   88]
per-ex loss: 0.333538  [   78/   88]
per-ex loss: 0.331765  [   79/   88]
per-ex loss: 0.362362  [   80/   88]
per-ex loss: 0.511835  [   81/   88]
per-ex loss: 0.320639  [   82/   88]
per-ex loss: 0.455967  [   83/   88]
per-ex loss: 0.609998  [   84/   88]
per-ex loss: 0.357251  [   85/   88]
per-ex loss: 0.324760  [   86/   88]
per-ex loss: 0.344987  [   87/   88]
per-ex loss: 0.309401  [   88/   88]
Train Error: Avg loss: 0.42778735
validation Error: 
 Avg loss: 0.53324417 
 F1: 0.491689 
 Precision: 0.622072 
 Recall: 0.406491
 IoU: 0.325987

test Error: 
 Avg loss: 0.50340201 
 F1: 0.538390 
 Precision: 0.650128 
 Recall: 0.459427
 IoU: 0.368354

We have finished training iteration 127
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_125_.pth
per-ex loss: 0.333030  [    1/   88]
per-ex loss: 0.613537  [    2/   88]
per-ex loss: 0.336081  [    3/   88]
per-ex loss: 0.326581  [    4/   88]
per-ex loss: 0.482849  [    5/   88]
per-ex loss: 0.458500  [    6/   88]
per-ex loss: 0.289672  [    7/   88]
per-ex loss: 0.500536  [    8/   88]
per-ex loss: 0.350361  [    9/   88]
per-ex loss: 0.519995  [   10/   88]
per-ex loss: 0.326217  [   11/   88]
per-ex loss: 0.389569  [   12/   88]
per-ex loss: 0.453081  [   13/   88]
per-ex loss: 0.600756  [   14/   88]
per-ex loss: 0.498239  [   15/   88]
per-ex loss: 0.657608  [   16/   88]
per-ex loss: 0.379682  [   17/   88]
per-ex loss: 0.407872  [   18/   88]
per-ex loss: 0.313894  [   19/   88]
per-ex loss: 0.598542  [   20/   88]
per-ex loss: 0.315327  [   21/   88]
per-ex loss: 0.337705  [   22/   88]
per-ex loss: 0.333213  [   23/   88]
per-ex loss: 0.483912  [   24/   88]
per-ex loss: 0.564403  [   25/   88]
per-ex loss: 0.551621  [   26/   88]
per-ex loss: 0.415045  [   27/   88]
per-ex loss: 0.514871  [   28/   88]
per-ex loss: 0.333386  [   29/   88]
per-ex loss: 0.328878  [   30/   88]
per-ex loss: 0.525927  [   31/   88]
per-ex loss: 0.508388  [   32/   88]
per-ex loss: 0.368135  [   33/   88]
per-ex loss: 0.621372  [   34/   88]
per-ex loss: 0.351662  [   35/   88]
per-ex loss: 0.496802  [   36/   88]
per-ex loss: 0.439243  [   37/   88]
per-ex loss: 0.477846  [   38/   88]
per-ex loss: 0.596920  [   39/   88]
per-ex loss: 0.398857  [   40/   88]
per-ex loss: 0.315820  [   41/   88]
per-ex loss: 0.353267  [   42/   88]
per-ex loss: 0.309576  [   43/   88]
per-ex loss: 0.557227  [   44/   88]
per-ex loss: 0.354857  [   45/   88]
per-ex loss: 0.519323  [   46/   88]
per-ex loss: 0.395122  [   47/   88]
per-ex loss: 0.397758  [   48/   88]
per-ex loss: 0.337666  [   49/   88]
per-ex loss: 0.408429  [   50/   88]
per-ex loss: 0.315106  [   51/   88]
per-ex loss: 0.362882  [   52/   88]
per-ex loss: 0.524230  [   53/   88]
per-ex loss: 0.364739  [   54/   88]
per-ex loss: 0.359388  [   55/   88]
per-ex loss: 0.475141  [   56/   88]
per-ex loss: 0.339459  [   57/   88]
per-ex loss: 0.407410  [   58/   88]
per-ex loss: 0.359593  [   59/   88]
per-ex loss: 0.380175  [   60/   88]
per-ex loss: 0.519043  [   61/   88]
per-ex loss: 0.252449  [   62/   88]
per-ex loss: 0.542494  [   63/   88]
per-ex loss: 0.446736  [   64/   88]
per-ex loss: 0.284645  [   65/   88]
per-ex loss: 0.379876  [   66/   88]
per-ex loss: 0.306485  [   67/   88]
per-ex loss: 0.378450  [   68/   88]
per-ex loss: 0.545948  [   69/   88]
per-ex loss: 0.634830  [   70/   88]
per-ex loss: 0.289393  [   71/   88]
per-ex loss: 0.439743  [   72/   88]
per-ex loss: 0.361622  [   73/   88]
per-ex loss: 0.583681  [   74/   88]
per-ex loss: 0.548786  [   75/   88]
per-ex loss: 0.494938  [   76/   88]
per-ex loss: 0.396356  [   77/   88]
per-ex loss: 0.445100  [   78/   88]
per-ex loss: 0.354475  [   79/   88]
per-ex loss: 0.518214  [   80/   88]
per-ex loss: 0.433902  [   81/   88]
per-ex loss: 0.325889  [   82/   88]
per-ex loss: 0.276893  [   83/   88]
per-ex loss: 0.424069  [   84/   88]
per-ex loss: 0.414881  [   85/   88]
per-ex loss: 0.558746  [   86/   88]
per-ex loss: 0.424431  [   87/   88]
per-ex loss: 0.537193  [   88/   88]
Train Error: Avg loss: 0.42905171
validation Error: 
 Avg loss: 0.51295159 
 F1: 0.516497 
 Precision: 0.565947 
 Recall: 0.474993
 IoU: 0.348160

test Error: 
 Avg loss: 0.48187243 
 F1: 0.558128 
 Precision: 0.573862 
 Recall: 0.543233
 IoU: 0.387085

We have finished training iteration 128
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_126_.pth
per-ex loss: 0.477134  [    1/   88]
per-ex loss: 0.394087  [    2/   88]
per-ex loss: 0.356310  [    3/   88]
per-ex loss: 0.570699  [    4/   88]
per-ex loss: 0.356514  [    5/   88]
per-ex loss: 0.335449  [    6/   88]
per-ex loss: 0.523422  [    7/   88]
per-ex loss: 0.555055  [    8/   88]
per-ex loss: 0.397959  [    9/   88]
per-ex loss: 0.397055  [   10/   88]
per-ex loss: 0.615063  [   11/   88]
per-ex loss: 0.489578  [   12/   88]
per-ex loss: 0.366404  [   13/   88]
per-ex loss: 0.498228  [   14/   88]
per-ex loss: 0.392905  [   15/   88]
per-ex loss: 0.580276  [   16/   88]
per-ex loss: 0.448285  [   17/   88]
per-ex loss: 0.284049  [   18/   88]
per-ex loss: 0.554704  [   19/   88]
per-ex loss: 0.626219  [   20/   88]
per-ex loss: 0.448984  [   21/   88]
per-ex loss: 0.291776  [   22/   88]
per-ex loss: 0.387013  [   23/   88]
per-ex loss: 0.376064  [   24/   88]
per-ex loss: 0.356981  [   25/   88]
per-ex loss: 0.285977  [   26/   88]
per-ex loss: 0.500100  [   27/   88]
per-ex loss: 0.390538  [   28/   88]
per-ex loss: 0.461863  [   29/   88]
per-ex loss: 0.396940  [   30/   88]
per-ex loss: 0.403246  [   31/   88]
per-ex loss: 0.337260  [   32/   88]
per-ex loss: 0.308859  [   33/   88]
per-ex loss: 0.314765  [   34/   88]
per-ex loss: 0.445683  [   35/   88]
per-ex loss: 0.447812  [   36/   88]
per-ex loss: 0.329821  [   37/   88]
per-ex loss: 0.628254  [   38/   88]
per-ex loss: 0.404522  [   39/   88]
per-ex loss: 0.526763  [   40/   88]
per-ex loss: 0.359556  [   41/   88]
per-ex loss: 0.440287  [   42/   88]
per-ex loss: 0.307885  [   43/   88]
per-ex loss: 0.546031  [   44/   88]
per-ex loss: 0.512467  [   45/   88]
per-ex loss: 0.513001  [   46/   88]
per-ex loss: 0.294725  [   47/   88]
per-ex loss: 0.466497  [   48/   88]
per-ex loss: 0.338168  [   49/   88]
per-ex loss: 0.399279  [   50/   88]
per-ex loss: 0.386492  [   51/   88]
per-ex loss: 0.361735  [   52/   88]
per-ex loss: 0.359652  [   53/   88]
per-ex loss: 0.386580  [   54/   88]
per-ex loss: 0.553748  [   55/   88]
per-ex loss: 0.541746  [   56/   88]
per-ex loss: 0.306303  [   57/   88]
per-ex loss: 0.623197  [   58/   88]
per-ex loss: 0.352713  [   59/   88]
per-ex loss: 0.336692  [   60/   88]
per-ex loss: 0.355517  [   61/   88]
per-ex loss: 0.456833  [   62/   88]
per-ex loss: 0.310095  [   63/   88]
per-ex loss: 0.395039  [   64/   88]
per-ex loss: 0.536410  [   65/   88]
per-ex loss: 0.668729  [   66/   88]
per-ex loss: 0.475959  [   67/   88]
per-ex loss: 0.548912  [   68/   88]
per-ex loss: 0.533765  [   69/   88]
per-ex loss: 0.330077  [   70/   88]
per-ex loss: 0.308358  [   71/   88]
per-ex loss: 0.325328  [   72/   88]
per-ex loss: 0.309747  [   73/   88]
per-ex loss: 0.598751  [   74/   88]
per-ex loss: 0.469447  [   75/   88]
per-ex loss: 0.543705  [   76/   88]
per-ex loss: 0.338667  [   77/   88]
per-ex loss: 0.567754  [   78/   88]
per-ex loss: 0.328593  [   79/   88]
per-ex loss: 0.325944  [   80/   88]
per-ex loss: 0.576743  [   81/   88]
per-ex loss: 0.405415  [   82/   88]
per-ex loss: 0.330150  [   83/   88]
per-ex loss: 0.347681  [   84/   88]
per-ex loss: 0.524990  [   85/   88]
per-ex loss: 0.356522  [   86/   88]
per-ex loss: 0.213873  [   87/   88]
per-ex loss: 0.606997  [   88/   88]
Train Error: Avg loss: 0.42885649
validation Error: 
 Avg loss: 0.51837562 
 F1: 0.511446 
 Precision: 0.569302 
 Recall: 0.464264
 IoU: 0.343585

test Error: 
 Avg loss: 0.47358019 
 F1: 0.571486 
 Precision: 0.600878 
 Recall: 0.544836
 IoU: 0.400057

We have finished training iteration 129
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_127_.pth
per-ex loss: 0.354918  [    1/   88]
per-ex loss: 0.415826  [    2/   88]
per-ex loss: 0.347738  [    3/   88]
per-ex loss: 0.282890  [    4/   88]
per-ex loss: 0.218359  [    5/   88]
per-ex loss: 0.620574  [    6/   88]
per-ex loss: 0.328267  [    7/   88]
per-ex loss: 0.385919  [    8/   88]
per-ex loss: 0.435244  [    9/   88]
per-ex loss: 0.441667  [   10/   88]
per-ex loss: 0.350368  [   11/   88]
per-ex loss: 0.581325  [   12/   88]
per-ex loss: 0.349924  [   13/   88]
per-ex loss: 0.325422  [   14/   88]
per-ex loss: 0.380907  [   15/   88]
per-ex loss: 0.319366  [   16/   88]
per-ex loss: 0.320421  [   17/   88]
per-ex loss: 0.343979  [   18/   88]
per-ex loss: 0.549519  [   19/   88]
per-ex loss: 0.373256  [   20/   88]
per-ex loss: 0.377641  [   21/   88]
per-ex loss: 0.411667  [   22/   88]
per-ex loss: 0.341861  [   23/   88]
per-ex loss: 0.289451  [   24/   88]
per-ex loss: 0.316104  [   25/   88]
per-ex loss: 0.288839  [   26/   88]
per-ex loss: 0.584356  [   27/   88]
per-ex loss: 0.362799  [   28/   88]
per-ex loss: 0.505007  [   29/   88]
per-ex loss: 0.467127  [   30/   88]
per-ex loss: 0.534701  [   31/   88]
per-ex loss: 0.367762  [   32/   88]
per-ex loss: 0.620157  [   33/   88]
per-ex loss: 0.419862  [   34/   88]
per-ex loss: 0.282248  [   35/   88]
per-ex loss: 0.319912  [   36/   88]
per-ex loss: 0.558526  [   37/   88]
per-ex loss: 0.386033  [   38/   88]
per-ex loss: 0.358831  [   39/   88]
per-ex loss: 0.451216  [   40/   88]
per-ex loss: 0.406878  [   41/   88]
per-ex loss: 0.503369  [   42/   88]
per-ex loss: 0.596922  [   43/   88]
per-ex loss: 0.564305  [   44/   88]
per-ex loss: 0.356872  [   45/   88]
per-ex loss: 0.431872  [   46/   88]
per-ex loss: 0.537065  [   47/   88]
per-ex loss: 0.468515  [   48/   88]
per-ex loss: 0.481776  [   49/   88]
per-ex loss: 0.436377  [   50/   88]
per-ex loss: 0.550217  [   51/   88]
per-ex loss: 0.353457  [   52/   88]
per-ex loss: 0.635780  [   53/   88]
per-ex loss: 0.464305  [   54/   88]
per-ex loss: 0.333172  [   55/   88]
per-ex loss: 0.327906  [   56/   88]
per-ex loss: 0.310918  [   57/   88]
per-ex loss: 0.405413  [   58/   88]
per-ex loss: 0.425847  [   59/   88]
per-ex loss: 0.322140  [   60/   88]
per-ex loss: 0.481945  [   61/   88]
per-ex loss: 0.512816  [   62/   88]
per-ex loss: 0.393610  [   63/   88]
per-ex loss: 0.325301  [   64/   88]
per-ex loss: 0.374332  [   65/   88]
per-ex loss: 0.385612  [   66/   88]
per-ex loss: 0.366713  [   67/   88]
per-ex loss: 0.577853  [   68/   88]
per-ex loss: 0.534900  [   69/   88]
per-ex loss: 0.315726  [   70/   88]
per-ex loss: 0.513756  [   71/   88]
per-ex loss: 0.351506  [   72/   88]
per-ex loss: 0.342662  [   73/   88]
per-ex loss: 0.391145  [   74/   88]
per-ex loss: 0.505494  [   75/   88]
per-ex loss: 0.596002  [   76/   88]
per-ex loss: 0.597214  [   77/   88]
per-ex loss: 0.535408  [   78/   88]
per-ex loss: 0.574898  [   79/   88]
per-ex loss: 0.304109  [   80/   88]
per-ex loss: 0.328701  [   81/   88]
per-ex loss: 0.330310  [   82/   88]
per-ex loss: 0.452157  [   83/   88]
per-ex loss: 0.528033  [   84/   88]
per-ex loss: 0.525302  [   85/   88]
per-ex loss: 0.363111  [   86/   88]
per-ex loss: 0.553244  [   87/   88]
per-ex loss: 0.439910  [   88/   88]
Train Error: Avg loss: 0.42482799
validation Error: 
 Avg loss: 0.51776210 
 F1: 0.511726 
 Precision: 0.567595 
 Recall: 0.465870
 IoU: 0.343838

test Error: 
 Avg loss: 0.48353358 
 F1: 0.559519 
 Precision: 0.591451 
 Recall: 0.530858
 IoU: 0.388425

We have finished training iteration 130
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_109_.pth
per-ex loss: 0.429770  [    1/   88]
per-ex loss: 0.398136  [    2/   88]
per-ex loss: 0.403072  [    3/   88]
per-ex loss: 0.520730  [    4/   88]
per-ex loss: 0.429559  [    5/   88]
per-ex loss: 0.323728  [    6/   88]
per-ex loss: 0.588773  [    7/   88]
per-ex loss: 0.336151  [    8/   88]
per-ex loss: 0.405802  [    9/   88]
per-ex loss: 0.548308  [   10/   88]
per-ex loss: 0.339848  [   11/   88]
per-ex loss: 0.289377  [   12/   88]
per-ex loss: 0.309153  [   13/   88]
per-ex loss: 0.309377  [   14/   88]
per-ex loss: 0.552944  [   15/   88]
per-ex loss: 0.428089  [   16/   88]
per-ex loss: 0.286258  [   17/   88]
per-ex loss: 0.376481  [   18/   88]
per-ex loss: 0.587068  [   19/   88]
per-ex loss: 0.233051  [   20/   88]
per-ex loss: 0.530944  [   21/   88]
per-ex loss: 0.516362  [   22/   88]
per-ex loss: 0.616978  [   23/   88]
per-ex loss: 0.443267  [   24/   88]
per-ex loss: 0.368344  [   25/   88]
per-ex loss: 0.339132  [   26/   88]
per-ex loss: 0.376164  [   27/   88]
per-ex loss: 0.360710  [   28/   88]
per-ex loss: 0.538016  [   29/   88]
per-ex loss: 0.569897  [   30/   88]
per-ex loss: 0.373392  [   31/   88]
per-ex loss: 0.612118  [   32/   88]
per-ex loss: 0.618571  [   33/   88]
per-ex loss: 0.460233  [   34/   88]
per-ex loss: 0.354092  [   35/   88]
per-ex loss: 0.387367  [   36/   88]
per-ex loss: 0.325457  [   37/   88]
per-ex loss: 0.380573  [   38/   88]
per-ex loss: 0.315690  [   39/   88]
per-ex loss: 0.503884  [   40/   88]
per-ex loss: 0.466793  [   41/   88]
per-ex loss: 0.354782  [   42/   88]
per-ex loss: 0.338550  [   43/   88]
per-ex loss: 0.311921  [   44/   88]
per-ex loss: 0.512080  [   45/   88]
per-ex loss: 0.446830  [   46/   88]
per-ex loss: 0.427540  [   47/   88]
per-ex loss: 0.565146  [   48/   88]
per-ex loss: 0.493351  [   49/   88]
per-ex loss: 0.356669  [   50/   88]
per-ex loss: 0.525420  [   51/   88]
per-ex loss: 0.409490  [   52/   88]
per-ex loss: 0.354515  [   53/   88]
per-ex loss: 0.659467  [   54/   88]
per-ex loss: 0.513388  [   55/   88]
per-ex loss: 0.426590  [   56/   88]
per-ex loss: 0.610913  [   57/   88]
per-ex loss: 0.313231  [   58/   88]
per-ex loss: 0.536295  [   59/   88]
per-ex loss: 0.335897  [   60/   88]
per-ex loss: 0.327829  [   61/   88]
per-ex loss: 0.388481  [   62/   88]
per-ex loss: 0.333043  [   63/   88]
per-ex loss: 0.527724  [   64/   88]
per-ex loss: 0.382612  [   65/   88]
per-ex loss: 0.527615  [   66/   88]
per-ex loss: 0.401388  [   67/   88]
per-ex loss: 0.367441  [   68/   88]
per-ex loss: 0.365838  [   69/   88]
per-ex loss: 0.303125  [   70/   88]
per-ex loss: 0.318241  [   71/   88]
per-ex loss: 0.427003  [   72/   88]
per-ex loss: 0.563067  [   73/   88]
per-ex loss: 0.391577  [   74/   88]
per-ex loss: 0.358941  [   75/   88]
per-ex loss: 0.317248  [   76/   88]
per-ex loss: 0.360308  [   77/   88]
per-ex loss: 0.543800  [   78/   88]
per-ex loss: 0.385751  [   79/   88]
per-ex loss: 0.277384  [   80/   88]
per-ex loss: 0.381130  [   81/   88]
per-ex loss: 0.504221  [   82/   88]
per-ex loss: 0.437151  [   83/   88]
per-ex loss: 0.545516  [   84/   88]
per-ex loss: 0.410649  [   85/   88]
per-ex loss: 0.495611  [   86/   88]
per-ex loss: 0.305557  [   87/   88]
per-ex loss: 0.299467  [   88/   88]
Train Error: Avg loss: 0.42378926
validation Error: 
 Avg loss: 0.51493976 
 F1: 0.512385 
 Precision: 0.587067 
 Recall: 0.454560
 IoU: 0.344434

test Error: 
 Avg loss: 0.48005964 
 F1: 0.567416 
 Precision: 0.613298 
 Recall: 0.527922
 IoU: 0.396079

We have finished training iteration 131
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_129_.pth
per-ex loss: 0.333675  [    1/   88]
per-ex loss: 0.378469  [    2/   88]
per-ex loss: 0.433667  [    3/   88]
per-ex loss: 0.361762  [    4/   88]
per-ex loss: 0.558064  [    5/   88]
per-ex loss: 0.329778  [    6/   88]
per-ex loss: 0.466367  [    7/   88]
per-ex loss: 0.502970  [    8/   88]
per-ex loss: 0.400099  [    9/   88]
per-ex loss: 0.376784  [   10/   88]
per-ex loss: 0.360432  [   11/   88]
per-ex loss: 0.328423  [   12/   88]
per-ex loss: 0.405548  [   13/   88]
per-ex loss: 0.328957  [   14/   88]
per-ex loss: 0.513080  [   15/   88]
per-ex loss: 0.607191  [   16/   88]
per-ex loss: 0.605270  [   17/   88]
per-ex loss: 0.428444  [   18/   88]
per-ex loss: 0.551227  [   19/   88]
per-ex loss: 0.346230  [   20/   88]
per-ex loss: 0.401795  [   21/   88]
per-ex loss: 0.437208  [   22/   88]
per-ex loss: 0.458611  [   23/   88]
per-ex loss: 0.523623  [   24/   88]
per-ex loss: 0.514051  [   25/   88]
per-ex loss: 0.287766  [   26/   88]
per-ex loss: 0.571884  [   27/   88]
per-ex loss: 0.364282  [   28/   88]
per-ex loss: 0.612422  [   29/   88]
per-ex loss: 0.492987  [   30/   88]
per-ex loss: 0.603038  [   31/   88]
per-ex loss: 0.605983  [   32/   88]
per-ex loss: 0.386292  [   33/   88]
per-ex loss: 0.383380  [   34/   88]
per-ex loss: 0.458651  [   35/   88]
per-ex loss: 0.404414  [   36/   88]
per-ex loss: 0.490559  [   37/   88]
per-ex loss: 0.620554  [   38/   88]
per-ex loss: 0.472480  [   39/   88]
per-ex loss: 0.386301  [   40/   88]
per-ex loss: 0.361817  [   41/   88]
per-ex loss: 0.317800  [   42/   88]
per-ex loss: 0.450616  [   43/   88]
per-ex loss: 0.361966  [   44/   88]
per-ex loss: 0.232851  [   45/   88]
per-ex loss: 0.598139  [   46/   88]
per-ex loss: 0.536681  [   47/   88]
per-ex loss: 0.364509  [   48/   88]
per-ex loss: 0.419671  [   49/   88]
per-ex loss: 0.373254  [   50/   88]
per-ex loss: 0.328573  [   51/   88]
per-ex loss: 0.348642  [   52/   88]
per-ex loss: 0.269961  [   53/   88]
per-ex loss: 0.558849  [   54/   88]
per-ex loss: 0.276170  [   55/   88]
per-ex loss: 0.455129  [   56/   88]
per-ex loss: 0.338216  [   57/   88]
per-ex loss: 0.292818  [   58/   88]
per-ex loss: 0.348029  [   59/   88]
per-ex loss: 0.460195  [   60/   88]
per-ex loss: 0.527591  [   61/   88]
per-ex loss: 0.346960  [   62/   88]
per-ex loss: 0.323518  [   63/   88]
per-ex loss: 0.559659  [   64/   88]
per-ex loss: 0.529259  [   65/   88]
per-ex loss: 0.393102  [   66/   88]
per-ex loss: 0.407759  [   67/   88]
per-ex loss: 0.373520  [   68/   88]
per-ex loss: 0.295805  [   69/   88]
per-ex loss: 0.524301  [   70/   88]
per-ex loss: 0.323308  [   71/   88]
per-ex loss: 0.386345  [   72/   88]
per-ex loss: 0.527502  [   73/   88]
per-ex loss: 0.549702  [   74/   88]
per-ex loss: 0.404253  [   75/   88]
per-ex loss: 0.524699  [   76/   88]
per-ex loss: 0.547581  [   77/   88]
per-ex loss: 0.504650  [   78/   88]
per-ex loss: 0.310892  [   79/   88]
per-ex loss: 0.316317  [   80/   88]
per-ex loss: 0.370249  [   81/   88]
per-ex loss: 0.331847  [   82/   88]
per-ex loss: 0.363629  [   83/   88]
per-ex loss: 0.437327  [   84/   88]
per-ex loss: 0.317856  [   85/   88]
per-ex loss: 0.335148  [   86/   88]
per-ex loss: 0.319914  [   87/   88]
per-ex loss: 0.520201  [   88/   88]
Train Error: Avg loss: 0.42567612
validation Error: 
 Avg loss: 0.53534429 
 F1: 0.489839 
 Precision: 0.547721 
 Recall: 0.443021
 IoU: 0.324362

test Error: 
 Avg loss: 0.49100940 
 F1: 0.550474 
 Precision: 0.590080 
 Recall: 0.515851
 IoU: 0.379761

We have finished training iteration 132
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_130_.pth
per-ex loss: 0.535538  [    1/   88]
per-ex loss: 0.524874  [    2/   88]
per-ex loss: 0.344822  [    3/   88]
per-ex loss: 0.561803  [    4/   88]
per-ex loss: 0.391432  [    5/   88]
per-ex loss: 0.310005  [    6/   88]
per-ex loss: 0.539032  [    7/   88]
per-ex loss: 0.308326  [    8/   88]
per-ex loss: 0.331754  [    9/   88]
per-ex loss: 0.316414  [   10/   88]
per-ex loss: 0.446570  [   11/   88]
per-ex loss: 0.456116  [   12/   88]
per-ex loss: 0.282646  [   13/   88]
per-ex loss: 0.606588  [   14/   88]
per-ex loss: 0.302499  [   15/   88]
per-ex loss: 0.611753  [   16/   88]
per-ex loss: 0.272714  [   17/   88]
per-ex loss: 0.468900  [   18/   88]
per-ex loss: 0.541053  [   19/   88]
per-ex loss: 0.361454  [   20/   88]
per-ex loss: 0.408160  [   21/   88]
per-ex loss: 0.306358  [   22/   88]
per-ex loss: 0.519389  [   23/   88]
per-ex loss: 0.476557  [   24/   88]
per-ex loss: 0.317084  [   25/   88]
per-ex loss: 0.326949  [   26/   88]
per-ex loss: 0.518437  [   27/   88]
per-ex loss: 0.382427  [   28/   88]
per-ex loss: 0.506612  [   29/   88]
per-ex loss: 0.289342  [   30/   88]
per-ex loss: 0.364527  [   31/   88]
per-ex loss: 0.384604  [   32/   88]
per-ex loss: 0.336837  [   33/   88]
per-ex loss: 0.275910  [   34/   88]
per-ex loss: 0.518791  [   35/   88]
per-ex loss: 0.548258  [   36/   88]
per-ex loss: 0.350397  [   37/   88]
per-ex loss: 0.519678  [   38/   88]
per-ex loss: 0.555808  [   39/   88]
per-ex loss: 0.448918  [   40/   88]
per-ex loss: 0.414593  [   41/   88]
per-ex loss: 0.388392  [   42/   88]
per-ex loss: 0.422392  [   43/   88]
per-ex loss: 0.523267  [   44/   88]
per-ex loss: 0.416942  [   45/   88]
per-ex loss: 0.494410  [   46/   88]
per-ex loss: 0.400500  [   47/   88]
per-ex loss: 0.370965  [   48/   88]
per-ex loss: 0.229554  [   49/   88]
per-ex loss: 0.387050  [   50/   88]
per-ex loss: 0.328544  [   51/   88]
per-ex loss: 0.455000  [   52/   88]
per-ex loss: 0.494759  [   53/   88]
per-ex loss: 0.341861  [   54/   88]
per-ex loss: 0.391541  [   55/   88]
per-ex loss: 0.366905  [   56/   88]
per-ex loss: 0.603357  [   57/   88]
per-ex loss: 0.446490  [   58/   88]
per-ex loss: 0.468186  [   59/   88]
per-ex loss: 0.368859  [   60/   88]
per-ex loss: 0.356031  [   61/   88]
per-ex loss: 0.612791  [   62/   88]
per-ex loss: 0.312253  [   63/   88]
per-ex loss: 0.409632  [   64/   88]
per-ex loss: 0.518778  [   65/   88]
per-ex loss: 0.389432  [   66/   88]
per-ex loss: 0.401403  [   67/   88]
per-ex loss: 0.367489  [   68/   88]
per-ex loss: 0.282722  [   69/   88]
per-ex loss: 0.618375  [   70/   88]
per-ex loss: 0.422503  [   71/   88]
per-ex loss: 0.594256  [   72/   88]
per-ex loss: 0.369650  [   73/   88]
per-ex loss: 0.550303  [   74/   88]
per-ex loss: 0.538489  [   75/   88]
per-ex loss: 0.508124  [   76/   88]
per-ex loss: 0.353057  [   77/   88]
per-ex loss: 0.306352  [   78/   88]
per-ex loss: 0.320050  [   79/   88]
per-ex loss: 0.308027  [   80/   88]
per-ex loss: 0.376977  [   81/   88]
per-ex loss: 0.372327  [   82/   88]
per-ex loss: 0.299020  [   83/   88]
per-ex loss: 0.444565  [   84/   88]
per-ex loss: 0.351610  [   85/   88]
per-ex loss: 0.438377  [   86/   88]
per-ex loss: 0.409892  [   87/   88]
per-ex loss: 0.563311  [   88/   88]
Train Error: Avg loss: 0.42021249
validation Error: 
 Avg loss: 0.52300953 
 F1: 0.502688 
 Precision: 0.592878 
 Recall: 0.436315
 IoU: 0.335727

test Error: 
 Avg loss: 0.48899828 
 F1: 0.556823 
 Precision: 0.610472 
 Recall: 0.511841
 IoU: 0.385831

We have finished training iteration 133
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_123_.pth
per-ex loss: 0.314948  [    1/   88]
per-ex loss: 0.541328  [    2/   88]
per-ex loss: 0.574748  [    3/   88]
per-ex loss: 0.350000  [    4/   88]
per-ex loss: 0.430337  [    5/   88]
per-ex loss: 0.537475  [    6/   88]
per-ex loss: 0.306308  [    7/   88]
per-ex loss: 0.342083  [    8/   88]
per-ex loss: 0.400191  [    9/   88]
per-ex loss: 0.452200  [   10/   88]
per-ex loss: 0.600802  [   11/   88]
per-ex loss: 0.320234  [   12/   88]
per-ex loss: 0.395955  [   13/   88]
per-ex loss: 0.538375  [   14/   88]
per-ex loss: 0.557317  [   15/   88]
per-ex loss: 0.306095  [   16/   88]
per-ex loss: 0.556297  [   17/   88]
per-ex loss: 0.309368  [   18/   88]
per-ex loss: 0.346799  [   19/   88]
per-ex loss: 0.372069  [   20/   88]
per-ex loss: 0.419641  [   21/   88]
per-ex loss: 0.343313  [   22/   88]
per-ex loss: 0.258155  [   23/   88]
per-ex loss: 0.519807  [   24/   88]
per-ex loss: 0.393209  [   25/   88]
per-ex loss: 0.440859  [   26/   88]
per-ex loss: 0.618619  [   27/   88]
per-ex loss: 0.563552  [   28/   88]
per-ex loss: 0.363323  [   29/   88]
per-ex loss: 0.386591  [   30/   88]
per-ex loss: 0.340068  [   31/   88]
per-ex loss: 0.362229  [   32/   88]
per-ex loss: 0.358224  [   33/   88]
per-ex loss: 0.356757  [   34/   88]
per-ex loss: 0.459082  [   35/   88]
per-ex loss: 0.459847  [   36/   88]
per-ex loss: 0.277350  [   37/   88]
per-ex loss: 0.309217  [   38/   88]
per-ex loss: 0.303794  [   39/   88]
per-ex loss: 0.354936  [   40/   88]
per-ex loss: 0.404346  [   41/   88]
per-ex loss: 0.369429  [   42/   88]
per-ex loss: 0.332464  [   43/   88]
per-ex loss: 0.551972  [   44/   88]
per-ex loss: 0.275693  [   45/   88]
per-ex loss: 0.514369  [   46/   88]
per-ex loss: 0.620690  [   47/   88]
per-ex loss: 0.381516  [   48/   88]
per-ex loss: 0.488820  [   49/   88]
per-ex loss: 0.531914  [   50/   88]
per-ex loss: 0.509226  [   51/   88]
per-ex loss: 0.520731  [   52/   88]
per-ex loss: 0.393120  [   53/   88]
per-ex loss: 0.603937  [   54/   88]
per-ex loss: 0.512876  [   55/   88]
per-ex loss: 0.278622  [   56/   88]
per-ex loss: 0.399081  [   57/   88]
per-ex loss: 0.495746  [   58/   88]
per-ex loss: 0.361944  [   59/   88]
per-ex loss: 0.347929  [   60/   88]
per-ex loss: 0.371724  [   61/   88]
per-ex loss: 0.512820  [   62/   88]
per-ex loss: 0.526628  [   63/   88]
per-ex loss: 0.516695  [   64/   88]
per-ex loss: 0.343173  [   65/   88]
per-ex loss: 0.425073  [   66/   88]
per-ex loss: 0.373970  [   67/   88]
per-ex loss: 0.373747  [   68/   88]
per-ex loss: 0.407334  [   69/   88]
per-ex loss: 0.341690  [   70/   88]
per-ex loss: 0.516630  [   71/   88]
per-ex loss: 0.399254  [   72/   88]
per-ex loss: 0.300130  [   73/   88]
per-ex loss: 0.469828  [   74/   88]
per-ex loss: 0.527462  [   75/   88]
per-ex loss: 0.374564  [   76/   88]
per-ex loss: 0.278722  [   77/   88]
per-ex loss: 0.331993  [   78/   88]
per-ex loss: 0.322834  [   79/   88]
per-ex loss: 0.490757  [   80/   88]
per-ex loss: 0.285649  [   81/   88]
per-ex loss: 0.467144  [   82/   88]
per-ex loss: 0.496177  [   83/   88]
per-ex loss: 0.582162  [   84/   88]
per-ex loss: 0.282123  [   85/   88]
per-ex loss: 0.601256  [   86/   88]
per-ex loss: 0.662900  [   87/   88]
per-ex loss: 0.355447  [   88/   88]
Train Error: Avg loss: 0.42356607
validation Error: 
 Avg loss: 0.51787829 
 F1: 0.512671 
 Precision: 0.592365 
 Recall: 0.451878
 IoU: 0.344693

test Error: 
 Avg loss: 0.47893411 
 F1: 0.566116 
 Precision: 0.614740 
 Recall: 0.524619
 IoU: 0.394813

We have finished training iteration 134
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_132_.pth
per-ex loss: 0.333297  [    1/   88]
per-ex loss: 0.460154  [    2/   88]
per-ex loss: 0.568870  [    3/   88]
per-ex loss: 0.522964  [    4/   88]
per-ex loss: 0.347617  [    5/   88]
per-ex loss: 0.599233  [    6/   88]
per-ex loss: 0.520410  [    7/   88]
per-ex loss: 0.393126  [    8/   88]
per-ex loss: 0.339918  [    9/   88]
per-ex loss: 0.315804  [   10/   88]
per-ex loss: 0.385929  [   11/   88]
per-ex loss: 0.297298  [   12/   88]
per-ex loss: 0.443190  [   13/   88]
per-ex loss: 0.326329  [   14/   88]
per-ex loss: 0.339014  [   15/   88]
per-ex loss: 0.356825  [   16/   88]
per-ex loss: 0.458577  [   17/   88]
per-ex loss: 0.545065  [   18/   88]
per-ex loss: 0.383758  [   19/   88]
per-ex loss: 0.327763  [   20/   88]
per-ex loss: 0.440165  [   21/   88]
per-ex loss: 0.392140  [   22/   88]
per-ex loss: 0.627447  [   23/   88]
per-ex loss: 0.511592  [   24/   88]
per-ex loss: 0.308136  [   25/   88]
per-ex loss: 0.370284  [   26/   88]
per-ex loss: 0.331158  [   27/   88]
per-ex loss: 0.494379  [   28/   88]
per-ex loss: 0.502206  [   29/   88]
per-ex loss: 0.415208  [   30/   88]
per-ex loss: 0.558721  [   31/   88]
per-ex loss: 0.364341  [   32/   88]
per-ex loss: 0.613073  [   33/   88]
per-ex loss: 0.404364  [   34/   88]
per-ex loss: 0.442505  [   35/   88]
per-ex loss: 0.610749  [   36/   88]
per-ex loss: 0.394005  [   37/   88]
per-ex loss: 0.349485  [   38/   88]
per-ex loss: 0.351409  [   39/   88]
per-ex loss: 0.537606  [   40/   88]
per-ex loss: 0.374388  [   41/   88]
per-ex loss: 0.341694  [   42/   88]
per-ex loss: 0.374747  [   43/   88]
per-ex loss: 0.522290  [   44/   88]
per-ex loss: 0.305622  [   45/   88]
per-ex loss: 0.311089  [   46/   88]
per-ex loss: 0.338517  [   47/   88]
per-ex loss: 0.271055  [   48/   88]
per-ex loss: 0.283923  [   49/   88]
per-ex loss: 0.492422  [   50/   88]
per-ex loss: 0.501535  [   51/   88]
per-ex loss: 0.555015  [   52/   88]
per-ex loss: 0.397505  [   53/   88]
per-ex loss: 0.557577  [   54/   88]
per-ex loss: 0.442237  [   55/   88]
per-ex loss: 0.299208  [   56/   88]
per-ex loss: 0.377061  [   57/   88]
per-ex loss: 0.363619  [   58/   88]
per-ex loss: 0.475119  [   59/   88]
per-ex loss: 0.543655  [   60/   88]
per-ex loss: 0.391288  [   61/   88]
per-ex loss: 0.511195  [   62/   88]
per-ex loss: 0.512917  [   63/   88]
per-ex loss: 0.411757  [   64/   88]
per-ex loss: 0.215994  [   65/   88]
per-ex loss: 0.596482  [   66/   88]
per-ex loss: 0.299755  [   67/   88]
per-ex loss: 0.401989  [   68/   88]
per-ex loss: 0.393539  [   69/   88]
per-ex loss: 0.316246  [   70/   88]
per-ex loss: 0.417238  [   71/   88]
per-ex loss: 0.356483  [   72/   88]
per-ex loss: 0.573155  [   73/   88]
per-ex loss: 0.371634  [   74/   88]
per-ex loss: 0.310819  [   75/   88]
per-ex loss: 0.318662  [   76/   88]
per-ex loss: 0.317384  [   77/   88]
per-ex loss: 0.297584  [   78/   88]
per-ex loss: 0.548357  [   79/   88]
per-ex loss: 0.274850  [   80/   88]
per-ex loss: 0.478094  [   81/   88]
per-ex loss: 0.508760  [   82/   88]
per-ex loss: 0.511589  [   83/   88]
per-ex loss: 0.500261  [   84/   88]
per-ex loss: 0.430808  [   85/   88]
per-ex loss: 0.342423  [   86/   88]
per-ex loss: 0.361362  [   87/   88]
per-ex loss: 0.627745  [   88/   88]
Train Error: Avg loss: 0.42057677
validation Error: 
 Avg loss: 0.52084069 
 F1: 0.510593 
 Precision: 0.608369 
 Recall: 0.439894
 IoU: 0.342816

test Error: 
 Avg loss: 0.47895468 
 F1: 0.567957 
 Precision: 0.637778 
 Recall: 0.511915
 IoU: 0.396606

We have finished training iteration 135
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_133_.pth
per-ex loss: 0.360256  [    1/   88]
per-ex loss: 0.515490  [    2/   88]
per-ex loss: 0.347657  [    3/   88]
per-ex loss: 0.444554  [    4/   88]
per-ex loss: 0.448103  [    5/   88]
per-ex loss: 0.464171  [    6/   88]
per-ex loss: 0.392554  [    7/   88]
per-ex loss: 0.309779  [    8/   88]
per-ex loss: 0.423391  [    9/   88]
per-ex loss: 0.589777  [   10/   88]
per-ex loss: 0.469774  [   11/   88]
per-ex loss: 0.453524  [   12/   88]
per-ex loss: 0.605641  [   13/   88]
per-ex loss: 0.532557  [   14/   88]
per-ex loss: 0.515977  [   15/   88]
per-ex loss: 0.514734  [   16/   88]
per-ex loss: 0.315777  [   17/   88]
per-ex loss: 0.341303  [   18/   88]
per-ex loss: 0.308964  [   19/   88]
per-ex loss: 0.613248  [   20/   88]
per-ex loss: 0.380999  [   21/   88]
per-ex loss: 0.352069  [   22/   88]
per-ex loss: 0.527377  [   23/   88]
per-ex loss: 0.516232  [   24/   88]
per-ex loss: 0.314116  [   25/   88]
per-ex loss: 0.210724  [   26/   88]
per-ex loss: 0.390643  [   27/   88]
per-ex loss: 0.544803  [   28/   88]
per-ex loss: 0.420598  [   29/   88]
per-ex loss: 0.608918  [   30/   88]
per-ex loss: 0.531338  [   31/   88]
per-ex loss: 0.337328  [   32/   88]
per-ex loss: 0.457988  [   33/   88]
per-ex loss: 0.474219  [   34/   88]
per-ex loss: 0.392997  [   35/   88]
per-ex loss: 0.480383  [   36/   88]
per-ex loss: 0.348117  [   37/   88]
per-ex loss: 0.365848  [   38/   88]
per-ex loss: 0.421837  [   39/   88]
per-ex loss: 0.407992  [   40/   88]
per-ex loss: 0.478424  [   41/   88]
per-ex loss: 0.334573  [   42/   88]
per-ex loss: 0.362129  [   43/   88]
per-ex loss: 0.290054  [   44/   88]
per-ex loss: 0.314034  [   45/   88]
per-ex loss: 0.555628  [   46/   88]
per-ex loss: 0.356210  [   47/   88]
per-ex loss: 0.624312  [   48/   88]
per-ex loss: 0.351523  [   49/   88]
per-ex loss: 0.552584  [   50/   88]
per-ex loss: 0.447228  [   51/   88]
per-ex loss: 0.272069  [   52/   88]
per-ex loss: 0.414429  [   53/   88]
per-ex loss: 0.287260  [   54/   88]
per-ex loss: 0.394340  [   55/   88]
per-ex loss: 0.392063  [   56/   88]
per-ex loss: 0.325109  [   57/   88]
per-ex loss: 0.573959  [   58/   88]
per-ex loss: 0.358193  [   59/   88]
per-ex loss: 0.387252  [   60/   88]
per-ex loss: 0.345420  [   61/   88]
per-ex loss: 0.413019  [   62/   88]
per-ex loss: 0.303607  [   63/   88]
per-ex loss: 0.323064  [   64/   88]
per-ex loss: 0.272059  [   65/   88]
per-ex loss: 0.559787  [   66/   88]
per-ex loss: 0.614254  [   67/   88]
per-ex loss: 0.291497  [   68/   88]
per-ex loss: 0.374246  [   69/   88]
per-ex loss: 0.405719  [   70/   88]
per-ex loss: 0.578803  [   71/   88]
per-ex loss: 0.536408  [   72/   88]
per-ex loss: 0.335579  [   73/   88]
per-ex loss: 0.323457  [   74/   88]
per-ex loss: 0.436729  [   75/   88]
per-ex loss: 0.319155  [   76/   88]
per-ex loss: 0.514360  [   77/   88]
per-ex loss: 0.548479  [   78/   88]
per-ex loss: 0.531829  [   79/   88]
per-ex loss: 0.362993  [   80/   88]
per-ex loss: 0.342126  [   81/   88]
per-ex loss: 0.468415  [   82/   88]
per-ex loss: 0.416561  [   83/   88]
per-ex loss: 0.424148  [   84/   88]
per-ex loss: 0.367384  [   85/   88]
per-ex loss: 0.513753  [   86/   88]
per-ex loss: 0.374216  [   87/   88]
per-ex loss: 0.344544  [   88/   88]
Train Error: Avg loss: 0.42232661
validation Error: 
 Avg loss: 0.52326638 
 F1: 0.510944 
 Precision: 0.549240 
 Recall: 0.477639
 IoU: 0.343132

test Error: 
 Avg loss: 0.47278526 
 F1: 0.569836 
 Precision: 0.590387 
 Recall: 0.550667
 IoU: 0.398441

We have finished training iteration 136
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_134_.pth
per-ex loss: 0.444445  [    1/   88]
per-ex loss: 0.414928  [    2/   88]
per-ex loss: 0.314869  [    3/   88]
per-ex loss: 0.517995  [    4/   88]
per-ex loss: 0.539638  [    5/   88]
per-ex loss: 0.517406  [    6/   88]
per-ex loss: 0.320548  [    7/   88]
per-ex loss: 0.386587  [    8/   88]
per-ex loss: 0.389582  [    9/   88]
per-ex loss: 0.562509  [   10/   88]
per-ex loss: 0.594391  [   11/   88]
per-ex loss: 0.608484  [   12/   88]
per-ex loss: 0.327344  [   13/   88]
per-ex loss: 0.317114  [   14/   88]
per-ex loss: 0.310998  [   15/   88]
per-ex loss: 0.326641  [   16/   88]
per-ex loss: 0.449920  [   17/   88]
per-ex loss: 0.390719  [   18/   88]
per-ex loss: 0.321104  [   19/   88]
per-ex loss: 0.367212  [   20/   88]
per-ex loss: 0.539032  [   21/   88]
per-ex loss: 0.651985  [   22/   88]
per-ex loss: 0.351540  [   23/   88]
per-ex loss: 0.347811  [   24/   88]
per-ex loss: 0.359004  [   25/   88]
per-ex loss: 0.462024  [   26/   88]
per-ex loss: 0.284213  [   27/   88]
per-ex loss: 0.565766  [   28/   88]
per-ex loss: 0.445140  [   29/   88]
per-ex loss: 0.347315  [   30/   88]
per-ex loss: 0.614604  [   31/   88]
per-ex loss: 0.539383  [   32/   88]
per-ex loss: 0.530384  [   33/   88]
per-ex loss: 0.285276  [   34/   88]
per-ex loss: 0.355433  [   35/   88]
per-ex loss: 0.539989  [   36/   88]
per-ex loss: 0.432047  [   37/   88]
per-ex loss: 0.543878  [   38/   88]
per-ex loss: 0.388833  [   39/   88]
per-ex loss: 0.594235  [   40/   88]
per-ex loss: 0.264622  [   41/   88]
per-ex loss: 0.305893  [   42/   88]
per-ex loss: 0.416821  [   43/   88]
per-ex loss: 0.443350  [   44/   88]
per-ex loss: 0.475889  [   45/   88]
per-ex loss: 0.494947  [   46/   88]
per-ex loss: 0.306049  [   47/   88]
per-ex loss: 0.339765  [   48/   88]
per-ex loss: 0.527026  [   49/   88]
per-ex loss: 0.486624  [   50/   88]
per-ex loss: 0.509347  [   51/   88]
per-ex loss: 0.291345  [   52/   88]
per-ex loss: 0.408043  [   53/   88]
per-ex loss: 0.530774  [   54/   88]
per-ex loss: 0.520629  [   55/   88]
per-ex loss: 0.320878  [   56/   88]
per-ex loss: 0.570349  [   57/   88]
per-ex loss: 0.249790  [   58/   88]
per-ex loss: 0.350129  [   59/   88]
per-ex loss: 0.353161  [   60/   88]
per-ex loss: 0.402461  [   61/   88]
per-ex loss: 0.326639  [   62/   88]
per-ex loss: 0.364672  [   63/   88]
per-ex loss: 0.413809  [   64/   88]
per-ex loss: 0.285208  [   65/   88]
per-ex loss: 0.574549  [   66/   88]
per-ex loss: 0.371980  [   67/   88]
per-ex loss: 0.389849  [   68/   88]
per-ex loss: 0.343054  [   69/   88]
per-ex loss: 0.586034  [   70/   88]
per-ex loss: 0.335565  [   71/   88]
per-ex loss: 0.319245  [   72/   88]
per-ex loss: 0.386558  [   73/   88]
per-ex loss: 0.355497  [   74/   88]
per-ex loss: 0.515346  [   75/   88]
per-ex loss: 0.451384  [   76/   88]
per-ex loss: 0.464446  [   77/   88]
per-ex loss: 0.385349  [   78/   88]
per-ex loss: 0.307334  [   79/   88]
per-ex loss: 0.526885  [   80/   88]
per-ex loss: 0.444201  [   81/   88]
per-ex loss: 0.495693  [   82/   88]
per-ex loss: 0.447838  [   83/   88]
per-ex loss: 0.307041  [   84/   88]
per-ex loss: 0.328112  [   85/   88]
per-ex loss: 0.464491  [   86/   88]
per-ex loss: 0.400647  [   87/   88]
per-ex loss: 0.390452  [   88/   88]
Train Error: Avg loss: 0.42216019
validation Error: 
 Avg loss: 0.54122227 
 F1: 0.486056 
 Precision: 0.521082 
 Recall: 0.455442
 IoU: 0.321053

test Error: 
 Avg loss: 0.49235277 
 F1: 0.552582 
 Precision: 0.579532 
 Recall: 0.528027
 IoU: 0.381770

We have finished training iteration 137
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_135_.pth
per-ex loss: 0.532714  [    1/   88]
per-ex loss: 0.290499  [    2/   88]
per-ex loss: 0.360045  [    3/   88]
per-ex loss: 0.302601  [    4/   88]
per-ex loss: 0.419945  [    5/   88]
per-ex loss: 0.266993  [    6/   88]
per-ex loss: 0.306399  [    7/   88]
per-ex loss: 0.352452  [    8/   88]
per-ex loss: 0.679231  [    9/   88]
per-ex loss: 0.303091  [   10/   88]
per-ex loss: 0.604326  [   11/   88]
per-ex loss: 0.551203  [   12/   88]
per-ex loss: 0.454950  [   13/   88]
per-ex loss: 0.387108  [   14/   88]
per-ex loss: 0.277757  [   15/   88]
per-ex loss: 0.384014  [   16/   88]
per-ex loss: 0.521390  [   17/   88]
per-ex loss: 0.353657  [   18/   88]
per-ex loss: 0.295002  [   19/   88]
per-ex loss: 0.408939  [   20/   88]
per-ex loss: 0.449248  [   21/   88]
per-ex loss: 0.334857  [   22/   88]
per-ex loss: 0.542384  [   23/   88]
per-ex loss: 0.505674  [   24/   88]
per-ex loss: 0.313142  [   25/   88]
per-ex loss: 0.490158  [   26/   88]
per-ex loss: 0.525779  [   27/   88]
per-ex loss: 0.366084  [   28/   88]
per-ex loss: 0.402230  [   29/   88]
per-ex loss: 0.311371  [   30/   88]
per-ex loss: 0.310266  [   31/   88]
per-ex loss: 0.412174  [   32/   88]
per-ex loss: 0.407884  [   33/   88]
per-ex loss: 0.367877  [   34/   88]
per-ex loss: 0.601035  [   35/   88]
per-ex loss: 0.377691  [   36/   88]
per-ex loss: 0.316916  [   37/   88]
per-ex loss: 0.629871  [   38/   88]
per-ex loss: 0.326148  [   39/   88]
per-ex loss: 0.508278  [   40/   88]
per-ex loss: 0.420127  [   41/   88]
per-ex loss: 0.422558  [   42/   88]
per-ex loss: 0.335029  [   43/   88]
per-ex loss: 0.553883  [   44/   88]
per-ex loss: 0.482060  [   45/   88]
per-ex loss: 0.450899  [   46/   88]
per-ex loss: 0.352315  [   47/   88]
per-ex loss: 0.274181  [   48/   88]
per-ex loss: 0.459078  [   49/   88]
per-ex loss: 0.597721  [   50/   88]
per-ex loss: 0.352960  [   51/   88]
per-ex loss: 0.396761  [   52/   88]
per-ex loss: 0.390036  [   53/   88]
per-ex loss: 0.322231  [   54/   88]
per-ex loss: 0.356201  [   55/   88]
per-ex loss: 0.306022  [   56/   88]
per-ex loss: 0.585246  [   57/   88]
per-ex loss: 0.408085  [   58/   88]
per-ex loss: 0.332638  [   59/   88]
per-ex loss: 0.524431  [   60/   88]
per-ex loss: 0.473760  [   61/   88]
per-ex loss: 0.398304  [   62/   88]
per-ex loss: 0.506396  [   63/   88]
per-ex loss: 0.436953  [   64/   88]
per-ex loss: 0.637655  [   65/   88]
per-ex loss: 0.443415  [   66/   88]
per-ex loss: 0.312175  [   67/   88]
per-ex loss: 0.207294  [   68/   88]
per-ex loss: 0.605514  [   69/   88]
per-ex loss: 0.349971  [   70/   88]
per-ex loss: 0.363068  [   71/   88]
per-ex loss: 0.415377  [   72/   88]
per-ex loss: 0.521350  [   73/   88]
per-ex loss: 0.311527  [   74/   88]
per-ex loss: 0.378806  [   75/   88]
per-ex loss: 0.398481  [   76/   88]
per-ex loss: 0.314541  [   77/   88]
per-ex loss: 0.415005  [   78/   88]
per-ex loss: 0.552653  [   79/   88]
per-ex loss: 0.356316  [   80/   88]
per-ex loss: 0.468431  [   81/   88]
per-ex loss: 0.349994  [   82/   88]
per-ex loss: 0.558564  [   83/   88]
per-ex loss: 0.496313  [   84/   88]
per-ex loss: 0.532857  [   85/   88]
per-ex loss: 0.398297  [   86/   88]
per-ex loss: 0.329060  [   87/   88]
per-ex loss: 0.506592  [   88/   88]
Train Error: Avg loss: 0.41946034
validation Error: 
 Avg loss: 0.52030315 
 F1: 0.508615 
 Precision: 0.527649 
 Recall: 0.490907
 IoU: 0.341036

test Error: 
 Avg loss: 0.47297251 
 F1: 0.570032 
 Precision: 0.561310 
 Recall: 0.579030
 IoU: 0.398633

We have finished training iteration 138
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_136_.pth
per-ex loss: 0.262365  [    1/   88]
per-ex loss: 0.332743  [    2/   88]
per-ex loss: 0.502249  [    3/   88]
per-ex loss: 0.308666  [    4/   88]
per-ex loss: 0.540776  [    5/   88]
per-ex loss: 0.314641  [    6/   88]
per-ex loss: 0.391818  [    7/   88]
per-ex loss: 0.525206  [    8/   88]
per-ex loss: 0.546901  [    9/   88]
per-ex loss: 0.301902  [   10/   88]
per-ex loss: 0.400814  [   11/   88]
per-ex loss: 0.310026  [   12/   88]
per-ex loss: 0.408364  [   13/   88]
per-ex loss: 0.368274  [   14/   88]
per-ex loss: 0.286946  [   15/   88]
per-ex loss: 0.563865  [   16/   88]
per-ex loss: 0.416628  [   17/   88]
per-ex loss: 0.258080  [   18/   88]
per-ex loss: 0.342073  [   19/   88]
per-ex loss: 0.396340  [   20/   88]
per-ex loss: 0.523677  [   21/   88]
per-ex loss: 0.325708  [   22/   88]
per-ex loss: 0.573167  [   23/   88]
per-ex loss: 0.400850  [   24/   88]
per-ex loss: 0.520292  [   25/   88]
per-ex loss: 0.348049  [   26/   88]
per-ex loss: 0.378115  [   27/   88]
per-ex loss: 0.596747  [   28/   88]
per-ex loss: 0.456707  [   29/   88]
per-ex loss: 0.352619  [   30/   88]
per-ex loss: 0.345321  [   31/   88]
per-ex loss: 0.578028  [   32/   88]
per-ex loss: 0.356964  [   33/   88]
per-ex loss: 0.463004  [   34/   88]
per-ex loss: 0.389261  [   35/   88]
per-ex loss: 0.424532  [   36/   88]
per-ex loss: 0.316072  [   37/   88]
per-ex loss: 0.605357  [   38/   88]
per-ex loss: 0.620472  [   39/   88]
per-ex loss: 0.525383  [   40/   88]
per-ex loss: 0.395107  [   41/   88]
per-ex loss: 0.291847  [   42/   88]
per-ex loss: 0.357811  [   43/   88]
per-ex loss: 0.327683  [   44/   88]
per-ex loss: 0.606267  [   45/   88]
per-ex loss: 0.451292  [   46/   88]
per-ex loss: 0.512857  [   47/   88]
per-ex loss: 0.362018  [   48/   88]
per-ex loss: 0.489033  [   49/   88]
per-ex loss: 0.556432  [   50/   88]
per-ex loss: 0.318934  [   51/   88]
per-ex loss: 0.507790  [   52/   88]
per-ex loss: 0.421757  [   53/   88]
per-ex loss: 0.369246  [   54/   88]
per-ex loss: 0.442881  [   55/   88]
per-ex loss: 0.297741  [   56/   88]
per-ex loss: 0.311269  [   57/   88]
per-ex loss: 0.301042  [   58/   88]
per-ex loss: 0.479827  [   59/   88]
per-ex loss: 0.336358  [   60/   88]
per-ex loss: 0.311686  [   61/   88]
per-ex loss: 0.509394  [   62/   88]
per-ex loss: 0.403796  [   63/   88]
per-ex loss: 0.367518  [   64/   88]
per-ex loss: 0.534074  [   65/   88]
per-ex loss: 0.300418  [   66/   88]
per-ex loss: 0.528673  [   67/   88]
per-ex loss: 0.494495  [   68/   88]
per-ex loss: 0.375535  [   69/   88]
per-ex loss: 0.318179  [   70/   88]
per-ex loss: 0.448397  [   71/   88]
per-ex loss: 0.455935  [   72/   88]
per-ex loss: 0.385403  [   73/   88]
per-ex loss: 0.343833  [   74/   88]
per-ex loss: 0.512466  [   75/   88]
per-ex loss: 0.444079  [   76/   88]
per-ex loss: 0.320864  [   77/   88]
per-ex loss: 0.237587  [   78/   88]
per-ex loss: 0.497180  [   79/   88]
per-ex loss: 0.438046  [   80/   88]
per-ex loss: 0.624846  [   81/   88]
per-ex loss: 0.526347  [   82/   88]
per-ex loss: 0.373464  [   83/   88]
per-ex loss: 0.389247  [   84/   88]
per-ex loss: 0.604321  [   85/   88]
per-ex loss: 0.345800  [   86/   88]
per-ex loss: 0.484447  [   87/   88]
per-ex loss: 0.353381  [   88/   88]
Train Error: Avg loss: 0.41983642
validation Error: 
 Avg loss: 0.52982244 
 F1: 0.499073 
 Precision: 0.523225 
 Recall: 0.477051
 IoU: 0.332510

test Error: 
 Avg loss: 0.49472667 
 F1: 0.550074 
 Precision: 0.553008 
 Recall: 0.547171
 IoU: 0.379381

We have finished training iteration 139
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_137_.pth
per-ex loss: 0.471456  [    1/   88]
per-ex loss: 0.352520  [    2/   88]
per-ex loss: 0.358228  [    3/   88]
per-ex loss: 0.365792  [    4/   88]
per-ex loss: 0.307162  [    5/   88]
per-ex loss: 0.423654  [    6/   88]
per-ex loss: 0.454654  [    7/   88]
per-ex loss: 0.626331  [    8/   88]
per-ex loss: 0.348438  [    9/   88]
per-ex loss: 0.510821  [   10/   88]
per-ex loss: 0.267207  [   11/   88]
per-ex loss: 0.348420  [   12/   88]
per-ex loss: 0.619893  [   13/   88]
per-ex loss: 0.657481  [   14/   88]
per-ex loss: 0.591908  [   15/   88]
per-ex loss: 0.568330  [   16/   88]
per-ex loss: 0.444052  [   17/   88]
per-ex loss: 0.373552  [   18/   88]
per-ex loss: 0.392948  [   19/   88]
per-ex loss: 0.339381  [   20/   88]
per-ex loss: 0.380127  [   21/   88]
per-ex loss: 0.410599  [   22/   88]
per-ex loss: 0.292738  [   23/   88]
per-ex loss: 0.445551  [   24/   88]
per-ex loss: 0.570567  [   25/   88]
per-ex loss: 0.391433  [   26/   88]
per-ex loss: 0.398122  [   27/   88]
per-ex loss: 0.537639  [   28/   88]
per-ex loss: 0.468378  [   29/   88]
per-ex loss: 0.508321  [   30/   88]
per-ex loss: 0.367292  [   31/   88]
per-ex loss: 0.523462  [   32/   88]
per-ex loss: 0.352445  [   33/   88]
per-ex loss: 0.387875  [   34/   88]
per-ex loss: 0.336663  [   35/   88]
per-ex loss: 0.445354  [   36/   88]
per-ex loss: 0.541663  [   37/   88]
per-ex loss: 0.343165  [   38/   88]
per-ex loss: 0.565581  [   39/   88]
per-ex loss: 0.313947  [   40/   88]
per-ex loss: 0.398845  [   41/   88]
per-ex loss: 0.533651  [   42/   88]
per-ex loss: 0.418006  [   43/   88]
per-ex loss: 0.343638  [   44/   88]
per-ex loss: 0.448479  [   45/   88]
per-ex loss: 0.419308  [   46/   88]
per-ex loss: 0.365654  [   47/   88]
per-ex loss: 0.316433  [   48/   88]
per-ex loss: 0.344677  [   49/   88]
per-ex loss: 0.363050  [   50/   88]
per-ex loss: 0.323161  [   51/   88]
per-ex loss: 0.526002  [   52/   88]
per-ex loss: 0.436401  [   53/   88]
per-ex loss: 0.534277  [   54/   88]
per-ex loss: 0.513856  [   55/   88]
per-ex loss: 0.374036  [   56/   88]
per-ex loss: 0.528815  [   57/   88]
per-ex loss: 0.231739  [   58/   88]
per-ex loss: 0.384282  [   59/   88]
per-ex loss: 0.318943  [   60/   88]
per-ex loss: 0.318871  [   61/   88]
per-ex loss: 0.315051  [   62/   88]
per-ex loss: 0.284122  [   63/   88]
per-ex loss: 0.281621  [   64/   88]
per-ex loss: 0.484092  [   65/   88]
per-ex loss: 0.660519  [   66/   88]
per-ex loss: 0.491767  [   67/   88]
per-ex loss: 0.382199  [   68/   88]
per-ex loss: 0.530687  [   69/   88]
per-ex loss: 0.526790  [   70/   88]
per-ex loss: 0.540826  [   71/   88]
per-ex loss: 0.324425  [   72/   88]
per-ex loss: 0.365948  [   73/   88]
per-ex loss: 0.523527  [   74/   88]
per-ex loss: 0.322067  [   75/   88]
per-ex loss: 0.590342  [   76/   88]
per-ex loss: 0.428139  [   77/   88]
per-ex loss: 0.335009  [   78/   88]
per-ex loss: 0.468521  [   79/   88]
per-ex loss: 0.391198  [   80/   88]
per-ex loss: 0.596884  [   81/   88]
per-ex loss: 0.577127  [   82/   88]
per-ex loss: 0.394471  [   83/   88]
per-ex loss: 0.365791  [   84/   88]
per-ex loss: 0.451479  [   85/   88]
per-ex loss: 0.330160  [   86/   88]
per-ex loss: 0.536546  [   87/   88]
per-ex loss: 0.371683  [   88/   88]
Train Error: Avg loss: 0.42859388
validation Error: 
 Avg loss: 0.53124927 
 F1: 0.505366 
 Precision: 0.557242 
 Recall: 0.462326
 IoU: 0.338120

test Error: 
 Avg loss: 0.47734192 
 F1: 0.566767 
 Precision: 0.609536 
 Recall: 0.529606
 IoU: 0.395446

We have finished training iteration 140
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_138_.pth
per-ex loss: 0.512013  [    1/   88]
per-ex loss: 0.526473  [    2/   88]
per-ex loss: 0.535052  [    3/   88]
per-ex loss: 0.312075  [    4/   88]
per-ex loss: 0.492240  [    5/   88]
per-ex loss: 0.602645  [    6/   88]
per-ex loss: 0.605814  [    7/   88]
per-ex loss: 0.533626  [    8/   88]
per-ex loss: 0.560470  [    9/   88]
per-ex loss: 0.543469  [   10/   88]
per-ex loss: 0.400270  [   11/   88]
per-ex loss: 0.294604  [   12/   88]
per-ex loss: 0.352052  [   13/   88]
per-ex loss: 0.352939  [   14/   88]
per-ex loss: 0.360264  [   15/   88]
per-ex loss: 0.315966  [   16/   88]
per-ex loss: 0.406516  [   17/   88]
per-ex loss: 0.542925  [   18/   88]
per-ex loss: 0.490997  [   19/   88]
per-ex loss: 0.615407  [   20/   88]
per-ex loss: 0.312202  [   21/   88]
per-ex loss: 0.582557  [   22/   88]
per-ex loss: 0.415906  [   23/   88]
per-ex loss: 0.345419  [   24/   88]
per-ex loss: 0.212183  [   25/   88]
per-ex loss: 0.366614  [   26/   88]
per-ex loss: 0.462547  [   27/   88]
per-ex loss: 0.305260  [   28/   88]
per-ex loss: 0.396561  [   29/   88]
per-ex loss: 0.484930  [   30/   88]
per-ex loss: 0.550493  [   31/   88]
per-ex loss: 0.359006  [   32/   88]
per-ex loss: 0.603228  [   33/   88]
per-ex loss: 0.442241  [   34/   88]
per-ex loss: 0.505726  [   35/   88]
per-ex loss: 0.615808  [   36/   88]
per-ex loss: 0.286366  [   37/   88]
per-ex loss: 0.321350  [   38/   88]
per-ex loss: 0.538741  [   39/   88]
per-ex loss: 0.409865  [   40/   88]
per-ex loss: 0.536971  [   41/   88]
per-ex loss: 0.304513  [   42/   88]
per-ex loss: 0.390352  [   43/   88]
per-ex loss: 0.392556  [   44/   88]
per-ex loss: 0.572160  [   45/   88]
per-ex loss: 0.454046  [   46/   88]
per-ex loss: 0.317094  [   47/   88]
per-ex loss: 0.379473  [   48/   88]
per-ex loss: 0.375731  [   49/   88]
per-ex loss: 0.353130  [   50/   88]
per-ex loss: 0.551695  [   51/   88]
per-ex loss: 0.509500  [   52/   88]
per-ex loss: 0.453175  [   53/   88]
per-ex loss: 0.572027  [   54/   88]
per-ex loss: 0.443507  [   55/   88]
per-ex loss: 0.401403  [   56/   88]
per-ex loss: 0.338934  [   57/   88]
per-ex loss: 0.428157  [   58/   88]
per-ex loss: 0.369040  [   59/   88]
per-ex loss: 0.284003  [   60/   88]
per-ex loss: 0.418857  [   61/   88]
per-ex loss: 0.348872  [   62/   88]
per-ex loss: 0.404884  [   63/   88]
per-ex loss: 0.406650  [   64/   88]
per-ex loss: 0.348289  [   65/   88]
per-ex loss: 0.340512  [   66/   88]
per-ex loss: 0.310395  [   67/   88]
per-ex loss: 0.339479  [   68/   88]
per-ex loss: 0.393691  [   69/   88]
per-ex loss: 0.317739  [   70/   88]
per-ex loss: 0.305072  [   71/   88]
per-ex loss: 0.309429  [   72/   88]
per-ex loss: 0.264728  [   73/   88]
per-ex loss: 0.360421  [   74/   88]
per-ex loss: 0.369370  [   75/   88]
per-ex loss: 0.505718  [   76/   88]
per-ex loss: 0.513927  [   77/   88]
per-ex loss: 0.625016  [   78/   88]
per-ex loss: 0.517250  [   79/   88]
per-ex loss: 0.369967  [   80/   88]
per-ex loss: 0.374353  [   81/   88]
per-ex loss: 0.300975  [   82/   88]
per-ex loss: 0.393990  [   83/   88]
per-ex loss: 0.309740  [   84/   88]
per-ex loss: 0.282506  [   85/   88]
per-ex loss: 0.513678  [   86/   88]
per-ex loss: 0.468163  [   87/   88]
per-ex loss: 0.389655  [   88/   88]
Train Error: Avg loss: 0.42163195
validation Error: 
 Avg loss: 0.53609597 
 F1: 0.493960 
 Precision: 0.598601 
 Recall: 0.420460
 IoU: 0.327986

test Error: 
 Avg loss: 0.49988895 
 F1: 0.546988 
 Precision: 0.641264 
 Recall: 0.476880
 IoU: 0.376451

We have finished training iteration 141
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_139_.pth
per-ex loss: 0.518165  [    1/   88]
per-ex loss: 0.536341  [    2/   88]
per-ex loss: 0.398559  [    3/   88]
per-ex loss: 0.294725  [    4/   88]
per-ex loss: 0.512200  [    5/   88]
per-ex loss: 0.361848  [    6/   88]
per-ex loss: 0.381661  [    7/   88]
per-ex loss: 0.266729  [    8/   88]
per-ex loss: 0.353246  [    9/   88]
per-ex loss: 0.493168  [   10/   88]
per-ex loss: 0.523351  [   11/   88]
per-ex loss: 0.393970  [   12/   88]
per-ex loss: 0.579806  [   13/   88]
per-ex loss: 0.278782  [   14/   88]
per-ex loss: 0.468408  [   15/   88]
per-ex loss: 0.380544  [   16/   88]
per-ex loss: 0.334298  [   17/   88]
per-ex loss: 0.499877  [   18/   88]
per-ex loss: 0.533192  [   19/   88]
per-ex loss: 0.289129  [   20/   88]
per-ex loss: 0.381088  [   21/   88]
per-ex loss: 0.523618  [   22/   88]
per-ex loss: 0.366032  [   23/   88]
per-ex loss: 0.555628  [   24/   88]
per-ex loss: 0.436176  [   25/   88]
per-ex loss: 0.457625  [   26/   88]
per-ex loss: 0.584717  [   27/   88]
per-ex loss: 0.396756  [   28/   88]
per-ex loss: 0.413407  [   29/   88]
per-ex loss: 0.422490  [   30/   88]
per-ex loss: 0.402939  [   31/   88]
per-ex loss: 0.479381  [   32/   88]
per-ex loss: 0.314970  [   33/   88]
per-ex loss: 0.364713  [   34/   88]
per-ex loss: 0.279648  [   35/   88]
per-ex loss: 0.317788  [   36/   88]
per-ex loss: 0.444321  [   37/   88]
per-ex loss: 0.349492  [   38/   88]
per-ex loss: 0.531675  [   39/   88]
per-ex loss: 0.559514  [   40/   88]
per-ex loss: 0.390299  [   41/   88]
per-ex loss: 0.305898  [   42/   88]
per-ex loss: 0.349818  [   43/   88]
per-ex loss: 0.582204  [   44/   88]
per-ex loss: 0.321476  [   45/   88]
per-ex loss: 0.332579  [   46/   88]
per-ex loss: 0.503801  [   47/   88]
per-ex loss: 0.355714  [   48/   88]
per-ex loss: 0.486412  [   49/   88]
per-ex loss: 0.543768  [   50/   88]
per-ex loss: 0.649653  [   51/   88]
per-ex loss: 0.306599  [   52/   88]
per-ex loss: 0.375075  [   53/   88]
per-ex loss: 0.375478  [   54/   88]
per-ex loss: 0.462963  [   55/   88]
per-ex loss: 0.353928  [   56/   88]
per-ex loss: 0.489807  [   57/   88]
per-ex loss: 0.546236  [   58/   88]
per-ex loss: 0.395227  [   59/   88]
per-ex loss: 0.494279  [   60/   88]
per-ex loss: 0.330473  [   61/   88]
per-ex loss: 0.553179  [   62/   88]
per-ex loss: 0.423102  [   63/   88]
per-ex loss: 0.388102  [   64/   88]
per-ex loss: 0.432066  [   65/   88]
per-ex loss: 0.315036  [   66/   88]
per-ex loss: 0.286058  [   67/   88]
per-ex loss: 0.230927  [   68/   88]
per-ex loss: 0.624442  [   69/   88]
per-ex loss: 0.433313  [   70/   88]
per-ex loss: 0.361044  [   71/   88]
per-ex loss: 0.314915  [   72/   88]
per-ex loss: 0.317849  [   73/   88]
per-ex loss: 0.427658  [   74/   88]
per-ex loss: 0.624348  [   75/   88]
per-ex loss: 0.493876  [   76/   88]
per-ex loss: 0.326113  [   77/   88]
per-ex loss: 0.319874  [   78/   88]
per-ex loss: 0.453273  [   79/   88]
per-ex loss: 0.600040  [   80/   88]
per-ex loss: 0.308425  [   81/   88]
per-ex loss: 0.302207  [   82/   88]
per-ex loss: 0.386162  [   83/   88]
per-ex loss: 0.335111  [   84/   88]
per-ex loss: 0.385760  [   85/   88]
per-ex loss: 0.346714  [   86/   88]
per-ex loss: 0.433537  [   87/   88]
per-ex loss: 0.294466  [   88/   88]
Train Error: Avg loss: 0.41646894
validation Error: 
 Avg loss: 0.52280281 
 F1: 0.508036 
 Precision: 0.611470 
 Recall: 0.434531
 IoU: 0.340514

test Error: 
 Avg loss: 0.48875938 
 F1: 0.554160 
 Precision: 0.635559 
 Recall: 0.491244
 IoU: 0.383279

We have finished training iteration 142
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_140_.pth
per-ex loss: 0.459058  [    1/   88]
per-ex loss: 0.519969  [    2/   88]
per-ex loss: 0.500340  [    3/   88]
per-ex loss: 0.464432  [    4/   88]
per-ex loss: 0.284606  [    5/   88]
per-ex loss: 0.512170  [    6/   88]
per-ex loss: 0.441619  [    7/   88]
per-ex loss: 0.607438  [    8/   88]
per-ex loss: 0.410516  [    9/   88]
per-ex loss: 0.321398  [   10/   88]
per-ex loss: 0.579383  [   11/   88]
per-ex loss: 0.561254  [   12/   88]
per-ex loss: 0.537855  [   13/   88]
per-ex loss: 0.443818  [   14/   88]
per-ex loss: 0.289892  [   15/   88]
per-ex loss: 0.428100  [   16/   88]
per-ex loss: 0.272066  [   17/   88]
per-ex loss: 0.357809  [   18/   88]
per-ex loss: 0.405578  [   19/   88]
per-ex loss: 0.382434  [   20/   88]
per-ex loss: 0.321387  [   21/   88]
per-ex loss: 0.280072  [   22/   88]
per-ex loss: 0.431750  [   23/   88]
per-ex loss: 0.351311  [   24/   88]
per-ex loss: 0.305677  [   25/   88]
per-ex loss: 0.285251  [   26/   88]
per-ex loss: 0.378986  [   27/   88]
per-ex loss: 0.455109  [   28/   88]
per-ex loss: 0.305543  [   29/   88]
per-ex loss: 0.389956  [   30/   88]
per-ex loss: 0.526877  [   31/   88]
per-ex loss: 0.358918  [   32/   88]
per-ex loss: 0.603550  [   33/   88]
per-ex loss: 0.309798  [   34/   88]
per-ex loss: 0.541060  [   35/   88]
per-ex loss: 0.503069  [   36/   88]
per-ex loss: 0.608286  [   37/   88]
per-ex loss: 0.612904  [   38/   88]
per-ex loss: 0.312865  [   39/   88]
per-ex loss: 0.429333  [   40/   88]
per-ex loss: 0.323355  [   41/   88]
per-ex loss: 0.445076  [   42/   88]
per-ex loss: 0.353096  [   43/   88]
per-ex loss: 0.406484  [   44/   88]
per-ex loss: 0.526271  [   45/   88]
per-ex loss: 0.322893  [   46/   88]
per-ex loss: 0.349326  [   47/   88]
per-ex loss: 0.325209  [   48/   88]
per-ex loss: 0.522741  [   49/   88]
per-ex loss: 0.393332  [   50/   88]
per-ex loss: 0.518384  [   51/   88]
per-ex loss: 0.389475  [   52/   88]
per-ex loss: 0.398779  [   53/   88]
per-ex loss: 0.234597  [   54/   88]
per-ex loss: 0.420899  [   55/   88]
per-ex loss: 0.308867  [   56/   88]
per-ex loss: 0.297576  [   57/   88]
per-ex loss: 0.321439  [   58/   88]
per-ex loss: 0.375551  [   59/   88]
per-ex loss: 0.389855  [   60/   88]
per-ex loss: 0.358897  [   61/   88]
per-ex loss: 0.552564  [   62/   88]
per-ex loss: 0.603777  [   63/   88]
per-ex loss: 0.360824  [   64/   88]
per-ex loss: 0.373688  [   65/   88]
per-ex loss: 0.377243  [   66/   88]
per-ex loss: 0.458376  [   67/   88]
per-ex loss: 0.598233  [   68/   88]
per-ex loss: 0.355020  [   69/   88]
per-ex loss: 0.378022  [   70/   88]
per-ex loss: 0.536992  [   71/   88]
per-ex loss: 0.476969  [   72/   88]
per-ex loss: 0.571089  [   73/   88]
per-ex loss: 0.370829  [   74/   88]
per-ex loss: 0.504294  [   75/   88]
per-ex loss: 0.336148  [   76/   88]
per-ex loss: 0.323993  [   77/   88]
per-ex loss: 0.522993  [   78/   88]
per-ex loss: 0.360498  [   79/   88]
per-ex loss: 0.373190  [   80/   88]
per-ex loss: 0.565634  [   81/   88]
per-ex loss: 0.358911  [   82/   88]
per-ex loss: 0.358995  [   83/   88]
per-ex loss: 0.340469  [   84/   88]
per-ex loss: 0.278171  [   85/   88]
per-ex loss: 0.414515  [   86/   88]
per-ex loss: 0.452551  [   87/   88]
per-ex loss: 0.508053  [   88/   88]
Train Error: Avg loss: 0.41801797
validation Error: 
 Avg loss: 0.52709698 
 F1: 0.503136 
 Precision: 0.630740 
 Recall: 0.418476
 IoU: 0.336127

test Error: 
 Avg loss: 0.48280170 
 F1: 0.566782 
 Precision: 0.661674 
 Recall: 0.495693
 IoU: 0.395461

We have finished training iteration 143
Deleting model ./unet_att_res_nzo_train/saved_model_wrapper/models/UNet_141_.pth
slurmstepd: error: *** STEP 16871.0 ON aga2 CANCELLED AT 2025-01-14T21:12:20 DUE TO TIME LIMIT ***
