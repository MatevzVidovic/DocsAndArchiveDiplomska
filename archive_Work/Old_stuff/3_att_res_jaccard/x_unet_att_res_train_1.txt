unet_att_main_just_train.py do_log: False
Log file name: log_14_13-59-12_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(sd='unet_att_res_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_att_res.yaml', yo=None, ntibp=None, ptp=None, map=None)
YAML: {'path_to_data': './Data/vein_and_sclera_data', 'batch_size': 1, 'learning_rate': 1e-05, 'num_of_dataloader_workers': 7, 'train_epoch_size_limit': 400, 'num_epochs_per_training_iteration': 1, 'cleanup_k': 3, 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': True, 'loss_fn_name': 'MCDL', 'alphas': [], 'dataset_option': 'aug_tf', 'zero_out_non_sclera': True, 'add_sclera_to_img': False, 'add_bcosfire_to_img': True, 'add_coye_to_img': True, 'model_type': 'res_att', 'model': '64_2_6', 'input_width': 2048, 'input_height': 1024, 'input_channels': 5, 'output_channels': 2, 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq'}
Validation phase: False
Namespace(sd='unet_att_res_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_att_res.yaml', yo=None, ntibp=None, ptp=None, map=None)
Device: cuda
dataset_aug_tf.py do_log: False
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 88
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 88
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 88
val dataloader num of batches: 27
test dataloader num of batches: 12
Created new model instance.
per-ex loss: 0.881973  [    1/   88]
per-ex loss: 0.956903  [    2/   88]
per-ex loss: 0.953819  [    3/   88]
per-ex loss: 0.861276  [    4/   88]
per-ex loss: 0.904292  [    5/   88]
per-ex loss: 0.904710  [    6/   88]
per-ex loss: 0.941208  [    7/   88]
per-ex loss: 0.844907  [    8/   88]
per-ex loss: 0.801157  [    9/   88]
per-ex loss: 0.794326  [   10/   88]
per-ex loss: 0.887991  [   11/   88]
per-ex loss: 0.862053  [   12/   88]
per-ex loss: 0.950342  [   13/   88]
per-ex loss: 0.935616  [   14/   88]
per-ex loss: 0.932754  [   15/   88]
per-ex loss: 0.769514  [   16/   88]
per-ex loss: 0.897744  [   17/   88]
per-ex loss: 0.943049  [   18/   88]
per-ex loss: 0.910677  [   19/   88]
per-ex loss: 0.860736  [   20/   88]
per-ex loss: 0.818094  [   21/   88]
per-ex loss: 0.840555  [   22/   88]
per-ex loss: 0.833189  [   23/   88]
per-ex loss: 0.887205  [   24/   88]
per-ex loss: 0.906373  [   25/   88]
per-ex loss: 0.790851  [   26/   88]
per-ex loss: 0.760227  [   27/   88]
per-ex loss: 0.783558  [   28/   88]
per-ex loss: 0.904012  [   29/   88]
per-ex loss: 0.862801  [   30/   88]
per-ex loss: 0.863341  [   31/   88]
per-ex loss: 0.868614  [   32/   88]
per-ex loss: 0.704062  [   33/   88]
per-ex loss: 0.853631  [   34/   88]
per-ex loss: 0.824561  [   35/   88]
per-ex loss: 0.745701  [   36/   88]
per-ex loss: 0.829941  [   37/   88]
per-ex loss: 0.907081  [   38/   88]
per-ex loss: 0.806284  [   39/   88]
per-ex loss: 0.580158  [   40/   88]
per-ex loss: 0.712759  [   41/   88]
per-ex loss: 0.582960  [   42/   88]
per-ex loss: 0.664773  [   43/   88]
per-ex loss: 0.873270  [   44/   88]
per-ex loss: 0.556831  [   45/   88]
per-ex loss: 0.814467  [   46/   88]
per-ex loss: 0.842545  [   47/   88]
per-ex loss: 0.775429  [   48/   88]
per-ex loss: 0.734005  [   49/   88]
per-ex loss: 0.630458  [   50/   88]
per-ex loss: 0.584074  [   51/   88]
per-ex loss: 0.595308  [   52/   88]
per-ex loss: 0.717620  [   53/   88]
per-ex loss: 0.647874  [   54/   88]
per-ex loss: 0.715607  [   55/   88]
per-ex loss: 0.862475  [   56/   88]
per-ex loss: 0.595609  [   57/   88]
per-ex loss: 0.781539  [   58/   88]
per-ex loss: 0.517526  [   59/   88]
per-ex loss: 0.649347  [   60/   88]
per-ex loss: 0.769456  [   61/   88]
per-ex loss: 0.754030  [   62/   88]
per-ex loss: 0.756404  [   63/   88]
per-ex loss: 0.491284  [   64/   88]
per-ex loss: 0.652929  [   65/   88]
per-ex loss: 0.797480  [   66/   88]
per-ex loss: 0.596105  [   67/   88]
per-ex loss: 0.760595  [   68/   88]
per-ex loss: 0.867624  [   69/   88]
per-ex loss: 0.742470  [   70/   88]
per-ex loss: 0.580258  [   71/   88]
per-ex loss: 0.646733  [   72/   88]
per-ex loss: 0.513858  [   73/   88]
per-ex loss: 0.693007  [   74/   88]
per-ex loss: 0.661529  [   75/   88]
per-ex loss: 0.588098  [   76/   88]
per-ex loss: 0.667373  [   77/   88]
per-ex loss: 0.611859  [   78/   88]
per-ex loss: 0.621082  [   79/   88]
per-ex loss: 0.785010  [   80/   88]
per-ex loss: 0.567937  [   81/   88]
per-ex loss: 0.554553  [   82/   88]
per-ex loss: 0.740790  [   83/   88]
per-ex loss: 0.746547  [   84/   88]
per-ex loss: 0.524730  [   85/   88]
per-ex loss: 0.573430  [   86/   88]
per-ex loss: 0.565925  [   87/   88]
per-ex loss: 0.692212  [   88/   88]
Train Error: Avg loss: 0.75617123
validation Error: 
 Avg loss: 0.66699410 
 F1: 0.395985 
 Precision: 0.620233 
 Recall: 0.290833
 IoU: 0.246871

test Error: 
 Avg loss: 0.62086800 
 F1: 0.495057 
 Precision: 0.590093 
 Recall: 0.426387
 IoU: 0.328954

We have finished training iteration 1
per-ex loss: 0.514165  [    1/   88]
per-ex loss: 0.512425  [    2/   88]
per-ex loss: 0.798508  [    3/   88]
per-ex loss: 0.589814  [    4/   88]
per-ex loss: 0.635877  [    5/   88]
per-ex loss: 0.603792  [    6/   88]
per-ex loss: 0.473683  [    7/   88]
per-ex loss: 0.434275  [    8/   88]
per-ex loss: 0.512804  [    9/   88]
per-ex loss: 0.524921  [   10/   88]
per-ex loss: 0.716792  [   11/   88]
per-ex loss: 0.792801  [   12/   88]
per-ex loss: 0.773444  [   13/   88]
per-ex loss: 0.725229  [   14/   88]
per-ex loss: 0.651790  [   15/   88]
per-ex loss: 0.638416  [   16/   88]
per-ex loss: 0.541973  [   17/   88]
per-ex loss: 0.556753  [   18/   88]
per-ex loss: 0.469748  [   19/   88]
per-ex loss: 0.726750  [   20/   88]
per-ex loss: 0.730657  [   21/   88]
per-ex loss: 0.622452  [   22/   88]
per-ex loss: 0.735907  [   23/   88]
per-ex loss: 0.750096  [   24/   88]
per-ex loss: 0.788132  [   25/   88]
per-ex loss: 0.654126  [   26/   88]
per-ex loss: 0.570128  [   27/   88]
per-ex loss: 0.735972  [   28/   88]
per-ex loss: 0.766041  [   29/   88]
per-ex loss: 0.540008  [   30/   88]
per-ex loss: 0.688772  [   31/   88]
per-ex loss: 0.566604  [   32/   88]
per-ex loss: 0.536876  [   33/   88]
per-ex loss: 0.666044  [   34/   88]
per-ex loss: 0.704509  [   35/   88]
per-ex loss: 0.700965  [   36/   88]
per-ex loss: 0.450901  [   37/   88]
per-ex loss: 0.459858  [   38/   88]
per-ex loss: 0.436584  [   39/   88]
per-ex loss: 0.424460  [   40/   88]
per-ex loss: 0.502172  [   41/   88]
per-ex loss: 0.445216  [   42/   88]
per-ex loss: 0.468704  [   43/   88]
per-ex loss: 0.729118  [   44/   88]
per-ex loss: 0.775748  [   45/   88]
per-ex loss: 0.702734  [   46/   88]
per-ex loss: 0.445613  [   47/   88]
per-ex loss: 0.776366  [   48/   88]
per-ex loss: 0.550276  [   49/   88]
per-ex loss: 0.626953  [   50/   88]
per-ex loss: 0.621590  [   51/   88]
per-ex loss: 0.563273  [   52/   88]
per-ex loss: 0.722905  [   53/   88]
per-ex loss: 0.615192  [   54/   88]
per-ex loss: 0.520746  [   55/   88]
per-ex loss: 0.675940  [   56/   88]
per-ex loss: 0.563684  [   57/   88]
per-ex loss: 0.612825  [   58/   88]
per-ex loss: 0.617860  [   59/   88]
per-ex loss: 0.724562  [   60/   88]
per-ex loss: 0.647482  [   61/   88]
per-ex loss: 0.587818  [   62/   88]
per-ex loss: 0.592091  [   63/   88]
per-ex loss: 0.667382  [   64/   88]
per-ex loss: 0.625157  [   65/   88]
per-ex loss: 0.632185  [   66/   88]
per-ex loss: 0.528317  [   67/   88]
per-ex loss: 0.488202  [   68/   88]
per-ex loss: 0.539727  [   69/   88]
per-ex loss: 0.545367  [   70/   88]
per-ex loss: 0.465613  [   71/   88]
per-ex loss: 0.745795  [   72/   88]
per-ex loss: 0.643811  [   73/   88]
per-ex loss: 0.708942  [   74/   88]
per-ex loss: 0.593899  [   75/   88]
per-ex loss: 0.490450  [   76/   88]
per-ex loss: 0.651863  [   77/   88]
per-ex loss: 0.535947  [   78/   88]
per-ex loss: 0.511788  [   79/   88]
per-ex loss: 0.538731  [   80/   88]
per-ex loss: 0.780822  [   81/   88]
per-ex loss: 0.691617  [   82/   88]
per-ex loss: 0.542044  [   83/   88]
per-ex loss: 0.728909  [   84/   88]
per-ex loss: 0.392738  [   85/   88]
per-ex loss: 0.492807  [   86/   88]
per-ex loss: 0.482169  [   87/   88]
per-ex loss: 0.515633  [   88/   88]
Train Error: Avg loss: 0.60627088
validation Error: 
 Avg loss: 0.62963671 
 F1: 0.402510 
 Precision: 0.673657 
 Recall: 0.286995
 IoU: 0.251964

test Error: 
 Avg loss: 0.57326569 
 F1: 0.495983 
 Precision: 0.682462 
 Recall: 0.389543
 IoU: 0.329773

We have finished training iteration 2
per-ex loss: 0.673632  [    1/   88]
per-ex loss: 0.587345  [    2/   88]
per-ex loss: 0.708008  [    3/   88]
per-ex loss: 0.716130  [    4/   88]
per-ex loss: 0.767276  [    5/   88]
per-ex loss: 0.605551  [    6/   88]
per-ex loss: 0.413121  [    7/   88]
per-ex loss: 0.401823  [    8/   88]
per-ex loss: 0.461115  [    9/   88]
per-ex loss: 0.711660  [   10/   88]
per-ex loss: 0.640281  [   11/   88]
per-ex loss: 0.593982  [   12/   88]
per-ex loss: 0.544587  [   13/   88]
per-ex loss: 0.491149  [   14/   88]
per-ex loss: 0.580302  [   15/   88]
per-ex loss: 0.640664  [   16/   88]
per-ex loss: 0.493385  [   17/   88]
per-ex loss: 0.602333  [   18/   88]
per-ex loss: 0.562976  [   19/   88]
per-ex loss: 0.438350  [   20/   88]
per-ex loss: 0.511971  [   21/   88]
per-ex loss: 0.454672  [   22/   88]
per-ex loss: 0.764812  [   23/   88]
per-ex loss: 0.628987  [   24/   88]
per-ex loss: 0.538367  [   25/   88]
per-ex loss: 0.516664  [   26/   88]
per-ex loss: 0.758285  [   27/   88]
per-ex loss: 0.653936  [   28/   88]
per-ex loss: 0.489729  [   29/   88]
per-ex loss: 0.524178  [   30/   88]
per-ex loss: 0.647953  [   31/   88]
per-ex loss: 0.635140  [   32/   88]
per-ex loss: 0.700224  [   33/   88]
per-ex loss: 0.505594  [   34/   88]
per-ex loss: 0.606525  [   35/   88]
per-ex loss: 0.728578  [   36/   88]
per-ex loss: 0.428118  [   37/   88]
per-ex loss: 0.640409  [   38/   88]
per-ex loss: 0.667267  [   39/   88]
per-ex loss: 0.599674  [   40/   88]
per-ex loss: 0.683478  [   41/   88]
per-ex loss: 0.503188  [   42/   88]
per-ex loss: 0.558290  [   43/   88]
per-ex loss: 0.482340  [   44/   88]
per-ex loss: 0.684042  [   45/   88]
per-ex loss: 0.688430  [   46/   88]
per-ex loss: 0.457416  [   47/   88]
per-ex loss: 0.732830  [   48/   88]
per-ex loss: 0.463185  [   49/   88]
per-ex loss: 0.727556  [   50/   88]
per-ex loss: 0.608092  [   51/   88]
per-ex loss: 0.455475  [   52/   88]
per-ex loss: 0.499587  [   53/   88]
per-ex loss: 0.617927  [   54/   88]
per-ex loss: 0.733928  [   55/   88]
per-ex loss: 0.695248  [   56/   88]
per-ex loss: 0.579863  [   57/   88]
per-ex loss: 0.575563  [   58/   88]
per-ex loss: 0.442836  [   59/   88]
per-ex loss: 0.485555  [   60/   88]
per-ex loss: 0.518387  [   61/   88]
per-ex loss: 0.421662  [   62/   88]
per-ex loss: 0.587465  [   63/   88]
per-ex loss: 0.726217  [   64/   88]
per-ex loss: 0.530936  [   65/   88]
per-ex loss: 0.468827  [   66/   88]
per-ex loss: 0.561118  [   67/   88]
per-ex loss: 0.610052  [   68/   88]
per-ex loss: 0.561589  [   69/   88]
per-ex loss: 0.799102  [   70/   88]
per-ex loss: 0.670542  [   71/   88]
per-ex loss: 0.779764  [   72/   88]
per-ex loss: 0.410189  [   73/   88]
per-ex loss: 0.718622  [   74/   88]
per-ex loss: 0.427915  [   75/   88]
per-ex loss: 0.410097  [   76/   88]
per-ex loss: 0.689249  [   77/   88]
per-ex loss: 0.418781  [   78/   88]
per-ex loss: 0.427234  [   79/   88]
per-ex loss: 0.625925  [   80/   88]
per-ex loss: 0.762145  [   81/   88]
per-ex loss: 0.645152  [   82/   88]
per-ex loss: 0.682886  [   83/   88]
per-ex loss: 0.709087  [   84/   88]
per-ex loss: 0.518499  [   85/   88]
per-ex loss: 0.419762  [   86/   88]
per-ex loss: 0.503236  [   87/   88]
per-ex loss: 0.524233  [   88/   88]
Train Error: Avg loss: 0.58452561
validation Error: 
 Avg loss: 0.61943694 
 F1: 0.424734 
 Precision: 0.417148 
 Recall: 0.432601
 IoU: 0.269627

test Error: 
 Avg loss: 0.57890931 
 F1: 0.490812 
 Precision: 0.416482 
 Recall: 0.597437
 IoU: 0.325216

We have finished training iteration 3
per-ex loss: 0.515169  [    1/   88]
per-ex loss: 0.447725  [    2/   88]
per-ex loss: 0.694772  [    3/   88]
per-ex loss: 0.447608  [    4/   88]
per-ex loss: 0.577552  [    5/   88]
per-ex loss: 0.585470  [    6/   88]
per-ex loss: 0.479974  [    7/   88]
per-ex loss: 0.588654  [    8/   88]
per-ex loss: 0.605529  [    9/   88]
per-ex loss: 0.566832  [   10/   88]
per-ex loss: 0.612951  [   11/   88]
per-ex loss: 0.453139  [   12/   88]
per-ex loss: 0.457520  [   13/   88]
per-ex loss: 0.661924  [   14/   88]
per-ex loss: 0.695083  [   15/   88]
per-ex loss: 0.611091  [   16/   88]
per-ex loss: 0.757141  [   17/   88]
per-ex loss: 0.702970  [   18/   88]
per-ex loss: 0.713977  [   19/   88]
per-ex loss: 0.662939  [   20/   88]
per-ex loss: 0.443345  [   21/   88]
per-ex loss: 0.680315  [   22/   88]
per-ex loss: 0.463455  [   23/   88]
per-ex loss: 0.491542  [   24/   88]
per-ex loss: 0.560386  [   25/   88]
per-ex loss: 0.526961  [   26/   88]
per-ex loss: 0.526112  [   27/   88]
per-ex loss: 0.659706  [   28/   88]
per-ex loss: 0.555302  [   29/   88]
per-ex loss: 0.429446  [   30/   88]
per-ex loss: 0.606078  [   31/   88]
per-ex loss: 0.736989  [   32/   88]
per-ex loss: 0.591897  [   33/   88]
per-ex loss: 0.526147  [   34/   88]
per-ex loss: 0.447530  [   35/   88]
per-ex loss: 0.674111  [   36/   88]
per-ex loss: 0.502244  [   37/   88]
per-ex loss: 0.495556  [   38/   88]
per-ex loss: 0.519930  [   39/   88]
per-ex loss: 0.654907  [   40/   88]
per-ex loss: 0.671313  [   41/   88]
per-ex loss: 0.716954  [   42/   88]
per-ex loss: 0.724515  [   43/   88]
per-ex loss: 0.472534  [   44/   88]
per-ex loss: 0.587642  [   45/   88]
per-ex loss: 0.494034  [   46/   88]
per-ex loss: 0.516695  [   47/   88]
per-ex loss: 0.690792  [   48/   88]
per-ex loss: 0.625352  [   49/   88]
per-ex loss: 0.556669  [   50/   88]
per-ex loss: 0.498597  [   51/   88]
per-ex loss: 0.635066  [   52/   88]
per-ex loss: 0.467857  [   53/   88]
per-ex loss: 0.467581  [   54/   88]
per-ex loss: 0.476240  [   55/   88]
per-ex loss: 0.432182  [   56/   88]
per-ex loss: 0.681783  [   57/   88]
per-ex loss: 0.566549  [   58/   88]
per-ex loss: 0.497568  [   59/   88]
per-ex loss: 0.701397  [   60/   88]
per-ex loss: 0.395578  [   61/   88]
per-ex loss: 0.549898  [   62/   88]
per-ex loss: 0.624199  [   63/   88]
per-ex loss: 0.720723  [   64/   88]
per-ex loss: 0.746622  [   65/   88]
per-ex loss: 0.586265  [   66/   88]
per-ex loss: 0.754662  [   67/   88]
per-ex loss: 0.378886  [   68/   88]
per-ex loss: 0.637132  [   69/   88]
per-ex loss: 0.626959  [   70/   88]
per-ex loss: 0.434599  [   71/   88]
per-ex loss: 0.512674  [   72/   88]
per-ex loss: 0.663638  [   73/   88]
per-ex loss: 0.473017  [   74/   88]
per-ex loss: 0.639156  [   75/   88]
per-ex loss: 0.399579  [   76/   88]
per-ex loss: 0.752491  [   77/   88]
per-ex loss: 0.369689  [   78/   88]
per-ex loss: 0.669667  [   79/   88]
per-ex loss: 0.440452  [   80/   88]
per-ex loss: 0.606289  [   81/   88]
per-ex loss: 0.431992  [   82/   88]
per-ex loss: 0.392793  [   83/   88]
per-ex loss: 0.706282  [   84/   88]
per-ex loss: 0.502674  [   85/   88]
per-ex loss: 0.452305  [   86/   88]
per-ex loss: 0.461167  [   87/   88]
per-ex loss: 0.411794  [   88/   88]
Train Error: Avg loss: 0.56536913
validation Error: 
 Avg loss: 0.59404730 
 F1: 0.435586 
 Precision: 0.543419 
 Recall: 0.363462
 IoU: 0.278434

test Error: 
 Avg loss: 0.54371918 
 F1: 0.519628 
 Precision: 0.570531 
 Recall: 0.477064
 IoU: 0.351012

We have finished training iteration 4
per-ex loss: 0.514598  [    1/   88]
per-ex loss: 0.725161  [    2/   88]
per-ex loss: 0.632543  [    3/   88]
per-ex loss: 0.637090  [    4/   88]
per-ex loss: 0.505508  [    5/   88]
per-ex loss: 0.425732  [    6/   88]
per-ex loss: 0.644331  [    7/   88]
per-ex loss: 0.554316  [    8/   88]
per-ex loss: 0.723320  [    9/   88]
per-ex loss: 0.408508  [   10/   88]
per-ex loss: 0.445762  [   11/   88]
per-ex loss: 0.687497  [   12/   88]
per-ex loss: 0.630347  [   13/   88]
per-ex loss: 0.615359  [   14/   88]
per-ex loss: 0.697732  [   15/   88]
per-ex loss: 0.503395  [   16/   88]
per-ex loss: 0.726039  [   17/   88]
per-ex loss: 0.524810  [   18/   88]
per-ex loss: 0.658491  [   19/   88]
per-ex loss: 0.458394  [   20/   88]
per-ex loss: 0.701923  [   21/   88]
per-ex loss: 0.673154  [   22/   88]
per-ex loss: 0.433124  [   23/   88]
per-ex loss: 0.615067  [   24/   88]
per-ex loss: 0.713161  [   25/   88]
per-ex loss: 0.463457  [   26/   88]
per-ex loss: 0.586104  [   27/   88]
per-ex loss: 0.434465  [   28/   88]
per-ex loss: 0.601258  [   29/   88]
per-ex loss: 0.429361  [   30/   88]
per-ex loss: 0.505194  [   31/   88]
per-ex loss: 0.431132  [   32/   88]
per-ex loss: 0.598965  [   33/   88]
per-ex loss: 0.354837  [   34/   88]
per-ex loss: 0.581922  [   35/   88]
per-ex loss: 0.540059  [   36/   88]
per-ex loss: 0.423234  [   37/   88]
per-ex loss: 0.381410  [   38/   88]
per-ex loss: 0.458535  [   39/   88]
per-ex loss: 0.595433  [   40/   88]
per-ex loss: 0.699694  [   41/   88]
per-ex loss: 0.424229  [   42/   88]
per-ex loss: 0.759327  [   43/   88]
per-ex loss: 0.485557  [   44/   88]
per-ex loss: 0.691944  [   45/   88]
per-ex loss: 0.532635  [   46/   88]
per-ex loss: 0.646857  [   47/   88]
per-ex loss: 0.587682  [   48/   88]
per-ex loss: 0.603754  [   49/   88]
per-ex loss: 0.467439  [   50/   88]
per-ex loss: 0.398139  [   51/   88]
per-ex loss: 0.397522  [   52/   88]
per-ex loss: 0.425307  [   53/   88]
per-ex loss: 0.462538  [   54/   88]
per-ex loss: 0.657071  [   55/   88]
per-ex loss: 0.534339  [   56/   88]
per-ex loss: 0.664533  [   57/   88]
per-ex loss: 0.609437  [   58/   88]
per-ex loss: 0.599300  [   59/   88]
per-ex loss: 0.551420  [   60/   88]
per-ex loss: 0.743380  [   61/   88]
per-ex loss: 0.512679  [   62/   88]
per-ex loss: 0.494697  [   63/   88]
per-ex loss: 0.446987  [   64/   88]
per-ex loss: 0.508748  [   65/   88]
per-ex loss: 0.584202  [   66/   88]
per-ex loss: 0.681957  [   67/   88]
per-ex loss: 0.636180  [   68/   88]
per-ex loss: 0.463829  [   69/   88]
per-ex loss: 0.443706  [   70/   88]
per-ex loss: 0.674323  [   71/   88]
per-ex loss: 0.683144  [   72/   88]
per-ex loss: 0.547420  [   73/   88]
per-ex loss: 0.442774  [   74/   88]
per-ex loss: 0.660751  [   75/   88]
per-ex loss: 0.409891  [   76/   88]
per-ex loss: 0.655223  [   77/   88]
per-ex loss: 0.657392  [   78/   88]
per-ex loss: 0.586751  [   79/   88]
per-ex loss: 0.503135  [   80/   88]
per-ex loss: 0.431379  [   81/   88]
per-ex loss: 0.521201  [   82/   88]
per-ex loss: 0.459598  [   83/   88]
per-ex loss: 0.554731  [   84/   88]
per-ex loss: 0.417821  [   85/   88]
per-ex loss: 0.747988  [   86/   88]
per-ex loss: 0.443175  [   87/   88]
per-ex loss: 0.678358  [   88/   88]
Train Error: Avg loss: 0.55716903
validation Error: 
 Avg loss: 0.59317291 
 F1: 0.442331 
 Precision: 0.410149 
 Recall: 0.479994
 IoU: 0.283970

test Error: 
 Avg loss: 0.52539103 
 F1: 0.537225 
 Precision: 0.503986 
 Recall: 0.575157
 IoU: 0.367264

We have finished training iteration 5
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_1_.pth
per-ex loss: 0.688679  [    1/   88]
per-ex loss: 0.570446  [    2/   88]
per-ex loss: 0.745728  [    3/   88]
per-ex loss: 0.693892  [    4/   88]
per-ex loss: 0.463380  [    5/   88]
per-ex loss: 0.645117  [    6/   88]
per-ex loss: 0.634226  [    7/   88]
per-ex loss: 0.447248  [    8/   88]
per-ex loss: 0.512899  [    9/   88]
per-ex loss: 0.676749  [   10/   88]
per-ex loss: 0.548358  [   11/   88]
per-ex loss: 0.744788  [   12/   88]
per-ex loss: 0.662847  [   13/   88]
per-ex loss: 0.642416  [   14/   88]
per-ex loss: 0.507327  [   15/   88]
per-ex loss: 0.447330  [   16/   88]
per-ex loss: 0.707739  [   17/   88]
per-ex loss: 0.691494  [   18/   88]
per-ex loss: 0.606655  [   19/   88]
per-ex loss: 0.415262  [   20/   88]
per-ex loss: 0.582663  [   21/   88]
per-ex loss: 0.470185  [   22/   88]
per-ex loss: 0.535640  [   23/   88]
per-ex loss: 0.440623  [   24/   88]
per-ex loss: 0.506739  [   25/   88]
per-ex loss: 0.405342  [   26/   88]
per-ex loss: 0.495646  [   27/   88]
per-ex loss: 0.463101  [   28/   88]
per-ex loss: 0.402032  [   29/   88]
per-ex loss: 0.710954  [   30/   88]
per-ex loss: 0.493905  [   31/   88]
per-ex loss: 0.554383  [   32/   88]
per-ex loss: 0.612887  [   33/   88]
per-ex loss: 0.700883  [   34/   88]
per-ex loss: 0.685618  [   35/   88]
per-ex loss: 0.420204  [   36/   88]
per-ex loss: 0.402956  [   37/   88]
per-ex loss: 0.486735  [   38/   88]
per-ex loss: 0.383519  [   39/   88]
per-ex loss: 0.405003  [   40/   88]
per-ex loss: 0.494931  [   41/   88]
per-ex loss: 0.766048  [   42/   88]
per-ex loss: 0.373454  [   43/   88]
per-ex loss: 0.454632  [   44/   88]
per-ex loss: 0.472481  [   45/   88]
per-ex loss: 0.470623  [   46/   88]
per-ex loss: 0.732661  [   47/   88]
per-ex loss: 0.529016  [   48/   88]
per-ex loss: 0.408683  [   49/   88]
per-ex loss: 0.484897  [   50/   88]
per-ex loss: 0.673339  [   51/   88]
per-ex loss: 0.406050  [   52/   88]
per-ex loss: 0.642890  [   53/   88]
per-ex loss: 0.569735  [   54/   88]
per-ex loss: 0.505516  [   55/   88]
per-ex loss: 0.653196  [   56/   88]
per-ex loss: 0.607563  [   57/   88]
per-ex loss: 0.617345  [   58/   88]
per-ex loss: 0.660710  [   59/   88]
per-ex loss: 0.420991  [   60/   88]
per-ex loss: 0.429580  [   61/   88]
per-ex loss: 0.416842  [   62/   88]
per-ex loss: 0.416975  [   63/   88]
per-ex loss: 0.560582  [   64/   88]
per-ex loss: 0.525033  [   65/   88]
per-ex loss: 0.597457  [   66/   88]
per-ex loss: 0.504338  [   67/   88]
per-ex loss: 0.604936  [   68/   88]
per-ex loss: 0.530904  [   69/   88]
per-ex loss: 0.656181  [   70/   88]
per-ex loss: 0.622277  [   71/   88]
per-ex loss: 0.626190  [   72/   88]
per-ex loss: 0.507207  [   73/   88]
per-ex loss: 0.601161  [   74/   88]
per-ex loss: 0.480161  [   75/   88]
per-ex loss: 0.506027  [   76/   88]
per-ex loss: 0.473416  [   77/   88]
per-ex loss: 0.539810  [   78/   88]
per-ex loss: 0.687977  [   79/   88]
per-ex loss: 0.417461  [   80/   88]
per-ex loss: 0.605979  [   81/   88]
per-ex loss: 0.696601  [   82/   88]
per-ex loss: 0.604726  [   83/   88]
per-ex loss: 0.513460  [   84/   88]
per-ex loss: 0.456352  [   85/   88]
per-ex loss: 0.490735  [   86/   88]
per-ex loss: 0.515450  [   87/   88]
per-ex loss: 0.748090  [   88/   88]
Train Error: Avg loss: 0.55104851
validation Error: 
 Avg loss: 0.60757958 
 F1: 0.412632 
 Precision: 0.691520 
 Recall: 0.294045
 IoU: 0.259948

test Error: 
 Avg loss: 0.55019955 
 F1: 0.505177 
 Precision: 0.702332 
 Recall: 0.394450
 IoU: 0.337951

We have finished training iteration 6
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_2_.pth
per-ex loss: 0.398310  [    1/   88]
per-ex loss: 0.550972  [    2/   88]
per-ex loss: 0.777470  [    3/   88]
per-ex loss: 0.642479  [    4/   88]
per-ex loss: 0.537045  [    5/   88]
per-ex loss: 0.439645  [    6/   88]
per-ex loss: 0.564282  [    7/   88]
per-ex loss: 0.555121  [    8/   88]
per-ex loss: 0.494334  [    9/   88]
per-ex loss: 0.609747  [   10/   88]
per-ex loss: 0.608870  [   11/   88]
per-ex loss: 0.369604  [   12/   88]
per-ex loss: 0.387599  [   13/   88]
per-ex loss: 0.413777  [   14/   88]
per-ex loss: 0.477487  [   15/   88]
per-ex loss: 0.565992  [   16/   88]
per-ex loss: 0.463810  [   17/   88]
per-ex loss: 0.416647  [   18/   88]
per-ex loss: 0.578851  [   19/   88]
per-ex loss: 0.526915  [   20/   88]
per-ex loss: 0.489701  [   21/   88]
per-ex loss: 0.465177  [   22/   88]
per-ex loss: 0.700720  [   23/   88]
per-ex loss: 0.493238  [   24/   88]
per-ex loss: 0.407786  [   25/   88]
per-ex loss: 0.638645  [   26/   88]
per-ex loss: 0.417222  [   27/   88]
per-ex loss: 0.461844  [   28/   88]
per-ex loss: 0.426907  [   29/   88]
per-ex loss: 0.648759  [   30/   88]
per-ex loss: 0.752050  [   31/   88]
per-ex loss: 0.511861  [   32/   88]
per-ex loss: 0.467244  [   33/   88]
per-ex loss: 0.431263  [   34/   88]
per-ex loss: 0.608576  [   35/   88]
per-ex loss: 0.629620  [   36/   88]
per-ex loss: 0.428939  [   37/   88]
per-ex loss: 0.654362  [   38/   88]
per-ex loss: 0.618917  [   39/   88]
per-ex loss: 0.481106  [   40/   88]
per-ex loss: 0.543159  [   41/   88]
per-ex loss: 0.446883  [   42/   88]
per-ex loss: 0.679143  [   43/   88]
per-ex loss: 0.659552  [   44/   88]
per-ex loss: 0.440548  [   45/   88]
per-ex loss: 0.734365  [   46/   88]
per-ex loss: 0.606670  [   47/   88]
per-ex loss: 0.599201  [   48/   88]
per-ex loss: 0.622770  [   49/   88]
per-ex loss: 0.665569  [   50/   88]
per-ex loss: 0.468682  [   51/   88]
per-ex loss: 0.521925  [   52/   88]
per-ex loss: 0.611285  [   53/   88]
per-ex loss: 0.650036  [   54/   88]
per-ex loss: 0.446565  [   55/   88]
per-ex loss: 0.530763  [   56/   88]
per-ex loss: 0.462099  [   57/   88]
per-ex loss: 0.512763  [   58/   88]
per-ex loss: 0.670090  [   59/   88]
per-ex loss: 0.681464  [   60/   88]
per-ex loss: 0.443544  [   61/   88]
per-ex loss: 0.398532  [   62/   88]
per-ex loss: 0.466763  [   63/   88]
per-ex loss: 0.687691  [   64/   88]
per-ex loss: 0.419981  [   65/   88]
per-ex loss: 0.492577  [   66/   88]
per-ex loss: 0.715165  [   67/   88]
per-ex loss: 0.752534  [   68/   88]
per-ex loss: 0.703901  [   69/   88]
per-ex loss: 0.440886  [   70/   88]
per-ex loss: 0.645019  [   71/   88]
per-ex loss: 0.598675  [   72/   88]
per-ex loss: 0.614585  [   73/   88]
per-ex loss: 0.588647  [   74/   88]
per-ex loss: 0.652330  [   75/   88]
per-ex loss: 0.774773  [   76/   88]
per-ex loss: 0.601457  [   77/   88]
per-ex loss: 0.700982  [   78/   88]
per-ex loss: 0.597457  [   79/   88]
per-ex loss: 0.427445  [   80/   88]
per-ex loss: 0.566745  [   81/   88]
per-ex loss: 0.396256  [   82/   88]
per-ex loss: 0.560155  [   83/   88]
per-ex loss: 0.460593  [   84/   88]
per-ex loss: 0.433699  [   85/   88]
per-ex loss: 0.712486  [   86/   88]
per-ex loss: 0.509315  [   87/   88]
per-ex loss: 0.421348  [   88/   88]
Train Error: Avg loss: 0.55056779
validation Error: 
 Avg loss: 0.58182930 
 F1: 0.448380 
 Precision: 0.441724 
 Recall: 0.455239
 IoU: 0.288975

test Error: 
 Avg loss: 0.52317055 
 F1: 0.533655 
 Precision: 0.504900 
 Recall: 0.565883
 IoU: 0.363936

We have finished training iteration 7
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_3_.pth
per-ex loss: 0.600267  [    1/   88]
per-ex loss: 0.652919  [    2/   88]
per-ex loss: 0.489013  [    3/   88]
per-ex loss: 0.549857  [    4/   88]
per-ex loss: 0.497812  [    5/   88]
per-ex loss: 0.602621  [    6/   88]
per-ex loss: 0.581920  [    7/   88]
per-ex loss: 0.725653  [    8/   88]
per-ex loss: 0.413908  [    9/   88]
per-ex loss: 0.491348  [   10/   88]
per-ex loss: 0.552805  [   11/   88]
per-ex loss: 0.529216  [   12/   88]
per-ex loss: 0.655323  [   13/   88]
per-ex loss: 0.629251  [   14/   88]
per-ex loss: 0.422558  [   15/   88]
per-ex loss: 0.464823  [   16/   88]
per-ex loss: 0.578421  [   17/   88]
per-ex loss: 0.426211  [   18/   88]
per-ex loss: 0.611343  [   19/   88]
per-ex loss: 0.712595  [   20/   88]
per-ex loss: 0.709910  [   21/   88]
per-ex loss: 0.446306  [   22/   88]
per-ex loss: 0.717196  [   23/   88]
per-ex loss: 0.573836  [   24/   88]
per-ex loss: 0.519689  [   25/   88]
per-ex loss: 0.439958  [   26/   88]
per-ex loss: 0.478079  [   27/   88]
per-ex loss: 0.531268  [   28/   88]
per-ex loss: 0.596661  [   29/   88]
per-ex loss: 0.665752  [   30/   88]
per-ex loss: 0.603786  [   31/   88]
per-ex loss: 0.604109  [   32/   88]
per-ex loss: 0.688618  [   33/   88]
per-ex loss: 0.615222  [   34/   88]
per-ex loss: 0.518681  [   35/   88]
per-ex loss: 0.539472  [   36/   88]
per-ex loss: 0.639844  [   37/   88]
per-ex loss: 0.684570  [   38/   88]
per-ex loss: 0.419611  [   39/   88]
per-ex loss: 0.699635  [   40/   88]
per-ex loss: 0.507756  [   41/   88]
per-ex loss: 0.405780  [   42/   88]
per-ex loss: 0.420251  [   43/   88]
per-ex loss: 0.620721  [   44/   88]
per-ex loss: 0.591053  [   45/   88]
per-ex loss: 0.453420  [   46/   88]
per-ex loss: 0.430791  [   47/   88]
per-ex loss: 0.426561  [   48/   88]
per-ex loss: 0.550654  [   49/   88]
per-ex loss: 0.636466  [   50/   88]
per-ex loss: 0.395567  [   51/   88]
per-ex loss: 0.497971  [   52/   88]
per-ex loss: 0.628444  [   53/   88]
per-ex loss: 0.672931  [   54/   88]
per-ex loss: 0.490313  [   55/   88]
per-ex loss: 0.596455  [   56/   88]
per-ex loss: 0.372976  [   57/   88]
per-ex loss: 0.488989  [   58/   88]
per-ex loss: 0.445218  [   59/   88]
per-ex loss: 0.413261  [   60/   88]
per-ex loss: 0.629909  [   61/   88]
per-ex loss: 0.382695  [   62/   88]
per-ex loss: 0.416750  [   63/   88]
per-ex loss: 0.533780  [   64/   88]
per-ex loss: 0.444876  [   65/   88]
per-ex loss: 0.403184  [   66/   88]
per-ex loss: 0.504493  [   67/   88]
per-ex loss: 0.399315  [   68/   88]
per-ex loss: 0.442889  [   69/   88]
per-ex loss: 0.652614  [   70/   88]
per-ex loss: 0.663375  [   71/   88]
per-ex loss: 0.397650  [   72/   88]
per-ex loss: 0.347090  [   73/   88]
per-ex loss: 0.763978  [   74/   88]
per-ex loss: 0.741808  [   75/   88]
per-ex loss: 0.739934  [   76/   88]
per-ex loss: 0.626853  [   77/   88]
per-ex loss: 0.648163  [   78/   88]
per-ex loss: 0.469719  [   79/   88]
per-ex loss: 0.557251  [   80/   88]
per-ex loss: 0.488871  [   81/   88]
per-ex loss: 0.425621  [   82/   88]
per-ex loss: 0.467747  [   83/   88]
per-ex loss: 0.441670  [   84/   88]
per-ex loss: 0.531493  [   85/   88]
per-ex loss: 0.679879  [   86/   88]
per-ex loss: 0.657774  [   87/   88]
per-ex loss: 0.431052  [   88/   88]
Train Error: Avg loss: 0.54336451
validation Error: 
 Avg loss: 0.61397020 
 F1: 0.415684 
 Precision: 0.402059 
 Recall: 0.430265
 IoU: 0.262374

test Error: 
 Avg loss: 0.52443202 
 F1: 0.531601 
 Precision: 0.537204 
 Recall: 0.526114
 IoU: 0.362028

We have finished training iteration 8
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_6_.pth
per-ex loss: 0.499822  [    1/   88]
per-ex loss: 0.717144  [    2/   88]
per-ex loss: 0.606605  [    3/   88]
per-ex loss: 0.611658  [    4/   88]
per-ex loss: 0.455317  [    5/   88]
per-ex loss: 0.721589  [    6/   88]
per-ex loss: 0.592749  [    7/   88]
per-ex loss: 0.486824  [    8/   88]
per-ex loss: 0.390654  [    9/   88]
per-ex loss: 0.596232  [   10/   88]
per-ex loss: 0.471673  [   11/   88]
per-ex loss: 0.459164  [   12/   88]
per-ex loss: 0.609414  [   13/   88]
per-ex loss: 0.658551  [   14/   88]
per-ex loss: 0.392572  [   15/   88]
per-ex loss: 0.498874  [   16/   88]
per-ex loss: 0.417232  [   17/   88]
per-ex loss: 0.399017  [   18/   88]
per-ex loss: 0.401911  [   19/   88]
per-ex loss: 0.375060  [   20/   88]
per-ex loss: 0.494878  [   21/   88]
per-ex loss: 0.441855  [   22/   88]
per-ex loss: 0.434102  [   23/   88]
per-ex loss: 0.482212  [   24/   88]
per-ex loss: 0.350108  [   25/   88]
per-ex loss: 0.634376  [   26/   88]
per-ex loss: 0.514718  [   27/   88]
per-ex loss: 0.708445  [   28/   88]
per-ex loss: 0.700280  [   29/   88]
per-ex loss: 0.442215  [   30/   88]
per-ex loss: 0.752470  [   31/   88]
per-ex loss: 0.421999  [   32/   88]
per-ex loss: 0.774360  [   33/   88]
per-ex loss: 0.351606  [   34/   88]
per-ex loss: 0.473094  [   35/   88]
per-ex loss: 0.382500  [   36/   88]
per-ex loss: 0.646756  [   37/   88]
per-ex loss: 0.683537  [   38/   88]
per-ex loss: 0.521463  [   39/   88]
per-ex loss: 0.474506  [   40/   88]
per-ex loss: 0.638836  [   41/   88]
per-ex loss: 0.621622  [   42/   88]
per-ex loss: 0.473644  [   43/   88]
per-ex loss: 0.534431  [   44/   88]
per-ex loss: 0.435282  [   45/   88]
per-ex loss: 0.461726  [   46/   88]
per-ex loss: 0.604458  [   47/   88]
per-ex loss: 0.561256  [   48/   88]
per-ex loss: 0.482579  [   49/   88]
per-ex loss: 0.689635  [   50/   88]
per-ex loss: 0.653023  [   51/   88]
per-ex loss: 0.390628  [   52/   88]
per-ex loss: 0.687038  [   53/   88]
per-ex loss: 0.732833  [   54/   88]
per-ex loss: 0.578832  [   55/   88]
per-ex loss: 0.502619  [   56/   88]
per-ex loss: 0.432700  [   57/   88]
per-ex loss: 0.456784  [   58/   88]
per-ex loss: 0.637967  [   59/   88]
per-ex loss: 0.591680  [   60/   88]
per-ex loss: 0.574021  [   61/   88]
per-ex loss: 0.416595  [   62/   88]
per-ex loss: 0.375991  [   63/   88]
per-ex loss: 0.414458  [   64/   88]
per-ex loss: 0.598489  [   65/   88]
per-ex loss: 0.589358  [   66/   88]
per-ex loss: 0.703502  [   67/   88]
per-ex loss: 0.425932  [   68/   88]
per-ex loss: 0.465531  [   69/   88]
per-ex loss: 0.659839  [   70/   88]
per-ex loss: 0.406790  [   71/   88]
per-ex loss: 0.612022  [   72/   88]
per-ex loss: 0.672627  [   73/   88]
per-ex loss: 0.441503  [   74/   88]
per-ex loss: 0.564960  [   75/   88]
per-ex loss: 0.558262  [   76/   88]
per-ex loss: 0.423331  [   77/   88]
per-ex loss: 0.624254  [   78/   88]
per-ex loss: 0.528404  [   79/   88]
per-ex loss: 0.680485  [   80/   88]
per-ex loss: 0.480977  [   81/   88]
per-ex loss: 0.613459  [   82/   88]
per-ex loss: 0.754753  [   83/   88]
per-ex loss: 0.470888  [   84/   88]
per-ex loss: 0.548357  [   85/   88]
per-ex loss: 0.495680  [   86/   88]
per-ex loss: 0.716484  [   87/   88]
per-ex loss: 0.417509  [   88/   88]
Train Error: Avg loss: 0.53919975
validation Error: 
 Avg loss: 0.69887233 
 F1: 0.312985 
 Precision: 0.220784 
 Recall: 0.537413
 IoU: 0.185526

test Error: 
 Avg loss: 0.59826168 
 F1: 0.426499 
 Precision: 0.324974 
 Recall: 0.620282
 IoU: 0.271051

We have finished training iteration 9
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_4_.pth
per-ex loss: 0.689836  [    1/   88]
per-ex loss: 0.484776  [    2/   88]
per-ex loss: 0.436870  [    3/   88]
per-ex loss: 0.532580  [    4/   88]
per-ex loss: 0.595727  [    5/   88]
per-ex loss: 0.479444  [    6/   88]
per-ex loss: 0.630255  [    7/   88]
per-ex loss: 0.438189  [    8/   88]
per-ex loss: 0.655015  [    9/   88]
per-ex loss: 0.607125  [   10/   88]
per-ex loss: 0.490723  [   11/   88]
per-ex loss: 0.427714  [   12/   88]
per-ex loss: 0.606445  [   13/   88]
per-ex loss: 0.521237  [   14/   88]
per-ex loss: 0.460688  [   15/   88]
per-ex loss: 0.521736  [   16/   88]
per-ex loss: 0.385384  [   17/   88]
per-ex loss: 0.600920  [   18/   88]
per-ex loss: 0.631626  [   19/   88]
per-ex loss: 0.591466  [   20/   88]
per-ex loss: 0.445297  [   21/   88]
per-ex loss: 0.743665  [   22/   88]
per-ex loss: 0.685705  [   23/   88]
per-ex loss: 0.715550  [   24/   88]
per-ex loss: 0.721884  [   25/   88]
per-ex loss: 0.391488  [   26/   88]
per-ex loss: 0.410691  [   27/   88]
per-ex loss: 0.565937  [   28/   88]
per-ex loss: 0.565011  [   29/   88]
per-ex loss: 0.610061  [   30/   88]
per-ex loss: 0.479817  [   31/   88]
per-ex loss: 0.432270  [   32/   88]
per-ex loss: 0.713504  [   33/   88]
per-ex loss: 0.373493  [   34/   88]
per-ex loss: 0.552423  [   35/   88]
per-ex loss: 0.639709  [   36/   88]
per-ex loss: 0.580266  [   37/   88]
per-ex loss: 0.429258  [   38/   88]
per-ex loss: 0.399135  [   39/   88]
per-ex loss: 0.435566  [   40/   88]
per-ex loss: 0.445404  [   41/   88]
per-ex loss: 0.433379  [   42/   88]
per-ex loss: 0.692111  [   43/   88]
per-ex loss: 0.466898  [   44/   88]
per-ex loss: 0.406480  [   45/   88]
per-ex loss: 0.495508  [   46/   88]
per-ex loss: 0.602277  [   47/   88]
per-ex loss: 0.530025  [   48/   88]
per-ex loss: 0.708944  [   49/   88]
per-ex loss: 0.501128  [   50/   88]
per-ex loss: 0.472442  [   51/   88]
per-ex loss: 0.434949  [   52/   88]
per-ex loss: 0.594378  [   53/   88]
per-ex loss: 0.712990  [   54/   88]
per-ex loss: 0.594910  [   55/   88]
per-ex loss: 0.512671  [   56/   88]
per-ex loss: 0.423581  [   57/   88]
per-ex loss: 0.391361  [   58/   88]
per-ex loss: 0.517916  [   59/   88]
per-ex loss: 0.631490  [   60/   88]
per-ex loss: 0.666617  [   61/   88]
per-ex loss: 0.718579  [   62/   88]
per-ex loss: 0.626150  [   63/   88]
per-ex loss: 0.463989  [   64/   88]
per-ex loss: 0.422482  [   65/   88]
per-ex loss: 0.642352  [   66/   88]
per-ex loss: 0.571968  [   67/   88]
per-ex loss: 0.484878  [   68/   88]
per-ex loss: 0.440520  [   69/   88]
per-ex loss: 0.492616  [   70/   88]
per-ex loss: 0.526036  [   71/   88]
per-ex loss: 0.398694  [   72/   88]
per-ex loss: 0.519349  [   73/   88]
per-ex loss: 0.441359  [   74/   88]
per-ex loss: 0.585826  [   75/   88]
per-ex loss: 0.624970  [   76/   88]
per-ex loss: 0.606437  [   77/   88]
per-ex loss: 0.656822  [   78/   88]
per-ex loss: 0.430471  [   79/   88]
per-ex loss: 0.365633  [   80/   88]
per-ex loss: 0.688605  [   81/   88]
per-ex loss: 0.412247  [   82/   88]
per-ex loss: 0.438611  [   83/   88]
per-ex loss: 0.655112  [   84/   88]
per-ex loss: 0.510954  [   85/   88]
per-ex loss: 0.702807  [   86/   88]
per-ex loss: 0.432339  [   87/   88]
per-ex loss: 0.540560  [   88/   88]
Train Error: Avg loss: 0.53766266
validation Error: 
 Avg loss: 0.56391710 
 F1: 0.467773 
 Precision: 0.554396 
 Recall: 0.404562
 IoU: 0.305290

test Error: 
 Avg loss: 0.51495769 
 F1: 0.544393 
 Precision: 0.597204 
 Recall: 0.500164
 IoU: 0.373997

We have finished training iteration 10
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_8_.pth
per-ex loss: 0.368905  [    1/   88]
per-ex loss: 0.388724  [    2/   88]
per-ex loss: 0.390987  [    3/   88]
per-ex loss: 0.457725  [    4/   88]
per-ex loss: 0.452008  [    5/   88]
per-ex loss: 0.387902  [    6/   88]
per-ex loss: 0.485311  [    7/   88]
per-ex loss: 0.491651  [    8/   88]
per-ex loss: 0.586983  [    9/   88]
per-ex loss: 0.544824  [   10/   88]
per-ex loss: 0.635327  [   11/   88]
per-ex loss: 0.565797  [   12/   88]
per-ex loss: 0.419152  [   13/   88]
per-ex loss: 0.621637  [   14/   88]
per-ex loss: 0.407007  [   15/   88]
per-ex loss: 0.411097  [   16/   88]
per-ex loss: 0.414661  [   17/   88]
per-ex loss: 0.428359  [   18/   88]
per-ex loss: 0.723861  [   19/   88]
per-ex loss: 0.730515  [   20/   88]
per-ex loss: 0.454646  [   21/   88]
per-ex loss: 0.420987  [   22/   88]
per-ex loss: 0.356825  [   23/   88]
per-ex loss: 0.438287  [   24/   88]
per-ex loss: 0.710935  [   25/   88]
per-ex loss: 0.624693  [   26/   88]
per-ex loss: 0.489830  [   27/   88]
per-ex loss: 0.475560  [   28/   88]
per-ex loss: 0.630766  [   29/   88]
per-ex loss: 0.663901  [   30/   88]
per-ex loss: 0.611972  [   31/   88]
per-ex loss: 0.576619  [   32/   88]
per-ex loss: 0.535630  [   33/   88]
per-ex loss: 0.649157  [   34/   88]
per-ex loss: 0.682627  [   35/   88]
per-ex loss: 0.505945  [   36/   88]
per-ex loss: 0.653790  [   37/   88]
per-ex loss: 0.492308  [   38/   88]
per-ex loss: 0.679850  [   39/   88]
per-ex loss: 0.447142  [   40/   88]
per-ex loss: 0.491919  [   41/   88]
per-ex loss: 0.466316  [   42/   88]
per-ex loss: 0.668361  [   43/   88]
per-ex loss: 0.738079  [   44/   88]
per-ex loss: 0.642677  [   45/   88]
per-ex loss: 0.597924  [   46/   88]
per-ex loss: 0.498811  [   47/   88]
per-ex loss: 0.660456  [   48/   88]
per-ex loss: 0.486292  [   49/   88]
per-ex loss: 0.486033  [   50/   88]
per-ex loss: 0.705927  [   51/   88]
per-ex loss: 0.616572  [   52/   88]
per-ex loss: 0.660100  [   53/   88]
per-ex loss: 0.574630  [   54/   88]
per-ex loss: 0.637343  [   55/   88]
per-ex loss: 0.420960  [   56/   88]
per-ex loss: 0.596721  [   57/   88]
per-ex loss: 0.604731  [   58/   88]
per-ex loss: 0.447490  [   59/   88]
per-ex loss: 0.417772  [   60/   88]
per-ex loss: 0.477406  [   61/   88]
per-ex loss: 0.497024  [   62/   88]
per-ex loss: 0.769937  [   63/   88]
per-ex loss: 0.486923  [   64/   88]
per-ex loss: 0.435073  [   65/   88]
per-ex loss: 0.378036  [   66/   88]
per-ex loss: 0.464996  [   67/   88]
per-ex loss: 0.483263  [   68/   88]
per-ex loss: 0.589260  [   69/   88]
per-ex loss: 0.405884  [   70/   88]
per-ex loss: 0.368319  [   71/   88]
per-ex loss: 0.474830  [   72/   88]
per-ex loss: 0.443135  [   73/   88]
per-ex loss: 0.554224  [   74/   88]
per-ex loss: 0.510300  [   75/   88]
per-ex loss: 0.431355  [   76/   88]
per-ex loss: 0.492295  [   77/   88]
per-ex loss: 0.700576  [   78/   88]
per-ex loss: 0.538785  [   79/   88]
per-ex loss: 0.655699  [   80/   88]
per-ex loss: 0.608209  [   81/   88]
per-ex loss: 0.678831  [   82/   88]
per-ex loss: 0.601649  [   83/   88]
per-ex loss: 0.444696  [   84/   88]
per-ex loss: 0.430719  [   85/   88]
per-ex loss: 0.535535  [   86/   88]
per-ex loss: 0.643932  [   87/   88]
per-ex loss: 0.484891  [   88/   88]
Train Error: Avg loss: 0.53430454
validation Error: 
 Avg loss: 0.58196574 
 F1: 0.456145 
 Precision: 0.425619 
 Recall: 0.491387
 IoU: 0.295458

test Error: 
 Avg loss: 0.51045114 
 F1: 0.543599 
 Precision: 0.515901 
 Recall: 0.574440
 IoU: 0.373248

We have finished training iteration 11
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_9_.pth
per-ex loss: 0.630715  [    1/   88]
per-ex loss: 0.449440  [    2/   88]
per-ex loss: 0.639521  [    3/   88]
per-ex loss: 0.429170  [    4/   88]
per-ex loss: 0.701021  [    5/   88]
per-ex loss: 0.683281  [    6/   88]
per-ex loss: 0.625625  [    7/   88]
per-ex loss: 0.408986  [    8/   88]
per-ex loss: 0.507564  [    9/   88]
per-ex loss: 0.494282  [   10/   88]
per-ex loss: 0.470530  [   11/   88]
per-ex loss: 0.403380  [   12/   88]
per-ex loss: 0.448388  [   13/   88]
per-ex loss: 0.479062  [   14/   88]
per-ex loss: 0.438961  [   15/   88]
per-ex loss: 0.698551  [   16/   88]
per-ex loss: 0.507866  [   17/   88]
per-ex loss: 0.407953  [   18/   88]
per-ex loss: 0.595273  [   19/   88]
per-ex loss: 0.547879  [   20/   88]
per-ex loss: 0.665267  [   21/   88]
per-ex loss: 0.592037  [   22/   88]
per-ex loss: 0.724681  [   23/   88]
per-ex loss: 0.635439  [   24/   88]
per-ex loss: 0.465626  [   25/   88]
per-ex loss: 0.481926  [   26/   88]
per-ex loss: 0.586741  [   27/   88]
per-ex loss: 0.426019  [   28/   88]
per-ex loss: 0.502891  [   29/   88]
per-ex loss: 0.742607  [   30/   88]
per-ex loss: 0.392973  [   31/   88]
per-ex loss: 0.589944  [   32/   88]
per-ex loss: 0.673182  [   33/   88]
per-ex loss: 0.466836  [   34/   88]
per-ex loss: 0.563505  [   35/   88]
per-ex loss: 0.571185  [   36/   88]
per-ex loss: 0.568540  [   37/   88]
per-ex loss: 0.432953  [   38/   88]
per-ex loss: 0.631265  [   39/   88]
per-ex loss: 0.422622  [   40/   88]
per-ex loss: 0.428181  [   41/   88]
per-ex loss: 0.354421  [   42/   88]
per-ex loss: 0.397979  [   43/   88]
per-ex loss: 0.480093  [   44/   88]
per-ex loss: 0.469414  [   45/   88]
per-ex loss: 0.689161  [   46/   88]
per-ex loss: 0.419443  [   47/   88]
per-ex loss: 0.642127  [   48/   88]
per-ex loss: 0.451838  [   49/   88]
per-ex loss: 0.496295  [   50/   88]
per-ex loss: 0.584552  [   51/   88]
per-ex loss: 0.372730  [   52/   88]
per-ex loss: 0.420375  [   53/   88]
per-ex loss: 0.650420  [   54/   88]
per-ex loss: 0.544199  [   55/   88]
per-ex loss: 0.716441  [   56/   88]
per-ex loss: 0.553235  [   57/   88]
per-ex loss: 0.486690  [   58/   88]
per-ex loss: 0.389618  [   59/   88]
per-ex loss: 0.447260  [   60/   88]
per-ex loss: 0.414707  [   61/   88]
per-ex loss: 0.620277  [   62/   88]
per-ex loss: 0.572979  [   63/   88]
per-ex loss: 0.374403  [   64/   88]
per-ex loss: 0.475417  [   65/   88]
per-ex loss: 0.487594  [   66/   88]
per-ex loss: 0.608679  [   67/   88]
per-ex loss: 0.714722  [   68/   88]
per-ex loss: 0.387902  [   69/   88]
per-ex loss: 0.631833  [   70/   88]
per-ex loss: 0.616530  [   71/   88]
per-ex loss: 0.579128  [   72/   88]
per-ex loss: 0.463828  [   73/   88]
per-ex loss: 0.497388  [   74/   88]
per-ex loss: 0.704691  [   75/   88]
per-ex loss: 0.429466  [   76/   88]
per-ex loss: 0.484914  [   77/   88]
per-ex loss: 0.668973  [   78/   88]
per-ex loss: 0.602069  [   79/   88]
per-ex loss: 0.410749  [   80/   88]
per-ex loss: 0.645219  [   81/   88]
per-ex loss: 0.444369  [   82/   88]
per-ex loss: 0.424184  [   83/   88]
per-ex loss: 0.597614  [   84/   88]
per-ex loss: 0.430765  [   85/   88]
per-ex loss: 0.481623  [   86/   88]
per-ex loss: 0.640497  [   87/   88]
per-ex loss: 0.648806  [   88/   88]
Train Error: Avg loss: 0.53133509
validation Error: 
 Avg loss: 0.56933683 
 F1: 0.471437 
 Precision: 0.506104 
 Recall: 0.441215
 IoU: 0.308418

test Error: 
 Avg loss: 0.50123419 
 F1: 0.557882 
 Precision: 0.588642 
 Recall: 0.530177
 IoU: 0.386849

We have finished training iteration 12
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_5_.pth
per-ex loss: 0.485816  [    1/   88]
per-ex loss: 0.445079  [    2/   88]
per-ex loss: 0.608786  [    3/   88]
per-ex loss: 0.612921  [    4/   88]
per-ex loss: 0.409647  [    5/   88]
per-ex loss: 0.581502  [    6/   88]
per-ex loss: 0.613416  [    7/   88]
per-ex loss: 0.703651  [    8/   88]
per-ex loss: 0.591149  [    9/   88]
per-ex loss: 0.415183  [   10/   88]
per-ex loss: 0.720580  [   11/   88]
per-ex loss: 0.634737  [   12/   88]
per-ex loss: 0.520815  [   13/   88]
per-ex loss: 0.604881  [   14/   88]
per-ex loss: 0.445422  [   15/   88]
per-ex loss: 0.486322  [   16/   88]
per-ex loss: 0.518165  [   17/   88]
per-ex loss: 0.641801  [   18/   88]
per-ex loss: 0.511955  [   19/   88]
per-ex loss: 0.495057  [   20/   88]
per-ex loss: 0.431880  [   21/   88]
per-ex loss: 0.618975  [   22/   88]
per-ex loss: 0.557519  [   23/   88]
per-ex loss: 0.437076  [   24/   88]
per-ex loss: 0.645364  [   25/   88]
per-ex loss: 0.645521  [   26/   88]
per-ex loss: 0.428663  [   27/   88]
per-ex loss: 0.424259  [   28/   88]
per-ex loss: 0.364308  [   29/   88]
per-ex loss: 0.417865  [   30/   88]
per-ex loss: 0.576716  [   31/   88]
per-ex loss: 0.447079  [   32/   88]
per-ex loss: 0.560163  [   33/   88]
per-ex loss: 0.381179  [   34/   88]
per-ex loss: 0.573985  [   35/   88]
per-ex loss: 0.533914  [   36/   88]
per-ex loss: 0.377817  [   37/   88]
per-ex loss: 0.576060  [   38/   88]
per-ex loss: 0.644392  [   39/   88]
per-ex loss: 0.612354  [   40/   88]
per-ex loss: 0.466757  [   41/   88]
per-ex loss: 0.725485  [   42/   88]
per-ex loss: 0.391834  [   43/   88]
per-ex loss: 0.391063  [   44/   88]
per-ex loss: 0.493893  [   45/   88]
per-ex loss: 0.410528  [   46/   88]
per-ex loss: 0.696874  [   47/   88]
per-ex loss: 0.547674  [   48/   88]
per-ex loss: 0.607445  [   49/   88]
per-ex loss: 0.413643  [   50/   88]
per-ex loss: 0.348714  [   51/   88]
per-ex loss: 0.459567  [   52/   88]
per-ex loss: 0.686247  [   53/   88]
per-ex loss: 0.494723  [   54/   88]
per-ex loss: 0.420081  [   55/   88]
per-ex loss: 0.493862  [   56/   88]
per-ex loss: 0.555122  [   57/   88]
per-ex loss: 0.622009  [   58/   88]
per-ex loss: 0.337334  [   59/   88]
per-ex loss: 0.426914  [   60/   88]
per-ex loss: 0.460680  [   61/   88]
per-ex loss: 0.447409  [   62/   88]
per-ex loss: 0.373014  [   63/   88]
per-ex loss: 0.390033  [   64/   88]
per-ex loss: 0.641407  [   65/   88]
per-ex loss: 0.523380  [   66/   88]
per-ex loss: 0.603302  [   67/   88]
per-ex loss: 0.389244  [   68/   88]
per-ex loss: 0.653386  [   69/   88]
per-ex loss: 0.448312  [   70/   88]
per-ex loss: 0.541488  [   71/   88]
per-ex loss: 0.598923  [   72/   88]
per-ex loss: 0.692159  [   73/   88]
per-ex loss: 0.442168  [   74/   88]
per-ex loss: 0.422374  [   75/   88]
per-ex loss: 0.472575  [   76/   88]
per-ex loss: 0.437850  [   77/   88]
per-ex loss: 0.674138  [   78/   88]
per-ex loss: 0.644392  [   79/   88]
per-ex loss: 0.603965  [   80/   88]
per-ex loss: 0.515198  [   81/   88]
per-ex loss: 0.479541  [   82/   88]
per-ex loss: 0.579193  [   83/   88]
per-ex loss: 0.686866  [   84/   88]
per-ex loss: 0.589378  [   85/   88]
per-ex loss: 0.397325  [   86/   88]
per-ex loss: 0.624006  [   87/   88]
per-ex loss: 0.696095  [   88/   88]
Train Error: Avg loss: 0.52635843
validation Error: 
 Avg loss: 0.56003854 
 F1: 0.470387 
 Precision: 0.543681 
 Recall: 0.414507
 IoU: 0.307520

test Error: 
 Avg loss: 0.51060916 
 F1: 0.546735 
 Precision: 0.587258 
 Recall: 0.511443
 IoU: 0.376211

We have finished training iteration 13
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_11_.pth
per-ex loss: 0.353824  [    1/   88]
per-ex loss: 0.483799  [    2/   88]
per-ex loss: 0.419937  [    3/   88]
per-ex loss: 0.371596  [    4/   88]
per-ex loss: 0.493797  [    5/   88]
per-ex loss: 0.443361  [    6/   88]
per-ex loss: 0.372559  [    7/   88]
per-ex loss: 0.432353  [    8/   88]
per-ex loss: 0.606302  [    9/   88]
per-ex loss: 0.786398  [   10/   88]
per-ex loss: 0.675918  [   11/   88]
per-ex loss: 0.758658  [   12/   88]
per-ex loss: 0.690839  [   13/   88]
per-ex loss: 0.666873  [   14/   88]
per-ex loss: 0.650074  [   15/   88]
per-ex loss: 0.403220  [   16/   88]
per-ex loss: 0.570646  [   17/   88]
per-ex loss: 0.364140  [   18/   88]
per-ex loss: 0.474696  [   19/   88]
per-ex loss: 0.391754  [   20/   88]
per-ex loss: 0.698872  [   21/   88]
per-ex loss: 0.670729  [   22/   88]
per-ex loss: 0.696762  [   23/   88]
per-ex loss: 0.440158  [   24/   88]
per-ex loss: 0.685752  [   25/   88]
per-ex loss: 0.651074  [   26/   88]
per-ex loss: 0.602962  [   27/   88]
per-ex loss: 0.576131  [   28/   88]
per-ex loss: 0.601134  [   29/   88]
per-ex loss: 0.691797  [   30/   88]
per-ex loss: 0.411959  [   31/   88]
per-ex loss: 0.552825  [   32/   88]
per-ex loss: 0.593946  [   33/   88]
per-ex loss: 0.442460  [   34/   88]
per-ex loss: 0.430711  [   35/   88]
per-ex loss: 0.674528  [   36/   88]
per-ex loss: 0.474593  [   37/   88]
per-ex loss: 0.473788  [   38/   88]
per-ex loss: 0.657873  [   39/   88]
per-ex loss: 0.481371  [   40/   88]
per-ex loss: 0.481870  [   41/   88]
per-ex loss: 0.709172  [   42/   88]
per-ex loss: 0.438294  [   43/   88]
per-ex loss: 0.398989  [   44/   88]
per-ex loss: 0.720412  [   45/   88]
per-ex loss: 0.434504  [   46/   88]
per-ex loss: 0.559031  [   47/   88]
per-ex loss: 0.554609  [   48/   88]
per-ex loss: 0.398306  [   49/   88]
per-ex loss: 0.431977  [   50/   88]
per-ex loss: 0.588289  [   51/   88]
per-ex loss: 0.470686  [   52/   88]
per-ex loss: 0.632891  [   53/   88]
per-ex loss: 0.539687  [   54/   88]
per-ex loss: 0.416301  [   55/   88]
per-ex loss: 0.419904  [   56/   88]
per-ex loss: 0.563834  [   57/   88]
per-ex loss: 0.543531  [   58/   88]
per-ex loss: 0.390086  [   59/   88]
per-ex loss: 0.577198  [   60/   88]
per-ex loss: 0.396949  [   61/   88]
per-ex loss: 0.370317  [   62/   88]
per-ex loss: 0.502166  [   63/   88]
per-ex loss: 0.581468  [   64/   88]
per-ex loss: 0.407190  [   65/   88]
per-ex loss: 0.447999  [   66/   88]
per-ex loss: 0.420592  [   67/   88]
per-ex loss: 0.440740  [   68/   88]
per-ex loss: 0.550175  [   69/   88]
per-ex loss: 0.671857  [   70/   88]
per-ex loss: 0.443265  [   71/   88]
per-ex loss: 0.461313  [   72/   88]
per-ex loss: 0.485934  [   73/   88]
per-ex loss: 0.481132  [   74/   88]
per-ex loss: 0.409589  [   75/   88]
per-ex loss: 0.472552  [   76/   88]
per-ex loss: 0.616991  [   77/   88]
per-ex loss: 0.598780  [   78/   88]
per-ex loss: 0.630855  [   79/   88]
per-ex loss: 0.526554  [   80/   88]
per-ex loss: 0.494883  [   81/   88]
per-ex loss: 0.415678  [   82/   88]
per-ex loss: 0.461130  [   83/   88]
per-ex loss: 0.556346  [   84/   88]
per-ex loss: 0.682515  [   85/   88]
per-ex loss: 0.593092  [   86/   88]
per-ex loss: 0.621692  [   87/   88]
per-ex loss: 0.600827  [   88/   88]
Train Error: Avg loss: 0.52877643
validation Error: 
 Avg loss: 0.56624877 
 F1: 0.471445 
 Precision: 0.450811 
 Recall: 0.494060
 IoU: 0.308426

test Error: 
 Avg loss: 0.50587860 
 F1: 0.549302 
 Precision: 0.511805 
 Recall: 0.592728
 IoU: 0.378647

We have finished training iteration 14
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_7_.pth
per-ex loss: 0.482140  [    1/   88]
per-ex loss: 0.413492  [    2/   88]
per-ex loss: 0.618264  [    3/   88]
per-ex loss: 0.463205  [    4/   88]
per-ex loss: 0.462161  [    5/   88]
per-ex loss: 0.596545  [    6/   88]
per-ex loss: 0.604543  [    7/   88]
per-ex loss: 0.669478  [    8/   88]
per-ex loss: 0.565177  [    9/   88]
per-ex loss: 0.467973  [   10/   88]
per-ex loss: 0.660740  [   11/   88]
per-ex loss: 0.619417  [   12/   88]
per-ex loss: 0.594735  [   13/   88]
per-ex loss: 0.474338  [   14/   88]
per-ex loss: 0.685293  [   15/   88]
per-ex loss: 0.575644  [   16/   88]
per-ex loss: 0.602042  [   17/   88]
per-ex loss: 0.461253  [   18/   88]
per-ex loss: 0.560343  [   19/   88]
per-ex loss: 0.703990  [   20/   88]
per-ex loss: 0.488323  [   21/   88]
per-ex loss: 0.406600  [   22/   88]
per-ex loss: 0.403869  [   23/   88]
per-ex loss: 0.469901  [   24/   88]
per-ex loss: 0.420299  [   25/   88]
per-ex loss: 0.475630  [   26/   88]
per-ex loss: 0.453004  [   27/   88]
per-ex loss: 0.417425  [   28/   88]
per-ex loss: 0.464780  [   29/   88]
per-ex loss: 0.452398  [   30/   88]
per-ex loss: 0.597407  [   31/   88]
per-ex loss: 0.438481  [   32/   88]
per-ex loss: 0.570948  [   33/   88]
per-ex loss: 0.479303  [   34/   88]
per-ex loss: 0.436206  [   35/   88]
per-ex loss: 0.615453  [   36/   88]
per-ex loss: 0.656945  [   37/   88]
per-ex loss: 0.730833  [   38/   88]
per-ex loss: 0.610908  [   39/   88]
per-ex loss: 0.449310  [   40/   88]
per-ex loss: 0.387729  [   41/   88]
per-ex loss: 0.406115  [   42/   88]
per-ex loss: 0.400626  [   43/   88]
per-ex loss: 0.513489  [   44/   88]
per-ex loss: 0.723656  [   45/   88]
per-ex loss: 0.517484  [   46/   88]
per-ex loss: 0.422609  [   47/   88]
per-ex loss: 0.412315  [   48/   88]
per-ex loss: 0.435422  [   49/   88]
per-ex loss: 0.476224  [   50/   88]
per-ex loss: 0.671022  [   51/   88]
per-ex loss: 0.343915  [   52/   88]
per-ex loss: 0.463068  [   53/   88]
per-ex loss: 0.709153  [   54/   88]
per-ex loss: 0.718392  [   55/   88]
per-ex loss: 0.533032  [   56/   88]
per-ex loss: 0.406561  [   57/   88]
per-ex loss: 0.377100  [   58/   88]
per-ex loss: 0.657974  [   59/   88]
per-ex loss: 0.609360  [   60/   88]
per-ex loss: 0.416161  [   61/   88]
per-ex loss: 0.660900  [   62/   88]
per-ex loss: 0.593246  [   63/   88]
per-ex loss: 0.498278  [   64/   88]
per-ex loss: 0.638301  [   65/   88]
per-ex loss: 0.497093  [   66/   88]
per-ex loss: 0.617670  [   67/   88]
per-ex loss: 0.407043  [   68/   88]
per-ex loss: 0.637314  [   69/   88]
per-ex loss: 0.370614  [   70/   88]
per-ex loss: 0.601835  [   71/   88]
per-ex loss: 0.573241  [   72/   88]
per-ex loss: 0.442255  [   73/   88]
per-ex loss: 0.388914  [   74/   88]
per-ex loss: 0.630269  [   75/   88]
per-ex loss: 0.518310  [   76/   88]
per-ex loss: 0.689121  [   77/   88]
per-ex loss: 0.454711  [   78/   88]
per-ex loss: 0.410078  [   79/   88]
per-ex loss: 0.472549  [   80/   88]
per-ex loss: 0.531308  [   81/   88]
per-ex loss: 0.556331  [   82/   88]
per-ex loss: 0.554171  [   83/   88]
per-ex loss: 0.424250  [   84/   88]
per-ex loss: 0.386200  [   85/   88]
per-ex loss: 0.685887  [   86/   88]
per-ex loss: 0.580888  [   87/   88]
per-ex loss: 0.686022  [   88/   88]
Train Error: Avg loss: 0.52757958
validation Error: 
 Avg loss: 0.57143781 
 F1: 0.462849 
 Precision: 0.473546 
 Recall: 0.452625
 IoU: 0.301108

test Error: 
 Avg loss: 0.50271035 
 F1: 0.555290 
 Precision: 0.571990 
 Recall: 0.539537
 IoU: 0.384360

We have finished training iteration 15
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_12_.pth
per-ex loss: 0.477672  [    1/   88]
per-ex loss: 0.700347  [    2/   88]
per-ex loss: 0.702694  [    3/   88]
per-ex loss: 0.431979  [    4/   88]
per-ex loss: 0.428806  [    5/   88]
per-ex loss: 0.467665  [    6/   88]
per-ex loss: 0.662242  [    7/   88]
per-ex loss: 0.592005  [    8/   88]
per-ex loss: 0.465103  [    9/   88]
per-ex loss: 0.577563  [   10/   88]
per-ex loss: 0.569833  [   11/   88]
per-ex loss: 0.373355  [   12/   88]
per-ex loss: 0.533478  [   13/   88]
per-ex loss: 0.495816  [   14/   88]
per-ex loss: 0.598431  [   15/   88]
per-ex loss: 0.425978  [   16/   88]
per-ex loss: 0.421235  [   17/   88]
per-ex loss: 0.381281  [   18/   88]
per-ex loss: 0.704483  [   19/   88]
per-ex loss: 0.421637  [   20/   88]
per-ex loss: 0.598198  [   21/   88]
per-ex loss: 0.395258  [   22/   88]
per-ex loss: 0.389800  [   23/   88]
per-ex loss: 0.661440  [   24/   88]
per-ex loss: 0.497610  [   25/   88]
per-ex loss: 0.382526  [   26/   88]
per-ex loss: 0.556136  [   27/   88]
per-ex loss: 0.661735  [   28/   88]
per-ex loss: 0.575369  [   29/   88]
per-ex loss: 0.659296  [   30/   88]
per-ex loss: 0.393445  [   31/   88]
per-ex loss: 0.395583  [   32/   88]
per-ex loss: 0.370581  [   33/   88]
per-ex loss: 0.502780  [   34/   88]
per-ex loss: 0.681822  [   35/   88]
per-ex loss: 0.472428  [   36/   88]
per-ex loss: 0.442164  [   37/   88]
per-ex loss: 0.402469  [   38/   88]
per-ex loss: 0.650214  [   39/   88]
per-ex loss: 0.355787  [   40/   88]
per-ex loss: 0.423276  [   41/   88]
per-ex loss: 0.576034  [   42/   88]
per-ex loss: 0.411170  [   43/   88]
per-ex loss: 0.696589  [   44/   88]
per-ex loss: 0.382035  [   45/   88]
per-ex loss: 0.481016  [   46/   88]
per-ex loss: 0.549541  [   47/   88]
per-ex loss: 0.562810  [   48/   88]
per-ex loss: 0.592505  [   49/   88]
per-ex loss: 0.478190  [   50/   88]
per-ex loss: 0.659233  [   51/   88]
per-ex loss: 0.621900  [   52/   88]
per-ex loss: 0.490518  [   53/   88]
per-ex loss: 0.406516  [   54/   88]
per-ex loss: 0.460874  [   55/   88]
per-ex loss: 0.708220  [   56/   88]
per-ex loss: 0.449643  [   57/   88]
per-ex loss: 0.441501  [   58/   88]
per-ex loss: 0.452601  [   59/   88]
per-ex loss: 0.580837  [   60/   88]
per-ex loss: 0.446988  [   61/   88]
per-ex loss: 0.739274  [   62/   88]
per-ex loss: 0.626265  [   63/   88]
per-ex loss: 0.397407  [   64/   88]
per-ex loss: 0.669311  [   65/   88]
per-ex loss: 0.706963  [   66/   88]
per-ex loss: 0.577453  [   67/   88]
per-ex loss: 0.616316  [   68/   88]
per-ex loss: 0.411207  [   69/   88]
per-ex loss: 0.455748  [   70/   88]
per-ex loss: 0.617194  [   71/   88]
per-ex loss: 0.626069  [   72/   88]
per-ex loss: 0.606537  [   73/   88]
per-ex loss: 0.503331  [   74/   88]
per-ex loss: 0.603249  [   75/   88]
per-ex loss: 0.648436  [   76/   88]
per-ex loss: 0.457976  [   77/   88]
per-ex loss: 0.450370  [   78/   88]
per-ex loss: 0.623227  [   79/   88]
per-ex loss: 0.529575  [   80/   88]
per-ex loss: 0.478024  [   81/   88]
per-ex loss: 0.476311  [   82/   88]
per-ex loss: 0.360237  [   83/   88]
per-ex loss: 0.460657  [   84/   88]
per-ex loss: 0.575979  [   85/   88]
per-ex loss: 0.436214  [   86/   88]
per-ex loss: 0.476048  [   87/   88]
per-ex loss: 0.432150  [   88/   88]
Train Error: Avg loss: 0.52170182
validation Error: 
 Avg loss: 0.55771872 
 F1: 0.471200 
 Precision: 0.530273 
 Recall: 0.423970
 IoU: 0.308216

test Error: 
 Avg loss: 0.50910363 
 F1: 0.547764 
 Precision: 0.580217 
 Recall: 0.518749
 IoU: 0.377187

We have finished training iteration 16
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_14_.pth
per-ex loss: 0.608081  [    1/   88]
per-ex loss: 0.604385  [    2/   88]
per-ex loss: 0.697007  [    3/   88]
per-ex loss: 0.479376  [    4/   88]
per-ex loss: 0.621867  [    5/   88]
per-ex loss: 0.584132  [    6/   88]
per-ex loss: 0.646449  [    7/   88]
per-ex loss: 0.410079  [    8/   88]
per-ex loss: 0.559639  [    9/   88]
per-ex loss: 0.628352  [   10/   88]
per-ex loss: 0.429962  [   11/   88]
per-ex loss: 0.386271  [   12/   88]
per-ex loss: 0.462250  [   13/   88]
per-ex loss: 0.423301  [   14/   88]
per-ex loss: 0.481591  [   15/   88]
per-ex loss: 0.603621  [   16/   88]
per-ex loss: 0.481324  [   17/   88]
per-ex loss: 0.480799  [   18/   88]
per-ex loss: 0.602955  [   19/   88]
per-ex loss: 0.626099  [   20/   88]
per-ex loss: 0.674335  [   21/   88]
per-ex loss: 0.408949  [   22/   88]
per-ex loss: 0.623418  [   23/   88]
per-ex loss: 0.351531  [   24/   88]
per-ex loss: 0.526419  [   25/   88]
per-ex loss: 0.613036  [   26/   88]
per-ex loss: 0.406080  [   27/   88]
per-ex loss: 0.705777  [   28/   88]
per-ex loss: 0.574124  [   29/   88]
per-ex loss: 0.570743  [   30/   88]
per-ex loss: 0.444402  [   31/   88]
per-ex loss: 0.564149  [   32/   88]
per-ex loss: 0.646882  [   33/   88]
per-ex loss: 0.406336  [   34/   88]
per-ex loss: 0.455053  [   35/   88]
per-ex loss: 0.715820  [   36/   88]
per-ex loss: 0.364567  [   37/   88]
per-ex loss: 0.347640  [   38/   88]
per-ex loss: 0.690745  [   39/   88]
per-ex loss: 0.437033  [   40/   88]
per-ex loss: 0.476283  [   41/   88]
per-ex loss: 0.632692  [   42/   88]
per-ex loss: 0.485205  [   43/   88]
per-ex loss: 0.436746  [   44/   88]
per-ex loss: 0.414785  [   45/   88]
per-ex loss: 0.734556  [   46/   88]
per-ex loss: 0.674979  [   47/   88]
per-ex loss: 0.373687  [   48/   88]
per-ex loss: 0.401376  [   49/   88]
per-ex loss: 0.545515  [   50/   88]
per-ex loss: 0.409994  [   51/   88]
per-ex loss: 0.465250  [   52/   88]
per-ex loss: 0.382263  [   53/   88]
per-ex loss: 0.486764  [   54/   88]
per-ex loss: 0.417523  [   55/   88]
per-ex loss: 0.544761  [   56/   88]
per-ex loss: 0.347338  [   57/   88]
per-ex loss: 0.440495  [   58/   88]
per-ex loss: 0.438686  [   59/   88]
per-ex loss: 0.469974  [   60/   88]
per-ex loss: 0.726103  [   61/   88]
per-ex loss: 0.539308  [   62/   88]
per-ex loss: 0.396613  [   63/   88]
per-ex loss: 0.587495  [   64/   88]
per-ex loss: 0.687585  [   65/   88]
per-ex loss: 0.435107  [   66/   88]
per-ex loss: 0.672557  [   67/   88]
per-ex loss: 0.411958  [   68/   88]
per-ex loss: 0.543442  [   69/   88]
per-ex loss: 0.443529  [   70/   88]
per-ex loss: 0.621619  [   71/   88]
per-ex loss: 0.693393  [   72/   88]
per-ex loss: 0.711792  [   73/   88]
per-ex loss: 0.494310  [   74/   88]
per-ex loss: 0.457385  [   75/   88]
per-ex loss: 0.471311  [   76/   88]
per-ex loss: 0.618730  [   77/   88]
per-ex loss: 0.402458  [   78/   88]
per-ex loss: 0.478789  [   79/   88]
per-ex loss: 0.485275  [   80/   88]
per-ex loss: 0.422759  [   81/   88]
per-ex loss: 0.385180  [   82/   88]
per-ex loss: 0.485309  [   83/   88]
per-ex loss: 0.598699  [   84/   88]
per-ex loss: 0.611071  [   85/   88]
per-ex loss: 0.487392  [   86/   88]
per-ex loss: 0.579333  [   87/   88]
per-ex loss: 0.584556  [   88/   88]
Train Error: Avg loss: 0.52223310
validation Error: 
 Avg loss: 0.55630989 
 F1: 0.471492 
 Precision: 0.587769 
 Recall: 0.393623
 IoU: 0.308466

test Error: 
 Avg loss: 0.50596798 
 F1: 0.551780 
 Precision: 0.620383 
 Recall: 0.496839
 IoU: 0.381006

We have finished training iteration 17
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_15_.pth
per-ex loss: 0.375638  [    1/   88]
per-ex loss: 0.472483  [    2/   88]
per-ex loss: 0.486225  [    3/   88]
per-ex loss: 0.617910  [    4/   88]
per-ex loss: 0.656929  [    5/   88]
per-ex loss: 0.454507  [    6/   88]
per-ex loss: 0.653567  [    7/   88]
per-ex loss: 0.613198  [    8/   88]
per-ex loss: 0.724610  [    9/   88]
per-ex loss: 0.542776  [   10/   88]
per-ex loss: 0.624525  [   11/   88]
per-ex loss: 0.418890  [   12/   88]
per-ex loss: 0.433997  [   13/   88]
per-ex loss: 0.407690  [   14/   88]
per-ex loss: 0.646982  [   15/   88]
per-ex loss: 0.536803  [   16/   88]
per-ex loss: 0.562729  [   17/   88]
per-ex loss: 0.643210  [   18/   88]
per-ex loss: 0.425407  [   19/   88]
per-ex loss: 0.661558  [   20/   88]
per-ex loss: 0.649864  [   21/   88]
per-ex loss: 0.733923  [   22/   88]
per-ex loss: 0.686650  [   23/   88]
per-ex loss: 0.555916  [   24/   88]
per-ex loss: 0.505050  [   25/   88]
per-ex loss: 0.419939  [   26/   88]
per-ex loss: 0.603939  [   27/   88]
per-ex loss: 0.646448  [   28/   88]
per-ex loss: 0.567564  [   29/   88]
per-ex loss: 0.440980  [   30/   88]
per-ex loss: 0.355862  [   31/   88]
per-ex loss: 0.576835  [   32/   88]
per-ex loss: 0.375088  [   33/   88]
per-ex loss: 0.547265  [   34/   88]
per-ex loss: 0.417036  [   35/   88]
per-ex loss: 0.408283  [   36/   88]
per-ex loss: 0.410481  [   37/   88]
per-ex loss: 0.415596  [   38/   88]
per-ex loss: 0.375927  [   39/   88]
per-ex loss: 0.710034  [   40/   88]
per-ex loss: 0.344981  [   41/   88]
per-ex loss: 0.471629  [   42/   88]
per-ex loss: 0.651326  [   43/   88]
per-ex loss: 0.360427  [   44/   88]
per-ex loss: 0.415860  [   45/   88]
per-ex loss: 0.416035  [   46/   88]
per-ex loss: 0.582903  [   47/   88]
per-ex loss: 0.597969  [   48/   88]
per-ex loss: 0.558929  [   49/   88]
per-ex loss: 0.466736  [   50/   88]
per-ex loss: 0.593785  [   51/   88]
per-ex loss: 0.323529  [   52/   88]
per-ex loss: 0.495249  [   53/   88]
per-ex loss: 0.401773  [   54/   88]
per-ex loss: 0.446549  [   55/   88]
per-ex loss: 0.623650  [   56/   88]
per-ex loss: 0.404013  [   57/   88]
per-ex loss: 0.367439  [   58/   88]
per-ex loss: 0.566586  [   59/   88]
per-ex loss: 0.613674  [   60/   88]
per-ex loss: 0.468182  [   61/   88]
per-ex loss: 0.640149  [   62/   88]
per-ex loss: 0.695544  [   63/   88]
per-ex loss: 0.447964  [   64/   88]
per-ex loss: 0.505830  [   65/   88]
per-ex loss: 0.638865  [   66/   88]
per-ex loss: 0.695371  [   67/   88]
per-ex loss: 0.546368  [   68/   88]
per-ex loss: 0.425566  [   69/   88]
per-ex loss: 0.559517  [   70/   88]
per-ex loss: 0.712864  [   71/   88]
per-ex loss: 0.489722  [   72/   88]
per-ex loss: 0.537167  [   73/   88]
per-ex loss: 0.447433  [   74/   88]
per-ex loss: 0.475509  [   75/   88]
per-ex loss: 0.492295  [   76/   88]
per-ex loss: 0.427282  [   77/   88]
per-ex loss: 0.594876  [   78/   88]
per-ex loss: 0.419235  [   79/   88]
per-ex loss: 0.478336  [   80/   88]
per-ex loss: 0.405367  [   81/   88]
per-ex loss: 0.668223  [   82/   88]
per-ex loss: 0.535451  [   83/   88]
per-ex loss: 0.423866  [   84/   88]
per-ex loss: 0.649325  [   85/   88]
per-ex loss: 0.458891  [   86/   88]
per-ex loss: 0.574904  [   87/   88]
per-ex loss: 0.631502  [   88/   88]
Train Error: Avg loss: 0.52398818
validation Error: 
 Avg loss: 0.58633710 
 F1: 0.436779 
 Precision: 0.522955 
 Recall: 0.374986
 IoU: 0.279410

test Error: 
 Avg loss: 0.52389545 
 F1: 0.527191 
 Precision: 0.613777 
 Recall: 0.462014
 IoU: 0.357950

We have finished training iteration 18
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_10_.pth
per-ex loss: 0.611849  [    1/   88]
per-ex loss: 0.691860  [    2/   88]
per-ex loss: 0.603821  [    3/   88]
per-ex loss: 0.464937  [    4/   88]
per-ex loss: 0.404564  [    5/   88]
per-ex loss: 0.462054  [    6/   88]
per-ex loss: 0.694516  [    7/   88]
per-ex loss: 0.367863  [    8/   88]
per-ex loss: 0.562573  [    9/   88]
per-ex loss: 0.444588  [   10/   88]
per-ex loss: 0.428383  [   11/   88]
per-ex loss: 0.428806  [   12/   88]
per-ex loss: 0.425746  [   13/   88]
per-ex loss: 0.647373  [   14/   88]
per-ex loss: 0.570090  [   15/   88]
per-ex loss: 0.469931  [   16/   88]
per-ex loss: 0.560784  [   17/   88]
per-ex loss: 0.399928  [   18/   88]
per-ex loss: 0.470321  [   19/   88]
per-ex loss: 0.649193  [   20/   88]
per-ex loss: 0.479601  [   21/   88]
per-ex loss: 0.583767  [   22/   88]
per-ex loss: 0.439061  [   23/   88]
per-ex loss: 0.450147  [   24/   88]
per-ex loss: 0.465210  [   25/   88]
per-ex loss: 0.676913  [   26/   88]
per-ex loss: 0.547936  [   27/   88]
per-ex loss: 0.391285  [   28/   88]
per-ex loss: 0.487778  [   29/   88]
per-ex loss: 0.382760  [   30/   88]
per-ex loss: 0.580644  [   31/   88]
per-ex loss: 0.397818  [   32/   88]
per-ex loss: 0.470545  [   33/   88]
per-ex loss: 0.644962  [   34/   88]
per-ex loss: 0.608769  [   35/   88]
per-ex loss: 0.554138  [   36/   88]
per-ex loss: 0.492296  [   37/   88]
per-ex loss: 0.697510  [   38/   88]
per-ex loss: 0.381896  [   39/   88]
per-ex loss: 0.704254  [   40/   88]
per-ex loss: 0.573537  [   41/   88]
per-ex loss: 0.548325  [   42/   88]
per-ex loss: 0.548658  [   43/   88]
per-ex loss: 0.513411  [   44/   88]
per-ex loss: 0.510275  [   45/   88]
per-ex loss: 0.631434  [   46/   88]
per-ex loss: 0.356081  [   47/   88]
per-ex loss: 0.430047  [   48/   88]
per-ex loss: 0.572707  [   49/   88]
per-ex loss: 0.393918  [   50/   88]
per-ex loss: 0.470455  [   51/   88]
per-ex loss: 0.393187  [   52/   88]
per-ex loss: 0.602700  [   53/   88]
per-ex loss: 0.419211  [   54/   88]
per-ex loss: 0.384345  [   55/   88]
per-ex loss: 0.710748  [   56/   88]
per-ex loss: 0.429710  [   57/   88]
per-ex loss: 0.470381  [   58/   88]
per-ex loss: 0.703261  [   59/   88]
per-ex loss: 0.440862  [   60/   88]
per-ex loss: 0.543516  [   61/   88]
per-ex loss: 0.424906  [   62/   88]
per-ex loss: 0.393915  [   63/   88]
per-ex loss: 0.646526  [   64/   88]
per-ex loss: 0.539007  [   65/   88]
per-ex loss: 0.416197  [   66/   88]
per-ex loss: 0.698406  [   67/   88]
per-ex loss: 0.410665  [   68/   88]
per-ex loss: 0.629656  [   69/   88]
per-ex loss: 0.611801  [   70/   88]
per-ex loss: 0.539984  [   71/   88]
per-ex loss: 0.607010  [   72/   88]
per-ex loss: 0.626455  [   73/   88]
per-ex loss: 0.579807  [   74/   88]
per-ex loss: 0.390834  [   75/   88]
per-ex loss: 0.479730  [   76/   88]
per-ex loss: 0.412902  [   77/   88]
per-ex loss: 0.381055  [   78/   88]
per-ex loss: 0.469252  [   79/   88]
per-ex loss: 0.379880  [   80/   88]
per-ex loss: 0.624408  [   81/   88]
per-ex loss: 0.645069  [   82/   88]
per-ex loss: 0.423957  [   83/   88]
per-ex loss: 0.642601  [   84/   88]
per-ex loss: 0.582206  [   85/   88]
per-ex loss: 0.728381  [   86/   88]
per-ex loss: 0.374222  [   87/   88]
per-ex loss: 0.371767  [   88/   88]
Train Error: Avg loss: 0.51674815
validation Error: 
 Avg loss: 0.56551514 
 F1: 0.456333 
 Precision: 0.530799 
 Recall: 0.400191
 IoU: 0.295617

test Error: 
 Avg loss: 0.50731666 
 F1: 0.543430 
 Precision: 0.587451 
 Recall: 0.505547
 IoU: 0.373089

We have finished training iteration 19
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_13_.pth
per-ex loss: 0.457798  [    1/   88]
per-ex loss: 0.562851  [    2/   88]
per-ex loss: 0.538284  [    3/   88]
per-ex loss: 0.670359  [    4/   88]
per-ex loss: 0.683359  [    5/   88]
per-ex loss: 0.646291  [    6/   88]
per-ex loss: 0.461291  [    7/   88]
per-ex loss: 0.572215  [    8/   88]
per-ex loss: 0.679972  [    9/   88]
per-ex loss: 0.483062  [   10/   88]
per-ex loss: 0.493620  [   11/   88]
per-ex loss: 0.381448  [   12/   88]
per-ex loss: 0.500349  [   13/   88]
per-ex loss: 0.604522  [   14/   88]
per-ex loss: 0.504727  [   15/   88]
per-ex loss: 0.398858  [   16/   88]
per-ex loss: 0.676028  [   17/   88]
per-ex loss: 0.396944  [   18/   88]
per-ex loss: 0.558427  [   19/   88]
per-ex loss: 0.409217  [   20/   88]
per-ex loss: 0.410743  [   21/   88]
per-ex loss: 0.628269  [   22/   88]
per-ex loss: 0.465459  [   23/   88]
per-ex loss: 0.425506  [   24/   88]
per-ex loss: 0.622172  [   25/   88]
per-ex loss: 0.399300  [   26/   88]
per-ex loss: 0.450354  [   27/   88]
per-ex loss: 0.597820  [   28/   88]
per-ex loss: 0.427100  [   29/   88]
per-ex loss: 0.434119  [   30/   88]
per-ex loss: 0.435203  [   31/   88]
per-ex loss: 0.718445  [   32/   88]
per-ex loss: 0.639198  [   33/   88]
per-ex loss: 0.664957  [   34/   88]
per-ex loss: 0.464560  [   35/   88]
per-ex loss: 0.634561  [   36/   88]
per-ex loss: 0.601771  [   37/   88]
per-ex loss: 0.457767  [   38/   88]
per-ex loss: 0.638642  [   39/   88]
per-ex loss: 0.403411  [   40/   88]
per-ex loss: 0.427506  [   41/   88]
per-ex loss: 0.341983  [   42/   88]
per-ex loss: 0.604118  [   43/   88]
per-ex loss: 0.394849  [   44/   88]
per-ex loss: 0.699609  [   45/   88]
per-ex loss: 0.589683  [   46/   88]
per-ex loss: 0.398771  [   47/   88]
per-ex loss: 0.520253  [   48/   88]
per-ex loss: 0.454485  [   49/   88]
per-ex loss: 0.420699  [   50/   88]
per-ex loss: 0.499230  [   51/   88]
per-ex loss: 0.638114  [   52/   88]
per-ex loss: 0.661753  [   53/   88]
per-ex loss: 0.541652  [   54/   88]
per-ex loss: 0.555244  [   55/   88]
per-ex loss: 0.430590  [   56/   88]
per-ex loss: 0.582115  [   57/   88]
per-ex loss: 0.415088  [   58/   88]
per-ex loss: 0.496716  [   59/   88]
per-ex loss: 0.458985  [   60/   88]
per-ex loss: 0.697396  [   61/   88]
per-ex loss: 0.552891  [   62/   88]
per-ex loss: 0.534129  [   63/   88]
per-ex loss: 0.442398  [   64/   88]
per-ex loss: 0.449065  [   65/   88]
per-ex loss: 0.361872  [   66/   88]
per-ex loss: 0.552161  [   67/   88]
per-ex loss: 0.631987  [   68/   88]
per-ex loss: 0.401174  [   69/   88]
per-ex loss: 0.378621  [   70/   88]
per-ex loss: 0.446657  [   71/   88]
per-ex loss: 0.436426  [   72/   88]
per-ex loss: 0.416038  [   73/   88]
per-ex loss: 0.648688  [   74/   88]
per-ex loss: 0.642441  [   75/   88]
per-ex loss: 0.420738  [   76/   88]
per-ex loss: 0.368936  [   77/   88]
per-ex loss: 0.586384  [   78/   88]
per-ex loss: 0.363105  [   79/   88]
per-ex loss: 0.585712  [   80/   88]
per-ex loss: 0.390811  [   81/   88]
per-ex loss: 0.629041  [   82/   88]
per-ex loss: 0.473104  [   83/   88]
per-ex loss: 0.702719  [   84/   88]
per-ex loss: 0.448840  [   85/   88]
per-ex loss: 0.423283  [   86/   88]
per-ex loss: 0.361049  [   87/   88]
per-ex loss: 0.566853  [   88/   88]
Train Error: Avg loss: 0.51412433
validation Error: 
 Avg loss: 0.55921317 
 F1: 0.475999 
 Precision: 0.497164 
 Recall: 0.456563
 IoU: 0.312335

test Error: 
 Avg loss: 0.49507834 
 F1: 0.562462 
 Precision: 0.591526 
 Recall: 0.536121
 IoU: 0.391268

We have finished training iteration 20
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_18_.pth
per-ex loss: 0.438822  [    1/   88]
per-ex loss: 0.422378  [    2/   88]
per-ex loss: 0.571776  [    3/   88]
per-ex loss: 0.418434  [    4/   88]
per-ex loss: 0.631889  [    5/   88]
per-ex loss: 0.389909  [    6/   88]
per-ex loss: 0.410167  [    7/   88]
per-ex loss: 0.389662  [    8/   88]
per-ex loss: 0.311892  [    9/   88]
per-ex loss: 0.586778  [   10/   88]
per-ex loss: 0.385386  [   11/   88]
per-ex loss: 0.339558  [   12/   88]
per-ex loss: 0.481217  [   13/   88]
per-ex loss: 0.372375  [   14/   88]
per-ex loss: 0.565010  [   15/   88]
per-ex loss: 0.363484  [   16/   88]
per-ex loss: 0.382720  [   17/   88]
per-ex loss: 0.706028  [   18/   88]
per-ex loss: 0.592646  [   19/   88]
per-ex loss: 0.418405  [   20/   88]
per-ex loss: 0.479395  [   21/   88]
per-ex loss: 0.395497  [   22/   88]
per-ex loss: 0.478842  [   23/   88]
per-ex loss: 0.468354  [   24/   88]
per-ex loss: 0.665848  [   25/   88]
per-ex loss: 0.425641  [   26/   88]
per-ex loss: 0.613746  [   27/   88]
per-ex loss: 0.565470  [   28/   88]
per-ex loss: 0.596881  [   29/   88]
per-ex loss: 0.609859  [   30/   88]
per-ex loss: 0.413468  [   31/   88]
per-ex loss: 0.613216  [   32/   88]
per-ex loss: 0.664052  [   33/   88]
per-ex loss: 0.414773  [   34/   88]
per-ex loss: 0.712813  [   35/   88]
per-ex loss: 0.544767  [   36/   88]
per-ex loss: 0.492616  [   37/   88]
per-ex loss: 0.570304  [   38/   88]
per-ex loss: 0.582235  [   39/   88]
per-ex loss: 0.580442  [   40/   88]
per-ex loss: 0.389993  [   41/   88]
per-ex loss: 0.474190  [   42/   88]
per-ex loss: 0.436253  [   43/   88]
per-ex loss: 0.462509  [   44/   88]
per-ex loss: 0.586409  [   45/   88]
per-ex loss: 0.579038  [   46/   88]
per-ex loss: 0.427406  [   47/   88]
per-ex loss: 0.621999  [   48/   88]
per-ex loss: 0.478329  [   49/   88]
per-ex loss: 0.615271  [   50/   88]
per-ex loss: 0.606972  [   51/   88]
per-ex loss: 0.416467  [   52/   88]
per-ex loss: 0.630226  [   53/   88]
per-ex loss: 0.514782  [   54/   88]
per-ex loss: 0.458469  [   55/   88]
per-ex loss: 0.456399  [   56/   88]
per-ex loss: 0.465428  [   57/   88]
per-ex loss: 0.555991  [   58/   88]
per-ex loss: 0.480602  [   59/   88]
per-ex loss: 0.690137  [   60/   88]
per-ex loss: 0.380962  [   61/   88]
per-ex loss: 0.720450  [   62/   88]
per-ex loss: 0.423787  [   63/   88]
per-ex loss: 0.458215  [   64/   88]
per-ex loss: 0.637093  [   65/   88]
per-ex loss: 0.642635  [   66/   88]
per-ex loss: 0.706943  [   67/   88]
per-ex loss: 0.629369  [   68/   88]
per-ex loss: 0.690828  [   69/   88]
per-ex loss: 0.453492  [   70/   88]
per-ex loss: 0.432249  [   71/   88]
per-ex loss: 0.387074  [   72/   88]
per-ex loss: 0.465473  [   73/   88]
per-ex loss: 0.571344  [   74/   88]
per-ex loss: 0.611676  [   75/   88]
per-ex loss: 0.462088  [   76/   88]
per-ex loss: 0.493161  [   77/   88]
per-ex loss: 0.662816  [   78/   88]
per-ex loss: 0.410575  [   79/   88]
per-ex loss: 0.620670  [   80/   88]
per-ex loss: 0.493074  [   81/   88]
per-ex loss: 0.464110  [   82/   88]
per-ex loss: 0.642593  [   83/   88]
per-ex loss: 0.444739  [   84/   88]
per-ex loss: 0.375674  [   85/   88]
per-ex loss: 0.555849  [   86/   88]
per-ex loss: 0.507017  [   87/   88]
per-ex loss: 0.414832  [   88/   88]
Train Error: Avg loss: 0.51332291
validation Error: 
 Avg loss: 0.56059793 
 F1: 0.466216 
 Precision: 0.478458 
 Recall: 0.454586
 IoU: 0.303965

test Error: 
 Avg loss: 0.51028527 
 F1: 0.540878 
 Precision: 0.528541 
 Recall: 0.553806
 IoU: 0.370688

We have finished training iteration 21
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_19_.pth
per-ex loss: 0.357454  [    1/   88]
per-ex loss: 0.644703  [    2/   88]
per-ex loss: 0.418514  [    3/   88]
per-ex loss: 0.587991  [    4/   88]
per-ex loss: 0.453570  [    5/   88]
per-ex loss: 0.685684  [    6/   88]
per-ex loss: 0.420747  [    7/   88]
per-ex loss: 0.453392  [    8/   88]
per-ex loss: 0.603361  [    9/   88]
per-ex loss: 0.420423  [   10/   88]
per-ex loss: 0.593600  [   11/   88]
per-ex loss: 0.666114  [   12/   88]
per-ex loss: 0.448572  [   13/   88]
per-ex loss: 0.571932  [   14/   88]
per-ex loss: 0.640719  [   15/   88]
per-ex loss: 0.549797  [   16/   88]
per-ex loss: 0.411108  [   17/   88]
per-ex loss: 0.561184  [   18/   88]
per-ex loss: 0.460069  [   19/   88]
per-ex loss: 0.381429  [   20/   88]
per-ex loss: 0.473981  [   21/   88]
per-ex loss: 0.658843  [   22/   88]
per-ex loss: 0.501530  [   23/   88]
per-ex loss: 0.532041  [   24/   88]
per-ex loss: 0.389274  [   25/   88]
per-ex loss: 0.685809  [   26/   88]
per-ex loss: 0.426514  [   27/   88]
per-ex loss: 0.541243  [   28/   88]
per-ex loss: 0.601210  [   29/   88]
per-ex loss: 0.487048  [   30/   88]
per-ex loss: 0.467351  [   31/   88]
per-ex loss: 0.421422  [   32/   88]
per-ex loss: 0.538091  [   33/   88]
per-ex loss: 0.476752  [   34/   88]
per-ex loss: 0.404162  [   35/   88]
per-ex loss: 0.346539  [   36/   88]
per-ex loss: 0.639938  [   37/   88]
per-ex loss: 0.451506  [   38/   88]
per-ex loss: 0.602192  [   39/   88]
per-ex loss: 0.567197  [   40/   88]
per-ex loss: 0.360960  [   41/   88]
per-ex loss: 0.370574  [   42/   88]
per-ex loss: 0.660933  [   43/   88]
per-ex loss: 0.423504  [   44/   88]
per-ex loss: 0.633611  [   45/   88]
per-ex loss: 0.395045  [   46/   88]
per-ex loss: 0.563067  [   47/   88]
per-ex loss: 0.596789  [   48/   88]
per-ex loss: 0.394592  [   49/   88]
per-ex loss: 0.425058  [   50/   88]
per-ex loss: 0.608666  [   51/   88]
per-ex loss: 0.706975  [   52/   88]
per-ex loss: 0.432163  [   53/   88]
per-ex loss: 0.389453  [   54/   88]
per-ex loss: 0.613152  [   55/   88]
per-ex loss: 0.442891  [   56/   88]
per-ex loss: 0.409344  [   57/   88]
per-ex loss: 0.391232  [   58/   88]
per-ex loss: 0.692427  [   59/   88]
per-ex loss: 0.565162  [   60/   88]
per-ex loss: 0.585859  [   61/   88]
per-ex loss: 0.424161  [   62/   88]
per-ex loss: 0.612920  [   63/   88]
per-ex loss: 0.440845  [   64/   88]
per-ex loss: 0.455807  [   65/   88]
per-ex loss: 0.435975  [   66/   88]
per-ex loss: 0.417264  [   67/   88]
per-ex loss: 0.383293  [   68/   88]
per-ex loss: 0.542754  [   69/   88]
per-ex loss: 0.596848  [   70/   88]
per-ex loss: 0.697155  [   71/   88]
per-ex loss: 0.394681  [   72/   88]
per-ex loss: 0.602018  [   73/   88]
per-ex loss: 0.470408  [   74/   88]
per-ex loss: 0.618265  [   75/   88]
per-ex loss: 0.654254  [   76/   88]
per-ex loss: 0.693224  [   77/   88]
per-ex loss: 0.486220  [   78/   88]
per-ex loss: 0.424303  [   79/   88]
per-ex loss: 0.574962  [   80/   88]
per-ex loss: 0.670528  [   81/   88]
per-ex loss: 0.530700  [   82/   88]
per-ex loss: 0.360382  [   83/   88]
per-ex loss: 0.545298  [   84/   88]
per-ex loss: 0.466766  [   85/   88]
per-ex loss: 0.464662  [   86/   88]
per-ex loss: 0.556280  [   87/   88]
per-ex loss: 0.352174  [   88/   88]
Train Error: Avg loss: 0.51223416
validation Error: 
 Avg loss: 0.62540521 
 F1: 0.400926 
 Precision: 0.339699 
 Recall: 0.489077
 IoU: 0.250724

test Error: 
 Avg loss: 0.52311887 
 F1: 0.522429 
 Precision: 0.478662 
 Recall: 0.575004
 IoU: 0.353573

We have finished training iteration 22
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_20_.pth
per-ex loss: 0.575446  [    1/   88]
per-ex loss: 0.436001  [    2/   88]
per-ex loss: 0.409394  [    3/   88]
per-ex loss: 0.528804  [    4/   88]
per-ex loss: 0.501324  [    5/   88]
per-ex loss: 0.445735  [    6/   88]
per-ex loss: 0.363837  [    7/   88]
per-ex loss: 0.651337  [    8/   88]
per-ex loss: 0.458832  [    9/   88]
per-ex loss: 0.470069  [   10/   88]
per-ex loss: 0.384584  [   11/   88]
per-ex loss: 0.549732  [   12/   88]
per-ex loss: 0.438509  [   13/   88]
per-ex loss: 0.588128  [   14/   88]
per-ex loss: 0.610654  [   15/   88]
per-ex loss: 0.613396  [   16/   88]
per-ex loss: 0.627021  [   17/   88]
per-ex loss: 0.674775  [   18/   88]
per-ex loss: 0.476852  [   19/   88]
per-ex loss: 0.589803  [   20/   88]
per-ex loss: 0.685941  [   21/   88]
per-ex loss: 0.393623  [   22/   88]
per-ex loss: 0.520542  [   23/   88]
per-ex loss: 0.478270  [   24/   88]
per-ex loss: 0.627922  [   25/   88]
per-ex loss: 0.462806  [   26/   88]
per-ex loss: 0.577979  [   27/   88]
per-ex loss: 0.422428  [   28/   88]
per-ex loss: 0.474184  [   29/   88]
per-ex loss: 0.384182  [   30/   88]
per-ex loss: 0.685832  [   31/   88]
per-ex loss: 0.395600  [   32/   88]
per-ex loss: 0.441519  [   33/   88]
per-ex loss: 0.384346  [   34/   88]
per-ex loss: 0.689875  [   35/   88]
per-ex loss: 0.609903  [   36/   88]
per-ex loss: 0.391402  [   37/   88]
per-ex loss: 0.434906  [   38/   88]
per-ex loss: 0.558067  [   39/   88]
per-ex loss: 0.612770  [   40/   88]
per-ex loss: 0.653964  [   41/   88]
per-ex loss: 0.366686  [   42/   88]
per-ex loss: 0.388105  [   43/   88]
per-ex loss: 0.677298  [   44/   88]
per-ex loss: 0.559231  [   45/   88]
per-ex loss: 0.556494  [   46/   88]
per-ex loss: 0.638836  [   47/   88]
per-ex loss: 0.571816  [   48/   88]
per-ex loss: 0.438381  [   49/   88]
per-ex loss: 0.462630  [   50/   88]
per-ex loss: 0.528491  [   51/   88]
per-ex loss: 0.440252  [   52/   88]
per-ex loss: 0.671358  [   53/   88]
per-ex loss: 0.588393  [   54/   88]
per-ex loss: 0.590773  [   55/   88]
per-ex loss: 0.684571  [   56/   88]
per-ex loss: 0.395661  [   57/   88]
per-ex loss: 0.645077  [   58/   88]
per-ex loss: 0.480545  [   59/   88]
per-ex loss: 0.528482  [   60/   88]
per-ex loss: 0.483333  [   61/   88]
per-ex loss: 0.676886  [   62/   88]
per-ex loss: 0.378329  [   63/   88]
per-ex loss: 0.450848  [   64/   88]
per-ex loss: 0.676059  [   65/   88]
per-ex loss: 0.404058  [   66/   88]
per-ex loss: 0.447032  [   67/   88]
per-ex loss: 0.389912  [   68/   88]
per-ex loss: 0.610324  [   69/   88]
per-ex loss: 0.543824  [   70/   88]
per-ex loss: 0.464947  [   71/   88]
per-ex loss: 0.390884  [   72/   88]
per-ex loss: 0.416500  [   73/   88]
per-ex loss: 0.696406  [   74/   88]
per-ex loss: 0.454558  [   75/   88]
per-ex loss: 0.506607  [   76/   88]
per-ex loss: 0.390608  [   77/   88]
per-ex loss: 0.416095  [   78/   88]
per-ex loss: 0.475352  [   79/   88]
per-ex loss: 0.472671  [   80/   88]
per-ex loss: 0.573567  [   81/   88]
per-ex loss: 0.429927  [   82/   88]
per-ex loss: 0.443756  [   83/   88]
per-ex loss: 0.669959  [   84/   88]
per-ex loss: 0.356723  [   85/   88]
per-ex loss: 0.366633  [   86/   88]
per-ex loss: 0.547250  [   87/   88]
per-ex loss: 0.356959  [   88/   88]
Train Error: Avg loss: 0.51117477
validation Error: 
 Avg loss: 0.55568748 
 F1: 0.472016 
 Precision: 0.574653 
 Recall: 0.400486
 IoU: 0.308914

test Error: 
 Avg loss: 0.50500146 
 F1: 0.549030 
 Precision: 0.622572 
 Recall: 0.491027
 IoU: 0.378388

We have finished training iteration 23
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_21_.pth
per-ex loss: 0.485685  [    1/   88]
per-ex loss: 0.368557  [    2/   88]
per-ex loss: 0.389499  [    3/   88]
per-ex loss: 0.376812  [    4/   88]
per-ex loss: 0.498016  [    5/   88]
per-ex loss: 0.451205  [    6/   88]
per-ex loss: 0.400572  [    7/   88]
per-ex loss: 0.581192  [    8/   88]
per-ex loss: 0.396992  [    9/   88]
per-ex loss: 0.614876  [   10/   88]
per-ex loss: 0.573991  [   11/   88]
per-ex loss: 0.364725  [   12/   88]
per-ex loss: 0.443445  [   13/   88]
per-ex loss: 0.710736  [   14/   88]
per-ex loss: 0.584885  [   15/   88]
per-ex loss: 0.423125  [   16/   88]
per-ex loss: 0.585416  [   17/   88]
per-ex loss: 0.676196  [   18/   88]
per-ex loss: 0.386387  [   19/   88]
per-ex loss: 0.396848  [   20/   88]
per-ex loss: 0.608603  [   21/   88]
per-ex loss: 0.622927  [   22/   88]
per-ex loss: 0.438189  [   23/   88]
per-ex loss: 0.669840  [   24/   88]
per-ex loss: 0.705913  [   25/   88]
per-ex loss: 0.474153  [   26/   88]
per-ex loss: 0.585986  [   27/   88]
per-ex loss: 0.559718  [   28/   88]
per-ex loss: 0.380666  [   29/   88]
per-ex loss: 0.556125  [   30/   88]
per-ex loss: 0.665124  [   31/   88]
per-ex loss: 0.587889  [   32/   88]
per-ex loss: 0.416372  [   33/   88]
per-ex loss: 0.448082  [   34/   88]
per-ex loss: 0.604393  [   35/   88]
per-ex loss: 0.441340  [   36/   88]
per-ex loss: 0.402635  [   37/   88]
per-ex loss: 0.394905  [   38/   88]
per-ex loss: 0.592242  [   39/   88]
per-ex loss: 0.411859  [   40/   88]
per-ex loss: 0.635877  [   41/   88]
per-ex loss: 0.638385  [   42/   88]
per-ex loss: 0.690173  [   43/   88]
per-ex loss: 0.435292  [   44/   88]
per-ex loss: 0.603676  [   45/   88]
per-ex loss: 0.359036  [   46/   88]
per-ex loss: 0.531213  [   47/   88]
per-ex loss: 0.622534  [   48/   88]
per-ex loss: 0.641735  [   49/   88]
per-ex loss: 0.607990  [   50/   88]
per-ex loss: 0.417222  [   51/   88]
per-ex loss: 0.472976  [   52/   88]
per-ex loss: 0.675926  [   53/   88]
per-ex loss: 0.514492  [   54/   88]
per-ex loss: 0.471899  [   55/   88]
per-ex loss: 0.465642  [   56/   88]
per-ex loss: 0.675312  [   57/   88]
per-ex loss: 0.671525  [   58/   88]
per-ex loss: 0.404599  [   59/   88]
per-ex loss: 0.608196  [   60/   88]
per-ex loss: 0.556790  [   61/   88]
per-ex loss: 0.643803  [   62/   88]
per-ex loss: 0.382566  [   63/   88]
per-ex loss: 0.426753  [   64/   88]
per-ex loss: 0.634516  [   65/   88]
per-ex loss: 0.458198  [   66/   88]
per-ex loss: 0.327860  [   67/   88]
per-ex loss: 0.564418  [   68/   88]
per-ex loss: 0.478367  [   69/   88]
per-ex loss: 0.481852  [   70/   88]
per-ex loss: 0.431684  [   71/   88]
per-ex loss: 0.331266  [   72/   88]
per-ex loss: 0.384311  [   73/   88]
per-ex loss: 0.454376  [   74/   88]
per-ex loss: 0.708926  [   75/   88]
per-ex loss: 0.661455  [   76/   88]
per-ex loss: 0.487225  [   77/   88]
per-ex loss: 0.403922  [   78/   88]
per-ex loss: 0.433755  [   79/   88]
per-ex loss: 0.583525  [   80/   88]
per-ex loss: 0.428470  [   81/   88]
per-ex loss: 0.547560  [   82/   88]
per-ex loss: 0.548448  [   83/   88]
per-ex loss: 0.458856  [   84/   88]
per-ex loss: 0.566070  [   85/   88]
per-ex loss: 0.408062  [   86/   88]
per-ex loss: 0.409469  [   87/   88]
per-ex loss: 0.409104  [   88/   88]
Train Error: Avg loss: 0.51172077
validation Error: 
 Avg loss: 0.56459728 
 F1: 0.457818 
 Precision: 0.455078 
 Recall: 0.460590
 IoU: 0.296863

test Error: 
 Avg loss: 0.49953612 
 F1: 0.552375 
 Precision: 0.543664 
 Recall: 0.561369
 IoU: 0.381573

We have finished training iteration 24
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_22_.pth
per-ex loss: 0.379652  [    1/   88]
per-ex loss: 0.726587  [    2/   88]
per-ex loss: 0.446741  [    3/   88]
per-ex loss: 0.383578  [    4/   88]
per-ex loss: 0.608646  [    5/   88]
per-ex loss: 0.637307  [    6/   88]
per-ex loss: 0.487712  [    7/   88]
per-ex loss: 0.460486  [    8/   88]
per-ex loss: 0.593301  [    9/   88]
per-ex loss: 0.394021  [   10/   88]
per-ex loss: 0.375250  [   11/   88]
per-ex loss: 0.698104  [   12/   88]
per-ex loss: 0.421421  [   13/   88]
per-ex loss: 0.623324  [   14/   88]
per-ex loss: 0.421700  [   15/   88]
per-ex loss: 0.415377  [   16/   88]
per-ex loss: 0.443973  [   17/   88]
per-ex loss: 0.374797  [   18/   88]
per-ex loss: 0.630607  [   19/   88]
per-ex loss: 0.548380  [   20/   88]
per-ex loss: 0.440087  [   21/   88]
per-ex loss: 0.445493  [   22/   88]
per-ex loss: 0.581262  [   23/   88]
per-ex loss: 0.414718  [   24/   88]
per-ex loss: 0.401236  [   25/   88]
per-ex loss: 0.589177  [   26/   88]
per-ex loss: 0.421421  [   27/   88]
per-ex loss: 0.602494  [   28/   88]
per-ex loss: 0.658996  [   29/   88]
per-ex loss: 0.607508  [   30/   88]
per-ex loss: 0.401378  [   31/   88]
per-ex loss: 0.529708  [   32/   88]
per-ex loss: 0.622747  [   33/   88]
per-ex loss: 0.449744  [   34/   88]
per-ex loss: 0.380639  [   35/   88]
per-ex loss: 0.480223  [   36/   88]
per-ex loss: 0.619212  [   37/   88]
per-ex loss: 0.708148  [   38/   88]
per-ex loss: 0.557091  [   39/   88]
per-ex loss: 0.668229  [   40/   88]
per-ex loss: 0.401955  [   41/   88]
per-ex loss: 0.629182  [   42/   88]
per-ex loss: 0.579146  [   43/   88]
per-ex loss: 0.425361  [   44/   88]
per-ex loss: 0.341340  [   45/   88]
per-ex loss: 0.395517  [   46/   88]
per-ex loss: 0.446865  [   47/   88]
per-ex loss: 0.455773  [   48/   88]
per-ex loss: 0.595158  [   49/   88]
per-ex loss: 0.520528  [   50/   88]
per-ex loss: 0.465776  [   51/   88]
per-ex loss: 0.467265  [   52/   88]
per-ex loss: 0.562033  [   53/   88]
per-ex loss: 0.557607  [   54/   88]
per-ex loss: 0.712751  [   55/   88]
per-ex loss: 0.619650  [   56/   88]
per-ex loss: 0.567618  [   57/   88]
per-ex loss: 0.677162  [   58/   88]
per-ex loss: 0.547071  [   59/   88]
per-ex loss: 0.626216  [   60/   88]
per-ex loss: 0.439864  [   61/   88]
per-ex loss: 0.527460  [   62/   88]
per-ex loss: 0.339204  [   63/   88]
per-ex loss: 0.420150  [   64/   88]
per-ex loss: 0.383703  [   65/   88]
per-ex loss: 0.438674  [   66/   88]
per-ex loss: 0.477909  [   67/   88]
per-ex loss: 0.435894  [   68/   88]
per-ex loss: 0.625214  [   69/   88]
per-ex loss: 0.394199  [   70/   88]
per-ex loss: 0.404688  [   71/   88]
per-ex loss: 0.664211  [   72/   88]
per-ex loss: 0.535825  [   73/   88]
per-ex loss: 0.629205  [   74/   88]
per-ex loss: 0.658577  [   75/   88]
per-ex loss: 0.510927  [   76/   88]
per-ex loss: 0.367668  [   77/   88]
per-ex loss: 0.449042  [   78/   88]
per-ex loss: 0.703153  [   79/   88]
per-ex loss: 0.375942  [   80/   88]
per-ex loss: 0.393880  [   81/   88]
per-ex loss: 0.443056  [   82/   88]
per-ex loss: 0.399124  [   83/   88]
per-ex loss: 0.639265  [   84/   88]
per-ex loss: 0.566309  [   85/   88]
per-ex loss: 0.423321  [   86/   88]
per-ex loss: 0.559259  [   87/   88]
per-ex loss: 0.463411  [   88/   88]
Train Error: Avg loss: 0.51038157
validation Error: 
 Avg loss: 0.54753297 
 F1: 0.487369 
 Precision: 0.550645 
 Recall: 0.437136
 IoU: 0.322199

test Error: 
 Avg loss: 0.49123153 
 F1: 0.563399 
 Precision: 0.611791 
 Recall: 0.522101
 IoU: 0.392175

We have finished training iteration 25
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_16_.pth
per-ex loss: 0.630601  [    1/   88]
per-ex loss: 0.401270  [    2/   88]
per-ex loss: 0.571902  [    3/   88]
per-ex loss: 0.448899  [    4/   88]
per-ex loss: 0.514656  [    5/   88]
per-ex loss: 0.597765  [    6/   88]
per-ex loss: 0.584430  [    7/   88]
per-ex loss: 0.402000  [    8/   88]
per-ex loss: 0.596947  [    9/   88]
per-ex loss: 0.388335  [   10/   88]
per-ex loss: 0.407023  [   11/   88]
per-ex loss: 0.541347  [   12/   88]
per-ex loss: 0.619126  [   13/   88]
per-ex loss: 0.337371  [   14/   88]
per-ex loss: 0.452124  [   15/   88]
per-ex loss: 0.519135  [   16/   88]
per-ex loss: 0.672161  [   17/   88]
per-ex loss: 0.671107  [   18/   88]
per-ex loss: 0.618296  [   19/   88]
per-ex loss: 0.379845  [   20/   88]
per-ex loss: 0.629920  [   21/   88]
per-ex loss: 0.632749  [   22/   88]
per-ex loss: 0.690232  [   23/   88]
per-ex loss: 0.436300  [   24/   88]
per-ex loss: 0.452566  [   25/   88]
per-ex loss: 0.422676  [   26/   88]
per-ex loss: 0.541731  [   27/   88]
per-ex loss: 0.524853  [   28/   88]
per-ex loss: 0.455797  [   29/   88]
per-ex loss: 0.463039  [   30/   88]
per-ex loss: 0.466594  [   31/   88]
per-ex loss: 0.544869  [   32/   88]
per-ex loss: 0.423030  [   33/   88]
per-ex loss: 0.562101  [   34/   88]
per-ex loss: 0.557928  [   35/   88]
per-ex loss: 0.382657  [   36/   88]
per-ex loss: 0.452190  [   37/   88]
per-ex loss: 0.393383  [   38/   88]
per-ex loss: 0.604813  [   39/   88]
per-ex loss: 0.555561  [   40/   88]
per-ex loss: 0.437013  [   41/   88]
per-ex loss: 0.651211  [   42/   88]
per-ex loss: 0.426395  [   43/   88]
per-ex loss: 0.570397  [   44/   88]
per-ex loss: 0.428830  [   45/   88]
per-ex loss: 0.346160  [   46/   88]
per-ex loss: 0.485768  [   47/   88]
per-ex loss: 0.452439  [   48/   88]
per-ex loss: 0.594388  [   49/   88]
per-ex loss: 0.400294  [   50/   88]
per-ex loss: 0.674848  [   51/   88]
per-ex loss: 0.687651  [   52/   88]
per-ex loss: 0.366168  [   53/   88]
per-ex loss: 0.691595  [   54/   88]
per-ex loss: 0.470340  [   55/   88]
per-ex loss: 0.373370  [   56/   88]
per-ex loss: 0.689479  [   57/   88]
per-ex loss: 0.712355  [   58/   88]
per-ex loss: 0.620538  [   59/   88]
per-ex loss: 0.456437  [   60/   88]
per-ex loss: 0.662548  [   61/   88]
per-ex loss: 0.463902  [   62/   88]
per-ex loss: 0.415383  [   63/   88]
per-ex loss: 0.536664  [   64/   88]
per-ex loss: 0.555015  [   65/   88]
per-ex loss: 0.420116  [   66/   88]
per-ex loss: 0.353215  [   67/   88]
per-ex loss: 0.379357  [   68/   88]
per-ex loss: 0.426129  [   69/   88]
per-ex loss: 0.449071  [   70/   88]
per-ex loss: 0.603285  [   71/   88]
per-ex loss: 0.643295  [   72/   88]
per-ex loss: 0.358185  [   73/   88]
per-ex loss: 0.600171  [   74/   88]
per-ex loss: 0.391194  [   75/   88]
per-ex loss: 0.712589  [   76/   88]
per-ex loss: 0.429748  [   77/   88]
per-ex loss: 0.405657  [   78/   88]
per-ex loss: 0.368417  [   79/   88]
per-ex loss: 0.481441  [   80/   88]
per-ex loss: 0.465376  [   81/   88]
per-ex loss: 0.587706  [   82/   88]
per-ex loss: 0.538098  [   83/   88]
per-ex loss: 0.400557  [   84/   88]
per-ex loss: 0.505638  [   85/   88]
per-ex loss: 0.567978  [   86/   88]
per-ex loss: 0.349726  [   87/   88]
per-ex loss: 0.446447  [   88/   88]
Train Error: Avg loss: 0.50681715
validation Error: 
 Avg loss: 0.58106667 
 F1: 0.442663 
 Precision: 0.401065 
 Recall: 0.493889
 IoU: 0.284243

test Error: 
 Avg loss: 0.49576507 
 F1: 0.553318 
 Precision: 0.517637 
 Recall: 0.594282
 IoU: 0.382474

We have finished training iteration 26
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_24_.pth
per-ex loss: 0.452092  [    1/   88]
per-ex loss: 0.379539  [    2/   88]
per-ex loss: 0.699679  [    3/   88]
per-ex loss: 0.552728  [    4/   88]
per-ex loss: 0.622883  [    5/   88]
per-ex loss: 0.536504  [    6/   88]
per-ex loss: 0.649980  [    7/   88]
per-ex loss: 0.548858  [    8/   88]
per-ex loss: 0.409557  [    9/   88]
per-ex loss: 0.392878  [   10/   88]
per-ex loss: 0.371572  [   11/   88]
per-ex loss: 0.671562  [   12/   88]
per-ex loss: 0.447582  [   13/   88]
per-ex loss: 0.429271  [   14/   88]
per-ex loss: 0.458153  [   15/   88]
per-ex loss: 0.548423  [   16/   88]
per-ex loss: 0.688606  [   17/   88]
per-ex loss: 0.383852  [   18/   88]
per-ex loss: 0.540919  [   19/   88]
per-ex loss: 0.445362  [   20/   88]
per-ex loss: 0.474320  [   21/   88]
per-ex loss: 0.382607  [   22/   88]
per-ex loss: 0.453505  [   23/   88]
per-ex loss: 0.446257  [   24/   88]
per-ex loss: 0.538760  [   25/   88]
per-ex loss: 0.563552  [   26/   88]
per-ex loss: 0.329094  [   27/   88]
per-ex loss: 0.470394  [   28/   88]
per-ex loss: 0.359925  [   29/   88]
per-ex loss: 0.522628  [   30/   88]
per-ex loss: 0.554835  [   31/   88]
per-ex loss: 0.392668  [   32/   88]
per-ex loss: 0.430277  [   33/   88]
per-ex loss: 0.415877  [   34/   88]
per-ex loss: 0.592701  [   35/   88]
per-ex loss: 0.571212  [   36/   88]
per-ex loss: 0.554222  [   37/   88]
per-ex loss: 0.509004  [   38/   88]
per-ex loss: 0.623687  [   39/   88]
per-ex loss: 0.619055  [   40/   88]
per-ex loss: 0.509585  [   41/   88]
per-ex loss: 0.658131  [   42/   88]
per-ex loss: 0.447678  [   43/   88]
per-ex loss: 0.350206  [   44/   88]
per-ex loss: 0.561579  [   45/   88]
per-ex loss: 0.459275  [   46/   88]
per-ex loss: 0.681677  [   47/   88]
per-ex loss: 0.451821  [   48/   88]
per-ex loss: 0.411708  [   49/   88]
per-ex loss: 0.557072  [   50/   88]
per-ex loss: 0.643200  [   51/   88]
per-ex loss: 0.705765  [   52/   88]
per-ex loss: 0.419829  [   53/   88]
per-ex loss: 0.443862  [   54/   88]
per-ex loss: 0.687567  [   55/   88]
per-ex loss: 0.576519  [   56/   88]
per-ex loss: 0.396433  [   57/   88]
per-ex loss: 0.408244  [   58/   88]
per-ex loss: 0.412462  [   59/   88]
per-ex loss: 0.461567  [   60/   88]
per-ex loss: 0.688023  [   61/   88]
per-ex loss: 0.446010  [   62/   88]
per-ex loss: 0.411281  [   63/   88]
per-ex loss: 0.430311  [   64/   88]
per-ex loss: 0.479778  [   65/   88]
per-ex loss: 0.381534  [   66/   88]
per-ex loss: 0.661023  [   67/   88]
per-ex loss: 0.376806  [   68/   88]
per-ex loss: 0.619082  [   69/   88]
per-ex loss: 0.368688  [   70/   88]
per-ex loss: 0.422976  [   71/   88]
per-ex loss: 0.586750  [   72/   88]
per-ex loss: 0.598113  [   73/   88]
per-ex loss: 0.614521  [   74/   88]
per-ex loss: 0.519779  [   75/   88]
per-ex loss: 0.406287  [   76/   88]
per-ex loss: 0.337877  [   77/   88]
per-ex loss: 0.441541  [   78/   88]
per-ex loss: 0.612478  [   79/   88]
per-ex loss: 0.608739  [   80/   88]
per-ex loss: 0.679682  [   81/   88]
per-ex loss: 0.366541  [   82/   88]
per-ex loss: 0.631247  [   83/   88]
per-ex loss: 0.562086  [   84/   88]
per-ex loss: 0.447765  [   85/   88]
per-ex loss: 0.436997  [   86/   88]
per-ex loss: 0.415632  [   87/   88]
per-ex loss: 0.600229  [   88/   88]
Train Error: Avg loss: 0.50489244
validation Error: 
 Avg loss: 0.54676589 
 F1: 0.487026 
 Precision: 0.506751 
 Recall: 0.468779
 IoU: 0.321900

test Error: 
 Avg loss: 0.49256183 
 F1: 0.558583 
 Precision: 0.566698 
 Recall: 0.550698
 IoU: 0.387524

We have finished training iteration 27
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_17_.pth
per-ex loss: 0.413673  [    1/   88]
per-ex loss: 0.484962  [    2/   88]
per-ex loss: 0.434049  [    3/   88]
per-ex loss: 0.610487  [    4/   88]
per-ex loss: 0.442854  [    5/   88]
per-ex loss: 0.588579  [    6/   88]
per-ex loss: 0.672150  [    7/   88]
per-ex loss: 0.415740  [    8/   88]
per-ex loss: 0.362796  [    9/   88]
per-ex loss: 0.562195  [   10/   88]
per-ex loss: 0.374195  [   11/   88]
per-ex loss: 0.620853  [   12/   88]
per-ex loss: 0.441140  [   13/   88]
per-ex loss: 0.360661  [   14/   88]
per-ex loss: 0.627112  [   15/   88]
per-ex loss: 0.704418  [   16/   88]
per-ex loss: 0.685869  [   17/   88]
per-ex loss: 0.336539  [   18/   88]
per-ex loss: 0.409651  [   19/   88]
per-ex loss: 0.588586  [   20/   88]
per-ex loss: 0.378559  [   21/   88]
per-ex loss: 0.392023  [   22/   88]
per-ex loss: 0.616492  [   23/   88]
per-ex loss: 0.624509  [   24/   88]
per-ex loss: 0.383619  [   25/   88]
per-ex loss: 0.432985  [   26/   88]
per-ex loss: 0.567137  [   27/   88]
per-ex loss: 0.685105  [   28/   88]
per-ex loss: 0.403104  [   29/   88]
per-ex loss: 0.461266  [   30/   88]
per-ex loss: 0.697212  [   31/   88]
per-ex loss: 0.620646  [   32/   88]
per-ex loss: 0.667902  [   33/   88]
per-ex loss: 0.431409  [   34/   88]
per-ex loss: 0.357893  [   35/   88]
per-ex loss: 0.433430  [   36/   88]
per-ex loss: 0.459408  [   37/   88]
per-ex loss: 0.491368  [   38/   88]
per-ex loss: 0.488174  [   39/   88]
per-ex loss: 0.587591  [   40/   88]
per-ex loss: 0.496277  [   41/   88]
per-ex loss: 0.631159  [   42/   88]
per-ex loss: 0.423729  [   43/   88]
per-ex loss: 0.441117  [   44/   88]
per-ex loss: 0.527070  [   45/   88]
per-ex loss: 0.586336  [   46/   88]
per-ex loss: 0.515128  [   47/   88]
per-ex loss: 0.412117  [   48/   88]
per-ex loss: 0.632244  [   49/   88]
per-ex loss: 0.544771  [   50/   88]
per-ex loss: 0.401832  [   51/   88]
per-ex loss: 0.379407  [   52/   88]
per-ex loss: 0.466481  [   53/   88]
per-ex loss: 0.627727  [   54/   88]
per-ex loss: 0.423949  [   55/   88]
per-ex loss: 0.525325  [   56/   88]
per-ex loss: 0.440107  [   57/   88]
per-ex loss: 0.616834  [   58/   88]
per-ex loss: 0.456446  [   59/   88]
per-ex loss: 0.596822  [   60/   88]
per-ex loss: 0.561793  [   61/   88]
per-ex loss: 0.430161  [   62/   88]
per-ex loss: 0.462864  [   63/   88]
per-ex loss: 0.392678  [   64/   88]
per-ex loss: 0.577048  [   65/   88]
per-ex loss: 0.583837  [   66/   88]
per-ex loss: 0.357111  [   67/   88]
per-ex loss: 0.405612  [   68/   88]
per-ex loss: 0.427115  [   69/   88]
per-ex loss: 0.413233  [   70/   88]
per-ex loss: 0.394329  [   71/   88]
per-ex loss: 0.438487  [   72/   88]
per-ex loss: 0.467060  [   73/   88]
per-ex loss: 0.561199  [   74/   88]
per-ex loss: 0.431362  [   75/   88]
per-ex loss: 0.543428  [   76/   88]
per-ex loss: 0.709434  [   77/   88]
per-ex loss: 0.434546  [   78/   88]
per-ex loss: 0.551971  [   79/   88]
per-ex loss: 0.560647  [   80/   88]
per-ex loss: 0.593470  [   81/   88]
per-ex loss: 0.497751  [   82/   88]
per-ex loss: 0.576735  [   83/   88]
per-ex loss: 0.663762  [   84/   88]
per-ex loss: 0.397796  [   85/   88]
per-ex loss: 0.684246  [   86/   88]
per-ex loss: 0.417877  [   87/   88]
per-ex loss: 0.527527  [   88/   88]
Train Error: Avg loss: 0.50595791
validation Error: 
 Avg loss: 0.54956315 
 F1: 0.480118 
 Precision: 0.471875 
 Recall: 0.488654
 IoU: 0.315891

test Error: 
 Avg loss: 0.49722164 
 F1: 0.552682 
 Precision: 0.519030 
 Recall: 0.591000
 IoU: 0.381866

We have finished training iteration 28
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_26_.pth
per-ex loss: 0.466507  [    1/   88]
per-ex loss: 0.615245  [    2/   88]
per-ex loss: 0.388706  [    3/   88]
per-ex loss: 0.345792  [    4/   88]
per-ex loss: 0.468204  [    5/   88]
per-ex loss: 0.545484  [    6/   88]
per-ex loss: 0.466028  [    7/   88]
per-ex loss: 0.535689  [    8/   88]
per-ex loss: 0.446702  [    9/   88]
per-ex loss: 0.431481  [   10/   88]
per-ex loss: 0.706015  [   11/   88]
per-ex loss: 0.400812  [   12/   88]
per-ex loss: 0.690506  [   13/   88]
per-ex loss: 0.706952  [   14/   88]
per-ex loss: 0.419763  [   15/   88]
per-ex loss: 0.588275  [   16/   88]
per-ex loss: 0.525821  [   17/   88]
per-ex loss: 0.676785  [   18/   88]
per-ex loss: 0.461975  [   19/   88]
per-ex loss: 0.619521  [   20/   88]
per-ex loss: 0.630125  [   21/   88]
per-ex loss: 0.588804  [   22/   88]
per-ex loss: 0.633657  [   23/   88]
per-ex loss: 0.584725  [   24/   88]
per-ex loss: 0.692795  [   25/   88]
per-ex loss: 0.418576  [   26/   88]
per-ex loss: 0.527056  [   27/   88]
per-ex loss: 0.728508  [   28/   88]
per-ex loss: 0.471532  [   29/   88]
per-ex loss: 0.473998  [   30/   88]
per-ex loss: 0.426850  [   31/   88]
per-ex loss: 0.450094  [   32/   88]
per-ex loss: 0.626690  [   33/   88]
per-ex loss: 0.621566  [   34/   88]
per-ex loss: 0.375915  [   35/   88]
per-ex loss: 0.395587  [   36/   88]
per-ex loss: 0.589112  [   37/   88]
per-ex loss: 0.419006  [   38/   88]
per-ex loss: 0.575722  [   39/   88]
per-ex loss: 0.598001  [   40/   88]
per-ex loss: 0.415400  [   41/   88]
per-ex loss: 0.336830  [   42/   88]
per-ex loss: 0.395894  [   43/   88]
per-ex loss: 0.512720  [   44/   88]
per-ex loss: 0.433773  [   45/   88]
per-ex loss: 0.497273  [   46/   88]
per-ex loss: 0.370612  [   47/   88]
per-ex loss: 0.452443  [   48/   88]
per-ex loss: 0.350537  [   49/   88]
per-ex loss: 0.413764  [   50/   88]
per-ex loss: 0.582390  [   51/   88]
per-ex loss: 0.573103  [   52/   88]
per-ex loss: 0.368821  [   53/   88]
per-ex loss: 0.399088  [   54/   88]
per-ex loss: 0.635607  [   55/   88]
per-ex loss: 0.406476  [   56/   88]
per-ex loss: 0.409406  [   57/   88]
per-ex loss: 0.595073  [   58/   88]
per-ex loss: 0.373353  [   59/   88]
per-ex loss: 0.465241  [   60/   88]
per-ex loss: 0.376969  [   61/   88]
per-ex loss: 0.574839  [   62/   88]
per-ex loss: 0.457351  [   63/   88]
per-ex loss: 0.413464  [   64/   88]
per-ex loss: 0.453252  [   65/   88]
per-ex loss: 0.389420  [   66/   88]
per-ex loss: 0.549706  [   67/   88]
per-ex loss: 0.643913  [   68/   88]
per-ex loss: 0.649616  [   69/   88]
per-ex loss: 0.468059  [   70/   88]
per-ex loss: 0.562664  [   71/   88]
per-ex loss: 0.431349  [   72/   88]
per-ex loss: 0.556497  [   73/   88]
per-ex loss: 0.659193  [   74/   88]
per-ex loss: 0.392770  [   75/   88]
per-ex loss: 0.455891  [   76/   88]
per-ex loss: 0.391464  [   77/   88]
per-ex loss: 0.678139  [   78/   88]
per-ex loss: 0.443904  [   79/   88]
per-ex loss: 0.548819  [   80/   88]
per-ex loss: 0.449603  [   81/   88]
per-ex loss: 0.617185  [   82/   88]
per-ex loss: 0.554502  [   83/   88]
per-ex loss: 0.593360  [   84/   88]
per-ex loss: 0.612902  [   85/   88]
per-ex loss: 0.397211  [   86/   88]
per-ex loss: 0.383160  [   87/   88]
per-ex loss: 0.380125  [   88/   88]
Train Error: Avg loss: 0.50463308
validation Error: 
 Avg loss: 0.55435160 
 F1: 0.476674 
 Precision: 0.549607 
 Recall: 0.420830
 IoU: 0.312917

test Error: 
 Avg loss: 0.49887365 
 F1: 0.550588 
 Precision: 0.621266 
 Recall: 0.494349
 IoU: 0.379870

We have finished training iteration 29
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_23_.pth
per-ex loss: 0.439884  [    1/   88]
per-ex loss: 0.458084  [    2/   88]
per-ex loss: 0.432148  [    3/   88]
per-ex loss: 0.694773  [    4/   88]
per-ex loss: 0.427044  [    5/   88]
per-ex loss: 0.420092  [    6/   88]
per-ex loss: 0.682408  [    7/   88]
per-ex loss: 0.608520  [    8/   88]
per-ex loss: 0.538224  [    9/   88]
per-ex loss: 0.633940  [   10/   88]
per-ex loss: 0.415292  [   11/   88]
per-ex loss: 0.441729  [   12/   88]
per-ex loss: 0.553912  [   13/   88]
per-ex loss: 0.330858  [   14/   88]
per-ex loss: 0.626719  [   15/   88]
per-ex loss: 0.366945  [   16/   88]
per-ex loss: 0.633505  [   17/   88]
per-ex loss: 0.391061  [   18/   88]
per-ex loss: 0.690719  [   19/   88]
per-ex loss: 0.435699  [   20/   88]
per-ex loss: 0.605827  [   21/   88]
per-ex loss: 0.414606  [   22/   88]
per-ex loss: 0.420848  [   23/   88]
per-ex loss: 0.382598  [   24/   88]
per-ex loss: 0.533370  [   25/   88]
per-ex loss: 0.551855  [   26/   88]
per-ex loss: 0.583556  [   27/   88]
per-ex loss: 0.567037  [   28/   88]
per-ex loss: 0.388324  [   29/   88]
per-ex loss: 0.588312  [   30/   88]
per-ex loss: 0.441026  [   31/   88]
per-ex loss: 0.416505  [   32/   88]
per-ex loss: 0.658841  [   33/   88]
per-ex loss: 0.480793  [   34/   88]
per-ex loss: 0.569228  [   35/   88]
per-ex loss: 0.411558  [   36/   88]
per-ex loss: 0.677101  [   37/   88]
per-ex loss: 0.442451  [   38/   88]
per-ex loss: 0.418178  [   39/   88]
per-ex loss: 0.589902  [   40/   88]
per-ex loss: 0.411093  [   41/   88]
per-ex loss: 0.458532  [   42/   88]
per-ex loss: 0.467868  [   43/   88]
per-ex loss: 0.380645  [   44/   88]
per-ex loss: 0.425353  [   45/   88]
per-ex loss: 0.498013  [   46/   88]
per-ex loss: 0.582281  [   47/   88]
per-ex loss: 0.409842  [   48/   88]
per-ex loss: 0.445217  [   49/   88]
per-ex loss: 0.644356  [   50/   88]
per-ex loss: 0.604995  [   51/   88]
per-ex loss: 0.393237  [   52/   88]
per-ex loss: 0.611958  [   53/   88]
per-ex loss: 0.337352  [   54/   88]
per-ex loss: 0.537098  [   55/   88]
per-ex loss: 0.428462  [   56/   88]
per-ex loss: 0.354131  [   57/   88]
per-ex loss: 0.501735  [   58/   88]
per-ex loss: 0.551202  [   59/   88]
per-ex loss: 0.615552  [   60/   88]
per-ex loss: 0.373635  [   61/   88]
per-ex loss: 0.562468  [   62/   88]
per-ex loss: 0.667767  [   63/   88]
per-ex loss: 0.346927  [   64/   88]
per-ex loss: 0.417849  [   65/   88]
per-ex loss: 0.367455  [   66/   88]
per-ex loss: 0.400572  [   67/   88]
per-ex loss: 0.393082  [   68/   88]
per-ex loss: 0.457881  [   69/   88]
per-ex loss: 0.355732  [   70/   88]
per-ex loss: 0.651106  [   71/   88]
per-ex loss: 0.342036  [   72/   88]
per-ex loss: 0.594122  [   73/   88]
per-ex loss: 0.527067  [   74/   88]
per-ex loss: 0.685202  [   75/   88]
per-ex loss: 0.353848  [   76/   88]
per-ex loss: 0.359908  [   77/   88]
per-ex loss: 0.429092  [   78/   88]
per-ex loss: 0.619589  [   79/   88]
per-ex loss: 0.541267  [   80/   88]
per-ex loss: 0.454276  [   81/   88]
per-ex loss: 0.635048  [   82/   88]
per-ex loss: 0.526896  [   83/   88]
per-ex loss: 0.442455  [   84/   88]
per-ex loss: 0.585134  [   85/   88]
per-ex loss: 0.577157  [   86/   88]
per-ex loss: 0.474233  [   87/   88]
per-ex loss: 0.665864  [   88/   88]
Train Error: Avg loss: 0.49806889
validation Error: 
 Avg loss: 0.56529328 
 F1: 0.461976 
 Precision: 0.439800 
 Recall: 0.486508
 IoU: 0.300370

test Error: 
 Avg loss: 0.50166491 
 F1: 0.548247 
 Precision: 0.535164 
 Recall: 0.561985
 IoU: 0.377645

We have finished training iteration 30
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_28_.pth
per-ex loss: 0.467239  [    1/   88]
per-ex loss: 0.451636  [    2/   88]
per-ex loss: 0.381705  [    3/   88]
per-ex loss: 0.418815  [    4/   88]
per-ex loss: 0.391497  [    5/   88]
per-ex loss: 0.521988  [    6/   88]
per-ex loss: 0.601068  [    7/   88]
per-ex loss: 0.538383  [    8/   88]
per-ex loss: 0.391494  [    9/   88]
per-ex loss: 0.405080  [   10/   88]
per-ex loss: 0.421491  [   11/   88]
per-ex loss: 0.672715  [   12/   88]
per-ex loss: 0.432215  [   13/   88]
per-ex loss: 0.529147  [   14/   88]
per-ex loss: 0.410029  [   15/   88]
per-ex loss: 0.706851  [   16/   88]
per-ex loss: 0.424058  [   17/   88]
per-ex loss: 0.568478  [   18/   88]
per-ex loss: 0.638834  [   19/   88]
per-ex loss: 0.426057  [   20/   88]
per-ex loss: 0.607780  [   21/   88]
per-ex loss: 0.689324  [   22/   88]
per-ex loss: 0.413135  [   23/   88]
per-ex loss: 0.428509  [   24/   88]
per-ex loss: 0.383832  [   25/   88]
per-ex loss: 0.367155  [   26/   88]
per-ex loss: 0.621644  [   27/   88]
per-ex loss: 0.319763  [   28/   88]
per-ex loss: 0.428271  [   29/   88]
per-ex loss: 0.549769  [   30/   88]
per-ex loss: 0.647629  [   31/   88]
per-ex loss: 0.441479  [   32/   88]
per-ex loss: 0.359595  [   33/   88]
per-ex loss: 0.439732  [   34/   88]
per-ex loss: 0.371410  [   35/   88]
per-ex loss: 0.375251  [   36/   88]
per-ex loss: 0.435194  [   37/   88]
per-ex loss: 0.396395  [   38/   88]
per-ex loss: 0.468059  [   39/   88]
per-ex loss: 0.562480  [   40/   88]
per-ex loss: 0.574837  [   41/   88]
per-ex loss: 0.558080  [   42/   88]
per-ex loss: 0.386824  [   43/   88]
per-ex loss: 0.562824  [   44/   88]
per-ex loss: 0.542272  [   45/   88]
per-ex loss: 0.443351  [   46/   88]
per-ex loss: 0.382235  [   47/   88]
per-ex loss: 0.447727  [   48/   88]
per-ex loss: 0.445397  [   49/   88]
per-ex loss: 0.443223  [   50/   88]
per-ex loss: 0.414591  [   51/   88]
per-ex loss: 0.536459  [   52/   88]
per-ex loss: 0.523381  [   53/   88]
per-ex loss: 0.351185  [   54/   88]
per-ex loss: 0.670959  [   55/   88]
per-ex loss: 0.644170  [   56/   88]
per-ex loss: 0.423235  [   57/   88]
per-ex loss: 0.608755  [   58/   88]
per-ex loss: 0.637630  [   59/   88]
per-ex loss: 0.430202  [   60/   88]
per-ex loss: 0.670296  [   61/   88]
per-ex loss: 0.580106  [   62/   88]
per-ex loss: 0.603280  [   63/   88]
per-ex loss: 0.391877  [   64/   88]
per-ex loss: 0.556354  [   65/   88]
per-ex loss: 0.472366  [   66/   88]
per-ex loss: 0.597157  [   67/   88]
per-ex loss: 0.417377  [   68/   88]
per-ex loss: 0.616441  [   69/   88]
per-ex loss: 0.694644  [   70/   88]
per-ex loss: 0.390971  [   71/   88]
per-ex loss: 0.703200  [   72/   88]
per-ex loss: 0.589012  [   73/   88]
per-ex loss: 0.473729  [   74/   88]
per-ex loss: 0.585549  [   75/   88]
per-ex loss: 0.449678  [   76/   88]
per-ex loss: 0.604364  [   77/   88]
per-ex loss: 0.525777  [   78/   88]
per-ex loss: 0.571308  [   79/   88]
per-ex loss: 0.410689  [   80/   88]
per-ex loss: 0.412806  [   81/   88]
per-ex loss: 0.608306  [   82/   88]
per-ex loss: 0.696408  [   83/   88]
per-ex loss: 0.403400  [   84/   88]
per-ex loss: 0.630876  [   85/   88]
per-ex loss: 0.423720  [   86/   88]
per-ex loss: 0.374790  [   87/   88]
per-ex loss: 0.434397  [   88/   88]
Train Error: Avg loss: 0.50024320
validation Error: 
 Avg loss: 0.54334137 
 F1: 0.488105 
 Precision: 0.636472 
 Recall: 0.395834
 IoU: 0.322844

test Error: 
 Avg loss: 0.49733435 
 F1: 0.551103 
 Precision: 0.673488 
 Recall: 0.466358
 IoU: 0.380360

We have finished training iteration 31
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_29_.pth
per-ex loss: 0.413939  [    1/   88]
per-ex loss: 0.403737  [    2/   88]
per-ex loss: 0.674188  [    3/   88]
per-ex loss: 0.418139  [    4/   88]
per-ex loss: 0.427803  [    5/   88]
per-ex loss: 0.420071  [    6/   88]
per-ex loss: 0.406361  [    7/   88]
per-ex loss: 0.423708  [    8/   88]
per-ex loss: 0.433889  [    9/   88]
per-ex loss: 0.358915  [   10/   88]
per-ex loss: 0.370710  [   11/   88]
per-ex loss: 0.365853  [   12/   88]
per-ex loss: 0.443775  [   13/   88]
per-ex loss: 0.353720  [   14/   88]
per-ex loss: 0.570706  [   15/   88]
per-ex loss: 0.525747  [   16/   88]
per-ex loss: 0.608932  [   17/   88]
per-ex loss: 0.628329  [   18/   88]
per-ex loss: 0.442121  [   19/   88]
per-ex loss: 0.663797  [   20/   88]
per-ex loss: 0.433950  [   21/   88]
per-ex loss: 0.414464  [   22/   88]
per-ex loss: 0.680546  [   23/   88]
per-ex loss: 0.539692  [   24/   88]
per-ex loss: 0.508525  [   25/   88]
per-ex loss: 0.583721  [   26/   88]
per-ex loss: 0.554612  [   27/   88]
per-ex loss: 0.675724  [   28/   88]
per-ex loss: 0.590964  [   29/   88]
per-ex loss: 0.577372  [   30/   88]
per-ex loss: 0.457854  [   31/   88]
per-ex loss: 0.416272  [   32/   88]
per-ex loss: 0.434432  [   33/   88]
per-ex loss: 0.459004  [   34/   88]
per-ex loss: 0.522821  [   35/   88]
per-ex loss: 0.461960  [   36/   88]
per-ex loss: 0.574005  [   37/   88]
per-ex loss: 0.429023  [   38/   88]
per-ex loss: 0.475098  [   39/   88]
per-ex loss: 0.310652  [   40/   88]
per-ex loss: 0.427415  [   41/   88]
per-ex loss: 0.513492  [   42/   88]
per-ex loss: 0.601555  [   43/   88]
per-ex loss: 0.336041  [   44/   88]
per-ex loss: 0.404537  [   45/   88]
per-ex loss: 0.535388  [   46/   88]
per-ex loss: 0.433907  [   47/   88]
per-ex loss: 0.376838  [   48/   88]
per-ex loss: 0.634467  [   49/   88]
per-ex loss: 0.528503  [   50/   88]
per-ex loss: 0.668191  [   51/   88]
per-ex loss: 0.595951  [   52/   88]
per-ex loss: 0.455094  [   53/   88]
per-ex loss: 0.580665  [   54/   88]
per-ex loss: 0.369247  [   55/   88]
per-ex loss: 0.400780  [   56/   88]
per-ex loss: 0.546616  [   57/   88]
per-ex loss: 0.579178  [   58/   88]
per-ex loss: 0.472663  [   59/   88]
per-ex loss: 0.733783  [   60/   88]
per-ex loss: 0.595315  [   61/   88]
per-ex loss: 0.374910  [   62/   88]
per-ex loss: 0.356531  [   63/   88]
per-ex loss: 0.398576  [   64/   88]
per-ex loss: 0.641482  [   65/   88]
per-ex loss: 0.392198  [   66/   88]
per-ex loss: 0.442229  [   67/   88]
per-ex loss: 0.644371  [   68/   88]
per-ex loss: 0.697708  [   69/   88]
per-ex loss: 0.549021  [   70/   88]
per-ex loss: 0.610493  [   71/   88]
per-ex loss: 0.471280  [   72/   88]
per-ex loss: 0.542785  [   73/   88]
per-ex loss: 0.688204  [   74/   88]
per-ex loss: 0.567028  [   75/   88]
per-ex loss: 0.558633  [   76/   88]
per-ex loss: 0.477931  [   77/   88]
per-ex loss: 0.449382  [   78/   88]
per-ex loss: 0.415218  [   79/   88]
per-ex loss: 0.476968  [   80/   88]
per-ex loss: 0.367269  [   81/   88]
per-ex loss: 0.587577  [   82/   88]
per-ex loss: 0.595895  [   83/   88]
per-ex loss: 0.686131  [   84/   88]
per-ex loss: 0.433330  [   85/   88]
per-ex loss: 0.357382  [   86/   88]
per-ex loss: 0.384688  [   87/   88]
per-ex loss: 0.594047  [   88/   88]
Train Error: Avg loss: 0.50006845
validation Error: 
 Avg loss: 0.54871850 
 F1: 0.479041 
 Precision: 0.489616 
 Recall: 0.468912
 IoU: 0.314959

test Error: 
 Avg loss: 0.49130193 
 F1: 0.559509 
 Precision: 0.572851 
 Recall: 0.546773
 IoU: 0.388415

We have finished training iteration 32
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_30_.pth
per-ex loss: 0.429672  [    1/   88]
per-ex loss: 0.530122  [    2/   88]
per-ex loss: 0.446996  [    3/   88]
per-ex loss: 0.537733  [    4/   88]
per-ex loss: 0.607295  [    5/   88]
per-ex loss: 0.612072  [    6/   88]
per-ex loss: 0.477053  [    7/   88]
per-ex loss: 0.419615  [    8/   88]
per-ex loss: 0.520788  [    9/   88]
per-ex loss: 0.554738  [   10/   88]
per-ex loss: 0.651790  [   11/   88]
per-ex loss: 0.558606  [   12/   88]
per-ex loss: 0.562313  [   13/   88]
per-ex loss: 0.373565  [   14/   88]
per-ex loss: 0.573489  [   15/   88]
per-ex loss: 0.414273  [   16/   88]
per-ex loss: 0.470587  [   17/   88]
per-ex loss: 0.528441  [   18/   88]
per-ex loss: 0.415449  [   19/   88]
per-ex loss: 0.415432  [   20/   88]
per-ex loss: 0.454718  [   21/   88]
per-ex loss: 0.434386  [   22/   88]
per-ex loss: 0.470410  [   23/   88]
per-ex loss: 0.559304  [   24/   88]
per-ex loss: 0.554305  [   25/   88]
per-ex loss: 0.558266  [   26/   88]
per-ex loss: 0.431143  [   27/   88]
per-ex loss: 0.402890  [   28/   88]
per-ex loss: 0.515948  [   29/   88]
per-ex loss: 0.386343  [   30/   88]
per-ex loss: 0.467884  [   31/   88]
per-ex loss: 0.561287  [   32/   88]
per-ex loss: 0.456922  [   33/   88]
per-ex loss: 0.561691  [   34/   88]
per-ex loss: 0.377200  [   35/   88]
per-ex loss: 0.463920  [   36/   88]
per-ex loss: 0.722064  [   37/   88]
per-ex loss: 0.674803  [   38/   88]
per-ex loss: 0.547143  [   39/   88]
per-ex loss: 0.677726  [   40/   88]
per-ex loss: 0.365582  [   41/   88]
per-ex loss: 0.419627  [   42/   88]
per-ex loss: 0.624679  [   43/   88]
per-ex loss: 0.403485  [   44/   88]
per-ex loss: 0.409450  [   45/   88]
per-ex loss: 0.515390  [   46/   88]
per-ex loss: 0.444432  [   47/   88]
per-ex loss: 0.701747  [   48/   88]
per-ex loss: 0.421824  [   49/   88]
per-ex loss: 0.382892  [   50/   88]
per-ex loss: 0.345489  [   51/   88]
per-ex loss: 0.696844  [   52/   88]
per-ex loss: 0.397744  [   53/   88]
per-ex loss: 0.332140  [   54/   88]
per-ex loss: 0.561117  [   55/   88]
per-ex loss: 0.362889  [   56/   88]
per-ex loss: 0.427767  [   57/   88]
per-ex loss: 0.422608  [   58/   88]
per-ex loss: 0.569391  [   59/   88]
per-ex loss: 0.317436  [   60/   88]
per-ex loss: 0.399936  [   61/   88]
per-ex loss: 0.686405  [   62/   88]
per-ex loss: 0.412128  [   63/   88]
per-ex loss: 0.539843  [   64/   88]
per-ex loss: 0.641071  [   65/   88]
per-ex loss: 0.474400  [   66/   88]
per-ex loss: 0.595630  [   67/   88]
per-ex loss: 0.423877  [   68/   88]
per-ex loss: 0.625417  [   69/   88]
per-ex loss: 0.577909  [   70/   88]
per-ex loss: 0.475201  [   71/   88]
per-ex loss: 0.627560  [   72/   88]
per-ex loss: 0.690244  [   73/   88]
per-ex loss: 0.388909  [   74/   88]
per-ex loss: 0.412785  [   75/   88]
per-ex loss: 0.597516  [   76/   88]
per-ex loss: 0.441449  [   77/   88]
per-ex loss: 0.397124  [   78/   88]
per-ex loss: 0.598191  [   79/   88]
per-ex loss: 0.458718  [   80/   88]
per-ex loss: 0.369211  [   81/   88]
per-ex loss: 0.356274  [   82/   88]
per-ex loss: 0.632455  [   83/   88]
per-ex loss: 0.376489  [   84/   88]
per-ex loss: 0.406407  [   85/   88]
per-ex loss: 0.370198  [   86/   88]
per-ex loss: 0.664975  [   87/   88]
per-ex loss: 0.751146  [   88/   88]
Train Error: Avg loss: 0.49877704
validation Error: 
 Avg loss: 0.60399653 
 F1: 0.416834 
 Precision: 0.622409 
 Recall: 0.313341
 IoU: 0.263291

test Error: 
 Avg loss: 0.53952586 
 F1: 0.505234 
 Precision: 0.741518 
 Recall: 0.383146
 IoU: 0.338002

We have finished training iteration 33
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_25_.pth
per-ex loss: 0.661256  [    1/   88]
per-ex loss: 0.327981  [    2/   88]
per-ex loss: 0.355908  [    3/   88]
per-ex loss: 0.348454  [    4/   88]
per-ex loss: 0.442567  [    5/   88]
per-ex loss: 0.619881  [    6/   88]
per-ex loss: 0.433471  [    7/   88]
per-ex loss: 0.453226  [    8/   88]
per-ex loss: 0.405495  [    9/   88]
per-ex loss: 0.379597  [   10/   88]
per-ex loss: 0.626566  [   11/   88]
per-ex loss: 0.389007  [   12/   88]
per-ex loss: 0.602369  [   13/   88]
per-ex loss: 0.632772  [   14/   88]
per-ex loss: 0.448914  [   15/   88]
per-ex loss: 0.573962  [   16/   88]
per-ex loss: 0.558860  [   17/   88]
per-ex loss: 0.707992  [   18/   88]
per-ex loss: 0.599845  [   19/   88]
per-ex loss: 0.351821  [   20/   88]
per-ex loss: 0.412588  [   21/   88]
per-ex loss: 0.555646  [   22/   88]
per-ex loss: 0.407712  [   23/   88]
per-ex loss: 0.436749  [   24/   88]
per-ex loss: 0.452294  [   25/   88]
per-ex loss: 0.542286  [   26/   88]
per-ex loss: 0.407894  [   27/   88]
per-ex loss: 0.478545  [   28/   88]
per-ex loss: 0.607177  [   29/   88]
per-ex loss: 0.611166  [   30/   88]
per-ex loss: 0.461795  [   31/   88]
per-ex loss: 0.422427  [   32/   88]
per-ex loss: 0.495511  [   33/   88]
per-ex loss: 0.665769  [   34/   88]
per-ex loss: 0.402842  [   35/   88]
per-ex loss: 0.386120  [   36/   88]
per-ex loss: 0.421146  [   37/   88]
per-ex loss: 0.434668  [   38/   88]
per-ex loss: 0.586352  [   39/   88]
per-ex loss: 0.345249  [   40/   88]
per-ex loss: 0.635722  [   41/   88]
per-ex loss: 0.675758  [   42/   88]
per-ex loss: 0.498054  [   43/   88]
per-ex loss: 0.452038  [   44/   88]
per-ex loss: 0.433209  [   45/   88]
per-ex loss: 0.659359  [   46/   88]
per-ex loss: 0.552183  [   47/   88]
per-ex loss: 0.675866  [   48/   88]
per-ex loss: 0.489963  [   49/   88]
per-ex loss: 0.659790  [   50/   88]
per-ex loss: 0.381705  [   51/   88]
per-ex loss: 0.431432  [   52/   88]
per-ex loss: 0.534332  [   53/   88]
per-ex loss: 0.646063  [   54/   88]
per-ex loss: 0.445324  [   55/   88]
per-ex loss: 0.406199  [   56/   88]
per-ex loss: 0.419307  [   57/   88]
per-ex loss: 0.347169  [   58/   88]
per-ex loss: 0.500251  [   59/   88]
per-ex loss: 0.455726  [   60/   88]
per-ex loss: 0.397111  [   61/   88]
per-ex loss: 0.452871  [   62/   88]
per-ex loss: 0.384213  [   63/   88]
per-ex loss: 0.443327  [   64/   88]
per-ex loss: 0.377627  [   65/   88]
per-ex loss: 0.608947  [   66/   88]
per-ex loss: 0.576872  [   67/   88]
per-ex loss: 0.408664  [   68/   88]
per-ex loss: 0.351302  [   69/   88]
per-ex loss: 0.610724  [   70/   88]
per-ex loss: 0.630756  [   71/   88]
per-ex loss: 0.487616  [   72/   88]
per-ex loss: 0.380254  [   73/   88]
per-ex loss: 0.590591  [   74/   88]
per-ex loss: 0.462335  [   75/   88]
per-ex loss: 0.577172  [   76/   88]
per-ex loss: 0.542467  [   77/   88]
per-ex loss: 0.395813  [   78/   88]
per-ex loss: 0.539179  [   79/   88]
per-ex loss: 0.392687  [   80/   88]
per-ex loss: 0.683097  [   81/   88]
per-ex loss: 0.558540  [   82/   88]
per-ex loss: 0.552857  [   83/   88]
per-ex loss: 0.461883  [   84/   88]
per-ex loss: 0.584680  [   85/   88]
per-ex loss: 0.562954  [   86/   88]
per-ex loss: 0.653143  [   87/   88]
per-ex loss: 0.538654  [   88/   88]
Train Error: Avg loss: 0.49954165
validation Error: 
 Avg loss: 0.53484142 
 F1: 0.497886 
 Precision: 0.558214 
 Recall: 0.449325
 IoU: 0.331456

test Error: 
 Avg loss: 0.48713176 
 F1: 0.564872 
 Precision: 0.596936 
 Recall: 0.536077
 IoU: 0.393604

We have finished training iteration 34
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_32_.pth
per-ex loss: 0.423036  [    1/   88]
per-ex loss: 0.405464  [    2/   88]
per-ex loss: 0.406061  [    3/   88]
per-ex loss: 0.627433  [    4/   88]
per-ex loss: 0.503079  [    5/   88]
per-ex loss: 0.580678  [    6/   88]
per-ex loss: 0.415260  [    7/   88]
per-ex loss: 0.549582  [    8/   88]
per-ex loss: 0.400737  [    9/   88]
per-ex loss: 0.562794  [   10/   88]
per-ex loss: 0.457677  [   11/   88]
per-ex loss: 0.599927  [   12/   88]
per-ex loss: 0.464460  [   13/   88]
per-ex loss: 0.636744  [   14/   88]
per-ex loss: 0.607387  [   15/   88]
per-ex loss: 0.486357  [   16/   88]
per-ex loss: 0.541195  [   17/   88]
per-ex loss: 0.440145  [   18/   88]
per-ex loss: 0.388027  [   19/   88]
per-ex loss: 0.533167  [   20/   88]
per-ex loss: 0.584884  [   21/   88]
per-ex loss: 0.564674  [   22/   88]
per-ex loss: 0.373554  [   23/   88]
per-ex loss: 0.569832  [   24/   88]
per-ex loss: 0.401529  [   25/   88]
per-ex loss: 0.374162  [   26/   88]
per-ex loss: 0.402895  [   27/   88]
per-ex loss: 0.333986  [   28/   88]
per-ex loss: 0.415022  [   29/   88]
per-ex loss: 0.424117  [   30/   88]
per-ex loss: 0.631057  [   31/   88]
per-ex loss: 0.459940  [   32/   88]
per-ex loss: 0.399446  [   33/   88]
per-ex loss: 0.746322  [   34/   88]
per-ex loss: 0.384764  [   35/   88]
per-ex loss: 0.696926  [   36/   88]
per-ex loss: 0.488357  [   37/   88]
per-ex loss: 0.322195  [   38/   88]
per-ex loss: 0.547781  [   39/   88]
per-ex loss: 0.448352  [   40/   88]
per-ex loss: 0.530732  [   41/   88]
per-ex loss: 0.662755  [   42/   88]
per-ex loss: 0.526014  [   43/   88]
per-ex loss: 0.585155  [   44/   88]
per-ex loss: 0.376095  [   45/   88]
per-ex loss: 0.753404  [   46/   88]
per-ex loss: 0.367537  [   47/   88]
per-ex loss: 0.407507  [   48/   88]
per-ex loss: 0.610817  [   49/   88]
per-ex loss: 0.577810  [   50/   88]
per-ex loss: 0.410862  [   51/   88]
per-ex loss: 0.569226  [   52/   88]
per-ex loss: 0.433715  [   53/   88]
per-ex loss: 0.427468  [   54/   88]
per-ex loss: 0.434605  [   55/   88]
per-ex loss: 0.404920  [   56/   88]
per-ex loss: 0.441052  [   57/   88]
per-ex loss: 0.632684  [   58/   88]
per-ex loss: 0.398604  [   59/   88]
per-ex loss: 0.464505  [   60/   88]
per-ex loss: 0.441593  [   61/   88]
per-ex loss: 0.371859  [   62/   88]
per-ex loss: 0.611355  [   63/   88]
per-ex loss: 0.373479  [   64/   88]
per-ex loss: 0.526135  [   65/   88]
per-ex loss: 0.711777  [   66/   88]
per-ex loss: 0.556490  [   67/   88]
per-ex loss: 0.551273  [   68/   88]
per-ex loss: 0.467420  [   69/   88]
per-ex loss: 0.597876  [   70/   88]
per-ex loss: 0.439342  [   71/   88]
per-ex loss: 0.450908  [   72/   88]
per-ex loss: 0.577331  [   73/   88]
per-ex loss: 0.638501  [   74/   88]
per-ex loss: 0.404338  [   75/   88]
per-ex loss: 0.535817  [   76/   88]
per-ex loss: 0.677888  [   77/   88]
per-ex loss: 0.646570  [   78/   88]
per-ex loss: 0.578109  [   79/   88]
per-ex loss: 0.545031  [   80/   88]
per-ex loss: 0.403046  [   81/   88]
per-ex loss: 0.387705  [   82/   88]
per-ex loss: 0.618417  [   83/   88]
per-ex loss: 0.361998  [   84/   88]
per-ex loss: 0.436389  [   85/   88]
per-ex loss: 0.381878  [   86/   88]
per-ex loss: 0.610505  [   87/   88]
per-ex loss: 0.581252  [   88/   88]
Train Error: Avg loss: 0.50112220
validation Error: 
 Avg loss: 0.55109507 
 F1: 0.478562 
 Precision: 0.524958 
 Recall: 0.439701
 IoU: 0.314546

test Error: 
 Avg loss: 0.49764251 
 F1: 0.558060 
 Precision: 0.599032 
 Recall: 0.522334
 IoU: 0.387021

We have finished training iteration 35
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_33_.pth
per-ex loss: 0.413543  [    1/   88]
per-ex loss: 0.602792  [    2/   88]
per-ex loss: 0.660322  [    3/   88]
per-ex loss: 0.440812  [    4/   88]
per-ex loss: 0.538478  [    5/   88]
per-ex loss: 0.364261  [    6/   88]
per-ex loss: 0.410249  [    7/   88]
per-ex loss: 0.596388  [    8/   88]
per-ex loss: 0.360316  [    9/   88]
per-ex loss: 0.603019  [   10/   88]
per-ex loss: 0.422181  [   11/   88]
per-ex loss: 0.428141  [   12/   88]
per-ex loss: 0.613876  [   13/   88]
per-ex loss: 0.572166  [   14/   88]
per-ex loss: 0.540316  [   15/   88]
per-ex loss: 0.630308  [   16/   88]
per-ex loss: 0.430648  [   17/   88]
per-ex loss: 0.458502  [   18/   88]
per-ex loss: 0.631116  [   19/   88]
per-ex loss: 0.558354  [   20/   88]
per-ex loss: 0.371775  [   21/   88]
per-ex loss: 0.405452  [   22/   88]
per-ex loss: 0.544101  [   23/   88]
per-ex loss: 0.675958  [   24/   88]
per-ex loss: 0.427989  [   25/   88]
per-ex loss: 0.530523  [   26/   88]
per-ex loss: 0.423373  [   27/   88]
per-ex loss: 0.473538  [   28/   88]
per-ex loss: 0.400472  [   29/   88]
per-ex loss: 0.364074  [   30/   88]
per-ex loss: 0.362657  [   31/   88]
per-ex loss: 0.335875  [   32/   88]
per-ex loss: 0.380483  [   33/   88]
per-ex loss: 0.561502  [   34/   88]
per-ex loss: 0.377465  [   35/   88]
per-ex loss: 0.658367  [   36/   88]
per-ex loss: 0.544952  [   37/   88]
per-ex loss: 0.498624  [   38/   88]
per-ex loss: 0.623605  [   39/   88]
per-ex loss: 0.660224  [   40/   88]
per-ex loss: 0.527035  [   41/   88]
per-ex loss: 0.573667  [   42/   88]
per-ex loss: 0.477395  [   43/   88]
per-ex loss: 0.577346  [   44/   88]
per-ex loss: 0.545909  [   45/   88]
per-ex loss: 0.451007  [   46/   88]
per-ex loss: 0.694288  [   47/   88]
per-ex loss: 0.504017  [   48/   88]
per-ex loss: 0.447533  [   49/   88]
per-ex loss: 0.422545  [   50/   88]
per-ex loss: 0.415761  [   51/   88]
per-ex loss: 0.429142  [   52/   88]
per-ex loss: 0.389360  [   53/   88]
per-ex loss: 0.364731  [   54/   88]
per-ex loss: 0.566010  [   55/   88]
per-ex loss: 0.460658  [   56/   88]
per-ex loss: 0.614008  [   57/   88]
per-ex loss: 0.648833  [   58/   88]
per-ex loss: 0.618805  [   59/   88]
per-ex loss: 0.352603  [   60/   88]
per-ex loss: 0.672889  [   61/   88]
per-ex loss: 0.457148  [   62/   88]
per-ex loss: 0.578236  [   63/   88]
per-ex loss: 0.419083  [   64/   88]
per-ex loss: 0.692934  [   65/   88]
per-ex loss: 0.662480  [   66/   88]
per-ex loss: 0.397183  [   67/   88]
per-ex loss: 0.619758  [   68/   88]
per-ex loss: 0.461718  [   69/   88]
per-ex loss: 0.458933  [   70/   88]
per-ex loss: 0.389030  [   71/   88]
per-ex loss: 0.683138  [   72/   88]
per-ex loss: 0.573507  [   73/   88]
per-ex loss: 0.379639  [   74/   88]
per-ex loss: 0.679284  [   75/   88]
per-ex loss: 0.627157  [   76/   88]
per-ex loss: 0.438372  [   77/   88]
per-ex loss: 0.533281  [   78/   88]
per-ex loss: 0.379381  [   79/   88]
per-ex loss: 0.307547  [   80/   88]
per-ex loss: 0.558452  [   81/   88]
per-ex loss: 0.478271  [   82/   88]
per-ex loss: 0.454740  [   83/   88]
per-ex loss: 0.584335  [   84/   88]
per-ex loss: 0.428025  [   85/   88]
per-ex loss: 0.435266  [   86/   88]
per-ex loss: 0.404188  [   87/   88]
per-ex loss: 0.429388  [   88/   88]
Train Error: Avg loss: 0.50182742
validation Error: 
 Avg loss: 0.54735691 
 F1: 0.486850 
 Precision: 0.610900 
 Recall: 0.404676
 IoU: 0.321746

test Error: 
 Avg loss: 0.49547506 
 F1: 0.560110 
 Precision: 0.649306 
 Recall: 0.492460
 IoU: 0.388995

We have finished training iteration 36
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_27_.pth
per-ex loss: 0.676611  [    1/   88]
per-ex loss: 0.683264  [    2/   88]
per-ex loss: 0.653554  [    3/   88]
per-ex loss: 0.399980  [    4/   88]
per-ex loss: 0.688195  [    5/   88]
per-ex loss: 0.455546  [    6/   88]
per-ex loss: 0.342580  [    7/   88]
per-ex loss: 0.457626  [    8/   88]
per-ex loss: 0.611964  [    9/   88]
per-ex loss: 0.589243  [   10/   88]
per-ex loss: 0.429362  [   11/   88]
per-ex loss: 0.389993  [   12/   88]
per-ex loss: 0.413945  [   13/   88]
per-ex loss: 0.579372  [   14/   88]
per-ex loss: 0.413360  [   15/   88]
per-ex loss: 0.562728  [   16/   88]
per-ex loss: 0.609629  [   17/   88]
per-ex loss: 0.538338  [   18/   88]
per-ex loss: 0.443199  [   19/   88]
per-ex loss: 0.434135  [   20/   88]
per-ex loss: 0.586446  [   21/   88]
per-ex loss: 0.651177  [   22/   88]
per-ex loss: 0.638086  [   23/   88]
per-ex loss: 0.404139  [   24/   88]
per-ex loss: 0.442174  [   25/   88]
per-ex loss: 0.624921  [   26/   88]
per-ex loss: 0.343511  [   27/   88]
per-ex loss: 0.365148  [   28/   88]
per-ex loss: 0.353065  [   29/   88]
per-ex loss: 0.462578  [   30/   88]
per-ex loss: 0.435280  [   31/   88]
per-ex loss: 0.554194  [   32/   88]
per-ex loss: 0.440239  [   33/   88]
per-ex loss: 0.606364  [   34/   88]
per-ex loss: 0.424987  [   35/   88]
per-ex loss: 0.691082  [   36/   88]
per-ex loss: 0.425026  [   37/   88]
per-ex loss: 0.358418  [   38/   88]
per-ex loss: 0.533969  [   39/   88]
per-ex loss: 0.558233  [   40/   88]
per-ex loss: 0.410814  [   41/   88]
per-ex loss: 0.605444  [   42/   88]
per-ex loss: 0.645834  [   43/   88]
per-ex loss: 0.368595  [   44/   88]
per-ex loss: 0.562984  [   45/   88]
per-ex loss: 0.425399  [   46/   88]
per-ex loss: 0.593381  [   47/   88]
per-ex loss: 0.443028  [   48/   88]
per-ex loss: 0.530001  [   49/   88]
per-ex loss: 0.426206  [   50/   88]
per-ex loss: 0.437656  [   51/   88]
per-ex loss: 0.504672  [   52/   88]
per-ex loss: 0.674659  [   53/   88]
per-ex loss: 0.417116  [   54/   88]
per-ex loss: 0.530035  [   55/   88]
per-ex loss: 0.385316  [   56/   88]
per-ex loss: 0.391224  [   57/   88]
per-ex loss: 0.530399  [   58/   88]
per-ex loss: 0.599403  [   59/   88]
per-ex loss: 0.380192  [   60/   88]
per-ex loss: 0.457395  [   61/   88]
per-ex loss: 0.359232  [   62/   88]
per-ex loss: 0.566559  [   63/   88]
per-ex loss: 0.388582  [   64/   88]
per-ex loss: 0.346173  [   65/   88]
per-ex loss: 0.610476  [   66/   88]
per-ex loss: 0.327065  [   67/   88]
per-ex loss: 0.434928  [   68/   88]
per-ex loss: 0.441157  [   69/   88]
per-ex loss: 0.518437  [   70/   88]
per-ex loss: 0.431285  [   71/   88]
per-ex loss: 0.324286  [   72/   88]
per-ex loss: 0.384529  [   73/   88]
per-ex loss: 0.427279  [   74/   88]
per-ex loss: 0.397770  [   75/   88]
per-ex loss: 0.396336  [   76/   88]
per-ex loss: 0.699691  [   77/   88]
per-ex loss: 0.604018  [   78/   88]
per-ex loss: 0.378345  [   79/   88]
per-ex loss: 0.532238  [   80/   88]
per-ex loss: 0.663230  [   81/   88]
per-ex loss: 0.371996  [   82/   88]
per-ex loss: 0.513330  [   83/   88]
per-ex loss: 0.538883  [   84/   88]
per-ex loss: 0.590113  [   85/   88]
per-ex loss: 0.583658  [   86/   88]
per-ex loss: 0.580361  [   87/   88]
per-ex loss: 0.466573  [   88/   88]
Train Error: Avg loss: 0.49395392
validation Error: 
 Avg loss: 0.54089902 
 F1: 0.485780 
 Precision: 0.492766 
 Recall: 0.478990
 IoU: 0.320812

test Error: 
 Avg loss: 0.49102467 
 F1: 0.561122 
 Precision: 0.554524 
 Recall: 0.567879
 IoU: 0.389972

We have finished training iteration 37
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_35_.pth
per-ex loss: 0.500374  [    1/   88]
per-ex loss: 0.622806  [    2/   88]
per-ex loss: 0.456499  [    3/   88]
per-ex loss: 0.449952  [    4/   88]
per-ex loss: 0.436154  [    5/   88]
per-ex loss: 0.410106  [    6/   88]
per-ex loss: 0.676902  [    7/   88]
per-ex loss: 0.403159  [    8/   88]
per-ex loss: 0.409712  [    9/   88]
per-ex loss: 0.394044  [   10/   88]
per-ex loss: 0.365716  [   11/   88]
per-ex loss: 0.434582  [   12/   88]
per-ex loss: 0.399103  [   13/   88]
per-ex loss: 0.621692  [   14/   88]
per-ex loss: 0.597178  [   15/   88]
per-ex loss: 0.445721  [   16/   88]
per-ex loss: 0.430139  [   17/   88]
per-ex loss: 0.465069  [   18/   88]
per-ex loss: 0.359268  [   19/   88]
per-ex loss: 0.422216  [   20/   88]
per-ex loss: 0.428994  [   21/   88]
per-ex loss: 0.452882  [   22/   88]
per-ex loss: 0.421868  [   23/   88]
per-ex loss: 0.603081  [   24/   88]
per-ex loss: 0.574573  [   25/   88]
per-ex loss: 0.676428  [   26/   88]
per-ex loss: 0.577274  [   27/   88]
per-ex loss: 0.442457  [   28/   88]
per-ex loss: 0.639904  [   29/   88]
per-ex loss: 0.592924  [   30/   88]
per-ex loss: 0.466377  [   31/   88]
per-ex loss: 0.395603  [   32/   88]
per-ex loss: 0.538645  [   33/   88]
per-ex loss: 0.421778  [   34/   88]
per-ex loss: 0.393760  [   35/   88]
per-ex loss: 0.529261  [   36/   88]
per-ex loss: 0.665903  [   37/   88]
per-ex loss: 0.419259  [   38/   88]
per-ex loss: 0.380697  [   39/   88]
per-ex loss: 0.367126  [   40/   88]
per-ex loss: 0.563218  [   41/   88]
per-ex loss: 0.679923  [   42/   88]
per-ex loss: 0.326019  [   43/   88]
per-ex loss: 0.600712  [   44/   88]
per-ex loss: 0.436936  [   45/   88]
per-ex loss: 0.414863  [   46/   88]
per-ex loss: 0.598094  [   47/   88]
per-ex loss: 0.660053  [   48/   88]
per-ex loss: 0.448718  [   49/   88]
per-ex loss: 0.561711  [   50/   88]
per-ex loss: 0.506816  [   51/   88]
per-ex loss: 0.455647  [   52/   88]
per-ex loss: 0.556159  [   53/   88]
per-ex loss: 0.383227  [   54/   88]
per-ex loss: 0.405005  [   55/   88]
per-ex loss: 0.397373  [   56/   88]
per-ex loss: 0.405873  [   57/   88]
per-ex loss: 0.548557  [   58/   88]
per-ex loss: 0.604305  [   59/   88]
per-ex loss: 0.575762  [   60/   88]
per-ex loss: 0.420540  [   61/   88]
per-ex loss: 0.407914  [   62/   88]
per-ex loss: 0.589365  [   63/   88]
per-ex loss: 0.697027  [   64/   88]
per-ex loss: 0.361323  [   65/   88]
per-ex loss: 0.370824  [   66/   88]
per-ex loss: 0.636415  [   67/   88]
per-ex loss: 0.629601  [   68/   88]
per-ex loss: 0.404768  [   69/   88]
per-ex loss: 0.561503  [   70/   88]
per-ex loss: 0.633572  [   71/   88]
per-ex loss: 0.397795  [   72/   88]
per-ex loss: 0.578851  [   73/   88]
per-ex loss: 0.447622  [   74/   88]
per-ex loss: 0.697286  [   75/   88]
per-ex loss: 0.360308  [   76/   88]
per-ex loss: 0.520540  [   77/   88]
per-ex loss: 0.582401  [   78/   88]
per-ex loss: 0.603549  [   79/   88]
per-ex loss: 0.548596  [   80/   88]
per-ex loss: 0.420428  [   81/   88]
per-ex loss: 0.358889  [   82/   88]
per-ex loss: 0.594920  [   83/   88]
per-ex loss: 0.392853  [   84/   88]
per-ex loss: 0.383953  [   85/   88]
per-ex loss: 0.401613  [   86/   88]
per-ex loss: 0.559217  [   87/   88]
per-ex loss: 0.540892  [   88/   88]
Train Error: Avg loss: 0.49455361
validation Error: 
 Avg loss: 0.53281350 
 F1: 0.497622 
 Precision: 0.604452 
 Recall: 0.422882
 IoU: 0.331223

test Error: 
 Avg loss: 0.49006859 
 F1: 0.564127 
 Precision: 0.619254 
 Recall: 0.518013
 IoU: 0.392881

We have finished training iteration 38
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_36_.pth
per-ex loss: 0.567909  [    1/   88]
per-ex loss: 0.602392  [    2/   88]
per-ex loss: 0.682003  [    3/   88]
per-ex loss: 0.469689  [    4/   88]
per-ex loss: 0.450035  [    5/   88]
per-ex loss: 0.565869  [    6/   88]
per-ex loss: 0.658318  [    7/   88]
per-ex loss: 0.374975  [    8/   88]
per-ex loss: 0.587094  [    9/   88]
per-ex loss: 0.615984  [   10/   88]
per-ex loss: 0.398542  [   11/   88]
per-ex loss: 0.361881  [   12/   88]
per-ex loss: 0.426595  [   13/   88]
per-ex loss: 0.458655  [   14/   88]
per-ex loss: 0.564651  [   15/   88]
per-ex loss: 0.388882  [   16/   88]
per-ex loss: 0.561405  [   17/   88]
per-ex loss: 0.420330  [   18/   88]
per-ex loss: 0.516518  [   19/   88]
per-ex loss: 0.634267  [   20/   88]
per-ex loss: 0.368509  [   21/   88]
per-ex loss: 0.392179  [   22/   88]
per-ex loss: 0.340005  [   23/   88]
per-ex loss: 0.423812  [   24/   88]
per-ex loss: 0.468513  [   25/   88]
per-ex loss: 0.376979  [   26/   88]
per-ex loss: 0.433695  [   27/   88]
per-ex loss: 0.693640  [   28/   88]
per-ex loss: 0.330511  [   29/   88]
per-ex loss: 0.395027  [   30/   88]
per-ex loss: 0.559778  [   31/   88]
per-ex loss: 0.410301  [   32/   88]
per-ex loss: 0.387918  [   33/   88]
per-ex loss: 0.416448  [   34/   88]
per-ex loss: 0.411672  [   35/   88]
per-ex loss: 0.421793  [   36/   88]
per-ex loss: 0.560966  [   37/   88]
per-ex loss: 0.673382  [   38/   88]
per-ex loss: 0.370329  [   39/   88]
per-ex loss: 0.646218  [   40/   88]
per-ex loss: 0.379860  [   41/   88]
per-ex loss: 0.455899  [   42/   88]
per-ex loss: 0.429093  [   43/   88]
per-ex loss: 0.411938  [   44/   88]
per-ex loss: 0.496231  [   45/   88]
per-ex loss: 0.526820  [   46/   88]
per-ex loss: 0.420359  [   47/   88]
per-ex loss: 0.517514  [   48/   88]
per-ex loss: 0.673705  [   49/   88]
per-ex loss: 0.557292  [   50/   88]
per-ex loss: 0.432913  [   51/   88]
per-ex loss: 0.660090  [   52/   88]
per-ex loss: 0.584302  [   53/   88]
per-ex loss: 0.445657  [   54/   88]
per-ex loss: 0.520083  [   55/   88]
per-ex loss: 0.518695  [   56/   88]
per-ex loss: 0.566454  [   57/   88]
per-ex loss: 0.681304  [   58/   88]
per-ex loss: 0.382824  [   59/   88]
per-ex loss: 0.415309  [   60/   88]
per-ex loss: 0.430723  [   61/   88]
per-ex loss: 0.351259  [   62/   88]
per-ex loss: 0.686500  [   63/   88]
per-ex loss: 0.407499  [   64/   88]
per-ex loss: 0.624089  [   65/   88]
per-ex loss: 0.409157  [   66/   88]
per-ex loss: 0.354257  [   67/   88]
per-ex loss: 0.348050  [   68/   88]
per-ex loss: 0.566303  [   69/   88]
per-ex loss: 0.576417  [   70/   88]
per-ex loss: 0.351835  [   71/   88]
per-ex loss: 0.620751  [   72/   88]
per-ex loss: 0.366031  [   73/   88]
per-ex loss: 0.588115  [   74/   88]
per-ex loss: 0.541938  [   75/   88]
per-ex loss: 0.590447  [   76/   88]
per-ex loss: 0.360513  [   77/   88]
per-ex loss: 0.589740  [   78/   88]
per-ex loss: 0.593962  [   79/   88]
per-ex loss: 0.438968  [   80/   88]
per-ex loss: 0.436241  [   81/   88]
per-ex loss: 0.445100  [   82/   88]
per-ex loss: 0.666364  [   83/   88]
per-ex loss: 0.456800  [   84/   88]
per-ex loss: 0.597525  [   85/   88]
per-ex loss: 0.475723  [   86/   88]
per-ex loss: 0.549463  [   87/   88]
per-ex loss: 0.444941  [   88/   88]
Train Error: Avg loss: 0.49241733
validation Error: 
 Avg loss: 0.53713382 
 F1: 0.492554 
 Precision: 0.524798 
 Recall: 0.464042
 IoU: 0.326747

test Error: 
 Avg loss: 0.48107199 
 F1: 0.569641 
 Precision: 0.582727 
 Recall: 0.557129
 IoU: 0.398250

We have finished training iteration 39
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_31_.pth
per-ex loss: 0.665890  [    1/   88]
per-ex loss: 0.551446  [    2/   88]
per-ex loss: 0.574753  [    3/   88]
per-ex loss: 0.640112  [    4/   88]
per-ex loss: 0.636495  [    5/   88]
per-ex loss: 0.579813  [    6/   88]
per-ex loss: 0.372891  [    7/   88]
per-ex loss: 0.523537  [    8/   88]
per-ex loss: 0.354030  [    9/   88]
per-ex loss: 0.461653  [   10/   88]
per-ex loss: 0.403305  [   11/   88]
per-ex loss: 0.426392  [   12/   88]
per-ex loss: 0.373468  [   13/   88]
per-ex loss: 0.517360  [   14/   88]
per-ex loss: 0.381238  [   15/   88]
per-ex loss: 0.389174  [   16/   88]
per-ex loss: 0.620703  [   17/   88]
per-ex loss: 0.480465  [   18/   88]
per-ex loss: 0.633984  [   19/   88]
per-ex loss: 0.505792  [   20/   88]
per-ex loss: 0.354180  [   21/   88]
per-ex loss: 0.577451  [   22/   88]
per-ex loss: 0.375800  [   23/   88]
per-ex loss: 0.536226  [   24/   88]
per-ex loss: 0.425827  [   25/   88]
per-ex loss: 0.624075  [   26/   88]
per-ex loss: 0.445985  [   27/   88]
per-ex loss: 0.411590  [   28/   88]
per-ex loss: 0.405054  [   29/   88]
per-ex loss: 0.520858  [   30/   88]
per-ex loss: 0.468415  [   31/   88]
per-ex loss: 0.668361  [   32/   88]
per-ex loss: 0.408385  [   33/   88]
per-ex loss: 0.422987  [   34/   88]
per-ex loss: 0.430895  [   35/   88]
per-ex loss: 0.558333  [   36/   88]
per-ex loss: 0.619827  [   37/   88]
per-ex loss: 0.432526  [   38/   88]
per-ex loss: 0.443086  [   39/   88]
per-ex loss: 0.370364  [   40/   88]
per-ex loss: 0.448852  [   41/   88]
per-ex loss: 0.646005  [   42/   88]
per-ex loss: 0.334541  [   43/   88]
per-ex loss: 0.651303  [   44/   88]
per-ex loss: 0.553949  [   45/   88]
per-ex loss: 0.657430  [   46/   88]
per-ex loss: 0.541458  [   47/   88]
per-ex loss: 0.461492  [   48/   88]
per-ex loss: 0.457182  [   49/   88]
per-ex loss: 0.535619  [   50/   88]
per-ex loss: 0.439681  [   51/   88]
per-ex loss: 0.413095  [   52/   88]
per-ex loss: 0.547293  [   53/   88]
per-ex loss: 0.469020  [   54/   88]
per-ex loss: 0.389673  [   55/   88]
per-ex loss: 0.672116  [   56/   88]
per-ex loss: 0.403608  [   57/   88]
per-ex loss: 0.677444  [   58/   88]
per-ex loss: 0.381880  [   59/   88]
per-ex loss: 0.384536  [   60/   88]
per-ex loss: 0.447052  [   61/   88]
per-ex loss: 0.426414  [   62/   88]
per-ex loss: 0.527285  [   63/   88]
per-ex loss: 0.543451  [   64/   88]
per-ex loss: 0.558266  [   65/   88]
per-ex loss: 0.361125  [   66/   88]
per-ex loss: 0.393999  [   67/   88]
per-ex loss: 0.424263  [   68/   88]
per-ex loss: 0.593261  [   69/   88]
per-ex loss: 0.610851  [   70/   88]
per-ex loss: 0.543638  [   71/   88]
per-ex loss: 0.394654  [   72/   88]
per-ex loss: 0.388835  [   73/   88]
per-ex loss: 0.424773  [   74/   88]
per-ex loss: 0.365812  [   75/   88]
per-ex loss: 0.598006  [   76/   88]
per-ex loss: 0.380182  [   77/   88]
per-ex loss: 0.568775  [   78/   88]
per-ex loss: 0.544555  [   79/   88]
per-ex loss: 0.533941  [   80/   88]
per-ex loss: 0.704588  [   81/   88]
per-ex loss: 0.322897  [   82/   88]
per-ex loss: 0.586210  [   83/   88]
per-ex loss: 0.402859  [   84/   88]
per-ex loss: 0.347394  [   85/   88]
per-ex loss: 0.742490  [   86/   88]
per-ex loss: 0.616869  [   87/   88]
per-ex loss: 0.602631  [   88/   88]
Train Error: Avg loss: 0.49561337
validation Error: 
 Avg loss: 0.53331562 
 F1: 0.494400 
 Precision: 0.560036 
 Recall: 0.442535
 IoU: 0.328374

test Error: 
 Avg loss: 0.48688729 
 F1: 0.565887 
 Precision: 0.594541 
 Recall: 0.539868
 IoU: 0.394590

We have finished training iteration 40
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_37_.pth
per-ex loss: 0.566082  [    1/   88]
per-ex loss: 0.394129  [    2/   88]
per-ex loss: 0.347929  [    3/   88]
per-ex loss: 0.446195  [    4/   88]
per-ex loss: 0.591397  [    5/   88]
per-ex loss: 0.380058  [    6/   88]
per-ex loss: 0.645017  [    7/   88]
per-ex loss: 0.511093  [    8/   88]
per-ex loss: 0.646814  [    9/   88]
per-ex loss: 0.447121  [   10/   88]
per-ex loss: 0.469664  [   11/   88]
per-ex loss: 0.549692  [   12/   88]
per-ex loss: 0.468826  [   13/   88]
per-ex loss: 0.425521  [   14/   88]
per-ex loss: 0.705158  [   15/   88]
per-ex loss: 0.405670  [   16/   88]
per-ex loss: 0.396382  [   17/   88]
per-ex loss: 0.411947  [   18/   88]
per-ex loss: 0.402118  [   19/   88]
per-ex loss: 0.575769  [   20/   88]
per-ex loss: 0.421975  [   21/   88]
per-ex loss: 0.394944  [   22/   88]
per-ex loss: 0.347606  [   23/   88]
per-ex loss: 0.451315  [   24/   88]
per-ex loss: 0.361422  [   25/   88]
per-ex loss: 0.637568  [   26/   88]
per-ex loss: 0.704345  [   27/   88]
per-ex loss: 0.553947  [   28/   88]
per-ex loss: 0.569966  [   29/   88]
per-ex loss: 0.418542  [   30/   88]
per-ex loss: 0.330483  [   31/   88]
per-ex loss: 0.702305  [   32/   88]
per-ex loss: 0.465807  [   33/   88]
per-ex loss: 0.333760  [   34/   88]
per-ex loss: 0.543763  [   35/   88]
per-ex loss: 0.560725  [   36/   88]
per-ex loss: 0.553193  [   37/   88]
per-ex loss: 0.425089  [   38/   88]
per-ex loss: 0.459179  [   39/   88]
per-ex loss: 0.520447  [   40/   88]
per-ex loss: 0.570646  [   41/   88]
per-ex loss: 0.351385  [   42/   88]
per-ex loss: 0.376596  [   43/   88]
per-ex loss: 0.616816  [   44/   88]
per-ex loss: 0.599112  [   45/   88]
per-ex loss: 0.591528  [   46/   88]
per-ex loss: 0.378583  [   47/   88]
per-ex loss: 0.381658  [   48/   88]
per-ex loss: 0.676484  [   49/   88]
per-ex loss: 0.448624  [   50/   88]
per-ex loss: 0.408924  [   51/   88]
per-ex loss: 0.414142  [   52/   88]
per-ex loss: 0.649027  [   53/   88]
per-ex loss: 0.412822  [   54/   88]
per-ex loss: 0.354105  [   55/   88]
per-ex loss: 0.534660  [   56/   88]
per-ex loss: 0.633111  [   57/   88]
per-ex loss: 0.373705  [   58/   88]
per-ex loss: 0.530337  [   59/   88]
per-ex loss: 0.607487  [   60/   88]
per-ex loss: 0.585303  [   61/   88]
per-ex loss: 0.374802  [   62/   88]
per-ex loss: 0.448478  [   63/   88]
per-ex loss: 0.392274  [   64/   88]
per-ex loss: 0.440354  [   65/   88]
per-ex loss: 0.692733  [   66/   88]
per-ex loss: 0.481228  [   67/   88]
per-ex loss: 0.692453  [   68/   88]
per-ex loss: 0.652878  [   69/   88]
per-ex loss: 0.408912  [   70/   88]
per-ex loss: 0.400858  [   71/   88]
per-ex loss: 0.583395  [   72/   88]
per-ex loss: 0.458529  [   73/   88]
per-ex loss: 0.400743  [   74/   88]
per-ex loss: 0.520473  [   75/   88]
per-ex loss: 0.615804  [   76/   88]
per-ex loss: 0.632369  [   77/   88]
per-ex loss: 0.440475  [   78/   88]
per-ex loss: 0.490580  [   79/   88]
per-ex loss: 0.551999  [   80/   88]
per-ex loss: 0.416783  [   81/   88]
per-ex loss: 0.582565  [   82/   88]
per-ex loss: 0.506093  [   83/   88]
per-ex loss: 0.413766  [   84/   88]
per-ex loss: 0.529429  [   85/   88]
per-ex loss: 0.405070  [   86/   88]
per-ex loss: 0.379906  [   87/   88]
per-ex loss: 0.371205  [   88/   88]
Train Error: Avg loss: 0.49229747
validation Error: 
 Avg loss: 0.54658340 
 F1: 0.489132 
 Precision: 0.510448 
 Recall: 0.469525
 IoU: 0.323742

test Error: 
 Avg loss: 0.48210889 
 F1: 0.569022 
 Precision: 0.581958 
 Recall: 0.556649
 IoU: 0.397646

We have finished training iteration 41
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_39_.pth
per-ex loss: 0.581471  [    1/   88]
per-ex loss: 0.350901  [    2/   88]
per-ex loss: 0.458915  [    3/   88]
per-ex loss: 0.395180  [    4/   88]
per-ex loss: 0.389278  [    5/   88]
per-ex loss: 0.456215  [    6/   88]
per-ex loss: 0.606302  [    7/   88]
per-ex loss: 0.568042  [    8/   88]
per-ex loss: 0.502705  [    9/   88]
per-ex loss: 0.455120  [   10/   88]
per-ex loss: 0.656659  [   11/   88]
per-ex loss: 0.485726  [   12/   88]
per-ex loss: 0.372419  [   13/   88]
per-ex loss: 0.521620  [   14/   88]
per-ex loss: 0.599469  [   15/   88]
per-ex loss: 0.314187  [   16/   88]
per-ex loss: 0.385461  [   17/   88]
per-ex loss: 0.680056  [   18/   88]
per-ex loss: 0.418204  [   19/   88]
per-ex loss: 0.587070  [   20/   88]
per-ex loss: 0.616334  [   21/   88]
per-ex loss: 0.407658  [   22/   88]
per-ex loss: 0.610132  [   23/   88]
per-ex loss: 0.417631  [   24/   88]
per-ex loss: 0.568152  [   25/   88]
per-ex loss: 0.353312  [   26/   88]
per-ex loss: 0.616606  [   27/   88]
per-ex loss: 0.646297  [   28/   88]
per-ex loss: 0.562493  [   29/   88]
per-ex loss: 0.555597  [   30/   88]
per-ex loss: 0.547471  [   31/   88]
per-ex loss: 0.412213  [   32/   88]
per-ex loss: 0.558338  [   33/   88]
per-ex loss: 0.628224  [   34/   88]
per-ex loss: 0.353989  [   35/   88]
per-ex loss: 0.388658  [   36/   88]
per-ex loss: 0.439535  [   37/   88]
per-ex loss: 0.591834  [   38/   88]
per-ex loss: 0.422854  [   39/   88]
per-ex loss: 0.462163  [   40/   88]
per-ex loss: 0.602335  [   41/   88]
per-ex loss: 0.395284  [   42/   88]
per-ex loss: 0.444494  [   43/   88]
per-ex loss: 0.671110  [   44/   88]
per-ex loss: 0.649550  [   45/   88]
per-ex loss: 0.402010  [   46/   88]
per-ex loss: 0.396118  [   47/   88]
per-ex loss: 0.468892  [   48/   88]
per-ex loss: 0.372979  [   49/   88]
per-ex loss: 0.587793  [   50/   88]
per-ex loss: 0.543107  [   51/   88]
per-ex loss: 0.506515  [   52/   88]
per-ex loss: 0.428926  [   53/   88]
per-ex loss: 0.380141  [   54/   88]
per-ex loss: 0.536604  [   55/   88]
per-ex loss: 0.402685  [   56/   88]
per-ex loss: 0.532632  [   57/   88]
per-ex loss: 0.650187  [   58/   88]
per-ex loss: 0.436290  [   59/   88]
per-ex loss: 0.426891  [   60/   88]
per-ex loss: 0.461174  [   61/   88]
per-ex loss: 0.402718  [   62/   88]
per-ex loss: 0.579341  [   63/   88]
per-ex loss: 0.534690  [   64/   88]
per-ex loss: 0.379163  [   65/   88]
per-ex loss: 0.329536  [   66/   88]
per-ex loss: 0.532693  [   67/   88]
per-ex loss: 0.385110  [   68/   88]
per-ex loss: 0.419174  [   69/   88]
per-ex loss: 0.670938  [   70/   88]
per-ex loss: 0.554969  [   71/   88]
per-ex loss: 0.385902  [   72/   88]
per-ex loss: 0.514363  [   73/   88]
per-ex loss: 0.523358  [   74/   88]
per-ex loss: 0.630094  [   75/   88]
per-ex loss: 0.426846  [   76/   88]
per-ex loss: 0.681120  [   77/   88]
per-ex loss: 0.375734  [   78/   88]
per-ex loss: 0.437616  [   79/   88]
per-ex loss: 0.401751  [   80/   88]
per-ex loss: 0.399891  [   81/   88]
per-ex loss: 0.414495  [   82/   88]
per-ex loss: 0.365001  [   83/   88]
per-ex loss: 0.572943  [   84/   88]
per-ex loss: 0.602887  [   85/   88]
per-ex loss: 0.415269  [   86/   88]
per-ex loss: 0.518123  [   87/   88]
per-ex loss: 0.395933  [   88/   88]
Train Error: Avg loss: 0.48965761
validation Error: 
 Avg loss: 0.53399376 
 F1: 0.495697 
 Precision: 0.565453 
 Recall: 0.441262
 IoU: 0.329519

test Error: 
 Avg loss: 0.48515972 
 F1: 0.565079 
 Precision: 0.613843 
 Recall: 0.523493
 IoU: 0.393805

We have finished training iteration 42
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_34_.pth
per-ex loss: 0.464669  [    1/   88]
per-ex loss: 0.356855  [    2/   88]
per-ex loss: 0.682330  [    3/   88]
per-ex loss: 0.415937  [    4/   88]
per-ex loss: 0.615098  [    5/   88]
per-ex loss: 0.369463  [    6/   88]
per-ex loss: 0.353476  [    7/   88]
per-ex loss: 0.555199  [    8/   88]
per-ex loss: 0.378375  [    9/   88]
per-ex loss: 0.654346  [   10/   88]
per-ex loss: 0.327470  [   11/   88]
per-ex loss: 0.525177  [   12/   88]
per-ex loss: 0.399604  [   13/   88]
per-ex loss: 0.355357  [   14/   88]
per-ex loss: 0.398052  [   15/   88]
per-ex loss: 0.581927  [   16/   88]
per-ex loss: 0.433331  [   17/   88]
per-ex loss: 0.379178  [   18/   88]
per-ex loss: 0.448426  [   19/   88]
per-ex loss: 0.414017  [   20/   88]
per-ex loss: 0.570147  [   21/   88]
per-ex loss: 0.607183  [   22/   88]
per-ex loss: 0.635082  [   23/   88]
per-ex loss: 0.404205  [   24/   88]
per-ex loss: 0.455074  [   25/   88]
per-ex loss: 0.629584  [   26/   88]
per-ex loss: 0.674387  [   27/   88]
per-ex loss: 0.439433  [   28/   88]
per-ex loss: 0.503134  [   29/   88]
per-ex loss: 0.507912  [   30/   88]
per-ex loss: 0.419055  [   31/   88]
per-ex loss: 0.645989  [   32/   88]
per-ex loss: 0.652392  [   33/   88]
per-ex loss: 0.406672  [   34/   88]
per-ex loss: 0.590722  [   35/   88]
per-ex loss: 0.444956  [   36/   88]
per-ex loss: 0.413919  [   37/   88]
per-ex loss: 0.461983  [   38/   88]
per-ex loss: 0.320108  [   39/   88]
per-ex loss: 0.555082  [   40/   88]
per-ex loss: 0.459317  [   41/   88]
per-ex loss: 0.619585  [   42/   88]
per-ex loss: 0.431370  [   43/   88]
per-ex loss: 0.389888  [   44/   88]
per-ex loss: 0.530859  [   45/   88]
per-ex loss: 0.416265  [   46/   88]
per-ex loss: 0.532217  [   47/   88]
per-ex loss: 0.569697  [   48/   88]
per-ex loss: 0.412070  [   49/   88]
per-ex loss: 0.626330  [   50/   88]
per-ex loss: 0.352288  [   51/   88]
per-ex loss: 0.385038  [   52/   88]
per-ex loss: 0.605099  [   53/   88]
per-ex loss: 0.406928  [   54/   88]
per-ex loss: 0.405182  [   55/   88]
per-ex loss: 0.558496  [   56/   88]
per-ex loss: 0.754510  [   57/   88]
per-ex loss: 0.602677  [   58/   88]
per-ex loss: 0.441307  [   59/   88]
per-ex loss: 0.645712  [   60/   88]
per-ex loss: 0.368711  [   61/   88]
per-ex loss: 0.601272  [   62/   88]
per-ex loss: 0.516775  [   63/   88]
per-ex loss: 0.447929  [   64/   88]
per-ex loss: 0.459571  [   65/   88]
per-ex loss: 0.570673  [   66/   88]
per-ex loss: 0.421601  [   67/   88]
per-ex loss: 0.581764  [   68/   88]
per-ex loss: 0.521593  [   69/   88]
per-ex loss: 0.392001  [   70/   88]
per-ex loss: 0.363044  [   71/   88]
per-ex loss: 0.556848  [   72/   88]
per-ex loss: 0.328804  [   73/   88]
per-ex loss: 0.548509  [   74/   88]
per-ex loss: 0.391346  [   75/   88]
per-ex loss: 0.505442  [   76/   88]
per-ex loss: 0.375211  [   77/   88]
per-ex loss: 0.409428  [   78/   88]
per-ex loss: 0.520012  [   79/   88]
per-ex loss: 0.584548  [   80/   88]
per-ex loss: 0.520621  [   81/   88]
per-ex loss: 0.434802  [   82/   88]
per-ex loss: 0.427317  [   83/   88]
per-ex loss: 0.411637  [   84/   88]
per-ex loss: 0.685743  [   85/   88]
per-ex loss: 0.358310  [   86/   88]
per-ex loss: 0.493706  [   87/   88]
per-ex loss: 0.428988  [   88/   88]
Train Error: Avg loss: 0.48654941
validation Error: 
 Avg loss: 0.54215705 
 F1: 0.490872 
 Precision: 0.518885 
 Recall: 0.465728
 IoU: 0.325268

test Error: 
 Avg loss: 0.48712619 
 F1: 0.565428 
 Precision: 0.577383 
 Recall: 0.553959
 IoU: 0.394144

We have finished training iteration 43
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_41_.pth
per-ex loss: 0.382237  [    1/   88]
per-ex loss: 0.477110  [    2/   88]
per-ex loss: 0.383237  [    3/   88]
per-ex loss: 0.675944  [    4/   88]
per-ex loss: 0.595332  [    5/   88]
per-ex loss: 0.531210  [    6/   88]
per-ex loss: 0.525472  [    7/   88]
per-ex loss: 0.661974  [    8/   88]
per-ex loss: 0.523266  [    9/   88]
per-ex loss: 0.390258  [   10/   88]
per-ex loss: 0.356640  [   11/   88]
per-ex loss: 0.593922  [   12/   88]
per-ex loss: 0.355443  [   13/   88]
per-ex loss: 0.375676  [   14/   88]
per-ex loss: 0.578509  [   15/   88]
per-ex loss: 0.592101  [   16/   88]
per-ex loss: 0.642195  [   17/   88]
per-ex loss: 0.403217  [   18/   88]
per-ex loss: 0.681698  [   19/   88]
per-ex loss: 0.299879  [   20/   88]
per-ex loss: 0.640715  [   21/   88]
per-ex loss: 0.627741  [   22/   88]
per-ex loss: 0.571977  [   23/   88]
per-ex loss: 0.411171  [   24/   88]
per-ex loss: 0.655810  [   25/   88]
per-ex loss: 0.569568  [   26/   88]
per-ex loss: 0.357839  [   27/   88]
per-ex loss: 0.382631  [   28/   88]
per-ex loss: 0.381033  [   29/   88]
per-ex loss: 0.415323  [   30/   88]
per-ex loss: 0.555897  [   31/   88]
per-ex loss: 0.421157  [   32/   88]
per-ex loss: 0.599318  [   33/   88]
per-ex loss: 0.433930  [   34/   88]
per-ex loss: 0.603250  [   35/   88]
per-ex loss: 0.326700  [   36/   88]
per-ex loss: 0.677920  [   37/   88]
per-ex loss: 0.489777  [   38/   88]
per-ex loss: 0.595972  [   39/   88]
per-ex loss: 0.579177  [   40/   88]
per-ex loss: 0.375585  [   41/   88]
per-ex loss: 0.694461  [   42/   88]
per-ex loss: 0.429221  [   43/   88]
per-ex loss: 0.362823  [   44/   88]
per-ex loss: 0.566363  [   45/   88]
per-ex loss: 0.391416  [   46/   88]
per-ex loss: 0.412065  [   47/   88]
per-ex loss: 0.670811  [   48/   88]
per-ex loss: 0.555046  [   49/   88]
per-ex loss: 0.354449  [   50/   88]
per-ex loss: 0.486980  [   51/   88]
per-ex loss: 0.461507  [   52/   88]
per-ex loss: 0.597457  [   53/   88]
per-ex loss: 0.582103  [   54/   88]
per-ex loss: 0.437900  [   55/   88]
per-ex loss: 0.653179  [   56/   88]
per-ex loss: 0.460952  [   57/   88]
per-ex loss: 0.473465  [   58/   88]
per-ex loss: 0.420374  [   59/   88]
per-ex loss: 0.501619  [   60/   88]
per-ex loss: 0.535072  [   61/   88]
per-ex loss: 0.445055  [   62/   88]
per-ex loss: 0.373197  [   63/   88]
per-ex loss: 0.372991  [   64/   88]
per-ex loss: 0.411180  [   65/   88]
per-ex loss: 0.576945  [   66/   88]
per-ex loss: 0.422682  [   67/   88]
per-ex loss: 0.350117  [   68/   88]
per-ex loss: 0.548619  [   69/   88]
per-ex loss: 0.605723  [   70/   88]
per-ex loss: 0.422287  [   71/   88]
per-ex loss: 0.542354  [   72/   88]
per-ex loss: 0.468133  [   73/   88]
per-ex loss: 0.407684  [   74/   88]
per-ex loss: 0.529286  [   75/   88]
per-ex loss: 0.530861  [   76/   88]
per-ex loss: 0.375458  [   77/   88]
per-ex loss: 0.549096  [   78/   88]
per-ex loss: 0.419371  [   79/   88]
per-ex loss: 0.421360  [   80/   88]
per-ex loss: 0.428359  [   81/   88]
per-ex loss: 0.446591  [   82/   88]
per-ex loss: 0.433778  [   83/   88]
per-ex loss: 0.383475  [   84/   88]
per-ex loss: 0.521912  [   85/   88]
per-ex loss: 0.365730  [   86/   88]
per-ex loss: 0.604779  [   87/   88]
per-ex loss: 0.458234  [   88/   88]
Train Error: Avg loss: 0.49045832
validation Error: 
 Avg loss: 0.54375635 
 F1: 0.482629 
 Precision: 0.547016 
 Recall: 0.431802
 IoU: 0.318069

test Error: 
 Avg loss: 0.49598274 
 F1: 0.555597 
 Precision: 0.613395 
 Recall: 0.507753
 IoU: 0.384655

We have finished training iteration 44
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_42_.pth
per-ex loss: 0.597365  [    1/   88]
per-ex loss: 0.355294  [    2/   88]
per-ex loss: 0.375556  [    3/   88]
per-ex loss: 0.505124  [    4/   88]
per-ex loss: 0.466800  [    5/   88]
per-ex loss: 0.357733  [    6/   88]
per-ex loss: 0.368748  [    7/   88]
per-ex loss: 0.393398  [    8/   88]
per-ex loss: 0.332159  [    9/   88]
per-ex loss: 0.431900  [   10/   88]
per-ex loss: 0.663588  [   11/   88]
per-ex loss: 0.384008  [   12/   88]
per-ex loss: 0.566879  [   13/   88]
per-ex loss: 0.555650  [   14/   88]
per-ex loss: 0.446076  [   15/   88]
per-ex loss: 0.401659  [   16/   88]
per-ex loss: 0.675795  [   17/   88]
per-ex loss: 0.494724  [   18/   88]
per-ex loss: 0.379618  [   19/   88]
per-ex loss: 0.393534  [   20/   88]
per-ex loss: 0.499130  [   21/   88]
per-ex loss: 0.606420  [   22/   88]
per-ex loss: 0.676888  [   23/   88]
per-ex loss: 0.571463  [   24/   88]
per-ex loss: 0.352325  [   25/   88]
per-ex loss: 0.606483  [   26/   88]
per-ex loss: 0.547475  [   27/   88]
per-ex loss: 0.434194  [   28/   88]
per-ex loss: 0.424542  [   29/   88]
per-ex loss: 0.554187  [   30/   88]
per-ex loss: 0.339157  [   31/   88]
per-ex loss: 0.565455  [   32/   88]
per-ex loss: 0.526127  [   33/   88]
per-ex loss: 0.459180  [   34/   88]
per-ex loss: 0.407834  [   35/   88]
per-ex loss: 0.393659  [   36/   88]
per-ex loss: 0.615837  [   37/   88]
per-ex loss: 0.602269  [   38/   88]
per-ex loss: 0.410258  [   39/   88]
per-ex loss: 0.373292  [   40/   88]
per-ex loss: 0.437043  [   41/   88]
per-ex loss: 0.548552  [   42/   88]
per-ex loss: 0.377961  [   43/   88]
per-ex loss: 0.503817  [   44/   88]
per-ex loss: 0.315637  [   45/   88]
per-ex loss: 0.428058  [   46/   88]
per-ex loss: 0.543176  [   47/   88]
per-ex loss: 0.435639  [   48/   88]
per-ex loss: 0.382066  [   49/   88]
per-ex loss: 0.412459  [   50/   88]
per-ex loss: 0.495112  [   51/   88]
per-ex loss: 0.383857  [   52/   88]
per-ex loss: 0.646863  [   53/   88]
per-ex loss: 0.410202  [   54/   88]
per-ex loss: 0.526117  [   55/   88]
per-ex loss: 0.421850  [   56/   88]
per-ex loss: 0.386783  [   57/   88]
per-ex loss: 0.421292  [   58/   88]
per-ex loss: 0.420557  [   59/   88]
per-ex loss: 0.630004  [   60/   88]
per-ex loss: 0.532170  [   61/   88]
per-ex loss: 0.393461  [   62/   88]
per-ex loss: 0.719768  [   63/   88]
per-ex loss: 0.355738  [   64/   88]
per-ex loss: 0.362832  [   65/   88]
per-ex loss: 0.578008  [   66/   88]
per-ex loss: 0.468503  [   67/   88]
per-ex loss: 0.367910  [   68/   88]
per-ex loss: 0.477820  [   69/   88]
per-ex loss: 0.382382  [   70/   88]
per-ex loss: 0.647151  [   71/   88]
per-ex loss: 0.554493  [   72/   88]
per-ex loss: 0.616384  [   73/   88]
per-ex loss: 0.696070  [   74/   88]
per-ex loss: 0.510616  [   75/   88]
per-ex loss: 0.403773  [   76/   88]
per-ex loss: 0.574960  [   77/   88]
per-ex loss: 0.668679  [   78/   88]
per-ex loss: 0.426252  [   79/   88]
per-ex loss: 0.599094  [   80/   88]
per-ex loss: 0.613978  [   81/   88]
per-ex loss: 0.447026  [   82/   88]
per-ex loss: 0.663409  [   83/   88]
per-ex loss: 0.598745  [   84/   88]
per-ex loss: 0.667678  [   85/   88]
per-ex loss: 0.524873  [   86/   88]
per-ex loss: 0.471895  [   87/   88]
per-ex loss: 0.332536  [   88/   88]
Train Error: Avg loss: 0.48744357
validation Error: 
 Avg loss: 0.57144608 
 F1: 0.457075 
 Precision: 0.437386 
 Recall: 0.478620
 IoU: 0.296239

test Error: 
 Avg loss: 0.48956859 
 F1: 0.557465 
 Precision: 0.557372 
 Recall: 0.557558
 IoU: 0.386448

We have finished training iteration 45
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_43_.pth
per-ex loss: 0.656883  [    1/   88]
per-ex loss: 0.557490  [    2/   88]
per-ex loss: 0.575816  [    3/   88]
per-ex loss: 0.561242  [    4/   88]
per-ex loss: 0.501450  [    5/   88]
per-ex loss: 0.503699  [    6/   88]
per-ex loss: 0.606080  [    7/   88]
per-ex loss: 0.399898  [    8/   88]
per-ex loss: 0.360371  [    9/   88]
per-ex loss: 0.370307  [   10/   88]
per-ex loss: 0.387854  [   11/   88]
per-ex loss: 0.538064  [   12/   88]
per-ex loss: 0.385890  [   13/   88]
per-ex loss: 0.409653  [   14/   88]
per-ex loss: 0.382138  [   15/   88]
per-ex loss: 0.364420  [   16/   88]
per-ex loss: 0.403448  [   17/   88]
per-ex loss: 0.634132  [   18/   88]
per-ex loss: 0.313706  [   19/   88]
per-ex loss: 0.585097  [   20/   88]
per-ex loss: 0.402651  [   21/   88]
per-ex loss: 0.383696  [   22/   88]
per-ex loss: 0.406012  [   23/   88]
per-ex loss: 0.593208  [   24/   88]
per-ex loss: 0.551865  [   25/   88]
per-ex loss: 0.427776  [   26/   88]
per-ex loss: 0.526937  [   27/   88]
per-ex loss: 0.436744  [   28/   88]
per-ex loss: 0.600312  [   29/   88]
per-ex loss: 0.387190  [   30/   88]
per-ex loss: 0.458274  [   31/   88]
per-ex loss: 0.363635  [   32/   88]
per-ex loss: 0.347173  [   33/   88]
per-ex loss: 0.682323  [   34/   88]
per-ex loss: 0.350260  [   35/   88]
per-ex loss: 0.707097  [   36/   88]
per-ex loss: 0.500606  [   37/   88]
per-ex loss: 0.416406  [   38/   88]
per-ex loss: 0.403422  [   39/   88]
per-ex loss: 0.647263  [   40/   88]
per-ex loss: 0.454337  [   41/   88]
per-ex loss: 0.452020  [   42/   88]
per-ex loss: 0.388564  [   43/   88]
per-ex loss: 0.374854  [   44/   88]
per-ex loss: 0.569301  [   45/   88]
per-ex loss: 0.440893  [   46/   88]
per-ex loss: 0.407821  [   47/   88]
per-ex loss: 0.405665  [   48/   88]
per-ex loss: 0.561612  [   49/   88]
per-ex loss: 0.628129  [   50/   88]
per-ex loss: 0.625188  [   51/   88]
per-ex loss: 0.350422  [   52/   88]
per-ex loss: 0.658607  [   53/   88]
per-ex loss: 0.425627  [   54/   88]
per-ex loss: 0.523000  [   55/   88]
per-ex loss: 0.524468  [   56/   88]
per-ex loss: 0.441505  [   57/   88]
per-ex loss: 0.660657  [   58/   88]
per-ex loss: 0.622379  [   59/   88]
per-ex loss: 0.383815  [   60/   88]
per-ex loss: 0.340426  [   61/   88]
per-ex loss: 0.437036  [   62/   88]
per-ex loss: 0.518636  [   63/   88]
per-ex loss: 0.571103  [   64/   88]
per-ex loss: 0.654749  [   65/   88]
per-ex loss: 0.418685  [   66/   88]
per-ex loss: 0.470412  [   67/   88]
per-ex loss: 0.456259  [   68/   88]
per-ex loss: 0.568304  [   69/   88]
per-ex loss: 0.399063  [   70/   88]
per-ex loss: 0.406477  [   71/   88]
per-ex loss: 0.350433  [   72/   88]
per-ex loss: 0.400163  [   73/   88]
per-ex loss: 0.592893  [   74/   88]
per-ex loss: 0.592381  [   75/   88]
per-ex loss: 0.357900  [   76/   88]
per-ex loss: 0.559383  [   77/   88]
per-ex loss: 0.436530  [   78/   88]
per-ex loss: 0.519525  [   79/   88]
per-ex loss: 0.542314  [   80/   88]
per-ex loss: 0.376378  [   81/   88]
per-ex loss: 0.665141  [   82/   88]
per-ex loss: 0.387381  [   83/   88]
per-ex loss: 0.403562  [   84/   88]
per-ex loss: 0.558480  [   85/   88]
per-ex loss: 0.515393  [   86/   88]
per-ex loss: 0.610758  [   87/   88]
per-ex loss: 0.579526  [   88/   88]
Train Error: Avg loss: 0.48498456
validation Error: 
 Avg loss: 0.53991804 
 F1: 0.487243 
 Precision: 0.493158 
 Recall: 0.481469
 IoU: 0.322090

test Error: 
 Avg loss: 0.48661912 
 F1: 0.564632 
 Precision: 0.556467 
 Recall: 0.573041
 IoU: 0.393371

We have finished training iteration 46
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_44_.pth
per-ex loss: 0.409530  [    1/   88]
per-ex loss: 0.553141  [    2/   88]
per-ex loss: 0.381625  [    3/   88]
per-ex loss: 0.570521  [    4/   88]
per-ex loss: 0.374523  [    5/   88]
per-ex loss: 0.598381  [    6/   88]
per-ex loss: 0.500909  [    7/   88]
per-ex loss: 0.388218  [    8/   88]
per-ex loss: 0.451138  [    9/   88]
per-ex loss: 0.410460  [   10/   88]
per-ex loss: 0.347450  [   11/   88]
per-ex loss: 0.479960  [   12/   88]
per-ex loss: 0.384303  [   13/   88]
per-ex loss: 0.344025  [   14/   88]
per-ex loss: 0.685843  [   15/   88]
per-ex loss: 0.360369  [   16/   88]
per-ex loss: 0.589849  [   17/   88]
per-ex loss: 0.365495  [   18/   88]
per-ex loss: 0.498433  [   19/   88]
per-ex loss: 0.496808  [   20/   88]
per-ex loss: 0.376993  [   21/   88]
per-ex loss: 0.413407  [   22/   88]
per-ex loss: 0.548816  [   23/   88]
per-ex loss: 0.600089  [   24/   88]
per-ex loss: 0.636061  [   25/   88]
per-ex loss: 0.296821  [   26/   88]
per-ex loss: 0.517221  [   27/   88]
per-ex loss: 0.519526  [   28/   88]
per-ex loss: 0.677258  [   29/   88]
per-ex loss: 0.546307  [   30/   88]
per-ex loss: 0.458105  [   31/   88]
per-ex loss: 0.567286  [   32/   88]
per-ex loss: 0.590395  [   33/   88]
per-ex loss: 0.578243  [   34/   88]
per-ex loss: 0.399010  [   35/   88]
per-ex loss: 0.376254  [   36/   88]
per-ex loss: 0.647478  [   37/   88]
per-ex loss: 0.422962  [   38/   88]
per-ex loss: 0.546146  [   39/   88]
per-ex loss: 0.439055  [   40/   88]
per-ex loss: 0.403912  [   41/   88]
per-ex loss: 0.589669  [   42/   88]
per-ex loss: 0.348640  [   43/   88]
per-ex loss: 0.454546  [   44/   88]
per-ex loss: 0.413433  [   45/   88]
per-ex loss: 0.334132  [   46/   88]
per-ex loss: 0.370509  [   47/   88]
per-ex loss: 0.317718  [   48/   88]
per-ex loss: 0.627476  [   49/   88]
per-ex loss: 0.434867  [   50/   88]
per-ex loss: 0.352936  [   51/   88]
per-ex loss: 0.542479  [   52/   88]
per-ex loss: 0.675161  [   53/   88]
per-ex loss: 0.646160  [   54/   88]
per-ex loss: 0.557538  [   55/   88]
per-ex loss: 0.564725  [   56/   88]
per-ex loss: 0.389032  [   57/   88]
per-ex loss: 0.442875  [   58/   88]
per-ex loss: 0.398549  [   59/   88]
per-ex loss: 0.538387  [   60/   88]
per-ex loss: 0.546461  [   61/   88]
per-ex loss: 0.527539  [   62/   88]
per-ex loss: 0.686368  [   63/   88]
per-ex loss: 0.587219  [   64/   88]
per-ex loss: 0.451150  [   65/   88]
per-ex loss: 0.479862  [   66/   88]
per-ex loss: 0.458645  [   67/   88]
per-ex loss: 0.435233  [   68/   88]
per-ex loss: 0.543980  [   69/   88]
per-ex loss: 0.399503  [   70/   88]
per-ex loss: 0.421160  [   71/   88]
per-ex loss: 0.421258  [   72/   88]
per-ex loss: 0.364967  [   73/   88]
per-ex loss: 0.555210  [   74/   88]
per-ex loss: 0.426938  [   75/   88]
per-ex loss: 0.428249  [   76/   88]
per-ex loss: 0.558941  [   77/   88]
per-ex loss: 0.439740  [   78/   88]
per-ex loss: 0.584261  [   79/   88]
per-ex loss: 0.661172  [   80/   88]
per-ex loss: 0.586261  [   81/   88]
per-ex loss: 0.380269  [   82/   88]
per-ex loss: 0.389862  [   83/   88]
per-ex loss: 0.355112  [   84/   88]
per-ex loss: 0.628691  [   85/   88]
per-ex loss: 0.660944  [   86/   88]
per-ex loss: 0.365960  [   87/   88]
per-ex loss: 0.648497  [   88/   88]
Train Error: Avg loss: 0.48573418
validation Error: 
 Avg loss: 0.56480804 
 F1: 0.456670 
 Precision: 0.458267 
 Recall: 0.455085
 IoU: 0.295899

test Error: 
 Avg loss: 0.50509422 
 F1: 0.543172 
 Precision: 0.543554 
 Recall: 0.542791
 IoU: 0.372846

We have finished training iteration 47
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_45_.pth
per-ex loss: 0.392662  [    1/   88]
per-ex loss: 0.308071  [    2/   88]
per-ex loss: 0.548047  [    3/   88]
per-ex loss: 0.400010  [    4/   88]
per-ex loss: 0.668460  [    5/   88]
per-ex loss: 0.571335  [    6/   88]
per-ex loss: 0.364682  [    7/   88]
per-ex loss: 0.353193  [    8/   88]
per-ex loss: 0.385341  [    9/   88]
per-ex loss: 0.578770  [   10/   88]
per-ex loss: 0.426313  [   11/   88]
per-ex loss: 0.570419  [   12/   88]
per-ex loss: 0.515439  [   13/   88]
per-ex loss: 0.380054  [   14/   88]
per-ex loss: 0.405257  [   15/   88]
per-ex loss: 0.641463  [   16/   88]
per-ex loss: 0.429664  [   17/   88]
per-ex loss: 0.412006  [   18/   88]
per-ex loss: 0.573668  [   19/   88]
per-ex loss: 0.395081  [   20/   88]
per-ex loss: 0.368668  [   21/   88]
per-ex loss: 0.411259  [   22/   88]
per-ex loss: 0.519507  [   23/   88]
per-ex loss: 0.431551  [   24/   88]
per-ex loss: 0.529503  [   25/   88]
per-ex loss: 0.368110  [   26/   88]
per-ex loss: 0.397034  [   27/   88]
per-ex loss: 0.593566  [   28/   88]
per-ex loss: 0.334124  [   29/   88]
per-ex loss: 0.347505  [   30/   88]
per-ex loss: 0.655357  [   31/   88]
per-ex loss: 0.594495  [   32/   88]
per-ex loss: 0.416906  [   33/   88]
per-ex loss: 0.554957  [   34/   88]
per-ex loss: 0.416117  [   35/   88]
per-ex loss: 0.398385  [   36/   88]
per-ex loss: 0.574588  [   37/   88]
per-ex loss: 0.543715  [   38/   88]
per-ex loss: 0.385657  [   39/   88]
per-ex loss: 0.466744  [   40/   88]
per-ex loss: 0.500696  [   41/   88]
per-ex loss: 0.376464  [   42/   88]
per-ex loss: 0.484857  [   43/   88]
per-ex loss: 0.453352  [   44/   88]
per-ex loss: 0.557784  [   45/   88]
per-ex loss: 0.599435  [   46/   88]
per-ex loss: 0.401999  [   47/   88]
per-ex loss: 0.490689  [   48/   88]
per-ex loss: 0.354746  [   49/   88]
per-ex loss: 0.309466  [   50/   88]
per-ex loss: 0.561006  [   51/   88]
per-ex loss: 0.572958  [   52/   88]
per-ex loss: 0.414257  [   53/   88]
per-ex loss: 0.358488  [   54/   88]
per-ex loss: 0.549791  [   55/   88]
per-ex loss: 0.384478  [   56/   88]
per-ex loss: 0.526346  [   57/   88]
per-ex loss: 0.427229  [   58/   88]
per-ex loss: 0.614742  [   59/   88]
per-ex loss: 0.358976  [   60/   88]
per-ex loss: 0.609408  [   61/   88]
per-ex loss: 0.394988  [   62/   88]
per-ex loss: 0.384804  [   63/   88]
per-ex loss: 0.346394  [   64/   88]
per-ex loss: 0.653992  [   65/   88]
per-ex loss: 0.410794  [   66/   88]
per-ex loss: 0.605121  [   67/   88]
per-ex loss: 0.594051  [   68/   88]
per-ex loss: 0.642719  [   69/   88]
per-ex loss: 0.671376  [   70/   88]
per-ex loss: 0.682945  [   71/   88]
per-ex loss: 0.668859  [   72/   88]
per-ex loss: 0.598217  [   73/   88]
per-ex loss: 0.403378  [   74/   88]
per-ex loss: 0.581551  [   75/   88]
per-ex loss: 0.482096  [   76/   88]
per-ex loss: 0.459825  [   77/   88]
per-ex loss: 0.540876  [   78/   88]
per-ex loss: 0.490368  [   79/   88]
per-ex loss: 0.449167  [   80/   88]
per-ex loss: 0.656936  [   81/   88]
per-ex loss: 0.486580  [   82/   88]
per-ex loss: 0.420443  [   83/   88]
per-ex loss: 0.399233  [   84/   88]
per-ex loss: 0.567314  [   85/   88]
per-ex loss: 0.421374  [   86/   88]
per-ex loss: 0.395065  [   87/   88]
per-ex loss: 0.357345  [   88/   88]
Train Error: Avg loss: 0.48068934
validation Error: 
 Avg loss: 0.53110269 
 F1: 0.499854 
 Precision: 0.544379 
 Recall: 0.462062
 IoU: 0.333203

test Error: 
 Avg loss: 0.48720803 
 F1: 0.567835 
 Precision: 0.578464 
 Recall: 0.557589
 IoU: 0.396487

We have finished training iteration 48
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_46_.pth
per-ex loss: 0.350370  [    1/   88]
per-ex loss: 0.349510  [    2/   88]
per-ex loss: 0.382253  [    3/   88]
per-ex loss: 0.294789  [    4/   88]
per-ex loss: 0.604699  [    5/   88]
per-ex loss: 0.569419  [    6/   88]
per-ex loss: 0.553435  [    7/   88]
per-ex loss: 0.439799  [    8/   88]
per-ex loss: 0.475885  [    9/   88]
per-ex loss: 0.512611  [   10/   88]
per-ex loss: 0.622595  [   11/   88]
per-ex loss: 0.586002  [   12/   88]
per-ex loss: 0.685351  [   13/   88]
per-ex loss: 0.571129  [   14/   88]
per-ex loss: 0.412044  [   15/   88]
per-ex loss: 0.410970  [   16/   88]
per-ex loss: 0.382896  [   17/   88]
per-ex loss: 0.380280  [   18/   88]
per-ex loss: 0.584305  [   19/   88]
per-ex loss: 0.435673  [   20/   88]
per-ex loss: 0.419011  [   21/   88]
per-ex loss: 0.546205  [   22/   88]
per-ex loss: 0.372660  [   23/   88]
per-ex loss: 0.359062  [   24/   88]
per-ex loss: 0.440872  [   25/   88]
per-ex loss: 0.395115  [   26/   88]
per-ex loss: 0.346393  [   27/   88]
per-ex loss: 0.656118  [   28/   88]
per-ex loss: 0.614041  [   29/   88]
per-ex loss: 0.502458  [   30/   88]
per-ex loss: 0.588057  [   31/   88]
per-ex loss: 0.598428  [   32/   88]
per-ex loss: 0.507965  [   33/   88]
per-ex loss: 0.464944  [   34/   88]
per-ex loss: 0.344753  [   35/   88]
per-ex loss: 0.497631  [   36/   88]
per-ex loss: 0.619468  [   37/   88]
per-ex loss: 0.585107  [   38/   88]
per-ex loss: 0.616412  [   39/   88]
per-ex loss: 0.593344  [   40/   88]
per-ex loss: 0.350192  [   41/   88]
per-ex loss: 0.353087  [   42/   88]
per-ex loss: 0.594139  [   43/   88]
per-ex loss: 0.670242  [   44/   88]
per-ex loss: 0.539324  [   45/   88]
per-ex loss: 0.501339  [   46/   88]
per-ex loss: 0.490295  [   47/   88]
per-ex loss: 0.353286  [   48/   88]
per-ex loss: 0.353109  [   49/   88]
per-ex loss: 0.647545  [   50/   88]
per-ex loss: 0.415119  [   51/   88]
per-ex loss: 0.433175  [   52/   88]
per-ex loss: 0.681408  [   53/   88]
per-ex loss: 0.388152  [   54/   88]
per-ex loss: 0.388583  [   55/   88]
per-ex loss: 0.410772  [   56/   88]
per-ex loss: 0.626702  [   57/   88]
per-ex loss: 0.442513  [   58/   88]
per-ex loss: 0.527656  [   59/   88]
per-ex loss: 0.396950  [   60/   88]
per-ex loss: 0.475925  [   61/   88]
per-ex loss: 0.344366  [   62/   88]
per-ex loss: 0.431206  [   63/   88]
per-ex loss: 0.674969  [   64/   88]
per-ex loss: 0.573481  [   65/   88]
per-ex loss: 0.659757  [   66/   88]
per-ex loss: 0.448147  [   67/   88]
per-ex loss: 0.378919  [   68/   88]
per-ex loss: 0.417217  [   69/   88]
per-ex loss: 0.526303  [   70/   88]
per-ex loss: 0.421523  [   71/   88]
per-ex loss: 0.408166  [   72/   88]
per-ex loss: 0.349098  [   73/   88]
per-ex loss: 0.588290  [   74/   88]
per-ex loss: 0.356824  [   75/   88]
per-ex loss: 0.438167  [   76/   88]
per-ex loss: 0.471188  [   77/   88]
per-ex loss: 0.414840  [   78/   88]
per-ex loss: 0.421668  [   79/   88]
per-ex loss: 0.649071  [   80/   88]
per-ex loss: 0.626317  [   81/   88]
per-ex loss: 0.481718  [   82/   88]
per-ex loss: 0.389534  [   83/   88]
per-ex loss: 0.375910  [   84/   88]
per-ex loss: 0.447331  [   85/   88]
per-ex loss: 0.591913  [   86/   88]
per-ex loss: 0.566228  [   87/   88]
per-ex loss: 0.410274  [   88/   88]
Train Error: Avg loss: 0.48379540
validation Error: 
 Avg loss: 0.53837546 
 F1: 0.494915 
 Precision: 0.598989 
 Recall: 0.421653
 IoU: 0.328828

test Error: 
 Avg loss: 0.49564176 
 F1: 0.557081 
 Precision: 0.655297 
 Recall: 0.484469
 IoU: 0.386079

We have finished training iteration 49
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_47_.pth
per-ex loss: 0.399339  [    1/   88]
per-ex loss: 0.600701  [    2/   88]
per-ex loss: 0.454716  [    3/   88]
per-ex loss: 0.385970  [    4/   88]
per-ex loss: 0.383834  [    5/   88]
per-ex loss: 0.437665  [    6/   88]
per-ex loss: 0.403285  [    7/   88]
per-ex loss: 0.414106  [    8/   88]
per-ex loss: 0.385295  [    9/   88]
per-ex loss: 0.418098  [   10/   88]
per-ex loss: 0.590793  [   11/   88]
per-ex loss: 0.375071  [   12/   88]
per-ex loss: 0.462387  [   13/   88]
per-ex loss: 0.634998  [   14/   88]
per-ex loss: 0.426101  [   15/   88]
per-ex loss: 0.645181  [   16/   88]
per-ex loss: 0.421052  [   17/   88]
per-ex loss: 0.371667  [   18/   88]
per-ex loss: 0.584369  [   19/   88]
per-ex loss: 0.453415  [   20/   88]
per-ex loss: 0.390888  [   21/   88]
per-ex loss: 0.384974  [   22/   88]
per-ex loss: 0.601845  [   23/   88]
per-ex loss: 0.409528  [   24/   88]
per-ex loss: 0.354448  [   25/   88]
per-ex loss: 0.300693  [   26/   88]
per-ex loss: 0.577914  [   27/   88]
per-ex loss: 0.564695  [   28/   88]
per-ex loss: 0.631978  [   29/   88]
per-ex loss: 0.346842  [   30/   88]
per-ex loss: 0.641623  [   31/   88]
per-ex loss: 0.404310  [   32/   88]
per-ex loss: 0.389312  [   33/   88]
per-ex loss: 0.344683  [   34/   88]
per-ex loss: 0.447716  [   35/   88]
per-ex loss: 0.686489  [   36/   88]
per-ex loss: 0.500972  [   37/   88]
per-ex loss: 0.567725  [   38/   88]
per-ex loss: 0.368448  [   39/   88]
per-ex loss: 0.438301  [   40/   88]
per-ex loss: 0.628652  [   41/   88]
per-ex loss: 0.608671  [   42/   88]
per-ex loss: 0.680541  [   43/   88]
per-ex loss: 0.673593  [   44/   88]
per-ex loss: 0.675483  [   45/   88]
per-ex loss: 0.368205  [   46/   88]
per-ex loss: 0.479680  [   47/   88]
per-ex loss: 0.498620  [   48/   88]
per-ex loss: 0.366059  [   49/   88]
per-ex loss: 0.492269  [   50/   88]
per-ex loss: 0.476210  [   51/   88]
per-ex loss: 0.562589  [   52/   88]
per-ex loss: 0.355709  [   53/   88]
per-ex loss: 0.604841  [   54/   88]
per-ex loss: 0.533414  [   55/   88]
per-ex loss: 0.662505  [   56/   88]
per-ex loss: 0.416555  [   57/   88]
per-ex loss: 0.352549  [   58/   88]
per-ex loss: 0.538436  [   59/   88]
per-ex loss: 0.457709  [   60/   88]
per-ex loss: 0.507417  [   61/   88]
per-ex loss: 0.544763  [   62/   88]
per-ex loss: 0.554517  [   63/   88]
per-ex loss: 0.379188  [   64/   88]
per-ex loss: 0.390113  [   65/   88]
per-ex loss: 0.407268  [   66/   88]
per-ex loss: 0.670189  [   67/   88]
per-ex loss: 0.363877  [   68/   88]
per-ex loss: 0.619217  [   69/   88]
per-ex loss: 0.398058  [   70/   88]
per-ex loss: 0.480642  [   71/   88]
per-ex loss: 0.322045  [   72/   88]
per-ex loss: 0.561566  [   73/   88]
per-ex loss: 0.387314  [   74/   88]
per-ex loss: 0.385914  [   75/   88]
per-ex loss: 0.502457  [   76/   88]
per-ex loss: 0.344982  [   77/   88]
per-ex loss: 0.536443  [   78/   88]
per-ex loss: 0.604179  [   79/   88]
per-ex loss: 0.610559  [   80/   88]
per-ex loss: 0.379769  [   81/   88]
per-ex loss: 0.410496  [   82/   88]
per-ex loss: 0.383683  [   83/   88]
per-ex loss: 0.458830  [   84/   88]
per-ex loss: 0.653090  [   85/   88]
per-ex loss: 0.574636  [   86/   88]
per-ex loss: 0.596429  [   87/   88]
per-ex loss: 0.498514  [   88/   88]
Train Error: Avg loss: 0.48363493
validation Error: 
 Avg loss: 0.53940437 
 F1: 0.485173 
 Precision: 0.537429 
 Recall: 0.442179
 IoU: 0.320283

test Error: 
 Avg loss: 0.49052319 
 F1: 0.557203 
 Precision: 0.600570 
 Recall: 0.519677
 IoU: 0.386196

We have finished training iteration 50
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_40_.pth
per-ex loss: 0.371050  [    1/   88]
per-ex loss: 0.414096  [    2/   88]
per-ex loss: 0.440391  [    3/   88]
per-ex loss: 0.588481  [    4/   88]
per-ex loss: 0.561721  [    5/   88]
per-ex loss: 0.579557  [    6/   88]
per-ex loss: 0.394343  [    7/   88]
per-ex loss: 0.600521  [    8/   88]
per-ex loss: 0.496967  [    9/   88]
per-ex loss: 0.382475  [   10/   88]
per-ex loss: 0.419053  [   11/   88]
per-ex loss: 0.649235  [   12/   88]
per-ex loss: 0.378865  [   13/   88]
per-ex loss: 0.562621  [   14/   88]
per-ex loss: 0.684354  [   15/   88]
per-ex loss: 0.684764  [   16/   88]
per-ex loss: 0.659684  [   17/   88]
per-ex loss: 0.399933  [   18/   88]
per-ex loss: 0.480006  [   19/   88]
per-ex loss: 0.356094  [   20/   88]
per-ex loss: 0.412123  [   21/   88]
per-ex loss: 0.636496  [   22/   88]
per-ex loss: 0.537944  [   23/   88]
per-ex loss: 0.381360  [   24/   88]
per-ex loss: 0.526603  [   25/   88]
per-ex loss: 0.416207  [   26/   88]
per-ex loss: 0.450682  [   27/   88]
per-ex loss: 0.544392  [   28/   88]
per-ex loss: 0.653398  [   29/   88]
per-ex loss: 0.610169  [   30/   88]
per-ex loss: 0.434312  [   31/   88]
per-ex loss: 0.656959  [   32/   88]
per-ex loss: 0.568928  [   33/   88]
per-ex loss: 0.379595  [   34/   88]
per-ex loss: 0.634166  [   35/   88]
per-ex loss: 0.368424  [   36/   88]
per-ex loss: 0.571629  [   37/   88]
per-ex loss: 0.382563  [   38/   88]
per-ex loss: 0.539253  [   39/   88]
per-ex loss: 0.409680  [   40/   88]
per-ex loss: 0.556181  [   41/   88]
per-ex loss: 0.388214  [   42/   88]
per-ex loss: 0.344193  [   43/   88]
per-ex loss: 0.372336  [   44/   88]
per-ex loss: 0.672655  [   45/   88]
per-ex loss: 0.366014  [   46/   88]
per-ex loss: 0.437400  [   47/   88]
per-ex loss: 0.393236  [   48/   88]
per-ex loss: 0.377987  [   49/   88]
per-ex loss: 0.500307  [   50/   88]
per-ex loss: 0.554884  [   51/   88]
per-ex loss: 0.419270  [   52/   88]
per-ex loss: 0.618487  [   53/   88]
per-ex loss: 0.425595  [   54/   88]
per-ex loss: 0.368426  [   55/   88]
per-ex loss: 0.452414  [   56/   88]
per-ex loss: 0.561116  [   57/   88]
per-ex loss: 0.364530  [   58/   88]
per-ex loss: 0.555013  [   59/   88]
per-ex loss: 0.386123  [   60/   88]
per-ex loss: 0.531792  [   61/   88]
per-ex loss: 0.407331  [   62/   88]
per-ex loss: 0.464888  [   63/   88]
per-ex loss: 0.533511  [   64/   88]
per-ex loss: 0.613540  [   65/   88]
per-ex loss: 0.403214  [   66/   88]
per-ex loss: 0.669708  [   67/   88]
per-ex loss: 0.519063  [   68/   88]
per-ex loss: 0.431346  [   69/   88]
per-ex loss: 0.608569  [   70/   88]
per-ex loss: 0.513481  [   71/   88]
per-ex loss: 0.586969  [   72/   88]
per-ex loss: 0.415732  [   73/   88]
per-ex loss: 0.443978  [   74/   88]
per-ex loss: 0.506606  [   75/   88]
per-ex loss: 0.351655  [   76/   88]
per-ex loss: 0.404554  [   77/   88]
per-ex loss: 0.420007  [   78/   88]
per-ex loss: 0.318095  [   79/   88]
per-ex loss: 0.450280  [   80/   88]
per-ex loss: 0.629698  [   81/   88]
per-ex loss: 0.594273  [   82/   88]
per-ex loss: 0.387603  [   83/   88]
per-ex loss: 0.581281  [   84/   88]
per-ex loss: 0.366637  [   85/   88]
per-ex loss: 0.315795  [   86/   88]
per-ex loss: 0.484414  [   87/   88]
per-ex loss: 0.435782  [   88/   88]
Train Error: Avg loss: 0.48549178
validation Error: 
 Avg loss: 0.53420338 
 F1: 0.496850 
 Precision: 0.506966 
 Recall: 0.487129
 IoU: 0.330539

test Error: 
 Avg loss: 0.48538986 
 F1: 0.565670 
 Precision: 0.554726 
 Recall: 0.577055
 IoU: 0.394379

We have finished training iteration 51
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_49_.pth
per-ex loss: 0.561759  [    1/   88]
per-ex loss: 0.677694  [    2/   88]
per-ex loss: 0.590734  [    3/   88]
per-ex loss: 0.486643  [    4/   88]
per-ex loss: 0.418486  [    5/   88]
per-ex loss: 0.570697  [    6/   88]
per-ex loss: 0.386414  [    7/   88]
per-ex loss: 0.316194  [    8/   88]
per-ex loss: 0.641302  [    9/   88]
per-ex loss: 0.585756  [   10/   88]
per-ex loss: 0.436093  [   11/   88]
per-ex loss: 0.437246  [   12/   88]
per-ex loss: 0.588627  [   13/   88]
per-ex loss: 0.600721  [   14/   88]
per-ex loss: 0.592202  [   15/   88]
per-ex loss: 0.403022  [   16/   88]
per-ex loss: 0.393157  [   17/   88]
per-ex loss: 0.360504  [   18/   88]
per-ex loss: 0.338428  [   19/   88]
per-ex loss: 0.372702  [   20/   88]
per-ex loss: 0.431460  [   21/   88]
per-ex loss: 0.354713  [   22/   88]
per-ex loss: 0.641222  [   23/   88]
per-ex loss: 0.378642  [   24/   88]
per-ex loss: 0.644132  [   25/   88]
per-ex loss: 0.546808  [   26/   88]
per-ex loss: 0.352854  [   27/   88]
per-ex loss: 0.418719  [   28/   88]
per-ex loss: 0.460199  [   29/   88]
per-ex loss: 0.377128  [   30/   88]
per-ex loss: 0.347754  [   31/   88]
per-ex loss: 0.353527  [   32/   88]
per-ex loss: 0.377363  [   33/   88]
per-ex loss: 0.590037  [   34/   88]
per-ex loss: 0.545326  [   35/   88]
per-ex loss: 0.372185  [   36/   88]
per-ex loss: 0.390055  [   37/   88]
per-ex loss: 0.462528  [   38/   88]
per-ex loss: 0.528320  [   39/   88]
per-ex loss: 0.425931  [   40/   88]
per-ex loss: 0.383392  [   41/   88]
per-ex loss: 0.564604  [   42/   88]
per-ex loss: 0.676177  [   43/   88]
per-ex loss: 0.531091  [   44/   88]
per-ex loss: 0.559042  [   45/   88]
per-ex loss: 0.507449  [   46/   88]
per-ex loss: 0.557030  [   47/   88]
per-ex loss: 0.354730  [   48/   88]
per-ex loss: 0.410431  [   49/   88]
per-ex loss: 0.657918  [   50/   88]
per-ex loss: 0.390601  [   51/   88]
per-ex loss: 0.470845  [   52/   88]
per-ex loss: 0.638056  [   53/   88]
per-ex loss: 0.395681  [   54/   88]
per-ex loss: 0.378999  [   55/   88]
per-ex loss: 0.681931  [   56/   88]
per-ex loss: 0.403302  [   57/   88]
per-ex loss: 0.395375  [   58/   88]
per-ex loss: 0.406501  [   59/   88]
per-ex loss: 0.358541  [   60/   88]
per-ex loss: 0.533998  [   61/   88]
per-ex loss: 0.615440  [   62/   88]
per-ex loss: 0.538095  [   63/   88]
per-ex loss: 0.555221  [   64/   88]
per-ex loss: 0.429286  [   65/   88]
per-ex loss: 0.402380  [   66/   88]
per-ex loss: 0.664207  [   67/   88]
per-ex loss: 0.587047  [   68/   88]
per-ex loss: 0.435338  [   69/   88]
per-ex loss: 0.500118  [   70/   88]
per-ex loss: 0.394774  [   71/   88]
per-ex loss: 0.439059  [   72/   88]
per-ex loss: 0.568343  [   73/   88]
per-ex loss: 0.498462  [   74/   88]
per-ex loss: 0.674628  [   75/   88]
per-ex loss: 0.552272  [   76/   88]
per-ex loss: 0.399799  [   77/   88]
per-ex loss: 0.534946  [   78/   88]
per-ex loss: 0.303335  [   79/   88]
per-ex loss: 0.414683  [   80/   88]
per-ex loss: 0.555717  [   81/   88]
per-ex loss: 0.518561  [   82/   88]
per-ex loss: 0.346910  [   83/   88]
per-ex loss: 0.416344  [   84/   88]
per-ex loss: 0.629576  [   85/   88]
per-ex loss: 0.425639  [   86/   88]
per-ex loss: 0.357657  [   87/   88]
per-ex loss: 0.549366  [   88/   88]
Train Error: Avg loss: 0.48091119
validation Error: 
 Avg loss: 0.53669425 
 F1: 0.490831 
 Precision: 0.587303 
 Recall: 0.421581
 IoU: 0.325232

test Error: 
 Avg loss: 0.50000338 
 F1: 0.549001 
 Precision: 0.648309 
 Recall: 0.476075
 IoU: 0.378360

We have finished training iteration 52
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_50_.pth
per-ex loss: 0.627874  [    1/   88]
per-ex loss: 0.393291  [    2/   88]
per-ex loss: 0.648028  [    3/   88]
per-ex loss: 0.333508  [    4/   88]
per-ex loss: 0.413422  [    5/   88]
per-ex loss: 0.344493  [    6/   88]
per-ex loss: 0.535905  [    7/   88]
per-ex loss: 0.529437  [    8/   88]
per-ex loss: 0.561978  [    9/   88]
per-ex loss: 0.482403  [   10/   88]
per-ex loss: 0.391672  [   11/   88]
per-ex loss: 0.446995  [   12/   88]
per-ex loss: 0.479350  [   13/   88]
per-ex loss: 0.486592  [   14/   88]
per-ex loss: 0.352554  [   15/   88]
per-ex loss: 0.410388  [   16/   88]
per-ex loss: 0.357451  [   17/   88]
per-ex loss: 0.645511  [   18/   88]
per-ex loss: 0.615010  [   19/   88]
per-ex loss: 0.391265  [   20/   88]
per-ex loss: 0.366449  [   21/   88]
per-ex loss: 0.434866  [   22/   88]
per-ex loss: 0.395356  [   23/   88]
per-ex loss: 0.386997  [   24/   88]
per-ex loss: 0.482299  [   25/   88]
per-ex loss: 0.549492  [   26/   88]
per-ex loss: 0.407480  [   27/   88]
per-ex loss: 0.416926  [   28/   88]
per-ex loss: 0.517384  [   29/   88]
per-ex loss: 0.344989  [   30/   88]
per-ex loss: 0.358344  [   31/   88]
per-ex loss: 0.517983  [   32/   88]
per-ex loss: 0.656288  [   33/   88]
per-ex loss: 0.455778  [   34/   88]
per-ex loss: 0.391228  [   35/   88]
per-ex loss: 0.413170  [   36/   88]
per-ex loss: 0.449439  [   37/   88]
per-ex loss: 0.359435  [   38/   88]
per-ex loss: 0.608116  [   39/   88]
per-ex loss: 0.564427  [   40/   88]
per-ex loss: 0.323544  [   41/   88]
per-ex loss: 0.454558  [   42/   88]
per-ex loss: 0.451295  [   43/   88]
per-ex loss: 0.666450  [   44/   88]
per-ex loss: 0.294856  [   45/   88]
per-ex loss: 0.548709  [   46/   88]
per-ex loss: 0.652933  [   47/   88]
per-ex loss: 0.387024  [   48/   88]
per-ex loss: 0.556919  [   49/   88]
per-ex loss: 0.419796  [   50/   88]
per-ex loss: 0.400703  [   51/   88]
per-ex loss: 0.437647  [   52/   88]
per-ex loss: 0.446363  [   53/   88]
per-ex loss: 0.352534  [   54/   88]
per-ex loss: 0.576268  [   55/   88]
per-ex loss: 0.596999  [   56/   88]
per-ex loss: 0.602322  [   57/   88]
per-ex loss: 0.656108  [   58/   88]
per-ex loss: 0.553429  [   59/   88]
per-ex loss: 0.498147  [   60/   88]
per-ex loss: 0.386648  [   61/   88]
per-ex loss: 0.583426  [   62/   88]
per-ex loss: 0.399460  [   63/   88]
per-ex loss: 0.645255  [   64/   88]
per-ex loss: 0.455105  [   65/   88]
per-ex loss: 0.590918  [   66/   88]
per-ex loss: 0.621458  [   67/   88]
per-ex loss: 0.547374  [   68/   88]
per-ex loss: 0.394354  [   69/   88]
per-ex loss: 0.352564  [   70/   88]
per-ex loss: 0.424917  [   71/   88]
per-ex loss: 0.402639  [   72/   88]
per-ex loss: 0.593014  [   73/   88]
per-ex loss: 0.650665  [   74/   88]
per-ex loss: 0.587743  [   75/   88]
per-ex loss: 0.367202  [   76/   88]
per-ex loss: 0.351941  [   77/   88]
per-ex loss: 0.566090  [   78/   88]
per-ex loss: 0.429784  [   79/   88]
per-ex loss: 0.639397  [   80/   88]
per-ex loss: 0.554823  [   81/   88]
per-ex loss: 0.415683  [   82/   88]
per-ex loss: 0.553274  [   83/   88]
per-ex loss: 0.657886  [   84/   88]
per-ex loss: 0.544029  [   85/   88]
per-ex loss: 0.420199  [   86/   88]
per-ex loss: 0.421091  [   87/   88]
per-ex loss: 0.677450  [   88/   88]
Train Error: Avg loss: 0.48448370
validation Error: 
 Avg loss: 0.54105792 
 F1: 0.487716 
 Precision: 0.498879 
 Recall: 0.477042
 IoU: 0.322503

test Error: 
 Avg loss: 0.49295207 
 F1: 0.561209 
 Precision: 0.539814 
 Recall: 0.584369
 IoU: 0.390056

We have finished training iteration 53
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_51_.pth
per-ex loss: 0.619139  [    1/   88]
per-ex loss: 0.540921  [    2/   88]
per-ex loss: 0.538680  [    3/   88]
per-ex loss: 0.639716  [    4/   88]
per-ex loss: 0.497534  [    5/   88]
per-ex loss: 0.649769  [    6/   88]
per-ex loss: 0.539780  [    7/   88]
per-ex loss: 0.376750  [    8/   88]
per-ex loss: 0.491807  [    9/   88]
per-ex loss: 0.350036  [   10/   88]
per-ex loss: 0.626432  [   11/   88]
per-ex loss: 0.372077  [   12/   88]
per-ex loss: 0.585682  [   13/   88]
per-ex loss: 0.555640  [   14/   88]
per-ex loss: 0.397633  [   15/   88]
per-ex loss: 0.683982  [   16/   88]
per-ex loss: 0.415474  [   17/   88]
per-ex loss: 0.517286  [   18/   88]
per-ex loss: 0.560102  [   19/   88]
per-ex loss: 0.399402  [   20/   88]
per-ex loss: 0.552185  [   21/   88]
per-ex loss: 0.553629  [   22/   88]
per-ex loss: 0.635345  [   23/   88]
per-ex loss: 0.509927  [   24/   88]
per-ex loss: 0.482062  [   25/   88]
per-ex loss: 0.434317  [   26/   88]
per-ex loss: 0.666064  [   27/   88]
per-ex loss: 0.427289  [   28/   88]
per-ex loss: 0.371433  [   29/   88]
per-ex loss: 0.418020  [   30/   88]
per-ex loss: 0.458511  [   31/   88]
per-ex loss: 0.399558  [   32/   88]
per-ex loss: 0.638866  [   33/   88]
per-ex loss: 0.513053  [   34/   88]
per-ex loss: 0.391195  [   35/   88]
per-ex loss: 0.588768  [   36/   88]
per-ex loss: 0.405082  [   37/   88]
per-ex loss: 0.398749  [   38/   88]
per-ex loss: 0.391463  [   39/   88]
per-ex loss: 0.322577  [   40/   88]
per-ex loss: 0.649285  [   41/   88]
per-ex loss: 0.437861  [   42/   88]
per-ex loss: 0.502401  [   43/   88]
per-ex loss: 0.579904  [   44/   88]
per-ex loss: 0.651341  [   45/   88]
per-ex loss: 0.380536  [   46/   88]
per-ex loss: 0.355851  [   47/   88]
per-ex loss: 0.491970  [   48/   88]
per-ex loss: 0.445675  [   49/   88]
per-ex loss: 0.646494  [   50/   88]
per-ex loss: 0.463578  [   51/   88]
per-ex loss: 0.556959  [   52/   88]
per-ex loss: 0.472008  [   53/   88]
per-ex loss: 0.465968  [   54/   88]
per-ex loss: 0.314587  [   55/   88]
per-ex loss: 0.523977  [   56/   88]
per-ex loss: 0.590726  [   57/   88]
per-ex loss: 0.392214  [   58/   88]
per-ex loss: 0.624449  [   59/   88]
per-ex loss: 0.598875  [   60/   88]
per-ex loss: 0.400598  [   61/   88]
per-ex loss: 0.412560  [   62/   88]
per-ex loss: 0.688407  [   63/   88]
per-ex loss: 0.497429  [   64/   88]
per-ex loss: 0.400318  [   65/   88]
per-ex loss: 0.423549  [   66/   88]
per-ex loss: 0.393244  [   67/   88]
per-ex loss: 0.337721  [   68/   88]
per-ex loss: 0.341793  [   69/   88]
per-ex loss: 0.348389  [   70/   88]
per-ex loss: 0.388972  [   71/   88]
per-ex loss: 0.519330  [   72/   88]
per-ex loss: 0.342179  [   73/   88]
per-ex loss: 0.405428  [   74/   88]
per-ex loss: 0.337799  [   75/   88]
per-ex loss: 0.597491  [   76/   88]
per-ex loss: 0.411070  [   77/   88]
per-ex loss: 0.443563  [   78/   88]
per-ex loss: 0.578656  [   79/   88]
per-ex loss: 0.543321  [   80/   88]
per-ex loss: 0.625997  [   81/   88]
per-ex loss: 0.601650  [   82/   88]
per-ex loss: 0.362993  [   83/   88]
per-ex loss: 0.380595  [   84/   88]
per-ex loss: 0.343523  [   85/   88]
per-ex loss: 0.416401  [   86/   88]
per-ex loss: 0.382584  [   87/   88]
per-ex loss: 0.369730  [   88/   88]
Train Error: Avg loss: 0.48131687
validation Error: 
 Avg loss: 0.52675166 
 F1: 0.503604 
 Precision: 0.581826 
 Recall: 0.443922
 IoU: 0.336545

test Error: 
 Avg loss: 0.48530260 
 F1: 0.567387 
 Precision: 0.618902 
 Recall: 0.523788
 IoU: 0.396050

We have finished training iteration 54
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_52_.pth
per-ex loss: 0.659655  [    1/   88]
per-ex loss: 0.457439  [    2/   88]
per-ex loss: 0.540148  [    3/   88]
per-ex loss: 0.338593  [    4/   88]
per-ex loss: 0.568107  [    5/   88]
per-ex loss: 0.401655  [    6/   88]
per-ex loss: 0.591923  [    7/   88]
per-ex loss: 0.628201  [    8/   88]
per-ex loss: 0.528738  [    9/   88]
per-ex loss: 0.640525  [   10/   88]
per-ex loss: 0.462509  [   11/   88]
per-ex loss: 0.499107  [   12/   88]
per-ex loss: 0.665083  [   13/   88]
per-ex loss: 0.516901  [   14/   88]
per-ex loss: 0.371718  [   15/   88]
per-ex loss: 0.372141  [   16/   88]
per-ex loss: 0.366973  [   17/   88]
per-ex loss: 0.575231  [   18/   88]
per-ex loss: 0.349194  [   19/   88]
per-ex loss: 0.405879  [   20/   88]
per-ex loss: 0.464047  [   21/   88]
per-ex loss: 0.532667  [   22/   88]
per-ex loss: 0.359935  [   23/   88]
per-ex loss: 0.438301  [   24/   88]
per-ex loss: 0.429689  [   25/   88]
per-ex loss: 0.390161  [   26/   88]
per-ex loss: 0.310258  [   27/   88]
per-ex loss: 0.395289  [   28/   88]
per-ex loss: 0.567665  [   29/   88]
per-ex loss: 0.581261  [   30/   88]
per-ex loss: 0.411069  [   31/   88]
per-ex loss: 0.478457  [   32/   88]
per-ex loss: 0.558706  [   33/   88]
per-ex loss: 0.509362  [   34/   88]
per-ex loss: 0.585277  [   35/   88]
per-ex loss: 0.521304  [   36/   88]
per-ex loss: 0.345290  [   37/   88]
per-ex loss: 0.566995  [   38/   88]
per-ex loss: 0.393594  [   39/   88]
per-ex loss: 0.414310  [   40/   88]
per-ex loss: 0.434029  [   41/   88]
per-ex loss: 0.666525  [   42/   88]
per-ex loss: 0.353778  [   43/   88]
per-ex loss: 0.373122  [   44/   88]
per-ex loss: 0.519447  [   45/   88]
per-ex loss: 0.382231  [   46/   88]
per-ex loss: 0.408982  [   47/   88]
per-ex loss: 0.614884  [   48/   88]
per-ex loss: 0.371490  [   49/   88]
per-ex loss: 0.650194  [   50/   88]
per-ex loss: 0.356927  [   51/   88]
per-ex loss: 0.641641  [   52/   88]
per-ex loss: 0.469958  [   53/   88]
per-ex loss: 0.649509  [   54/   88]
per-ex loss: 0.412746  [   55/   88]
per-ex loss: 0.489369  [   56/   88]
per-ex loss: 0.585226  [   57/   88]
per-ex loss: 0.371549  [   58/   88]
per-ex loss: 0.438537  [   59/   88]
per-ex loss: 0.659806  [   60/   88]
per-ex loss: 0.454847  [   61/   88]
per-ex loss: 0.582842  [   62/   88]
per-ex loss: 0.410467  [   63/   88]
per-ex loss: 0.494705  [   64/   88]
per-ex loss: 0.410318  [   65/   88]
per-ex loss: 0.382117  [   66/   88]
per-ex loss: 0.462812  [   67/   88]
per-ex loss: 0.363321  [   68/   88]
per-ex loss: 0.353136  [   69/   88]
per-ex loss: 0.562005  [   70/   88]
per-ex loss: 0.526531  [   71/   88]
per-ex loss: 0.706702  [   72/   88]
per-ex loss: 0.401654  [   73/   88]
per-ex loss: 0.416067  [   74/   88]
per-ex loss: 0.596911  [   75/   88]
per-ex loss: 0.617312  [   76/   88]
per-ex loss: 0.582830  [   77/   88]
per-ex loss: 0.541807  [   78/   88]
per-ex loss: 0.574427  [   79/   88]
per-ex loss: 0.450257  [   80/   88]
per-ex loss: 0.366432  [   81/   88]
per-ex loss: 0.578129  [   82/   88]
per-ex loss: 0.402990  [   83/   88]
per-ex loss: 0.273582  [   84/   88]
per-ex loss: 0.398899  [   85/   88]
per-ex loss: 0.566851  [   86/   88]
per-ex loss: 0.352992  [   87/   88]
per-ex loss: 0.346170  [   88/   88]
Train Error: Avg loss: 0.47977748
validation Error: 
 Avg loss: 0.53826639 
 F1: 0.485262 
 Precision: 0.542423 
 Recall: 0.439000
 IoU: 0.320361

test Error: 
 Avg loss: 0.48621693 
 F1: 0.565000 
 Precision: 0.620084 
 Recall: 0.518904
 IoU: 0.393728

We have finished training iteration 55
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_53_.pth
per-ex loss: 0.383528  [    1/   88]
per-ex loss: 0.380711  [    2/   88]
per-ex loss: 0.637948  [    3/   88]
per-ex loss: 0.580481  [    4/   88]
per-ex loss: 0.397427  [    5/   88]
per-ex loss: 0.643721  [    6/   88]
per-ex loss: 0.594902  [    7/   88]
per-ex loss: 0.414468  [    8/   88]
per-ex loss: 0.343684  [    9/   88]
per-ex loss: 0.543282  [   10/   88]
per-ex loss: 0.680622  [   11/   88]
per-ex loss: 0.529582  [   12/   88]
per-ex loss: 0.342116  [   13/   88]
per-ex loss: 0.440791  [   14/   88]
per-ex loss: 0.492097  [   15/   88]
per-ex loss: 0.606495  [   16/   88]
per-ex loss: 0.538733  [   17/   88]
per-ex loss: 0.432660  [   18/   88]
per-ex loss: 0.352054  [   19/   88]
per-ex loss: 0.432831  [   20/   88]
per-ex loss: 0.614489  [   21/   88]
per-ex loss: 0.654709  [   22/   88]
per-ex loss: 0.369542  [   23/   88]
per-ex loss: 0.541538  [   24/   88]
per-ex loss: 0.382093  [   25/   88]
per-ex loss: 0.371765  [   26/   88]
per-ex loss: 0.353656  [   27/   88]
per-ex loss: 0.452594  [   28/   88]
per-ex loss: 0.424565  [   29/   88]
per-ex loss: 0.338145  [   30/   88]
per-ex loss: 0.539438  [   31/   88]
per-ex loss: 0.391976  [   32/   88]
per-ex loss: 0.404252  [   33/   88]
per-ex loss: 0.536652  [   34/   88]
per-ex loss: 0.333152  [   35/   88]
per-ex loss: 0.384376  [   36/   88]
per-ex loss: 0.390971  [   37/   88]
per-ex loss: 0.415909  [   38/   88]
per-ex loss: 0.444981  [   39/   88]
per-ex loss: 0.670668  [   40/   88]
per-ex loss: 0.405247  [   41/   88]
per-ex loss: 0.423479  [   42/   88]
per-ex loss: 0.392868  [   43/   88]
per-ex loss: 0.433572  [   44/   88]
per-ex loss: 0.582568  [   45/   88]
per-ex loss: 0.509797  [   46/   88]
per-ex loss: 0.619983  [   47/   88]
per-ex loss: 0.589713  [   48/   88]
per-ex loss: 0.389932  [   49/   88]
per-ex loss: 0.432072  [   50/   88]
per-ex loss: 0.619513  [   51/   88]
per-ex loss: 0.580730  [   52/   88]
per-ex loss: 0.379407  [   53/   88]
per-ex loss: 0.368167  [   54/   88]
per-ex loss: 0.465630  [   55/   88]
per-ex loss: 0.505142  [   56/   88]
per-ex loss: 0.446884  [   57/   88]
per-ex loss: 0.639745  [   58/   88]
per-ex loss: 0.579658  [   59/   88]
per-ex loss: 0.367461  [   60/   88]
per-ex loss: 0.374096  [   61/   88]
per-ex loss: 0.312313  [   62/   88]
per-ex loss: 0.699746  [   63/   88]
per-ex loss: 0.412825  [   64/   88]
per-ex loss: 0.558310  [   65/   88]
per-ex loss: 0.402978  [   66/   88]
per-ex loss: 0.401961  [   67/   88]
per-ex loss: 0.532772  [   68/   88]
per-ex loss: 0.369038  [   69/   88]
per-ex loss: 0.667742  [   70/   88]
per-ex loss: 0.532418  [   71/   88]
per-ex loss: 0.601597  [   72/   88]
per-ex loss: 0.395182  [   73/   88]
per-ex loss: 0.364961  [   74/   88]
per-ex loss: 0.583414  [   75/   88]
per-ex loss: 0.288775  [   76/   88]
per-ex loss: 0.467586  [   77/   88]
per-ex loss: 0.585024  [   78/   88]
per-ex loss: 0.643928  [   79/   88]
per-ex loss: 0.547562  [   80/   88]
per-ex loss: 0.561615  [   81/   88]
per-ex loss: 0.404693  [   82/   88]
per-ex loss: 0.497588  [   83/   88]
per-ex loss: 0.396940  [   84/   88]
per-ex loss: 0.560749  [   85/   88]
per-ex loss: 0.454744  [   86/   88]
per-ex loss: 0.503284  [   87/   88]
per-ex loss: 0.508589  [   88/   88]
Train Error: Avg loss: 0.47892729
validation Error: 
 Avg loss: 0.56069320 
 F1: 0.458963 
 Precision: 0.432016 
 Recall: 0.489496
 IoU: 0.297827

test Error: 
 Avg loss: 0.49515727 
 F1: 0.553338 
 Precision: 0.529108 
 Recall: 0.579895
 IoU: 0.382493

We have finished training iteration 56
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_38_.pth
per-ex loss: 0.362906  [    1/   88]
per-ex loss: 0.576252  [    2/   88]
per-ex loss: 0.661851  [    3/   88]
per-ex loss: 0.540648  [    4/   88]
per-ex loss: 0.473629  [    5/   88]
per-ex loss: 0.428927  [    6/   88]
per-ex loss: 0.321169  [    7/   88]
per-ex loss: 0.374891  [    8/   88]
per-ex loss: 0.367181  [    9/   88]
per-ex loss: 0.574199  [   10/   88]
per-ex loss: 0.373859  [   11/   88]
per-ex loss: 0.405108  [   12/   88]
per-ex loss: 0.406591  [   13/   88]
per-ex loss: 0.595498  [   14/   88]
per-ex loss: 0.308853  [   15/   88]
per-ex loss: 0.461907  [   16/   88]
per-ex loss: 0.507597  [   17/   88]
per-ex loss: 0.568129  [   18/   88]
per-ex loss: 0.344327  [   19/   88]
per-ex loss: 0.456084  [   20/   88]
per-ex loss: 0.433673  [   21/   88]
per-ex loss: 0.410784  [   22/   88]
per-ex loss: 0.492820  [   23/   88]
per-ex loss: 0.395157  [   24/   88]
per-ex loss: 0.361208  [   25/   88]
per-ex loss: 0.595504  [   26/   88]
per-ex loss: 0.495073  [   27/   88]
per-ex loss: 0.443607  [   28/   88]
per-ex loss: 0.582453  [   29/   88]
per-ex loss: 0.600766  [   30/   88]
per-ex loss: 0.529077  [   31/   88]
per-ex loss: 0.341325  [   32/   88]
per-ex loss: 0.405790  [   33/   88]
per-ex loss: 0.565981  [   34/   88]
per-ex loss: 0.625335  [   35/   88]
per-ex loss: 0.364585  [   36/   88]
per-ex loss: 0.395626  [   37/   88]
per-ex loss: 0.390033  [   38/   88]
per-ex loss: 0.520561  [   39/   88]
per-ex loss: 0.456918  [   40/   88]
per-ex loss: 0.396694  [   41/   88]
per-ex loss: 0.605460  [   42/   88]
per-ex loss: 0.359040  [   43/   88]
per-ex loss: 0.581663  [   44/   88]
per-ex loss: 0.543638  [   45/   88]
per-ex loss: 0.435664  [   46/   88]
per-ex loss: 0.525721  [   47/   88]
per-ex loss: 0.678458  [   48/   88]
per-ex loss: 0.667060  [   49/   88]
per-ex loss: 0.659549  [   50/   88]
per-ex loss: 0.388031  [   51/   88]
per-ex loss: 0.480052  [   52/   88]
per-ex loss: 0.380722  [   53/   88]
per-ex loss: 0.620975  [   54/   88]
per-ex loss: 0.513193  [   55/   88]
per-ex loss: 0.362929  [   56/   88]
per-ex loss: 0.468258  [   57/   88]
per-ex loss: 0.606599  [   58/   88]
per-ex loss: 0.517469  [   59/   88]
per-ex loss: 0.400795  [   60/   88]
per-ex loss: 0.644040  [   61/   88]
per-ex loss: 0.417756  [   62/   88]
per-ex loss: 0.669714  [   63/   88]
per-ex loss: 0.609908  [   64/   88]
per-ex loss: 0.389832  [   65/   88]
per-ex loss: 0.566086  [   66/   88]
per-ex loss: 0.575076  [   67/   88]
per-ex loss: 0.339339  [   68/   88]
per-ex loss: 0.439061  [   69/   88]
per-ex loss: 0.405394  [   70/   88]
per-ex loss: 0.401646  [   71/   88]
per-ex loss: 0.353568  [   72/   88]
per-ex loss: 0.548558  [   73/   88]
per-ex loss: 0.347805  [   74/   88]
per-ex loss: 0.424057  [   75/   88]
per-ex loss: 0.445556  [   76/   88]
per-ex loss: 0.508317  [   77/   88]
per-ex loss: 0.545788  [   78/   88]
per-ex loss: 0.352085  [   79/   88]
per-ex loss: 0.442596  [   80/   88]
per-ex loss: 0.649259  [   81/   88]
per-ex loss: 0.389615  [   82/   88]
per-ex loss: 0.474298  [   83/   88]
per-ex loss: 0.441619  [   84/   88]
per-ex loss: 0.582073  [   85/   88]
per-ex loss: 0.369977  [   86/   88]
per-ex loss: 0.518806  [   87/   88]
per-ex loss: 0.555121  [   88/   88]
Train Error: Avg loss: 0.47857730
validation Error: 
 Avg loss: 0.54482242 
 F1: 0.478430 
 Precision: 0.465106 
 Recall: 0.492540
 IoU: 0.314432

test Error: 
 Avg loss: 0.48594572 
 F1: 0.560434 
 Precision: 0.543669 
 Recall: 0.578266
 IoU: 0.389307

We have finished training iteration 57
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_55_.pth
per-ex loss: 0.396347  [    1/   88]
per-ex loss: 0.482138  [    2/   88]
per-ex loss: 0.374526  [    3/   88]
per-ex loss: 0.596931  [    4/   88]
per-ex loss: 0.496415  [    5/   88]
per-ex loss: 0.423015  [    6/   88]
per-ex loss: 0.487614  [    7/   88]
per-ex loss: 0.395528  [    8/   88]
per-ex loss: 0.349992  [    9/   88]
per-ex loss: 0.383064  [   10/   88]
per-ex loss: 0.588728  [   11/   88]
per-ex loss: 0.558786  [   12/   88]
per-ex loss: 0.385809  [   13/   88]
per-ex loss: 0.392712  [   14/   88]
per-ex loss: 0.662684  [   15/   88]
per-ex loss: 0.383608  [   16/   88]
per-ex loss: 0.415558  [   17/   88]
per-ex loss: 0.665979  [   18/   88]
per-ex loss: 0.663743  [   19/   88]
per-ex loss: 0.561800  [   20/   88]
per-ex loss: 0.402575  [   21/   88]
per-ex loss: 0.371190  [   22/   88]
per-ex loss: 0.622717  [   23/   88]
per-ex loss: 0.521485  [   24/   88]
per-ex loss: 0.332139  [   25/   88]
per-ex loss: 0.393173  [   26/   88]
per-ex loss: 0.379131  [   27/   88]
per-ex loss: 0.547266  [   28/   88]
per-ex loss: 0.350287  [   29/   88]
per-ex loss: 0.548345  [   30/   88]
per-ex loss: 0.546791  [   31/   88]
per-ex loss: 0.629281  [   32/   88]
per-ex loss: 0.364878  [   33/   88]
per-ex loss: 0.527082  [   34/   88]
per-ex loss: 0.583705  [   35/   88]
per-ex loss: 0.436818  [   36/   88]
per-ex loss: 0.383202  [   37/   88]
per-ex loss: 0.381104  [   38/   88]
per-ex loss: 0.499751  [   39/   88]
per-ex loss: 0.644797  [   40/   88]
per-ex loss: 0.576489  [   41/   88]
per-ex loss: 0.542031  [   42/   88]
per-ex loss: 0.403087  [   43/   88]
per-ex loss: 0.411875  [   44/   88]
per-ex loss: 0.360703  [   45/   88]
per-ex loss: 0.371952  [   46/   88]
per-ex loss: 0.697928  [   47/   88]
per-ex loss: 0.397579  [   48/   88]
per-ex loss: 0.427287  [   49/   88]
per-ex loss: 0.390902  [   50/   88]
per-ex loss: 0.382522  [   51/   88]
per-ex loss: 0.358715  [   52/   88]
per-ex loss: 0.314450  [   53/   88]
per-ex loss: 0.518090  [   54/   88]
per-ex loss: 0.394201  [   55/   88]
per-ex loss: 0.434512  [   56/   88]
per-ex loss: 0.536951  [   57/   88]
per-ex loss: 0.398064  [   58/   88]
per-ex loss: 0.407821  [   59/   88]
per-ex loss: 0.580253  [   60/   88]
per-ex loss: 0.341722  [   61/   88]
per-ex loss: 0.516679  [   62/   88]
per-ex loss: 0.587072  [   63/   88]
per-ex loss: 0.529503  [   64/   88]
per-ex loss: 0.350380  [   65/   88]
per-ex loss: 0.448651  [   66/   88]
per-ex loss: 0.557805  [   67/   88]
per-ex loss: 0.453185  [   68/   88]
per-ex loss: 0.404678  [   69/   88]
per-ex loss: 0.383300  [   70/   88]
per-ex loss: 0.670221  [   71/   88]
per-ex loss: 0.603163  [   72/   88]
per-ex loss: 0.451353  [   73/   88]
per-ex loss: 0.612743  [   74/   88]
per-ex loss: 0.574776  [   75/   88]
per-ex loss: 0.514563  [   76/   88]
per-ex loss: 0.391725  [   77/   88]
per-ex loss: 0.675992  [   78/   88]
per-ex loss: 0.556816  [   79/   88]
per-ex loss: 0.436274  [   80/   88]
per-ex loss: 0.606778  [   81/   88]
per-ex loss: 0.626652  [   82/   88]
per-ex loss: 0.531948  [   83/   88]
per-ex loss: 0.406243  [   84/   88]
per-ex loss: 0.453135  [   85/   88]
per-ex loss: 0.419303  [   86/   88]
per-ex loss: 0.567364  [   87/   88]
per-ex loss: 0.534616  [   88/   88]
Train Error: Avg loss: 0.48003124
validation Error: 
 Avg loss: 0.52771213 
 F1: 0.500899 
 Precision: 0.609840 
 Recall: 0.424981
 IoU: 0.334133

test Error: 
 Avg loss: 0.48568474 
 F1: 0.564235 
 Precision: 0.636104 
 Recall: 0.506958
 IoU: 0.392986

We have finished training iteration 58
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_56_.pth
per-ex loss: 0.315581  [    1/   88]
per-ex loss: 0.375591  [    2/   88]
per-ex loss: 0.599984  [    3/   88]
per-ex loss: 0.561706  [    4/   88]
per-ex loss: 0.578415  [    5/   88]
per-ex loss: 0.406323  [    6/   88]
per-ex loss: 0.405644  [    7/   88]
per-ex loss: 0.340477  [    8/   88]
per-ex loss: 0.457389  [    9/   88]
per-ex loss: 0.537263  [   10/   88]
per-ex loss: 0.459744  [   11/   88]
per-ex loss: 0.631284  [   12/   88]
per-ex loss: 0.433247  [   13/   88]
per-ex loss: 0.404658  [   14/   88]
per-ex loss: 0.358989  [   15/   88]
per-ex loss: 0.386071  [   16/   88]
per-ex loss: 0.442131  [   17/   88]
per-ex loss: 0.548719  [   18/   88]
per-ex loss: 0.382144  [   19/   88]
per-ex loss: 0.516269  [   20/   88]
per-ex loss: 0.400013  [   21/   88]
per-ex loss: 0.346916  [   22/   88]
per-ex loss: 0.462817  [   23/   88]
per-ex loss: 0.363575  [   24/   88]
per-ex loss: 0.658892  [   25/   88]
per-ex loss: 0.340180  [   26/   88]
per-ex loss: 0.377120  [   27/   88]
per-ex loss: 0.390794  [   28/   88]
per-ex loss: 0.639991  [   29/   88]
per-ex loss: 0.642874  [   30/   88]
per-ex loss: 0.550146  [   31/   88]
per-ex loss: 0.418151  [   32/   88]
per-ex loss: 0.370690  [   33/   88]
per-ex loss: 0.340208  [   34/   88]
per-ex loss: 0.614415  [   35/   88]
per-ex loss: 0.398135  [   36/   88]
per-ex loss: 0.342723  [   37/   88]
per-ex loss: 0.606931  [   38/   88]
per-ex loss: 0.546920  [   39/   88]
per-ex loss: 0.560572  [   40/   88]
per-ex loss: 0.622917  [   41/   88]
per-ex loss: 0.492882  [   42/   88]
per-ex loss: 0.405963  [   43/   88]
per-ex loss: 0.343101  [   44/   88]
per-ex loss: 0.577009  [   45/   88]
per-ex loss: 0.393655  [   46/   88]
per-ex loss: 0.389879  [   47/   88]
per-ex loss: 0.686047  [   48/   88]
per-ex loss: 0.512782  [   49/   88]
per-ex loss: 0.505605  [   50/   88]
per-ex loss: 0.293615  [   51/   88]
per-ex loss: 0.342660  [   52/   88]
per-ex loss: 0.674858  [   53/   88]
per-ex loss: 0.501125  [   54/   88]
per-ex loss: 0.669475  [   55/   88]
per-ex loss: 0.554673  [   56/   88]
per-ex loss: 0.582857  [   57/   88]
per-ex loss: 0.551989  [   58/   88]
per-ex loss: 0.619335  [   59/   88]
per-ex loss: 0.437279  [   60/   88]
per-ex loss: 0.426420  [   61/   88]
per-ex loss: 0.391163  [   62/   88]
per-ex loss: 0.367517  [   63/   88]
per-ex loss: 0.558900  [   64/   88]
per-ex loss: 0.433295  [   65/   88]
per-ex loss: 0.648196  [   66/   88]
per-ex loss: 0.422320  [   67/   88]
per-ex loss: 0.477881  [   68/   88]
per-ex loss: 0.369540  [   69/   88]
per-ex loss: 0.519404  [   70/   88]
per-ex loss: 0.581602  [   71/   88]
per-ex loss: 0.518043  [   72/   88]
per-ex loss: 0.381058  [   73/   88]
per-ex loss: 0.434456  [   74/   88]
per-ex loss: 0.414026  [   75/   88]
per-ex loss: 0.610375  [   76/   88]
per-ex loss: 0.576467  [   77/   88]
per-ex loss: 0.384641  [   78/   88]
per-ex loss: 0.503932  [   79/   88]
per-ex loss: 0.576095  [   80/   88]
per-ex loss: 0.450455  [   81/   88]
per-ex loss: 0.578507  [   82/   88]
per-ex loss: 0.536530  [   83/   88]
per-ex loss: 0.371435  [   84/   88]
per-ex loss: 0.387107  [   85/   88]
per-ex loss: 0.421306  [   86/   88]
per-ex loss: 0.411850  [   87/   88]
per-ex loss: 0.601812  [   88/   88]
Train Error: Avg loss: 0.47756509
validation Error: 
 Avg loss: 0.54367955 
 F1: 0.487210 
 Precision: 0.481691 
 Recall: 0.492856
 IoU: 0.322060

test Error: 
 Avg loss: 0.48834354 
 F1: 0.561841 
 Precision: 0.548868 
 Recall: 0.575443
 IoU: 0.390667

We have finished training iteration 59
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_57_.pth
per-ex loss: 0.346809  [    1/   88]
per-ex loss: 0.540221  [    2/   88]
per-ex loss: 0.620477  [    3/   88]
per-ex loss: 0.410884  [    4/   88]
per-ex loss: 0.309667  [    5/   88]
per-ex loss: 0.534847  [    6/   88]
per-ex loss: 0.345960  [    7/   88]
per-ex loss: 0.592834  [    8/   88]
per-ex loss: 0.412177  [    9/   88]
per-ex loss: 0.618034  [   10/   88]
per-ex loss: 0.606852  [   11/   88]
per-ex loss: 0.442426  [   12/   88]
per-ex loss: 0.464788  [   13/   88]
per-ex loss: 0.534369  [   14/   88]
per-ex loss: 0.326499  [   15/   88]
per-ex loss: 0.366102  [   16/   88]
per-ex loss: 0.527388  [   17/   88]
per-ex loss: 0.385719  [   18/   88]
per-ex loss: 0.397260  [   19/   88]
per-ex loss: 0.657122  [   20/   88]
per-ex loss: 0.388511  [   21/   88]
per-ex loss: 0.608479  [   22/   88]
per-ex loss: 0.528228  [   23/   88]
per-ex loss: 0.545241  [   24/   88]
per-ex loss: 0.346668  [   25/   88]
per-ex loss: 0.438963  [   26/   88]
per-ex loss: 0.574418  [   27/   88]
per-ex loss: 0.552345  [   28/   88]
per-ex loss: 0.562387  [   29/   88]
per-ex loss: 0.576426  [   30/   88]
per-ex loss: 0.491715  [   31/   88]
per-ex loss: 0.652110  [   32/   88]
per-ex loss: 0.543923  [   33/   88]
per-ex loss: 0.642580  [   34/   88]
per-ex loss: 0.582770  [   35/   88]
per-ex loss: 0.566878  [   36/   88]
per-ex loss: 0.361060  [   37/   88]
per-ex loss: 0.391277  [   38/   88]
per-ex loss: 0.411434  [   39/   88]
per-ex loss: 0.458285  [   40/   88]
per-ex loss: 0.345468  [   41/   88]
per-ex loss: 0.654479  [   42/   88]
per-ex loss: 0.402460  [   43/   88]
per-ex loss: 0.424707  [   44/   88]
per-ex loss: 0.423596  [   45/   88]
per-ex loss: 0.339363  [   46/   88]
per-ex loss: 0.386956  [   47/   88]
per-ex loss: 0.375678  [   48/   88]
per-ex loss: 0.364599  [   49/   88]
per-ex loss: 0.469641  [   50/   88]
per-ex loss: 0.606901  [   51/   88]
per-ex loss: 0.396311  [   52/   88]
per-ex loss: 0.402968  [   53/   88]
per-ex loss: 0.683361  [   54/   88]
per-ex loss: 0.371206  [   55/   88]
per-ex loss: 0.417169  [   56/   88]
per-ex loss: 0.671955  [   57/   88]
per-ex loss: 0.384587  [   58/   88]
per-ex loss: 0.504049  [   59/   88]
per-ex loss: 0.443130  [   60/   88]
per-ex loss: 0.577731  [   61/   88]
per-ex loss: 0.362886  [   62/   88]
per-ex loss: 0.423291  [   63/   88]
per-ex loss: 0.664938  [   64/   88]
per-ex loss: 0.585138  [   65/   88]
per-ex loss: 0.394485  [   66/   88]
per-ex loss: 0.544051  [   67/   88]
per-ex loss: 0.529996  [   68/   88]
per-ex loss: 0.411018  [   69/   88]
per-ex loss: 0.420960  [   70/   88]
per-ex loss: 0.444074  [   71/   88]
per-ex loss: 0.479642  [   72/   88]
per-ex loss: 0.517878  [   73/   88]
per-ex loss: 0.366055  [   74/   88]
per-ex loss: 0.436596  [   75/   88]
per-ex loss: 0.369174  [   76/   88]
per-ex loss: 0.478006  [   77/   88]
per-ex loss: 0.350671  [   78/   88]
per-ex loss: 0.534619  [   79/   88]
per-ex loss: 0.482774  [   80/   88]
per-ex loss: 0.402927  [   81/   88]
per-ex loss: 0.358464  [   82/   88]
per-ex loss: 0.626153  [   83/   88]
per-ex loss: 0.430131  [   84/   88]
per-ex loss: 0.591972  [   85/   88]
per-ex loss: 0.399829  [   86/   88]
per-ex loss: 0.554571  [   87/   88]
per-ex loss: 0.347668  [   88/   88]
Train Error: Avg loss: 0.47520932
validation Error: 
 Avg loss: 0.52613258 
 F1: 0.505014 
 Precision: 0.561309 
 Recall: 0.458982
 IoU: 0.337805

test Error: 
 Avg loss: 0.48255081 
 F1: 0.569028 
 Precision: 0.593609 
 Recall: 0.546402
 IoU: 0.397651

We have finished training iteration 60
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_48_.pth
per-ex loss: 0.346870  [    1/   88]
per-ex loss: 0.368819  [    2/   88]
per-ex loss: 0.315646  [    3/   88]
per-ex loss: 0.411307  [    4/   88]
per-ex loss: 0.555708  [    5/   88]
per-ex loss: 0.346660  [    6/   88]
per-ex loss: 0.354524  [    7/   88]
per-ex loss: 0.608212  [    8/   88]
per-ex loss: 0.654094  [    9/   88]
per-ex loss: 0.556963  [   10/   88]
per-ex loss: 0.422008  [   11/   88]
per-ex loss: 0.403486  [   12/   88]
per-ex loss: 0.361495  [   13/   88]
per-ex loss: 0.414149  [   14/   88]
per-ex loss: 0.428237  [   15/   88]
per-ex loss: 0.583543  [   16/   88]
per-ex loss: 0.520515  [   17/   88]
per-ex loss: 0.392572  [   18/   88]
per-ex loss: 0.450553  [   19/   88]
per-ex loss: 0.423680  [   20/   88]
per-ex loss: 0.309994  [   21/   88]
per-ex loss: 0.477233  [   22/   88]
per-ex loss: 0.331926  [   23/   88]
per-ex loss: 0.581980  [   24/   88]
per-ex loss: 0.344552  [   25/   88]
per-ex loss: 0.529505  [   26/   88]
per-ex loss: 0.458940  [   27/   88]
per-ex loss: 0.493864  [   28/   88]
per-ex loss: 0.351895  [   29/   88]
per-ex loss: 0.578720  [   30/   88]
per-ex loss: 0.355517  [   31/   88]
per-ex loss: 0.562165  [   32/   88]
per-ex loss: 0.441520  [   33/   88]
per-ex loss: 0.594978  [   34/   88]
per-ex loss: 0.533650  [   35/   88]
per-ex loss: 0.696510  [   36/   88]
per-ex loss: 0.530350  [   37/   88]
per-ex loss: 0.418770  [   38/   88]
per-ex loss: 0.442705  [   39/   88]
per-ex loss: 0.596199  [   40/   88]
per-ex loss: 0.398761  [   41/   88]
per-ex loss: 0.366193  [   42/   88]
per-ex loss: 0.500447  [   43/   88]
per-ex loss: 0.625206  [   44/   88]
per-ex loss: 0.351205  [   45/   88]
per-ex loss: 0.343401  [   46/   88]
per-ex loss: 0.502875  [   47/   88]
per-ex loss: 0.692776  [   48/   88]
per-ex loss: 0.381187  [   49/   88]
per-ex loss: 0.596468  [   50/   88]
per-ex loss: 0.458949  [   51/   88]
per-ex loss: 0.482797  [   52/   88]
per-ex loss: 0.414017  [   53/   88]
per-ex loss: 0.507370  [   54/   88]
per-ex loss: 0.518022  [   55/   88]
per-ex loss: 0.563799  [   56/   88]
per-ex loss: 0.411556  [   57/   88]
per-ex loss: 0.599761  [   58/   88]
per-ex loss: 0.578744  [   59/   88]
per-ex loss: 0.613075  [   60/   88]
per-ex loss: 0.526817  [   61/   88]
per-ex loss: 0.635577  [   62/   88]
per-ex loss: 0.394163  [   63/   88]
per-ex loss: 0.490963  [   64/   88]
per-ex loss: 0.382685  [   65/   88]
per-ex loss: 0.425637  [   66/   88]
per-ex loss: 0.668973  [   67/   88]
per-ex loss: 0.563160  [   68/   88]
per-ex loss: 0.506946  [   69/   88]
per-ex loss: 0.397940  [   70/   88]
per-ex loss: 0.393942  [   71/   88]
per-ex loss: 0.341033  [   72/   88]
per-ex loss: 0.410834  [   73/   88]
per-ex loss: 0.427932  [   74/   88]
per-ex loss: 0.430565  [   75/   88]
per-ex loss: 0.619648  [   76/   88]
per-ex loss: 0.619830  [   77/   88]
per-ex loss: 0.414426  [   78/   88]
per-ex loss: 0.613265  [   79/   88]
per-ex loss: 0.403088  [   80/   88]
per-ex loss: 0.430835  [   81/   88]
per-ex loss: 0.350850  [   82/   88]
per-ex loss: 0.482478  [   83/   88]
per-ex loss: 0.492856  [   84/   88]
per-ex loss: 0.646973  [   85/   88]
per-ex loss: 0.327197  [   86/   88]
per-ex loss: 0.671942  [   87/   88]
per-ex loss: 0.372648  [   88/   88]
Train Error: Avg loss: 0.47613438
validation Error: 
 Avg loss: 0.53132734 
 F1: 0.500831 
 Precision: 0.557012 
 Recall: 0.454944
 IoU: 0.334072

test Error: 
 Avg loss: 0.47994194 
 F1: 0.573666 
 Precision: 0.612907 
 Recall: 0.539148
 IoU: 0.402196

We have finished training iteration 61
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_59_.pth
per-ex loss: 0.368264  [    1/   88]
per-ex loss: 0.357199  [    2/   88]
per-ex loss: 0.330574  [    3/   88]
per-ex loss: 0.501501  [    4/   88]
per-ex loss: 0.356882  [    5/   88]
per-ex loss: 0.478150  [    6/   88]
per-ex loss: 0.379956  [    7/   88]
per-ex loss: 0.402808  [    8/   88]
per-ex loss: 0.575259  [    9/   88]
per-ex loss: 0.396325  [   10/   88]
per-ex loss: 0.460222  [   11/   88]
per-ex loss: 0.412328  [   12/   88]
per-ex loss: 0.416248  [   13/   88]
per-ex loss: 0.531640  [   14/   88]
per-ex loss: 0.459861  [   15/   88]
per-ex loss: 0.621288  [   16/   88]
per-ex loss: 0.442221  [   17/   88]
per-ex loss: 0.408954  [   18/   88]
per-ex loss: 0.365500  [   19/   88]
per-ex loss: 0.353582  [   20/   88]
per-ex loss: 0.535399  [   21/   88]
per-ex loss: 0.585144  [   22/   88]
per-ex loss: 0.434475  [   23/   88]
per-ex loss: 0.357179  [   24/   88]
per-ex loss: 0.501960  [   25/   88]
per-ex loss: 0.553886  [   26/   88]
per-ex loss: 0.413170  [   27/   88]
per-ex loss: 0.401036  [   28/   88]
per-ex loss: 0.469536  [   29/   88]
per-ex loss: 0.299244  [   30/   88]
per-ex loss: 0.479498  [   31/   88]
per-ex loss: 0.376098  [   32/   88]
per-ex loss: 0.379694  [   33/   88]
per-ex loss: 0.641164  [   34/   88]
per-ex loss: 0.415882  [   35/   88]
per-ex loss: 0.580540  [   36/   88]
per-ex loss: 0.357077  [   37/   88]
per-ex loss: 0.395405  [   38/   88]
per-ex loss: 0.639141  [   39/   88]
per-ex loss: 0.503290  [   40/   88]
per-ex loss: 0.456232  [   41/   88]
per-ex loss: 0.564380  [   42/   88]
per-ex loss: 0.367247  [   43/   88]
per-ex loss: 0.656648  [   44/   88]
per-ex loss: 0.615065  [   45/   88]
per-ex loss: 0.384687  [   46/   88]
per-ex loss: 0.356841  [   47/   88]
per-ex loss: 0.409790  [   48/   88]
per-ex loss: 0.438721  [   49/   88]
per-ex loss: 0.636110  [   50/   88]
per-ex loss: 0.324564  [   51/   88]
per-ex loss: 0.519752  [   52/   88]
per-ex loss: 0.584077  [   53/   88]
per-ex loss: 0.636924  [   54/   88]
per-ex loss: 0.513073  [   55/   88]
per-ex loss: 0.626048  [   56/   88]
per-ex loss: 0.543793  [   57/   88]
per-ex loss: 0.387647  [   58/   88]
per-ex loss: 0.579770  [   59/   88]
per-ex loss: 0.366847  [   60/   88]
per-ex loss: 0.561354  [   61/   88]
per-ex loss: 0.342064  [   62/   88]
per-ex loss: 0.376120  [   63/   88]
per-ex loss: 0.542912  [   64/   88]
per-ex loss: 0.647343  [   65/   88]
per-ex loss: 0.444655  [   66/   88]
per-ex loss: 0.373888  [   67/   88]
per-ex loss: 0.460748  [   68/   88]
per-ex loss: 0.393133  [   69/   88]
per-ex loss: 0.388897  [   70/   88]
per-ex loss: 0.691092  [   71/   88]
per-ex loss: 0.378198  [   72/   88]
per-ex loss: 0.401112  [   73/   88]
per-ex loss: 0.586092  [   74/   88]
per-ex loss: 0.362033  [   75/   88]
per-ex loss: 0.694493  [   76/   88]
per-ex loss: 0.461742  [   77/   88]
per-ex loss: 0.567002  [   78/   88]
per-ex loss: 0.558247  [   79/   88]
per-ex loss: 0.538862  [   80/   88]
per-ex loss: 0.575494  [   81/   88]
per-ex loss: 0.511698  [   82/   88]
per-ex loss: 0.542020  [   83/   88]
per-ex loss: 0.469434  [   84/   88]
per-ex loss: 0.586709  [   85/   88]
per-ex loss: 0.396465  [   86/   88]
per-ex loss: 0.511581  [   87/   88]
per-ex loss: 0.336146  [   88/   88]
Train Error: Avg loss: 0.47278786
validation Error: 
 Avg loss: 0.55733480 
 F1: 0.470894 
 Precision: 0.495508 
 Recall: 0.448610
 IoU: 0.307954

test Error: 
 Avg loss: 0.48233276 
 F1: 0.565117 
 Precision: 0.612969 
 Recall: 0.524195
 IoU: 0.393842

We have finished training iteration 62
Deleting model ./unet_att_res_train/saved_model_wrapper/models/UNet_58_.pth
slurmstepd: error: *** STEP 16862.0 ON aga2 CANCELLED AT 2025-01-14T16:09:22 ***
