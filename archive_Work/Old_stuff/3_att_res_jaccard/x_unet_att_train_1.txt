unet_att_main_just_train.py do_log: False
Log file name: log_14_13-55-58_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(sd='unet_att_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_att.yaml', yo=None, ntibp=None, ptp=None, map=None)
YAML: {'path_to_data': './Data/vein_and_sclera_data', 'batch_size': 1, 'learning_rate': 1e-05, 'num_of_dataloader_workers': 7, 'train_epoch_size_limit': 400, 'num_epochs_per_training_iteration': 1, 'cleanup_k': 3, 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': True, 'loss_fn_name': 'MCDL', 'alphas': [], 'dataset_option': 'aug_tf', 'zero_out_non_sclera': True, 'add_sclera_to_img': False, 'add_bcosfire_to_img': True, 'add_coye_to_img': True, 'model_type': 'att', 'model': '64_2_6', 'input_width': 1024, 'input_height': 512, 'input_channels': 5, 'output_channels': 2, 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq'}
Validation phase: False
Namespace(sd='unet_att_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_att.yaml', yo=None, ntibp=None, ptp=None, map=None)
Device: cuda
dataset_aug_tf.py do_log: False
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 88
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 88
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 88
val dataloader num of batches: 27
test dataloader num of batches: 12
Created new model instance.
per-ex loss: 0.872391  [    1/   88]
per-ex loss: 0.894208  [    2/   88]
per-ex loss: 0.911858  [    3/   88]
per-ex loss: 0.960531  [    4/   88]
per-ex loss: 0.761140  [    5/   88]
per-ex loss: 0.879799  [    6/   88]
per-ex loss: 0.904022  [    7/   88]
per-ex loss: 0.833269  [    8/   88]
per-ex loss: 0.913005  [    9/   88]
per-ex loss: 0.925405  [   10/   88]
per-ex loss: 0.759983  [   11/   88]
per-ex loss: 0.887388  [   12/   88]
per-ex loss: 0.795048  [   13/   88]
per-ex loss: 0.777474  [   14/   88]
per-ex loss: 0.804664  [   15/   88]
per-ex loss: 0.732494  [   16/   88]
per-ex loss: 0.749678  [   17/   88]
per-ex loss: 0.927301  [   18/   88]
per-ex loss: 0.803658  [   19/   88]
per-ex loss: 0.806464  [   20/   88]
per-ex loss: 0.623875  [   21/   88]
per-ex loss: 0.777558  [   22/   88]
per-ex loss: 0.883502  [   23/   88]
per-ex loss: 0.909505  [   24/   88]
per-ex loss: 0.844892  [   25/   88]
per-ex loss: 0.937956  [   26/   88]
per-ex loss: 0.896154  [   27/   88]
per-ex loss: 0.814041  [   28/   88]
per-ex loss: 0.927512  [   29/   88]
per-ex loss: 0.792434  [   30/   88]
per-ex loss: 0.852839  [   31/   88]
per-ex loss: 0.902104  [   32/   88]
per-ex loss: 0.837328  [   33/   88]
per-ex loss: 0.782235  [   34/   88]
per-ex loss: 0.832671  [   35/   88]
per-ex loss: 0.927736  [   36/   88]
per-ex loss: 0.939302  [   37/   88]
per-ex loss: 0.815163  [   38/   88]
per-ex loss: 0.848306  [   39/   88]
per-ex loss: 0.763266  [   40/   88]
per-ex loss: 0.925780  [   41/   88]
per-ex loss: 0.929504  [   42/   88]
per-ex loss: 0.899091  [   43/   88]
per-ex loss: 0.759732  [   44/   88]
per-ex loss: 0.736723  [   45/   88]
per-ex loss: 0.855024  [   46/   88]
per-ex loss: 0.801387  [   47/   88]
per-ex loss: 0.931293  [   48/   88]
per-ex loss: 0.891228  [   49/   88]
per-ex loss: 0.814442  [   50/   88]
per-ex loss: 0.887020  [   51/   88]
per-ex loss: 0.719493  [   52/   88]
per-ex loss: 0.885337  [   53/   88]
per-ex loss: 0.890325  [   54/   88]
per-ex loss: 0.730356  [   55/   88]
per-ex loss: 0.912458  [   56/   88]
per-ex loss: 0.887150  [   57/   88]
per-ex loss: 0.870937  [   58/   88]
per-ex loss: 0.642011  [   59/   88]
per-ex loss: 0.873434  [   60/   88]
per-ex loss: 0.894952  [   61/   88]
per-ex loss: 0.832512  [   62/   88]
per-ex loss: 0.637482  [   63/   88]
per-ex loss: 0.753202  [   64/   88]
per-ex loss: 0.891782  [   65/   88]
per-ex loss: 0.926052  [   66/   88]
per-ex loss: 0.586617  [   67/   88]
per-ex loss: 0.867560  [   68/   88]
per-ex loss: 0.816824  [   69/   88]
per-ex loss: 0.801314  [   70/   88]
per-ex loss: 0.883430  [   71/   88]
per-ex loss: 0.773092  [   72/   88]
per-ex loss: 0.929711  [   73/   88]
per-ex loss: 0.733021  [   74/   88]
per-ex loss: 0.917681  [   75/   88]
per-ex loss: 0.723585  [   76/   88]
per-ex loss: 0.655948  [   77/   88]
per-ex loss: 0.823236  [   78/   88]
per-ex loss: 0.649429  [   79/   88]
per-ex loss: 0.799638  [   80/   88]
per-ex loss: 0.627100  [   81/   88]
per-ex loss: 0.661896  [   82/   88]
per-ex loss: 0.741014  [   83/   88]
per-ex loss: 0.698054  [   84/   88]
per-ex loss: 0.831704  [   85/   88]
per-ex loss: 0.679641  [   86/   88]
per-ex loss: 0.714212  [   87/   88]
per-ex loss: 0.905607  [   88/   88]
Train Error: Avg loss: 0.82280889
validation Error: 
 Avg loss: 0.76549053 
 F1: 0.360057 
 Precision: 0.487469 
 Recall: 0.285448
 IoU: 0.219555

test Error: 
 Avg loss: 0.74872770 
 F1: 0.411217 
 Precision: 0.434033 
 Recall: 0.390680
 IoU: 0.258825

We have finished training iteration 1
slurmstepd: error: *** STEP 16859.0 ON aga1 CANCELLED AT 2025-01-14T13:57:52 ***
