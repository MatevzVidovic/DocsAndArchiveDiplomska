unet_att_main_just_train.py do_log: False
Log file name: log_14_13-59-18_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(sd='unet_att_j_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_att_j.yaml', yo=None, ntibp=None, ptp=None, map=None)
YAML: {'path_to_data': './Data/vein_and_sclera_data', 'batch_size': 1, 'learning_rate': 1e-05, 'num_of_dataloader_workers': 7, 'train_epoch_size_limit': 400, 'num_epochs_per_training_iteration': 1, 'cleanup_k': 3, 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': True, 'loss_fn_name': 'JACCARD', 'alphas': [], 'dataset_option': 'aug_tf', 'zero_out_non_sclera': True, 'add_sclera_to_img': False, 'add_bcosfire_to_img': True, 'add_coye_to_img': True, 'model_type': 'att', 'model': '64_2_6', 'input_width': 2048, 'input_height': 1024, 'input_channels': 5, 'output_channels': 2, 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq'}
Validation phase: False
Namespace(sd='unet_att_j_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_att_j.yaml', yo=None, ntibp=None, ptp=None, map=None)
Device: cuda
dataset_aug_tf.py do_log: False
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 88
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 88
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 88
val dataloader num of batches: 27
test dataloader num of batches: 12
Created new model instance.
per-ex loss: 0.976405  [    1/   88]
per-ex loss: 0.982247  [    2/   88]
per-ex loss: 0.948439  [    3/   88]
per-ex loss: 0.965735  [    4/   88]
per-ex loss: 0.907411  [    5/   88]
per-ex loss: 0.944154  [    6/   88]
per-ex loss: 0.951204  [    7/   88]
per-ex loss: 0.955582  [    8/   88]
per-ex loss: 0.890778  [    9/   88]
per-ex loss: 0.924981  [   10/   88]
per-ex loss: 0.958262  [   11/   88]
per-ex loss: 0.950377  [   12/   88]
per-ex loss: 0.905327  [   13/   88]
per-ex loss: 0.931856  [   14/   88]
per-ex loss: 0.943981  [   15/   88]
per-ex loss: 0.794262  [   16/   88]
per-ex loss: 0.936439  [   17/   88]
per-ex loss: 0.835050  [   18/   88]
per-ex loss: 0.931852  [   19/   88]
per-ex loss: 0.912184  [   20/   88]
per-ex loss: 0.961947  [   21/   88]
per-ex loss: 0.960215  [   22/   88]
per-ex loss: 0.933443  [   23/   88]
per-ex loss: 0.782028  [   24/   88]
per-ex loss: 0.840346  [   25/   88]
per-ex loss: 0.951605  [   26/   88]
per-ex loss: 0.940898  [   27/   88]
per-ex loss: 0.817957  [   28/   88]
per-ex loss: 0.950267  [   29/   88]
per-ex loss: 0.948531  [   30/   88]
per-ex loss: 0.943151  [   31/   88]
per-ex loss: 0.845098  [   32/   88]
per-ex loss: 0.801186  [   33/   88]
per-ex loss: 0.890790  [   34/   88]
per-ex loss: 0.828721  [   35/   88]
per-ex loss: 0.940161  [   36/   88]
per-ex loss: 0.871877  [   37/   88]
per-ex loss: 0.905465  [   38/   88]
per-ex loss: 0.919840  [   39/   88]
per-ex loss: 0.797209  [   40/   88]
per-ex loss: 0.919885  [   41/   88]
per-ex loss: 0.876846  [   42/   88]
per-ex loss: 0.818497  [   43/   88]
per-ex loss: 0.850219  [   44/   88]
per-ex loss: 0.918013  [   45/   88]
per-ex loss: 0.926333  [   46/   88]
per-ex loss: 0.824574  [   47/   88]
per-ex loss: 0.839647  [   48/   88]
per-ex loss: 0.751467  [   49/   88]
per-ex loss: 0.822184  [   50/   88]
per-ex loss: 0.722421  [   51/   88]
per-ex loss: 0.851039  [   52/   88]
per-ex loss: 0.827608  [   53/   88]
per-ex loss: 0.753332  [   54/   88]
per-ex loss: 0.944485  [   55/   88]
per-ex loss: 0.884295  [   56/   88]
per-ex loss: 0.887420  [   57/   88]
per-ex loss: 0.880467  [   58/   88]
per-ex loss: 0.744066  [   59/   88]
per-ex loss: 0.923404  [   60/   88]
per-ex loss: 0.889111  [   61/   88]
per-ex loss: 0.831056  [   62/   88]
per-ex loss: 0.766800  [   63/   88]
per-ex loss: 0.810890  [   64/   88]
per-ex loss: 0.680180  [   65/   88]
per-ex loss: 0.648988  [   66/   88]
per-ex loss: 0.739871  [   67/   88]
per-ex loss: 0.764259  [   68/   88]
per-ex loss: 0.903879  [   69/   88]
per-ex loss: 0.718126  [   70/   88]
per-ex loss: 0.929270  [   71/   88]
per-ex loss: 0.861087  [   72/   88]
per-ex loss: 0.682735  [   73/   88]
per-ex loss: 0.729367  [   74/   88]
per-ex loss: 0.752849  [   75/   88]
per-ex loss: 0.811388  [   76/   88]
per-ex loss: 0.764759  [   77/   88]
per-ex loss: 0.857253  [   78/   88]
per-ex loss: 0.896854  [   79/   88]
per-ex loss: 0.658292  [   80/   88]
per-ex loss: 0.836903  [   81/   88]
per-ex loss: 0.723843  [   82/   88]
per-ex loss: 0.691171  [   83/   88]
per-ex loss: 0.783481  [   84/   88]
per-ex loss: 0.753383  [   85/   88]
per-ex loss: 0.866954  [   86/   88]
per-ex loss: 0.742710  [   87/   88]
per-ex loss: 0.675249  [   88/   88]
Train Error: Avg loss: 0.85357010
validation Error: 
 Avg loss: 0.79571772 
 F1: 0.398733 
 Precision: 0.382859 
 Recall: 0.415980
 IoU: 0.249011

test Error: 
 Avg loss: 0.77891747 
 F1: 0.441914 
 Precision: 0.361659 
 Recall: 0.567945
 IoU: 0.283626

We have finished training iteration 1
per-ex loss: 0.719506  [    1/   88]
per-ex loss: 0.721503  [    2/   88]
per-ex loss: 0.647614  [    3/   88]
per-ex loss: 0.845485  [    4/   88]
per-ex loss: 0.827245  [    5/   88]
per-ex loss: 0.710112  [    6/   88]
per-ex loss: 0.720954  [    7/   88]
per-ex loss: 0.802799  [    8/   88]
per-ex loss: 0.641803  [    9/   88]
per-ex loss: 0.817794  [   10/   88]
per-ex loss: 0.831380  [   11/   88]
per-ex loss: 0.742186  [   12/   88]
per-ex loss: 0.749319  [   13/   88]
per-ex loss: 0.879539  [   14/   88]
per-ex loss: 0.761947  [   15/   88]
per-ex loss: 0.710528  [   16/   88]
per-ex loss: 0.621176  [   17/   88]
per-ex loss: 0.824141  [   18/   88]
per-ex loss: 0.776576  [   19/   88]
per-ex loss: 0.659953  [   20/   88]
per-ex loss: 0.802686  [   21/   88]
per-ex loss: 0.652088  [   22/   88]
per-ex loss: 0.806946  [   23/   88]
per-ex loss: 0.719860  [   24/   88]
per-ex loss: 0.719880  [   25/   88]
per-ex loss: 0.569900  [   26/   88]
per-ex loss: 0.634560  [   27/   88]
per-ex loss: 0.769416  [   28/   88]
per-ex loss: 0.833771  [   29/   88]
per-ex loss: 0.843912  [   30/   88]
per-ex loss: 0.738985  [   31/   88]
per-ex loss: 0.643281  [   32/   88]
per-ex loss: 0.645143  [   33/   88]
per-ex loss: 0.831957  [   34/   88]
per-ex loss: 0.861141  [   35/   88]
per-ex loss: 0.844435  [   36/   88]
per-ex loss: 0.780497  [   37/   88]
per-ex loss: 0.756842  [   38/   88]
per-ex loss: 0.821448  [   39/   88]
per-ex loss: 0.817477  [   40/   88]
per-ex loss: 0.850241  [   41/   88]
per-ex loss: 0.684947  [   42/   88]
per-ex loss: 0.657873  [   43/   88]
per-ex loss: 0.733787  [   44/   88]
per-ex loss: 0.720924  [   45/   88]
per-ex loss: 0.729929  [   46/   88]
per-ex loss: 0.698780  [   47/   88]
per-ex loss: 0.821522  [   48/   88]
per-ex loss: 0.703707  [   49/   88]
per-ex loss: 0.694852  [   50/   88]
per-ex loss: 0.648897  [   51/   88]
per-ex loss: 0.879147  [   52/   88]
per-ex loss: 0.839098  [   53/   88]
per-ex loss: 0.653274  [   54/   88]
per-ex loss: 0.671176  [   55/   88]
per-ex loss: 0.752650  [   56/   88]
per-ex loss: 0.775634  [   57/   88]
per-ex loss: 0.845834  [   58/   88]
per-ex loss: 0.625477  [   59/   88]
per-ex loss: 0.825158  [   60/   88]
per-ex loss: 0.882008  [   61/   88]
per-ex loss: 0.737674  [   62/   88]
per-ex loss: 0.711262  [   63/   88]
per-ex loss: 0.762601  [   64/   88]
per-ex loss: 0.800181  [   65/   88]
per-ex loss: 0.706106  [   66/   88]
per-ex loss: 0.657338  [   67/   88]
per-ex loss: 0.866593  [   68/   88]
per-ex loss: 0.858375  [   69/   88]
per-ex loss: 0.837862  [   70/   88]
per-ex loss: 0.834689  [   71/   88]
per-ex loss: 0.591112  [   72/   88]
per-ex loss: 0.594352  [   73/   88]
per-ex loss: 0.879923  [   74/   88]
per-ex loss: 0.830248  [   75/   88]
per-ex loss: 0.861988  [   76/   88]
per-ex loss: 0.708507  [   77/   88]
per-ex loss: 0.657243  [   78/   88]
per-ex loss: 0.831475  [   79/   88]
per-ex loss: 0.791081  [   80/   88]
per-ex loss: 0.599539  [   81/   88]
per-ex loss: 0.608758  [   82/   88]
per-ex loss: 0.687353  [   83/   88]
per-ex loss: 0.881825  [   84/   88]
per-ex loss: 0.702362  [   85/   88]
per-ex loss: 0.602240  [   86/   88]
per-ex loss: 0.812056  [   87/   88]
per-ex loss: 0.715405  [   88/   88]
Train Error: Avg loss: 0.74919144
validation Error: 
 Avg loss: 0.76482736 
 F1: 0.408879 
 Precision: 0.528370 
 Recall: 0.333465
 IoU: 0.256975

test Error: 
 Avg loss: 0.71624455 
 F1: 0.502326 
 Precision: 0.573027 
 Recall: 0.447155
 IoU: 0.335404

We have finished training iteration 2
per-ex loss: 0.805877  [    1/   88]
per-ex loss: 0.704123  [    2/   88]
per-ex loss: 0.666705  [    3/   88]
per-ex loss: 0.728238  [    4/   88]
per-ex loss: 0.676374  [    5/   88]
per-ex loss: 0.834642  [    6/   88]
per-ex loss: 0.876313  [    7/   88]
per-ex loss: 0.875794  [    8/   88]
per-ex loss: 0.808806  [    9/   88]
per-ex loss: 0.601716  [   10/   88]
per-ex loss: 0.705128  [   11/   88]
per-ex loss: 0.541474  [   12/   88]
per-ex loss: 0.853136  [   13/   88]
per-ex loss: 0.625139  [   14/   88]
per-ex loss: 0.697125  [   15/   88]
per-ex loss: 0.667788  [   16/   88]
per-ex loss: 0.685873  [   17/   88]
per-ex loss: 0.755724  [   18/   88]
per-ex loss: 0.780558  [   19/   88]
per-ex loss: 0.739669  [   20/   88]
per-ex loss: 0.561836  [   21/   88]
per-ex loss: 0.805435  [   22/   88]
per-ex loss: 0.633751  [   23/   88]
per-ex loss: 0.832058  [   24/   88]
per-ex loss: 0.750430  [   25/   88]
per-ex loss: 0.598324  [   26/   88]
per-ex loss: 0.802111  [   27/   88]
per-ex loss: 0.751418  [   28/   88]
per-ex loss: 0.761111  [   29/   88]
per-ex loss: 0.618404  [   30/   88]
per-ex loss: 0.746350  [   31/   88]
per-ex loss: 0.800367  [   32/   88]
per-ex loss: 0.788191  [   33/   88]
per-ex loss: 0.654307  [   34/   88]
per-ex loss: 0.837429  [   35/   88]
per-ex loss: 0.820036  [   36/   88]
per-ex loss: 0.635948  [   37/   88]
per-ex loss: 0.692050  [   38/   88]
per-ex loss: 0.852504  [   39/   88]
per-ex loss: 0.653875  [   40/   88]
per-ex loss: 0.704821  [   41/   88]
per-ex loss: 0.795184  [   42/   88]
per-ex loss: 0.772202  [   43/   88]
per-ex loss: 0.706747  [   44/   88]
per-ex loss: 0.669051  [   45/   88]
per-ex loss: 0.665377  [   46/   88]
per-ex loss: 0.695232  [   47/   88]
per-ex loss: 0.642850  [   48/   88]
per-ex loss: 0.842697  [   49/   88]
per-ex loss: 0.804431  [   50/   88]
per-ex loss: 0.824157  [   51/   88]
per-ex loss: 0.833838  [   52/   88]
per-ex loss: 0.661264  [   53/   88]
per-ex loss: 0.622304  [   54/   88]
per-ex loss: 0.597500  [   55/   88]
per-ex loss: 0.686022  [   56/   88]
per-ex loss: 0.652735  [   57/   88]
per-ex loss: 0.836328  [   58/   88]
per-ex loss: 0.657435  [   59/   88]
per-ex loss: 0.773790  [   60/   88]
per-ex loss: 0.800323  [   61/   88]
per-ex loss: 0.664707  [   62/   88]
per-ex loss: 0.670040  [   63/   88]
per-ex loss: 0.591739  [   64/   88]
per-ex loss: 0.631829  [   65/   88]
per-ex loss: 0.805334  [   66/   88]
per-ex loss: 0.651322  [   67/   88]
per-ex loss: 0.867896  [   68/   88]
per-ex loss: 0.837284  [   69/   88]
per-ex loss: 0.578666  [   70/   88]
per-ex loss: 0.708562  [   71/   88]
per-ex loss: 0.841642  [   72/   88]
per-ex loss: 0.608248  [   73/   88]
per-ex loss: 0.781074  [   74/   88]
per-ex loss: 0.770400  [   75/   88]
per-ex loss: 0.749070  [   76/   88]
per-ex loss: 0.653169  [   77/   88]
per-ex loss: 0.813120  [   78/   88]
per-ex loss: 0.568383  [   79/   88]
per-ex loss: 0.828145  [   80/   88]
per-ex loss: 0.858317  [   81/   88]
per-ex loss: 0.784964  [   82/   88]
per-ex loss: 0.773902  [   83/   88]
per-ex loss: 0.801853  [   84/   88]
per-ex loss: 0.672519  [   85/   88]
per-ex loss: 0.676552  [   86/   88]
per-ex loss: 0.781656  [   87/   88]
per-ex loss: 0.639503  [   88/   88]
Train Error: Avg loss: 0.72818549
validation Error: 
 Avg loss: 0.74067910 
 F1: 0.444535 
 Precision: 0.518555 
 Recall: 0.389007
 IoU: 0.285789

test Error: 
 Avg loss: 0.69019615 
 F1: 0.530336 
 Precision: 0.592413 
 Recall: 0.480034
 IoU: 0.360855

We have finished training iteration 3
per-ex loss: 0.589335  [    1/   88]
per-ex loss: 0.829574  [    2/   88]
per-ex loss: 0.709042  [    3/   88]
per-ex loss: 0.785388  [    4/   88]
per-ex loss: 0.814132  [    5/   88]
per-ex loss: 0.799280  [    6/   88]
per-ex loss: 0.576584  [    7/   88]
per-ex loss: 0.641479  [    8/   88]
per-ex loss: 0.663169  [    9/   88]
per-ex loss: 0.854674  [   10/   88]
per-ex loss: 0.686378  [   11/   88]
per-ex loss: 0.599183  [   12/   88]
per-ex loss: 0.737939  [   13/   88]
per-ex loss: 0.786361  [   14/   88]
per-ex loss: 0.773114  [   15/   88]
per-ex loss: 0.598025  [   16/   88]
per-ex loss: 0.573088  [   17/   88]
per-ex loss: 0.828288  [   18/   88]
per-ex loss: 0.661191  [   19/   88]
per-ex loss: 0.792097  [   20/   88]
per-ex loss: 0.725453  [   21/   88]
per-ex loss: 0.623936  [   22/   88]
per-ex loss: 0.748176  [   23/   88]
per-ex loss: 0.661085  [   24/   88]
per-ex loss: 0.656372  [   25/   88]
per-ex loss: 0.851394  [   26/   88]
per-ex loss: 0.788047  [   27/   88]
per-ex loss: 0.826783  [   28/   88]
per-ex loss: 0.823837  [   29/   88]
per-ex loss: 0.762750  [   30/   88]
per-ex loss: 0.698162  [   31/   88]
per-ex loss: 0.810537  [   32/   88]
per-ex loss: 0.637362  [   33/   88]
per-ex loss: 0.826302  [   34/   88]
per-ex loss: 0.596868  [   35/   88]
per-ex loss: 0.773659  [   36/   88]
per-ex loss: 0.712307  [   37/   88]
per-ex loss: 0.554668  [   38/   88]
per-ex loss: 0.818445  [   39/   88]
per-ex loss: 0.854072  [   40/   88]
per-ex loss: 0.794414  [   41/   88]
per-ex loss: 0.782924  [   42/   88]
per-ex loss: 0.855994  [   43/   88]
per-ex loss: 0.843188  [   44/   88]
per-ex loss: 0.741977  [   45/   88]
per-ex loss: 0.619628  [   46/   88]
per-ex loss: 0.599214  [   47/   88]
per-ex loss: 0.666280  [   48/   88]
per-ex loss: 0.767721  [   49/   88]
per-ex loss: 0.617375  [   50/   88]
per-ex loss: 0.714464  [   51/   88]
per-ex loss: 0.682856  [   52/   88]
per-ex loss: 0.665554  [   53/   88]
per-ex loss: 0.683384  [   54/   88]
per-ex loss: 0.734180  [   55/   88]
per-ex loss: 0.815160  [   56/   88]
per-ex loss: 0.588960  [   57/   88]
per-ex loss: 0.746900  [   58/   88]
per-ex loss: 0.760786  [   59/   88]
per-ex loss: 0.630653  [   60/   88]
per-ex loss: 0.684574  [   61/   88]
per-ex loss: 0.879121  [   62/   88]
per-ex loss: 0.769878  [   63/   88]
per-ex loss: 0.776935  [   64/   88]
per-ex loss: 0.610213  [   65/   88]
per-ex loss: 0.788433  [   66/   88]
per-ex loss: 0.610022  [   67/   88]
per-ex loss: 0.599610  [   68/   88]
per-ex loss: 0.662541  [   69/   88]
per-ex loss: 0.656977  [   70/   88]
per-ex loss: 0.621711  [   71/   88]
per-ex loss: 0.805970  [   72/   88]
per-ex loss: 0.683381  [   73/   88]
per-ex loss: 0.830099  [   74/   88]
per-ex loss: 0.590636  [   75/   88]
per-ex loss: 0.812032  [   76/   88]
per-ex loss: 0.752786  [   77/   88]
per-ex loss: 0.777329  [   78/   88]
per-ex loss: 0.597210  [   79/   88]
per-ex loss: 0.740310  [   80/   88]
per-ex loss: 0.736029  [   81/   88]
per-ex loss: 0.595953  [   82/   88]
per-ex loss: 0.596956  [   83/   88]
per-ex loss: 0.856964  [   84/   88]
per-ex loss: 0.703479  [   85/   88]
per-ex loss: 0.573968  [   86/   88]
per-ex loss: 0.650042  [   87/   88]
per-ex loss: 0.814452  [   88/   88]
Train Error: Avg loss: 0.71747452
validation Error: 
 Avg loss: 0.74301159 
 F1: 0.436180 
 Precision: 0.649969 
 Recall: 0.328221
 IoU: 0.278919

test Error: 
 Avg loss: 0.69582291 
 F1: 0.518386 
 Precision: 0.672174 
 Recall: 0.421867
 IoU: 0.349879

We have finished training iteration 4
per-ex loss: 0.776036  [    1/   88]
per-ex loss: 0.586634  [    2/   88]
per-ex loss: 0.796355  [    3/   88]
per-ex loss: 0.754892  [    4/   88]
per-ex loss: 0.600075  [    5/   88]
per-ex loss: 0.814109  [    6/   88]
per-ex loss: 0.742315  [    7/   88]
per-ex loss: 0.738591  [    8/   88]
per-ex loss: 0.846150  [    9/   88]
per-ex loss: 0.664759  [   10/   88]
per-ex loss: 0.725076  [   11/   88]
per-ex loss: 0.654726  [   12/   88]
per-ex loss: 0.608788  [   13/   88]
per-ex loss: 0.769938  [   14/   88]
per-ex loss: 0.759787  [   15/   88]
per-ex loss: 0.839793  [   16/   88]
per-ex loss: 0.768786  [   17/   88]
per-ex loss: 0.759193  [   18/   88]
per-ex loss: 0.658417  [   19/   88]
per-ex loss: 0.671881  [   20/   88]
per-ex loss: 0.650848  [   21/   88]
per-ex loss: 0.609571  [   22/   88]
per-ex loss: 0.732346  [   23/   88]
per-ex loss: 0.645582  [   24/   88]
per-ex loss: 0.752099  [   25/   88]
per-ex loss: 0.730862  [   26/   88]
per-ex loss: 0.659735  [   27/   88]
per-ex loss: 0.857807  [   28/   88]
per-ex loss: 0.621585  [   29/   88]
per-ex loss: 0.687299  [   30/   88]
per-ex loss: 0.790421  [   31/   88]
per-ex loss: 0.658529  [   32/   88]
per-ex loss: 0.831703  [   33/   88]
per-ex loss: 0.743194  [   34/   88]
per-ex loss: 0.542388  [   35/   88]
per-ex loss: 0.557863  [   36/   88]
per-ex loss: 0.794744  [   37/   88]
per-ex loss: 0.834392  [   38/   88]
per-ex loss: 0.681077  [   39/   88]
per-ex loss: 0.590752  [   40/   88]
per-ex loss: 0.766203  [   41/   88]
per-ex loss: 0.776776  [   42/   88]
per-ex loss: 0.852832  [   43/   88]
per-ex loss: 0.827459  [   44/   88]
per-ex loss: 0.841191  [   45/   88]
per-ex loss: 0.673002  [   46/   88]
per-ex loss: 0.750318  [   47/   88]
per-ex loss: 0.697619  [   48/   88]
per-ex loss: 0.794017  [   49/   88]
per-ex loss: 0.816509  [   50/   88]
per-ex loss: 0.659613  [   51/   88]
per-ex loss: 0.605248  [   52/   88]
per-ex loss: 0.793957  [   53/   88]
per-ex loss: 0.842075  [   54/   88]
per-ex loss: 0.571142  [   55/   88]
per-ex loss: 0.787793  [   56/   88]
per-ex loss: 0.571706  [   57/   88]
per-ex loss: 0.587507  [   58/   88]
per-ex loss: 0.778961  [   59/   88]
per-ex loss: 0.810282  [   60/   88]
per-ex loss: 0.653903  [   61/   88]
per-ex loss: 0.587584  [   62/   88]
per-ex loss: 0.647074  [   63/   88]
per-ex loss: 0.817084  [   64/   88]
per-ex loss: 0.673600  [   65/   88]
per-ex loss: 0.857938  [   66/   88]
per-ex loss: 0.659068  [   67/   88]
per-ex loss: 0.819017  [   68/   88]
per-ex loss: 0.588178  [   69/   88]
per-ex loss: 0.700695  [   70/   88]
per-ex loss: 0.788482  [   71/   88]
per-ex loss: 0.625436  [   72/   88]
per-ex loss: 0.630530  [   73/   88]
per-ex loss: 0.670538  [   74/   88]
per-ex loss: 0.791231  [   75/   88]
per-ex loss: 0.562791  [   76/   88]
per-ex loss: 0.567778  [   77/   88]
per-ex loss: 0.571662  [   78/   88]
per-ex loss: 0.662317  [   79/   88]
per-ex loss: 0.849733  [   80/   88]
per-ex loss: 0.693135  [   81/   88]
per-ex loss: 0.613271  [   82/   88]
per-ex loss: 0.635538  [   83/   88]
per-ex loss: 0.835330  [   84/   88]
per-ex loss: 0.632559  [   85/   88]
per-ex loss: 0.634799  [   86/   88]
per-ex loss: 0.837443  [   87/   88]
per-ex loss: 0.665391  [   88/   88]
Train Error: Avg loss: 0.71119790
validation Error: 
 Avg loss: 0.74116147 
 F1: 0.430455 
 Precision: 0.544092 
 Recall: 0.356084
 IoU: 0.274254

test Error: 
 Avg loss: 0.68700062 
 F1: 0.528591 
 Precision: 0.604548 
 Recall: 0.469591
 IoU: 0.359242

We have finished training iteration 5
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_1_.pth
per-ex loss: 0.832131  [    1/   88]
per-ex loss: 0.762428  [    2/   88]
per-ex loss: 0.560664  [    3/   88]
per-ex loss: 0.796745  [    4/   88]
per-ex loss: 0.620260  [    5/   88]
per-ex loss: 0.707230  [    6/   88]
per-ex loss: 0.792260  [    7/   88]
per-ex loss: 0.675315  [    8/   88]
per-ex loss: 0.586871  [    9/   88]
per-ex loss: 0.618620  [   10/   88]
per-ex loss: 0.532010  [   11/   88]
per-ex loss: 0.661179  [   12/   88]
per-ex loss: 0.759872  [   13/   88]
per-ex loss: 0.761018  [   14/   88]
per-ex loss: 0.645516  [   15/   88]
per-ex loss: 0.578337  [   16/   88]
per-ex loss: 0.650606  [   17/   88]
per-ex loss: 0.829008  [   18/   88]
per-ex loss: 0.594663  [   19/   88]
per-ex loss: 0.571221  [   20/   88]
per-ex loss: 0.844754  [   21/   88]
per-ex loss: 0.794837  [   22/   88]
per-ex loss: 0.749610  [   23/   88]
per-ex loss: 0.864135  [   24/   88]
per-ex loss: 0.672485  [   25/   88]
per-ex loss: 0.642613  [   26/   88]
per-ex loss: 0.787557  [   27/   88]
per-ex loss: 0.801722  [   28/   88]
per-ex loss: 0.752118  [   29/   88]
per-ex loss: 0.582310  [   30/   88]
per-ex loss: 0.750230  [   31/   88]
per-ex loss: 0.623017  [   32/   88]
per-ex loss: 0.640150  [   33/   88]
per-ex loss: 0.597621  [   34/   88]
per-ex loss: 0.599969  [   35/   88]
per-ex loss: 0.768434  [   36/   88]
per-ex loss: 0.685654  [   37/   88]
per-ex loss: 0.821025  [   38/   88]
per-ex loss: 0.679659  [   39/   88]
per-ex loss: 0.655446  [   40/   88]
per-ex loss: 0.584357  [   41/   88]
per-ex loss: 0.665089  [   42/   88]
per-ex loss: 0.661495  [   43/   88]
per-ex loss: 0.699601  [   44/   88]
per-ex loss: 0.643251  [   45/   88]
per-ex loss: 0.802890  [   46/   88]
per-ex loss: 0.875400  [   47/   88]
per-ex loss: 0.572811  [   48/   88]
per-ex loss: 0.781744  [   49/   88]
per-ex loss: 0.574622  [   50/   88]
per-ex loss: 0.724929  [   51/   88]
per-ex loss: 0.798944  [   52/   88]
per-ex loss: 0.826869  [   53/   88]
per-ex loss: 0.674522  [   54/   88]
per-ex loss: 0.774001  [   55/   88]
per-ex loss: 0.782278  [   56/   88]
per-ex loss: 0.622005  [   57/   88]
per-ex loss: 0.861754  [   58/   88]
per-ex loss: 0.761271  [   59/   88]
per-ex loss: 0.604768  [   60/   88]
per-ex loss: 0.684030  [   61/   88]
per-ex loss: 0.833307  [   62/   88]
per-ex loss: 0.862174  [   63/   88]
per-ex loss: 0.626706  [   64/   88]
per-ex loss: 0.718369  [   65/   88]
per-ex loss: 0.692477  [   66/   88]
per-ex loss: 0.822502  [   67/   88]
per-ex loss: 0.766667  [   68/   88]
per-ex loss: 0.557632  [   69/   88]
per-ex loss: 0.844780  [   70/   88]
per-ex loss: 0.637665  [   71/   88]
per-ex loss: 0.728630  [   72/   88]
per-ex loss: 0.695580  [   73/   88]
per-ex loss: 0.802946  [   74/   88]
per-ex loss: 0.830196  [   75/   88]
per-ex loss: 0.840769  [   76/   88]
per-ex loss: 0.604317  [   77/   88]
per-ex loss: 0.760133  [   78/   88]
per-ex loss: 0.597459  [   79/   88]
per-ex loss: 0.605276  [   80/   88]
per-ex loss: 0.654288  [   81/   88]
per-ex loss: 0.748670  [   82/   88]
per-ex loss: 0.764576  [   83/   88]
per-ex loss: 0.802862  [   84/   88]
per-ex loss: 0.807779  [   85/   88]
per-ex loss: 0.630857  [   86/   88]
per-ex loss: 0.766299  [   87/   88]
per-ex loss: 0.612562  [   88/   88]
Train Error: Avg loss: 0.70956139
validation Error: 
 Avg loss: 0.73055727 
 F1: 0.450588 
 Precision: 0.511380 
 Recall: 0.402714
 IoU: 0.290812

test Error: 
 Avg loss: 0.67498125 
 F1: 0.546118 
 Precision: 0.586889 
 Recall: 0.510644
 IoU: 0.375628

We have finished training iteration 6
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_2_.pth
per-ex loss: 0.633039  [    1/   88]
per-ex loss: 0.606758  [    2/   88]
per-ex loss: 0.829391  [    3/   88]
per-ex loss: 0.654572  [    4/   88]
per-ex loss: 0.750353  [    5/   88]
per-ex loss: 0.674184  [    6/   88]
per-ex loss: 0.785535  [    7/   88]
per-ex loss: 0.618305  [    8/   88]
per-ex loss: 0.592956  [    9/   88]
per-ex loss: 0.847964  [   10/   88]
per-ex loss: 0.628953  [   11/   88]
per-ex loss: 0.538522  [   12/   88]
per-ex loss: 0.796365  [   13/   88]
per-ex loss: 0.722469  [   14/   88]
per-ex loss: 0.722603  [   15/   88]
per-ex loss: 0.569195  [   16/   88]
per-ex loss: 0.788627  [   17/   88]
per-ex loss: 0.602467  [   18/   88]
per-ex loss: 0.764649  [   19/   88]
per-ex loss: 0.810529  [   20/   88]
per-ex loss: 0.654550  [   21/   88]
per-ex loss: 0.804444  [   22/   88]
per-ex loss: 0.678315  [   23/   88]
per-ex loss: 0.842623  [   24/   88]
per-ex loss: 0.685226  [   25/   88]
per-ex loss: 0.620292  [   26/   88]
per-ex loss: 0.584140  [   27/   88]
per-ex loss: 0.822341  [   28/   88]
per-ex loss: 0.582421  [   29/   88]
per-ex loss: 0.693916  [   30/   88]
per-ex loss: 0.799139  [   31/   88]
per-ex loss: 0.633745  [   32/   88]
per-ex loss: 0.742339  [   33/   88]
per-ex loss: 0.746571  [   34/   88]
per-ex loss: 0.773735  [   35/   88]
per-ex loss: 0.682079  [   36/   88]
per-ex loss: 0.598987  [   37/   88]
per-ex loss: 0.677436  [   38/   88]
per-ex loss: 0.692699  [   39/   88]
per-ex loss: 0.815441  [   40/   88]
per-ex loss: 0.779434  [   41/   88]
per-ex loss: 0.717289  [   42/   88]
per-ex loss: 0.771533  [   43/   88]
per-ex loss: 0.734325  [   44/   88]
per-ex loss: 0.586532  [   45/   88]
per-ex loss: 0.786449  [   46/   88]
per-ex loss: 0.619838  [   47/   88]
per-ex loss: 0.626774  [   48/   88]
per-ex loss: 0.642481  [   49/   88]
per-ex loss: 0.716110  [   50/   88]
per-ex loss: 0.672901  [   51/   88]
per-ex loss: 0.648994  [   52/   88]
per-ex loss: 0.792711  [   53/   88]
per-ex loss: 0.583255  [   54/   88]
per-ex loss: 0.727359  [   55/   88]
per-ex loss: 0.853834  [   56/   88]
per-ex loss: 0.595462  [   57/   88]
per-ex loss: 0.765857  [   58/   88]
per-ex loss: 0.827469  [   59/   88]
per-ex loss: 0.595684  [   60/   88]
per-ex loss: 0.664358  [   61/   88]
per-ex loss: 0.767521  [   62/   88]
per-ex loss: 0.821563  [   63/   88]
per-ex loss: 0.625853  [   64/   88]
per-ex loss: 0.668724  [   65/   88]
per-ex loss: 0.631085  [   66/   88]
per-ex loss: 0.610875  [   67/   88]
per-ex loss: 0.837347  [   68/   88]
per-ex loss: 0.528490  [   69/   88]
per-ex loss: 0.565507  [   70/   88]
per-ex loss: 0.693600  [   71/   88]
per-ex loss: 0.595619  [   72/   88]
per-ex loss: 0.813011  [   73/   88]
per-ex loss: 0.614621  [   74/   88]
per-ex loss: 0.765964  [   75/   88]
per-ex loss: 0.578579  [   76/   88]
per-ex loss: 0.846433  [   77/   88]
per-ex loss: 0.754184  [   78/   88]
per-ex loss: 0.834368  [   79/   88]
per-ex loss: 0.795922  [   80/   88]
per-ex loss: 0.798419  [   81/   88]
per-ex loss: 0.749043  [   82/   88]
per-ex loss: 0.567980  [   83/   88]
per-ex loss: 0.789532  [   84/   88]
per-ex loss: 0.589088  [   85/   88]
per-ex loss: 0.864638  [   86/   88]
per-ex loss: 0.641700  [   87/   88]
per-ex loss: 0.779750  [   88/   88]
Train Error: Avg loss: 0.70347661
validation Error: 
 Avg loss: 0.74584994 
 F1: 0.424107 
 Precision: 0.576222 
 Recall: 0.335532
 IoU: 0.269122

test Error: 
 Avg loss: 0.68623126 
 F1: 0.525611 
 Precision: 0.650884 
 Recall: 0.440777
 IoU: 0.356494

We have finished training iteration 7
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_4_.pth
per-ex loss: 0.587820  [    1/   88]
per-ex loss: 0.841129  [    2/   88]
per-ex loss: 0.627610  [    3/   88]
per-ex loss: 0.812552  [    4/   88]
per-ex loss: 0.717675  [    5/   88]
per-ex loss: 0.607745  [    6/   88]
per-ex loss: 0.594667  [    7/   88]
per-ex loss: 0.592770  [    8/   88]
per-ex loss: 0.744669  [    9/   88]
per-ex loss: 0.589630  [   10/   88]
per-ex loss: 0.677132  [   11/   88]
per-ex loss: 0.792859  [   12/   88]
per-ex loss: 0.586204  [   13/   88]
per-ex loss: 0.630522  [   14/   88]
per-ex loss: 0.662127  [   15/   88]
per-ex loss: 0.544701  [   16/   88]
per-ex loss: 0.755738  [   17/   88]
per-ex loss: 0.845614  [   18/   88]
per-ex loss: 0.663056  [   19/   88]
per-ex loss: 0.806096  [   20/   88]
per-ex loss: 0.836834  [   21/   88]
per-ex loss: 0.624862  [   22/   88]
per-ex loss: 0.811438  [   23/   88]
per-ex loss: 0.817584  [   24/   88]
per-ex loss: 0.608550  [   25/   88]
per-ex loss: 0.587068  [   26/   88]
per-ex loss: 0.719811  [   27/   88]
per-ex loss: 0.620116  [   28/   88]
per-ex loss: 0.827049  [   29/   88]
per-ex loss: 0.773428  [   30/   88]
per-ex loss: 0.751487  [   31/   88]
per-ex loss: 0.567618  [   32/   88]
per-ex loss: 0.826435  [   33/   88]
per-ex loss: 0.748965  [   34/   88]
per-ex loss: 0.876167  [   35/   88]
per-ex loss: 0.742991  [   36/   88]
per-ex loss: 0.730056  [   37/   88]
per-ex loss: 0.770872  [   38/   88]
per-ex loss: 0.816079  [   39/   88]
per-ex loss: 0.761224  [   40/   88]
per-ex loss: 0.795432  [   41/   88]
per-ex loss: 0.624044  [   42/   88]
per-ex loss: 0.811270  [   43/   88]
per-ex loss: 0.842693  [   44/   88]
per-ex loss: 0.825926  [   45/   88]
per-ex loss: 0.675583  [   46/   88]
per-ex loss: 0.674251  [   47/   88]
per-ex loss: 0.757124  [   48/   88]
per-ex loss: 0.847310  [   49/   88]
per-ex loss: 0.527672  [   50/   88]
per-ex loss: 0.686504  [   51/   88]
per-ex loss: 0.699185  [   52/   88]
per-ex loss: 0.762167  [   53/   88]
per-ex loss: 0.653275  [   54/   88]
per-ex loss: 0.703619  [   55/   88]
per-ex loss: 0.586709  [   56/   88]
per-ex loss: 0.587754  [   57/   88]
per-ex loss: 0.637944  [   58/   88]
per-ex loss: 0.638286  [   59/   88]
per-ex loss: 0.768265  [   60/   88]
per-ex loss: 0.764998  [   61/   88]
per-ex loss: 0.739493  [   62/   88]
per-ex loss: 0.538506  [   63/   88]
per-ex loss: 0.623638  [   64/   88]
per-ex loss: 0.666856  [   65/   88]
per-ex loss: 0.788822  [   66/   88]
per-ex loss: 0.618324  [   67/   88]
per-ex loss: 0.599280  [   68/   88]
per-ex loss: 0.660370  [   69/   88]
per-ex loss: 0.745565  [   70/   88]
per-ex loss: 0.645883  [   71/   88]
per-ex loss: 0.665328  [   72/   88]
per-ex loss: 0.634645  [   73/   88]
per-ex loss: 0.674659  [   74/   88]
per-ex loss: 0.766108  [   75/   88]
per-ex loss: 0.770512  [   76/   88]
per-ex loss: 0.655154  [   77/   88]
per-ex loss: 0.567698  [   78/   88]
per-ex loss: 0.570422  [   79/   88]
per-ex loss: 0.571000  [   80/   88]
per-ex loss: 0.856989  [   81/   88]
per-ex loss: 0.565904  [   82/   88]
per-ex loss: 0.792236  [   83/   88]
per-ex loss: 0.814925  [   84/   88]
per-ex loss: 0.787655  [   85/   88]
per-ex loss: 0.740893  [   86/   88]
per-ex loss: 0.781996  [   87/   88]
per-ex loss: 0.643578  [   88/   88]
Train Error: Avg loss: 0.70290234
validation Error: 
 Avg loss: 0.72593202 
 F1: 0.457474 
 Precision: 0.492790 
 Recall: 0.426881
 IoU: 0.296574

test Error: 
 Avg loss: 0.66702768 
 F1: 0.552152 
 Precision: 0.580929 
 Recall: 0.526092
 IoU: 0.381361

We have finished training iteration 8
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_5_.pth
per-ex loss: 0.598993  [    1/   88]
per-ex loss: 0.595703  [    2/   88]
per-ex loss: 0.749786  [    3/   88]
per-ex loss: 0.610598  [    4/   88]
per-ex loss: 0.826865  [    5/   88]
per-ex loss: 0.840703  [    6/   88]
per-ex loss: 0.621652  [    7/   88]
per-ex loss: 0.673327  [    8/   88]
per-ex loss: 0.640214  [    9/   88]
per-ex loss: 0.756652  [   10/   88]
per-ex loss: 0.779404  [   11/   88]
per-ex loss: 0.795152  [   12/   88]
per-ex loss: 0.637786  [   13/   88]
per-ex loss: 0.675800  [   14/   88]
per-ex loss: 0.623706  [   15/   88]
per-ex loss: 0.773093  [   16/   88]
per-ex loss: 0.677986  [   17/   88]
per-ex loss: 0.786198  [   18/   88]
per-ex loss: 0.586089  [   19/   88]
per-ex loss: 0.682717  [   20/   88]
per-ex loss: 0.550613  [   21/   88]
per-ex loss: 0.590356  [   22/   88]
per-ex loss: 0.751059  [   23/   88]
per-ex loss: 0.530405  [   24/   88]
per-ex loss: 0.549524  [   25/   88]
per-ex loss: 0.655188  [   26/   88]
per-ex loss: 0.577572  [   27/   88]
per-ex loss: 0.748379  [   28/   88]
per-ex loss: 0.825762  [   29/   88]
per-ex loss: 0.840030  [   30/   88]
per-ex loss: 0.740063  [   31/   88]
per-ex loss: 0.654510  [   32/   88]
per-ex loss: 0.753828  [   33/   88]
per-ex loss: 0.722637  [   34/   88]
per-ex loss: 0.684780  [   35/   88]
per-ex loss: 0.646771  [   36/   88]
per-ex loss: 0.681299  [   37/   88]
per-ex loss: 0.554225  [   38/   88]
per-ex loss: 0.610408  [   39/   88]
per-ex loss: 0.789989  [   40/   88]
per-ex loss: 0.740971  [   41/   88]
per-ex loss: 0.831587  [   42/   88]
per-ex loss: 0.845843  [   43/   88]
per-ex loss: 0.827988  [   44/   88]
per-ex loss: 0.559530  [   45/   88]
per-ex loss: 0.579435  [   46/   88]
per-ex loss: 0.596892  [   47/   88]
per-ex loss: 0.650972  [   48/   88]
per-ex loss: 0.702319  [   49/   88]
per-ex loss: 0.647139  [   50/   88]
per-ex loss: 0.655656  [   51/   88]
per-ex loss: 0.826994  [   52/   88]
per-ex loss: 0.764846  [   53/   88]
per-ex loss: 0.605935  [   54/   88]
per-ex loss: 0.594656  [   55/   88]
per-ex loss: 0.588493  [   56/   88]
per-ex loss: 0.525231  [   57/   88]
per-ex loss: 0.857891  [   58/   88]
per-ex loss: 0.639808  [   59/   88]
per-ex loss: 0.831289  [   60/   88]
per-ex loss: 0.780968  [   61/   88]
per-ex loss: 0.662504  [   62/   88]
per-ex loss: 0.759356  [   63/   88]
per-ex loss: 0.848361  [   64/   88]
per-ex loss: 0.536469  [   65/   88]
per-ex loss: 0.659358  [   66/   88]
per-ex loss: 0.683761  [   67/   88]
per-ex loss: 0.798841  [   68/   88]
per-ex loss: 0.609304  [   69/   88]
per-ex loss: 0.601892  [   70/   88]
per-ex loss: 0.787937  [   71/   88]
per-ex loss: 0.677066  [   72/   88]
per-ex loss: 0.844765  [   73/   88]
per-ex loss: 0.757222  [   74/   88]
per-ex loss: 0.822354  [   75/   88]
per-ex loss: 0.671339  [   76/   88]
per-ex loss: 0.578242  [   77/   88]
per-ex loss: 0.741849  [   78/   88]
per-ex loss: 0.720505  [   79/   88]
per-ex loss: 0.709068  [   80/   88]
per-ex loss: 0.781914  [   81/   88]
per-ex loss: 0.630165  [   82/   88]
per-ex loss: 0.808869  [   83/   88]
per-ex loss: 0.594599  [   84/   88]
per-ex loss: 0.770541  [   85/   88]
per-ex loss: 0.597211  [   86/   88]
per-ex loss: 0.761362  [   87/   88]
per-ex loss: 0.801315  [   88/   88]
Train Error: Avg loss: 0.69614134
validation Error: 
 Avg loss: 0.74991065 
 F1: 0.421394 
 Precision: 0.454966 
 Recall: 0.392436
 IoU: 0.266940

test Error: 
 Avg loss: 0.68203447 
 F1: 0.533633 
 Precision: 0.584656 
 Recall: 0.490800
 IoU: 0.363915

We have finished training iteration 9
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_7_.pth
per-ex loss: 0.673627  [    1/   88]
per-ex loss: 0.641712  [    2/   88]
per-ex loss: 0.682068  [    3/   88]
per-ex loss: 0.684263  [    4/   88]
per-ex loss: 0.590297  [    5/   88]
per-ex loss: 0.748940  [    6/   88]
per-ex loss: 0.630805  [    7/   88]
per-ex loss: 0.635328  [    8/   88]
per-ex loss: 0.825464  [    9/   88]
per-ex loss: 0.567014  [   10/   88]
per-ex loss: 0.601461  [   11/   88]
per-ex loss: 0.840871  [   12/   88]
per-ex loss: 0.788983  [   13/   88]
per-ex loss: 0.738371  [   14/   88]
per-ex loss: 0.846333  [   15/   88]
per-ex loss: 0.666016  [   16/   88]
per-ex loss: 0.823746  [   17/   88]
per-ex loss: 0.756792  [   18/   88]
per-ex loss: 0.747085  [   19/   88]
per-ex loss: 0.776815  [   20/   88]
per-ex loss: 0.622186  [   21/   88]
per-ex loss: 0.862818  [   22/   88]
per-ex loss: 0.781775  [   23/   88]
per-ex loss: 0.614934  [   24/   88]
per-ex loss: 0.560079  [   25/   88]
per-ex loss: 0.736233  [   26/   88]
per-ex loss: 0.587479  [   27/   88]
per-ex loss: 0.790745  [   28/   88]
per-ex loss: 0.845952  [   29/   88]
per-ex loss: 0.540020  [   30/   88]
per-ex loss: 0.807828  [   31/   88]
per-ex loss: 0.594804  [   32/   88]
per-ex loss: 0.653801  [   33/   88]
per-ex loss: 0.639166  [   34/   88]
per-ex loss: 0.814003  [   35/   88]
per-ex loss: 0.593796  [   36/   88]
per-ex loss: 0.579199  [   37/   88]
per-ex loss: 0.563052  [   38/   88]
per-ex loss: 0.676429  [   39/   88]
per-ex loss: 0.701743  [   40/   88]
per-ex loss: 0.775718  [   41/   88]
per-ex loss: 0.650954  [   42/   88]
per-ex loss: 0.843065  [   43/   88]
per-ex loss: 0.769691  [   44/   88]
per-ex loss: 0.751541  [   45/   88]
per-ex loss: 0.604625  [   46/   88]
per-ex loss: 0.768259  [   47/   88]
per-ex loss: 0.783761  [   48/   88]
per-ex loss: 0.586319  [   49/   88]
per-ex loss: 0.652386  [   50/   88]
per-ex loss: 0.841688  [   51/   88]
per-ex loss: 0.639611  [   52/   88]
per-ex loss: 0.746094  [   53/   88]
per-ex loss: 0.791216  [   54/   88]
per-ex loss: 0.801393  [   55/   88]
per-ex loss: 0.555659  [   56/   88]
per-ex loss: 0.622761  [   57/   88]
per-ex loss: 0.603655  [   58/   88]
per-ex loss: 0.797172  [   59/   88]
per-ex loss: 0.744174  [   60/   88]
per-ex loss: 0.661185  [   61/   88]
per-ex loss: 0.573411  [   62/   88]
per-ex loss: 0.739243  [   63/   88]
per-ex loss: 0.786750  [   64/   88]
per-ex loss: 0.789097  [   65/   88]
per-ex loss: 0.603453  [   66/   88]
per-ex loss: 0.736982  [   67/   88]
per-ex loss: 0.577184  [   68/   88]
per-ex loss: 0.624839  [   69/   88]
per-ex loss: 0.621152  [   70/   88]
per-ex loss: 0.605334  [   71/   88]
per-ex loss: 0.519241  [   72/   88]
per-ex loss: 0.627068  [   73/   88]
per-ex loss: 0.763911  [   74/   88]
per-ex loss: 0.771514  [   75/   88]
per-ex loss: 0.671670  [   76/   88]
per-ex loss: 0.664871  [   77/   88]
per-ex loss: 0.854920  [   78/   88]
per-ex loss: 0.706616  [   79/   88]
per-ex loss: 0.767567  [   80/   88]
per-ex loss: 0.841304  [   81/   88]
per-ex loss: 0.614580  [   82/   88]
per-ex loss: 0.567794  [   83/   88]
per-ex loss: 0.663225  [   84/   88]
per-ex loss: 0.829291  [   85/   88]
per-ex loss: 0.677094  [   86/   88]
per-ex loss: 0.789285  [   87/   88]
per-ex loss: 0.606728  [   88/   88]
Train Error: Avg loss: 0.69792138
validation Error: 
 Avg loss: 0.71553864 
 F1: 0.471288 
 Precision: 0.549762 
 Recall: 0.412419
 IoU: 0.308291

test Error: 
 Avg loss: 0.66709971 
 F1: 0.550059 
 Precision: 0.586652 
 Recall: 0.517763
 IoU: 0.379367

We have finished training iteration 10
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_3_.pth
per-ex loss: 0.812269  [    1/   88]
per-ex loss: 0.528191  [    2/   88]
per-ex loss: 0.553795  [    3/   88]
per-ex loss: 0.808203  [    4/   88]
per-ex loss: 0.570578  [    5/   88]
per-ex loss: 0.651535  [    6/   88]
per-ex loss: 0.605844  [    7/   88]
per-ex loss: 0.664414  [    8/   88]
per-ex loss: 0.841238  [    9/   88]
per-ex loss: 0.674415  [   10/   88]
per-ex loss: 0.585349  [   11/   88]
per-ex loss: 0.577204  [   12/   88]
per-ex loss: 0.829040  [   13/   88]
per-ex loss: 0.787867  [   14/   88]
per-ex loss: 0.658543  [   15/   88]
per-ex loss: 0.626392  [   16/   88]
per-ex loss: 0.600856  [   17/   88]
per-ex loss: 0.632219  [   18/   88]
per-ex loss: 0.798425  [   19/   88]
per-ex loss: 0.606084  [   20/   88]
per-ex loss: 0.766728  [   21/   88]
per-ex loss: 0.604531  [   22/   88]
per-ex loss: 0.835348  [   23/   88]
per-ex loss: 0.798372  [   24/   88]
per-ex loss: 0.625186  [   25/   88]
per-ex loss: 0.772671  [   26/   88]
per-ex loss: 0.812056  [   27/   88]
per-ex loss: 0.629420  [   28/   88]
per-ex loss: 0.776369  [   29/   88]
per-ex loss: 0.804164  [   30/   88]
per-ex loss: 0.808286  [   31/   88]
per-ex loss: 0.637801  [   32/   88]
per-ex loss: 0.643135  [   33/   88]
per-ex loss: 0.752910  [   34/   88]
per-ex loss: 0.568488  [   35/   88]
per-ex loss: 0.576729  [   36/   88]
per-ex loss: 0.752535  [   37/   88]
per-ex loss: 0.738186  [   38/   88]
per-ex loss: 0.737742  [   39/   88]
per-ex loss: 0.768339  [   40/   88]
per-ex loss: 0.678593  [   41/   88]
per-ex loss: 0.734396  [   42/   88]
per-ex loss: 0.730635  [   43/   88]
per-ex loss: 0.840336  [   44/   88]
per-ex loss: 0.618832  [   45/   88]
per-ex loss: 0.663417  [   46/   88]
per-ex loss: 0.684684  [   47/   88]
per-ex loss: 0.611551  [   48/   88]
per-ex loss: 0.569696  [   49/   88]
per-ex loss: 0.771102  [   50/   88]
per-ex loss: 0.755666  [   51/   88]
per-ex loss: 0.738818  [   52/   88]
per-ex loss: 0.583707  [   53/   88]
per-ex loss: 0.570436  [   54/   88]
per-ex loss: 0.611183  [   55/   88]
per-ex loss: 0.627127  [   56/   88]
per-ex loss: 0.585027  [   57/   88]
per-ex loss: 0.707389  [   58/   88]
per-ex loss: 0.545899  [   59/   88]
per-ex loss: 0.777060  [   60/   88]
per-ex loss: 0.698656  [   61/   88]
per-ex loss: 0.636354  [   62/   88]
per-ex loss: 0.761647  [   63/   88]
per-ex loss: 0.549642  [   64/   88]
per-ex loss: 0.582678  [   65/   88]
per-ex loss: 0.782320  [   66/   88]
per-ex loss: 0.682037  [   67/   88]
per-ex loss: 0.832117  [   68/   88]
per-ex loss: 0.659715  [   69/   88]
per-ex loss: 0.811830  [   70/   88]
per-ex loss: 0.591320  [   71/   88]
per-ex loss: 0.646093  [   72/   88]
per-ex loss: 0.568170  [   73/   88]
per-ex loss: 0.819219  [   74/   88]
per-ex loss: 0.827804  [   75/   88]
per-ex loss: 0.611287  [   76/   88]
per-ex loss: 0.711180  [   77/   88]
per-ex loss: 0.568674  [   78/   88]
per-ex loss: 0.679693  [   79/   88]
per-ex loss: 0.748810  [   80/   88]
per-ex loss: 0.849086  [   81/   88]
per-ex loss: 0.801096  [   82/   88]
per-ex loss: 0.709792  [   83/   88]
per-ex loss: 0.724837  [   84/   88]
per-ex loss: 0.803568  [   85/   88]
per-ex loss: 0.705914  [   86/   88]
per-ex loss: 0.638182  [   87/   88]
per-ex loss: 0.707660  [   88/   88]
Train Error: Avg loss: 0.69245903
validation Error: 
 Avg loss: 0.72645858 
 F1: 0.456991 
 Precision: 0.435177 
 Recall: 0.481107
 IoU: 0.296169

test Error: 
 Avg loss: 0.67038260 
 F1: 0.548151 
 Precision: 0.515035 
 Recall: 0.585818
 IoU: 0.377554

We have finished training iteration 11
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_9_.pth
per-ex loss: 0.806186  [    1/   88]
per-ex loss: 0.679180  [    2/   88]
per-ex loss: 0.809750  [    3/   88]
per-ex loss: 0.612676  [    4/   88]
per-ex loss: 0.631657  [    5/   88]
per-ex loss: 0.669096  [    6/   88]
per-ex loss: 0.627699  [    7/   88]
per-ex loss: 0.627056  [    8/   88]
per-ex loss: 0.675484  [    9/   88]
per-ex loss: 0.778045  [   10/   88]
per-ex loss: 0.593198  [   11/   88]
per-ex loss: 0.776240  [   12/   88]
per-ex loss: 0.605451  [   13/   88]
per-ex loss: 0.703111  [   14/   88]
per-ex loss: 0.818723  [   15/   88]
per-ex loss: 0.813424  [   16/   88]
per-ex loss: 0.744438  [   17/   88]
per-ex loss: 0.797664  [   18/   88]
per-ex loss: 0.824948  [   19/   88]
per-ex loss: 0.763067  [   20/   88]
per-ex loss: 0.773147  [   21/   88]
per-ex loss: 0.687804  [   22/   88]
per-ex loss: 0.721991  [   23/   88]
per-ex loss: 0.567807  [   24/   88]
per-ex loss: 0.741187  [   25/   88]
per-ex loss: 0.580693  [   26/   88]
per-ex loss: 0.760902  [   27/   88]
per-ex loss: 0.781861  [   28/   88]
per-ex loss: 0.691042  [   29/   88]
per-ex loss: 0.614361  [   30/   88]
per-ex loss: 0.802631  [   31/   88]
per-ex loss: 0.580459  [   32/   88]
per-ex loss: 0.677706  [   33/   88]
per-ex loss: 0.820516  [   34/   88]
per-ex loss: 0.674775  [   35/   88]
per-ex loss: 0.734533  [   36/   88]
per-ex loss: 0.608549  [   37/   88]
per-ex loss: 0.833264  [   38/   88]
per-ex loss: 0.773661  [   39/   88]
per-ex loss: 0.747504  [   40/   88]
per-ex loss: 0.647963  [   41/   88]
per-ex loss: 0.623195  [   42/   88]
per-ex loss: 0.823029  [   43/   88]
per-ex loss: 0.656052  [   44/   88]
per-ex loss: 0.720231  [   45/   88]
per-ex loss: 0.538654  [   46/   88]
per-ex loss: 0.557673  [   47/   88]
per-ex loss: 0.742841  [   48/   88]
per-ex loss: 0.586666  [   49/   88]
per-ex loss: 0.592070  [   50/   88]
per-ex loss: 0.752086  [   51/   88]
per-ex loss: 0.628797  [   52/   88]
per-ex loss: 0.764092  [   53/   88]
per-ex loss: 0.678469  [   54/   88]
per-ex loss: 0.547021  [   55/   88]
per-ex loss: 0.583515  [   56/   88]
per-ex loss: 0.708370  [   57/   88]
per-ex loss: 0.770394  [   58/   88]
per-ex loss: 0.791415  [   59/   88]
per-ex loss: 0.509091  [   60/   88]
per-ex loss: 0.743668  [   61/   88]
per-ex loss: 0.638783  [   62/   88]
per-ex loss: 0.546254  [   63/   88]
per-ex loss: 0.570129  [   64/   88]
per-ex loss: 0.611452  [   65/   88]
per-ex loss: 0.661695  [   66/   88]
per-ex loss: 0.810107  [   67/   88]
per-ex loss: 0.815377  [   68/   88]
per-ex loss: 0.849967  [   69/   88]
per-ex loss: 0.839781  [   70/   88]
per-ex loss: 0.734009  [   71/   88]
per-ex loss: 0.609264  [   72/   88]
per-ex loss: 0.565911  [   73/   88]
per-ex loss: 0.657862  [   74/   88]
per-ex loss: 0.620166  [   75/   88]
per-ex loss: 0.577861  [   76/   88]
per-ex loss: 0.787931  [   77/   88]
per-ex loss: 0.545699  [   78/   88]
per-ex loss: 0.763538  [   79/   88]
per-ex loss: 0.852281  [   80/   88]
per-ex loss: 0.654273  [   81/   88]
per-ex loss: 0.802451  [   82/   88]
per-ex loss: 0.649518  [   83/   88]
per-ex loss: 0.609544  [   84/   88]
per-ex loss: 0.642878  [   85/   88]
per-ex loss: 0.625682  [   86/   88]
per-ex loss: 0.682769  [   87/   88]
per-ex loss: 0.722510  [   88/   88]
Train Error: Avg loss: 0.69255080
validation Error: 
 Avg loss: 0.74435974 
 F1: 0.424793 
 Precision: 0.414204 
 Recall: 0.435937
 IoU: 0.269674

test Error: 
 Avg loss: 0.66996609 
 F1: 0.548073 
 Precision: 0.547848 
 Recall: 0.548299
 IoU: 0.377480

We have finished training iteration 12
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_6_.pth
per-ex loss: 0.752517  [    1/   88]
per-ex loss: 0.768561  [    2/   88]
per-ex loss: 0.713612  [    3/   88]
per-ex loss: 0.569583  [    4/   88]
per-ex loss: 0.614017  [    5/   88]
per-ex loss: 0.828298  [    6/   88]
per-ex loss: 0.797347  [    7/   88]
per-ex loss: 0.851013  [    8/   88]
per-ex loss: 0.547693  [    9/   88]
per-ex loss: 0.739652  [   10/   88]
per-ex loss: 0.601118  [   11/   88]
per-ex loss: 0.728230  [   12/   88]
per-ex loss: 0.689338  [   13/   88]
per-ex loss: 0.624324  [   14/   88]
per-ex loss: 0.724460  [   15/   88]
per-ex loss: 0.624536  [   16/   88]
per-ex loss: 0.633689  [   17/   88]
per-ex loss: 0.787168  [   18/   88]
per-ex loss: 0.797447  [   19/   88]
per-ex loss: 0.640854  [   20/   88]
per-ex loss: 0.817442  [   21/   88]
per-ex loss: 0.787879  [   22/   88]
per-ex loss: 0.802192  [   23/   88]
per-ex loss: 0.596000  [   24/   88]
per-ex loss: 0.597884  [   25/   88]
per-ex loss: 0.608088  [   26/   88]
per-ex loss: 0.571786  [   27/   88]
per-ex loss: 0.767851  [   28/   88]
per-ex loss: 0.796240  [   29/   88]
per-ex loss: 0.614245  [   30/   88]
per-ex loss: 0.732902  [   31/   88]
per-ex loss: 0.693211  [   32/   88]
per-ex loss: 0.648992  [   33/   88]
per-ex loss: 0.757857  [   34/   88]
per-ex loss: 0.660218  [   35/   88]
per-ex loss: 0.833917  [   36/   88]
per-ex loss: 0.602203  [   37/   88]
per-ex loss: 0.558356  [   38/   88]
per-ex loss: 0.614159  [   39/   88]
per-ex loss: 0.676482  [   40/   88]
per-ex loss: 0.630363  [   41/   88]
per-ex loss: 0.620567  [   42/   88]
per-ex loss: 0.806433  [   43/   88]
per-ex loss: 0.600664  [   44/   88]
per-ex loss: 0.586147  [   45/   88]
per-ex loss: 0.653441  [   46/   88]
per-ex loss: 0.664717  [   47/   88]
per-ex loss: 0.802619  [   48/   88]
per-ex loss: 0.655973  [   49/   88]
per-ex loss: 0.599130  [   50/   88]
per-ex loss: 0.840563  [   51/   88]
per-ex loss: 0.739519  [   52/   88]
per-ex loss: 0.815584  [   53/   88]
per-ex loss: 0.849820  [   54/   88]
per-ex loss: 0.586368  [   55/   88]
per-ex loss: 0.651268  [   56/   88]
per-ex loss: 0.551465  [   57/   88]
per-ex loss: 0.733790  [   58/   88]
per-ex loss: 0.567192  [   59/   88]
per-ex loss: 0.746158  [   60/   88]
per-ex loss: 0.758691  [   61/   88]
per-ex loss: 0.832752  [   62/   88]
per-ex loss: 0.551066  [   63/   88]
per-ex loss: 0.828301  [   64/   88]
per-ex loss: 0.656075  [   65/   88]
per-ex loss: 0.679993  [   66/   88]
per-ex loss: 0.589896  [   67/   88]
per-ex loss: 0.603150  [   68/   88]
per-ex loss: 0.727216  [   69/   88]
per-ex loss: 0.555884  [   70/   88]
per-ex loss: 0.565978  [   71/   88]
per-ex loss: 0.669470  [   72/   88]
per-ex loss: 0.738460  [   73/   88]
per-ex loss: 0.725511  [   74/   88]
per-ex loss: 0.538101  [   75/   88]
per-ex loss: 0.760134  [   76/   88]
per-ex loss: 0.561261  [   77/   88]
per-ex loss: 0.556429  [   78/   88]
per-ex loss: 0.697870  [   79/   88]
per-ex loss: 0.706987  [   80/   88]
per-ex loss: 0.764171  [   81/   88]
per-ex loss: 0.705692  [   82/   88]
per-ex loss: 0.767943  [   83/   88]
per-ex loss: 0.542940  [   84/   88]
per-ex loss: 0.822532  [   85/   88]
per-ex loss: 0.773212  [   86/   88]
per-ex loss: 0.816207  [   87/   88]
per-ex loss: 0.838745  [   88/   88]
Train Error: Avg loss: 0.69099778
validation Error: 
 Avg loss: 0.78088949 
 F1: 0.372847 
 Precision: 0.367860 
 Recall: 0.377972
 IoU: 0.229141

test Error: 
 Avg loss: 0.69564995 
 F1: 0.516214 
 Precision: 0.516987 
 Recall: 0.515444
 IoU: 0.347903

We have finished training iteration 13
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_11_.pth
per-ex loss: 0.765382  [    1/   88]
per-ex loss: 0.585938  [    2/   88]
per-ex loss: 0.787161  [    3/   88]
per-ex loss: 0.719768  [    4/   88]
per-ex loss: 0.749244  [    5/   88]
per-ex loss: 0.589648  [    6/   88]
per-ex loss: 0.690311  [    7/   88]
per-ex loss: 0.671208  [    8/   88]
per-ex loss: 0.626014  [    9/   88]
per-ex loss: 0.798250  [   10/   88]
per-ex loss: 0.789583  [   11/   88]
per-ex loss: 0.789653  [   12/   88]
per-ex loss: 0.707946  [   13/   88]
per-ex loss: 0.655174  [   14/   88]
per-ex loss: 0.644362  [   15/   88]
per-ex loss: 0.684202  [   16/   88]
per-ex loss: 0.794677  [   17/   88]
per-ex loss: 0.655104  [   18/   88]
per-ex loss: 0.782376  [   19/   88]
per-ex loss: 0.596130  [   20/   88]
per-ex loss: 0.552868  [   21/   88]
per-ex loss: 0.766475  [   22/   88]
per-ex loss: 0.604156  [   23/   88]
per-ex loss: 0.652337  [   24/   88]
per-ex loss: 0.811574  [   25/   88]
per-ex loss: 0.845087  [   26/   88]
per-ex loss: 0.779608  [   27/   88]
per-ex loss: 0.637777  [   28/   88]
per-ex loss: 0.602198  [   29/   88]
per-ex loss: 0.568017  [   30/   88]
per-ex loss: 0.679749  [   31/   88]
per-ex loss: 0.821246  [   32/   88]
per-ex loss: 0.565136  [   33/   88]
per-ex loss: 0.648480  [   34/   88]
per-ex loss: 0.758075  [   35/   88]
per-ex loss: 0.806363  [   36/   88]
per-ex loss: 0.587398  [   37/   88]
per-ex loss: 0.718946  [   38/   88]
per-ex loss: 0.766996  [   39/   88]
per-ex loss: 0.539228  [   40/   88]
per-ex loss: 0.760027  [   41/   88]
per-ex loss: 0.787496  [   42/   88]
per-ex loss: 0.573673  [   43/   88]
per-ex loss: 0.549156  [   44/   88]
per-ex loss: 0.735238  [   45/   88]
per-ex loss: 0.842236  [   46/   88]
per-ex loss: 0.711035  [   47/   88]
per-ex loss: 0.667867  [   48/   88]
per-ex loss: 0.524836  [   49/   88]
per-ex loss: 0.650566  [   50/   88]
per-ex loss: 0.599653  [   51/   88]
per-ex loss: 0.562291  [   52/   88]
per-ex loss: 0.731528  [   53/   88]
per-ex loss: 0.747413  [   54/   88]
per-ex loss: 0.646458  [   55/   88]
per-ex loss: 0.645212  [   56/   88]
per-ex loss: 0.613743  [   57/   88]
per-ex loss: 0.808721  [   58/   88]
per-ex loss: 0.660365  [   59/   88]
per-ex loss: 0.761545  [   60/   88]
per-ex loss: 0.630314  [   61/   88]
per-ex loss: 0.714848  [   62/   88]
per-ex loss: 0.581611  [   63/   88]
per-ex loss: 0.729193  [   64/   88]
per-ex loss: 0.835249  [   65/   88]
per-ex loss: 0.553573  [   66/   88]
per-ex loss: 0.611571  [   67/   88]
per-ex loss: 0.748244  [   68/   88]
per-ex loss: 0.826505  [   69/   88]
per-ex loss: 0.624885  [   70/   88]
per-ex loss: 0.753793  [   71/   88]
per-ex loss: 0.628691  [   72/   88]
per-ex loss: 0.579001  [   73/   88]
per-ex loss: 0.731941  [   74/   88]
per-ex loss: 0.565924  [   75/   88]
per-ex loss: 0.711450  [   76/   88]
per-ex loss: 0.713387  [   77/   88]
per-ex loss: 0.624272  [   78/   88]
per-ex loss: 0.595158  [   79/   88]
per-ex loss: 0.785704  [   80/   88]
per-ex loss: 0.611468  [   81/   88]
per-ex loss: 0.592697  [   82/   88]
per-ex loss: 0.842744  [   83/   88]
per-ex loss: 0.715305  [   84/   88]
per-ex loss: 0.548604  [   85/   88]
per-ex loss: 0.821594  [   86/   88]
per-ex loss: 0.615658  [   87/   88]
per-ex loss: 0.802882  [   88/   88]
Train Error: Avg loss: 0.68714966
validation Error: 
 Avg loss: 0.71489011 
 F1: 0.473625 
 Precision: 0.478028 
 Recall: 0.469302
 IoU: 0.310294

test Error: 
 Avg loss: 0.66439673 
 F1: 0.549822 
 Precision: 0.550749 
 Recall: 0.548899
 IoU: 0.379141

We have finished training iteration 14
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_12_.pth
per-ex loss: 0.797896  [    1/   88]
per-ex loss: 0.568711  [    2/   88]
per-ex loss: 0.760367  [    3/   88]
per-ex loss: 0.682189  [    4/   88]
per-ex loss: 0.593240  [    5/   88]
per-ex loss: 0.689820  [    6/   88]
per-ex loss: 0.780423  [    7/   88]
per-ex loss: 0.564453  [    8/   88]
per-ex loss: 0.740707  [    9/   88]
per-ex loss: 0.741413  [   10/   88]
per-ex loss: 0.742607  [   11/   88]
per-ex loss: 0.621830  [   12/   88]
per-ex loss: 0.641565  [   13/   88]
per-ex loss: 0.632126  [   14/   88]
per-ex loss: 0.663315  [   15/   88]
per-ex loss: 0.722386  [   16/   88]
per-ex loss: 0.650460  [   17/   88]
per-ex loss: 0.851934  [   18/   88]
per-ex loss: 0.829008  [   19/   88]
per-ex loss: 0.733917  [   20/   88]
per-ex loss: 0.582612  [   21/   88]
per-ex loss: 0.519794  [   22/   88]
per-ex loss: 0.727010  [   23/   88]
per-ex loss: 0.612507  [   24/   88]
per-ex loss: 0.571392  [   25/   88]
per-ex loss: 0.559883  [   26/   88]
per-ex loss: 0.831327  [   27/   88]
per-ex loss: 0.598314  [   28/   88]
per-ex loss: 0.595433  [   29/   88]
per-ex loss: 0.624319  [   30/   88]
per-ex loss: 0.649468  [   31/   88]
per-ex loss: 0.726552  [   32/   88]
per-ex loss: 0.805093  [   33/   88]
per-ex loss: 0.717893  [   34/   88]
per-ex loss: 0.732084  [   35/   88]
per-ex loss: 0.702329  [   36/   88]
per-ex loss: 0.796716  [   37/   88]
per-ex loss: 0.566862  [   38/   88]
per-ex loss: 0.678659  [   39/   88]
per-ex loss: 0.600100  [   40/   88]
per-ex loss: 0.792528  [   41/   88]
per-ex loss: 0.566820  [   42/   88]
per-ex loss: 0.588208  [   43/   88]
per-ex loss: 0.801018  [   44/   88]
per-ex loss: 0.646017  [   45/   88]
per-ex loss: 0.634013  [   46/   88]
per-ex loss: 0.645601  [   47/   88]
per-ex loss: 0.504466  [   48/   88]
per-ex loss: 0.547204  [   49/   88]
per-ex loss: 0.879808  [   50/   88]
per-ex loss: 0.585953  [   51/   88]
per-ex loss: 0.769967  [   52/   88]
per-ex loss: 0.727822  [   53/   88]
per-ex loss: 0.583169  [   54/   88]
per-ex loss: 0.574958  [   55/   88]
per-ex loss: 0.793774  [   56/   88]
per-ex loss: 0.779719  [   57/   88]
per-ex loss: 0.817414  [   58/   88]
per-ex loss: 0.569099  [   59/   88]
per-ex loss: 0.529523  [   60/   88]
per-ex loss: 0.790276  [   61/   88]
per-ex loss: 0.779844  [   62/   88]
per-ex loss: 0.645916  [   63/   88]
per-ex loss: 0.628407  [   64/   88]
per-ex loss: 0.647183  [   65/   88]
per-ex loss: 0.590329  [   66/   88]
per-ex loss: 0.747477  [   67/   88]
per-ex loss: 0.736346  [   68/   88]
per-ex loss: 0.734868  [   69/   88]
per-ex loss: 0.657168  [   70/   88]
per-ex loss: 0.655369  [   71/   88]
per-ex loss: 0.777473  [   72/   88]
per-ex loss: 0.745269  [   73/   88]
per-ex loss: 0.748548  [   74/   88]
per-ex loss: 0.768538  [   75/   88]
per-ex loss: 0.579608  [   76/   88]
per-ex loss: 0.776895  [   77/   88]
per-ex loss: 0.608952  [   78/   88]
per-ex loss: 0.816008  [   79/   88]
per-ex loss: 0.636094  [   80/   88]
per-ex loss: 0.600863  [   81/   88]
per-ex loss: 0.674499  [   82/   88]
per-ex loss: 0.799645  [   83/   88]
per-ex loss: 0.757699  [   84/   88]
per-ex loss: 0.655910  [   85/   88]
per-ex loss: 0.729524  [   86/   88]
per-ex loss: 0.664315  [   87/   88]
per-ex loss: 0.835743  [   88/   88]
Train Error: Avg loss: 0.68559730
validation Error: 
 Avg loss: 0.72000297 
 F1: 0.457472 
 Precision: 0.609310 
 Recall: 0.366213
 IoU: 0.296573

test Error: 
 Avg loss: 0.66707946 
 F1: 0.543287 
 Precision: 0.655864 
 Recall: 0.463695
 IoU: 0.372954

We have finished training iteration 15
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_13_.pth
per-ex loss: 0.633172  [    1/   88]
per-ex loss: 0.589392  [    2/   88]
per-ex loss: 0.787528  [    3/   88]
per-ex loss: 0.534370  [    4/   88]
per-ex loss: 0.750387  [    5/   88]
per-ex loss: 0.833940  [    6/   88]
per-ex loss: 0.627366  [    7/   88]
per-ex loss: 0.824447  [    8/   88]
per-ex loss: 0.540725  [    9/   88]
per-ex loss: 0.736949  [   10/   88]
per-ex loss: 0.527967  [   11/   88]
per-ex loss: 0.660433  [   12/   88]
per-ex loss: 0.742147  [   13/   88]
per-ex loss: 0.669826  [   14/   88]
per-ex loss: 0.577345  [   15/   88]
per-ex loss: 0.752893  [   16/   88]
per-ex loss: 0.651157  [   17/   88]
per-ex loss: 0.808722  [   18/   88]
per-ex loss: 0.780799  [   19/   88]
per-ex loss: 0.769129  [   20/   88]
per-ex loss: 0.588345  [   21/   88]
per-ex loss: 0.625618  [   22/   88]
per-ex loss: 0.638931  [   23/   88]
per-ex loss: 0.570630  [   24/   88]
per-ex loss: 0.712565  [   25/   88]
per-ex loss: 0.601201  [   26/   88]
per-ex loss: 0.711722  [   27/   88]
per-ex loss: 0.603702  [   28/   88]
per-ex loss: 0.630356  [   29/   88]
per-ex loss: 0.571302  [   30/   88]
per-ex loss: 0.636451  [   31/   88]
per-ex loss: 0.550195  [   32/   88]
per-ex loss: 0.667645  [   33/   88]
per-ex loss: 0.745547  [   34/   88]
per-ex loss: 0.782133  [   35/   88]
per-ex loss: 0.793041  [   36/   88]
per-ex loss: 0.850571  [   37/   88]
per-ex loss: 0.742200  [   38/   88]
per-ex loss: 0.535606  [   39/   88]
per-ex loss: 0.563514  [   40/   88]
per-ex loss: 0.611023  [   41/   88]
per-ex loss: 0.548360  [   42/   88]
per-ex loss: 0.619914  [   43/   88]
per-ex loss: 0.598323  [   44/   88]
per-ex loss: 0.741042  [   45/   88]
per-ex loss: 0.668754  [   46/   88]
per-ex loss: 0.536355  [   47/   88]
per-ex loss: 0.602080  [   48/   88]
per-ex loss: 0.645842  [   49/   88]
per-ex loss: 0.631368  [   50/   88]
per-ex loss: 0.669496  [   51/   88]
per-ex loss: 0.771377  [   52/   88]
per-ex loss: 0.736974  [   53/   88]
per-ex loss: 0.683190  [   54/   88]
per-ex loss: 0.776389  [   55/   88]
per-ex loss: 0.556618  [   56/   88]
per-ex loss: 0.814091  [   57/   88]
per-ex loss: 0.550988  [   58/   88]
per-ex loss: 0.791304  [   59/   88]
per-ex loss: 0.791092  [   60/   88]
per-ex loss: 0.591274  [   61/   88]
per-ex loss: 0.809221  [   62/   88]
per-ex loss: 0.762565  [   63/   88]
per-ex loss: 0.538984  [   64/   88]
per-ex loss: 0.714456  [   65/   88]
per-ex loss: 0.777483  [   66/   88]
per-ex loss: 0.510252  [   67/   88]
per-ex loss: 0.840129  [   68/   88]
per-ex loss: 0.752999  [   69/   88]
per-ex loss: 0.816826  [   70/   88]
per-ex loss: 0.632929  [   71/   88]
per-ex loss: 0.651216  [   72/   88]
per-ex loss: 0.651761  [   73/   88]
per-ex loss: 0.841970  [   74/   88]
per-ex loss: 0.801003  [   75/   88]
per-ex loss: 0.779140  [   76/   88]
per-ex loss: 0.661511  [   77/   88]
per-ex loss: 0.634676  [   78/   88]
per-ex loss: 0.686720  [   79/   88]
per-ex loss: 0.868709  [   80/   88]
per-ex loss: 0.713160  [   81/   88]
per-ex loss: 0.833405  [   82/   88]
per-ex loss: 0.676292  [   83/   88]
per-ex loss: 0.830306  [   84/   88]
per-ex loss: 0.595002  [   85/   88]
per-ex loss: 0.642526  [   86/   88]
per-ex loss: 0.776839  [   87/   88]
per-ex loss: 0.760444  [   88/   88]
Train Error: Avg loss: 0.68654943
validation Error: 
 Avg loss: 0.73698412 
 F1: 0.437435 
 Precision: 0.400733 
 Recall: 0.481538
 IoU: 0.279947

test Error: 
 Avg loss: 0.67127129 
 F1: 0.539772 
 Precision: 0.509295 
 Recall: 0.574129
 IoU: 0.369649

We have finished training iteration 16
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_8_.pth
per-ex loss: 0.770825  [    1/   88]
per-ex loss: 0.618308  [    2/   88]
per-ex loss: 0.779458  [    3/   88]
per-ex loss: 0.805823  [    4/   88]
per-ex loss: 0.772483  [    5/   88]
per-ex loss: 0.652174  [    6/   88]
per-ex loss: 0.581846  [    7/   88]
per-ex loss: 0.584867  [    8/   88]
per-ex loss: 0.706512  [    9/   88]
per-ex loss: 0.786311  [   10/   88]
per-ex loss: 0.803022  [   11/   88]
per-ex loss: 0.832575  [   12/   88]
per-ex loss: 0.844480  [   13/   88]
per-ex loss: 0.657201  [   14/   88]
per-ex loss: 0.581077  [   15/   88]
per-ex loss: 0.658524  [   16/   88]
per-ex loss: 0.584741  [   17/   88]
per-ex loss: 0.693002  [   18/   88]
per-ex loss: 0.757066  [   19/   88]
per-ex loss: 0.592602  [   20/   88]
per-ex loss: 0.728940  [   21/   88]
per-ex loss: 0.614506  [   22/   88]
per-ex loss: 0.696405  [   23/   88]
per-ex loss: 0.627897  [   24/   88]
per-ex loss: 0.724100  [   25/   88]
per-ex loss: 0.643035  [   26/   88]
per-ex loss: 0.752555  [   27/   88]
per-ex loss: 0.739337  [   28/   88]
per-ex loss: 0.843727  [   29/   88]
per-ex loss: 0.618638  [   30/   88]
per-ex loss: 0.626561  [   31/   88]
per-ex loss: 0.797170  [   32/   88]
per-ex loss: 0.787301  [   33/   88]
per-ex loss: 0.521360  [   34/   88]
per-ex loss: 0.823389  [   35/   88]
per-ex loss: 0.570274  [   36/   88]
per-ex loss: 0.765553  [   37/   88]
per-ex loss: 0.792126  [   38/   88]
per-ex loss: 0.776025  [   39/   88]
per-ex loss: 0.557737  [   40/   88]
per-ex loss: 0.622082  [   41/   88]
per-ex loss: 0.571315  [   42/   88]
per-ex loss: 0.604150  [   43/   88]
per-ex loss: 0.829424  [   44/   88]
per-ex loss: 0.585181  [   45/   88]
per-ex loss: 0.770331  [   46/   88]
per-ex loss: 0.632858  [   47/   88]
per-ex loss: 0.672666  [   48/   88]
per-ex loss: 0.750714  [   49/   88]
per-ex loss: 0.744797  [   50/   88]
per-ex loss: 0.608986  [   51/   88]
per-ex loss: 0.781545  [   52/   88]
per-ex loss: 0.720968  [   53/   88]
per-ex loss: 0.555510  [   54/   88]
per-ex loss: 0.553550  [   55/   88]
per-ex loss: 0.735173  [   56/   88]
per-ex loss: 0.655095  [   57/   88]
per-ex loss: 0.488876  [   58/   88]
per-ex loss: 0.746918  [   59/   88]
per-ex loss: 0.642228  [   60/   88]
per-ex loss: 0.560243  [   61/   88]
per-ex loss: 0.614237  [   62/   88]
per-ex loss: 0.722470  [   63/   88]
per-ex loss: 0.641020  [   64/   88]
per-ex loss: 0.795220  [   65/   88]
per-ex loss: 0.760199  [   66/   88]
per-ex loss: 0.597315  [   67/   88]
per-ex loss: 0.836823  [   68/   88]
per-ex loss: 0.567061  [   69/   88]
per-ex loss: 0.752499  [   70/   88]
per-ex loss: 0.826750  [   71/   88]
per-ex loss: 0.620053  [   72/   88]
per-ex loss: 0.569121  [   73/   88]
per-ex loss: 0.645865  [   74/   88]
per-ex loss: 0.716597  [   75/   88]
per-ex loss: 0.642792  [   76/   88]
per-ex loss: 0.644320  [   77/   88]
per-ex loss: 0.601058  [   78/   88]
per-ex loss: 0.692922  [   79/   88]
per-ex loss: 0.555336  [   80/   88]
per-ex loss: 0.598387  [   81/   88]
per-ex loss: 0.822399  [   82/   88]
per-ex loss: 0.527317  [   83/   88]
per-ex loss: 0.795221  [   84/   88]
per-ex loss: 0.543392  [   85/   88]
per-ex loss: 0.834790  [   86/   88]
per-ex loss: 0.513539  [   87/   88]
per-ex loss: 0.666389  [   88/   88]
Train Error: Avg loss: 0.68187767
validation Error: 
 Avg loss: 0.72663512 
 F1: 0.447332 
 Precision: 0.584291 
 Recall: 0.362387
 IoU: 0.288105

test Error: 
 Avg loss: 0.67017996 
 F1: 0.542887 
 Precision: 0.662254 
 Recall: 0.459979
 IoU: 0.372577

We have finished training iteration 17
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_15_.pth
per-ex loss: 0.764417  [    1/   88]
per-ex loss: 0.859260  [    2/   88]
per-ex loss: 0.793409  [    3/   88]
per-ex loss: 0.725721  [    4/   88]
per-ex loss: 0.541665  [    5/   88]
per-ex loss: 0.618099  [    6/   88]
per-ex loss: 0.717627  [    7/   88]
per-ex loss: 0.821799  [    8/   88]
per-ex loss: 0.720453  [    9/   88]
per-ex loss: 0.514622  [   10/   88]
per-ex loss: 0.639958  [   11/   88]
per-ex loss: 0.827194  [   12/   88]
per-ex loss: 0.608326  [   13/   88]
per-ex loss: 0.616572  [   14/   88]
per-ex loss: 0.601773  [   15/   88]
per-ex loss: 0.835661  [   16/   88]
per-ex loss: 0.581746  [   17/   88]
per-ex loss: 0.646953  [   18/   88]
per-ex loss: 0.750400  [   19/   88]
per-ex loss: 0.709620  [   20/   88]
per-ex loss: 0.611391  [   21/   88]
per-ex loss: 0.725213  [   22/   88]
per-ex loss: 0.547667  [   23/   88]
per-ex loss: 0.832668  [   24/   88]
per-ex loss: 0.788458  [   25/   88]
per-ex loss: 0.812250  [   26/   88]
per-ex loss: 0.655572  [   27/   88]
per-ex loss: 0.536695  [   28/   88]
per-ex loss: 0.553859  [   29/   88]
per-ex loss: 0.804639  [   30/   88]
per-ex loss: 0.783966  [   31/   88]
per-ex loss: 0.723603  [   32/   88]
per-ex loss: 0.751877  [   33/   88]
per-ex loss: 0.591896  [   34/   88]
per-ex loss: 0.770536  [   35/   88]
per-ex loss: 0.763700  [   36/   88]
per-ex loss: 0.638899  [   37/   88]
per-ex loss: 0.640767  [   38/   88]
per-ex loss: 0.629235  [   39/   88]
per-ex loss: 0.742595  [   40/   88]
per-ex loss: 0.637387  [   41/   88]
per-ex loss: 0.726475  [   42/   88]
per-ex loss: 0.580343  [   43/   88]
per-ex loss: 0.585562  [   44/   88]
per-ex loss: 0.540280  [   45/   88]
per-ex loss: 0.572896  [   46/   88]
per-ex loss: 0.760791  [   47/   88]
per-ex loss: 0.702415  [   48/   88]
per-ex loss: 0.758761  [   49/   88]
per-ex loss: 0.637945  [   50/   88]
per-ex loss: 0.778626  [   51/   88]
per-ex loss: 0.795674  [   52/   88]
per-ex loss: 0.539121  [   53/   88]
per-ex loss: 0.770638  [   54/   88]
per-ex loss: 0.622674  [   55/   88]
per-ex loss: 0.729223  [   56/   88]
per-ex loss: 0.805529  [   57/   88]
per-ex loss: 0.733045  [   58/   88]
per-ex loss: 0.734379  [   59/   88]
per-ex loss: 0.629547  [   60/   88]
per-ex loss: 0.650832  [   61/   88]
per-ex loss: 0.591351  [   62/   88]
per-ex loss: 0.638742  [   63/   88]
per-ex loss: 0.833152  [   64/   88]
per-ex loss: 0.616271  [   65/   88]
per-ex loss: 0.579849  [   66/   88]
per-ex loss: 0.834535  [   67/   88]
per-ex loss: 0.556202  [   68/   88]
per-ex loss: 0.835757  [   69/   88]
per-ex loss: 0.617485  [   70/   88]
per-ex loss: 0.626817  [   71/   88]
per-ex loss: 0.800212  [   72/   88]
per-ex loss: 0.530962  [   73/   88]
per-ex loss: 0.663736  [   74/   88]
per-ex loss: 0.773703  [   75/   88]
per-ex loss: 0.790513  [   76/   88]
per-ex loss: 0.631200  [   77/   88]
per-ex loss: 0.531397  [   78/   88]
per-ex loss: 0.617190  [   79/   88]
per-ex loss: 0.589522  [   80/   88]
per-ex loss: 0.600305  [   81/   88]
per-ex loss: 0.572562  [   82/   88]
per-ex loss: 0.782786  [   83/   88]
per-ex loss: 0.622945  [   84/   88]
per-ex loss: 0.669455  [   85/   88]
per-ex loss: 0.732281  [   86/   88]
per-ex loss: 0.813526  [   87/   88]
per-ex loss: 0.646195  [   88/   88]
Train Error: Avg loss: 0.68399497
validation Error: 
 Avg loss: 0.72750230 
 F1: 0.452722 
 Precision: 0.466745 
 Recall: 0.439517
 IoU: 0.292593

test Error: 
 Avg loss: 0.66217253 
 F1: 0.554170 
 Precision: 0.565169 
 Recall: 0.543591
 IoU: 0.383288

We have finished training iteration 18
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_16_.pth
per-ex loss: 0.575916  [    1/   88]
per-ex loss: 0.715551  [    2/   88]
per-ex loss: 0.753243  [    3/   88]
per-ex loss: 0.637416  [    4/   88]
per-ex loss: 0.637757  [    5/   88]
per-ex loss: 0.686743  [    6/   88]
per-ex loss: 0.569343  [    7/   88]
per-ex loss: 0.769398  [    8/   88]
per-ex loss: 0.608455  [    9/   88]
per-ex loss: 0.625485  [   10/   88]
per-ex loss: 0.731591  [   11/   88]
per-ex loss: 0.720403  [   12/   88]
per-ex loss: 0.579934  [   13/   88]
per-ex loss: 0.820340  [   14/   88]
per-ex loss: 0.815124  [   15/   88]
per-ex loss: 0.682517  [   16/   88]
per-ex loss: 0.628137  [   17/   88]
per-ex loss: 0.768181  [   18/   88]
per-ex loss: 0.635496  [   19/   88]
per-ex loss: 0.675666  [   20/   88]
per-ex loss: 0.600092  [   21/   88]
per-ex loss: 0.557233  [   22/   88]
per-ex loss: 0.561373  [   23/   88]
per-ex loss: 0.750994  [   24/   88]
per-ex loss: 0.537934  [   25/   88]
per-ex loss: 0.560980  [   26/   88]
per-ex loss: 0.730764  [   27/   88]
per-ex loss: 0.634039  [   28/   88]
per-ex loss: 0.778174  [   29/   88]
per-ex loss: 0.834768  [   30/   88]
per-ex loss: 0.722708  [   31/   88]
per-ex loss: 0.602441  [   32/   88]
per-ex loss: 0.570553  [   33/   88]
per-ex loss: 0.607272  [   34/   88]
per-ex loss: 0.800160  [   35/   88]
per-ex loss: 0.766757  [   36/   88]
per-ex loss: 0.784446  [   37/   88]
per-ex loss: 0.504546  [   38/   88]
per-ex loss: 0.583462  [   39/   88]
per-ex loss: 0.792172  [   40/   88]
per-ex loss: 0.745830  [   41/   88]
per-ex loss: 0.834237  [   42/   88]
per-ex loss: 0.602382  [   43/   88]
per-ex loss: 0.759725  [   44/   88]
per-ex loss: 0.645273  [   45/   88]
per-ex loss: 0.725865  [   46/   88]
per-ex loss: 0.564206  [   47/   88]
per-ex loss: 0.615849  [   48/   88]
per-ex loss: 0.609144  [   49/   88]
per-ex loss: 0.621886  [   50/   88]
per-ex loss: 0.789957  [   51/   88]
per-ex loss: 0.829935  [   52/   88]
per-ex loss: 0.787139  [   53/   88]
per-ex loss: 0.657760  [   54/   88]
per-ex loss: 0.668317  [   55/   88]
per-ex loss: 0.543646  [   56/   88]
per-ex loss: 0.530095  [   57/   88]
per-ex loss: 0.515784  [   58/   88]
per-ex loss: 0.568680  [   59/   88]
per-ex loss: 0.704667  [   60/   88]
per-ex loss: 0.643059  [   61/   88]
per-ex loss: 0.557895  [   62/   88]
per-ex loss: 0.690629  [   63/   88]
per-ex loss: 0.787402  [   64/   88]
per-ex loss: 0.791891  [   65/   88]
per-ex loss: 0.764196  [   66/   88]
per-ex loss: 0.600888  [   67/   88]
per-ex loss: 0.807309  [   68/   88]
per-ex loss: 0.651253  [   69/   88]
per-ex loss: 0.786407  [   70/   88]
per-ex loss: 0.736978  [   71/   88]
per-ex loss: 0.731820  [   72/   88]
per-ex loss: 0.608382  [   73/   88]
per-ex loss: 0.650029  [   74/   88]
per-ex loss: 0.614181  [   75/   88]
per-ex loss: 0.776823  [   76/   88]
per-ex loss: 0.816386  [   77/   88]
per-ex loss: 0.544921  [   78/   88]
per-ex loss: 0.565032  [   79/   88]
per-ex loss: 0.802643  [   80/   88]
per-ex loss: 0.744037  [   81/   88]
per-ex loss: 0.560199  [   82/   88]
per-ex loss: 0.660602  [   83/   88]
per-ex loss: 0.622419  [   84/   88]
per-ex loss: 0.622882  [   85/   88]
per-ex loss: 0.832671  [   86/   88]
per-ex loss: 0.768535  [   87/   88]
per-ex loss: 0.831921  [   88/   88]
Train Error: Avg loss: 0.67965152
validation Error: 
 Avg loss: 0.72605752 
 F1: 0.448443 
 Precision: 0.515839 
 Recall: 0.396622
 IoU: 0.289028

test Error: 
 Avg loss: 0.66030315 
 F1: 0.557020 
 Precision: 0.628734 
 Recall: 0.499991
 IoU: 0.386021

We have finished training iteration 19
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_17_.pth
per-ex loss: 0.782424  [    1/   88]
per-ex loss: 0.601689  [    2/   88]
per-ex loss: 0.812069  [    3/   88]
per-ex loss: 0.564415  [    4/   88]
per-ex loss: 0.788769  [    5/   88]
per-ex loss: 0.826579  [    6/   88]
per-ex loss: 0.508582  [    7/   88]
per-ex loss: 0.642776  [    8/   88]
per-ex loss: 0.745163  [    9/   88]
per-ex loss: 0.662452  [   10/   88]
per-ex loss: 0.641388  [   11/   88]
per-ex loss: 0.760659  [   12/   88]
per-ex loss: 0.584508  [   13/   88]
per-ex loss: 0.784892  [   14/   88]
per-ex loss: 0.627020  [   15/   88]
per-ex loss: 0.771045  [   16/   88]
per-ex loss: 0.783855  [   17/   88]
per-ex loss: 0.756548  [   18/   88]
per-ex loss: 0.668423  [   19/   88]
per-ex loss: 0.798674  [   20/   88]
per-ex loss: 0.558797  [   21/   88]
per-ex loss: 0.635164  [   22/   88]
per-ex loss: 0.632921  [   23/   88]
per-ex loss: 0.711831  [   24/   88]
per-ex loss: 0.751985  [   25/   88]
per-ex loss: 0.659534  [   26/   88]
per-ex loss: 0.635402  [   27/   88]
per-ex loss: 0.573525  [   28/   88]
per-ex loss: 0.763702  [   29/   88]
per-ex loss: 0.642535  [   30/   88]
per-ex loss: 0.730508  [   31/   88]
per-ex loss: 0.680184  [   32/   88]
per-ex loss: 0.571543  [   33/   88]
per-ex loss: 0.723855  [   34/   88]
per-ex loss: 0.788792  [   35/   88]
per-ex loss: 0.575034  [   36/   88]
per-ex loss: 0.777474  [   37/   88]
per-ex loss: 0.777102  [   38/   88]
per-ex loss: 0.585349  [   39/   88]
per-ex loss: 0.770501  [   40/   88]
per-ex loss: 0.601631  [   41/   88]
per-ex loss: 0.761330  [   42/   88]
per-ex loss: 0.774565  [   43/   88]
per-ex loss: 0.794525  [   44/   88]
per-ex loss: 0.541533  [   45/   88]
per-ex loss: 0.561335  [   46/   88]
per-ex loss: 0.574881  [   47/   88]
per-ex loss: 0.718653  [   48/   88]
per-ex loss: 0.764721  [   49/   88]
per-ex loss: 0.537901  [   50/   88]
per-ex loss: 0.601872  [   51/   88]
per-ex loss: 0.819154  [   52/   88]
per-ex loss: 0.596536  [   53/   88]
per-ex loss: 0.647328  [   54/   88]
per-ex loss: 0.684186  [   55/   88]
per-ex loss: 0.792162  [   56/   88]
per-ex loss: 0.707066  [   57/   88]
per-ex loss: 0.689308  [   58/   88]
per-ex loss: 0.842804  [   59/   88]
per-ex loss: 0.729958  [   60/   88]
per-ex loss: 0.571970  [   61/   88]
per-ex loss: 0.786164  [   62/   88]
per-ex loss: 0.539467  [   63/   88]
per-ex loss: 0.725362  [   64/   88]
per-ex loss: 0.634247  [   65/   88]
per-ex loss: 0.672867  [   66/   88]
per-ex loss: 0.552594  [   67/   88]
per-ex loss: 0.703306  [   68/   88]
per-ex loss: 0.559050  [   69/   88]
per-ex loss: 0.835413  [   70/   88]
per-ex loss: 0.627545  [   71/   88]
per-ex loss: 0.589749  [   72/   88]
per-ex loss: 0.587167  [   73/   88]
per-ex loss: 0.803085  [   74/   88]
per-ex loss: 0.833429  [   75/   88]
per-ex loss: 0.584697  [   76/   88]
per-ex loss: 0.612552  [   77/   88]
per-ex loss: 0.620461  [   78/   88]
per-ex loss: 0.606556  [   79/   88]
per-ex loss: 0.611759  [   80/   88]
per-ex loss: 0.792248  [   81/   88]
per-ex loss: 0.510860  [   82/   88]
per-ex loss: 0.588772  [   83/   88]
per-ex loss: 0.609843  [   84/   88]
per-ex loss: 0.761803  [   85/   88]
per-ex loss: 0.817558  [   86/   88]
per-ex loss: 0.765421  [   87/   88]
per-ex loss: 0.500918  [   88/   88]
Train Error: Avg loss: 0.67959062
validation Error: 
 Avg loss: 0.71724844 
 F1: 0.462256 
 Precision: 0.573325 
 Recall: 0.387237
 IoU: 0.300606

test Error: 
 Avg loss: 0.66450539 
 F1: 0.548829 
 Precision: 0.626308 
 Recall: 0.488409
 IoU: 0.378197

We have finished training iteration 20
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_18_.pth
per-ex loss: 0.841238  [    1/   88]
per-ex loss: 0.742456  [    2/   88]
per-ex loss: 0.740356  [    3/   88]
per-ex loss: 0.548250  [    4/   88]
per-ex loss: 0.757275  [    5/   88]
per-ex loss: 0.762285  [    6/   88]
per-ex loss: 0.707477  [    7/   88]
per-ex loss: 0.633735  [    8/   88]
per-ex loss: 0.786118  [    9/   88]
per-ex loss: 0.522343  [   10/   88]
per-ex loss: 0.615655  [   11/   88]
per-ex loss: 0.753550  [   12/   88]
per-ex loss: 0.642950  [   13/   88]
per-ex loss: 0.567739  [   14/   88]
per-ex loss: 0.553033  [   15/   88]
per-ex loss: 0.705070  [   16/   88]
per-ex loss: 0.653468  [   17/   88]
per-ex loss: 0.606302  [   18/   88]
per-ex loss: 0.732467  [   19/   88]
per-ex loss: 0.598015  [   20/   88]
per-ex loss: 0.610884  [   21/   88]
per-ex loss: 0.747156  [   22/   88]
per-ex loss: 0.613387  [   23/   88]
per-ex loss: 0.545277  [   24/   88]
per-ex loss: 0.824437  [   25/   88]
per-ex loss: 0.846471  [   26/   88]
per-ex loss: 0.722571  [   27/   88]
per-ex loss: 0.567380  [   28/   88]
per-ex loss: 0.531129  [   29/   88]
per-ex loss: 0.800389  [   30/   88]
per-ex loss: 0.576971  [   31/   88]
per-ex loss: 0.617445  [   32/   88]
per-ex loss: 0.759071  [   33/   88]
per-ex loss: 0.556759  [   34/   88]
per-ex loss: 0.793511  [   35/   88]
per-ex loss: 0.610825  [   36/   88]
per-ex loss: 0.840868  [   37/   88]
per-ex loss: 0.832431  [   38/   88]
per-ex loss: 0.760462  [   39/   88]
per-ex loss: 0.774053  [   40/   88]
per-ex loss: 0.621802  [   41/   88]
per-ex loss: 0.635151  [   42/   88]
per-ex loss: 0.609368  [   43/   88]
per-ex loss: 0.649621  [   44/   88]
per-ex loss: 0.675109  [   45/   88]
per-ex loss: 0.618456  [   46/   88]
per-ex loss: 0.718741  [   47/   88]
per-ex loss: 0.733881  [   48/   88]
per-ex loss: 0.712548  [   49/   88]
per-ex loss: 0.555538  [   50/   88]
per-ex loss: 0.590885  [   51/   88]
per-ex loss: 0.542670  [   52/   88]
per-ex loss: 0.571601  [   53/   88]
per-ex loss: 0.666406  [   54/   88]
per-ex loss: 0.827725  [   55/   88]
per-ex loss: 0.604978  [   56/   88]
per-ex loss: 0.757359  [   57/   88]
per-ex loss: 0.711300  [   58/   88]
per-ex loss: 0.600481  [   59/   88]
per-ex loss: 0.584988  [   60/   88]
per-ex loss: 0.575132  [   61/   88]
per-ex loss: 0.535665  [   62/   88]
per-ex loss: 0.789296  [   63/   88]
per-ex loss: 0.775444  [   64/   88]
per-ex loss: 0.572865  [   65/   88]
per-ex loss: 0.802882  [   66/   88]
per-ex loss: 0.747292  [   67/   88]
per-ex loss: 0.823044  [   68/   88]
per-ex loss: 0.502252  [   69/   88]
per-ex loss: 0.790283  [   70/   88]
per-ex loss: 0.788255  [   71/   88]
per-ex loss: 0.563190  [   72/   88]
per-ex loss: 0.652305  [   73/   88]
per-ex loss: 0.723159  [   74/   88]
per-ex loss: 0.564440  [   75/   88]
per-ex loss: 0.781124  [   76/   88]
per-ex loss: 0.725964  [   77/   88]
per-ex loss: 0.784926  [   78/   88]
per-ex loss: 0.649479  [   79/   88]
per-ex loss: 0.737824  [   80/   88]
per-ex loss: 0.810798  [   81/   88]
per-ex loss: 0.800576  [   82/   88]
per-ex loss: 0.593429  [   83/   88]
per-ex loss: 0.641539  [   84/   88]
per-ex loss: 0.621022  [   85/   88]
per-ex loss: 0.654983  [   86/   88]
per-ex loss: 0.552323  [   87/   88]
per-ex loss: 0.635980  [   88/   88]
Train Error: Avg loss: 0.67710606
validation Error: 
 Avg loss: 0.70393860 
 F1: 0.481530 
 Precision: 0.534660 
 Recall: 0.438004
 IoU: 0.317115

test Error: 
 Avg loss: 0.65307764 
 F1: 0.563003 
 Precision: 0.595112 
 Recall: 0.534182
 IoU: 0.391791

We have finished training iteration 21
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_19_.pth
per-ex loss: 0.535787  [    1/   88]
per-ex loss: 0.576421  [    2/   88]
per-ex loss: 0.645373  [    3/   88]
per-ex loss: 0.821501  [    4/   88]
per-ex loss: 0.626407  [    5/   88]
per-ex loss: 0.587551  [    6/   88]
per-ex loss: 0.652767  [    7/   88]
per-ex loss: 0.565137  [    8/   88]
per-ex loss: 0.617542  [    9/   88]
per-ex loss: 0.603325  [   10/   88]
per-ex loss: 0.512198  [   11/   88]
per-ex loss: 0.647229  [   12/   88]
per-ex loss: 0.722500  [   13/   88]
per-ex loss: 0.573353  [   14/   88]
per-ex loss: 0.780252  [   15/   88]
per-ex loss: 0.590564  [   16/   88]
per-ex loss: 0.525319  [   17/   88]
per-ex loss: 0.741855  [   18/   88]
per-ex loss: 0.568485  [   19/   88]
per-ex loss: 0.648073  [   20/   88]
per-ex loss: 0.677054  [   21/   88]
per-ex loss: 0.781498  [   22/   88]
per-ex loss: 0.561297  [   23/   88]
per-ex loss: 0.583801  [   24/   88]
per-ex loss: 0.546171  [   25/   88]
per-ex loss: 0.689085  [   26/   88]
per-ex loss: 0.808573  [   27/   88]
per-ex loss: 0.730227  [   28/   88]
per-ex loss: 0.549792  [   29/   88]
per-ex loss: 0.771893  [   30/   88]
per-ex loss: 0.758380  [   31/   88]
per-ex loss: 0.762168  [   32/   88]
per-ex loss: 0.525464  [   33/   88]
per-ex loss: 0.600881  [   34/   88]
per-ex loss: 0.782031  [   35/   88]
per-ex loss: 0.810349  [   36/   88]
per-ex loss: 0.798468  [   37/   88]
per-ex loss: 0.722903  [   38/   88]
per-ex loss: 0.723345  [   39/   88]
per-ex loss: 0.819697  [   40/   88]
per-ex loss: 0.867426  [   41/   88]
per-ex loss: 0.761437  [   42/   88]
per-ex loss: 0.775744  [   43/   88]
per-ex loss: 0.649605  [   44/   88]
per-ex loss: 0.656541  [   45/   88]
per-ex loss: 0.593649  [   46/   88]
per-ex loss: 0.576783  [   47/   88]
per-ex loss: 0.559778  [   48/   88]
per-ex loss: 0.823929  [   49/   88]
per-ex loss: 0.776576  [   50/   88]
per-ex loss: 0.739709  [   51/   88]
per-ex loss: 0.787122  [   52/   88]
per-ex loss: 0.626606  [   53/   88]
per-ex loss: 0.531212  [   54/   88]
per-ex loss: 0.762582  [   55/   88]
per-ex loss: 0.592971  [   56/   88]
per-ex loss: 0.741956  [   57/   88]
per-ex loss: 0.659399  [   58/   88]
per-ex loss: 0.600071  [   59/   88]
per-ex loss: 0.833358  [   60/   88]
per-ex loss: 0.611816  [   61/   88]
per-ex loss: 0.833148  [   62/   88]
per-ex loss: 0.788092  [   63/   88]
per-ex loss: 0.555529  [   64/   88]
per-ex loss: 0.757130  [   65/   88]
per-ex loss: 0.789786  [   66/   88]
per-ex loss: 0.733382  [   67/   88]
per-ex loss: 0.626907  [   68/   88]
per-ex loss: 0.763759  [   69/   88]
per-ex loss: 0.714216  [   70/   88]
per-ex loss: 0.727861  [   71/   88]
per-ex loss: 0.584736  [   72/   88]
per-ex loss: 0.757622  [   73/   88]
per-ex loss: 0.753184  [   74/   88]
per-ex loss: 0.653663  [   75/   88]
per-ex loss: 0.593774  [   76/   88]
per-ex loss: 0.574823  [   77/   88]
per-ex loss: 0.555137  [   78/   88]
per-ex loss: 0.544245  [   79/   88]
per-ex loss: 0.649136  [   80/   88]
per-ex loss: 0.807252  [   81/   88]
per-ex loss: 0.722028  [   82/   88]
per-ex loss: 0.779232  [   83/   88]
per-ex loss: 0.659814  [   84/   88]
per-ex loss: 0.594012  [   85/   88]
per-ex loss: 0.682721  [   86/   88]
per-ex loss: 0.622335  [   87/   88]
per-ex loss: 0.623038  [   88/   88]
Train Error: Avg loss: 0.67635848
validation Error: 
 Avg loss: 0.74182041 
 F1: 0.424082 
 Precision: 0.426303 
 Recall: 0.421884
 IoU: 0.269101

test Error: 
 Avg loss: 0.66359512 
 F1: 0.550853 
 Precision: 0.571889 
 Recall: 0.531309
 IoU: 0.380122

We have finished training iteration 22
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_20_.pth
per-ex loss: 0.784993  [    1/   88]
per-ex loss: 0.515942  [    2/   88]
per-ex loss: 0.722369  [    3/   88]
per-ex loss: 0.598386  [    4/   88]
per-ex loss: 0.635257  [    5/   88]
per-ex loss: 0.728442  [    6/   88]
per-ex loss: 0.586168  [    7/   88]
per-ex loss: 0.858802  [    8/   88]
per-ex loss: 0.792871  [    9/   88]
per-ex loss: 0.581080  [   10/   88]
per-ex loss: 0.551321  [   11/   88]
per-ex loss: 0.663305  [   12/   88]
per-ex loss: 0.536772  [   13/   88]
per-ex loss: 0.813596  [   14/   88]
per-ex loss: 0.748176  [   15/   88]
per-ex loss: 0.794117  [   16/   88]
per-ex loss: 0.573466  [   17/   88]
per-ex loss: 0.778166  [   18/   88]
per-ex loss: 0.786144  [   19/   88]
per-ex loss: 0.550714  [   20/   88]
per-ex loss: 0.519422  [   21/   88]
per-ex loss: 0.558931  [   22/   88]
per-ex loss: 0.714532  [   23/   88]
per-ex loss: 0.779371  [   24/   88]
per-ex loss: 0.559846  [   25/   88]
per-ex loss: 0.631041  [   26/   88]
per-ex loss: 0.723898  [   27/   88]
per-ex loss: 0.639880  [   28/   88]
per-ex loss: 0.820116  [   29/   88]
per-ex loss: 0.550671  [   30/   88]
per-ex loss: 0.607040  [   31/   88]
per-ex loss: 0.779392  [   32/   88]
per-ex loss: 0.581134  [   33/   88]
per-ex loss: 0.538447  [   34/   88]
per-ex loss: 0.585841  [   35/   88]
per-ex loss: 0.636898  [   36/   88]
per-ex loss: 0.844009  [   37/   88]
per-ex loss: 0.767768  [   38/   88]
per-ex loss: 0.780804  [   39/   88]
per-ex loss: 0.720716  [   40/   88]
per-ex loss: 0.737594  [   41/   88]
per-ex loss: 0.772803  [   42/   88]
per-ex loss: 0.710815  [   43/   88]
per-ex loss: 0.702741  [   44/   88]
per-ex loss: 0.725271  [   45/   88]
per-ex loss: 0.619210  [   46/   88]
per-ex loss: 0.632253  [   47/   88]
per-ex loss: 0.652531  [   48/   88]
per-ex loss: 0.773818  [   49/   88]
per-ex loss: 0.673825  [   50/   88]
per-ex loss: 0.587056  [   51/   88]
per-ex loss: 0.784010  [   52/   88]
per-ex loss: 0.610734  [   53/   88]
per-ex loss: 0.594644  [   54/   88]
per-ex loss: 0.727259  [   55/   88]
per-ex loss: 0.616738  [   56/   88]
per-ex loss: 0.717228  [   57/   88]
per-ex loss: 0.632143  [   58/   88]
per-ex loss: 0.568591  [   59/   88]
per-ex loss: 0.518259  [   60/   88]
per-ex loss: 0.828726  [   61/   88]
per-ex loss: 0.593341  [   62/   88]
per-ex loss: 0.592657  [   63/   88]
per-ex loss: 0.824136  [   64/   88]
per-ex loss: 0.624195  [   65/   88]
per-ex loss: 0.605733  [   66/   88]
per-ex loss: 0.648492  [   67/   88]
per-ex loss: 0.819986  [   68/   88]
per-ex loss: 0.841575  [   69/   88]
per-ex loss: 0.548204  [   70/   88]
per-ex loss: 0.790946  [   71/   88]
per-ex loss: 0.508709  [   72/   88]
per-ex loss: 0.704372  [   73/   88]
per-ex loss: 0.798631  [   74/   88]
per-ex loss: 0.608023  [   75/   88]
per-ex loss: 0.585204  [   76/   88]
per-ex loss: 0.626188  [   77/   88]
per-ex loss: 0.733662  [   78/   88]
per-ex loss: 0.672548  [   79/   88]
per-ex loss: 0.774158  [   80/   88]
per-ex loss: 0.634968  [   81/   88]
per-ex loss: 0.610674  [   82/   88]
per-ex loss: 0.615397  [   83/   88]
per-ex loss: 0.757965  [   84/   88]
per-ex loss: 0.635878  [   85/   88]
per-ex loss: 0.736154  [   86/   88]
per-ex loss: 0.735486  [   87/   88]
per-ex loss: 0.644219  [   88/   88]
Train Error: Avg loss: 0.67497266
validation Error: 
 Avg loss: 0.70638858 
 F1: 0.476436 
 Precision: 0.526097 
 Recall: 0.435342
 IoU: 0.312711

test Error: 
 Avg loss: 0.65398498 
 F1: 0.562242 
 Precision: 0.590370 
 Recall: 0.536672
 IoU: 0.391054

We have finished training iteration 23
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_10_.pth
per-ex loss: 0.781702  [    1/   88]
per-ex loss: 0.732115  [    2/   88]
per-ex loss: 0.596033  [    3/   88]
per-ex loss: 0.790063  [    4/   88]
per-ex loss: 0.737647  [    5/   88]
per-ex loss: 0.636735  [    6/   88]
per-ex loss: 0.700100  [    7/   88]
per-ex loss: 0.807022  [    8/   88]
per-ex loss: 0.774225  [    9/   88]
per-ex loss: 0.573579  [   10/   88]
per-ex loss: 0.774530  [   11/   88]
per-ex loss: 0.604570  [   12/   88]
per-ex loss: 0.669676  [   13/   88]
per-ex loss: 0.613765  [   14/   88]
per-ex loss: 0.710369  [   15/   88]
per-ex loss: 0.556088  [   16/   88]
per-ex loss: 0.774752  [   17/   88]
per-ex loss: 0.598100  [   18/   88]
per-ex loss: 0.657347  [   19/   88]
per-ex loss: 0.707399  [   20/   88]
per-ex loss: 0.574750  [   21/   88]
per-ex loss: 0.632939  [   22/   88]
per-ex loss: 0.800273  [   23/   88]
per-ex loss: 0.609885  [   24/   88]
per-ex loss: 0.792866  [   25/   88]
per-ex loss: 0.821645  [   26/   88]
per-ex loss: 0.518130  [   27/   88]
per-ex loss: 0.610154  [   28/   88]
per-ex loss: 0.671642  [   29/   88]
per-ex loss: 0.781582  [   30/   88]
per-ex loss: 0.593748  [   31/   88]
per-ex loss: 0.581238  [   32/   88]
per-ex loss: 0.816122  [   33/   88]
per-ex loss: 0.534546  [   34/   88]
per-ex loss: 0.530917  [   35/   88]
per-ex loss: 0.817020  [   36/   88]
per-ex loss: 0.775036  [   37/   88]
per-ex loss: 0.553071  [   38/   88]
per-ex loss: 0.594355  [   39/   88]
per-ex loss: 0.612532  [   40/   88]
per-ex loss: 0.610535  [   41/   88]
per-ex loss: 0.660432  [   42/   88]
per-ex loss: 0.572757  [   43/   88]
per-ex loss: 0.607732  [   44/   88]
per-ex loss: 0.753721  [   45/   88]
per-ex loss: 0.746157  [   46/   88]
per-ex loss: 0.842967  [   47/   88]
per-ex loss: 0.559683  [   48/   88]
per-ex loss: 0.590454  [   49/   88]
per-ex loss: 0.635957  [   50/   88]
per-ex loss: 0.802916  [   51/   88]
per-ex loss: 0.540031  [   52/   88]
per-ex loss: 0.730746  [   53/   88]
per-ex loss: 0.529016  [   54/   88]
per-ex loss: 0.723171  [   55/   88]
per-ex loss: 0.739284  [   56/   88]
per-ex loss: 0.565439  [   57/   88]
per-ex loss: 0.809473  [   58/   88]
per-ex loss: 0.774043  [   59/   88]
per-ex loss: 0.671649  [   60/   88]
per-ex loss: 0.617267  [   61/   88]
per-ex loss: 0.632243  [   62/   88]
per-ex loss: 0.633651  [   63/   88]
per-ex loss: 0.584507  [   64/   88]
per-ex loss: 0.738753  [   65/   88]
per-ex loss: 0.788765  [   66/   88]
per-ex loss: 0.698305  [   67/   88]
per-ex loss: 0.631438  [   68/   88]
per-ex loss: 0.566768  [   69/   88]
per-ex loss: 0.792361  [   70/   88]
per-ex loss: 0.709221  [   71/   88]
per-ex loss: 0.723394  [   72/   88]
per-ex loss: 0.540990  [   73/   88]
per-ex loss: 0.688749  [   74/   88]
per-ex loss: 0.626190  [   75/   88]
per-ex loss: 0.639101  [   76/   88]
per-ex loss: 0.628523  [   77/   88]
per-ex loss: 0.533921  [   78/   88]
per-ex loss: 0.760062  [   79/   88]
per-ex loss: 0.572132  [   80/   88]
per-ex loss: 0.780231  [   81/   88]
per-ex loss: 0.645799  [   82/   88]
per-ex loss: 0.755586  [   83/   88]
per-ex loss: 0.771821  [   84/   88]
per-ex loss: 0.565433  [   85/   88]
per-ex loss: 0.551153  [   86/   88]
per-ex loss: 0.843864  [   87/   88]
per-ex loss: 0.700467  [   88/   88]
Train Error: Avg loss: 0.67278556
validation Error: 
 Avg loss: 0.75484314 
 F1: 0.407980 
 Precision: 0.564242 
 Recall: 0.319498
 IoU: 0.256266

test Error: 
 Avg loss: 0.68965003 
 F1: 0.510516 
 Precision: 0.718681 
 Recall: 0.395856
 IoU: 0.342747

We have finished training iteration 24
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_22_.pth
per-ex loss: 0.736577  [    1/   88]
per-ex loss: 0.773862  [    2/   88]
per-ex loss: 0.546910  [    3/   88]
per-ex loss: 0.805876  [    4/   88]
per-ex loss: 0.826350  [    5/   88]
per-ex loss: 0.680795  [    6/   88]
per-ex loss: 0.569968  [    7/   88]
per-ex loss: 0.577716  [    8/   88]
per-ex loss: 0.832384  [    9/   88]
per-ex loss: 0.805605  [   10/   88]
per-ex loss: 0.760295  [   11/   88]
per-ex loss: 0.624956  [   12/   88]
per-ex loss: 0.837941  [   13/   88]
per-ex loss: 0.825258  [   14/   88]
per-ex loss: 0.809441  [   15/   88]
per-ex loss: 0.647973  [   16/   88]
per-ex loss: 0.748256  [   17/   88]
per-ex loss: 0.792277  [   18/   88]
per-ex loss: 0.762516  [   19/   88]
per-ex loss: 0.658705  [   20/   88]
per-ex loss: 0.572084  [   21/   88]
per-ex loss: 0.711019  [   22/   88]
per-ex loss: 0.687950  [   23/   88]
per-ex loss: 0.712924  [   24/   88]
per-ex loss: 0.674784  [   25/   88]
per-ex loss: 0.695492  [   26/   88]
per-ex loss: 0.716572  [   27/   88]
per-ex loss: 0.752092  [   28/   88]
per-ex loss: 0.638774  [   29/   88]
per-ex loss: 0.560245  [   30/   88]
per-ex loss: 0.632157  [   31/   88]
per-ex loss: 0.596751  [   32/   88]
per-ex loss: 0.730933  [   33/   88]
per-ex loss: 0.740634  [   34/   88]
per-ex loss: 0.732754  [   35/   88]
per-ex loss: 0.546656  [   36/   88]
per-ex loss: 0.548698  [   37/   88]
per-ex loss: 0.554755  [   38/   88]
per-ex loss: 0.518509  [   39/   88]
per-ex loss: 0.588763  [   40/   88]
per-ex loss: 0.793013  [   41/   88]
per-ex loss: 0.565687  [   42/   88]
per-ex loss: 0.650385  [   43/   88]
per-ex loss: 0.597275  [   44/   88]
per-ex loss: 0.811362  [   45/   88]
per-ex loss: 0.601019  [   46/   88]
per-ex loss: 0.804545  [   47/   88]
per-ex loss: 0.601347  [   48/   88]
per-ex loss: 0.608433  [   49/   88]
per-ex loss: 0.604990  [   50/   88]
per-ex loss: 0.711999  [   51/   88]
per-ex loss: 0.525360  [   52/   88]
per-ex loss: 0.556059  [   53/   88]
per-ex loss: 0.780106  [   54/   88]
per-ex loss: 0.551855  [   55/   88]
per-ex loss: 0.556082  [   56/   88]
per-ex loss: 0.678342  [   57/   88]
per-ex loss: 0.563872  [   58/   88]
per-ex loss: 0.638675  [   59/   88]
per-ex loss: 0.790865  [   60/   88]
per-ex loss: 0.591240  [   61/   88]
per-ex loss: 0.587814  [   62/   88]
per-ex loss: 0.772619  [   63/   88]
per-ex loss: 0.825518  [   64/   88]
per-ex loss: 0.722650  [   65/   88]
per-ex loss: 0.571540  [   66/   88]
per-ex loss: 0.661479  [   67/   88]
per-ex loss: 0.726019  [   68/   88]
per-ex loss: 0.616069  [   69/   88]
per-ex loss: 0.506782  [   70/   88]
per-ex loss: 0.600423  [   71/   88]
per-ex loss: 0.597520  [   72/   88]
per-ex loss: 0.822789  [   73/   88]
per-ex loss: 0.641495  [   74/   88]
per-ex loss: 0.764146  [   75/   88]
per-ex loss: 0.628335  [   76/   88]
per-ex loss: 0.663948  [   77/   88]
per-ex loss: 0.708696  [   78/   88]
per-ex loss: 0.713017  [   79/   88]
per-ex loss: 0.740671  [   80/   88]
per-ex loss: 0.754854  [   81/   88]
per-ex loss: 0.695165  [   82/   88]
per-ex loss: 0.751359  [   83/   88]
per-ex loss: 0.596964  [   84/   88]
per-ex loss: 0.707267  [   85/   88]
per-ex loss: 0.812760  [   86/   88]
per-ex loss: 0.561067  [   87/   88]
per-ex loss: 0.543203  [   88/   88]
Train Error: Avg loss: 0.67514646
validation Error: 
 Avg loss: 0.73842320 
 F1: 0.434499 
 Precision: 0.437201 
 Recall: 0.431831
 IoU: 0.277547

test Error: 
 Avg loss: 0.67038768 
 F1: 0.540367 
 Precision: 0.539559 
 Recall: 0.541177
 IoU: 0.370207

We have finished training iteration 25
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_14_.pth
per-ex loss: 0.592178  [    1/   88]
per-ex loss: 0.543351  [    2/   88]
per-ex loss: 0.611191  [    3/   88]
per-ex loss: 0.618551  [    4/   88]
per-ex loss: 0.637619  [    5/   88]
per-ex loss: 0.555523  [    6/   88]
per-ex loss: 0.600123  [    7/   88]
per-ex loss: 0.780527  [    8/   88]
per-ex loss: 0.533979  [    9/   88]
per-ex loss: 0.769302  [   10/   88]
per-ex loss: 0.608718  [   11/   88]
per-ex loss: 0.511202  [   12/   88]
per-ex loss: 0.771104  [   13/   88]
per-ex loss: 0.834370  [   14/   88]
per-ex loss: 0.494788  [   15/   88]
per-ex loss: 0.514585  [   16/   88]
per-ex loss: 0.551660  [   17/   88]
per-ex loss: 0.550057  [   18/   88]
per-ex loss: 0.773814  [   19/   88]
per-ex loss: 0.765456  [   20/   88]
per-ex loss: 0.603882  [   21/   88]
per-ex loss: 0.756987  [   22/   88]
per-ex loss: 0.784446  [   23/   88]
per-ex loss: 0.593169  [   24/   88]
per-ex loss: 0.661114  [   25/   88]
per-ex loss: 0.702340  [   26/   88]
per-ex loss: 0.775260  [   27/   88]
per-ex loss: 0.705097  [   28/   88]
per-ex loss: 0.791890  [   29/   88]
per-ex loss: 0.626302  [   30/   88]
per-ex loss: 0.649442  [   31/   88]
per-ex loss: 0.590362  [   32/   88]
per-ex loss: 0.712201  [   33/   88]
per-ex loss: 0.700573  [   34/   88]
per-ex loss: 0.737689  [   35/   88]
per-ex loss: 0.590826  [   36/   88]
per-ex loss: 0.559590  [   37/   88]
per-ex loss: 0.803993  [   38/   88]
per-ex loss: 0.609487  [   39/   88]
per-ex loss: 0.657380  [   40/   88]
per-ex loss: 0.813139  [   41/   88]
per-ex loss: 0.739102  [   42/   88]
per-ex loss: 0.780309  [   43/   88]
per-ex loss: 0.584978  [   44/   88]
per-ex loss: 0.587798  [   45/   88]
per-ex loss: 0.810380  [   46/   88]
per-ex loss: 0.768755  [   47/   88]
per-ex loss: 0.577639  [   48/   88]
per-ex loss: 0.624548  [   49/   88]
per-ex loss: 0.589742  [   50/   88]
per-ex loss: 0.750660  [   51/   88]
per-ex loss: 0.736500  [   52/   88]
per-ex loss: 0.567348  [   53/   88]
per-ex loss: 0.578354  [   54/   88]
per-ex loss: 0.720881  [   55/   88]
per-ex loss: 0.724180  [   56/   88]
per-ex loss: 0.810004  [   57/   88]
per-ex loss: 0.777625  [   58/   88]
per-ex loss: 0.789069  [   59/   88]
per-ex loss: 0.789267  [   60/   88]
per-ex loss: 0.635063  [   61/   88]
per-ex loss: 0.679652  [   62/   88]
per-ex loss: 0.563808  [   63/   88]
per-ex loss: 0.560586  [   64/   88]
per-ex loss: 0.595765  [   65/   88]
per-ex loss: 0.732661  [   66/   88]
per-ex loss: 0.707923  [   67/   88]
per-ex loss: 0.633796  [   68/   88]
per-ex loss: 0.810949  [   69/   88]
per-ex loss: 0.530368  [   70/   88]
per-ex loss: 0.725152  [   71/   88]
per-ex loss: 0.567001  [   72/   88]
per-ex loss: 0.727500  [   73/   88]
per-ex loss: 0.562920  [   74/   88]
per-ex loss: 0.810782  [   75/   88]
per-ex loss: 0.769841  [   76/   88]
per-ex loss: 0.623661  [   77/   88]
per-ex loss: 0.612462  [   78/   88]
per-ex loss: 0.610263  [   79/   88]
per-ex loss: 0.676248  [   80/   88]
per-ex loss: 0.824916  [   81/   88]
per-ex loss: 0.735554  [   82/   88]
per-ex loss: 0.754965  [   83/   88]
per-ex loss: 0.667973  [   84/   88]
per-ex loss: 0.710037  [   85/   88]
per-ex loss: 0.639501  [   86/   88]
per-ex loss: 0.628640  [   87/   88]
per-ex loss: 0.658210  [   88/   88]
Train Error: Avg loss: 0.67166595
validation Error: 
 Avg loss: 0.73926466 
 F1: 0.429356 
 Precision: 0.412278 
 Recall: 0.447910
 IoU: 0.273363

test Error: 
 Avg loss: 0.65901896 
 F1: 0.550600 
 Precision: 0.547699 
 Recall: 0.553533
 IoU: 0.379882

We have finished training iteration 26
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_24_.pth
per-ex loss: 0.639795  [    1/   88]
per-ex loss: 0.742623  [    2/   88]
per-ex loss: 0.779332  [    3/   88]
per-ex loss: 0.760366  [    4/   88]
per-ex loss: 0.591747  [    5/   88]
per-ex loss: 0.787263  [    6/   88]
per-ex loss: 0.609741  [    7/   88]
per-ex loss: 0.696157  [    8/   88]
per-ex loss: 0.593560  [    9/   88]
per-ex loss: 0.632315  [   10/   88]
per-ex loss: 0.733197  [   11/   88]
per-ex loss: 0.600477  [   12/   88]
per-ex loss: 0.516593  [   13/   88]
per-ex loss: 0.763090  [   14/   88]
per-ex loss: 0.708222  [   15/   88]
per-ex loss: 0.775109  [   16/   88]
per-ex loss: 0.777710  [   17/   88]
per-ex loss: 0.777408  [   18/   88]
per-ex loss: 0.797475  [   19/   88]
per-ex loss: 0.584451  [   20/   88]
per-ex loss: 0.661772  [   21/   88]
per-ex loss: 0.738302  [   22/   88]
per-ex loss: 0.630164  [   23/   88]
per-ex loss: 0.637313  [   24/   88]
per-ex loss: 0.769665  [   25/   88]
per-ex loss: 0.563211  [   26/   88]
per-ex loss: 0.581130  [   27/   88]
per-ex loss: 0.698275  [   28/   88]
per-ex loss: 0.599933  [   29/   88]
per-ex loss: 0.577124  [   30/   88]
per-ex loss: 0.567911  [   31/   88]
per-ex loss: 0.586862  [   32/   88]
per-ex loss: 0.741083  [   33/   88]
per-ex loss: 0.820737  [   34/   88]
per-ex loss: 0.616817  [   35/   88]
per-ex loss: 0.583174  [   36/   88]
per-ex loss: 0.743476  [   37/   88]
per-ex loss: 0.783903  [   38/   88]
per-ex loss: 0.538804  [   39/   88]
per-ex loss: 0.743150  [   40/   88]
per-ex loss: 0.513791  [   41/   88]
per-ex loss: 0.656847  [   42/   88]
per-ex loss: 0.628578  [   43/   88]
per-ex loss: 0.564102  [   44/   88]
per-ex loss: 0.775080  [   45/   88]
per-ex loss: 0.554813  [   46/   88]
per-ex loss: 0.535979  [   47/   88]
per-ex loss: 0.705421  [   48/   88]
per-ex loss: 0.766474  [   49/   88]
per-ex loss: 0.702476  [   50/   88]
per-ex loss: 0.826210  [   51/   88]
per-ex loss: 0.816424  [   52/   88]
per-ex loss: 0.750676  [   53/   88]
per-ex loss: 0.669236  [   54/   88]
per-ex loss: 0.542140  [   55/   88]
per-ex loss: 0.676387  [   56/   88]
per-ex loss: 0.715886  [   57/   88]
per-ex loss: 0.570785  [   58/   88]
per-ex loss: 0.565939  [   59/   88]
per-ex loss: 0.634790  [   60/   88]
per-ex loss: 0.608114  [   61/   88]
per-ex loss: 0.842656  [   62/   88]
per-ex loss: 0.610887  [   63/   88]
per-ex loss: 0.776769  [   64/   88]
per-ex loss: 0.609376  [   65/   88]
per-ex loss: 0.579245  [   66/   88]
per-ex loss: 0.558058  [   67/   88]
per-ex loss: 0.777042  [   68/   88]
per-ex loss: 0.571014  [   69/   88]
per-ex loss: 0.829568  [   70/   88]
per-ex loss: 0.538680  [   71/   88]
per-ex loss: 0.591982  [   72/   88]
per-ex loss: 0.607663  [   73/   88]
per-ex loss: 0.728822  [   74/   88]
per-ex loss: 0.521903  [   75/   88]
per-ex loss: 0.777231  [   76/   88]
per-ex loss: 0.764377  [   77/   88]
per-ex loss: 0.588327  [   78/   88]
per-ex loss: 0.647258  [   79/   88]
per-ex loss: 0.678832  [   80/   88]
per-ex loss: 0.816032  [   81/   88]
per-ex loss: 0.791665  [   82/   88]
per-ex loss: 0.702760  [   83/   88]
per-ex loss: 0.720589  [   84/   88]
per-ex loss: 0.569520  [   85/   88]
per-ex loss: 0.775650  [   86/   88]
per-ex loss: 0.491163  [   87/   88]
per-ex loss: 0.617908  [   88/   88]
Train Error: Avg loss: 0.66948372
validation Error: 
 Avg loss: 0.72063167 
 F1: 0.459829 
 Precision: 0.514103 
 Recall: 0.415921
 IoU: 0.298557

test Error: 
 Avg loss: 0.65440032 
 F1: 0.558013 
 Precision: 0.639541 
 Recall: 0.494920
 IoU: 0.386975

We have finished training iteration 27
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_25_.pth
per-ex loss: 0.804689  [    1/   88]
per-ex loss: 0.771641  [    2/   88]
per-ex loss: 0.607309  [    3/   88]
per-ex loss: 0.658685  [    4/   88]
per-ex loss: 0.530469  [    5/   88]
per-ex loss: 0.624648  [    6/   88]
per-ex loss: 0.743300  [    7/   88]
per-ex loss: 0.631550  [    8/   88]
per-ex loss: 0.773167  [    9/   88]
per-ex loss: 0.705838  [   10/   88]
per-ex loss: 0.568486  [   11/   88]
per-ex loss: 0.560853  [   12/   88]
per-ex loss: 0.604388  [   13/   88]
per-ex loss: 0.764723  [   14/   88]
per-ex loss: 0.722653  [   15/   88]
per-ex loss: 0.561662  [   16/   88]
per-ex loss: 0.766676  [   17/   88]
per-ex loss: 0.597637  [   18/   88]
per-ex loss: 0.751989  [   19/   88]
per-ex loss: 0.576903  [   20/   88]
per-ex loss: 0.584902  [   21/   88]
per-ex loss: 0.592977  [   22/   88]
per-ex loss: 0.634745  [   23/   88]
per-ex loss: 0.834591  [   24/   88]
per-ex loss: 0.724839  [   25/   88]
per-ex loss: 0.605164  [   26/   88]
per-ex loss: 0.545314  [   27/   88]
per-ex loss: 0.799074  [   28/   88]
per-ex loss: 0.702659  [   29/   88]
per-ex loss: 0.621866  [   30/   88]
per-ex loss: 0.561083  [   31/   88]
per-ex loss: 0.620328  [   32/   88]
per-ex loss: 0.785712  [   33/   88]
per-ex loss: 0.553965  [   34/   88]
per-ex loss: 0.565339  [   35/   88]
per-ex loss: 0.557414  [   36/   88]
per-ex loss: 0.513565  [   37/   88]
per-ex loss: 0.631072  [   38/   88]
per-ex loss: 0.723619  [   39/   88]
per-ex loss: 0.715241  [   40/   88]
per-ex loss: 0.781489  [   41/   88]
per-ex loss: 0.564822  [   42/   88]
per-ex loss: 0.582218  [   43/   88]
per-ex loss: 0.725058  [   44/   88]
per-ex loss: 0.803305  [   45/   88]
per-ex loss: 0.479594  [   46/   88]
per-ex loss: 0.817612  [   47/   88]
per-ex loss: 0.739454  [   48/   88]
per-ex loss: 0.816016  [   49/   88]
per-ex loss: 0.638713  [   50/   88]
per-ex loss: 0.848466  [   51/   88]
per-ex loss: 0.585502  [   52/   88]
per-ex loss: 0.763452  [   53/   88]
per-ex loss: 0.774670  [   54/   88]
per-ex loss: 0.646752  [   55/   88]
per-ex loss: 0.793179  [   56/   88]
per-ex loss: 0.768456  [   57/   88]
per-ex loss: 0.771859  [   58/   88]
per-ex loss: 0.638669  [   59/   88]
per-ex loss: 0.655820  [   60/   88]
per-ex loss: 0.592157  [   61/   88]
per-ex loss: 0.744406  [   62/   88]
per-ex loss: 0.712263  [   63/   88]
per-ex loss: 0.693033  [   64/   88]
per-ex loss: 0.629217  [   65/   88]
per-ex loss: 0.661364  [   66/   88]
per-ex loss: 0.605511  [   67/   88]
per-ex loss: 0.576704  [   68/   88]
per-ex loss: 0.778306  [   69/   88]
per-ex loss: 0.535302  [   70/   88]
per-ex loss: 0.636479  [   71/   88]
per-ex loss: 0.656464  [   72/   88]
per-ex loss: 0.620900  [   73/   88]
per-ex loss: 0.814866  [   74/   88]
per-ex loss: 0.590133  [   75/   88]
per-ex loss: 0.732058  [   76/   88]
per-ex loss: 0.631726  [   77/   88]
per-ex loss: 0.714658  [   78/   88]
per-ex loss: 0.554832  [   79/   88]
per-ex loss: 0.767620  [   80/   88]
per-ex loss: 0.551354  [   81/   88]
per-ex loss: 0.740052  [   82/   88]
per-ex loss: 0.608676  [   83/   88]
per-ex loss: 0.745879  [   84/   88]
per-ex loss: 0.517936  [   85/   88]
per-ex loss: 0.649089  [   86/   88]
per-ex loss: 0.826345  [   87/   88]
per-ex loss: 0.830520  [   88/   88]
Train Error: Avg loss: 0.67174654
validation Error: 
 Avg loss: 0.71434081 
 F1: 0.467218 
 Precision: 0.509201 
 Recall: 0.431631
 IoU: 0.304817

test Error: 
 Avg loss: 0.65542689 
 F1: 0.559326 
 Precision: 0.600474 
 Recall: 0.523455
 IoU: 0.388239

We have finished training iteration 28
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_26_.pth
per-ex loss: 0.778587  [    1/   88]
per-ex loss: 0.675176  [    2/   88]
per-ex loss: 0.546760  [    3/   88]
per-ex loss: 0.784352  [    4/   88]
per-ex loss: 0.672734  [    5/   88]
per-ex loss: 0.604434  [    6/   88]
per-ex loss: 0.706574  [    7/   88]
per-ex loss: 0.730584  [    8/   88]
per-ex loss: 0.723876  [    9/   88]
per-ex loss: 0.572644  [   10/   88]
per-ex loss: 0.813765  [   11/   88]
per-ex loss: 0.583133  [   12/   88]
per-ex loss: 0.819992  [   13/   88]
per-ex loss: 0.582963  [   14/   88]
per-ex loss: 0.551566  [   15/   88]
per-ex loss: 0.800237  [   16/   88]
per-ex loss: 0.536223  [   17/   88]
per-ex loss: 0.762471  [   18/   88]
per-ex loss: 0.569640  [   19/   88]
per-ex loss: 0.571633  [   20/   88]
per-ex loss: 0.767009  [   21/   88]
per-ex loss: 0.752494  [   22/   88]
per-ex loss: 0.687054  [   23/   88]
per-ex loss: 0.710786  [   24/   88]
per-ex loss: 0.554954  [   25/   88]
per-ex loss: 0.548384  [   26/   88]
per-ex loss: 0.566753  [   27/   88]
per-ex loss: 0.631821  [   28/   88]
per-ex loss: 0.590880  [   29/   88]
per-ex loss: 0.591950  [   30/   88]
per-ex loss: 0.589255  [   31/   88]
per-ex loss: 0.814427  [   32/   88]
per-ex loss: 0.635650  [   33/   88]
per-ex loss: 0.806362  [   34/   88]
per-ex loss: 0.710122  [   35/   88]
per-ex loss: 0.707203  [   36/   88]
per-ex loss: 0.533156  [   37/   88]
per-ex loss: 0.796038  [   38/   88]
per-ex loss: 0.611297  [   39/   88]
per-ex loss: 0.541162  [   40/   88]
per-ex loss: 0.549096  [   41/   88]
per-ex loss: 0.593503  [   42/   88]
per-ex loss: 0.477928  [   43/   88]
per-ex loss: 0.775011  [   44/   88]
per-ex loss: 0.767710  [   45/   88]
per-ex loss: 0.781627  [   46/   88]
per-ex loss: 0.835034  [   47/   88]
per-ex loss: 0.602838  [   48/   88]
per-ex loss: 0.627930  [   49/   88]
per-ex loss: 0.521715  [   50/   88]
per-ex loss: 0.587040  [   51/   88]
per-ex loss: 0.595215  [   52/   88]
per-ex loss: 0.834507  [   53/   88]
per-ex loss: 0.507931  [   54/   88]
per-ex loss: 0.769483  [   55/   88]
per-ex loss: 0.694881  [   56/   88]
per-ex loss: 0.607058  [   57/   88]
per-ex loss: 0.754736  [   58/   88]
per-ex loss: 0.613866  [   59/   88]
per-ex loss: 0.598429  [   60/   88]
per-ex loss: 0.636573  [   61/   88]
per-ex loss: 0.652804  [   62/   88]
per-ex loss: 0.633008  [   63/   88]
per-ex loss: 0.643312  [   64/   88]
per-ex loss: 0.825348  [   65/   88]
per-ex loss: 0.610899  [   66/   88]
per-ex loss: 0.718591  [   67/   88]
per-ex loss: 0.654587  [   68/   88]
per-ex loss: 0.730958  [   69/   88]
per-ex loss: 0.762963  [   70/   88]
per-ex loss: 0.638188  [   71/   88]
per-ex loss: 0.752681  [   72/   88]
per-ex loss: 0.549563  [   73/   88]
per-ex loss: 0.747091  [   74/   88]
per-ex loss: 0.748512  [   75/   88]
per-ex loss: 0.803686  [   76/   88]
per-ex loss: 0.681681  [   77/   88]
per-ex loss: 0.777117  [   78/   88]
per-ex loss: 0.697544  [   79/   88]
per-ex loss: 0.594230  [   80/   88]
per-ex loss: 0.535174  [   81/   88]
per-ex loss: 0.698268  [   82/   88]
per-ex loss: 0.721329  [   83/   88]
per-ex loss: 0.597159  [   84/   88]
per-ex loss: 0.607380  [   85/   88]
per-ex loss: 0.617243  [   86/   88]
per-ex loss: 0.792056  [   87/   88]
per-ex loss: 0.582939  [   88/   88]
Train Error: Avg loss: 0.66636957
validation Error: 
 Avg loss: 0.70098334 
 F1: 0.486574 
 Precision: 0.568187 
 Recall: 0.425462
 IoU: 0.321505

test Error: 
 Avg loss: 0.64908653 
 F1: 0.565220 
 Precision: 0.632245 
 Recall: 0.511043
 IoU: 0.393941

We have finished training iteration 29
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_27_.pth
per-ex loss: 0.620110  [    1/   88]
per-ex loss: 0.595939  [    2/   88]
per-ex loss: 0.753825  [    3/   88]
per-ex loss: 0.611548  [    4/   88]
per-ex loss: 0.637673  [    5/   88]
per-ex loss: 0.652631  [    6/   88]
per-ex loss: 0.561467  [    7/   88]
per-ex loss: 0.632625  [    8/   88]
per-ex loss: 0.734671  [    9/   88]
per-ex loss: 0.751406  [   10/   88]
per-ex loss: 0.808909  [   11/   88]
per-ex loss: 0.565221  [   12/   88]
per-ex loss: 0.597760  [   13/   88]
per-ex loss: 0.722686  [   14/   88]
per-ex loss: 0.786516  [   15/   88]
per-ex loss: 0.734929  [   16/   88]
per-ex loss: 0.573280  [   17/   88]
per-ex loss: 0.739184  [   18/   88]
per-ex loss: 0.560446  [   19/   88]
per-ex loss: 0.843950  [   20/   88]
per-ex loss: 0.576242  [   21/   88]
per-ex loss: 0.554954  [   22/   88]
per-ex loss: 0.772113  [   23/   88]
per-ex loss: 0.719181  [   24/   88]
per-ex loss: 0.831274  [   25/   88]
per-ex loss: 0.702108  [   26/   88]
per-ex loss: 0.698450  [   27/   88]
per-ex loss: 0.832429  [   28/   88]
per-ex loss: 0.624375  [   29/   88]
per-ex loss: 0.783592  [   30/   88]
per-ex loss: 0.526024  [   31/   88]
per-ex loss: 0.765747  [   32/   88]
per-ex loss: 0.586753  [   33/   88]
per-ex loss: 0.724586  [   34/   88]
per-ex loss: 0.579950  [   35/   88]
per-ex loss: 0.751863  [   36/   88]
per-ex loss: 0.606818  [   37/   88]
per-ex loss: 0.775420  [   38/   88]
per-ex loss: 0.788485  [   39/   88]
per-ex loss: 0.620128  [   40/   88]
per-ex loss: 0.635427  [   41/   88]
per-ex loss: 0.807846  [   42/   88]
per-ex loss: 0.762350  [   43/   88]
per-ex loss: 0.599035  [   44/   88]
per-ex loss: 0.602024  [   45/   88]
per-ex loss: 0.559363  [   46/   88]
per-ex loss: 0.554481  [   47/   88]
per-ex loss: 0.551988  [   48/   88]
per-ex loss: 0.569093  [   49/   88]
per-ex loss: 0.609854  [   50/   88]
per-ex loss: 0.541354  [   51/   88]
per-ex loss: 0.619051  [   52/   88]
per-ex loss: 0.761468  [   53/   88]
per-ex loss: 0.607924  [   54/   88]
per-ex loss: 0.728377  [   55/   88]
per-ex loss: 0.778231  [   56/   88]
per-ex loss: 0.587797  [   57/   88]
per-ex loss: 0.694728  [   58/   88]
per-ex loss: 0.658438  [   59/   88]
per-ex loss: 0.645474  [   60/   88]
per-ex loss: 0.546825  [   61/   88]
per-ex loss: 0.618753  [   62/   88]
per-ex loss: 0.799128  [   63/   88]
per-ex loss: 0.599737  [   64/   88]
per-ex loss: 0.670839  [   65/   88]
per-ex loss: 0.558381  [   66/   88]
per-ex loss: 0.769201  [   67/   88]
per-ex loss: 0.486943  [   68/   88]
per-ex loss: 0.812439  [   69/   88]
per-ex loss: 0.761661  [   70/   88]
per-ex loss: 0.556086  [   71/   88]
per-ex loss: 0.628790  [   72/   88]
per-ex loss: 0.745452  [   73/   88]
per-ex loss: 0.618082  [   74/   88]
per-ex loss: 0.597725  [   75/   88]
per-ex loss: 0.604942  [   76/   88]
per-ex loss: 0.768721  [   77/   88]
per-ex loss: 0.751093  [   78/   88]
per-ex loss: 0.553820  [   79/   88]
per-ex loss: 0.546017  [   80/   88]
per-ex loss: 0.688304  [   81/   88]
per-ex loss: 0.568084  [   82/   88]
per-ex loss: 0.702802  [   83/   88]
per-ex loss: 0.809067  [   84/   88]
per-ex loss: 0.483426  [   85/   88]
per-ex loss: 0.794971  [   86/   88]
per-ex loss: 0.707201  [   87/   88]
per-ex loss: 0.674000  [   88/   88]
Train Error: Avg loss: 0.66593251
validation Error: 
 Avg loss: 0.71035819 
 F1: 0.473530 
 Precision: 0.529437 
 Recall: 0.428302
 IoU: 0.310212

test Error: 
 Avg loss: 0.64824825 
 F1: 0.566901 
 Precision: 0.621148 
 Recall: 0.521368
 IoU: 0.395577

We have finished training iteration 30
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_28_.pth
per-ex loss: 0.550748  [    1/   88]
per-ex loss: 0.544769  [    2/   88]
per-ex loss: 0.706068  [    3/   88]
per-ex loss: 0.623618  [    4/   88]
per-ex loss: 0.689326  [    5/   88]
per-ex loss: 0.796978  [    6/   88]
per-ex loss: 0.747290  [    7/   88]
per-ex loss: 0.614643  [    8/   88]
per-ex loss: 0.700213  [    9/   88]
per-ex loss: 0.682991  [   10/   88]
per-ex loss: 0.600554  [   11/   88]
per-ex loss: 0.565637  [   12/   88]
per-ex loss: 0.755386  [   13/   88]
per-ex loss: 0.767049  [   14/   88]
per-ex loss: 0.615896  [   15/   88]
per-ex loss: 0.723063  [   16/   88]
per-ex loss: 0.745317  [   17/   88]
per-ex loss: 0.773862  [   18/   88]
per-ex loss: 0.818807  [   19/   88]
per-ex loss: 0.565113  [   20/   88]
per-ex loss: 0.558178  [   21/   88]
per-ex loss: 0.490002  [   22/   88]
per-ex loss: 0.742630  [   23/   88]
per-ex loss: 0.806425  [   24/   88]
per-ex loss: 0.631855  [   25/   88]
per-ex loss: 0.782633  [   26/   88]
per-ex loss: 0.748868  [   27/   88]
per-ex loss: 0.768757  [   28/   88]
per-ex loss: 0.670305  [   29/   88]
per-ex loss: 0.556971  [   30/   88]
per-ex loss: 0.717684  [   31/   88]
per-ex loss: 0.540948  [   32/   88]
per-ex loss: 0.635747  [   33/   88]
per-ex loss: 0.540691  [   34/   88]
per-ex loss: 0.758118  [   35/   88]
per-ex loss: 0.796108  [   36/   88]
per-ex loss: 0.647167  [   37/   88]
per-ex loss: 0.725426  [   38/   88]
per-ex loss: 0.543397  [   39/   88]
per-ex loss: 0.803556  [   40/   88]
per-ex loss: 0.538678  [   41/   88]
per-ex loss: 0.746669  [   42/   88]
per-ex loss: 0.734261  [   43/   88]
per-ex loss: 0.722909  [   44/   88]
per-ex loss: 0.765341  [   45/   88]
per-ex loss: 0.749157  [   46/   88]
per-ex loss: 0.793178  [   47/   88]
per-ex loss: 0.586024  [   48/   88]
per-ex loss: 0.633225  [   49/   88]
per-ex loss: 0.810524  [   50/   88]
per-ex loss: 0.771918  [   51/   88]
per-ex loss: 0.661411  [   52/   88]
per-ex loss: 0.567899  [   53/   88]
per-ex loss: 0.599091  [   54/   88]
per-ex loss: 0.631973  [   55/   88]
per-ex loss: 0.697102  [   56/   88]
per-ex loss: 0.579377  [   57/   88]
per-ex loss: 0.576361  [   58/   88]
per-ex loss: 0.733071  [   59/   88]
per-ex loss: 0.584057  [   60/   88]
per-ex loss: 0.780194  [   61/   88]
per-ex loss: 0.662964  [   62/   88]
per-ex loss: 0.645820  [   63/   88]
per-ex loss: 0.606002  [   64/   88]
per-ex loss: 0.532620  [   65/   88]
per-ex loss: 0.582878  [   66/   88]
per-ex loss: 0.769276  [   67/   88]
per-ex loss: 0.819515  [   68/   88]
per-ex loss: 0.615941  [   69/   88]
per-ex loss: 0.726324  [   70/   88]
per-ex loss: 0.846864  [   71/   88]
per-ex loss: 0.500219  [   72/   88]
per-ex loss: 0.785273  [   73/   88]
per-ex loss: 0.674662  [   74/   88]
per-ex loss: 0.644682  [   75/   88]
per-ex loss: 0.604082  [   76/   88]
per-ex loss: 0.549334  [   77/   88]
per-ex loss: 0.593697  [   78/   88]
per-ex loss: 0.605680  [   79/   88]
per-ex loss: 0.599138  [   80/   88]
per-ex loss: 0.535993  [   81/   88]
per-ex loss: 0.737243  [   82/   88]
per-ex loss: 0.793570  [   83/   88]
per-ex loss: 0.573017  [   84/   88]
per-ex loss: 0.543295  [   85/   88]
per-ex loss: 0.639195  [   86/   88]
per-ex loss: 0.616462  [   87/   88]
per-ex loss: 0.532075  [   88/   88]
Train Error: Avg loss: 0.66648908
validation Error: 
 Avg loss: 0.72254077 
 F1: 0.457604 
 Precision: 0.543285 
 Recall: 0.395267
 IoU: 0.296684

test Error: 
 Avg loss: 0.65908946 
 F1: 0.552407 
 Precision: 0.652084 
 Recall: 0.479163
 IoU: 0.381604

We have finished training iteration 31
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_23_.pth
per-ex loss: 0.739450  [    1/   88]
per-ex loss: 0.721402  [    2/   88]
per-ex loss: 0.805717  [    3/   88]
per-ex loss: 0.826727  [    4/   88]
per-ex loss: 0.530274  [    5/   88]
per-ex loss: 0.589615  [    6/   88]
per-ex loss: 0.536781  [    7/   88]
per-ex loss: 0.609724  [    8/   88]
per-ex loss: 0.544299  [    9/   88]
per-ex loss: 0.508762  [   10/   88]
per-ex loss: 0.577271  [   11/   88]
per-ex loss: 0.558542  [   12/   88]
per-ex loss: 0.753759  [   13/   88]
per-ex loss: 0.581499  [   14/   88]
per-ex loss: 0.630324  [   15/   88]
per-ex loss: 0.734786  [   16/   88]
per-ex loss: 0.570068  [   17/   88]
per-ex loss: 0.727972  [   18/   88]
per-ex loss: 0.814399  [   19/   88]
per-ex loss: 0.715030  [   20/   88]
per-ex loss: 0.732361  [   21/   88]
per-ex loss: 0.781427  [   22/   88]
per-ex loss: 0.749979  [   23/   88]
per-ex loss: 0.728234  [   24/   88]
per-ex loss: 0.795940  [   25/   88]
per-ex loss: 0.746991  [   26/   88]
per-ex loss: 0.797022  [   27/   88]
per-ex loss: 0.730969  [   28/   88]
per-ex loss: 0.815384  [   29/   88]
per-ex loss: 0.537442  [   30/   88]
per-ex loss: 0.748877  [   31/   88]
per-ex loss: 0.643417  [   32/   88]
per-ex loss: 0.737011  [   33/   88]
per-ex loss: 0.753497  [   34/   88]
per-ex loss: 0.552401  [   35/   88]
per-ex loss: 0.581191  [   36/   88]
per-ex loss: 0.708098  [   37/   88]
per-ex loss: 0.716194  [   38/   88]
per-ex loss: 0.500154  [   39/   88]
per-ex loss: 0.583038  [   40/   88]
per-ex loss: 0.604492  [   41/   88]
per-ex loss: 0.580784  [   42/   88]
per-ex loss: 0.725727  [   43/   88]
per-ex loss: 0.589018  [   44/   88]
per-ex loss: 0.833290  [   45/   88]
per-ex loss: 0.785832  [   46/   88]
per-ex loss: 0.716092  [   47/   88]
per-ex loss: 0.588055  [   48/   88]
per-ex loss: 0.772870  [   49/   88]
per-ex loss: 0.599162  [   50/   88]
per-ex loss: 0.782191  [   51/   88]
per-ex loss: 0.556149  [   52/   88]
per-ex loss: 0.515905  [   53/   88]
per-ex loss: 0.608899  [   54/   88]
per-ex loss: 0.806876  [   55/   88]
per-ex loss: 0.607186  [   56/   88]
per-ex loss: 0.794423  [   57/   88]
per-ex loss: 0.701351  [   58/   88]
per-ex loss: 0.755581  [   59/   88]
per-ex loss: 0.570969  [   60/   88]
per-ex loss: 0.629845  [   61/   88]
per-ex loss: 0.507064  [   62/   88]
per-ex loss: 0.645923  [   63/   88]
per-ex loss: 0.544739  [   64/   88]
per-ex loss: 0.769351  [   65/   88]
per-ex loss: 0.557752  [   66/   88]
per-ex loss: 0.615212  [   67/   88]
per-ex loss: 0.611537  [   68/   88]
per-ex loss: 0.687675  [   69/   88]
per-ex loss: 0.624760  [   70/   88]
per-ex loss: 0.642703  [   71/   88]
per-ex loss: 0.590797  [   72/   88]
per-ex loss: 0.602777  [   73/   88]
per-ex loss: 0.584256  [   74/   88]
per-ex loss: 0.797991  [   75/   88]
per-ex loss: 0.594190  [   76/   88]
per-ex loss: 0.585665  [   77/   88]
per-ex loss: 0.518814  [   78/   88]
per-ex loss: 0.701594  [   79/   88]
per-ex loss: 0.540029  [   80/   88]
per-ex loss: 0.761566  [   81/   88]
per-ex loss: 0.685938  [   82/   88]
per-ex loss: 0.511962  [   83/   88]
per-ex loss: 0.737487  [   84/   88]
per-ex loss: 0.589283  [   85/   88]
per-ex loss: 0.762950  [   86/   88]
per-ex loss: 0.594146  [   87/   88]
per-ex loss: 0.807267  [   88/   88]
Train Error: Avg loss: 0.66266080
validation Error: 
 Avg loss: 0.73671937 
 F1: 0.433335 
 Precision: 0.561971 
 Recall: 0.352619
 IoU: 0.276597

test Error: 
 Avg loss: 0.66589863 
 F1: 0.540908 
 Precision: 0.704420 
 Recall: 0.439005
 IoU: 0.370715

We have finished training iteration 32
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_30_.pth
per-ex loss: 0.838070  [    1/   88]
per-ex loss: 0.605572  [    2/   88]
per-ex loss: 0.554169  [    3/   88]
per-ex loss: 0.766985  [    4/   88]
per-ex loss: 0.599330  [    5/   88]
per-ex loss: 0.636925  [    6/   88]
per-ex loss: 0.550002  [    7/   88]
per-ex loss: 0.763804  [    8/   88]
per-ex loss: 0.608491  [    9/   88]
per-ex loss: 0.693635  [   10/   88]
per-ex loss: 0.576670  [   11/   88]
per-ex loss: 0.664450  [   12/   88]
per-ex loss: 0.748049  [   13/   88]
per-ex loss: 0.688816  [   14/   88]
per-ex loss: 0.484440  [   15/   88]
per-ex loss: 0.539235  [   16/   88]
per-ex loss: 0.779959  [   17/   88]
per-ex loss: 0.741252  [   18/   88]
per-ex loss: 0.697874  [   19/   88]
per-ex loss: 0.755753  [   20/   88]
per-ex loss: 0.632503  [   21/   88]
per-ex loss: 0.579756  [   22/   88]
per-ex loss: 0.628521  [   23/   88]
per-ex loss: 0.695224  [   24/   88]
per-ex loss: 0.797823  [   25/   88]
per-ex loss: 0.564277  [   26/   88]
per-ex loss: 0.715553  [   27/   88]
per-ex loss: 0.717784  [   28/   88]
per-ex loss: 0.756390  [   29/   88]
per-ex loss: 0.581374  [   30/   88]
per-ex loss: 0.761641  [   31/   88]
per-ex loss: 0.596686  [   32/   88]
per-ex loss: 0.796928  [   33/   88]
per-ex loss: 0.577924  [   34/   88]
per-ex loss: 0.817147  [   35/   88]
per-ex loss: 0.680788  [   36/   88]
per-ex loss: 0.800672  [   37/   88]
per-ex loss: 0.631401  [   38/   88]
per-ex loss: 0.612539  [   39/   88]
per-ex loss: 0.743703  [   40/   88]
per-ex loss: 0.684277  [   41/   88]
per-ex loss: 0.823645  [   42/   88]
per-ex loss: 0.698468  [   43/   88]
per-ex loss: 0.507610  [   44/   88]
per-ex loss: 0.633792  [   45/   88]
per-ex loss: 0.548412  [   46/   88]
per-ex loss: 0.549203  [   47/   88]
per-ex loss: 0.529231  [   48/   88]
per-ex loss: 0.704362  [   49/   88]
per-ex loss: 0.521217  [   50/   88]
per-ex loss: 0.758790  [   51/   88]
per-ex loss: 0.750817  [   52/   88]
per-ex loss: 0.625530  [   53/   88]
per-ex loss: 0.813359  [   54/   88]
per-ex loss: 0.526641  [   55/   88]
per-ex loss: 0.574658  [   56/   88]
per-ex loss: 0.772701  [   57/   88]
per-ex loss: 0.802163  [   58/   88]
per-ex loss: 0.768327  [   59/   88]
per-ex loss: 0.628374  [   60/   88]
per-ex loss: 0.599130  [   61/   88]
per-ex loss: 0.559184  [   62/   88]
per-ex loss: 0.767054  [   63/   88]
per-ex loss: 0.829902  [   64/   88]
per-ex loss: 0.550799  [   65/   88]
per-ex loss: 0.599214  [   66/   88]
per-ex loss: 0.551584  [   67/   88]
per-ex loss: 0.624405  [   68/   88]
per-ex loss: 0.545438  [   69/   88]
per-ex loss: 0.566330  [   70/   88]
per-ex loss: 0.805119  [   71/   88]
per-ex loss: 0.634949  [   72/   88]
per-ex loss: 0.553497  [   73/   88]
per-ex loss: 0.566770  [   74/   88]
per-ex loss: 0.769422  [   75/   88]
per-ex loss: 0.731496  [   76/   88]
per-ex loss: 0.550456  [   77/   88]
per-ex loss: 0.790694  [   78/   88]
per-ex loss: 0.699936  [   79/   88]
per-ex loss: 0.608121  [   80/   88]
per-ex loss: 0.601334  [   81/   88]
per-ex loss: 0.583991  [   82/   88]
per-ex loss: 0.543268  [   83/   88]
per-ex loss: 0.775727  [   84/   88]
per-ex loss: 0.584138  [   85/   88]
per-ex loss: 0.779770  [   86/   88]
per-ex loss: 0.596245  [   87/   88]
per-ex loss: 0.807840  [   88/   88]
Train Error: Avg loss: 0.66340346
validation Error: 
 Avg loss: 0.71507014 
 F1: 0.462590 
 Precision: 0.457511 
 Recall: 0.467783
 IoU: 0.300889

test Error: 
 Avg loss: 0.65295020 
 F1: 0.561766 
 Precision: 0.555887 
 Recall: 0.567772
 IoU: 0.390595

We have finished training iteration 33
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_31_.pth
per-ex loss: 0.544244  [    1/   88]
per-ex loss: 0.708055  [    2/   88]
per-ex loss: 0.550245  [    3/   88]
per-ex loss: 0.552679  [    4/   88]
per-ex loss: 0.614865  [    5/   88]
per-ex loss: 0.545878  [    6/   88]
per-ex loss: 0.515557  [    7/   88]
per-ex loss: 0.778640  [    8/   88]
per-ex loss: 0.713675  [    9/   88]
per-ex loss: 0.549604  [   10/   88]
per-ex loss: 0.832198  [   11/   88]
per-ex loss: 0.613017  [   12/   88]
per-ex loss: 0.583677  [   13/   88]
per-ex loss: 0.835611  [   14/   88]
per-ex loss: 0.804780  [   15/   88]
per-ex loss: 0.804922  [   16/   88]
per-ex loss: 0.612144  [   17/   88]
per-ex loss: 0.561252  [   18/   88]
per-ex loss: 0.740437  [   19/   88]
per-ex loss: 0.502898  [   20/   88]
per-ex loss: 0.739848  [   21/   88]
per-ex loss: 0.743068  [   22/   88]
per-ex loss: 0.629115  [   23/   88]
per-ex loss: 0.834227  [   24/   88]
per-ex loss: 0.627158  [   25/   88]
per-ex loss: 0.631112  [   26/   88]
per-ex loss: 0.640351  [   27/   88]
per-ex loss: 0.611284  [   28/   88]
per-ex loss: 0.598523  [   29/   88]
per-ex loss: 0.698235  [   30/   88]
per-ex loss: 0.620746  [   31/   88]
per-ex loss: 0.747272  [   32/   88]
per-ex loss: 0.576348  [   33/   88]
per-ex loss: 0.533239  [   34/   88]
per-ex loss: 0.746049  [   35/   88]
per-ex loss: 0.579980  [   36/   88]
per-ex loss: 0.592579  [   37/   88]
per-ex loss: 0.755030  [   38/   88]
per-ex loss: 0.757290  [   39/   88]
per-ex loss: 0.606723  [   40/   88]
per-ex loss: 0.798741  [   41/   88]
per-ex loss: 0.727341  [   42/   88]
per-ex loss: 0.726196  [   43/   88]
per-ex loss: 0.649912  [   44/   88]
per-ex loss: 0.625210  [   45/   88]
per-ex loss: 0.690695  [   46/   88]
per-ex loss: 0.803524  [   47/   88]
per-ex loss: 0.598690  [   48/   88]
per-ex loss: 0.692522  [   49/   88]
per-ex loss: 0.661031  [   50/   88]
per-ex loss: 0.752480  [   51/   88]
per-ex loss: 0.595767  [   52/   88]
per-ex loss: 0.614772  [   53/   88]
per-ex loss: 0.579415  [   54/   88]
per-ex loss: 0.527165  [   55/   88]
per-ex loss: 0.794376  [   56/   88]
per-ex loss: 0.638243  [   57/   88]
per-ex loss: 0.782041  [   58/   88]
per-ex loss: 0.604646  [   59/   88]
per-ex loss: 0.627459  [   60/   88]
per-ex loss: 0.534770  [   61/   88]
per-ex loss: 0.736523  [   62/   88]
per-ex loss: 0.732662  [   63/   88]
per-ex loss: 0.569518  [   64/   88]
per-ex loss: 0.775684  [   65/   88]
per-ex loss: 0.582683  [   66/   88]
per-ex loss: 0.700941  [   67/   88]
per-ex loss: 0.567057  [   68/   88]
per-ex loss: 0.688780  [   69/   88]
per-ex loss: 0.746833  [   70/   88]
per-ex loss: 0.783394  [   71/   88]
per-ex loss: 0.584856  [   72/   88]
per-ex loss: 0.757284  [   73/   88]
per-ex loss: 0.783570  [   74/   88]
per-ex loss: 0.708016  [   75/   88]
per-ex loss: 0.543060  [   76/   88]
per-ex loss: 0.619514  [   77/   88]
per-ex loss: 0.554059  [   78/   88]
per-ex loss: 0.532931  [   79/   88]
per-ex loss: 0.702368  [   80/   88]
per-ex loss: 0.736451  [   81/   88]
per-ex loss: 0.621352  [   82/   88]
per-ex loss: 0.691732  [   83/   88]
per-ex loss: 0.639225  [   84/   88]
per-ex loss: 0.821086  [   85/   88]
per-ex loss: 0.816393  [   86/   88]
per-ex loss: 0.522694  [   87/   88]
per-ex loss: 0.559374  [   88/   88]
Train Error: Avg loss: 0.66290442
validation Error: 
 Avg loss: 0.71416147 
 F1: 0.467238 
 Precision: 0.549404 
 Recall: 0.406451
 IoU: 0.304834

test Error: 
 Avg loss: 0.65241219 
 F1: 0.562541 
 Precision: 0.654317 
 Recall: 0.493344
 IoU: 0.391344

We have finished training iteration 34
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_32_.pth
per-ex loss: 0.617000  [    1/   88]
per-ex loss: 0.527068  [    2/   88]
per-ex loss: 0.738113  [    3/   88]
per-ex loss: 0.821345  [    4/   88]
per-ex loss: 0.771837  [    5/   88]
per-ex loss: 0.578491  [    6/   88]
per-ex loss: 0.643057  [    7/   88]
per-ex loss: 0.610046  [    8/   88]
per-ex loss: 0.510347  [    9/   88]
per-ex loss: 0.607949  [   10/   88]
per-ex loss: 0.764178  [   11/   88]
per-ex loss: 0.623223  [   12/   88]
per-ex loss: 0.534384  [   13/   88]
per-ex loss: 0.581006  [   14/   88]
per-ex loss: 0.728689  [   15/   88]
per-ex loss: 0.532523  [   16/   88]
per-ex loss: 0.737119  [   17/   88]
per-ex loss: 0.777192  [   18/   88]
per-ex loss: 0.592451  [   19/   88]
per-ex loss: 0.542229  [   20/   88]
per-ex loss: 0.754532  [   21/   88]
per-ex loss: 0.715838  [   22/   88]
per-ex loss: 0.640574  [   23/   88]
per-ex loss: 0.686013  [   24/   88]
per-ex loss: 0.735573  [   25/   88]
per-ex loss: 0.740977  [   26/   88]
per-ex loss: 0.767134  [   27/   88]
per-ex loss: 0.761300  [   28/   88]
per-ex loss: 0.725116  [   29/   88]
per-ex loss: 0.580172  [   30/   88]
per-ex loss: 0.812151  [   31/   88]
per-ex loss: 0.777316  [   32/   88]
per-ex loss: 0.754627  [   33/   88]
per-ex loss: 0.628355  [   34/   88]
per-ex loss: 0.565528  [   35/   88]
per-ex loss: 0.560022  [   36/   88]
per-ex loss: 0.666840  [   37/   88]
per-ex loss: 0.577019  [   38/   88]
per-ex loss: 0.613475  [   39/   88]
per-ex loss: 0.715720  [   40/   88]
per-ex loss: 0.840730  [   41/   88]
per-ex loss: 0.500470  [   42/   88]
per-ex loss: 0.728989  [   43/   88]
per-ex loss: 0.602676  [   44/   88]
per-ex loss: 0.733028  [   45/   88]
per-ex loss: 0.764037  [   46/   88]
per-ex loss: 0.756608  [   47/   88]
per-ex loss: 0.785544  [   48/   88]
per-ex loss: 0.612061  [   49/   88]
per-ex loss: 0.484105  [   50/   88]
per-ex loss: 0.550391  [   51/   88]
per-ex loss: 0.764984  [   52/   88]
per-ex loss: 0.553577  [   53/   88]
per-ex loss: 0.534567  [   54/   88]
per-ex loss: 0.734948  [   55/   88]
per-ex loss: 0.584213  [   56/   88]
per-ex loss: 0.687972  [   57/   88]
per-ex loss: 0.762965  [   58/   88]
per-ex loss: 0.588374  [   59/   88]
per-ex loss: 0.756351  [   60/   88]
per-ex loss: 0.561444  [   61/   88]
per-ex loss: 0.548782  [   62/   88]
per-ex loss: 0.642685  [   63/   88]
per-ex loss: 0.801375  [   64/   88]
per-ex loss: 0.763452  [   65/   88]
per-ex loss: 0.814952  [   66/   88]
per-ex loss: 0.814648  [   67/   88]
per-ex loss: 0.610252  [   68/   88]
per-ex loss: 0.617264  [   69/   88]
per-ex loss: 0.715726  [   70/   88]
per-ex loss: 0.599068  [   71/   88]
per-ex loss: 0.612085  [   72/   88]
per-ex loss: 0.624231  [   73/   88]
per-ex loss: 0.788304  [   74/   88]
per-ex loss: 0.566568  [   75/   88]
per-ex loss: 0.718265  [   76/   88]
per-ex loss: 0.529736  [   77/   88]
per-ex loss: 0.770226  [   78/   88]
per-ex loss: 0.810170  [   79/   88]
per-ex loss: 0.729812  [   80/   88]
per-ex loss: 0.603948  [   81/   88]
per-ex loss: 0.713109  [   82/   88]
per-ex loss: 0.666493  [   83/   88]
per-ex loss: 0.558117  [   84/   88]
per-ex loss: 0.589061  [   85/   88]
per-ex loss: 0.625467  [   86/   88]
per-ex loss: 0.524841  [   87/   88]
per-ex loss: 0.563360  [   88/   88]
Train Error: Avg loss: 0.66393819
validation Error: 
 Avg loss: 0.70361957 
 F1: 0.482941 
 Precision: 0.568814 
 Recall: 0.419596
 IoU: 0.318341

test Error: 
 Avg loss: 0.64826320 
 F1: 0.563831 
 Precision: 0.637932 
 Recall: 0.505152
 IoU: 0.392593

We have finished training iteration 35
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_33_.pth
per-ex loss: 0.589322  [    1/   88]
per-ex loss: 0.702527  [    2/   88]
per-ex loss: 0.703471  [    3/   88]
per-ex loss: 0.605471  [    4/   88]
per-ex loss: 0.576225  [    5/   88]
per-ex loss: 0.630041  [    6/   88]
per-ex loss: 0.753604  [    7/   88]
per-ex loss: 0.525536  [    8/   88]
per-ex loss: 0.634472  [    9/   88]
per-ex loss: 0.569873  [   10/   88]
per-ex loss: 0.832953  [   11/   88]
per-ex loss: 0.614726  [   12/   88]
per-ex loss: 0.561652  [   13/   88]
per-ex loss: 0.624705  [   14/   88]
per-ex loss: 0.590678  [   15/   88]
per-ex loss: 0.707549  [   16/   88]
per-ex loss: 0.595036  [   17/   88]
per-ex loss: 0.542727  [   18/   88]
per-ex loss: 0.558414  [   19/   88]
per-ex loss: 0.822429  [   20/   88]
per-ex loss: 0.816748  [   21/   88]
per-ex loss: 0.781834  [   22/   88]
per-ex loss: 0.522301  [   23/   88]
per-ex loss: 0.535059  [   24/   88]
per-ex loss: 0.617032  [   25/   88]
per-ex loss: 0.759206  [   26/   88]
per-ex loss: 0.686501  [   27/   88]
per-ex loss: 0.819630  [   28/   88]
per-ex loss: 0.606657  [   29/   88]
per-ex loss: 0.742570  [   30/   88]
per-ex loss: 0.583117  [   31/   88]
per-ex loss: 0.704512  [   32/   88]
per-ex loss: 0.740777  [   33/   88]
per-ex loss: 0.761071  [   34/   88]
per-ex loss: 0.734119  [   35/   88]
per-ex loss: 0.614604  [   36/   88]
per-ex loss: 0.685456  [   37/   88]
per-ex loss: 0.545241  [   38/   88]
per-ex loss: 0.745254  [   39/   88]
per-ex loss: 0.618845  [   40/   88]
per-ex loss: 0.772662  [   41/   88]
per-ex loss: 0.685318  [   42/   88]
per-ex loss: 0.587097  [   43/   88]
per-ex loss: 0.490479  [   44/   88]
per-ex loss: 0.718582  [   45/   88]
per-ex loss: 0.588336  [   46/   88]
per-ex loss: 0.651867  [   47/   88]
per-ex loss: 0.801908  [   48/   88]
per-ex loss: 0.816974  [   49/   88]
per-ex loss: 0.772447  [   50/   88]
per-ex loss: 0.531221  [   51/   88]
per-ex loss: 0.746637  [   52/   88]
per-ex loss: 0.570380  [   53/   88]
per-ex loss: 0.733664  [   54/   88]
per-ex loss: 0.764752  [   55/   88]
per-ex loss: 0.740378  [   56/   88]
per-ex loss: 0.744684  [   57/   88]
per-ex loss: 0.618869  [   58/   88]
per-ex loss: 0.575617  [   59/   88]
per-ex loss: 0.642052  [   60/   88]
per-ex loss: 0.710722  [   61/   88]
per-ex loss: 0.742553  [   62/   88]
per-ex loss: 0.618095  [   63/   88]
per-ex loss: 0.533739  [   64/   88]
per-ex loss: 0.497249  [   65/   88]
per-ex loss: 0.571411  [   66/   88]
per-ex loss: 0.748186  [   67/   88]
per-ex loss: 0.709776  [   68/   88]
per-ex loss: 0.634044  [   69/   88]
per-ex loss: 0.645296  [   70/   88]
per-ex loss: 0.802389  [   71/   88]
per-ex loss: 0.574268  [   72/   88]
per-ex loss: 0.790444  [   73/   88]
per-ex loss: 0.705586  [   74/   88]
per-ex loss: 0.750771  [   75/   88]
per-ex loss: 0.561087  [   76/   88]
per-ex loss: 0.542552  [   77/   88]
per-ex loss: 0.671318  [   78/   88]
per-ex loss: 0.738475  [   79/   88]
per-ex loss: 0.616628  [   80/   88]
per-ex loss: 0.621674  [   81/   88]
per-ex loss: 0.780115  [   82/   88]
per-ex loss: 0.619577  [   83/   88]
per-ex loss: 0.770062  [   84/   88]
per-ex loss: 0.558785  [   85/   88]
per-ex loss: 0.584540  [   86/   88]
per-ex loss: 0.528012  [   87/   88]
per-ex loss: 0.729715  [   88/   88]
Train Error: Avg loss: 0.66219248
validation Error: 
 Avg loss: 0.70038989 
 F1: 0.482915 
 Precision: 0.530433 
 Recall: 0.443211
 IoU: 0.318318

test Error: 
 Avg loss: 0.64401791 
 F1: 0.569509 
 Precision: 0.616823 
 Recall: 0.528937
 IoU: 0.398122

We have finished training iteration 36
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_34_.pth
per-ex loss: 0.644144  [    1/   88]
per-ex loss: 0.575333  [    2/   88]
per-ex loss: 0.817352  [    3/   88]
per-ex loss: 0.516143  [    4/   88]
per-ex loss: 0.697574  [    5/   88]
per-ex loss: 0.551137  [    6/   88]
per-ex loss: 0.752081  [    7/   88]
per-ex loss: 0.571800  [    8/   88]
per-ex loss: 0.722256  [    9/   88]
per-ex loss: 0.567637  [   10/   88]
per-ex loss: 0.593624  [   11/   88]
per-ex loss: 0.813611  [   12/   88]
per-ex loss: 0.811850  [   13/   88]
per-ex loss: 0.595445  [   14/   88]
per-ex loss: 0.794303  [   15/   88]
per-ex loss: 0.621313  [   16/   88]
per-ex loss: 0.776827  [   17/   88]
per-ex loss: 0.716315  [   18/   88]
per-ex loss: 0.759617  [   19/   88]
per-ex loss: 0.566476  [   20/   88]
per-ex loss: 0.638688  [   21/   88]
per-ex loss: 0.621273  [   22/   88]
per-ex loss: 0.668254  [   23/   88]
per-ex loss: 0.721028  [   24/   88]
per-ex loss: 0.598586  [   25/   88]
per-ex loss: 0.690994  [   26/   88]
per-ex loss: 0.615168  [   27/   88]
per-ex loss: 0.616040  [   28/   88]
per-ex loss: 0.798933  [   29/   88]
per-ex loss: 0.763118  [   30/   88]
per-ex loss: 0.809088  [   31/   88]
per-ex loss: 0.545651  [   32/   88]
per-ex loss: 0.581280  [   33/   88]
per-ex loss: 0.560590  [   34/   88]
per-ex loss: 0.673172  [   35/   88]
per-ex loss: 0.535524  [   36/   88]
per-ex loss: 0.747974  [   37/   88]
per-ex loss: 0.510599  [   38/   88]
per-ex loss: 0.751474  [   39/   88]
per-ex loss: 0.625606  [   40/   88]
per-ex loss: 0.526086  [   41/   88]
per-ex loss: 0.701851  [   42/   88]
per-ex loss: 0.608360  [   43/   88]
per-ex loss: 0.810790  [   44/   88]
per-ex loss: 0.539886  [   45/   88]
per-ex loss: 0.593561  [   46/   88]
per-ex loss: 0.774023  [   47/   88]
per-ex loss: 0.666022  [   48/   88]
per-ex loss: 0.594326  [   49/   88]
per-ex loss: 0.798168  [   50/   88]
per-ex loss: 0.733327  [   51/   88]
per-ex loss: 0.582769  [   52/   88]
per-ex loss: 0.623062  [   53/   88]
per-ex loss: 0.529377  [   54/   88]
per-ex loss: 0.475347  [   55/   88]
per-ex loss: 0.523212  [   56/   88]
per-ex loss: 0.759656  [   57/   88]
per-ex loss: 0.741107  [   58/   88]
per-ex loss: 0.760870  [   59/   88]
per-ex loss: 0.522745  [   60/   88]
per-ex loss: 0.762440  [   61/   88]
per-ex loss: 0.549813  [   62/   88]
per-ex loss: 0.730287  [   63/   88]
per-ex loss: 0.714512  [   64/   88]
per-ex loss: 0.588054  [   65/   88]
per-ex loss: 0.767661  [   66/   88]
per-ex loss: 0.623930  [   67/   88]
per-ex loss: 0.743738  [   68/   88]
per-ex loss: 0.562676  [   69/   88]
per-ex loss: 0.809791  [   70/   88]
per-ex loss: 0.591573  [   71/   88]
per-ex loss: 0.769007  [   72/   88]
per-ex loss: 0.611844  [   73/   88]
per-ex loss: 0.709868  [   74/   88]
per-ex loss: 0.571129  [   75/   88]
per-ex loss: 0.567514  [   76/   88]
per-ex loss: 0.574573  [   77/   88]
per-ex loss: 0.624069  [   78/   88]
per-ex loss: 0.720454  [   79/   88]
per-ex loss: 0.538982  [   80/   88]
per-ex loss: 0.758670  [   81/   88]
per-ex loss: 0.689694  [   82/   88]
per-ex loss: 0.584946  [   83/   88]
per-ex loss: 0.736345  [   84/   88]
per-ex loss: 0.711586  [   85/   88]
per-ex loss: 0.617503  [   86/   88]
per-ex loss: 0.490772  [   87/   88]
per-ex loss: 0.633966  [   88/   88]
Train Error: Avg loss: 0.65633888
validation Error: 
 Avg loss: 0.70443937 
 F1: 0.480011 
 Precision: 0.563572 
 Recall: 0.418030
 IoU: 0.315799

test Error: 
 Avg loss: 0.64974137 
 F1: 0.563624 
 Precision: 0.631256 
 Recall: 0.509081
 IoU: 0.392393

We have finished training iteration 37
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_21_.pth
per-ex loss: 0.556276  [    1/   88]
per-ex loss: 0.800298  [    2/   88]
per-ex loss: 0.786572  [    3/   88]
per-ex loss: 0.755302  [    4/   88]
per-ex loss: 0.528248  [    5/   88]
per-ex loss: 0.762690  [    6/   88]
per-ex loss: 0.770068  [    7/   88]
per-ex loss: 0.623190  [    8/   88]
per-ex loss: 0.627430  [    9/   88]
per-ex loss: 0.817749  [   10/   88]
per-ex loss: 0.621475  [   11/   88]
per-ex loss: 0.596774  [   12/   88]
per-ex loss: 0.742482  [   13/   88]
per-ex loss: 0.665340  [   14/   88]
per-ex loss: 0.591077  [   15/   88]
per-ex loss: 0.691463  [   16/   88]
per-ex loss: 0.556336  [   17/   88]
per-ex loss: 0.516667  [   18/   88]
per-ex loss: 0.579244  [   19/   88]
per-ex loss: 0.738451  [   20/   88]
per-ex loss: 0.565002  [   21/   88]
per-ex loss: 0.594927  [   22/   88]
per-ex loss: 0.763177  [   23/   88]
per-ex loss: 0.739409  [   24/   88]
per-ex loss: 0.586258  [   25/   88]
per-ex loss: 0.561038  [   26/   88]
per-ex loss: 0.774847  [   27/   88]
per-ex loss: 0.552378  [   28/   88]
per-ex loss: 0.710627  [   29/   88]
per-ex loss: 0.640975  [   30/   88]
per-ex loss: 0.768235  [   31/   88]
per-ex loss: 0.587218  [   32/   88]
per-ex loss: 0.812021  [   33/   88]
per-ex loss: 0.526997  [   34/   88]
per-ex loss: 0.541392  [   35/   88]
per-ex loss: 0.586462  [   36/   88]
per-ex loss: 0.607835  [   37/   88]
per-ex loss: 0.785310  [   38/   88]
per-ex loss: 0.772436  [   39/   88]
per-ex loss: 0.707288  [   40/   88]
per-ex loss: 0.722208  [   41/   88]
per-ex loss: 0.773604  [   42/   88]
per-ex loss: 0.661701  [   43/   88]
per-ex loss: 0.603889  [   44/   88]
per-ex loss: 0.627803  [   45/   88]
per-ex loss: 0.527517  [   46/   88]
per-ex loss: 0.592227  [   47/   88]
per-ex loss: 0.707897  [   48/   88]
per-ex loss: 0.749132  [   49/   88]
per-ex loss: 0.628472  [   50/   88]
per-ex loss: 0.589504  [   51/   88]
per-ex loss: 0.507297  [   52/   88]
per-ex loss: 0.783831  [   53/   88]
per-ex loss: 0.591245  [   54/   88]
per-ex loss: 0.782399  [   55/   88]
per-ex loss: 0.835934  [   56/   88]
per-ex loss: 0.714024  [   57/   88]
per-ex loss: 0.697813  [   58/   88]
per-ex loss: 0.740041  [   59/   88]
per-ex loss: 0.813002  [   60/   88]
per-ex loss: 0.571233  [   61/   88]
per-ex loss: 0.717827  [   62/   88]
per-ex loss: 0.598048  [   63/   88]
per-ex loss: 0.636678  [   64/   88]
per-ex loss: 0.579189  [   65/   88]
per-ex loss: 0.463715  [   66/   88]
per-ex loss: 0.635399  [   67/   88]
per-ex loss: 0.525479  [   68/   88]
per-ex loss: 0.526980  [   69/   88]
per-ex loss: 0.560695  [   70/   88]
per-ex loss: 0.682942  [   71/   88]
per-ex loss: 0.609384  [   72/   88]
per-ex loss: 0.744546  [   73/   88]
per-ex loss: 0.748055  [   74/   88]
per-ex loss: 0.829675  [   75/   88]
per-ex loss: 0.667670  [   76/   88]
per-ex loss: 0.688062  [   77/   88]
per-ex loss: 0.562634  [   78/   88]
per-ex loss: 0.714459  [   79/   88]
per-ex loss: 0.517637  [   80/   88]
per-ex loss: 0.549465  [   81/   88]
per-ex loss: 0.616872  [   82/   88]
per-ex loss: 0.753089  [   83/   88]
per-ex loss: 0.576331  [   84/   88]
per-ex loss: 0.533738  [   85/   88]
per-ex loss: 0.563067  [   86/   88]
per-ex loss: 0.589195  [   87/   88]
per-ex loss: 0.787800  [   88/   88]
Train Error: Avg loss: 0.65579962
validation Error: 
 Avg loss: 0.70725020 
 F1: 0.474976 
 Precision: 0.526065 
 Recall: 0.432932
 IoU: 0.311455

test Error: 
 Avg loss: 0.65293717 
 F1: 0.560423 
 Precision: 0.606319 
 Recall: 0.520986
 IoU: 0.389297

We have finished training iteration 38
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_35_.pth
per-ex loss: 0.554950  [    1/   88]
per-ex loss: 0.689494  [    2/   88]
per-ex loss: 0.551141  [    3/   88]
per-ex loss: 0.829738  [    4/   88]
per-ex loss: 0.790097  [    5/   88]
per-ex loss: 0.813975  [    6/   88]
per-ex loss: 0.544681  [    7/   88]
per-ex loss: 0.713362  [    8/   88]
per-ex loss: 0.570088  [    9/   88]
per-ex loss: 0.822401  [   10/   88]
per-ex loss: 0.756836  [   11/   88]
per-ex loss: 0.775405  [   12/   88]
per-ex loss: 0.705046  [   13/   88]
per-ex loss: 0.822827  [   14/   88]
per-ex loss: 0.563926  [   15/   88]
per-ex loss: 0.588879  [   16/   88]
per-ex loss: 0.840859  [   17/   88]
per-ex loss: 0.741124  [   18/   88]
per-ex loss: 0.532344  [   19/   88]
per-ex loss: 0.611625  [   20/   88]
per-ex loss: 0.596854  [   21/   88]
per-ex loss: 0.640675  [   22/   88]
per-ex loss: 0.725349  [   23/   88]
per-ex loss: 0.714023  [   24/   88]
per-ex loss: 0.753957  [   25/   88]
per-ex loss: 0.604993  [   26/   88]
per-ex loss: 0.559032  [   27/   88]
per-ex loss: 0.597990  [   28/   88]
per-ex loss: 0.719137  [   29/   88]
per-ex loss: 0.709140  [   30/   88]
per-ex loss: 0.530123  [   31/   88]
per-ex loss: 0.709038  [   32/   88]
per-ex loss: 0.619737  [   33/   88]
per-ex loss: 0.590965  [   34/   88]
per-ex loss: 0.686723  [   35/   88]
per-ex loss: 0.750700  [   36/   88]
per-ex loss: 0.529191  [   37/   88]
per-ex loss: 0.744827  [   38/   88]
per-ex loss: 0.714284  [   39/   88]
per-ex loss: 0.766727  [   40/   88]
per-ex loss: 0.550911  [   41/   88]
per-ex loss: 0.547805  [   42/   88]
per-ex loss: 0.639004  [   43/   88]
per-ex loss: 0.636882  [   44/   88]
per-ex loss: 0.624261  [   45/   88]
per-ex loss: 0.588778  [   46/   88]
per-ex loss: 0.543434  [   47/   88]
per-ex loss: 0.598067  [   48/   88]
per-ex loss: 0.588028  [   49/   88]
per-ex loss: 0.549430  [   50/   88]
per-ex loss: 0.741822  [   51/   88]
per-ex loss: 0.565767  [   52/   88]
per-ex loss: 0.552692  [   53/   88]
per-ex loss: 0.693016  [   54/   88]
per-ex loss: 0.515082  [   55/   88]
per-ex loss: 0.807207  [   56/   88]
per-ex loss: 0.615968  [   57/   88]
per-ex loss: 0.493309  [   58/   88]
per-ex loss: 0.783635  [   59/   88]
per-ex loss: 0.716571  [   60/   88]
per-ex loss: 0.559819  [   61/   88]
per-ex loss: 0.693283  [   62/   88]
per-ex loss: 0.583231  [   63/   88]
per-ex loss: 0.526483  [   64/   88]
per-ex loss: 0.529793  [   65/   88]
per-ex loss: 0.724517  [   66/   88]
per-ex loss: 0.772749  [   67/   88]
per-ex loss: 0.596073  [   68/   88]
per-ex loss: 0.761649  [   69/   88]
per-ex loss: 0.648292  [   70/   88]
per-ex loss: 0.542657  [   71/   88]
per-ex loss: 0.653228  [   72/   88]
per-ex loss: 0.615824  [   73/   88]
per-ex loss: 0.797257  [   74/   88]
per-ex loss: 0.621134  [   75/   88]
per-ex loss: 0.794181  [   76/   88]
per-ex loss: 0.734261  [   77/   88]
per-ex loss: 0.607420  [   78/   88]
per-ex loss: 0.761572  [   79/   88]
per-ex loss: 0.487166  [   80/   88]
per-ex loss: 0.760790  [   81/   88]
per-ex loss: 0.585652  [   82/   88]
per-ex loss: 0.774041  [   83/   88]
per-ex loss: 0.640373  [   84/   88]
per-ex loss: 0.716772  [   85/   88]
per-ex loss: 0.604154  [   86/   88]
per-ex loss: 0.850305  [   87/   88]
per-ex loss: 0.589893  [   88/   88]
Train Error: Avg loss: 0.65871025
validation Error: 
 Avg loss: 0.73285550 
 F1: 0.439415 
 Precision: 0.477741 
 Recall: 0.406781
 IoU: 0.281570

test Error: 
 Avg loss: 0.65106316 
 F1: 0.562142 
 Precision: 0.626483 
 Recall: 0.509787
 IoU: 0.390958

We have finished training iteration 39
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_37_.pth
per-ex loss: 0.721267  [    1/   88]
per-ex loss: 0.800912  [    2/   88]
per-ex loss: 0.701543  [    3/   88]
per-ex loss: 0.831078  [    4/   88]
per-ex loss: 0.642156  [    5/   88]
per-ex loss: 0.705150  [    6/   88]
per-ex loss: 0.565974  [    7/   88]
per-ex loss: 0.523875  [    8/   88]
per-ex loss: 0.586307  [    9/   88]
per-ex loss: 0.621511  [   10/   88]
per-ex loss: 0.636196  [   11/   88]
per-ex loss: 0.610610  [   12/   88]
per-ex loss: 0.702806  [   13/   88]
per-ex loss: 0.537876  [   14/   88]
per-ex loss: 0.579485  [   15/   88]
per-ex loss: 0.609946  [   16/   88]
per-ex loss: 0.792543  [   17/   88]
per-ex loss: 0.533018  [   18/   88]
per-ex loss: 0.714342  [   19/   88]
per-ex loss: 0.774990  [   20/   88]
per-ex loss: 0.706749  [   21/   88]
per-ex loss: 0.614990  [   22/   88]
per-ex loss: 0.594807  [   23/   88]
per-ex loss: 0.717377  [   24/   88]
per-ex loss: 0.721899  [   25/   88]
per-ex loss: 0.531645  [   26/   88]
per-ex loss: 0.567028  [   27/   88]
per-ex loss: 0.755973  [   28/   88]
per-ex loss: 0.629763  [   29/   88]
per-ex loss: 0.764293  [   30/   88]
per-ex loss: 0.601521  [   31/   88]
per-ex loss: 0.617978  [   32/   88]
per-ex loss: 0.489136  [   33/   88]
per-ex loss: 0.566998  [   34/   88]
per-ex loss: 0.529608  [   35/   88]
per-ex loss: 0.576732  [   36/   88]
per-ex loss: 0.624871  [   37/   88]
per-ex loss: 0.583481  [   38/   88]
per-ex loss: 0.545622  [   39/   88]
per-ex loss: 0.741959  [   40/   88]
per-ex loss: 0.679908  [   41/   88]
per-ex loss: 0.799052  [   42/   88]
per-ex loss: 0.515539  [   43/   88]
per-ex loss: 0.749502  [   44/   88]
per-ex loss: 0.642493  [   45/   88]
per-ex loss: 0.744788  [   46/   88]
per-ex loss: 0.596111  [   47/   88]
per-ex loss: 0.773932  [   48/   88]
per-ex loss: 0.471749  [   49/   88]
per-ex loss: 0.550392  [   50/   88]
per-ex loss: 0.629635  [   51/   88]
per-ex loss: 0.520967  [   52/   88]
per-ex loss: 0.529983  [   53/   88]
per-ex loss: 0.735994  [   54/   88]
per-ex loss: 0.533603  [   55/   88]
per-ex loss: 0.553833  [   56/   88]
per-ex loss: 0.826601  [   57/   88]
per-ex loss: 0.592508  [   58/   88]
per-ex loss: 0.751351  [   59/   88]
per-ex loss: 0.725302  [   60/   88]
per-ex loss: 0.815052  [   61/   88]
per-ex loss: 0.777703  [   62/   88]
per-ex loss: 0.756142  [   63/   88]
per-ex loss: 0.828962  [   64/   88]
per-ex loss: 0.577327  [   65/   88]
per-ex loss: 0.574334  [   66/   88]
per-ex loss: 0.793693  [   67/   88]
per-ex loss: 0.681746  [   68/   88]
per-ex loss: 0.722003  [   69/   88]
per-ex loss: 0.700749  [   70/   88]
per-ex loss: 0.762673  [   71/   88]
per-ex loss: 0.757829  [   72/   88]
per-ex loss: 0.616086  [   73/   88]
per-ex loss: 0.738982  [   74/   88]
per-ex loss: 0.541997  [   75/   88]
per-ex loss: 0.698526  [   76/   88]
per-ex loss: 0.582349  [   77/   88]
per-ex loss: 0.828049  [   78/   88]
per-ex loss: 0.554211  [   79/   88]
per-ex loss: 0.780372  [   80/   88]
per-ex loss: 0.529201  [   81/   88]
per-ex loss: 0.560427  [   82/   88]
per-ex loss: 0.571108  [   83/   88]
per-ex loss: 0.545088  [   84/   88]
per-ex loss: 0.552336  [   85/   88]
per-ex loss: 0.592016  [   86/   88]
per-ex loss: 0.713774  [   87/   88]
per-ex loss: 0.705238  [   88/   88]
Train Error: Avg loss: 0.65290036
validation Error: 
 Avg loss: 0.70271240 
 F1: 0.478563 
 Precision: 0.599810 
 Recall: 0.398091
 IoU: 0.314547

test Error: 
 Avg loss: 0.65085709 
 F1: 0.560054 
 Precision: 0.677201 
 Recall: 0.477459
 IoU: 0.388940

We have finished training iteration 40
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_38_.pth
per-ex loss: 0.719318  [    1/   88]
per-ex loss: 0.784479  [    2/   88]
per-ex loss: 0.620033  [    3/   88]
per-ex loss: 0.701165  [    4/   88]
per-ex loss: 0.545582  [    5/   88]
per-ex loss: 0.587950  [    6/   88]
per-ex loss: 0.816172  [    7/   88]
per-ex loss: 0.496327  [    8/   88]
per-ex loss: 0.551005  [    9/   88]
per-ex loss: 0.743722  [   10/   88]
per-ex loss: 0.588963  [   11/   88]
per-ex loss: 0.564208  [   12/   88]
per-ex loss: 0.595629  [   13/   88]
per-ex loss: 0.764610  [   14/   88]
per-ex loss: 0.820352  [   15/   88]
per-ex loss: 0.706599  [   16/   88]
per-ex loss: 0.718380  [   17/   88]
per-ex loss: 0.597778  [   18/   88]
per-ex loss: 0.741313  [   19/   88]
per-ex loss: 0.535018  [   20/   88]
per-ex loss: 0.586379  [   21/   88]
per-ex loss: 0.539389  [   22/   88]
per-ex loss: 0.643114  [   23/   88]
per-ex loss: 0.741501  [   24/   88]
per-ex loss: 0.583091  [   25/   88]
per-ex loss: 0.587649  [   26/   88]
per-ex loss: 0.675822  [   27/   88]
per-ex loss: 0.779545  [   28/   88]
per-ex loss: 0.601310  [   29/   88]
per-ex loss: 0.525811  [   30/   88]
per-ex loss: 0.824435  [   31/   88]
per-ex loss: 0.726466  [   32/   88]
per-ex loss: 0.618682  [   33/   88]
per-ex loss: 0.694008  [   34/   88]
per-ex loss: 0.601002  [   35/   88]
per-ex loss: 0.544415  [   36/   88]
per-ex loss: 0.546369  [   37/   88]
per-ex loss: 0.629960  [   38/   88]
per-ex loss: 0.623772  [   39/   88]
per-ex loss: 0.750246  [   40/   88]
per-ex loss: 0.768261  [   41/   88]
per-ex loss: 0.529406  [   42/   88]
per-ex loss: 0.733564  [   43/   88]
per-ex loss: 0.748172  [   44/   88]
per-ex loss: 0.611335  [   45/   88]
per-ex loss: 0.766430  [   46/   88]
per-ex loss: 0.563122  [   47/   88]
per-ex loss: 0.811765  [   48/   88]
per-ex loss: 0.532548  [   49/   88]
per-ex loss: 0.567917  [   50/   88]
per-ex loss: 0.760472  [   51/   88]
per-ex loss: 0.682199  [   52/   88]
per-ex loss: 0.753911  [   53/   88]
per-ex loss: 0.693797  [   54/   88]
per-ex loss: 0.595569  [   55/   88]
per-ex loss: 0.606656  [   56/   88]
per-ex loss: 0.553984  [   57/   88]
per-ex loss: 0.590261  [   58/   88]
per-ex loss: 0.584761  [   59/   88]
per-ex loss: 0.715326  [   60/   88]
per-ex loss: 0.661880  [   61/   88]
per-ex loss: 0.815039  [   62/   88]
per-ex loss: 0.646183  [   63/   88]
per-ex loss: 0.788276  [   64/   88]
per-ex loss: 0.567202  [   65/   88]
per-ex loss: 0.603820  [   66/   88]
per-ex loss: 0.822443  [   67/   88]
per-ex loss: 0.518840  [   68/   88]
per-ex loss: 0.516174  [   69/   88]
per-ex loss: 0.586920  [   70/   88]
per-ex loss: 0.554421  [   71/   88]
per-ex loss: 0.727589  [   72/   88]
per-ex loss: 0.592200  [   73/   88]
per-ex loss: 0.704304  [   74/   88]
per-ex loss: 0.525497  [   75/   88]
per-ex loss: 0.536127  [   76/   88]
per-ex loss: 0.753963  [   77/   88]
per-ex loss: 0.577142  [   78/   88]
per-ex loss: 0.587881  [   79/   88]
per-ex loss: 0.698559  [   80/   88]
per-ex loss: 0.721831  [   81/   88]
per-ex loss: 0.625231  [   82/   88]
per-ex loss: 0.788442  [   83/   88]
per-ex loss: 0.714696  [   84/   88]
per-ex loss: 0.556450  [   85/   88]
per-ex loss: 0.817971  [   86/   88]
per-ex loss: 0.762980  [   87/   88]
per-ex loss: 0.685339  [   88/   88]
Train Error: Avg loss: 0.65509579
validation Error: 
 Avg loss: 0.72594244 
 F1: 0.448662 
 Precision: 0.445027 
 Recall: 0.452357
 IoU: 0.289210

test Error: 
 Avg loss: 0.65040533 
 F1: 0.563435 
 Precision: 0.570544 
 Recall: 0.556500
 IoU: 0.392209

We have finished training iteration 41
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_39_.pth
per-ex loss: 0.562891  [    1/   88]
per-ex loss: 0.508649  [    2/   88]
per-ex loss: 0.516146  [    3/   88]
per-ex loss: 0.750796  [    4/   88]
per-ex loss: 0.540856  [    5/   88]
per-ex loss: 0.530845  [    6/   88]
per-ex loss: 0.737301  [    7/   88]
per-ex loss: 0.678743  [    8/   88]
per-ex loss: 0.541888  [    9/   88]
per-ex loss: 0.800144  [   10/   88]
per-ex loss: 0.803561  [   11/   88]
per-ex loss: 0.802496  [   12/   88]
per-ex loss: 0.732752  [   13/   88]
per-ex loss: 0.754644  [   14/   88]
per-ex loss: 0.561566  [   15/   88]
per-ex loss: 0.743353  [   16/   88]
per-ex loss: 0.813680  [   17/   88]
per-ex loss: 0.562819  [   18/   88]
per-ex loss: 0.636515  [   19/   88]
per-ex loss: 0.748517  [   20/   88]
per-ex loss: 0.699468  [   21/   88]
per-ex loss: 0.541190  [   22/   88]
per-ex loss: 0.716550  [   23/   88]
per-ex loss: 0.674419  [   24/   88]
per-ex loss: 0.578858  [   25/   88]
per-ex loss: 0.605753  [   26/   88]
per-ex loss: 0.723433  [   27/   88]
per-ex loss: 0.497670  [   28/   88]
per-ex loss: 0.574249  [   29/   88]
per-ex loss: 0.736132  [   30/   88]
per-ex loss: 0.712503  [   31/   88]
per-ex loss: 0.741986  [   32/   88]
per-ex loss: 0.707702  [   33/   88]
per-ex loss: 0.552718  [   34/   88]
per-ex loss: 0.546083  [   35/   88]
per-ex loss: 0.603241  [   36/   88]
per-ex loss: 0.586379  [   37/   88]
per-ex loss: 0.758016  [   38/   88]
per-ex loss: 0.823352  [   39/   88]
per-ex loss: 0.528228  [   40/   88]
per-ex loss: 0.778068  [   41/   88]
per-ex loss: 0.568577  [   42/   88]
per-ex loss: 0.624900  [   43/   88]
per-ex loss: 0.717061  [   44/   88]
per-ex loss: 0.755925  [   45/   88]
per-ex loss: 0.614328  [   46/   88]
per-ex loss: 0.637963  [   47/   88]
per-ex loss: 0.602173  [   48/   88]
per-ex loss: 0.804913  [   49/   88]
per-ex loss: 0.552482  [   50/   88]
per-ex loss: 0.780498  [   51/   88]
per-ex loss: 0.649619  [   52/   88]
per-ex loss: 0.538892  [   53/   88]
per-ex loss: 0.597870  [   54/   88]
per-ex loss: 0.538808  [   55/   88]
per-ex loss: 0.712478  [   56/   88]
per-ex loss: 0.718983  [   57/   88]
per-ex loss: 0.706769  [   58/   88]
per-ex loss: 0.700726  [   59/   88]
per-ex loss: 0.696096  [   60/   88]
per-ex loss: 0.566801  [   61/   88]
per-ex loss: 0.646140  [   62/   88]
per-ex loss: 0.808182  [   63/   88]
per-ex loss: 0.627081  [   64/   88]
per-ex loss: 0.461888  [   65/   88]
per-ex loss: 0.583800  [   66/   88]
per-ex loss: 0.824479  [   67/   88]
per-ex loss: 0.820931  [   68/   88]
per-ex loss: 0.584625  [   69/   88]
per-ex loss: 0.577032  [   70/   88]
per-ex loss: 0.709167  [   71/   88]
per-ex loss: 0.656616  [   72/   88]
per-ex loss: 0.551379  [   73/   88]
per-ex loss: 0.536433  [   74/   88]
per-ex loss: 0.600529  [   75/   88]
per-ex loss: 0.579193  [   76/   88]
per-ex loss: 0.568395  [   77/   88]
per-ex loss: 0.704219  [   78/   88]
per-ex loss: 0.573925  [   79/   88]
per-ex loss: 0.629248  [   80/   88]
per-ex loss: 0.631495  [   81/   88]
per-ex loss: 0.805162  [   82/   88]
per-ex loss: 0.589442  [   83/   88]
per-ex loss: 0.754817  [   84/   88]
per-ex loss: 0.594049  [   85/   88]
per-ex loss: 0.785205  [   86/   88]
per-ex loss: 0.736225  [   87/   88]
per-ex loss: 0.591714  [   88/   88]
Train Error: Avg loss: 0.65490217
validation Error: 
 Avg loss: 0.69601723 
 F1: 0.493510 
 Precision: 0.563993 
 Recall: 0.438686
 IoU: 0.327589

test Error: 
 Avg loss: 0.64324219 
 F1: 0.568225 
 Precision: 0.620678 
 Recall: 0.523946
 IoU: 0.396867

We have finished training iteration 42
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_40_.pth
per-ex loss: 0.619077  [    1/   88]
per-ex loss: 0.677562  [    2/   88]
per-ex loss: 0.661490  [    3/   88]
per-ex loss: 0.819697  [    4/   88]
per-ex loss: 0.633956  [    5/   88]
per-ex loss: 0.586046  [    6/   88]
per-ex loss: 0.582044  [    7/   88]
per-ex loss: 0.551340  [    8/   88]
per-ex loss: 0.754774  [    9/   88]
per-ex loss: 0.548308  [   10/   88]
per-ex loss: 0.691321  [   11/   88]
per-ex loss: 0.713070  [   12/   88]
per-ex loss: 0.780784  [   13/   88]
per-ex loss: 0.703144  [   14/   88]
per-ex loss: 0.763132  [   15/   88]
per-ex loss: 0.507658  [   16/   88]
per-ex loss: 0.717317  [   17/   88]
per-ex loss: 0.710698  [   18/   88]
per-ex loss: 0.514987  [   19/   88]
per-ex loss: 0.618035  [   20/   88]
per-ex loss: 0.763042  [   21/   88]
per-ex loss: 0.554459  [   22/   88]
per-ex loss: 0.523618  [   23/   88]
per-ex loss: 0.538989  [   24/   88]
per-ex loss: 0.744053  [   25/   88]
per-ex loss: 0.575955  [   26/   88]
per-ex loss: 0.613449  [   27/   88]
per-ex loss: 0.615637  [   28/   88]
per-ex loss: 0.709711  [   29/   88]
per-ex loss: 0.698314  [   30/   88]
per-ex loss: 0.555797  [   31/   88]
per-ex loss: 0.520967  [   32/   88]
per-ex loss: 0.555804  [   33/   88]
per-ex loss: 0.573256  [   34/   88]
per-ex loss: 0.545351  [   35/   88]
per-ex loss: 0.813377  [   36/   88]
per-ex loss: 0.597976  [   37/   88]
per-ex loss: 0.697016  [   38/   88]
per-ex loss: 0.735966  [   39/   88]
per-ex loss: 0.760906  [   40/   88]
per-ex loss: 0.803491  [   41/   88]
per-ex loss: 0.579317  [   42/   88]
per-ex loss: 0.549979  [   43/   88]
per-ex loss: 0.776572  [   44/   88]
per-ex loss: 0.713709  [   45/   88]
per-ex loss: 0.641512  [   46/   88]
per-ex loss: 0.592256  [   47/   88]
per-ex loss: 0.738633  [   48/   88]
per-ex loss: 0.721070  [   49/   88]
per-ex loss: 0.653971  [   50/   88]
per-ex loss: 0.777049  [   51/   88]
per-ex loss: 0.602333  [   52/   88]
per-ex loss: 0.525838  [   53/   88]
per-ex loss: 0.604278  [   54/   88]
per-ex loss: 0.740722  [   55/   88]
per-ex loss: 0.699796  [   56/   88]
per-ex loss: 0.807515  [   57/   88]
per-ex loss: 0.530233  [   58/   88]
per-ex loss: 0.635779  [   59/   88]
per-ex loss: 0.562549  [   60/   88]
per-ex loss: 0.623109  [   61/   88]
per-ex loss: 0.628490  [   62/   88]
per-ex loss: 0.620669  [   63/   88]
per-ex loss: 0.731951  [   64/   88]
per-ex loss: 0.724822  [   65/   88]
per-ex loss: 0.514818  [   66/   88]
per-ex loss: 0.800411  [   67/   88]
per-ex loss: 0.812304  [   68/   88]
per-ex loss: 0.621664  [   69/   88]
per-ex loss: 0.755862  [   70/   88]
per-ex loss: 0.769001  [   71/   88]
per-ex loss: 0.587390  [   72/   88]
per-ex loss: 0.741470  [   73/   88]
per-ex loss: 0.558874  [   74/   88]
per-ex loss: 0.755062  [   75/   88]
per-ex loss: 0.581397  [   76/   88]
per-ex loss: 0.787358  [   77/   88]
per-ex loss: 0.541926  [   78/   88]
per-ex loss: 0.500734  [   79/   88]
per-ex loss: 0.581887  [   80/   88]
per-ex loss: 0.568393  [   81/   88]
per-ex loss: 0.809156  [   82/   88]
per-ex loss: 0.754454  [   83/   88]
per-ex loss: 0.455105  [   84/   88]
per-ex loss: 0.586292  [   85/   88]
per-ex loss: 0.701692  [   86/   88]
per-ex loss: 0.569133  [   87/   88]
per-ex loss: 0.592563  [   88/   88]
Train Error: Avg loss: 0.65205310
validation Error: 
 Avg loss: 0.70384141 
 F1: 0.477075 
 Precision: 0.614593 
 Recall: 0.389845
 IoU: 0.313262

test Error: 
 Avg loss: 0.65234052 
 F1: 0.555689 
 Precision: 0.699721 
 Recall: 0.460830
 IoU: 0.384743

We have finished training iteration 43
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_41_.pth
per-ex loss: 0.763023  [    1/   88]
per-ex loss: 0.543741  [    2/   88]
per-ex loss: 0.572390  [    3/   88]
per-ex loss: 0.600046  [    4/   88]
per-ex loss: 0.610681  [    5/   88]
per-ex loss: 0.560304  [    6/   88]
per-ex loss: 0.634052  [    7/   88]
per-ex loss: 0.608024  [    8/   88]
per-ex loss: 0.556888  [    9/   88]
per-ex loss: 0.463752  [   10/   88]
per-ex loss: 0.577718  [   11/   88]
per-ex loss: 0.634403  [   12/   88]
per-ex loss: 0.581000  [   13/   88]
per-ex loss: 0.695217  [   14/   88]
per-ex loss: 0.623479  [   15/   88]
per-ex loss: 0.507434  [   16/   88]
per-ex loss: 0.543634  [   17/   88]
per-ex loss: 0.748820  [   18/   88]
per-ex loss: 0.559394  [   19/   88]
per-ex loss: 0.768902  [   20/   88]
per-ex loss: 0.721933  [   21/   88]
per-ex loss: 0.786775  [   22/   88]
per-ex loss: 0.508066  [   23/   88]
per-ex loss: 0.814710  [   24/   88]
per-ex loss: 0.712789  [   25/   88]
per-ex loss: 0.619425  [   26/   88]
per-ex loss: 0.758992  [   27/   88]
per-ex loss: 0.682667  [   28/   88]
per-ex loss: 0.602796  [   29/   88]
per-ex loss: 0.713952  [   30/   88]
per-ex loss: 0.524747  [   31/   88]
per-ex loss: 0.528616  [   32/   88]
per-ex loss: 0.563828  [   33/   88]
per-ex loss: 0.528471  [   34/   88]
per-ex loss: 0.704850  [   35/   88]
per-ex loss: 0.770379  [   36/   88]
per-ex loss: 0.829103  [   37/   88]
per-ex loss: 0.743236  [   38/   88]
per-ex loss: 0.644483  [   39/   88]
per-ex loss: 0.636220  [   40/   88]
per-ex loss: 0.783147  [   41/   88]
per-ex loss: 0.567902  [   42/   88]
per-ex loss: 0.838300  [   43/   88]
per-ex loss: 0.774240  [   44/   88]
per-ex loss: 0.578249  [   45/   88]
per-ex loss: 0.559470  [   46/   88]
per-ex loss: 0.736112  [   47/   88]
per-ex loss: 0.580379  [   48/   88]
per-ex loss: 0.750893  [   49/   88]
per-ex loss: 0.685645  [   50/   88]
per-ex loss: 0.718435  [   51/   88]
per-ex loss: 0.714844  [   52/   88]
per-ex loss: 0.681241  [   53/   88]
per-ex loss: 0.612028  [   54/   88]
per-ex loss: 0.796821  [   55/   88]
per-ex loss: 0.651657  [   56/   88]
per-ex loss: 0.705043  [   57/   88]
per-ex loss: 0.740801  [   58/   88]
per-ex loss: 0.809821  [   59/   88]
per-ex loss: 0.602817  [   60/   88]
per-ex loss: 0.584520  [   61/   88]
per-ex loss: 0.686751  [   62/   88]
per-ex loss: 0.705254  [   63/   88]
per-ex loss: 0.542082  [   64/   88]
per-ex loss: 0.788208  [   65/   88]
per-ex loss: 0.714796  [   66/   88]
per-ex loss: 0.740135  [   67/   88]
per-ex loss: 0.540455  [   68/   88]
per-ex loss: 0.707716  [   69/   88]
per-ex loss: 0.599829  [   70/   88]
per-ex loss: 0.760690  [   71/   88]
per-ex loss: 0.744287  [   72/   88]
per-ex loss: 0.590505  [   73/   88]
per-ex loss: 0.510382  [   74/   88]
per-ex loss: 0.801219  [   75/   88]
per-ex loss: 0.736721  [   76/   88]
per-ex loss: 0.609789  [   77/   88]
per-ex loss: 0.598753  [   78/   88]
per-ex loss: 0.611734  [   79/   88]
per-ex loss: 0.717351  [   80/   88]
per-ex loss: 0.641002  [   81/   88]
per-ex loss: 0.822627  [   82/   88]
per-ex loss: 0.532298  [   83/   88]
per-ex loss: 0.584830  [   84/   88]
per-ex loss: 0.757482  [   85/   88]
per-ex loss: 0.584774  [   86/   88]
per-ex loss: 0.530058  [   87/   88]
per-ex loss: 0.607767  [   88/   88]
Train Error: Avg loss: 0.65669091
validation Error: 
 Avg loss: 0.69635412 
 F1: 0.489844 
 Precision: 0.587937 
 Recall: 0.419802
 IoU: 0.324366

test Error: 
 Avg loss: 0.64861394 
 F1: 0.565373 
 Precision: 0.625468 
 Recall: 0.515814
 IoU: 0.394091

We have finished training iteration 44
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_29_.pth
per-ex loss: 0.638500  [    1/   88]
per-ex loss: 0.711274  [    2/   88]
per-ex loss: 0.709370  [    3/   88]
per-ex loss: 0.556515  [    4/   88]
per-ex loss: 0.755807  [    5/   88]
per-ex loss: 0.786950  [    6/   88]
per-ex loss: 0.816844  [    7/   88]
per-ex loss: 0.745921  [    8/   88]
per-ex loss: 0.528474  [    9/   88]
per-ex loss: 0.755796  [   10/   88]
per-ex loss: 0.602786  [   11/   88]
per-ex loss: 0.751514  [   12/   88]
per-ex loss: 0.621335  [   13/   88]
per-ex loss: 0.759177  [   14/   88]
per-ex loss: 0.511333  [   15/   88]
per-ex loss: 0.592107  [   16/   88]
per-ex loss: 0.752738  [   17/   88]
per-ex loss: 0.736505  [   18/   88]
per-ex loss: 0.588027  [   19/   88]
per-ex loss: 0.566371  [   20/   88]
per-ex loss: 0.522012  [   21/   88]
per-ex loss: 0.626531  [   22/   88]
per-ex loss: 0.508123  [   23/   88]
per-ex loss: 0.696958  [   24/   88]
per-ex loss: 0.543805  [   25/   88]
per-ex loss: 0.560228  [   26/   88]
per-ex loss: 0.562101  [   27/   88]
per-ex loss: 0.549736  [   28/   88]
per-ex loss: 0.477344  [   29/   88]
per-ex loss: 0.695585  [   30/   88]
per-ex loss: 0.676104  [   31/   88]
per-ex loss: 0.568025  [   32/   88]
per-ex loss: 0.633252  [   33/   88]
per-ex loss: 0.549007  [   34/   88]
per-ex loss: 0.617271  [   35/   88]
per-ex loss: 0.713451  [   36/   88]
per-ex loss: 0.521387  [   37/   88]
per-ex loss: 0.804595  [   38/   88]
per-ex loss: 0.711890  [   39/   88]
per-ex loss: 0.702630  [   40/   88]
per-ex loss: 0.461431  [   41/   88]
per-ex loss: 0.681310  [   42/   88]
per-ex loss: 0.587400  [   43/   88]
per-ex loss: 0.737528  [   44/   88]
per-ex loss: 0.593349  [   45/   88]
per-ex loss: 0.814025  [   46/   88]
per-ex loss: 0.579052  [   47/   88]
per-ex loss: 0.596721  [   48/   88]
per-ex loss: 0.591313  [   49/   88]
per-ex loss: 0.602826  [   50/   88]
per-ex loss: 0.542988  [   51/   88]
per-ex loss: 0.762693  [   52/   88]
per-ex loss: 0.804981  [   53/   88]
per-ex loss: 0.563364  [   54/   88]
per-ex loss: 0.791714  [   55/   88]
per-ex loss: 0.731887  [   56/   88]
per-ex loss: 0.814349  [   57/   88]
per-ex loss: 0.601320  [   58/   88]
per-ex loss: 0.587394  [   59/   88]
per-ex loss: 0.539163  [   60/   88]
per-ex loss: 0.703699  [   61/   88]
per-ex loss: 0.788574  [   62/   88]
per-ex loss: 0.721217  [   63/   88]
per-ex loss: 0.617816  [   64/   88]
per-ex loss: 0.537860  [   65/   88]
per-ex loss: 0.795674  [   66/   88]
per-ex loss: 0.742931  [   67/   88]
per-ex loss: 0.531583  [   68/   88]
per-ex loss: 0.605929  [   69/   88]
per-ex loss: 0.722859  [   70/   88]
per-ex loss: 0.587994  [   71/   88]
per-ex loss: 0.551928  [   72/   88]
per-ex loss: 0.643334  [   73/   88]
per-ex loss: 0.755148  [   74/   88]
per-ex loss: 0.714165  [   75/   88]
per-ex loss: 0.535912  [   76/   88]
per-ex loss: 0.677498  [   77/   88]
per-ex loss: 0.706352  [   78/   88]
per-ex loss: 0.535140  [   79/   88]
per-ex loss: 0.650203  [   80/   88]
per-ex loss: 0.593098  [   81/   88]
per-ex loss: 0.739049  [   82/   88]
per-ex loss: 0.798171  [   83/   88]
per-ex loss: 0.575677  [   84/   88]
per-ex loss: 0.742508  [   85/   88]
per-ex loss: 0.716945  [   86/   88]
per-ex loss: 0.768043  [   87/   88]
per-ex loss: 0.517743  [   88/   88]
Train Error: Avg loss: 0.65101406
validation Error: 
 Avg loss: 0.71095443 
 F1: 0.474884 
 Precision: 0.494435 
 Recall: 0.456821
 IoU: 0.311376

test Error: 
 Avg loss: 0.64979126 
 F1: 0.564545 
 Precision: 0.580441 
 Recall: 0.549495
 IoU: 0.393286

We have finished training iteration 45
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_43_.pth
per-ex loss: 0.708297  [    1/   88]
per-ex loss: 0.568135  [    2/   88]
per-ex loss: 0.731657  [    3/   88]
per-ex loss: 0.543230  [    4/   88]
per-ex loss: 0.724626  [    5/   88]
per-ex loss: 0.531949  [    6/   88]
per-ex loss: 0.709882  [    7/   88]
per-ex loss: 0.609565  [    8/   88]
per-ex loss: 0.736299  [    9/   88]
per-ex loss: 0.818394  [   10/   88]
per-ex loss: 0.586123  [   11/   88]
per-ex loss: 0.706180  [   12/   88]
per-ex loss: 0.605867  [   13/   88]
per-ex loss: 0.537387  [   14/   88]
per-ex loss: 0.575510  [   15/   88]
per-ex loss: 0.703029  [   16/   88]
per-ex loss: 0.690268  [   17/   88]
per-ex loss: 0.704071  [   18/   88]
per-ex loss: 0.545950  [   19/   88]
per-ex loss: 0.564644  [   20/   88]
per-ex loss: 0.786162  [   21/   88]
per-ex loss: 0.737496  [   22/   88]
per-ex loss: 0.582322  [   23/   88]
per-ex loss: 0.715212  [   24/   88]
per-ex loss: 0.446821  [   25/   88]
per-ex loss: 0.777069  [   26/   88]
per-ex loss: 0.740623  [   27/   88]
per-ex loss: 0.692277  [   28/   88]
per-ex loss: 0.565321  [   29/   88]
per-ex loss: 0.769411  [   30/   88]
per-ex loss: 0.671864  [   31/   88]
per-ex loss: 0.653175  [   32/   88]
per-ex loss: 0.796331  [   33/   88]
per-ex loss: 0.584358  [   34/   88]
per-ex loss: 0.628081  [   35/   88]
per-ex loss: 0.620482  [   36/   88]
per-ex loss: 0.583397  [   37/   88]
per-ex loss: 0.504768  [   38/   88]
per-ex loss: 0.519118  [   39/   88]
per-ex loss: 0.554301  [   40/   88]
per-ex loss: 0.710459  [   41/   88]
per-ex loss: 0.719654  [   42/   88]
per-ex loss: 0.538018  [   43/   88]
per-ex loss: 0.567292  [   44/   88]
per-ex loss: 0.536446  [   45/   88]
per-ex loss: 0.700268  [   46/   88]
per-ex loss: 0.808157  [   47/   88]
per-ex loss: 0.605231  [   48/   88]
per-ex loss: 0.812178  [   49/   88]
per-ex loss: 0.770890  [   50/   88]
per-ex loss: 0.594994  [   51/   88]
per-ex loss: 0.584419  [   52/   88]
per-ex loss: 0.790870  [   53/   88]
per-ex loss: 0.564094  [   54/   88]
per-ex loss: 0.580403  [   55/   88]
per-ex loss: 0.722014  [   56/   88]
per-ex loss: 0.596188  [   57/   88]
per-ex loss: 0.787391  [   58/   88]
per-ex loss: 0.577209  [   59/   88]
per-ex loss: 0.758643  [   60/   88]
per-ex loss: 0.790292  [   61/   88]
per-ex loss: 0.756885  [   62/   88]
per-ex loss: 0.576776  [   63/   88]
per-ex loss: 0.549477  [   64/   88]
per-ex loss: 0.770072  [   65/   88]
per-ex loss: 0.592936  [   66/   88]
per-ex loss: 0.720097  [   67/   88]
per-ex loss: 0.622990  [   68/   88]
per-ex loss: 0.582957  [   69/   88]
per-ex loss: 0.537526  [   70/   88]
per-ex loss: 0.604989  [   71/   88]
per-ex loss: 0.565260  [   72/   88]
per-ex loss: 0.776238  [   73/   88]
per-ex loss: 0.522891  [   74/   88]
per-ex loss: 0.676872  [   75/   88]
per-ex loss: 0.739232  [   76/   88]
per-ex loss: 0.609753  [   77/   88]
per-ex loss: 0.821012  [   78/   88]
per-ex loss: 0.585109  [   79/   88]
per-ex loss: 0.741125  [   80/   88]
per-ex loss: 0.646941  [   81/   88]
per-ex loss: 0.807138  [   82/   88]
per-ex loss: 0.580974  [   83/   88]
per-ex loss: 0.628357  [   84/   88]
per-ex loss: 0.576563  [   85/   88]
per-ex loss: 0.761384  [   86/   88]
per-ex loss: 0.787931  [   87/   88]
per-ex loss: 0.645918  [   88/   88]
Train Error: Avg loss: 0.65604735
validation Error: 
 Avg loss: 0.69781414 
 F1: 0.491717 
 Precision: 0.517400 
 Recall: 0.468463
 IoU: 0.326011

test Error: 
 Avg loss: 0.64738838 
 F1: 0.567195 
 Precision: 0.563244 
 Recall: 0.571202
 IoU: 0.395863

We have finished training iteration 46
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_36_.pth
per-ex loss: 0.723024  [    1/   88]
per-ex loss: 0.543863  [    2/   88]
per-ex loss: 0.769228  [    3/   88]
per-ex loss: 0.639029  [    4/   88]
per-ex loss: 0.806237  [    5/   88]
per-ex loss: 0.619298  [    6/   88]
per-ex loss: 0.585320  [    7/   88]
per-ex loss: 0.589651  [    8/   88]
per-ex loss: 0.506437  [    9/   88]
per-ex loss: 0.532289  [   10/   88]
per-ex loss: 0.572057  [   11/   88]
per-ex loss: 0.608753  [   12/   88]
per-ex loss: 0.479732  [   13/   88]
per-ex loss: 0.746771  [   14/   88]
per-ex loss: 0.782407  [   15/   88]
per-ex loss: 0.606386  [   16/   88]
per-ex loss: 0.513414  [   17/   88]
per-ex loss: 0.689651  [   18/   88]
per-ex loss: 0.779262  [   19/   88]
per-ex loss: 0.685136  [   20/   88]
per-ex loss: 0.683024  [   21/   88]
per-ex loss: 0.626962  [   22/   88]
per-ex loss: 0.754597  [   23/   88]
per-ex loss: 0.713220  [   24/   88]
per-ex loss: 0.525764  [   25/   88]
per-ex loss: 0.578847  [   26/   88]
per-ex loss: 0.715815  [   27/   88]
per-ex loss: 0.800230  [   28/   88]
per-ex loss: 0.685013  [   29/   88]
per-ex loss: 0.749872  [   30/   88]
per-ex loss: 0.537560  [   31/   88]
per-ex loss: 0.653074  [   32/   88]
per-ex loss: 0.679538  [   33/   88]
per-ex loss: 0.767541  [   34/   88]
per-ex loss: 0.740171  [   35/   88]
per-ex loss: 0.712959  [   36/   88]
per-ex loss: 0.554984  [   37/   88]
per-ex loss: 0.611144  [   38/   88]
per-ex loss: 0.550154  [   39/   88]
per-ex loss: 0.693285  [   40/   88]
per-ex loss: 0.530346  [   41/   88]
per-ex loss: 0.795026  [   42/   88]
per-ex loss: 0.527275  [   43/   88]
per-ex loss: 0.732308  [   44/   88]
per-ex loss: 0.619294  [   45/   88]
per-ex loss: 0.515306  [   46/   88]
per-ex loss: 0.632185  [   47/   88]
per-ex loss: 0.763465  [   48/   88]
per-ex loss: 0.784962  [   49/   88]
per-ex loss: 0.791457  [   50/   88]
per-ex loss: 0.700682  [   51/   88]
per-ex loss: 0.669900  [   52/   88]
per-ex loss: 0.709253  [   53/   88]
per-ex loss: 0.517500  [   54/   88]
per-ex loss: 0.619521  [   55/   88]
per-ex loss: 0.557715  [   56/   88]
per-ex loss: 0.784725  [   57/   88]
per-ex loss: 0.519133  [   58/   88]
per-ex loss: 0.709439  [   59/   88]
per-ex loss: 0.725305  [   60/   88]
per-ex loss: 0.732411  [   61/   88]
per-ex loss: 0.588055  [   62/   88]
per-ex loss: 0.573210  [   63/   88]
per-ex loss: 0.810375  [   64/   88]
per-ex loss: 0.564392  [   65/   88]
per-ex loss: 0.521777  [   66/   88]
per-ex loss: 0.600530  [   67/   88]
per-ex loss: 0.767939  [   68/   88]
per-ex loss: 0.570170  [   69/   88]
per-ex loss: 0.737511  [   70/   88]
per-ex loss: 0.514242  [   71/   88]
per-ex loss: 0.779025  [   72/   88]
per-ex loss: 0.539946  [   73/   88]
per-ex loss: 0.679223  [   74/   88]
per-ex loss: 0.526626  [   75/   88]
per-ex loss: 0.612065  [   76/   88]
per-ex loss: 0.602599  [   77/   88]
per-ex loss: 0.564381  [   78/   88]
per-ex loss: 0.747785  [   79/   88]
per-ex loss: 0.592690  [   80/   88]
per-ex loss: 0.807325  [   81/   88]
per-ex loss: 0.570640  [   82/   88]
per-ex loss: 0.638361  [   83/   88]
per-ex loss: 0.565651  [   84/   88]
per-ex loss: 0.547370  [   85/   88]
per-ex loss: 0.569218  [   86/   88]
per-ex loss: 0.851649  [   87/   88]
per-ex loss: 0.583525  [   88/   88]
Train Error: Avg loss: 0.64858165
validation Error: 
 Avg loss: 0.71391281 
 F1: 0.469648 
 Precision: 0.458041 
 Recall: 0.481859
 IoU: 0.306889

test Error: 
 Avg loss: 0.64429398 
 F1: 0.568233 
 Precision: 0.561209 
 Recall: 0.575435
 IoU: 0.396875

We have finished training iteration 47
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_45_.pth
per-ex loss: 0.587705  [    1/   88]
per-ex loss: 0.729908  [    2/   88]
per-ex loss: 0.565537  [    3/   88]
per-ex loss: 0.794434  [    4/   88]
per-ex loss: 0.647765  [    5/   88]
per-ex loss: 0.550523  [    6/   88]
per-ex loss: 0.711667  [    7/   88]
per-ex loss: 0.618098  [    8/   88]
per-ex loss: 0.579390  [    9/   88]
per-ex loss: 0.609347  [   10/   88]
per-ex loss: 0.532364  [   11/   88]
per-ex loss: 0.756623  [   12/   88]
per-ex loss: 0.782035  [   13/   88]
per-ex loss: 0.543435  [   14/   88]
per-ex loss: 0.613469  [   15/   88]
per-ex loss: 0.555187  [   16/   88]
per-ex loss: 0.515479  [   17/   88]
per-ex loss: 0.701281  [   18/   88]
per-ex loss: 0.703469  [   19/   88]
per-ex loss: 0.549127  [   20/   88]
per-ex loss: 0.727412  [   21/   88]
per-ex loss: 0.779801  [   22/   88]
per-ex loss: 0.594057  [   23/   88]
per-ex loss: 0.585596  [   24/   88]
per-ex loss: 0.569887  [   25/   88]
per-ex loss: 0.725539  [   26/   88]
per-ex loss: 0.597524  [   27/   88]
per-ex loss: 0.749948  [   28/   88]
per-ex loss: 0.676788  [   29/   88]
per-ex loss: 0.760928  [   30/   88]
per-ex loss: 0.597689  [   31/   88]
per-ex loss: 0.768143  [   32/   88]
per-ex loss: 0.705088  [   33/   88]
per-ex loss: 0.779348  [   34/   88]
per-ex loss: 0.743505  [   35/   88]
per-ex loss: 0.552596  [   36/   88]
per-ex loss: 0.625493  [   37/   88]
per-ex loss: 0.557770  [   38/   88]
per-ex loss: 0.807902  [   39/   88]
per-ex loss: 0.554012  [   40/   88]
per-ex loss: 0.516380  [   41/   88]
per-ex loss: 0.611420  [   42/   88]
per-ex loss: 0.571966  [   43/   88]
per-ex loss: 0.699381  [   44/   88]
per-ex loss: 0.754476  [   45/   88]
per-ex loss: 0.562457  [   46/   88]
per-ex loss: 0.708828  [   47/   88]
per-ex loss: 0.537330  [   48/   88]
per-ex loss: 0.827502  [   49/   88]
per-ex loss: 0.528897  [   50/   88]
per-ex loss: 0.499848  [   51/   88]
per-ex loss: 0.527881  [   52/   88]
per-ex loss: 0.628579  [   53/   88]
per-ex loss: 0.598902  [   54/   88]
per-ex loss: 0.566302  [   55/   88]
per-ex loss: 0.477983  [   56/   88]
per-ex loss: 0.580936  [   57/   88]
per-ex loss: 0.666200  [   58/   88]
per-ex loss: 0.805074  [   59/   88]
per-ex loss: 0.542563  [   60/   88]
per-ex loss: 0.754361  [   61/   88]
per-ex loss: 0.557536  [   62/   88]
per-ex loss: 0.789219  [   63/   88]
per-ex loss: 0.707172  [   64/   88]
per-ex loss: 0.597023  [   65/   88]
per-ex loss: 0.721478  [   66/   88]
per-ex loss: 0.761618  [   67/   88]
per-ex loss: 0.818949  [   68/   88]
per-ex loss: 0.521541  [   69/   88]
per-ex loss: 0.698897  [   70/   88]
per-ex loss: 0.704971  [   71/   88]
per-ex loss: 0.712096  [   72/   88]
per-ex loss: 0.573744  [   73/   88]
per-ex loss: 0.780160  [   74/   88]
per-ex loss: 0.538058  [   75/   88]
per-ex loss: 0.671447  [   76/   88]
per-ex loss: 0.586579  [   77/   88]
per-ex loss: 0.624664  [   78/   88]
per-ex loss: 0.567417  [   79/   88]
per-ex loss: 0.752883  [   80/   88]
per-ex loss: 0.584426  [   81/   88]
per-ex loss: 0.686149  [   82/   88]
per-ex loss: 0.718732  [   83/   88]
per-ex loss: 0.814999  [   84/   88]
per-ex loss: 0.604878  [   85/   88]
per-ex loss: 0.730690  [   86/   88]
per-ex loss: 0.614018  [   87/   88]
per-ex loss: 0.582121  [   88/   88]
Train Error: Avg loss: 0.64880225
validation Error: 
 Avg loss: 0.69967292 
 F1: 0.486603 
 Precision: 0.531244 
 Recall: 0.448884
 IoU: 0.321531

test Error: 
 Avg loss: 0.64391615 
 F1: 0.569166 
 Precision: 0.606921 
 Recall: 0.535834
 IoU: 0.397787

We have finished training iteration 48
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_46_.pth
per-ex loss: 0.559638  [    1/   88]
per-ex loss: 0.779445  [    2/   88]
per-ex loss: 0.578480  [    3/   88]
per-ex loss: 0.519450  [    4/   88]
per-ex loss: 0.712542  [    5/   88]
per-ex loss: 0.732961  [    6/   88]
per-ex loss: 0.613335  [    7/   88]
per-ex loss: 0.572894  [    8/   88]
per-ex loss: 0.568114  [    9/   88]
per-ex loss: 0.717741  [   10/   88]
per-ex loss: 0.510631  [   11/   88]
per-ex loss: 0.617831  [   12/   88]
per-ex loss: 0.769312  [   13/   88]
per-ex loss: 0.723511  [   14/   88]
per-ex loss: 0.594052  [   15/   88]
per-ex loss: 0.714679  [   16/   88]
per-ex loss: 0.612336  [   17/   88]
per-ex loss: 0.754569  [   18/   88]
per-ex loss: 0.539911  [   19/   88]
per-ex loss: 0.552046  [   20/   88]
per-ex loss: 0.739108  [   21/   88]
per-ex loss: 0.826728  [   22/   88]
per-ex loss: 0.689998  [   23/   88]
per-ex loss: 0.662548  [   24/   88]
per-ex loss: 0.790860  [   25/   88]
per-ex loss: 0.630435  [   26/   88]
per-ex loss: 0.766760  [   27/   88]
per-ex loss: 0.785615  [   28/   88]
per-ex loss: 0.680320  [   29/   88]
per-ex loss: 0.556405  [   30/   88]
per-ex loss: 0.736454  [   31/   88]
per-ex loss: 0.546215  [   32/   88]
per-ex loss: 0.545168  [   33/   88]
per-ex loss: 0.585124  [   34/   88]
per-ex loss: 0.736282  [   35/   88]
per-ex loss: 0.523407  [   36/   88]
per-ex loss: 0.581945  [   37/   88]
per-ex loss: 0.475963  [   38/   88]
per-ex loss: 0.585202  [   39/   88]
per-ex loss: 0.816237  [   40/   88]
per-ex loss: 0.791526  [   41/   88]
per-ex loss: 0.516959  [   42/   88]
per-ex loss: 0.786985  [   43/   88]
per-ex loss: 0.540652  [   44/   88]
per-ex loss: 0.528328  [   45/   88]
per-ex loss: 0.519527  [   46/   88]
per-ex loss: 0.663370  [   47/   88]
per-ex loss: 0.535076  [   48/   88]
per-ex loss: 0.829948  [   49/   88]
per-ex loss: 0.578765  [   50/   88]
per-ex loss: 0.563370  [   51/   88]
per-ex loss: 0.722987  [   52/   88]
per-ex loss: 0.604308  [   53/   88]
per-ex loss: 0.618369  [   54/   88]
per-ex loss: 0.560228  [   55/   88]
per-ex loss: 0.600957  [   56/   88]
per-ex loss: 0.531449  [   57/   88]
per-ex loss: 0.769286  [   58/   88]
per-ex loss: 0.576181  [   59/   88]
per-ex loss: 0.603106  [   60/   88]
per-ex loss: 0.779894  [   61/   88]
per-ex loss: 0.675366  [   62/   88]
per-ex loss: 0.507047  [   63/   88]
per-ex loss: 0.544669  [   64/   88]
per-ex loss: 0.746787  [   65/   88]
per-ex loss: 0.537329  [   66/   88]
per-ex loss: 0.729439  [   67/   88]
per-ex loss: 0.681696  [   68/   88]
per-ex loss: 0.744117  [   69/   88]
per-ex loss: 0.638248  [   70/   88]
per-ex loss: 0.733921  [   71/   88]
per-ex loss: 0.557236  [   72/   88]
per-ex loss: 0.612799  [   73/   88]
per-ex loss: 0.806244  [   74/   88]
per-ex loss: 0.526083  [   75/   88]
per-ex loss: 0.670216  [   76/   88]
per-ex loss: 0.559152  [   77/   88]
per-ex loss: 0.751513  [   78/   88]
per-ex loss: 0.708895  [   79/   88]
per-ex loss: 0.796878  [   80/   88]
per-ex loss: 0.643864  [   81/   88]
per-ex loss: 0.812187  [   82/   88]
per-ex loss: 0.703015  [   83/   88]
per-ex loss: 0.597478  [   84/   88]
per-ex loss: 0.590165  [   85/   88]
per-ex loss: 0.686159  [   86/   88]
per-ex loss: 0.579258  [   87/   88]
per-ex loss: 0.468862  [   88/   88]
Train Error: Avg loss: 0.64584258
validation Error: 
 Avg loss: 0.69877634 
 F1: 0.489438 
 Precision: 0.560260 
 Recall: 0.434512
 IoU: 0.324011

test Error: 
 Avg loss: 0.63933432 
 F1: 0.568488 
 Precision: 0.646155 
 Recall: 0.507489
 IoU: 0.397124

We have finished training iteration 49
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_47_.pth
per-ex loss: 0.540322  [    1/   88]
per-ex loss: 0.627753  [    2/   88]
per-ex loss: 0.782648  [    3/   88]
per-ex loss: 0.750533  [    4/   88]
per-ex loss: 0.703645  [    5/   88]
per-ex loss: 0.632615  [    6/   88]
per-ex loss: 0.652794  [    7/   88]
per-ex loss: 0.546293  [    8/   88]
per-ex loss: 0.726676  [    9/   88]
per-ex loss: 0.717619  [   10/   88]
per-ex loss: 0.516388  [   11/   88]
per-ex loss: 0.547479  [   12/   88]
per-ex loss: 0.696085  [   13/   88]
per-ex loss: 0.807588  [   14/   88]
per-ex loss: 0.709497  [   15/   88]
per-ex loss: 0.594608  [   16/   88]
per-ex loss: 0.523503  [   17/   88]
per-ex loss: 0.557276  [   18/   88]
per-ex loss: 0.700558  [   19/   88]
per-ex loss: 0.537092  [   20/   88]
per-ex loss: 0.666274  [   21/   88]
per-ex loss: 0.482934  [   22/   88]
per-ex loss: 0.735752  [   23/   88]
per-ex loss: 0.514599  [   24/   88]
per-ex loss: 0.747746  [   25/   88]
per-ex loss: 0.762063  [   26/   88]
per-ex loss: 0.501231  [   27/   88]
per-ex loss: 0.729141  [   28/   88]
per-ex loss: 0.624315  [   29/   88]
per-ex loss: 0.821199  [   30/   88]
per-ex loss: 0.755005  [   31/   88]
per-ex loss: 0.617591  [   32/   88]
per-ex loss: 0.552173  [   33/   88]
per-ex loss: 0.541136  [   34/   88]
per-ex loss: 0.635124  [   35/   88]
per-ex loss: 0.685401  [   36/   88]
per-ex loss: 0.720750  [   37/   88]
per-ex loss: 0.606675  [   38/   88]
per-ex loss: 0.557306  [   39/   88]
per-ex loss: 0.587237  [   40/   88]
per-ex loss: 0.716559  [   41/   88]
per-ex loss: 0.568129  [   42/   88]
per-ex loss: 0.797001  [   43/   88]
per-ex loss: 0.484775  [   44/   88]
per-ex loss: 0.561681  [   45/   88]
per-ex loss: 0.740694  [   46/   88]
per-ex loss: 0.612153  [   47/   88]
per-ex loss: 0.805491  [   48/   88]
per-ex loss: 0.583199  [   49/   88]
per-ex loss: 0.536439  [   50/   88]
per-ex loss: 0.688506  [   51/   88]
per-ex loss: 0.538856  [   52/   88]
per-ex loss: 0.786723  [   53/   88]
per-ex loss: 0.656754  [   54/   88]
per-ex loss: 0.559362  [   55/   88]
per-ex loss: 0.697602  [   56/   88]
per-ex loss: 0.762193  [   57/   88]
per-ex loss: 0.557631  [   58/   88]
per-ex loss: 0.819308  [   59/   88]
per-ex loss: 0.592957  [   60/   88]
per-ex loss: 0.610277  [   61/   88]
per-ex loss: 0.602811  [   62/   88]
per-ex loss: 0.773141  [   63/   88]
per-ex loss: 0.646122  [   64/   88]
per-ex loss: 0.798752  [   65/   88]
per-ex loss: 0.577210  [   66/   88]
per-ex loss: 0.771249  [   67/   88]
per-ex loss: 0.587928  [   68/   88]
per-ex loss: 0.740088  [   69/   88]
per-ex loss: 0.508932  [   70/   88]
per-ex loss: 0.559505  [   71/   88]
per-ex loss: 0.593787  [   72/   88]
per-ex loss: 0.681113  [   73/   88]
per-ex loss: 0.667459  [   74/   88]
per-ex loss: 0.748903  [   75/   88]
per-ex loss: 0.777973  [   76/   88]
per-ex loss: 0.726359  [   77/   88]
per-ex loss: 0.503848  [   78/   88]
per-ex loss: 0.553823  [   79/   88]
per-ex loss: 0.585456  [   80/   88]
per-ex loss: 0.767510  [   81/   88]
per-ex loss: 0.720482  [   82/   88]
per-ex loss: 0.611543  [   83/   88]
per-ex loss: 0.574986  [   84/   88]
per-ex loss: 0.542543  [   85/   88]
per-ex loss: 0.585822  [   86/   88]
per-ex loss: 0.632289  [   87/   88]
per-ex loss: 0.761595  [   88/   88]
Train Error: Avg loss: 0.64763797
validation Error: 
 Avg loss: 0.69644326 
 F1: 0.491405 
 Precision: 0.591388 
 Recall: 0.420341
 IoU: 0.325737

test Error: 
 Avg loss: 0.64504368 
 F1: 0.567209 
 Precision: 0.646634 
 Recall: 0.505160
 IoU: 0.395877

We have finished training iteration 50
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_48_.pth
per-ex loss: 0.618434  [    1/   88]
per-ex loss: 0.542874  [    2/   88]
per-ex loss: 0.805376  [    3/   88]
per-ex loss: 0.538934  [    4/   88]
per-ex loss: 0.761945  [    5/   88]
per-ex loss: 0.721146  [    6/   88]
per-ex loss: 0.553990  [    7/   88]
per-ex loss: 0.767792  [    8/   88]
per-ex loss: 0.777590  [    9/   88]
per-ex loss: 0.761367  [   10/   88]
per-ex loss: 0.508260  [   11/   88]
per-ex loss: 0.760760  [   12/   88]
per-ex loss: 0.542437  [   13/   88]
per-ex loss: 0.652743  [   14/   88]
per-ex loss: 0.640668  [   15/   88]
per-ex loss: 0.678194  [   16/   88]
per-ex loss: 0.604927  [   17/   88]
per-ex loss: 0.609052  [   18/   88]
per-ex loss: 0.624130  [   19/   88]
per-ex loss: 0.693159  [   20/   88]
per-ex loss: 0.711159  [   21/   88]
per-ex loss: 0.709841  [   22/   88]
per-ex loss: 0.760426  [   23/   88]
per-ex loss: 0.521134  [   24/   88]
per-ex loss: 0.786508  [   25/   88]
per-ex loss: 0.666612  [   26/   88]
per-ex loss: 0.579328  [   27/   88]
per-ex loss: 0.675439  [   28/   88]
per-ex loss: 0.617669  [   29/   88]
per-ex loss: 0.791544  [   30/   88]
per-ex loss: 0.564584  [   31/   88]
per-ex loss: 0.515609  [   32/   88]
per-ex loss: 0.711337  [   33/   88]
per-ex loss: 0.554071  [   34/   88]
per-ex loss: 0.644195  [   35/   88]
per-ex loss: 0.749723  [   36/   88]
per-ex loss: 0.754698  [   37/   88]
per-ex loss: 0.640748  [   38/   88]
per-ex loss: 0.692356  [   39/   88]
per-ex loss: 0.540260  [   40/   88]
per-ex loss: 0.756571  [   41/   88]
per-ex loss: 0.810884  [   42/   88]
per-ex loss: 0.550407  [   43/   88]
per-ex loss: 0.790599  [   44/   88]
per-ex loss: 0.709766  [   45/   88]
per-ex loss: 0.707795  [   46/   88]
per-ex loss: 0.582505  [   47/   88]
per-ex loss: 0.544836  [   48/   88]
per-ex loss: 0.708159  [   49/   88]
per-ex loss: 0.580522  [   50/   88]
per-ex loss: 0.618004  [   51/   88]
per-ex loss: 0.567655  [   52/   88]
per-ex loss: 0.788869  [   53/   88]
per-ex loss: 0.503922  [   54/   88]
per-ex loss: 0.558663  [   55/   88]
per-ex loss: 0.735514  [   56/   88]
per-ex loss: 0.514108  [   57/   88]
per-ex loss: 0.510130  [   58/   88]
per-ex loss: 0.738684  [   59/   88]
per-ex loss: 0.667668  [   60/   88]
per-ex loss: 0.786705  [   61/   88]
per-ex loss: 0.596151  [   62/   88]
per-ex loss: 0.755427  [   63/   88]
per-ex loss: 0.586240  [   64/   88]
per-ex loss: 0.586946  [   65/   88]
per-ex loss: 0.701808  [   66/   88]
per-ex loss: 0.578170  [   67/   88]
per-ex loss: 0.556700  [   68/   88]
per-ex loss: 0.636322  [   69/   88]
per-ex loss: 0.699779  [   70/   88]
per-ex loss: 0.535587  [   71/   88]
per-ex loss: 0.584080  [   72/   88]
per-ex loss: 0.431366  [   73/   88]
per-ex loss: 0.552234  [   74/   88]
per-ex loss: 0.485832  [   75/   88]
per-ex loss: 0.590082  [   76/   88]
per-ex loss: 0.521503  [   77/   88]
per-ex loss: 0.519339  [   78/   88]
per-ex loss: 0.823802  [   79/   88]
per-ex loss: 0.756258  [   80/   88]
per-ex loss: 0.585181  [   81/   88]
per-ex loss: 0.824908  [   82/   88]
per-ex loss: 0.689612  [   83/   88]
per-ex loss: 0.542030  [   84/   88]
per-ex loss: 0.576955  [   85/   88]
per-ex loss: 0.725106  [   86/   88]
per-ex loss: 0.775280  [   87/   88]
per-ex loss: 0.547820  [   88/   88]
Train Error: Avg loss: 0.64599435
validation Error: 
 Avg loss: 0.72048347 
 F1: 0.458517 
 Precision: 0.566937 
 Recall: 0.384908
 IoU: 0.297452

test Error: 
 Avg loss: 0.66111891 
 F1: 0.549590 
 Precision: 0.668486 
 Recall: 0.466601
 IoU: 0.378920

We have finished training iteration 51
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_49_.pth
per-ex loss: 0.693611  [    1/   88]
per-ex loss: 0.537852  [    2/   88]
per-ex loss: 0.622484  [    3/   88]
per-ex loss: 0.557888  [    4/   88]
per-ex loss: 0.775582  [    5/   88]
per-ex loss: 0.530906  [    6/   88]
per-ex loss: 0.590993  [    7/   88]
per-ex loss: 0.704599  [    8/   88]
per-ex loss: 0.579541  [    9/   88]
per-ex loss: 0.516048  [   10/   88]
per-ex loss: 0.563340  [   11/   88]
per-ex loss: 0.560592  [   12/   88]
per-ex loss: 0.576564  [   13/   88]
per-ex loss: 0.543076  [   14/   88]
per-ex loss: 0.785623  [   15/   88]
per-ex loss: 0.819890  [   16/   88]
per-ex loss: 0.819938  [   17/   88]
per-ex loss: 0.565259  [   18/   88]
per-ex loss: 0.693302  [   19/   88]
per-ex loss: 0.547554  [   20/   88]
per-ex loss: 0.796311  [   21/   88]
per-ex loss: 0.725454  [   22/   88]
per-ex loss: 0.527754  [   23/   88]
per-ex loss: 0.736193  [   24/   88]
per-ex loss: 0.778841  [   25/   88]
per-ex loss: 0.716336  [   26/   88]
per-ex loss: 0.716481  [   27/   88]
per-ex loss: 0.767522  [   28/   88]
per-ex loss: 0.709486  [   29/   88]
per-ex loss: 0.604518  [   30/   88]
per-ex loss: 0.552012  [   31/   88]
per-ex loss: 0.726388  [   32/   88]
per-ex loss: 0.629679  [   33/   88]
per-ex loss: 0.683643  [   34/   88]
per-ex loss: 0.672964  [   35/   88]
per-ex loss: 0.631692  [   36/   88]
per-ex loss: 0.573657  [   37/   88]
per-ex loss: 0.659750  [   38/   88]
per-ex loss: 0.520621  [   39/   88]
per-ex loss: 0.579890  [   40/   88]
per-ex loss: 0.571629  [   41/   88]
per-ex loss: 0.817833  [   42/   88]
per-ex loss: 0.611711  [   43/   88]
per-ex loss: 0.502938  [   44/   88]
per-ex loss: 0.602137  [   45/   88]
per-ex loss: 0.798477  [   46/   88]
per-ex loss: 0.545846  [   47/   88]
per-ex loss: 0.808783  [   48/   88]
per-ex loss: 0.457012  [   49/   88]
per-ex loss: 0.760279  [   50/   88]
per-ex loss: 0.574777  [   51/   88]
per-ex loss: 0.608915  [   52/   88]
per-ex loss: 0.619424  [   53/   88]
per-ex loss: 0.670637  [   54/   88]
per-ex loss: 0.468334  [   55/   88]
per-ex loss: 0.661214  [   56/   88]
per-ex loss: 0.527267  [   57/   88]
per-ex loss: 0.722030  [   58/   88]
per-ex loss: 0.527496  [   59/   88]
per-ex loss: 0.544863  [   60/   88]
per-ex loss: 0.710452  [   61/   88]
per-ex loss: 0.740154  [   62/   88]
per-ex loss: 0.782742  [   63/   88]
per-ex loss: 0.723372  [   64/   88]
per-ex loss: 0.819294  [   65/   88]
per-ex loss: 0.685057  [   66/   88]
per-ex loss: 0.527691  [   67/   88]
per-ex loss: 0.573822  [   68/   88]
per-ex loss: 0.616499  [   69/   88]
per-ex loss: 0.627881  [   70/   88]
per-ex loss: 0.547554  [   71/   88]
per-ex loss: 0.748652  [   72/   88]
per-ex loss: 0.584447  [   73/   88]
per-ex loss: 0.726938  [   74/   88]
per-ex loss: 0.739311  [   75/   88]
per-ex loss: 0.602326  [   76/   88]
per-ex loss: 0.579287  [   77/   88]
per-ex loss: 0.770573  [   78/   88]
per-ex loss: 0.544223  [   79/   88]
per-ex loss: 0.757790  [   80/   88]
per-ex loss: 0.766752  [   81/   88]
per-ex loss: 0.527203  [   82/   88]
per-ex loss: 0.603269  [   83/   88]
per-ex loss: 0.581649  [   84/   88]
per-ex loss: 0.706715  [   85/   88]
per-ex loss: 0.584613  [   86/   88]
per-ex loss: 0.700200  [   87/   88]
per-ex loss: 0.691383  [   88/   88]
Train Error: Avg loss: 0.64619643
validation Error: 
 Avg loss: 0.70344211 
 F1: 0.482751 
 Precision: 0.544151 
 Recall: 0.433802
 IoU: 0.318175

test Error: 
 Avg loss: 0.63642153 
 F1: 0.573667 
 Precision: 0.647316 
 Recall: 0.515065
 IoU: 0.402197

We have finished training iteration 52
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_50_.pth
per-ex loss: 0.591039  [    1/   88]
per-ex loss: 0.501252  [    2/   88]
per-ex loss: 0.591055  [    3/   88]
per-ex loss: 0.623157  [    4/   88]
per-ex loss: 0.732468  [    5/   88]
per-ex loss: 0.533480  [    6/   88]
per-ex loss: 0.603633  [    7/   88]
per-ex loss: 0.630108  [    8/   88]
per-ex loss: 0.700386  [    9/   88]
per-ex loss: 0.694216  [   10/   88]
per-ex loss: 0.617694  [   11/   88]
per-ex loss: 0.809110  [   12/   88]
per-ex loss: 0.757729  [   13/   88]
per-ex loss: 0.806979  [   14/   88]
per-ex loss: 0.802347  [   15/   88]
per-ex loss: 0.612368  [   16/   88]
per-ex loss: 0.558132  [   17/   88]
per-ex loss: 0.781515  [   18/   88]
per-ex loss: 0.612659  [   19/   88]
per-ex loss: 0.794226  [   20/   88]
per-ex loss: 0.727215  [   21/   88]
per-ex loss: 0.561530  [   22/   88]
per-ex loss: 0.669199  [   23/   88]
per-ex loss: 0.659305  [   24/   88]
per-ex loss: 0.562426  [   25/   88]
per-ex loss: 0.514414  [   26/   88]
per-ex loss: 0.513922  [   27/   88]
per-ex loss: 0.692975  [   28/   88]
per-ex loss: 0.520014  [   29/   88]
per-ex loss: 0.700661  [   30/   88]
per-ex loss: 0.592553  [   31/   88]
per-ex loss: 0.551243  [   32/   88]
per-ex loss: 0.554935  [   33/   88]
per-ex loss: 0.621497  [   34/   88]
per-ex loss: 0.634599  [   35/   88]
per-ex loss: 0.650850  [   36/   88]
per-ex loss: 0.549851  [   37/   88]
per-ex loss: 0.753679  [   38/   88]
per-ex loss: 0.573547  [   39/   88]
per-ex loss: 0.727971  [   40/   88]
per-ex loss: 0.574231  [   41/   88]
per-ex loss: 0.508433  [   42/   88]
per-ex loss: 0.717467  [   43/   88]
per-ex loss: 0.672091  [   44/   88]
per-ex loss: 0.531787  [   45/   88]
per-ex loss: 0.579081  [   46/   88]
per-ex loss: 0.780794  [   47/   88]
per-ex loss: 0.693365  [   48/   88]
per-ex loss: 0.517828  [   49/   88]
per-ex loss: 0.582164  [   50/   88]
per-ex loss: 0.739232  [   51/   88]
per-ex loss: 0.768280  [   52/   88]
per-ex loss: 0.613913  [   53/   88]
per-ex loss: 0.578541  [   54/   88]
per-ex loss: 0.719915  [   55/   88]
per-ex loss: 0.693532  [   56/   88]
per-ex loss: 0.761478  [   57/   88]
per-ex loss: 0.708283  [   58/   88]
per-ex loss: 0.734568  [   59/   88]
per-ex loss: 0.545709  [   60/   88]
per-ex loss: 0.756157  [   61/   88]
per-ex loss: 0.722496  [   62/   88]
per-ex loss: 0.513749  [   63/   88]
per-ex loss: 0.719829  [   64/   88]
per-ex loss: 0.605732  [   65/   88]
per-ex loss: 0.522063  [   66/   88]
per-ex loss: 0.571013  [   67/   88]
per-ex loss: 0.636242  [   68/   88]
per-ex loss: 0.561696  [   69/   88]
per-ex loss: 0.801797  [   70/   88]
per-ex loss: 0.771664  [   71/   88]
per-ex loss: 0.526624  [   72/   88]
per-ex loss: 0.739687  [   73/   88]
per-ex loss: 0.584808  [   74/   88]
per-ex loss: 0.561453  [   75/   88]
per-ex loss: 0.643596  [   76/   88]
per-ex loss: 0.513131  [   77/   88]
per-ex loss: 0.738407  [   78/   88]
per-ex loss: 0.817929  [   79/   88]
per-ex loss: 0.776979  [   80/   88]
per-ex loss: 0.746591  [   81/   88]
per-ex loss: 0.489007  [   82/   88]
per-ex loss: 0.709930  [   83/   88]
per-ex loss: 0.830370  [   84/   88]
per-ex loss: 0.521720  [   85/   88]
per-ex loss: 0.554983  [   86/   88]
per-ex loss: 0.749955  [   87/   88]
per-ex loss: 0.599076  [   88/   88]
Train Error: Avg loss: 0.64765131
validation Error: 
 Avg loss: 0.69700940 
 F1: 0.485563 
 Precision: 0.517568 
 Recall: 0.457286
 IoU: 0.320623

test Error: 
 Avg loss: 0.64392555 
 F1: 0.569047 
 Precision: 0.588137 
 Recall: 0.551158
 IoU: 0.397670

We have finished training iteration 53
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_51_.pth
per-ex loss: 0.803839  [    1/   88]
per-ex loss: 0.618395  [    2/   88]
per-ex loss: 0.532967  [    3/   88]
per-ex loss: 0.739607  [    4/   88]
per-ex loss: 0.571711  [    5/   88]
per-ex loss: 0.598286  [    6/   88]
per-ex loss: 0.549137  [    7/   88]
per-ex loss: 0.579227  [    8/   88]
per-ex loss: 0.707692  [    9/   88]
per-ex loss: 0.577670  [   10/   88]
per-ex loss: 0.579784  [   11/   88]
per-ex loss: 0.617311  [   12/   88]
per-ex loss: 0.541563  [   13/   88]
per-ex loss: 0.687085  [   14/   88]
per-ex loss: 0.535294  [   15/   88]
per-ex loss: 0.815834  [   16/   88]
per-ex loss: 0.608568  [   17/   88]
per-ex loss: 0.527038  [   18/   88]
per-ex loss: 0.657411  [   19/   88]
per-ex loss: 0.574957  [   20/   88]
per-ex loss: 0.699441  [   21/   88]
per-ex loss: 0.538820  [   22/   88]
per-ex loss: 0.661345  [   23/   88]
per-ex loss: 0.583546  [   24/   88]
per-ex loss: 0.613152  [   25/   88]
per-ex loss: 0.708753  [   26/   88]
per-ex loss: 0.609721  [   27/   88]
per-ex loss: 0.729798  [   28/   88]
per-ex loss: 0.455152  [   29/   88]
per-ex loss: 0.596511  [   30/   88]
per-ex loss: 0.751344  [   31/   88]
per-ex loss: 0.518114  [   32/   88]
per-ex loss: 0.810149  [   33/   88]
per-ex loss: 0.656227  [   34/   88]
per-ex loss: 0.601946  [   35/   88]
per-ex loss: 0.603544  [   36/   88]
per-ex loss: 0.738723  [   37/   88]
per-ex loss: 0.759059  [   38/   88]
per-ex loss: 0.722469  [   39/   88]
per-ex loss: 0.711695  [   40/   88]
per-ex loss: 0.776535  [   41/   88]
per-ex loss: 0.562593  [   42/   88]
per-ex loss: 0.545259  [   43/   88]
per-ex loss: 0.591398  [   44/   88]
per-ex loss: 0.717701  [   45/   88]
per-ex loss: 0.569927  [   46/   88]
per-ex loss: 0.827021  [   47/   88]
per-ex loss: 0.573305  [   48/   88]
per-ex loss: 0.787753  [   49/   88]
per-ex loss: 0.593754  [   50/   88]
per-ex loss: 0.510952  [   51/   88]
per-ex loss: 0.557704  [   52/   88]
per-ex loss: 0.619192  [   53/   88]
per-ex loss: 0.647392  [   54/   88]
per-ex loss: 0.536005  [   55/   88]
per-ex loss: 0.662291  [   56/   88]
per-ex loss: 0.752609  [   57/   88]
per-ex loss: 0.706116  [   58/   88]
per-ex loss: 0.780821  [   59/   88]
per-ex loss: 0.779532  [   60/   88]
per-ex loss: 0.628866  [   61/   88]
per-ex loss: 0.560356  [   62/   88]
per-ex loss: 0.545213  [   63/   88]
per-ex loss: 0.661978  [   64/   88]
per-ex loss: 0.725639  [   65/   88]
per-ex loss: 0.738140  [   66/   88]
per-ex loss: 0.702734  [   67/   88]
per-ex loss: 0.748137  [   68/   88]
per-ex loss: 0.515003  [   69/   88]
per-ex loss: 0.590926  [   70/   88]
per-ex loss: 0.686623  [   71/   88]
per-ex loss: 0.524971  [   72/   88]
per-ex loss: 0.548208  [   73/   88]
per-ex loss: 0.807055  [   74/   88]
per-ex loss: 0.705519  [   75/   88]
per-ex loss: 0.770508  [   76/   88]
per-ex loss: 0.504393  [   77/   88]
per-ex loss: 0.746129  [   78/   88]
per-ex loss: 0.567120  [   79/   88]
per-ex loss: 0.793657  [   80/   88]
per-ex loss: 0.531168  [   81/   88]
per-ex loss: 0.580740  [   82/   88]
per-ex loss: 0.754818  [   83/   88]
per-ex loss: 0.534074  [   84/   88]
per-ex loss: 0.709112  [   85/   88]
per-ex loss: 0.474497  [   86/   88]
per-ex loss: 0.710149  [   87/   88]
per-ex loss: 0.516065  [   88/   88]
Train Error: Avg loss: 0.64173346
validation Error: 
 Avg loss: 0.72166576 
 F1: 0.457219 
 Precision: 0.530910 
 Recall: 0.401492
 IoU: 0.296361

test Error: 
 Avg loss: 0.64897566 
 F1: 0.560060 
 Precision: 0.666513 
 Recall: 0.482928
 IoU: 0.388947

We have finished training iteration 54
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_52_.pth
per-ex loss: 0.614204  [    1/   88]
per-ex loss: 0.551517  [    2/   88]
per-ex loss: 0.464477  [    3/   88]
per-ex loss: 0.505492  [    4/   88]
per-ex loss: 0.824961  [    5/   88]
per-ex loss: 0.745965  [    6/   88]
per-ex loss: 0.774844  [    7/   88]
per-ex loss: 0.520934  [    8/   88]
per-ex loss: 0.550208  [    9/   88]
per-ex loss: 0.549769  [   10/   88]
per-ex loss: 0.769042  [   11/   88]
per-ex loss: 0.528940  [   12/   88]
per-ex loss: 0.748652  [   13/   88]
per-ex loss: 0.667351  [   14/   88]
per-ex loss: 0.693686  [   15/   88]
per-ex loss: 0.712376  [   16/   88]
per-ex loss: 0.782566  [   17/   88]
per-ex loss: 0.613207  [   18/   88]
per-ex loss: 0.610003  [   19/   88]
per-ex loss: 0.635965  [   20/   88]
per-ex loss: 0.567425  [   21/   88]
per-ex loss: 0.742381  [   22/   88]
per-ex loss: 0.738637  [   23/   88]
per-ex loss: 0.605158  [   24/   88]
per-ex loss: 0.487609  [   25/   88]
per-ex loss: 0.734367  [   26/   88]
per-ex loss: 0.727928  [   27/   88]
per-ex loss: 0.707129  [   28/   88]
per-ex loss: 0.791359  [   29/   88]
per-ex loss: 0.560192  [   30/   88]
per-ex loss: 0.711175  [   31/   88]
per-ex loss: 0.778194  [   32/   88]
per-ex loss: 0.769095  [   33/   88]
per-ex loss: 0.566263  [   34/   88]
per-ex loss: 0.731295  [   35/   88]
per-ex loss: 0.595964  [   36/   88]
per-ex loss: 0.569582  [   37/   88]
per-ex loss: 0.686020  [   38/   88]
per-ex loss: 0.705487  [   39/   88]
per-ex loss: 0.563801  [   40/   88]
per-ex loss: 0.644797  [   41/   88]
per-ex loss: 0.557064  [   42/   88]
per-ex loss: 0.744722  [   43/   88]
per-ex loss: 0.729035  [   44/   88]
per-ex loss: 0.799443  [   45/   88]
per-ex loss: 0.563114  [   46/   88]
per-ex loss: 0.688913  [   47/   88]
per-ex loss: 0.644821  [   48/   88]
per-ex loss: 0.580450  [   49/   88]
per-ex loss: 0.791671  [   50/   88]
per-ex loss: 0.533100  [   51/   88]
per-ex loss: 0.720356  [   52/   88]
per-ex loss: 0.562326  [   53/   88]
per-ex loss: 0.450322  [   54/   88]
per-ex loss: 0.801687  [   55/   88]
per-ex loss: 0.605922  [   56/   88]
per-ex loss: 0.549144  [   57/   88]
per-ex loss: 0.576765  [   58/   88]
per-ex loss: 0.630615  [   59/   88]
per-ex loss: 0.566275  [   60/   88]
per-ex loss: 0.615953  [   61/   88]
per-ex loss: 0.694931  [   62/   88]
per-ex loss: 0.555322  [   63/   88]
per-ex loss: 0.718557  [   64/   88]
per-ex loss: 0.792066  [   65/   88]
per-ex loss: 0.729260  [   66/   88]
per-ex loss: 0.676563  [   67/   88]
per-ex loss: 0.537121  [   68/   88]
per-ex loss: 0.575842  [   69/   88]
per-ex loss: 0.583309  [   70/   88]
per-ex loss: 0.700313  [   71/   88]
per-ex loss: 0.553747  [   72/   88]
per-ex loss: 0.547971  [   73/   88]
per-ex loss: 0.717118  [   74/   88]
per-ex loss: 0.595930  [   75/   88]
per-ex loss: 0.778597  [   76/   88]
per-ex loss: 0.605674  [   77/   88]
per-ex loss: 0.585971  [   78/   88]
per-ex loss: 0.548060  [   79/   88]
per-ex loss: 0.573197  [   80/   88]
per-ex loss: 0.763602  [   81/   88]
per-ex loss: 0.711945  [   82/   88]
per-ex loss: 0.555414  [   83/   88]
per-ex loss: 0.719113  [   84/   88]
per-ex loss: 0.540062  [   85/   88]
per-ex loss: 0.523075  [   86/   88]
per-ex loss: 0.588246  [   87/   88]
per-ex loss: 0.520525  [   88/   88]
Train Error: Avg loss: 0.64262778
validation Error: 
 Avg loss: 0.70659405 
 F1: 0.477173 
 Precision: 0.571411 
 Recall: 0.409619
 IoU: 0.313347

test Error: 
 Avg loss: 0.64648434 
 F1: 0.565123 
 Precision: 0.665744 
 Recall: 0.490924
 IoU: 0.393847

We have finished training iteration 55
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_53_.pth
per-ex loss: 0.552271  [    1/   88]
per-ex loss: 0.515976  [    2/   88]
per-ex loss: 0.767364  [    3/   88]
per-ex loss: 0.562152  [    4/   88]
per-ex loss: 0.759594  [    5/   88]
per-ex loss: 0.431649  [    6/   88]
per-ex loss: 0.751250  [    7/   88]
per-ex loss: 0.720104  [    8/   88]
per-ex loss: 0.729874  [    9/   88]
per-ex loss: 0.490089  [   10/   88]
per-ex loss: 0.612550  [   11/   88]
per-ex loss: 0.547276  [   12/   88]
per-ex loss: 0.832876  [   13/   88]
per-ex loss: 0.587449  [   14/   88]
per-ex loss: 0.591029  [   15/   88]
per-ex loss: 0.627443  [   16/   88]
per-ex loss: 0.791646  [   17/   88]
per-ex loss: 0.704091  [   18/   88]
per-ex loss: 0.674896  [   19/   88]
per-ex loss: 0.707389  [   20/   88]
per-ex loss: 0.739643  [   21/   88]
per-ex loss: 0.505667  [   22/   88]
per-ex loss: 0.607421  [   23/   88]
per-ex loss: 0.520714  [   24/   88]
per-ex loss: 0.650266  [   25/   88]
per-ex loss: 0.523911  [   26/   88]
per-ex loss: 0.803720  [   27/   88]
per-ex loss: 0.558467  [   28/   88]
per-ex loss: 0.631634  [   29/   88]
per-ex loss: 0.734883  [   30/   88]
per-ex loss: 0.536134  [   31/   88]
per-ex loss: 0.794618  [   32/   88]
per-ex loss: 0.742141  [   33/   88]
per-ex loss: 0.576878  [   34/   88]
per-ex loss: 0.577914  [   35/   88]
per-ex loss: 0.813046  [   36/   88]
per-ex loss: 0.668067  [   37/   88]
per-ex loss: 0.554924  [   38/   88]
per-ex loss: 0.748871  [   39/   88]
per-ex loss: 0.732248  [   40/   88]
per-ex loss: 0.525479  [   41/   88]
per-ex loss: 0.574711  [   42/   88]
per-ex loss: 0.571392  [   43/   88]
per-ex loss: 0.687734  [   44/   88]
per-ex loss: 0.675673  [   45/   88]
per-ex loss: 0.576792  [   46/   88]
per-ex loss: 0.540788  [   47/   88]
per-ex loss: 0.776275  [   48/   88]
per-ex loss: 0.744501  [   49/   88]
per-ex loss: 0.582837  [   50/   88]
per-ex loss: 0.695048  [   51/   88]
per-ex loss: 0.518034  [   52/   88]
per-ex loss: 0.732131  [   53/   88]
per-ex loss: 0.585633  [   54/   88]
per-ex loss: 0.689811  [   55/   88]
per-ex loss: 0.615320  [   56/   88]
per-ex loss: 0.761530  [   57/   88]
per-ex loss: 0.574307  [   58/   88]
per-ex loss: 0.580126  [   59/   88]
per-ex loss: 0.573610  [   60/   88]
per-ex loss: 0.686156  [   61/   88]
per-ex loss: 0.598701  [   62/   88]
per-ex loss: 0.644887  [   63/   88]
per-ex loss: 0.706644  [   64/   88]
per-ex loss: 0.530122  [   65/   88]
per-ex loss: 0.732700  [   66/   88]
per-ex loss: 0.707562  [   67/   88]
per-ex loss: 0.576277  [   68/   88]
per-ex loss: 0.565066  [   69/   88]
per-ex loss: 0.530377  [   70/   88]
per-ex loss: 0.620824  [   71/   88]
per-ex loss: 0.699298  [   72/   88]
per-ex loss: 0.809536  [   73/   88]
per-ex loss: 0.624416  [   74/   88]
per-ex loss: 0.751059  [   75/   88]
per-ex loss: 0.547855  [   76/   88]
per-ex loss: 0.609167  [   77/   88]
per-ex loss: 0.566004  [   78/   88]
per-ex loss: 0.747012  [   79/   88]
per-ex loss: 0.741831  [   80/   88]
per-ex loss: 0.749497  [   81/   88]
per-ex loss: 0.784640  [   82/   88]
per-ex loss: 0.614581  [   83/   88]
per-ex loss: 0.512041  [   84/   88]
per-ex loss: 0.704039  [   85/   88]
per-ex loss: 0.524362  [   86/   88]
per-ex loss: 0.517404  [   87/   88]
per-ex loss: 0.537125  [   88/   88]
Train Error: Avg loss: 0.64203473
validation Error: 
 Avg loss: 0.70095583 
 F1: 0.483551 
 Precision: 0.656229 
 Recall: 0.382818
 IoU: 0.318871

test Error: 
 Avg loss: 0.65198462 
 F1: 0.556490 
 Precision: 0.690118 
 Recall: 0.466216
 IoU: 0.385512

We have finished training iteration 56
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_54_.pth
per-ex loss: 0.794564  [    1/   88]
per-ex loss: 0.511884  [    2/   88]
per-ex loss: 0.572706  [    3/   88]
per-ex loss: 0.517266  [    4/   88]
per-ex loss: 0.582865  [    5/   88]
per-ex loss: 0.601233  [    6/   88]
per-ex loss: 0.754816  [    7/   88]
per-ex loss: 0.655111  [    8/   88]
per-ex loss: 0.561408  [    9/   88]
per-ex loss: 0.499811  [   10/   88]
per-ex loss: 0.744066  [   11/   88]
per-ex loss: 0.680552  [   12/   88]
per-ex loss: 0.539192  [   13/   88]
per-ex loss: 0.567941  [   14/   88]
per-ex loss: 0.719288  [   15/   88]
per-ex loss: 0.560617  [   16/   88]
per-ex loss: 0.608980  [   17/   88]
per-ex loss: 0.604597  [   18/   88]
per-ex loss: 0.690362  [   19/   88]
per-ex loss: 0.733686  [   20/   88]
per-ex loss: 0.700118  [   21/   88]
per-ex loss: 0.720328  [   22/   88]
per-ex loss: 0.664714  [   23/   88]
per-ex loss: 0.677659  [   24/   88]
per-ex loss: 0.699988  [   25/   88]
per-ex loss: 0.736250  [   26/   88]
per-ex loss: 0.483908  [   27/   88]
per-ex loss: 0.544851  [   28/   88]
per-ex loss: 0.659800  [   29/   88]
per-ex loss: 0.706192  [   30/   88]
per-ex loss: 0.530347  [   31/   88]
per-ex loss: 0.547647  [   32/   88]
per-ex loss: 0.767721  [   33/   88]
per-ex loss: 0.663299  [   34/   88]
per-ex loss: 0.530031  [   35/   88]
per-ex loss: 0.603382  [   36/   88]
per-ex loss: 0.612325  [   37/   88]
per-ex loss: 0.758116  [   38/   88]
per-ex loss: 0.814805  [   39/   88]
per-ex loss: 0.783603  [   40/   88]
per-ex loss: 0.570896  [   41/   88]
per-ex loss: 0.618874  [   42/   88]
per-ex loss: 0.726688  [   43/   88]
per-ex loss: 0.638881  [   44/   88]
per-ex loss: 0.754326  [   45/   88]
per-ex loss: 0.508874  [   46/   88]
per-ex loss: 0.740815  [   47/   88]
per-ex loss: 0.548200  [   48/   88]
per-ex loss: 0.605807  [   49/   88]
per-ex loss: 0.782303  [   50/   88]
per-ex loss: 0.578168  [   51/   88]
per-ex loss: 0.597222  [   52/   88]
per-ex loss: 0.590331  [   53/   88]
per-ex loss: 0.543693  [   54/   88]
per-ex loss: 0.481136  [   55/   88]
per-ex loss: 0.520980  [   56/   88]
per-ex loss: 0.701079  [   57/   88]
per-ex loss: 0.741183  [   58/   88]
per-ex loss: 0.809769  [   59/   88]
per-ex loss: 0.753570  [   60/   88]
per-ex loss: 0.528680  [   61/   88]
per-ex loss: 0.540063  [   62/   88]
per-ex loss: 0.568037  [   63/   88]
per-ex loss: 0.604296  [   64/   88]
per-ex loss: 0.780565  [   65/   88]
per-ex loss: 0.702303  [   66/   88]
per-ex loss: 0.526204  [   67/   88]
per-ex loss: 0.602542  [   68/   88]
per-ex loss: 0.825694  [   69/   88]
per-ex loss: 0.549789  [   70/   88]
per-ex loss: 0.821393  [   71/   88]
per-ex loss: 0.813639  [   72/   88]
per-ex loss: 0.519801  [   73/   88]
per-ex loss: 0.612598  [   74/   88]
per-ex loss: 0.687291  [   75/   88]
per-ex loss: 0.794682  [   76/   88]
per-ex loss: 0.610763  [   77/   88]
per-ex loss: 0.426557  [   78/   88]
per-ex loss: 0.695791  [   79/   88]
per-ex loss: 0.523913  [   80/   88]
per-ex loss: 0.708942  [   81/   88]
per-ex loss: 0.798229  [   82/   88]
per-ex loss: 0.555635  [   83/   88]
per-ex loss: 0.697906  [   84/   88]
per-ex loss: 0.553310  [   85/   88]
per-ex loss: 0.749170  [   86/   88]
per-ex loss: 0.613636  [   87/   88]
per-ex loss: 0.569934  [   88/   88]
Train Error: Avg loss: 0.64204759
validation Error: 
 Avg loss: 0.69581949 
 F1: 0.492985 
 Precision: 0.580177 
 Recall: 0.428577
 IoU: 0.327127

test Error: 
 Avg loss: 0.63853453 
 F1: 0.573834 
 Precision: 0.655530 
 Recall: 0.510244
 IoU: 0.402361

We have finished training iteration 57
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_55_.pth
per-ex loss: 0.797305  [    1/   88]
per-ex loss: 0.601548  [    2/   88]
per-ex loss: 0.468197  [    3/   88]
per-ex loss: 0.771735  [    4/   88]
per-ex loss: 0.749992  [    5/   88]
per-ex loss: 0.564857  [    6/   88]
per-ex loss: 0.704638  [    7/   88]
per-ex loss: 0.595887  [    8/   88]
per-ex loss: 0.703665  [    9/   88]
per-ex loss: 0.520214  [   10/   88]
per-ex loss: 0.642563  [   11/   88]
per-ex loss: 0.595973  [   12/   88]
per-ex loss: 0.674642  [   13/   88]
per-ex loss: 0.802952  [   14/   88]
per-ex loss: 0.738965  [   15/   88]
per-ex loss: 0.586599  [   16/   88]
per-ex loss: 0.544197  [   17/   88]
per-ex loss: 0.680114  [   18/   88]
per-ex loss: 0.737310  [   19/   88]
per-ex loss: 0.605707  [   20/   88]
per-ex loss: 0.765301  [   21/   88]
per-ex loss: 0.531164  [   22/   88]
per-ex loss: 0.504423  [   23/   88]
per-ex loss: 0.745081  [   24/   88]
per-ex loss: 0.574289  [   25/   88]
per-ex loss: 0.545576  [   26/   88]
per-ex loss: 0.694564  [   27/   88]
per-ex loss: 0.552972  [   28/   88]
per-ex loss: 0.649775  [   29/   88]
per-ex loss: 0.513729  [   30/   88]
per-ex loss: 0.697907  [   31/   88]
per-ex loss: 0.525345  [   32/   88]
per-ex loss: 0.712171  [   33/   88]
per-ex loss: 0.737796  [   34/   88]
per-ex loss: 0.563625  [   35/   88]
per-ex loss: 0.603937  [   36/   88]
per-ex loss: 0.588965  [   37/   88]
per-ex loss: 0.754790  [   38/   88]
per-ex loss: 0.573085  [   39/   88]
per-ex loss: 0.537606  [   40/   88]
per-ex loss: 0.509858  [   41/   88]
per-ex loss: 0.558479  [   42/   88]
per-ex loss: 0.691003  [   43/   88]
per-ex loss: 0.575464  [   44/   88]
per-ex loss: 0.692701  [   45/   88]
per-ex loss: 0.797186  [   46/   88]
per-ex loss: 0.527912  [   47/   88]
per-ex loss: 0.605395  [   48/   88]
per-ex loss: 0.433830  [   49/   88]
per-ex loss: 0.543130  [   50/   88]
per-ex loss: 0.585325  [   51/   88]
per-ex loss: 0.765549  [   52/   88]
per-ex loss: 0.719554  [   53/   88]
per-ex loss: 0.558830  [   54/   88]
per-ex loss: 0.506091  [   55/   88]
per-ex loss: 0.723839  [   56/   88]
per-ex loss: 0.693370  [   57/   88]
per-ex loss: 0.690455  [   58/   88]
per-ex loss: 0.797279  [   59/   88]
per-ex loss: 0.563051  [   60/   88]
per-ex loss: 0.571880  [   61/   88]
per-ex loss: 0.611079  [   62/   88]
per-ex loss: 0.716465  [   63/   88]
per-ex loss: 0.576797  [   64/   88]
per-ex loss: 0.600978  [   65/   88]
per-ex loss: 0.568162  [   66/   88]
per-ex loss: 0.579234  [   67/   88]
per-ex loss: 0.569926  [   68/   88]
per-ex loss: 0.785337  [   69/   88]
per-ex loss: 0.686344  [   70/   88]
per-ex loss: 0.702299  [   71/   88]
per-ex loss: 0.636488  [   72/   88]
per-ex loss: 0.695948  [   73/   88]
per-ex loss: 0.517953  [   74/   88]
per-ex loss: 0.735941  [   75/   88]
per-ex loss: 0.704748  [   76/   88]
per-ex loss: 0.767661  [   77/   88]
per-ex loss: 0.549816  [   78/   88]
per-ex loss: 0.611874  [   79/   88]
per-ex loss: 0.746649  [   80/   88]
per-ex loss: 0.706368  [   81/   88]
per-ex loss: 0.511214  [   82/   88]
per-ex loss: 0.738334  [   83/   88]
per-ex loss: 0.782588  [   84/   88]
per-ex loss: 0.792170  [   85/   88]
per-ex loss: 0.517984  [   86/   88]
per-ex loss: 0.823382  [   87/   88]
per-ex loss: 0.555302  [   88/   88]
Train Error: Avg loss: 0.64045888
validation Error: 
 Avg loss: 0.71009924 
 F1: 0.467858 
 Precision: 0.539777 
 Recall: 0.412851
 IoU: 0.305362

test Error: 
 Avg loss: 0.64590882 
 F1: 0.564680 
 Precision: 0.652269 
 Recall: 0.497830
 IoU: 0.393417

We have finished training iteration 58
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_56_.pth
per-ex loss: 0.560827  [    1/   88]
per-ex loss: 0.732279  [    2/   88]
per-ex loss: 0.735570  [    3/   88]
per-ex loss: 0.578735  [    4/   88]
per-ex loss: 0.612365  [    5/   88]
per-ex loss: 0.815749  [    6/   88]
per-ex loss: 0.559638  [    7/   88]
per-ex loss: 0.663520  [    8/   88]
per-ex loss: 0.697680  [    9/   88]
per-ex loss: 0.630373  [   10/   88]
per-ex loss: 0.497061  [   11/   88]
per-ex loss: 0.540486  [   12/   88]
per-ex loss: 0.513670  [   13/   88]
per-ex loss: 0.801323  [   14/   88]
per-ex loss: 0.598289  [   15/   88]
per-ex loss: 0.573415  [   16/   88]
per-ex loss: 0.511390  [   17/   88]
per-ex loss: 0.707193  [   18/   88]
per-ex loss: 0.704955  [   19/   88]
per-ex loss: 0.713692  [   20/   88]
per-ex loss: 0.793735  [   21/   88]
per-ex loss: 0.520639  [   22/   88]
per-ex loss: 0.743441  [   23/   88]
per-ex loss: 0.744099  [   24/   88]
per-ex loss: 0.663718  [   25/   88]
per-ex loss: 0.723263  [   26/   88]
per-ex loss: 0.559661  [   27/   88]
per-ex loss: 0.526594  [   28/   88]
per-ex loss: 0.596765  [   29/   88]
per-ex loss: 0.710822  [   30/   88]
per-ex loss: 0.473768  [   31/   88]
per-ex loss: 0.736486  [   32/   88]
per-ex loss: 0.736362  [   33/   88]
per-ex loss: 0.655672  [   34/   88]
per-ex loss: 0.675749  [   35/   88]
per-ex loss: 0.767098  [   36/   88]
per-ex loss: 0.643276  [   37/   88]
per-ex loss: 0.550557  [   38/   88]
per-ex loss: 0.792897  [   39/   88]
per-ex loss: 0.588258  [   40/   88]
per-ex loss: 0.700215  [   41/   88]
per-ex loss: 0.557225  [   42/   88]
per-ex loss: 0.777805  [   43/   88]
per-ex loss: 0.667788  [   44/   88]
per-ex loss: 0.527517  [   45/   88]
per-ex loss: 0.551970  [   46/   88]
per-ex loss: 0.583256  [   47/   88]
per-ex loss: 0.754566  [   48/   88]
per-ex loss: 0.543579  [   49/   88]
per-ex loss: 0.614268  [   50/   88]
per-ex loss: 0.612117  [   51/   88]
per-ex loss: 0.593368  [   52/   88]
per-ex loss: 0.818275  [   53/   88]
per-ex loss: 0.744327  [   54/   88]
per-ex loss: 0.500508  [   55/   88]
per-ex loss: 0.554680  [   56/   88]
per-ex loss: 0.740166  [   57/   88]
per-ex loss: 0.709759  [   58/   88]
per-ex loss: 0.570361  [   59/   88]
per-ex loss: 0.504427  [   60/   88]
per-ex loss: 0.605196  [   61/   88]
per-ex loss: 0.531170  [   62/   88]
per-ex loss: 0.465766  [   63/   88]
per-ex loss: 0.640467  [   64/   88]
per-ex loss: 0.757764  [   65/   88]
per-ex loss: 0.570627  [   66/   88]
per-ex loss: 0.570108  [   67/   88]
per-ex loss: 0.588444  [   68/   88]
per-ex loss: 0.721063  [   69/   88]
per-ex loss: 0.618303  [   70/   88]
per-ex loss: 0.596936  [   71/   88]
per-ex loss: 0.755550  [   72/   88]
per-ex loss: 0.695057  [   73/   88]
per-ex loss: 0.542371  [   74/   88]
per-ex loss: 0.545319  [   75/   88]
per-ex loss: 0.799807  [   76/   88]
per-ex loss: 0.567743  [   77/   88]
per-ex loss: 0.521768  [   78/   88]
per-ex loss: 0.775119  [   79/   88]
per-ex loss: 0.791059  [   80/   88]
per-ex loss: 0.745109  [   81/   88]
per-ex loss: 0.506941  [   82/   88]
per-ex loss: 0.600266  [   83/   88]
per-ex loss: 0.631287  [   84/   88]
per-ex loss: 0.737254  [   85/   88]
per-ex loss: 0.574475  [   86/   88]
per-ex loss: 0.465537  [   87/   88]
per-ex loss: 0.829565  [   88/   88]
Train Error: Avg loss: 0.64008312
validation Error: 
 Avg loss: 0.70471064 
 F1: 0.480853 
 Precision: 0.606436 
 Recall: 0.398359
 IoU: 0.316528

test Error: 
 Avg loss: 0.65284415 
 F1: 0.552366 
 Precision: 0.694017 
 Recall: 0.458737
 IoU: 0.381565

We have finished training iteration 59
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_44_.pth
per-ex loss: 0.535903  [    1/   88]
per-ex loss: 0.565092  [    2/   88]
per-ex loss: 0.746592  [    3/   88]
per-ex loss: 0.715364  [    4/   88]
per-ex loss: 0.555393  [    5/   88]
per-ex loss: 0.647583  [    6/   88]
per-ex loss: 0.499196  [    7/   88]
per-ex loss: 0.577269  [    8/   88]
per-ex loss: 0.582125  [    9/   88]
per-ex loss: 0.783384  [   10/   88]
per-ex loss: 0.795247  [   11/   88]
per-ex loss: 0.555103  [   12/   88]
per-ex loss: 0.703890  [   13/   88]
per-ex loss: 0.567587  [   14/   88]
per-ex loss: 0.517140  [   15/   88]
per-ex loss: 0.751244  [   16/   88]
per-ex loss: 0.752165  [   17/   88]
per-ex loss: 0.823911  [   18/   88]
per-ex loss: 0.622030  [   19/   88]
per-ex loss: 0.505275  [   20/   88]
per-ex loss: 0.565680  [   21/   88]
per-ex loss: 0.736585  [   22/   88]
per-ex loss: 0.785885  [   23/   88]
per-ex loss: 0.616210  [   24/   88]
per-ex loss: 0.665771  [   25/   88]
per-ex loss: 0.600644  [   26/   88]
per-ex loss: 0.564477  [   27/   88]
per-ex loss: 0.769089  [   28/   88]
per-ex loss: 0.569028  [   29/   88]
per-ex loss: 0.694694  [   30/   88]
per-ex loss: 0.581279  [   31/   88]
per-ex loss: 0.698207  [   32/   88]
per-ex loss: 0.565058  [   33/   88]
per-ex loss: 0.504809  [   34/   88]
per-ex loss: 0.555722  [   35/   88]
per-ex loss: 0.751605  [   36/   88]
per-ex loss: 0.807312  [   37/   88]
per-ex loss: 0.753676  [   38/   88]
per-ex loss: 0.793266  [   39/   88]
per-ex loss: 0.548546  [   40/   88]
per-ex loss: 0.729934  [   41/   88]
per-ex loss: 0.595704  [   42/   88]
per-ex loss: 0.704571  [   43/   88]
per-ex loss: 0.510690  [   44/   88]
per-ex loss: 0.598344  [   45/   88]
per-ex loss: 0.619563  [   46/   88]
per-ex loss: 0.567147  [   47/   88]
per-ex loss: 0.829136  [   48/   88]
per-ex loss: 0.558972  [   49/   88]
per-ex loss: 0.433506  [   50/   88]
per-ex loss: 0.771599  [   51/   88]
per-ex loss: 0.617271  [   52/   88]
per-ex loss: 0.798313  [   53/   88]
per-ex loss: 0.588706  [   54/   88]
per-ex loss: 0.467234  [   55/   88]
per-ex loss: 0.749739  [   56/   88]
per-ex loss: 0.687344  [   57/   88]
per-ex loss: 0.514863  [   58/   88]
per-ex loss: 0.774833  [   59/   88]
per-ex loss: 0.718401  [   60/   88]
per-ex loss: 0.503860  [   61/   88]
per-ex loss: 0.697645  [   62/   88]
per-ex loss: 0.556686  [   63/   88]
per-ex loss: 0.525680  [   64/   88]
per-ex loss: 0.696252  [   65/   88]
per-ex loss: 0.513323  [   66/   88]
per-ex loss: 0.541085  [   67/   88]
per-ex loss: 0.562658  [   68/   88]
per-ex loss: 0.516716  [   69/   88]
per-ex loss: 0.619809  [   70/   88]
per-ex loss: 0.734306  [   71/   88]
per-ex loss: 0.569826  [   72/   88]
per-ex loss: 0.568602  [   73/   88]
per-ex loss: 0.589547  [   74/   88]
per-ex loss: 0.560968  [   75/   88]
per-ex loss: 0.789684  [   76/   88]
per-ex loss: 0.699855  [   77/   88]
per-ex loss: 0.601164  [   78/   88]
per-ex loss: 0.811304  [   79/   88]
per-ex loss: 0.735286  [   80/   88]
per-ex loss: 0.710933  [   81/   88]
per-ex loss: 0.542235  [   82/   88]
per-ex loss: 0.575478  [   83/   88]
per-ex loss: 0.721797  [   84/   88]
per-ex loss: 0.684165  [   85/   88]
per-ex loss: 0.546404  [   86/   88]
per-ex loss: 0.760015  [   87/   88]
per-ex loss: 0.721940  [   88/   88]
Train Error: Avg loss: 0.64116054
validation Error: 
 Avg loss: 0.71068055 
 F1: 0.468150 
 Precision: 0.491372 
 Recall: 0.447024
 IoU: 0.305611

test Error: 
 Avg loss: 0.64212978 
 F1: 0.571259 
 Precision: 0.595507 
 Recall: 0.548909
 IoU: 0.399834

We have finished training iteration 60
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_58_.pth
per-ex loss: 0.761777  [    1/   88]
per-ex loss: 0.706676  [    2/   88]
per-ex loss: 0.689126  [    3/   88]
per-ex loss: 0.851933  [    4/   88]
per-ex loss: 0.575688  [    5/   88]
per-ex loss: 0.688327  [    6/   88]
per-ex loss: 0.651108  [    7/   88]
per-ex loss: 0.600482  [    8/   88]
per-ex loss: 0.723428  [    9/   88]
per-ex loss: 0.594096  [   10/   88]
per-ex loss: 0.614161  [   11/   88]
per-ex loss: 0.568117  [   12/   88]
per-ex loss: 0.514424  [   13/   88]
per-ex loss: 0.700630  [   14/   88]
per-ex loss: 0.782820  [   15/   88]
per-ex loss: 0.631069  [   16/   88]
per-ex loss: 0.547999  [   17/   88]
per-ex loss: 0.544658  [   18/   88]
per-ex loss: 0.493576  [   19/   88]
per-ex loss: 0.482375  [   20/   88]
per-ex loss: 0.721696  [   21/   88]
per-ex loss: 0.717188  [   22/   88]
per-ex loss: 0.515655  [   23/   88]
per-ex loss: 0.441306  [   24/   88]
per-ex loss: 0.512298  [   25/   88]
per-ex loss: 0.526454  [   26/   88]
per-ex loss: 0.555353  [   27/   88]
per-ex loss: 0.686409  [   28/   88]
per-ex loss: 0.760286  [   29/   88]
per-ex loss: 0.695276  [   30/   88]
per-ex loss: 0.542164  [   31/   88]
per-ex loss: 0.531624  [   32/   88]
per-ex loss: 0.725027  [   33/   88]
per-ex loss: 0.664107  [   34/   88]
per-ex loss: 0.659764  [   35/   88]
per-ex loss: 0.690629  [   36/   88]
per-ex loss: 0.726569  [   37/   88]
per-ex loss: 0.526491  [   38/   88]
per-ex loss: 0.753169  [   39/   88]
per-ex loss: 0.574344  [   40/   88]
per-ex loss: 0.552725  [   41/   88]
per-ex loss: 0.742706  [   42/   88]
per-ex loss: 0.805528  [   43/   88]
per-ex loss: 0.655472  [   44/   88]
per-ex loss: 0.605196  [   45/   88]
per-ex loss: 0.549784  [   46/   88]
per-ex loss: 0.539449  [   47/   88]
per-ex loss: 0.537909  [   48/   88]
per-ex loss: 0.618578  [   49/   88]
per-ex loss: 0.572107  [   50/   88]
per-ex loss: 0.608596  [   51/   88]
per-ex loss: 0.536018  [   52/   88]
per-ex loss: 0.562197  [   53/   88]
per-ex loss: 0.509835  [   54/   88]
per-ex loss: 0.548965  [   55/   88]
per-ex loss: 0.804456  [   56/   88]
per-ex loss: 0.763588  [   57/   88]
per-ex loss: 0.604095  [   58/   88]
per-ex loss: 0.776672  [   59/   88]
per-ex loss: 0.778041  [   60/   88]
per-ex loss: 0.840991  [   61/   88]
per-ex loss: 0.739331  [   62/   88]
per-ex loss: 0.581855  [   63/   88]
per-ex loss: 0.670839  [   64/   88]
per-ex loss: 0.547751  [   65/   88]
per-ex loss: 0.565838  [   66/   88]
per-ex loss: 0.533478  [   67/   88]
per-ex loss: 0.746866  [   68/   88]
per-ex loss: 0.714899  [   69/   88]
per-ex loss: 0.562168  [   70/   88]
per-ex loss: 0.793718  [   71/   88]
per-ex loss: 0.678794  [   72/   88]
per-ex loss: 0.558330  [   73/   88]
per-ex loss: 0.809612  [   74/   88]
per-ex loss: 0.547177  [   75/   88]
per-ex loss: 0.635921  [   76/   88]
per-ex loss: 0.511235  [   77/   88]
per-ex loss: 0.760522  [   78/   88]
per-ex loss: 0.753842  [   79/   88]
per-ex loss: 0.702659  [   80/   88]
per-ex loss: 0.585592  [   81/   88]
per-ex loss: 0.585272  [   82/   88]
per-ex loss: 0.574579  [   83/   88]
per-ex loss: 0.731224  [   84/   88]
per-ex loss: 0.716273  [   85/   88]
per-ex loss: 0.817014  [   86/   88]
per-ex loss: 0.569360  [   87/   88]
per-ex loss: 0.624610  [   88/   88]
Train Error: Avg loss: 0.64068123
validation Error: 
 Avg loss: 0.72662339 
 F1: 0.442073 
 Precision: 0.443322 
 Recall: 0.440831
 IoU: 0.283757

test Error: 
 Avg loss: 0.64119679 
 F1: 0.571073 
 Precision: 0.587553 
 Recall: 0.555492
 IoU: 0.399651

We have finished training iteration 61
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_59_.pth
per-ex loss: 0.714534  [    1/   88]
per-ex loss: 0.755182  [    2/   88]
per-ex loss: 0.579893  [    3/   88]
per-ex loss: 0.650093  [    4/   88]
per-ex loss: 0.602332  [    5/   88]
per-ex loss: 0.570587  [    6/   88]
per-ex loss: 0.618876  [    7/   88]
per-ex loss: 0.549442  [    8/   88]
per-ex loss: 0.538158  [    9/   88]
per-ex loss: 0.590168  [   10/   88]
per-ex loss: 0.796231  [   11/   88]
per-ex loss: 0.722884  [   12/   88]
per-ex loss: 0.669538  [   13/   88]
per-ex loss: 0.597539  [   14/   88]
per-ex loss: 0.733214  [   15/   88]
per-ex loss: 0.583534  [   16/   88]
per-ex loss: 0.570993  [   17/   88]
per-ex loss: 0.568889  [   18/   88]
per-ex loss: 0.689233  [   19/   88]
per-ex loss: 0.521034  [   20/   88]
per-ex loss: 0.770962  [   21/   88]
per-ex loss: 0.612624  [   22/   88]
per-ex loss: 0.570698  [   23/   88]
per-ex loss: 0.708059  [   24/   88]
per-ex loss: 0.733449  [   25/   88]
per-ex loss: 0.503797  [   26/   88]
per-ex loss: 0.593900  [   27/   88]
per-ex loss: 0.561188  [   28/   88]
per-ex loss: 0.594321  [   29/   88]
per-ex loss: 0.703269  [   30/   88]
per-ex loss: 0.556841  [   31/   88]
per-ex loss: 0.819667  [   32/   88]
per-ex loss: 0.560604  [   33/   88]
per-ex loss: 0.625067  [   34/   88]
per-ex loss: 0.545689  [   35/   88]
per-ex loss: 0.704248  [   36/   88]
per-ex loss: 0.736880  [   37/   88]
per-ex loss: 0.683647  [   38/   88]
per-ex loss: 0.786686  [   39/   88]
per-ex loss: 0.584780  [   40/   88]
per-ex loss: 0.633954  [   41/   88]
per-ex loss: 0.570594  [   42/   88]
per-ex loss: 0.726897  [   43/   88]
per-ex loss: 0.571747  [   44/   88]
per-ex loss: 0.726896  [   45/   88]
per-ex loss: 0.486004  [   46/   88]
per-ex loss: 0.694453  [   47/   88]
per-ex loss: 0.543175  [   48/   88]
per-ex loss: 0.586951  [   49/   88]
per-ex loss: 0.810250  [   50/   88]
per-ex loss: 0.752610  [   51/   88]
per-ex loss: 0.494056  [   52/   88]
per-ex loss: 0.783142  [   53/   88]
per-ex loss: 0.652286  [   54/   88]
per-ex loss: 0.803143  [   55/   88]
per-ex loss: 0.502554  [   56/   88]
per-ex loss: 0.667210  [   57/   88]
per-ex loss: 0.489300  [   58/   88]
per-ex loss: 0.726274  [   59/   88]
per-ex loss: 0.558469  [   60/   88]
per-ex loss: 0.752167  [   61/   88]
per-ex loss: 0.648896  [   62/   88]
per-ex loss: 0.531072  [   63/   88]
per-ex loss: 0.529824  [   64/   88]
per-ex loss: 0.713524  [   65/   88]
per-ex loss: 0.549702  [   66/   88]
per-ex loss: 0.562868  [   67/   88]
per-ex loss: 0.805405  [   68/   88]
per-ex loss: 0.578290  [   69/   88]
per-ex loss: 0.487985  [   70/   88]
per-ex loss: 0.715620  [   71/   88]
per-ex loss: 0.514746  [   72/   88]
per-ex loss: 0.516412  [   73/   88]
per-ex loss: 0.876140  [   74/   88]
per-ex loss: 0.782028  [   75/   88]
per-ex loss: 0.544570  [   76/   88]
per-ex loss: 0.800277  [   77/   88]
per-ex loss: 0.644391  [   78/   88]
per-ex loss: 0.517727  [   79/   88]
per-ex loss: 0.537054  [   80/   88]
per-ex loss: 0.430456  [   81/   88]
per-ex loss: 0.795850  [   82/   88]
per-ex loss: 0.738902  [   83/   88]
per-ex loss: 0.555312  [   84/   88]
per-ex loss: 0.706749  [   85/   88]
per-ex loss: 0.692972  [   86/   88]
per-ex loss: 0.664838  [   87/   88]
per-ex loss: 0.613486  [   88/   88]
Train Error: Avg loss: 0.63824953
validation Error: 
 Avg loss: 0.71466707 
 F1: 0.463246 
 Precision: 0.451424 
 Recall: 0.475704
 IoU: 0.301444

test Error: 
 Avg loss: 0.64221745 
 F1: 0.569540 
 Precision: 0.574240 
 Recall: 0.564917
 IoU: 0.398152

We have finished training iteration 62
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_60_.pth
per-ex loss: 0.541571  [    1/   88]
per-ex loss: 0.694967  [    2/   88]
per-ex loss: 0.712293  [    3/   88]
per-ex loss: 0.670512  [    4/   88]
per-ex loss: 0.716160  [    5/   88]
per-ex loss: 0.509461  [    6/   88]
per-ex loss: 0.627772  [    7/   88]
per-ex loss: 0.519901  [    8/   88]
per-ex loss: 0.773963  [    9/   88]
per-ex loss: 0.552413  [   10/   88]
per-ex loss: 0.760999  [   11/   88]
per-ex loss: 0.637963  [   12/   88]
per-ex loss: 0.510066  [   13/   88]
per-ex loss: 0.600439  [   14/   88]
per-ex loss: 0.579916  [   15/   88]
per-ex loss: 0.662067  [   16/   88]
per-ex loss: 0.808871  [   17/   88]
per-ex loss: 0.691617  [   18/   88]
per-ex loss: 0.541609  [   19/   88]
per-ex loss: 0.550253  [   20/   88]
per-ex loss: 0.751817  [   21/   88]
per-ex loss: 0.553616  [   22/   88]
per-ex loss: 0.584769  [   23/   88]
per-ex loss: 0.732639  [   24/   88]
per-ex loss: 0.564353  [   25/   88]
per-ex loss: 0.511494  [   26/   88]
per-ex loss: 0.572850  [   27/   88]
per-ex loss: 0.830402  [   28/   88]
per-ex loss: 0.761004  [   29/   88]
per-ex loss: 0.591511  [   30/   88]
per-ex loss: 0.571723  [   31/   88]
per-ex loss: 0.748472  [   32/   88]
per-ex loss: 0.517821  [   33/   88]
per-ex loss: 0.783990  [   34/   88]
per-ex loss: 0.505132  [   35/   88]
per-ex loss: 0.624034  [   36/   88]
per-ex loss: 0.575712  [   37/   88]
per-ex loss: 0.548741  [   38/   88]
per-ex loss: 0.599358  [   39/   88]
per-ex loss: 0.715470  [   40/   88]
per-ex loss: 0.646717  [   41/   88]
per-ex loss: 0.484118  [   42/   88]
per-ex loss: 0.718349  [   43/   88]
per-ex loss: 0.767280  [   44/   88]
per-ex loss: 0.741118  [   45/   88]
per-ex loss: 0.558365  [   46/   88]
per-ex loss: 0.696249  [   47/   88]
per-ex loss: 0.586537  [   48/   88]
per-ex loss: 0.727179  [   49/   88]
per-ex loss: 0.705719  [   50/   88]
per-ex loss: 0.450374  [   51/   88]
per-ex loss: 0.552610  [   52/   88]
per-ex loss: 0.729639  [   53/   88]
per-ex loss: 0.795133  [   54/   88]
per-ex loss: 0.802413  [   55/   88]
per-ex loss: 0.630516  [   56/   88]
per-ex loss: 0.721575  [   57/   88]
per-ex loss: 0.503055  [   58/   88]
per-ex loss: 0.540881  [   59/   88]
per-ex loss: 0.666785  [   60/   88]
per-ex loss: 0.725938  [   61/   88]
per-ex loss: 0.640334  [   62/   88]
per-ex loss: 0.535509  [   63/   88]
per-ex loss: 0.610159  [   64/   88]
per-ex loss: 0.578771  [   65/   88]
per-ex loss: 0.626154  [   66/   88]
per-ex loss: 0.748318  [   67/   88]
per-ex loss: 0.790352  [   68/   88]
per-ex loss: 0.709777  [   69/   88]
per-ex loss: 0.725774  [   70/   88]
per-ex loss: 0.506392  [   71/   88]
per-ex loss: 0.599841  [   72/   88]
per-ex loss: 0.695035  [   73/   88]
per-ex loss: 0.787120  [   74/   88]
per-ex loss: 0.715267  [   75/   88]
per-ex loss: 0.558770  [   76/   88]
per-ex loss: 0.613062  [   77/   88]
per-ex loss: 0.626957  [   78/   88]
per-ex loss: 0.492188  [   79/   88]
per-ex loss: 0.669249  [   80/   88]
per-ex loss: 0.584634  [   81/   88]
per-ex loss: 0.509831  [   82/   88]
per-ex loss: 0.799167  [   83/   88]
per-ex loss: 0.816075  [   84/   88]
per-ex loss: 0.730958  [   85/   88]
per-ex loss: 0.709744  [   86/   88]
per-ex loss: 0.586386  [   87/   88]
per-ex loss: 0.556508  [   88/   88]
Train Error: Avg loss: 0.64296139
validation Error: 
 Avg loss: 0.70925369 
 F1: 0.468288 
 Precision: 0.609718 
 Recall: 0.380116
 IoU: 0.305728

test Error: 
 Avg loss: 0.65696521 
 F1: 0.550676 
 Precision: 0.698847 
 Recall: 0.454345
 IoU: 0.379954

We have finished training iteration 63
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_61_.pth
per-ex loss: 0.708238  [    1/   88]
per-ex loss: 0.526155  [    2/   88]
per-ex loss: 0.538233  [    3/   88]
per-ex loss: 0.770620  [    4/   88]
per-ex loss: 0.540013  [    5/   88]
per-ex loss: 0.534547  [    6/   88]
per-ex loss: 0.706734  [    7/   88]
per-ex loss: 0.496006  [    8/   88]
per-ex loss: 0.787448  [    9/   88]
per-ex loss: 0.509247  [   10/   88]
per-ex loss: 0.602394  [   11/   88]
per-ex loss: 0.573521  [   12/   88]
per-ex loss: 0.804422  [   13/   88]
per-ex loss: 0.799246  [   14/   88]
per-ex loss: 0.555590  [   15/   88]
per-ex loss: 0.502813  [   16/   88]
per-ex loss: 0.503293  [   17/   88]
per-ex loss: 0.751566  [   18/   88]
per-ex loss: 0.732328  [   19/   88]
per-ex loss: 0.763900  [   20/   88]
per-ex loss: 0.759984  [   21/   88]
per-ex loss: 0.750190  [   22/   88]
per-ex loss: 0.731104  [   23/   88]
per-ex loss: 0.548333  [   24/   88]
per-ex loss: 0.730086  [   25/   88]
per-ex loss: 0.401573  [   26/   88]
per-ex loss: 0.707968  [   27/   88]
per-ex loss: 0.661818  [   28/   88]
per-ex loss: 0.790774  [   29/   88]
per-ex loss: 0.521804  [   30/   88]
per-ex loss: 0.562096  [   31/   88]
per-ex loss: 0.702655  [   32/   88]
per-ex loss: 0.650162  [   33/   88]
per-ex loss: 0.688875  [   34/   88]
per-ex loss: 0.511021  [   35/   88]
per-ex loss: 0.587605  [   36/   88]
per-ex loss: 0.748861  [   37/   88]
per-ex loss: 0.665579  [   38/   88]
per-ex loss: 0.567074  [   39/   88]
per-ex loss: 0.601376  [   40/   88]
per-ex loss: 0.569068  [   41/   88]
per-ex loss: 0.766914  [   42/   88]
per-ex loss: 0.695238  [   43/   88]
per-ex loss: 0.687652  [   44/   88]
per-ex loss: 0.616352  [   45/   88]
per-ex loss: 0.819659  [   46/   88]
per-ex loss: 0.748780  [   47/   88]
per-ex loss: 0.708497  [   48/   88]
per-ex loss: 0.609729  [   49/   88]
per-ex loss: 0.513296  [   50/   88]
per-ex loss: 0.586295  [   51/   88]
per-ex loss: 0.777201  [   52/   88]
per-ex loss: 0.610939  [   53/   88]
per-ex loss: 0.528906  [   54/   88]
per-ex loss: 0.696244  [   55/   88]
per-ex loss: 0.719690  [   56/   88]
per-ex loss: 0.537287  [   57/   88]
per-ex loss: 0.660826  [   58/   88]
per-ex loss: 0.794765  [   59/   88]
per-ex loss: 0.808208  [   60/   88]
per-ex loss: 0.568105  [   61/   88]
per-ex loss: 0.578093  [   62/   88]
per-ex loss: 0.493711  [   63/   88]
per-ex loss: 0.547677  [   64/   88]
per-ex loss: 0.672958  [   65/   88]
per-ex loss: 0.643033  [   66/   88]
per-ex loss: 0.614912  [   67/   88]
per-ex loss: 0.762839  [   68/   88]
per-ex loss: 0.524545  [   69/   88]
per-ex loss: 0.631460  [   70/   88]
per-ex loss: 0.749239  [   71/   88]
per-ex loss: 0.603104  [   72/   88]
per-ex loss: 0.520928  [   73/   88]
per-ex loss: 0.555859  [   74/   88]
per-ex loss: 0.472479  [   75/   88]
per-ex loss: 0.537786  [   76/   88]
per-ex loss: 0.737122  [   77/   88]
per-ex loss: 0.598410  [   78/   88]
per-ex loss: 0.622343  [   79/   88]
per-ex loss: 0.691727  [   80/   88]
per-ex loss: 0.587547  [   81/   88]
per-ex loss: 0.728117  [   82/   88]
per-ex loss: 0.618559  [   83/   88]
per-ex loss: 0.722179  [   84/   88]
per-ex loss: 0.531485  [   85/   88]
per-ex loss: 0.577343  [   86/   88]
per-ex loss: 0.552938  [   87/   88]
per-ex loss: 0.582697  [   88/   88]
Train Error: Avg loss: 0.63724993
validation Error: 
 Avg loss: 0.69558826 
 F1: 0.492344 
 Precision: 0.584672 
 Recall: 0.425199
 IoU: 0.326562

test Error: 
 Avg loss: 0.64525718 
 F1: 0.568700 
 Precision: 0.642340 
 Recall: 0.510207
 IoU: 0.397331

We have finished training iteration 64
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_62_.pth
per-ex loss: 0.631738  [    1/   88]
per-ex loss: 0.703658  [    2/   88]
per-ex loss: 0.747802  [    3/   88]
per-ex loss: 0.516550  [    4/   88]
per-ex loss: 0.736788  [    5/   88]
per-ex loss: 0.742059  [    6/   88]
per-ex loss: 0.812808  [    7/   88]
per-ex loss: 0.506064  [    8/   88]
per-ex loss: 0.541483  [    9/   88]
per-ex loss: 0.568568  [   10/   88]
per-ex loss: 0.637952  [   11/   88]
per-ex loss: 0.606587  [   12/   88]
per-ex loss: 0.805957  [   13/   88]
per-ex loss: 0.517892  [   14/   88]
per-ex loss: 0.588002  [   15/   88]
per-ex loss: 0.564719  [   16/   88]
per-ex loss: 0.518162  [   17/   88]
per-ex loss: 0.464504  [   18/   88]
per-ex loss: 0.594718  [   19/   88]
per-ex loss: 0.724086  [   20/   88]
per-ex loss: 0.701530  [   21/   88]
per-ex loss: 0.775378  [   22/   88]
per-ex loss: 0.505131  [   23/   88]
per-ex loss: 0.537540  [   24/   88]
per-ex loss: 0.588639  [   25/   88]
per-ex loss: 0.505502  [   26/   88]
per-ex loss: 0.526003  [   27/   88]
per-ex loss: 0.680910  [   28/   88]
per-ex loss: 0.678656  [   29/   88]
per-ex loss: 0.571524  [   30/   88]
per-ex loss: 0.595417  [   31/   88]
per-ex loss: 0.620495  [   32/   88]
per-ex loss: 0.573340  [   33/   88]
per-ex loss: 0.673321  [   34/   88]
per-ex loss: 0.620875  [   35/   88]
per-ex loss: 0.660666  [   36/   88]
per-ex loss: 0.588530  [   37/   88]
per-ex loss: 0.525888  [   38/   88]
per-ex loss: 0.783551  [   39/   88]
per-ex loss: 0.547633  [   40/   88]
per-ex loss: 0.732067  [   41/   88]
per-ex loss: 0.476161  [   42/   88]
per-ex loss: 0.527572  [   43/   88]
per-ex loss: 0.479659  [   44/   88]
per-ex loss: 0.599559  [   45/   88]
per-ex loss: 0.820713  [   46/   88]
per-ex loss: 0.675488  [   47/   88]
per-ex loss: 0.558763  [   48/   88]
per-ex loss: 0.525765  [   49/   88]
per-ex loss: 0.813256  [   50/   88]
per-ex loss: 0.572506  [   51/   88]
per-ex loss: 0.601515  [   52/   88]
per-ex loss: 0.698061  [   53/   88]
per-ex loss: 0.715477  [   54/   88]
per-ex loss: 0.801216  [   55/   88]
per-ex loss: 0.580078  [   56/   88]
per-ex loss: 0.644784  [   57/   88]
per-ex loss: 0.737420  [   58/   88]
per-ex loss: 0.487344  [   59/   88]
per-ex loss: 0.590108  [   60/   88]
per-ex loss: 0.751228  [   61/   88]
per-ex loss: 0.564489  [   62/   88]
per-ex loss: 0.740263  [   63/   88]
per-ex loss: 0.705899  [   64/   88]
per-ex loss: 0.682000  [   65/   88]
per-ex loss: 0.806135  [   66/   88]
per-ex loss: 0.599052  [   67/   88]
per-ex loss: 0.683333  [   68/   88]
per-ex loss: 0.787675  [   69/   88]
per-ex loss: 0.765482  [   70/   88]
per-ex loss: 0.565744  [   71/   88]
per-ex loss: 0.556919  [   72/   88]
per-ex loss: 0.735887  [   73/   88]
per-ex loss: 0.609674  [   74/   88]
per-ex loss: 0.592268  [   75/   88]
per-ex loss: 0.720162  [   76/   88]
per-ex loss: 0.720371  [   77/   88]
per-ex loss: 0.702342  [   78/   88]
per-ex loss: 0.592875  [   79/   88]
per-ex loss: 0.526030  [   80/   88]
per-ex loss: 0.589356  [   81/   88]
per-ex loss: 0.653078  [   82/   88]
per-ex loss: 0.763586  [   83/   88]
per-ex loss: 0.706948  [   84/   88]
per-ex loss: 0.752235  [   85/   88]
per-ex loss: 0.530618  [   86/   88]
per-ex loss: 0.726527  [   87/   88]
per-ex loss: 0.544203  [   88/   88]
Train Error: Avg loss: 0.63752865
validation Error: 
 Avg loss: 0.70644193 
 F1: 0.480420 
 Precision: 0.504443 
 Recall: 0.458580
 IoU: 0.316153

test Error: 
 Avg loss: 0.63652493 
 F1: 0.576404 
 Precision: 0.603049 
 Recall: 0.552014
 IoU: 0.404893

We have finished training iteration 65
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_63_.pth
per-ex loss: 0.538641  [    1/   88]
per-ex loss: 0.798621  [    2/   88]
per-ex loss: 0.828894  [    3/   88]
per-ex loss: 0.747424  [    4/   88]
per-ex loss: 0.557178  [    5/   88]
per-ex loss: 0.601548  [    6/   88]
per-ex loss: 0.731555  [    7/   88]
per-ex loss: 0.750533  [    8/   88]
per-ex loss: 0.587755  [    9/   88]
per-ex loss: 0.512757  [   10/   88]
per-ex loss: 0.550767  [   11/   88]
per-ex loss: 0.692039  [   12/   88]
per-ex loss: 0.764186  [   13/   88]
per-ex loss: 0.674264  [   14/   88]
per-ex loss: 0.581893  [   15/   88]
per-ex loss: 0.560761  [   16/   88]
per-ex loss: 0.617983  [   17/   88]
per-ex loss: 0.516058  [   18/   88]
per-ex loss: 0.735611  [   19/   88]
per-ex loss: 0.697516  [   20/   88]
per-ex loss: 0.742803  [   21/   88]
per-ex loss: 0.712576  [   22/   88]
per-ex loss: 0.583496  [   23/   88]
per-ex loss: 0.796036  [   24/   88]
per-ex loss: 0.613295  [   25/   88]
per-ex loss: 0.673219  [   26/   88]
per-ex loss: 0.531490  [   27/   88]
per-ex loss: 0.571652  [   28/   88]
per-ex loss: 0.600026  [   29/   88]
per-ex loss: 0.503321  [   30/   88]
per-ex loss: 0.531911  [   31/   88]
per-ex loss: 0.503807  [   32/   88]
per-ex loss: 0.523914  [   33/   88]
per-ex loss: 0.534814  [   34/   88]
per-ex loss: 0.714895  [   35/   88]
per-ex loss: 0.565303  [   36/   88]
per-ex loss: 0.506620  [   37/   88]
per-ex loss: 0.675129  [   38/   88]
per-ex loss: 0.741666  [   39/   88]
per-ex loss: 0.811420  [   40/   88]
per-ex loss: 0.569786  [   41/   88]
per-ex loss: 0.549082  [   42/   88]
per-ex loss: 0.598129  [   43/   88]
per-ex loss: 0.753313  [   44/   88]
per-ex loss: 0.451013  [   45/   88]
per-ex loss: 0.606396  [   46/   88]
per-ex loss: 0.640371  [   47/   88]
per-ex loss: 0.731382  [   48/   88]
per-ex loss: 0.761958  [   49/   88]
per-ex loss: 0.493355  [   50/   88]
per-ex loss: 0.706381  [   51/   88]
per-ex loss: 0.682982  [   52/   88]
per-ex loss: 0.813536  [   53/   88]
per-ex loss: 0.667891  [   54/   88]
per-ex loss: 0.692972  [   55/   88]
per-ex loss: 0.544950  [   56/   88]
per-ex loss: 0.618250  [   57/   88]
per-ex loss: 0.595983  [   58/   88]
per-ex loss: 0.796108  [   59/   88]
per-ex loss: 0.568889  [   60/   88]
per-ex loss: 0.756724  [   61/   88]
per-ex loss: 0.526296  [   62/   88]
per-ex loss: 0.566653  [   63/   88]
per-ex loss: 0.786212  [   64/   88]
per-ex loss: 0.596886  [   65/   88]
per-ex loss: 0.473573  [   66/   88]
per-ex loss: 0.640532  [   67/   88]
per-ex loss: 0.689980  [   68/   88]
per-ex loss: 0.590946  [   69/   88]
per-ex loss: 0.509847  [   70/   88]
per-ex loss: 0.712990  [   71/   88]
per-ex loss: 0.698696  [   72/   88]
per-ex loss: 0.532886  [   73/   88]
per-ex loss: 0.534175  [   74/   88]
per-ex loss: 0.795473  [   75/   88]
per-ex loss: 0.579309  [   76/   88]
per-ex loss: 0.736248  [   77/   88]
per-ex loss: 0.556785  [   78/   88]
per-ex loss: 0.548947  [   79/   88]
per-ex loss: 0.538562  [   80/   88]
per-ex loss: 0.567892  [   81/   88]
per-ex loss: 0.723290  [   82/   88]
per-ex loss: 0.664889  [   83/   88]
per-ex loss: 0.702668  [   84/   88]
per-ex loss: 0.723143  [   85/   88]
per-ex loss: 0.571181  [   86/   88]
per-ex loss: 0.627573  [   87/   88]
per-ex loss: 0.683725  [   88/   88]
Train Error: Avg loss: 0.63593397
validation Error: 
 Avg loss: 0.69847896 
 F1: 0.490485 
 Precision: 0.514034 
 Recall: 0.468999
 IoU: 0.324929

test Error: 
 Avg loss: 0.63273049 
 F1: 0.578386 
 Precision: 0.600307 
 Recall: 0.558009
 IoU: 0.406852

We have finished training iteration 66
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_42_.pth
per-ex loss: 0.703223  [    1/   88]
per-ex loss: 0.536403  [    2/   88]
per-ex loss: 0.728886  [    3/   88]
per-ex loss: 0.646850  [    4/   88]
per-ex loss: 0.550385  [    5/   88]
per-ex loss: 0.573164  [    6/   88]
per-ex loss: 0.814014  [    7/   88]
per-ex loss: 0.662822  [    8/   88]
per-ex loss: 0.509056  [    9/   88]
per-ex loss: 0.498643  [   10/   88]
per-ex loss: 0.694792  [   11/   88]
per-ex loss: 0.532595  [   12/   88]
per-ex loss: 0.783143  [   13/   88]
per-ex loss: 0.589469  [   14/   88]
per-ex loss: 0.498677  [   15/   88]
per-ex loss: 0.793339  [   16/   88]
per-ex loss: 0.767099  [   17/   88]
per-ex loss: 0.605677  [   18/   88]
per-ex loss: 0.691521  [   19/   88]
per-ex loss: 0.577557  [   20/   88]
per-ex loss: 0.667245  [   21/   88]
per-ex loss: 0.571179  [   22/   88]
per-ex loss: 0.571753  [   23/   88]
per-ex loss: 0.648097  [   24/   88]
per-ex loss: 0.514795  [   25/   88]
per-ex loss: 0.556203  [   26/   88]
per-ex loss: 0.732355  [   27/   88]
per-ex loss: 0.592894  [   28/   88]
per-ex loss: 0.688346  [   29/   88]
per-ex loss: 0.741429  [   30/   88]
per-ex loss: 0.513421  [   31/   88]
per-ex loss: 0.537605  [   32/   88]
per-ex loss: 0.790203  [   33/   88]
per-ex loss: 0.574106  [   34/   88]
per-ex loss: 0.687301  [   35/   88]
per-ex loss: 0.697108  [   36/   88]
per-ex loss: 0.784942  [   37/   88]
per-ex loss: 0.550137  [   38/   88]
per-ex loss: 0.700990  [   39/   88]
per-ex loss: 0.759498  [   40/   88]
per-ex loss: 0.693840  [   41/   88]
per-ex loss: 0.669973  [   42/   88]
per-ex loss: 0.724890  [   43/   88]
per-ex loss: 0.508193  [   44/   88]
per-ex loss: 0.736431  [   45/   88]
per-ex loss: 0.730567  [   46/   88]
per-ex loss: 0.600072  [   47/   88]
per-ex loss: 0.570022  [   48/   88]
per-ex loss: 0.542662  [   49/   88]
per-ex loss: 0.566679  [   50/   88]
per-ex loss: 0.776713  [   51/   88]
per-ex loss: 0.513480  [   52/   88]
per-ex loss: 0.628632  [   53/   88]
per-ex loss: 0.719125  [   54/   88]
per-ex loss: 0.594628  [   55/   88]
per-ex loss: 0.705653  [   56/   88]
per-ex loss: 0.516861  [   57/   88]
per-ex loss: 0.713169  [   58/   88]
per-ex loss: 0.564260  [   59/   88]
per-ex loss: 0.749714  [   60/   88]
per-ex loss: 0.737570  [   61/   88]
per-ex loss: 0.559308  [   62/   88]
per-ex loss: 0.773127  [   63/   88]
per-ex loss: 0.551378  [   64/   88]
per-ex loss: 0.601075  [   65/   88]
per-ex loss: 0.602120  [   66/   88]
per-ex loss: 0.710353  [   67/   88]
per-ex loss: 0.618763  [   68/   88]
per-ex loss: 0.583038  [   69/   88]
per-ex loss: 0.536855  [   70/   88]
per-ex loss: 0.602718  [   71/   88]
per-ex loss: 0.463250  [   72/   88]
per-ex loss: 0.532913  [   73/   88]
per-ex loss: 0.700217  [   74/   88]
per-ex loss: 0.484920  [   75/   88]
per-ex loss: 0.413588  [   76/   88]
per-ex loss: 0.669553  [   77/   88]
per-ex loss: 0.740711  [   78/   88]
per-ex loss: 0.507653  [   79/   88]
per-ex loss: 0.812922  [   80/   88]
per-ex loss: 0.538879  [   81/   88]
per-ex loss: 0.513353  [   82/   88]
per-ex loss: 0.564952  [   83/   88]
per-ex loss: 0.516562  [   84/   88]
per-ex loss: 0.779882  [   85/   88]
per-ex loss: 0.584976  [   86/   88]
per-ex loss: 0.769539  [   87/   88]
per-ex loss: 0.732485  [   88/   88]
Train Error: Avg loss: 0.63255849
validation Error: 
 Avg loss: 0.72091880 
 F1: 0.455403 
 Precision: 0.456428 
 Recall: 0.454383
 IoU: 0.294836

test Error: 
 Avg loss: 0.63605876 
 F1: 0.574191 
 Precision: 0.606297 
 Recall: 0.545314
 IoU: 0.402713

We have finished training iteration 67
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_65_.pth
per-ex loss: 0.679119  [    1/   88]
per-ex loss: 0.412673  [    2/   88]
per-ex loss: 0.759791  [    3/   88]
per-ex loss: 0.800849  [    4/   88]
per-ex loss: 0.711331  [    5/   88]
per-ex loss: 0.553280  [    6/   88]
per-ex loss: 0.667593  [    7/   88]
per-ex loss: 0.558924  [    8/   88]
per-ex loss: 0.483203  [    9/   88]
per-ex loss: 0.554142  [   10/   88]
per-ex loss: 0.652412  [   11/   88]
per-ex loss: 0.698952  [   12/   88]
per-ex loss: 0.502411  [   13/   88]
per-ex loss: 0.570455  [   14/   88]
per-ex loss: 0.689388  [   15/   88]
per-ex loss: 0.680639  [   16/   88]
per-ex loss: 0.745889  [   17/   88]
per-ex loss: 0.804296  [   18/   88]
per-ex loss: 0.726143  [   19/   88]
per-ex loss: 0.505227  [   20/   88]
per-ex loss: 0.528151  [   21/   88]
per-ex loss: 0.547061  [   22/   88]
per-ex loss: 0.688114  [   23/   88]
per-ex loss: 0.518285  [   24/   88]
per-ex loss: 0.556270  [   25/   88]
per-ex loss: 0.748994  [   26/   88]
per-ex loss: 0.641663  [   27/   88]
per-ex loss: 0.741329  [   28/   88]
per-ex loss: 0.567768  [   29/   88]
per-ex loss: 0.556029  [   30/   88]
per-ex loss: 0.637001  [   31/   88]
per-ex loss: 0.746041  [   32/   88]
per-ex loss: 0.714462  [   33/   88]
per-ex loss: 0.563747  [   34/   88]
per-ex loss: 0.556032  [   35/   88]
per-ex loss: 0.535491  [   36/   88]
per-ex loss: 0.478096  [   37/   88]
per-ex loss: 0.794127  [   38/   88]
per-ex loss: 0.544286  [   39/   88]
per-ex loss: 0.689314  [   40/   88]
per-ex loss: 0.473101  [   41/   88]
per-ex loss: 0.753821  [   42/   88]
per-ex loss: 0.738261  [   43/   88]
per-ex loss: 0.567400  [   44/   88]
per-ex loss: 0.532761  [   45/   88]
per-ex loss: 0.793083  [   46/   88]
per-ex loss: 0.684557  [   47/   88]
per-ex loss: 0.505815  [   48/   88]
per-ex loss: 0.558578  [   49/   88]
per-ex loss: 0.803809  [   50/   88]
per-ex loss: 0.578974  [   51/   88]
per-ex loss: 0.588133  [   52/   88]
per-ex loss: 0.680610  [   53/   88]
per-ex loss: 0.572744  [   54/   88]
per-ex loss: 0.583769  [   55/   88]
per-ex loss: 0.563775  [   56/   88]
per-ex loss: 0.733563  [   57/   88]
per-ex loss: 0.807689  [   58/   88]
per-ex loss: 0.752336  [   59/   88]
per-ex loss: 0.776938  [   60/   88]
per-ex loss: 0.586655  [   61/   88]
per-ex loss: 0.746917  [   62/   88]
per-ex loss: 0.615762  [   63/   88]
per-ex loss: 0.795450  [   64/   88]
per-ex loss: 0.709641  [   65/   88]
per-ex loss: 0.586341  [   66/   88]
per-ex loss: 0.623245  [   67/   88]
per-ex loss: 0.684591  [   68/   88]
per-ex loss: 0.533344  [   69/   88]
per-ex loss: 0.586788  [   70/   88]
per-ex loss: 0.519720  [   71/   88]
per-ex loss: 0.515211  [   72/   88]
per-ex loss: 0.565936  [   73/   88]
per-ex loss: 0.533531  [   74/   88]
per-ex loss: 0.712086  [   75/   88]
per-ex loss: 0.606674  [   76/   88]
per-ex loss: 0.805554  [   77/   88]
per-ex loss: 0.515370  [   78/   88]
per-ex loss: 0.663224  [   79/   88]
per-ex loss: 0.553584  [   80/   88]
per-ex loss: 0.672862  [   81/   88]
per-ex loss: 0.527183  [   82/   88]
per-ex loss: 0.638970  [   83/   88]
per-ex loss: 0.592685  [   84/   88]
per-ex loss: 0.522695  [   85/   88]
per-ex loss: 0.596429  [   86/   88]
per-ex loss: 0.759876  [   87/   88]
per-ex loss: 0.751500  [   88/   88]
Train Error: Avg loss: 0.63273320
validation Error: 
 Avg loss: 0.71308879 
 F1: 0.466234 
 Precision: 0.465678 
 Recall: 0.466791
 IoU: 0.303980

test Error: 
 Avg loss: 0.64335246 
 F1: 0.570524 
 Precision: 0.580758 
 Recall: 0.560644
 IoU: 0.399114

We have finished training iteration 68
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_66_.pth
per-ex loss: 0.506702  [    1/   88]
per-ex loss: 0.560759  [    2/   88]
per-ex loss: 0.722486  [    3/   88]
per-ex loss: 0.712734  [    4/   88]
per-ex loss: 0.804304  [    5/   88]
per-ex loss: 0.488963  [    6/   88]
per-ex loss: 0.592378  [    7/   88]
per-ex loss: 0.577634  [    8/   88]
per-ex loss: 0.573372  [    9/   88]
per-ex loss: 0.693775  [   10/   88]
per-ex loss: 0.781861  [   11/   88]
per-ex loss: 0.618341  [   12/   88]
per-ex loss: 0.530123  [   13/   88]
per-ex loss: 0.755407  [   14/   88]
per-ex loss: 0.685067  [   15/   88]
per-ex loss: 0.761916  [   16/   88]
per-ex loss: 0.740244  [   17/   88]
per-ex loss: 0.495809  [   18/   88]
per-ex loss: 0.709551  [   19/   88]
per-ex loss: 0.546932  [   20/   88]
per-ex loss: 0.536826  [   21/   88]
per-ex loss: 0.561922  [   22/   88]
per-ex loss: 0.785352  [   23/   88]
per-ex loss: 0.638873  [   24/   88]
per-ex loss: 0.578629  [   25/   88]
per-ex loss: 0.560767  [   26/   88]
per-ex loss: 0.717378  [   27/   88]
per-ex loss: 0.518396  [   28/   88]
per-ex loss: 0.699408  [   29/   88]
per-ex loss: 0.500693  [   30/   88]
per-ex loss: 0.594804  [   31/   88]
per-ex loss: 0.802541  [   32/   88]
per-ex loss: 0.629004  [   33/   88]
per-ex loss: 0.739208  [   34/   88]
per-ex loss: 0.547026  [   35/   88]
per-ex loss: 0.560026  [   36/   88]
per-ex loss: 0.671121  [   37/   88]
per-ex loss: 0.656134  [   38/   88]
per-ex loss: 0.516892  [   39/   88]
per-ex loss: 0.710252  [   40/   88]
per-ex loss: 0.613291  [   41/   88]
per-ex loss: 0.660306  [   42/   88]
per-ex loss: 0.528866  [   43/   88]
per-ex loss: 0.485642  [   44/   88]
per-ex loss: 0.471964  [   45/   88]
per-ex loss: 0.615196  [   46/   88]
per-ex loss: 0.531834  [   47/   88]
per-ex loss: 0.565047  [   48/   88]
per-ex loss: 0.563396  [   49/   88]
per-ex loss: 0.767653  [   50/   88]
per-ex loss: 0.703969  [   51/   88]
per-ex loss: 0.515622  [   52/   88]
per-ex loss: 0.772991  [   53/   88]
per-ex loss: 0.559497  [   54/   88]
per-ex loss: 0.809926  [   55/   88]
per-ex loss: 0.788939  [   56/   88]
per-ex loss: 0.632422  [   57/   88]
per-ex loss: 0.704181  [   58/   88]
per-ex loss: 0.515016  [   59/   88]
per-ex loss: 0.721369  [   60/   88]
per-ex loss: 0.804396  [   61/   88]
per-ex loss: 0.407502  [   62/   88]
per-ex loss: 0.675314  [   63/   88]
per-ex loss: 0.577790  [   64/   88]
per-ex loss: 0.738003  [   65/   88]
per-ex loss: 0.593596  [   66/   88]
per-ex loss: 0.667684  [   67/   88]
per-ex loss: 0.564312  [   68/   88]
per-ex loss: 0.787112  [   69/   88]
per-ex loss: 0.530297  [   70/   88]
per-ex loss: 0.564652  [   71/   88]
per-ex loss: 0.766245  [   72/   88]
per-ex loss: 0.734466  [   73/   88]
per-ex loss: 0.593084  [   74/   88]
per-ex loss: 0.580723  [   75/   88]
per-ex loss: 0.704484  [   76/   88]
per-ex loss: 0.675883  [   77/   88]
per-ex loss: 0.539172  [   78/   88]
per-ex loss: 0.694027  [   79/   88]
per-ex loss: 0.744235  [   80/   88]
per-ex loss: 0.499623  [   81/   88]
per-ex loss: 0.593979  [   82/   88]
per-ex loss: 0.738007  [   83/   88]
per-ex loss: 0.704601  [   84/   88]
per-ex loss: 0.536845  [   85/   88]
per-ex loss: 0.606740  [   86/   88]
per-ex loss: 0.569060  [   87/   88]
per-ex loss: 0.644123  [   88/   88]
Train Error: Avg loss: 0.63348505
validation Error: 
 Avg loss: 0.69416153 
 F1: 0.490051 
 Precision: 0.560281 
 Recall: 0.435467
 IoU: 0.324548

test Error: 
 Avg loss: 0.64121227 
 F1: 0.567947 
 Precision: 0.622088 
 Recall: 0.522476
 IoU: 0.396597

We have finished training iteration 69
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_67_.pth
per-ex loss: 0.668112  [    1/   88]
per-ex loss: 0.631942  [    2/   88]
per-ex loss: 0.673918  [    3/   88]
per-ex loss: 0.543376  [    4/   88]
per-ex loss: 0.580889  [    5/   88]
per-ex loss: 0.724459  [    6/   88]
per-ex loss: 0.578945  [    7/   88]
per-ex loss: 0.516255  [    8/   88]
per-ex loss: 0.494784  [    9/   88]
per-ex loss: 0.721896  [   10/   88]
per-ex loss: 0.576491  [   11/   88]
per-ex loss: 0.725114  [   12/   88]
per-ex loss: 0.618800  [   13/   88]
per-ex loss: 0.554085  [   14/   88]
per-ex loss: 0.581677  [   15/   88]
per-ex loss: 0.745382  [   16/   88]
per-ex loss: 0.694973  [   17/   88]
per-ex loss: 0.517968  [   18/   88]
per-ex loss: 0.753075  [   19/   88]
per-ex loss: 0.802230  [   20/   88]
per-ex loss: 0.633559  [   21/   88]
per-ex loss: 0.730101  [   22/   88]
per-ex loss: 0.833838  [   23/   88]
per-ex loss: 0.695889  [   24/   88]
per-ex loss: 0.576134  [   25/   88]
per-ex loss: 0.614353  [   26/   88]
per-ex loss: 0.483107  [   27/   88]
per-ex loss: 0.568181  [   28/   88]
per-ex loss: 0.539441  [   29/   88]
per-ex loss: 0.539057  [   30/   88]
per-ex loss: 0.758490  [   31/   88]
per-ex loss: 0.707956  [   32/   88]
per-ex loss: 0.680620  [   33/   88]
per-ex loss: 0.542968  [   34/   88]
per-ex loss: 0.675184  [   35/   88]
per-ex loss: 0.525894  [   36/   88]
per-ex loss: 0.786906  [   37/   88]
per-ex loss: 0.673564  [   38/   88]
per-ex loss: 0.699283  [   39/   88]
per-ex loss: 0.735739  [   40/   88]
per-ex loss: 0.730103  [   41/   88]
per-ex loss: 0.746142  [   42/   88]
per-ex loss: 0.394842  [   43/   88]
per-ex loss: 0.759640  [   44/   88]
per-ex loss: 0.515677  [   45/   88]
per-ex loss: 0.553383  [   46/   88]
per-ex loss: 0.617353  [   47/   88]
per-ex loss: 0.781997  [   48/   88]
per-ex loss: 0.571885  [   49/   88]
per-ex loss: 0.559765  [   50/   88]
per-ex loss: 0.721554  [   51/   88]
per-ex loss: 0.715992  [   52/   88]
per-ex loss: 0.618940  [   53/   88]
per-ex loss: 0.546120  [   54/   88]
per-ex loss: 0.724431  [   55/   88]
per-ex loss: 0.612203  [   56/   88]
per-ex loss: 0.790410  [   57/   88]
per-ex loss: 0.505207  [   58/   88]
per-ex loss: 0.555415  [   59/   88]
per-ex loss: 0.592705  [   60/   88]
per-ex loss: 0.562321  [   61/   88]
per-ex loss: 0.717928  [   62/   88]
per-ex loss: 0.826494  [   63/   88]
per-ex loss: 0.591079  [   64/   88]
per-ex loss: 0.689550  [   65/   88]
per-ex loss: 0.532168  [   66/   88]
per-ex loss: 0.761103  [   67/   88]
per-ex loss: 0.739927  [   68/   88]
per-ex loss: 0.572386  [   69/   88]
per-ex loss: 0.550768  [   70/   88]
per-ex loss: 0.735084  [   71/   88]
per-ex loss: 0.793324  [   72/   88]
per-ex loss: 0.574193  [   73/   88]
per-ex loss: 0.667322  [   74/   88]
per-ex loss: 0.550679  [   75/   88]
per-ex loss: 0.542428  [   76/   88]
per-ex loss: 0.540229  [   77/   88]
per-ex loss: 0.497218  [   78/   88]
per-ex loss: 0.695751  [   79/   88]
per-ex loss: 0.482563  [   80/   88]
per-ex loss: 0.500615  [   81/   88]
per-ex loss: 0.559234  [   82/   88]
per-ex loss: 0.773336  [   83/   88]
per-ex loss: 0.613234  [   84/   88]
per-ex loss: 0.615671  [   85/   88]
per-ex loss: 0.683249  [   86/   88]
per-ex loss: 0.507672  [   87/   88]
per-ex loss: 0.508560  [   88/   88]
Train Error: Avg loss: 0.63298281
validation Error: 
 Avg loss: 0.70731072 
 F1: 0.474837 
 Precision: 0.528830 
 Recall: 0.430849
 IoU: 0.311336

test Error: 
 Avg loss: 0.63914297 
 F1: 0.573216 
 Precision: 0.643180 
 Recall: 0.516980
 IoU: 0.401754

We have finished training iteration 70
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_68_.pth
per-ex loss: 0.519920  [    1/   88]
per-ex loss: 0.524753  [    2/   88]
per-ex loss: 0.791525  [    3/   88]
per-ex loss: 0.669841  [    4/   88]
per-ex loss: 0.683185  [    5/   88]
per-ex loss: 0.573257  [    6/   88]
per-ex loss: 0.620046  [    7/   88]
per-ex loss: 0.720138  [    8/   88]
per-ex loss: 0.487107  [    9/   88]
per-ex loss: 0.700213  [   10/   88]
per-ex loss: 0.573600  [   11/   88]
per-ex loss: 0.539154  [   12/   88]
per-ex loss: 0.647784  [   13/   88]
per-ex loss: 0.601998  [   14/   88]
per-ex loss: 0.548560  [   15/   88]
per-ex loss: 0.479342  [   16/   88]
per-ex loss: 0.701494  [   17/   88]
per-ex loss: 0.622735  [   18/   88]
per-ex loss: 0.716769  [   19/   88]
per-ex loss: 0.547840  [   20/   88]
per-ex loss: 0.696868  [   21/   88]
per-ex loss: 0.744511  [   22/   88]
per-ex loss: 0.642480  [   23/   88]
per-ex loss: 0.793829  [   24/   88]
per-ex loss: 0.587515  [   25/   88]
per-ex loss: 0.547788  [   26/   88]
per-ex loss: 0.694182  [   27/   88]
per-ex loss: 0.508024  [   28/   88]
per-ex loss: 0.722930  [   29/   88]
per-ex loss: 0.624176  [   30/   88]
per-ex loss: 0.532517  [   31/   88]
per-ex loss: 0.717340  [   32/   88]
per-ex loss: 0.752661  [   33/   88]
per-ex loss: 0.544376  [   34/   88]
per-ex loss: 0.522071  [   35/   88]
per-ex loss: 0.515494  [   36/   88]
per-ex loss: 0.724072  [   37/   88]
per-ex loss: 0.553080  [   38/   88]
per-ex loss: 0.654838  [   39/   88]
per-ex loss: 0.786327  [   40/   88]
per-ex loss: 0.755882  [   41/   88]
per-ex loss: 0.498409  [   42/   88]
per-ex loss: 0.626806  [   43/   88]
per-ex loss: 0.543871  [   44/   88]
per-ex loss: 0.603571  [   45/   88]
per-ex loss: 0.505903  [   46/   88]
per-ex loss: 0.597907  [   47/   88]
per-ex loss: 0.541555  [   48/   88]
per-ex loss: 0.571695  [   49/   88]
per-ex loss: 0.534325  [   50/   88]
per-ex loss: 0.747932  [   51/   88]
per-ex loss: 0.501434  [   52/   88]
per-ex loss: 0.737753  [   53/   88]
per-ex loss: 0.539108  [   54/   88]
per-ex loss: 0.505161  [   55/   88]
per-ex loss: 0.516062  [   56/   88]
per-ex loss: 0.768094  [   57/   88]
per-ex loss: 0.589390  [   58/   88]
per-ex loss: 0.654887  [   59/   88]
per-ex loss: 0.767076  [   60/   88]
per-ex loss: 0.807471  [   61/   88]
per-ex loss: 0.749745  [   62/   88]
per-ex loss: 0.445158  [   63/   88]
per-ex loss: 0.739430  [   64/   88]
per-ex loss: 0.784732  [   65/   88]
per-ex loss: 0.589689  [   66/   88]
per-ex loss: 0.593607  [   67/   88]
per-ex loss: 0.592796  [   68/   88]
per-ex loss: 0.761095  [   69/   88]
per-ex loss: 0.582793  [   70/   88]
per-ex loss: 0.811106  [   71/   88]
per-ex loss: 0.690441  [   72/   88]
per-ex loss: 0.642118  [   73/   88]
per-ex loss: 0.549242  [   74/   88]
per-ex loss: 0.541579  [   75/   88]
per-ex loss: 0.742720  [   76/   88]
per-ex loss: 0.768992  [   77/   88]
per-ex loss: 0.799280  [   78/   88]
per-ex loss: 0.669905  [   79/   88]
per-ex loss: 0.707848  [   80/   88]
per-ex loss: 0.507399  [   81/   88]
per-ex loss: 0.563357  [   82/   88]
per-ex loss: 0.683290  [   83/   88]
per-ex loss: 0.686555  [   84/   88]
per-ex loss: 0.568905  [   85/   88]
per-ex loss: 0.797706  [   86/   88]
per-ex loss: 0.615821  [   87/   88]
per-ex loss: 0.520600  [   88/   88]
Train Error: Avg loss: 0.63357435
validation Error: 
 Avg loss: 0.69996958 
 F1: 0.484665 
 Precision: 0.516359 
 Recall: 0.456637
 IoU: 0.319840

test Error: 
 Avg loss: 0.63787796 
 F1: 0.571682 
 Precision: 0.614788 
 Recall: 0.534224
 IoU: 0.400248

We have finished training iteration 71
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_57_.pth
per-ex loss: 0.777098  [    1/   88]
per-ex loss: 0.578074  [    2/   88]
per-ex loss: 0.663805  [    3/   88]
per-ex loss: 0.466055  [    4/   88]
per-ex loss: 0.780447  [    5/   88]
per-ex loss: 0.497384  [    6/   88]
per-ex loss: 0.660495  [    7/   88]
per-ex loss: 0.624003  [    8/   88]
per-ex loss: 0.540413  [    9/   88]
per-ex loss: 0.575487  [   10/   88]
per-ex loss: 0.769025  [   11/   88]
per-ex loss: 0.559865  [   12/   88]
per-ex loss: 0.580302  [   13/   88]
per-ex loss: 0.534988  [   14/   88]
per-ex loss: 0.708990  [   15/   88]
per-ex loss: 0.744180  [   16/   88]
per-ex loss: 0.554222  [   17/   88]
per-ex loss: 0.479979  [   18/   88]
per-ex loss: 0.751652  [   19/   88]
per-ex loss: 0.733709  [   20/   88]
per-ex loss: 0.678259  [   21/   88]
per-ex loss: 0.530097  [   22/   88]
per-ex loss: 0.749157  [   23/   88]
per-ex loss: 0.583245  [   24/   88]
per-ex loss: 0.543752  [   25/   88]
per-ex loss: 0.609945  [   26/   88]
per-ex loss: 0.499622  [   27/   88]
per-ex loss: 0.597281  [   28/   88]
per-ex loss: 0.599976  [   29/   88]
per-ex loss: 0.799221  [   30/   88]
per-ex loss: 0.744140  [   31/   88]
per-ex loss: 0.713223  [   32/   88]
per-ex loss: 0.715928  [   33/   88]
per-ex loss: 0.464251  [   34/   88]
per-ex loss: 0.709092  [   35/   88]
per-ex loss: 0.804970  [   36/   88]
per-ex loss: 0.643737  [   37/   88]
per-ex loss: 0.659445  [   38/   88]
per-ex loss: 0.523170  [   39/   88]
per-ex loss: 0.704134  [   40/   88]
per-ex loss: 0.512047  [   41/   88]
per-ex loss: 0.544247  [   42/   88]
per-ex loss: 0.704263  [   43/   88]
per-ex loss: 0.543123  [   44/   88]
per-ex loss: 0.563615  [   45/   88]
per-ex loss: 0.702722  [   46/   88]
per-ex loss: 0.708037  [   47/   88]
per-ex loss: 0.679183  [   48/   88]
per-ex loss: 0.656101  [   49/   88]
per-ex loss: 0.588553  [   50/   88]
per-ex loss: 0.608743  [   51/   88]
per-ex loss: 0.593984  [   52/   88]
per-ex loss: 0.555486  [   53/   88]
per-ex loss: 0.806274  [   54/   88]
per-ex loss: 0.562235  [   55/   88]
per-ex loss: 0.687966  [   56/   88]
per-ex loss: 0.718919  [   57/   88]
per-ex loss: 0.785937  [   58/   88]
per-ex loss: 0.498810  [   59/   88]
per-ex loss: 0.729600  [   60/   88]
per-ex loss: 0.693035  [   61/   88]
per-ex loss: 0.425415  [   62/   88]
per-ex loss: 0.519876  [   63/   88]
per-ex loss: 0.549794  [   64/   88]
per-ex loss: 0.616501  [   65/   88]
per-ex loss: 0.547041  [   66/   88]
per-ex loss: 0.555976  [   67/   88]
per-ex loss: 0.747184  [   68/   88]
per-ex loss: 0.741606  [   69/   88]
per-ex loss: 0.748042  [   70/   88]
per-ex loss: 0.554218  [   71/   88]
per-ex loss: 0.511392  [   72/   88]
per-ex loss: 0.536091  [   73/   88]
per-ex loss: 0.733506  [   74/   88]
per-ex loss: 0.576072  [   75/   88]
per-ex loss: 0.524115  [   76/   88]
per-ex loss: 0.666502  [   77/   88]
per-ex loss: 0.787519  [   78/   88]
per-ex loss: 0.555093  [   79/   88]
per-ex loss: 0.607645  [   80/   88]
per-ex loss: 0.806963  [   81/   88]
per-ex loss: 0.551948  [   82/   88]
per-ex loss: 0.714040  [   83/   88]
per-ex loss: 0.559611  [   84/   88]
per-ex loss: 0.525684  [   85/   88]
per-ex loss: 0.688888  [   86/   88]
per-ex loss: 0.545051  [   87/   88]
per-ex loss: 0.570098  [   88/   88]
Train Error: Avg loss: 0.62910881
validation Error: 
 Avg loss: 0.70311161 
 F1: 0.480844 
 Precision: 0.521595 
 Recall: 0.445999
 IoU: 0.316520

test Error: 
 Avg loss: 0.63880615 
 F1: 0.574763 
 Precision: 0.623209 
 Recall: 0.533305
 IoU: 0.403275

We have finished training iteration 72
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_70_.pth
per-ex loss: 0.696048  [    1/   88]
per-ex loss: 0.789381  [    2/   88]
per-ex loss: 0.524004  [    3/   88]
per-ex loss: 0.650191  [    4/   88]
per-ex loss: 0.697853  [    5/   88]
per-ex loss: 0.544489  [    6/   88]
per-ex loss: 0.738849  [    7/   88]
per-ex loss: 0.793942  [    8/   88]
per-ex loss: 0.525122  [    9/   88]
per-ex loss: 0.652058  [   10/   88]
per-ex loss: 0.715630  [   11/   88]
per-ex loss: 0.703716  [   12/   88]
per-ex loss: 0.585537  [   13/   88]
per-ex loss: 0.536165  [   14/   88]
per-ex loss: 0.530059  [   15/   88]
per-ex loss: 0.600157  [   16/   88]
per-ex loss: 0.801354  [   17/   88]
per-ex loss: 0.596775  [   18/   88]
per-ex loss: 0.589097  [   19/   88]
per-ex loss: 0.639537  [   20/   88]
per-ex loss: 0.776335  [   21/   88]
per-ex loss: 0.573671  [   22/   88]
per-ex loss: 0.609079  [   23/   88]
per-ex loss: 0.550137  [   24/   88]
per-ex loss: 0.657783  [   25/   88]
per-ex loss: 0.732497  [   26/   88]
per-ex loss: 0.510603  [   27/   88]
per-ex loss: 0.682352  [   28/   88]
per-ex loss: 0.659346  [   29/   88]
per-ex loss: 0.749677  [   30/   88]
per-ex loss: 0.751201  [   31/   88]
per-ex loss: 0.711644  [   32/   88]
per-ex loss: 0.630904  [   33/   88]
per-ex loss: 0.544471  [   34/   88]
per-ex loss: 0.738441  [   35/   88]
per-ex loss: 0.736370  [   36/   88]
per-ex loss: 0.534765  [   37/   88]
per-ex loss: 0.673922  [   38/   88]
per-ex loss: 0.764689  [   39/   88]
per-ex loss: 0.515617  [   40/   88]
per-ex loss: 0.516475  [   41/   88]
per-ex loss: 0.743756  [   42/   88]
per-ex loss: 0.695169  [   43/   88]
per-ex loss: 0.731703  [   44/   88]
per-ex loss: 0.600079  [   45/   88]
per-ex loss: 0.780751  [   46/   88]
per-ex loss: 0.647127  [   47/   88]
per-ex loss: 0.772615  [   48/   88]
per-ex loss: 0.608375  [   49/   88]
per-ex loss: 0.759173  [   50/   88]
per-ex loss: 0.578643  [   51/   88]
per-ex loss: 0.587724  [   52/   88]
per-ex loss: 0.516161  [   53/   88]
per-ex loss: 0.541981  [   54/   88]
per-ex loss: 0.698988  [   55/   88]
per-ex loss: 0.500378  [   56/   88]
per-ex loss: 0.570051  [   57/   88]
per-ex loss: 0.534974  [   58/   88]
per-ex loss: 0.577540  [   59/   88]
per-ex loss: 0.537299  [   60/   88]
per-ex loss: 0.729356  [   61/   88]
per-ex loss: 0.460003  [   62/   88]
per-ex loss: 0.737343  [   63/   88]
per-ex loss: 0.474307  [   64/   88]
per-ex loss: 0.532623  [   65/   88]
per-ex loss: 0.592925  [   66/   88]
per-ex loss: 0.688608  [   67/   88]
per-ex loss: 0.579437  [   68/   88]
per-ex loss: 0.513056  [   69/   88]
per-ex loss: 0.559829  [   70/   88]
per-ex loss: 0.544361  [   71/   88]
per-ex loss: 0.558058  [   72/   88]
per-ex loss: 0.592568  [   73/   88]
per-ex loss: 0.715801  [   74/   88]
per-ex loss: 0.814954  [   75/   88]
per-ex loss: 0.473068  [   76/   88]
per-ex loss: 0.596147  [   77/   88]
per-ex loss: 0.800861  [   78/   88]
per-ex loss: 0.506297  [   79/   88]
per-ex loss: 0.539434  [   80/   88]
per-ex loss: 0.744441  [   81/   88]
per-ex loss: 0.521006  [   82/   88]
per-ex loss: 0.595255  [   83/   88]
per-ex loss: 0.542059  [   84/   88]
per-ex loss: 0.780013  [   85/   88]
per-ex loss: 0.697007  [   86/   88]
per-ex loss: 0.613208  [   87/   88]
per-ex loss: 0.390581  [   88/   88]
Train Error: Avg loss: 0.62960267
validation Error: 
 Avg loss: 0.70432806 
 F1: 0.478503 
 Precision: 0.532834 
 Recall: 0.434227
 IoU: 0.314495

test Error: 
 Avg loss: 0.63192441 
 F1: 0.578213 
 Precision: 0.657833 
 Recall: 0.515786
 IoU: 0.406681

We have finished training iteration 73
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_71_.pth
per-ex loss: 0.624971  [    1/   88]
per-ex loss: 0.677127  [    2/   88]
per-ex loss: 0.604335  [    3/   88]
per-ex loss: 0.535922  [    4/   88]
per-ex loss: 0.504321  [    5/   88]
per-ex loss: 0.517566  [    6/   88]
per-ex loss: 0.563214  [    7/   88]
per-ex loss: 0.551798  [    8/   88]
per-ex loss: 0.775389  [    9/   88]
per-ex loss: 0.754242  [   10/   88]
per-ex loss: 0.753727  [   11/   88]
per-ex loss: 0.501423  [   12/   88]
per-ex loss: 0.759439  [   13/   88]
per-ex loss: 0.567328  [   14/   88]
per-ex loss: 0.542280  [   15/   88]
per-ex loss: 0.655748  [   16/   88]
per-ex loss: 0.634498  [   17/   88]
per-ex loss: 0.644242  [   18/   88]
per-ex loss: 0.525425  [   19/   88]
per-ex loss: 0.703087  [   20/   88]
per-ex loss: 0.801884  [   21/   88]
per-ex loss: 0.544903  [   22/   88]
per-ex loss: 0.722323  [   23/   88]
per-ex loss: 0.545923  [   24/   88]
per-ex loss: 0.743823  [   25/   88]
per-ex loss: 0.694157  [   26/   88]
per-ex loss: 0.691555  [   27/   88]
per-ex loss: 0.484459  [   28/   88]
per-ex loss: 0.525507  [   29/   88]
per-ex loss: 0.745610  [   30/   88]
per-ex loss: 0.531426  [   31/   88]
per-ex loss: 0.728044  [   32/   88]
per-ex loss: 0.781067  [   33/   88]
per-ex loss: 0.692786  [   34/   88]
per-ex loss: 0.748411  [   35/   88]
per-ex loss: 0.677463  [   36/   88]
per-ex loss: 0.690794  [   37/   88]
per-ex loss: 0.714478  [   38/   88]
per-ex loss: 0.581950  [   39/   88]
per-ex loss: 0.694654  [   40/   88]
per-ex loss: 0.565522  [   41/   88]
per-ex loss: 0.729261  [   42/   88]
per-ex loss: 0.747576  [   43/   88]
per-ex loss: 0.598727  [   44/   88]
per-ex loss: 0.744765  [   45/   88]
per-ex loss: 0.713624  [   46/   88]
per-ex loss: 0.538728  [   47/   88]
per-ex loss: 0.751294  [   48/   88]
per-ex loss: 0.563493  [   49/   88]
per-ex loss: 0.517257  [   50/   88]
per-ex loss: 0.528525  [   51/   88]
per-ex loss: 0.785555  [   52/   88]
per-ex loss: 0.718725  [   53/   88]
per-ex loss: 0.435421  [   54/   88]
per-ex loss: 0.670198  [   55/   88]
per-ex loss: 0.534802  [   56/   88]
per-ex loss: 0.697927  [   57/   88]
per-ex loss: 0.566197  [   58/   88]
per-ex loss: 0.582421  [   59/   88]
per-ex loss: 0.505971  [   60/   88]
per-ex loss: 0.501864  [   61/   88]
per-ex loss: 0.598706  [   62/   88]
per-ex loss: 0.659365  [   63/   88]
per-ex loss: 0.570123  [   64/   88]
per-ex loss: 0.492624  [   65/   88]
per-ex loss: 0.652840  [   66/   88]
per-ex loss: 0.543918  [   67/   88]
per-ex loss: 0.560205  [   68/   88]
per-ex loss: 0.531698  [   69/   88]
per-ex loss: 0.594543  [   70/   88]
per-ex loss: 0.812788  [   71/   88]
per-ex loss: 0.599976  [   72/   88]
per-ex loss: 0.557722  [   73/   88]
per-ex loss: 0.574553  [   74/   88]
per-ex loss: 0.689356  [   75/   88]
per-ex loss: 0.733713  [   76/   88]
per-ex loss: 0.544943  [   77/   88]
per-ex loss: 0.640606  [   78/   88]
per-ex loss: 0.610708  [   79/   88]
per-ex loss: 0.587646  [   80/   88]
per-ex loss: 0.607470  [   81/   88]
per-ex loss: 0.572961  [   82/   88]
per-ex loss: 0.775187  [   83/   88]
per-ex loss: 0.783746  [   84/   88]
per-ex loss: 0.524957  [   85/   88]
per-ex loss: 0.572386  [   86/   88]
per-ex loss: 0.504291  [   87/   88]
per-ex loss: 0.758645  [   88/   88]
Train Error: Avg loss: 0.62944123
validation Error: 
 Avg loss: 0.70412965 
 F1: 0.478451 
 Precision: 0.647066 
 Recall: 0.379548
 IoU: 0.314450

test Error: 
 Avg loss: 0.65645855 
 F1: 0.546534 
 Precision: 0.723670 
 Recall: 0.439062
 IoU: 0.376021

We have finished training iteration 74
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_72_.pth
per-ex loss: 0.577108  [    1/   88]
per-ex loss: 0.771598  [    2/   88]
per-ex loss: 0.671662  [    3/   88]
per-ex loss: 0.683442  [    4/   88]
per-ex loss: 0.686957  [    5/   88]
per-ex loss: 0.539397  [    6/   88]
per-ex loss: 0.607042  [    7/   88]
per-ex loss: 0.541444  [    8/   88]
per-ex loss: 0.489951  [    9/   88]
per-ex loss: 0.673437  [   10/   88]
per-ex loss: 0.798480  [   11/   88]
per-ex loss: 0.676040  [   12/   88]
per-ex loss: 0.783610  [   13/   88]
per-ex loss: 0.604587  [   14/   88]
per-ex loss: 0.734095  [   15/   88]
per-ex loss: 0.750710  [   16/   88]
per-ex loss: 0.732376  [   17/   88]
per-ex loss: 0.502934  [   18/   88]
per-ex loss: 0.709659  [   19/   88]
per-ex loss: 0.554952  [   20/   88]
per-ex loss: 0.660564  [   21/   88]
per-ex loss: 0.578086  [   22/   88]
per-ex loss: 0.518444  [   23/   88]
per-ex loss: 0.511310  [   24/   88]
per-ex loss: 0.590341  [   25/   88]
per-ex loss: 0.659018  [   26/   88]
per-ex loss: 0.753071  [   27/   88]
per-ex loss: 0.664419  [   28/   88]
per-ex loss: 0.563016  [   29/   88]
per-ex loss: 0.553723  [   30/   88]
per-ex loss: 0.577386  [   31/   88]
per-ex loss: 0.706291  [   32/   88]
per-ex loss: 0.532798  [   33/   88]
per-ex loss: 0.709922  [   34/   88]
per-ex loss: 0.644604  [   35/   88]
per-ex loss: 0.572996  [   36/   88]
per-ex loss: 0.627599  [   37/   88]
per-ex loss: 0.702832  [   38/   88]
per-ex loss: 0.737990  [   39/   88]
per-ex loss: 0.551279  [   40/   88]
per-ex loss: 0.613382  [   41/   88]
per-ex loss: 0.628023  [   42/   88]
per-ex loss: 0.734674  [   43/   88]
per-ex loss: 0.376807  [   44/   88]
per-ex loss: 0.600971  [   45/   88]
per-ex loss: 0.727768  [   46/   88]
per-ex loss: 0.679043  [   47/   88]
per-ex loss: 0.578043  [   48/   88]
per-ex loss: 0.618284  [   49/   88]
per-ex loss: 0.565951  [   50/   88]
per-ex loss: 0.664764  [   51/   88]
per-ex loss: 0.499457  [   52/   88]
per-ex loss: 0.717720  [   53/   88]
per-ex loss: 0.570940  [   54/   88]
per-ex loss: 0.799779  [   55/   88]
per-ex loss: 0.698704  [   56/   88]
per-ex loss: 0.734249  [   57/   88]
per-ex loss: 0.547593  [   58/   88]
per-ex loss: 0.736580  [   59/   88]
per-ex loss: 0.564233  [   60/   88]
per-ex loss: 0.594744  [   61/   88]
per-ex loss: 0.541423  [   62/   88]
per-ex loss: 0.549420  [   63/   88]
per-ex loss: 0.484519  [   64/   88]
per-ex loss: 0.761736  [   65/   88]
per-ex loss: 0.539966  [   66/   88]
per-ex loss: 0.553529  [   67/   88]
per-ex loss: 0.481565  [   68/   88]
per-ex loss: 0.528408  [   69/   88]
per-ex loss: 0.746929  [   70/   88]
per-ex loss: 0.510110  [   71/   88]
per-ex loss: 0.544960  [   72/   88]
per-ex loss: 0.711721  [   73/   88]
per-ex loss: 0.818811  [   74/   88]
per-ex loss: 0.747784  [   75/   88]
per-ex loss: 0.529962  [   76/   88]
per-ex loss: 0.460507  [   77/   88]
per-ex loss: 0.774903  [   78/   88]
per-ex loss: 0.532408  [   79/   88]
per-ex loss: 0.785098  [   80/   88]
per-ex loss: 0.632633  [   81/   88]
per-ex loss: 0.597177  [   82/   88]
per-ex loss: 0.788953  [   83/   88]
per-ex loss: 0.588930  [   84/   88]
per-ex loss: 0.681475  [   85/   88]
per-ex loss: 0.539683  [   86/   88]
per-ex loss: 0.747955  [   87/   88]
per-ex loss: 0.501977  [   88/   88]
Train Error: Avg loss: 0.62999343
validation Error: 
 Avg loss: 0.69605610 
 F1: 0.489785 
 Precision: 0.528926 
 Recall: 0.456038
 IoU: 0.324315

test Error: 
 Avg loss: 0.63430609 
 F1: 0.577581 
 Precision: 0.611743 
 Recall: 0.547033
 IoU: 0.406056

We have finished training iteration 75
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_73_.pth
per-ex loss: 0.522466  [    1/   88]
per-ex loss: 0.742471  [    2/   88]
per-ex loss: 0.680612  [    3/   88]
per-ex loss: 0.601009  [    4/   88]
per-ex loss: 0.536680  [    5/   88]
per-ex loss: 0.669159  [    6/   88]
per-ex loss: 0.741659  [    7/   88]
per-ex loss: 0.528375  [    8/   88]
per-ex loss: 0.592070  [    9/   88]
per-ex loss: 0.548409  [   10/   88]
per-ex loss: 0.500759  [   11/   88]
per-ex loss: 0.707727  [   12/   88]
per-ex loss: 0.567741  [   13/   88]
per-ex loss: 0.507981  [   14/   88]
per-ex loss: 0.731566  [   15/   88]
per-ex loss: 0.803691  [   16/   88]
per-ex loss: 0.555048  [   17/   88]
per-ex loss: 0.498739  [   18/   88]
per-ex loss: 0.566343  [   19/   88]
per-ex loss: 0.702692  [   20/   88]
per-ex loss: 0.597402  [   21/   88]
per-ex loss: 0.724528  [   22/   88]
per-ex loss: 0.780088  [   23/   88]
per-ex loss: 0.531563  [   24/   88]
per-ex loss: 0.575266  [   25/   88]
per-ex loss: 0.527300  [   26/   88]
per-ex loss: 0.496642  [   27/   88]
per-ex loss: 0.593027  [   28/   88]
per-ex loss: 0.653215  [   29/   88]
per-ex loss: 0.681505  [   30/   88]
per-ex loss: 0.538203  [   31/   88]
per-ex loss: 0.642539  [   32/   88]
per-ex loss: 0.793310  [   33/   88]
per-ex loss: 0.647710  [   34/   88]
per-ex loss: 0.575413  [   35/   88]
per-ex loss: 0.577889  [   36/   88]
per-ex loss: 0.685022  [   37/   88]
per-ex loss: 0.714625  [   38/   88]
per-ex loss: 0.778788  [   39/   88]
per-ex loss: 0.529189  [   40/   88]
per-ex loss: 0.540606  [   41/   88]
per-ex loss: 0.773458  [   42/   88]
per-ex loss: 0.702990  [   43/   88]
per-ex loss: 0.658192  [   44/   88]
per-ex loss: 0.584319  [   45/   88]
per-ex loss: 0.749000  [   46/   88]
per-ex loss: 0.471955  [   47/   88]
per-ex loss: 0.463760  [   48/   88]
per-ex loss: 0.564195  [   49/   88]
per-ex loss: 0.796994  [   50/   88]
per-ex loss: 0.617530  [   51/   88]
per-ex loss: 0.709554  [   52/   88]
per-ex loss: 0.737364  [   53/   88]
per-ex loss: 0.555008  [   54/   88]
per-ex loss: 0.493091  [   55/   88]
per-ex loss: 0.510411  [   56/   88]
per-ex loss: 0.565618  [   57/   88]
per-ex loss: 0.686649  [   58/   88]
per-ex loss: 0.645687  [   59/   88]
per-ex loss: 0.633660  [   60/   88]
per-ex loss: 0.844649  [   61/   88]
per-ex loss: 0.588287  [   62/   88]
per-ex loss: 0.553801  [   63/   88]
per-ex loss: 0.470603  [   64/   88]
per-ex loss: 0.613698  [   65/   88]
per-ex loss: 0.811426  [   66/   88]
per-ex loss: 0.515862  [   67/   88]
per-ex loss: 0.682684  [   68/   88]
per-ex loss: 0.706301  [   69/   88]
per-ex loss: 0.560074  [   70/   88]
per-ex loss: 0.780994  [   71/   88]
per-ex loss: 0.744807  [   72/   88]
per-ex loss: 0.735594  [   73/   88]
per-ex loss: 0.795611  [   74/   88]
per-ex loss: 0.747662  [   75/   88]
per-ex loss: 0.600274  [   76/   88]
per-ex loss: 0.736552  [   77/   88]
per-ex loss: 0.525617  [   78/   88]
per-ex loss: 0.539545  [   79/   88]
per-ex loss: 0.606228  [   80/   88]
per-ex loss: 0.747638  [   81/   88]
per-ex loss: 0.544861  [   82/   88]
per-ex loss: 0.638226  [   83/   88]
per-ex loss: 0.422840  [   84/   88]
per-ex loss: 0.696623  [   85/   88]
per-ex loss: 0.586038  [   86/   88]
per-ex loss: 0.536711  [   87/   88]
per-ex loss: 0.728775  [   88/   88]
Train Error: Avg loss: 0.63034596
validation Error: 
 Avg loss: 0.70277750 
 F1: 0.478929 
 Precision: 0.481344 
 Recall: 0.476539
 IoU: 0.314863

test Error: 
 Avg loss: 0.64269342 
 F1: 0.568825 
 Precision: 0.564013 
 Recall: 0.573721
 IoU: 0.397454

We have finished training iteration 76
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_74_.pth
per-ex loss: 0.457259  [    1/   88]
per-ex loss: 0.724590  [    2/   88]
per-ex loss: 0.813976  [    3/   88]
per-ex loss: 0.568185  [    4/   88]
per-ex loss: 0.541992  [    5/   88]
per-ex loss: 0.584201  [    6/   88]
per-ex loss: 0.651690  [    7/   88]
per-ex loss: 0.622921  [    8/   88]
per-ex loss: 0.668334  [    9/   88]
per-ex loss: 0.734124  [   10/   88]
per-ex loss: 0.504032  [   11/   88]
per-ex loss: 0.552325  [   12/   88]
per-ex loss: 0.659041  [   13/   88]
per-ex loss: 0.531673  [   14/   88]
per-ex loss: 0.751976  [   15/   88]
per-ex loss: 0.505914  [   16/   88]
per-ex loss: 0.547943  [   17/   88]
per-ex loss: 0.557781  [   18/   88]
per-ex loss: 0.782094  [   19/   88]
per-ex loss: 0.791275  [   20/   88]
per-ex loss: 0.725379  [   21/   88]
per-ex loss: 0.522814  [   22/   88]
per-ex loss: 0.513394  [   23/   88]
per-ex loss: 0.494665  [   24/   88]
per-ex loss: 0.516794  [   25/   88]
per-ex loss: 0.746728  [   26/   88]
per-ex loss: 0.672778  [   27/   88]
per-ex loss: 0.527195  [   28/   88]
per-ex loss: 0.631877  [   29/   88]
per-ex loss: 0.566665  [   30/   88]
per-ex loss: 0.795152  [   31/   88]
per-ex loss: 0.509395  [   32/   88]
per-ex loss: 0.700431  [   33/   88]
per-ex loss: 0.608438  [   34/   88]
per-ex loss: 0.699852  [   35/   88]
per-ex loss: 0.728879  [   36/   88]
per-ex loss: 0.687400  [   37/   88]
per-ex loss: 0.540760  [   38/   88]
per-ex loss: 0.790592  [   39/   88]
per-ex loss: 0.759948  [   40/   88]
per-ex loss: 0.553902  [   41/   88]
per-ex loss: 0.678809  [   42/   88]
per-ex loss: 0.776870  [   43/   88]
per-ex loss: 0.710163  [   44/   88]
per-ex loss: 0.577385  [   45/   88]
per-ex loss: 0.474653  [   46/   88]
per-ex loss: 0.487427  [   47/   88]
per-ex loss: 0.736125  [   48/   88]
per-ex loss: 0.530991  [   49/   88]
per-ex loss: 0.696291  [   50/   88]
per-ex loss: 0.534538  [   51/   88]
per-ex loss: 0.699683  [   52/   88]
per-ex loss: 0.523424  [   53/   88]
per-ex loss: 0.506131  [   54/   88]
per-ex loss: 0.650296  [   55/   88]
per-ex loss: 0.661329  [   56/   88]
per-ex loss: 0.594489  [   57/   88]
per-ex loss: 0.534857  [   58/   88]
per-ex loss: 0.706034  [   59/   88]
per-ex loss: 0.741441  [   60/   88]
per-ex loss: 0.554251  [   61/   88]
per-ex loss: 0.579818  [   62/   88]
per-ex loss: 0.468581  [   63/   88]
per-ex loss: 0.777618  [   64/   88]
per-ex loss: 0.714205  [   65/   88]
per-ex loss: 0.754051  [   66/   88]
per-ex loss: 0.565533  [   67/   88]
per-ex loss: 0.609906  [   68/   88]
per-ex loss: 0.624382  [   69/   88]
per-ex loss: 0.645594  [   70/   88]
per-ex loss: 0.793931  [   71/   88]
per-ex loss: 0.568884  [   72/   88]
per-ex loss: 0.512679  [   73/   88]
per-ex loss: 0.724294  [   74/   88]
per-ex loss: 0.591040  [   75/   88]
per-ex loss: 0.615576  [   76/   88]
per-ex loss: 0.589130  [   77/   88]
per-ex loss: 0.574179  [   78/   88]
per-ex loss: 0.731146  [   79/   88]
per-ex loss: 0.760427  [   80/   88]
per-ex loss: 0.587021  [   81/   88]
per-ex loss: 0.473070  [   82/   88]
per-ex loss: 0.509501  [   83/   88]
per-ex loss: 0.601163  [   84/   88]
per-ex loss: 0.628309  [   85/   88]
per-ex loss: 0.709926  [   86/   88]
per-ex loss: 0.788353  [   87/   88]
per-ex loss: 0.614504  [   88/   88]
Train Error: Avg loss: 0.62877700
validation Error: 
 Avg loss: 0.70326424 
 F1: 0.484757 
 Precision: 0.509356 
 Recall: 0.462425
 IoU: 0.319921

test Error: 
 Avg loss: 0.63652545 
 F1: 0.577389 
 Precision: 0.605757 
 Recall: 0.551559
 IoU: 0.405865

We have finished training iteration 77
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_75_.pth
per-ex loss: 0.586575  [    1/   88]
per-ex loss: 0.640467  [    2/   88]
per-ex loss: 0.693426  [    3/   88]
per-ex loss: 0.627929  [    4/   88]
per-ex loss: 0.749035  [    5/   88]
per-ex loss: 0.603227  [    6/   88]
per-ex loss: 0.670967  [    7/   88]
per-ex loss: 0.776616  [    8/   88]
per-ex loss: 0.526579  [    9/   88]
per-ex loss: 0.794280  [   10/   88]
per-ex loss: 0.734543  [   11/   88]
per-ex loss: 0.685246  [   12/   88]
per-ex loss: 0.532659  [   13/   88]
per-ex loss: 0.715279  [   14/   88]
per-ex loss: 0.542009  [   15/   88]
per-ex loss: 0.738207  [   16/   88]
per-ex loss: 0.526809  [   17/   88]
per-ex loss: 0.789655  [   18/   88]
per-ex loss: 0.630511  [   19/   88]
per-ex loss: 0.574175  [   20/   88]
per-ex loss: 0.624037  [   21/   88]
per-ex loss: 0.608776  [   22/   88]
per-ex loss: 0.555918  [   23/   88]
per-ex loss: 0.554923  [   24/   88]
per-ex loss: 0.540562  [   25/   88]
per-ex loss: 0.547876  [   26/   88]
per-ex loss: 0.533166  [   27/   88]
per-ex loss: 0.548118  [   28/   88]
per-ex loss: 0.654092  [   29/   88]
per-ex loss: 0.701568  [   30/   88]
per-ex loss: 0.779008  [   31/   88]
per-ex loss: 0.594518  [   32/   88]
per-ex loss: 0.683195  [   33/   88]
per-ex loss: 0.420012  [   34/   88]
per-ex loss: 0.683912  [   35/   88]
per-ex loss: 0.704014  [   36/   88]
per-ex loss: 0.560730  [   37/   88]
per-ex loss: 0.751530  [   38/   88]
per-ex loss: 0.535110  [   39/   88]
per-ex loss: 0.532025  [   40/   88]
per-ex loss: 0.587357  [   41/   88]
per-ex loss: 0.503290  [   42/   88]
per-ex loss: 0.704234  [   43/   88]
per-ex loss: 0.586645  [   44/   88]
per-ex loss: 0.818052  [   45/   88]
per-ex loss: 0.496786  [   46/   88]
per-ex loss: 0.487531  [   47/   88]
per-ex loss: 0.683424  [   48/   88]
per-ex loss: 0.639726  [   49/   88]
per-ex loss: 0.470809  [   50/   88]
per-ex loss: 0.778210  [   51/   88]
per-ex loss: 0.780201  [   52/   88]
per-ex loss: 0.556851  [   53/   88]
per-ex loss: 0.720649  [   54/   88]
per-ex loss: 0.612750  [   55/   88]
per-ex loss: 0.796702  [   56/   88]
per-ex loss: 0.734587  [   57/   88]
per-ex loss: 0.554816  [   58/   88]
per-ex loss: 0.704704  [   59/   88]
per-ex loss: 0.537023  [   60/   88]
per-ex loss: 0.665110  [   61/   88]
per-ex loss: 0.524522  [   62/   88]
per-ex loss: 0.679390  [   63/   88]
per-ex loss: 0.519544  [   64/   88]
per-ex loss: 0.523036  [   65/   88]
per-ex loss: 0.495763  [   66/   88]
per-ex loss: 0.743445  [   67/   88]
per-ex loss: 0.570426  [   68/   88]
per-ex loss: 0.716530  [   69/   88]
per-ex loss: 0.738923  [   70/   88]
per-ex loss: 0.521648  [   71/   88]
per-ex loss: 0.544032  [   72/   88]
per-ex loss: 0.605288  [   73/   88]
per-ex loss: 0.570384  [   74/   88]
per-ex loss: 0.547428  [   75/   88]
per-ex loss: 0.724795  [   76/   88]
per-ex loss: 0.547112  [   77/   88]
per-ex loss: 0.500308  [   78/   88]
per-ex loss: 0.731660  [   79/   88]
per-ex loss: 0.602179  [   80/   88]
per-ex loss: 0.572617  [   81/   88]
per-ex loss: 0.763097  [   82/   88]
per-ex loss: 0.496332  [   83/   88]
per-ex loss: 0.742367  [   84/   88]
per-ex loss: 0.743346  [   85/   88]
per-ex loss: 0.742205  [   86/   88]
per-ex loss: 0.563051  [   87/   88]
per-ex loss: 0.555301  [   88/   88]
Train Error: Avg loss: 0.62790305
validation Error: 
 Avg loss: 0.70229861 
 F1: 0.478867 
 Precision: 0.651295 
 Recall: 0.378627
 IoU: 0.314810

test Error: 
 Avg loss: 0.65167110 
 F1: 0.554267 
 Precision: 0.725872 
 Recall: 0.448286
 IoU: 0.383381

We have finished training iteration 78
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_76_.pth
per-ex loss: 0.624458  [    1/   88]
per-ex loss: 0.802059  [    2/   88]
per-ex loss: 0.696668  [    3/   88]
per-ex loss: 0.580267  [    4/   88]
per-ex loss: 0.558557  [    5/   88]
per-ex loss: 0.561802  [    6/   88]
per-ex loss: 0.734527  [    7/   88]
per-ex loss: 0.524851  [    8/   88]
per-ex loss: 0.575625  [    9/   88]
per-ex loss: 0.687396  [   10/   88]
per-ex loss: 0.570349  [   11/   88]
per-ex loss: 0.679268  [   12/   88]
per-ex loss: 0.768701  [   13/   88]
per-ex loss: 0.576064  [   14/   88]
per-ex loss: 0.478760  [   15/   88]
per-ex loss: 0.742214  [   16/   88]
per-ex loss: 0.683696  [   17/   88]
per-ex loss: 0.502663  [   18/   88]
per-ex loss: 0.572620  [   19/   88]
per-ex loss: 0.490443  [   20/   88]
per-ex loss: 0.703908  [   21/   88]
per-ex loss: 0.816144  [   22/   88]
per-ex loss: 0.539440  [   23/   88]
per-ex loss: 0.558113  [   24/   88]
per-ex loss: 0.565141  [   25/   88]
per-ex loss: 0.787323  [   26/   88]
per-ex loss: 0.678486  [   27/   88]
per-ex loss: 0.559189  [   28/   88]
per-ex loss: 0.596788  [   29/   88]
per-ex loss: 0.659121  [   30/   88]
per-ex loss: 0.525702  [   31/   88]
per-ex loss: 0.503573  [   32/   88]
per-ex loss: 0.624529  [   33/   88]
per-ex loss: 0.673687  [   34/   88]
per-ex loss: 0.542694  [   35/   88]
per-ex loss: 0.534389  [   36/   88]
per-ex loss: 0.508342  [   37/   88]
per-ex loss: 0.649993  [   38/   88]
per-ex loss: 0.528103  [   39/   88]
per-ex loss: 0.789016  [   40/   88]
per-ex loss: 0.750579  [   41/   88]
per-ex loss: 0.506211  [   42/   88]
per-ex loss: 0.493505  [   43/   88]
per-ex loss: 0.560050  [   44/   88]
per-ex loss: 0.561647  [   45/   88]
per-ex loss: 0.738761  [   46/   88]
per-ex loss: 0.500263  [   47/   88]
per-ex loss: 0.724114  [   48/   88]
per-ex loss: 0.539632  [   49/   88]
per-ex loss: 0.675753  [   50/   88]
per-ex loss: 0.801389  [   51/   88]
per-ex loss: 0.761830  [   52/   88]
per-ex loss: 0.658986  [   53/   88]
per-ex loss: 0.596417  [   54/   88]
per-ex loss: 0.555370  [   55/   88]
per-ex loss: 0.544192  [   56/   88]
per-ex loss: 0.560156  [   57/   88]
per-ex loss: 0.590951  [   58/   88]
per-ex loss: 0.607078  [   59/   88]
per-ex loss: 0.678579  [   60/   88]
per-ex loss: 0.743618  [   61/   88]
per-ex loss: 0.573047  [   62/   88]
per-ex loss: 0.728639  [   63/   88]
per-ex loss: 0.732540  [   64/   88]
per-ex loss: 0.582766  [   65/   88]
per-ex loss: 0.715901  [   66/   88]
per-ex loss: 0.780613  [   67/   88]
per-ex loss: 0.708478  [   68/   88]
per-ex loss: 0.495748  [   69/   88]
per-ex loss: 0.789139  [   70/   88]
per-ex loss: 0.711489  [   71/   88]
per-ex loss: 0.579853  [   72/   88]
per-ex loss: 0.536436  [   73/   88]
per-ex loss: 0.527283  [   74/   88]
per-ex loss: 0.427562  [   75/   88]
per-ex loss: 0.706389  [   76/   88]
per-ex loss: 0.618741  [   77/   88]
per-ex loss: 0.683519  [   78/   88]
per-ex loss: 0.715672  [   79/   88]
per-ex loss: 0.588650  [   80/   88]
per-ex loss: 0.516861  [   81/   88]
per-ex loss: 0.718197  [   82/   88]
per-ex loss: 0.606255  [   83/   88]
per-ex loss: 0.799188  [   84/   88]
per-ex loss: 0.757782  [   85/   88]
per-ex loss: 0.501216  [   86/   88]
per-ex loss: 0.547710  [   87/   88]
per-ex loss: 0.757382  [   88/   88]
Train Error: Avg loss: 0.62853192
validation Error: 
 Avg loss: 0.70463038 
 F1: 0.482264 
 Precision: 0.536995 
 Recall: 0.437658
 IoU: 0.317752

test Error: 
 Avg loss: 0.63913444 
 F1: 0.573615 
 Precision: 0.634333 
 Recall: 0.523506
 IoU: 0.402146

We have finished training iteration 79
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_77_.pth
per-ex loss: 0.579740  [    1/   88]
per-ex loss: 0.764111  [    2/   88]
per-ex loss: 0.728354  [    3/   88]
per-ex loss: 0.655759  [    4/   88]
per-ex loss: 0.626606  [    5/   88]
per-ex loss: 0.521995  [    6/   88]
per-ex loss: 0.528804  [    7/   88]
per-ex loss: 0.733521  [    8/   88]
per-ex loss: 0.467856  [    9/   88]
per-ex loss: 0.548501  [   10/   88]
per-ex loss: 0.623557  [   11/   88]
per-ex loss: 0.658352  [   12/   88]
per-ex loss: 0.757253  [   13/   88]
per-ex loss: 0.536489  [   14/   88]
per-ex loss: 0.711431  [   15/   88]
per-ex loss: 0.697482  [   16/   88]
per-ex loss: 0.707793  [   17/   88]
per-ex loss: 0.782741  [   18/   88]
per-ex loss: 0.525079  [   19/   88]
per-ex loss: 0.740215  [   20/   88]
per-ex loss: 0.470100  [   21/   88]
per-ex loss: 0.715564  [   22/   88]
per-ex loss: 0.786864  [   23/   88]
per-ex loss: 0.700213  [   24/   88]
per-ex loss: 0.514991  [   25/   88]
per-ex loss: 0.578197  [   26/   88]
per-ex loss: 0.734163  [   27/   88]
per-ex loss: 0.468695  [   28/   88]
per-ex loss: 0.615463  [   29/   88]
per-ex loss: 0.733681  [   30/   88]
per-ex loss: 0.624771  [   31/   88]
per-ex loss: 0.562092  [   32/   88]
per-ex loss: 0.544740  [   33/   88]
per-ex loss: 0.522002  [   34/   88]
per-ex loss: 0.558824  [   35/   88]
per-ex loss: 0.754081  [   36/   88]
per-ex loss: 0.578770  [   37/   88]
per-ex loss: 0.588150  [   38/   88]
per-ex loss: 0.532740  [   39/   88]
per-ex loss: 0.491727  [   40/   88]
per-ex loss: 0.677267  [   41/   88]
per-ex loss: 0.682069  [   42/   88]
per-ex loss: 0.699996  [   43/   88]
per-ex loss: 0.665466  [   44/   88]
per-ex loss: 0.541169  [   45/   88]
per-ex loss: 0.564358  [   46/   88]
per-ex loss: 0.553910  [   47/   88]
per-ex loss: 0.681315  [   48/   88]
per-ex loss: 0.493666  [   49/   88]
per-ex loss: 0.669975  [   50/   88]
per-ex loss: 0.582719  [   51/   88]
per-ex loss: 0.550645  [   52/   88]
per-ex loss: 0.741206  [   53/   88]
per-ex loss: 0.797327  [   54/   88]
per-ex loss: 0.506643  [   55/   88]
per-ex loss: 0.448029  [   56/   88]
per-ex loss: 0.550586  [   57/   88]
per-ex loss: 0.508998  [   58/   88]
per-ex loss: 0.645420  [   59/   88]
per-ex loss: 0.593840  [   60/   88]
per-ex loss: 0.611108  [   61/   88]
per-ex loss: 0.691592  [   62/   88]
per-ex loss: 0.732367  [   63/   88]
per-ex loss: 0.709666  [   64/   88]
per-ex loss: 0.683175  [   65/   88]
per-ex loss: 0.797114  [   66/   88]
per-ex loss: 0.704071  [   67/   88]
per-ex loss: 0.732198  [   68/   88]
per-ex loss: 0.797664  [   69/   88]
per-ex loss: 0.520234  [   70/   88]
per-ex loss: 0.518435  [   71/   88]
per-ex loss: 0.527357  [   72/   88]
per-ex loss: 0.726925  [   73/   88]
per-ex loss: 0.740295  [   74/   88]
per-ex loss: 0.769596  [   75/   88]
per-ex loss: 0.759484  [   76/   88]
per-ex loss: 0.517732  [   77/   88]
per-ex loss: 0.638844  [   78/   88]
per-ex loss: 0.523925  [   79/   88]
per-ex loss: 0.520269  [   80/   88]
per-ex loss: 0.618132  [   81/   88]
per-ex loss: 0.519880  [   82/   88]
per-ex loss: 0.687294  [   83/   88]
per-ex loss: 0.524336  [   84/   88]
per-ex loss: 0.573246  [   85/   88]
per-ex loss: 0.574648  [   86/   88]
per-ex loss: 0.581037  [   87/   88]
per-ex loss: 0.733595  [   88/   88]
Train Error: Avg loss: 0.62682150
validation Error: 
 Avg loss: 0.75480939 
 F1: 0.415727 
 Precision: 0.475482 
 Recall: 0.369314
 IoU: 0.262409

test Error: 
 Avg loss: 0.66357200 
 F1: 0.536866 
 Precision: 0.669756 
 Recall: 0.447980
 IoU: 0.366929

We have finished training iteration 80
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_78_.pth
per-ex loss: 0.735187  [    1/   88]
per-ex loss: 0.703808  [    2/   88]
per-ex loss: 0.576762  [    3/   88]
per-ex loss: 0.615613  [    4/   88]
per-ex loss: 0.474749  [    5/   88]
per-ex loss: 0.722066  [    6/   88]
per-ex loss: 0.800538  [    7/   88]
per-ex loss: 0.547381  [    8/   88]
per-ex loss: 0.573740  [    9/   88]
per-ex loss: 0.727779  [   10/   88]
per-ex loss: 0.747941  [   11/   88]
per-ex loss: 0.648352  [   12/   88]
per-ex loss: 0.792757  [   13/   88]
per-ex loss: 0.532523  [   14/   88]
per-ex loss: 0.568989  [   15/   88]
per-ex loss: 0.556544  [   16/   88]
per-ex loss: 0.498672  [   17/   88]
per-ex loss: 0.730294  [   18/   88]
per-ex loss: 0.534728  [   19/   88]
per-ex loss: 0.697637  [   20/   88]
per-ex loss: 0.751508  [   21/   88]
per-ex loss: 0.583865  [   22/   88]
per-ex loss: 0.567854  [   23/   88]
per-ex loss: 0.527101  [   24/   88]
per-ex loss: 0.568983  [   25/   88]
per-ex loss: 0.760358  [   26/   88]
per-ex loss: 0.796200  [   27/   88]
per-ex loss: 0.505062  [   28/   88]
per-ex loss: 0.493542  [   29/   88]
per-ex loss: 0.761644  [   30/   88]
per-ex loss: 0.544331  [   31/   88]
per-ex loss: 0.591433  [   32/   88]
per-ex loss: 0.485696  [   33/   88]
per-ex loss: 0.556049  [   34/   88]
per-ex loss: 0.509077  [   35/   88]
per-ex loss: 0.679150  [   36/   88]
per-ex loss: 0.708740  [   37/   88]
per-ex loss: 0.814733  [   38/   88]
per-ex loss: 0.514376  [   39/   88]
per-ex loss: 0.791194  [   40/   88]
per-ex loss: 0.565782  [   41/   88]
per-ex loss: 0.671363  [   42/   88]
per-ex loss: 0.589684  [   43/   88]
per-ex loss: 0.698720  [   44/   88]
per-ex loss: 0.698487  [   45/   88]
per-ex loss: 0.699768  [   46/   88]
per-ex loss: 0.526957  [   47/   88]
per-ex loss: 0.584869  [   48/   88]
per-ex loss: 0.531334  [   49/   88]
per-ex loss: 0.582524  [   50/   88]
per-ex loss: 0.526266  [   51/   88]
per-ex loss: 0.678067  [   52/   88]
per-ex loss: 0.701615  [   53/   88]
per-ex loss: 0.565275  [   54/   88]
per-ex loss: 0.739825  [   55/   88]
per-ex loss: 0.756100  [   56/   88]
per-ex loss: 0.484609  [   57/   88]
per-ex loss: 0.723972  [   58/   88]
per-ex loss: 0.630586  [   59/   88]
per-ex loss: 0.523773  [   60/   88]
per-ex loss: 0.598841  [   61/   88]
per-ex loss: 0.611446  [   62/   88]
per-ex loss: 0.512596  [   63/   88]
per-ex loss: 0.789690  [   64/   88]
per-ex loss: 0.403046  [   65/   88]
per-ex loss: 0.675876  [   66/   88]
per-ex loss: 0.626022  [   67/   88]
per-ex loss: 0.711685  [   68/   88]
per-ex loss: 0.745933  [   69/   88]
per-ex loss: 0.682440  [   70/   88]
per-ex loss: 0.496475  [   71/   88]
per-ex loss: 0.765810  [   72/   88]
per-ex loss: 0.618519  [   73/   88]
per-ex loss: 0.633688  [   74/   88]
per-ex loss: 0.573885  [   75/   88]
per-ex loss: 0.646463  [   76/   88]
per-ex loss: 0.725465  [   77/   88]
per-ex loss: 0.690521  [   78/   88]
per-ex loss: 0.526618  [   79/   88]
per-ex loss: 0.692902  [   80/   88]
per-ex loss: 0.544737  [   81/   88]
per-ex loss: 0.747511  [   82/   88]
per-ex loss: 0.597252  [   83/   88]
per-ex loss: 0.494987  [   84/   88]
per-ex loss: 0.548922  [   85/   88]
per-ex loss: 0.771470  [   86/   88]
per-ex loss: 0.560338  [   87/   88]
per-ex loss: 0.714682  [   88/   88]
Train Error: Avg loss: 0.63050406
validation Error: 
 Avg loss: 0.71221463 
 F1: 0.467265 
 Precision: 0.507247 
 Recall: 0.433126
 IoU: 0.304857

test Error: 
 Avg loss: 0.64289235 
 F1: 0.565104 
 Precision: 0.624387 
 Recall: 0.516102
 IoU: 0.393829

We have finished training iteration 81
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_79_.pth
per-ex loss: 0.703469  [    1/   88]
per-ex loss: 0.602724  [    2/   88]
per-ex loss: 0.791330  [    3/   88]
per-ex loss: 0.781860  [    4/   88]
per-ex loss: 0.545084  [    5/   88]
per-ex loss: 0.458404  [    6/   88]
per-ex loss: 0.697226  [    7/   88]
per-ex loss: 0.566163  [    8/   88]
per-ex loss: 0.582134  [    9/   88]
per-ex loss: 0.566275  [   10/   88]
per-ex loss: 0.551191  [   11/   88]
per-ex loss: 0.582193  [   12/   88]
per-ex loss: 0.513836  [   13/   88]
per-ex loss: 0.660741  [   14/   88]
per-ex loss: 0.474214  [   15/   88]
per-ex loss: 0.668242  [   16/   88]
per-ex loss: 0.749465  [   17/   88]
per-ex loss: 0.529968  [   18/   88]
per-ex loss: 0.731020  [   19/   88]
per-ex loss: 0.780595  [   20/   88]
per-ex loss: 0.711021  [   21/   88]
per-ex loss: 0.693573  [   22/   88]
per-ex loss: 0.707442  [   23/   88]
per-ex loss: 0.710264  [   24/   88]
per-ex loss: 0.683740  [   25/   88]
per-ex loss: 0.856696  [   26/   88]
per-ex loss: 0.719110  [   27/   88]
per-ex loss: 0.787028  [   28/   88]
per-ex loss: 0.588158  [   29/   88]
per-ex loss: 0.562720  [   30/   88]
per-ex loss: 0.736854  [   31/   88]
per-ex loss: 0.620183  [   32/   88]
per-ex loss: 0.535853  [   33/   88]
per-ex loss: 0.660944  [   34/   88]
per-ex loss: 0.521562  [   35/   88]
per-ex loss: 0.627969  [   36/   88]
per-ex loss: 0.654516  [   37/   88]
per-ex loss: 0.513635  [   38/   88]
per-ex loss: 0.591047  [   39/   88]
per-ex loss: 0.683505  [   40/   88]
per-ex loss: 0.557097  [   41/   88]
per-ex loss: 0.572376  [   42/   88]
per-ex loss: 0.513275  [   43/   88]
per-ex loss: 0.683029  [   44/   88]
per-ex loss: 0.522800  [   45/   88]
per-ex loss: 0.493875  [   46/   88]
per-ex loss: 0.493689  [   47/   88]
per-ex loss: 0.584733  [   48/   88]
per-ex loss: 0.789227  [   49/   88]
per-ex loss: 0.650907  [   50/   88]
per-ex loss: 0.722697  [   51/   88]
per-ex loss: 0.609304  [   52/   88]
per-ex loss: 0.677685  [   53/   88]
per-ex loss: 0.761655  [   54/   88]
per-ex loss: 0.763049  [   55/   88]
per-ex loss: 0.598339  [   56/   88]
per-ex loss: 0.592252  [   57/   88]
per-ex loss: 0.570191  [   58/   88]
per-ex loss: 0.599421  [   59/   88]
per-ex loss: 0.591258  [   60/   88]
per-ex loss: 0.512469  [   61/   88]
per-ex loss: 0.546618  [   62/   88]
per-ex loss: 0.757068  [   63/   88]
per-ex loss: 0.710948  [   64/   88]
per-ex loss: 0.416144  [   65/   88]
per-ex loss: 0.739237  [   66/   88]
per-ex loss: 0.738397  [   67/   88]
per-ex loss: 0.495043  [   68/   88]
per-ex loss: 0.506927  [   69/   88]
per-ex loss: 0.513306  [   70/   88]
per-ex loss: 0.570842  [   71/   88]
per-ex loss: 0.551476  [   72/   88]
per-ex loss: 0.681966  [   73/   88]
per-ex loss: 0.702772  [   74/   88]
per-ex loss: 0.508278  [   75/   88]
per-ex loss: 0.748920  [   76/   88]
per-ex loss: 0.705158  [   77/   88]
per-ex loss: 0.523239  [   78/   88]
per-ex loss: 0.678201  [   79/   88]
per-ex loss: 0.595141  [   80/   88]
per-ex loss: 0.792683  [   81/   88]
per-ex loss: 0.744511  [   82/   88]
per-ex loss: 0.503627  [   83/   88]
per-ex loss: 0.561134  [   84/   88]
per-ex loss: 0.556154  [   85/   88]
per-ex loss: 0.537511  [   86/   88]
per-ex loss: 0.599925  [   87/   88]
per-ex loss: 0.760154  [   88/   88]
Train Error: Avg loss: 0.62848482
validation Error: 
 Avg loss: 0.70560194 
 F1: 0.481180 
 Precision: 0.587237 
 Recall: 0.407571
 IoU: 0.316812

test Error: 
 Avg loss: 0.64027280 
 F1: 0.568365 
 Precision: 0.694135 
 Recall: 0.481179
 IoU: 0.397004

We have finished training iteration 82
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_80_.pth
per-ex loss: 0.747974  [    1/   88]
per-ex loss: 0.547370  [    2/   88]
per-ex loss: 0.710892  [    3/   88]
per-ex loss: 0.495024  [    4/   88]
per-ex loss: 0.709019  [    5/   88]
per-ex loss: 0.475581  [    6/   88]
per-ex loss: 0.626736  [    7/   88]
per-ex loss: 0.554132  [    8/   88]
per-ex loss: 0.519909  [    9/   88]
per-ex loss: 0.517365  [   10/   88]
per-ex loss: 0.697757  [   11/   88]
per-ex loss: 0.498868  [   12/   88]
per-ex loss: 0.580104  [   13/   88]
per-ex loss: 0.546045  [   14/   88]
per-ex loss: 0.810779  [   15/   88]
per-ex loss: 0.520516  [   16/   88]
per-ex loss: 0.542221  [   17/   88]
per-ex loss: 0.537410  [   18/   88]
per-ex loss: 0.623575  [   19/   88]
per-ex loss: 0.630587  [   20/   88]
per-ex loss: 0.515336  [   21/   88]
per-ex loss: 0.677009  [   22/   88]
per-ex loss: 0.740421  [   23/   88]
per-ex loss: 0.675136  [   24/   88]
per-ex loss: 0.499091  [   25/   88]
per-ex loss: 0.672034  [   26/   88]
per-ex loss: 0.605052  [   27/   88]
per-ex loss: 0.597554  [   28/   88]
per-ex loss: 0.515554  [   29/   88]
per-ex loss: 0.584636  [   30/   88]
per-ex loss: 0.786003  [   31/   88]
per-ex loss: 0.792026  [   32/   88]
per-ex loss: 0.649413  [   33/   88]
per-ex loss: 0.590686  [   34/   88]
per-ex loss: 0.665625  [   35/   88]
per-ex loss: 0.573261  [   36/   88]
per-ex loss: 0.689791  [   37/   88]
per-ex loss: 0.662554  [   38/   88]
per-ex loss: 0.750924  [   39/   88]
per-ex loss: 0.704320  [   40/   88]
per-ex loss: 0.556201  [   41/   88]
per-ex loss: 0.511214  [   42/   88]
per-ex loss: 0.679775  [   43/   88]
per-ex loss: 0.701541  [   44/   88]
per-ex loss: 0.483660  [   45/   88]
per-ex loss: 0.543085  [   46/   88]
per-ex loss: 0.702857  [   47/   88]
per-ex loss: 0.737172  [   48/   88]
per-ex loss: 0.557607  [   49/   88]
per-ex loss: 0.778071  [   50/   88]
per-ex loss: 0.461232  [   51/   88]
per-ex loss: 0.580047  [   52/   88]
per-ex loss: 0.726413  [   53/   88]
per-ex loss: 0.684735  [   54/   88]
per-ex loss: 0.733706  [   55/   88]
per-ex loss: 0.771485  [   56/   88]
per-ex loss: 0.728074  [   57/   88]
per-ex loss: 0.702772  [   58/   88]
per-ex loss: 0.539648  [   59/   88]
per-ex loss: 0.530752  [   60/   88]
per-ex loss: 0.762171  [   61/   88]
per-ex loss: 0.555150  [   62/   88]
per-ex loss: 0.712553  [   63/   88]
per-ex loss: 0.732137  [   64/   88]
per-ex loss: 0.659339  [   65/   88]
per-ex loss: 0.520369  [   66/   88]
per-ex loss: 0.780657  [   67/   88]
per-ex loss: 0.580596  [   68/   88]
per-ex loss: 0.545047  [   69/   88]
per-ex loss: 0.569249  [   70/   88]
per-ex loss: 0.485453  [   71/   88]
per-ex loss: 0.698025  [   72/   88]
per-ex loss: 0.587600  [   73/   88]
per-ex loss: 0.561830  [   74/   88]
per-ex loss: 0.774074  [   75/   88]
per-ex loss: 0.506132  [   76/   88]
per-ex loss: 0.419120  [   77/   88]
per-ex loss: 0.499936  [   78/   88]
per-ex loss: 0.732501  [   79/   88]
per-ex loss: 0.784118  [   80/   88]
per-ex loss: 0.659919  [   81/   88]
per-ex loss: 0.741673  [   82/   88]
per-ex loss: 0.604594  [   83/   88]
per-ex loss: 0.594208  [   84/   88]
per-ex loss: 0.548467  [   85/   88]
per-ex loss: 0.548824  [   86/   88]
per-ex loss: 0.567857  [   87/   88]
per-ex loss: 0.562072  [   88/   88]
Train Error: Avg loss: 0.62320461
validation Error: 
 Avg loss: 0.69559597 
 F1: 0.491668 
 Precision: 0.525543 
 Recall: 0.461896
 IoU: 0.325968

test Error: 
 Avg loss: 0.63999693 
 F1: 0.571468 
 Precision: 0.594510 
 Recall: 0.550146
 IoU: 0.400039

We have finished training iteration 83
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_81_.pth
per-ex loss: 0.693934  [    1/   88]
per-ex loss: 0.527655  [    2/   88]
per-ex loss: 0.570974  [    3/   88]
per-ex loss: 0.701285  [    4/   88]
per-ex loss: 0.686125  [    5/   88]
per-ex loss: 0.783983  [    6/   88]
per-ex loss: 0.612980  [    7/   88]
per-ex loss: 0.706344  [    8/   88]
per-ex loss: 0.492255  [    9/   88]
per-ex loss: 0.479193  [   10/   88]
per-ex loss: 0.562782  [   11/   88]
per-ex loss: 0.786454  [   12/   88]
per-ex loss: 0.549930  [   13/   88]
per-ex loss: 0.655248  [   14/   88]
per-ex loss: 0.555828  [   15/   88]
per-ex loss: 0.499750  [   16/   88]
per-ex loss: 0.507587  [   17/   88]
per-ex loss: 0.551656  [   18/   88]
per-ex loss: 0.744185  [   19/   88]
per-ex loss: 0.511653  [   20/   88]
per-ex loss: 0.769641  [   21/   88]
per-ex loss: 0.489920  [   22/   88]
per-ex loss: 0.759546  [   23/   88]
per-ex loss: 0.530593  [   24/   88]
per-ex loss: 0.682187  [   25/   88]
per-ex loss: 0.578389  [   26/   88]
per-ex loss: 0.731528  [   27/   88]
per-ex loss: 0.637030  [   28/   88]
per-ex loss: 0.733973  [   29/   88]
per-ex loss: 0.676945  [   30/   88]
per-ex loss: 0.775755  [   31/   88]
per-ex loss: 0.569830  [   32/   88]
per-ex loss: 0.624153  [   33/   88]
per-ex loss: 0.601418  [   34/   88]
per-ex loss: 0.464746  [   35/   88]
per-ex loss: 0.651897  [   36/   88]
per-ex loss: 0.460043  [   37/   88]
per-ex loss: 0.721413  [   38/   88]
per-ex loss: 0.703724  [   39/   88]
per-ex loss: 0.710941  [   40/   88]
per-ex loss: 0.704591  [   41/   88]
per-ex loss: 0.797896  [   42/   88]
per-ex loss: 0.648059  [   43/   88]
per-ex loss: 0.485131  [   44/   88]
per-ex loss: 0.490254  [   45/   88]
per-ex loss: 0.588870  [   46/   88]
per-ex loss: 0.574596  [   47/   88]
per-ex loss: 0.518220  [   48/   88]
per-ex loss: 0.524991  [   49/   88]
per-ex loss: 0.667660  [   50/   88]
per-ex loss: 0.552822  [   51/   88]
per-ex loss: 0.579183  [   52/   88]
per-ex loss: 0.713351  [   53/   88]
per-ex loss: 0.551366  [   54/   88]
per-ex loss: 0.552225  [   55/   88]
per-ex loss: 0.576240  [   56/   88]
per-ex loss: 0.530220  [   57/   88]
per-ex loss: 0.537197  [   58/   88]
per-ex loss: 0.530714  [   59/   88]
per-ex loss: 0.781823  [   60/   88]
per-ex loss: 0.520607  [   61/   88]
per-ex loss: 0.686466  [   62/   88]
per-ex loss: 0.713250  [   63/   88]
per-ex loss: 0.697902  [   64/   88]
per-ex loss: 0.670749  [   65/   88]
per-ex loss: 0.492134  [   66/   88]
per-ex loss: 0.547427  [   67/   88]
per-ex loss: 0.536538  [   68/   88]
per-ex loss: 0.607815  [   69/   88]
per-ex loss: 0.749550  [   70/   88]
per-ex loss: 0.692646  [   71/   88]
per-ex loss: 0.798270  [   72/   88]
per-ex loss: 0.759053  [   73/   88]
per-ex loss: 0.546930  [   74/   88]
per-ex loss: 0.772930  [   75/   88]
per-ex loss: 0.563026  [   76/   88]
per-ex loss: 0.564339  [   77/   88]
per-ex loss: 0.722600  [   78/   88]
per-ex loss: 0.887861  [   79/   88]
per-ex loss: 0.658618  [   80/   88]
per-ex loss: 0.506708  [   81/   88]
per-ex loss: 0.751063  [   82/   88]
per-ex loss: 0.773677  [   83/   88]
per-ex loss: 0.503457  [   84/   88]
per-ex loss: 0.567421  [   85/   88]
per-ex loss: 0.626068  [   86/   88]
per-ex loss: 0.499260  [   87/   88]
per-ex loss: 0.585438  [   88/   88]
Train Error: Avg loss: 0.62455322
validation Error: 
 Avg loss: 0.69943287 
 F1: 0.486832 
 Precision: 0.540863 
 Recall: 0.442617
 IoU: 0.321731

test Error: 
 Avg loss: 0.63137387 
 F1: 0.578843 
 Precision: 0.644542 
 Recall: 0.525299
 IoU: 0.407304

We have finished training iteration 84
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_82_.pth
per-ex loss: 0.691094  [    1/   88]
per-ex loss: 0.771519  [    2/   88]
per-ex loss: 0.577460  [    3/   88]
per-ex loss: 0.540576  [    4/   88]
per-ex loss: 0.502372  [    5/   88]
per-ex loss: 0.548190  [    6/   88]
per-ex loss: 0.465587  [    7/   88]
per-ex loss: 0.739175  [    8/   88]
per-ex loss: 0.555846  [    9/   88]
per-ex loss: 0.539644  [   10/   88]
per-ex loss: 0.794564  [   11/   88]
per-ex loss: 0.559467  [   12/   88]
per-ex loss: 0.709389  [   13/   88]
per-ex loss: 0.758756  [   14/   88]
per-ex loss: 0.736936  [   15/   88]
per-ex loss: 0.554155  [   16/   88]
per-ex loss: 0.510270  [   17/   88]
per-ex loss: 0.786224  [   18/   88]
per-ex loss: 0.794372  [   19/   88]
per-ex loss: 0.461808  [   20/   88]
per-ex loss: 0.685869  [   21/   88]
per-ex loss: 0.530227  [   22/   88]
per-ex loss: 0.533195  [   23/   88]
per-ex loss: 0.694399  [   24/   88]
per-ex loss: 0.573396  [   25/   88]
per-ex loss: 0.568635  [   26/   88]
per-ex loss: 0.508734  [   27/   88]
per-ex loss: 0.495703  [   28/   88]
per-ex loss: 0.775644  [   29/   88]
per-ex loss: 0.637484  [   30/   88]
per-ex loss: 0.744476  [   31/   88]
per-ex loss: 0.560769  [   32/   88]
per-ex loss: 0.670833  [   33/   88]
per-ex loss: 0.703594  [   34/   88]
per-ex loss: 0.608607  [   35/   88]
per-ex loss: 0.602950  [   36/   88]
per-ex loss: 0.735946  [   37/   88]
per-ex loss: 0.727313  [   38/   88]
per-ex loss: 0.523142  [   39/   88]
per-ex loss: 0.535740  [   40/   88]
per-ex loss: 0.551241  [   41/   88]
per-ex loss: 0.706946  [   42/   88]
per-ex loss: 0.510275  [   43/   88]
per-ex loss: 0.679361  [   44/   88]
per-ex loss: 0.586151  [   45/   88]
per-ex loss: 0.713484  [   46/   88]
per-ex loss: 0.693609  [   47/   88]
per-ex loss: 0.647603  [   48/   88]
per-ex loss: 0.389083  [   49/   88]
per-ex loss: 0.505869  [   50/   88]
per-ex loss: 0.619785  [   51/   88]
per-ex loss: 0.544432  [   52/   88]
per-ex loss: 0.738293  [   53/   88]
per-ex loss: 0.532245  [   54/   88]
per-ex loss: 0.795046  [   55/   88]
per-ex loss: 0.611743  [   56/   88]
per-ex loss: 0.737457  [   57/   88]
per-ex loss: 0.557704  [   58/   88]
per-ex loss: 0.721997  [   59/   88]
per-ex loss: 0.734733  [   60/   88]
per-ex loss: 0.550398  [   61/   88]
per-ex loss: 0.782934  [   62/   88]
per-ex loss: 0.693426  [   63/   88]
per-ex loss: 0.504896  [   64/   88]
per-ex loss: 0.720037  [   65/   88]
per-ex loss: 0.555634  [   66/   88]
per-ex loss: 0.565396  [   67/   88]
per-ex loss: 0.662723  [   68/   88]
per-ex loss: 0.727864  [   69/   88]
per-ex loss: 0.623788  [   70/   88]
per-ex loss: 0.503581  [   71/   88]
per-ex loss: 0.514274  [   72/   88]
per-ex loss: 0.585485  [   73/   88]
per-ex loss: 0.768320  [   74/   88]
per-ex loss: 0.668991  [   75/   88]
per-ex loss: 0.663300  [   76/   88]
per-ex loss: 0.504645  [   77/   88]
per-ex loss: 0.634634  [   78/   88]
per-ex loss: 0.541271  [   79/   88]
per-ex loss: 0.527107  [   80/   88]
per-ex loss: 0.670385  [   81/   88]
per-ex loss: 0.554910  [   82/   88]
per-ex loss: 0.668443  [   83/   88]
per-ex loss: 0.595279  [   84/   88]
per-ex loss: 0.731831  [   85/   88]
per-ex loss: 0.529249  [   86/   88]
per-ex loss: 0.566175  [   87/   88]
per-ex loss: 0.482013  [   88/   88]
Train Error: Avg loss: 0.62145576
validation Error: 
 Avg loss: 0.69819829 
 F1: 0.488880 
 Precision: 0.590932 
 Recall: 0.416885
 IoU: 0.323521

test Error: 
 Avg loss: 0.64334671 
 F1: 0.565231 
 Precision: 0.681504 
 Recall: 0.482850
 IoU: 0.393952

We have finished training iteration 85
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_83_.pth
per-ex loss: 0.748049  [    1/   88]
per-ex loss: 0.534306  [    2/   88]
per-ex loss: 0.483781  [    3/   88]
per-ex loss: 0.521293  [    4/   88]
per-ex loss: 0.805185  [    5/   88]
per-ex loss: 0.564610  [    6/   88]
per-ex loss: 0.562077  [    7/   88]
per-ex loss: 0.564853  [    8/   88]
per-ex loss: 0.604114  [    9/   88]
per-ex loss: 0.646654  [   10/   88]
per-ex loss: 0.563909  [   11/   88]
per-ex loss: 0.514098  [   12/   88]
per-ex loss: 0.715073  [   13/   88]
per-ex loss: 0.557296  [   14/   88]
per-ex loss: 0.519728  [   15/   88]
per-ex loss: 0.737322  [   16/   88]
per-ex loss: 0.555817  [   17/   88]
per-ex loss: 0.564909  [   18/   88]
per-ex loss: 0.732585  [   19/   88]
per-ex loss: 0.544168  [   20/   88]
per-ex loss: 0.780177  [   21/   88]
per-ex loss: 0.654721  [   22/   88]
per-ex loss: 0.508595  [   23/   88]
per-ex loss: 0.645808  [   24/   88]
per-ex loss: 0.564761  [   25/   88]
per-ex loss: 0.790541  [   26/   88]
per-ex loss: 0.525581  [   27/   88]
per-ex loss: 0.705489  [   28/   88]
per-ex loss: 0.578164  [   29/   88]
per-ex loss: 0.597141  [   30/   88]
per-ex loss: 0.702528  [   31/   88]
per-ex loss: 0.689036  [   32/   88]
per-ex loss: 0.659500  [   33/   88]
per-ex loss: 0.672398  [   34/   88]
per-ex loss: 0.720249  [   35/   88]
per-ex loss: 0.584652  [   36/   88]
per-ex loss: 0.595857  [   37/   88]
per-ex loss: 0.841367  [   38/   88]
per-ex loss: 0.754023  [   39/   88]
per-ex loss: 0.704810  [   40/   88]
per-ex loss: 0.758858  [   41/   88]
per-ex loss: 0.583374  [   42/   88]
per-ex loss: 0.521202  [   43/   88]
per-ex loss: 0.758711  [   44/   88]
per-ex loss: 0.690339  [   45/   88]
per-ex loss: 0.742563  [   46/   88]
per-ex loss: 0.552558  [   47/   88]
per-ex loss: 0.616707  [   48/   88]
per-ex loss: 0.525340  [   49/   88]
per-ex loss: 0.524889  [   50/   88]
per-ex loss: 0.695102  [   51/   88]
per-ex loss: 0.706591  [   52/   88]
per-ex loss: 0.484517  [   53/   88]
per-ex loss: 0.506633  [   54/   88]
per-ex loss: 0.765791  [   55/   88]
per-ex loss: 0.713120  [   56/   88]
per-ex loss: 0.791133  [   57/   88]
per-ex loss: 0.620256  [   58/   88]
per-ex loss: 0.723624  [   59/   88]
per-ex loss: 0.547104  [   60/   88]
per-ex loss: 0.461287  [   61/   88]
per-ex loss: 0.671787  [   62/   88]
per-ex loss: 0.468147  [   63/   88]
per-ex loss: 0.493650  [   64/   88]
per-ex loss: 0.536133  [   65/   88]
per-ex loss: 0.526488  [   66/   88]
per-ex loss: 0.647638  [   67/   88]
per-ex loss: 0.737276  [   68/   88]
per-ex loss: 0.545883  [   69/   88]
per-ex loss: 0.701660  [   70/   88]
per-ex loss: 0.560449  [   71/   88]
per-ex loss: 0.696214  [   72/   88]
per-ex loss: 0.654643  [   73/   88]
per-ex loss: 0.665797  [   74/   88]
per-ex loss: 0.549013  [   75/   88]
per-ex loss: 0.514283  [   76/   88]
per-ex loss: 0.405377  [   77/   88]
per-ex loss: 0.510430  [   78/   88]
per-ex loss: 0.646521  [   79/   88]
per-ex loss: 0.713529  [   80/   88]
per-ex loss: 0.625378  [   81/   88]
per-ex loss: 0.606263  [   82/   88]
per-ex loss: 0.534867  [   83/   88]
per-ex loss: 0.775318  [   84/   88]
per-ex loss: 0.559409  [   85/   88]
per-ex loss: 0.751688  [   86/   88]
per-ex loss: 0.585409  [   87/   88]
per-ex loss: 0.466724  [   88/   88]
Train Error: Avg loss: 0.62221482
validation Error: 
 Avg loss: 0.70436882 
 F1: 0.476381 
 Precision: 0.511008 
 Recall: 0.446149
 IoU: 0.312664

test Error: 
 Avg loss: 0.64195365 
 F1: 0.570145 
 Precision: 0.614806 
 Recall: 0.531534
 IoU: 0.398744

We have finished training iteration 86
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_84_.pth
per-ex loss: 0.561269  [    1/   88]
per-ex loss: 0.494223  [    2/   88]
per-ex loss: 0.547980  [    3/   88]
per-ex loss: 0.380543  [    4/   88]
per-ex loss: 0.604019  [    5/   88]
per-ex loss: 0.627573  [    6/   88]
per-ex loss: 0.621736  [    7/   88]
per-ex loss: 0.568815  [    8/   88]
per-ex loss: 0.753897  [    9/   88]
per-ex loss: 0.589735  [   10/   88]
per-ex loss: 0.453150  [   11/   88]
per-ex loss: 0.519603  [   12/   88]
per-ex loss: 0.515902  [   13/   88]
per-ex loss: 0.482314  [   14/   88]
per-ex loss: 0.529593  [   15/   88]
per-ex loss: 0.697586  [   16/   88]
per-ex loss: 0.575312  [   17/   88]
per-ex loss: 0.571528  [   18/   88]
per-ex loss: 0.563963  [   19/   88]
per-ex loss: 0.472237  [   20/   88]
per-ex loss: 0.720312  [   21/   88]
per-ex loss: 0.558882  [   22/   88]
per-ex loss: 0.651060  [   23/   88]
per-ex loss: 0.668961  [   24/   88]
per-ex loss: 0.546005  [   25/   88]
per-ex loss: 0.725737  [   26/   88]
per-ex loss: 0.719800  [   27/   88]
per-ex loss: 0.778158  [   28/   88]
per-ex loss: 0.610266  [   29/   88]
per-ex loss: 0.536605  [   30/   88]
per-ex loss: 0.540686  [   31/   88]
per-ex loss: 0.514240  [   32/   88]
per-ex loss: 0.522927  [   33/   88]
per-ex loss: 0.702635  [   34/   88]
per-ex loss: 0.493185  [   35/   88]
per-ex loss: 0.640802  [   36/   88]
per-ex loss: 0.647751  [   37/   88]
per-ex loss: 0.617081  [   38/   88]
per-ex loss: 0.738636  [   39/   88]
per-ex loss: 0.532879  [   40/   88]
per-ex loss: 0.502298  [   41/   88]
per-ex loss: 0.606155  [   42/   88]
per-ex loss: 0.540678  [   43/   88]
per-ex loss: 0.644685  [   44/   88]
per-ex loss: 0.571338  [   45/   88]
per-ex loss: 0.547426  [   46/   88]
per-ex loss: 0.603038  [   47/   88]
per-ex loss: 0.463421  [   48/   88]
per-ex loss: 0.721040  [   49/   88]
per-ex loss: 0.498449  [   50/   88]
per-ex loss: 0.705741  [   51/   88]
per-ex loss: 0.691589  [   52/   88]
per-ex loss: 0.502505  [   53/   88]
per-ex loss: 0.539795  [   54/   88]
per-ex loss: 0.658066  [   55/   88]
per-ex loss: 0.664526  [   56/   88]
per-ex loss: 0.567209  [   57/   88]
per-ex loss: 0.696987  [   58/   88]
per-ex loss: 0.519442  [   59/   88]
per-ex loss: 0.809790  [   60/   88]
per-ex loss: 0.522417  [   61/   88]
per-ex loss: 0.765452  [   62/   88]
per-ex loss: 0.494496  [   63/   88]
per-ex loss: 0.797465  [   64/   88]
per-ex loss: 0.765295  [   65/   88]
per-ex loss: 0.745668  [   66/   88]
per-ex loss: 0.668540  [   67/   88]
per-ex loss: 0.571342  [   68/   88]
per-ex loss: 0.758398  [   69/   88]
per-ex loss: 0.701540  [   70/   88]
per-ex loss: 0.792459  [   71/   88]
per-ex loss: 0.791367  [   72/   88]
per-ex loss: 0.608018  [   73/   88]
per-ex loss: 0.623304  [   74/   88]
per-ex loss: 0.598793  [   75/   88]
per-ex loss: 0.736350  [   76/   88]
per-ex loss: 0.552966  [   77/   88]
per-ex loss: 0.794974  [   78/   88]
per-ex loss: 0.699039  [   79/   88]
per-ex loss: 0.732268  [   80/   88]
per-ex loss: 0.733501  [   81/   88]
per-ex loss: 0.626582  [   82/   88]
per-ex loss: 0.671855  [   83/   88]
per-ex loss: 0.795271  [   84/   88]
per-ex loss: 0.490306  [   85/   88]
per-ex loss: 0.753764  [   86/   88]
per-ex loss: 0.770525  [   87/   88]
per-ex loss: 0.736537  [   88/   88]
Train Error: Avg loss: 0.62443466
validation Error: 
 Avg loss: 0.71548679 
 F1: 0.462451 
 Precision: 0.500648 
 Recall: 0.429669
 IoU: 0.300771

test Error: 
 Avg loss: 0.64730387 
 F1: 0.560081 
 Precision: 0.635388 
 Recall: 0.500734
 IoU: 0.388967

We have finished training iteration 87
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_85_.pth
per-ex loss: 0.581282  [    1/   88]
per-ex loss: 0.432128  [    2/   88]
per-ex loss: 0.571807  [    3/   88]
per-ex loss: 0.638005  [    4/   88]
per-ex loss: 0.726069  [    5/   88]
per-ex loss: 0.598947  [    6/   88]
per-ex loss: 0.794579  [    7/   88]
per-ex loss: 0.510429  [    8/   88]
per-ex loss: 0.558976  [    9/   88]
per-ex loss: 0.513197  [   10/   88]
per-ex loss: 0.754253  [   11/   88]
per-ex loss: 0.607914  [   12/   88]
per-ex loss: 0.783145  [   13/   88]
per-ex loss: 0.718891  [   14/   88]
per-ex loss: 0.534566  [   15/   88]
per-ex loss: 0.552320  [   16/   88]
per-ex loss: 0.562223  [   17/   88]
per-ex loss: 0.539669  [   18/   88]
per-ex loss: 0.596324  [   19/   88]
per-ex loss: 0.502871  [   20/   88]
per-ex loss: 0.535368  [   21/   88]
per-ex loss: 0.676084  [   22/   88]
per-ex loss: 0.744872  [   23/   88]
per-ex loss: 0.644314  [   24/   88]
per-ex loss: 0.514595  [   25/   88]
per-ex loss: 0.717153  [   26/   88]
per-ex loss: 0.514278  [   27/   88]
per-ex loss: 0.788010  [   28/   88]
per-ex loss: 0.610947  [   29/   88]
per-ex loss: 0.548485  [   30/   88]
per-ex loss: 0.634348  [   31/   88]
per-ex loss: 0.590866  [   32/   88]
per-ex loss: 0.733567  [   33/   88]
per-ex loss: 0.616884  [   34/   88]
per-ex loss: 0.701637  [   35/   88]
per-ex loss: 0.535985  [   36/   88]
per-ex loss: 0.627440  [   37/   88]
per-ex loss: 0.680435  [   38/   88]
per-ex loss: 0.745696  [   39/   88]
per-ex loss: 0.686620  [   40/   88]
per-ex loss: 0.540982  [   41/   88]
per-ex loss: 0.576326  [   42/   88]
per-ex loss: 0.776532  [   43/   88]
per-ex loss: 0.562119  [   44/   88]
per-ex loss: 0.475971  [   45/   88]
per-ex loss: 0.567356  [   46/   88]
per-ex loss: 0.530532  [   47/   88]
per-ex loss: 0.617631  [   48/   88]
per-ex loss: 0.697993  [   49/   88]
per-ex loss: 0.483640  [   50/   88]
per-ex loss: 0.508407  [   51/   88]
per-ex loss: 0.730225  [   52/   88]
per-ex loss: 0.758856  [   53/   88]
per-ex loss: 0.487927  [   54/   88]
per-ex loss: 0.494054  [   55/   88]
per-ex loss: 0.765868  [   56/   88]
per-ex loss: 0.673834  [   57/   88]
per-ex loss: 0.527840  [   58/   88]
per-ex loss: 0.584557  [   59/   88]
per-ex loss: 0.725236  [   60/   88]
per-ex loss: 0.751789  [   61/   88]
per-ex loss: 0.627787  [   62/   88]
per-ex loss: 0.518080  [   63/   88]
per-ex loss: 0.596702  [   64/   88]
per-ex loss: 0.694973  [   65/   88]
per-ex loss: 0.606389  [   66/   88]
per-ex loss: 0.497920  [   67/   88]
per-ex loss: 0.765793  [   68/   88]
per-ex loss: 0.632546  [   69/   88]
per-ex loss: 0.636354  [   70/   88]
per-ex loss: 0.640840  [   71/   88]
per-ex loss: 0.675296  [   72/   88]
per-ex loss: 0.723447  [   73/   88]
per-ex loss: 0.532820  [   74/   88]
per-ex loss: 0.676585  [   75/   88]
per-ex loss: 0.582634  [   76/   88]
per-ex loss: 0.711376  [   77/   88]
per-ex loss: 0.593807  [   78/   88]
per-ex loss: 0.619306  [   79/   88]
per-ex loss: 0.495178  [   80/   88]
per-ex loss: 0.800498  [   81/   88]
per-ex loss: 0.768394  [   82/   88]
per-ex loss: 0.669326  [   83/   88]
per-ex loss: 0.559175  [   84/   88]
per-ex loss: 0.765392  [   85/   88]
per-ex loss: 0.733420  [   86/   88]
per-ex loss: 0.717555  [   87/   88]
per-ex loss: 0.583144  [   88/   88]
Train Error: Avg loss: 0.62713172
validation Error: 
 Avg loss: 0.68968480 
 F1: 0.498381 
 Precision: 0.572039 
 Recall: 0.441527
 IoU: 0.331895

test Error: 
 Avg loss: 0.64309232 
 F1: 0.567042 
 Precision: 0.629160 
 Recall: 0.516088
 IoU: 0.395714

We have finished training iteration 88
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_86_.pth
per-ex loss: 0.796438  [    1/   88]
per-ex loss: 0.517613  [    2/   88]
per-ex loss: 0.551973  [    3/   88]
per-ex loss: 0.739134  [    4/   88]
per-ex loss: 0.704711  [    5/   88]
per-ex loss: 0.560571  [    6/   88]
per-ex loss: 0.730761  [    7/   88]
per-ex loss: 0.657561  [    8/   88]
per-ex loss: 0.545025  [    9/   88]
per-ex loss: 0.591704  [   10/   88]
per-ex loss: 0.473557  [   11/   88]
per-ex loss: 0.580671  [   12/   88]
per-ex loss: 0.775182  [   13/   88]
per-ex loss: 0.527298  [   14/   88]
per-ex loss: 0.711417  [   15/   88]
per-ex loss: 0.618163  [   16/   88]
per-ex loss: 0.685259  [   17/   88]
per-ex loss: 0.817284  [   18/   88]
per-ex loss: 0.775928  [   19/   88]
per-ex loss: 0.587794  [   20/   88]
per-ex loss: 0.561403  [   21/   88]
per-ex loss: 0.520038  [   22/   88]
per-ex loss: 0.555042  [   23/   88]
per-ex loss: 0.522628  [   24/   88]
per-ex loss: 0.684670  [   25/   88]
per-ex loss: 0.561781  [   26/   88]
per-ex loss: 0.619991  [   27/   88]
per-ex loss: 0.700792  [   28/   88]
per-ex loss: 0.698365  [   29/   88]
per-ex loss: 0.658898  [   30/   88]
per-ex loss: 0.574238  [   31/   88]
per-ex loss: 0.528926  [   32/   88]
per-ex loss: 0.711428  [   33/   88]
per-ex loss: 0.716617  [   34/   88]
per-ex loss: 0.513066  [   35/   88]
per-ex loss: 0.654665  [   36/   88]
per-ex loss: 0.536735  [   37/   88]
per-ex loss: 0.490060  [   38/   88]
per-ex loss: 0.567260  [   39/   88]
per-ex loss: 0.627930  [   40/   88]
per-ex loss: 0.704530  [   41/   88]
per-ex loss: 0.688030  [   42/   88]
per-ex loss: 0.529729  [   43/   88]
per-ex loss: 0.535199  [   44/   88]
per-ex loss: 0.782875  [   45/   88]
per-ex loss: 0.552520  [   46/   88]
per-ex loss: 0.779540  [   47/   88]
per-ex loss: 0.798899  [   48/   88]
per-ex loss: 0.714188  [   49/   88]
per-ex loss: 0.540524  [   50/   88]
per-ex loss: 0.492212  [   51/   88]
per-ex loss: 0.768499  [   52/   88]
per-ex loss: 0.616158  [   53/   88]
per-ex loss: 0.594135  [   54/   88]
per-ex loss: 0.695102  [   55/   88]
per-ex loss: 0.673233  [   56/   88]
per-ex loss: 0.591554  [   57/   88]
per-ex loss: 0.789142  [   58/   88]
per-ex loss: 0.751003  [   59/   88]
per-ex loss: 0.694627  [   60/   88]
per-ex loss: 0.585653  [   61/   88]
per-ex loss: 0.553465  [   62/   88]
per-ex loss: 0.591904  [   63/   88]
per-ex loss: 0.554949  [   64/   88]
per-ex loss: 0.731284  [   65/   88]
per-ex loss: 0.694969  [   66/   88]
per-ex loss: 0.632686  [   67/   88]
per-ex loss: 0.452749  [   68/   88]
per-ex loss: 0.557382  [   69/   88]
per-ex loss: 0.572680  [   70/   88]
per-ex loss: 0.584475  [   71/   88]
per-ex loss: 0.541804  [   72/   88]
per-ex loss: 0.705963  [   73/   88]
per-ex loss: 0.505802  [   74/   88]
per-ex loss: 0.594967  [   75/   88]
per-ex loss: 0.735596  [   76/   88]
per-ex loss: 0.496151  [   77/   88]
per-ex loss: 0.499083  [   78/   88]
per-ex loss: 0.433661  [   79/   88]
per-ex loss: 0.649636  [   80/   88]
per-ex loss: 0.734502  [   81/   88]
per-ex loss: 0.453062  [   82/   88]
per-ex loss: 0.700683  [   83/   88]
per-ex loss: 0.736654  [   84/   88]
per-ex loss: 0.741119  [   85/   88]
per-ex loss: 0.606066  [   86/   88]
per-ex loss: 0.697630  [   87/   88]
per-ex loss: 0.773514  [   88/   88]
Train Error: Avg loss: 0.62911774
validation Error: 
 Avg loss: 0.69348075 
 F1: 0.494703 
 Precision: 0.548566 
 Recall: 0.450472
 IoU: 0.328641

test Error: 
 Avg loss: 0.62770776 
 F1: 0.583872 
 Precision: 0.638622 
 Recall: 0.537768
 IoU: 0.412301

We have finished training iteration 89
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_87_.pth
per-ex loss: 0.450604  [    1/   88]
per-ex loss: 0.713948  [    2/   88]
per-ex loss: 0.499712  [    3/   88]
per-ex loss: 0.686696  [    4/   88]
per-ex loss: 0.556796  [    5/   88]
per-ex loss: 0.643617  [    6/   88]
per-ex loss: 0.551811  [    7/   88]
per-ex loss: 0.542889  [    8/   88]
per-ex loss: 0.646080  [    9/   88]
per-ex loss: 0.623257  [   10/   88]
per-ex loss: 0.489930  [   11/   88]
per-ex loss: 0.560999  [   12/   88]
per-ex loss: 0.664976  [   13/   88]
per-ex loss: 0.774269  [   14/   88]
per-ex loss: 0.689701  [   15/   88]
per-ex loss: 0.570234  [   16/   88]
per-ex loss: 0.559113  [   17/   88]
per-ex loss: 0.564454  [   18/   88]
per-ex loss: 0.676350  [   19/   88]
per-ex loss: 0.506721  [   20/   88]
per-ex loss: 0.778331  [   21/   88]
per-ex loss: 0.554948  [   22/   88]
per-ex loss: 0.580449  [   23/   88]
per-ex loss: 0.516887  [   24/   88]
per-ex loss: 0.739607  [   25/   88]
per-ex loss: 0.524838  [   26/   88]
per-ex loss: 0.707932  [   27/   88]
per-ex loss: 0.404631  [   28/   88]
per-ex loss: 0.552337  [   29/   88]
per-ex loss: 0.743654  [   30/   88]
per-ex loss: 0.703737  [   31/   88]
per-ex loss: 0.619020  [   32/   88]
per-ex loss: 0.537542  [   33/   88]
per-ex loss: 0.659699  [   34/   88]
per-ex loss: 0.543602  [   35/   88]
per-ex loss: 0.659284  [   36/   88]
per-ex loss: 0.702214  [   37/   88]
per-ex loss: 0.576085  [   38/   88]
per-ex loss: 0.787880  [   39/   88]
per-ex loss: 0.724441  [   40/   88]
per-ex loss: 0.560024  [   41/   88]
per-ex loss: 0.740505  [   42/   88]
per-ex loss: 0.547936  [   43/   88]
per-ex loss: 0.613915  [   44/   88]
per-ex loss: 0.549279  [   45/   88]
per-ex loss: 0.739665  [   46/   88]
per-ex loss: 0.718089  [   47/   88]
per-ex loss: 0.597204  [   48/   88]
per-ex loss: 0.739826  [   49/   88]
per-ex loss: 0.494907  [   50/   88]
per-ex loss: 0.791490  [   51/   88]
per-ex loss: 0.539309  [   52/   88]
per-ex loss: 0.689853  [   53/   88]
per-ex loss: 0.652859  [   54/   88]
per-ex loss: 0.527139  [   55/   88]
per-ex loss: 0.760527  [   56/   88]
per-ex loss: 0.766851  [   57/   88]
per-ex loss: 0.793287  [   58/   88]
per-ex loss: 0.640871  [   59/   88]
per-ex loss: 0.592829  [   60/   88]
per-ex loss: 0.495449  [   61/   88]
per-ex loss: 0.569493  [   62/   88]
per-ex loss: 0.564037  [   63/   88]
per-ex loss: 0.593204  [   64/   88]
per-ex loss: 0.713472  [   65/   88]
per-ex loss: 0.722154  [   66/   88]
per-ex loss: 0.489271  [   67/   88]
per-ex loss: 0.504764  [   68/   88]
per-ex loss: 0.704337  [   69/   88]
per-ex loss: 0.768824  [   70/   88]
per-ex loss: 0.695552  [   71/   88]
per-ex loss: 0.700280  [   72/   88]
per-ex loss: 0.660277  [   73/   88]
per-ex loss: 0.502903  [   74/   88]
per-ex loss: 0.547228  [   75/   88]
per-ex loss: 0.467827  [   76/   88]
per-ex loss: 0.687440  [   77/   88]
per-ex loss: 0.616271  [   78/   88]
per-ex loss: 0.550449  [   79/   88]
per-ex loss: 0.503334  [   80/   88]
per-ex loss: 0.511527  [   81/   88]
per-ex loss: 0.576119  [   82/   88]
per-ex loss: 0.525535  [   83/   88]
per-ex loss: 0.798971  [   84/   88]
per-ex loss: 0.748250  [   85/   88]
per-ex loss: 0.737567  [   86/   88]
per-ex loss: 0.567541  [   87/   88]
per-ex loss: 0.584738  [   88/   88]
Train Error: Avg loss: 0.62218699
validation Error: 
 Avg loss: 0.69420952 
 F1: 0.493458 
 Precision: 0.548498 
 Recall: 0.448457
 IoU: 0.327544

test Error: 
 Avg loss: 0.63107185 
 F1: 0.580067 
 Precision: 0.638424 
 Recall: 0.531484
 IoU: 0.408517

We have finished training iteration 90
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_64_.pth
per-ex loss: 0.644063  [    1/   88]
per-ex loss: 0.546755  [    2/   88]
per-ex loss: 0.778885  [    3/   88]
per-ex loss: 0.525786  [    4/   88]
per-ex loss: 0.736217  [    5/   88]
per-ex loss: 0.486358  [    6/   88]
per-ex loss: 0.727993  [    7/   88]
per-ex loss: 0.550994  [    8/   88]
per-ex loss: 0.635101  [    9/   88]
per-ex loss: 0.548644  [   10/   88]
per-ex loss: 0.499220  [   11/   88]
per-ex loss: 0.508434  [   12/   88]
per-ex loss: 0.524841  [   13/   88]
per-ex loss: 0.591810  [   14/   88]
per-ex loss: 0.699350  [   15/   88]
per-ex loss: 0.612199  [   16/   88]
per-ex loss: 0.483010  [   17/   88]
per-ex loss: 0.712229  [   18/   88]
per-ex loss: 0.595144  [   19/   88]
per-ex loss: 0.383326  [   20/   88]
per-ex loss: 0.694106  [   21/   88]
per-ex loss: 0.691511  [   22/   88]
per-ex loss: 0.542069  [   23/   88]
per-ex loss: 0.672277  [   24/   88]
per-ex loss: 0.544341  [   25/   88]
per-ex loss: 0.483615  [   26/   88]
per-ex loss: 0.530113  [   27/   88]
per-ex loss: 0.773903  [   28/   88]
per-ex loss: 0.559144  [   29/   88]
per-ex loss: 0.601300  [   30/   88]
per-ex loss: 0.559258  [   31/   88]
per-ex loss: 0.522158  [   32/   88]
per-ex loss: 0.538915  [   33/   88]
per-ex loss: 0.503354  [   34/   88]
per-ex loss: 0.641614  [   35/   88]
per-ex loss: 0.738095  [   36/   88]
per-ex loss: 0.787334  [   37/   88]
per-ex loss: 0.470941  [   38/   88]
per-ex loss: 0.717371  [   39/   88]
per-ex loss: 0.500268  [   40/   88]
per-ex loss: 0.658674  [   41/   88]
per-ex loss: 0.644681  [   42/   88]
per-ex loss: 0.557707  [   43/   88]
per-ex loss: 0.731565  [   44/   88]
per-ex loss: 0.683565  [   45/   88]
per-ex loss: 0.509621  [   46/   88]
per-ex loss: 0.757219  [   47/   88]
per-ex loss: 0.684262  [   48/   88]
per-ex loss: 0.489304  [   49/   88]
per-ex loss: 0.721843  [   50/   88]
per-ex loss: 0.662003  [   51/   88]
per-ex loss: 0.730222  [   52/   88]
per-ex loss: 0.582837  [   53/   88]
per-ex loss: 0.790567  [   54/   88]
per-ex loss: 0.636698  [   55/   88]
per-ex loss: 0.521079  [   56/   88]
per-ex loss: 0.555731  [   57/   88]
per-ex loss: 0.798594  [   58/   88]
per-ex loss: 0.765724  [   59/   88]
per-ex loss: 0.542577  [   60/   88]
per-ex loss: 0.560724  [   61/   88]
per-ex loss: 0.779504  [   62/   88]
per-ex loss: 0.560472  [   63/   88]
per-ex loss: 0.555497  [   64/   88]
per-ex loss: 0.571918  [   65/   88]
per-ex loss: 0.700491  [   66/   88]
per-ex loss: 0.610690  [   67/   88]
per-ex loss: 0.535508  [   68/   88]
per-ex loss: 0.683102  [   69/   88]
per-ex loss: 0.538451  [   70/   88]
per-ex loss: 0.494650  [   71/   88]
per-ex loss: 0.596774  [   72/   88]
per-ex loss: 0.747841  [   73/   88]
per-ex loss: 0.776078  [   74/   88]
per-ex loss: 0.645571  [   75/   88]
per-ex loss: 0.449426  [   76/   88]
per-ex loss: 0.565684  [   77/   88]
per-ex loss: 0.464214  [   78/   88]
per-ex loss: 0.734352  [   79/   88]
per-ex loss: 0.665165  [   80/   88]
per-ex loss: 0.801049  [   81/   88]
per-ex loss: 0.504994  [   82/   88]
per-ex loss: 0.722898  [   83/   88]
per-ex loss: 0.542402  [   84/   88]
per-ex loss: 0.537767  [   85/   88]
per-ex loss: 0.706029  [   86/   88]
per-ex loss: 0.726671  [   87/   88]
per-ex loss: 0.607660  [   88/   88]
Train Error: Avg loss: 0.61672830
validation Error: 
 Avg loss: 0.69250679 
 F1: 0.493910 
 Precision: 0.542299 
 Recall: 0.453449
 IoU: 0.327942

test Error: 
 Avg loss: 0.63264216 
 F1: 0.578681 
 Precision: 0.623895 
 Recall: 0.539578
 IoU: 0.407144

We have finished training iteration 91
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_69_.pth
per-ex loss: 0.565296  [    1/   88]
per-ex loss: 0.789630  [    2/   88]
per-ex loss: 0.506034  [    3/   88]
per-ex loss: 0.513433  [    4/   88]
per-ex loss: 0.512895  [    5/   88]
per-ex loss: 0.494351  [    6/   88]
per-ex loss: 0.557152  [    7/   88]
per-ex loss: 0.636814  [    8/   88]
per-ex loss: 0.490625  [    9/   88]
per-ex loss: 0.596964  [   10/   88]
per-ex loss: 0.542688  [   11/   88]
per-ex loss: 0.491391  [   12/   88]
per-ex loss: 0.567251  [   13/   88]
per-ex loss: 0.472862  [   14/   88]
per-ex loss: 0.706824  [   15/   88]
per-ex loss: 0.692529  [   16/   88]
per-ex loss: 0.641391  [   17/   88]
per-ex loss: 0.580906  [   18/   88]
per-ex loss: 0.640658  [   19/   88]
per-ex loss: 0.524372  [   20/   88]
per-ex loss: 0.585863  [   21/   88]
per-ex loss: 0.690097  [   22/   88]
per-ex loss: 0.606095  [   23/   88]
per-ex loss: 0.523275  [   24/   88]
per-ex loss: 0.725036  [   25/   88]
per-ex loss: 0.621893  [   26/   88]
per-ex loss: 0.673432  [   27/   88]
per-ex loss: 0.534329  [   28/   88]
per-ex loss: 0.393049  [   29/   88]
per-ex loss: 0.752034  [   30/   88]
per-ex loss: 0.666683  [   31/   88]
per-ex loss: 0.523571  [   32/   88]
per-ex loss: 0.501377  [   33/   88]
per-ex loss: 0.584729  [   34/   88]
per-ex loss: 0.676094  [   35/   88]
per-ex loss: 0.493194  [   36/   88]
per-ex loss: 0.497422  [   37/   88]
per-ex loss: 0.694820  [   38/   88]
per-ex loss: 0.568199  [   39/   88]
per-ex loss: 0.758448  [   40/   88]
per-ex loss: 0.553485  [   41/   88]
per-ex loss: 0.521524  [   42/   88]
per-ex loss: 0.624263  [   43/   88]
per-ex loss: 0.558766  [   44/   88]
per-ex loss: 0.535263  [   45/   88]
per-ex loss: 0.659437  [   46/   88]
per-ex loss: 0.526055  [   47/   88]
per-ex loss: 0.581026  [   48/   88]
per-ex loss: 0.574810  [   49/   88]
per-ex loss: 0.740557  [   50/   88]
per-ex loss: 0.797243  [   51/   88]
per-ex loss: 0.518073  [   52/   88]
per-ex loss: 0.579144  [   53/   88]
per-ex loss: 0.661289  [   54/   88]
per-ex loss: 0.724806  [   55/   88]
per-ex loss: 0.785016  [   56/   88]
per-ex loss: 0.736836  [   57/   88]
per-ex loss: 0.572633  [   58/   88]
per-ex loss: 0.513632  [   59/   88]
per-ex loss: 0.764438  [   60/   88]
per-ex loss: 0.706323  [   61/   88]
per-ex loss: 0.473817  [   62/   88]
per-ex loss: 0.485063  [   63/   88]
per-ex loss: 0.683011  [   64/   88]
per-ex loss: 0.718942  [   65/   88]
per-ex loss: 0.535179  [   66/   88]
per-ex loss: 0.534287  [   67/   88]
per-ex loss: 0.743394  [   68/   88]
per-ex loss: 0.695152  [   69/   88]
per-ex loss: 0.571954  [   70/   88]
per-ex loss: 0.607108  [   71/   88]
per-ex loss: 0.620488  [   72/   88]
per-ex loss: 0.531621  [   73/   88]
per-ex loss: 0.551614  [   74/   88]
per-ex loss: 0.521125  [   75/   88]
per-ex loss: 0.692343  [   76/   88]
per-ex loss: 0.673281  [   77/   88]
per-ex loss: 0.775574  [   78/   88]
per-ex loss: 0.778011  [   79/   88]
per-ex loss: 0.649364  [   80/   88]
per-ex loss: 0.614363  [   81/   88]
per-ex loss: 0.512635  [   82/   88]
per-ex loss: 0.707302  [   83/   88]
per-ex loss: 0.753039  [   84/   88]
per-ex loss: 0.749808  [   85/   88]
per-ex loss: 0.736583  [   86/   88]
per-ex loss: 0.728927  [   87/   88]
per-ex loss: 0.770085  [   88/   88]
Train Error: Avg loss: 0.61639087
validation Error: 
 Avg loss: 0.70423008 
 F1: 0.476713 
 Precision: 0.455687 
 Recall: 0.499772
 IoU: 0.312950

test Error: 
 Avg loss: 0.63973382 
 F1: 0.568318 
 Precision: 0.549510 
 Recall: 0.588459
 IoU: 0.396958

We have finished training iteration 92
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_90_.pth
per-ex loss: 0.733062  [    1/   88]
per-ex loss: 0.535207  [    2/   88]
per-ex loss: 0.621036  [    3/   88]
per-ex loss: 0.531532  [    4/   88]
per-ex loss: 0.714692  [    5/   88]
per-ex loss: 0.600692  [    6/   88]
per-ex loss: 0.532920  [    7/   88]
per-ex loss: 0.736156  [    8/   88]
per-ex loss: 0.678547  [    9/   88]
per-ex loss: 0.604704  [   10/   88]
per-ex loss: 0.748898  [   11/   88]
per-ex loss: 0.556617  [   12/   88]
per-ex loss: 0.556983  [   13/   88]
per-ex loss: 0.450477  [   14/   88]
per-ex loss: 0.618406  [   15/   88]
per-ex loss: 0.665266  [   16/   88]
per-ex loss: 0.386473  [   17/   88]
per-ex loss: 0.701563  [   18/   88]
per-ex loss: 0.725091  [   19/   88]
per-ex loss: 0.542654  [   20/   88]
per-ex loss: 0.740257  [   21/   88]
per-ex loss: 0.570101  [   22/   88]
per-ex loss: 0.455510  [   23/   88]
per-ex loss: 0.493921  [   24/   88]
per-ex loss: 0.707042  [   25/   88]
per-ex loss: 0.498851  [   26/   88]
per-ex loss: 0.507408  [   27/   88]
per-ex loss: 0.787487  [   28/   88]
per-ex loss: 0.580022  [   29/   88]
per-ex loss: 0.499677  [   30/   88]
per-ex loss: 0.710400  [   31/   88]
per-ex loss: 0.694710  [   32/   88]
per-ex loss: 0.761251  [   33/   88]
per-ex loss: 0.789328  [   34/   88]
per-ex loss: 0.768918  [   35/   88]
per-ex loss: 0.516675  [   36/   88]
per-ex loss: 0.526856  [   37/   88]
per-ex loss: 0.536591  [   38/   88]
per-ex loss: 0.529877  [   39/   88]
per-ex loss: 0.640395  [   40/   88]
per-ex loss: 0.711248  [   41/   88]
per-ex loss: 0.656349  [   42/   88]
per-ex loss: 0.667211  [   43/   88]
per-ex loss: 0.523941  [   44/   88]
per-ex loss: 0.642440  [   45/   88]
per-ex loss: 0.780574  [   46/   88]
per-ex loss: 0.513792  [   47/   88]
per-ex loss: 0.701975  [   48/   88]
per-ex loss: 0.583128  [   49/   88]
per-ex loss: 0.567778  [   50/   88]
per-ex loss: 0.585672  [   51/   88]
per-ex loss: 0.545523  [   52/   88]
per-ex loss: 0.662679  [   53/   88]
per-ex loss: 0.790220  [   54/   88]
per-ex loss: 0.552100  [   55/   88]
per-ex loss: 0.601486  [   56/   88]
per-ex loss: 0.614485  [   57/   88]
per-ex loss: 0.623060  [   58/   88]
per-ex loss: 0.556298  [   59/   88]
per-ex loss: 0.725573  [   60/   88]
per-ex loss: 0.736865  [   61/   88]
per-ex loss: 0.641428  [   62/   88]
per-ex loss: 0.502399  [   63/   88]
per-ex loss: 0.796520  [   64/   88]
per-ex loss: 0.477606  [   65/   88]
per-ex loss: 0.735693  [   66/   88]
per-ex loss: 0.633059  [   67/   88]
per-ex loss: 0.536127  [   68/   88]
per-ex loss: 0.569475  [   69/   88]
per-ex loss: 0.572082  [   70/   88]
per-ex loss: 0.617903  [   71/   88]
per-ex loss: 0.756368  [   72/   88]
per-ex loss: 0.563134  [   73/   88]
per-ex loss: 0.700389  [   74/   88]
per-ex loss: 0.502567  [   75/   88]
per-ex loss: 0.688102  [   76/   88]
per-ex loss: 0.699142  [   77/   88]
per-ex loss: 0.554684  [   78/   88]
per-ex loss: 0.560316  [   79/   88]
per-ex loss: 0.486715  [   80/   88]
per-ex loss: 0.774661  [   81/   88]
per-ex loss: 0.732752  [   82/   88]
per-ex loss: 0.754126  [   83/   88]
per-ex loss: 0.496421  [   84/   88]
per-ex loss: 0.458062  [   85/   88]
per-ex loss: 0.560098  [   86/   88]
per-ex loss: 0.797847  [   87/   88]
per-ex loss: 0.786642  [   88/   88]
Train Error: Avg loss: 0.62332919
validation Error: 
 Avg loss: 0.70909207 
 F1: 0.474215 
 Precision: 0.549704 
 Recall: 0.416956
 IoU: 0.310801

test Error: 
 Avg loss: 0.63616391 
 F1: 0.572887 
 Precision: 0.673404 
 Recall: 0.498481
 IoU: 0.401431

We have finished training iteration 93
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_89_.pth
per-ex loss: 0.722188  [    1/   88]
per-ex loss: 0.760710  [    2/   88]
per-ex loss: 0.514700  [    3/   88]
per-ex loss: 0.532641  [    4/   88]
per-ex loss: 0.766776  [    5/   88]
per-ex loss: 0.620144  [    6/   88]
per-ex loss: 0.465448  [    7/   88]
per-ex loss: 0.774830  [    8/   88]
per-ex loss: 0.789107  [    9/   88]
per-ex loss: 0.487472  [   10/   88]
per-ex loss: 0.556658  [   11/   88]
per-ex loss: 0.520696  [   12/   88]
per-ex loss: 0.531369  [   13/   88]
per-ex loss: 0.813092  [   14/   88]
per-ex loss: 0.431350  [   15/   88]
per-ex loss: 0.644031  [   16/   88]
per-ex loss: 0.535572  [   17/   88]
per-ex loss: 0.709422  [   18/   88]
per-ex loss: 0.545845  [   19/   88]
per-ex loss: 0.562393  [   20/   88]
per-ex loss: 0.562175  [   21/   88]
per-ex loss: 0.538664  [   22/   88]
per-ex loss: 0.749520  [   23/   88]
per-ex loss: 0.508416  [   24/   88]
per-ex loss: 0.784071  [   25/   88]
per-ex loss: 0.485363  [   26/   88]
per-ex loss: 0.532818  [   27/   88]
per-ex loss: 0.550981  [   28/   88]
per-ex loss: 0.530072  [   29/   88]
per-ex loss: 0.653759  [   30/   88]
per-ex loss: 0.577064  [   31/   88]
per-ex loss: 0.485594  [   32/   88]
per-ex loss: 0.802256  [   33/   88]
per-ex loss: 0.733007  [   34/   88]
per-ex loss: 0.527532  [   35/   88]
per-ex loss: 0.668933  [   36/   88]
per-ex loss: 0.724052  [   37/   88]
per-ex loss: 0.728766  [   38/   88]
per-ex loss: 0.673173  [   39/   88]
per-ex loss: 0.736731  [   40/   88]
per-ex loss: 0.763659  [   41/   88]
per-ex loss: 0.451111  [   42/   88]
per-ex loss: 0.542445  [   43/   88]
per-ex loss: 0.556389  [   44/   88]
per-ex loss: 0.690670  [   45/   88]
per-ex loss: 0.560973  [   46/   88]
per-ex loss: 0.489851  [   47/   88]
per-ex loss: 0.601445  [   48/   88]
per-ex loss: 0.716325  [   49/   88]
per-ex loss: 0.715667  [   50/   88]
per-ex loss: 0.588763  [   51/   88]
per-ex loss: 0.571643  [   52/   88]
per-ex loss: 0.638459  [   53/   88]
per-ex loss: 0.478506  [   54/   88]
per-ex loss: 0.700318  [   55/   88]
per-ex loss: 0.565194  [   56/   88]
per-ex loss: 0.637998  [   57/   88]
per-ex loss: 0.607029  [   58/   88]
per-ex loss: 0.713017  [   59/   88]
per-ex loss: 0.485329  [   60/   88]
per-ex loss: 0.684811  [   61/   88]
per-ex loss: 0.518898  [   62/   88]
per-ex loss: 0.694267  [   63/   88]
per-ex loss: 0.635240  [   64/   88]
per-ex loss: 0.603816  [   65/   88]
per-ex loss: 0.540982  [   66/   88]
per-ex loss: 0.592491  [   67/   88]
per-ex loss: 0.495025  [   68/   88]
per-ex loss: 0.506745  [   69/   88]
per-ex loss: 0.589401  [   70/   88]
per-ex loss: 0.723708  [   71/   88]
per-ex loss: 0.529836  [   72/   88]
per-ex loss: 0.664262  [   73/   88]
per-ex loss: 0.771952  [   74/   88]
per-ex loss: 0.533249  [   75/   88]
per-ex loss: 0.751279  [   76/   88]
per-ex loss: 0.524059  [   77/   88]
per-ex loss: 0.709878  [   78/   88]
per-ex loss: 0.785094  [   79/   88]
per-ex loss: 0.500866  [   80/   88]
per-ex loss: 0.589175  [   81/   88]
per-ex loss: 0.646056  [   82/   88]
per-ex loss: 0.641761  [   83/   88]
per-ex loss: 0.652491  [   84/   88]
per-ex loss: 0.740559  [   85/   88]
per-ex loss: 0.642584  [   86/   88]
per-ex loss: 0.553933  [   87/   88]
per-ex loss: 0.692572  [   88/   88]
Train Error: Avg loss: 0.61849058
validation Error: 
 Avg loss: 0.70200886 
 F1: 0.479716 
 Precision: 0.540860 
 Recall: 0.430992
 IoU: 0.315544

test Error: 
 Avg loss: 0.64210722 
 F1: 0.564702 
 Precision: 0.649223 
 Recall: 0.499653
 IoU: 0.393439

We have finished training iteration 94
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_92_.pth
per-ex loss: 0.841770  [    1/   88]
per-ex loss: 0.600544  [    2/   88]
per-ex loss: 0.729302  [    3/   88]
per-ex loss: 0.695681  [    4/   88]
per-ex loss: 0.474683  [    5/   88]
per-ex loss: 0.484401  [    6/   88]
per-ex loss: 0.610776  [    7/   88]
per-ex loss: 0.700324  [    8/   88]
per-ex loss: 0.732403  [    9/   88]
per-ex loss: 0.504609  [   10/   88]
per-ex loss: 0.566971  [   11/   88]
per-ex loss: 0.570787  [   12/   88]
per-ex loss: 0.507459  [   13/   88]
per-ex loss: 0.843278  [   14/   88]
per-ex loss: 0.702800  [   15/   88]
per-ex loss: 0.612777  [   16/   88]
per-ex loss: 0.508796  [   17/   88]
per-ex loss: 0.538658  [   18/   88]
per-ex loss: 0.746935  [   19/   88]
per-ex loss: 0.504722  [   20/   88]
per-ex loss: 0.634690  [   21/   88]
per-ex loss: 0.530036  [   22/   88]
per-ex loss: 0.572113  [   23/   88]
per-ex loss: 0.651007  [   24/   88]
per-ex loss: 0.785393  [   25/   88]
per-ex loss: 0.715046  [   26/   88]
per-ex loss: 0.669009  [   27/   88]
per-ex loss: 0.529537  [   28/   88]
per-ex loss: 0.571140  [   29/   88]
per-ex loss: 0.728728  [   30/   88]
per-ex loss: 0.592345  [   31/   88]
per-ex loss: 0.502973  [   32/   88]
per-ex loss: 0.532511  [   33/   88]
per-ex loss: 0.654928  [   34/   88]
per-ex loss: 0.786918  [   35/   88]
per-ex loss: 0.554461  [   36/   88]
per-ex loss: 0.617964  [   37/   88]
per-ex loss: 0.563484  [   38/   88]
per-ex loss: 0.757473  [   39/   88]
per-ex loss: 0.810614  [   40/   88]
per-ex loss: 0.483544  [   41/   88]
per-ex loss: 0.452900  [   42/   88]
per-ex loss: 0.690642  [   43/   88]
per-ex loss: 0.455091  [   44/   88]
per-ex loss: 0.504494  [   45/   88]
per-ex loss: 0.523198  [   46/   88]
per-ex loss: 0.615124  [   47/   88]
per-ex loss: 0.667438  [   48/   88]
per-ex loss: 0.488285  [   49/   88]
per-ex loss: 0.587049  [   50/   88]
per-ex loss: 0.663786  [   51/   88]
per-ex loss: 0.392220  [   52/   88]
per-ex loss: 0.773384  [   53/   88]
per-ex loss: 0.760527  [   54/   88]
per-ex loss: 0.484613  [   55/   88]
per-ex loss: 0.731698  [   56/   88]
per-ex loss: 0.772115  [   57/   88]
per-ex loss: 0.548650  [   58/   88]
per-ex loss: 0.706148  [   59/   88]
per-ex loss: 0.702106  [   60/   88]
per-ex loss: 0.684859  [   61/   88]
per-ex loss: 0.690496  [   62/   88]
per-ex loss: 0.695576  [   63/   88]
per-ex loss: 0.743411  [   64/   88]
per-ex loss: 0.644367  [   65/   88]
per-ex loss: 0.571679  [   66/   88]
per-ex loss: 0.534211  [   67/   88]
per-ex loss: 0.604134  [   68/   88]
per-ex loss: 0.563964  [   69/   88]
per-ex loss: 0.745373  [   70/   88]
per-ex loss: 0.493898  [   71/   88]
per-ex loss: 0.527802  [   72/   88]
per-ex loss: 0.741000  [   73/   88]
per-ex loss: 0.490584  [   74/   88]
per-ex loss: 0.714572  [   75/   88]
per-ex loss: 0.521152  [   76/   88]
per-ex loss: 0.629221  [   77/   88]
per-ex loss: 0.563824  [   78/   88]
per-ex loss: 0.533169  [   79/   88]
per-ex loss: 0.532738  [   80/   88]
per-ex loss: 0.546963  [   81/   88]
per-ex loss: 0.530750  [   82/   88]
per-ex loss: 0.579500  [   83/   88]
per-ex loss: 0.717488  [   84/   88]
per-ex loss: 0.550662  [   85/   88]
per-ex loss: 0.767195  [   86/   88]
per-ex loss: 0.695482  [   87/   88]
per-ex loss: 0.555494  [   88/   88]
Train Error: Avg loss: 0.61830254
validation Error: 
 Avg loss: 0.69807410 
 F1: 0.489988 
 Precision: 0.577963 
 Recall: 0.425258
 IoU: 0.324493

test Error: 
 Avg loss: 0.63647607 
 F1: 0.573243 
 Precision: 0.665550 
 Recall: 0.503422
 IoU: 0.401781

We have finished training iteration 95
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_93_.pth
per-ex loss: 0.551185  [    1/   88]
per-ex loss: 0.577510  [    2/   88]
per-ex loss: 0.690035  [    3/   88]
per-ex loss: 0.514190  [    4/   88]
per-ex loss: 0.771834  [    5/   88]
per-ex loss: 0.657897  [    6/   88]
per-ex loss: 0.497467  [    7/   88]
per-ex loss: 0.684449  [    8/   88]
per-ex loss: 0.501681  [    9/   88]
per-ex loss: 0.754669  [   10/   88]
per-ex loss: 0.535794  [   11/   88]
per-ex loss: 0.591406  [   12/   88]
per-ex loss: 0.635936  [   13/   88]
per-ex loss: 0.545539  [   14/   88]
per-ex loss: 0.502800  [   15/   88]
per-ex loss: 0.585811  [   16/   88]
per-ex loss: 0.701656  [   17/   88]
per-ex loss: 0.681403  [   18/   88]
per-ex loss: 0.536193  [   19/   88]
per-ex loss: 0.548214  [   20/   88]
per-ex loss: 0.572583  [   21/   88]
per-ex loss: 0.508394  [   22/   88]
per-ex loss: 0.537873  [   23/   88]
per-ex loss: 0.512482  [   24/   88]
per-ex loss: 0.560340  [   25/   88]
per-ex loss: 0.625917  [   26/   88]
per-ex loss: 0.657812  [   27/   88]
per-ex loss: 0.772009  [   28/   88]
per-ex loss: 0.545833  [   29/   88]
per-ex loss: 0.512048  [   30/   88]
per-ex loss: 0.584039  [   31/   88]
per-ex loss: 0.652099  [   32/   88]
per-ex loss: 0.533846  [   33/   88]
per-ex loss: 0.744732  [   34/   88]
per-ex loss: 0.506928  [   35/   88]
per-ex loss: 0.525994  [   36/   88]
per-ex loss: 0.746240  [   37/   88]
per-ex loss: 0.578824  [   38/   88]
per-ex loss: 0.650524  [   39/   88]
per-ex loss: 0.462761  [   40/   88]
per-ex loss: 0.552345  [   41/   88]
per-ex loss: 0.493141  [   42/   88]
per-ex loss: 0.511467  [   43/   88]
per-ex loss: 0.376221  [   44/   88]
per-ex loss: 0.562051  [   45/   88]
per-ex loss: 0.585807  [   46/   88]
per-ex loss: 0.538945  [   47/   88]
per-ex loss: 0.808288  [   48/   88]
per-ex loss: 0.552349  [   49/   88]
per-ex loss: 0.519909  [   50/   88]
per-ex loss: 0.584923  [   51/   88]
per-ex loss: 0.671050  [   52/   88]
per-ex loss: 0.724800  [   53/   88]
per-ex loss: 0.664261  [   54/   88]
per-ex loss: 0.504470  [   55/   88]
per-ex loss: 0.534241  [   56/   88]
per-ex loss: 0.720219  [   57/   88]
per-ex loss: 0.776971  [   58/   88]
per-ex loss: 0.736125  [   59/   88]
per-ex loss: 0.790793  [   60/   88]
per-ex loss: 0.458363  [   61/   88]
per-ex loss: 0.752359  [   62/   88]
per-ex loss: 0.658873  [   63/   88]
per-ex loss: 0.636267  [   64/   88]
per-ex loss: 0.615954  [   65/   88]
per-ex loss: 0.671540  [   66/   88]
per-ex loss: 0.726511  [   67/   88]
per-ex loss: 0.707936  [   68/   88]
per-ex loss: 0.643318  [   69/   88]
per-ex loss: 0.533297  [   70/   88]
per-ex loss: 0.626928  [   71/   88]
per-ex loss: 0.549649  [   72/   88]
per-ex loss: 0.528517  [   73/   88]
per-ex loss: 0.689497  [   74/   88]
per-ex loss: 0.704186  [   75/   88]
per-ex loss: 0.499488  [   76/   88]
per-ex loss: 0.778876  [   77/   88]
per-ex loss: 0.732899  [   78/   88]
per-ex loss: 0.840152  [   79/   88]
per-ex loss: 0.565417  [   80/   88]
per-ex loss: 0.512322  [   81/   88]
per-ex loss: 0.758650  [   82/   88]
per-ex loss: 0.594586  [   83/   88]
per-ex loss: 0.737442  [   84/   88]
per-ex loss: 0.742258  [   85/   88]
per-ex loss: 0.733076  [   86/   88]
per-ex loss: 0.570112  [   87/   88]
per-ex loss: 0.704372  [   88/   88]
Train Error: Avg loss: 0.61747919
validation Error: 
 Avg loss: 0.69260033 
 F1: 0.494141 
 Precision: 0.520599 
 Recall: 0.470242
 IoU: 0.328146

test Error: 
 Avg loss: 0.63553815 
 F1: 0.569971 
 Precision: 0.599423 
 Recall: 0.543277
 IoU: 0.398573

We have finished training iteration 96
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_94_.pth
per-ex loss: 0.693404  [    1/   88]
per-ex loss: 0.516605  [    2/   88]
per-ex loss: 0.627532  [    3/   88]
per-ex loss: 0.540404  [    4/   88]
per-ex loss: 0.690349  [    5/   88]
per-ex loss: 0.564108  [    6/   88]
per-ex loss: 0.705036  [    7/   88]
per-ex loss: 0.753227  [    8/   88]
per-ex loss: 0.568513  [    9/   88]
per-ex loss: 0.519429  [   10/   88]
per-ex loss: 0.562688  [   11/   88]
per-ex loss: 0.485431  [   12/   88]
per-ex loss: 0.621674  [   13/   88]
per-ex loss: 0.586935  [   14/   88]
per-ex loss: 0.559704  [   15/   88]
per-ex loss: 0.734345  [   16/   88]
per-ex loss: 0.562044  [   17/   88]
per-ex loss: 0.667378  [   18/   88]
per-ex loss: 0.551107  [   19/   88]
per-ex loss: 0.662828  [   20/   88]
per-ex loss: 0.513733  [   21/   88]
per-ex loss: 0.691464  [   22/   88]
per-ex loss: 0.521336  [   23/   88]
per-ex loss: 0.622830  [   24/   88]
per-ex loss: 0.728198  [   25/   88]
per-ex loss: 0.587657  [   26/   88]
per-ex loss: 0.587800  [   27/   88]
per-ex loss: 0.628891  [   28/   88]
per-ex loss: 0.732349  [   29/   88]
per-ex loss: 0.594873  [   30/   88]
per-ex loss: 0.506215  [   31/   88]
per-ex loss: 0.453194  [   32/   88]
per-ex loss: 0.726790  [   33/   88]
per-ex loss: 0.714284  [   34/   88]
per-ex loss: 0.565081  [   35/   88]
per-ex loss: 0.672395  [   36/   88]
per-ex loss: 0.608603  [   37/   88]
per-ex loss: 0.709067  [   38/   88]
per-ex loss: 0.702748  [   39/   88]
per-ex loss: 0.554913  [   40/   88]
per-ex loss: 0.727051  [   41/   88]
per-ex loss: 0.490877  [   42/   88]
per-ex loss: 0.512434  [   43/   88]
per-ex loss: 0.790649  [   44/   88]
per-ex loss: 0.483595  [   45/   88]
per-ex loss: 0.540666  [   46/   88]
per-ex loss: 0.766340  [   47/   88]
per-ex loss: 0.514989  [   48/   88]
per-ex loss: 0.641059  [   49/   88]
per-ex loss: 0.632351  [   50/   88]
per-ex loss: 0.466910  [   51/   88]
per-ex loss: 0.709200  [   52/   88]
per-ex loss: 0.463722  [   53/   88]
per-ex loss: 0.491702  [   54/   88]
per-ex loss: 0.679704  [   55/   88]
per-ex loss: 0.500514  [   56/   88]
per-ex loss: 0.736577  [   57/   88]
per-ex loss: 0.793259  [   58/   88]
per-ex loss: 0.555461  [   59/   88]
per-ex loss: 0.739650  [   60/   88]
per-ex loss: 0.685017  [   61/   88]
per-ex loss: 0.604499  [   62/   88]
per-ex loss: 0.710378  [   63/   88]
per-ex loss: 0.789675  [   64/   88]
per-ex loss: 0.648147  [   65/   88]
per-ex loss: 0.566181  [   66/   88]
per-ex loss: 0.535845  [   67/   88]
per-ex loss: 0.667895  [   68/   88]
per-ex loss: 0.390777  [   69/   88]
per-ex loss: 0.565173  [   70/   88]
per-ex loss: 0.799418  [   71/   88]
per-ex loss: 0.605874  [   72/   88]
per-ex loss: 0.800185  [   73/   88]
per-ex loss: 0.705446  [   74/   88]
per-ex loss: 0.770068  [   75/   88]
per-ex loss: 0.532694  [   76/   88]
per-ex loss: 0.704502  [   77/   88]
per-ex loss: 0.463427  [   78/   88]
per-ex loss: 0.555899  [   79/   88]
per-ex loss: 0.557114  [   80/   88]
per-ex loss: 0.481933  [   81/   88]
per-ex loss: 0.560176  [   82/   88]
per-ex loss: 0.498546  [   83/   88]
per-ex loss: 0.781731  [   84/   88]
per-ex loss: 0.668515  [   85/   88]
per-ex loss: 0.527363  [   86/   88]
per-ex loss: 0.582270  [   87/   88]
per-ex loss: 0.703474  [   88/   88]
Train Error: Avg loss: 0.61697840
validation Error: 
 Avg loss: 0.69373288 
 F1: 0.486725 
 Precision: 0.553200 
 Recall: 0.434513
 IoU: 0.321637

test Error: 
 Avg loss: 0.63958544 
 F1: 0.569178 
 Precision: 0.644555 
 Recall: 0.509585
 IoU: 0.397798

We have finished training iteration 97
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_95_.pth
per-ex loss: 0.675527  [    1/   88]
per-ex loss: 0.452975  [    2/   88]
per-ex loss: 0.774211  [    3/   88]
per-ex loss: 0.578842  [    4/   88]
per-ex loss: 0.506222  [    5/   88]
per-ex loss: 0.651248  [    6/   88]
per-ex loss: 0.508684  [    7/   88]
per-ex loss: 0.559287  [    8/   88]
per-ex loss: 0.773910  [    9/   88]
per-ex loss: 0.745122  [   10/   88]
per-ex loss: 0.672577  [   11/   88]
per-ex loss: 0.580066  [   12/   88]
per-ex loss: 0.716639  [   13/   88]
per-ex loss: 0.572918  [   14/   88]
per-ex loss: 0.634010  [   15/   88]
per-ex loss: 0.465462  [   16/   88]
per-ex loss: 0.716315  [   17/   88]
per-ex loss: 0.435526  [   18/   88]
per-ex loss: 0.557787  [   19/   88]
per-ex loss: 0.538382  [   20/   88]
per-ex loss: 0.719098  [   21/   88]
per-ex loss: 0.682727  [   22/   88]
per-ex loss: 0.748446  [   23/   88]
per-ex loss: 0.476687  [   24/   88]
per-ex loss: 0.691210  [   25/   88]
per-ex loss: 0.592260  [   26/   88]
per-ex loss: 0.504690  [   27/   88]
per-ex loss: 0.547383  [   28/   88]
per-ex loss: 0.725762  [   29/   88]
per-ex loss: 0.512045  [   30/   88]
per-ex loss: 0.714186  [   31/   88]
per-ex loss: 0.588769  [   32/   88]
per-ex loss: 0.550104  [   33/   88]
per-ex loss: 0.767479  [   34/   88]
per-ex loss: 0.584504  [   35/   88]
per-ex loss: 0.502423  [   36/   88]
per-ex loss: 0.697188  [   37/   88]
per-ex loss: 0.645901  [   38/   88]
per-ex loss: 0.604589  [   39/   88]
per-ex loss: 0.638219  [   40/   88]
per-ex loss: 0.658240  [   41/   88]
per-ex loss: 0.542055  [   42/   88]
per-ex loss: 0.761940  [   43/   88]
per-ex loss: 0.530368  [   44/   88]
per-ex loss: 0.658957  [   45/   88]
per-ex loss: 0.512940  [   46/   88]
per-ex loss: 0.623135  [   47/   88]
per-ex loss: 0.516656  [   48/   88]
per-ex loss: 0.602699  [   49/   88]
per-ex loss: 0.546642  [   50/   88]
per-ex loss: 0.559879  [   51/   88]
per-ex loss: 0.546169  [   52/   88]
per-ex loss: 0.732943  [   53/   88]
per-ex loss: 0.739089  [   54/   88]
per-ex loss: 0.485376  [   55/   88]
per-ex loss: 0.701624  [   56/   88]
per-ex loss: 0.556850  [   57/   88]
per-ex loss: 0.449540  [   58/   88]
per-ex loss: 0.641581  [   59/   88]
per-ex loss: 0.690488  [   60/   88]
per-ex loss: 0.552529  [   61/   88]
per-ex loss: 0.552186  [   62/   88]
per-ex loss: 0.478809  [   63/   88]
per-ex loss: 0.477143  [   64/   88]
per-ex loss: 0.634861  [   65/   88]
per-ex loss: 0.733373  [   66/   88]
per-ex loss: 0.491297  [   67/   88]
per-ex loss: 0.687286  [   68/   88]
per-ex loss: 0.765383  [   69/   88]
per-ex loss: 0.614049  [   70/   88]
per-ex loss: 0.692681  [   71/   88]
per-ex loss: 0.635727  [   72/   88]
per-ex loss: 0.553347  [   73/   88]
per-ex loss: 0.508637  [   74/   88]
per-ex loss: 0.792184  [   75/   88]
per-ex loss: 0.511277  [   76/   88]
per-ex loss: 0.799405  [   77/   88]
per-ex loss: 0.723647  [   78/   88]
per-ex loss: 0.546733  [   79/   88]
per-ex loss: 0.687976  [   80/   88]
per-ex loss: 0.663924  [   81/   88]
per-ex loss: 0.573782  [   82/   88]
per-ex loss: 0.516619  [   83/   88]
per-ex loss: 0.735101  [   84/   88]
per-ex loss: 0.743135  [   85/   88]
per-ex loss: 0.530103  [   86/   88]
per-ex loss: 0.781957  [   87/   88]
per-ex loss: 0.446641  [   88/   88]
Train Error: Avg loss: 0.61473116
validation Error: 
 Avg loss: 0.70219475 
 F1: 0.479851 
 Precision: 0.481166 
 Recall: 0.478543
 IoU: 0.315660

test Error: 
 Avg loss: 0.63611456 
 F1: 0.575468 
 Precision: 0.583764 
 Recall: 0.567405
 IoU: 0.403970

We have finished training iteration 98
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_96_.pth
per-ex loss: 0.662274  [    1/   88]
per-ex loss: 0.500065  [    2/   88]
per-ex loss: 0.560050  [    3/   88]
per-ex loss: 0.648790  [    4/   88]
per-ex loss: 0.748187  [    5/   88]
per-ex loss: 0.504269  [    6/   88]
per-ex loss: 0.467932  [    7/   88]
per-ex loss: 0.592988  [    8/   88]
per-ex loss: 0.772709  [    9/   88]
per-ex loss: 0.604219  [   10/   88]
per-ex loss: 0.590964  [   11/   88]
per-ex loss: 0.585028  [   12/   88]
per-ex loss: 0.710468  [   13/   88]
per-ex loss: 0.628362  [   14/   88]
per-ex loss: 0.466403  [   15/   88]
per-ex loss: 0.514538  [   16/   88]
per-ex loss: 0.571000  [   17/   88]
per-ex loss: 0.489809  [   18/   88]
per-ex loss: 0.618838  [   19/   88]
per-ex loss: 0.628879  [   20/   88]
per-ex loss: 0.617181  [   21/   88]
per-ex loss: 0.660481  [   22/   88]
per-ex loss: 0.691078  [   23/   88]
per-ex loss: 0.537617  [   24/   88]
per-ex loss: 0.501533  [   25/   88]
per-ex loss: 0.755835  [   26/   88]
per-ex loss: 0.724863  [   27/   88]
per-ex loss: 0.529631  [   28/   88]
per-ex loss: 0.772792  [   29/   88]
per-ex loss: 0.798092  [   30/   88]
per-ex loss: 0.666572  [   31/   88]
per-ex loss: 0.568988  [   32/   88]
per-ex loss: 0.524411  [   33/   88]
per-ex loss: 0.764357  [   34/   88]
per-ex loss: 0.648535  [   35/   88]
per-ex loss: 0.589846  [   36/   88]
per-ex loss: 0.705878  [   37/   88]
per-ex loss: 0.727925  [   38/   88]
per-ex loss: 0.791269  [   39/   88]
per-ex loss: 0.508417  [   40/   88]
per-ex loss: 0.547194  [   41/   88]
per-ex loss: 0.495196  [   42/   88]
per-ex loss: 0.561964  [   43/   88]
per-ex loss: 0.714116  [   44/   88]
per-ex loss: 0.758661  [   45/   88]
per-ex loss: 0.543325  [   46/   88]
per-ex loss: 0.700638  [   47/   88]
per-ex loss: 0.519190  [   48/   88]
per-ex loss: 0.789699  [   49/   88]
per-ex loss: 0.718703  [   50/   88]
per-ex loss: 0.583737  [   51/   88]
per-ex loss: 0.681326  [   52/   88]
per-ex loss: 0.666984  [   53/   88]
per-ex loss: 0.731336  [   54/   88]
per-ex loss: 0.700897  [   55/   88]
per-ex loss: 0.692402  [   56/   88]
per-ex loss: 0.721113  [   57/   88]
per-ex loss: 0.500690  [   58/   88]
per-ex loss: 0.504281  [   59/   88]
per-ex loss: 0.540816  [   60/   88]
per-ex loss: 0.718927  [   61/   88]
per-ex loss: 0.540696  [   62/   88]
per-ex loss: 0.744132  [   63/   88]
per-ex loss: 0.418381  [   64/   88]
per-ex loss: 0.540820  [   65/   88]
per-ex loss: 0.513945  [   66/   88]
per-ex loss: 0.570370  [   67/   88]
per-ex loss: 0.526101  [   68/   88]
per-ex loss: 0.589519  [   69/   88]
per-ex loss: 0.551726  [   70/   88]
per-ex loss: 0.666879  [   71/   88]
per-ex loss: 0.663954  [   72/   88]
per-ex loss: 0.607635  [   73/   88]
per-ex loss: 0.700196  [   74/   88]
per-ex loss: 0.511133  [   75/   88]
per-ex loss: 0.556406  [   76/   88]
per-ex loss: 0.691166  [   77/   88]
per-ex loss: 0.779853  [   78/   88]
per-ex loss: 0.609979  [   79/   88]
per-ex loss: 0.777468  [   80/   88]
per-ex loss: 0.740977  [   81/   88]
per-ex loss: 0.524980  [   82/   88]
per-ex loss: 0.549290  [   83/   88]
per-ex loss: 0.539735  [   84/   88]
per-ex loss: 0.735434  [   85/   88]
per-ex loss: 0.500837  [   86/   88]
per-ex loss: 0.742630  [   87/   88]
per-ex loss: 0.671456  [   88/   88]
Train Error: Avg loss: 0.62395413
validation Error: 
 Avg loss: 0.69598482 
 F1: 0.489151 
 Precision: 0.551914 
 Recall: 0.439206
 IoU: 0.323759

test Error: 
 Avg loss: 0.63932314 
 F1: 0.570436 
 Precision: 0.631656 
 Recall: 0.520035
 IoU: 0.399028

We have finished training iteration 99
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_97_.pth
per-ex loss: 0.533458  [    1/   88]
per-ex loss: 0.775637  [    2/   88]
per-ex loss: 0.498776  [    3/   88]
per-ex loss: 0.499919  [    4/   88]
per-ex loss: 0.740957  [    5/   88]
per-ex loss: 0.588939  [    6/   88]
per-ex loss: 0.771296  [    7/   88]
per-ex loss: 0.691094  [    8/   88]
per-ex loss: 0.574005  [    9/   88]
per-ex loss: 0.462530  [   10/   88]
per-ex loss: 0.569251  [   11/   88]
per-ex loss: 0.462441  [   12/   88]
per-ex loss: 0.443256  [   13/   88]
per-ex loss: 0.460254  [   14/   88]
per-ex loss: 0.524772  [   15/   88]
per-ex loss: 0.532996  [   16/   88]
per-ex loss: 0.783043  [   17/   88]
per-ex loss: 0.587176  [   18/   88]
per-ex loss: 0.625753  [   19/   88]
per-ex loss: 0.587988  [   20/   88]
per-ex loss: 0.607173  [   21/   88]
per-ex loss: 0.536457  [   22/   88]
per-ex loss: 0.677142  [   23/   88]
per-ex loss: 0.537802  [   24/   88]
per-ex loss: 0.437040  [   25/   88]
per-ex loss: 0.758009  [   26/   88]
per-ex loss: 0.612730  [   27/   88]
per-ex loss: 0.513866  [   28/   88]
per-ex loss: 0.745119  [   29/   88]
per-ex loss: 0.555901  [   30/   88]
per-ex loss: 0.658692  [   31/   88]
per-ex loss: 0.482613  [   32/   88]
per-ex loss: 0.672217  [   33/   88]
per-ex loss: 0.725221  [   34/   88]
per-ex loss: 0.628555  [   35/   88]
per-ex loss: 0.699089  [   36/   88]
per-ex loss: 0.688771  [   37/   88]
per-ex loss: 0.500768  [   38/   88]
per-ex loss: 0.634716  [   39/   88]
per-ex loss: 0.823073  [   40/   88]
per-ex loss: 0.721536  [   41/   88]
per-ex loss: 0.699419  [   42/   88]
per-ex loss: 0.679239  [   43/   88]
per-ex loss: 0.587195  [   44/   88]
per-ex loss: 0.553884  [   45/   88]
per-ex loss: 0.707645  [   46/   88]
per-ex loss: 0.648927  [   47/   88]
per-ex loss: 0.709070  [   48/   88]
per-ex loss: 0.718294  [   49/   88]
per-ex loss: 0.597871  [   50/   88]
per-ex loss: 0.526301  [   51/   88]
per-ex loss: 0.780401  [   52/   88]
per-ex loss: 0.660219  [   53/   88]
per-ex loss: 0.713862  [   54/   88]
per-ex loss: 0.550835  [   55/   88]
per-ex loss: 0.571367  [   56/   88]
per-ex loss: 0.515462  [   57/   88]
per-ex loss: 0.717334  [   58/   88]
per-ex loss: 0.658097  [   59/   88]
per-ex loss: 0.783091  [   60/   88]
per-ex loss: 0.671259  [   61/   88]
per-ex loss: 0.677091  [   62/   88]
per-ex loss: 0.626712  [   63/   88]
per-ex loss: 0.537647  [   64/   88]
per-ex loss: 0.543164  [   65/   88]
per-ex loss: 0.514451  [   66/   88]
per-ex loss: 0.802226  [   67/   88]
per-ex loss: 0.580826  [   68/   88]
per-ex loss: 0.518090  [   69/   88]
per-ex loss: 0.727944  [   70/   88]
per-ex loss: 0.488047  [   71/   88]
per-ex loss: 0.823248  [   72/   88]
per-ex loss: 0.573293  [   73/   88]
per-ex loss: 0.584394  [   74/   88]
per-ex loss: 0.495052  [   75/   88]
per-ex loss: 0.785219  [   76/   88]
per-ex loss: 0.450437  [   77/   88]
per-ex loss: 0.497923  [   78/   88]
per-ex loss: 0.601625  [   79/   88]
per-ex loss: 0.747005  [   80/   88]
per-ex loss: 0.734260  [   81/   88]
per-ex loss: 0.490583  [   82/   88]
per-ex loss: 0.732099  [   83/   88]
per-ex loss: 0.582618  [   84/   88]
per-ex loss: 0.629901  [   85/   88]
per-ex loss: 0.577254  [   86/   88]
per-ex loss: 0.649177  [   87/   88]
per-ex loss: 0.700191  [   88/   88]
Train Error: Avg loss: 0.62102593
validation Error: 
 Avg loss: 0.69481964 
 F1: 0.492637 
 Precision: 0.601947 
 Recall: 0.416926
 IoU: 0.326821

test Error: 
 Avg loss: 0.64112878 
 F1: 0.567138 
 Precision: 0.680608 
 Recall: 0.486098
 IoU: 0.395808

We have finished training iteration 100
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_98_.pth
per-ex loss: 0.646170  [    1/   88]
per-ex loss: 0.776068  [    2/   88]
per-ex loss: 0.508436  [    3/   88]
per-ex loss: 0.489591  [    4/   88]
per-ex loss: 0.684719  [    5/   88]
per-ex loss: 0.445434  [    6/   88]
per-ex loss: 0.654304  [    7/   88]
per-ex loss: 0.549901  [    8/   88]
per-ex loss: 0.601989  [    9/   88]
per-ex loss: 0.679866  [   10/   88]
per-ex loss: 0.567657  [   11/   88]
per-ex loss: 0.500888  [   12/   88]
per-ex loss: 0.783195  [   13/   88]
per-ex loss: 0.557375  [   14/   88]
per-ex loss: 0.859789  [   15/   88]
per-ex loss: 0.543873  [   16/   88]
per-ex loss: 0.542816  [   17/   88]
per-ex loss: 0.659711  [   18/   88]
per-ex loss: 0.729200  [   19/   88]
per-ex loss: 0.721855  [   20/   88]
per-ex loss: 0.501284  [   21/   88]
per-ex loss: 0.607997  [   22/   88]
per-ex loss: 0.541685  [   23/   88]
per-ex loss: 0.754376  [   24/   88]
per-ex loss: 0.550567  [   25/   88]
per-ex loss: 0.610346  [   26/   88]
per-ex loss: 0.491094  [   27/   88]
per-ex loss: 0.581189  [   28/   88]
per-ex loss: 0.743339  [   29/   88]
per-ex loss: 0.741468  [   30/   88]
per-ex loss: 0.470151  [   31/   88]
per-ex loss: 0.691460  [   32/   88]
per-ex loss: 0.660010  [   33/   88]
per-ex loss: 0.598242  [   34/   88]
per-ex loss: 0.486909  [   35/   88]
per-ex loss: 0.687510  [   36/   88]
per-ex loss: 0.759044  [   37/   88]
per-ex loss: 0.701789  [   38/   88]
per-ex loss: 0.396319  [   39/   88]
per-ex loss: 0.630176  [   40/   88]
per-ex loss: 0.732632  [   41/   88]
per-ex loss: 0.784961  [   42/   88]
per-ex loss: 0.576712  [   43/   88]
per-ex loss: 0.461302  [   44/   88]
per-ex loss: 0.538108  [   45/   88]
per-ex loss: 0.507448  [   46/   88]
per-ex loss: 0.609402  [   47/   88]
per-ex loss: 0.804667  [   48/   88]
per-ex loss: 0.542208  [   49/   88]
per-ex loss: 0.659485  [   50/   88]
per-ex loss: 0.480653  [   51/   88]
per-ex loss: 0.582680  [   52/   88]
per-ex loss: 0.664315  [   53/   88]
per-ex loss: 0.698540  [   54/   88]
per-ex loss: 0.526594  [   55/   88]
per-ex loss: 0.736859  [   56/   88]
per-ex loss: 0.586137  [   57/   88]
per-ex loss: 0.566203  [   58/   88]
per-ex loss: 0.497173  [   59/   88]
per-ex loss: 0.525193  [   60/   88]
per-ex loss: 0.568221  [   61/   88]
per-ex loss: 0.528489  [   62/   88]
per-ex loss: 0.535275  [   63/   88]
per-ex loss: 0.719948  [   64/   88]
per-ex loss: 0.554542  [   65/   88]
per-ex loss: 0.722358  [   66/   88]
per-ex loss: 0.665338  [   67/   88]
per-ex loss: 0.694421  [   68/   88]
per-ex loss: 0.569145  [   69/   88]
per-ex loss: 0.586896  [   70/   88]
per-ex loss: 0.552639  [   71/   88]
per-ex loss: 0.529755  [   72/   88]
per-ex loss: 0.498219  [   73/   88]
per-ex loss: 0.783053  [   74/   88]
per-ex loss: 0.703202  [   75/   88]
per-ex loss: 0.717001  [   76/   88]
per-ex loss: 0.716564  [   77/   88]
per-ex loss: 0.689951  [   78/   88]
per-ex loss: 0.549177  [   79/   88]
per-ex loss: 0.654364  [   80/   88]
per-ex loss: 0.626980  [   81/   88]
per-ex loss: 0.746934  [   82/   88]
per-ex loss: 0.782878  [   83/   88]
per-ex loss: 0.553621  [   84/   88]
per-ex loss: 0.574661  [   85/   88]
per-ex loss: 0.468147  [   86/   88]
per-ex loss: 0.615434  [   87/   88]
per-ex loss: 0.528203  [   88/   88]
Train Error: Avg loss: 0.61618729
validation Error: 
 Avg loss: 0.69864355 
 F1: 0.486252 
 Precision: 0.521369 
 Recall: 0.455567
 IoU: 0.321224

test Error: 
 Avg loss: 0.63864018 
 F1: 0.574329 
 Precision: 0.607075 
 Recall: 0.544935
 IoU: 0.402848

We have finished training iteration 101
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_99_.pth
per-ex loss: 0.497459  [    1/   88]
per-ex loss: 0.663879  [    2/   88]
per-ex loss: 0.704684  [    3/   88]
per-ex loss: 0.789133  [    4/   88]
per-ex loss: 0.502648  [    5/   88]
per-ex loss: 0.597643  [    6/   88]
per-ex loss: 0.615304  [    7/   88]
per-ex loss: 0.500395  [    8/   88]
per-ex loss: 0.545022  [    9/   88]
per-ex loss: 0.754647  [   10/   88]
per-ex loss: 0.507257  [   11/   88]
per-ex loss: 0.447110  [   12/   88]
per-ex loss: 0.660307  [   13/   88]
per-ex loss: 0.772646  [   14/   88]
per-ex loss: 0.721655  [   15/   88]
per-ex loss: 0.758806  [   16/   88]
per-ex loss: 0.611690  [   17/   88]
per-ex loss: 0.568859  [   18/   88]
per-ex loss: 0.605829  [   19/   88]
per-ex loss: 0.521780  [   20/   88]
per-ex loss: 0.595549  [   21/   88]
per-ex loss: 0.795630  [   22/   88]
per-ex loss: 0.641434  [   23/   88]
per-ex loss: 0.646995  [   24/   88]
per-ex loss: 0.635882  [   25/   88]
per-ex loss: 0.733975  [   26/   88]
per-ex loss: 0.572039  [   27/   88]
per-ex loss: 0.741654  [   28/   88]
per-ex loss: 0.493515  [   29/   88]
per-ex loss: 0.584505  [   30/   88]
per-ex loss: 0.531992  [   31/   88]
per-ex loss: 0.599274  [   32/   88]
per-ex loss: 0.698832  [   33/   88]
per-ex loss: 0.479521  [   34/   88]
per-ex loss: 0.737093  [   35/   88]
per-ex loss: 0.466950  [   36/   88]
per-ex loss: 0.557922  [   37/   88]
per-ex loss: 0.740339  [   38/   88]
per-ex loss: 0.531176  [   39/   88]
per-ex loss: 0.565469  [   40/   88]
per-ex loss: 0.682459  [   41/   88]
per-ex loss: 0.794596  [   42/   88]
per-ex loss: 0.526150  [   43/   88]
per-ex loss: 0.771481  [   44/   88]
per-ex loss: 0.607858  [   45/   88]
per-ex loss: 0.535301  [   46/   88]
per-ex loss: 0.503353  [   47/   88]
per-ex loss: 0.392896  [   48/   88]
per-ex loss: 0.554026  [   49/   88]
per-ex loss: 0.462041  [   50/   88]
per-ex loss: 0.691761  [   51/   88]
per-ex loss: 0.542631  [   52/   88]
per-ex loss: 0.677820  [   53/   88]
per-ex loss: 0.618713  [   54/   88]
per-ex loss: 0.564360  [   55/   88]
per-ex loss: 0.682674  [   56/   88]
per-ex loss: 0.734845  [   57/   88]
per-ex loss: 0.622757  [   58/   88]
per-ex loss: 0.793821  [   59/   88]
per-ex loss: 0.726567  [   60/   88]
per-ex loss: 0.743728  [   61/   88]
per-ex loss: 0.729258  [   62/   88]
per-ex loss: 0.518826  [   63/   88]
per-ex loss: 0.555826  [   64/   88]
per-ex loss: 0.722771  [   65/   88]
per-ex loss: 0.573603  [   66/   88]
per-ex loss: 0.755870  [   67/   88]
per-ex loss: 0.719135  [   68/   88]
per-ex loss: 0.533090  [   69/   88]
per-ex loss: 0.574229  [   70/   88]
per-ex loss: 0.644615  [   71/   88]
per-ex loss: 0.713266  [   72/   88]
per-ex loss: 0.617487  [   73/   88]
per-ex loss: 0.739129  [   74/   88]
per-ex loss: 0.710119  [   75/   88]
per-ex loss: 0.671497  [   76/   88]
per-ex loss: 0.549121  [   77/   88]
per-ex loss: 0.498088  [   78/   88]
per-ex loss: 0.548407  [   79/   88]
per-ex loss: 0.484489  [   80/   88]
per-ex loss: 0.546908  [   81/   88]
per-ex loss: 0.520758  [   82/   88]
per-ex loss: 0.525863  [   83/   88]
per-ex loss: 0.520911  [   84/   88]
per-ex loss: 0.469009  [   85/   88]
per-ex loss: 0.596020  [   86/   88]
per-ex loss: 0.500303  [   87/   88]
per-ex loss: 0.653924  [   88/   88]
Train Error: Avg loss: 0.61498704
validation Error: 
 Avg loss: 0.70815529 
 F1: 0.473435 
 Precision: 0.516199 
 Recall: 0.437215
 IoU: 0.310131

test Error: 
 Avg loss: 0.63741444 
 F1: 0.569366 
 Precision: 0.638354 
 Recall: 0.513834
 IoU: 0.397981

We have finished training iteration 102
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_100_.pth
per-ex loss: 0.490446  [    1/   88]
per-ex loss: 0.643306  [    2/   88]
per-ex loss: 0.530946  [    3/   88]
per-ex loss: 0.785999  [    4/   88]
per-ex loss: 0.725851  [    5/   88]
per-ex loss: 0.573159  [    6/   88]
per-ex loss: 0.662811  [    7/   88]
per-ex loss: 0.728462  [    8/   88]
per-ex loss: 0.852511  [    9/   88]
per-ex loss: 0.622604  [   10/   88]
per-ex loss: 0.611000  [   11/   88]
per-ex loss: 0.639354  [   12/   88]
per-ex loss: 0.568879  [   13/   88]
per-ex loss: 0.746806  [   14/   88]
per-ex loss: 0.653912  [   15/   88]
per-ex loss: 0.516621  [   16/   88]
per-ex loss: 0.692701  [   17/   88]
per-ex loss: 0.703799  [   18/   88]
per-ex loss: 0.623037  [   19/   88]
per-ex loss: 0.651165  [   20/   88]
per-ex loss: 0.639868  [   21/   88]
per-ex loss: 0.596612  [   22/   88]
per-ex loss: 0.695618  [   23/   88]
per-ex loss: 0.649300  [   24/   88]
per-ex loss: 0.568021  [   25/   88]
per-ex loss: 0.528866  [   26/   88]
per-ex loss: 0.526054  [   27/   88]
per-ex loss: 0.678679  [   28/   88]
per-ex loss: 0.642739  [   29/   88]
per-ex loss: 0.548451  [   30/   88]
per-ex loss: 0.543069  [   31/   88]
per-ex loss: 0.738705  [   32/   88]
per-ex loss: 0.700876  [   33/   88]
per-ex loss: 0.792995  [   34/   88]
per-ex loss: 0.594446  [   35/   88]
per-ex loss: 0.749344  [   36/   88]
per-ex loss: 0.555466  [   37/   88]
per-ex loss: 0.572827  [   38/   88]
per-ex loss: 0.499693  [   39/   88]
per-ex loss: 0.802294  [   40/   88]
per-ex loss: 0.690168  [   41/   88]
per-ex loss: 0.454874  [   42/   88]
per-ex loss: 0.563308  [   43/   88]
per-ex loss: 0.564921  [   44/   88]
per-ex loss: 0.535940  [   45/   88]
per-ex loss: 0.724177  [   46/   88]
per-ex loss: 0.790905  [   47/   88]
per-ex loss: 0.580753  [   48/   88]
per-ex loss: 0.495751  [   49/   88]
per-ex loss: 0.620366  [   50/   88]
per-ex loss: 0.485068  [   51/   88]
per-ex loss: 0.534485  [   52/   88]
per-ex loss: 0.578237  [   53/   88]
per-ex loss: 0.562685  [   54/   88]
per-ex loss: 0.450555  [   55/   88]
per-ex loss: 0.527072  [   56/   88]
per-ex loss: 0.742656  [   57/   88]
per-ex loss: 0.681682  [   58/   88]
per-ex loss: 0.641952  [   59/   88]
per-ex loss: 0.507804  [   60/   88]
per-ex loss: 0.766699  [   61/   88]
per-ex loss: 0.558802  [   62/   88]
per-ex loss: 0.504118  [   63/   88]
per-ex loss: 0.736630  [   64/   88]
per-ex loss: 0.527809  [   65/   88]
per-ex loss: 0.751670  [   66/   88]
per-ex loss: 0.518017  [   67/   88]
per-ex loss: 0.678107  [   68/   88]
per-ex loss: 0.543265  [   69/   88]
per-ex loss: 0.457813  [   70/   88]
per-ex loss: 0.741064  [   71/   88]
per-ex loss: 0.689801  [   72/   88]
per-ex loss: 0.702592  [   73/   88]
per-ex loss: 0.548615  [   74/   88]
per-ex loss: 0.481703  [   75/   88]
per-ex loss: 0.736064  [   76/   88]
per-ex loss: 0.733593  [   77/   88]
per-ex loss: 0.537575  [   78/   88]
per-ex loss: 0.721496  [   79/   88]
per-ex loss: 0.531259  [   80/   88]
per-ex loss: 0.524708  [   81/   88]
per-ex loss: 0.553936  [   82/   88]
per-ex loss: 0.690952  [   83/   88]
per-ex loss: 0.490202  [   84/   88]
per-ex loss: 0.481884  [   85/   88]
per-ex loss: 0.635225  [   86/   88]
per-ex loss: 0.520865  [   87/   88]
per-ex loss: 0.368110  [   88/   88]
Train Error: Avg loss: 0.61492302
validation Error: 
 Avg loss: 0.68789701 
 F1: 0.498078 
 Precision: 0.612805 
 Recall: 0.419535
 IoU: 0.331627

test Error: 
 Avg loss: 0.64370230 
 F1: 0.565883 
 Precision: 0.672116 
 Recall: 0.488649
 IoU: 0.394587

We have finished training iteration 103
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_101_.pth
per-ex loss: 0.503915  [    1/   88]
per-ex loss: 0.663354  [    2/   88]
per-ex loss: 0.764944  [    3/   88]
per-ex loss: 0.458199  [    4/   88]
per-ex loss: 0.528057  [    5/   88]
per-ex loss: 0.698372  [    6/   88]
per-ex loss: 0.555235  [    7/   88]
per-ex loss: 0.532277  [    8/   88]
per-ex loss: 0.744729  [    9/   88]
per-ex loss: 0.561154  [   10/   88]
per-ex loss: 0.591740  [   11/   88]
per-ex loss: 0.512500  [   12/   88]
per-ex loss: 0.564418  [   13/   88]
per-ex loss: 0.538337  [   14/   88]
per-ex loss: 0.468288  [   15/   88]
per-ex loss: 0.782947  [   16/   88]
per-ex loss: 0.628444  [   17/   88]
per-ex loss: 0.480874  [   18/   88]
per-ex loss: 0.788486  [   19/   88]
per-ex loss: 0.559832  [   20/   88]
per-ex loss: 0.569508  [   21/   88]
per-ex loss: 0.542791  [   22/   88]
per-ex loss: 0.702786  [   23/   88]
per-ex loss: 0.652609  [   24/   88]
per-ex loss: 0.565509  [   25/   88]
per-ex loss: 0.435592  [   26/   88]
per-ex loss: 0.490999  [   27/   88]
per-ex loss: 0.535304  [   28/   88]
per-ex loss: 0.520935  [   29/   88]
per-ex loss: 0.621935  [   30/   88]
per-ex loss: 0.721556  [   31/   88]
per-ex loss: 0.714442  [   32/   88]
per-ex loss: 0.544640  [   33/   88]
per-ex loss: 0.579941  [   34/   88]
per-ex loss: 0.596026  [   35/   88]
per-ex loss: 0.778515  [   36/   88]
per-ex loss: 0.492504  [   37/   88]
per-ex loss: 0.518435  [   38/   88]
per-ex loss: 0.617983  [   39/   88]
per-ex loss: 0.744834  [   40/   88]
per-ex loss: 0.565940  [   41/   88]
per-ex loss: 0.500794  [   42/   88]
per-ex loss: 0.736494  [   43/   88]
per-ex loss: 0.719506  [   44/   88]
per-ex loss: 0.643236  [   45/   88]
per-ex loss: 0.722148  [   46/   88]
per-ex loss: 0.554275  [   47/   88]
per-ex loss: 0.783531  [   48/   88]
per-ex loss: 0.577883  [   49/   88]
per-ex loss: 0.635135  [   50/   88]
per-ex loss: 0.598060  [   51/   88]
per-ex loss: 0.738768  [   52/   88]
per-ex loss: 0.534711  [   53/   88]
per-ex loss: 0.566957  [   54/   88]
per-ex loss: 0.729401  [   55/   88]
per-ex loss: 0.678091  [   56/   88]
per-ex loss: 0.418666  [   57/   88]
per-ex loss: 0.718739  [   58/   88]
per-ex loss: 0.474628  [   59/   88]
per-ex loss: 0.457950  [   60/   88]
per-ex loss: 0.658550  [   61/   88]
per-ex loss: 0.690861  [   62/   88]
per-ex loss: 0.669454  [   63/   88]
per-ex loss: 0.554466  [   64/   88]
per-ex loss: 0.683815  [   65/   88]
per-ex loss: 0.526045  [   66/   88]
per-ex loss: 0.678609  [   67/   88]
per-ex loss: 0.577439  [   68/   88]
per-ex loss: 0.486686  [   69/   88]
per-ex loss: 0.558627  [   70/   88]
per-ex loss: 0.503885  [   71/   88]
per-ex loss: 0.697456  [   72/   88]
per-ex loss: 0.790017  [   73/   88]
per-ex loss: 0.693972  [   74/   88]
per-ex loss: 0.661134  [   75/   88]
per-ex loss: 0.762058  [   76/   88]
per-ex loss: 0.762597  [   77/   88]
per-ex loss: 0.539684  [   78/   88]
per-ex loss: 0.533060  [   79/   88]
per-ex loss: 0.656939  [   80/   88]
per-ex loss: 0.524922  [   81/   88]
per-ex loss: 0.721781  [   82/   88]
per-ex loss: 0.671574  [   83/   88]
per-ex loss: 0.697109  [   84/   88]
per-ex loss: 0.602186  [   85/   88]
per-ex loss: 0.527634  [   86/   88]
per-ex loss: 0.736417  [   87/   88]
per-ex loss: 0.657706  [   88/   88]
Train Error: Avg loss: 0.61423348
validation Error: 
 Avg loss: 0.69253271 
 F1: 0.497108 
 Precision: 0.577207 
 Recall: 0.436530
 IoU: 0.330767

test Error: 
 Avg loss: 0.63545280 
 F1: 0.574905 
 Precision: 0.651574 
 Recall: 0.514380
 IoU: 0.403416

We have finished training iteration 104
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_102_.pth
per-ex loss: 0.500440  [    1/   88]
per-ex loss: 0.502537  [    2/   88]
per-ex loss: 0.697966  [    3/   88]
per-ex loss: 0.509301  [    4/   88]
per-ex loss: 0.648347  [    5/   88]
per-ex loss: 0.568056  [    6/   88]
per-ex loss: 0.565455  [    7/   88]
per-ex loss: 0.783168  [    8/   88]
per-ex loss: 0.490261  [    9/   88]
per-ex loss: 0.520198  [   10/   88]
per-ex loss: 0.651542  [   11/   88]
per-ex loss: 0.789805  [   12/   88]
per-ex loss: 0.731581  [   13/   88]
per-ex loss: 0.688751  [   14/   88]
per-ex loss: 0.761467  [   15/   88]
per-ex loss: 0.646111  [   16/   88]
per-ex loss: 0.697646  [   17/   88]
per-ex loss: 0.495622  [   18/   88]
per-ex loss: 0.655251  [   19/   88]
per-ex loss: 0.532777  [   20/   88]
per-ex loss: 0.618877  [   21/   88]
per-ex loss: 0.548356  [   22/   88]
per-ex loss: 0.714721  [   23/   88]
per-ex loss: 0.685553  [   24/   88]
per-ex loss: 0.732767  [   25/   88]
per-ex loss: 0.531063  [   26/   88]
per-ex loss: 0.699009  [   27/   88]
per-ex loss: 0.737968  [   28/   88]
per-ex loss: 0.634253  [   29/   88]
per-ex loss: 0.533962  [   30/   88]
per-ex loss: 0.550861  [   31/   88]
per-ex loss: 0.592907  [   32/   88]
per-ex loss: 0.552295  [   33/   88]
per-ex loss: 0.768030  [   34/   88]
per-ex loss: 0.455541  [   35/   88]
per-ex loss: 0.574445  [   36/   88]
per-ex loss: 0.657084  [   37/   88]
per-ex loss: 0.492209  [   38/   88]
per-ex loss: 0.522571  [   39/   88]
per-ex loss: 0.720049  [   40/   88]
per-ex loss: 0.723207  [   41/   88]
per-ex loss: 0.493150  [   42/   88]
per-ex loss: 0.589070  [   43/   88]
per-ex loss: 0.514872  [   44/   88]
per-ex loss: 0.592309  [   45/   88]
per-ex loss: 0.384026  [   46/   88]
per-ex loss: 0.730322  [   47/   88]
per-ex loss: 0.641148  [   48/   88]
per-ex loss: 0.519604  [   49/   88]
per-ex loss: 0.743160  [   50/   88]
per-ex loss: 0.528860  [   51/   88]
per-ex loss: 0.687495  [   52/   88]
per-ex loss: 0.623072  [   53/   88]
per-ex loss: 0.742357  [   54/   88]
per-ex loss: 0.621988  [   55/   88]
per-ex loss: 0.536753  [   56/   88]
per-ex loss: 0.508983  [   57/   88]
per-ex loss: 0.611003  [   58/   88]
per-ex loss: 0.549977  [   59/   88]
per-ex loss: 0.798380  [   60/   88]
per-ex loss: 0.661591  [   61/   88]
per-ex loss: 0.470917  [   62/   88]
per-ex loss: 0.744876  [   63/   88]
per-ex loss: 0.543701  [   64/   88]
per-ex loss: 0.472407  [   65/   88]
per-ex loss: 0.730840  [   66/   88]
per-ex loss: 0.494836  [   67/   88]
per-ex loss: 0.537309  [   68/   88]
per-ex loss: 0.641839  [   69/   88]
per-ex loss: 0.590141  [   70/   88]
per-ex loss: 0.648034  [   71/   88]
per-ex loss: 0.753334  [   72/   88]
per-ex loss: 0.703174  [   73/   88]
per-ex loss: 0.517065  [   74/   88]
per-ex loss: 0.517103  [   75/   88]
per-ex loss: 0.538727  [   76/   88]
per-ex loss: 0.770842  [   77/   88]
per-ex loss: 0.721900  [   78/   88]
per-ex loss: 0.603054  [   79/   88]
per-ex loss: 0.560005  [   80/   88]
per-ex loss: 0.467265  [   81/   88]
per-ex loss: 0.781193  [   82/   88]
per-ex loss: 0.563136  [   83/   88]
per-ex loss: 0.703555  [   84/   88]
per-ex loss: 0.466549  [   85/   88]
per-ex loss: 0.570499  [   86/   88]
per-ex loss: 0.527953  [   87/   88]
per-ex loss: 0.518041  [   88/   88]
Train Error: Avg loss: 0.61043661
validation Error: 
 Avg loss: 0.69186626 
 F1: 0.497755 
 Precision: 0.580845 
 Recall: 0.435462
 IoU: 0.331340

test Error: 
 Avg loss: 0.63540537 
 F1: 0.576282 
 Precision: 0.656040 
 Recall: 0.513815
 IoU: 0.404773

We have finished training iteration 105
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_91_.pth
per-ex loss: 0.756618  [    1/   88]
per-ex loss: 0.738930  [    2/   88]
per-ex loss: 0.629720  [    3/   88]
per-ex loss: 0.551453  [    4/   88]
per-ex loss: 0.519791  [    5/   88]
per-ex loss: 0.743280  [    6/   88]
per-ex loss: 0.725577  [    7/   88]
per-ex loss: 0.577238  [    8/   88]
per-ex loss: 0.741146  [    9/   88]
per-ex loss: 0.582760  [   10/   88]
per-ex loss: 0.505724  [   11/   88]
per-ex loss: 0.703016  [   12/   88]
per-ex loss: 0.797712  [   13/   88]
per-ex loss: 0.371473  [   14/   88]
per-ex loss: 0.661970  [   15/   88]
per-ex loss: 0.750291  [   16/   88]
per-ex loss: 0.548488  [   17/   88]
per-ex loss: 0.782869  [   18/   88]
per-ex loss: 0.539797  [   19/   88]
per-ex loss: 0.694702  [   20/   88]
per-ex loss: 0.539151  [   21/   88]
per-ex loss: 0.720472  [   22/   88]
per-ex loss: 0.632248  [   23/   88]
per-ex loss: 0.531992  [   24/   88]
per-ex loss: 0.502955  [   25/   88]
per-ex loss: 0.691907  [   26/   88]
per-ex loss: 0.489408  [   27/   88]
per-ex loss: 0.645933  [   28/   88]
per-ex loss: 0.642362  [   29/   88]
per-ex loss: 0.792893  [   30/   88]
per-ex loss: 0.520681  [   31/   88]
per-ex loss: 0.505378  [   32/   88]
per-ex loss: 0.471070  [   33/   88]
per-ex loss: 0.451334  [   34/   88]
per-ex loss: 0.685843  [   35/   88]
per-ex loss: 0.581077  [   36/   88]
per-ex loss: 0.566452  [   37/   88]
per-ex loss: 0.688639  [   38/   88]
per-ex loss: 0.593571  [   39/   88]
per-ex loss: 0.529287  [   40/   88]
per-ex loss: 0.577502  [   41/   88]
per-ex loss: 0.693281  [   42/   88]
per-ex loss: 0.528008  [   43/   88]
per-ex loss: 0.764770  [   44/   88]
per-ex loss: 0.762428  [   45/   88]
per-ex loss: 0.488912  [   46/   88]
per-ex loss: 0.783984  [   47/   88]
per-ex loss: 0.776705  [   48/   88]
per-ex loss: 0.600825  [   49/   88]
per-ex loss: 0.756771  [   50/   88]
per-ex loss: 0.551758  [   51/   88]
per-ex loss: 0.613108  [   52/   88]
per-ex loss: 0.584482  [   53/   88]
per-ex loss: 0.524817  [   54/   88]
per-ex loss: 0.694901  [   55/   88]
per-ex loss: 0.641283  [   56/   88]
per-ex loss: 0.733891  [   57/   88]
per-ex loss: 0.671042  [   58/   88]
per-ex loss: 0.551369  [   59/   88]
per-ex loss: 0.457401  [   60/   88]
per-ex loss: 0.526251  [   61/   88]
per-ex loss: 0.620945  [   62/   88]
per-ex loss: 0.517373  [   63/   88]
per-ex loss: 0.512365  [   64/   88]
per-ex loss: 0.706531  [   65/   88]
per-ex loss: 0.715035  [   66/   88]
per-ex loss: 0.486114  [   67/   88]
per-ex loss: 0.750240  [   68/   88]
per-ex loss: 0.498648  [   69/   88]
per-ex loss: 0.672985  [   70/   88]
per-ex loss: 0.658196  [   71/   88]
per-ex loss: 0.544309  [   72/   88]
per-ex loss: 0.549248  [   73/   88]
per-ex loss: 0.633635  [   74/   88]
per-ex loss: 0.530950  [   75/   88]
per-ex loss: 0.750859  [   76/   88]
per-ex loss: 0.542318  [   77/   88]
per-ex loss: 0.541739  [   78/   88]
per-ex loss: 0.424897  [   79/   88]
per-ex loss: 0.555616  [   80/   88]
per-ex loss: 0.486614  [   81/   88]
per-ex loss: 0.475393  [   82/   88]
per-ex loss: 0.731674  [   83/   88]
per-ex loss: 0.548760  [   84/   88]
per-ex loss: 0.665535  [   85/   88]
per-ex loss: 0.739807  [   86/   88]
per-ex loss: 0.597870  [   87/   88]
per-ex loss: 0.588816  [   88/   88]
Train Error: Avg loss: 0.61403594
validation Error: 
 Avg loss: 0.69100935 
 F1: 0.496845 
 Precision: 0.603169 
 Recall: 0.422389
 IoU: 0.330535

test Error: 
 Avg loss: 0.64700671 
 F1: 0.561663 
 Precision: 0.681245 
 Recall: 0.477794
 IoU: 0.390495

We have finished training iteration 106
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_104_.pth
per-ex loss: 0.489046  [    1/   88]
per-ex loss: 0.783001  [    2/   88]
per-ex loss: 0.731985  [    3/   88]
per-ex loss: 0.560063  [    4/   88]
per-ex loss: 0.520028  [    5/   88]
per-ex loss: 0.777810  [    6/   88]
per-ex loss: 0.670935  [    7/   88]
per-ex loss: 0.647114  [    8/   88]
per-ex loss: 0.658520  [    9/   88]
per-ex loss: 0.717806  [   10/   88]
per-ex loss: 0.567996  [   11/   88]
per-ex loss: 0.529090  [   12/   88]
per-ex loss: 0.711452  [   13/   88]
per-ex loss: 0.610936  [   14/   88]
per-ex loss: 0.780623  [   15/   88]
per-ex loss: 0.758161  [   16/   88]
per-ex loss: 0.666994  [   17/   88]
per-ex loss: 0.512657  [   18/   88]
per-ex loss: 0.747562  [   19/   88]
per-ex loss: 0.627220  [   20/   88]
per-ex loss: 0.677629  [   21/   88]
per-ex loss: 0.783776  [   22/   88]
per-ex loss: 0.368401  [   23/   88]
per-ex loss: 0.749694  [   24/   88]
per-ex loss: 0.535354  [   25/   88]
per-ex loss: 0.491578  [   26/   88]
per-ex loss: 0.717405  [   27/   88]
per-ex loss: 0.504772  [   28/   88]
per-ex loss: 0.532568  [   29/   88]
per-ex loss: 0.522788  [   30/   88]
per-ex loss: 0.698240  [   31/   88]
per-ex loss: 0.559702  [   32/   88]
per-ex loss: 0.780423  [   33/   88]
per-ex loss: 0.577697  [   34/   88]
per-ex loss: 0.577880  [   35/   88]
per-ex loss: 0.474842  [   36/   88]
per-ex loss: 0.731710  [   37/   88]
per-ex loss: 0.640889  [   38/   88]
per-ex loss: 0.650780  [   39/   88]
per-ex loss: 0.730918  [   40/   88]
per-ex loss: 0.660432  [   41/   88]
per-ex loss: 0.780955  [   42/   88]
per-ex loss: 0.500814  [   43/   88]
per-ex loss: 0.557727  [   44/   88]
per-ex loss: 0.569544  [   45/   88]
per-ex loss: 0.675448  [   46/   88]
per-ex loss: 0.500986  [   47/   88]
per-ex loss: 0.535159  [   48/   88]
per-ex loss: 0.689979  [   49/   88]
per-ex loss: 0.585695  [   50/   88]
per-ex loss: 0.547439  [   51/   88]
per-ex loss: 0.587933  [   52/   88]
per-ex loss: 0.507483  [   53/   88]
per-ex loss: 0.635718  [   54/   88]
per-ex loss: 0.730351  [   55/   88]
per-ex loss: 0.552863  [   56/   88]
per-ex loss: 0.539405  [   57/   88]
per-ex loss: 0.529928  [   58/   88]
per-ex loss: 0.668040  [   59/   88]
per-ex loss: 0.731518  [   60/   88]
per-ex loss: 0.536439  [   61/   88]
per-ex loss: 0.592876  [   62/   88]
per-ex loss: 0.698121  [   63/   88]
per-ex loss: 0.452195  [   64/   88]
per-ex loss: 0.457971  [   65/   88]
per-ex loss: 0.517780  [   66/   88]
per-ex loss: 0.545948  [   67/   88]
per-ex loss: 0.655185  [   68/   88]
per-ex loss: 0.549694  [   69/   88]
per-ex loss: 0.502705  [   70/   88]
per-ex loss: 0.556084  [   71/   88]
per-ex loss: 0.546290  [   72/   88]
per-ex loss: 0.690235  [   73/   88]
per-ex loss: 0.748221  [   74/   88]
per-ex loss: 0.540589  [   75/   88]
per-ex loss: 0.490528  [   76/   88]
per-ex loss: 0.642294  [   77/   88]
per-ex loss: 0.771747  [   78/   88]
per-ex loss: 0.688371  [   79/   88]
per-ex loss: 0.572373  [   80/   88]
per-ex loss: 0.588362  [   81/   88]
per-ex loss: 0.728266  [   82/   88]
per-ex loss: 0.629851  [   83/   88]
per-ex loss: 0.565609  [   84/   88]
per-ex loss: 0.484693  [   85/   88]
per-ex loss: 0.724942  [   86/   88]
per-ex loss: 0.462689  [   87/   88]
per-ex loss: 0.503127  [   88/   88]
Train Error: Avg loss: 0.61257555
validation Error: 
 Avg loss: 0.69228621 
 F1: 0.494243 
 Precision: 0.621434 
 Recall: 0.410271
 IoU: 0.328236

test Error: 
 Avg loss: 0.64037448 
 F1: 0.566862 
 Precision: 0.670262 
 Recall: 0.491101
 IoU: 0.395539

We have finished training iteration 107
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_105_.pth
per-ex loss: 0.717431  [    1/   88]
per-ex loss: 0.530741  [    2/   88]
per-ex loss: 0.707766  [    3/   88]
per-ex loss: 0.640029  [    4/   88]
per-ex loss: 0.685582  [    5/   88]
per-ex loss: 0.672741  [    6/   88]
per-ex loss: 0.587462  [    7/   88]
per-ex loss: 0.754873  [    8/   88]
per-ex loss: 0.739701  [    9/   88]
per-ex loss: 0.583405  [   10/   88]
per-ex loss: 0.787498  [   11/   88]
per-ex loss: 0.486638  [   12/   88]
per-ex loss: 0.562073  [   13/   88]
per-ex loss: 0.534294  [   14/   88]
per-ex loss: 0.548553  [   15/   88]
per-ex loss: 0.511017  [   16/   88]
per-ex loss: 0.635654  [   17/   88]
per-ex loss: 0.557412  [   18/   88]
per-ex loss: 0.477948  [   19/   88]
per-ex loss: 0.504304  [   20/   88]
per-ex loss: 0.784734  [   21/   88]
per-ex loss: 0.624364  [   22/   88]
per-ex loss: 0.450559  [   23/   88]
per-ex loss: 0.777245  [   24/   88]
per-ex loss: 0.645557  [   25/   88]
per-ex loss: 0.724434  [   26/   88]
per-ex loss: 0.485824  [   27/   88]
per-ex loss: 0.572067  [   28/   88]
per-ex loss: 0.523289  [   29/   88]
per-ex loss: 0.520786  [   30/   88]
per-ex loss: 0.543066  [   31/   88]
per-ex loss: 0.563108  [   32/   88]
per-ex loss: 0.658875  [   33/   88]
per-ex loss: 0.525013  [   34/   88]
per-ex loss: 0.466251  [   35/   88]
per-ex loss: 0.738371  [   36/   88]
per-ex loss: 0.693743  [   37/   88]
per-ex loss: 0.486732  [   38/   88]
per-ex loss: 0.736598  [   39/   88]
per-ex loss: 0.484940  [   40/   88]
per-ex loss: 0.736233  [   41/   88]
per-ex loss: 0.623989  [   42/   88]
per-ex loss: 0.702574  [   43/   88]
per-ex loss: 0.666581  [   44/   88]
per-ex loss: 0.599721  [   45/   88]
per-ex loss: 0.655360  [   46/   88]
per-ex loss: 0.578531  [   47/   88]
per-ex loss: 0.499433  [   48/   88]
per-ex loss: 0.744719  [   49/   88]
per-ex loss: 0.700262  [   50/   88]
per-ex loss: 0.553909  [   51/   88]
per-ex loss: 0.516213  [   52/   88]
per-ex loss: 0.596367  [   53/   88]
per-ex loss: 0.486923  [   54/   88]
per-ex loss: 0.719448  [   55/   88]
per-ex loss: 0.572815  [   56/   88]
per-ex loss: 0.742519  [   57/   88]
per-ex loss: 0.790169  [   58/   88]
per-ex loss: 0.578127  [   59/   88]
per-ex loss: 0.675987  [   60/   88]
per-ex loss: 0.552561  [   61/   88]
per-ex loss: 0.620094  [   62/   88]
per-ex loss: 0.504294  [   63/   88]
per-ex loss: 0.369923  [   64/   88]
per-ex loss: 0.476673  [   65/   88]
per-ex loss: 0.581532  [   66/   88]
per-ex loss: 0.467173  [   67/   88]
per-ex loss: 0.778821  [   68/   88]
per-ex loss: 0.656493  [   69/   88]
per-ex loss: 0.514881  [   70/   88]
per-ex loss: 0.682396  [   71/   88]
per-ex loss: 0.524334  [   72/   88]
per-ex loss: 0.548770  [   73/   88]
per-ex loss: 0.685730  [   74/   88]
per-ex loss: 0.523342  [   75/   88]
per-ex loss: 0.651746  [   76/   88]
per-ex loss: 0.548093  [   77/   88]
per-ex loss: 0.607225  [   78/   88]
per-ex loss: 0.660177  [   79/   88]
per-ex loss: 0.482861  [   80/   88]
per-ex loss: 0.752738  [   81/   88]
per-ex loss: 0.523759  [   82/   88]
per-ex loss: 0.482615  [   83/   88]
per-ex loss: 0.751255  [   84/   88]
per-ex loss: 0.742419  [   85/   88]
per-ex loss: 0.610317  [   86/   88]
per-ex loss: 0.721864  [   87/   88]
per-ex loss: 0.651322  [   88/   88]
Train Error: Avg loss: 0.60959051
validation Error: 
 Avg loss: 0.68235704 
 F1: 0.506997 
 Precision: 0.570892 
 Recall: 0.455965
 IoU: 0.339582

test Error: 
 Avg loss: 0.63325022 
 F1: 0.576549 
 Precision: 0.620897 
 Recall: 0.538113
 IoU: 0.405036

We have finished training iteration 108
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_106_.pth
per-ex loss: 0.737220  [    1/   88]
per-ex loss: 0.769939  [    2/   88]
per-ex loss: 0.742620  [    3/   88]
per-ex loss: 0.506555  [    4/   88]
per-ex loss: 0.549236  [    5/   88]
per-ex loss: 0.668004  [    6/   88]
per-ex loss: 0.534062  [    7/   88]
per-ex loss: 0.773913  [    8/   88]
per-ex loss: 0.716468  [    9/   88]
per-ex loss: 0.730209  [   10/   88]
per-ex loss: 0.445818  [   11/   88]
per-ex loss: 0.586623  [   12/   88]
per-ex loss: 0.503424  [   13/   88]
per-ex loss: 0.581316  [   14/   88]
per-ex loss: 0.526546  [   15/   88]
per-ex loss: 0.505206  [   16/   88]
per-ex loss: 0.485688  [   17/   88]
per-ex loss: 0.650406  [   18/   88]
per-ex loss: 0.532957  [   19/   88]
per-ex loss: 0.659010  [   20/   88]
per-ex loss: 0.487440  [   21/   88]
per-ex loss: 0.780847  [   22/   88]
per-ex loss: 0.615928  [   23/   88]
per-ex loss: 0.664173  [   24/   88]
per-ex loss: 0.585982  [   25/   88]
per-ex loss: 0.727421  [   26/   88]
per-ex loss: 0.466142  [   27/   88]
per-ex loss: 0.633217  [   28/   88]
per-ex loss: 0.690706  [   29/   88]
per-ex loss: 0.502159  [   30/   88]
per-ex loss: 0.734572  [   31/   88]
per-ex loss: 0.683428  [   32/   88]
per-ex loss: 0.645011  [   33/   88]
per-ex loss: 0.724705  [   34/   88]
per-ex loss: 0.457747  [   35/   88]
per-ex loss: 0.675099  [   36/   88]
per-ex loss: 0.694278  [   37/   88]
per-ex loss: 0.526025  [   38/   88]
per-ex loss: 0.525826  [   39/   88]
per-ex loss: 0.754415  [   40/   88]
per-ex loss: 0.509343  [   41/   88]
per-ex loss: 0.772822  [   42/   88]
per-ex loss: 0.690147  [   43/   88]
per-ex loss: 0.709096  [   44/   88]
per-ex loss: 0.494690  [   45/   88]
per-ex loss: 0.682631  [   46/   88]
per-ex loss: 0.507923  [   47/   88]
per-ex loss: 0.744273  [   48/   88]
per-ex loss: 0.774240  [   49/   88]
per-ex loss: 0.789073  [   50/   88]
per-ex loss: 0.623471  [   51/   88]
per-ex loss: 0.555241  [   52/   88]
per-ex loss: 0.580124  [   53/   88]
per-ex loss: 0.569312  [   54/   88]
per-ex loss: 0.620126  [   55/   88]
per-ex loss: 0.559700  [   56/   88]
per-ex loss: 0.528246  [   57/   88]
per-ex loss: 0.356256  [   58/   88]
per-ex loss: 0.535377  [   59/   88]
per-ex loss: 0.505297  [   60/   88]
per-ex loss: 0.659703  [   61/   88]
per-ex loss: 0.582461  [   62/   88]
per-ex loss: 0.570381  [   63/   88]
per-ex loss: 0.641123  [   64/   88]
per-ex loss: 0.466471  [   65/   88]
per-ex loss: 0.544342  [   66/   88]
per-ex loss: 0.608616  [   67/   88]
per-ex loss: 0.762684  [   68/   88]
per-ex loss: 0.607755  [   69/   88]
per-ex loss: 0.497763  [   70/   88]
per-ex loss: 0.707552  [   71/   88]
per-ex loss: 0.535004  [   72/   88]
per-ex loss: 0.532391  [   73/   88]
per-ex loss: 0.733476  [   74/   88]
per-ex loss: 0.529492  [   75/   88]
per-ex loss: 0.703470  [   76/   88]
per-ex loss: 0.592603  [   77/   88]
per-ex loss: 0.496045  [   78/   88]
per-ex loss: 0.715389  [   79/   88]
per-ex loss: 0.707155  [   80/   88]
per-ex loss: 0.522438  [   81/   88]
per-ex loss: 0.490195  [   82/   88]
per-ex loss: 0.651860  [   83/   88]
per-ex loss: 0.491314  [   84/   88]
per-ex loss: 0.553592  [   85/   88]
per-ex loss: 0.595443  [   86/   88]
per-ex loss: 0.534419  [   87/   88]
per-ex loss: 0.575923  [   88/   88]
Train Error: Avg loss: 0.60791808
validation Error: 
 Avg loss: 0.69010582 
 F1: 0.497195 
 Precision: 0.595170 
 Recall: 0.426918
 IoU: 0.330845

test Error: 
 Avg loss: 0.63729688 
 F1: 0.572013 
 Precision: 0.669409 
 Recall: 0.499359
 IoU: 0.400573

We have finished training iteration 109
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_107_.pth
per-ex loss: 0.636629  [    1/   88]
per-ex loss: 0.495525  [    2/   88]
per-ex loss: 0.595961  [    3/   88]
per-ex loss: 0.372561  [    4/   88]
per-ex loss: 0.576346  [    5/   88]
per-ex loss: 0.648895  [    6/   88]
per-ex loss: 0.461511  [    7/   88]
per-ex loss: 0.584325  [    8/   88]
per-ex loss: 0.570530  [    9/   88]
per-ex loss: 0.620737  [   10/   88]
per-ex loss: 0.486751  [   11/   88]
per-ex loss: 0.584830  [   12/   88]
per-ex loss: 0.715455  [   13/   88]
per-ex loss: 0.552036  [   14/   88]
per-ex loss: 0.501660  [   15/   88]
per-ex loss: 0.716933  [   16/   88]
per-ex loss: 0.514887  [   17/   88]
per-ex loss: 0.588173  [   18/   88]
per-ex loss: 0.590266  [   19/   88]
per-ex loss: 0.457546  [   20/   88]
per-ex loss: 0.545072  [   21/   88]
per-ex loss: 0.540088  [   22/   88]
per-ex loss: 0.531157  [   23/   88]
per-ex loss: 0.732312  [   24/   88]
per-ex loss: 0.540515  [   25/   88]
per-ex loss: 0.746033  [   26/   88]
per-ex loss: 0.713099  [   27/   88]
per-ex loss: 0.625175  [   28/   88]
per-ex loss: 0.488216  [   29/   88]
per-ex loss: 0.560103  [   30/   88]
per-ex loss: 0.705973  [   31/   88]
per-ex loss: 0.599433  [   32/   88]
per-ex loss: 0.687024  [   33/   88]
per-ex loss: 0.622969  [   34/   88]
per-ex loss: 0.497467  [   35/   88]
per-ex loss: 0.574535  [   36/   88]
per-ex loss: 0.546321  [   37/   88]
per-ex loss: 0.782659  [   38/   88]
per-ex loss: 0.622311  [   39/   88]
per-ex loss: 0.788244  [   40/   88]
per-ex loss: 0.640851  [   41/   88]
per-ex loss: 0.814876  [   42/   88]
per-ex loss: 0.496087  [   43/   88]
per-ex loss: 0.503380  [   44/   88]
per-ex loss: 0.733906  [   45/   88]
per-ex loss: 0.583254  [   46/   88]
per-ex loss: 0.689791  [   47/   88]
per-ex loss: 0.536525  [   48/   88]
per-ex loss: 0.785126  [   49/   88]
per-ex loss: 0.781544  [   50/   88]
per-ex loss: 0.724455  [   51/   88]
per-ex loss: 0.722598  [   52/   88]
per-ex loss: 0.632905  [   53/   88]
per-ex loss: 0.549531  [   54/   88]
per-ex loss: 0.683846  [   55/   88]
per-ex loss: 0.550654  [   56/   88]
per-ex loss: 0.648544  [   57/   88]
per-ex loss: 0.571518  [   58/   88]
per-ex loss: 0.760114  [   59/   88]
per-ex loss: 0.766551  [   60/   88]
per-ex loss: 0.679177  [   61/   88]
per-ex loss: 0.689263  [   62/   88]
per-ex loss: 0.450687  [   63/   88]
per-ex loss: 0.534119  [   64/   88]
per-ex loss: 0.482077  [   65/   88]
per-ex loss: 0.635879  [   66/   88]
per-ex loss: 0.681122  [   67/   88]
per-ex loss: 0.690876  [   68/   88]
per-ex loss: 0.487454  [   69/   88]
per-ex loss: 0.759103  [   70/   88]
per-ex loss: 0.607308  [   71/   88]
per-ex loss: 0.535808  [   72/   88]
per-ex loss: 0.727763  [   73/   88]
per-ex loss: 0.500470  [   74/   88]
per-ex loss: 0.498120  [   75/   88]
per-ex loss: 0.691548  [   76/   88]
per-ex loss: 0.564029  [   77/   88]
per-ex loss: 0.561100  [   78/   88]
per-ex loss: 0.612293  [   79/   88]
per-ex loss: 0.562022  [   80/   88]
per-ex loss: 0.493181  [   81/   88]
per-ex loss: 0.700918  [   82/   88]
per-ex loss: 0.521475  [   83/   88]
per-ex loss: 0.741315  [   84/   88]
per-ex loss: 0.519930  [   85/   88]
per-ex loss: 0.501053  [   86/   88]
per-ex loss: 0.745112  [   87/   88]
per-ex loss: 0.545471  [   88/   88]
Train Error: Avg loss: 0.60926126
validation Error: 
 Avg loss: 0.68940864 
 F1: 0.498512 
 Precision: 0.588499 
 Recall: 0.432395
 IoU: 0.332012

test Error: 
 Avg loss: 0.64012627 
 F1: 0.568124 
 Precision: 0.650185 
 Recall: 0.504455
 IoU: 0.396769

We have finished training iteration 110
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_88_.pth
per-ex loss: 0.643152  [    1/   88]
per-ex loss: 0.436546  [    2/   88]
per-ex loss: 0.627589  [    3/   88]
per-ex loss: 0.582834  [    4/   88]
per-ex loss: 0.511668  [    5/   88]
per-ex loss: 0.663708  [    6/   88]
per-ex loss: 0.697632  [    7/   88]
per-ex loss: 0.627093  [    8/   88]
per-ex loss: 0.722107  [    9/   88]
per-ex loss: 0.595196  [   10/   88]
per-ex loss: 0.436099  [   11/   88]
per-ex loss: 0.477722  [   12/   88]
per-ex loss: 0.496444  [   13/   88]
per-ex loss: 0.781997  [   14/   88]
per-ex loss: 0.720918  [   15/   88]
per-ex loss: 0.516575  [   16/   88]
per-ex loss: 0.624329  [   17/   88]
per-ex loss: 0.594189  [   18/   88]
per-ex loss: 0.738777  [   19/   88]
per-ex loss: 0.784865  [   20/   88]
per-ex loss: 0.533128  [   21/   88]
per-ex loss: 0.573255  [   22/   88]
per-ex loss: 0.570434  [   23/   88]
per-ex loss: 0.638056  [   24/   88]
per-ex loss: 0.558663  [   25/   88]
per-ex loss: 0.754185  [   26/   88]
per-ex loss: 0.487254  [   27/   88]
per-ex loss: 0.747361  [   28/   88]
per-ex loss: 0.684224  [   29/   88]
per-ex loss: 0.511876  [   30/   88]
per-ex loss: 0.777008  [   31/   88]
per-ex loss: 0.482817  [   32/   88]
per-ex loss: 0.510727  [   33/   88]
per-ex loss: 0.533056  [   34/   88]
per-ex loss: 0.700606  [   35/   88]
per-ex loss: 0.780105  [   36/   88]
per-ex loss: 0.626469  [   37/   88]
per-ex loss: 0.531063  [   38/   88]
per-ex loss: 0.493083  [   39/   88]
per-ex loss: 0.547226  [   40/   88]
per-ex loss: 0.556564  [   41/   88]
per-ex loss: 0.641250  [   42/   88]
per-ex loss: 0.547753  [   43/   88]
per-ex loss: 0.693026  [   44/   88]
per-ex loss: 0.723043  [   45/   88]
per-ex loss: 0.653177  [   46/   88]
per-ex loss: 0.480945  [   47/   88]
per-ex loss: 0.733023  [   48/   88]
per-ex loss: 0.554408  [   49/   88]
per-ex loss: 0.491640  [   50/   88]
per-ex loss: 0.578190  [   51/   88]
per-ex loss: 0.749281  [   52/   88]
per-ex loss: 0.545838  [   53/   88]
per-ex loss: 0.580941  [   54/   88]
per-ex loss: 0.576445  [   55/   88]
per-ex loss: 0.475517  [   56/   88]
per-ex loss: 0.795318  [   57/   88]
per-ex loss: 0.617910  [   58/   88]
per-ex loss: 0.737017  [   59/   88]
per-ex loss: 0.763547  [   60/   88]
per-ex loss: 0.717184  [   61/   88]
per-ex loss: 0.615820  [   62/   88]
per-ex loss: 0.635719  [   63/   88]
per-ex loss: 0.491335  [   64/   88]
per-ex loss: 0.779929  [   65/   88]
per-ex loss: 0.682339  [   66/   88]
per-ex loss: 0.591496  [   67/   88]
per-ex loss: 0.522861  [   68/   88]
per-ex loss: 0.569751  [   69/   88]
per-ex loss: 0.562347  [   70/   88]
per-ex loss: 0.670078  [   71/   88]
per-ex loss: 0.501431  [   72/   88]
per-ex loss: 0.528650  [   73/   88]
per-ex loss: 0.713387  [   74/   88]
per-ex loss: 0.519491  [   75/   88]
per-ex loss: 0.508152  [   76/   88]
per-ex loss: 0.674919  [   77/   88]
per-ex loss: 0.739530  [   78/   88]
per-ex loss: 0.519742  [   79/   88]
per-ex loss: 0.696752  [   80/   88]
per-ex loss: 0.531148  [   81/   88]
per-ex loss: 0.498600  [   82/   88]
per-ex loss: 0.733239  [   83/   88]
per-ex loss: 0.642023  [   84/   88]
per-ex loss: 0.532912  [   85/   88]
per-ex loss: 0.491979  [   86/   88]
per-ex loss: 0.404789  [   87/   88]
per-ex loss: 0.711063  [   88/   88]
Train Error: Avg loss: 0.60910833
validation Error: 
 Avg loss: 0.70326831 
 F1: 0.478822 
 Precision: 0.488277 
 Recall: 0.469725
 IoU: 0.314770

test Error: 
 Avg loss: 0.63372709 
 F1: 0.575792 
 Precision: 0.596430 
 Recall: 0.556534
 IoU: 0.404289

We have finished training iteration 111
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_109_.pth
per-ex loss: 0.726165  [    1/   88]
per-ex loss: 0.736396  [    2/   88]
per-ex loss: 0.780481  [    3/   88]
per-ex loss: 0.698459  [    4/   88]
per-ex loss: 0.540149  [    5/   88]
per-ex loss: 0.702441  [    6/   88]
per-ex loss: 0.583112  [    7/   88]
per-ex loss: 0.721805  [    8/   88]
per-ex loss: 0.743302  [    9/   88]
per-ex loss: 0.533661  [   10/   88]
per-ex loss: 0.535677  [   11/   88]
per-ex loss: 0.732429  [   12/   88]
per-ex loss: 0.542428  [   13/   88]
per-ex loss: 0.555547  [   14/   88]
per-ex loss: 0.490589  [   15/   88]
per-ex loss: 0.486573  [   16/   88]
per-ex loss: 0.780320  [   17/   88]
per-ex loss: 0.592228  [   18/   88]
per-ex loss: 0.601448  [   19/   88]
per-ex loss: 0.725084  [   20/   88]
per-ex loss: 0.614858  [   21/   88]
per-ex loss: 0.572308  [   22/   88]
per-ex loss: 0.729727  [   23/   88]
per-ex loss: 0.616057  [   24/   88]
per-ex loss: 0.497365  [   25/   88]
per-ex loss: 0.508978  [   26/   88]
per-ex loss: 0.747072  [   27/   88]
per-ex loss: 0.648388  [   28/   88]
per-ex loss: 0.582807  [   29/   88]
per-ex loss: 0.530706  [   30/   88]
per-ex loss: 0.542202  [   31/   88]
per-ex loss: 0.689799  [   32/   88]
per-ex loss: 0.519788  [   33/   88]
per-ex loss: 0.647900  [   34/   88]
per-ex loss: 0.375637  [   35/   88]
per-ex loss: 0.525019  [   36/   88]
per-ex loss: 0.523746  [   37/   88]
per-ex loss: 0.679688  [   38/   88]
per-ex loss: 0.454828  [   39/   88]
per-ex loss: 0.725140  [   40/   88]
per-ex loss: 0.556010  [   41/   88]
per-ex loss: 0.688542  [   42/   88]
per-ex loss: 0.515023  [   43/   88]
per-ex loss: 0.794894  [   44/   88]
per-ex loss: 0.563258  [   45/   88]
per-ex loss: 0.479131  [   46/   88]
per-ex loss: 0.482670  [   47/   88]
per-ex loss: 0.461209  [   48/   88]
per-ex loss: 0.643454  [   49/   88]
per-ex loss: 0.641148  [   50/   88]
per-ex loss: 0.509391  [   51/   88]
per-ex loss: 0.490753  [   52/   88]
per-ex loss: 0.656767  [   53/   88]
per-ex loss: 0.664325  [   54/   88]
per-ex loss: 0.782645  [   55/   88]
per-ex loss: 0.602862  [   56/   88]
per-ex loss: 0.551445  [   57/   88]
per-ex loss: 0.618604  [   58/   88]
per-ex loss: 0.497716  [   59/   88]
per-ex loss: 0.618604  [   60/   88]
per-ex loss: 0.694401  [   61/   88]
per-ex loss: 0.643794  [   62/   88]
per-ex loss: 0.575140  [   63/   88]
per-ex loss: 0.695440  [   64/   88]
per-ex loss: 0.501342  [   65/   88]
per-ex loss: 0.759786  [   66/   88]
per-ex loss: 0.728640  [   67/   88]
per-ex loss: 0.479965  [   68/   88]
per-ex loss: 0.578052  [   69/   88]
per-ex loss: 0.752333  [   70/   88]
per-ex loss: 0.723363  [   71/   88]
per-ex loss: 0.514332  [   72/   88]
per-ex loss: 0.630712  [   73/   88]
per-ex loss: 0.547382  [   74/   88]
per-ex loss: 0.549441  [   75/   88]
per-ex loss: 0.707138  [   76/   88]
per-ex loss: 0.486486  [   77/   88]
per-ex loss: 0.516646  [   78/   88]
per-ex loss: 0.679889  [   79/   88]
per-ex loss: 0.606263  [   80/   88]
per-ex loss: 0.456121  [   81/   88]
per-ex loss: 0.512243  [   82/   88]
per-ex loss: 0.586392  [   83/   88]
per-ex loss: 0.734843  [   84/   88]
per-ex loss: 0.571379  [   85/   88]
per-ex loss: 0.767290  [   86/   88]
per-ex loss: 0.537537  [   87/   88]
per-ex loss: 0.671345  [   88/   88]
Train Error: Avg loss: 0.60870974
validation Error: 
 Avg loss: 0.69478680 
 F1: 0.492189 
 Precision: 0.531833 
 Recall: 0.458045
 IoU: 0.326426

test Error: 
 Avg loss: 0.63367573 
 F1: 0.575070 
 Precision: 0.628808 
 Recall: 0.529794
 IoU: 0.403578

We have finished training iteration 112
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_110_.pth
per-ex loss: 0.480226  [    1/   88]
per-ex loss: 0.361708  [    2/   88]
per-ex loss: 0.669722  [    3/   88]
per-ex loss: 0.524457  [    4/   88]
per-ex loss: 0.573089  [    5/   88]
per-ex loss: 0.515308  [    6/   88]
per-ex loss: 0.518435  [    7/   88]
per-ex loss: 0.797666  [    8/   88]
per-ex loss: 0.508852  [    9/   88]
per-ex loss: 0.481730  [   10/   88]
per-ex loss: 0.744804  [   11/   88]
per-ex loss: 0.448457  [   12/   88]
per-ex loss: 0.639531  [   13/   88]
per-ex loss: 0.583689  [   14/   88]
per-ex loss: 0.742619  [   15/   88]
per-ex loss: 0.578527  [   16/   88]
per-ex loss: 0.593472  [   17/   88]
per-ex loss: 0.599978  [   18/   88]
per-ex loss: 0.559101  [   19/   88]
per-ex loss: 0.632705  [   20/   88]
per-ex loss: 0.772275  [   21/   88]
per-ex loss: 0.723019  [   22/   88]
per-ex loss: 0.552711  [   23/   88]
per-ex loss: 0.527853  [   24/   88]
per-ex loss: 0.678425  [   25/   88]
per-ex loss: 0.728442  [   26/   88]
per-ex loss: 0.512101  [   27/   88]
per-ex loss: 0.622898  [   28/   88]
per-ex loss: 0.619997  [   29/   88]
per-ex loss: 0.552384  [   30/   88]
per-ex loss: 0.615710  [   31/   88]
per-ex loss: 0.494409  [   32/   88]
per-ex loss: 0.804515  [   33/   88]
per-ex loss: 0.493717  [   34/   88]
per-ex loss: 0.724338  [   35/   88]
per-ex loss: 0.539251  [   36/   88]
per-ex loss: 0.625543  [   37/   88]
per-ex loss: 0.540100  [   38/   88]
per-ex loss: 0.505651  [   39/   88]
per-ex loss: 0.570904  [   40/   88]
per-ex loss: 0.773202  [   41/   88]
per-ex loss: 0.665299  [   42/   88]
per-ex loss: 0.703165  [   43/   88]
per-ex loss: 0.433348  [   44/   88]
per-ex loss: 0.588881  [   45/   88]
per-ex loss: 0.754625  [   46/   88]
per-ex loss: 0.640362  [   47/   88]
per-ex loss: 0.560410  [   48/   88]
per-ex loss: 0.498787  [   49/   88]
per-ex loss: 0.780533  [   50/   88]
per-ex loss: 0.750801  [   51/   88]
per-ex loss: 0.540377  [   52/   88]
per-ex loss: 0.510984  [   53/   88]
per-ex loss: 0.707699  [   54/   88]
per-ex loss: 0.546662  [   55/   88]
per-ex loss: 0.682532  [   56/   88]
per-ex loss: 0.654458  [   57/   88]
per-ex loss: 0.696405  [   58/   88]
per-ex loss: 0.441642  [   59/   88]
per-ex loss: 0.682493  [   60/   88]
per-ex loss: 0.528364  [   61/   88]
per-ex loss: 0.711565  [   62/   88]
per-ex loss: 0.550176  [   63/   88]
per-ex loss: 0.496292  [   64/   88]
per-ex loss: 0.696629  [   65/   88]
per-ex loss: 0.487993  [   66/   88]
per-ex loss: 0.702698  [   67/   88]
per-ex loss: 0.496545  [   68/   88]
per-ex loss: 0.681379  [   69/   88]
per-ex loss: 0.577959  [   70/   88]
per-ex loss: 0.729891  [   71/   88]
per-ex loss: 0.733573  [   72/   88]
per-ex loss: 0.515863  [   73/   88]
per-ex loss: 0.723917  [   74/   88]
per-ex loss: 0.536419  [   75/   88]
per-ex loss: 0.605017  [   76/   88]
per-ex loss: 0.764379  [   77/   88]
per-ex loss: 0.548990  [   78/   88]
per-ex loss: 0.559633  [   79/   88]
per-ex loss: 0.528785  [   80/   88]
per-ex loss: 0.631997  [   81/   88]
per-ex loss: 0.713452  [   82/   88]
per-ex loss: 0.646982  [   83/   88]
per-ex loss: 0.634421  [   84/   88]
per-ex loss: 0.492627  [   85/   88]
per-ex loss: 0.643640  [   86/   88]
per-ex loss: 0.531462  [   87/   88]
per-ex loss: 0.498442  [   88/   88]
Train Error: Avg loss: 0.60613722
validation Error: 
 Avg loss: 0.69392333 
 F1: 0.492005 
 Precision: 0.592322 
 Recall: 0.420747
 IoU: 0.326265

test Error: 
 Avg loss: 0.64299692 
 F1: 0.566450 
 Precision: 0.672033 
 Recall: 0.489539
 IoU: 0.395138

We have finished training iteration 113
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_111_.pth
per-ex loss: 0.542388  [    1/   88]
per-ex loss: 0.715527  [    2/   88]
per-ex loss: 0.772653  [    3/   88]
per-ex loss: 0.496730  [    4/   88]
per-ex loss: 0.683257  [    5/   88]
per-ex loss: 0.548498  [    6/   88]
per-ex loss: 0.488907  [    7/   88]
per-ex loss: 0.633869  [    8/   88]
per-ex loss: 0.488361  [    9/   88]
per-ex loss: 0.516531  [   10/   88]
per-ex loss: 0.562965  [   11/   88]
per-ex loss: 0.619296  [   12/   88]
per-ex loss: 0.556954  [   13/   88]
per-ex loss: 0.788772  [   14/   88]
per-ex loss: 0.748254  [   15/   88]
per-ex loss: 0.676822  [   16/   88]
per-ex loss: 0.651630  [   17/   88]
per-ex loss: 0.541170  [   18/   88]
per-ex loss: 0.504998  [   19/   88]
per-ex loss: 0.492485  [   20/   88]
per-ex loss: 0.604717  [   21/   88]
per-ex loss: 0.638041  [   22/   88]
per-ex loss: 0.635349  [   23/   88]
per-ex loss: 0.490162  [   24/   88]
per-ex loss: 0.687090  [   25/   88]
per-ex loss: 0.686011  [   26/   88]
per-ex loss: 0.605583  [   27/   88]
per-ex loss: 0.535804  [   28/   88]
per-ex loss: 0.495641  [   29/   88]
per-ex loss: 0.650553  [   30/   88]
per-ex loss: 0.506641  [   31/   88]
per-ex loss: 0.733627  [   32/   88]
per-ex loss: 0.553577  [   33/   88]
per-ex loss: 0.560733  [   34/   88]
per-ex loss: 0.389617  [   35/   88]
per-ex loss: 0.727619  [   36/   88]
per-ex loss: 0.793859  [   37/   88]
per-ex loss: 0.726565  [   38/   88]
per-ex loss: 0.727346  [   39/   88]
per-ex loss: 0.607510  [   40/   88]
per-ex loss: 0.734106  [   41/   88]
per-ex loss: 0.697605  [   42/   88]
per-ex loss: 0.556722  [   43/   88]
per-ex loss: 0.631827  [   44/   88]
per-ex loss: 0.763506  [   45/   88]
per-ex loss: 0.768963  [   46/   88]
per-ex loss: 0.689726  [   47/   88]
per-ex loss: 0.536348  [   48/   88]
per-ex loss: 0.788248  [   49/   88]
per-ex loss: 0.508137  [   50/   88]
per-ex loss: 0.587637  [   51/   88]
per-ex loss: 0.767687  [   52/   88]
per-ex loss: 0.522714  [   53/   88]
per-ex loss: 0.581274  [   54/   88]
per-ex loss: 0.732164  [   55/   88]
per-ex loss: 0.462613  [   56/   88]
per-ex loss: 0.632771  [   57/   88]
per-ex loss: 0.567561  [   58/   88]
per-ex loss: 0.594836  [   59/   88]
per-ex loss: 0.706284  [   60/   88]
per-ex loss: 0.554547  [   61/   88]
per-ex loss: 0.534415  [   62/   88]
per-ex loss: 0.615254  [   63/   88]
per-ex loss: 0.452467  [   64/   88]
per-ex loss: 0.476556  [   65/   88]
per-ex loss: 0.475844  [   66/   88]
per-ex loss: 0.686455  [   67/   88]
per-ex loss: 0.511309  [   68/   88]
per-ex loss: 0.527252  [   69/   88]
per-ex loss: 0.524064  [   70/   88]
per-ex loss: 0.630563  [   71/   88]
per-ex loss: 0.536353  [   72/   88]
per-ex loss: 0.552243  [   73/   88]
per-ex loss: 0.653188  [   74/   88]
per-ex loss: 0.712939  [   75/   88]
per-ex loss: 0.593654  [   76/   88]
per-ex loss: 0.767021  [   77/   88]
per-ex loss: 0.726757  [   78/   88]
per-ex loss: 0.547181  [   79/   88]
per-ex loss: 0.537026  [   80/   88]
per-ex loss: 0.812578  [   81/   88]
per-ex loss: 0.631294  [   82/   88]
per-ex loss: 0.686232  [   83/   88]
per-ex loss: 0.706454  [   84/   88]
per-ex loss: 0.619449  [   85/   88]
per-ex loss: 0.458939  [   86/   88]
per-ex loss: 0.522288  [   87/   88]
per-ex loss: 0.456554  [   88/   88]
Train Error: Avg loss: 0.61051952
validation Error: 
 Avg loss: 0.69153909 
 F1: 0.494626 
 Precision: 0.578550 
 Recall: 0.431965
 IoU: 0.328573

test Error: 
 Avg loss: 0.63543676 
 F1: 0.575068 
 Precision: 0.657261 
 Recall: 0.511148
 IoU: 0.403576

We have finished training iteration 114
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_112_.pth
per-ex loss: 0.654431  [    1/   88]
per-ex loss: 0.581001  [    2/   88]
per-ex loss: 0.444826  [    3/   88]
per-ex loss: 0.539387  [    4/   88]
per-ex loss: 0.521731  [    5/   88]
per-ex loss: 0.464073  [    6/   88]
per-ex loss: 0.525802  [    7/   88]
per-ex loss: 0.785461  [    8/   88]
per-ex loss: 0.746008  [    9/   88]
per-ex loss: 0.483056  [   10/   88]
per-ex loss: 0.522809  [   11/   88]
per-ex loss: 0.731909  [   12/   88]
per-ex loss: 0.574494  [   13/   88]
per-ex loss: 0.712004  [   14/   88]
per-ex loss: 0.676832  [   15/   88]
per-ex loss: 0.756055  [   16/   88]
per-ex loss: 0.631922  [   17/   88]
per-ex loss: 0.529908  [   18/   88]
per-ex loss: 0.618255  [   19/   88]
per-ex loss: 0.741778  [   20/   88]
per-ex loss: 0.633209  [   21/   88]
per-ex loss: 0.614329  [   22/   88]
per-ex loss: 0.571392  [   23/   88]
per-ex loss: 0.713984  [   24/   88]
per-ex loss: 0.604416  [   25/   88]
per-ex loss: 0.484778  [   26/   88]
per-ex loss: 0.515787  [   27/   88]
per-ex loss: 0.503640  [   28/   88]
per-ex loss: 0.540976  [   29/   88]
per-ex loss: 0.689991  [   30/   88]
per-ex loss: 0.480716  [   31/   88]
per-ex loss: 0.561109  [   32/   88]
per-ex loss: 0.441409  [   33/   88]
per-ex loss: 0.620294  [   34/   88]
per-ex loss: 0.539132  [   35/   88]
per-ex loss: 0.619686  [   36/   88]
per-ex loss: 0.354193  [   37/   88]
per-ex loss: 0.778966  [   38/   88]
per-ex loss: 0.575403  [   39/   88]
per-ex loss: 0.556090  [   40/   88]
per-ex loss: 0.710813  [   41/   88]
per-ex loss: 0.477608  [   42/   88]
per-ex loss: 0.579731  [   43/   88]
per-ex loss: 0.772632  [   44/   88]
per-ex loss: 0.565349  [   45/   88]
per-ex loss: 0.682763  [   46/   88]
per-ex loss: 0.725269  [   47/   88]
per-ex loss: 0.771093  [   48/   88]
per-ex loss: 0.726422  [   49/   88]
per-ex loss: 0.490057  [   50/   88]
per-ex loss: 0.488571  [   51/   88]
per-ex loss: 0.526740  [   52/   88]
per-ex loss: 0.688745  [   53/   88]
per-ex loss: 0.535787  [   54/   88]
per-ex loss: 0.739787  [   55/   88]
per-ex loss: 0.599978  [   56/   88]
per-ex loss: 0.514535  [   57/   88]
per-ex loss: 0.667619  [   58/   88]
per-ex loss: 0.560467  [   59/   88]
per-ex loss: 0.508203  [   60/   88]
per-ex loss: 0.581232  [   61/   88]
per-ex loss: 0.657637  [   62/   88]
per-ex loss: 0.488353  [   63/   88]
per-ex loss: 0.510894  [   64/   88]
per-ex loss: 0.796557  [   65/   88]
per-ex loss: 0.558739  [   66/   88]
per-ex loss: 0.569493  [   67/   88]
per-ex loss: 0.514732  [   68/   88]
per-ex loss: 0.725825  [   69/   88]
per-ex loss: 0.516968  [   70/   88]
per-ex loss: 0.738649  [   71/   88]
per-ex loss: 0.762170  [   72/   88]
per-ex loss: 0.557170  [   73/   88]
per-ex loss: 0.629264  [   74/   88]
per-ex loss: 0.697163  [   75/   88]
per-ex loss: 0.771521  [   76/   88]
per-ex loss: 0.474735  [   77/   88]
per-ex loss: 0.477932  [   78/   88]
per-ex loss: 0.648071  [   79/   88]
per-ex loss: 0.704808  [   80/   88]
per-ex loss: 0.544836  [   81/   88]
per-ex loss: 0.640675  [   82/   88]
per-ex loss: 0.725320  [   83/   88]
per-ex loss: 0.680074  [   84/   88]
per-ex loss: 0.486780  [   85/   88]
per-ex loss: 0.498521  [   86/   88]
per-ex loss: 0.686414  [   87/   88]
per-ex loss: 0.671435  [   88/   88]
Train Error: Avg loss: 0.60556114
validation Error: 
 Avg loss: 0.69542421 
 F1: 0.486697 
 Precision: 0.546999 
 Recall: 0.438370
 IoU: 0.321612

test Error: 
 Avg loss: 0.63917906 
 F1: 0.571330 
 Precision: 0.635300 
 Recall: 0.519064
 IoU: 0.399903

We have finished training iteration 115
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_113_.pth
per-ex loss: 0.475008  [    1/   88]
per-ex loss: 0.512458  [    2/   88]
per-ex loss: 0.682250  [    3/   88]
per-ex loss: 0.612493  [    4/   88]
per-ex loss: 0.449758  [    5/   88]
per-ex loss: 0.798654  [    6/   88]
per-ex loss: 0.705789  [    7/   88]
per-ex loss: 0.430367  [    8/   88]
per-ex loss: 0.484184  [    9/   88]
per-ex loss: 0.775993  [   10/   88]
per-ex loss: 0.568155  [   11/   88]
per-ex loss: 0.572318  [   12/   88]
per-ex loss: 0.757000  [   13/   88]
per-ex loss: 0.754898  [   14/   88]
per-ex loss: 0.497468  [   15/   88]
per-ex loss: 0.530140  [   16/   88]
per-ex loss: 0.505102  [   17/   88]
per-ex loss: 0.447189  [   18/   88]
per-ex loss: 0.651900  [   19/   88]
per-ex loss: 0.554954  [   20/   88]
per-ex loss: 0.539268  [   21/   88]
per-ex loss: 0.528213  [   22/   88]
per-ex loss: 0.454048  [   23/   88]
per-ex loss: 0.517811  [   24/   88]
per-ex loss: 0.659930  [   25/   88]
per-ex loss: 0.779938  [   26/   88]
per-ex loss: 0.516775  [   27/   88]
per-ex loss: 0.625077  [   28/   88]
per-ex loss: 0.516196  [   29/   88]
per-ex loss: 0.710357  [   30/   88]
per-ex loss: 0.739609  [   31/   88]
per-ex loss: 0.365104  [   32/   88]
per-ex loss: 0.698303  [   33/   88]
per-ex loss: 0.761937  [   34/   88]
per-ex loss: 0.759237  [   35/   88]
per-ex loss: 0.715904  [   36/   88]
per-ex loss: 0.688209  [   37/   88]
per-ex loss: 0.552314  [   38/   88]
per-ex loss: 0.591533  [   39/   88]
per-ex loss: 0.665654  [   40/   88]
per-ex loss: 0.621973  [   41/   88]
per-ex loss: 0.685723  [   42/   88]
per-ex loss: 0.731074  [   43/   88]
per-ex loss: 0.503419  [   44/   88]
per-ex loss: 0.659358  [   45/   88]
per-ex loss: 0.483445  [   46/   88]
per-ex loss: 0.590120  [   47/   88]
per-ex loss: 0.619966  [   48/   88]
per-ex loss: 0.635674  [   49/   88]
per-ex loss: 0.666966  [   50/   88]
per-ex loss: 0.532865  [   51/   88]
per-ex loss: 0.498435  [   52/   88]
per-ex loss: 0.785373  [   53/   88]
per-ex loss: 0.471833  [   54/   88]
per-ex loss: 0.578436  [   55/   88]
per-ex loss: 0.548878  [   56/   88]
per-ex loss: 0.593325  [   57/   88]
per-ex loss: 0.761495  [   58/   88]
per-ex loss: 0.721534  [   59/   88]
per-ex loss: 0.687000  [   60/   88]
per-ex loss: 0.549398  [   61/   88]
per-ex loss: 0.547265  [   62/   88]
per-ex loss: 0.536897  [   63/   88]
per-ex loss: 0.716163  [   64/   88]
per-ex loss: 0.605946  [   65/   88]
per-ex loss: 0.718003  [   66/   88]
per-ex loss: 0.475420  [   67/   88]
per-ex loss: 0.508766  [   68/   88]
per-ex loss: 0.567614  [   69/   88]
per-ex loss: 0.635597  [   70/   88]
per-ex loss: 0.635698  [   71/   88]
per-ex loss: 0.750766  [   72/   88]
per-ex loss: 0.720432  [   73/   88]
per-ex loss: 0.583499  [   74/   88]
per-ex loss: 0.606624  [   75/   88]
per-ex loss: 0.499421  [   76/   88]
per-ex loss: 0.521804  [   77/   88]
per-ex loss: 0.537574  [   78/   88]
per-ex loss: 0.578181  [   79/   88]
per-ex loss: 0.636740  [   80/   88]
per-ex loss: 0.557635  [   81/   88]
per-ex loss: 0.747764  [   82/   88]
per-ex loss: 0.498133  [   83/   88]
per-ex loss: 0.483671  [   84/   88]
per-ex loss: 0.462686  [   85/   88]
per-ex loss: 0.567810  [   86/   88]
per-ex loss: 0.692147  [   87/   88]
per-ex loss: 0.519133  [   88/   88]
Train Error: Avg loss: 0.60214970
validation Error: 
 Avg loss: 0.70808242 
 F1: 0.470417 
 Precision: 0.465595 
 Recall: 0.475340
 IoU: 0.307546

test Error: 
 Avg loss: 0.64016912 
 F1: 0.568701 
 Precision: 0.566628 
 Recall: 0.570790
 IoU: 0.397332

We have finished training iteration 116
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_114_.pth
per-ex loss: 0.513704  [    1/   88]
per-ex loss: 0.667215  [    2/   88]
per-ex loss: 0.742064  [    3/   88]
per-ex loss: 0.492284  [    4/   88]
per-ex loss: 0.489248  [    5/   88]
per-ex loss: 0.557832  [    6/   88]
per-ex loss: 0.440569  [    7/   88]
per-ex loss: 0.558629  [    8/   88]
per-ex loss: 0.637170  [    9/   88]
per-ex loss: 0.795522  [   10/   88]
per-ex loss: 0.550924  [   11/   88]
per-ex loss: 0.676367  [   12/   88]
per-ex loss: 0.683652  [   13/   88]
per-ex loss: 0.738316  [   14/   88]
per-ex loss: 0.570477  [   15/   88]
per-ex loss: 0.489428  [   16/   88]
per-ex loss: 0.532431  [   17/   88]
per-ex loss: 0.519031  [   18/   88]
per-ex loss: 0.682098  [   19/   88]
per-ex loss: 0.775015  [   20/   88]
per-ex loss: 0.776297  [   21/   88]
per-ex loss: 0.510129  [   22/   88]
per-ex loss: 0.770437  [   23/   88]
per-ex loss: 0.740083  [   24/   88]
per-ex loss: 0.639906  [   25/   88]
per-ex loss: 0.740620  [   26/   88]
per-ex loss: 0.516686  [   27/   88]
per-ex loss: 0.505169  [   28/   88]
per-ex loss: 0.627122  [   29/   88]
per-ex loss: 0.712913  [   30/   88]
per-ex loss: 0.789497  [   31/   88]
per-ex loss: 0.783053  [   32/   88]
per-ex loss: 0.481166  [   33/   88]
per-ex loss: 0.517455  [   34/   88]
per-ex loss: 0.607092  [   35/   88]
per-ex loss: 0.712963  [   36/   88]
per-ex loss: 0.566128  [   37/   88]
per-ex loss: 0.657303  [   38/   88]
per-ex loss: 0.514597  [   39/   88]
per-ex loss: 0.775999  [   40/   88]
per-ex loss: 0.509174  [   41/   88]
per-ex loss: 0.509814  [   42/   88]
per-ex loss: 0.561308  [   43/   88]
per-ex loss: 0.455637  [   44/   88]
per-ex loss: 0.511958  [   45/   88]
per-ex loss: 0.552439  [   46/   88]
per-ex loss: 0.558647  [   47/   88]
per-ex loss: 0.617594  [   48/   88]
per-ex loss: 0.719141  [   49/   88]
per-ex loss: 0.511837  [   50/   88]
per-ex loss: 0.727987  [   51/   88]
per-ex loss: 0.563199  [   52/   88]
per-ex loss: 0.670900  [   53/   88]
per-ex loss: 0.732259  [   54/   88]
per-ex loss: 0.502852  [   55/   88]
per-ex loss: 0.692076  [   56/   88]
per-ex loss: 0.685785  [   57/   88]
per-ex loss: 0.534070  [   58/   88]
per-ex loss: 0.526221  [   59/   88]
per-ex loss: 0.530760  [   60/   88]
per-ex loss: 0.584070  [   61/   88]
per-ex loss: 0.722999  [   62/   88]
per-ex loss: 0.488180  [   63/   88]
per-ex loss: 0.597963  [   64/   88]
per-ex loss: 0.660388  [   65/   88]
per-ex loss: 0.618010  [   66/   88]
per-ex loss: 0.453505  [   67/   88]
per-ex loss: 0.472157  [   68/   88]
per-ex loss: 0.718821  [   69/   88]
per-ex loss: 0.685658  [   70/   88]
per-ex loss: 0.489400  [   71/   88]
per-ex loss: 0.695149  [   72/   88]
per-ex loss: 0.536791  [   73/   88]
per-ex loss: 0.602733  [   74/   88]
per-ex loss: 0.759544  [   75/   88]
per-ex loss: 0.540904  [   76/   88]
per-ex loss: 0.474492  [   77/   88]
per-ex loss: 0.591396  [   78/   88]
per-ex loss: 0.698619  [   79/   88]
per-ex loss: 0.399313  [   80/   88]
per-ex loss: 0.552030  [   81/   88]
per-ex loss: 0.617312  [   82/   88]
per-ex loss: 0.604005  [   83/   88]
per-ex loss: 0.702873  [   84/   88]
per-ex loss: 0.676701  [   85/   88]
per-ex loss: 0.632763  [   86/   88]
per-ex loss: 0.581032  [   87/   88]
per-ex loss: 0.578170  [   88/   88]
Train Error: Avg loss: 0.60755942
validation Error: 
 Avg loss: 0.69864799 
 F1: 0.486604 
 Precision: 0.507261 
 Recall: 0.467564
 IoU: 0.321532

test Error: 
 Avg loss: 0.63590776 
 F1: 0.574333 
 Precision: 0.600861 
 Recall: 0.550048
 IoU: 0.402852

We have finished training iteration 117
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_115_.pth
per-ex loss: 0.448195  [    1/   88]
per-ex loss: 0.752846  [    2/   88]
per-ex loss: 0.717331  [    3/   88]
per-ex loss: 0.613965  [    4/   88]
per-ex loss: 0.663134  [    5/   88]
per-ex loss: 0.514888  [    6/   88]
per-ex loss: 0.726571  [    7/   88]
per-ex loss: 0.652623  [    8/   88]
per-ex loss: 0.686277  [    9/   88]
per-ex loss: 0.520009  [   10/   88]
per-ex loss: 0.593854  [   11/   88]
per-ex loss: 0.489480  [   12/   88]
per-ex loss: 0.608829  [   13/   88]
per-ex loss: 0.498843  [   14/   88]
per-ex loss: 0.504311  [   15/   88]
per-ex loss: 0.770911  [   16/   88]
per-ex loss: 0.697073  [   17/   88]
per-ex loss: 0.616805  [   18/   88]
per-ex loss: 0.552608  [   19/   88]
per-ex loss: 0.590552  [   20/   88]
per-ex loss: 0.473214  [   21/   88]
per-ex loss: 0.711981  [   22/   88]
per-ex loss: 0.520165  [   23/   88]
per-ex loss: 0.618961  [   24/   88]
per-ex loss: 0.476418  [   25/   88]
per-ex loss: 0.548425  [   26/   88]
per-ex loss: 0.634451  [   27/   88]
per-ex loss: 0.716211  [   28/   88]
per-ex loss: 0.535039  [   29/   88]
per-ex loss: 0.555125  [   30/   88]
per-ex loss: 0.610915  [   31/   88]
per-ex loss: 0.582391  [   32/   88]
per-ex loss: 0.761173  [   33/   88]
per-ex loss: 0.489100  [   34/   88]
per-ex loss: 0.649796  [   35/   88]
per-ex loss: 0.706987  [   36/   88]
per-ex loss: 0.604006  [   37/   88]
per-ex loss: 0.651863  [   38/   88]
per-ex loss: 0.761324  [   39/   88]
per-ex loss: 0.531227  [   40/   88]
per-ex loss: 0.528909  [   41/   88]
per-ex loss: 0.560678  [   42/   88]
per-ex loss: 0.777010  [   43/   88]
per-ex loss: 0.543533  [   44/   88]
per-ex loss: 0.589573  [   45/   88]
per-ex loss: 0.638539  [   46/   88]
per-ex loss: 0.493262  [   47/   88]
per-ex loss: 0.517073  [   48/   88]
per-ex loss: 0.741359  [   49/   88]
per-ex loss: 0.643910  [   50/   88]
per-ex loss: 0.565493  [   51/   88]
per-ex loss: 0.690880  [   52/   88]
per-ex loss: 0.698726  [   53/   88]
per-ex loss: 0.745081  [   54/   88]
per-ex loss: 0.502023  [   55/   88]
per-ex loss: 0.682119  [   56/   88]
per-ex loss: 0.478489  [   57/   88]
per-ex loss: 0.526075  [   58/   88]
per-ex loss: 0.716143  [   59/   88]
per-ex loss: 0.734705  [   60/   88]
per-ex loss: 0.792569  [   61/   88]
per-ex loss: 0.570923  [   62/   88]
per-ex loss: 0.593378  [   63/   88]
per-ex loss: 0.549460  [   64/   88]
per-ex loss: 0.491457  [   65/   88]
per-ex loss: 0.605009  [   66/   88]
per-ex loss: 0.508672  [   67/   88]
per-ex loss: 0.586389  [   68/   88]
per-ex loss: 0.702634  [   69/   88]
per-ex loss: 0.581113  [   70/   88]
per-ex loss: 0.681247  [   71/   88]
per-ex loss: 0.525024  [   72/   88]
per-ex loss: 0.563589  [   73/   88]
per-ex loss: 0.657338  [   74/   88]
per-ex loss: 0.722797  [   75/   88]
per-ex loss: 0.508655  [   76/   88]
per-ex loss: 0.768417  [   77/   88]
per-ex loss: 0.455609  [   78/   88]
per-ex loss: 0.479286  [   79/   88]
per-ex loss: 0.714744  [   80/   88]
per-ex loss: 0.781552  [   81/   88]
per-ex loss: 0.421529  [   82/   88]
per-ex loss: 0.702258  [   83/   88]
per-ex loss: 0.487940  [   84/   88]
per-ex loss: 0.714764  [   85/   88]
per-ex loss: 0.494053  [   86/   88]
per-ex loss: 0.607051  [   87/   88]
per-ex loss: 0.573766  [   88/   88]
Train Error: Avg loss: 0.60878043
validation Error: 
 Avg loss: 0.70034371 
 F1: 0.486423 
 Precision: 0.626845 
 Recall: 0.397400
 IoU: 0.321373

test Error: 
 Avg loss: 0.65008632 
 F1: 0.555992 
 Precision: 0.695665 
 Recall: 0.463028
 IoU: 0.385034

We have finished training iteration 118
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_116_.pth
per-ex loss: 0.575088  [    1/   88]
per-ex loss: 0.561207  [    2/   88]
per-ex loss: 0.477720  [    3/   88]
per-ex loss: 0.678974  [    4/   88]
per-ex loss: 0.700136  [    5/   88]
per-ex loss: 0.706583  [    6/   88]
per-ex loss: 0.559175  [    7/   88]
per-ex loss: 0.718675  [    8/   88]
per-ex loss: 0.589164  [    9/   88]
per-ex loss: 0.490239  [   10/   88]
per-ex loss: 0.792722  [   11/   88]
per-ex loss: 0.688899  [   12/   88]
per-ex loss: 0.721541  [   13/   88]
per-ex loss: 0.581416  [   14/   88]
per-ex loss: 0.735429  [   15/   88]
per-ex loss: 0.537702  [   16/   88]
per-ex loss: 0.496364  [   17/   88]
per-ex loss: 0.712147  [   18/   88]
per-ex loss: 0.733536  [   19/   88]
per-ex loss: 0.495761  [   20/   88]
per-ex loss: 0.480568  [   21/   88]
per-ex loss: 0.716385  [   22/   88]
per-ex loss: 0.544394  [   23/   88]
per-ex loss: 0.583316  [   24/   88]
per-ex loss: 0.561491  [   25/   88]
per-ex loss: 0.638038  [   26/   88]
per-ex loss: 0.714911  [   27/   88]
per-ex loss: 0.646340  [   28/   88]
per-ex loss: 0.521408  [   29/   88]
per-ex loss: 0.577037  [   30/   88]
per-ex loss: 0.501420  [   31/   88]
per-ex loss: 0.747133  [   32/   88]
per-ex loss: 0.513363  [   33/   88]
per-ex loss: 0.453403  [   34/   88]
per-ex loss: 0.710543  [   35/   88]
per-ex loss: 0.604745  [   36/   88]
per-ex loss: 0.596225  [   37/   88]
per-ex loss: 0.769045  [   38/   88]
per-ex loss: 0.662006  [   39/   88]
per-ex loss: 0.762216  [   40/   88]
per-ex loss: 0.480834  [   41/   88]
per-ex loss: 0.648795  [   42/   88]
per-ex loss: 0.537126  [   43/   88]
per-ex loss: 0.497478  [   44/   88]
per-ex loss: 0.635405  [   45/   88]
per-ex loss: 0.488763  [   46/   88]
per-ex loss: 0.515623  [   47/   88]
per-ex loss: 0.494741  [   48/   88]
per-ex loss: 0.609568  [   49/   88]
per-ex loss: 0.551069  [   50/   88]
per-ex loss: 0.711161  [   51/   88]
per-ex loss: 0.672328  [   52/   88]
per-ex loss: 0.568640  [   53/   88]
per-ex loss: 0.544506  [   54/   88]
per-ex loss: 0.494468  [   55/   88]
per-ex loss: 0.617957  [   56/   88]
per-ex loss: 0.733188  [   57/   88]
per-ex loss: 0.700917  [   58/   88]
per-ex loss: 0.697418  [   59/   88]
per-ex loss: 0.528250  [   60/   88]
per-ex loss: 0.749381  [   61/   88]
per-ex loss: 0.529142  [   62/   88]
per-ex loss: 0.721467  [   63/   88]
per-ex loss: 0.587939  [   64/   88]
per-ex loss: 0.705862  [   65/   88]
per-ex loss: 0.587446  [   66/   88]
per-ex loss: 0.544047  [   67/   88]
per-ex loss: 0.766537  [   68/   88]
per-ex loss: 0.541270  [   69/   88]
per-ex loss: 0.555816  [   70/   88]
per-ex loss: 0.634301  [   71/   88]
per-ex loss: 0.501672  [   72/   88]
per-ex loss: 0.736851  [   73/   88]
per-ex loss: 0.536060  [   74/   88]
per-ex loss: 0.500094  [   75/   88]
per-ex loss: 0.627994  [   76/   88]
per-ex loss: 0.792993  [   77/   88]
per-ex loss: 0.444471  [   78/   88]
per-ex loss: 0.533045  [   79/   88]
per-ex loss: 0.576777  [   80/   88]
per-ex loss: 0.648365  [   81/   88]
per-ex loss: 0.493412  [   82/   88]
per-ex loss: 0.792969  [   83/   88]
per-ex loss: 0.440951  [   84/   88]
per-ex loss: 0.565588  [   85/   88]
per-ex loss: 0.615906  [   86/   88]
per-ex loss: 0.687277  [   87/   88]
per-ex loss: 0.723807  [   88/   88]
Train Error: Avg loss: 0.61052431
validation Error: 
 Avg loss: 0.69095851 
 F1: 0.499723 
 Precision: 0.568287 
 Recall: 0.445922
 IoU: 0.333087

test Error: 
 Avg loss: 0.63164795 
 F1: 0.578258 
 Precision: 0.658646 
 Recall: 0.515358
 IoU: 0.406725

We have finished training iteration 119
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_117_.pth
per-ex loss: 0.691342  [    1/   88]
per-ex loss: 0.629502  [    2/   88]
per-ex loss: 0.769897  [    3/   88]
per-ex loss: 0.481790  [    4/   88]
per-ex loss: 0.608783  [    5/   88]
per-ex loss: 0.699030  [    6/   88]
per-ex loss: 0.464659  [    7/   88]
per-ex loss: 0.532551  [    8/   88]
per-ex loss: 0.547795  [    9/   88]
per-ex loss: 0.445948  [   10/   88]
per-ex loss: 0.459732  [   11/   88]
per-ex loss: 0.471175  [   12/   88]
per-ex loss: 0.707477  [   13/   88]
per-ex loss: 0.545125  [   14/   88]
per-ex loss: 0.737852  [   15/   88]
per-ex loss: 0.572937  [   16/   88]
per-ex loss: 0.608117  [   17/   88]
per-ex loss: 0.656373  [   18/   88]
per-ex loss: 0.739090  [   19/   88]
per-ex loss: 0.709627  [   20/   88]
per-ex loss: 0.613770  [   21/   88]
per-ex loss: 0.695800  [   22/   88]
per-ex loss: 0.479977  [   23/   88]
per-ex loss: 0.506316  [   24/   88]
per-ex loss: 0.740331  [   25/   88]
per-ex loss: 0.788475  [   26/   88]
per-ex loss: 0.525100  [   27/   88]
per-ex loss: 0.622112  [   28/   88]
per-ex loss: 0.546636  [   29/   88]
per-ex loss: 0.560142  [   30/   88]
per-ex loss: 0.767961  [   31/   88]
per-ex loss: 0.773120  [   32/   88]
per-ex loss: 0.525986  [   33/   88]
per-ex loss: 0.500351  [   34/   88]
per-ex loss: 0.661416  [   35/   88]
per-ex loss: 0.528390  [   36/   88]
per-ex loss: 0.744663  [   37/   88]
per-ex loss: 0.580818  [   38/   88]
per-ex loss: 0.736381  [   39/   88]
per-ex loss: 0.511167  [   40/   88]
per-ex loss: 0.583932  [   41/   88]
per-ex loss: 0.649190  [   42/   88]
per-ex loss: 0.626199  [   43/   88]
per-ex loss: 0.578646  [   44/   88]
per-ex loss: 0.512233  [   45/   88]
per-ex loss: 0.551923  [   46/   88]
per-ex loss: 0.593398  [   47/   88]
per-ex loss: 0.428413  [   48/   88]
per-ex loss: 0.495031  [   49/   88]
per-ex loss: 0.528467  [   50/   88]
per-ex loss: 0.491324  [   51/   88]
per-ex loss: 0.706029  [   52/   88]
per-ex loss: 0.750207  [   53/   88]
per-ex loss: 0.536704  [   54/   88]
per-ex loss: 0.539692  [   55/   88]
per-ex loss: 0.676504  [   56/   88]
per-ex loss: 0.487661  [   57/   88]
per-ex loss: 0.688746  [   58/   88]
per-ex loss: 0.544611  [   59/   88]
per-ex loss: 0.759686  [   60/   88]
per-ex loss: 0.610203  [   61/   88]
per-ex loss: 0.536608  [   62/   88]
per-ex loss: 0.561595  [   63/   88]
per-ex loss: 0.478393  [   64/   88]
per-ex loss: 0.725626  [   65/   88]
per-ex loss: 0.608132  [   66/   88]
per-ex loss: 0.677309  [   67/   88]
per-ex loss: 0.757388  [   68/   88]
per-ex loss: 0.541472  [   69/   88]
per-ex loss: 0.612266  [   70/   88]
per-ex loss: 0.726945  [   71/   88]
per-ex loss: 0.629422  [   72/   88]
per-ex loss: 0.512004  [   73/   88]
per-ex loss: 0.694381  [   74/   88]
per-ex loss: 0.445124  [   75/   88]
per-ex loss: 0.696968  [   76/   88]
per-ex loss: 0.366632  [   77/   88]
per-ex loss: 0.548550  [   78/   88]
per-ex loss: 0.582653  [   79/   88]
per-ex loss: 0.728846  [   80/   88]
per-ex loss: 0.477471  [   81/   88]
per-ex loss: 0.685688  [   82/   88]
per-ex loss: 0.493395  [   83/   88]
per-ex loss: 0.525329  [   84/   88]
per-ex loss: 0.777315  [   85/   88]
per-ex loss: 0.721081  [   86/   88]
per-ex loss: 0.564216  [   87/   88]
per-ex loss: 0.685487  [   88/   88]
Train Error: Avg loss: 0.60439556
validation Error: 
 Avg loss: 0.69765055 
 F1: 0.487308 
 Precision: 0.540452 
 Recall: 0.443680
 IoU: 0.322146

test Error: 
 Avg loss: 0.63883957 
 F1: 0.569131 
 Precision: 0.649306 
 Recall: 0.506580
 IoU: 0.397752

We have finished training iteration 120
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_118_.pth
per-ex loss: 0.548347  [    1/   88]
per-ex loss: 0.515768  [    2/   88]
per-ex loss: 0.777767  [    3/   88]
per-ex loss: 0.560195  [    4/   88]
per-ex loss: 0.617820  [    5/   88]
per-ex loss: 0.494558  [    6/   88]
per-ex loss: 0.692480  [    7/   88]
per-ex loss: 0.587771  [    8/   88]
per-ex loss: 0.505902  [    9/   88]
per-ex loss: 0.528428  [   10/   88]
per-ex loss: 0.644723  [   11/   88]
per-ex loss: 0.553440  [   12/   88]
per-ex loss: 0.463710  [   13/   88]
per-ex loss: 0.486440  [   14/   88]
per-ex loss: 0.454170  [   15/   88]
per-ex loss: 0.727365  [   16/   88]
per-ex loss: 0.661665  [   17/   88]
per-ex loss: 0.501216  [   18/   88]
per-ex loss: 0.581113  [   19/   88]
per-ex loss: 0.768027  [   20/   88]
per-ex loss: 0.768600  [   21/   88]
per-ex loss: 0.591053  [   22/   88]
per-ex loss: 0.723026  [   23/   88]
per-ex loss: 0.554380  [   24/   88]
per-ex loss: 0.576268  [   25/   88]
per-ex loss: 0.488686  [   26/   88]
per-ex loss: 0.520906  [   27/   88]
per-ex loss: 0.530298  [   28/   88]
per-ex loss: 0.604140  [   29/   88]
per-ex loss: 0.521240  [   30/   88]
per-ex loss: 0.623824  [   31/   88]
per-ex loss: 0.469765  [   32/   88]
per-ex loss: 0.492710  [   33/   88]
per-ex loss: 0.686688  [   34/   88]
per-ex loss: 0.534466  [   35/   88]
per-ex loss: 0.688552  [   36/   88]
per-ex loss: 0.523533  [   37/   88]
per-ex loss: 0.697444  [   38/   88]
per-ex loss: 0.683957  [   39/   88]
per-ex loss: 0.487327  [   40/   88]
per-ex loss: 0.615406  [   41/   88]
per-ex loss: 0.458521  [   42/   88]
per-ex loss: 0.726691  [   43/   88]
per-ex loss: 0.567495  [   44/   88]
per-ex loss: 0.440145  [   45/   88]
per-ex loss: 0.782631  [   46/   88]
per-ex loss: 0.480486  [   47/   88]
per-ex loss: 0.350992  [   48/   88]
per-ex loss: 0.757535  [   49/   88]
per-ex loss: 0.517486  [   50/   88]
per-ex loss: 0.542316  [   51/   88]
per-ex loss: 0.637723  [   52/   88]
per-ex loss: 0.669437  [   53/   88]
per-ex loss: 0.618723  [   54/   88]
per-ex loss: 0.627791  [   55/   88]
per-ex loss: 0.703322  [   56/   88]
per-ex loss: 0.618457  [   57/   88]
per-ex loss: 0.512364  [   58/   88]
per-ex loss: 0.775234  [   59/   88]
per-ex loss: 0.480351  [   60/   88]
per-ex loss: 0.728189  [   61/   88]
per-ex loss: 0.774723  [   62/   88]
per-ex loss: 0.581202  [   63/   88]
per-ex loss: 0.698632  [   64/   88]
per-ex loss: 0.492668  [   65/   88]
per-ex loss: 0.547048  [   66/   88]
per-ex loss: 0.553855  [   67/   88]
per-ex loss: 0.780258  [   68/   88]
per-ex loss: 0.753113  [   69/   88]
per-ex loss: 0.713328  [   70/   88]
per-ex loss: 0.698831  [   71/   88]
per-ex loss: 0.719847  [   72/   88]
per-ex loss: 0.518847  [   73/   88]
per-ex loss: 0.674969  [   74/   88]
per-ex loss: 0.500748  [   75/   88]
per-ex loss: 0.418651  [   76/   88]
per-ex loss: 0.555109  [   77/   88]
per-ex loss: 0.517197  [   78/   88]
per-ex loss: 0.450237  [   79/   88]
per-ex loss: 0.633639  [   80/   88]
per-ex loss: 0.584783  [   81/   88]
per-ex loss: 0.626081  [   82/   88]
per-ex loss: 0.485641  [   83/   88]
per-ex loss: 0.654528  [   84/   88]
per-ex loss: 0.553146  [   85/   88]
per-ex loss: 0.707355  [   86/   88]
per-ex loss: 0.751194  [   87/   88]
per-ex loss: 0.714602  [   88/   88]
Train Error: Avg loss: 0.59896923
validation Error: 
 Avg loss: 0.69962660 
 F1: 0.487599 
 Precision: 0.521987 
 Recall: 0.457462
 IoU: 0.322401

test Error: 
 Avg loss: 0.64346355 
 F1: 0.563972 
 Precision: 0.620805 
 Recall: 0.516672
 IoU: 0.392731

We have finished training iteration 121
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_119_.pth
per-ex loss: 0.685793  [    1/   88]
per-ex loss: 0.676784  [    2/   88]
per-ex loss: 0.705421  [    3/   88]
per-ex loss: 0.529365  [    4/   88]
per-ex loss: 0.777142  [    5/   88]
per-ex loss: 0.763540  [    6/   88]
per-ex loss: 0.520813  [    7/   88]
per-ex loss: 0.613101  [    8/   88]
per-ex loss: 0.683904  [    9/   88]
per-ex loss: 0.478883  [   10/   88]
per-ex loss: 0.422180  [   11/   88]
per-ex loss: 0.569722  [   12/   88]
per-ex loss: 0.455983  [   13/   88]
per-ex loss: 0.721432  [   14/   88]
per-ex loss: 0.634198  [   15/   88]
per-ex loss: 0.550147  [   16/   88]
per-ex loss: 0.407793  [   17/   88]
per-ex loss: 0.638533  [   18/   88]
per-ex loss: 0.625290  [   19/   88]
per-ex loss: 0.696575  [   20/   88]
per-ex loss: 0.709635  [   21/   88]
per-ex loss: 0.703583  [   22/   88]
per-ex loss: 0.566200  [   23/   88]
per-ex loss: 0.497574  [   24/   88]
per-ex loss: 0.507667  [   25/   88]
per-ex loss: 0.576334  [   26/   88]
per-ex loss: 0.760024  [   27/   88]
per-ex loss: 0.503508  [   28/   88]
per-ex loss: 0.482199  [   29/   88]
per-ex loss: 0.618500  [   30/   88]
per-ex loss: 0.543951  [   31/   88]
per-ex loss: 0.810414  [   32/   88]
per-ex loss: 0.803955  [   33/   88]
per-ex loss: 0.734008  [   34/   88]
per-ex loss: 0.488612  [   35/   88]
per-ex loss: 0.525558  [   36/   88]
per-ex loss: 0.613127  [   37/   88]
per-ex loss: 0.546780  [   38/   88]
per-ex loss: 0.475317  [   39/   88]
per-ex loss: 0.500118  [   40/   88]
per-ex loss: 0.647804  [   41/   88]
per-ex loss: 0.555476  [   42/   88]
per-ex loss: 0.559884  [   43/   88]
per-ex loss: 0.503013  [   44/   88]
per-ex loss: 0.652077  [   45/   88]
per-ex loss: 0.552753  [   46/   88]
per-ex loss: 0.492513  [   47/   88]
per-ex loss: 0.651597  [   48/   88]
per-ex loss: 0.565167  [   49/   88]
per-ex loss: 0.567365  [   50/   88]
per-ex loss: 0.718366  [   51/   88]
per-ex loss: 0.487086  [   52/   88]
per-ex loss: 0.685992  [   53/   88]
per-ex loss: 0.720250  [   54/   88]
per-ex loss: 0.528020  [   55/   88]
per-ex loss: 0.558183  [   56/   88]
per-ex loss: 0.641613  [   57/   88]
per-ex loss: 0.477924  [   58/   88]
per-ex loss: 0.587380  [   59/   88]
per-ex loss: 0.565061  [   60/   88]
per-ex loss: 0.732556  [   61/   88]
per-ex loss: 0.516679  [   62/   88]
per-ex loss: 0.571195  [   63/   88]
per-ex loss: 0.708559  [   64/   88]
per-ex loss: 0.512276  [   65/   88]
per-ex loss: 0.740631  [   66/   88]
per-ex loss: 0.606895  [   67/   88]
per-ex loss: 0.687486  [   68/   88]
per-ex loss: 0.613269  [   69/   88]
per-ex loss: 0.624851  [   70/   88]
per-ex loss: 0.743064  [   71/   88]
per-ex loss: 0.555503  [   72/   88]
per-ex loss: 0.601713  [   73/   88]
per-ex loss: 0.515585  [   74/   88]
per-ex loss: 0.610662  [   75/   88]
per-ex loss: 0.674358  [   76/   88]
per-ex loss: 0.752695  [   77/   88]
per-ex loss: 0.520275  [   78/   88]
per-ex loss: 0.698808  [   79/   88]
per-ex loss: 0.811742  [   80/   88]
per-ex loss: 0.629760  [   81/   88]
per-ex loss: 0.505841  [   82/   88]
per-ex loss: 0.638923  [   83/   88]
per-ex loss: 0.468604  [   84/   88]
per-ex loss: 0.557239  [   85/   88]
per-ex loss: 0.686254  [   86/   88]
per-ex loss: 0.590984  [   87/   88]
per-ex loss: 0.794740  [   88/   88]
Train Error: Avg loss: 0.60809466
validation Error: 
 Avg loss: 0.70205230 
 F1: 0.483150 
 Precision: 0.590723 
 Recall: 0.408720
 IoU: 0.318522

test Error: 
 Avg loss: 0.64958640 
 F1: 0.555113 
 Precision: 0.686844 
 Recall: 0.465780
 IoU: 0.384191

We have finished training iteration 122
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_120_.pth
per-ex loss: 0.607848  [    1/   88]
per-ex loss: 0.782107  [    2/   88]
per-ex loss: 0.548538  [    3/   88]
per-ex loss: 0.720890  [    4/   88]
per-ex loss: 0.646180  [    5/   88]
per-ex loss: 0.549786  [    6/   88]
per-ex loss: 0.754601  [    7/   88]
per-ex loss: 0.394731  [    8/   88]
per-ex loss: 0.539330  [    9/   88]
per-ex loss: 0.485637  [   10/   88]
per-ex loss: 0.751615  [   11/   88]
per-ex loss: 0.627379  [   12/   88]
per-ex loss: 0.740893  [   13/   88]
per-ex loss: 0.497744  [   14/   88]
per-ex loss: 0.738759  [   15/   88]
per-ex loss: 0.768495  [   16/   88]
per-ex loss: 0.446671  [   17/   88]
per-ex loss: 0.580198  [   18/   88]
per-ex loss: 0.659862  [   19/   88]
per-ex loss: 0.602468  [   20/   88]
per-ex loss: 0.511616  [   21/   88]
per-ex loss: 0.698189  [   22/   88]
per-ex loss: 0.531979  [   23/   88]
per-ex loss: 0.655278  [   24/   88]
per-ex loss: 0.639439  [   25/   88]
per-ex loss: 0.527395  [   26/   88]
per-ex loss: 0.514329  [   27/   88]
per-ex loss: 0.538564  [   28/   88]
per-ex loss: 0.499565  [   29/   88]
per-ex loss: 0.496122  [   30/   88]
per-ex loss: 0.549723  [   31/   88]
per-ex loss: 0.495805  [   32/   88]
per-ex loss: 0.447918  [   33/   88]
per-ex loss: 0.751725  [   34/   88]
per-ex loss: 0.501012  [   35/   88]
per-ex loss: 0.558024  [   36/   88]
per-ex loss: 0.731251  [   37/   88]
per-ex loss: 0.491380  [   38/   88]
per-ex loss: 0.544444  [   39/   88]
per-ex loss: 0.591133  [   40/   88]
per-ex loss: 0.700005  [   41/   88]
per-ex loss: 0.574899  [   42/   88]
per-ex loss: 0.611357  [   43/   88]
per-ex loss: 0.715317  [   44/   88]
per-ex loss: 0.681746  [   45/   88]
per-ex loss: 0.530092  [   46/   88]
per-ex loss: 0.495449  [   47/   88]
per-ex loss: 0.726128  [   48/   88]
per-ex loss: 0.622350  [   49/   88]
per-ex loss: 0.563411  [   50/   88]
per-ex loss: 0.594341  [   51/   88]
per-ex loss: 0.627014  [   52/   88]
per-ex loss: 0.581094  [   53/   88]
per-ex loss: 0.653921  [   54/   88]
per-ex loss: 0.454533  [   55/   88]
per-ex loss: 0.733455  [   56/   88]
per-ex loss: 0.736207  [   57/   88]
per-ex loss: 0.535973  [   58/   88]
per-ex loss: 0.523132  [   59/   88]
per-ex loss: 0.496167  [   60/   88]
per-ex loss: 0.530982  [   61/   88]
per-ex loss: 0.684693  [   62/   88]
per-ex loss: 0.496895  [   63/   88]
per-ex loss: 0.512752  [   64/   88]
per-ex loss: 0.576040  [   65/   88]
per-ex loss: 0.710259  [   66/   88]
per-ex loss: 0.466967  [   67/   88]
per-ex loss: 0.561444  [   68/   88]
per-ex loss: 0.615951  [   69/   88]
per-ex loss: 0.730800  [   70/   88]
per-ex loss: 0.543218  [   71/   88]
per-ex loss: 0.703451  [   72/   88]
per-ex loss: 0.682291  [   73/   88]
per-ex loss: 0.476098  [   74/   88]
per-ex loss: 0.675400  [   75/   88]
per-ex loss: 0.497981  [   76/   88]
per-ex loss: 0.781986  [   77/   88]
per-ex loss: 0.456532  [   78/   88]
per-ex loss: 0.601669  [   79/   88]
per-ex loss: 0.497336  [   80/   88]
per-ex loss: 0.522959  [   81/   88]
per-ex loss: 0.562891  [   82/   88]
per-ex loss: 0.718629  [   83/   88]
per-ex loss: 0.632525  [   84/   88]
per-ex loss: 0.722875  [   85/   88]
per-ex loss: 0.785957  [   86/   88]
per-ex loss: 0.697044  [   87/   88]
per-ex loss: 0.772966  [   88/   88]
Train Error: Avg loss: 0.60333868
validation Error: 
 Avg loss: 0.70567818 
 F1: 0.479187 
 Precision: 0.501529 
 Recall: 0.458751
 IoU: 0.315087

test Error: 
 Avg loss: 0.64275686 
 F1: 0.565990 
 Precision: 0.611764 
 Recall: 0.526589
 IoU: 0.394691

We have finished training iteration 123
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_121_.pth
per-ex loss: 0.479181  [    1/   88]
per-ex loss: 0.543816  [    2/   88]
per-ex loss: 0.711219  [    3/   88]
per-ex loss: 0.554357  [    4/   88]
per-ex loss: 0.500491  [    5/   88]
per-ex loss: 0.516426  [    6/   88]
per-ex loss: 0.581995  [    7/   88]
per-ex loss: 0.726255  [    8/   88]
per-ex loss: 0.501236  [    9/   88]
per-ex loss: 0.692849  [   10/   88]
per-ex loss: 0.616576  [   11/   88]
per-ex loss: 0.450915  [   12/   88]
per-ex loss: 0.751201  [   13/   88]
per-ex loss: 0.695142  [   14/   88]
per-ex loss: 0.446378  [   15/   88]
per-ex loss: 0.601357  [   16/   88]
per-ex loss: 0.523238  [   17/   88]
per-ex loss: 0.724030  [   18/   88]
per-ex loss: 0.730454  [   19/   88]
per-ex loss: 0.533465  [   20/   88]
per-ex loss: 0.477147  [   21/   88]
per-ex loss: 0.581845  [   22/   88]
per-ex loss: 0.685142  [   23/   88]
per-ex loss: 0.721524  [   24/   88]
per-ex loss: 0.691256  [   25/   88]
per-ex loss: 0.510726  [   26/   88]
per-ex loss: 0.772338  [   27/   88]
per-ex loss: 0.734205  [   28/   88]
per-ex loss: 0.535141  [   29/   88]
per-ex loss: 0.553337  [   30/   88]
per-ex loss: 0.606033  [   31/   88]
per-ex loss: 0.505688  [   32/   88]
per-ex loss: 0.491272  [   33/   88]
per-ex loss: 0.575753  [   34/   88]
per-ex loss: 0.668690  [   35/   88]
per-ex loss: 0.707935  [   36/   88]
per-ex loss: 0.557207  [   37/   88]
per-ex loss: 0.640230  [   38/   88]
per-ex loss: 0.491735  [   39/   88]
per-ex loss: 0.742963  [   40/   88]
per-ex loss: 0.728459  [   41/   88]
per-ex loss: 0.731878  [   42/   88]
per-ex loss: 0.486071  [   43/   88]
per-ex loss: 0.534302  [   44/   88]
per-ex loss: 0.821041  [   45/   88]
per-ex loss: 0.626805  [   46/   88]
per-ex loss: 0.524686  [   47/   88]
per-ex loss: 0.663889  [   48/   88]
per-ex loss: 0.620279  [   49/   88]
per-ex loss: 0.592320  [   50/   88]
per-ex loss: 0.571694  [   51/   88]
per-ex loss: 0.528709  [   52/   88]
per-ex loss: 0.556481  [   53/   88]
per-ex loss: 0.545301  [   54/   88]
per-ex loss: 0.785197  [   55/   88]
per-ex loss: 0.514546  [   56/   88]
per-ex loss: 0.515818  [   57/   88]
per-ex loss: 0.646818  [   58/   88]
per-ex loss: 0.748442  [   59/   88]
per-ex loss: 0.501299  [   60/   88]
per-ex loss: 0.789513  [   61/   88]
per-ex loss: 0.544448  [   62/   88]
per-ex loss: 0.700828  [   63/   88]
per-ex loss: 0.528975  [   64/   88]
per-ex loss: 0.635555  [   65/   88]
per-ex loss: 0.430843  [   66/   88]
per-ex loss: 0.524996  [   67/   88]
per-ex loss: 0.777970  [   68/   88]
per-ex loss: 0.538129  [   69/   88]
per-ex loss: 0.681554  [   70/   88]
per-ex loss: 0.771383  [   71/   88]
per-ex loss: 0.621026  [   72/   88]
per-ex loss: 0.452873  [   73/   88]
per-ex loss: 0.469954  [   74/   88]
per-ex loss: 0.661003  [   75/   88]
per-ex loss: 0.723642  [   76/   88]
per-ex loss: 0.526926  [   77/   88]
per-ex loss: 0.604200  [   78/   88]
per-ex loss: 0.686388  [   79/   88]
per-ex loss: 0.645108  [   80/   88]
per-ex loss: 0.571714  [   81/   88]
per-ex loss: 0.532616  [   82/   88]
per-ex loss: 0.532721  [   83/   88]
per-ex loss: 0.666474  [   84/   88]
per-ex loss: 0.618807  [   85/   88]
per-ex loss: 0.440105  [   86/   88]
per-ex loss: 0.635350  [   87/   88]
per-ex loss: 0.584842  [   88/   88]
Train Error: Avg loss: 0.60537189
validation Error: 
 Avg loss: 0.70494536 
 F1: 0.478533 
 Precision: 0.482019 
 Recall: 0.475096
 IoU: 0.314520

test Error: 
 Avg loss: 0.63924667 
 F1: 0.570201 
 Precision: 0.583496 
 Recall: 0.557499
 IoU: 0.398798

We have finished training iteration 124
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_122_.pth
per-ex loss: 0.427750  [    1/   88]
per-ex loss: 0.733826  [    2/   88]
per-ex loss: 0.597025  [    3/   88]
per-ex loss: 0.677732  [    4/   88]
per-ex loss: 0.439747  [    5/   88]
per-ex loss: 0.731667  [    6/   88]
per-ex loss: 0.739771  [    7/   88]
per-ex loss: 0.559972  [    8/   88]
per-ex loss: 0.492309  [    9/   88]
per-ex loss: 0.544180  [   10/   88]
per-ex loss: 0.575390  [   11/   88]
per-ex loss: 0.629954  [   12/   88]
per-ex loss: 0.776460  [   13/   88]
per-ex loss: 0.628955  [   14/   88]
per-ex loss: 0.697064  [   15/   88]
per-ex loss: 0.775863  [   16/   88]
per-ex loss: 0.588082  [   17/   88]
per-ex loss: 0.587936  [   18/   88]
per-ex loss: 0.483521  [   19/   88]
per-ex loss: 0.754200  [   20/   88]
per-ex loss: 0.418537  [   21/   88]
per-ex loss: 0.462585  [   22/   88]
per-ex loss: 0.745618  [   23/   88]
per-ex loss: 0.679911  [   24/   88]
per-ex loss: 0.618870  [   25/   88]
per-ex loss: 0.514053  [   26/   88]
per-ex loss: 0.702396  [   27/   88]
per-ex loss: 0.482421  [   28/   88]
per-ex loss: 0.606561  [   29/   88]
per-ex loss: 0.576843  [   30/   88]
per-ex loss: 0.688053  [   31/   88]
per-ex loss: 0.537947  [   32/   88]
per-ex loss: 0.667787  [   33/   88]
per-ex loss: 0.550909  [   34/   88]
per-ex loss: 0.610740  [   35/   88]
per-ex loss: 0.524639  [   36/   88]
per-ex loss: 0.731068  [   37/   88]
per-ex loss: 0.514512  [   38/   88]
per-ex loss: 0.779566  [   39/   88]
per-ex loss: 0.478219  [   40/   88]
per-ex loss: 0.590131  [   41/   88]
per-ex loss: 0.645796  [   42/   88]
per-ex loss: 0.527940  [   43/   88]
per-ex loss: 0.451134  [   44/   88]
per-ex loss: 0.503991  [   45/   88]
per-ex loss: 0.617813  [   46/   88]
per-ex loss: 0.624529  [   47/   88]
per-ex loss: 0.704411  [   48/   88]
per-ex loss: 0.360725  [   49/   88]
per-ex loss: 0.523584  [   50/   88]
per-ex loss: 0.805913  [   51/   88]
per-ex loss: 0.539157  [   52/   88]
per-ex loss: 0.767577  [   53/   88]
per-ex loss: 0.585597  [   54/   88]
per-ex loss: 0.500660  [   55/   88]
per-ex loss: 0.633751  [   56/   88]
per-ex loss: 0.689864  [   57/   88]
per-ex loss: 0.566857  [   58/   88]
per-ex loss: 0.545627  [   59/   88]
per-ex loss: 0.518882  [   60/   88]
per-ex loss: 0.517705  [   61/   88]
per-ex loss: 0.735759  [   62/   88]
per-ex loss: 0.732789  [   63/   88]
per-ex loss: 0.654291  [   64/   88]
per-ex loss: 0.557261  [   65/   88]
per-ex loss: 0.619940  [   66/   88]
per-ex loss: 0.755083  [   67/   88]
per-ex loss: 0.733704  [   68/   88]
per-ex loss: 0.760164  [   69/   88]
per-ex loss: 0.491059  [   70/   88]
per-ex loss: 0.705033  [   71/   88]
per-ex loss: 0.719433  [   72/   88]
per-ex loss: 0.602396  [   73/   88]
per-ex loss: 0.562953  [   74/   88]
per-ex loss: 0.531031  [   75/   88]
per-ex loss: 0.457972  [   76/   88]
per-ex loss: 0.639051  [   77/   88]
per-ex loss: 0.486962  [   78/   88]
per-ex loss: 0.546705  [   79/   88]
per-ex loss: 0.685415  [   80/   88]
per-ex loss: 0.528095  [   81/   88]
per-ex loss: 0.571631  [   82/   88]
per-ex loss: 0.532413  [   83/   88]
per-ex loss: 0.532261  [   84/   88]
per-ex loss: 0.483342  [   85/   88]
per-ex loss: 0.495999  [   86/   88]
per-ex loss: 0.549506  [   87/   88]
per-ex loss: 0.728518  [   88/   88]
Train Error: Avg loss: 0.60173920
validation Error: 
 Avg loss: 0.71374858 
 F1: 0.467140 
 Precision: 0.539745 
 Recall: 0.411752
 IoU: 0.304750

test Error: 
 Avg loss: 0.64649820 
 F1: 0.558814 
 Precision: 0.670706 
 Recall: 0.478917
 IoU: 0.387746

We have finished training iteration 125
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_123_.pth
per-ex loss: 0.486736  [    1/   88]
per-ex loss: 0.719047  [    2/   88]
per-ex loss: 0.721833  [    3/   88]
per-ex loss: 0.500737  [    4/   88]
per-ex loss: 0.513452  [    5/   88]
per-ex loss: 0.610745  [    6/   88]
per-ex loss: 0.736846  [    7/   88]
per-ex loss: 0.775193  [    8/   88]
per-ex loss: 0.542943  [    9/   88]
per-ex loss: 0.507086  [   10/   88]
per-ex loss: 0.745070  [   11/   88]
per-ex loss: 0.721150  [   12/   88]
per-ex loss: 0.558936  [   13/   88]
per-ex loss: 0.596257  [   14/   88]
per-ex loss: 0.731397  [   15/   88]
per-ex loss: 0.522129  [   16/   88]
per-ex loss: 0.479072  [   17/   88]
per-ex loss: 0.492549  [   18/   88]
per-ex loss: 0.567629  [   19/   88]
per-ex loss: 0.680235  [   20/   88]
per-ex loss: 0.633960  [   21/   88]
per-ex loss: 0.509844  [   22/   88]
per-ex loss: 0.715890  [   23/   88]
per-ex loss: 0.478364  [   24/   88]
per-ex loss: 0.693013  [   25/   88]
per-ex loss: 0.583628  [   26/   88]
per-ex loss: 0.508471  [   27/   88]
per-ex loss: 0.483489  [   28/   88]
per-ex loss: 0.693025  [   29/   88]
per-ex loss: 0.571488  [   30/   88]
per-ex loss: 0.734416  [   31/   88]
per-ex loss: 0.490844  [   32/   88]
per-ex loss: 0.579558  [   33/   88]
per-ex loss: 0.774144  [   34/   88]
per-ex loss: 0.568962  [   35/   88]
per-ex loss: 0.558719  [   36/   88]
per-ex loss: 0.856112  [   37/   88]
per-ex loss: 0.454307  [   38/   88]
per-ex loss: 0.713367  [   39/   88]
per-ex loss: 0.567153  [   40/   88]
per-ex loss: 0.610532  [   41/   88]
per-ex loss: 0.446613  [   42/   88]
per-ex loss: 0.697206  [   43/   88]
per-ex loss: 0.495720  [   44/   88]
per-ex loss: 0.481780  [   45/   88]
per-ex loss: 0.729703  [   46/   88]
per-ex loss: 0.374074  [   47/   88]
per-ex loss: 0.737025  [   48/   88]
per-ex loss: 0.595840  [   49/   88]
per-ex loss: 0.533401  [   50/   88]
per-ex loss: 0.776935  [   51/   88]
per-ex loss: 0.520802  [   52/   88]
per-ex loss: 0.571006  [   53/   88]
per-ex loss: 0.520060  [   54/   88]
per-ex loss: 0.552332  [   55/   88]
per-ex loss: 0.661315  [   56/   88]
per-ex loss: 0.472289  [   57/   88]
per-ex loss: 0.678568  [   58/   88]
per-ex loss: 0.524774  [   59/   88]
per-ex loss: 0.618812  [   60/   88]
per-ex loss: 0.571611  [   61/   88]
per-ex loss: 0.649444  [   62/   88]
per-ex loss: 0.756566  [   63/   88]
per-ex loss: 0.699536  [   64/   88]
per-ex loss: 0.492268  [   65/   88]
per-ex loss: 0.735757  [   66/   88]
per-ex loss: 0.552322  [   67/   88]
per-ex loss: 0.682476  [   68/   88]
per-ex loss: 0.515155  [   69/   88]
per-ex loss: 0.564031  [   70/   88]
per-ex loss: 0.469751  [   71/   88]
per-ex loss: 0.717061  [   72/   88]
per-ex loss: 0.520692  [   73/   88]
per-ex loss: 0.604874  [   74/   88]
per-ex loss: 0.538105  [   75/   88]
per-ex loss: 0.654921  [   76/   88]
per-ex loss: 0.514371  [   77/   88]
per-ex loss: 0.451779  [   78/   88]
per-ex loss: 0.619684  [   79/   88]
per-ex loss: 0.621165  [   80/   88]
per-ex loss: 0.669766  [   81/   88]
per-ex loss: 0.673767  [   82/   88]
per-ex loss: 0.628851  [   83/   88]
per-ex loss: 0.560253  [   84/   88]
per-ex loss: 0.602803  [   85/   88]
per-ex loss: 0.770176  [   86/   88]
per-ex loss: 0.656220  [   87/   88]
per-ex loss: 0.644793  [   88/   88]
Train Error: Avg loss: 0.60357701
validation Error: 
 Avg loss: 0.69629183 
 F1: 0.487800 
 Precision: 0.510914 
 Recall: 0.466688
 IoU: 0.322577

test Error: 
 Avg loss: 0.63262755 
 F1: 0.575412 
 Precision: 0.613342 
 Recall: 0.541900
 IoU: 0.403915

We have finished training iteration 126
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_124_.pth
per-ex loss: 0.469381  [    1/   88]
per-ex loss: 0.640300  [    2/   88]
per-ex loss: 0.512474  [    3/   88]
per-ex loss: 0.605397  [    4/   88]
per-ex loss: 0.694478  [    5/   88]
per-ex loss: 0.694241  [    6/   88]
per-ex loss: 0.688573  [    7/   88]
per-ex loss: 0.775102  [    8/   88]
per-ex loss: 0.538566  [    9/   88]
per-ex loss: 0.545818  [   10/   88]
per-ex loss: 0.585025  [   11/   88]
per-ex loss: 0.493459  [   12/   88]
per-ex loss: 0.626214  [   13/   88]
per-ex loss: 0.590008  [   14/   88]
per-ex loss: 0.554111  [   15/   88]
per-ex loss: 0.612231  [   16/   88]
per-ex loss: 0.593994  [   17/   88]
per-ex loss: 0.656662  [   18/   88]
per-ex loss: 0.337002  [   19/   88]
per-ex loss: 0.728620  [   20/   88]
per-ex loss: 0.497062  [   21/   88]
per-ex loss: 0.790058  [   22/   88]
per-ex loss: 0.546461  [   23/   88]
per-ex loss: 0.508769  [   24/   88]
per-ex loss: 0.562467  [   25/   88]
per-ex loss: 0.566724  [   26/   88]
per-ex loss: 0.579481  [   27/   88]
per-ex loss: 0.700388  [   28/   88]
per-ex loss: 0.498927  [   29/   88]
per-ex loss: 0.480037  [   30/   88]
per-ex loss: 0.504409  [   31/   88]
per-ex loss: 0.467441  [   32/   88]
per-ex loss: 0.649085  [   33/   88]
per-ex loss: 0.704568  [   34/   88]
per-ex loss: 0.687832  [   35/   88]
per-ex loss: 0.803573  [   36/   88]
per-ex loss: 0.508653  [   37/   88]
per-ex loss: 0.503677  [   38/   88]
per-ex loss: 0.715428  [   39/   88]
per-ex loss: 0.432559  [   40/   88]
per-ex loss: 0.658727  [   41/   88]
per-ex loss: 0.479227  [   42/   88]
per-ex loss: 0.769929  [   43/   88]
per-ex loss: 0.533093  [   44/   88]
per-ex loss: 0.489075  [   45/   88]
per-ex loss: 0.518670  [   46/   88]
per-ex loss: 0.528426  [   47/   88]
per-ex loss: 0.737934  [   48/   88]
per-ex loss: 0.572629  [   49/   88]
per-ex loss: 0.479628  [   50/   88]
per-ex loss: 0.665373  [   51/   88]
per-ex loss: 0.618286  [   52/   88]
per-ex loss: 0.557213  [   53/   88]
per-ex loss: 0.784492  [   54/   88]
per-ex loss: 0.625528  [   55/   88]
per-ex loss: 0.532154  [   56/   88]
per-ex loss: 0.602118  [   57/   88]
per-ex loss: 0.600757  [   58/   88]
per-ex loss: 0.721290  [   59/   88]
per-ex loss: 0.765007  [   60/   88]
per-ex loss: 0.491911  [   61/   88]
per-ex loss: 0.647677  [   62/   88]
per-ex loss: 0.693448  [   63/   88]
per-ex loss: 0.530758  [   64/   88]
per-ex loss: 0.716722  [   65/   88]
per-ex loss: 0.679081  [   66/   88]
per-ex loss: 0.725080  [   67/   88]
per-ex loss: 0.444067  [   68/   88]
per-ex loss: 0.734726  [   69/   88]
per-ex loss: 0.611773  [   70/   88]
per-ex loss: 0.599295  [   71/   88]
per-ex loss: 0.772114  [   72/   88]
per-ex loss: 0.483886  [   73/   88]
per-ex loss: 0.471891  [   74/   88]
per-ex loss: 0.543740  [   75/   88]
per-ex loss: 0.714391  [   76/   88]
per-ex loss: 0.712224  [   77/   88]
per-ex loss: 0.523133  [   78/   88]
per-ex loss: 0.532825  [   79/   88]
per-ex loss: 0.548265  [   80/   88]
per-ex loss: 0.523341  [   81/   88]
per-ex loss: 0.783373  [   82/   88]
per-ex loss: 0.632000  [   83/   88]
per-ex loss: 0.771301  [   84/   88]
per-ex loss: 0.553089  [   85/   88]
per-ex loss: 0.448877  [   86/   88]
per-ex loss: 0.499475  [   87/   88]
per-ex loss: 0.590312  [   88/   88]
Train Error: Avg loss: 0.60076799
validation Error: 
 Avg loss: 0.69797800 
 F1: 0.486487 
 Precision: 0.568325 
 Recall: 0.425252
 IoU: 0.321429

test Error: 
 Avg loss: 0.64578721 
 F1: 0.563584 
 Precision: 0.667269 
 Recall: 0.487788
 IoU: 0.392354

We have finished training iteration 127
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_125_.pth
per-ex loss: 0.662777  [    1/   88]
per-ex loss: 0.571087  [    2/   88]
per-ex loss: 0.379363  [    3/   88]
per-ex loss: 0.520818  [    4/   88]
per-ex loss: 0.564555  [    5/   88]
per-ex loss: 0.767575  [    6/   88]
per-ex loss: 0.642744  [    7/   88]
per-ex loss: 0.684025  [    8/   88]
per-ex loss: 0.625738  [    9/   88]
per-ex loss: 0.505272  [   10/   88]
per-ex loss: 0.769770  [   11/   88]
per-ex loss: 0.622955  [   12/   88]
per-ex loss: 0.619223  [   13/   88]
per-ex loss: 0.719945  [   14/   88]
per-ex loss: 0.713100  [   15/   88]
per-ex loss: 0.447867  [   16/   88]
per-ex loss: 0.491637  [   17/   88]
per-ex loss: 0.709251  [   18/   88]
per-ex loss: 0.707326  [   19/   88]
per-ex loss: 0.489177  [   20/   88]
per-ex loss: 0.425324  [   21/   88]
per-ex loss: 0.609785  [   22/   88]
per-ex loss: 0.530550  [   23/   88]
per-ex loss: 0.715373  [   24/   88]
per-ex loss: 0.731228  [   25/   88]
per-ex loss: 0.481920  [   26/   88]
per-ex loss: 0.509041  [   27/   88]
per-ex loss: 0.477444  [   28/   88]
per-ex loss: 0.634351  [   29/   88]
per-ex loss: 0.523398  [   30/   88]
per-ex loss: 0.774544  [   31/   88]
per-ex loss: 0.526853  [   32/   88]
per-ex loss: 0.594368  [   33/   88]
per-ex loss: 0.755390  [   34/   88]
per-ex loss: 0.513176  [   35/   88]
per-ex loss: 0.605065  [   36/   88]
per-ex loss: 0.496445  [   37/   88]
per-ex loss: 0.520340  [   38/   88]
per-ex loss: 0.730355  [   39/   88]
per-ex loss: 0.592006  [   40/   88]
per-ex loss: 0.771361  [   41/   88]
per-ex loss: 0.581740  [   42/   88]
per-ex loss: 0.517651  [   43/   88]
per-ex loss: 0.491000  [   44/   88]
per-ex loss: 0.747785  [   45/   88]
per-ex loss: 0.506773  [   46/   88]
per-ex loss: 0.578913  [   47/   88]
per-ex loss: 0.497350  [   48/   88]
per-ex loss: 0.554039  [   49/   88]
per-ex loss: 0.673020  [   50/   88]
per-ex loss: 0.735210  [   51/   88]
per-ex loss: 0.490003  [   52/   88]
per-ex loss: 0.475817  [   53/   88]
per-ex loss: 0.678367  [   54/   88]
per-ex loss: 0.530063  [   55/   88]
per-ex loss: 0.674093  [   56/   88]
per-ex loss: 0.467380  [   57/   88]
per-ex loss: 0.651813  [   58/   88]
per-ex loss: 0.688655  [   59/   88]
per-ex loss: 0.550237  [   60/   88]
per-ex loss: 0.484422  [   61/   88]
per-ex loss: 0.499413  [   62/   88]
per-ex loss: 0.682358  [   63/   88]
per-ex loss: 0.705347  [   64/   88]
per-ex loss: 0.779145  [   65/   88]
per-ex loss: 0.573811  [   66/   88]
per-ex loss: 0.630628  [   67/   88]
per-ex loss: 0.532861  [   68/   88]
per-ex loss: 0.533949  [   69/   88]
per-ex loss: 0.784702  [   70/   88]
per-ex loss: 0.734823  [   71/   88]
per-ex loss: 0.446670  [   72/   88]
per-ex loss: 0.585339  [   73/   88]
per-ex loss: 0.618207  [   74/   88]
per-ex loss: 0.452235  [   75/   88]
per-ex loss: 0.477839  [   76/   88]
per-ex loss: 0.528510  [   77/   88]
per-ex loss: 0.638354  [   78/   88]
per-ex loss: 0.711747  [   79/   88]
per-ex loss: 0.623656  [   80/   88]
per-ex loss: 0.570972  [   81/   88]
per-ex loss: 0.667770  [   82/   88]
per-ex loss: 0.615399  [   83/   88]
per-ex loss: 0.450526  [   84/   88]
per-ex loss: 0.719820  [   85/   88]
per-ex loss: 0.551063  [   86/   88]
per-ex loss: 0.679958  [   87/   88]
per-ex loss: 0.500081  [   88/   88]
Train Error: Avg loss: 0.59775039
validation Error: 
 Avg loss: 0.69678242 
 F1: 0.490067 
 Precision: 0.566620 
 Recall: 0.431737
 IoU: 0.324562

test Error: 
 Avg loss: 0.64041997 
 F1: 0.567525 
 Precision: 0.658584 
 Recall: 0.498588
 IoU: 0.396185

We have finished training iteration 128
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_126_.pth
per-ex loss: 0.585198  [    1/   88]
per-ex loss: 0.574151  [    2/   88]
per-ex loss: 0.709293  [    3/   88]
per-ex loss: 0.654194  [    4/   88]
per-ex loss: 0.726741  [    5/   88]
per-ex loss: 0.508461  [    6/   88]
per-ex loss: 0.642769  [    7/   88]
per-ex loss: 0.558690  [    8/   88]
per-ex loss: 0.461163  [    9/   88]
per-ex loss: 0.598074  [   10/   88]
per-ex loss: 0.559867  [   11/   88]
per-ex loss: 0.548285  [   12/   88]
per-ex loss: 0.778551  [   13/   88]
per-ex loss: 0.449366  [   14/   88]
per-ex loss: 0.625832  [   15/   88]
per-ex loss: 0.760082  [   16/   88]
per-ex loss: 0.524436  [   17/   88]
per-ex loss: 0.610515  [   18/   88]
per-ex loss: 0.751895  [   19/   88]
per-ex loss: 0.771245  [   20/   88]
per-ex loss: 0.507150  [   21/   88]
per-ex loss: 0.670761  [   22/   88]
per-ex loss: 0.436009  [   23/   88]
per-ex loss: 0.563770  [   24/   88]
per-ex loss: 0.594922  [   25/   88]
per-ex loss: 0.555940  [   26/   88]
per-ex loss: 0.374343  [   27/   88]
per-ex loss: 0.598762  [   28/   88]
per-ex loss: 0.779961  [   29/   88]
per-ex loss: 0.557215  [   30/   88]
per-ex loss: 0.536599  [   31/   88]
per-ex loss: 0.454003  [   32/   88]
per-ex loss: 0.710997  [   33/   88]
per-ex loss: 0.465111  [   34/   88]
per-ex loss: 0.737974  [   35/   88]
per-ex loss: 0.684024  [   36/   88]
per-ex loss: 0.493660  [   37/   88]
per-ex loss: 0.588720  [   38/   88]
per-ex loss: 0.693112  [   39/   88]
per-ex loss: 0.767821  [   40/   88]
per-ex loss: 0.714903  [   41/   88]
per-ex loss: 0.509256  [   42/   88]
per-ex loss: 0.494522  [   43/   88]
per-ex loss: 0.580252  [   44/   88]
per-ex loss: 0.732930  [   45/   88]
per-ex loss: 0.623531  [   46/   88]
per-ex loss: 0.718419  [   47/   88]
per-ex loss: 0.506442  [   48/   88]
per-ex loss: 0.577544  [   49/   88]
per-ex loss: 0.689088  [   50/   88]
per-ex loss: 0.524226  [   51/   88]
per-ex loss: 0.686549  [   52/   88]
per-ex loss: 0.563607  [   53/   88]
per-ex loss: 0.612823  [   54/   88]
per-ex loss: 0.494319  [   55/   88]
per-ex loss: 0.616060  [   56/   88]
per-ex loss: 0.434282  [   57/   88]
per-ex loss: 0.661129  [   58/   88]
per-ex loss: 0.479456  [   59/   88]
per-ex loss: 0.528116  [   60/   88]
per-ex loss: 0.609504  [   61/   88]
per-ex loss: 0.496697  [   62/   88]
per-ex loss: 0.529741  [   63/   88]
per-ex loss: 0.772821  [   64/   88]
per-ex loss: 0.703383  [   65/   88]
per-ex loss: 0.751070  [   66/   88]
per-ex loss: 0.496046  [   67/   88]
per-ex loss: 0.697328  [   68/   88]
per-ex loss: 0.615304  [   69/   88]
per-ex loss: 0.481084  [   70/   88]
per-ex loss: 0.470035  [   71/   88]
per-ex loss: 0.613798  [   72/   88]
per-ex loss: 0.524511  [   73/   88]
per-ex loss: 0.518493  [   74/   88]
per-ex loss: 0.685920  [   75/   88]
per-ex loss: 0.528086  [   76/   88]
per-ex loss: 0.781420  [   77/   88]
per-ex loss: 0.589024  [   78/   88]
per-ex loss: 0.539599  [   79/   88]
per-ex loss: 0.516682  [   80/   88]
per-ex loss: 0.517793  [   81/   88]
per-ex loss: 0.636468  [   82/   88]
per-ex loss: 0.540065  [   83/   88]
per-ex loss: 0.667550  [   84/   88]
per-ex loss: 0.723857  [   85/   88]
per-ex loss: 0.504778  [   86/   88]
per-ex loss: 0.728824  [   87/   88]
per-ex loss: 0.713958  [   88/   88]
Train Error: Avg loss: 0.60080628
validation Error: 
 Avg loss: 0.69549211 
 F1: 0.487106 
 Precision: 0.590379 
 Recall: 0.414584
 IoU: 0.321969

test Error: 
 Avg loss: 0.64483098 
 F1: 0.563258 
 Precision: 0.680739 
 Recall: 0.480358
 IoU: 0.392038

We have finished training iteration 129
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_127_.pth
per-ex loss: 0.524080  [    1/   88]
per-ex loss: 0.729806  [    2/   88]
per-ex loss: 0.604731  [    3/   88]
per-ex loss: 0.573915  [    4/   88]
per-ex loss: 0.493457  [    5/   88]
per-ex loss: 0.657735  [    6/   88]
per-ex loss: 0.722132  [    7/   88]
per-ex loss: 0.511897  [    8/   88]
per-ex loss: 0.490117  [    9/   88]
per-ex loss: 0.522447  [   10/   88]
per-ex loss: 0.609619  [   11/   88]
per-ex loss: 0.511981  [   12/   88]
per-ex loss: 0.622976  [   13/   88]
per-ex loss: 0.428894  [   14/   88]
per-ex loss: 0.786375  [   15/   88]
per-ex loss: 0.608712  [   16/   88]
per-ex loss: 0.504346  [   17/   88]
per-ex loss: 0.760628  [   18/   88]
per-ex loss: 0.526853  [   19/   88]
per-ex loss: 0.445698  [   20/   88]
per-ex loss: 0.511414  [   21/   88]
per-ex loss: 0.739655  [   22/   88]
per-ex loss: 0.563660  [   23/   88]
per-ex loss: 0.533424  [   24/   88]
per-ex loss: 0.618191  [   25/   88]
per-ex loss: 0.642897  [   26/   88]
per-ex loss: 0.524918  [   27/   88]
per-ex loss: 0.745323  [   28/   88]
per-ex loss: 0.734899  [   29/   88]
per-ex loss: 0.533209  [   30/   88]
per-ex loss: 0.594915  [   31/   88]
per-ex loss: 0.658853  [   32/   88]
per-ex loss: 0.534512  [   33/   88]
per-ex loss: 0.537431  [   34/   88]
per-ex loss: 0.692352  [   35/   88]
per-ex loss: 0.721650  [   36/   88]
per-ex loss: 0.576112  [   37/   88]
per-ex loss: 0.708262  [   38/   88]
per-ex loss: 0.685206  [   39/   88]
per-ex loss: 0.696075  [   40/   88]
per-ex loss: 0.720446  [   41/   88]
per-ex loss: 0.515854  [   42/   88]
per-ex loss: 0.505012  [   43/   88]
per-ex loss: 0.767888  [   44/   88]
per-ex loss: 0.556588  [   45/   88]
per-ex loss: 0.648375  [   46/   88]
per-ex loss: 0.552689  [   47/   88]
per-ex loss: 0.362076  [   48/   88]
per-ex loss: 0.684406  [   49/   88]
per-ex loss: 0.493426  [   50/   88]
per-ex loss: 0.777228  [   51/   88]
per-ex loss: 0.666310  [   52/   88]
per-ex loss: 0.453024  [   53/   88]
per-ex loss: 0.466608  [   54/   88]
per-ex loss: 0.525357  [   55/   88]
per-ex loss: 0.431560  [   56/   88]
per-ex loss: 0.568451  [   57/   88]
per-ex loss: 0.658076  [   58/   88]
per-ex loss: 0.487846  [   59/   88]
per-ex loss: 0.568059  [   60/   88]
per-ex loss: 0.680874  [   61/   88]
per-ex loss: 0.525084  [   62/   88]
per-ex loss: 0.482518  [   63/   88]
per-ex loss: 0.564944  [   64/   88]
per-ex loss: 0.534033  [   65/   88]
per-ex loss: 0.688233  [   66/   88]
per-ex loss: 0.543874  [   67/   88]
per-ex loss: 0.698833  [   68/   88]
per-ex loss: 0.788431  [   69/   88]
per-ex loss: 0.581287  [   70/   88]
per-ex loss: 0.684213  [   71/   88]
per-ex loss: 0.682018  [   72/   88]
per-ex loss: 0.639210  [   73/   88]
per-ex loss: 0.495847  [   74/   88]
per-ex loss: 0.724525  [   75/   88]
per-ex loss: 0.670574  [   76/   88]
per-ex loss: 0.741973  [   77/   88]
per-ex loss: 0.500739  [   78/   88]
per-ex loss: 0.552680  [   79/   88]
per-ex loss: 0.545905  [   80/   88]
per-ex loss: 0.786552  [   81/   88]
per-ex loss: 0.488394  [   82/   88]
per-ex loss: 0.651308  [   83/   88]
per-ex loss: 0.594205  [   84/   88]
per-ex loss: 0.611142  [   85/   88]
per-ex loss: 0.751812  [   86/   88]
per-ex loss: 0.779063  [   87/   88]
per-ex loss: 0.515333  [   88/   88]
Train Error: Avg loss: 0.60343462
validation Error: 
 Avg loss: 0.71457213 
 F1: 0.460417 
 Precision: 0.431005 
 Recall: 0.494138
 IoU: 0.299053

test Error: 
 Avg loss: 0.64304460 
 F1: 0.564213 
 Precision: 0.546758 
 Recall: 0.582819
 IoU: 0.392964

We have finished training iteration 130
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_128_.pth
per-ex loss: 0.650539  [    1/   88]
per-ex loss: 0.524567  [    2/   88]
per-ex loss: 0.763683  [    3/   88]
per-ex loss: 0.653323  [    4/   88]
per-ex loss: 0.719211  [    5/   88]
per-ex loss: 0.519712  [    6/   88]
per-ex loss: 0.689291  [    7/   88]
per-ex loss: 0.503145  [    8/   88]
per-ex loss: 0.498938  [    9/   88]
per-ex loss: 0.492365  [   10/   88]
per-ex loss: 0.516413  [   11/   88]
per-ex loss: 0.542844  [   12/   88]
per-ex loss: 0.724049  [   13/   88]
per-ex loss: 0.753676  [   14/   88]
per-ex loss: 0.563647  [   15/   88]
per-ex loss: 0.568079  [   16/   88]
per-ex loss: 0.738469  [   17/   88]
per-ex loss: 0.687252  [   18/   88]
per-ex loss: 0.647417  [   19/   88]
per-ex loss: 0.548635  [   20/   88]
per-ex loss: 0.493316  [   21/   88]
per-ex loss: 0.738466  [   22/   88]
per-ex loss: 0.601252  [   23/   88]
per-ex loss: 0.676267  [   24/   88]
per-ex loss: 0.530410  [   25/   88]
per-ex loss: 0.587570  [   26/   88]
per-ex loss: 0.524874  [   27/   88]
per-ex loss: 0.714608  [   28/   88]
per-ex loss: 0.490660  [   29/   88]
per-ex loss: 0.729021  [   30/   88]
per-ex loss: 0.527604  [   31/   88]
per-ex loss: 0.529468  [   32/   88]
per-ex loss: 0.569427  [   33/   88]
per-ex loss: 0.500769  [   34/   88]
per-ex loss: 0.498333  [   35/   88]
per-ex loss: 0.444782  [   36/   88]
per-ex loss: 0.563824  [   37/   88]
per-ex loss: 0.619770  [   38/   88]
per-ex loss: 0.470619  [   39/   88]
per-ex loss: 0.798377  [   40/   88]
per-ex loss: 0.675714  [   41/   88]
per-ex loss: 0.691935  [   42/   88]
per-ex loss: 0.560974  [   43/   88]
per-ex loss: 0.806025  [   44/   88]
per-ex loss: 0.725941  [   45/   88]
per-ex loss: 0.594550  [   46/   88]
per-ex loss: 0.685226  [   47/   88]
per-ex loss: 0.488169  [   48/   88]
per-ex loss: 0.599016  [   49/   88]
per-ex loss: 0.535197  [   50/   88]
per-ex loss: 0.442137  [   51/   88]
per-ex loss: 0.361712  [   52/   88]
per-ex loss: 0.621927  [   53/   88]
per-ex loss: 0.522657  [   54/   88]
per-ex loss: 0.710559  [   55/   88]
per-ex loss: 0.555363  [   56/   88]
per-ex loss: 0.787112  [   57/   88]
per-ex loss: 0.678386  [   58/   88]
per-ex loss: 0.547889  [   59/   88]
per-ex loss: 0.597177  [   60/   88]
per-ex loss: 0.439293  [   61/   88]
per-ex loss: 0.626474  [   62/   88]
per-ex loss: 0.631684  [   63/   88]
per-ex loss: 0.556684  [   64/   88]
per-ex loss: 0.482982  [   65/   88]
per-ex loss: 0.560532  [   66/   88]
per-ex loss: 0.538309  [   67/   88]
per-ex loss: 0.456658  [   68/   88]
per-ex loss: 0.502043  [   69/   88]
per-ex loss: 0.751770  [   70/   88]
per-ex loss: 0.520640  [   71/   88]
per-ex loss: 0.521261  [   72/   88]
per-ex loss: 0.714157  [   73/   88]
per-ex loss: 0.543377  [   74/   88]
per-ex loss: 0.725190  [   75/   88]
per-ex loss: 0.632854  [   76/   88]
per-ex loss: 0.723126  [   77/   88]
per-ex loss: 0.586361  [   78/   88]
per-ex loss: 0.690852  [   79/   88]
per-ex loss: 0.739596  [   80/   88]
per-ex loss: 0.626229  [   81/   88]
per-ex loss: 0.581143  [   82/   88]
per-ex loss: 0.513747  [   83/   88]
per-ex loss: 0.619687  [   84/   88]
per-ex loss: 0.614358  [   85/   88]
per-ex loss: 0.619346  [   86/   88]
per-ex loss: 0.494276  [   87/   88]
per-ex loss: 0.779723  [   88/   88]
Train Error: Avg loss: 0.60107600
validation Error: 
 Avg loss: 0.69782562 
 F1: 0.487113 
 Precision: 0.542359 
 Recall: 0.442082
 IoU: 0.321976

test Error: 
 Avg loss: 0.63737904 
 F1: 0.569146 
 Precision: 0.635809 
 Recall: 0.515135
 IoU: 0.397767

We have finished training iteration 131
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_129_.pth
per-ex loss: 0.651446  [    1/   88]
per-ex loss: 0.608614  [    2/   88]
per-ex loss: 0.545583  [    3/   88]
per-ex loss: 0.730348  [    4/   88]
per-ex loss: 0.691561  [    5/   88]
per-ex loss: 0.589693  [    6/   88]
per-ex loss: 0.430201  [    7/   88]
per-ex loss: 0.462048  [    8/   88]
per-ex loss: 0.532813  [    9/   88]
per-ex loss: 0.654075  [   10/   88]
per-ex loss: 0.662317  [   11/   88]
per-ex loss: 0.513135  [   12/   88]
per-ex loss: 0.479991  [   13/   88]
per-ex loss: 0.721142  [   14/   88]
per-ex loss: 0.474245  [   15/   88]
per-ex loss: 0.536017  [   16/   88]
per-ex loss: 0.618183  [   17/   88]
per-ex loss: 0.564852  [   18/   88]
per-ex loss: 0.745155  [   19/   88]
per-ex loss: 0.669781  [   20/   88]
per-ex loss: 0.773699  [   21/   88]
per-ex loss: 0.766141  [   22/   88]
per-ex loss: 0.498255  [   23/   88]
per-ex loss: 0.461237  [   24/   88]
per-ex loss: 0.588529  [   25/   88]
per-ex loss: 0.460592  [   26/   88]
per-ex loss: 0.772562  [   27/   88]
per-ex loss: 0.545931  [   28/   88]
per-ex loss: 0.490509  [   29/   88]
per-ex loss: 0.536195  [   30/   88]
per-ex loss: 0.735045  [   31/   88]
per-ex loss: 0.489490  [   32/   88]
per-ex loss: 0.719632  [   33/   88]
per-ex loss: 0.520233  [   34/   88]
per-ex loss: 0.786012  [   35/   88]
per-ex loss: 0.601730  [   36/   88]
per-ex loss: 0.510303  [   37/   88]
per-ex loss: 0.524039  [   38/   88]
per-ex loss: 0.699591  [   39/   88]
per-ex loss: 0.518513  [   40/   88]
per-ex loss: 0.715782  [   41/   88]
per-ex loss: 0.615032  [   42/   88]
per-ex loss: 0.656405  [   43/   88]
per-ex loss: 0.722755  [   44/   88]
per-ex loss: 0.708365  [   45/   88]
per-ex loss: 0.619367  [   46/   88]
per-ex loss: 0.561520  [   47/   88]
per-ex loss: 0.470958  [   48/   88]
per-ex loss: 0.772727  [   49/   88]
per-ex loss: 0.710190  [   50/   88]
per-ex loss: 0.584614  [   51/   88]
per-ex loss: 0.496865  [   52/   88]
per-ex loss: 0.601180  [   53/   88]
per-ex loss: 0.630490  [   54/   88]
per-ex loss: 0.698591  [   55/   88]
per-ex loss: 0.552841  [   56/   88]
per-ex loss: 0.548140  [   57/   88]
per-ex loss: 0.483722  [   58/   88]
per-ex loss: 0.492894  [   59/   88]
per-ex loss: 0.512507  [   60/   88]
per-ex loss: 0.621371  [   61/   88]
per-ex loss: 0.570501  [   62/   88]
per-ex loss: 0.687626  [   63/   88]
per-ex loss: 0.594458  [   64/   88]
per-ex loss: 0.560947  [   65/   88]
per-ex loss: 0.619155  [   66/   88]
per-ex loss: 0.506512  [   67/   88]
per-ex loss: 0.480341  [   68/   88]
per-ex loss: 0.564277  [   69/   88]
per-ex loss: 0.683221  [   70/   88]
per-ex loss: 0.484727  [   71/   88]
per-ex loss: 0.489521  [   72/   88]
per-ex loss: 0.571732  [   73/   88]
per-ex loss: 0.376617  [   74/   88]
per-ex loss: 0.685549  [   75/   88]
per-ex loss: 0.679686  [   76/   88]
per-ex loss: 0.604606  [   77/   88]
per-ex loss: 0.775950  [   78/   88]
per-ex loss: 0.758138  [   79/   88]
per-ex loss: 0.497222  [   80/   88]
per-ex loss: 0.456802  [   81/   88]
per-ex loss: 0.576168  [   82/   88]
per-ex loss: 0.693464  [   83/   88]
per-ex loss: 0.445164  [   84/   88]
per-ex loss: 0.650898  [   85/   88]
per-ex loss: 0.578230  [   86/   88]
per-ex loss: 0.703014  [   87/   88]
per-ex loss: 0.550457  [   88/   88]
Train Error: Avg loss: 0.59659933
validation Error: 
 Avg loss: 0.69322895 
 F1: 0.492613 
 Precision: 0.519226 
 Recall: 0.468596
 IoU: 0.326800

test Error: 
 Avg loss: 0.63699661 
 F1: 0.569077 
 Precision: 0.606420 
 Recall: 0.536065
 IoU: 0.397699

We have finished training iteration 132
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_130_.pth
per-ex loss: 0.694614  [    1/   88]
per-ex loss: 0.538853  [    2/   88]
per-ex loss: 0.465150  [    3/   88]
per-ex loss: 0.689018  [    4/   88]
per-ex loss: 0.743264  [    5/   88]
per-ex loss: 0.731888  [    6/   88]
per-ex loss: 0.538780  [    7/   88]
per-ex loss: 0.618557  [    8/   88]
per-ex loss: 0.494181  [    9/   88]
per-ex loss: 0.443493  [   10/   88]
per-ex loss: 0.496802  [   11/   88]
per-ex loss: 0.605193  [   12/   88]
per-ex loss: 0.607554  [   13/   88]
per-ex loss: 0.758076  [   14/   88]
per-ex loss: 0.551723  [   15/   88]
per-ex loss: 0.584825  [   16/   88]
per-ex loss: 0.524478  [   17/   88]
per-ex loss: 0.485928  [   18/   88]
per-ex loss: 0.554517  [   19/   88]
per-ex loss: 0.487731  [   20/   88]
per-ex loss: 0.445959  [   21/   88]
per-ex loss: 0.770501  [   22/   88]
per-ex loss: 0.449250  [   23/   88]
per-ex loss: 0.561919  [   24/   88]
per-ex loss: 0.611424  [   25/   88]
per-ex loss: 0.733860  [   26/   88]
per-ex loss: 0.540207  [   27/   88]
per-ex loss: 0.707391  [   28/   88]
per-ex loss: 0.505154  [   29/   88]
per-ex loss: 0.522918  [   30/   88]
per-ex loss: 0.697626  [   31/   88]
per-ex loss: 0.503749  [   32/   88]
per-ex loss: 0.351887  [   33/   88]
per-ex loss: 0.673214  [   34/   88]
per-ex loss: 0.626091  [   35/   88]
per-ex loss: 0.649682  [   36/   88]
per-ex loss: 0.578798  [   37/   88]
per-ex loss: 0.444323  [   38/   88]
per-ex loss: 0.547222  [   39/   88]
per-ex loss: 0.766175  [   40/   88]
per-ex loss: 0.490389  [   41/   88]
per-ex loss: 0.538512  [   42/   88]
per-ex loss: 0.723831  [   43/   88]
per-ex loss: 0.577075  [   44/   88]
per-ex loss: 0.697837  [   45/   88]
per-ex loss: 0.576408  [   46/   88]
per-ex loss: 0.487389  [   47/   88]
per-ex loss: 0.526083  [   48/   88]
per-ex loss: 0.502915  [   49/   88]
per-ex loss: 0.642916  [   50/   88]
per-ex loss: 0.497284  [   51/   88]
per-ex loss: 0.674711  [   52/   88]
per-ex loss: 0.541763  [   53/   88]
per-ex loss: 0.497752  [   54/   88]
per-ex loss: 0.667659  [   55/   88]
per-ex loss: 0.781556  [   56/   88]
per-ex loss: 0.681932  [   57/   88]
per-ex loss: 0.745245  [   58/   88]
per-ex loss: 0.600135  [   59/   88]
per-ex loss: 0.716658  [   60/   88]
per-ex loss: 0.654544  [   61/   88]
per-ex loss: 0.772449  [   62/   88]
per-ex loss: 0.565714  [   63/   88]
per-ex loss: 0.533748  [   64/   88]
per-ex loss: 0.595311  [   65/   88]
per-ex loss: 0.490459  [   66/   88]
per-ex loss: 0.503259  [   67/   88]
per-ex loss: 0.687230  [   68/   88]
per-ex loss: 0.776739  [   69/   88]
per-ex loss: 0.613054  [   70/   88]
per-ex loss: 0.518246  [   71/   88]
per-ex loss: 0.694397  [   72/   88]
per-ex loss: 0.729638  [   73/   88]
per-ex loss: 0.593144  [   74/   88]
per-ex loss: 0.501221  [   75/   88]
per-ex loss: 0.580557  [   76/   88]
per-ex loss: 0.469336  [   77/   88]
per-ex loss: 0.629236  [   78/   88]
per-ex loss: 0.637480  [   79/   88]
per-ex loss: 0.717177  [   80/   88]
per-ex loss: 0.687881  [   81/   88]
per-ex loss: 0.600676  [   82/   88]
per-ex loss: 0.518098  [   83/   88]
per-ex loss: 0.545133  [   84/   88]
per-ex loss: 0.709914  [   85/   88]
per-ex loss: 0.793849  [   86/   88]
per-ex loss: 0.693325  [   87/   88]
per-ex loss: 0.518439  [   88/   88]
Train Error: Avg loss: 0.60000314
validation Error: 
 Avg loss: 0.69872210 
 F1: 0.487559 
 Precision: 0.508485 
 Recall: 0.468287
 IoU: 0.322366

test Error: 
 Avg loss: 0.63403901 
 F1: 0.576132 
 Precision: 0.606565 
 Recall: 0.548607
 IoU: 0.404625

We have finished training iteration 133
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_131_.pth
per-ex loss: 0.760802  [    1/   88]
per-ex loss: 0.567735  [    2/   88]
per-ex loss: 0.469976  [    3/   88]
per-ex loss: 0.691581  [    4/   88]
per-ex loss: 0.714942  [    5/   88]
per-ex loss: 0.592929  [    6/   88]
per-ex loss: 0.362916  [    7/   88]
per-ex loss: 0.460568  [    8/   88]
per-ex loss: 0.686753  [    9/   88]
per-ex loss: 0.716283  [   10/   88]
per-ex loss: 0.728690  [   11/   88]
per-ex loss: 0.684105  [   12/   88]
per-ex loss: 0.575828  [   13/   88]
per-ex loss: 0.486166  [   14/   88]
per-ex loss: 0.759223  [   15/   88]
per-ex loss: 0.574763  [   16/   88]
per-ex loss: 0.625719  [   17/   88]
per-ex loss: 0.616976  [   18/   88]
per-ex loss: 0.517744  [   19/   88]
per-ex loss: 0.648184  [   20/   88]
per-ex loss: 0.542979  [   21/   88]
per-ex loss: 0.520816  [   22/   88]
per-ex loss: 0.709212  [   23/   88]
per-ex loss: 0.676765  [   24/   88]
per-ex loss: 0.523684  [   25/   88]
per-ex loss: 0.656756  [   26/   88]
per-ex loss: 0.597627  [   27/   88]
per-ex loss: 0.580830  [   28/   88]
per-ex loss: 0.768122  [   29/   88]
per-ex loss: 0.563425  [   30/   88]
per-ex loss: 0.467242  [   31/   88]
per-ex loss: 0.687050  [   32/   88]
per-ex loss: 0.498160  [   33/   88]
per-ex loss: 0.559647  [   34/   88]
per-ex loss: 0.510340  [   35/   88]
per-ex loss: 0.773792  [   36/   88]
per-ex loss: 0.511800  [   37/   88]
per-ex loss: 0.634942  [   38/   88]
per-ex loss: 0.774418  [   39/   88]
per-ex loss: 0.732677  [   40/   88]
per-ex loss: 0.563288  [   41/   88]
per-ex loss: 0.548506  [   42/   88]
per-ex loss: 0.500699  [   43/   88]
per-ex loss: 0.534488  [   44/   88]
per-ex loss: 0.476382  [   45/   88]
per-ex loss: 0.506360  [   46/   88]
per-ex loss: 0.692828  [   47/   88]
per-ex loss: 0.689590  [   48/   88]
per-ex loss: 0.683988  [   49/   88]
per-ex loss: 0.636154  [   50/   88]
per-ex loss: 0.561416  [   51/   88]
per-ex loss: 0.703160  [   52/   88]
per-ex loss: 0.527817  [   53/   88]
per-ex loss: 0.485185  [   54/   88]
per-ex loss: 0.513621  [   55/   88]
per-ex loss: 0.483281  [   56/   88]
per-ex loss: 0.538470  [   57/   88]
per-ex loss: 0.508997  [   58/   88]
per-ex loss: 0.504797  [   59/   88]
per-ex loss: 0.564672  [   60/   88]
per-ex loss: 0.738944  [   61/   88]
per-ex loss: 0.717982  [   62/   88]
per-ex loss: 0.615712  [   63/   88]
per-ex loss: 0.661168  [   64/   88]
per-ex loss: 0.721786  [   65/   88]
per-ex loss: 0.495907  [   66/   88]
per-ex loss: 0.489004  [   67/   88]
per-ex loss: 0.445769  [   68/   88]
per-ex loss: 0.623997  [   69/   88]
per-ex loss: 0.487160  [   70/   88]
per-ex loss: 0.435636  [   71/   88]
per-ex loss: 0.612297  [   72/   88]
per-ex loss: 0.608312  [   73/   88]
per-ex loss: 0.555240  [   74/   88]
per-ex loss: 0.618665  [   75/   88]
per-ex loss: 0.574507  [   76/   88]
per-ex loss: 0.714360  [   77/   88]
per-ex loss: 0.593127  [   78/   88]
per-ex loss: 0.486087  [   79/   88]
per-ex loss: 0.631772  [   80/   88]
per-ex loss: 0.531893  [   81/   88]
per-ex loss: 0.767889  [   82/   88]
per-ex loss: 0.586165  [   83/   88]
per-ex loss: 0.706222  [   84/   88]
per-ex loss: 0.466847  [   85/   88]
per-ex loss: 0.728143  [   86/   88]
per-ex loss: 0.642416  [   87/   88]
per-ex loss: 0.492875  [   88/   88]
Train Error: Avg loss: 0.59665624
validation Error: 
 Avg loss: 0.68902269 
 F1: 0.497886 
 Precision: 0.558378 
 Recall: 0.449219
 IoU: 0.331457

test Error: 
 Avg loss: 0.63586452 
 F1: 0.573613 
 Precision: 0.635300 
 Recall: 0.522845
 IoU: 0.402144

We have finished training iteration 134
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_132_.pth
per-ex loss: 0.575274  [    1/   88]
per-ex loss: 0.462527  [    2/   88]
per-ex loss: 0.612549  [    3/   88]
per-ex loss: 0.621266  [    4/   88]
per-ex loss: 0.521026  [    5/   88]
per-ex loss: 0.727630  [    6/   88]
per-ex loss: 0.579506  [    7/   88]
per-ex loss: 0.742785  [    8/   88]
per-ex loss: 0.601371  [    9/   88]
per-ex loss: 0.563347  [   10/   88]
per-ex loss: 0.584767  [   11/   88]
per-ex loss: 0.572666  [   12/   88]
per-ex loss: 0.638459  [   13/   88]
per-ex loss: 0.516762  [   14/   88]
per-ex loss: 0.464671  [   15/   88]
per-ex loss: 0.630470  [   16/   88]
per-ex loss: 0.590180  [   17/   88]
per-ex loss: 0.678255  [   18/   88]
per-ex loss: 0.505515  [   19/   88]
per-ex loss: 0.492731  [   20/   88]
per-ex loss: 0.462865  [   21/   88]
per-ex loss: 0.477239  [   22/   88]
per-ex loss: 0.740734  [   23/   88]
per-ex loss: 0.534550  [   24/   88]
per-ex loss: 0.490091  [   25/   88]
per-ex loss: 0.433120  [   26/   88]
per-ex loss: 0.709080  [   27/   88]
per-ex loss: 0.547845  [   28/   88]
per-ex loss: 0.504431  [   29/   88]
per-ex loss: 0.630071  [   30/   88]
per-ex loss: 0.674167  [   31/   88]
per-ex loss: 0.543813  [   32/   88]
per-ex loss: 0.649214  [   33/   88]
per-ex loss: 0.717898  [   34/   88]
per-ex loss: 0.645656  [   35/   88]
per-ex loss: 0.525724  [   36/   88]
per-ex loss: 0.770933  [   37/   88]
per-ex loss: 0.698686  [   38/   88]
per-ex loss: 0.508867  [   39/   88]
per-ex loss: 0.637482  [   40/   88]
per-ex loss: 0.742211  [   41/   88]
per-ex loss: 0.716706  [   42/   88]
per-ex loss: 0.635902  [   43/   88]
per-ex loss: 0.520016  [   44/   88]
per-ex loss: 0.466407  [   45/   88]
per-ex loss: 0.518498  [   46/   88]
per-ex loss: 0.530847  [   47/   88]
per-ex loss: 0.775044  [   48/   88]
per-ex loss: 0.771471  [   49/   88]
per-ex loss: 0.689917  [   50/   88]
per-ex loss: 0.748487  [   51/   88]
per-ex loss: 0.685488  [   52/   88]
per-ex loss: 0.712780  [   53/   88]
per-ex loss: 0.544314  [   54/   88]
per-ex loss: 0.727938  [   55/   88]
per-ex loss: 0.518011  [   56/   88]
per-ex loss: 0.548283  [   57/   88]
per-ex loss: 0.544590  [   58/   88]
per-ex loss: 0.524171  [   59/   88]
per-ex loss: 0.778892  [   60/   88]
per-ex loss: 0.546061  [   61/   88]
per-ex loss: 0.697940  [   62/   88]
per-ex loss: 0.571444  [   63/   88]
per-ex loss: 0.680728  [   64/   88]
per-ex loss: 0.528749  [   65/   88]
per-ex loss: 0.638587  [   66/   88]
per-ex loss: 0.726704  [   67/   88]
per-ex loss: 0.449417  [   68/   88]
per-ex loss: 0.787056  [   69/   88]
per-ex loss: 0.688213  [   70/   88]
per-ex loss: 0.552342  [   71/   88]
per-ex loss: 0.545118  [   72/   88]
per-ex loss: 0.719043  [   73/   88]
per-ex loss: 0.755085  [   74/   88]
per-ex loss: 0.665919  [   75/   88]
per-ex loss: 0.647834  [   76/   88]
per-ex loss: 0.566332  [   77/   88]
per-ex loss: 0.536329  [   78/   88]
per-ex loss: 0.470590  [   79/   88]
per-ex loss: 0.623693  [   80/   88]
per-ex loss: 0.695164  [   81/   88]
per-ex loss: 0.581555  [   82/   88]
per-ex loss: 0.546024  [   83/   88]
per-ex loss: 0.482192  [   84/   88]
per-ex loss: 0.486819  [   85/   88]
per-ex loss: 0.780242  [   86/   88]
per-ex loss: 0.506966  [   87/   88]
per-ex loss: 0.455637  [   88/   88]
Train Error: Avg loss: 0.60472699
validation Error: 
 Avg loss: 0.69385576 
 F1: 0.492566 
 Precision: 0.598954 
 Recall: 0.418272
 IoU: 0.326758

test Error: 
 Avg loss: 0.64222842 
 F1: 0.565432 
 Precision: 0.682049 
 Recall: 0.482871
 IoU: 0.394148

We have finished training iteration 135
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_133_.pth
per-ex loss: 0.766624  [    1/   88]
per-ex loss: 0.483270  [    2/   88]
per-ex loss: 0.566406  [    3/   88]
per-ex loss: 0.700571  [    4/   88]
per-ex loss: 0.472024  [    5/   88]
per-ex loss: 0.500529  [    6/   88]
per-ex loss: 0.476777  [    7/   88]
per-ex loss: 0.725027  [    8/   88]
per-ex loss: 0.587492  [    9/   88]
per-ex loss: 0.438725  [   10/   88]
per-ex loss: 0.458886  [   11/   88]
per-ex loss: 0.711580  [   12/   88]
per-ex loss: 0.678978  [   13/   88]
per-ex loss: 0.541422  [   14/   88]
per-ex loss: 0.545875  [   15/   88]
per-ex loss: 0.441217  [   16/   88]
per-ex loss: 0.509310  [   17/   88]
per-ex loss: 0.587188  [   18/   88]
per-ex loss: 0.695783  [   19/   88]
per-ex loss: 0.502633  [   20/   88]
per-ex loss: 0.693996  [   21/   88]
per-ex loss: 0.769003  [   22/   88]
per-ex loss: 0.556591  [   23/   88]
per-ex loss: 0.535936  [   24/   88]
per-ex loss: 0.613512  [   25/   88]
per-ex loss: 0.513808  [   26/   88]
per-ex loss: 0.532788  [   27/   88]
per-ex loss: 0.489341  [   28/   88]
per-ex loss: 0.500941  [   29/   88]
per-ex loss: 0.654810  [   30/   88]
per-ex loss: 0.504917  [   31/   88]
per-ex loss: 0.738276  [   32/   88]
per-ex loss: 0.572030  [   33/   88]
per-ex loss: 0.516758  [   34/   88]
per-ex loss: 0.791524  [   35/   88]
per-ex loss: 0.540356  [   36/   88]
per-ex loss: 0.465962  [   37/   88]
per-ex loss: 0.782574  [   38/   88]
per-ex loss: 0.575003  [   39/   88]
per-ex loss: 0.644021  [   40/   88]
per-ex loss: 0.478546  [   41/   88]
per-ex loss: 0.743560  [   42/   88]
per-ex loss: 0.557672  [   43/   88]
per-ex loss: 0.634967  [   44/   88]
per-ex loss: 0.786065  [   45/   88]
per-ex loss: 0.520917  [   46/   88]
per-ex loss: 0.626031  [   47/   88]
per-ex loss: 0.623878  [   48/   88]
per-ex loss: 0.605953  [   49/   88]
per-ex loss: 0.597311  [   50/   88]
per-ex loss: 0.591890  [   51/   88]
per-ex loss: 0.563255  [   52/   88]
per-ex loss: 0.485810  [   53/   88]
per-ex loss: 0.559421  [   54/   88]
per-ex loss: 0.767970  [   55/   88]
per-ex loss: 0.457409  [   56/   88]
per-ex loss: 0.410868  [   57/   88]
per-ex loss: 0.691189  [   58/   88]
per-ex loss: 0.712587  [   59/   88]
per-ex loss: 0.710104  [   60/   88]
per-ex loss: 0.522740  [   61/   88]
per-ex loss: 0.675398  [   62/   88]
per-ex loss: 0.717246  [   63/   88]
per-ex loss: 0.620403  [   64/   88]
per-ex loss: 0.607010  [   65/   88]
per-ex loss: 0.510743  [   66/   88]
per-ex loss: 0.619664  [   67/   88]
per-ex loss: 0.658129  [   68/   88]
per-ex loss: 0.610715  [   69/   88]
per-ex loss: 0.690546  [   70/   88]
per-ex loss: 0.621750  [   71/   88]
per-ex loss: 0.370240  [   72/   88]
per-ex loss: 0.752947  [   73/   88]
per-ex loss: 0.687298  [   74/   88]
per-ex loss: 0.577113  [   75/   88]
per-ex loss: 0.713556  [   76/   88]
per-ex loss: 0.549520  [   77/   88]
per-ex loss: 0.724937  [   78/   88]
per-ex loss: 0.558392  [   79/   88]
per-ex loss: 0.645520  [   80/   88]
per-ex loss: 0.514524  [   81/   88]
per-ex loss: 0.673944  [   82/   88]
per-ex loss: 0.482600  [   83/   88]
per-ex loss: 0.694132  [   84/   88]
per-ex loss: 0.550659  [   85/   88]
per-ex loss: 0.480198  [   86/   88]
per-ex loss: 0.511305  [   87/   88]
per-ex loss: 0.524626  [   88/   88]
Train Error: Avg loss: 0.59515593
validation Error: 
 Avg loss: 0.69187162 
 F1: 0.497154 
 Precision: 0.581309 
 Recall: 0.434284
 IoU: 0.330809

test Error: 
 Avg loss: 0.63605723 
 F1: 0.573645 
 Precision: 0.659204 
 Recall: 0.507744
 IoU: 0.402175

We have finished training iteration 136
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_134_.pth
per-ex loss: 0.607745  [    1/   88]
per-ex loss: 0.345551  [    2/   88]
per-ex loss: 0.727583  [    3/   88]
per-ex loss: 0.740692  [    4/   88]
per-ex loss: 0.774716  [    5/   88]
per-ex loss: 0.771280  [    6/   88]
per-ex loss: 0.771507  [    7/   88]
per-ex loss: 0.772142  [    8/   88]
per-ex loss: 0.544715  [    9/   88]
per-ex loss: 0.554932  [   10/   88]
per-ex loss: 0.463452  [   11/   88]
per-ex loss: 0.624767  [   12/   88]
per-ex loss: 0.509713  [   13/   88]
per-ex loss: 0.541626  [   14/   88]
per-ex loss: 0.578197  [   15/   88]
per-ex loss: 0.533851  [   16/   88]
per-ex loss: 0.614469  [   17/   88]
per-ex loss: 0.778132  [   18/   88]
per-ex loss: 0.668018  [   19/   88]
per-ex loss: 0.708319  [   20/   88]
per-ex loss: 0.526444  [   21/   88]
per-ex loss: 0.449464  [   22/   88]
per-ex loss: 0.605105  [   23/   88]
per-ex loss: 0.717087  [   24/   88]
per-ex loss: 0.461610  [   25/   88]
per-ex loss: 0.615859  [   26/   88]
per-ex loss: 0.706192  [   27/   88]
per-ex loss: 0.699226  [   28/   88]
per-ex loss: 0.767126  [   29/   88]
per-ex loss: 0.680021  [   30/   88]
per-ex loss: 0.614878  [   31/   88]
per-ex loss: 0.480241  [   32/   88]
per-ex loss: 0.670803  [   33/   88]
per-ex loss: 0.574320  [   34/   88]
per-ex loss: 0.723378  [   35/   88]
per-ex loss: 0.590112  [   36/   88]
per-ex loss: 0.475663  [   37/   88]
per-ex loss: 0.601079  [   38/   88]
per-ex loss: 0.469652  [   39/   88]
per-ex loss: 0.689153  [   40/   88]
per-ex loss: 0.592620  [   41/   88]
per-ex loss: 0.606630  [   42/   88]
per-ex loss: 0.507604  [   43/   88]
per-ex loss: 0.720377  [   44/   88]
per-ex loss: 0.539542  [   45/   88]
per-ex loss: 0.540509  [   46/   88]
per-ex loss: 0.543765  [   47/   88]
per-ex loss: 0.483712  [   48/   88]
per-ex loss: 0.658198  [   49/   88]
per-ex loss: 0.508865  [   50/   88]
per-ex loss: 0.454201  [   51/   88]
per-ex loss: 0.718038  [   52/   88]
per-ex loss: 0.538648  [   53/   88]
per-ex loss: 0.677765  [   54/   88]
per-ex loss: 0.562416  [   55/   88]
per-ex loss: 0.544556  [   56/   88]
per-ex loss: 0.686877  [   57/   88]
per-ex loss: 0.495390  [   58/   88]
per-ex loss: 0.535446  [   59/   88]
per-ex loss: 0.670119  [   60/   88]
per-ex loss: 0.551562  [   61/   88]
per-ex loss: 0.483470  [   62/   88]
per-ex loss: 0.490999  [   63/   88]
per-ex loss: 0.598719  [   64/   88]
per-ex loss: 0.767965  [   65/   88]
per-ex loss: 0.531498  [   66/   88]
per-ex loss: 0.697904  [   67/   88]
per-ex loss: 0.615370  [   68/   88]
per-ex loss: 0.480010  [   69/   88]
per-ex loss: 0.517361  [   70/   88]
per-ex loss: 0.482837  [   71/   88]
per-ex loss: 0.552677  [   72/   88]
per-ex loss: 0.550552  [   73/   88]
per-ex loss: 0.509239  [   74/   88]
per-ex loss: 0.490300  [   75/   88]
per-ex loss: 0.755657  [   76/   88]
per-ex loss: 0.518841  [   77/   88]
per-ex loss: 0.612487  [   78/   88]
per-ex loss: 0.523157  [   79/   88]
per-ex loss: 0.550453  [   80/   88]
per-ex loss: 0.709522  [   81/   88]
per-ex loss: 0.650835  [   82/   88]
per-ex loss: 0.693940  [   83/   88]
per-ex loss: 0.716944  [   84/   88]
per-ex loss: 0.517639  [   85/   88]
per-ex loss: 0.636549  [   86/   88]
per-ex loss: 0.465932  [   87/   88]
per-ex loss: 0.707124  [   88/   88]
Train Error: Avg loss: 0.59901861
validation Error: 
 Avg loss: 0.69311995 
 F1: 0.492005 
 Precision: 0.526789 
 Recall: 0.461530
 IoU: 0.326264

test Error: 
 Avg loss: 0.63576119 
 F1: 0.572753 
 Precision: 0.605532 
 Recall: 0.543340
 IoU: 0.401299

We have finished training iteration 137
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_135_.pth
per-ex loss: 0.527997  [    1/   88]
per-ex loss: 0.689413  [    2/   88]
per-ex loss: 0.771500  [    3/   88]
per-ex loss: 0.501031  [    4/   88]
per-ex loss: 0.754471  [    5/   88]
per-ex loss: 0.716001  [    6/   88]
per-ex loss: 0.508991  [    7/   88]
per-ex loss: 0.387988  [    8/   88]
per-ex loss: 0.571272  [    9/   88]
per-ex loss: 0.463539  [   10/   88]
per-ex loss: 0.481992  [   11/   88]
per-ex loss: 0.617998  [   12/   88]
per-ex loss: 0.552458  [   13/   88]
per-ex loss: 0.771700  [   14/   88]
per-ex loss: 0.715724  [   15/   88]
per-ex loss: 0.577375  [   16/   88]
per-ex loss: 0.506929  [   17/   88]
per-ex loss: 0.682900  [   18/   88]
per-ex loss: 0.735548  [   19/   88]
per-ex loss: 0.470873  [   20/   88]
per-ex loss: 0.493739  [   21/   88]
per-ex loss: 0.506410  [   22/   88]
per-ex loss: 0.708058  [   23/   88]
per-ex loss: 0.478189  [   24/   88]
per-ex loss: 0.582373  [   25/   88]
per-ex loss: 0.565373  [   26/   88]
per-ex loss: 0.716858  [   27/   88]
per-ex loss: 0.677393  [   28/   88]
per-ex loss: 0.540526  [   29/   88]
per-ex loss: 0.531189  [   30/   88]
per-ex loss: 0.717094  [   31/   88]
per-ex loss: 0.749301  [   32/   88]
per-ex loss: 0.567395  [   33/   88]
per-ex loss: 0.531752  [   34/   88]
per-ex loss: 0.541979  [   35/   88]
per-ex loss: 0.716816  [   36/   88]
per-ex loss: 0.522184  [   37/   88]
per-ex loss: 0.688049  [   38/   88]
per-ex loss: 0.586725  [   39/   88]
per-ex loss: 0.539813  [   40/   88]
per-ex loss: 0.614485  [   41/   88]
per-ex loss: 0.507415  [   42/   88]
per-ex loss: 0.507171  [   43/   88]
per-ex loss: 0.457858  [   44/   88]
per-ex loss: 0.616837  [   45/   88]
per-ex loss: 0.721934  [   46/   88]
per-ex loss: 0.541686  [   47/   88]
per-ex loss: 0.633674  [   48/   88]
per-ex loss: 0.692861  [   49/   88]
per-ex loss: 0.604862  [   50/   88]
per-ex loss: 0.485059  [   51/   88]
per-ex loss: 0.501310  [   52/   88]
per-ex loss: 0.530700  [   53/   88]
per-ex loss: 0.693795  [   54/   88]
per-ex loss: 0.624257  [   55/   88]
per-ex loss: 0.719434  [   56/   88]
per-ex loss: 0.663393  [   57/   88]
per-ex loss: 0.485483  [   58/   88]
per-ex loss: 0.712127  [   59/   88]
per-ex loss: 0.424150  [   60/   88]
per-ex loss: 0.750881  [   61/   88]
per-ex loss: 0.524726  [   62/   88]
per-ex loss: 0.588339  [   63/   88]
per-ex loss: 0.500957  [   64/   88]
per-ex loss: 0.670556  [   65/   88]
per-ex loss: 0.349134  [   66/   88]
per-ex loss: 0.577842  [   67/   88]
per-ex loss: 0.487972  [   68/   88]
per-ex loss: 0.483200  [   69/   88]
per-ex loss: 0.622816  [   70/   88]
per-ex loss: 0.494671  [   71/   88]
per-ex loss: 0.567780  [   72/   88]
per-ex loss: 0.677047  [   73/   88]
per-ex loss: 0.728733  [   74/   88]
per-ex loss: 0.780000  [   75/   88]
per-ex loss: 0.579487  [   76/   88]
per-ex loss: 0.612502  [   77/   88]
per-ex loss: 0.646938  [   78/   88]
per-ex loss: 0.694243  [   79/   88]
per-ex loss: 0.617973  [   80/   88]
per-ex loss: 0.604208  [   81/   88]
per-ex loss: 0.543411  [   82/   88]
per-ex loss: 0.489954  [   83/   88]
per-ex loss: 0.788096  [   84/   88]
per-ex loss: 0.699713  [   85/   88]
per-ex loss: 0.553888  [   86/   88]
per-ex loss: 0.457051  [   87/   88]
per-ex loss: 0.710355  [   88/   88]
Train Error: Avg loss: 0.59668043
validation Error: 
 Avg loss: 0.70261638 
 F1: 0.479230 
 Precision: 0.528100 
 Recall: 0.438639
 IoU: 0.315123

test Error: 
 Avg loss: 0.64468270 
 F1: 0.559516 
 Precision: 0.634632 
 Recall: 0.500300
 IoU: 0.388422

We have finished training iteration 138
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_136_.pth
per-ex loss: 0.551235  [    1/   88]
per-ex loss: 0.593028  [    2/   88]
per-ex loss: 0.664545  [    3/   88]
per-ex loss: 0.629472  [    4/   88]
per-ex loss: 0.658989  [    5/   88]
per-ex loss: 0.775786  [    6/   88]
per-ex loss: 0.772002  [    7/   88]
per-ex loss: 0.635713  [    8/   88]
per-ex loss: 0.632604  [    9/   88]
per-ex loss: 0.564915  [   10/   88]
per-ex loss: 0.688393  [   11/   88]
per-ex loss: 0.756782  [   12/   88]
per-ex loss: 0.696278  [   13/   88]
per-ex loss: 0.593220  [   14/   88]
per-ex loss: 0.650598  [   15/   88]
per-ex loss: 0.513509  [   16/   88]
per-ex loss: 0.725996  [   17/   88]
per-ex loss: 0.622464  [   18/   88]
per-ex loss: 0.505531  [   19/   88]
per-ex loss: 0.504508  [   20/   88]
per-ex loss: 0.757399  [   21/   88]
per-ex loss: 0.715454  [   22/   88]
per-ex loss: 0.577321  [   23/   88]
per-ex loss: 0.623563  [   24/   88]
per-ex loss: 0.560989  [   25/   88]
per-ex loss: 0.654922  [   26/   88]
per-ex loss: 0.591789  [   27/   88]
per-ex loss: 0.465406  [   28/   88]
per-ex loss: 0.502273  [   29/   88]
per-ex loss: 0.498428  [   30/   88]
per-ex loss: 0.718932  [   31/   88]
per-ex loss: 0.522092  [   32/   88]
per-ex loss: 0.506744  [   33/   88]
per-ex loss: 0.523000  [   34/   88]
per-ex loss: 0.767157  [   35/   88]
per-ex loss: 0.489382  [   36/   88]
per-ex loss: 0.642210  [   37/   88]
per-ex loss: 0.618376  [   38/   88]
per-ex loss: 0.457239  [   39/   88]
per-ex loss: 0.689869  [   40/   88]
per-ex loss: 0.548206  [   41/   88]
per-ex loss: 0.591114  [   42/   88]
per-ex loss: 0.580638  [   43/   88]
per-ex loss: 0.592349  [   44/   88]
per-ex loss: 0.775289  [   45/   88]
per-ex loss: 0.745070  [   46/   88]
per-ex loss: 0.391087  [   47/   88]
per-ex loss: 0.456621  [   48/   88]
per-ex loss: 0.538169  [   49/   88]
per-ex loss: 0.684972  [   50/   88]
per-ex loss: 0.526302  [   51/   88]
per-ex loss: 0.492389  [   52/   88]
per-ex loss: 0.710572  [   53/   88]
per-ex loss: 0.768061  [   54/   88]
per-ex loss: 0.801540  [   55/   88]
per-ex loss: 0.679123  [   56/   88]
per-ex loss: 0.520536  [   57/   88]
per-ex loss: 0.483703  [   58/   88]
per-ex loss: 0.587964  [   59/   88]
per-ex loss: 0.579777  [   60/   88]
per-ex loss: 0.703665  [   61/   88]
per-ex loss: 0.565136  [   62/   88]
per-ex loss: 0.704579  [   63/   88]
per-ex loss: 0.545064  [   64/   88]
per-ex loss: 0.580342  [   65/   88]
per-ex loss: 0.517553  [   66/   88]
per-ex loss: 0.721274  [   67/   88]
per-ex loss: 0.691835  [   68/   88]
per-ex loss: 0.699587  [   69/   88]
per-ex loss: 0.690183  [   70/   88]
per-ex loss: 0.496932  [   71/   88]
per-ex loss: 0.519169  [   72/   88]
per-ex loss: 0.483310  [   73/   88]
per-ex loss: 0.505233  [   74/   88]
per-ex loss: 0.442264  [   75/   88]
per-ex loss: 0.567744  [   76/   88]
per-ex loss: 0.738752  [   77/   88]
per-ex loss: 0.503330  [   78/   88]
per-ex loss: 0.538978  [   79/   88]
per-ex loss: 0.483289  [   80/   88]
per-ex loss: 0.513477  [   81/   88]
per-ex loss: 0.709977  [   82/   88]
per-ex loss: 0.538619  [   83/   88]
per-ex loss: 0.555674  [   84/   88]
per-ex loss: 0.697913  [   85/   88]
per-ex loss: 0.458304  [   86/   88]
per-ex loss: 0.481767  [   87/   88]
per-ex loss: 0.453496  [   88/   88]
Train Error: Avg loss: 0.59976183
validation Error: 
 Avg loss: 0.70458713 
 F1: 0.474798 
 Precision: 0.604272 
 Recall: 0.391018
 IoU: 0.311302

test Error: 
 Avg loss: 0.65008317 
 F1: 0.554923 
 Precision: 0.703901 
 Recall: 0.457991
 IoU: 0.384010

We have finished training iteration 139
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_137_.pth
per-ex loss: 0.477014  [    1/   88]
per-ex loss: 0.612812  [    2/   88]
per-ex loss: 0.485363  [    3/   88]
per-ex loss: 0.476038  [    4/   88]
per-ex loss: 0.763061  [    5/   88]
per-ex loss: 0.500400  [    6/   88]
per-ex loss: 0.469103  [    7/   88]
per-ex loss: 0.572100  [    8/   88]
per-ex loss: 0.454055  [    9/   88]
per-ex loss: 0.532208  [   10/   88]
per-ex loss: 0.525712  [   11/   88]
per-ex loss: 0.563834  [   12/   88]
per-ex loss: 0.499774  [   13/   88]
per-ex loss: 0.704940  [   14/   88]
per-ex loss: 0.584568  [   15/   88]
per-ex loss: 0.682996  [   16/   88]
per-ex loss: 0.557719  [   17/   88]
per-ex loss: 0.535689  [   18/   88]
per-ex loss: 0.410623  [   19/   88]
per-ex loss: 0.646569  [   20/   88]
per-ex loss: 0.494198  [   21/   88]
per-ex loss: 0.688935  [   22/   88]
per-ex loss: 0.756591  [   23/   88]
per-ex loss: 0.548612  [   24/   88]
per-ex loss: 0.632546  [   25/   88]
per-ex loss: 0.457739  [   26/   88]
per-ex loss: 0.478653  [   27/   88]
per-ex loss: 0.712855  [   28/   88]
per-ex loss: 0.557982  [   29/   88]
per-ex loss: 0.708634  [   30/   88]
per-ex loss: 0.549504  [   31/   88]
per-ex loss: 0.476139  [   32/   88]
per-ex loss: 0.586453  [   33/   88]
per-ex loss: 0.455778  [   34/   88]
per-ex loss: 0.535621  [   35/   88]
per-ex loss: 0.494549  [   36/   88]
per-ex loss: 0.687662  [   37/   88]
per-ex loss: 0.578967  [   38/   88]
per-ex loss: 0.532083  [   39/   88]
per-ex loss: 0.510850  [   40/   88]
per-ex loss: 0.712670  [   41/   88]
per-ex loss: 0.721735  [   42/   88]
per-ex loss: 0.622407  [   43/   88]
per-ex loss: 0.580162  [   44/   88]
per-ex loss: 0.520062  [   45/   88]
per-ex loss: 0.683963  [   46/   88]
per-ex loss: 0.611315  [   47/   88]
per-ex loss: 0.737064  [   48/   88]
per-ex loss: 0.702489  [   49/   88]
per-ex loss: 0.708305  [   50/   88]
per-ex loss: 0.682091  [   51/   88]
per-ex loss: 0.449291  [   52/   88]
per-ex loss: 0.626867  [   53/   88]
per-ex loss: 0.610436  [   54/   88]
per-ex loss: 0.773896  [   55/   88]
per-ex loss: 0.773885  [   56/   88]
per-ex loss: 0.583463  [   57/   88]
per-ex loss: 0.709443  [   58/   88]
per-ex loss: 0.614943  [   59/   88]
per-ex loss: 0.521025  [   60/   88]
per-ex loss: 0.607463  [   61/   88]
per-ex loss: 0.485107  [   62/   88]
per-ex loss: 0.531407  [   63/   88]
per-ex loss: 0.678554  [   64/   88]
per-ex loss: 0.663118  [   65/   88]
per-ex loss: 0.547566  [   66/   88]
per-ex loss: 0.417957  [   67/   88]
per-ex loss: 0.474696  [   68/   88]
per-ex loss: 0.551684  [   69/   88]
per-ex loss: 0.452315  [   70/   88]
per-ex loss: 0.704095  [   71/   88]
per-ex loss: 0.426630  [   72/   88]
per-ex loss: 0.785345  [   73/   88]
per-ex loss: 0.494212  [   74/   88]
per-ex loss: 0.618371  [   75/   88]
per-ex loss: 0.667543  [   76/   88]
per-ex loss: 0.775339  [   77/   88]
per-ex loss: 0.529423  [   78/   88]
per-ex loss: 0.590101  [   79/   88]
per-ex loss: 0.717299  [   80/   88]
per-ex loss: 0.539259  [   81/   88]
per-ex loss: 0.657076  [   82/   88]
per-ex loss: 0.602312  [   83/   88]
per-ex loss: 0.585169  [   84/   88]
per-ex loss: 0.775167  [   85/   88]
per-ex loss: 0.674449  [   86/   88]
per-ex loss: 0.518934  [   87/   88]
per-ex loss: 0.473933  [   88/   88]
Train Error: Avg loss: 0.59078365
validation Error: 
 Avg loss: 0.69431063 
 F1: 0.492073 
 Precision: 0.632200 
 Recall: 0.402794
 IoU: 0.326324

test Error: 
 Avg loss: 0.64383091 
 F1: 0.562710 
 Precision: 0.695384 
 Recall: 0.472551
 IoU: 0.391508

We have finished training iteration 140
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_138_.pth
per-ex loss: 0.464623  [    1/   88]
per-ex loss: 0.483738  [    2/   88]
per-ex loss: 0.732833  [    3/   88]
per-ex loss: 0.583210  [    4/   88]
per-ex loss: 0.538850  [    5/   88]
per-ex loss: 0.456921  [    6/   88]
per-ex loss: 0.668628  [    7/   88]
per-ex loss: 0.553450  [    8/   88]
per-ex loss: 0.720713  [    9/   88]
per-ex loss: 0.507164  [   10/   88]
per-ex loss: 0.596076  [   11/   88]
per-ex loss: 0.627990  [   12/   88]
per-ex loss: 0.482218  [   13/   88]
per-ex loss: 0.446967  [   14/   88]
per-ex loss: 0.517145  [   15/   88]
per-ex loss: 0.547359  [   16/   88]
per-ex loss: 0.454980  [   17/   88]
per-ex loss: 0.531096  [   18/   88]
per-ex loss: 0.676894  [   19/   88]
per-ex loss: 0.578202  [   20/   88]
per-ex loss: 0.709314  [   21/   88]
per-ex loss: 0.460536  [   22/   88]
per-ex loss: 0.456022  [   23/   88]
per-ex loss: 0.713868  [   24/   88]
per-ex loss: 0.520642  [   25/   88]
per-ex loss: 0.689406  [   26/   88]
per-ex loss: 0.623224  [   27/   88]
per-ex loss: 0.620766  [   28/   88]
per-ex loss: 0.774352  [   29/   88]
per-ex loss: 0.699092  [   30/   88]
per-ex loss: 0.588112  [   31/   88]
per-ex loss: 0.392669  [   32/   88]
per-ex loss: 0.475700  [   33/   88]
per-ex loss: 0.720112  [   34/   88]
per-ex loss: 0.640330  [   35/   88]
per-ex loss: 0.541213  [   36/   88]
per-ex loss: 0.554993  [   37/   88]
per-ex loss: 0.607281  [   38/   88]
per-ex loss: 0.443247  [   39/   88]
per-ex loss: 0.488454  [   40/   88]
per-ex loss: 0.524065  [   41/   88]
per-ex loss: 0.782189  [   42/   88]
per-ex loss: 0.654165  [   43/   88]
per-ex loss: 0.695861  [   44/   88]
per-ex loss: 0.698225  [   45/   88]
per-ex loss: 0.548458  [   46/   88]
per-ex loss: 0.624874  [   47/   88]
per-ex loss: 0.458121  [   48/   88]
per-ex loss: 0.621071  [   49/   88]
per-ex loss: 0.676761  [   50/   88]
per-ex loss: 0.410895  [   51/   88]
per-ex loss: 0.597665  [   52/   88]
per-ex loss: 0.743889  [   53/   88]
per-ex loss: 0.534807  [   54/   88]
per-ex loss: 0.686594  [   55/   88]
per-ex loss: 0.491536  [   56/   88]
per-ex loss: 0.538188  [   57/   88]
per-ex loss: 0.602970  [   58/   88]
per-ex loss: 0.508840  [   59/   88]
per-ex loss: 0.722758  [   60/   88]
per-ex loss: 0.762898  [   61/   88]
per-ex loss: 0.575646  [   62/   88]
per-ex loss: 0.643014  [   63/   88]
per-ex loss: 0.599874  [   64/   88]
per-ex loss: 0.567417  [   65/   88]
per-ex loss: 0.545872  [   66/   88]
per-ex loss: 0.680147  [   67/   88]
per-ex loss: 0.729078  [   68/   88]
per-ex loss: 0.606949  [   69/   88]
per-ex loss: 0.777051  [   70/   88]
per-ex loss: 0.554293  [   71/   88]
per-ex loss: 0.530814  [   72/   88]
per-ex loss: 0.531669  [   73/   88]
per-ex loss: 0.715975  [   74/   88]
per-ex loss: 0.683132  [   75/   88]
per-ex loss: 0.515376  [   76/   88]
per-ex loss: 0.722613  [   77/   88]
per-ex loss: 0.476002  [   78/   88]
per-ex loss: 0.601551  [   79/   88]
per-ex loss: 0.636798  [   80/   88]
per-ex loss: 0.543981  [   81/   88]
per-ex loss: 0.713878  [   82/   88]
per-ex loss: 0.780844  [   83/   88]
per-ex loss: 0.355500  [   84/   88]
per-ex loss: 0.499506  [   85/   88]
per-ex loss: 0.526272  [   86/   88]
per-ex loss: 0.736171  [   87/   88]
per-ex loss: 0.509331  [   88/   88]
Train Error: Avg loss: 0.59240877
validation Error: 
 Avg loss: 0.68967825 
 F1: 0.498362 
 Precision: 0.558702 
 Recall: 0.449785
 IoU: 0.331879

test Error: 
 Avg loss: 0.63407930 
 F1: 0.574834 
 Precision: 0.640741 
 Recall: 0.521220
 IoU: 0.403345

We have finished training iteration 141
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_139_.pth
per-ex loss: 0.546564  [    1/   88]
per-ex loss: 0.659274  [    2/   88]
per-ex loss: 0.629797  [    3/   88]
per-ex loss: 0.794203  [    4/   88]
per-ex loss: 0.554953  [    5/   88]
per-ex loss: 0.794847  [    6/   88]
per-ex loss: 0.528891  [    7/   88]
per-ex loss: 0.727984  [    8/   88]
per-ex loss: 0.514171  [    9/   88]
per-ex loss: 0.537607  [   10/   88]
per-ex loss: 0.432520  [   11/   88]
per-ex loss: 0.710067  [   12/   88]
per-ex loss: 0.554086  [   13/   88]
per-ex loss: 0.605191  [   14/   88]
per-ex loss: 0.634624  [   15/   88]
per-ex loss: 0.495796  [   16/   88]
per-ex loss: 0.691356  [   17/   88]
per-ex loss: 0.470428  [   18/   88]
per-ex loss: 0.478140  [   19/   88]
per-ex loss: 0.691023  [   20/   88]
per-ex loss: 0.521894  [   21/   88]
per-ex loss: 0.506387  [   22/   88]
per-ex loss: 0.616151  [   23/   88]
per-ex loss: 0.579736  [   24/   88]
per-ex loss: 0.781386  [   25/   88]
per-ex loss: 0.751955  [   26/   88]
per-ex loss: 0.544825  [   27/   88]
per-ex loss: 0.342801  [   28/   88]
per-ex loss: 0.562455  [   29/   88]
per-ex loss: 0.718305  [   30/   88]
per-ex loss: 0.709705  [   31/   88]
per-ex loss: 0.714371  [   32/   88]
per-ex loss: 0.779732  [   33/   88]
per-ex loss: 0.560198  [   34/   88]
per-ex loss: 0.594200  [   35/   88]
per-ex loss: 0.579567  [   36/   88]
per-ex loss: 0.463368  [   37/   88]
per-ex loss: 0.539326  [   38/   88]
per-ex loss: 0.512729  [   39/   88]
per-ex loss: 0.496547  [   40/   88]
per-ex loss: 0.685314  [   41/   88]
per-ex loss: 0.638183  [   42/   88]
per-ex loss: 0.597933  [   43/   88]
per-ex loss: 0.580651  [   44/   88]
per-ex loss: 0.741842  [   45/   88]
per-ex loss: 0.617963  [   46/   88]
per-ex loss: 0.543669  [   47/   88]
per-ex loss: 0.535351  [   48/   88]
per-ex loss: 0.514179  [   49/   88]
per-ex loss: 0.719322  [   50/   88]
per-ex loss: 0.498062  [   51/   88]
per-ex loss: 0.568716  [   52/   88]
per-ex loss: 0.493614  [   53/   88]
per-ex loss: 0.615980  [   54/   88]
per-ex loss: 0.722227  [   55/   88]
per-ex loss: 0.694332  [   56/   88]
per-ex loss: 0.447294  [   57/   88]
per-ex loss: 0.489686  [   58/   88]
per-ex loss: 0.703359  [   59/   88]
per-ex loss: 0.464946  [   60/   88]
per-ex loss: 0.466665  [   61/   88]
per-ex loss: 0.460369  [   62/   88]
per-ex loss: 0.684476  [   63/   88]
per-ex loss: 0.532329  [   64/   88]
per-ex loss: 0.539954  [   65/   88]
per-ex loss: 0.550149  [   66/   88]
per-ex loss: 0.596302  [   67/   88]
per-ex loss: 0.732462  [   68/   88]
per-ex loss: 0.685072  [   69/   88]
per-ex loss: 0.601927  [   70/   88]
per-ex loss: 0.561138  [   71/   88]
per-ex loss: 0.727296  [   72/   88]
per-ex loss: 0.541612  [   73/   88]
per-ex loss: 0.527092  [   74/   88]
per-ex loss: 0.529019  [   75/   88]
per-ex loss: 0.722085  [   76/   88]
per-ex loss: 0.452436  [   77/   88]
per-ex loss: 0.647510  [   78/   88]
per-ex loss: 0.704989  [   79/   88]
per-ex loss: 0.704542  [   80/   88]
per-ex loss: 0.712416  [   81/   88]
per-ex loss: 0.415196  [   82/   88]
per-ex loss: 0.667811  [   83/   88]
per-ex loss: 0.493897  [   84/   88]
per-ex loss: 0.752217  [   85/   88]
per-ex loss: 0.443533  [   86/   88]
per-ex loss: 0.521996  [   87/   88]
per-ex loss: 0.663199  [   88/   88]
Train Error: Avg loss: 0.59588040
validation Error: 
 Avg loss: 0.69480672 
 F1: 0.490887 
 Precision: 0.548849 
 Recall: 0.443998
 IoU: 0.325282

test Error: 
 Avg loss: 0.63433934 
 F1: 0.575444 
 Precision: 0.635528 
 Recall: 0.525739
 IoU: 0.403946

We have finished training iteration 142
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_140_.pth
per-ex loss: 0.632349  [    1/   88]
per-ex loss: 0.702189  [    2/   88]
per-ex loss: 0.695489  [    3/   88]
per-ex loss: 0.530103  [    4/   88]
per-ex loss: 0.467495  [    5/   88]
per-ex loss: 0.441601  [    6/   88]
per-ex loss: 0.621677  [    7/   88]
per-ex loss: 0.632913  [    8/   88]
per-ex loss: 0.676568  [    9/   88]
per-ex loss: 0.688739  [   10/   88]
per-ex loss: 0.496202  [   11/   88]
per-ex loss: 0.578779  [   12/   88]
per-ex loss: 0.520511  [   13/   88]
per-ex loss: 0.443682  [   14/   88]
per-ex loss: 0.471412  [   15/   88]
per-ex loss: 0.719047  [   16/   88]
per-ex loss: 0.550642  [   17/   88]
per-ex loss: 0.475966  [   18/   88]
per-ex loss: 0.526327  [   19/   88]
per-ex loss: 0.522022  [   20/   88]
per-ex loss: 0.535317  [   21/   88]
per-ex loss: 0.514542  [   22/   88]
per-ex loss: 0.703693  [   23/   88]
per-ex loss: 0.622213  [   24/   88]
per-ex loss: 0.720820  [   25/   88]
per-ex loss: 0.548474  [   26/   88]
per-ex loss: 0.483280  [   27/   88]
per-ex loss: 0.779815  [   28/   88]
per-ex loss: 0.634959  [   29/   88]
per-ex loss: 0.521651  [   30/   88]
per-ex loss: 0.572114  [   31/   88]
per-ex loss: 0.528458  [   32/   88]
per-ex loss: 0.622356  [   33/   88]
per-ex loss: 0.712177  [   34/   88]
per-ex loss: 0.486101  [   35/   88]
per-ex loss: 0.683404  [   36/   88]
per-ex loss: 0.473788  [   37/   88]
per-ex loss: 0.694423  [   38/   88]
per-ex loss: 0.738654  [   39/   88]
per-ex loss: 0.537214  [   40/   88]
per-ex loss: 0.466947  [   41/   88]
per-ex loss: 0.563400  [   42/   88]
per-ex loss: 0.715735  [   43/   88]
per-ex loss: 0.705494  [   44/   88]
per-ex loss: 0.571482  [   45/   88]
per-ex loss: 0.498564  [   46/   88]
per-ex loss: 0.522383  [   47/   88]
per-ex loss: 0.690809  [   48/   88]
per-ex loss: 0.629146  [   49/   88]
per-ex loss: 0.705875  [   50/   88]
per-ex loss: 0.504930  [   51/   88]
per-ex loss: 0.488660  [   52/   88]
per-ex loss: 0.529269  [   53/   88]
per-ex loss: 0.584140  [   54/   88]
per-ex loss: 0.351643  [   55/   88]
per-ex loss: 0.470226  [   56/   88]
per-ex loss: 0.505183  [   57/   88]
per-ex loss: 0.498663  [   58/   88]
per-ex loss: 0.722074  [   59/   88]
per-ex loss: 0.706437  [   60/   88]
per-ex loss: 0.764171  [   61/   88]
per-ex loss: 0.616843  [   62/   88]
per-ex loss: 0.547417  [   63/   88]
per-ex loss: 0.511257  [   64/   88]
per-ex loss: 0.581672  [   65/   88]
per-ex loss: 0.560380  [   66/   88]
per-ex loss: 0.705818  [   67/   88]
per-ex loss: 0.571225  [   68/   88]
per-ex loss: 0.475543  [   69/   88]
per-ex loss: 0.629040  [   70/   88]
per-ex loss: 0.613653  [   71/   88]
per-ex loss: 0.772081  [   72/   88]
per-ex loss: 0.618932  [   73/   88]
per-ex loss: 0.765536  [   74/   88]
per-ex loss: 0.642607  [   75/   88]
per-ex loss: 0.775007  [   76/   88]
per-ex loss: 0.552984  [   77/   88]
per-ex loss: 0.554409  [   78/   88]
per-ex loss: 0.490162  [   79/   88]
per-ex loss: 0.611022  [   80/   88]
per-ex loss: 0.660002  [   81/   88]
per-ex loss: 0.718531  [   82/   88]
per-ex loss: 0.755477  [   83/   88]
per-ex loss: 0.480656  [   84/   88]
per-ex loss: 0.595150  [   85/   88]
per-ex loss: 0.737299  [   86/   88]
per-ex loss: 0.539000  [   87/   88]
per-ex loss: 0.495528  [   88/   88]
Train Error: Avg loss: 0.59408667
validation Error: 
 Avg loss: 0.69012656 
 F1: 0.496970 
 Precision: 0.554321 
 Recall: 0.450373
 IoU: 0.330645

test Error: 
 Avg loss: 0.63685602 
 F1: 0.570264 
 Precision: 0.627173 
 Recall: 0.522823
 IoU: 0.398860

We have finished training iteration 143
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_141_.pth
per-ex loss: 0.552042  [    1/   88]
per-ex loss: 0.545449  [    2/   88]
per-ex loss: 0.485530  [    3/   88]
per-ex loss: 0.731650  [    4/   88]
per-ex loss: 0.744060  [    5/   88]
per-ex loss: 0.483591  [    6/   88]
per-ex loss: 0.465218  [    7/   88]
per-ex loss: 0.507192  [    8/   88]
per-ex loss: 0.762159  [    9/   88]
per-ex loss: 0.467047  [   10/   88]
per-ex loss: 0.709490  [   11/   88]
per-ex loss: 0.699366  [   12/   88]
per-ex loss: 0.642559  [   13/   88]
per-ex loss: 0.569487  [   14/   88]
per-ex loss: 0.687263  [   15/   88]
per-ex loss: 0.437311  [   16/   88]
per-ex loss: 0.479873  [   17/   88]
per-ex loss: 0.537124  [   18/   88]
per-ex loss: 0.486180  [   19/   88]
per-ex loss: 0.585688  [   20/   88]
per-ex loss: 0.673533  [   21/   88]
per-ex loss: 0.522525  [   22/   88]
per-ex loss: 0.479850  [   23/   88]
per-ex loss: 0.469747  [   24/   88]
per-ex loss: 0.626902  [   25/   88]
per-ex loss: 0.715862  [   26/   88]
per-ex loss: 0.752235  [   27/   88]
per-ex loss: 0.657521  [   28/   88]
per-ex loss: 0.722200  [   29/   88]
per-ex loss: 0.535854  [   30/   88]
per-ex loss: 0.495274  [   31/   88]
per-ex loss: 0.709411  [   32/   88]
per-ex loss: 0.577917  [   33/   88]
per-ex loss: 0.522961  [   34/   88]
per-ex loss: 0.488347  [   35/   88]
per-ex loss: 0.544150  [   36/   88]
per-ex loss: 0.595204  [   37/   88]
per-ex loss: 0.526352  [   38/   88]
per-ex loss: 0.656136  [   39/   88]
per-ex loss: 0.610488  [   40/   88]
per-ex loss: 0.585420  [   41/   88]
per-ex loss: 0.572742  [   42/   88]
per-ex loss: 0.634888  [   43/   88]
per-ex loss: 0.680188  [   44/   88]
per-ex loss: 0.365502  [   45/   88]
per-ex loss: 0.536507  [   46/   88]
per-ex loss: 0.523212  [   47/   88]
per-ex loss: 0.685084  [   48/   88]
per-ex loss: 0.467489  [   49/   88]
per-ex loss: 0.590953  [   50/   88]
per-ex loss: 0.688927  [   51/   88]
per-ex loss: 0.708029  [   52/   88]
per-ex loss: 0.598119  [   53/   88]
per-ex loss: 0.753686  [   54/   88]
per-ex loss: 0.771310  [   55/   88]
per-ex loss: 0.504899  [   56/   88]
per-ex loss: 0.642173  [   57/   88]
per-ex loss: 0.486342  [   58/   88]
per-ex loss: 0.522138  [   59/   88]
per-ex loss: 0.445242  [   60/   88]
per-ex loss: 0.567476  [   61/   88]
per-ex loss: 0.597901  [   62/   88]
per-ex loss: 0.586148  [   63/   88]
per-ex loss: 0.408019  [   64/   88]
per-ex loss: 0.516264  [   65/   88]
per-ex loss: 0.711567  [   66/   88]
per-ex loss: 0.775983  [   67/   88]
per-ex loss: 0.526385  [   68/   88]
per-ex loss: 0.473217  [   69/   88]
per-ex loss: 0.539188  [   70/   88]
per-ex loss: 0.642305  [   71/   88]
per-ex loss: 0.502450  [   72/   88]
per-ex loss: 0.491838  [   73/   88]
per-ex loss: 0.616842  [   74/   88]
per-ex loss: 0.528406  [   75/   88]
per-ex loss: 0.432632  [   76/   88]
per-ex loss: 0.529189  [   77/   88]
per-ex loss: 0.683012  [   78/   88]
per-ex loss: 0.711607  [   79/   88]
per-ex loss: 0.778773  [   80/   88]
per-ex loss: 0.674758  [   81/   88]
per-ex loss: 0.641782  [   82/   88]
per-ex loss: 0.536249  [   83/   88]
per-ex loss: 0.761873  [   84/   88]
per-ex loss: 0.680473  [   85/   88]
per-ex loss: 0.609021  [   86/   88]
per-ex loss: 0.692284  [   87/   88]
per-ex loss: 0.495579  [   88/   88]
Train Error: Avg loss: 0.59016837
validation Error: 
 Avg loss: 0.69822955 
 F1: 0.485679 
 Precision: 0.499135 
 Recall: 0.472929
 IoU: 0.320724

test Error: 
 Avg loss: 0.63078652 
 F1: 0.576817 
 Precision: 0.597142 
 Recall: 0.557830
 IoU: 0.405301

We have finished training iteration 144
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_142_.pth
per-ex loss: 0.434008  [    1/   88]
per-ex loss: 0.707875  [    2/   88]
per-ex loss: 0.478546  [    3/   88]
per-ex loss: 0.534709  [    4/   88]
per-ex loss: 0.681550  [    5/   88]
per-ex loss: 0.677210  [    6/   88]
per-ex loss: 0.646280  [    7/   88]
per-ex loss: 0.711163  [    8/   88]
per-ex loss: 0.447950  [    9/   88]
per-ex loss: 0.513990  [   10/   88]
per-ex loss: 0.512290  [   11/   88]
per-ex loss: 0.516451  [   12/   88]
per-ex loss: 0.550372  [   13/   88]
per-ex loss: 0.694365  [   14/   88]
per-ex loss: 0.552622  [   15/   88]
per-ex loss: 0.521044  [   16/   88]
per-ex loss: 0.436458  [   17/   88]
per-ex loss: 0.479024  [   18/   88]
per-ex loss: 0.639507  [   19/   88]
per-ex loss: 0.729572  [   20/   88]
per-ex loss: 0.489378  [   21/   88]
per-ex loss: 0.511788  [   22/   88]
per-ex loss: 0.619899  [   23/   88]
per-ex loss: 0.486927  [   24/   88]
per-ex loss: 0.708810  [   25/   88]
per-ex loss: 0.550493  [   26/   88]
per-ex loss: 0.562757  [   27/   88]
per-ex loss: 0.721439  [   28/   88]
per-ex loss: 0.336033  [   29/   88]
per-ex loss: 0.622463  [   30/   88]
per-ex loss: 0.629050  [   31/   88]
per-ex loss: 0.506297  [   32/   88]
per-ex loss: 0.577769  [   33/   88]
per-ex loss: 0.617906  [   34/   88]
per-ex loss: 0.754205  [   35/   88]
per-ex loss: 0.773646  [   36/   88]
per-ex loss: 0.691407  [   37/   88]
per-ex loss: 0.549269  [   38/   88]
per-ex loss: 0.587788  [   39/   88]
per-ex loss: 0.541249  [   40/   88]
per-ex loss: 0.743797  [   41/   88]
per-ex loss: 0.462254  [   42/   88]
per-ex loss: 0.701252  [   43/   88]
per-ex loss: 0.520131  [   44/   88]
per-ex loss: 0.487650  [   45/   88]
per-ex loss: 0.551043  [   46/   88]
per-ex loss: 0.522944  [   47/   88]
per-ex loss: 0.650470  [   48/   88]
per-ex loss: 0.754467  [   49/   88]
per-ex loss: 0.771760  [   50/   88]
per-ex loss: 0.768970  [   51/   88]
per-ex loss: 0.677578  [   52/   88]
per-ex loss: 0.605823  [   53/   88]
per-ex loss: 0.775504  [   54/   88]
per-ex loss: 0.577084  [   55/   88]
per-ex loss: 0.737189  [   56/   88]
per-ex loss: 0.543699  [   57/   88]
per-ex loss: 0.632710  [   58/   88]
per-ex loss: 0.494758  [   59/   88]
per-ex loss: 0.725121  [   60/   88]
per-ex loss: 0.601056  [   61/   88]
per-ex loss: 0.570226  [   62/   88]
per-ex loss: 0.740175  [   63/   88]
per-ex loss: 0.473857  [   64/   88]
per-ex loss: 0.634419  [   65/   88]
per-ex loss: 0.528576  [   66/   88]
per-ex loss: 0.501461  [   67/   88]
per-ex loss: 0.642104  [   68/   88]
per-ex loss: 0.588575  [   69/   88]
per-ex loss: 0.487539  [   70/   88]
per-ex loss: 0.452334  [   71/   88]
per-ex loss: 0.692893  [   72/   88]
per-ex loss: 0.688563  [   73/   88]
per-ex loss: 0.451832  [   74/   88]
per-ex loss: 0.685938  [   75/   88]
per-ex loss: 0.561003  [   76/   88]
per-ex loss: 0.513577  [   77/   88]
per-ex loss: 0.712386  [   78/   88]
per-ex loss: 0.713623  [   79/   88]
per-ex loss: 0.458518  [   80/   88]
per-ex loss: 0.416020  [   81/   88]
per-ex loss: 0.574844  [   82/   88]
per-ex loss: 0.593731  [   83/   88]
per-ex loss: 0.596625  [   84/   88]
per-ex loss: 0.522823  [   85/   88]
per-ex loss: 0.471259  [   86/   88]
per-ex loss: 0.465463  [   87/   88]
per-ex loss: 0.523484  [   88/   88]
Train Error: Avg loss: 0.58943936
validation Error: 
 Avg loss: 0.69895510 
 F1: 0.483252 
 Precision: 0.618075 
 Recall: 0.396715
 IoU: 0.318610

test Error: 
 Avg loss: 0.65176442 
 F1: 0.553678 
 Precision: 0.703892 
 Recall: 0.456301
 IoU: 0.382818

We have finished training iteration 145
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_143_.pth
per-ex loss: 0.724145  [    1/   88]
per-ex loss: 0.565747  [    2/   88]
per-ex loss: 0.609867  [    3/   88]
per-ex loss: 0.727730  [    4/   88]
per-ex loss: 0.592379  [    5/   88]
per-ex loss: 0.510607  [    6/   88]
per-ex loss: 0.562454  [    7/   88]
per-ex loss: 0.484545  [    8/   88]
per-ex loss: 0.617768  [    9/   88]
per-ex loss: 0.497372  [   10/   88]
per-ex loss: 0.471937  [   11/   88]
per-ex loss: 0.537316  [   12/   88]
per-ex loss: 0.536086  [   13/   88]
per-ex loss: 0.516724  [   14/   88]
per-ex loss: 0.681839  [   15/   88]
per-ex loss: 0.694535  [   16/   88]
per-ex loss: 0.704372  [   17/   88]
per-ex loss: 0.455818  [   18/   88]
per-ex loss: 0.718753  [   19/   88]
per-ex loss: 0.685341  [   20/   88]
per-ex loss: 0.452403  [   21/   88]
per-ex loss: 0.446070  [   22/   88]
per-ex loss: 0.675833  [   23/   88]
per-ex loss: 0.686918  [   24/   88]
per-ex loss: 0.716163  [   25/   88]
per-ex loss: 0.765041  [   26/   88]
per-ex loss: 0.509266  [   27/   88]
per-ex loss: 0.481329  [   28/   88]
per-ex loss: 0.519359  [   29/   88]
per-ex loss: 0.486356  [   30/   88]
per-ex loss: 0.676276  [   31/   88]
per-ex loss: 0.719169  [   32/   88]
per-ex loss: 0.570195  [   33/   88]
per-ex loss: 0.713744  [   34/   88]
per-ex loss: 0.629260  [   35/   88]
per-ex loss: 0.580378  [   36/   88]
per-ex loss: 0.694533  [   37/   88]
per-ex loss: 0.569788  [   38/   88]
per-ex loss: 0.630577  [   39/   88]
per-ex loss: 0.653940  [   40/   88]
per-ex loss: 0.480344  [   41/   88]
per-ex loss: 0.527331  [   42/   88]
per-ex loss: 0.626479  [   43/   88]
per-ex loss: 0.542846  [   44/   88]
per-ex loss: 0.448789  [   45/   88]
per-ex loss: 0.601675  [   46/   88]
per-ex loss: 0.774302  [   47/   88]
per-ex loss: 0.615805  [   48/   88]
per-ex loss: 0.498358  [   49/   88]
per-ex loss: 0.521985  [   50/   88]
per-ex loss: 0.573875  [   51/   88]
per-ex loss: 0.580970  [   52/   88]
per-ex loss: 0.737083  [   53/   88]
per-ex loss: 0.431066  [   54/   88]
per-ex loss: 0.428622  [   55/   88]
per-ex loss: 0.555414  [   56/   88]
per-ex loss: 0.622779  [   57/   88]
per-ex loss: 0.548677  [   58/   88]
per-ex loss: 0.482271  [   59/   88]
per-ex loss: 0.712042  [   60/   88]
per-ex loss: 0.788515  [   61/   88]
per-ex loss: 0.681509  [   62/   88]
per-ex loss: 0.773657  [   63/   88]
per-ex loss: 0.530873  [   64/   88]
per-ex loss: 0.744262  [   65/   88]
per-ex loss: 0.621305  [   66/   88]
per-ex loss: 0.768004  [   67/   88]
per-ex loss: 0.540844  [   68/   88]
per-ex loss: 0.547939  [   69/   88]
per-ex loss: 0.494133  [   70/   88]
per-ex loss: 0.583211  [   71/   88]
per-ex loss: 0.631107  [   72/   88]
per-ex loss: 0.474705  [   73/   88]
per-ex loss: 0.476035  [   74/   88]
per-ex loss: 0.516507  [   75/   88]
per-ex loss: 0.493131  [   76/   88]
per-ex loss: 0.524201  [   77/   88]
per-ex loss: 0.418686  [   78/   88]
per-ex loss: 0.696773  [   79/   88]
per-ex loss: 0.646973  [   80/   88]
per-ex loss: 0.488476  [   81/   88]
per-ex loss: 0.721041  [   82/   88]
per-ex loss: 0.507568  [   83/   88]
per-ex loss: 0.742202  [   84/   88]
per-ex loss: 0.586822  [   85/   88]
per-ex loss: 0.659848  [   86/   88]
per-ex loss: 0.568303  [   87/   88]
per-ex loss: 0.560485  [   88/   88]
Train Error: Avg loss: 0.59283821
validation Error: 
 Avg loss: 0.69059155 
 F1: 0.493171 
 Precision: 0.531855 
 Recall: 0.459733
 IoU: 0.327291

test Error: 
 Avg loss: 0.63196319 
 F1: 0.576334 
 Precision: 0.623306 
 Recall: 0.535946
 IoU: 0.404824

We have finished training iteration 146
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_144_.pth
per-ex loss: 0.657172  [    1/   88]
per-ex loss: 0.795514  [    2/   88]
per-ex loss: 0.718978  [    3/   88]
per-ex loss: 0.611034  [    4/   88]
per-ex loss: 0.689810  [    5/   88]
per-ex loss: 0.682010  [    6/   88]
per-ex loss: 0.630809  [    7/   88]
per-ex loss: 0.461094  [    8/   88]
per-ex loss: 0.564063  [    9/   88]
per-ex loss: 0.695452  [   10/   88]
per-ex loss: 0.685403  [   11/   88]
per-ex loss: 0.510987  [   12/   88]
per-ex loss: 0.587404  [   13/   88]
per-ex loss: 0.691808  [   14/   88]
per-ex loss: 0.717261  [   15/   88]
per-ex loss: 0.707507  [   16/   88]
per-ex loss: 0.452152  [   17/   88]
per-ex loss: 0.574545  [   18/   88]
per-ex loss: 0.691959  [   19/   88]
per-ex loss: 0.769616  [   20/   88]
per-ex loss: 0.702487  [   21/   88]
per-ex loss: 0.551359  [   22/   88]
per-ex loss: 0.397150  [   23/   88]
per-ex loss: 0.728905  [   24/   88]
per-ex loss: 0.749189  [   25/   88]
per-ex loss: 0.502111  [   26/   88]
per-ex loss: 0.505362  [   27/   88]
per-ex loss: 0.647738  [   28/   88]
per-ex loss: 0.612293  [   29/   88]
per-ex loss: 0.686254  [   30/   88]
per-ex loss: 0.483884  [   31/   88]
per-ex loss: 0.519150  [   32/   88]
per-ex loss: 0.542250  [   33/   88]
per-ex loss: 0.473904  [   34/   88]
per-ex loss: 0.773213  [   35/   88]
per-ex loss: 0.755822  [   36/   88]
per-ex loss: 0.643796  [   37/   88]
per-ex loss: 0.480110  [   38/   88]
per-ex loss: 0.609524  [   39/   88]
per-ex loss: 0.781894  [   40/   88]
per-ex loss: 0.500687  [   41/   88]
per-ex loss: 0.768151  [   42/   88]
per-ex loss: 0.743512  [   43/   88]
per-ex loss: 0.514176  [   44/   88]
per-ex loss: 0.736731  [   45/   88]
per-ex loss: 0.447526  [   46/   88]
per-ex loss: 0.392118  [   47/   88]
per-ex loss: 0.632237  [   48/   88]
per-ex loss: 0.546437  [   49/   88]
per-ex loss: 0.579246  [   50/   88]
per-ex loss: 0.475453  [   51/   88]
per-ex loss: 0.552685  [   52/   88]
per-ex loss: 0.532289  [   53/   88]
per-ex loss: 0.524292  [   54/   88]
per-ex loss: 0.718075  [   55/   88]
per-ex loss: 0.502021  [   56/   88]
per-ex loss: 0.468874  [   57/   88]
per-ex loss: 0.648493  [   58/   88]
per-ex loss: 0.636853  [   59/   88]
per-ex loss: 0.596118  [   60/   88]
per-ex loss: 0.515525  [   61/   88]
per-ex loss: 0.455011  [   62/   88]
per-ex loss: 0.546469  [   63/   88]
per-ex loss: 0.701303  [   64/   88]
per-ex loss: 0.551002  [   65/   88]
per-ex loss: 0.568964  [   66/   88]
per-ex loss: 0.549868  [   67/   88]
per-ex loss: 0.449870  [   68/   88]
per-ex loss: 0.624199  [   69/   88]
per-ex loss: 0.527676  [   70/   88]
per-ex loss: 0.716837  [   71/   88]
per-ex loss: 0.744330  [   72/   88]
per-ex loss: 0.521279  [   73/   88]
per-ex loss: 0.512684  [   74/   88]
per-ex loss: 0.589082  [   75/   88]
per-ex loss: 0.703710  [   76/   88]
per-ex loss: 0.629697  [   77/   88]
per-ex loss: 0.589270  [   78/   88]
per-ex loss: 0.513518  [   79/   88]
per-ex loss: 0.547574  [   80/   88]
per-ex loss: 0.480627  [   81/   88]
per-ex loss: 0.605926  [   82/   88]
per-ex loss: 0.504370  [   83/   88]
per-ex loss: 0.527879  [   84/   88]
per-ex loss: 0.455871  [   85/   88]
per-ex loss: 0.484664  [   86/   88]
per-ex loss: 0.475174  [   87/   88]
per-ex loss: 0.670361  [   88/   88]
Train Error: Avg loss: 0.59454188
validation Error: 
 Avg loss: 0.68930220 
 F1: 0.495944 
 Precision: 0.514069 
 Recall: 0.479055
 IoU: 0.329738

test Error: 
 Avg loss: 0.63212090 
 F1: 0.579360 
 Precision: 0.595811 
 Recall: 0.563794
 IoU: 0.407817

We have finished training iteration 147
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_145_.pth
per-ex loss: 0.703837  [    1/   88]
per-ex loss: 0.699597  [    2/   88]
per-ex loss: 0.530082  [    3/   88]
per-ex loss: 0.700615  [    4/   88]
per-ex loss: 0.493834  [    5/   88]
per-ex loss: 0.581922  [    6/   88]
per-ex loss: 0.688711  [    7/   88]
per-ex loss: 0.692019  [    8/   88]
per-ex loss: 0.480724  [    9/   88]
per-ex loss: 0.727636  [   10/   88]
per-ex loss: 0.573428  [   11/   88]
per-ex loss: 0.509007  [   12/   88]
per-ex loss: 0.788605  [   13/   88]
per-ex loss: 0.511756  [   14/   88]
per-ex loss: 0.689959  [   15/   88]
per-ex loss: 0.565157  [   16/   88]
per-ex loss: 0.714918  [   17/   88]
per-ex loss: 0.747068  [   18/   88]
per-ex loss: 0.613280  [   19/   88]
per-ex loss: 0.750499  [   20/   88]
per-ex loss: 0.531293  [   21/   88]
per-ex loss: 0.587387  [   22/   88]
per-ex loss: 0.655450  [   23/   88]
per-ex loss: 0.609318  [   24/   88]
per-ex loss: 0.693364  [   25/   88]
per-ex loss: 0.403942  [   26/   88]
per-ex loss: 0.772936  [   27/   88]
per-ex loss: 0.638629  [   28/   88]
per-ex loss: 0.574601  [   29/   88]
per-ex loss: 0.504205  [   30/   88]
per-ex loss: 0.546447  [   31/   88]
per-ex loss: 0.542266  [   32/   88]
per-ex loss: 0.489595  [   33/   88]
per-ex loss: 0.634180  [   34/   88]
per-ex loss: 0.533185  [   35/   88]
per-ex loss: 0.613584  [   36/   88]
per-ex loss: 0.672734  [   37/   88]
per-ex loss: 0.500259  [   38/   88]
per-ex loss: 0.569741  [   39/   88]
per-ex loss: 0.406996  [   40/   88]
per-ex loss: 0.535354  [   41/   88]
per-ex loss: 0.479080  [   42/   88]
per-ex loss: 0.618010  [   43/   88]
per-ex loss: 0.691334  [   44/   88]
per-ex loss: 0.434616  [   45/   88]
per-ex loss: 0.449879  [   46/   88]
per-ex loss: 0.532641  [   47/   88]
per-ex loss: 0.506148  [   48/   88]
per-ex loss: 0.712002  [   49/   88]
per-ex loss: 0.709875  [   50/   88]
per-ex loss: 0.719397  [   51/   88]
per-ex loss: 0.565489  [   52/   88]
per-ex loss: 0.454467  [   53/   88]
per-ex loss: 0.465676  [   54/   88]
per-ex loss: 0.563753  [   55/   88]
per-ex loss: 0.627254  [   56/   88]
per-ex loss: 0.534581  [   57/   88]
per-ex loss: 0.774848  [   58/   88]
per-ex loss: 0.456529  [   59/   88]
per-ex loss: 0.604269  [   60/   88]
per-ex loss: 0.775755  [   61/   88]
per-ex loss: 0.620203  [   62/   88]
per-ex loss: 0.549929  [   63/   88]
per-ex loss: 0.745177  [   64/   88]
per-ex loss: 0.687492  [   65/   88]
per-ex loss: 0.665584  [   66/   88]
per-ex loss: 0.504849  [   67/   88]
per-ex loss: 0.496919  [   68/   88]
per-ex loss: 0.746205  [   69/   88]
per-ex loss: 0.638958  [   70/   88]
per-ex loss: 0.547071  [   71/   88]
per-ex loss: 0.488501  [   72/   88]
per-ex loss: 0.565653  [   73/   88]
per-ex loss: 0.439924  [   74/   88]
per-ex loss: 0.503786  [   75/   88]
per-ex loss: 0.461311  [   76/   88]
per-ex loss: 0.666853  [   77/   88]
per-ex loss: 0.683819  [   78/   88]
per-ex loss: 0.502322  [   79/   88]
per-ex loss: 0.444601  [   80/   88]
per-ex loss: 0.500985  [   81/   88]
per-ex loss: 0.626202  [   82/   88]
per-ex loss: 0.578788  [   83/   88]
per-ex loss: 0.555890  [   84/   88]
per-ex loss: 0.343949  [   85/   88]
per-ex loss: 0.522432  [   86/   88]
per-ex loss: 0.566833  [   87/   88]
per-ex loss: 0.531689  [   88/   88]
Train Error: Avg loss: 0.58681422
validation Error: 
 Avg loss: 0.68770074 
 F1: 0.499548 
 Precision: 0.597443 
 Recall: 0.429218
 IoU: 0.332932

test Error: 
 Avg loss: 0.63448070 
 F1: 0.575176 
 Precision: 0.667801 
 Recall: 0.505116
 IoU: 0.403682

We have finished training iteration 148
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_146_.pth
per-ex loss: 0.563788  [    1/   88]
per-ex loss: 0.669188  [    2/   88]
per-ex loss: 0.682704  [    3/   88]
per-ex loss: 0.691232  [    4/   88]
per-ex loss: 0.678715  [    5/   88]
per-ex loss: 0.684474  [    6/   88]
per-ex loss: 0.528635  [    7/   88]
per-ex loss: 0.624480  [    8/   88]
per-ex loss: 0.478973  [    9/   88]
per-ex loss: 0.482467  [   10/   88]
per-ex loss: 0.753669  [   11/   88]
per-ex loss: 0.534578  [   12/   88]
per-ex loss: 0.529700  [   13/   88]
per-ex loss: 0.604975  [   14/   88]
per-ex loss: 0.725361  [   15/   88]
per-ex loss: 0.614953  [   16/   88]
per-ex loss: 0.767165  [   17/   88]
per-ex loss: 0.770610  [   18/   88]
per-ex loss: 0.450477  [   19/   88]
per-ex loss: 0.644910  [   20/   88]
per-ex loss: 0.528576  [   21/   88]
per-ex loss: 0.513338  [   22/   88]
per-ex loss: 0.519821  [   23/   88]
per-ex loss: 0.445587  [   24/   88]
per-ex loss: 0.496204  [   25/   88]
per-ex loss: 0.695723  [   26/   88]
per-ex loss: 0.427335  [   27/   88]
per-ex loss: 0.715326  [   28/   88]
per-ex loss: 0.505259  [   29/   88]
per-ex loss: 0.625907  [   30/   88]
per-ex loss: 0.581212  [   31/   88]
per-ex loss: 0.542642  [   32/   88]
per-ex loss: 0.783676  [   33/   88]
per-ex loss: 0.533499  [   34/   88]
per-ex loss: 0.584467  [   35/   88]
per-ex loss: 0.704204  [   36/   88]
per-ex loss: 0.666729  [   37/   88]
per-ex loss: 0.792562  [   38/   88]
per-ex loss: 0.713778  [   39/   88]
per-ex loss: 0.577231  [   40/   88]
per-ex loss: 0.612077  [   41/   88]
per-ex loss: 0.544136  [   42/   88]
per-ex loss: 0.539571  [   43/   88]
per-ex loss: 0.709894  [   44/   88]
per-ex loss: 0.598777  [   45/   88]
per-ex loss: 0.355072  [   46/   88]
per-ex loss: 0.735074  [   47/   88]
per-ex loss: 0.752899  [   48/   88]
per-ex loss: 0.601516  [   49/   88]
per-ex loss: 0.602283  [   50/   88]
per-ex loss: 0.433669  [   51/   88]
per-ex loss: 0.783477  [   52/   88]
per-ex loss: 0.604619  [   53/   88]
per-ex loss: 0.549339  [   54/   88]
per-ex loss: 0.591826  [   55/   88]
per-ex loss: 0.602373  [   56/   88]
per-ex loss: 0.565020  [   57/   88]
per-ex loss: 0.407637  [   58/   88]
per-ex loss: 0.480495  [   59/   88]
per-ex loss: 0.490645  [   60/   88]
per-ex loss: 0.493128  [   61/   88]
per-ex loss: 0.568285  [   62/   88]
per-ex loss: 0.560239  [   63/   88]
per-ex loss: 0.507317  [   64/   88]
per-ex loss: 0.706833  [   65/   88]
per-ex loss: 0.518602  [   66/   88]
per-ex loss: 0.544699  [   67/   88]
per-ex loss: 0.493078  [   68/   88]
per-ex loss: 0.766577  [   69/   88]
per-ex loss: 0.486925  [   70/   88]
per-ex loss: 0.712293  [   71/   88]
per-ex loss: 0.513118  [   72/   88]
per-ex loss: 0.479151  [   73/   88]
per-ex loss: 0.680294  [   74/   88]
per-ex loss: 0.468178  [   75/   88]
per-ex loss: 0.477638  [   76/   88]
per-ex loss: 0.502801  [   77/   88]
per-ex loss: 0.642746  [   78/   88]
per-ex loss: 0.430574  [   79/   88]
per-ex loss: 0.667967  [   80/   88]
per-ex loss: 0.523009  [   81/   88]
per-ex loss: 0.454655  [   82/   88]
per-ex loss: 0.655698  [   83/   88]
per-ex loss: 0.722265  [   84/   88]
per-ex loss: 0.689063  [   85/   88]
per-ex loss: 0.489007  [   86/   88]
per-ex loss: 0.535200  [   87/   88]
per-ex loss: 0.572692  [   88/   88]
Train Error: Avg loss: 0.58927912
validation Error: 
 Avg loss: 0.71434028 
 F1: 0.462535 
 Precision: 0.448403 
 Recall: 0.477587
 IoU: 0.300843

test Error: 
 Avg loss: 0.64037162 
 F1: 0.567899 
 Precision: 0.565837 
 Recall: 0.569975
 IoU: 0.396549

We have finished training iteration 149
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_147_.pth
per-ex loss: 0.693956  [    1/   88]
per-ex loss: 0.760547  [    2/   88]
per-ex loss: 0.520379  [    3/   88]
per-ex loss: 0.562250  [    4/   88]
per-ex loss: 0.760620  [    5/   88]
per-ex loss: 0.661307  [    6/   88]
per-ex loss: 0.495278  [    7/   88]
per-ex loss: 0.495061  [    8/   88]
per-ex loss: 0.575258  [    9/   88]
per-ex loss: 0.456880  [   10/   88]
per-ex loss: 0.724089  [   11/   88]
per-ex loss: 0.355458  [   12/   88]
per-ex loss: 0.512987  [   13/   88]
per-ex loss: 0.561169  [   14/   88]
per-ex loss: 0.702253  [   15/   88]
per-ex loss: 0.549911  [   16/   88]
per-ex loss: 0.740688  [   17/   88]
per-ex loss: 0.501077  [   18/   88]
per-ex loss: 0.510857  [   19/   88]
per-ex loss: 0.528472  [   20/   88]
per-ex loss: 0.493276  [   21/   88]
per-ex loss: 0.702593  [   22/   88]
per-ex loss: 0.520770  [   23/   88]
per-ex loss: 0.498516  [   24/   88]
per-ex loss: 0.632018  [   25/   88]
per-ex loss: 0.709540  [   26/   88]
per-ex loss: 0.768060  [   27/   88]
per-ex loss: 0.477733  [   28/   88]
per-ex loss: 0.678906  [   29/   88]
per-ex loss: 0.513156  [   30/   88]
per-ex loss: 0.561398  [   31/   88]
per-ex loss: 0.540650  [   32/   88]
per-ex loss: 0.767541  [   33/   88]
per-ex loss: 0.703635  [   34/   88]
per-ex loss: 0.618131  [   35/   88]
per-ex loss: 0.614776  [   36/   88]
per-ex loss: 0.694038  [   37/   88]
per-ex loss: 0.602673  [   38/   88]
per-ex loss: 0.598861  [   39/   88]
per-ex loss: 0.442888  [   40/   88]
per-ex loss: 0.622294  [   41/   88]
per-ex loss: 0.525304  [   42/   88]
per-ex loss: 0.689479  [   43/   88]
per-ex loss: 0.413681  [   44/   88]
per-ex loss: 0.570328  [   45/   88]
per-ex loss: 0.484202  [   46/   88]
per-ex loss: 0.635733  [   47/   88]
per-ex loss: 0.489288  [   48/   88]
per-ex loss: 0.762598  [   49/   88]
per-ex loss: 0.509551  [   50/   88]
per-ex loss: 0.546527  [   51/   88]
per-ex loss: 0.579039  [   52/   88]
per-ex loss: 0.728836  [   53/   88]
per-ex loss: 0.774077  [   54/   88]
per-ex loss: 0.537390  [   55/   88]
per-ex loss: 0.536914  [   56/   88]
per-ex loss: 0.551802  [   57/   88]
per-ex loss: 0.504679  [   58/   88]
per-ex loss: 0.656629  [   59/   88]
per-ex loss: 0.603939  [   60/   88]
per-ex loss: 0.527044  [   61/   88]
per-ex loss: 0.628933  [   62/   88]
per-ex loss: 0.545388  [   63/   88]
per-ex loss: 0.454396  [   64/   88]
per-ex loss: 0.677792  [   65/   88]
per-ex loss: 0.728861  [   66/   88]
per-ex loss: 0.456390  [   67/   88]
per-ex loss: 0.500570  [   68/   88]
per-ex loss: 0.696082  [   69/   88]
per-ex loss: 0.482863  [   70/   88]
per-ex loss: 0.723505  [   71/   88]
per-ex loss: 0.485164  [   72/   88]
per-ex loss: 0.540847  [   73/   88]
per-ex loss: 0.503228  [   74/   88]
per-ex loss: 0.670615  [   75/   88]
per-ex loss: 0.520671  [   76/   88]
per-ex loss: 0.582479  [   77/   88]
per-ex loss: 0.615827  [   78/   88]
per-ex loss: 0.632688  [   79/   88]
per-ex loss: 0.593692  [   80/   88]
per-ex loss: 0.762358  [   81/   88]
per-ex loss: 0.610888  [   82/   88]
per-ex loss: 0.668531  [   83/   88]
per-ex loss: 0.657976  [   84/   88]
per-ex loss: 0.690309  [   85/   88]
per-ex loss: 0.527389  [   86/   88]
per-ex loss: 0.482471  [   87/   88]
per-ex loss: 0.681321  [   88/   88]
Train Error: Avg loss: 0.59293431
validation Error: 
 Avg loss: 0.69821120 
 F1: 0.485519 
 Precision: 0.520318 
 Recall: 0.455083
 IoU: 0.320584

test Error: 
 Avg loss: 0.63539548 
 F1: 0.574693 
 Precision: 0.612920 
 Recall: 0.540954
 IoU: 0.403206

We have finished training iteration 150
Deleting model ./unet_att_j_train/saved_model_wrapper/models/UNet_103_.pth
Max training iterations reached: 150. Train_iter: 150, Initial_train_iter: 0
