unet_att_main_just_train.py do_log: False
Log file name: log_14_16-12-15_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(sd='unet_att_j_nzo_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_att_j_nzo.yaml', yo=None, ntibp=None, ptp=None, map=None)
YAML: {'path_to_data': './Data/vein_and_sclera_data', 'batch_size': 1, 'learning_rate': 1e-05, 'num_of_dataloader_workers': 7, 'train_epoch_size_limit': 400, 'num_epochs_per_training_iteration': 1, 'cleanup_k': 3, 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': True, 'loss_fn_name': 'JACCARD', 'alphas': [], 'dataset_option': 'aug_tf', 'zero_out_non_sclera': False, 'add_sclera_to_img': True, 'add_bcosfire_to_img': True, 'add_coye_to_img': True, 'model_type': 'att', 'model': '64_2_6', 'input_width': 2048, 'input_height': 1024, 'input_channels': 6, 'output_channels': 2, 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq'}
Validation phase: False
Namespace(sd='unet_att_j_nzo_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_att_j_nzo.yaml', yo=None, ntibp=None, ptp=None, map=None)
Device: cuda
dataset_aug_tf.py do_log: False
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 88
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 88
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 88
val dataloader num of batches: 27
test dataloader num of batches: 12
Created new model instance.
per-ex loss: 0.922593  [    1/   88]
per-ex loss: 0.976976  [    2/   88]
per-ex loss: 0.928653  [    3/   88]
per-ex loss: 0.953013  [    4/   88]
per-ex loss: 0.972021  [    5/   88]
per-ex loss: 0.982078  [    6/   88]
per-ex loss: 0.920792  [    7/   88]
per-ex loss: 0.955668  [    8/   88]
per-ex loss: 0.967703  [    9/   88]
per-ex loss: 0.956628  [   10/   88]
per-ex loss: 0.888983  [   11/   88]
per-ex loss: 0.955531  [   12/   88]
per-ex loss: 0.961145  [   13/   88]
per-ex loss: 0.813493  [   14/   88]
per-ex loss: 0.891180  [   15/   88]
per-ex loss: 0.958114  [   16/   88]
per-ex loss: 0.942809  [   17/   88]
per-ex loss: 0.910933  [   18/   88]
per-ex loss: 0.820247  [   19/   88]
per-ex loss: 0.948245  [   20/   88]
per-ex loss: 0.927357  [   21/   88]
per-ex loss: 0.835171  [   22/   88]
per-ex loss: 0.939958  [   23/   88]
per-ex loss: 0.918405  [   24/   88]
per-ex loss: 0.887867  [   25/   88]
per-ex loss: 0.889830  [   26/   88]
per-ex loss: 0.808383  [   27/   88]
per-ex loss: 0.963207  [   28/   88]
per-ex loss: 0.860806  [   29/   88]
per-ex loss: 0.951955  [   30/   88]
per-ex loss: 0.939918  [   31/   88]
per-ex loss: 0.922297  [   32/   88]
per-ex loss: 0.845713  [   33/   88]
per-ex loss: 0.944378  [   34/   88]
per-ex loss: 0.892541  [   35/   88]
per-ex loss: 0.889570  [   36/   88]
per-ex loss: 0.894731  [   37/   88]
per-ex loss: 0.890861  [   38/   88]
per-ex loss: 0.854625  [   39/   88]
per-ex loss: 0.932519  [   40/   88]
per-ex loss: 0.943378  [   41/   88]
per-ex loss: 0.943670  [   42/   88]
per-ex loss: 0.870917  [   43/   88]
per-ex loss: 0.940799  [   44/   88]
per-ex loss: 0.782283  [   45/   88]
per-ex loss: 0.801643  [   46/   88]
per-ex loss: 0.916925  [   47/   88]
per-ex loss: 0.916371  [   48/   88]
per-ex loss: 0.879537  [   49/   88]
per-ex loss: 0.923704  [   50/   88]
per-ex loss: 0.912906  [   51/   88]
per-ex loss: 0.882271  [   52/   88]
per-ex loss: 0.890682  [   53/   88]
per-ex loss: 0.845246  [   54/   88]
per-ex loss: 0.742420  [   55/   88]
per-ex loss: 0.728733  [   56/   88]
per-ex loss: 0.707883  [   57/   88]
per-ex loss: 0.801379  [   58/   88]
per-ex loss: 0.910282  [   59/   88]
per-ex loss: 0.737723  [   60/   88]
per-ex loss: 0.933930  [   61/   88]
per-ex loss: 0.906742  [   62/   88]
per-ex loss: 0.791985  [   63/   88]
per-ex loss: 0.943985  [   64/   88]
per-ex loss: 0.804243  [   65/   88]
per-ex loss: 0.935027  [   66/   88]
per-ex loss: 0.919992  [   67/   88]
per-ex loss: 0.821023  [   68/   88]
per-ex loss: 0.820766  [   69/   88]
per-ex loss: 0.734544  [   70/   88]
per-ex loss: 0.733959  [   71/   88]
per-ex loss: 0.814887  [   72/   88]
per-ex loss: 0.785987  [   73/   88]
per-ex loss: 0.735490  [   74/   88]
per-ex loss: 0.817653  [   75/   88]
per-ex loss: 0.913764  [   76/   88]
per-ex loss: 0.798434  [   77/   88]
per-ex loss: 0.883468  [   78/   88]
per-ex loss: 0.699538  [   79/   88]
per-ex loss: 0.740773  [   80/   88]
per-ex loss: 0.716326  [   81/   88]
per-ex loss: 0.778600  [   82/   88]
per-ex loss: 0.857891  [   83/   88]
per-ex loss: 0.794372  [   84/   88]
per-ex loss: 0.743702  [   85/   88]
per-ex loss: 0.701890  [   86/   88]
per-ex loss: 0.743001  [   87/   88]
per-ex loss: 0.754051  [   88/   88]
Train Error: Avg loss: 0.86729171
validation Error: 
 Avg loss: 0.82265632 
 F1: 0.366803 
 Precision: 0.335255 
 Recall: 0.404904
 IoU: 0.224592

test Error: 
 Avg loss: 0.81061295 
 F1: 0.387824 
 Precision: 0.307289 
 Recall: 0.525565
 IoU: 0.240559

We have finished training iteration 1
per-ex loss: 0.762517  [    1/   88]
per-ex loss: 0.869791  [    2/   88]
per-ex loss: 0.822549  [    3/   88]
per-ex loss: 0.823608  [    4/   88]
per-ex loss: 0.799188  [    5/   88]
per-ex loss: 0.720143  [    6/   88]
per-ex loss: 0.835204  [    7/   88]
per-ex loss: 0.704909  [    8/   88]
per-ex loss: 0.783296  [    9/   88]
per-ex loss: 0.770881  [   10/   88]
per-ex loss: 0.896024  [   11/   88]
per-ex loss: 0.849573  [   12/   88]
per-ex loss: 0.882871  [   13/   88]
per-ex loss: 0.861259  [   14/   88]
per-ex loss: 0.727482  [   15/   88]
per-ex loss: 0.745954  [   16/   88]
per-ex loss: 0.838359  [   17/   88]
per-ex loss: 0.829540  [   18/   88]
per-ex loss: 0.734881  [   19/   88]
per-ex loss: 0.681157  [   20/   88]
per-ex loss: 0.734958  [   21/   88]
per-ex loss: 0.778686  [   22/   88]
per-ex loss: 0.788094  [   23/   88]
per-ex loss: 0.848789  [   24/   88]
per-ex loss: 0.840979  [   25/   88]
per-ex loss: 0.699295  [   26/   88]
per-ex loss: 0.779317  [   27/   88]
per-ex loss: 0.664854  [   28/   88]
per-ex loss: 0.733188  [   29/   88]
per-ex loss: 0.881065  [   30/   88]
per-ex loss: 0.836068  [   31/   88]
per-ex loss: 0.838337  [   32/   88]
per-ex loss: 0.838577  [   33/   88]
per-ex loss: 0.844156  [   34/   88]
per-ex loss: 0.672608  [   35/   88]
per-ex loss: 0.630936  [   36/   88]
per-ex loss: 0.649912  [   37/   88]
per-ex loss: 0.655331  [   38/   88]
per-ex loss: 0.834735  [   39/   88]
per-ex loss: 0.646481  [   40/   88]
per-ex loss: 0.804178  [   41/   88]
per-ex loss: 0.861186  [   42/   88]
per-ex loss: 0.824321  [   43/   88]
per-ex loss: 0.795937  [   44/   88]
per-ex loss: 0.685010  [   45/   88]
per-ex loss: 0.851715  [   46/   88]
per-ex loss: 0.870829  [   47/   88]
per-ex loss: 0.731365  [   48/   88]
per-ex loss: 0.628614  [   49/   88]
per-ex loss: 0.679547  [   50/   88]
per-ex loss: 0.837782  [   51/   88]
per-ex loss: 0.577366  [   52/   88]
per-ex loss: 0.657946  [   53/   88]
per-ex loss: 0.654600  [   54/   88]
per-ex loss: 0.711183  [   55/   88]
per-ex loss: 0.778863  [   56/   88]
per-ex loss: 0.798542  [   57/   88]
per-ex loss: 0.592648  [   58/   88]
per-ex loss: 0.785683  [   59/   88]
per-ex loss: 0.710226  [   60/   88]
per-ex loss: 0.697837  [   61/   88]
per-ex loss: 0.718479  [   62/   88]
per-ex loss: 0.887663  [   63/   88]
per-ex loss: 0.732517  [   64/   88]
per-ex loss: 0.701629  [   65/   88]
per-ex loss: 0.841305  [   66/   88]
per-ex loss: 0.810519  [   67/   88]
per-ex loss: 0.624087  [   68/   88]
per-ex loss: 0.881978  [   69/   88]
per-ex loss: 0.859459  [   70/   88]
per-ex loss: 0.643516  [   71/   88]
per-ex loss: 0.599183  [   72/   88]
per-ex loss: 0.653224  [   73/   88]
per-ex loss: 0.863793  [   74/   88]
per-ex loss: 0.887079  [   75/   88]
per-ex loss: 0.809709  [   76/   88]
per-ex loss: 0.873101  [   77/   88]
per-ex loss: 0.619480  [   78/   88]
per-ex loss: 0.786796  [   79/   88]
per-ex loss: 0.674815  [   80/   88]
per-ex loss: 0.805252  [   81/   88]
per-ex loss: 0.645090  [   82/   88]
per-ex loss: 0.833405  [   83/   88]
per-ex loss: 0.763908  [   84/   88]
per-ex loss: 0.847237  [   85/   88]
per-ex loss: 0.641334  [   86/   88]
per-ex loss: 0.868323  [   87/   88]
per-ex loss: 0.690784  [   88/   88]
Train Error: Avg loss: 0.76402953
validation Error: 
 Avg loss: 0.76998143 
 F1: 0.416589 
 Precision: 0.464663 
 Recall: 0.377529
 IoU: 0.263096

test Error: 
 Avg loss: 0.73410345 
 F1: 0.484143 
 Precision: 0.460753 
 Recall: 0.510034
 IoU: 0.319385

We have finished training iteration 2
per-ex loss: 0.833069  [    1/   88]
per-ex loss: 0.693507  [    2/   88]
per-ex loss: 0.773157  [    3/   88]
per-ex loss: 0.682540  [    4/   88]
per-ex loss: 0.843445  [    5/   88]
per-ex loss: 0.823686  [    6/   88]
per-ex loss: 0.688820  [    7/   88]
per-ex loss: 0.614871  [    8/   88]
per-ex loss: 0.667623  [    9/   88]
per-ex loss: 0.818041  [   10/   88]
per-ex loss: 0.865896  [   11/   88]
per-ex loss: 0.873053  [   12/   88]
per-ex loss: 0.846025  [   13/   88]
per-ex loss: 0.871698  [   14/   88]
per-ex loss: 0.819236  [   15/   88]
per-ex loss: 0.777320  [   16/   88]
per-ex loss: 0.679910  [   17/   88]
per-ex loss: 0.859417  [   18/   88]
per-ex loss: 0.649899  [   19/   88]
per-ex loss: 0.709301  [   20/   88]
per-ex loss: 0.855572  [   21/   88]
per-ex loss: 0.752301  [   22/   88]
per-ex loss: 0.779601  [   23/   88]
per-ex loss: 0.747626  [   24/   88]
per-ex loss: 0.759137  [   25/   88]
per-ex loss: 0.693611  [   26/   88]
per-ex loss: 0.784545  [   27/   88]
per-ex loss: 0.834929  [   28/   88]
per-ex loss: 0.821728  [   29/   88]
per-ex loss: 0.702139  [   30/   88]
per-ex loss: 0.691025  [   31/   88]
per-ex loss: 0.843116  [   32/   88]
per-ex loss: 0.872696  [   33/   88]
per-ex loss: 0.803221  [   34/   88]
per-ex loss: 0.774586  [   35/   88]
per-ex loss: 0.628628  [   36/   88]
per-ex loss: 0.862589  [   37/   88]
per-ex loss: 0.704919  [   38/   88]
per-ex loss: 0.667649  [   39/   88]
per-ex loss: 0.637640  [   40/   88]
per-ex loss: 0.577369  [   41/   88]
per-ex loss: 0.671575  [   42/   88]
per-ex loss: 0.655944  [   43/   88]
per-ex loss: 0.646829  [   44/   88]
per-ex loss: 0.651180  [   45/   88]
per-ex loss: 0.819933  [   46/   88]
per-ex loss: 0.607363  [   47/   88]
per-ex loss: 0.713838  [   48/   88]
per-ex loss: 0.786717  [   49/   88]
per-ex loss: 0.579842  [   50/   88]
per-ex loss: 0.858118  [   51/   88]
per-ex loss: 0.705371  [   52/   88]
per-ex loss: 0.765250  [   53/   88]
per-ex loss: 0.805109  [   54/   88]
per-ex loss: 0.737789  [   55/   88]
per-ex loss: 0.693585  [   56/   88]
per-ex loss: 0.711594  [   57/   88]
per-ex loss: 0.772123  [   58/   88]
per-ex loss: 0.652861  [   59/   88]
per-ex loss: 0.836388  [   60/   88]
per-ex loss: 0.752793  [   61/   88]
per-ex loss: 0.639220  [   62/   88]
per-ex loss: 0.780940  [   63/   88]
per-ex loss: 0.637027  [   64/   88]
per-ex loss: 0.840545  [   65/   88]
per-ex loss: 0.823354  [   66/   88]
per-ex loss: 0.787337  [   67/   88]
per-ex loss: 0.803258  [   68/   88]
per-ex loss: 0.672675  [   69/   88]
per-ex loss: 0.713219  [   70/   88]
per-ex loss: 0.647864  [   71/   88]
per-ex loss: 0.759439  [   72/   88]
per-ex loss: 0.720224  [   73/   88]
per-ex loss: 0.632613  [   74/   88]
per-ex loss: 0.779379  [   75/   88]
per-ex loss: 0.804180  [   76/   88]
per-ex loss: 0.594613  [   77/   88]
per-ex loss: 0.814677  [   78/   88]
per-ex loss: 0.594801  [   79/   88]
per-ex loss: 0.858971  [   80/   88]
per-ex loss: 0.648989  [   81/   88]
per-ex loss: 0.654783  [   82/   88]
per-ex loss: 0.780213  [   83/   88]
per-ex loss: 0.611764  [   84/   88]
per-ex loss: 0.761082  [   85/   88]
per-ex loss: 0.679506  [   86/   88]
per-ex loss: 0.866644  [   87/   88]
per-ex loss: 0.635081  [   88/   88]
Train Error: Avg loss: 0.74036069
validation Error: 
 Avg loss: 0.75562314 
 F1: 0.417077 
 Precision: 0.632649 
 Recall: 0.311079
 IoU: 0.263486

test Error: 
 Avg loss: 0.71054918 
 F1: 0.501524 
 Precision: 0.631058 
 Recall: 0.416110
 IoU: 0.334689

We have finished training iteration 3
per-ex loss: 0.665380  [    1/   88]
per-ex loss: 0.570170  [    2/   88]
per-ex loss: 0.817130  [    3/   88]
per-ex loss: 0.609325  [    4/   88]
per-ex loss: 0.852176  [    5/   88]
per-ex loss: 0.762978  [    6/   88]
per-ex loss: 0.834132  [    7/   88]
per-ex loss: 0.684871  [    8/   88]
per-ex loss: 0.759668  [    9/   88]
per-ex loss: 0.829724  [   10/   88]
per-ex loss: 0.791352  [   11/   88]
per-ex loss: 0.800019  [   12/   88]
per-ex loss: 0.833454  [   13/   88]
per-ex loss: 0.681079  [   14/   88]
per-ex loss: 0.666676  [   15/   88]
per-ex loss: 0.833220  [   16/   88]
per-ex loss: 0.637384  [   17/   88]
per-ex loss: 0.725420  [   18/   88]
per-ex loss: 0.801662  [   19/   88]
per-ex loss: 0.757289  [   20/   88]
per-ex loss: 0.650750  [   21/   88]
per-ex loss: 0.628223  [   22/   88]
per-ex loss: 0.607682  [   23/   88]
per-ex loss: 0.807612  [   24/   88]
per-ex loss: 0.845551  [   25/   88]
per-ex loss: 0.783945  [   26/   88]
per-ex loss: 0.863169  [   27/   88]
per-ex loss: 0.727123  [   28/   88]
per-ex loss: 0.599024  [   29/   88]
per-ex loss: 0.606188  [   30/   88]
per-ex loss: 0.773500  [   31/   88]
per-ex loss: 0.739587  [   32/   88]
per-ex loss: 0.549576  [   33/   88]
per-ex loss: 0.584640  [   34/   88]
per-ex loss: 0.832017  [   35/   88]
per-ex loss: 0.722509  [   36/   88]
per-ex loss: 0.681197  [   37/   88]
per-ex loss: 0.637432  [   38/   88]
per-ex loss: 0.856939  [   39/   88]
per-ex loss: 0.600109  [   40/   88]
per-ex loss: 0.639863  [   41/   88]
per-ex loss: 0.610756  [   42/   88]
per-ex loss: 0.685803  [   43/   88]
per-ex loss: 0.850988  [   44/   88]
per-ex loss: 0.638893  [   45/   88]
per-ex loss: 0.690543  [   46/   88]
per-ex loss: 0.617753  [   47/   88]
per-ex loss: 0.796675  [   48/   88]
per-ex loss: 0.747261  [   49/   88]
per-ex loss: 0.639931  [   50/   88]
per-ex loss: 0.690747  [   51/   88]
per-ex loss: 0.844834  [   52/   88]
per-ex loss: 0.696589  [   53/   88]
per-ex loss: 0.715413  [   54/   88]
per-ex loss: 0.877298  [   55/   88]
per-ex loss: 0.690707  [   56/   88]
per-ex loss: 0.619602  [   57/   88]
per-ex loss: 0.766832  [   58/   88]
per-ex loss: 0.742505  [   59/   88]
per-ex loss: 0.651767  [   60/   88]
per-ex loss: 0.806571  [   61/   88]
per-ex loss: 0.793622  [   62/   88]
per-ex loss: 0.762614  [   63/   88]
per-ex loss: 0.860710  [   64/   88]
per-ex loss: 0.659537  [   65/   88]
per-ex loss: 0.564441  [   66/   88]
per-ex loss: 0.588382  [   67/   88]
per-ex loss: 0.827982  [   68/   88]
per-ex loss: 0.844453  [   69/   88]
per-ex loss: 0.624875  [   70/   88]
per-ex loss: 0.830577  [   71/   88]
per-ex loss: 0.801523  [   72/   88]
per-ex loss: 0.701409  [   73/   88]
per-ex loss: 0.566813  [   74/   88]
per-ex loss: 0.621646  [   75/   88]
per-ex loss: 0.742292  [   76/   88]
per-ex loss: 0.635939  [   77/   88]
per-ex loss: 0.776760  [   78/   88]
per-ex loss: 0.713672  [   79/   88]
per-ex loss: 0.807341  [   80/   88]
per-ex loss: 0.858719  [   81/   88]
per-ex loss: 0.700400  [   82/   88]
per-ex loss: 0.751635  [   83/   88]
per-ex loss: 0.768917  [   84/   88]
per-ex loss: 0.717011  [   85/   88]
per-ex loss: 0.804229  [   86/   88]
per-ex loss: 0.764670  [   87/   88]
per-ex loss: 0.768749  [   88/   88]
Train Error: Avg loss: 0.72602423
validation Error: 
 Avg loss: 0.75071692 
 F1: 0.441706 
 Precision: 0.463963 
 Recall: 0.421486
 IoU: 0.283455

test Error: 
 Avg loss: 0.71211601 
 F1: 0.511815 
 Precision: 0.470070 
 Recall: 0.561696
 IoU: 0.343919

We have finished training iteration 4
per-ex loss: 0.572288  [    1/   88]
per-ex loss: 0.770330  [    2/   88]
per-ex loss: 0.704974  [    3/   88]
per-ex loss: 0.814530  [    4/   88]
per-ex loss: 0.849665  [    5/   88]
per-ex loss: 0.672175  [    6/   88]
per-ex loss: 0.707506  [    7/   88]
per-ex loss: 0.848545  [    8/   88]
per-ex loss: 0.618739  [    9/   88]
per-ex loss: 0.822602  [   10/   88]
per-ex loss: 0.617021  [   11/   88]
per-ex loss: 0.759885  [   12/   88]
per-ex loss: 0.832882  [   13/   88]
per-ex loss: 0.811702  [   14/   88]
per-ex loss: 0.655066  [   15/   88]
per-ex loss: 0.565010  [   16/   88]
per-ex loss: 0.619314  [   17/   88]
per-ex loss: 0.637274  [   18/   88]
per-ex loss: 0.803079  [   19/   88]
per-ex loss: 0.781232  [   20/   88]
per-ex loss: 0.748560  [   21/   88]
per-ex loss: 0.843733  [   22/   88]
per-ex loss: 0.682692  [   23/   88]
per-ex loss: 0.656846  [   24/   88]
per-ex loss: 0.633572  [   25/   88]
per-ex loss: 0.688418  [   26/   88]
per-ex loss: 0.624639  [   27/   88]
per-ex loss: 0.779516  [   28/   88]
per-ex loss: 0.727082  [   29/   88]
per-ex loss: 0.791049  [   30/   88]
per-ex loss: 0.732324  [   31/   88]
per-ex loss: 0.812410  [   32/   88]
per-ex loss: 0.816021  [   33/   88]
per-ex loss: 0.547543  [   34/   88]
per-ex loss: 0.723431  [   35/   88]
per-ex loss: 0.719644  [   36/   88]
per-ex loss: 0.663776  [   37/   88]
per-ex loss: 0.642083  [   38/   88]
per-ex loss: 0.656290  [   39/   88]
per-ex loss: 0.589442  [   40/   88]
per-ex loss: 0.661114  [   41/   88]
per-ex loss: 0.629388  [   42/   88]
per-ex loss: 0.556261  [   43/   88]
per-ex loss: 0.703280  [   44/   88]
per-ex loss: 0.680009  [   45/   88]
per-ex loss: 0.745202  [   46/   88]
per-ex loss: 0.573558  [   47/   88]
per-ex loss: 0.777414  [   48/   88]
per-ex loss: 0.638599  [   49/   88]
per-ex loss: 0.763109  [   50/   88]
per-ex loss: 0.853362  [   51/   88]
per-ex loss: 0.631937  [   52/   88]
per-ex loss: 0.783075  [   53/   88]
per-ex loss: 0.682731  [   54/   88]
per-ex loss: 0.790913  [   55/   88]
per-ex loss: 0.820769  [   56/   88]
per-ex loss: 0.629020  [   57/   88]
per-ex loss: 0.771685  [   58/   88]
per-ex loss: 0.881691  [   59/   88]
per-ex loss: 0.615322  [   60/   88]
per-ex loss: 0.833954  [   61/   88]
per-ex loss: 0.811501  [   62/   88]
per-ex loss: 0.663810  [   63/   88]
per-ex loss: 0.648559  [   64/   88]
per-ex loss: 0.771710  [   65/   88]
per-ex loss: 0.759311  [   66/   88]
per-ex loss: 0.630707  [   67/   88]
per-ex loss: 0.742824  [   68/   88]
per-ex loss: 0.842797  [   69/   88]
per-ex loss: 0.608578  [   70/   88]
per-ex loss: 0.631271  [   71/   88]
per-ex loss: 0.704655  [   72/   88]
per-ex loss: 0.691025  [   73/   88]
per-ex loss: 0.833179  [   74/   88]
per-ex loss: 0.697544  [   75/   88]
per-ex loss: 0.755924  [   76/   88]
per-ex loss: 0.753445  [   77/   88]
per-ex loss: 0.573895  [   78/   88]
per-ex loss: 0.820514  [   79/   88]
per-ex loss: 0.661707  [   80/   88]
per-ex loss: 0.756535  [   81/   88]
per-ex loss: 0.833103  [   82/   88]
per-ex loss: 0.642724  [   83/   88]
per-ex loss: 0.817386  [   84/   88]
per-ex loss: 0.796764  [   85/   88]
per-ex loss: 0.795320  [   86/   88]
per-ex loss: 0.767093  [   87/   88]
per-ex loss: 0.824587  [   88/   88]
Train Error: Avg loss: 0.72047442
validation Error: 
 Avg loss: 0.74041936 
 F1: 0.434434 
 Precision: 0.621602 
 Recall: 0.333896
 IoU: 0.277493

test Error: 
 Avg loss: 0.69435839 
 F1: 0.514357 
 Precision: 0.623539 
 Recall: 0.437713
 IoU: 0.346218

We have finished training iteration 5
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_1_.pth
per-ex loss: 0.815502  [    1/   88]
per-ex loss: 0.769334  [    2/   88]
per-ex loss: 0.778843  [    3/   88]
per-ex loss: 0.803760  [    4/   88]
per-ex loss: 0.674232  [    5/   88]
per-ex loss: 0.788810  [    6/   88]
per-ex loss: 0.840320  [    7/   88]
per-ex loss: 0.611733  [    8/   88]
per-ex loss: 0.694398  [    9/   88]
per-ex loss: 0.758807  [   10/   88]
per-ex loss: 0.648848  [   11/   88]
per-ex loss: 0.699612  [   12/   88]
per-ex loss: 0.767096  [   13/   88]
per-ex loss: 0.801015  [   14/   88]
per-ex loss: 0.749179  [   15/   88]
per-ex loss: 0.810996  [   16/   88]
per-ex loss: 0.638751  [   17/   88]
per-ex loss: 0.708872  [   18/   88]
per-ex loss: 0.661615  [   19/   88]
per-ex loss: 0.874932  [   20/   88]
per-ex loss: 0.833459  [   21/   88]
per-ex loss: 0.703169  [   22/   88]
per-ex loss: 0.663704  [   23/   88]
per-ex loss: 0.762584  [   24/   88]
per-ex loss: 0.641640  [   25/   88]
per-ex loss: 0.780916  [   26/   88]
per-ex loss: 0.812664  [   27/   88]
per-ex loss: 0.622384  [   28/   88]
per-ex loss: 0.758816  [   29/   88]
per-ex loss: 0.777382  [   30/   88]
per-ex loss: 0.660257  [   31/   88]
per-ex loss: 0.752143  [   32/   88]
per-ex loss: 0.763584  [   33/   88]
per-ex loss: 0.610165  [   34/   88]
per-ex loss: 0.663199  [   35/   88]
per-ex loss: 0.634342  [   36/   88]
per-ex loss: 0.767765  [   37/   88]
per-ex loss: 0.610658  [   38/   88]
per-ex loss: 0.592088  [   39/   88]
per-ex loss: 0.667026  [   40/   88]
per-ex loss: 0.681260  [   41/   88]
per-ex loss: 0.573249  [   42/   88]
per-ex loss: 0.655610  [   43/   88]
per-ex loss: 0.527945  [   44/   88]
per-ex loss: 0.858112  [   45/   88]
per-ex loss: 0.654298  [   46/   88]
per-ex loss: 0.835354  [   47/   88]
per-ex loss: 0.720373  [   48/   88]
per-ex loss: 0.801797  [   49/   88]
per-ex loss: 0.648411  [   50/   88]
per-ex loss: 0.874645  [   51/   88]
per-ex loss: 0.788651  [   52/   88]
per-ex loss: 0.655659  [   53/   88]
per-ex loss: 0.819385  [   54/   88]
per-ex loss: 0.733329  [   55/   88]
per-ex loss: 0.685821  [   56/   88]
per-ex loss: 0.676898  [   57/   88]
per-ex loss: 0.692248  [   58/   88]
per-ex loss: 0.852749  [   59/   88]
per-ex loss: 0.620165  [   60/   88]
per-ex loss: 0.634448  [   61/   88]
per-ex loss: 0.610162  [   62/   88]
per-ex loss: 0.620020  [   63/   88]
per-ex loss: 0.633838  [   64/   88]
per-ex loss: 0.660846  [   65/   88]
per-ex loss: 0.572919  [   66/   88]
per-ex loss: 0.569783  [   67/   88]
per-ex loss: 0.567015  [   68/   88]
per-ex loss: 0.647318  [   69/   88]
per-ex loss: 0.776957  [   70/   88]
per-ex loss: 0.738404  [   71/   88]
per-ex loss: 0.858460  [   72/   88]
per-ex loss: 0.856691  [   73/   88]
per-ex loss: 0.734176  [   74/   88]
per-ex loss: 0.752393  [   75/   88]
per-ex loss: 0.835038  [   76/   88]
per-ex loss: 0.593216  [   77/   88]
per-ex loss: 0.815761  [   78/   88]
per-ex loss: 0.720429  [   79/   88]
per-ex loss: 0.835750  [   80/   88]
per-ex loss: 0.864430  [   81/   88]
per-ex loss: 0.616711  [   82/   88]
per-ex loss: 0.595364  [   83/   88]
per-ex loss: 0.597291  [   84/   88]
per-ex loss: 0.761717  [   85/   88]
per-ex loss: 0.791038  [   86/   88]
per-ex loss: 0.762666  [   87/   88]
per-ex loss: 0.729783  [   88/   88]
Train Error: Avg loss: 0.71685439
validation Error: 
 Avg loss: 0.73162462 
 F1: 0.453255 
 Precision: 0.540416 
 Recall: 0.390305
 IoU: 0.293038

test Error: 
 Avg loss: 0.68160188 
 F1: 0.534689 
 Precision: 0.551114 
 Recall: 0.519214
 IoU: 0.364898

We have finished training iteration 6
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_2_.pth
per-ex loss: 0.683449  [    1/   88]
per-ex loss: 0.785553  [    2/   88]
per-ex loss: 0.771632  [    3/   88]
per-ex loss: 0.736703  [    4/   88]
per-ex loss: 0.754248  [    5/   88]
per-ex loss: 0.843056  [    6/   88]
per-ex loss: 0.848199  [    7/   88]
per-ex loss: 0.793209  [    8/   88]
per-ex loss: 0.810079  [    9/   88]
per-ex loss: 0.717340  [   10/   88]
per-ex loss: 0.614984  [   11/   88]
per-ex loss: 0.808885  [   12/   88]
per-ex loss: 0.809273  [   13/   88]
per-ex loss: 0.781696  [   14/   88]
per-ex loss: 0.772284  [   15/   88]
per-ex loss: 0.701978  [   16/   88]
per-ex loss: 0.647296  [   17/   88]
per-ex loss: 0.690970  [   18/   88]
per-ex loss: 0.594965  [   19/   88]
per-ex loss: 0.670555  [   20/   88]
per-ex loss: 0.759067  [   21/   88]
per-ex loss: 0.784421  [   22/   88]
per-ex loss: 0.656305  [   23/   88]
per-ex loss: 0.728589  [   24/   88]
per-ex loss: 0.816201  [   25/   88]
per-ex loss: 0.660276  [   26/   88]
per-ex loss: 0.745914  [   27/   88]
per-ex loss: 0.603834  [   28/   88]
per-ex loss: 0.610885  [   29/   88]
per-ex loss: 0.762549  [   30/   88]
per-ex loss: 0.703510  [   31/   88]
per-ex loss: 0.800435  [   32/   88]
per-ex loss: 0.790670  [   33/   88]
per-ex loss: 0.665920  [   34/   88]
per-ex loss: 0.809880  [   35/   88]
per-ex loss: 0.802051  [   36/   88]
per-ex loss: 0.849565  [   37/   88]
per-ex loss: 0.758408  [   38/   88]
per-ex loss: 0.659527  [   39/   88]
per-ex loss: 0.788021  [   40/   88]
per-ex loss: 0.599500  [   41/   88]
per-ex loss: 0.544373  [   42/   88]
per-ex loss: 0.544987  [   43/   88]
per-ex loss: 0.765447  [   44/   88]
per-ex loss: 0.784324  [   45/   88]
per-ex loss: 0.662420  [   46/   88]
per-ex loss: 0.640128  [   47/   88]
per-ex loss: 0.586027  [   48/   88]
per-ex loss: 0.726250  [   49/   88]
per-ex loss: 0.675963  [   50/   88]
per-ex loss: 0.832226  [   51/   88]
per-ex loss: 0.595215  [   52/   88]
per-ex loss: 0.625134  [   53/   88]
per-ex loss: 0.831301  [   54/   88]
per-ex loss: 0.762749  [   55/   88]
per-ex loss: 0.635292  [   56/   88]
per-ex loss: 0.670135  [   57/   88]
per-ex loss: 0.662363  [   58/   88]
per-ex loss: 0.626480  [   59/   88]
per-ex loss: 0.625612  [   60/   88]
per-ex loss: 0.770040  [   61/   88]
per-ex loss: 0.647956  [   62/   88]
per-ex loss: 0.557090  [   63/   88]
per-ex loss: 0.849105  [   64/   88]
per-ex loss: 0.685253  [   65/   88]
per-ex loss: 0.741313  [   66/   88]
per-ex loss: 0.695825  [   67/   88]
per-ex loss: 0.691940  [   68/   88]
per-ex loss: 0.637652  [   69/   88]
per-ex loss: 0.651131  [   70/   88]
per-ex loss: 0.808167  [   71/   88]
per-ex loss: 0.793209  [   72/   88]
per-ex loss: 0.847191  [   73/   88]
per-ex loss: 0.657683  [   74/   88]
per-ex loss: 0.550166  [   75/   88]
per-ex loss: 0.784150  [   76/   88]
per-ex loss: 0.807526  [   77/   88]
per-ex loss: 0.662669  [   78/   88]
per-ex loss: 0.704268  [   79/   88]
per-ex loss: 0.590900  [   80/   88]
per-ex loss: 0.596087  [   81/   88]
per-ex loss: 0.741928  [   82/   88]
per-ex loss: 0.570804  [   83/   88]
per-ex loss: 0.603077  [   84/   88]
per-ex loss: 0.747343  [   85/   88]
per-ex loss: 0.558982  [   86/   88]
per-ex loss: 0.671253  [   87/   88]
per-ex loss: 0.634483  [   88/   88]
Train Error: Avg loss: 0.70733517
validation Error: 
 Avg loss: 0.73091588 
 F1: 0.445137 
 Precision: 0.547691 
 Recall: 0.374932
 IoU: 0.286287

test Error: 
 Avg loss: 0.68217677 
 F1: 0.531714 
 Precision: 0.566320 
 Recall: 0.501093
 IoU: 0.362132

We have finished training iteration 7
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_3_.pth
per-ex loss: 0.612313  [    1/   88]
per-ex loss: 0.541631  [    2/   88]
per-ex loss: 0.883210  [    3/   88]
per-ex loss: 0.768084  [    4/   88]
per-ex loss: 0.705067  [    5/   88]
per-ex loss: 0.785942  [    6/   88]
per-ex loss: 0.799786  [    7/   88]
per-ex loss: 0.783180  [    8/   88]
per-ex loss: 0.667026  [    9/   88]
per-ex loss: 0.601565  [   10/   88]
per-ex loss: 0.627447  [   11/   88]
per-ex loss: 0.624338  [   12/   88]
per-ex loss: 0.838887  [   13/   88]
per-ex loss: 0.673457  [   14/   88]
per-ex loss: 0.561032  [   15/   88]
per-ex loss: 0.644765  [   16/   88]
per-ex loss: 0.821879  [   17/   88]
per-ex loss: 0.834536  [   18/   88]
per-ex loss: 0.801631  [   19/   88]
per-ex loss: 0.665638  [   20/   88]
per-ex loss: 0.807491  [   21/   88]
per-ex loss: 0.663196  [   22/   88]
per-ex loss: 0.644137  [   23/   88]
per-ex loss: 0.583268  [   24/   88]
per-ex loss: 0.754550  [   25/   88]
per-ex loss: 0.646721  [   26/   88]
per-ex loss: 0.582618  [   27/   88]
per-ex loss: 0.707078  [   28/   88]
per-ex loss: 0.630487  [   29/   88]
per-ex loss: 0.747581  [   30/   88]
per-ex loss: 0.720838  [   31/   88]
per-ex loss: 0.654910  [   32/   88]
per-ex loss: 0.790625  [   33/   88]
per-ex loss: 0.854963  [   34/   88]
per-ex loss: 0.596140  [   35/   88]
per-ex loss: 0.613261  [   36/   88]
per-ex loss: 0.592856  [   37/   88]
per-ex loss: 0.762263  [   38/   88]
per-ex loss: 0.799777  [   39/   88]
per-ex loss: 0.833541  [   40/   88]
per-ex loss: 0.737766  [   41/   88]
per-ex loss: 0.609880  [   42/   88]
per-ex loss: 0.632235  [   43/   88]
per-ex loss: 0.598942  [   44/   88]
per-ex loss: 0.683210  [   45/   88]
per-ex loss: 0.751842  [   46/   88]
per-ex loss: 0.650805  [   47/   88]
per-ex loss: 0.682743  [   48/   88]
per-ex loss: 0.775601  [   49/   88]
per-ex loss: 0.585292  [   50/   88]
per-ex loss: 0.630334  [   51/   88]
per-ex loss: 0.689851  [   52/   88]
per-ex loss: 0.672579  [   53/   88]
per-ex loss: 0.553891  [   54/   88]
per-ex loss: 0.766099  [   55/   88]
per-ex loss: 0.739465  [   56/   88]
per-ex loss: 0.817455  [   57/   88]
per-ex loss: 0.522532  [   58/   88]
per-ex loss: 0.801289  [   59/   88]
per-ex loss: 0.624026  [   60/   88]
per-ex loss: 0.855816  [   61/   88]
per-ex loss: 0.622629  [   62/   88]
per-ex loss: 0.825024  [   63/   88]
per-ex loss: 0.689986  [   64/   88]
per-ex loss: 0.810545  [   65/   88]
per-ex loss: 0.767881  [   66/   88]
per-ex loss: 0.804358  [   67/   88]
per-ex loss: 0.559251  [   68/   88]
per-ex loss: 0.789056  [   69/   88]
per-ex loss: 0.806635  [   70/   88]
per-ex loss: 0.746403  [   71/   88]
per-ex loss: 0.674199  [   72/   88]
per-ex loss: 0.615179  [   73/   88]
per-ex loss: 0.832318  [   74/   88]
per-ex loss: 0.531463  [   75/   88]
per-ex loss: 0.759140  [   76/   88]
per-ex loss: 0.751173  [   77/   88]
per-ex loss: 0.644824  [   78/   88]
per-ex loss: 0.748369  [   79/   88]
per-ex loss: 0.776232  [   80/   88]
per-ex loss: 0.686434  [   81/   88]
per-ex loss: 0.748085  [   82/   88]
per-ex loss: 0.850371  [   83/   88]
per-ex loss: 0.684642  [   84/   88]
per-ex loss: 0.674999  [   85/   88]
per-ex loss: 0.751199  [   86/   88]
per-ex loss: 0.808486  [   87/   88]
per-ex loss: 0.784503  [   88/   88]
Train Error: Avg loss: 0.70855399
validation Error: 
 Avg loss: 0.73639137 
 F1: 0.440971 
 Precision: 0.516597 
 Recall: 0.384660
 IoU: 0.282850

test Error: 
 Avg loss: 0.69207214 
 F1: 0.514140 
 Precision: 0.557631 
 Recall: 0.476942
 IoU: 0.346022

We have finished training iteration 8
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_4_.pth
per-ex loss: 0.834096  [    1/   88]
per-ex loss: 0.577686  [    2/   88]
per-ex loss: 0.614879  [    3/   88]
per-ex loss: 0.798371  [    4/   88]
per-ex loss: 0.721367  [    5/   88]
per-ex loss: 0.777435  [    6/   88]
per-ex loss: 0.793344  [    7/   88]
per-ex loss: 0.604498  [    8/   88]
per-ex loss: 0.634386  [    9/   88]
per-ex loss: 0.724596  [   10/   88]
per-ex loss: 0.614279  [   11/   88]
per-ex loss: 0.780205  [   12/   88]
per-ex loss: 0.634710  [   13/   88]
per-ex loss: 0.838816  [   14/   88]
per-ex loss: 0.563183  [   15/   88]
per-ex loss: 0.773754  [   16/   88]
per-ex loss: 0.733374  [   17/   88]
per-ex loss: 0.847314  [   18/   88]
per-ex loss: 0.631241  [   19/   88]
per-ex loss: 0.778519  [   20/   88]
per-ex loss: 0.590389  [   21/   88]
per-ex loss: 0.824128  [   22/   88]
per-ex loss: 0.768042  [   23/   88]
per-ex loss: 0.652490  [   24/   88]
per-ex loss: 0.615025  [   25/   88]
per-ex loss: 0.628481  [   26/   88]
per-ex loss: 0.620484  [   27/   88]
per-ex loss: 0.833470  [   28/   88]
per-ex loss: 0.659931  [   29/   88]
per-ex loss: 0.675161  [   30/   88]
per-ex loss: 0.666365  [   31/   88]
per-ex loss: 0.648919  [   32/   88]
per-ex loss: 0.791797  [   33/   88]
per-ex loss: 0.727580  [   34/   88]
per-ex loss: 0.723509  [   35/   88]
per-ex loss: 0.697921  [   36/   88]
per-ex loss: 0.666917  [   37/   88]
per-ex loss: 0.806377  [   38/   88]
per-ex loss: 0.583410  [   39/   88]
per-ex loss: 0.627503  [   40/   88]
per-ex loss: 0.784025  [   41/   88]
per-ex loss: 0.803084  [   42/   88]
per-ex loss: 0.617626  [   43/   88]
per-ex loss: 0.721087  [   44/   88]
per-ex loss: 0.793205  [   45/   88]
per-ex loss: 0.545829  [   46/   88]
per-ex loss: 0.621993  [   47/   88]
per-ex loss: 0.535968  [   48/   88]
per-ex loss: 0.577474  [   49/   88]
per-ex loss: 0.797029  [   50/   88]
per-ex loss: 0.706950  [   51/   88]
per-ex loss: 0.570225  [   52/   88]
per-ex loss: 0.765102  [   53/   88]
per-ex loss: 0.765811  [   54/   88]
per-ex loss: 0.744148  [   55/   88]
per-ex loss: 0.806615  [   56/   88]
per-ex loss: 0.665565  [   57/   88]
per-ex loss: 0.696649  [   58/   88]
per-ex loss: 0.574460  [   59/   88]
per-ex loss: 0.674454  [   60/   88]
per-ex loss: 0.830009  [   61/   88]
per-ex loss: 0.852941  [   62/   88]
per-ex loss: 0.658100  [   63/   88]
per-ex loss: 0.794431  [   64/   88]
per-ex loss: 0.579484  [   65/   88]
per-ex loss: 0.841857  [   66/   88]
per-ex loss: 0.746142  [   67/   88]
per-ex loss: 0.859489  [   68/   88]
per-ex loss: 0.590062  [   69/   88]
per-ex loss: 0.648188  [   70/   88]
per-ex loss: 0.756744  [   71/   88]
per-ex loss: 0.761503  [   72/   88]
per-ex loss: 0.753084  [   73/   88]
per-ex loss: 0.659259  [   74/   88]
per-ex loss: 0.691446  [   75/   88]
per-ex loss: 0.592793  [   76/   88]
per-ex loss: 0.658917  [   77/   88]
per-ex loss: 0.602334  [   78/   88]
per-ex loss: 0.729821  [   79/   88]
per-ex loss: 0.624411  [   80/   88]
per-ex loss: 0.778474  [   81/   88]
per-ex loss: 0.617272  [   82/   88]
per-ex loss: 0.797022  [   83/   88]
per-ex loss: 0.760986  [   84/   88]
per-ex loss: 0.783966  [   85/   88]
per-ex loss: 0.649388  [   86/   88]
per-ex loss: 0.630788  [   87/   88]
per-ex loss: 0.535667  [   88/   88]
Train Error: Avg loss: 0.70040718
validation Error: 
 Avg loss: 0.71652765 
 F1: 0.467154 
 Precision: 0.587648 
 Recall: 0.387665
 IoU: 0.304762

test Error: 
 Avg loss: 0.66985028 
 F1: 0.544240 
 Precision: 0.606148 
 Recall: 0.493806
 IoU: 0.373853

We have finished training iteration 9
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_5_.pth
per-ex loss: 0.760496  [    1/   88]
per-ex loss: 0.591591  [    2/   88]
per-ex loss: 0.654586  [    3/   88]
per-ex loss: 0.568835  [    4/   88]
per-ex loss: 0.621982  [    5/   88]
per-ex loss: 0.845964  [    6/   88]
per-ex loss: 0.607135  [    7/   88]
per-ex loss: 0.594682  [    8/   88]
per-ex loss: 0.821026  [    9/   88]
per-ex loss: 0.664977  [   10/   88]
per-ex loss: 0.741803  [   11/   88]
per-ex loss: 0.773361  [   12/   88]
per-ex loss: 0.666275  [   13/   88]
per-ex loss: 0.741531  [   14/   88]
per-ex loss: 0.818162  [   15/   88]
per-ex loss: 0.724885  [   16/   88]
per-ex loss: 0.747578  [   17/   88]
per-ex loss: 0.661606  [   18/   88]
per-ex loss: 0.797688  [   19/   88]
per-ex loss: 0.610006  [   20/   88]
per-ex loss: 0.761744  [   21/   88]
per-ex loss: 0.751856  [   22/   88]
per-ex loss: 0.693345  [   23/   88]
per-ex loss: 0.646156  [   24/   88]
per-ex loss: 0.796896  [   25/   88]
per-ex loss: 0.789872  [   26/   88]
per-ex loss: 0.752911  [   27/   88]
per-ex loss: 0.647495  [   28/   88]
per-ex loss: 0.763715  [   29/   88]
per-ex loss: 0.562236  [   30/   88]
per-ex loss: 0.570678  [   31/   88]
per-ex loss: 0.577541  [   32/   88]
per-ex loss: 0.840295  [   33/   88]
per-ex loss: 0.556581  [   34/   88]
per-ex loss: 0.777206  [   35/   88]
per-ex loss: 0.794696  [   36/   88]
per-ex loss: 0.770214  [   37/   88]
per-ex loss: 0.584627  [   38/   88]
per-ex loss: 0.554037  [   39/   88]
per-ex loss: 0.669093  [   40/   88]
per-ex loss: 0.842833  [   41/   88]
per-ex loss: 0.652150  [   42/   88]
per-ex loss: 0.567649  [   43/   88]
per-ex loss: 0.579380  [   44/   88]
per-ex loss: 0.752181  [   45/   88]
per-ex loss: 0.627915  [   46/   88]
per-ex loss: 0.516416  [   47/   88]
per-ex loss: 0.833510  [   48/   88]
per-ex loss: 0.697178  [   49/   88]
per-ex loss: 0.553001  [   50/   88]
per-ex loss: 0.744766  [   51/   88]
per-ex loss: 0.683669  [   52/   88]
per-ex loss: 0.638902  [   53/   88]
per-ex loss: 0.763970  [   54/   88]
per-ex loss: 0.743566  [   55/   88]
per-ex loss: 0.804536  [   56/   88]
per-ex loss: 0.684445  [   57/   88]
per-ex loss: 0.795976  [   58/   88]
per-ex loss: 0.732486  [   59/   88]
per-ex loss: 0.821418  [   60/   88]
per-ex loss: 0.635932  [   61/   88]
per-ex loss: 0.815408  [   62/   88]
per-ex loss: 0.654497  [   63/   88]
per-ex loss: 0.825082  [   64/   88]
per-ex loss: 0.843838  [   65/   88]
per-ex loss: 0.598479  [   66/   88]
per-ex loss: 0.620650  [   67/   88]
per-ex loss: 0.661186  [   68/   88]
per-ex loss: 0.799363  [   69/   88]
per-ex loss: 0.843697  [   70/   88]
per-ex loss: 0.813519  [   71/   88]
per-ex loss: 0.698486  [   72/   88]
per-ex loss: 0.626544  [   73/   88]
per-ex loss: 0.657606  [   74/   88]
per-ex loss: 0.682857  [   75/   88]
per-ex loss: 0.646655  [   76/   88]
per-ex loss: 0.709372  [   77/   88]
per-ex loss: 0.837439  [   78/   88]
per-ex loss: 0.754538  [   79/   88]
per-ex loss: 0.640676  [   80/   88]
per-ex loss: 0.766066  [   81/   88]
per-ex loss: 0.596438  [   82/   88]
per-ex loss: 0.817975  [   83/   88]
per-ex loss: 0.589218  [   84/   88]
per-ex loss: 0.617486  [   85/   88]
per-ex loss: 0.741423  [   86/   88]
per-ex loss: 0.588676  [   87/   88]
per-ex loss: 0.539455  [   88/   88]
Train Error: Avg loss: 0.69924852
validation Error: 
 Avg loss: 0.72152344 
 F1: 0.462882 
 Precision: 0.567678 
 Recall: 0.390748
 IoU: 0.301136

test Error: 
 Avg loss: 0.67913918 
 F1: 0.534039 
 Precision: 0.576978 
 Recall: 0.497049
 IoU: 0.364293

We have finished training iteration 10
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_8_.pth
per-ex loss: 0.613529  [    1/   88]
per-ex loss: 0.636628  [    2/   88]
per-ex loss: 0.551369  [    3/   88]
per-ex loss: 0.780191  [    4/   88]
per-ex loss: 0.651031  [    5/   88]
per-ex loss: 0.649231  [    6/   88]
per-ex loss: 0.740496  [    7/   88]
per-ex loss: 0.565588  [    8/   88]
per-ex loss: 0.662783  [    9/   88]
per-ex loss: 0.752085  [   10/   88]
per-ex loss: 0.682395  [   11/   88]
per-ex loss: 0.771520  [   12/   88]
per-ex loss: 0.600102  [   13/   88]
per-ex loss: 0.842070  [   14/   88]
per-ex loss: 0.591743  [   15/   88]
per-ex loss: 0.619943  [   16/   88]
per-ex loss: 0.757499  [   17/   88]
per-ex loss: 0.809613  [   18/   88]
per-ex loss: 0.518777  [   19/   88]
per-ex loss: 0.710836  [   20/   88]
per-ex loss: 0.836624  [   21/   88]
per-ex loss: 0.773874  [   22/   88]
per-ex loss: 0.610514  [   23/   88]
per-ex loss: 0.722476  [   24/   88]
per-ex loss: 0.783128  [   25/   88]
per-ex loss: 0.651059  [   26/   88]
per-ex loss: 0.805396  [   27/   88]
per-ex loss: 0.761596  [   28/   88]
per-ex loss: 0.813023  [   29/   88]
per-ex loss: 0.796063  [   30/   88]
per-ex loss: 0.738052  [   31/   88]
per-ex loss: 0.633835  [   32/   88]
per-ex loss: 0.771678  [   33/   88]
per-ex loss: 0.564842  [   34/   88]
per-ex loss: 0.657986  [   35/   88]
per-ex loss: 0.669266  [   36/   88]
per-ex loss: 0.801852  [   37/   88]
per-ex loss: 0.628068  [   38/   88]
per-ex loss: 0.745777  [   39/   88]
per-ex loss: 0.558428  [   40/   88]
per-ex loss: 0.514622  [   41/   88]
per-ex loss: 0.847971  [   42/   88]
per-ex loss: 0.619334  [   43/   88]
per-ex loss: 0.758202  [   44/   88]
per-ex loss: 0.663796  [   45/   88]
per-ex loss: 0.685282  [   46/   88]
per-ex loss: 0.674004  [   47/   88]
per-ex loss: 0.811005  [   48/   88]
per-ex loss: 0.792390  [   49/   88]
per-ex loss: 0.697199  [   50/   88]
per-ex loss: 0.553095  [   51/   88]
per-ex loss: 0.743441  [   52/   88]
per-ex loss: 0.525942  [   53/   88]
per-ex loss: 0.583882  [   54/   88]
per-ex loss: 0.832277  [   55/   88]
per-ex loss: 0.670224  [   56/   88]
per-ex loss: 0.593830  [   57/   88]
per-ex loss: 0.823503  [   58/   88]
per-ex loss: 0.573458  [   59/   88]
per-ex loss: 0.831533  [   60/   88]
per-ex loss: 0.563546  [   61/   88]
per-ex loss: 0.774094  [   62/   88]
per-ex loss: 0.675718  [   63/   88]
per-ex loss: 0.602686  [   64/   88]
per-ex loss: 0.697580  [   65/   88]
per-ex loss: 0.847273  [   66/   88]
per-ex loss: 0.717417  [   67/   88]
per-ex loss: 0.573635  [   68/   88]
per-ex loss: 0.581312  [   69/   88]
per-ex loss: 0.800382  [   70/   88]
per-ex loss: 0.620416  [   71/   88]
per-ex loss: 0.748737  [   72/   88]
per-ex loss: 0.855354  [   73/   88]
per-ex loss: 0.790764  [   74/   88]
per-ex loss: 0.720897  [   75/   88]
per-ex loss: 0.752565  [   76/   88]
per-ex loss: 0.657944  [   77/   88]
per-ex loss: 0.766637  [   78/   88]
per-ex loss: 0.645373  [   79/   88]
per-ex loss: 0.632069  [   80/   88]
per-ex loss: 0.819400  [   81/   88]
per-ex loss: 0.788158  [   82/   88]
per-ex loss: 0.578060  [   83/   88]
per-ex loss: 0.776789  [   84/   88]
per-ex loss: 0.627244  [   85/   88]
per-ex loss: 0.577416  [   86/   88]
per-ex loss: 0.669711  [   87/   88]
per-ex loss: 0.711479  [   88/   88]
Train Error: Avg loss: 0.69543875
validation Error: 
 Avg loss: 0.71685965 
 F1: 0.465505 
 Precision: 0.559428 
 Recall: 0.398586
 IoU: 0.303360

test Error: 
 Avg loss: 0.67259322 
 F1: 0.538526 
 Precision: 0.580723 
 Recall: 0.502046
 IoU: 0.368481

We have finished training iteration 11
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_6_.pth
per-ex loss: 0.748406  [    1/   88]
per-ex loss: 0.523409  [    2/   88]
per-ex loss: 0.730259  [    3/   88]
per-ex loss: 0.560378  [    4/   88]
per-ex loss: 0.745669  [    5/   88]
per-ex loss: 0.667010  [    6/   88]
per-ex loss: 0.740086  [    7/   88]
per-ex loss: 0.829419  [    8/   88]
per-ex loss: 0.641542  [    9/   88]
per-ex loss: 0.781344  [   10/   88]
per-ex loss: 0.576303  [   11/   88]
per-ex loss: 0.640792  [   12/   88]
per-ex loss: 0.842198  [   13/   88]
per-ex loss: 0.609021  [   14/   88]
per-ex loss: 0.536428  [   15/   88]
per-ex loss: 0.626076  [   16/   88]
per-ex loss: 0.770124  [   17/   88]
per-ex loss: 0.726879  [   18/   88]
per-ex loss: 0.830461  [   19/   88]
per-ex loss: 0.633361  [   20/   88]
per-ex loss: 0.625683  [   21/   88]
per-ex loss: 0.684498  [   22/   88]
per-ex loss: 0.799760  [   23/   88]
per-ex loss: 0.802323  [   24/   88]
per-ex loss: 0.720436  [   25/   88]
per-ex loss: 0.704541  [   26/   88]
per-ex loss: 0.604290  [   27/   88]
per-ex loss: 0.648832  [   28/   88]
per-ex loss: 0.795628  [   29/   88]
per-ex loss: 0.646377  [   30/   88]
per-ex loss: 0.563100  [   31/   88]
per-ex loss: 0.763594  [   32/   88]
per-ex loss: 0.541253  [   33/   88]
per-ex loss: 0.657137  [   34/   88]
per-ex loss: 0.793339  [   35/   88]
per-ex loss: 0.689554  [   36/   88]
per-ex loss: 0.850029  [   37/   88]
per-ex loss: 0.765944  [   38/   88]
per-ex loss: 0.674298  [   39/   88]
per-ex loss: 0.636438  [   40/   88]
per-ex loss: 0.797824  [   41/   88]
per-ex loss: 0.521006  [   42/   88]
per-ex loss: 0.827406  [   43/   88]
per-ex loss: 0.734830  [   44/   88]
per-ex loss: 0.552998  [   45/   88]
per-ex loss: 0.609724  [   46/   88]
per-ex loss: 0.625353  [   47/   88]
per-ex loss: 0.584866  [   48/   88]
per-ex loss: 0.828235  [   49/   88]
per-ex loss: 0.854722  [   50/   88]
per-ex loss: 0.817576  [   51/   88]
per-ex loss: 0.580028  [   52/   88]
per-ex loss: 0.594824  [   53/   88]
per-ex loss: 0.549206  [   54/   88]
per-ex loss: 0.826045  [   55/   88]
per-ex loss: 0.742979  [   56/   88]
per-ex loss: 0.766686  [   57/   88]
per-ex loss: 0.620079  [   58/   88]
per-ex loss: 0.787569  [   59/   88]
per-ex loss: 0.626516  [   60/   88]
per-ex loss: 0.791005  [   61/   88]
per-ex loss: 0.574067  [   62/   88]
per-ex loss: 0.651842  [   63/   88]
per-ex loss: 0.773602  [   64/   88]
per-ex loss: 0.803815  [   65/   88]
per-ex loss: 0.756861  [   66/   88]
per-ex loss: 0.593554  [   67/   88]
per-ex loss: 0.632588  [   68/   88]
per-ex loss: 0.614883  [   69/   88]
per-ex loss: 0.648837  [   70/   88]
per-ex loss: 0.663239  [   71/   88]
per-ex loss: 0.789118  [   72/   88]
per-ex loss: 0.557213  [   73/   88]
per-ex loss: 0.580448  [   74/   88]
per-ex loss: 0.742687  [   75/   88]
per-ex loss: 0.737681  [   76/   88]
per-ex loss: 0.821661  [   77/   88]
per-ex loss: 0.619500  [   78/   88]
per-ex loss: 0.835307  [   79/   88]
per-ex loss: 0.657156  [   80/   88]
per-ex loss: 0.709342  [   81/   88]
per-ex loss: 0.596510  [   82/   88]
per-ex loss: 0.568128  [   83/   88]
per-ex loss: 0.658684  [   84/   88]
per-ex loss: 0.829631  [   85/   88]
per-ex loss: 0.729796  [   86/   88]
per-ex loss: 0.732663  [   87/   88]
per-ex loss: 0.596877  [   88/   88]
Train Error: Avg loss: 0.69137944
validation Error: 
 Avg loss: 0.72051577 
 F1: 0.465878 
 Precision: 0.458486 
 Recall: 0.473513
 IoU: 0.303678

test Error: 
 Avg loss: 0.67990680 
 F1: 0.534225 
 Precision: 0.488296 
 Recall: 0.589691
 IoU: 0.364466

We have finished training iteration 12
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_7_.pth
per-ex loss: 0.486419  [    1/   88]
per-ex loss: 0.741630  [    2/   88]
per-ex loss: 0.764073  [    3/   88]
per-ex loss: 0.553384  [    4/   88]
per-ex loss: 0.737677  [    5/   88]
per-ex loss: 0.602981  [    6/   88]
per-ex loss: 0.634944  [    7/   88]
per-ex loss: 0.778214  [    8/   88]
per-ex loss: 0.621897  [    9/   88]
per-ex loss: 0.773987  [   10/   88]
per-ex loss: 0.579277  [   11/   88]
per-ex loss: 0.746718  [   12/   88]
per-ex loss: 0.589078  [   13/   88]
per-ex loss: 0.827023  [   14/   88]
per-ex loss: 0.603883  [   15/   88]
per-ex loss: 0.744419  [   16/   88]
per-ex loss: 0.736983  [   17/   88]
per-ex loss: 0.716603  [   18/   88]
per-ex loss: 0.808388  [   19/   88]
per-ex loss: 0.782618  [   20/   88]
per-ex loss: 0.683541  [   21/   88]
per-ex loss: 0.630282  [   22/   88]
per-ex loss: 0.822424  [   23/   88]
per-ex loss: 0.849604  [   24/   88]
per-ex loss: 0.651024  [   25/   88]
per-ex loss: 0.556383  [   26/   88]
per-ex loss: 0.767234  [   27/   88]
per-ex loss: 0.712190  [   28/   88]
per-ex loss: 0.765242  [   29/   88]
per-ex loss: 0.771877  [   30/   88]
per-ex loss: 0.661889  [   31/   88]
per-ex loss: 0.598763  [   32/   88]
per-ex loss: 0.814310  [   33/   88]
per-ex loss: 0.570971  [   34/   88]
per-ex loss: 0.635535  [   35/   88]
per-ex loss: 0.660431  [   36/   88]
per-ex loss: 0.860731  [   37/   88]
per-ex loss: 0.761025  [   38/   88]
per-ex loss: 0.679426  [   39/   88]
per-ex loss: 0.843933  [   40/   88]
per-ex loss: 0.821138  [   41/   88]
per-ex loss: 0.617818  [   42/   88]
per-ex loss: 0.649619  [   43/   88]
per-ex loss: 0.746929  [   44/   88]
per-ex loss: 0.661981  [   45/   88]
per-ex loss: 0.597080  [   46/   88]
per-ex loss: 0.687502  [   47/   88]
per-ex loss: 0.649505  [   48/   88]
per-ex loss: 0.775096  [   49/   88]
per-ex loss: 0.634676  [   50/   88]
per-ex loss: 0.652675  [   51/   88]
per-ex loss: 0.586439  [   52/   88]
per-ex loss: 0.674316  [   53/   88]
per-ex loss: 0.760479  [   54/   88]
per-ex loss: 0.761629  [   55/   88]
per-ex loss: 0.741214  [   56/   88]
per-ex loss: 0.800036  [   57/   88]
per-ex loss: 0.640845  [   58/   88]
per-ex loss: 0.579747  [   59/   88]
per-ex loss: 0.739750  [   60/   88]
per-ex loss: 0.712795  [   61/   88]
per-ex loss: 0.721397  [   62/   88]
per-ex loss: 0.828226  [   63/   88]
per-ex loss: 0.589377  [   64/   88]
per-ex loss: 0.758420  [   65/   88]
per-ex loss: 0.735692  [   66/   88]
per-ex loss: 0.684140  [   67/   88]
per-ex loss: 0.637986  [   68/   88]
per-ex loss: 0.646500  [   69/   88]
per-ex loss: 0.569432  [   70/   88]
per-ex loss: 0.808948  [   71/   88]
per-ex loss: 0.632358  [   72/   88]
per-ex loss: 0.565755  [   73/   88]
per-ex loss: 0.607442  [   74/   88]
per-ex loss: 0.795857  [   75/   88]
per-ex loss: 0.550986  [   76/   88]
per-ex loss: 0.535594  [   77/   88]
per-ex loss: 0.584096  [   78/   88]
per-ex loss: 0.591412  [   79/   88]
per-ex loss: 0.835613  [   80/   88]
per-ex loss: 0.782293  [   81/   88]
per-ex loss: 0.637434  [   82/   88]
per-ex loss: 0.651171  [   83/   88]
per-ex loss: 0.632657  [   84/   88]
per-ex loss: 0.663087  [   85/   88]
per-ex loss: 0.576816  [   86/   88]
per-ex loss: 0.821957  [   87/   88]
per-ex loss: 0.734354  [   88/   88]
Train Error: Avg loss: 0.69087820
validation Error: 
 Avg loss: 0.74066284 
 F1: 0.423190 
 Precision: 0.656389 
 Recall: 0.312254
 IoU: 0.268383

test Error: 
 Avg loss: 0.68888256 
 F1: 0.518682 
 Precision: 0.673510 
 Recall: 0.421733
 IoU: 0.350149

We have finished training iteration 13
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_10_.pth
per-ex loss: 0.776597  [    1/   88]
per-ex loss: 0.797988  [    2/   88]
per-ex loss: 0.564089  [    3/   88]
per-ex loss: 0.737275  [    4/   88]
per-ex loss: 0.779819  [    5/   88]
per-ex loss: 0.858388  [    6/   88]
per-ex loss: 0.757801  [    7/   88]
per-ex loss: 0.813988  [    8/   88]
per-ex loss: 0.808178  [    9/   88]
per-ex loss: 0.579657  [   10/   88]
per-ex loss: 0.724863  [   11/   88]
per-ex loss: 0.538580  [   12/   88]
per-ex loss: 0.599618  [   13/   88]
per-ex loss: 0.810337  [   14/   88]
per-ex loss: 0.581809  [   15/   88]
per-ex loss: 0.726068  [   16/   88]
per-ex loss: 0.523648  [   17/   88]
per-ex loss: 0.693626  [   18/   88]
per-ex loss: 0.855182  [   19/   88]
per-ex loss: 0.538398  [   20/   88]
per-ex loss: 0.761182  [   21/   88]
per-ex loss: 0.558591  [   22/   88]
per-ex loss: 0.645843  [   23/   88]
per-ex loss: 0.872127  [   24/   88]
per-ex loss: 0.641391  [   25/   88]
per-ex loss: 0.640368  [   26/   88]
per-ex loss: 0.623729  [   27/   88]
per-ex loss: 0.545706  [   28/   88]
per-ex loss: 0.814992  [   29/   88]
per-ex loss: 0.749789  [   30/   88]
per-ex loss: 0.744601  [   31/   88]
per-ex loss: 0.555866  [   32/   88]
per-ex loss: 0.630268  [   33/   88]
per-ex loss: 0.607728  [   34/   88]
per-ex loss: 0.674933  [   35/   88]
per-ex loss: 0.754502  [   36/   88]
per-ex loss: 0.592580  [   37/   88]
per-ex loss: 0.770917  [   38/   88]
per-ex loss: 0.842046  [   39/   88]
per-ex loss: 0.600837  [   40/   88]
per-ex loss: 0.554932  [   41/   88]
per-ex loss: 0.610465  [   42/   88]
per-ex loss: 0.791464  [   43/   88]
per-ex loss: 0.810539  [   44/   88]
per-ex loss: 0.682055  [   45/   88]
per-ex loss: 0.651174  [   46/   88]
per-ex loss: 0.805955  [   47/   88]
per-ex loss: 0.727819  [   48/   88]
per-ex loss: 0.733296  [   49/   88]
per-ex loss: 0.685041  [   50/   88]
per-ex loss: 0.644512  [   51/   88]
per-ex loss: 0.661909  [   52/   88]
per-ex loss: 0.664907  [   53/   88]
per-ex loss: 0.768239  [   54/   88]
per-ex loss: 0.763138  [   55/   88]
per-ex loss: 0.728212  [   56/   88]
per-ex loss: 0.592566  [   57/   88]
per-ex loss: 0.646157  [   58/   88]
per-ex loss: 0.811148  [   59/   88]
per-ex loss: 0.618610  [   60/   88]
per-ex loss: 0.581954  [   61/   88]
per-ex loss: 0.734579  [   62/   88]
per-ex loss: 0.835223  [   63/   88]
per-ex loss: 0.779359  [   64/   88]
per-ex loss: 0.583218  [   65/   88]
per-ex loss: 0.584777  [   66/   88]
per-ex loss: 0.614125  [   67/   88]
per-ex loss: 0.602057  [   68/   88]
per-ex loss: 0.777440  [   69/   88]
per-ex loss: 0.665941  [   70/   88]
per-ex loss: 0.647593  [   71/   88]
per-ex loss: 0.646113  [   72/   88]
per-ex loss: 0.835027  [   73/   88]
per-ex loss: 0.598945  [   74/   88]
per-ex loss: 0.799585  [   75/   88]
per-ex loss: 0.660627  [   76/   88]
per-ex loss: 0.672757  [   77/   88]
per-ex loss: 0.781347  [   78/   88]
per-ex loss: 0.754494  [   79/   88]
per-ex loss: 0.752630  [   80/   88]
per-ex loss: 0.580974  [   81/   88]
per-ex loss: 0.633126  [   82/   88]
per-ex loss: 0.632282  [   83/   88]
per-ex loss: 0.732661  [   84/   88]
per-ex loss: 0.701369  [   85/   88]
per-ex loss: 0.800487  [   86/   88]
per-ex loss: 0.611961  [   87/   88]
per-ex loss: 0.594781  [   88/   88]
Train Error: Avg loss: 0.69138038
validation Error: 
 Avg loss: 0.71895075 
 F1: 0.461693 
 Precision: 0.527927 
 Recall: 0.410226
 IoU: 0.300131

test Error: 
 Avg loss: 0.66973045 
 F1: 0.544193 
 Precision: 0.563099 
 Recall: 0.526516
 IoU: 0.373809

We have finished training iteration 14
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_12_.pth
per-ex loss: 0.738608  [    1/   88]
per-ex loss: 0.616628  [    2/   88]
per-ex loss: 0.604819  [    3/   88]
per-ex loss: 0.730596  [    4/   88]
per-ex loss: 0.835265  [    5/   88]
per-ex loss: 0.866149  [    6/   88]
per-ex loss: 0.813504  [    7/   88]
per-ex loss: 0.792054  [    8/   88]
per-ex loss: 0.664747  [    9/   88]
per-ex loss: 0.731955  [   10/   88]
per-ex loss: 0.746497  [   11/   88]
per-ex loss: 0.568837  [   12/   88]
per-ex loss: 0.625132  [   13/   88]
per-ex loss: 0.668372  [   14/   88]
per-ex loss: 0.594305  [   15/   88]
per-ex loss: 0.738444  [   16/   88]
per-ex loss: 0.593028  [   17/   88]
per-ex loss: 0.663880  [   18/   88]
per-ex loss: 0.786311  [   19/   88]
per-ex loss: 0.722658  [   20/   88]
per-ex loss: 0.804546  [   21/   88]
per-ex loss: 0.827063  [   22/   88]
per-ex loss: 0.583229  [   23/   88]
per-ex loss: 0.622768  [   24/   88]
per-ex loss: 0.625805  [   25/   88]
per-ex loss: 0.601932  [   26/   88]
per-ex loss: 0.802681  [   27/   88]
per-ex loss: 0.838503  [   28/   88]
per-ex loss: 0.560825  [   29/   88]
per-ex loss: 0.652010  [   30/   88]
per-ex loss: 0.759560  [   31/   88]
per-ex loss: 0.714860  [   32/   88]
per-ex loss: 0.630423  [   33/   88]
per-ex loss: 0.515827  [   34/   88]
per-ex loss: 0.627269  [   35/   88]
per-ex loss: 0.849906  [   36/   88]
per-ex loss: 0.618196  [   37/   88]
per-ex loss: 0.728915  [   38/   88]
per-ex loss: 0.638683  [   39/   88]
per-ex loss: 0.630580  [   40/   88]
per-ex loss: 0.653715  [   41/   88]
per-ex loss: 0.589376  [   42/   88]
per-ex loss: 0.675254  [   43/   88]
per-ex loss: 0.585512  [   44/   88]
per-ex loss: 0.768578  [   45/   88]
per-ex loss: 0.763489  [   46/   88]
per-ex loss: 0.555700  [   47/   88]
per-ex loss: 0.737301  [   48/   88]
per-ex loss: 0.699185  [   49/   88]
per-ex loss: 0.820416  [   50/   88]
per-ex loss: 0.763682  [   51/   88]
per-ex loss: 0.594846  [   52/   88]
per-ex loss: 0.751354  [   53/   88]
per-ex loss: 0.752662  [   54/   88]
per-ex loss: 0.567920  [   55/   88]
per-ex loss: 0.595422  [   56/   88]
per-ex loss: 0.748611  [   57/   88]
per-ex loss: 0.598139  [   58/   88]
per-ex loss: 0.778532  [   59/   88]
per-ex loss: 0.729435  [   60/   88]
per-ex loss: 0.733370  [   61/   88]
per-ex loss: 0.719264  [   62/   88]
per-ex loss: 0.664293  [   63/   88]
per-ex loss: 0.595298  [   64/   88]
per-ex loss: 0.630562  [   65/   88]
per-ex loss: 0.666220  [   66/   88]
per-ex loss: 0.610400  [   67/   88]
per-ex loss: 0.800934  [   68/   88]
per-ex loss: 0.819778  [   69/   88]
per-ex loss: 0.533946  [   70/   88]
per-ex loss: 0.658594  [   71/   88]
per-ex loss: 0.732830  [   72/   88]
per-ex loss: 0.737255  [   73/   88]
per-ex loss: 0.789148  [   74/   88]
per-ex loss: 0.698533  [   75/   88]
per-ex loss: 0.655077  [   76/   88]
per-ex loss: 0.538052  [   77/   88]
per-ex loss: 0.574119  [   78/   88]
per-ex loss: 0.771382  [   79/   88]
per-ex loss: 0.652946  [   80/   88]
per-ex loss: 0.613052  [   81/   88]
per-ex loss: 0.570890  [   82/   88]
per-ex loss: 0.651611  [   83/   88]
per-ex loss: 0.746102  [   84/   88]
per-ex loss: 0.801676  [   85/   88]
per-ex loss: 0.759259  [   86/   88]
per-ex loss: 0.656252  [   87/   88]
per-ex loss: 0.602945  [   88/   88]
Train Error: Avg loss: 0.68691233
validation Error: 
 Avg loss: 0.71432472 
 F1: 0.466577 
 Precision: 0.589140 
 Recall: 0.386228
 IoU: 0.304272

test Error: 
 Avg loss: 0.66518310 
 F1: 0.547651 
 Precision: 0.609119 
 Recall: 0.497452
 IoU: 0.377079

We have finished training iteration 15
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_13_.pth
per-ex loss: 0.611714  [    1/   88]
per-ex loss: 0.613394  [    2/   88]
per-ex loss: 0.847762  [    3/   88]
per-ex loss: 0.633691  [    4/   88]
per-ex loss: 0.623640  [    5/   88]
per-ex loss: 0.764721  [    6/   88]
per-ex loss: 0.857409  [    7/   88]
per-ex loss: 0.569564  [    8/   88]
per-ex loss: 0.727834  [    9/   88]
per-ex loss: 0.739216  [   10/   88]
per-ex loss: 0.754424  [   11/   88]
per-ex loss: 0.644348  [   12/   88]
per-ex loss: 0.735417  [   13/   88]
per-ex loss: 0.564870  [   14/   88]
per-ex loss: 0.788619  [   15/   88]
per-ex loss: 0.827004  [   16/   88]
per-ex loss: 0.778139  [   17/   88]
per-ex loss: 0.786494  [   18/   88]
per-ex loss: 0.734547  [   19/   88]
per-ex loss: 0.722429  [   20/   88]
per-ex loss: 0.777897  [   21/   88]
per-ex loss: 0.820434  [   22/   88]
per-ex loss: 0.539547  [   23/   88]
per-ex loss: 0.854928  [   24/   88]
per-ex loss: 0.660635  [   25/   88]
per-ex loss: 0.706067  [   26/   88]
per-ex loss: 0.806259  [   27/   88]
per-ex loss: 0.589228  [   28/   88]
per-ex loss: 0.639989  [   29/   88]
per-ex loss: 0.638397  [   30/   88]
per-ex loss: 0.722667  [   31/   88]
per-ex loss: 0.547236  [   32/   88]
per-ex loss: 0.771760  [   33/   88]
per-ex loss: 0.691975  [   34/   88]
per-ex loss: 0.815951  [   35/   88]
per-ex loss: 0.825662  [   36/   88]
per-ex loss: 0.548292  [   37/   88]
per-ex loss: 0.779367  [   38/   88]
per-ex loss: 0.769903  [   39/   88]
per-ex loss: 0.635574  [   40/   88]
per-ex loss: 0.588662  [   41/   88]
per-ex loss: 0.815154  [   42/   88]
per-ex loss: 0.584332  [   43/   88]
per-ex loss: 0.515631  [   44/   88]
per-ex loss: 0.568922  [   45/   88]
per-ex loss: 0.804374  [   46/   88]
per-ex loss: 0.651515  [   47/   88]
per-ex loss: 0.727685  [   48/   88]
per-ex loss: 0.646452  [   49/   88]
per-ex loss: 0.619989  [   50/   88]
per-ex loss: 0.579350  [   51/   88]
per-ex loss: 0.657203  [   52/   88]
per-ex loss: 0.618314  [   53/   88]
per-ex loss: 0.657036  [   54/   88]
per-ex loss: 0.774847  [   55/   88]
per-ex loss: 0.639042  [   56/   88]
per-ex loss: 0.739363  [   57/   88]
per-ex loss: 0.803870  [   58/   88]
per-ex loss: 0.652801  [   59/   88]
per-ex loss: 0.708761  [   60/   88]
per-ex loss: 0.643166  [   61/   88]
per-ex loss: 0.737680  [   62/   88]
per-ex loss: 0.631916  [   63/   88]
per-ex loss: 0.573844  [   64/   88]
per-ex loss: 0.590107  [   65/   88]
per-ex loss: 0.602113  [   66/   88]
per-ex loss: 0.657445  [   67/   88]
per-ex loss: 0.740087  [   68/   88]
per-ex loss: 0.649717  [   69/   88]
per-ex loss: 0.731808  [   70/   88]
per-ex loss: 0.750167  [   71/   88]
per-ex loss: 0.583054  [   72/   88]
per-ex loss: 0.547111  [   73/   88]
per-ex loss: 0.509272  [   74/   88]
per-ex loss: 0.772372  [   75/   88]
per-ex loss: 0.800216  [   76/   88]
per-ex loss: 0.824232  [   77/   88]
per-ex loss: 0.624516  [   78/   88]
per-ex loss: 0.807125  [   79/   88]
per-ex loss: 0.730669  [   80/   88]
per-ex loss: 0.600412  [   81/   88]
per-ex loss: 0.596530  [   82/   88]
per-ex loss: 0.608819  [   83/   88]
per-ex loss: 0.533050  [   84/   88]
per-ex loss: 0.606390  [   85/   88]
per-ex loss: 0.627508  [   86/   88]
per-ex loss: 0.781892  [   87/   88]
per-ex loss: 0.586989  [   88/   88]
Train Error: Avg loss: 0.68486952
validation Error: 
 Avg loss: 0.71419426 
 F1: 0.466655 
 Precision: 0.593350 
 Recall: 0.384545
 IoU: 0.304338

test Error: 
 Avg loss: 0.67031976 
 F1: 0.543420 
 Precision: 0.605392 
 Recall: 0.492957
 IoU: 0.373079

We have finished training iteration 16
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_14_.pth
per-ex loss: 0.569337  [    1/   88]
per-ex loss: 0.759712  [    2/   88]
per-ex loss: 0.612210  [    3/   88]
per-ex loss: 0.742556  [    4/   88]
per-ex loss: 0.640223  [    5/   88]
per-ex loss: 0.610165  [    6/   88]
per-ex loss: 0.752493  [    7/   88]
per-ex loss: 0.788486  [    8/   88]
per-ex loss: 0.721160  [    9/   88]
per-ex loss: 0.803383  [   10/   88]
per-ex loss: 0.784639  [   11/   88]
per-ex loss: 0.544112  [   12/   88]
per-ex loss: 0.792075  [   13/   88]
per-ex loss: 0.722817  [   14/   88]
per-ex loss: 0.859716  [   15/   88]
per-ex loss: 0.580865  [   16/   88]
per-ex loss: 0.618789  [   17/   88]
per-ex loss: 0.587387  [   18/   88]
per-ex loss: 0.634197  [   19/   88]
per-ex loss: 0.743634  [   20/   88]
per-ex loss: 0.556833  [   21/   88]
per-ex loss: 0.592806  [   22/   88]
per-ex loss: 0.632611  [   23/   88]
per-ex loss: 0.653355  [   24/   88]
per-ex loss: 0.838637  [   25/   88]
per-ex loss: 0.834928  [   26/   88]
per-ex loss: 0.802702  [   27/   88]
per-ex loss: 0.811951  [   28/   88]
per-ex loss: 0.814636  [   29/   88]
per-ex loss: 0.637657  [   30/   88]
per-ex loss: 0.599365  [   31/   88]
per-ex loss: 0.574579  [   32/   88]
per-ex loss: 0.763059  [   33/   88]
per-ex loss: 0.597192  [   34/   88]
per-ex loss: 0.820459  [   35/   88]
per-ex loss: 0.751905  [   36/   88]
per-ex loss: 0.605028  [   37/   88]
per-ex loss: 0.706475  [   38/   88]
per-ex loss: 0.820200  [   39/   88]
per-ex loss: 0.714459  [   40/   88]
per-ex loss: 0.743332  [   41/   88]
per-ex loss: 0.720564  [   42/   88]
per-ex loss: 0.810419  [   43/   88]
per-ex loss: 0.521389  [   44/   88]
per-ex loss: 0.758923  [   45/   88]
per-ex loss: 0.639620  [   46/   88]
per-ex loss: 0.647432  [   47/   88]
per-ex loss: 0.574312  [   48/   88]
per-ex loss: 0.660756  [   49/   88]
per-ex loss: 0.667237  [   50/   88]
per-ex loss: 0.673303  [   51/   88]
per-ex loss: 0.598281  [   52/   88]
per-ex loss: 0.531215  [   53/   88]
per-ex loss: 0.677332  [   54/   88]
per-ex loss: 0.728733  [   55/   88]
per-ex loss: 0.737801  [   56/   88]
per-ex loss: 0.797924  [   57/   88]
per-ex loss: 0.591256  [   58/   88]
per-ex loss: 0.620198  [   59/   88]
per-ex loss: 0.548259  [   60/   88]
per-ex loss: 0.728202  [   61/   88]
per-ex loss: 0.637390  [   62/   88]
per-ex loss: 0.744879  [   63/   88]
per-ex loss: 0.595558  [   64/   88]
per-ex loss: 0.667978  [   65/   88]
per-ex loss: 0.817683  [   66/   88]
per-ex loss: 0.605407  [   67/   88]
per-ex loss: 0.640520  [   68/   88]
per-ex loss: 0.644485  [   69/   88]
per-ex loss: 0.782794  [   70/   88]
per-ex loss: 0.635819  [   71/   88]
per-ex loss: 0.737829  [   72/   88]
per-ex loss: 0.713198  [   73/   88]
per-ex loss: 0.569185  [   74/   88]
per-ex loss: 0.635752  [   75/   88]
per-ex loss: 0.821737  [   76/   88]
per-ex loss: 0.519649  [   77/   88]
per-ex loss: 0.577495  [   78/   88]
per-ex loss: 0.749970  [   79/   88]
per-ex loss: 0.665243  [   80/   88]
per-ex loss: 0.605430  [   81/   88]
per-ex loss: 0.704889  [   82/   88]
per-ex loss: 0.737729  [   83/   88]
per-ex loss: 0.604766  [   84/   88]
per-ex loss: 0.662780  [   85/   88]
per-ex loss: 0.808915  [   86/   88]
per-ex loss: 0.775599  [   87/   88]
per-ex loss: 0.582505  [   88/   88]
Train Error: Avg loss: 0.68430078
validation Error: 
 Avg loss: 0.71269849 
 F1: 0.472112 
 Precision: 0.568374 
 Recall: 0.403734
 IoU: 0.308996

test Error: 
 Avg loss: 0.66534406 
 F1: 0.549149 
 Precision: 0.583320 
 Recall: 0.518759
 IoU: 0.378501

We have finished training iteration 17
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_11_.pth
per-ex loss: 0.749020  [    1/   88]
per-ex loss: 0.630072  [    2/   88]
per-ex loss: 0.820665  [    3/   88]
per-ex loss: 0.520440  [    4/   88]
per-ex loss: 0.589474  [    5/   88]
per-ex loss: 0.716604  [    6/   88]
per-ex loss: 0.652055  [    7/   88]
per-ex loss: 0.676491  [    8/   88]
per-ex loss: 0.604392  [    9/   88]
per-ex loss: 0.794296  [   10/   88]
per-ex loss: 0.596073  [   11/   88]
per-ex loss: 0.809172  [   12/   88]
per-ex loss: 0.602032  [   13/   88]
per-ex loss: 0.588290  [   14/   88]
per-ex loss: 0.758138  [   15/   88]
per-ex loss: 0.666591  [   16/   88]
per-ex loss: 0.850933  [   17/   88]
per-ex loss: 0.785117  [   18/   88]
per-ex loss: 0.573306  [   19/   88]
per-ex loss: 0.747973  [   20/   88]
per-ex loss: 0.626491  [   21/   88]
per-ex loss: 0.605607  [   22/   88]
per-ex loss: 0.809309  [   23/   88]
per-ex loss: 0.758008  [   24/   88]
per-ex loss: 0.648270  [   25/   88]
per-ex loss: 0.550142  [   26/   88]
per-ex loss: 0.635362  [   27/   88]
per-ex loss: 0.613195  [   28/   88]
per-ex loss: 0.589202  [   29/   88]
per-ex loss: 0.655854  [   30/   88]
per-ex loss: 0.735973  [   31/   88]
per-ex loss: 0.724594  [   32/   88]
per-ex loss: 0.835679  [   33/   88]
per-ex loss: 0.571329  [   34/   88]
per-ex loss: 0.800832  [   35/   88]
per-ex loss: 0.735222  [   36/   88]
per-ex loss: 0.717767  [   37/   88]
per-ex loss: 0.543335  [   38/   88]
per-ex loss: 0.744744  [   39/   88]
per-ex loss: 0.845143  [   40/   88]
per-ex loss: 0.799585  [   41/   88]
per-ex loss: 0.713243  [   42/   88]
per-ex loss: 0.795422  [   43/   88]
per-ex loss: 0.584890  [   44/   88]
per-ex loss: 0.703042  [   45/   88]
per-ex loss: 0.615913  [   46/   88]
per-ex loss: 0.606840  [   47/   88]
per-ex loss: 0.646269  [   48/   88]
per-ex loss: 0.653101  [   49/   88]
per-ex loss: 0.729022  [   50/   88]
per-ex loss: 0.737570  [   51/   88]
per-ex loss: 0.596929  [   52/   88]
per-ex loss: 0.793988  [   53/   88]
per-ex loss: 0.662360  [   54/   88]
per-ex loss: 0.779332  [   55/   88]
per-ex loss: 0.655224  [   56/   88]
per-ex loss: 0.756532  [   57/   88]
per-ex loss: 0.810209  [   58/   88]
per-ex loss: 0.554210  [   59/   88]
per-ex loss: 0.721301  [   60/   88]
per-ex loss: 0.785157  [   61/   88]
per-ex loss: 0.763659  [   62/   88]
per-ex loss: 0.733852  [   63/   88]
per-ex loss: 0.830413  [   64/   88]
per-ex loss: 0.567885  [   65/   88]
per-ex loss: 0.511152  [   66/   88]
per-ex loss: 0.526573  [   67/   88]
per-ex loss: 0.811502  [   68/   88]
per-ex loss: 0.533288  [   69/   88]
per-ex loss: 0.770231  [   70/   88]
per-ex loss: 0.599150  [   71/   88]
per-ex loss: 0.621555  [   72/   88]
per-ex loss: 0.566901  [   73/   88]
per-ex loss: 0.554051  [   74/   88]
per-ex loss: 0.563215  [   75/   88]
per-ex loss: 0.791129  [   76/   88]
per-ex loss: 0.740290  [   77/   88]
per-ex loss: 0.583080  [   78/   88]
per-ex loss: 0.643591  [   79/   88]
per-ex loss: 0.611823  [   80/   88]
per-ex loss: 0.656015  [   81/   88]
per-ex loss: 0.679528  [   82/   88]
per-ex loss: 0.636425  [   83/   88]
per-ex loss: 0.616221  [   84/   88]
per-ex loss: 0.621587  [   85/   88]
per-ex loss: 0.584105  [   86/   88]
per-ex loss: 0.814347  [   87/   88]
per-ex loss: 0.628032  [   88/   88]
Train Error: Avg loss: 0.67882877
validation Error: 
 Avg loss: 0.71855418 
 F1: 0.459363 
 Precision: 0.574623 
 Recall: 0.382617
 IoU: 0.298164

test Error: 
 Avg loss: 0.66890688 
 F1: 0.539837 
 Precision: 0.620392 
 Recall: 0.477797
 IoU: 0.369710

We have finished training iteration 18
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_9_.pth
per-ex loss: 0.633424  [    1/   88]
per-ex loss: 0.519341  [    2/   88]
per-ex loss: 0.628461  [    3/   88]
per-ex loss: 0.750359  [    4/   88]
per-ex loss: 0.760002  [    5/   88]
per-ex loss: 0.778154  [    6/   88]
per-ex loss: 0.605814  [    7/   88]
per-ex loss: 0.765322  [    8/   88]
per-ex loss: 0.586256  [    9/   88]
per-ex loss: 0.722743  [   10/   88]
per-ex loss: 0.840830  [   11/   88]
per-ex loss: 0.638657  [   12/   88]
per-ex loss: 0.547449  [   13/   88]
per-ex loss: 0.643418  [   14/   88]
per-ex loss: 0.760380  [   15/   88]
per-ex loss: 0.807086  [   16/   88]
per-ex loss: 0.597421  [   17/   88]
per-ex loss: 0.735443  [   18/   88]
per-ex loss: 0.794282  [   19/   88]
per-ex loss: 0.723976  [   20/   88]
per-ex loss: 0.553787  [   21/   88]
per-ex loss: 0.697752  [   22/   88]
per-ex loss: 0.774743  [   23/   88]
per-ex loss: 0.816598  [   24/   88]
per-ex loss: 0.662819  [   25/   88]
per-ex loss: 0.586806  [   26/   88]
per-ex loss: 0.687278  [   27/   88]
per-ex loss: 0.654265  [   28/   88]
per-ex loss: 0.645074  [   29/   88]
per-ex loss: 0.632662  [   30/   88]
per-ex loss: 0.597154  [   31/   88]
per-ex loss: 0.614801  [   32/   88]
per-ex loss: 0.802135  [   33/   88]
per-ex loss: 0.697973  [   34/   88]
per-ex loss: 0.581304  [   35/   88]
per-ex loss: 0.640939  [   36/   88]
per-ex loss: 0.611589  [   37/   88]
per-ex loss: 0.745561  [   38/   88]
per-ex loss: 0.760089  [   39/   88]
per-ex loss: 0.580753  [   40/   88]
per-ex loss: 0.749091  [   41/   88]
per-ex loss: 0.610428  [   42/   88]
per-ex loss: 0.554699  [   43/   88]
per-ex loss: 0.817375  [   44/   88]
per-ex loss: 0.577495  [   45/   88]
per-ex loss: 0.614162  [   46/   88]
per-ex loss: 0.699355  [   47/   88]
per-ex loss: 0.727548  [   48/   88]
per-ex loss: 0.766520  [   49/   88]
per-ex loss: 0.832525  [   50/   88]
per-ex loss: 0.730022  [   51/   88]
per-ex loss: 0.784075  [   52/   88]
per-ex loss: 0.629837  [   53/   88]
per-ex loss: 0.569935  [   54/   88]
per-ex loss: 0.568592  [   55/   88]
per-ex loss: 0.718703  [   56/   88]
per-ex loss: 0.640709  [   57/   88]
per-ex loss: 0.762727  [   58/   88]
per-ex loss: 0.832243  [   59/   88]
per-ex loss: 0.627873  [   60/   88]
per-ex loss: 0.622089  [   61/   88]
per-ex loss: 0.824618  [   62/   88]
per-ex loss: 0.816387  [   63/   88]
per-ex loss: 0.665442  [   64/   88]
per-ex loss: 0.728591  [   65/   88]
per-ex loss: 0.603833  [   66/   88]
per-ex loss: 0.561402  [   67/   88]
per-ex loss: 0.748213  [   68/   88]
per-ex loss: 0.543513  [   69/   88]
per-ex loss: 0.559988  [   70/   88]
per-ex loss: 0.813412  [   71/   88]
per-ex loss: 0.747232  [   72/   88]
per-ex loss: 0.652979  [   73/   88]
per-ex loss: 0.801915  [   74/   88]
per-ex loss: 0.644982  [   75/   88]
per-ex loss: 0.552484  [   76/   88]
per-ex loss: 0.807335  [   77/   88]
per-ex loss: 0.521034  [   78/   88]
per-ex loss: 0.641556  [   79/   88]
per-ex loss: 0.709073  [   80/   88]
per-ex loss: 0.753029  [   81/   88]
per-ex loss: 0.754393  [   82/   88]
per-ex loss: 0.622810  [   83/   88]
per-ex loss: 0.634005  [   84/   88]
per-ex loss: 0.718116  [   85/   88]
per-ex loss: 0.565486  [   86/   88]
per-ex loss: 0.597049  [   87/   88]
per-ex loss: 0.566530  [   88/   88]
Train Error: Avg loss: 0.67925351
validation Error: 
 Avg loss: 0.71400347 
 F1: 0.462986 
 Precision: 0.562244 
 Recall: 0.393516
 IoU: 0.301225

test Error: 
 Avg loss: 0.66555296 
 F1: 0.547584 
 Precision: 0.613690 
 Recall: 0.494336
 IoU: 0.377016

We have finished training iteration 19
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_15_.pth
per-ex loss: 0.812400  [    1/   88]
per-ex loss: 0.573602  [    2/   88]
per-ex loss: 0.546635  [    3/   88]
per-ex loss: 0.612907  [    4/   88]
per-ex loss: 0.643545  [    5/   88]
per-ex loss: 0.560970  [    6/   88]
per-ex loss: 0.608154  [    7/   88]
per-ex loss: 0.575726  [    8/   88]
per-ex loss: 0.642447  [    9/   88]
per-ex loss: 0.771011  [   10/   88]
per-ex loss: 0.652669  [   11/   88]
per-ex loss: 0.846566  [   12/   88]
per-ex loss: 0.650542  [   13/   88]
per-ex loss: 0.733048  [   14/   88]
per-ex loss: 0.526631  [   15/   88]
per-ex loss: 0.637896  [   16/   88]
per-ex loss: 0.592124  [   17/   88]
per-ex loss: 0.886274  [   18/   88]
per-ex loss: 0.543423  [   19/   88]
per-ex loss: 0.712841  [   20/   88]
per-ex loss: 0.796340  [   21/   88]
per-ex loss: 0.552154  [   22/   88]
per-ex loss: 0.817126  [   23/   88]
per-ex loss: 0.754832  [   24/   88]
per-ex loss: 0.653291  [   25/   88]
per-ex loss: 0.814247  [   26/   88]
per-ex loss: 0.640009  [   27/   88]
per-ex loss: 0.724837  [   28/   88]
per-ex loss: 0.538079  [   29/   88]
per-ex loss: 0.790871  [   30/   88]
per-ex loss: 0.598531  [   31/   88]
per-ex loss: 0.789767  [   32/   88]
per-ex loss: 0.751950  [   33/   88]
per-ex loss: 0.785042  [   34/   88]
per-ex loss: 0.626827  [   35/   88]
per-ex loss: 0.592599  [   36/   88]
per-ex loss: 0.614406  [   37/   88]
per-ex loss: 0.608628  [   38/   88]
per-ex loss: 0.869281  [   39/   88]
per-ex loss: 0.546434  [   40/   88]
per-ex loss: 0.624443  [   41/   88]
per-ex loss: 0.740999  [   42/   88]
per-ex loss: 0.627989  [   43/   88]
per-ex loss: 0.846025  [   44/   88]
per-ex loss: 0.624490  [   45/   88]
per-ex loss: 0.588263  [   46/   88]
per-ex loss: 0.634632  [   47/   88]
per-ex loss: 0.513229  [   48/   88]
per-ex loss: 0.818612  [   49/   88]
per-ex loss: 0.755465  [   50/   88]
per-ex loss: 0.763127  [   51/   88]
per-ex loss: 0.624687  [   52/   88]
per-ex loss: 0.764524  [   53/   88]
per-ex loss: 0.713147  [   54/   88]
per-ex loss: 0.780394  [   55/   88]
per-ex loss: 0.727814  [   56/   88]
per-ex loss: 0.726571  [   57/   88]
per-ex loss: 0.782939  [   58/   88]
per-ex loss: 0.594646  [   59/   88]
per-ex loss: 0.664011  [   60/   88]
per-ex loss: 0.617543  [   61/   88]
per-ex loss: 0.608834  [   62/   88]
per-ex loss: 0.627626  [   63/   88]
per-ex loss: 0.708791  [   64/   88]
per-ex loss: 0.556080  [   65/   88]
per-ex loss: 0.703137  [   66/   88]
per-ex loss: 0.725528  [   67/   88]
per-ex loss: 0.822238  [   68/   88]
per-ex loss: 0.730722  [   69/   88]
per-ex loss: 0.547509  [   70/   88]
per-ex loss: 0.777806  [   71/   88]
per-ex loss: 0.804235  [   72/   88]
per-ex loss: 0.804707  [   73/   88]
per-ex loss: 0.751296  [   74/   88]
per-ex loss: 0.605207  [   75/   88]
per-ex loss: 0.567869  [   76/   88]
per-ex loss: 0.720111  [   77/   88]
per-ex loss: 0.517888  [   78/   88]
per-ex loss: 0.772792  [   79/   88]
per-ex loss: 0.511310  [   80/   88]
per-ex loss: 0.780222  [   81/   88]
per-ex loss: 0.584084  [   82/   88]
per-ex loss: 0.709390  [   83/   88]
per-ex loss: 0.652994  [   84/   88]
per-ex loss: 0.574988  [   85/   88]
per-ex loss: 0.648192  [   86/   88]
per-ex loss: 0.599803  [   87/   88]
per-ex loss: 0.726367  [   88/   88]
Train Error: Avg loss: 0.67804476
validation Error: 
 Avg loss: 0.71833556 
 F1: 0.456135 
 Precision: 0.518962 
 Recall: 0.406877
 IoU: 0.295450

test Error: 
 Avg loss: 0.66502989 
 F1: 0.546225 
 Precision: 0.587201 
 Recall: 0.510595
 IoU: 0.375729

We have finished training iteration 20
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_18_.pth
per-ex loss: 0.535800  [    1/   88]
per-ex loss: 0.731254  [    2/   88]
per-ex loss: 0.629158  [    3/   88]
per-ex loss: 0.563890  [    4/   88]
per-ex loss: 0.640924  [    5/   88]
per-ex loss: 0.467788  [    6/   88]
per-ex loss: 0.744188  [    7/   88]
per-ex loss: 0.817345  [    8/   88]
per-ex loss: 0.644393  [    9/   88]
per-ex loss: 0.729516  [   10/   88]
per-ex loss: 0.803591  [   11/   88]
per-ex loss: 0.610535  [   12/   88]
per-ex loss: 0.712345  [   13/   88]
per-ex loss: 0.801690  [   14/   88]
per-ex loss: 0.617906  [   15/   88]
per-ex loss: 0.666271  [   16/   88]
per-ex loss: 0.749227  [   17/   88]
per-ex loss: 0.773941  [   18/   88]
per-ex loss: 0.736716  [   19/   88]
per-ex loss: 0.632754  [   20/   88]
per-ex loss: 0.704537  [   21/   88]
per-ex loss: 0.621383  [   22/   88]
per-ex loss: 0.615646  [   23/   88]
per-ex loss: 0.607389  [   24/   88]
per-ex loss: 0.701576  [   25/   88]
per-ex loss: 0.595754  [   26/   88]
per-ex loss: 0.706927  [   27/   88]
per-ex loss: 0.616986  [   28/   88]
per-ex loss: 0.744690  [   29/   88]
per-ex loss: 0.622977  [   30/   88]
per-ex loss: 0.776314  [   31/   88]
per-ex loss: 0.742168  [   32/   88]
per-ex loss: 0.751839  [   33/   88]
per-ex loss: 0.571651  [   34/   88]
per-ex loss: 0.632106  [   35/   88]
per-ex loss: 0.748557  [   36/   88]
per-ex loss: 0.769849  [   37/   88]
per-ex loss: 0.728561  [   38/   88]
per-ex loss: 0.818541  [   39/   88]
per-ex loss: 0.561093  [   40/   88]
per-ex loss: 0.711400  [   41/   88]
per-ex loss: 0.666971  [   42/   88]
per-ex loss: 0.780156  [   43/   88]
per-ex loss: 0.613674  [   44/   88]
per-ex loss: 0.612206  [   45/   88]
per-ex loss: 0.566182  [   46/   88]
per-ex loss: 0.787403  [   47/   88]
per-ex loss: 0.517344  [   48/   88]
per-ex loss: 0.707227  [   49/   88]
per-ex loss: 0.713652  [   50/   88]
per-ex loss: 0.827842  [   51/   88]
per-ex loss: 0.793261  [   52/   88]
per-ex loss: 0.568138  [   53/   88]
per-ex loss: 0.574378  [   54/   88]
per-ex loss: 0.803049  [   55/   88]
per-ex loss: 0.634749  [   56/   88]
per-ex loss: 0.722607  [   57/   88]
per-ex loss: 0.651680  [   58/   88]
per-ex loss: 0.496869  [   59/   88]
per-ex loss: 0.758491  [   60/   88]
per-ex loss: 0.705017  [   61/   88]
per-ex loss: 0.612405  [   62/   88]
per-ex loss: 0.801136  [   63/   88]
per-ex loss: 0.826061  [   64/   88]
per-ex loss: 0.633097  [   65/   88]
per-ex loss: 0.611421  [   66/   88]
per-ex loss: 0.789426  [   67/   88]
per-ex loss: 0.810732  [   68/   88]
per-ex loss: 0.741483  [   69/   88]
per-ex loss: 0.577011  [   70/   88]
per-ex loss: 0.520914  [   71/   88]
per-ex loss: 0.624980  [   72/   88]
per-ex loss: 0.584675  [   73/   88]
per-ex loss: 0.553261  [   74/   88]
per-ex loss: 0.643731  [   75/   88]
per-ex loss: 0.821784  [   76/   88]
per-ex loss: 0.633190  [   77/   88]
per-ex loss: 0.560003  [   78/   88]
per-ex loss: 0.812286  [   79/   88]
per-ex loss: 0.666749  [   80/   88]
per-ex loss: 0.521810  [   81/   88]
per-ex loss: 0.522007  [   82/   88]
per-ex loss: 0.779184  [   83/   88]
per-ex loss: 0.618872  [   84/   88]
per-ex loss: 0.555440  [   85/   88]
per-ex loss: 0.669491  [   86/   88]
per-ex loss: 0.798251  [   87/   88]
per-ex loss: 0.604545  [   88/   88]
Train Error: Avg loss: 0.67447745
validation Error: 
 Avg loss: 0.70431306 
 F1: 0.474091 
 Precision: 0.574483 
 Recall: 0.403568
 IoU: 0.310695

test Error: 
 Avg loss: 0.66315527 
 F1: 0.545712 
 Precision: 0.620212 
 Recall: 0.487190
 IoU: 0.375243

We have finished training iteration 21
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_16_.pth
per-ex loss: 0.582266  [    1/   88]
per-ex loss: 0.707647  [    2/   88]
per-ex loss: 0.571758  [    3/   88]
per-ex loss: 0.615000  [    4/   88]
per-ex loss: 0.782735  [    5/   88]
per-ex loss: 0.835992  [    6/   88]
per-ex loss: 0.627942  [    7/   88]
per-ex loss: 0.655765  [    8/   88]
per-ex loss: 0.721485  [    9/   88]
per-ex loss: 0.783937  [   10/   88]
per-ex loss: 0.627420  [   11/   88]
per-ex loss: 0.730668  [   12/   88]
per-ex loss: 0.633289  [   13/   88]
per-ex loss: 0.595557  [   14/   88]
per-ex loss: 0.722116  [   15/   88]
per-ex loss: 0.594867  [   16/   88]
per-ex loss: 0.800494  [   17/   88]
per-ex loss: 0.610204  [   18/   88]
per-ex loss: 0.713509  [   19/   88]
per-ex loss: 0.828523  [   20/   88]
per-ex loss: 0.811710  [   21/   88]
per-ex loss: 0.565756  [   22/   88]
per-ex loss: 0.581945  [   23/   88]
per-ex loss: 0.592313  [   24/   88]
per-ex loss: 0.743308  [   25/   88]
per-ex loss: 0.774361  [   26/   88]
per-ex loss: 0.581135  [   27/   88]
per-ex loss: 0.506284  [   28/   88]
per-ex loss: 0.609012  [   29/   88]
per-ex loss: 0.589136  [   30/   88]
per-ex loss: 0.624723  [   31/   88]
per-ex loss: 0.739570  [   32/   88]
per-ex loss: 0.578351  [   33/   88]
per-ex loss: 0.539445  [   34/   88]
per-ex loss: 0.765691  [   35/   88]
per-ex loss: 0.715379  [   36/   88]
per-ex loss: 0.718567  [   37/   88]
per-ex loss: 0.527764  [   38/   88]
per-ex loss: 0.558802  [   39/   88]
per-ex loss: 0.612795  [   40/   88]
per-ex loss: 0.836979  [   41/   88]
per-ex loss: 0.835045  [   42/   88]
per-ex loss: 0.639609  [   43/   88]
per-ex loss: 0.640884  [   44/   88]
per-ex loss: 0.778548  [   45/   88]
per-ex loss: 0.644838  [   46/   88]
per-ex loss: 0.789650  [   47/   88]
per-ex loss: 0.635472  [   48/   88]
per-ex loss: 0.700173  [   49/   88]
per-ex loss: 0.554318  [   50/   88]
per-ex loss: 0.735140  [   51/   88]
per-ex loss: 0.620324  [   52/   88]
per-ex loss: 0.678264  [   53/   88]
per-ex loss: 0.539282  [   54/   88]
per-ex loss: 0.579079  [   55/   88]
per-ex loss: 0.652991  [   56/   88]
per-ex loss: 0.721427  [   57/   88]
per-ex loss: 0.792000  [   58/   88]
per-ex loss: 0.582737  [   59/   88]
per-ex loss: 0.809831  [   60/   88]
per-ex loss: 0.781811  [   61/   88]
per-ex loss: 0.606324  [   62/   88]
per-ex loss: 0.754127  [   63/   88]
per-ex loss: 0.715261  [   64/   88]
per-ex loss: 0.796504  [   65/   88]
per-ex loss: 0.606055  [   66/   88]
per-ex loss: 0.566005  [   67/   88]
per-ex loss: 0.754990  [   68/   88]
per-ex loss: 0.542061  [   69/   88]
per-ex loss: 0.705944  [   70/   88]
per-ex loss: 0.741887  [   71/   88]
per-ex loss: 0.616866  [   72/   88]
per-ex loss: 0.724803  [   73/   88]
per-ex loss: 0.606718  [   74/   88]
per-ex loss: 0.571375  [   75/   88]
per-ex loss: 0.680083  [   76/   88]
per-ex loss: 0.766321  [   77/   88]
per-ex loss: 0.557227  [   78/   88]
per-ex loss: 0.529503  [   79/   88]
per-ex loss: 0.782300  [   80/   88]
per-ex loss: 0.740978  [   81/   88]
per-ex loss: 0.529631  [   82/   88]
per-ex loss: 0.840274  [   83/   88]
per-ex loss: 0.772189  [   84/   88]
per-ex loss: 0.728334  [   85/   88]
per-ex loss: 0.615054  [   86/   88]
per-ex loss: 0.740151  [   87/   88]
per-ex loss: 0.625849  [   88/   88]
Train Error: Avg loss: 0.67320955
validation Error: 
 Avg loss: 0.71348359 
 F1: 0.465715 
 Precision: 0.533972 
 Recall: 0.412931
 IoU: 0.303539

test Error: 
 Avg loss: 0.65985262 
 F1: 0.554041 
 Precision: 0.590647 
 Recall: 0.521708
 IoU: 0.383165

We have finished training iteration 22
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_20_.pth
per-ex loss: 0.705419  [    1/   88]
per-ex loss: 0.718855  [    2/   88]
per-ex loss: 0.721313  [    3/   88]
per-ex loss: 0.765415  [    4/   88]
per-ex loss: 0.723254  [    5/   88]
per-ex loss: 0.730337  [    6/   88]
per-ex loss: 0.594120  [    7/   88]
per-ex loss: 0.603413  [    8/   88]
per-ex loss: 0.821058  [    9/   88]
per-ex loss: 0.718582  [   10/   88]
per-ex loss: 0.570349  [   11/   88]
per-ex loss: 0.686202  [   12/   88]
per-ex loss: 0.629646  [   13/   88]
per-ex loss: 0.577456  [   14/   88]
per-ex loss: 0.718691  [   15/   88]
per-ex loss: 0.589445  [   16/   88]
per-ex loss: 0.750410  [   17/   88]
per-ex loss: 0.531132  [   18/   88]
per-ex loss: 0.512467  [   19/   88]
per-ex loss: 0.563165  [   20/   88]
per-ex loss: 0.832701  [   21/   88]
per-ex loss: 0.759166  [   22/   88]
per-ex loss: 0.654545  [   23/   88]
per-ex loss: 0.525826  [   24/   88]
per-ex loss: 0.663771  [   25/   88]
per-ex loss: 0.531545  [   26/   88]
per-ex loss: 0.599258  [   27/   88]
per-ex loss: 0.537106  [   28/   88]
per-ex loss: 0.759082  [   29/   88]
per-ex loss: 0.789798  [   30/   88]
per-ex loss: 0.632303  [   31/   88]
per-ex loss: 0.539857  [   32/   88]
per-ex loss: 0.572536  [   33/   88]
per-ex loss: 0.755349  [   34/   88]
per-ex loss: 0.738751  [   35/   88]
per-ex loss: 0.768744  [   36/   88]
per-ex loss: 0.784155  [   37/   88]
per-ex loss: 0.711105  [   38/   88]
per-ex loss: 0.826169  [   39/   88]
per-ex loss: 0.580215  [   40/   88]
per-ex loss: 0.633814  [   41/   88]
per-ex loss: 0.780954  [   42/   88]
per-ex loss: 0.593150  [   43/   88]
per-ex loss: 0.644444  [   44/   88]
per-ex loss: 0.653272  [   45/   88]
per-ex loss: 0.574440  [   46/   88]
per-ex loss: 0.794487  [   47/   88]
per-ex loss: 0.566925  [   48/   88]
per-ex loss: 0.836037  [   49/   88]
per-ex loss: 0.764365  [   50/   88]
per-ex loss: 0.778360  [   51/   88]
per-ex loss: 0.724984  [   52/   88]
per-ex loss: 0.499127  [   53/   88]
per-ex loss: 0.793327  [   54/   88]
per-ex loss: 0.574263  [   55/   88]
per-ex loss: 0.714554  [   56/   88]
per-ex loss: 0.771548  [   57/   88]
per-ex loss: 0.834244  [   58/   88]
per-ex loss: 0.620212  [   59/   88]
per-ex loss: 0.559427  [   60/   88]
per-ex loss: 0.574550  [   61/   88]
per-ex loss: 0.549260  [   62/   88]
per-ex loss: 0.833845  [   63/   88]
per-ex loss: 0.501958  [   64/   88]
per-ex loss: 0.723659  [   65/   88]
per-ex loss: 0.735168  [   66/   88]
per-ex loss: 0.742970  [   67/   88]
per-ex loss: 0.623103  [   68/   88]
per-ex loss: 0.591073  [   69/   88]
per-ex loss: 0.640259  [   70/   88]
per-ex loss: 0.801410  [   71/   88]
per-ex loss: 0.816975  [   72/   88]
per-ex loss: 0.851005  [   73/   88]
per-ex loss: 0.612032  [   74/   88]
per-ex loss: 0.549321  [   75/   88]
per-ex loss: 0.609881  [   76/   88]
per-ex loss: 0.743955  [   77/   88]
per-ex loss: 0.685884  [   78/   88]
per-ex loss: 0.589275  [   79/   88]
per-ex loss: 0.722187  [   80/   88]
per-ex loss: 0.630135  [   81/   88]
per-ex loss: 0.784878  [   82/   88]
per-ex loss: 0.647576  [   83/   88]
per-ex loss: 0.636990  [   84/   88]
per-ex loss: 0.757099  [   85/   88]
per-ex loss: 0.648005  [   86/   88]
per-ex loss: 0.620663  [   87/   88]
per-ex loss: 0.621477  [   88/   88]
Train Error: Avg loss: 0.67442314
validation Error: 
 Avg loss: 0.74714594 
 F1: 0.412625 
 Precision: 0.387587 
 Recall: 0.441122
 IoU: 0.259942

test Error: 
 Avg loss: 0.69628834 
 F1: 0.504216 
 Precision: 0.470677 
 Recall: 0.542902
 IoU: 0.337092

We have finished training iteration 23
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_19_.pth
per-ex loss: 0.630111  [    1/   88]
per-ex loss: 0.652263  [    2/   88]
per-ex loss: 0.543983  [    3/   88]
per-ex loss: 0.837791  [    4/   88]
per-ex loss: 0.601495  [    5/   88]
per-ex loss: 0.599287  [    6/   88]
per-ex loss: 0.577205  [    7/   88]
per-ex loss: 0.549972  [    8/   88]
per-ex loss: 0.536529  [    9/   88]
per-ex loss: 0.568485  [   10/   88]
per-ex loss: 0.537220  [   11/   88]
per-ex loss: 0.562929  [   12/   88]
per-ex loss: 0.778713  [   13/   88]
per-ex loss: 0.574893  [   14/   88]
per-ex loss: 0.652916  [   15/   88]
per-ex loss: 0.475692  [   16/   88]
per-ex loss: 0.635797  [   17/   88]
per-ex loss: 0.610728  [   18/   88]
per-ex loss: 0.735804  [   19/   88]
per-ex loss: 0.748638  [   20/   88]
per-ex loss: 0.777415  [   21/   88]
per-ex loss: 0.748732  [   22/   88]
per-ex loss: 0.776080  [   23/   88]
per-ex loss: 0.787326  [   24/   88]
per-ex loss: 0.747398  [   25/   88]
per-ex loss: 0.640346  [   26/   88]
per-ex loss: 0.787233  [   27/   88]
per-ex loss: 0.817738  [   28/   88]
per-ex loss: 0.717271  [   29/   88]
per-ex loss: 0.709963  [   30/   88]
per-ex loss: 0.547105  [   31/   88]
per-ex loss: 0.787261  [   32/   88]
per-ex loss: 0.811204  [   33/   88]
per-ex loss: 0.585416  [   34/   88]
per-ex loss: 0.576581  [   35/   88]
per-ex loss: 0.655748  [   36/   88]
per-ex loss: 0.542034  [   37/   88]
per-ex loss: 0.804313  [   38/   88]
per-ex loss: 0.617747  [   39/   88]
per-ex loss: 0.644831  [   40/   88]
per-ex loss: 0.720573  [   41/   88]
per-ex loss: 0.628334  [   42/   88]
per-ex loss: 0.605598  [   43/   88]
per-ex loss: 0.698474  [   44/   88]
per-ex loss: 0.798265  [   45/   88]
per-ex loss: 0.635153  [   46/   88]
per-ex loss: 0.486702  [   47/   88]
per-ex loss: 0.573709  [   48/   88]
per-ex loss: 0.778285  [   49/   88]
per-ex loss: 0.784027  [   50/   88]
per-ex loss: 0.634163  [   51/   88]
per-ex loss: 0.786308  [   52/   88]
per-ex loss: 0.748573  [   53/   88]
per-ex loss: 0.671867  [   54/   88]
per-ex loss: 0.727773  [   55/   88]
per-ex loss: 0.616300  [   56/   88]
per-ex loss: 0.737785  [   57/   88]
per-ex loss: 0.598818  [   58/   88]
per-ex loss: 0.520398  [   59/   88]
per-ex loss: 0.778352  [   60/   88]
per-ex loss: 0.755360  [   61/   88]
per-ex loss: 0.824023  [   62/   88]
per-ex loss: 0.542308  [   63/   88]
per-ex loss: 0.725330  [   64/   88]
per-ex loss: 0.745334  [   65/   88]
per-ex loss: 0.602366  [   66/   88]
per-ex loss: 0.736369  [   67/   88]
per-ex loss: 0.733225  [   68/   88]
per-ex loss: 0.627927  [   69/   88]
per-ex loss: 0.759363  [   70/   88]
per-ex loss: 0.739070  [   71/   88]
per-ex loss: 0.604491  [   72/   88]
per-ex loss: 0.623448  [   73/   88]
per-ex loss: 0.626669  [   74/   88]
per-ex loss: 0.601813  [   75/   88]
per-ex loss: 0.812895  [   76/   88]
per-ex loss: 0.570943  [   77/   88]
per-ex loss: 0.664516  [   78/   88]
per-ex loss: 0.716970  [   79/   88]
per-ex loss: 0.796901  [   80/   88]
per-ex loss: 0.586197  [   81/   88]
per-ex loss: 0.836779  [   82/   88]
per-ex loss: 0.800168  [   83/   88]
per-ex loss: 0.799177  [   84/   88]
per-ex loss: 0.530748  [   85/   88]
per-ex loss: 0.547577  [   86/   88]
per-ex loss: 0.577017  [   87/   88]
per-ex loss: 0.630768  [   88/   88]
Train Error: Avg loss: 0.67203867
validation Error: 
 Avg loss: 0.70123333 
 F1: 0.481400 
 Precision: 0.519933 
 Recall: 0.448185
 IoU: 0.317003

test Error: 
 Avg loss: 0.65410931 
 F1: 0.559336 
 Precision: 0.566817 
 Recall: 0.552051
 IoU: 0.388249

We have finished training iteration 24
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_22_.pth
per-ex loss: 0.695394  [    1/   88]
per-ex loss: 0.727797  [    2/   88]
per-ex loss: 0.569090  [    3/   88]
per-ex loss: 0.669540  [    4/   88]
per-ex loss: 0.597469  [    5/   88]
per-ex loss: 0.602636  [    6/   88]
per-ex loss: 0.788853  [    7/   88]
per-ex loss: 0.594440  [    8/   88]
per-ex loss: 0.551697  [    9/   88]
per-ex loss: 0.756895  [   10/   88]
per-ex loss: 0.587612  [   11/   88]
per-ex loss: 0.577332  [   12/   88]
per-ex loss: 0.567525  [   13/   88]
per-ex loss: 0.795975  [   14/   88]
per-ex loss: 0.785788  [   15/   88]
per-ex loss: 0.546213  [   16/   88]
per-ex loss: 0.604692  [   17/   88]
per-ex loss: 0.582581  [   18/   88]
per-ex loss: 0.630379  [   19/   88]
per-ex loss: 0.637049  [   20/   88]
per-ex loss: 0.727826  [   21/   88]
per-ex loss: 0.522166  [   22/   88]
per-ex loss: 0.571766  [   23/   88]
per-ex loss: 0.810095  [   24/   88]
per-ex loss: 0.760906  [   25/   88]
per-ex loss: 0.617666  [   26/   88]
per-ex loss: 0.625159  [   27/   88]
per-ex loss: 0.541425  [   28/   88]
per-ex loss: 0.761918  [   29/   88]
per-ex loss: 0.775087  [   30/   88]
per-ex loss: 0.638685  [   31/   88]
per-ex loss: 0.766672  [   32/   88]
per-ex loss: 0.560744  [   33/   88]
per-ex loss: 0.503438  [   34/   88]
per-ex loss: 0.761508  [   35/   88]
per-ex loss: 0.719105  [   36/   88]
per-ex loss: 0.715047  [   37/   88]
per-ex loss: 0.560768  [   38/   88]
per-ex loss: 0.735228  [   39/   88]
per-ex loss: 0.616547  [   40/   88]
per-ex loss: 0.540180  [   41/   88]
per-ex loss: 0.571000  [   42/   88]
per-ex loss: 0.610925  [   43/   88]
per-ex loss: 0.830053  [   44/   88]
per-ex loss: 0.521668  [   45/   88]
per-ex loss: 0.549925  [   46/   88]
per-ex loss: 0.533431  [   47/   88]
per-ex loss: 0.588305  [   48/   88]
per-ex loss: 0.711035  [   49/   88]
per-ex loss: 0.747446  [   50/   88]
per-ex loss: 0.755762  [   51/   88]
per-ex loss: 0.607340  [   52/   88]
per-ex loss: 0.800145  [   53/   88]
per-ex loss: 0.524736  [   54/   88]
per-ex loss: 0.586266  [   55/   88]
per-ex loss: 0.799852  [   56/   88]
per-ex loss: 0.714737  [   57/   88]
per-ex loss: 0.555289  [   58/   88]
per-ex loss: 0.565339  [   59/   88]
per-ex loss: 0.632465  [   60/   88]
per-ex loss: 0.733997  [   61/   88]
per-ex loss: 0.869072  [   62/   88]
per-ex loss: 0.712437  [   63/   88]
per-ex loss: 0.646173  [   64/   88]
per-ex loss: 0.665377  [   65/   88]
per-ex loss: 0.738852  [   66/   88]
per-ex loss: 0.670246  [   67/   88]
per-ex loss: 0.778354  [   68/   88]
per-ex loss: 0.499098  [   69/   88]
per-ex loss: 0.714318  [   70/   88]
per-ex loss: 0.780954  [   71/   88]
per-ex loss: 0.629558  [   72/   88]
per-ex loss: 0.820969  [   73/   88]
per-ex loss: 0.798448  [   74/   88]
per-ex loss: 0.643178  [   75/   88]
per-ex loss: 0.541690  [   76/   88]
per-ex loss: 0.552944  [   77/   88]
per-ex loss: 0.730920  [   78/   88]
per-ex loss: 0.835542  [   79/   88]
per-ex loss: 0.742940  [   80/   88]
per-ex loss: 0.734618  [   81/   88]
per-ex loss: 0.656819  [   82/   88]
per-ex loss: 0.709108  [   83/   88]
per-ex loss: 0.796151  [   84/   88]
per-ex loss: 0.628016  [   85/   88]
per-ex loss: 0.824082  [   86/   88]
per-ex loss: 0.621739  [   87/   88]
per-ex loss: 0.729923  [   88/   88]
Train Error: Avg loss: 0.66718334
validation Error: 
 Avg loss: 0.71561866 
 F1: 0.457635 
 Precision: 0.518758 
 Recall: 0.409398
 IoU: 0.296710

test Error: 
 Avg loss: 0.66807143 
 F1: 0.541101 
 Precision: 0.591446 
 Recall: 0.498655
 IoU: 0.370897

We have finished training iteration 25
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_23_.pth
per-ex loss: 0.751219  [    1/   88]
per-ex loss: 0.622967  [    2/   88]
per-ex loss: 0.785777  [    3/   88]
per-ex loss: 0.628473  [    4/   88]
per-ex loss: 0.731378  [    5/   88]
per-ex loss: 0.801042  [    6/   88]
per-ex loss: 0.817010  [    7/   88]
per-ex loss: 0.630148  [    8/   88]
per-ex loss: 0.742986  [    9/   88]
per-ex loss: 0.570502  [   10/   88]
per-ex loss: 0.532387  [   11/   88]
per-ex loss: 0.614983  [   12/   88]
per-ex loss: 0.794471  [   13/   88]
per-ex loss: 0.711511  [   14/   88]
per-ex loss: 0.582379  [   15/   88]
per-ex loss: 0.713184  [   16/   88]
per-ex loss: 0.531780  [   17/   88]
per-ex loss: 0.777855  [   18/   88]
per-ex loss: 0.475336  [   19/   88]
per-ex loss: 0.603770  [   20/   88]
per-ex loss: 0.522658  [   21/   88]
per-ex loss: 0.778440  [   22/   88]
per-ex loss: 0.618272  [   23/   88]
per-ex loss: 0.617465  [   24/   88]
per-ex loss: 0.570087  [   25/   88]
per-ex loss: 0.781352  [   26/   88]
per-ex loss: 0.527414  [   27/   88]
per-ex loss: 0.650931  [   28/   88]
per-ex loss: 0.518057  [   29/   88]
per-ex loss: 0.730570  [   30/   88]
per-ex loss: 0.706499  [   31/   88]
per-ex loss: 0.637451  [   32/   88]
per-ex loss: 0.589430  [   33/   88]
per-ex loss: 0.553240  [   34/   88]
per-ex loss: 0.751023  [   35/   88]
per-ex loss: 0.581889  [   36/   88]
per-ex loss: 0.550302  [   37/   88]
per-ex loss: 0.776474  [   38/   88]
per-ex loss: 0.729189  [   39/   88]
per-ex loss: 0.583019  [   40/   88]
per-ex loss: 0.802157  [   41/   88]
per-ex loss: 0.634846  [   42/   88]
per-ex loss: 0.601646  [   43/   88]
per-ex loss: 0.624050  [   44/   88]
per-ex loss: 0.815049  [   45/   88]
per-ex loss: 0.847679  [   46/   88]
per-ex loss: 0.699136  [   47/   88]
per-ex loss: 0.646918  [   48/   88]
per-ex loss: 0.593639  [   49/   88]
per-ex loss: 0.834189  [   50/   88]
per-ex loss: 0.612731  [   51/   88]
per-ex loss: 0.569685  [   52/   88]
per-ex loss: 0.589404  [   53/   88]
per-ex loss: 0.661954  [   54/   88]
per-ex loss: 0.608531  [   55/   88]
per-ex loss: 0.559956  [   56/   88]
per-ex loss: 0.654769  [   57/   88]
per-ex loss: 0.761481  [   58/   88]
per-ex loss: 0.717083  [   59/   88]
per-ex loss: 0.814138  [   60/   88]
per-ex loss: 0.743497  [   61/   88]
per-ex loss: 0.546235  [   62/   88]
per-ex loss: 0.715757  [   63/   88]
per-ex loss: 0.751123  [   64/   88]
per-ex loss: 0.721677  [   65/   88]
per-ex loss: 0.527514  [   66/   88]
per-ex loss: 0.830721  [   67/   88]
per-ex loss: 0.705844  [   68/   88]
per-ex loss: 0.631485  [   69/   88]
per-ex loss: 0.637337  [   70/   88]
per-ex loss: 0.709213  [   71/   88]
per-ex loss: 0.816972  [   72/   88]
per-ex loss: 0.597904  [   73/   88]
per-ex loss: 0.736467  [   74/   88]
per-ex loss: 0.593643  [   75/   88]
per-ex loss: 0.576936  [   76/   88]
per-ex loss: 0.558198  [   77/   88]
per-ex loss: 0.741388  [   78/   88]
per-ex loss: 0.781873  [   79/   88]
per-ex loss: 0.779307  [   80/   88]
per-ex loss: 0.573093  [   81/   88]
per-ex loss: 0.607339  [   82/   88]
per-ex loss: 0.736747  [   83/   88]
per-ex loss: 0.599588  [   84/   88]
per-ex loss: 0.741317  [   85/   88]
per-ex loss: 0.640071  [   86/   88]
per-ex loss: 0.714741  [   87/   88]
per-ex loss: 0.581394  [   88/   88]
Train Error: Avg loss: 0.66749253
validation Error: 
 Avg loss: 0.70279447 
 F1: 0.480138 
 Precision: 0.633813 
 Recall: 0.386441
 IoU: 0.315909

test Error: 
 Avg loss: 0.65689933 
 F1: 0.552809 
 Precision: 0.668851 
 Recall: 0.471079
 IoU: 0.381988

We have finished training iteration 26
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_17_.pth
per-ex loss: 0.576623  [    1/   88]
per-ex loss: 0.783491  [    2/   88]
per-ex loss: 0.595704  [    3/   88]
per-ex loss: 0.633963  [    4/   88]
per-ex loss: 0.710831  [    5/   88]
per-ex loss: 0.715005  [    6/   88]
per-ex loss: 0.823903  [    7/   88]
per-ex loss: 0.609412  [    8/   88]
per-ex loss: 0.524921  [    9/   88]
per-ex loss: 0.636225  [   10/   88]
per-ex loss: 0.699946  [   11/   88]
per-ex loss: 0.492503  [   12/   88]
per-ex loss: 0.747762  [   13/   88]
per-ex loss: 0.683410  [   14/   88]
per-ex loss: 0.727988  [   15/   88]
per-ex loss: 0.594009  [   16/   88]
per-ex loss: 0.624307  [   17/   88]
per-ex loss: 0.600578  [   18/   88]
per-ex loss: 0.738545  [   19/   88]
per-ex loss: 0.626272  [   20/   88]
per-ex loss: 0.736686  [   21/   88]
per-ex loss: 0.623984  [   22/   88]
per-ex loss: 0.746711  [   23/   88]
per-ex loss: 0.555807  [   24/   88]
per-ex loss: 0.734040  [   25/   88]
per-ex loss: 0.562830  [   26/   88]
per-ex loss: 0.711825  [   27/   88]
per-ex loss: 0.577659  [   28/   88]
per-ex loss: 0.710398  [   29/   88]
per-ex loss: 0.577276  [   30/   88]
per-ex loss: 0.596306  [   31/   88]
per-ex loss: 0.772721  [   32/   88]
per-ex loss: 0.661604  [   33/   88]
per-ex loss: 0.700033  [   34/   88]
per-ex loss: 0.594957  [   35/   88]
per-ex loss: 0.587692  [   36/   88]
per-ex loss: 0.767531  [   37/   88]
per-ex loss: 0.619971  [   38/   88]
per-ex loss: 0.745120  [   39/   88]
per-ex loss: 0.746928  [   40/   88]
per-ex loss: 0.560303  [   41/   88]
per-ex loss: 0.635728  [   42/   88]
per-ex loss: 0.598907  [   43/   88]
per-ex loss: 0.522338  [   44/   88]
per-ex loss: 0.639353  [   45/   88]
per-ex loss: 0.791814  [   46/   88]
per-ex loss: 0.594642  [   47/   88]
per-ex loss: 0.651246  [   48/   88]
per-ex loss: 0.647049  [   49/   88]
per-ex loss: 0.608699  [   50/   88]
per-ex loss: 0.767476  [   51/   88]
per-ex loss: 0.613307  [   52/   88]
per-ex loss: 0.626936  [   53/   88]
per-ex loss: 0.580577  [   54/   88]
per-ex loss: 0.778405  [   55/   88]
per-ex loss: 0.506242  [   56/   88]
per-ex loss: 0.535157  [   57/   88]
per-ex loss: 0.646591  [   58/   88]
per-ex loss: 0.616425  [   59/   88]
per-ex loss: 0.574455  [   60/   88]
per-ex loss: 0.725508  [   61/   88]
per-ex loss: 0.745341  [   62/   88]
per-ex loss: 0.568458  [   63/   88]
per-ex loss: 0.709612  [   64/   88]
per-ex loss: 0.751108  [   65/   88]
per-ex loss: 0.782630  [   66/   88]
per-ex loss: 0.550534  [   67/   88]
per-ex loss: 0.610353  [   68/   88]
per-ex loss: 0.825599  [   69/   88]
per-ex loss: 0.709744  [   70/   88]
per-ex loss: 0.815412  [   71/   88]
per-ex loss: 0.555233  [   72/   88]
per-ex loss: 0.793727  [   73/   88]
per-ex loss: 0.738192  [   74/   88]
per-ex loss: 0.528087  [   75/   88]
per-ex loss: 0.800489  [   76/   88]
per-ex loss: 0.751637  [   77/   88]
per-ex loss: 0.576970  [   78/   88]
per-ex loss: 0.812095  [   79/   88]
per-ex loss: 0.805114  [   80/   88]
per-ex loss: 0.549779  [   81/   88]
per-ex loss: 0.707164  [   82/   88]
per-ex loss: 0.826516  [   83/   88]
per-ex loss: 0.530590  [   84/   88]
per-ex loss: 0.749577  [   85/   88]
per-ex loss: 0.798422  [   86/   88]
per-ex loss: 0.846038  [   87/   88]
per-ex loss: 0.704315  [   88/   88]
Train Error: Avg loss: 0.66862924
validation Error: 
 Avg loss: 0.69910095 
 F1: 0.485313 
 Precision: 0.511614 
 Recall: 0.461583
 IoU: 0.320404

test Error: 
 Avg loss: 0.65486089 
 F1: 0.557724 
 Precision: 0.565872 
 Recall: 0.549808
 IoU: 0.386697

We have finished training iteration 27
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_25_.pth
per-ex loss: 0.741257  [    1/   88]
per-ex loss: 0.732466  [    2/   88]
per-ex loss: 0.665618  [    3/   88]
per-ex loss: 0.758023  [    4/   88]
per-ex loss: 0.603700  [    5/   88]
per-ex loss: 0.687006  [    6/   88]
per-ex loss: 0.756385  [    7/   88]
per-ex loss: 0.564462  [    8/   88]
per-ex loss: 0.589565  [    9/   88]
per-ex loss: 0.739511  [   10/   88]
per-ex loss: 0.721226  [   11/   88]
per-ex loss: 0.605514  [   12/   88]
per-ex loss: 0.546854  [   13/   88]
per-ex loss: 0.624404  [   14/   88]
per-ex loss: 0.526799  [   15/   88]
per-ex loss: 0.732234  [   16/   88]
per-ex loss: 0.593784  [   17/   88]
per-ex loss: 0.813527  [   18/   88]
per-ex loss: 0.574945  [   19/   88]
per-ex loss: 0.616748  [   20/   88]
per-ex loss: 0.853578  [   21/   88]
per-ex loss: 0.753947  [   22/   88]
per-ex loss: 0.533750  [   23/   88]
per-ex loss: 0.773045  [   24/   88]
per-ex loss: 0.603095  [   25/   88]
per-ex loss: 0.698437  [   26/   88]
per-ex loss: 0.797517  [   27/   88]
per-ex loss: 0.570437  [   28/   88]
per-ex loss: 0.774692  [   29/   88]
per-ex loss: 0.551032  [   30/   88]
per-ex loss: 0.797989  [   31/   88]
per-ex loss: 0.519781  [   32/   88]
per-ex loss: 0.824498  [   33/   88]
per-ex loss: 0.625745  [   34/   88]
per-ex loss: 0.703406  [   35/   88]
per-ex loss: 0.546723  [   36/   88]
per-ex loss: 0.548673  [   37/   88]
per-ex loss: 0.506993  [   38/   88]
per-ex loss: 0.623268  [   39/   88]
per-ex loss: 0.714195  [   40/   88]
per-ex loss: 0.742609  [   41/   88]
per-ex loss: 0.592268  [   42/   88]
per-ex loss: 0.595908  [   43/   88]
per-ex loss: 0.743374  [   44/   88]
per-ex loss: 0.568188  [   45/   88]
per-ex loss: 0.656747  [   46/   88]
per-ex loss: 0.702734  [   47/   88]
per-ex loss: 0.773655  [   48/   88]
per-ex loss: 0.642008  [   49/   88]
per-ex loss: 0.572447  [   50/   88]
per-ex loss: 0.620767  [   51/   88]
per-ex loss: 0.707546  [   52/   88]
per-ex loss: 0.592154  [   53/   88]
per-ex loss: 0.558359  [   54/   88]
per-ex loss: 0.509773  [   55/   88]
per-ex loss: 0.779511  [   56/   88]
per-ex loss: 0.776163  [   57/   88]
per-ex loss: 0.794296  [   58/   88]
per-ex loss: 0.556393  [   59/   88]
per-ex loss: 0.763540  [   60/   88]
per-ex loss: 0.672632  [   61/   88]
per-ex loss: 0.702489  [   62/   88]
per-ex loss: 0.566435  [   63/   88]
per-ex loss: 0.562903  [   64/   88]
per-ex loss: 0.766648  [   65/   88]
per-ex loss: 0.616563  [   66/   88]
per-ex loss: 0.813615  [   67/   88]
per-ex loss: 0.806864  [   68/   88]
per-ex loss: 0.812872  [   69/   88]
per-ex loss: 0.754712  [   70/   88]
per-ex loss: 0.789113  [   71/   88]
per-ex loss: 0.710766  [   72/   88]
per-ex loss: 0.716997  [   73/   88]
per-ex loss: 0.610504  [   74/   88]
per-ex loss: 0.609865  [   75/   88]
per-ex loss: 0.593803  [   76/   88]
per-ex loss: 0.621107  [   77/   88]
per-ex loss: 0.629193  [   78/   88]
per-ex loss: 0.650401  [   79/   88]
per-ex loss: 0.634078  [   80/   88]
per-ex loss: 0.582960  [   81/   88]
per-ex loss: 0.622966  [   82/   88]
per-ex loss: 0.518826  [   83/   88]
per-ex loss: 0.623885  [   84/   88]
per-ex loss: 0.611830  [   85/   88]
per-ex loss: 0.741915  [   86/   88]
per-ex loss: 0.765671  [   87/   88]
per-ex loss: 0.728149  [   88/   88]
Train Error: Avg loss: 0.66592080
validation Error: 
 Avg loss: 0.72331689 
 F1: 0.444657 
 Precision: 0.694943 
 Recall: 0.326917
 IoU: 0.285890

test Error: 
 Avg loss: 0.67977450 
 F1: 0.524737 
 Precision: 0.719015 
 Recall: 0.413113
 IoU: 0.355690

We have finished training iteration 28
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_21_.pth
per-ex loss: 0.610692  [    1/   88]
per-ex loss: 0.605271  [    2/   88]
per-ex loss: 0.807441  [    3/   88]
per-ex loss: 0.543327  [    4/   88]
per-ex loss: 0.803506  [    5/   88]
per-ex loss: 0.596598  [    6/   88]
per-ex loss: 0.792650  [    7/   88]
per-ex loss: 0.590132  [    8/   88]
per-ex loss: 0.716293  [    9/   88]
per-ex loss: 0.602870  [   10/   88]
per-ex loss: 0.627351  [   11/   88]
per-ex loss: 0.761345  [   12/   88]
per-ex loss: 0.771230  [   13/   88]
per-ex loss: 0.647360  [   14/   88]
per-ex loss: 0.766384  [   15/   88]
per-ex loss: 0.589660  [   16/   88]
per-ex loss: 0.631353  [   17/   88]
per-ex loss: 0.573533  [   18/   88]
per-ex loss: 0.766833  [   19/   88]
per-ex loss: 0.787484  [   20/   88]
per-ex loss: 0.727770  [   21/   88]
per-ex loss: 0.792984  [   22/   88]
per-ex loss: 0.582685  [   23/   88]
per-ex loss: 0.824241  [   24/   88]
per-ex loss: 0.564443  [   25/   88]
per-ex loss: 0.581613  [   26/   88]
per-ex loss: 0.570259  [   27/   88]
per-ex loss: 0.728305  [   28/   88]
per-ex loss: 0.588713  [   29/   88]
per-ex loss: 0.542419  [   30/   88]
per-ex loss: 0.510407  [   31/   88]
per-ex loss: 0.715223  [   32/   88]
per-ex loss: 0.807737  [   33/   88]
per-ex loss: 0.562855  [   34/   88]
per-ex loss: 0.615222  [   35/   88]
per-ex loss: 0.720991  [   36/   88]
per-ex loss: 0.569844  [   37/   88]
per-ex loss: 0.684153  [   38/   88]
per-ex loss: 0.582488  [   39/   88]
per-ex loss: 0.665947  [   40/   88]
per-ex loss: 0.740225  [   41/   88]
per-ex loss: 0.513859  [   42/   88]
per-ex loss: 0.478223  [   43/   88]
per-ex loss: 0.741526  [   44/   88]
per-ex loss: 0.804114  [   45/   88]
per-ex loss: 0.739305  [   46/   88]
per-ex loss: 0.660759  [   47/   88]
per-ex loss: 0.789433  [   48/   88]
per-ex loss: 0.594275  [   49/   88]
per-ex loss: 0.579714  [   50/   88]
per-ex loss: 0.821272  [   51/   88]
per-ex loss: 0.821774  [   52/   88]
per-ex loss: 0.622898  [   53/   88]
per-ex loss: 0.766930  [   54/   88]
per-ex loss: 0.589058  [   55/   88]
per-ex loss: 0.578864  [   56/   88]
per-ex loss: 0.606314  [   57/   88]
per-ex loss: 0.509192  [   58/   88]
per-ex loss: 0.648456  [   59/   88]
per-ex loss: 0.728362  [   60/   88]
per-ex loss: 0.586303  [   61/   88]
per-ex loss: 0.627112  [   62/   88]
per-ex loss: 0.588618  [   63/   88]
per-ex loss: 0.757125  [   64/   88]
per-ex loss: 0.592124  [   65/   88]
per-ex loss: 0.733625  [   66/   88]
per-ex loss: 0.708782  [   67/   88]
per-ex loss: 0.597320  [   68/   88]
per-ex loss: 0.618882  [   69/   88]
per-ex loss: 0.684902  [   70/   88]
per-ex loss: 0.575737  [   71/   88]
per-ex loss: 0.577506  [   72/   88]
per-ex loss: 0.693852  [   73/   88]
per-ex loss: 0.755171  [   74/   88]
per-ex loss: 0.677625  [   75/   88]
per-ex loss: 0.769793  [   76/   88]
per-ex loss: 0.580980  [   77/   88]
per-ex loss: 0.526333  [   78/   88]
per-ex loss: 0.707051  [   79/   88]
per-ex loss: 0.703935  [   80/   88]
per-ex loss: 0.734172  [   81/   88]
per-ex loss: 0.687566  [   82/   88]
per-ex loss: 0.765362  [   83/   88]
per-ex loss: 0.523802  [   84/   88]
per-ex loss: 0.820754  [   85/   88]
per-ex loss: 0.571920  [   86/   88]
per-ex loss: 0.779181  [   87/   88]
per-ex loss: 0.612846  [   88/   88]
Train Error: Avg loss: 0.66389257
validation Error: 
 Avg loss: 0.69614189 
 F1: 0.489645 
 Precision: 0.555211 
 Recall: 0.437929
 IoU: 0.324192

test Error: 
 Avg loss: 0.65200556 
 F1: 0.563587 
 Precision: 0.600904 
 Recall: 0.530633
 IoU: 0.392357

We have finished training iteration 29
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_26_.pth
per-ex loss: 0.696074  [    1/   88]
per-ex loss: 0.774237  [    2/   88]
per-ex loss: 0.651570  [    3/   88]
per-ex loss: 0.642648  [    4/   88]
per-ex loss: 0.826902  [    5/   88]
per-ex loss: 0.492137  [    6/   88]
per-ex loss: 0.582532  [    7/   88]
per-ex loss: 0.602285  [    8/   88]
per-ex loss: 0.562498  [    9/   88]
per-ex loss: 0.810255  [   10/   88]
per-ex loss: 0.609762  [   11/   88]
per-ex loss: 0.718376  [   12/   88]
per-ex loss: 0.771654  [   13/   88]
per-ex loss: 0.751124  [   14/   88]
per-ex loss: 0.781156  [   15/   88]
per-ex loss: 0.614785  [   16/   88]
per-ex loss: 0.580281  [   17/   88]
per-ex loss: 0.717018  [   18/   88]
per-ex loss: 0.753860  [   19/   88]
per-ex loss: 0.630653  [   20/   88]
per-ex loss: 0.729084  [   21/   88]
per-ex loss: 0.537347  [   22/   88]
per-ex loss: 0.594556  [   23/   88]
per-ex loss: 0.762131  [   24/   88]
per-ex loss: 0.833281  [   25/   88]
per-ex loss: 0.639235  [   26/   88]
per-ex loss: 0.771237  [   27/   88]
per-ex loss: 0.563694  [   28/   88]
per-ex loss: 0.778902  [   29/   88]
per-ex loss: 0.738402  [   30/   88]
per-ex loss: 0.765562  [   31/   88]
per-ex loss: 0.756182  [   32/   88]
per-ex loss: 0.567867  [   33/   88]
per-ex loss: 0.706889  [   34/   88]
per-ex loss: 0.751307  [   35/   88]
per-ex loss: 0.607008  [   36/   88]
per-ex loss: 0.512919  [   37/   88]
per-ex loss: 0.825794  [   38/   88]
per-ex loss: 0.656347  [   39/   88]
per-ex loss: 0.635350  [   40/   88]
per-ex loss: 0.733994  [   41/   88]
per-ex loss: 0.545223  [   42/   88]
per-ex loss: 0.812635  [   43/   88]
per-ex loss: 0.482516  [   44/   88]
per-ex loss: 0.606776  [   45/   88]
per-ex loss: 0.714152  [   46/   88]
per-ex loss: 0.785269  [   47/   88]
per-ex loss: 0.527370  [   48/   88]
per-ex loss: 0.766938  [   49/   88]
per-ex loss: 0.620233  [   50/   88]
per-ex loss: 0.584629  [   51/   88]
per-ex loss: 0.655241  [   52/   88]
per-ex loss: 0.563412  [   53/   88]
per-ex loss: 0.781773  [   54/   88]
per-ex loss: 0.515922  [   55/   88]
per-ex loss: 0.707075  [   56/   88]
per-ex loss: 0.731764  [   57/   88]
per-ex loss: 0.538021  [   58/   88]
per-ex loss: 0.814145  [   59/   88]
per-ex loss: 0.573894  [   60/   88]
per-ex loss: 0.611920  [   61/   88]
per-ex loss: 0.475933  [   62/   88]
per-ex loss: 0.715552  [   63/   88]
per-ex loss: 0.640473  [   64/   88]
per-ex loss: 0.774015  [   65/   88]
per-ex loss: 0.614071  [   66/   88]
per-ex loss: 0.725456  [   67/   88]
per-ex loss: 0.748735  [   68/   88]
per-ex loss: 0.694316  [   69/   88]
per-ex loss: 0.744211  [   70/   88]
per-ex loss: 0.551988  [   71/   88]
per-ex loss: 0.621016  [   72/   88]
per-ex loss: 0.657439  [   73/   88]
per-ex loss: 0.621765  [   74/   88]
per-ex loss: 0.515336  [   75/   88]
per-ex loss: 0.670841  [   76/   88]
per-ex loss: 0.593868  [   77/   88]
per-ex loss: 0.782261  [   78/   88]
per-ex loss: 0.569183  [   79/   88]
per-ex loss: 0.726026  [   80/   88]
per-ex loss: 0.808544  [   81/   88]
per-ex loss: 0.588412  [   82/   88]
per-ex loss: 0.614635  [   83/   88]
per-ex loss: 0.711081  [   84/   88]
per-ex loss: 0.585491  [   85/   88]
per-ex loss: 0.555484  [   86/   88]
per-ex loss: 0.575748  [   87/   88]
per-ex loss: 0.618319  [   88/   88]
Train Error: Avg loss: 0.66450003
validation Error: 
 Avg loss: 0.70349641 
 F1: 0.479212 
 Precision: 0.608729 
 Recall: 0.395140
 IoU: 0.315108

test Error: 
 Avg loss: 0.66425471 
 F1: 0.543041 
 Precision: 0.658488 
 Recall: 0.462037
 IoU: 0.372723

We have finished training iteration 30
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_28_.pth
per-ex loss: 0.754983  [    1/   88]
per-ex loss: 0.586086  [    2/   88]
per-ex loss: 0.641501  [    3/   88]
per-ex loss: 0.514033  [    4/   88]
per-ex loss: 0.540793  [    5/   88]
per-ex loss: 0.729281  [    6/   88]
per-ex loss: 0.756442  [    7/   88]
per-ex loss: 0.827660  [    8/   88]
per-ex loss: 0.512925  [    9/   88]
per-ex loss: 0.719237  [   10/   88]
per-ex loss: 0.593534  [   11/   88]
per-ex loss: 0.665740  [   12/   88]
per-ex loss: 0.603561  [   13/   88]
per-ex loss: 0.696031  [   14/   88]
per-ex loss: 0.582945  [   15/   88]
per-ex loss: 0.781108  [   16/   88]
per-ex loss: 0.754569  [   17/   88]
per-ex loss: 0.610638  [   18/   88]
per-ex loss: 0.844104  [   19/   88]
per-ex loss: 0.619929  [   20/   88]
per-ex loss: 0.589666  [   21/   88]
per-ex loss: 0.776686  [   22/   88]
per-ex loss: 0.807649  [   23/   88]
per-ex loss: 0.556735  [   24/   88]
per-ex loss: 0.610836  [   25/   88]
per-ex loss: 0.742024  [   26/   88]
per-ex loss: 0.699351  [   27/   88]
per-ex loss: 0.754746  [   28/   88]
per-ex loss: 0.714884  [   29/   88]
per-ex loss: 0.542736  [   30/   88]
per-ex loss: 0.507610  [   31/   88]
per-ex loss: 0.509806  [   32/   88]
per-ex loss: 0.762837  [   33/   88]
per-ex loss: 0.608591  [   34/   88]
per-ex loss: 0.577938  [   35/   88]
per-ex loss: 0.781575  [   36/   88]
per-ex loss: 0.626453  [   37/   88]
per-ex loss: 0.615452  [   38/   88]
per-ex loss: 0.822462  [   39/   88]
per-ex loss: 0.740536  [   40/   88]
per-ex loss: 0.576909  [   41/   88]
per-ex loss: 0.704598  [   42/   88]
per-ex loss: 0.573377  [   43/   88]
per-ex loss: 0.622064  [   44/   88]
per-ex loss: 0.742603  [   45/   88]
per-ex loss: 0.609768  [   46/   88]
per-ex loss: 0.822922  [   47/   88]
per-ex loss: 0.703611  [   48/   88]
per-ex loss: 0.778227  [   49/   88]
per-ex loss: 0.484048  [   50/   88]
per-ex loss: 0.518786  [   51/   88]
per-ex loss: 0.607470  [   52/   88]
per-ex loss: 0.548149  [   53/   88]
per-ex loss: 0.762548  [   54/   88]
per-ex loss: 0.770013  [   55/   88]
per-ex loss: 0.632306  [   56/   88]
per-ex loss: 0.785665  [   57/   88]
per-ex loss: 0.570261  [   58/   88]
per-ex loss: 0.636965  [   59/   88]
per-ex loss: 0.494777  [   60/   88]
per-ex loss: 0.718614  [   61/   88]
per-ex loss: 0.526254  [   62/   88]
per-ex loss: 0.628582  [   63/   88]
per-ex loss: 0.710153  [   64/   88]
per-ex loss: 0.711866  [   65/   88]
per-ex loss: 0.680813  [   66/   88]
per-ex loss: 0.796674  [   67/   88]
per-ex loss: 0.681333  [   68/   88]
per-ex loss: 0.705379  [   69/   88]
per-ex loss: 0.730851  [   70/   88]
per-ex loss: 0.638506  [   71/   88]
per-ex loss: 0.748491  [   72/   88]
per-ex loss: 0.816729  [   73/   88]
per-ex loss: 0.580698  [   74/   88]
per-ex loss: 0.618434  [   75/   88]
per-ex loss: 0.793621  [   76/   88]
per-ex loss: 0.574997  [   77/   88]
per-ex loss: 0.566163  [   78/   88]
per-ex loss: 0.719911  [   79/   88]
per-ex loss: 0.638150  [   80/   88]
per-ex loss: 0.570092  [   81/   88]
per-ex loss: 0.607570  [   82/   88]
per-ex loss: 0.738626  [   83/   88]
per-ex loss: 0.551392  [   84/   88]
per-ex loss: 0.718267  [   85/   88]
per-ex loss: 0.553350  [   86/   88]
per-ex loss: 0.809187  [   87/   88]
per-ex loss: 0.596214  [   88/   88]
Train Error: Avg loss: 0.66317795
validation Error: 
 Avg loss: 0.71693336 
 F1: 0.461183 
 Precision: 0.706686 
 Recall: 0.342276
 IoU: 0.299700

test Error: 
 Avg loss: 0.68098059 
 F1: 0.515569 
 Precision: 0.746422 
 Recall: 0.393780
 IoU: 0.347317

We have finished training iteration 31
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_24_.pth
per-ex loss: 0.586548  [    1/   88]
per-ex loss: 0.698908  [    2/   88]
per-ex loss: 0.631384  [    3/   88]
per-ex loss: 0.734540  [    4/   88]
per-ex loss: 0.834948  [    5/   88]
per-ex loss: 0.718869  [    6/   88]
per-ex loss: 0.473136  [    7/   88]
per-ex loss: 0.563951  [    8/   88]
per-ex loss: 0.692364  [    9/   88]
per-ex loss: 0.734881  [   10/   88]
per-ex loss: 0.568771  [   11/   88]
per-ex loss: 0.597265  [   12/   88]
per-ex loss: 0.565529  [   13/   88]
per-ex loss: 0.627172  [   14/   88]
per-ex loss: 0.703840  [   15/   88]
per-ex loss: 0.525498  [   16/   88]
per-ex loss: 0.770541  [   17/   88]
per-ex loss: 0.626462  [   18/   88]
per-ex loss: 0.711741  [   19/   88]
per-ex loss: 0.837786  [   20/   88]
per-ex loss: 0.535197  [   21/   88]
per-ex loss: 0.625437  [   22/   88]
per-ex loss: 0.627440  [   23/   88]
per-ex loss: 0.808649  [   24/   88]
per-ex loss: 0.634915  [   25/   88]
per-ex loss: 0.785047  [   26/   88]
per-ex loss: 0.561740  [   27/   88]
per-ex loss: 0.775072  [   28/   88]
per-ex loss: 0.755476  [   29/   88]
per-ex loss: 0.603227  [   30/   88]
per-ex loss: 0.814065  [   31/   88]
per-ex loss: 0.591860  [   32/   88]
per-ex loss: 0.536293  [   33/   88]
per-ex loss: 0.596085  [   34/   88]
per-ex loss: 0.595477  [   35/   88]
per-ex loss: 0.558126  [   36/   88]
per-ex loss: 0.668608  [   37/   88]
per-ex loss: 0.706030  [   38/   88]
per-ex loss: 0.736279  [   39/   88]
per-ex loss: 0.746815  [   40/   88]
per-ex loss: 0.777024  [   41/   88]
per-ex loss: 0.596253  [   42/   88]
per-ex loss: 0.758329  [   43/   88]
per-ex loss: 0.740210  [   44/   88]
per-ex loss: 0.740044  [   45/   88]
per-ex loss: 0.564398  [   46/   88]
per-ex loss: 0.524681  [   47/   88]
per-ex loss: 0.521426  [   48/   88]
per-ex loss: 0.823463  [   49/   88]
per-ex loss: 0.699816  [   50/   88]
per-ex loss: 0.710857  [   51/   88]
per-ex loss: 0.739102  [   52/   88]
per-ex loss: 0.607633  [   53/   88]
per-ex loss: 0.566179  [   54/   88]
per-ex loss: 0.791200  [   55/   88]
per-ex loss: 0.788335  [   56/   88]
per-ex loss: 0.782129  [   57/   88]
per-ex loss: 0.641955  [   58/   88]
per-ex loss: 0.752320  [   59/   88]
per-ex loss: 0.544234  [   60/   88]
per-ex loss: 0.606172  [   61/   88]
per-ex loss: 0.546939  [   62/   88]
per-ex loss: 0.575144  [   63/   88]
per-ex loss: 0.656238  [   64/   88]
per-ex loss: 0.568707  [   65/   88]
per-ex loss: 0.712537  [   66/   88]
per-ex loss: 0.801735  [   67/   88]
per-ex loss: 0.714438  [   68/   88]
per-ex loss: 0.582385  [   69/   88]
per-ex loss: 0.530168  [   70/   88]
per-ex loss: 0.614246  [   71/   88]
per-ex loss: 0.755520  [   72/   88]
per-ex loss: 0.525082  [   73/   88]
per-ex loss: 0.626480  [   74/   88]
per-ex loss: 0.816752  [   75/   88]
per-ex loss: 0.624030  [   76/   88]
per-ex loss: 0.627542  [   77/   88]
per-ex loss: 0.746769  [   78/   88]
per-ex loss: 0.808667  [   79/   88]
per-ex loss: 0.497516  [   80/   88]
per-ex loss: 0.509630  [   81/   88]
per-ex loss: 0.714308  [   82/   88]
per-ex loss: 0.581508  [   83/   88]
per-ex loss: 0.690883  [   84/   88]
per-ex loss: 0.708729  [   85/   88]
per-ex loss: 0.710528  [   86/   88]
per-ex loss: 0.555749  [   87/   88]
per-ex loss: 0.551689  [   88/   88]
Train Error: Avg loss: 0.66017786
validation Error: 
 Avg loss: 0.70931349 
 F1: 0.466685 
 Precision: 0.493345 
 Recall: 0.442759
 IoU: 0.304363

test Error: 
 Avg loss: 0.66374957 
 F1: 0.544831 
 Precision: 0.549804 
 Recall: 0.539946
 IoU: 0.374410

We have finished training iteration 32
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_30_.pth
per-ex loss: 0.613337  [    1/   88]
per-ex loss: 0.744738  [    2/   88]
per-ex loss: 0.817884  [    3/   88]
per-ex loss: 0.554044  [    4/   88]
per-ex loss: 0.735129  [    5/   88]
per-ex loss: 0.720740  [    6/   88]
per-ex loss: 0.801123  [    7/   88]
per-ex loss: 0.603215  [    8/   88]
per-ex loss: 0.606408  [    9/   88]
per-ex loss: 0.697398  [   10/   88]
per-ex loss: 0.632872  [   11/   88]
per-ex loss: 0.480646  [   12/   88]
per-ex loss: 0.696331  [   13/   88]
per-ex loss: 0.760564  [   14/   88]
per-ex loss: 0.771435  [   15/   88]
per-ex loss: 0.627396  [   16/   88]
per-ex loss: 0.823705  [   17/   88]
per-ex loss: 0.819745  [   18/   88]
per-ex loss: 0.604752  [   19/   88]
per-ex loss: 0.498331  [   20/   88]
per-ex loss: 0.859797  [   21/   88]
per-ex loss: 0.595723  [   22/   88]
per-ex loss: 0.542066  [   23/   88]
per-ex loss: 0.812851  [   24/   88]
per-ex loss: 0.721130  [   25/   88]
per-ex loss: 0.586462  [   26/   88]
per-ex loss: 0.604934  [   27/   88]
per-ex loss: 0.815780  [   28/   88]
per-ex loss: 0.569457  [   29/   88]
per-ex loss: 0.719255  [   30/   88]
per-ex loss: 0.687841  [   31/   88]
per-ex loss: 0.727582  [   32/   88]
per-ex loss: 0.703795  [   33/   88]
per-ex loss: 0.652802  [   34/   88]
per-ex loss: 0.764962  [   35/   88]
per-ex loss: 0.550644  [   36/   88]
per-ex loss: 0.578009  [   37/   88]
per-ex loss: 0.648986  [   38/   88]
per-ex loss: 0.700044  [   39/   88]
per-ex loss: 0.641561  [   40/   88]
per-ex loss: 0.630294  [   41/   88]
per-ex loss: 0.583323  [   42/   88]
per-ex loss: 0.801041  [   43/   88]
per-ex loss: 0.705358  [   44/   88]
per-ex loss: 0.565185  [   45/   88]
per-ex loss: 0.540241  [   46/   88]
per-ex loss: 0.582728  [   47/   88]
per-ex loss: 0.770988  [   48/   88]
per-ex loss: 0.725394  [   49/   88]
per-ex loss: 0.756658  [   50/   88]
per-ex loss: 0.683821  [   51/   88]
per-ex loss: 0.782926  [   52/   88]
per-ex loss: 0.758031  [   53/   88]
per-ex loss: 0.572689  [   54/   88]
per-ex loss: 0.616565  [   55/   88]
per-ex loss: 0.770412  [   56/   88]
per-ex loss: 0.540967  [   57/   88]
per-ex loss: 0.583209  [   58/   88]
per-ex loss: 0.535428  [   59/   88]
per-ex loss: 0.583831  [   60/   88]
per-ex loss: 0.571979  [   61/   88]
per-ex loss: 0.568333  [   62/   88]
per-ex loss: 0.583413  [   63/   88]
per-ex loss: 0.778261  [   64/   88]
per-ex loss: 0.597815  [   65/   88]
per-ex loss: 0.562658  [   66/   88]
per-ex loss: 0.561458  [   67/   88]
per-ex loss: 0.693149  [   68/   88]
per-ex loss: 0.557004  [   69/   88]
per-ex loss: 0.684036  [   70/   88]
per-ex loss: 0.634256  [   71/   88]
per-ex loss: 0.712646  [   72/   88]
per-ex loss: 0.596184  [   73/   88]
per-ex loss: 0.806925  [   74/   88]
per-ex loss: 0.477557  [   75/   88]
per-ex loss: 0.777488  [   76/   88]
per-ex loss: 0.711110  [   77/   88]
per-ex loss: 0.639218  [   78/   88]
per-ex loss: 0.539778  [   79/   88]
per-ex loss: 0.598968  [   80/   88]
per-ex loss: 0.777942  [   81/   88]
per-ex loss: 0.583506  [   82/   88]
per-ex loss: 0.767258  [   83/   88]
per-ex loss: 0.781470  [   84/   88]
per-ex loss: 0.743134  [   85/   88]
per-ex loss: 0.632706  [   86/   88]
per-ex loss: 0.509379  [   87/   88]
per-ex loss: 0.614632  [   88/   88]
Train Error: Avg loss: 0.66187307
validation Error: 
 Avg loss: 0.70589407 
 F1: 0.475767 
 Precision: 0.479633 
 Recall: 0.471962
 IoU: 0.312135

test Error: 
 Avg loss: 0.65901276 
 F1: 0.553365 
 Precision: 0.538979 
 Recall: 0.568539
 IoU: 0.382518

We have finished training iteration 33
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_31_.pth
per-ex loss: 0.603814  [    1/   88]
per-ex loss: 0.541910  [    2/   88]
per-ex loss: 0.837584  [    3/   88]
per-ex loss: 0.617900  [    4/   88]
per-ex loss: 0.704243  [    5/   88]
per-ex loss: 0.709821  [    6/   88]
per-ex loss: 0.772420  [    7/   88]
per-ex loss: 0.690563  [    8/   88]
per-ex loss: 0.636302  [    9/   88]
per-ex loss: 0.719206  [   10/   88]
per-ex loss: 0.797503  [   11/   88]
per-ex loss: 0.737592  [   12/   88]
per-ex loss: 0.559674  [   13/   88]
per-ex loss: 0.615547  [   14/   88]
per-ex loss: 0.712860  [   15/   88]
per-ex loss: 0.631888  [   16/   88]
per-ex loss: 0.829793  [   17/   88]
per-ex loss: 0.581218  [   18/   88]
per-ex loss: 0.576461  [   19/   88]
per-ex loss: 0.579184  [   20/   88]
per-ex loss: 0.578314  [   21/   88]
per-ex loss: 0.697866  [   22/   88]
per-ex loss: 0.636328  [   23/   88]
per-ex loss: 0.602075  [   24/   88]
per-ex loss: 0.704422  [   25/   88]
per-ex loss: 0.709091  [   26/   88]
per-ex loss: 0.812671  [   27/   88]
per-ex loss: 0.582974  [   28/   88]
per-ex loss: 0.492212  [   29/   88]
per-ex loss: 0.792588  [   30/   88]
per-ex loss: 0.555874  [   31/   88]
per-ex loss: 0.725815  [   32/   88]
per-ex loss: 0.716604  [   33/   88]
per-ex loss: 0.625056  [   34/   88]
per-ex loss: 0.585097  [   35/   88]
per-ex loss: 0.721198  [   36/   88]
per-ex loss: 0.533567  [   37/   88]
per-ex loss: 0.563889  [   38/   88]
per-ex loss: 0.740249  [   39/   88]
per-ex loss: 0.743624  [   40/   88]
per-ex loss: 0.577911  [   41/   88]
per-ex loss: 0.598193  [   42/   88]
per-ex loss: 0.589863  [   43/   88]
per-ex loss: 0.730106  [   44/   88]
per-ex loss: 0.601081  [   45/   88]
per-ex loss: 0.769656  [   46/   88]
per-ex loss: 0.699020  [   47/   88]
per-ex loss: 0.511506  [   48/   88]
per-ex loss: 0.787200  [   49/   88]
per-ex loss: 0.771000  [   50/   88]
per-ex loss: 0.626422  [   51/   88]
per-ex loss: 0.572923  [   52/   88]
per-ex loss: 0.724888  [   53/   88]
per-ex loss: 0.546545  [   54/   88]
per-ex loss: 0.600509  [   55/   88]
per-ex loss: 0.850116  [   56/   88]
per-ex loss: 0.593856  [   57/   88]
per-ex loss: 0.623587  [   58/   88]
per-ex loss: 0.657523  [   59/   88]
per-ex loss: 0.606237  [   60/   88]
per-ex loss: 0.555185  [   61/   88]
per-ex loss: 0.789362  [   62/   88]
per-ex loss: 0.739251  [   63/   88]
per-ex loss: 0.696758  [   64/   88]
per-ex loss: 0.747997  [   65/   88]
per-ex loss: 0.529231  [   66/   88]
per-ex loss: 0.751311  [   67/   88]
per-ex loss: 0.726802  [   68/   88]
per-ex loss: 0.800881  [   69/   88]
per-ex loss: 0.714942  [   70/   88]
per-ex loss: 0.552605  [   71/   88]
per-ex loss: 0.562379  [   72/   88]
per-ex loss: 0.760491  [   73/   88]
per-ex loss: 0.587350  [   74/   88]
per-ex loss: 0.491315  [   75/   88]
per-ex loss: 0.615443  [   76/   88]
per-ex loss: 0.561536  [   77/   88]
per-ex loss: 0.601676  [   78/   88]
per-ex loss: 0.797571  [   79/   88]
per-ex loss: 0.512094  [   80/   88]
per-ex loss: 0.499618  [   81/   88]
per-ex loss: 0.750667  [   82/   88]
per-ex loss: 0.554094  [   83/   88]
per-ex loss: 0.782744  [   84/   88]
per-ex loss: 0.587563  [   85/   88]
per-ex loss: 0.811722  [   86/   88]
per-ex loss: 0.543147  [   87/   88]
per-ex loss: 0.585834  [   88/   88]
Train Error: Avg loss: 0.65707621
validation Error: 
 Avg loss: 0.70772358 
 F1: 0.466264 
 Precision: 0.639773 
 Recall: 0.366789
 IoU: 0.304006

test Error: 
 Avg loss: 0.66414229 
 F1: 0.543476 
 Precision: 0.666579 
 Recall: 0.458753
 IoU: 0.373132

We have finished training iteration 34
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_32_.pth
per-ex loss: 0.616667  [    1/   88]
per-ex loss: 0.555903  [    2/   88]
per-ex loss: 0.808876  [    3/   88]
per-ex loss: 0.610418  [    4/   88]
per-ex loss: 0.723129  [    5/   88]
per-ex loss: 0.607703  [    6/   88]
per-ex loss: 0.557273  [    7/   88]
per-ex loss: 0.545572  [    8/   88]
per-ex loss: 0.505352  [    9/   88]
per-ex loss: 0.735179  [   10/   88]
per-ex loss: 0.594600  [   11/   88]
per-ex loss: 0.618219  [   12/   88]
per-ex loss: 0.775861  [   13/   88]
per-ex loss: 0.755722  [   14/   88]
per-ex loss: 0.689155  [   15/   88]
per-ex loss: 0.817791  [   16/   88]
per-ex loss: 0.808543  [   17/   88]
per-ex loss: 0.774256  [   18/   88]
per-ex loss: 0.528538  [   19/   88]
per-ex loss: 0.805964  [   20/   88]
per-ex loss: 0.546220  [   21/   88]
per-ex loss: 0.635523  [   22/   88]
per-ex loss: 0.644593  [   23/   88]
per-ex loss: 0.677420  [   24/   88]
per-ex loss: 0.607154  [   25/   88]
per-ex loss: 0.626150  [   26/   88]
per-ex loss: 0.715858  [   27/   88]
per-ex loss: 0.696358  [   28/   88]
per-ex loss: 0.543287  [   29/   88]
per-ex loss: 0.738047  [   30/   88]
per-ex loss: 0.518524  [   31/   88]
per-ex loss: 0.685539  [   32/   88]
per-ex loss: 0.763832  [   33/   88]
per-ex loss: 0.545488  [   34/   88]
per-ex loss: 0.793607  [   35/   88]
per-ex loss: 0.610289  [   36/   88]
per-ex loss: 0.562327  [   37/   88]
per-ex loss: 0.595150  [   38/   88]
per-ex loss: 0.561763  [   39/   88]
per-ex loss: 0.702444  [   40/   88]
per-ex loss: 0.533999  [   41/   88]
per-ex loss: 0.761002  [   42/   88]
per-ex loss: 0.510556  [   43/   88]
per-ex loss: 0.621043  [   44/   88]
per-ex loss: 0.723709  [   45/   88]
per-ex loss: 0.794892  [   46/   88]
per-ex loss: 0.743049  [   47/   88]
per-ex loss: 0.680314  [   48/   88]
per-ex loss: 0.740951  [   49/   88]
per-ex loss: 0.730634  [   50/   88]
per-ex loss: 0.718496  [   51/   88]
per-ex loss: 0.564998  [   52/   88]
per-ex loss: 0.710383  [   53/   88]
per-ex loss: 0.537210  [   54/   88]
per-ex loss: 0.591333  [   55/   88]
per-ex loss: 0.822153  [   56/   88]
per-ex loss: 0.618470  [   57/   88]
per-ex loss: 0.595298  [   58/   88]
per-ex loss: 0.652672  [   59/   88]
per-ex loss: 0.586934  [   60/   88]
per-ex loss: 0.578729  [   61/   88]
per-ex loss: 0.853488  [   62/   88]
per-ex loss: 0.769767  [   63/   88]
per-ex loss: 0.568431  [   64/   88]
per-ex loss: 0.719801  [   65/   88]
per-ex loss: 0.596738  [   66/   88]
per-ex loss: 0.538219  [   67/   88]
per-ex loss: 0.723960  [   68/   88]
per-ex loss: 0.506677  [   69/   88]
per-ex loss: 0.569198  [   70/   88]
per-ex loss: 0.637953  [   71/   88]
per-ex loss: 0.490422  [   72/   88]
per-ex loss: 0.724895  [   73/   88]
per-ex loss: 0.809471  [   74/   88]
per-ex loss: 0.775234  [   75/   88]
per-ex loss: 0.795719  [   76/   88]
per-ex loss: 0.593988  [   77/   88]
per-ex loss: 0.681069  [   78/   88]
per-ex loss: 0.548585  [   79/   88]
per-ex loss: 0.581676  [   80/   88]
per-ex loss: 0.593361  [   81/   88]
per-ex loss: 0.724210  [   82/   88]
per-ex loss: 0.779691  [   83/   88]
per-ex loss: 0.625062  [   84/   88]
per-ex loss: 0.643439  [   85/   88]
per-ex loss: 0.779666  [   86/   88]
per-ex loss: 0.806215  [   87/   88]
per-ex loss: 0.582422  [   88/   88]
Train Error: Avg loss: 0.65955085
validation Error: 
 Avg loss: 0.71114089 
 F1: 0.465265 
 Precision: 0.688983 
 Recall: 0.351220
 IoU: 0.303156

test Error: 
 Avg loss: 0.67225680 
 F1: 0.525896 
 Precision: 0.728715 
 Recall: 0.411395
 IoU: 0.356756

We have finished training iteration 35
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_33_.pth
per-ex loss: 0.616554  [    1/   88]
per-ex loss: 0.607611  [    2/   88]
per-ex loss: 0.826864  [    3/   88]
per-ex loss: 0.595541  [    4/   88]
per-ex loss: 0.684718  [    5/   88]
per-ex loss: 0.753939  [    6/   88]
per-ex loss: 0.611257  [    7/   88]
per-ex loss: 0.557775  [    8/   88]
per-ex loss: 0.701558  [    9/   88]
per-ex loss: 0.621851  [   10/   88]
per-ex loss: 0.536804  [   11/   88]
per-ex loss: 0.651161  [   12/   88]
per-ex loss: 0.652794  [   13/   88]
per-ex loss: 0.540977  [   14/   88]
per-ex loss: 0.741774  [   15/   88]
per-ex loss: 0.666400  [   16/   88]
per-ex loss: 0.711183  [   17/   88]
per-ex loss: 0.758180  [   18/   88]
per-ex loss: 0.756522  [   19/   88]
per-ex loss: 0.777963  [   20/   88]
per-ex loss: 0.575996  [   21/   88]
per-ex loss: 0.701667  [   22/   88]
per-ex loss: 0.536159  [   23/   88]
per-ex loss: 0.655845  [   24/   88]
per-ex loss: 0.765700  [   25/   88]
per-ex loss: 0.567822  [   26/   88]
per-ex loss: 0.793273  [   27/   88]
per-ex loss: 0.576747  [   28/   88]
per-ex loss: 0.737705  [   29/   88]
per-ex loss: 0.567819  [   30/   88]
per-ex loss: 0.655239  [   31/   88]
per-ex loss: 0.714836  [   32/   88]
per-ex loss: 0.817322  [   33/   88]
per-ex loss: 0.544948  [   34/   88]
per-ex loss: 0.704857  [   35/   88]
per-ex loss: 0.680795  [   36/   88]
per-ex loss: 0.709084  [   37/   88]
per-ex loss: 0.589453  [   38/   88]
per-ex loss: 0.768522  [   39/   88]
per-ex loss: 0.748793  [   40/   88]
per-ex loss: 0.516084  [   41/   88]
per-ex loss: 0.601246  [   42/   88]
per-ex loss: 0.716864  [   43/   88]
per-ex loss: 0.635098  [   44/   88]
per-ex loss: 0.727259  [   45/   88]
per-ex loss: 0.517238  [   46/   88]
per-ex loss: 0.706940  [   47/   88]
per-ex loss: 0.819367  [   48/   88]
per-ex loss: 0.623341  [   49/   88]
per-ex loss: 0.537964  [   50/   88]
per-ex loss: 0.764468  [   51/   88]
per-ex loss: 0.809722  [   52/   88]
per-ex loss: 0.597523  [   53/   88]
per-ex loss: 0.518041  [   54/   88]
per-ex loss: 0.833192  [   55/   88]
per-ex loss: 0.532778  [   56/   88]
per-ex loss: 0.660333  [   57/   88]
per-ex loss: 0.769677  [   58/   88]
per-ex loss: 0.616280  [   59/   88]
per-ex loss: 0.612152  [   60/   88]
per-ex loss: 0.623478  [   61/   88]
per-ex loss: 0.605723  [   62/   88]
per-ex loss: 0.729472  [   63/   88]
per-ex loss: 0.807864  [   64/   88]
per-ex loss: 0.549762  [   65/   88]
per-ex loss: 0.572451  [   66/   88]
per-ex loss: 0.580414  [   67/   88]
per-ex loss: 0.752892  [   68/   88]
per-ex loss: 0.574514  [   69/   88]
per-ex loss: 0.786648  [   70/   88]
per-ex loss: 0.540917  [   71/   88]
per-ex loss: 0.739976  [   72/   88]
per-ex loss: 0.637617  [   73/   88]
per-ex loss: 0.454443  [   74/   88]
per-ex loss: 0.615777  [   75/   88]
per-ex loss: 0.586918  [   76/   88]
per-ex loss: 0.741342  [   77/   88]
per-ex loss: 0.615339  [   78/   88]
per-ex loss: 0.774598  [   79/   88]
per-ex loss: 0.718243  [   80/   88]
per-ex loss: 0.709610  [   81/   88]
per-ex loss: 0.577515  [   82/   88]
per-ex loss: 0.504453  [   83/   88]
per-ex loss: 0.593346  [   84/   88]
per-ex loss: 0.804205  [   85/   88]
per-ex loss: 0.621114  [   86/   88]
per-ex loss: 0.574651  [   87/   88]
per-ex loss: 0.766231  [   88/   88]
Train Error: Avg loss: 0.65976237
validation Error: 
 Avg loss: 0.69798462 
 F1: 0.484029 
 Precision: 0.515033 
 Recall: 0.456547
 IoU: 0.319287

test Error: 
 Avg loss: 0.65208350 
 F1: 0.554012 
 Precision: 0.592994 
 Recall: 0.519840
 IoU: 0.383138

We have finished training iteration 36
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_34_.pth
per-ex loss: 0.830121  [    1/   88]
per-ex loss: 0.786882  [    2/   88]
per-ex loss: 0.730268  [    3/   88]
per-ex loss: 0.613667  [    4/   88]
per-ex loss: 0.697006  [    5/   88]
per-ex loss: 0.619456  [    6/   88]
per-ex loss: 0.602833  [    7/   88]
per-ex loss: 0.748846  [    8/   88]
per-ex loss: 0.571038  [    9/   88]
per-ex loss: 0.569547  [   10/   88]
per-ex loss: 0.515230  [   11/   88]
per-ex loss: 0.618918  [   12/   88]
per-ex loss: 0.553378  [   13/   88]
per-ex loss: 0.649333  [   14/   88]
per-ex loss: 0.840665  [   15/   88]
per-ex loss: 0.519547  [   16/   88]
per-ex loss: 0.590777  [   17/   88]
per-ex loss: 0.574467  [   18/   88]
per-ex loss: 0.708619  [   19/   88]
per-ex loss: 0.630284  [   20/   88]
per-ex loss: 0.690343  [   21/   88]
per-ex loss: 0.785830  [   22/   88]
per-ex loss: 0.708780  [   23/   88]
per-ex loss: 0.706809  [   24/   88]
per-ex loss: 0.529412  [   25/   88]
per-ex loss: 0.563042  [   26/   88]
per-ex loss: 0.525483  [   27/   88]
per-ex loss: 0.553764  [   28/   88]
per-ex loss: 0.727963  [   29/   88]
per-ex loss: 0.810214  [   30/   88]
per-ex loss: 0.730470  [   31/   88]
per-ex loss: 0.783332  [   32/   88]
per-ex loss: 0.732094  [   33/   88]
per-ex loss: 0.684401  [   34/   88]
per-ex loss: 0.755605  [   35/   88]
per-ex loss: 0.753404  [   36/   88]
per-ex loss: 0.624403  [   37/   88]
per-ex loss: 0.739803  [   38/   88]
per-ex loss: 0.621384  [   39/   88]
per-ex loss: 0.660725  [   40/   88]
per-ex loss: 0.566865  [   41/   88]
per-ex loss: 0.485494  [   42/   88]
per-ex loss: 0.524246  [   43/   88]
per-ex loss: 0.746736  [   44/   88]
per-ex loss: 0.628824  [   45/   88]
per-ex loss: 0.822686  [   46/   88]
per-ex loss: 0.837605  [   47/   88]
per-ex loss: 0.764078  [   48/   88]
per-ex loss: 0.743646  [   49/   88]
per-ex loss: 0.714520  [   50/   88]
per-ex loss: 0.600008  [   51/   88]
per-ex loss: 0.604816  [   52/   88]
per-ex loss: 0.789296  [   53/   88]
per-ex loss: 0.650585  [   54/   88]
per-ex loss: 0.737607  [   55/   88]
per-ex loss: 0.588224  [   56/   88]
per-ex loss: 0.761273  [   57/   88]
per-ex loss: 0.591998  [   58/   88]
per-ex loss: 0.627836  [   59/   88]
per-ex loss: 0.556552  [   60/   88]
per-ex loss: 0.550399  [   61/   88]
per-ex loss: 0.570301  [   62/   88]
per-ex loss: 0.683079  [   63/   88]
per-ex loss: 0.589368  [   64/   88]
per-ex loss: 0.788821  [   65/   88]
per-ex loss: 0.544749  [   66/   88]
per-ex loss: 0.697397  [   67/   88]
per-ex loss: 0.624981  [   68/   88]
per-ex loss: 0.577819  [   69/   88]
per-ex loss: 0.576952  [   70/   88]
per-ex loss: 0.647289  [   71/   88]
per-ex loss: 0.604823  [   72/   88]
per-ex loss: 0.659369  [   73/   88]
per-ex loss: 0.493253  [   74/   88]
per-ex loss: 0.619763  [   75/   88]
per-ex loss: 0.785226  [   76/   88]
per-ex loss: 0.682817  [   77/   88]
per-ex loss: 0.850136  [   78/   88]
per-ex loss: 0.763321  [   79/   88]
per-ex loss: 0.531504  [   80/   88]
per-ex loss: 0.736760  [   81/   88]
per-ex loss: 0.483647  [   82/   88]
per-ex loss: 0.560892  [   83/   88]
per-ex loss: 0.713333  [   84/   88]
per-ex loss: 0.754011  [   85/   88]
per-ex loss: 0.786251  [   86/   88]
per-ex loss: 0.559963  [   87/   88]
per-ex loss: 0.603893  [   88/   88]
Train Error: Avg loss: 0.65955858
validation Error: 
 Avg loss: 0.70520709 
 F1: 0.472019 
 Precision: 0.570441 
 Recall: 0.402562
 IoU: 0.308917

test Error: 
 Avg loss: 0.66063039 
 F1: 0.545499 
 Precision: 0.612560 
 Recall: 0.491673
 IoU: 0.375042

We have finished training iteration 37
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_35_.pth
per-ex loss: 0.771731  [    1/   88]
per-ex loss: 0.805432  [    2/   88]
per-ex loss: 0.612207  [    3/   88]
per-ex loss: 0.744722  [    4/   88]
per-ex loss: 0.479088  [    5/   88]
per-ex loss: 0.605635  [    6/   88]
per-ex loss: 0.623360  [    7/   88]
per-ex loss: 0.740737  [    8/   88]
per-ex loss: 0.725777  [    9/   88]
per-ex loss: 0.760810  [   10/   88]
per-ex loss: 0.810650  [   11/   88]
per-ex loss: 0.728181  [   12/   88]
per-ex loss: 0.707466  [   13/   88]
per-ex loss: 0.736510  [   14/   88]
per-ex loss: 0.647727  [   15/   88]
per-ex loss: 0.630532  [   16/   88]
per-ex loss: 0.621107  [   17/   88]
per-ex loss: 0.688400  [   18/   88]
per-ex loss: 0.665629  [   19/   88]
per-ex loss: 0.660659  [   20/   88]
per-ex loss: 0.815627  [   21/   88]
per-ex loss: 0.512444  [   22/   88]
per-ex loss: 0.732082  [   23/   88]
per-ex loss: 0.560698  [   24/   88]
per-ex loss: 0.703574  [   25/   88]
per-ex loss: 0.571970  [   26/   88]
per-ex loss: 0.494825  [   27/   88]
per-ex loss: 0.802571  [   28/   88]
per-ex loss: 0.693219  [   29/   88]
per-ex loss: 0.832762  [   30/   88]
per-ex loss: 0.582675  [   31/   88]
per-ex loss: 0.536844  [   32/   88]
per-ex loss: 0.757791  [   33/   88]
per-ex loss: 0.750844  [   34/   88]
per-ex loss: 0.519076  [   35/   88]
per-ex loss: 0.598479  [   36/   88]
per-ex loss: 0.531303  [   37/   88]
per-ex loss: 0.625505  [   38/   88]
per-ex loss: 0.744145  [   39/   88]
per-ex loss: 0.514964  [   40/   88]
per-ex loss: 0.541610  [   41/   88]
per-ex loss: 0.588725  [   42/   88]
per-ex loss: 0.530948  [   43/   88]
per-ex loss: 0.665526  [   44/   88]
per-ex loss: 0.586420  [   45/   88]
per-ex loss: 0.701737  [   46/   88]
per-ex loss: 0.822761  [   47/   88]
per-ex loss: 0.767088  [   48/   88]
per-ex loss: 0.507949  [   49/   88]
per-ex loss: 0.827809  [   50/   88]
per-ex loss: 0.572272  [   51/   88]
per-ex loss: 0.614256  [   52/   88]
per-ex loss: 0.714460  [   53/   88]
per-ex loss: 0.765916  [   54/   88]
per-ex loss: 0.539056  [   55/   88]
per-ex loss: 0.590594  [   56/   88]
per-ex loss: 0.718974  [   57/   88]
per-ex loss: 0.723720  [   58/   88]
per-ex loss: 0.689045  [   59/   88]
per-ex loss: 0.748060  [   60/   88]
per-ex loss: 0.564122  [   61/   88]
per-ex loss: 0.557248  [   62/   88]
per-ex loss: 0.592983  [   63/   88]
per-ex loss: 0.541166  [   64/   88]
per-ex loss: 0.583866  [   65/   88]
per-ex loss: 0.649316  [   66/   88]
per-ex loss: 0.594332  [   67/   88]
per-ex loss: 0.721740  [   68/   88]
per-ex loss: 0.673086  [   69/   88]
per-ex loss: 0.573993  [   70/   88]
per-ex loss: 0.786665  [   71/   88]
per-ex loss: 0.495504  [   72/   88]
per-ex loss: 0.756092  [   73/   88]
per-ex loss: 0.530679  [   74/   88]
per-ex loss: 0.612360  [   75/   88]
per-ex loss: 0.572521  [   76/   88]
per-ex loss: 0.587742  [   77/   88]
per-ex loss: 0.754929  [   78/   88]
per-ex loss: 0.594009  [   79/   88]
per-ex loss: 0.625202  [   80/   88]
per-ex loss: 0.719902  [   81/   88]
per-ex loss: 0.542705  [   82/   88]
per-ex loss: 0.565781  [   83/   88]
per-ex loss: 0.744861  [   84/   88]
per-ex loss: 0.639255  [   85/   88]
per-ex loss: 0.596337  [   86/   88]
per-ex loss: 0.552407  [   87/   88]
per-ex loss: 0.796358  [   88/   88]
Train Error: Avg loss: 0.65215732
validation Error: 
 Avg loss: 0.68820189 
 F1: 0.495747 
 Precision: 0.588461 
 Recall: 0.428271
 IoU: 0.329563

test Error: 
 Avg loss: 0.64801412 
 F1: 0.563911 
 Precision: 0.629292 
 Recall: 0.510837
 IoU: 0.392671

We have finished training iteration 38
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_27_.pth
per-ex loss: 0.636243  [    1/   88]
per-ex loss: 0.604283  [    2/   88]
per-ex loss: 0.724184  [    3/   88]
per-ex loss: 0.808023  [    4/   88]
per-ex loss: 0.667910  [    5/   88]
per-ex loss: 0.533356  [    6/   88]
per-ex loss: 0.739547  [    7/   88]
per-ex loss: 0.834136  [    8/   88]
per-ex loss: 0.630631  [    9/   88]
per-ex loss: 0.617495  [   10/   88]
per-ex loss: 0.720045  [   11/   88]
per-ex loss: 0.633026  [   12/   88]
per-ex loss: 0.784522  [   13/   88]
per-ex loss: 0.613622  [   14/   88]
per-ex loss: 0.557631  [   15/   88]
per-ex loss: 0.604037  [   16/   88]
per-ex loss: 0.657167  [   17/   88]
per-ex loss: 0.522337  [   18/   88]
per-ex loss: 0.546802  [   19/   88]
per-ex loss: 0.544829  [   20/   88]
per-ex loss: 0.774650  [   21/   88]
per-ex loss: 0.625978  [   22/   88]
per-ex loss: 0.745319  [   23/   88]
per-ex loss: 0.583917  [   24/   88]
per-ex loss: 0.544581  [   25/   88]
per-ex loss: 0.707275  [   26/   88]
per-ex loss: 0.549249  [   27/   88]
per-ex loss: 0.603876  [   28/   88]
per-ex loss: 0.675025  [   29/   88]
per-ex loss: 0.604905  [   30/   88]
per-ex loss: 0.591128  [   31/   88]
per-ex loss: 0.522301  [   32/   88]
per-ex loss: 0.530580  [   33/   88]
per-ex loss: 0.775377  [   34/   88]
per-ex loss: 0.531626  [   35/   88]
per-ex loss: 0.571227  [   36/   88]
per-ex loss: 0.606177  [   37/   88]
per-ex loss: 0.720845  [   38/   88]
per-ex loss: 0.556819  [   39/   88]
per-ex loss: 0.492430  [   40/   88]
per-ex loss: 0.709053  [   41/   88]
per-ex loss: 0.596473  [   42/   88]
per-ex loss: 0.717228  [   43/   88]
per-ex loss: 0.703397  [   44/   88]
per-ex loss: 0.653451  [   45/   88]
per-ex loss: 0.556666  [   46/   88]
per-ex loss: 0.576074  [   47/   88]
per-ex loss: 0.753973  [   48/   88]
per-ex loss: 0.750706  [   49/   88]
per-ex loss: 0.782494  [   50/   88]
per-ex loss: 0.591804  [   51/   88]
per-ex loss: 0.706899  [   52/   88]
per-ex loss: 0.768951  [   53/   88]
per-ex loss: 0.581701  [   54/   88]
per-ex loss: 0.669226  [   55/   88]
per-ex loss: 0.712215  [   56/   88]
per-ex loss: 0.562354  [   57/   88]
per-ex loss: 0.726523  [   58/   88]
per-ex loss: 0.761751  [   59/   88]
per-ex loss: 0.517271  [   60/   88]
per-ex loss: 0.543352  [   61/   88]
per-ex loss: 0.754114  [   62/   88]
per-ex loss: 0.641595  [   63/   88]
per-ex loss: 0.595814  [   64/   88]
per-ex loss: 0.590333  [   65/   88]
per-ex loss: 0.559744  [   66/   88]
per-ex loss: 0.527968  [   67/   88]
per-ex loss: 0.811651  [   68/   88]
per-ex loss: 0.724224  [   69/   88]
per-ex loss: 0.504119  [   70/   88]
per-ex loss: 0.612962  [   71/   88]
per-ex loss: 0.814307  [   72/   88]
per-ex loss: 0.724526  [   73/   88]
per-ex loss: 0.524415  [   74/   88]
per-ex loss: 0.555065  [   75/   88]
per-ex loss: 0.847088  [   76/   88]
per-ex loss: 0.806138  [   77/   88]
per-ex loss: 0.771888  [   78/   88]
per-ex loss: 0.739598  [   79/   88]
per-ex loss: 0.731492  [   80/   88]
per-ex loss: 0.489773  [   81/   88]
per-ex loss: 0.817657  [   82/   88]
per-ex loss: 0.717961  [   83/   88]
per-ex loss: 0.743667  [   84/   88]
per-ex loss: 0.797388  [   85/   88]
per-ex loss: 0.622230  [   86/   88]
per-ex loss: 0.747731  [   87/   88]
per-ex loss: 0.616461  [   88/   88]
Train Error: Avg loss: 0.65482479
validation Error: 
 Avg loss: 0.70345900 
 F1: 0.475584 
 Precision: 0.530671 
 Recall: 0.430858
 IoU: 0.311978

test Error: 
 Avg loss: 0.65961751 
 F1: 0.550448 
 Precision: 0.576363 
 Recall: 0.526764
 IoU: 0.379737

We have finished training iteration 39
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_37_.pth
per-ex loss: 0.601206  [    1/   88]
per-ex loss: 0.572326  [    2/   88]
per-ex loss: 0.751067  [    3/   88]
per-ex loss: 0.638082  [    4/   88]
per-ex loss: 0.625262  [    5/   88]
per-ex loss: 0.538487  [    6/   88]
per-ex loss: 0.603091  [    7/   88]
per-ex loss: 0.700665  [    8/   88]
per-ex loss: 0.824454  [    9/   88]
per-ex loss: 0.812117  [   10/   88]
per-ex loss: 0.523942  [   11/   88]
per-ex loss: 0.572793  [   12/   88]
per-ex loss: 0.579346  [   13/   88]
per-ex loss: 0.727932  [   14/   88]
per-ex loss: 0.506301  [   15/   88]
per-ex loss: 0.700690  [   16/   88]
per-ex loss: 0.829352  [   17/   88]
per-ex loss: 0.514588  [   18/   88]
per-ex loss: 0.750954  [   19/   88]
per-ex loss: 0.654114  [   20/   88]
per-ex loss: 0.545230  [   21/   88]
per-ex loss: 0.501971  [   22/   88]
per-ex loss: 0.586574  [   23/   88]
per-ex loss: 0.743870  [   24/   88]
per-ex loss: 0.745398  [   25/   88]
per-ex loss: 0.691635  [   26/   88]
per-ex loss: 0.701900  [   27/   88]
per-ex loss: 0.601538  [   28/   88]
per-ex loss: 0.517061  [   29/   88]
per-ex loss: 0.566764  [   30/   88]
per-ex loss: 0.773791  [   31/   88]
per-ex loss: 0.481397  [   32/   88]
per-ex loss: 0.715810  [   33/   88]
per-ex loss: 0.818069  [   34/   88]
per-ex loss: 0.543859  [   35/   88]
per-ex loss: 0.723915  [   36/   88]
per-ex loss: 0.616179  [   37/   88]
per-ex loss: 0.786611  [   38/   88]
per-ex loss: 0.581297  [   39/   88]
per-ex loss: 0.549440  [   40/   88]
per-ex loss: 0.597390  [   41/   88]
per-ex loss: 0.755443  [   42/   88]
per-ex loss: 0.816214  [   43/   88]
per-ex loss: 0.595393  [   44/   88]
per-ex loss: 0.517377  [   45/   88]
per-ex loss: 0.773007  [   46/   88]
per-ex loss: 0.661549  [   47/   88]
per-ex loss: 0.573066  [   48/   88]
per-ex loss: 0.544403  [   49/   88]
per-ex loss: 0.510225  [   50/   88]
per-ex loss: 0.586459  [   51/   88]
per-ex loss: 0.764545  [   52/   88]
per-ex loss: 0.780734  [   53/   88]
per-ex loss: 0.682787  [   54/   88]
per-ex loss: 0.576213  [   55/   88]
per-ex loss: 0.608003  [   56/   88]
per-ex loss: 0.536284  [   57/   88]
per-ex loss: 0.561235  [   58/   88]
per-ex loss: 0.597554  [   59/   88]
per-ex loss: 0.756936  [   60/   88]
per-ex loss: 0.534287  [   61/   88]
per-ex loss: 0.712720  [   62/   88]
per-ex loss: 0.722993  [   63/   88]
per-ex loss: 0.592391  [   64/   88]
per-ex loss: 0.745862  [   65/   88]
per-ex loss: 0.573397  [   66/   88]
per-ex loss: 0.630292  [   67/   88]
per-ex loss: 0.594326  [   68/   88]
per-ex loss: 0.613240  [   69/   88]
per-ex loss: 0.733956  [   70/   88]
per-ex loss: 0.705312  [   71/   88]
per-ex loss: 0.807824  [   72/   88]
per-ex loss: 0.653373  [   73/   88]
per-ex loss: 0.604839  [   74/   88]
per-ex loss: 0.706301  [   75/   88]
per-ex loss: 0.492627  [   76/   88]
per-ex loss: 0.583185  [   77/   88]
per-ex loss: 0.762540  [   78/   88]
per-ex loss: 0.531876  [   79/   88]
per-ex loss: 0.702016  [   80/   88]
per-ex loss: 0.621997  [   81/   88]
per-ex loss: 0.587096  [   82/   88]
per-ex loss: 0.738864  [   83/   88]
per-ex loss: 0.665928  [   84/   88]
per-ex loss: 0.766376  [   85/   88]
per-ex loss: 0.579932  [   86/   88]
per-ex loss: 0.814146  [   87/   88]
per-ex loss: 0.741347  [   88/   88]
Train Error: Avg loss: 0.64923800
validation Error: 
 Avg loss: 0.70254473 
 F1: 0.482649 
 Precision: 0.483162 
 Recall: 0.482137
 IoU: 0.318087

test Error: 
 Avg loss: 0.65762624 
 F1: 0.553262 
 Precision: 0.536804 
 Recall: 0.570761
 IoU: 0.382420

We have finished training iteration 40
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_36_.pth
per-ex loss: 0.777130  [    1/   88]
per-ex loss: 0.814244  [    2/   88]
per-ex loss: 0.576773  [    3/   88]
per-ex loss: 0.559775  [    4/   88]
per-ex loss: 0.604781  [    5/   88]
per-ex loss: 0.714798  [    6/   88]
per-ex loss: 0.575265  [    7/   88]
per-ex loss: 0.697412  [    8/   88]
per-ex loss: 0.740573  [    9/   88]
per-ex loss: 0.731270  [   10/   88]
per-ex loss: 0.778262  [   11/   88]
per-ex loss: 0.708426  [   12/   88]
per-ex loss: 0.616310  [   13/   88]
per-ex loss: 0.600525  [   14/   88]
per-ex loss: 0.532018  [   15/   88]
per-ex loss: 0.614766  [   16/   88]
per-ex loss: 0.827179  [   17/   88]
per-ex loss: 0.808319  [   18/   88]
per-ex loss: 0.727909  [   19/   88]
per-ex loss: 0.702602  [   20/   88]
per-ex loss: 0.765772  [   21/   88]
per-ex loss: 0.526510  [   22/   88]
per-ex loss: 0.586865  [   23/   88]
per-ex loss: 0.536257  [   24/   88]
per-ex loss: 0.745461  [   25/   88]
per-ex loss: 0.817115  [   26/   88]
per-ex loss: 0.636343  [   27/   88]
per-ex loss: 0.511974  [   28/   88]
per-ex loss: 0.465628  [   29/   88]
per-ex loss: 0.564591  [   30/   88]
per-ex loss: 0.754989  [   31/   88]
per-ex loss: 0.637130  [   32/   88]
per-ex loss: 0.561435  [   33/   88]
per-ex loss: 0.601967  [   34/   88]
per-ex loss: 0.713047  [   35/   88]
per-ex loss: 0.593490  [   36/   88]
per-ex loss: 0.781688  [   37/   88]
per-ex loss: 0.642267  [   38/   88]
per-ex loss: 0.648414  [   39/   88]
per-ex loss: 0.613667  [   40/   88]
per-ex loss: 0.574813  [   41/   88]
per-ex loss: 0.806410  [   42/   88]
per-ex loss: 0.568546  [   43/   88]
per-ex loss: 0.697892  [   44/   88]
per-ex loss: 0.735367  [   45/   88]
per-ex loss: 0.704392  [   46/   88]
per-ex loss: 0.754831  [   47/   88]
per-ex loss: 0.561246  [   48/   88]
per-ex loss: 0.526078  [   49/   88]
per-ex loss: 0.507562  [   50/   88]
per-ex loss: 0.711134  [   51/   88]
per-ex loss: 0.768752  [   52/   88]
per-ex loss: 0.524031  [   53/   88]
per-ex loss: 0.812648  [   54/   88]
per-ex loss: 0.773021  [   55/   88]
per-ex loss: 0.637776  [   56/   88]
per-ex loss: 0.740733  [   57/   88]
per-ex loss: 0.591494  [   58/   88]
per-ex loss: 0.645175  [   59/   88]
per-ex loss: 0.794893  [   60/   88]
per-ex loss: 0.631423  [   61/   88]
per-ex loss: 0.606342  [   62/   88]
per-ex loss: 0.703850  [   63/   88]
per-ex loss: 0.626480  [   64/   88]
per-ex loss: 0.775646  [   65/   88]
per-ex loss: 0.559016  [   66/   88]
per-ex loss: 0.532813  [   67/   88]
per-ex loss: 0.558718  [   68/   88]
per-ex loss: 0.535617  [   69/   88]
per-ex loss: 0.727729  [   70/   88]
per-ex loss: 0.617519  [   71/   88]
per-ex loss: 0.698703  [   72/   88]
per-ex loss: 0.544840  [   73/   88]
per-ex loss: 0.538857  [   74/   88]
per-ex loss: 0.778051  [   75/   88]
per-ex loss: 0.586286  [   76/   88]
per-ex loss: 0.745508  [   77/   88]
per-ex loss: 0.772915  [   78/   88]
per-ex loss: 0.752427  [   79/   88]
per-ex loss: 0.710533  [   80/   88]
per-ex loss: 0.586963  [   81/   88]
per-ex loss: 0.527098  [   82/   88]
per-ex loss: 0.607152  [   83/   88]
per-ex loss: 0.498082  [   84/   88]
per-ex loss: 0.577167  [   85/   88]
per-ex loss: 0.727194  [   86/   88]
per-ex loss: 0.812043  [   87/   88]
per-ex loss: 0.576146  [   88/   88]
Train Error: Avg loss: 0.65641884
validation Error: 
 Avg loss: 0.69612253 
 F1: 0.484937 
 Precision: 0.639350 
 Recall: 0.390602
 IoU: 0.320078

test Error: 
 Avg loss: 0.65731033 
 F1: 0.552093 
 Precision: 0.675206 
 Recall: 0.466952
 IoU: 0.381304

We have finished training iteration 41
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_39_.pth
per-ex loss: 0.545805  [    1/   88]
per-ex loss: 0.516277  [    2/   88]
per-ex loss: 0.559300  [    3/   88]
per-ex loss: 0.681062  [    4/   88]
per-ex loss: 0.820824  [    5/   88]
per-ex loss: 0.757622  [    6/   88]
per-ex loss: 0.812925  [    7/   88]
per-ex loss: 0.628470  [    8/   88]
per-ex loss: 0.690203  [    9/   88]
per-ex loss: 0.521227  [   10/   88]
per-ex loss: 0.752343  [   11/   88]
per-ex loss: 0.830843  [   12/   88]
per-ex loss: 0.605824  [   13/   88]
per-ex loss: 0.770169  [   14/   88]
per-ex loss: 0.608299  [   15/   88]
per-ex loss: 0.739139  [   16/   88]
per-ex loss: 0.545836  [   17/   88]
per-ex loss: 0.589499  [   18/   88]
per-ex loss: 0.487679  [   19/   88]
per-ex loss: 0.601181  [   20/   88]
per-ex loss: 0.542636  [   21/   88]
per-ex loss: 0.707110  [   22/   88]
per-ex loss: 0.574639  [   23/   88]
per-ex loss: 0.737799  [   24/   88]
per-ex loss: 0.725075  [   25/   88]
per-ex loss: 0.603220  [   26/   88]
per-ex loss: 0.692739  [   27/   88]
per-ex loss: 0.589054  [   28/   88]
per-ex loss: 0.566799  [   29/   88]
per-ex loss: 0.810300  [   30/   88]
per-ex loss: 0.753356  [   31/   88]
per-ex loss: 0.569470  [   32/   88]
per-ex loss: 0.734130  [   33/   88]
per-ex loss: 0.633337  [   34/   88]
per-ex loss: 0.780052  [   35/   88]
per-ex loss: 0.798683  [   36/   88]
per-ex loss: 0.591154  [   37/   88]
per-ex loss: 0.698811  [   38/   88]
per-ex loss: 0.684679  [   39/   88]
per-ex loss: 0.735854  [   40/   88]
per-ex loss: 0.735900  [   41/   88]
per-ex loss: 0.527541  [   42/   88]
per-ex loss: 0.550055  [   43/   88]
per-ex loss: 0.523908  [   44/   88]
per-ex loss: 0.569627  [   45/   88]
per-ex loss: 0.724883  [   46/   88]
per-ex loss: 0.582746  [   47/   88]
per-ex loss: 0.559423  [   48/   88]
per-ex loss: 0.812009  [   49/   88]
per-ex loss: 0.576913  [   50/   88]
per-ex loss: 0.615340  [   51/   88]
per-ex loss: 0.683882  [   52/   88]
per-ex loss: 0.763403  [   53/   88]
per-ex loss: 0.593717  [   54/   88]
per-ex loss: 0.744194  [   55/   88]
per-ex loss: 0.578778  [   56/   88]
per-ex loss: 0.615985  [   57/   88]
per-ex loss: 0.740189  [   58/   88]
per-ex loss: 0.717029  [   59/   88]
per-ex loss: 0.688856  [   60/   88]
per-ex loss: 0.580690  [   61/   88]
per-ex loss: 0.592399  [   62/   88]
per-ex loss: 0.649626  [   63/   88]
per-ex loss: 0.720227  [   64/   88]
per-ex loss: 0.752084  [   65/   88]
per-ex loss: 0.788562  [   66/   88]
per-ex loss: 0.522889  [   67/   88]
per-ex loss: 0.752047  [   68/   88]
per-ex loss: 0.557194  [   69/   88]
per-ex loss: 0.712612  [   70/   88]
per-ex loss: 0.738758  [   71/   88]
per-ex loss: 0.763019  [   72/   88]
per-ex loss: 0.811315  [   73/   88]
per-ex loss: 0.617618  [   74/   88]
per-ex loss: 0.520792  [   75/   88]
per-ex loss: 0.475633  [   76/   88]
per-ex loss: 0.598986  [   77/   88]
per-ex loss: 0.828866  [   78/   88]
per-ex loss: 0.562259  [   79/   88]
per-ex loss: 0.548992  [   80/   88]
per-ex loss: 0.656191  [   81/   88]
per-ex loss: 0.563699  [   82/   88]
per-ex loss: 0.593428  [   83/   88]
per-ex loss: 0.623127  [   84/   88]
per-ex loss: 0.535517  [   85/   88]
per-ex loss: 0.588868  [   86/   88]
per-ex loss: 0.764515  [   87/   88]
per-ex loss: 0.598498  [   88/   88]
Train Error: Avg loss: 0.65359332
validation Error: 
 Avg loss: 0.70212865 
 F1: 0.479461 
 Precision: 0.686150 
 Recall: 0.368468
 IoU: 0.315323

test Error: 
 Avg loss: 0.65888364 
 F1: 0.547674 
 Precision: 0.710639 
 Recall: 0.445509
 IoU: 0.377101

We have finished training iteration 42
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_40_.pth
per-ex loss: 0.750485  [    1/   88]
per-ex loss: 0.613826  [    2/   88]
per-ex loss: 0.616981  [    3/   88]
per-ex loss: 0.714187  [    4/   88]
per-ex loss: 0.561076  [    5/   88]
per-ex loss: 0.569283  [    6/   88]
per-ex loss: 0.543849  [    7/   88]
per-ex loss: 0.754993  [    8/   88]
per-ex loss: 0.764928  [    9/   88]
per-ex loss: 0.557932  [   10/   88]
per-ex loss: 0.768403  [   11/   88]
per-ex loss: 0.759363  [   12/   88]
per-ex loss: 0.579976  [   13/   88]
per-ex loss: 0.483661  [   14/   88]
per-ex loss: 0.633441  [   15/   88]
per-ex loss: 0.803246  [   16/   88]
per-ex loss: 0.718130  [   17/   88]
per-ex loss: 0.608063  [   18/   88]
per-ex loss: 0.683844  [   19/   88]
per-ex loss: 0.561929  [   20/   88]
per-ex loss: 0.592090  [   21/   88]
per-ex loss: 0.628164  [   22/   88]
per-ex loss: 0.756556  [   23/   88]
per-ex loss: 0.513253  [   24/   88]
per-ex loss: 0.572636  [   25/   88]
per-ex loss: 0.746882  [   26/   88]
per-ex loss: 0.625618  [   27/   88]
per-ex loss: 0.832778  [   28/   88]
per-ex loss: 0.799478  [   29/   88]
per-ex loss: 0.792855  [   30/   88]
per-ex loss: 0.712884  [   31/   88]
per-ex loss: 0.731750  [   32/   88]
per-ex loss: 0.716011  [   33/   88]
per-ex loss: 0.793291  [   34/   88]
per-ex loss: 0.565440  [   35/   88]
per-ex loss: 0.782162  [   36/   88]
per-ex loss: 0.536122  [   37/   88]
per-ex loss: 0.758912  [   38/   88]
per-ex loss: 0.816725  [   39/   88]
per-ex loss: 0.576587  [   40/   88]
per-ex loss: 0.675025  [   41/   88]
per-ex loss: 0.601113  [   42/   88]
per-ex loss: 0.693968  [   43/   88]
per-ex loss: 0.542462  [   44/   88]
per-ex loss: 0.542684  [   45/   88]
per-ex loss: 0.579987  [   46/   88]
per-ex loss: 0.410865  [   47/   88]
per-ex loss: 0.668224  [   48/   88]
per-ex loss: 0.575995  [   49/   88]
per-ex loss: 0.750633  [   50/   88]
per-ex loss: 0.581053  [   51/   88]
per-ex loss: 0.705169  [   52/   88]
per-ex loss: 0.665660  [   53/   88]
per-ex loss: 0.794918  [   54/   88]
per-ex loss: 0.807957  [   55/   88]
per-ex loss: 0.523329  [   56/   88]
per-ex loss: 0.703201  [   57/   88]
per-ex loss: 0.591534  [   58/   88]
per-ex loss: 0.584920  [   59/   88]
per-ex loss: 0.501349  [   60/   88]
per-ex loss: 0.813664  [   61/   88]
per-ex loss: 0.571040  [   62/   88]
per-ex loss: 0.590169  [   63/   88]
per-ex loss: 0.503698  [   64/   88]
per-ex loss: 0.518603  [   65/   88]
per-ex loss: 0.530802  [   66/   88]
per-ex loss: 0.515543  [   67/   88]
per-ex loss: 0.708334  [   68/   88]
per-ex loss: 0.700485  [   69/   88]
per-ex loss: 0.725312  [   70/   88]
per-ex loss: 0.560618  [   71/   88]
per-ex loss: 0.630086  [   72/   88]
per-ex loss: 0.664127  [   73/   88]
per-ex loss: 0.778489  [   74/   88]
per-ex loss: 0.771079  [   75/   88]
per-ex loss: 0.538555  [   76/   88]
per-ex loss: 0.518498  [   77/   88]
per-ex loss: 0.715438  [   78/   88]
per-ex loss: 0.739689  [   79/   88]
per-ex loss: 0.600872  [   80/   88]
per-ex loss: 0.570714  [   81/   88]
per-ex loss: 0.620443  [   82/   88]
per-ex loss: 0.602766  [   83/   88]
per-ex loss: 0.572551  [   84/   88]
per-ex loss: 0.757011  [   85/   88]
per-ex loss: 0.600341  [   86/   88]
per-ex loss: 0.644203  [   87/   88]
per-ex loss: 0.721782  [   88/   88]
Train Error: Avg loss: 0.64980395
validation Error: 
 Avg loss: 0.69248241 
 F1: 0.490229 
 Precision: 0.519650 
 Recall: 0.463962
 IoU: 0.324705

test Error: 
 Avg loss: 0.64542817 
 F1: 0.565790 
 Precision: 0.590603 
 Recall: 0.542978
 IoU: 0.394496

We have finished training iteration 43
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_29_.pth
per-ex loss: 0.729100  [    1/   88]
per-ex loss: 0.566524  [    2/   88]
per-ex loss: 0.547420  [    3/   88]
per-ex loss: 0.604005  [    4/   88]
per-ex loss: 0.767967  [    5/   88]
per-ex loss: 0.728906  [    6/   88]
per-ex loss: 0.555026  [    7/   88]
per-ex loss: 0.431886  [    8/   88]
per-ex loss: 0.639977  [    9/   88]
per-ex loss: 0.562031  [   10/   88]
per-ex loss: 0.793265  [   11/   88]
per-ex loss: 0.769185  [   12/   88]
per-ex loss: 0.711025  [   13/   88]
per-ex loss: 0.732170  [   14/   88]
per-ex loss: 0.647633  [   15/   88]
per-ex loss: 0.680801  [   16/   88]
per-ex loss: 0.553442  [   17/   88]
per-ex loss: 0.744292  [   18/   88]
per-ex loss: 0.636952  [   19/   88]
per-ex loss: 0.535278  [   20/   88]
per-ex loss: 0.543983  [   21/   88]
per-ex loss: 0.755802  [   22/   88]
per-ex loss: 0.712181  [   23/   88]
per-ex loss: 0.666577  [   24/   88]
per-ex loss: 0.775354  [   25/   88]
per-ex loss: 0.700195  [   26/   88]
per-ex loss: 0.689668  [   27/   88]
per-ex loss: 0.498047  [   28/   88]
per-ex loss: 0.810835  [   29/   88]
per-ex loss: 0.647378  [   30/   88]
per-ex loss: 0.523286  [   31/   88]
per-ex loss: 0.709366  [   32/   88]
per-ex loss: 0.580354  [   33/   88]
per-ex loss: 0.564227  [   34/   88]
per-ex loss: 0.760994  [   35/   88]
per-ex loss: 0.567273  [   36/   88]
per-ex loss: 0.700525  [   37/   88]
per-ex loss: 0.505829  [   38/   88]
per-ex loss: 0.558886  [   39/   88]
per-ex loss: 0.699538  [   40/   88]
per-ex loss: 0.617019  [   41/   88]
per-ex loss: 0.578719  [   42/   88]
per-ex loss: 0.748615  [   43/   88]
per-ex loss: 0.593299  [   44/   88]
per-ex loss: 0.812591  [   45/   88]
per-ex loss: 0.772215  [   46/   88]
per-ex loss: 0.707015  [   47/   88]
per-ex loss: 0.579059  [   48/   88]
per-ex loss: 0.608292  [   49/   88]
per-ex loss: 0.618332  [   50/   88]
per-ex loss: 0.549584  [   51/   88]
per-ex loss: 0.741484  [   52/   88]
per-ex loss: 0.698719  [   53/   88]
per-ex loss: 0.816306  [   54/   88]
per-ex loss: 0.808233  [   55/   88]
per-ex loss: 0.582976  [   56/   88]
per-ex loss: 0.716525  [   57/   88]
per-ex loss: 0.567677  [   58/   88]
per-ex loss: 0.620012  [   59/   88]
per-ex loss: 0.581396  [   60/   88]
per-ex loss: 0.533359  [   61/   88]
per-ex loss: 0.555515  [   62/   88]
per-ex loss: 0.594800  [   63/   88]
per-ex loss: 0.583004  [   64/   88]
per-ex loss: 0.789989  [   65/   88]
per-ex loss: 0.603981  [   66/   88]
per-ex loss: 0.495684  [   67/   88]
per-ex loss: 0.770482  [   68/   88]
per-ex loss: 0.484440  [   69/   88]
per-ex loss: 0.691696  [   70/   88]
per-ex loss: 0.796965  [   71/   88]
per-ex loss: 0.751991  [   72/   88]
per-ex loss: 0.675098  [   73/   88]
per-ex loss: 0.756376  [   74/   88]
per-ex loss: 0.549801  [   75/   88]
per-ex loss: 0.791459  [   76/   88]
per-ex loss: 0.557128  [   77/   88]
per-ex loss: 0.596805  [   78/   88]
per-ex loss: 0.516257  [   79/   88]
per-ex loss: 0.725463  [   80/   88]
per-ex loss: 0.601256  [   81/   88]
per-ex loss: 0.759722  [   82/   88]
per-ex loss: 0.517924  [   83/   88]
per-ex loss: 0.576113  [   84/   88]
per-ex loss: 0.711829  [   85/   88]
per-ex loss: 0.557306  [   86/   88]
per-ex loss: 0.582010  [   87/   88]
per-ex loss: 0.606834  [   88/   88]
Train Error: Avg loss: 0.64725610
validation Error: 
 Avg loss: 0.68814068 
 F1: 0.496264 
 Precision: 0.585936 
 Recall: 0.430395
 IoU: 0.330020

test Error: 
 Avg loss: 0.64512433 
 F1: 0.562721 
 Precision: 0.641659 
 Recall: 0.501078
 IoU: 0.391518

We have finished training iteration 44
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_42_.pth
per-ex loss: 0.572933  [    1/   88]
per-ex loss: 0.587636  [    2/   88]
per-ex loss: 0.705705  [    3/   88]
per-ex loss: 0.529087  [    4/   88]
per-ex loss: 0.747159  [    5/   88]
per-ex loss: 0.784593  [    6/   88]
per-ex loss: 0.764040  [    7/   88]
per-ex loss: 0.543426  [    8/   88]
per-ex loss: 0.684909  [    9/   88]
per-ex loss: 0.701963  [   10/   88]
per-ex loss: 0.549538  [   11/   88]
per-ex loss: 0.625663  [   12/   88]
per-ex loss: 0.570907  [   13/   88]
per-ex loss: 0.729412  [   14/   88]
per-ex loss: 0.546286  [   15/   88]
per-ex loss: 0.686886  [   16/   88]
per-ex loss: 0.732670  [   17/   88]
per-ex loss: 0.712384  [   18/   88]
per-ex loss: 0.551629  [   19/   88]
per-ex loss: 0.589895  [   20/   88]
per-ex loss: 0.614219  [   21/   88]
per-ex loss: 0.635722  [   22/   88]
per-ex loss: 0.793995  [   23/   88]
per-ex loss: 0.647717  [   24/   88]
per-ex loss: 0.513171  [   25/   88]
per-ex loss: 0.572326  [   26/   88]
per-ex loss: 0.766930  [   27/   88]
per-ex loss: 0.720282  [   28/   88]
per-ex loss: 0.552746  [   29/   88]
per-ex loss: 0.589497  [   30/   88]
per-ex loss: 0.673283  [   31/   88]
per-ex loss: 0.578851  [   32/   88]
per-ex loss: 0.730960  [   33/   88]
per-ex loss: 0.707526  [   34/   88]
per-ex loss: 0.587099  [   35/   88]
per-ex loss: 0.762718  [   36/   88]
per-ex loss: 0.569815  [   37/   88]
per-ex loss: 0.516955  [   38/   88]
per-ex loss: 0.713625  [   39/   88]
per-ex loss: 0.569214  [   40/   88]
per-ex loss: 0.792310  [   41/   88]
per-ex loss: 0.583411  [   42/   88]
per-ex loss: 0.673723  [   43/   88]
per-ex loss: 0.754531  [   44/   88]
per-ex loss: 0.788496  [   45/   88]
per-ex loss: 0.562084  [   46/   88]
per-ex loss: 0.625904  [   47/   88]
per-ex loss: 0.747014  [   48/   88]
per-ex loss: 0.625990  [   49/   88]
per-ex loss: 0.528825  [   50/   88]
per-ex loss: 0.727192  [   51/   88]
per-ex loss: 0.548133  [   52/   88]
per-ex loss: 0.705986  [   53/   88]
per-ex loss: 0.584612  [   54/   88]
per-ex loss: 0.708690  [   55/   88]
per-ex loss: 0.600115  [   56/   88]
per-ex loss: 0.686322  [   57/   88]
per-ex loss: 0.503623  [   58/   88]
per-ex loss: 0.735509  [   59/   88]
per-ex loss: 0.621023  [   60/   88]
per-ex loss: 0.611062  [   61/   88]
per-ex loss: 0.759925  [   62/   88]
per-ex loss: 0.619683  [   63/   88]
per-ex loss: 0.690120  [   64/   88]
per-ex loss: 0.758805  [   65/   88]
per-ex loss: 0.565661  [   66/   88]
per-ex loss: 0.801616  [   67/   88]
per-ex loss: 0.563536  [   68/   88]
per-ex loss: 0.589891  [   69/   88]
per-ex loss: 0.539739  [   70/   88]
per-ex loss: 0.595095  [   71/   88]
per-ex loss: 0.770498  [   72/   88]
per-ex loss: 0.695591  [   73/   88]
per-ex loss: 0.814936  [   74/   88]
per-ex loss: 0.596083  [   75/   88]
per-ex loss: 0.485796  [   76/   88]
per-ex loss: 0.832315  [   77/   88]
per-ex loss: 0.799244  [   78/   88]
per-ex loss: 0.566087  [   79/   88]
per-ex loss: 0.826754  [   80/   88]
per-ex loss: 0.763841  [   81/   88]
per-ex loss: 0.824478  [   82/   88]
per-ex loss: 0.527975  [   83/   88]
per-ex loss: 0.525811  [   84/   88]
per-ex loss: 0.706739  [   85/   88]
per-ex loss: 0.590375  [   86/   88]
per-ex loss: 0.526540  [   87/   88]
per-ex loss: 0.558737  [   88/   88]
Train Error: Avg loss: 0.65163407
validation Error: 
 Avg loss: 0.69009680 
 F1: 0.494615 
 Precision: 0.546803 
 Recall: 0.451520
 IoU: 0.328564

test Error: 
 Avg loss: 0.64470324 
 F1: 0.566853 
 Precision: 0.607115 
 Recall: 0.531599
 IoU: 0.395530

We have finished training iteration 45
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_41_.pth
per-ex loss: 0.654904  [    1/   88]
per-ex loss: 0.497057  [    2/   88]
per-ex loss: 0.723379  [    3/   88]
per-ex loss: 0.759404  [    4/   88]
per-ex loss: 0.473121  [    5/   88]
per-ex loss: 0.756423  [    6/   88]
per-ex loss: 0.526778  [    7/   88]
per-ex loss: 0.528995  [    8/   88]
per-ex loss: 0.510718  [    9/   88]
per-ex loss: 0.803344  [   10/   88]
per-ex loss: 0.531416  [   11/   88]
per-ex loss: 0.663675  [   12/   88]
per-ex loss: 0.684749  [   13/   88]
per-ex loss: 0.680678  [   14/   88]
per-ex loss: 0.556010  [   15/   88]
per-ex loss: 0.692365  [   16/   88]
per-ex loss: 0.527342  [   17/   88]
per-ex loss: 0.562411  [   18/   88]
per-ex loss: 0.650854  [   19/   88]
per-ex loss: 0.737861  [   20/   88]
per-ex loss: 0.575450  [   21/   88]
per-ex loss: 0.572146  [   22/   88]
per-ex loss: 0.786316  [   23/   88]
per-ex loss: 0.815050  [   24/   88]
per-ex loss: 0.594538  [   25/   88]
per-ex loss: 0.555473  [   26/   88]
per-ex loss: 0.772053  [   27/   88]
per-ex loss: 0.554859  [   28/   88]
per-ex loss: 0.546860  [   29/   88]
per-ex loss: 0.598503  [   30/   88]
per-ex loss: 0.576984  [   31/   88]
per-ex loss: 0.780775  [   32/   88]
per-ex loss: 0.566436  [   33/   88]
per-ex loss: 0.531429  [   34/   88]
per-ex loss: 0.711451  [   35/   88]
per-ex loss: 0.525939  [   36/   88]
per-ex loss: 0.571218  [   37/   88]
per-ex loss: 0.687927  [   38/   88]
per-ex loss: 0.606336  [   39/   88]
per-ex loss: 0.729989  [   40/   88]
per-ex loss: 0.634378  [   41/   88]
per-ex loss: 0.746566  [   42/   88]
per-ex loss: 0.557939  [   43/   88]
per-ex loss: 0.751078  [   44/   88]
per-ex loss: 0.609899  [   45/   88]
per-ex loss: 0.634364  [   46/   88]
per-ex loss: 0.525017  [   47/   88]
per-ex loss: 0.770563  [   48/   88]
per-ex loss: 0.691352  [   49/   88]
per-ex loss: 0.617467  [   50/   88]
per-ex loss: 0.761103  [   51/   88]
per-ex loss: 0.519715  [   52/   88]
per-ex loss: 0.602674  [   53/   88]
per-ex loss: 0.560853  [   54/   88]
per-ex loss: 0.618451  [   55/   88]
per-ex loss: 0.595865  [   56/   88]
per-ex loss: 0.674574  [   57/   88]
per-ex loss: 0.573791  [   58/   88]
per-ex loss: 0.586785  [   59/   88]
per-ex loss: 0.478556  [   60/   88]
per-ex loss: 0.718704  [   61/   88]
per-ex loss: 0.614734  [   62/   88]
per-ex loss: 0.721961  [   63/   88]
per-ex loss: 0.569477  [   64/   88]
per-ex loss: 0.657109  [   65/   88]
per-ex loss: 0.795693  [   66/   88]
per-ex loss: 0.822482  [   67/   88]
per-ex loss: 0.756741  [   68/   88]
per-ex loss: 0.563135  [   69/   88]
per-ex loss: 0.699999  [   70/   88]
per-ex loss: 0.790960  [   71/   88]
per-ex loss: 0.751001  [   72/   88]
per-ex loss: 0.799991  [   73/   88]
per-ex loss: 0.531136  [   74/   88]
per-ex loss: 0.710778  [   75/   88]
per-ex loss: 0.624358  [   76/   88]
per-ex loss: 0.523429  [   77/   88]
per-ex loss: 0.747543  [   78/   88]
per-ex loss: 0.711474  [   79/   88]
per-ex loss: 0.801692  [   80/   88]
per-ex loss: 0.519188  [   81/   88]
per-ex loss: 0.766679  [   82/   88]
per-ex loss: 0.683254  [   83/   88]
per-ex loss: 0.694000  [   84/   88]
per-ex loss: 0.730354  [   85/   88]
per-ex loss: 0.597904  [   86/   88]
per-ex loss: 0.566920  [   87/   88]
per-ex loss: 0.597256  [   88/   88]
Train Error: Avg loss: 0.64500181
validation Error: 
 Avg loss: 0.68543336 
 F1: 0.501445 
 Precision: 0.588697 
 Recall: 0.436717
 IoU: 0.334619

test Error: 
 Avg loss: 0.64446697 
 F1: 0.568073 
 Precision: 0.625042 
 Recall: 0.520621
 IoU: 0.396719

We have finished training iteration 46
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_43_.pth
per-ex loss: 0.606085  [    1/   88]
per-ex loss: 0.765757  [    2/   88]
per-ex loss: 0.572532  [    3/   88]
per-ex loss: 0.689219  [    4/   88]
per-ex loss: 0.782709  [    5/   88]
per-ex loss: 0.609009  [    6/   88]
per-ex loss: 0.782094  [    7/   88]
per-ex loss: 0.533141  [    8/   88]
per-ex loss: 0.558361  [    9/   88]
per-ex loss: 0.540646  [   10/   88]
per-ex loss: 0.789465  [   11/   88]
per-ex loss: 0.741872  [   12/   88]
per-ex loss: 0.725670  [   13/   88]
per-ex loss: 0.483026  [   14/   88]
per-ex loss: 0.470030  [   15/   88]
per-ex loss: 0.746820  [   16/   88]
per-ex loss: 0.533834  [   17/   88]
per-ex loss: 0.576714  [   18/   88]
per-ex loss: 0.737864  [   19/   88]
per-ex loss: 0.749173  [   20/   88]
per-ex loss: 0.596902  [   21/   88]
per-ex loss: 0.509763  [   22/   88]
per-ex loss: 0.638663  [   23/   88]
per-ex loss: 0.559423  [   24/   88]
per-ex loss: 0.601864  [   25/   88]
per-ex loss: 0.550621  [   26/   88]
per-ex loss: 0.773449  [   27/   88]
per-ex loss: 0.593861  [   28/   88]
per-ex loss: 0.555272  [   29/   88]
per-ex loss: 0.583942  [   30/   88]
per-ex loss: 0.571328  [   31/   88]
per-ex loss: 0.823509  [   32/   88]
per-ex loss: 0.744565  [   33/   88]
per-ex loss: 0.641027  [   34/   88]
per-ex loss: 0.624874  [   35/   88]
per-ex loss: 0.612362  [   36/   88]
per-ex loss: 0.629372  [   37/   88]
per-ex loss: 0.691842  [   38/   88]
per-ex loss: 0.532612  [   39/   88]
per-ex loss: 0.607394  [   40/   88]
per-ex loss: 0.523140  [   41/   88]
per-ex loss: 0.606130  [   42/   88]
per-ex loss: 0.671428  [   43/   88]
per-ex loss: 0.725800  [   44/   88]
per-ex loss: 0.669685  [   45/   88]
per-ex loss: 0.539405  [   46/   88]
per-ex loss: 0.698470  [   47/   88]
per-ex loss: 0.576753  [   48/   88]
per-ex loss: 0.716063  [   49/   88]
per-ex loss: 0.511690  [   50/   88]
per-ex loss: 0.505918  [   51/   88]
per-ex loss: 0.706067  [   52/   88]
per-ex loss: 0.552305  [   53/   88]
per-ex loss: 0.742503  [   54/   88]
per-ex loss: 0.750721  [   55/   88]
per-ex loss: 0.756232  [   56/   88]
per-ex loss: 0.809478  [   57/   88]
per-ex loss: 0.696762  [   58/   88]
per-ex loss: 0.544714  [   59/   88]
per-ex loss: 0.731733  [   60/   88]
per-ex loss: 0.703588  [   61/   88]
per-ex loss: 0.719851  [   62/   88]
per-ex loss: 0.566917  [   63/   88]
per-ex loss: 0.594747  [   64/   88]
per-ex loss: 0.698548  [   65/   88]
per-ex loss: 0.552008  [   66/   88]
per-ex loss: 0.734418  [   67/   88]
per-ex loss: 0.682306  [   68/   88]
per-ex loss: 0.672065  [   69/   88]
per-ex loss: 0.469739  [   70/   88]
per-ex loss: 0.589288  [   71/   88]
per-ex loss: 0.806849  [   72/   88]
per-ex loss: 0.762816  [   73/   88]
per-ex loss: 0.819739  [   74/   88]
per-ex loss: 0.585834  [   75/   88]
per-ex loss: 0.578504  [   76/   88]
per-ex loss: 0.730891  [   77/   88]
per-ex loss: 0.707496  [   78/   88]
per-ex loss: 0.594225  [   79/   88]
per-ex loss: 0.560409  [   80/   88]
per-ex loss: 0.721953  [   81/   88]
per-ex loss: 0.524085  [   82/   88]
per-ex loss: 0.600031  [   83/   88]
per-ex loss: 0.711927  [   84/   88]
per-ex loss: 0.683023  [   85/   88]
per-ex loss: 0.790394  [   86/   88]
per-ex loss: 0.574629  [   87/   88]
per-ex loss: 0.550162  [   88/   88]
Train Error: Avg loss: 0.64497845
validation Error: 
 Avg loss: 0.70010799 
 F1: 0.481589 
 Precision: 0.595268 
 Recall: 0.404366
 IoU: 0.317166

test Error: 
 Avg loss: 0.65404814 
 F1: 0.548526 
 Precision: 0.675883 
 Recall: 0.461555
 IoU: 0.377910

We have finished training iteration 47
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_45_.pth
per-ex loss: 0.681626  [    1/   88]
per-ex loss: 0.709921  [    2/   88]
per-ex loss: 0.544432  [    3/   88]
per-ex loss: 0.442834  [    4/   88]
per-ex loss: 0.707185  [    5/   88]
per-ex loss: 0.591608  [    6/   88]
per-ex loss: 0.612260  [    7/   88]
per-ex loss: 0.565726  [    8/   88]
per-ex loss: 0.711516  [    9/   88]
per-ex loss: 0.760352  [   10/   88]
per-ex loss: 0.510887  [   11/   88]
per-ex loss: 0.669496  [   12/   88]
per-ex loss: 0.680940  [   13/   88]
per-ex loss: 0.741392  [   14/   88]
per-ex loss: 0.609495  [   15/   88]
per-ex loss: 0.669197  [   16/   88]
per-ex loss: 0.770115  [   17/   88]
per-ex loss: 0.764763  [   18/   88]
per-ex loss: 0.791666  [   19/   88]
per-ex loss: 0.594750  [   20/   88]
per-ex loss: 0.729249  [   21/   88]
per-ex loss: 0.523147  [   22/   88]
per-ex loss: 0.732083  [   23/   88]
per-ex loss: 0.548078  [   24/   88]
per-ex loss: 0.601334  [   25/   88]
per-ex loss: 0.561571  [   26/   88]
per-ex loss: 0.546910  [   27/   88]
per-ex loss: 0.741750  [   28/   88]
per-ex loss: 0.763551  [   29/   88]
per-ex loss: 0.583534  [   30/   88]
per-ex loss: 0.571923  [   31/   88]
per-ex loss: 0.720006  [   32/   88]
per-ex loss: 0.479111  [   33/   88]
per-ex loss: 0.785016  [   34/   88]
per-ex loss: 0.763372  [   35/   88]
per-ex loss: 0.573653  [   36/   88]
per-ex loss: 0.575650  [   37/   88]
per-ex loss: 0.535215  [   38/   88]
per-ex loss: 0.673313  [   39/   88]
per-ex loss: 0.540248  [   40/   88]
per-ex loss: 0.705702  [   41/   88]
per-ex loss: 0.667004  [   42/   88]
per-ex loss: 0.729876  [   43/   88]
per-ex loss: 0.697810  [   44/   88]
per-ex loss: 0.702781  [   45/   88]
per-ex loss: 0.597298  [   46/   88]
per-ex loss: 0.558698  [   47/   88]
per-ex loss: 0.525568  [   48/   88]
per-ex loss: 0.763843  [   49/   88]
per-ex loss: 0.571133  [   50/   88]
per-ex loss: 0.507081  [   51/   88]
per-ex loss: 0.620437  [   52/   88]
per-ex loss: 0.750488  [   53/   88]
per-ex loss: 0.723802  [   54/   88]
per-ex loss: 0.807036  [   55/   88]
per-ex loss: 0.550359  [   56/   88]
per-ex loss: 0.783955  [   57/   88]
per-ex loss: 0.732261  [   58/   88]
per-ex loss: 0.629274  [   59/   88]
per-ex loss: 0.581396  [   60/   88]
per-ex loss: 0.649472  [   61/   88]
per-ex loss: 0.557442  [   62/   88]
per-ex loss: 0.637268  [   63/   88]
per-ex loss: 0.742259  [   64/   88]
per-ex loss: 0.810961  [   65/   88]
per-ex loss: 0.553946  [   66/   88]
per-ex loss: 0.513491  [   67/   88]
per-ex loss: 0.614228  [   68/   88]
per-ex loss: 0.589278  [   69/   88]
per-ex loss: 0.558653  [   70/   88]
per-ex loss: 0.561957  [   71/   88]
per-ex loss: 0.572180  [   72/   88]
per-ex loss: 0.745785  [   73/   88]
per-ex loss: 0.767742  [   74/   88]
per-ex loss: 0.804865  [   75/   88]
per-ex loss: 0.519216  [   76/   88]
per-ex loss: 0.735471  [   77/   88]
per-ex loss: 0.559796  [   78/   88]
per-ex loss: 0.561059  [   79/   88]
per-ex loss: 0.519366  [   80/   88]
per-ex loss: 0.757053  [   81/   88]
per-ex loss: 0.551371  [   82/   88]
per-ex loss: 0.564071  [   83/   88]
per-ex loss: 0.805473  [   84/   88]
per-ex loss: 0.670805  [   85/   88]
per-ex loss: 0.642484  [   86/   88]
per-ex loss: 0.558980  [   87/   88]
per-ex loss: 0.720231  [   88/   88]
Train Error: Avg loss: 0.64465427
validation Error: 
 Avg loss: 0.69298636 
 F1: 0.490475 
 Precision: 0.509773 
 Recall: 0.472585
 IoU: 0.324920

test Error: 
 Avg loss: 0.65298355 
 F1: 0.559402 
 Precision: 0.554142 
 Recall: 0.564763
 IoU: 0.388312

We have finished training iteration 48
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_38_.pth
per-ex loss: 0.764060  [    1/   88]
per-ex loss: 0.518060  [    2/   88]
per-ex loss: 0.607702  [    3/   88]
per-ex loss: 0.585675  [    4/   88]
per-ex loss: 0.590893  [    5/   88]
per-ex loss: 0.530224  [    6/   88]
per-ex loss: 0.516073  [    7/   88]
per-ex loss: 0.607260  [    8/   88]
per-ex loss: 0.792856  [    9/   88]
per-ex loss: 0.608838  [   10/   88]
per-ex loss: 0.662359  [   11/   88]
per-ex loss: 0.729380  [   12/   88]
per-ex loss: 0.549250  [   13/   88]
per-ex loss: 0.815449  [   14/   88]
per-ex loss: 0.701433  [   15/   88]
per-ex loss: 0.543301  [   16/   88]
per-ex loss: 0.516686  [   17/   88]
per-ex loss: 0.474060  [   18/   88]
per-ex loss: 0.696924  [   19/   88]
per-ex loss: 0.556387  [   20/   88]
per-ex loss: 0.694318  [   21/   88]
per-ex loss: 0.576977  [   22/   88]
per-ex loss: 0.610679  [   23/   88]
per-ex loss: 0.763740  [   24/   88]
per-ex loss: 0.806010  [   25/   88]
per-ex loss: 0.509592  [   26/   88]
per-ex loss: 0.628465  [   27/   88]
per-ex loss: 0.648518  [   28/   88]
per-ex loss: 0.549447  [   29/   88]
per-ex loss: 0.746826  [   30/   88]
per-ex loss: 0.512524  [   31/   88]
per-ex loss: 0.744457  [   32/   88]
per-ex loss: 0.638454  [   33/   88]
per-ex loss: 0.574336  [   34/   88]
per-ex loss: 0.531666  [   35/   88]
per-ex loss: 0.701320  [   36/   88]
per-ex loss: 0.482810  [   37/   88]
per-ex loss: 0.745158  [   38/   88]
per-ex loss: 0.693767  [   39/   88]
per-ex loss: 0.563214  [   40/   88]
per-ex loss: 0.609395  [   41/   88]
per-ex loss: 0.513931  [   42/   88]
per-ex loss: 0.552772  [   43/   88]
per-ex loss: 0.549480  [   44/   88]
per-ex loss: 0.527113  [   45/   88]
per-ex loss: 0.581236  [   46/   88]
per-ex loss: 0.818638  [   47/   88]
per-ex loss: 0.756232  [   48/   88]
per-ex loss: 0.804771  [   49/   88]
per-ex loss: 0.740709  [   50/   88]
per-ex loss: 0.606819  [   51/   88]
per-ex loss: 0.714009  [   52/   88]
per-ex loss: 0.636531  [   53/   88]
per-ex loss: 0.714806  [   54/   88]
per-ex loss: 0.746066  [   55/   88]
per-ex loss: 0.539524  [   56/   88]
per-ex loss: 0.593924  [   57/   88]
per-ex loss: 0.777444  [   58/   88]
per-ex loss: 0.633599  [   59/   88]
per-ex loss: 0.753991  [   60/   88]
per-ex loss: 0.626333  [   61/   88]
per-ex loss: 0.644884  [   62/   88]
per-ex loss: 0.587169  [   63/   88]
per-ex loss: 0.529783  [   64/   88]
per-ex loss: 0.731671  [   65/   88]
per-ex loss: 0.634122  [   66/   88]
per-ex loss: 0.608380  [   67/   88]
per-ex loss: 0.813197  [   68/   88]
per-ex loss: 0.692279  [   69/   88]
per-ex loss: 0.781460  [   70/   88]
per-ex loss: 0.777283  [   71/   88]
per-ex loss: 0.815619  [   72/   88]
per-ex loss: 0.722073  [   73/   88]
per-ex loss: 0.714981  [   74/   88]
per-ex loss: 0.565268  [   75/   88]
per-ex loss: 0.703722  [   76/   88]
per-ex loss: 0.544429  [   77/   88]
per-ex loss: 0.593282  [   78/   88]
per-ex loss: 0.740919  [   79/   88]
per-ex loss: 0.734928  [   80/   88]
per-ex loss: 0.675777  [   81/   88]
per-ex loss: 0.626382  [   82/   88]
per-ex loss: 0.501388  [   83/   88]
per-ex loss: 0.762399  [   84/   88]
per-ex loss: 0.746913  [   85/   88]
per-ex loss: 0.713501  [   86/   88]
per-ex loss: 0.691269  [   87/   88]
per-ex loss: 0.604292  [   88/   88]
Train Error: Avg loss: 0.64936183
validation Error: 
 Avg loss: 0.70085803 
 F1: 0.476275 
 Precision: 0.596986 
 Recall: 0.396169
 IoU: 0.312573

test Error: 
 Avg loss: 0.66423690 
 F1: 0.540072 
 Precision: 0.654014 
 Recall: 0.459941
 IoU: 0.369931

We have finished training iteration 49
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_47_.pth
per-ex loss: 0.807101  [    1/   88]
per-ex loss: 0.611260  [    2/   88]
per-ex loss: 0.572419  [    3/   88]
per-ex loss: 0.534798  [    4/   88]
per-ex loss: 0.595186  [    5/   88]
per-ex loss: 0.816414  [    6/   88]
per-ex loss: 0.508564  [    7/   88]
per-ex loss: 0.764766  [    8/   88]
per-ex loss: 0.713507  [    9/   88]
per-ex loss: 0.723651  [   10/   88]
per-ex loss: 0.609253  [   11/   88]
per-ex loss: 0.712905  [   12/   88]
per-ex loss: 0.566075  [   13/   88]
per-ex loss: 0.685527  [   14/   88]
per-ex loss: 0.803426  [   15/   88]
per-ex loss: 0.552944  [   16/   88]
per-ex loss: 0.619474  [   17/   88]
per-ex loss: 0.763071  [   18/   88]
per-ex loss: 0.539263  [   19/   88]
per-ex loss: 0.753850  [   20/   88]
per-ex loss: 0.564889  [   21/   88]
per-ex loss: 0.551525  [   22/   88]
per-ex loss: 0.594687  [   23/   88]
per-ex loss: 0.520240  [   24/   88]
per-ex loss: 0.571541  [   25/   88]
per-ex loss: 0.749934  [   26/   88]
per-ex loss: 0.675955  [   27/   88]
per-ex loss: 0.561386  [   28/   88]
per-ex loss: 0.546201  [   29/   88]
per-ex loss: 0.540957  [   30/   88]
per-ex loss: 0.799404  [   31/   88]
per-ex loss: 0.500989  [   32/   88]
per-ex loss: 0.730939  [   33/   88]
per-ex loss: 0.552739  [   34/   88]
per-ex loss: 0.824299  [   35/   88]
per-ex loss: 0.764567  [   36/   88]
per-ex loss: 0.726351  [   37/   88]
per-ex loss: 0.563819  [   38/   88]
per-ex loss: 0.732473  [   39/   88]
per-ex loss: 0.530696  [   40/   88]
per-ex loss: 0.470531  [   41/   88]
per-ex loss: 0.490869  [   42/   88]
per-ex loss: 0.559709  [   43/   88]
per-ex loss: 0.685587  [   44/   88]
per-ex loss: 0.710226  [   45/   88]
per-ex loss: 0.734529  [   46/   88]
per-ex loss: 0.735041  [   47/   88]
per-ex loss: 0.604105  [   48/   88]
per-ex loss: 0.687527  [   49/   88]
per-ex loss: 0.608093  [   50/   88]
per-ex loss: 0.515177  [   51/   88]
per-ex loss: 0.608248  [   52/   88]
per-ex loss: 0.581465  [   53/   88]
per-ex loss: 0.527150  [   54/   88]
per-ex loss: 0.641847  [   55/   88]
per-ex loss: 0.756904  [   56/   88]
per-ex loss: 0.748490  [   57/   88]
per-ex loss: 0.700892  [   58/   88]
per-ex loss: 0.581496  [   59/   88]
per-ex loss: 0.718255  [   60/   88]
per-ex loss: 0.760149  [   61/   88]
per-ex loss: 0.703831  [   62/   88]
per-ex loss: 0.512368  [   63/   88]
per-ex loss: 0.586280  [   64/   88]
per-ex loss: 0.755037  [   65/   88]
per-ex loss: 0.792424  [   66/   88]
per-ex loss: 0.649066  [   67/   88]
per-ex loss: 0.784693  [   68/   88]
per-ex loss: 0.625587  [   69/   88]
per-ex loss: 0.675376  [   70/   88]
per-ex loss: 0.745315  [   71/   88]
per-ex loss: 0.521073  [   72/   88]
per-ex loss: 0.592241  [   73/   88]
per-ex loss: 0.558303  [   74/   88]
per-ex loss: 0.701513  [   75/   88]
per-ex loss: 0.752125  [   76/   88]
per-ex loss: 0.449330  [   77/   88]
per-ex loss: 0.594721  [   78/   88]
per-ex loss: 0.553026  [   79/   88]
per-ex loss: 0.530578  [   80/   88]
per-ex loss: 0.606385  [   81/   88]
per-ex loss: 0.786079  [   82/   88]
per-ex loss: 0.547384  [   83/   88]
per-ex loss: 0.767785  [   84/   88]
per-ex loss: 0.685801  [   85/   88]
per-ex loss: 0.732781  [   86/   88]
per-ex loss: 0.581507  [   87/   88]
per-ex loss: 0.602243  [   88/   88]
Train Error: Avg loss: 0.64404755
validation Error: 
 Avg loss: 0.69532362 
 F1: 0.492143 
 Precision: 0.537363 
 Recall: 0.453943
 IoU: 0.326386

test Error: 
 Avg loss: 0.65158833 
 F1: 0.556035 
 Precision: 0.592638 
 Recall: 0.523690
 IoU: 0.385075

We have finished training iteration 50
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_48_.pth
per-ex loss: 0.689777  [    1/   88]
per-ex loss: 0.556441  [    2/   88]
per-ex loss: 0.492089  [    3/   88]
per-ex loss: 0.507523  [    4/   88]
per-ex loss: 0.514709  [    5/   88]
per-ex loss: 0.754414  [    6/   88]
per-ex loss: 0.563668  [    7/   88]
per-ex loss: 0.566390  [    8/   88]
per-ex loss: 0.564533  [    9/   88]
per-ex loss: 0.838975  [   10/   88]
per-ex loss: 0.628477  [   11/   88]
per-ex loss: 0.544904  [   12/   88]
per-ex loss: 0.522138  [   13/   88]
per-ex loss: 0.557710  [   14/   88]
per-ex loss: 0.656532  [   15/   88]
per-ex loss: 0.768797  [   16/   88]
per-ex loss: 0.721835  [   17/   88]
per-ex loss: 0.573717  [   18/   88]
per-ex loss: 0.594151  [   19/   88]
per-ex loss: 0.566654  [   20/   88]
per-ex loss: 0.563399  [   21/   88]
per-ex loss: 0.742431  [   22/   88]
per-ex loss: 0.734291  [   23/   88]
per-ex loss: 0.511818  [   24/   88]
per-ex loss: 0.752814  [   25/   88]
per-ex loss: 0.761866  [   26/   88]
per-ex loss: 0.606571  [   27/   88]
per-ex loss: 0.753625  [   28/   88]
per-ex loss: 0.803201  [   29/   88]
per-ex loss: 0.610524  [   30/   88]
per-ex loss: 0.700819  [   31/   88]
per-ex loss: 0.604821  [   32/   88]
per-ex loss: 0.720226  [   33/   88]
per-ex loss: 0.605679  [   34/   88]
per-ex loss: 0.812226  [   35/   88]
per-ex loss: 0.754458  [   36/   88]
per-ex loss: 0.676690  [   37/   88]
per-ex loss: 0.714980  [   38/   88]
per-ex loss: 0.741773  [   39/   88]
per-ex loss: 0.487588  [   40/   88]
per-ex loss: 0.596274  [   41/   88]
per-ex loss: 0.625826  [   42/   88]
per-ex loss: 0.734750  [   43/   88]
per-ex loss: 0.520379  [   44/   88]
per-ex loss: 0.519358  [   45/   88]
per-ex loss: 0.786648  [   46/   88]
per-ex loss: 0.808893  [   47/   88]
per-ex loss: 0.509709  [   48/   88]
per-ex loss: 0.588197  [   49/   88]
per-ex loss: 0.549675  [   50/   88]
per-ex loss: 0.730764  [   51/   88]
per-ex loss: 0.555634  [   52/   88]
per-ex loss: 0.606310  [   53/   88]
per-ex loss: 0.817010  [   54/   88]
per-ex loss: 0.535341  [   55/   88]
per-ex loss: 0.533456  [   56/   88]
per-ex loss: 0.538078  [   57/   88]
per-ex loss: 0.781338  [   58/   88]
per-ex loss: 0.524188  [   59/   88]
per-ex loss: 0.535005  [   60/   88]
per-ex loss: 0.716223  [   61/   88]
per-ex loss: 0.689029  [   62/   88]
per-ex loss: 0.590235  [   63/   88]
per-ex loss: 0.660298  [   64/   88]
per-ex loss: 0.707728  [   65/   88]
per-ex loss: 0.607576  [   66/   88]
per-ex loss: 0.690471  [   67/   88]
per-ex loss: 0.479436  [   68/   88]
per-ex loss: 0.776389  [   69/   88]
per-ex loss: 0.700669  [   70/   88]
per-ex loss: 0.557439  [   71/   88]
per-ex loss: 0.530290  [   72/   88]
per-ex loss: 0.628646  [   73/   88]
per-ex loss: 0.715378  [   74/   88]
per-ex loss: 0.582925  [   75/   88]
per-ex loss: 0.783664  [   76/   88]
per-ex loss: 0.767502  [   77/   88]
per-ex loss: 0.559040  [   78/   88]
per-ex loss: 0.720082  [   79/   88]
per-ex loss: 0.644796  [   80/   88]
per-ex loss: 0.819259  [   81/   88]
per-ex loss: 0.702790  [   82/   88]
per-ex loss: 0.724675  [   83/   88]
per-ex loss: 0.705601  [   84/   88]
per-ex loss: 0.578575  [   85/   88]
per-ex loss: 0.559238  [   86/   88]
per-ex loss: 0.739658  [   87/   88]
per-ex loss: 0.582623  [   88/   88]
Train Error: Avg loss: 0.64500342
validation Error: 
 Avg loss: 0.68723554 
 F1: 0.494784 
 Precision: 0.530650 
 Recall: 0.463459
 IoU: 0.328713

test Error: 
 Avg loss: 0.64630958 
 F1: 0.563439 
 Precision: 0.589372 
 Recall: 0.539692
 IoU: 0.392214

We have finished training iteration 51
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_49_.pth
per-ex loss: 0.625655  [    1/   88]
per-ex loss: 0.548856  [    2/   88]
per-ex loss: 0.617776  [    3/   88]
per-ex loss: 0.470013  [    4/   88]
per-ex loss: 0.799665  [    5/   88]
per-ex loss: 0.683664  [    6/   88]
per-ex loss: 0.519776  [    7/   88]
per-ex loss: 0.569241  [    8/   88]
per-ex loss: 0.748442  [    9/   88]
per-ex loss: 0.625516  [   10/   88]
per-ex loss: 0.647049  [   11/   88]
per-ex loss: 0.670992  [   12/   88]
per-ex loss: 0.716297  [   13/   88]
per-ex loss: 0.784869  [   14/   88]
per-ex loss: 0.728689  [   15/   88]
per-ex loss: 0.692229  [   16/   88]
per-ex loss: 0.718271  [   17/   88]
per-ex loss: 0.602139  [   18/   88]
per-ex loss: 0.789732  [   19/   88]
per-ex loss: 0.552813  [   20/   88]
per-ex loss: 0.732755  [   21/   88]
per-ex loss: 0.562601  [   22/   88]
per-ex loss: 0.671015  [   23/   88]
per-ex loss: 0.551185  [   24/   88]
per-ex loss: 0.735049  [   25/   88]
per-ex loss: 0.610152  [   26/   88]
per-ex loss: 0.757272  [   27/   88]
per-ex loss: 0.515852  [   28/   88]
per-ex loss: 0.578773  [   29/   88]
per-ex loss: 0.764293  [   30/   88]
per-ex loss: 0.532811  [   31/   88]
per-ex loss: 0.609586  [   32/   88]
per-ex loss: 0.520015  [   33/   88]
per-ex loss: 0.565690  [   34/   88]
per-ex loss: 0.542722  [   35/   88]
per-ex loss: 0.721335  [   36/   88]
per-ex loss: 0.700746  [   37/   88]
per-ex loss: 0.600175  [   38/   88]
per-ex loss: 0.598572  [   39/   88]
per-ex loss: 0.751467  [   40/   88]
per-ex loss: 0.753715  [   41/   88]
per-ex loss: 0.522444  [   42/   88]
per-ex loss: 0.557871  [   43/   88]
per-ex loss: 0.739991  [   44/   88]
per-ex loss: 0.779864  [   45/   88]
per-ex loss: 0.730110  [   46/   88]
per-ex loss: 0.680637  [   47/   88]
per-ex loss: 0.571046  [   48/   88]
per-ex loss: 0.543474  [   49/   88]
per-ex loss: 0.482554  [   50/   88]
per-ex loss: 0.533295  [   51/   88]
per-ex loss: 0.721437  [   52/   88]
per-ex loss: 0.708279  [   53/   88]
per-ex loss: 0.736106  [   54/   88]
per-ex loss: 0.746019  [   55/   88]
per-ex loss: 0.569408  [   56/   88]
per-ex loss: 0.748377  [   57/   88]
per-ex loss: 0.565380  [   58/   88]
per-ex loss: 0.711243  [   59/   88]
per-ex loss: 0.508497  [   60/   88]
per-ex loss: 0.565517  [   61/   88]
per-ex loss: 0.555089  [   62/   88]
per-ex loss: 0.829907  [   63/   88]
per-ex loss: 0.806645  [   64/   88]
per-ex loss: 0.618155  [   65/   88]
per-ex loss: 0.693587  [   66/   88]
per-ex loss: 0.494415  [   67/   88]
per-ex loss: 0.533192  [   68/   88]
per-ex loss: 0.693158  [   69/   88]
per-ex loss: 0.599457  [   70/   88]
per-ex loss: 0.766189  [   71/   88]
per-ex loss: 0.758761  [   72/   88]
per-ex loss: 0.755198  [   73/   88]
per-ex loss: 0.807905  [   74/   88]
per-ex loss: 0.796833  [   75/   88]
per-ex loss: 0.553769  [   76/   88]
per-ex loss: 0.566805  [   77/   88]
per-ex loss: 0.517297  [   78/   88]
per-ex loss: 0.583728  [   79/   88]
per-ex loss: 0.651533  [   80/   88]
per-ex loss: 0.602572  [   81/   88]
per-ex loss: 0.508333  [   82/   88]
per-ex loss: 0.569725  [   83/   88]
per-ex loss: 0.790664  [   84/   88]
per-ex loss: 0.509743  [   85/   88]
per-ex loss: 0.611987  [   86/   88]
per-ex loss: 0.559394  [   87/   88]
per-ex loss: 0.636516  [   88/   88]
Train Error: Avg loss: 0.64294963
validation Error: 
 Avg loss: 0.69527441 
 F1: 0.484791 
 Precision: 0.638928 
 Recall: 0.390569
 IoU: 0.319950

test Error: 
 Avg loss: 0.65503479 
 F1: 0.551566 
 Precision: 0.685504 
 Recall: 0.461412
 IoU: 0.380801

We have finished training iteration 52
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_50_.pth
per-ex loss: 0.564244  [    1/   88]
per-ex loss: 0.663014  [    2/   88]
per-ex loss: 0.548260  [    3/   88]
per-ex loss: 0.576558  [    4/   88]
per-ex loss: 0.591192  [    5/   88]
per-ex loss: 0.531336  [    6/   88]
per-ex loss: 0.777066  [    7/   88]
per-ex loss: 0.754826  [    8/   88]
per-ex loss: 0.716061  [    9/   88]
per-ex loss: 0.738145  [   10/   88]
per-ex loss: 0.677606  [   11/   88]
per-ex loss: 0.542424  [   12/   88]
per-ex loss: 0.668361  [   13/   88]
per-ex loss: 0.761993  [   14/   88]
per-ex loss: 0.500176  [   15/   88]
per-ex loss: 0.520296  [   16/   88]
per-ex loss: 0.515567  [   17/   88]
per-ex loss: 0.573738  [   18/   88]
per-ex loss: 0.706658  [   19/   88]
per-ex loss: 0.794726  [   20/   88]
per-ex loss: 0.706409  [   21/   88]
per-ex loss: 0.636174  [   22/   88]
per-ex loss: 0.604127  [   23/   88]
per-ex loss: 0.744063  [   24/   88]
per-ex loss: 0.784390  [   25/   88]
per-ex loss: 0.746093  [   26/   88]
per-ex loss: 0.509359  [   27/   88]
per-ex loss: 0.703769  [   28/   88]
per-ex loss: 0.717594  [   29/   88]
per-ex loss: 0.578772  [   30/   88]
per-ex loss: 0.816108  [   31/   88]
per-ex loss: 0.664565  [   32/   88]
per-ex loss: 0.589288  [   33/   88]
per-ex loss: 0.576393  [   34/   88]
per-ex loss: 0.739787  [   35/   88]
per-ex loss: 0.630127  [   36/   88]
per-ex loss: 0.579745  [   37/   88]
per-ex loss: 0.585497  [   38/   88]
per-ex loss: 0.528895  [   39/   88]
per-ex loss: 0.667575  [   40/   88]
per-ex loss: 0.725323  [   41/   88]
per-ex loss: 0.722468  [   42/   88]
per-ex loss: 0.702919  [   43/   88]
per-ex loss: 0.761949  [   44/   88]
per-ex loss: 0.687488  [   45/   88]
per-ex loss: 0.555653  [   46/   88]
per-ex loss: 0.774872  [   47/   88]
per-ex loss: 0.695234  [   48/   88]
per-ex loss: 0.738612  [   49/   88]
per-ex loss: 0.616246  [   50/   88]
per-ex loss: 0.793242  [   51/   88]
per-ex loss: 0.502847  [   52/   88]
per-ex loss: 0.521212  [   53/   88]
per-ex loss: 0.736916  [   54/   88]
per-ex loss: 0.529361  [   55/   88]
per-ex loss: 0.563730  [   56/   88]
per-ex loss: 0.450878  [   57/   88]
per-ex loss: 0.589649  [   58/   88]
per-ex loss: 0.527509  [   59/   88]
per-ex loss: 0.574338  [   60/   88]
per-ex loss: 0.501410  [   61/   88]
per-ex loss: 0.712206  [   62/   88]
per-ex loss: 0.506326  [   63/   88]
per-ex loss: 0.590228  [   64/   88]
per-ex loss: 0.828905  [   65/   88]
per-ex loss: 0.507194  [   66/   88]
per-ex loss: 0.474746  [   67/   88]
per-ex loss: 0.552255  [   68/   88]
per-ex loss: 0.706677  [   69/   88]
per-ex loss: 0.734423  [   70/   88]
per-ex loss: 0.558261  [   71/   88]
per-ex loss: 0.534020  [   72/   88]
per-ex loss: 0.552399  [   73/   88]
per-ex loss: 0.703272  [   74/   88]
per-ex loss: 0.761245  [   75/   88]
per-ex loss: 0.612238  [   76/   88]
per-ex loss: 0.585702  [   77/   88]
per-ex loss: 0.820508  [   78/   88]
per-ex loss: 0.554995  [   79/   88]
per-ex loss: 0.638104  [   80/   88]
per-ex loss: 0.668085  [   81/   88]
per-ex loss: 0.797601  [   82/   88]
per-ex loss: 0.590246  [   83/   88]
per-ex loss: 0.631382  [   84/   88]
per-ex loss: 0.548308  [   85/   88]
per-ex loss: 0.755408  [   86/   88]
per-ex loss: 0.716522  [   87/   88]
per-ex loss: 0.645210  [   88/   88]
Train Error: Avg loss: 0.64078752
validation Error: 
 Avg loss: 0.69147555 
 F1: 0.493274 
 Precision: 0.494497 
 Recall: 0.492057
 IoU: 0.327381

test Error: 
 Avg loss: 0.64635865 
 F1: 0.563296 
 Precision: 0.564848 
 Recall: 0.561752
 IoU: 0.392075

We have finished training iteration 53
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_44_.pth
per-ex loss: 0.588154  [    1/   88]
per-ex loss: 0.755350  [    2/   88]
per-ex loss: 0.652245  [    3/   88]
per-ex loss: 0.608585  [    4/   88]
per-ex loss: 0.755272  [    5/   88]
per-ex loss: 0.656725  [    6/   88]
per-ex loss: 0.782091  [    7/   88]
per-ex loss: 0.608187  [    8/   88]
per-ex loss: 0.748848  [    9/   88]
per-ex loss: 0.647715  [   10/   88]
per-ex loss: 0.552796  [   11/   88]
per-ex loss: 0.521818  [   12/   88]
per-ex loss: 0.770335  [   13/   88]
per-ex loss: 0.529292  [   14/   88]
per-ex loss: 0.579490  [   15/   88]
per-ex loss: 0.801398  [   16/   88]
per-ex loss: 0.536428  [   17/   88]
per-ex loss: 0.553884  [   18/   88]
per-ex loss: 0.512902  [   19/   88]
per-ex loss: 0.735341  [   20/   88]
per-ex loss: 0.564685  [   21/   88]
per-ex loss: 0.759334  [   22/   88]
per-ex loss: 0.801347  [   23/   88]
per-ex loss: 0.549112  [   24/   88]
per-ex loss: 0.564846  [   25/   88]
per-ex loss: 0.777893  [   26/   88]
per-ex loss: 0.571535  [   27/   88]
per-ex loss: 0.566943  [   28/   88]
per-ex loss: 0.824929  [   29/   88]
per-ex loss: 0.746136  [   30/   88]
per-ex loss: 0.719183  [   31/   88]
per-ex loss: 0.744418  [   32/   88]
per-ex loss: 0.589604  [   33/   88]
per-ex loss: 0.714847  [   34/   88]
per-ex loss: 0.703511  [   35/   88]
per-ex loss: 0.550031  [   36/   88]
per-ex loss: 0.658633  [   37/   88]
per-ex loss: 0.553189  [   38/   88]
per-ex loss: 0.566071  [   39/   88]
per-ex loss: 0.742176  [   40/   88]
per-ex loss: 0.504691  [   41/   88]
per-ex loss: 0.808483  [   42/   88]
per-ex loss: 0.490833  [   43/   88]
per-ex loss: 0.706657  [   44/   88]
per-ex loss: 0.715688  [   45/   88]
per-ex loss: 0.425306  [   46/   88]
per-ex loss: 0.657458  [   47/   88]
per-ex loss: 0.556645  [   48/   88]
per-ex loss: 0.596406  [   49/   88]
per-ex loss: 0.602166  [   50/   88]
per-ex loss: 0.607238  [   51/   88]
per-ex loss: 0.526417  [   52/   88]
per-ex loss: 0.702833  [   53/   88]
per-ex loss: 0.741321  [   54/   88]
per-ex loss: 0.719845  [   55/   88]
per-ex loss: 0.631388  [   56/   88]
per-ex loss: 0.753359  [   57/   88]
per-ex loss: 0.744310  [   58/   88]
per-ex loss: 0.752776  [   59/   88]
per-ex loss: 0.689017  [   60/   88]
per-ex loss: 0.708540  [   61/   88]
per-ex loss: 0.510933  [   62/   88]
per-ex loss: 0.729238  [   63/   88]
per-ex loss: 0.603450  [   64/   88]
per-ex loss: 0.777444  [   65/   88]
per-ex loss: 0.655155  [   66/   88]
per-ex loss: 0.552856  [   67/   88]
per-ex loss: 0.604579  [   68/   88]
per-ex loss: 0.543833  [   69/   88]
per-ex loss: 0.560358  [   70/   88]
per-ex loss: 0.526014  [   71/   88]
per-ex loss: 0.562840  [   72/   88]
per-ex loss: 0.496493  [   73/   88]
per-ex loss: 0.613013  [   74/   88]
per-ex loss: 0.699276  [   75/   88]
per-ex loss: 0.509687  [   76/   88]
per-ex loss: 0.580958  [   77/   88]
per-ex loss: 0.668250  [   78/   88]
per-ex loss: 0.665142  [   79/   88]
per-ex loss: 0.529125  [   80/   88]
per-ex loss: 0.502815  [   81/   88]
per-ex loss: 0.579991  [   82/   88]
per-ex loss: 0.724142  [   83/   88]
per-ex loss: 0.581955  [   84/   88]
per-ex loss: 0.510902  [   85/   88]
per-ex loss: 0.807163  [   86/   88]
per-ex loss: 0.554883  [   87/   88]
per-ex loss: 0.701471  [   88/   88]
Train Error: Avg loss: 0.63816614
validation Error: 
 Avg loss: 0.68881375 
 F1: 0.498538 
 Precision: 0.608933 
 Recall: 0.422027
 IoU: 0.332035

test Error: 
 Avg loss: 0.64832866 
 F1: 0.561994 
 Precision: 0.650458 
 Recall: 0.494712
 IoU: 0.390815

We have finished training iteration 54
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_52_.pth
per-ex loss: 0.750707  [    1/   88]
per-ex loss: 0.540167  [    2/   88]
per-ex loss: 0.627526  [    3/   88]
per-ex loss: 0.560753  [    4/   88]
per-ex loss: 0.575519  [    5/   88]
per-ex loss: 0.792581  [    6/   88]
per-ex loss: 0.607640  [    7/   88]
per-ex loss: 0.615844  [    8/   88]
per-ex loss: 0.685256  [    9/   88]
per-ex loss: 0.736939  [   10/   88]
per-ex loss: 0.575941  [   11/   88]
per-ex loss: 0.531800  [   12/   88]
per-ex loss: 0.468514  [   13/   88]
per-ex loss: 0.533368  [   14/   88]
per-ex loss: 0.786490  [   15/   88]
per-ex loss: 0.505136  [   16/   88]
per-ex loss: 0.554882  [   17/   88]
per-ex loss: 0.573818  [   18/   88]
per-ex loss: 0.496905  [   19/   88]
per-ex loss: 0.748903  [   20/   88]
per-ex loss: 0.779209  [   21/   88]
per-ex loss: 0.696083  [   22/   88]
per-ex loss: 0.662167  [   23/   88]
per-ex loss: 0.741627  [   24/   88]
per-ex loss: 0.723033  [   25/   88]
per-ex loss: 0.654569  [   26/   88]
per-ex loss: 0.719278  [   27/   88]
per-ex loss: 0.577966  [   28/   88]
per-ex loss: 0.729524  [   29/   88]
per-ex loss: 0.752797  [   30/   88]
per-ex loss: 0.591230  [   31/   88]
per-ex loss: 0.632299  [   32/   88]
per-ex loss: 0.738631  [   33/   88]
per-ex loss: 0.710233  [   34/   88]
per-ex loss: 0.699602  [   35/   88]
per-ex loss: 0.526658  [   36/   88]
per-ex loss: 0.732373  [   37/   88]
per-ex loss: 0.713950  [   38/   88]
per-ex loss: 0.739001  [   39/   88]
per-ex loss: 0.749057  [   40/   88]
per-ex loss: 0.681434  [   41/   88]
per-ex loss: 0.523698  [   42/   88]
per-ex loss: 0.681222  [   43/   88]
per-ex loss: 0.633352  [   44/   88]
per-ex loss: 0.539870  [   45/   88]
per-ex loss: 0.523411  [   46/   88]
per-ex loss: 0.782091  [   47/   88]
per-ex loss: 0.801931  [   48/   88]
per-ex loss: 0.597067  [   49/   88]
per-ex loss: 0.540157  [   50/   88]
per-ex loss: 0.697542  [   51/   88]
per-ex loss: 0.762297  [   52/   88]
per-ex loss: 0.533695  [   53/   88]
per-ex loss: 0.555464  [   54/   88]
per-ex loss: 0.713392  [   55/   88]
per-ex loss: 0.717755  [   56/   88]
per-ex loss: 0.547968  [   57/   88]
per-ex loss: 0.611276  [   58/   88]
per-ex loss: 0.795879  [   59/   88]
per-ex loss: 0.774404  [   60/   88]
per-ex loss: 0.720799  [   61/   88]
per-ex loss: 0.801619  [   62/   88]
per-ex loss: 0.628869  [   63/   88]
per-ex loss: 0.814431  [   64/   88]
per-ex loss: 0.511560  [   65/   88]
per-ex loss: 0.513500  [   66/   88]
per-ex loss: 0.754471  [   67/   88]
per-ex loss: 0.523565  [   68/   88]
per-ex loss: 0.603858  [   69/   88]
per-ex loss: 0.714283  [   70/   88]
per-ex loss: 0.566338  [   71/   88]
per-ex loss: 0.583633  [   72/   88]
per-ex loss: 0.529702  [   73/   88]
per-ex loss: 0.569842  [   74/   88]
per-ex loss: 0.771328  [   75/   88]
per-ex loss: 0.455097  [   76/   88]
per-ex loss: 0.596052  [   77/   88]
per-ex loss: 0.483203  [   78/   88]
per-ex loss: 0.686484  [   79/   88]
per-ex loss: 0.570688  [   80/   88]
per-ex loss: 0.597040  [   81/   88]
per-ex loss: 0.542390  [   82/   88]
per-ex loss: 0.574069  [   83/   88]
per-ex loss: 0.596745  [   84/   88]
per-ex loss: 0.668737  [   85/   88]
per-ex loss: 0.504404  [   86/   88]
per-ex loss: 0.582620  [   87/   88]
per-ex loss: 0.621259  [   88/   88]
Train Error: Avg loss: 0.63907353
validation Error: 
 Avg loss: 0.70974557 
 F1: 0.468301 
 Precision: 0.569811 
 Recall: 0.397490
 IoU: 0.305740

test Error: 
 Avg loss: 0.66078808 
 F1: 0.544278 
 Precision: 0.640807 
 Recall: 0.473024
 IoU: 0.373889

We have finished training iteration 55
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_53_.pth
per-ex loss: 0.567113  [    1/   88]
per-ex loss: 0.611601  [    2/   88]
per-ex loss: 0.638753  [    3/   88]
per-ex loss: 0.651310  [    4/   88]
per-ex loss: 0.774990  [    5/   88]
per-ex loss: 0.658500  [    6/   88]
per-ex loss: 0.742566  [    7/   88]
per-ex loss: 0.549484  [    8/   88]
per-ex loss: 0.758160  [    9/   88]
per-ex loss: 0.713716  [   10/   88]
per-ex loss: 0.694458  [   11/   88]
per-ex loss: 0.533850  [   12/   88]
per-ex loss: 0.629418  [   13/   88]
per-ex loss: 0.591429  [   14/   88]
per-ex loss: 0.616141  [   15/   88]
per-ex loss: 0.515260  [   16/   88]
per-ex loss: 0.500686  [   17/   88]
per-ex loss: 0.754840  [   18/   88]
per-ex loss: 0.801560  [   19/   88]
per-ex loss: 0.525270  [   20/   88]
per-ex loss: 0.541885  [   21/   88]
per-ex loss: 0.808246  [   22/   88]
per-ex loss: 0.568664  [   23/   88]
per-ex loss: 0.569861  [   24/   88]
per-ex loss: 0.784529  [   25/   88]
per-ex loss: 0.721580  [   26/   88]
per-ex loss: 0.529206  [   27/   88]
per-ex loss: 0.512056  [   28/   88]
per-ex loss: 0.665869  [   29/   88]
per-ex loss: 0.576536  [   30/   88]
per-ex loss: 0.762273  [   31/   88]
per-ex loss: 0.550644  [   32/   88]
per-ex loss: 0.518270  [   33/   88]
per-ex loss: 0.652583  [   34/   88]
per-ex loss: 0.710960  [   35/   88]
per-ex loss: 0.560580  [   36/   88]
per-ex loss: 0.551753  [   37/   88]
per-ex loss: 0.567584  [   38/   88]
per-ex loss: 0.611056  [   39/   88]
per-ex loss: 0.709006  [   40/   88]
per-ex loss: 0.480750  [   41/   88]
per-ex loss: 0.583808  [   42/   88]
per-ex loss: 0.578429  [   43/   88]
per-ex loss: 0.680957  [   44/   88]
per-ex loss: 0.756792  [   45/   88]
per-ex loss: 0.626364  [   46/   88]
per-ex loss: 0.735223  [   47/   88]
per-ex loss: 0.748921  [   48/   88]
per-ex loss: 0.761096  [   49/   88]
per-ex loss: 0.724773  [   50/   88]
per-ex loss: 0.671439  [   51/   88]
per-ex loss: 0.571718  [   52/   88]
per-ex loss: 0.714099  [   53/   88]
per-ex loss: 0.604513  [   54/   88]
per-ex loss: 0.535375  [   55/   88]
per-ex loss: 0.696196  [   56/   88]
per-ex loss: 0.796180  [   57/   88]
per-ex loss: 0.601678  [   58/   88]
per-ex loss: 0.584359  [   59/   88]
per-ex loss: 0.805184  [   60/   88]
per-ex loss: 0.485242  [   61/   88]
per-ex loss: 0.439486  [   62/   88]
per-ex loss: 0.747192  [   63/   88]
per-ex loss: 0.774552  [   64/   88]
per-ex loss: 0.659142  [   65/   88]
per-ex loss: 0.544386  [   66/   88]
per-ex loss: 0.555327  [   67/   88]
per-ex loss: 0.728359  [   68/   88]
per-ex loss: 0.755177  [   69/   88]
per-ex loss: 0.658841  [   70/   88]
per-ex loss: 0.668731  [   71/   88]
per-ex loss: 0.682128  [   72/   88]
per-ex loss: 0.545722  [   73/   88]
per-ex loss: 0.755435  [   74/   88]
per-ex loss: 0.545940  [   75/   88]
per-ex loss: 0.602688  [   76/   88]
per-ex loss: 0.689368  [   77/   88]
per-ex loss: 0.800560  [   78/   88]
per-ex loss: 0.508972  [   79/   88]
per-ex loss: 0.511778  [   80/   88]
per-ex loss: 0.500100  [   81/   88]
per-ex loss: 0.590885  [   82/   88]
per-ex loss: 0.590839  [   83/   88]
per-ex loss: 0.762806  [   84/   88]
per-ex loss: 0.804969  [   85/   88]
per-ex loss: 0.595380  [   86/   88]
per-ex loss: 0.558728  [   87/   88]
per-ex loss: 0.605973  [   88/   88]
Train Error: Avg loss: 0.63896370
validation Error: 
 Avg loss: 0.70058870 
 F1: 0.483753 
 Precision: 0.588963 
 Recall: 0.410435
 IoU: 0.319046

test Error: 
 Avg loss: 0.65875917 
 F1: 0.546692 
 Precision: 0.646226 
 Recall: 0.473728
 IoU: 0.376171

We have finished training iteration 56
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_54_.pth
per-ex loss: 0.535314  [    1/   88]
per-ex loss: 0.718229  [    2/   88]
per-ex loss: 0.573928  [    3/   88]
per-ex loss: 0.558890  [    4/   88]
per-ex loss: 0.789392  [    5/   88]
per-ex loss: 0.652581  [    6/   88]
per-ex loss: 0.779362  [    7/   88]
per-ex loss: 0.507905  [    8/   88]
per-ex loss: 0.561484  [    9/   88]
per-ex loss: 0.477632  [   10/   88]
per-ex loss: 0.602267  [   11/   88]
per-ex loss: 0.759395  [   12/   88]
per-ex loss: 0.550965  [   13/   88]
per-ex loss: 0.681568  [   14/   88]
per-ex loss: 0.502814  [   15/   88]
per-ex loss: 0.511125  [   16/   88]
per-ex loss: 0.656149  [   17/   88]
per-ex loss: 0.745382  [   18/   88]
per-ex loss: 0.477691  [   19/   88]
per-ex loss: 0.529887  [   20/   88]
per-ex loss: 0.548851  [   21/   88]
per-ex loss: 0.606552  [   22/   88]
per-ex loss: 0.519874  [   23/   88]
per-ex loss: 0.475712  [   24/   88]
per-ex loss: 0.636840  [   25/   88]
per-ex loss: 0.709077  [   26/   88]
per-ex loss: 0.732750  [   27/   88]
per-ex loss: 0.659065  [   28/   88]
per-ex loss: 0.593437  [   29/   88]
per-ex loss: 0.580056  [   30/   88]
per-ex loss: 0.594959  [   31/   88]
per-ex loss: 0.817882  [   32/   88]
per-ex loss: 0.714990  [   33/   88]
per-ex loss: 0.561658  [   34/   88]
per-ex loss: 0.479359  [   35/   88]
per-ex loss: 0.738435  [   36/   88]
per-ex loss: 0.607010  [   37/   88]
per-ex loss: 0.825563  [   38/   88]
per-ex loss: 0.571669  [   39/   88]
per-ex loss: 0.703501  [   40/   88]
per-ex loss: 0.509426  [   41/   88]
per-ex loss: 0.526575  [   42/   88]
per-ex loss: 0.760641  [   43/   88]
per-ex loss: 0.504727  [   44/   88]
per-ex loss: 0.516392  [   45/   88]
per-ex loss: 0.720682  [   46/   88]
per-ex loss: 0.700138  [   47/   88]
per-ex loss: 0.552385  [   48/   88]
per-ex loss: 0.617003  [   49/   88]
per-ex loss: 0.751134  [   50/   88]
per-ex loss: 0.751763  [   51/   88]
per-ex loss: 0.684045  [   52/   88]
per-ex loss: 0.702772  [   53/   88]
per-ex loss: 0.631724  [   54/   88]
per-ex loss: 0.786233  [   55/   88]
per-ex loss: 0.722083  [   56/   88]
per-ex loss: 0.582920  [   57/   88]
per-ex loss: 0.721694  [   58/   88]
per-ex loss: 0.688683  [   59/   88]
per-ex loss: 0.703402  [   60/   88]
per-ex loss: 0.529225  [   61/   88]
per-ex loss: 0.630471  [   62/   88]
per-ex loss: 0.801227  [   63/   88]
per-ex loss: 0.656832  [   64/   88]
per-ex loss: 0.631928  [   65/   88]
per-ex loss: 0.556132  [   66/   88]
per-ex loss: 0.506986  [   67/   88]
per-ex loss: 0.756127  [   68/   88]
per-ex loss: 0.544260  [   69/   88]
per-ex loss: 0.548118  [   70/   88]
per-ex loss: 0.586260  [   71/   88]
per-ex loss: 0.584956  [   72/   88]
per-ex loss: 0.738130  [   73/   88]
per-ex loss: 0.739236  [   74/   88]
per-ex loss: 0.647218  [   75/   88]
per-ex loss: 0.769315  [   76/   88]
per-ex loss: 0.719569  [   77/   88]
per-ex loss: 0.793194  [   78/   88]
per-ex loss: 0.783424  [   79/   88]
per-ex loss: 0.667747  [   80/   88]
per-ex loss: 0.768719  [   81/   88]
per-ex loss: 0.619845  [   82/   88]
per-ex loss: 0.504931  [   83/   88]
per-ex loss: 0.628783  [   84/   88]
per-ex loss: 0.558468  [   85/   88]
per-ex loss: 0.809543  [   86/   88]
per-ex loss: 0.526392  [   87/   88]
per-ex loss: 0.570389  [   88/   88]
Train Error: Avg loss: 0.63819366
validation Error: 
 Avg loss: 0.69534590 
 F1: 0.485935 
 Precision: 0.551754 
 Recall: 0.434146
 IoU: 0.320947

test Error: 
 Avg loss: 0.65323910 
 F1: 0.555130 
 Precision: 0.607682 
 Recall: 0.510944
 IoU: 0.384207

We have finished training iteration 57
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_55_.pth
per-ex loss: 0.516248  [    1/   88]
per-ex loss: 0.529500  [    2/   88]
per-ex loss: 0.705669  [    3/   88]
per-ex loss: 0.572607  [    4/   88]
per-ex loss: 0.554462  [    5/   88]
per-ex loss: 0.815218  [    6/   88]
per-ex loss: 0.573924  [    7/   88]
per-ex loss: 0.708409  [    8/   88]
per-ex loss: 0.515180  [    9/   88]
per-ex loss: 0.771368  [   10/   88]
per-ex loss: 0.662732  [   11/   88]
per-ex loss: 0.551031  [   12/   88]
per-ex loss: 0.479252  [   13/   88]
per-ex loss: 0.423595  [   14/   88]
per-ex loss: 0.558942  [   15/   88]
per-ex loss: 0.576168  [   16/   88]
per-ex loss: 0.722827  [   17/   88]
per-ex loss: 0.739031  [   18/   88]
per-ex loss: 0.584482  [   19/   88]
per-ex loss: 0.758523  [   20/   88]
per-ex loss: 0.528425  [   21/   88]
per-ex loss: 0.529521  [   22/   88]
per-ex loss: 0.499400  [   23/   88]
per-ex loss: 0.523807  [   24/   88]
per-ex loss: 0.597613  [   25/   88]
per-ex loss: 0.691899  [   26/   88]
per-ex loss: 0.559390  [   27/   88]
per-ex loss: 0.557651  [   28/   88]
per-ex loss: 0.803345  [   29/   88]
per-ex loss: 0.667034  [   30/   88]
per-ex loss: 0.567900  [   31/   88]
per-ex loss: 0.670509  [   32/   88]
per-ex loss: 0.518703  [   33/   88]
per-ex loss: 0.687966  [   34/   88]
per-ex loss: 0.685579  [   35/   88]
per-ex loss: 0.494561  [   36/   88]
per-ex loss: 0.639557  [   37/   88]
per-ex loss: 0.757063  [   38/   88]
per-ex loss: 0.553214  [   39/   88]
per-ex loss: 0.613004  [   40/   88]
per-ex loss: 0.720786  [   41/   88]
per-ex loss: 0.562414  [   42/   88]
per-ex loss: 0.707851  [   43/   88]
per-ex loss: 0.548850  [   44/   88]
per-ex loss: 0.616193  [   45/   88]
per-ex loss: 0.729843  [   46/   88]
per-ex loss: 0.516954  [   47/   88]
per-ex loss: 0.579783  [   48/   88]
per-ex loss: 0.774014  [   49/   88]
per-ex loss: 0.695895  [   50/   88]
per-ex loss: 0.563536  [   51/   88]
per-ex loss: 0.747784  [   52/   88]
per-ex loss: 0.616782  [   53/   88]
per-ex loss: 0.723272  [   54/   88]
per-ex loss: 0.776275  [   55/   88]
per-ex loss: 0.729082  [   56/   88]
per-ex loss: 0.496550  [   57/   88]
per-ex loss: 0.585246  [   58/   88]
per-ex loss: 0.623929  [   59/   88]
per-ex loss: 0.725202  [   60/   88]
per-ex loss: 0.590468  [   61/   88]
per-ex loss: 0.799033  [   62/   88]
per-ex loss: 0.812968  [   63/   88]
per-ex loss: 0.565068  [   64/   88]
per-ex loss: 0.502533  [   65/   88]
per-ex loss: 0.571301  [   66/   88]
per-ex loss: 0.545746  [   67/   88]
per-ex loss: 0.752034  [   68/   88]
per-ex loss: 0.706435  [   69/   88]
per-ex loss: 0.773789  [   70/   88]
per-ex loss: 0.727874  [   71/   88]
per-ex loss: 0.474032  [   72/   88]
per-ex loss: 0.787924  [   73/   88]
per-ex loss: 0.714425  [   74/   88]
per-ex loss: 0.797618  [   75/   88]
per-ex loss: 0.820175  [   76/   88]
per-ex loss: 0.704488  [   77/   88]
per-ex loss: 0.598981  [   78/   88]
per-ex loss: 0.626602  [   79/   88]
per-ex loss: 0.765678  [   80/   88]
per-ex loss: 0.679950  [   81/   88]
per-ex loss: 0.747507  [   82/   88]
per-ex loss: 0.601936  [   83/   88]
per-ex loss: 0.594279  [   84/   88]
per-ex loss: 0.563272  [   85/   88]
per-ex loss: 0.692383  [   86/   88]
per-ex loss: 0.581777  [   87/   88]
per-ex loss: 0.612650  [   88/   88]
Train Error: Avg loss: 0.63882352
validation Error: 
 Avg loss: 0.70198639 
 F1: 0.479876 
 Precision: 0.581028 
 Recall: 0.408722
 IoU: 0.315683

test Error: 
 Avg loss: 0.65451082 
 F1: 0.547518 
 Precision: 0.648687 
 Recall: 0.473649
 IoU: 0.376954

We have finished training iteration 58
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_56_.pth
per-ex loss: 0.510110  [    1/   88]
per-ex loss: 0.545806  [    2/   88]
per-ex loss: 0.730930  [    3/   88]
per-ex loss: 0.762538  [    4/   88]
per-ex loss: 0.556815  [    5/   88]
per-ex loss: 0.551920  [    6/   88]
per-ex loss: 0.742694  [    7/   88]
per-ex loss: 0.539136  [    8/   88]
per-ex loss: 0.542093  [    9/   88]
per-ex loss: 0.577562  [   10/   88]
per-ex loss: 0.599691  [   11/   88]
per-ex loss: 0.745284  [   12/   88]
per-ex loss: 0.690768  [   13/   88]
per-ex loss: 0.555431  [   14/   88]
per-ex loss: 0.755997  [   15/   88]
per-ex loss: 0.542375  [   16/   88]
per-ex loss: 0.576337  [   17/   88]
per-ex loss: 0.606442  [   18/   88]
per-ex loss: 0.520110  [   19/   88]
per-ex loss: 0.739060  [   20/   88]
per-ex loss: 0.523517  [   21/   88]
per-ex loss: 0.541339  [   22/   88]
per-ex loss: 0.700175  [   23/   88]
per-ex loss: 0.594117  [   24/   88]
per-ex loss: 0.597781  [   25/   88]
per-ex loss: 0.646691  [   26/   88]
per-ex loss: 0.519498  [   27/   88]
per-ex loss: 0.778716  [   28/   88]
per-ex loss: 0.568801  [   29/   88]
per-ex loss: 0.787737  [   30/   88]
per-ex loss: 0.753874  [   31/   88]
per-ex loss: 0.740331  [   32/   88]
per-ex loss: 0.675311  [   33/   88]
per-ex loss: 0.522319  [   34/   88]
per-ex loss: 0.497033  [   35/   88]
per-ex loss: 0.579162  [   36/   88]
per-ex loss: 0.496264  [   37/   88]
per-ex loss: 0.666420  [   38/   88]
per-ex loss: 0.837536  [   39/   88]
per-ex loss: 0.735836  [   40/   88]
per-ex loss: 0.582449  [   41/   88]
per-ex loss: 0.706718  [   42/   88]
per-ex loss: 0.672296  [   43/   88]
per-ex loss: 0.488625  [   44/   88]
per-ex loss: 0.532386  [   45/   88]
per-ex loss: 0.755130  [   46/   88]
per-ex loss: 0.629776  [   47/   88]
per-ex loss: 0.660280  [   48/   88]
per-ex loss: 0.802500  [   49/   88]
per-ex loss: 0.744253  [   50/   88]
per-ex loss: 0.752130  [   51/   88]
per-ex loss: 0.590400  [   52/   88]
per-ex loss: 0.546165  [   53/   88]
per-ex loss: 0.514311  [   54/   88]
per-ex loss: 0.530905  [   55/   88]
per-ex loss: 0.585438  [   56/   88]
per-ex loss: 0.784837  [   57/   88]
per-ex loss: 0.477567  [   58/   88]
per-ex loss: 0.561633  [   59/   88]
per-ex loss: 0.506654  [   60/   88]
per-ex loss: 0.758937  [   61/   88]
per-ex loss: 0.558649  [   62/   88]
per-ex loss: 0.573750  [   63/   88]
per-ex loss: 0.741354  [   64/   88]
per-ex loss: 0.665682  [   65/   88]
per-ex loss: 0.783747  [   66/   88]
per-ex loss: 0.578393  [   67/   88]
per-ex loss: 0.543122  [   68/   88]
per-ex loss: 0.679692  [   69/   88]
per-ex loss: 0.557591  [   70/   88]
per-ex loss: 0.641964  [   71/   88]
per-ex loss: 0.694356  [   72/   88]
per-ex loss: 0.812350  [   73/   88]
per-ex loss: 0.777569  [   74/   88]
per-ex loss: 0.669477  [   75/   88]
per-ex loss: 0.701298  [   76/   88]
per-ex loss: 0.539492  [   77/   88]
per-ex loss: 0.649228  [   78/   88]
per-ex loss: 0.570808  [   79/   88]
per-ex loss: 0.616290  [   80/   88]
per-ex loss: 0.503727  [   81/   88]
per-ex loss: 0.708731  [   82/   88]
per-ex loss: 0.632233  [   83/   88]
per-ex loss: 0.756937  [   84/   88]
per-ex loss: 0.580238  [   85/   88]
per-ex loss: 0.713053  [   86/   88]
per-ex loss: 0.613520  [   87/   88]
per-ex loss: 0.819709  [   88/   88]
Train Error: Avg loss: 0.63658986
validation Error: 
 Avg loss: 0.69056076 
 F1: 0.494994 
 Precision: 0.636793 
 Recall: 0.404844
 IoU: 0.328898

test Error: 
 Avg loss: 0.65311257 
 F1: 0.556680 
 Precision: 0.641163 
 Recall: 0.491869
 IoU: 0.385694

We have finished training iteration 59
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_57_.pth
per-ex loss: 0.564417  [    1/   88]
per-ex loss: 0.657658  [    2/   88]
per-ex loss: 0.554904  [    3/   88]
per-ex loss: 0.713818  [    4/   88]
per-ex loss: 0.725128  [    5/   88]
per-ex loss: 0.519720  [    6/   88]
per-ex loss: 0.673956  [    7/   88]
per-ex loss: 0.735347  [    8/   88]
per-ex loss: 0.716878  [    9/   88]
per-ex loss: 0.801716  [   10/   88]
per-ex loss: 0.754888  [   11/   88]
per-ex loss: 0.669936  [   12/   88]
per-ex loss: 0.543365  [   13/   88]
per-ex loss: 0.523733  [   14/   88]
per-ex loss: 0.719938  [   15/   88]
per-ex loss: 0.570183  [   16/   88]
per-ex loss: 0.526907  [   17/   88]
per-ex loss: 0.558857  [   18/   88]
per-ex loss: 0.720736  [   19/   88]
per-ex loss: 0.516654  [   20/   88]
per-ex loss: 0.712933  [   21/   88]
per-ex loss: 0.692940  [   22/   88]
per-ex loss: 0.749407  [   23/   88]
per-ex loss: 0.575686  [   24/   88]
per-ex loss: 0.632877  [   25/   88]
per-ex loss: 0.571798  [   26/   88]
per-ex loss: 0.697294  [   27/   88]
per-ex loss: 0.542965  [   28/   88]
per-ex loss: 0.747379  [   29/   88]
per-ex loss: 0.517597  [   30/   88]
per-ex loss: 0.544316  [   31/   88]
per-ex loss: 0.467733  [   32/   88]
per-ex loss: 0.682482  [   33/   88]
per-ex loss: 0.587793  [   34/   88]
per-ex loss: 0.783382  [   35/   88]
per-ex loss: 0.496914  [   36/   88]
per-ex loss: 0.512961  [   37/   88]
per-ex loss: 0.525014  [   38/   88]
per-ex loss: 0.697939  [   39/   88]
per-ex loss: 0.519662  [   40/   88]
per-ex loss: 0.592825  [   41/   88]
per-ex loss: 0.625622  [   42/   88]
per-ex loss: 0.762203  [   43/   88]
per-ex loss: 0.515771  [   44/   88]
per-ex loss: 0.794040  [   45/   88]
per-ex loss: 0.619183  [   46/   88]
per-ex loss: 0.722422  [   47/   88]
per-ex loss: 0.736327  [   48/   88]
per-ex loss: 0.533197  [   49/   88]
per-ex loss: 0.704198  [   50/   88]
per-ex loss: 0.770147  [   51/   88]
per-ex loss: 0.627592  [   52/   88]
per-ex loss: 0.825450  [   53/   88]
per-ex loss: 0.459746  [   54/   88]
per-ex loss: 0.532355  [   55/   88]
per-ex loss: 0.567908  [   56/   88]
per-ex loss: 0.689673  [   57/   88]
per-ex loss: 0.801229  [   58/   88]
per-ex loss: 0.653566  [   59/   88]
per-ex loss: 0.726599  [   60/   88]
per-ex loss: 0.705110  [   61/   88]
per-ex loss: 0.494566  [   62/   88]
per-ex loss: 0.565733  [   63/   88]
per-ex loss: 0.495946  [   64/   88]
per-ex loss: 0.686602  [   65/   88]
per-ex loss: 0.743050  [   66/   88]
per-ex loss: 0.593139  [   67/   88]
per-ex loss: 0.761330  [   68/   88]
per-ex loss: 0.576614  [   69/   88]
per-ex loss: 0.753204  [   70/   88]
per-ex loss: 0.532782  [   71/   88]
per-ex loss: 0.597692  [   72/   88]
per-ex loss: 0.793261  [   73/   88]
per-ex loss: 0.557945  [   74/   88]
per-ex loss: 0.802930  [   75/   88]
per-ex loss: 0.760070  [   76/   88]
per-ex loss: 0.535344  [   77/   88]
per-ex loss: 0.608274  [   78/   88]
per-ex loss: 0.600189  [   79/   88]
per-ex loss: 0.611312  [   80/   88]
per-ex loss: 0.509334  [   81/   88]
per-ex loss: 0.555704  [   82/   88]
per-ex loss: 0.733105  [   83/   88]
per-ex loss: 0.776619  [   84/   88]
per-ex loss: 0.567970  [   85/   88]
per-ex loss: 0.752639  [   86/   88]
per-ex loss: 0.574376  [   87/   88]
per-ex loss: 0.582541  [   88/   88]
Train Error: Avg loss: 0.63767323
validation Error: 
 Avg loss: 0.69561833 
 F1: 0.487729 
 Precision: 0.565845 
 Recall: 0.428564
 IoU: 0.322514

test Error: 
 Avg loss: 0.64918642 
 F1: 0.561301 
 Precision: 0.592756 
 Recall: 0.533015
 IoU: 0.390144

We have finished training iteration 60
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_58_.pth
per-ex loss: 0.463810  [    1/   88]
per-ex loss: 0.497021  [    2/   88]
per-ex loss: 0.617204  [    3/   88]
per-ex loss: 0.717683  [    4/   88]
per-ex loss: 0.710980  [    5/   88]
per-ex loss: 0.515884  [    6/   88]
per-ex loss: 0.711989  [    7/   88]
per-ex loss: 0.690064  [    8/   88]
per-ex loss: 0.762119  [    9/   88]
per-ex loss: 0.666318  [   10/   88]
per-ex loss: 0.589895  [   11/   88]
per-ex loss: 0.518967  [   12/   88]
per-ex loss: 0.615292  [   13/   88]
per-ex loss: 0.818437  [   14/   88]
per-ex loss: 0.563583  [   15/   88]
per-ex loss: 0.786660  [   16/   88]
per-ex loss: 0.555191  [   17/   88]
per-ex loss: 0.706548  [   18/   88]
per-ex loss: 0.565283  [   19/   88]
per-ex loss: 0.652285  [   20/   88]
per-ex loss: 0.707705  [   21/   88]
per-ex loss: 0.664089  [   22/   88]
per-ex loss: 0.611442  [   23/   88]
per-ex loss: 0.627541  [   24/   88]
per-ex loss: 0.451993  [   25/   88]
per-ex loss: 0.726194  [   26/   88]
per-ex loss: 0.647854  [   27/   88]
per-ex loss: 0.636482  [   28/   88]
per-ex loss: 0.629077  [   29/   88]
per-ex loss: 0.577265  [   30/   88]
per-ex loss: 0.519104  [   31/   88]
per-ex loss: 0.557106  [   32/   88]
per-ex loss: 0.564513  [   33/   88]
per-ex loss: 0.556699  [   34/   88]
per-ex loss: 0.574755  [   35/   88]
per-ex loss: 0.796052  [   36/   88]
per-ex loss: 0.522127  [   37/   88]
per-ex loss: 0.539601  [   38/   88]
per-ex loss: 0.513991  [   39/   88]
per-ex loss: 0.574428  [   40/   88]
per-ex loss: 0.615101  [   41/   88]
per-ex loss: 0.818477  [   42/   88]
per-ex loss: 0.623394  [   43/   88]
per-ex loss: 0.742756  [   44/   88]
per-ex loss: 0.820629  [   45/   88]
per-ex loss: 0.741079  [   46/   88]
per-ex loss: 0.561027  [   47/   88]
per-ex loss: 0.767565  [   48/   88]
per-ex loss: 0.562797  [   49/   88]
per-ex loss: 0.740741  [   50/   88]
per-ex loss: 0.553812  [   51/   88]
per-ex loss: 0.717445  [   52/   88]
per-ex loss: 0.804403  [   53/   88]
per-ex loss: 0.730775  [   54/   88]
per-ex loss: 0.534009  [   55/   88]
per-ex loss: 0.623005  [   56/   88]
per-ex loss: 0.670206  [   57/   88]
per-ex loss: 0.746449  [   58/   88]
per-ex loss: 0.583416  [   59/   88]
per-ex loss: 0.531342  [   60/   88]
per-ex loss: 0.727081  [   61/   88]
per-ex loss: 0.520488  [   62/   88]
per-ex loss: 0.699825  [   63/   88]
per-ex loss: 0.534119  [   64/   88]
per-ex loss: 0.723285  [   65/   88]
per-ex loss: 0.500480  [   66/   88]
per-ex loss: 0.509825  [   67/   88]
per-ex loss: 0.787133  [   68/   88]
per-ex loss: 0.746614  [   69/   88]
per-ex loss: 0.689088  [   70/   88]
per-ex loss: 0.499037  [   71/   88]
per-ex loss: 0.763074  [   72/   88]
per-ex loss: 0.592180  [   73/   88]
per-ex loss: 0.504843  [   74/   88]
per-ex loss: 0.802259  [   75/   88]
per-ex loss: 0.547152  [   76/   88]
per-ex loss: 0.549604  [   77/   88]
per-ex loss: 0.571282  [   78/   88]
per-ex loss: 0.574872  [   79/   88]
per-ex loss: 0.744573  [   80/   88]
per-ex loss: 0.646379  [   81/   88]
per-ex loss: 0.709452  [   82/   88]
per-ex loss: 0.771099  [   83/   88]
per-ex loss: 0.705066  [   84/   88]
per-ex loss: 0.759120  [   85/   88]
per-ex loss: 0.551051  [   86/   88]
per-ex loss: 0.658296  [   87/   88]
per-ex loss: 0.600054  [   88/   88]
Train Error: Avg loss: 0.63862493
validation Error: 
 Avg loss: 0.69563661 
 F1: 0.486263 
 Precision: 0.579672 
 Recall: 0.418780
 IoU: 0.321233

test Error: 
 Avg loss: 0.65232344 
 F1: 0.556879 
 Precision: 0.616168 
 Recall: 0.507999
 IoU: 0.385886

We have finished training iteration 61
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_59_.pth
per-ex loss: 0.704736  [    1/   88]
per-ex loss: 0.559962  [    2/   88]
per-ex loss: 0.769366  [    3/   88]
per-ex loss: 0.824336  [    4/   88]
per-ex loss: 0.591072  [    5/   88]
per-ex loss: 0.551070  [    6/   88]
per-ex loss: 0.768232  [    7/   88]
per-ex loss: 0.790659  [    8/   88]
per-ex loss: 0.582836  [    9/   88]
per-ex loss: 0.564722  [   10/   88]
per-ex loss: 0.697533  [   11/   88]
per-ex loss: 0.534803  [   12/   88]
per-ex loss: 0.574037  [   13/   88]
per-ex loss: 0.673035  [   14/   88]
per-ex loss: 0.737194  [   15/   88]
per-ex loss: 0.718872  [   16/   88]
per-ex loss: 0.512900  [   17/   88]
per-ex loss: 0.752736  [   18/   88]
per-ex loss: 0.685510  [   19/   88]
per-ex loss: 0.598774  [   20/   88]
per-ex loss: 0.647742  [   21/   88]
per-ex loss: 0.565270  [   22/   88]
per-ex loss: 0.506959  [   23/   88]
per-ex loss: 0.604148  [   24/   88]
per-ex loss: 0.750021  [   25/   88]
per-ex loss: 0.635295  [   26/   88]
per-ex loss: 0.481964  [   27/   88]
per-ex loss: 0.512614  [   28/   88]
per-ex loss: 0.583201  [   29/   88]
per-ex loss: 0.544586  [   30/   88]
per-ex loss: 0.706671  [   31/   88]
per-ex loss: 0.572472  [   32/   88]
per-ex loss: 0.527261  [   33/   88]
per-ex loss: 0.552395  [   34/   88]
per-ex loss: 0.736753  [   35/   88]
per-ex loss: 0.797478  [   36/   88]
per-ex loss: 0.522051  [   37/   88]
per-ex loss: 0.665612  [   38/   88]
per-ex loss: 0.589214  [   39/   88]
per-ex loss: 0.783982  [   40/   88]
per-ex loss: 0.527883  [   41/   88]
per-ex loss: 0.709675  [   42/   88]
per-ex loss: 0.663043  [   43/   88]
per-ex loss: 0.693870  [   44/   88]
per-ex loss: 0.725162  [   45/   88]
per-ex loss: 0.532813  [   46/   88]
per-ex loss: 0.503661  [   47/   88]
per-ex loss: 0.745830  [   48/   88]
per-ex loss: 0.579455  [   49/   88]
per-ex loss: 0.601846  [   50/   88]
per-ex loss: 0.476577  [   51/   88]
per-ex loss: 0.598368  [   52/   88]
per-ex loss: 0.703165  [   53/   88]
per-ex loss: 0.511798  [   54/   88]
per-ex loss: 0.652517  [   55/   88]
per-ex loss: 0.535862  [   56/   88]
per-ex loss: 0.499494  [   57/   88]
per-ex loss: 0.783051  [   58/   88]
per-ex loss: 0.674238  [   59/   88]
per-ex loss: 0.581544  [   60/   88]
per-ex loss: 0.685806  [   61/   88]
per-ex loss: 0.536980  [   62/   88]
per-ex loss: 0.599644  [   63/   88]
per-ex loss: 0.759087  [   64/   88]
per-ex loss: 0.741630  [   65/   88]
per-ex loss: 0.602410  [   66/   88]
per-ex loss: 0.720996  [   67/   88]
per-ex loss: 0.698976  [   68/   88]
per-ex loss: 0.520843  [   69/   88]
per-ex loss: 0.568191  [   70/   88]
per-ex loss: 0.819268  [   71/   88]
per-ex loss: 0.582580  [   72/   88]
per-ex loss: 0.676512  [   73/   88]
per-ex loss: 0.740407  [   74/   88]
per-ex loss: 0.587091  [   75/   88]
per-ex loss: 0.495377  [   76/   88]
per-ex loss: 0.652647  [   77/   88]
per-ex loss: 0.741474  [   78/   88]
per-ex loss: 0.823727  [   79/   88]
per-ex loss: 0.549707  [   80/   88]
per-ex loss: 0.740318  [   81/   88]
per-ex loss: 0.702774  [   82/   88]
per-ex loss: 0.807235  [   83/   88]
per-ex loss: 0.743339  [   84/   88]
per-ex loss: 0.601656  [   85/   88]
per-ex loss: 0.542815  [   86/   88]
per-ex loss: 0.572633  [   87/   88]
per-ex loss: 0.567831  [   88/   88]
Train Error: Avg loss: 0.63818044
validation Error: 
 Avg loss: 0.68595375 
 F1: 0.502166 
 Precision: 0.568640 
 Recall: 0.449607
 IoU: 0.335261

test Error: 
 Avg loss: 0.64353436 
 F1: 0.567323 
 Precision: 0.603089 
 Recall: 0.535561
 IoU: 0.395988

We have finished training iteration 62
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_60_.pth
per-ex loss: 0.722310  [    1/   88]
per-ex loss: 0.753568  [    2/   88]
per-ex loss: 0.589161  [    3/   88]
per-ex loss: 0.806435  [    4/   88]
per-ex loss: 0.663794  [    5/   88]
per-ex loss: 0.747428  [    6/   88]
per-ex loss: 0.497393  [    7/   88]
per-ex loss: 0.458436  [    8/   88]
per-ex loss: 0.536134  [    9/   88]
per-ex loss: 0.808310  [   10/   88]
per-ex loss: 0.603929  [   11/   88]
per-ex loss: 0.695783  [   12/   88]
per-ex loss: 0.728246  [   13/   88]
per-ex loss: 0.590610  [   14/   88]
per-ex loss: 0.596142  [   15/   88]
per-ex loss: 0.766006  [   16/   88]
per-ex loss: 0.583311  [   17/   88]
per-ex loss: 0.720643  [   18/   88]
per-ex loss: 0.618585  [   19/   88]
per-ex loss: 0.745511  [   20/   88]
per-ex loss: 0.537413  [   21/   88]
per-ex loss: 0.537910  [   22/   88]
per-ex loss: 0.609556  [   23/   88]
per-ex loss: 0.683311  [   24/   88]
per-ex loss: 0.550249  [   25/   88]
per-ex loss: 0.782019  [   26/   88]
per-ex loss: 0.544520  [   27/   88]
per-ex loss: 0.585553  [   28/   88]
per-ex loss: 0.542001  [   29/   88]
per-ex loss: 0.602594  [   30/   88]
per-ex loss: 0.754406  [   31/   88]
per-ex loss: 0.760338  [   32/   88]
per-ex loss: 0.549803  [   33/   88]
per-ex loss: 0.783435  [   34/   88]
per-ex loss: 0.543289  [   35/   88]
per-ex loss: 0.799447  [   36/   88]
per-ex loss: 0.740877  [   37/   88]
per-ex loss: 0.543923  [   38/   88]
per-ex loss: 0.610610  [   39/   88]
per-ex loss: 0.527003  [   40/   88]
per-ex loss: 0.706968  [   41/   88]
per-ex loss: 0.486742  [   42/   88]
per-ex loss: 0.708770  [   43/   88]
per-ex loss: 0.516545  [   44/   88]
per-ex loss: 0.652495  [   45/   88]
per-ex loss: 0.572493  [   46/   88]
per-ex loss: 0.566224  [   47/   88]
per-ex loss: 0.758178  [   48/   88]
per-ex loss: 0.738283  [   49/   88]
per-ex loss: 0.807362  [   50/   88]
per-ex loss: 0.527133  [   51/   88]
per-ex loss: 0.789710  [   52/   88]
per-ex loss: 0.827223  [   53/   88]
per-ex loss: 0.567182  [   54/   88]
per-ex loss: 0.501577  [   55/   88]
per-ex loss: 0.638451  [   56/   88]
per-ex loss: 0.708382  [   57/   88]
per-ex loss: 0.739334  [   58/   88]
per-ex loss: 0.738446  [   59/   88]
per-ex loss: 0.696219  [   60/   88]
per-ex loss: 0.626888  [   61/   88]
per-ex loss: 0.595774  [   62/   88]
per-ex loss: 0.556814  [   63/   88]
per-ex loss: 0.401811  [   64/   88]
per-ex loss: 0.578007  [   65/   88]
per-ex loss: 0.733679  [   66/   88]
per-ex loss: 0.762403  [   67/   88]
per-ex loss: 0.540170  [   68/   88]
per-ex loss: 0.541967  [   69/   88]
per-ex loss: 0.514810  [   70/   88]
per-ex loss: 0.732325  [   71/   88]
per-ex loss: 0.511692  [   72/   88]
per-ex loss: 0.612677  [   73/   88]
per-ex loss: 0.704339  [   74/   88]
per-ex loss: 0.570929  [   75/   88]
per-ex loss: 0.686841  [   76/   88]
per-ex loss: 0.475171  [   77/   88]
per-ex loss: 0.509542  [   78/   88]
per-ex loss: 0.700296  [   79/   88]
per-ex loss: 0.519364  [   80/   88]
per-ex loss: 0.677357  [   81/   88]
per-ex loss: 0.503224  [   82/   88]
per-ex loss: 0.595174  [   83/   88]
per-ex loss: 0.564320  [   84/   88]
per-ex loss: 0.664913  [   85/   88]
per-ex loss: 0.740093  [   86/   88]
per-ex loss: 0.664359  [   87/   88]
per-ex loss: 0.582799  [   88/   88]
Train Error: Avg loss: 0.63563005
validation Error: 
 Avg loss: 0.69644470 
 F1: 0.492952 
 Precision: 0.534918 
 Recall: 0.457092
 IoU: 0.327097

test Error: 
 Avg loss: 0.64277364 
 F1: 0.566087 
 Precision: 0.609580 
 Recall: 0.528387
 IoU: 0.394785

We have finished training iteration 63
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_61_.pth
per-ex loss: 0.654756  [    1/   88]
per-ex loss: 0.516395  [    2/   88]
per-ex loss: 0.797103  [    3/   88]
per-ex loss: 0.738302  [    4/   88]
per-ex loss: 0.709670  [    5/   88]
per-ex loss: 0.626518  [    6/   88]
per-ex loss: 0.735424  [    7/   88]
per-ex loss: 0.760486  [    8/   88]
per-ex loss: 0.537272  [    9/   88]
per-ex loss: 0.576051  [   10/   88]
per-ex loss: 0.582008  [   11/   88]
per-ex loss: 0.746894  [   12/   88]
per-ex loss: 0.755064  [   13/   88]
per-ex loss: 0.677413  [   14/   88]
per-ex loss: 0.561804  [   15/   88]
per-ex loss: 0.471260  [   16/   88]
per-ex loss: 0.780472  [   17/   88]
per-ex loss: 0.494990  [   18/   88]
per-ex loss: 0.583552  [   19/   88]
per-ex loss: 0.521573  [   20/   88]
per-ex loss: 0.584005  [   21/   88]
per-ex loss: 0.739684  [   22/   88]
per-ex loss: 0.488078  [   23/   88]
per-ex loss: 0.780383  [   24/   88]
per-ex loss: 0.644754  [   25/   88]
per-ex loss: 0.668846  [   26/   88]
per-ex loss: 0.451935  [   27/   88]
per-ex loss: 0.644326  [   28/   88]
per-ex loss: 0.718475  [   29/   88]
per-ex loss: 0.646230  [   30/   88]
per-ex loss: 0.767526  [   31/   88]
per-ex loss: 0.602082  [   32/   88]
per-ex loss: 0.700242  [   33/   88]
per-ex loss: 0.605036  [   34/   88]
per-ex loss: 0.570732  [   35/   88]
per-ex loss: 0.604246  [   36/   88]
per-ex loss: 0.566291  [   37/   88]
per-ex loss: 0.733532  [   38/   88]
per-ex loss: 0.768690  [   39/   88]
per-ex loss: 0.496628  [   40/   88]
per-ex loss: 0.580558  [   41/   88]
per-ex loss: 0.620940  [   42/   88]
per-ex loss: 0.753855  [   43/   88]
per-ex loss: 0.737887  [   44/   88]
per-ex loss: 0.747604  [   45/   88]
per-ex loss: 0.633033  [   46/   88]
per-ex loss: 0.757006  [   47/   88]
per-ex loss: 0.709636  [   48/   88]
per-ex loss: 0.678881  [   49/   88]
per-ex loss: 0.557148  [   50/   88]
per-ex loss: 0.706077  [   51/   88]
per-ex loss: 0.766691  [   52/   88]
per-ex loss: 0.568862  [   53/   88]
per-ex loss: 0.502739  [   54/   88]
per-ex loss: 0.475494  [   55/   88]
per-ex loss: 0.796561  [   56/   88]
per-ex loss: 0.718090  [   57/   88]
per-ex loss: 0.546640  [   58/   88]
per-ex loss: 0.527562  [   59/   88]
per-ex loss: 0.550556  [   60/   88]
per-ex loss: 0.548725  [   61/   88]
per-ex loss: 0.819827  [   62/   88]
per-ex loss: 0.781412  [   63/   88]
per-ex loss: 0.714633  [   64/   88]
per-ex loss: 0.521967  [   65/   88]
per-ex loss: 0.555773  [   66/   88]
per-ex loss: 0.618701  [   67/   88]
per-ex loss: 0.556807  [   68/   88]
per-ex loss: 0.791973  [   69/   88]
per-ex loss: 0.501593  [   70/   88]
per-ex loss: 0.584164  [   71/   88]
per-ex loss: 0.557930  [   72/   88]
per-ex loss: 0.525821  [   73/   88]
per-ex loss: 0.501937  [   74/   88]
per-ex loss: 0.581209  [   75/   88]
per-ex loss: 0.550442  [   76/   88]
per-ex loss: 0.693363  [   77/   88]
per-ex loss: 0.568051  [   78/   88]
per-ex loss: 0.738352  [   79/   88]
per-ex loss: 0.559406  [   80/   88]
per-ex loss: 0.588113  [   81/   88]
per-ex loss: 0.656879  [   82/   88]
per-ex loss: 0.553620  [   83/   88]
per-ex loss: 0.702066  [   84/   88]
per-ex loss: 0.759194  [   85/   88]
per-ex loss: 0.646932  [   86/   88]
per-ex loss: 0.659950  [   87/   88]
per-ex loss: 0.487698  [   88/   88]
Train Error: Avg loss: 0.63489867
validation Error: 
 Avg loss: 0.69631006 
 F1: 0.485337 
 Precision: 0.593235 
 Recall: 0.410648
 IoU: 0.320426

test Error: 
 Avg loss: 0.65259404 
 F1: 0.553748 
 Precision: 0.632966 
 Recall: 0.492154
 IoU: 0.382885

We have finished training iteration 64
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_51_.pth
per-ex loss: 0.531043  [    1/   88]
per-ex loss: 0.762807  [    2/   88]
per-ex loss: 0.727893  [    3/   88]
per-ex loss: 0.477042  [    4/   88]
per-ex loss: 0.525546  [    5/   88]
per-ex loss: 0.499039  [    6/   88]
per-ex loss: 0.609583  [    7/   88]
per-ex loss: 0.602704  [    8/   88]
per-ex loss: 0.456452  [    9/   88]
per-ex loss: 0.578337  [   10/   88]
per-ex loss: 0.540123  [   11/   88]
per-ex loss: 0.711818  [   12/   88]
per-ex loss: 0.505323  [   13/   88]
per-ex loss: 0.504187  [   14/   88]
per-ex loss: 0.678570  [   15/   88]
per-ex loss: 0.697490  [   16/   88]
per-ex loss: 0.649312  [   17/   88]
per-ex loss: 0.552318  [   18/   88]
per-ex loss: 0.803201  [   19/   88]
per-ex loss: 0.533516  [   20/   88]
per-ex loss: 0.714804  [   21/   88]
per-ex loss: 0.531941  [   22/   88]
per-ex loss: 0.569289  [   23/   88]
per-ex loss: 0.425115  [   24/   88]
per-ex loss: 0.746212  [   25/   88]
per-ex loss: 0.606483  [   26/   88]
per-ex loss: 0.550008  [   27/   88]
per-ex loss: 0.657807  [   28/   88]
per-ex loss: 0.507200  [   29/   88]
per-ex loss: 0.530362  [   30/   88]
per-ex loss: 0.578343  [   31/   88]
per-ex loss: 0.570776  [   32/   88]
per-ex loss: 0.602475  [   33/   88]
per-ex loss: 0.810141  [   34/   88]
per-ex loss: 0.749933  [   35/   88]
per-ex loss: 0.562937  [   36/   88]
per-ex loss: 0.803953  [   37/   88]
per-ex loss: 0.775936  [   38/   88]
per-ex loss: 0.805162  [   39/   88]
per-ex loss: 0.673650  [   40/   88]
per-ex loss: 0.726518  [   41/   88]
per-ex loss: 0.656096  [   42/   88]
per-ex loss: 0.708493  [   43/   88]
per-ex loss: 0.624597  [   44/   88]
per-ex loss: 0.789242  [   45/   88]
per-ex loss: 0.730376  [   46/   88]
per-ex loss: 0.729301  [   47/   88]
per-ex loss: 0.516569  [   48/   88]
per-ex loss: 0.732235  [   49/   88]
per-ex loss: 0.654782  [   50/   88]
per-ex loss: 0.633939  [   51/   88]
per-ex loss: 0.622510  [   52/   88]
per-ex loss: 0.699160  [   53/   88]
per-ex loss: 0.748537  [   54/   88]
per-ex loss: 0.591238  [   55/   88]
per-ex loss: 0.721746  [   56/   88]
per-ex loss: 0.605269  [   57/   88]
per-ex loss: 0.779908  [   58/   88]
per-ex loss: 0.780638  [   59/   88]
per-ex loss: 0.613960  [   60/   88]
per-ex loss: 0.590356  [   61/   88]
per-ex loss: 0.756214  [   62/   88]
per-ex loss: 0.708550  [   63/   88]
per-ex loss: 0.712454  [   64/   88]
per-ex loss: 0.512094  [   65/   88]
per-ex loss: 0.581533  [   66/   88]
per-ex loss: 0.746280  [   67/   88]
per-ex loss: 0.763139  [   68/   88]
per-ex loss: 0.508490  [   69/   88]
per-ex loss: 0.516107  [   70/   88]
per-ex loss: 0.535667  [   71/   88]
per-ex loss: 0.576782  [   72/   88]
per-ex loss: 0.634072  [   73/   88]
per-ex loss: 0.599084  [   74/   88]
per-ex loss: 0.530069  [   75/   88]
per-ex loss: 0.561065  [   76/   88]
per-ex loss: 0.568842  [   77/   88]
per-ex loss: 0.549406  [   78/   88]
per-ex loss: 0.578586  [   79/   88]
per-ex loss: 0.654791  [   80/   88]
per-ex loss: 0.661767  [   81/   88]
per-ex loss: 0.512214  [   82/   88]
per-ex loss: 0.716104  [   83/   88]
per-ex loss: 0.574574  [   84/   88]
per-ex loss: 0.716173  [   85/   88]
per-ex loss: 0.513019  [   86/   88]
per-ex loss: 0.514968  [   87/   88]
per-ex loss: 0.734423  [   88/   88]
Train Error: Avg loss: 0.63080416
validation Error: 
 Avg loss: 0.70228319 
 F1: 0.478215 
 Precision: 0.579457 
 Recall: 0.407089
 IoU: 0.314246

test Error: 
 Avg loss: 0.65984113 
 F1: 0.542595 
 Precision: 0.653328 
 Recall: 0.463958
 IoU: 0.372302

We have finished training iteration 65
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_63_.pth
per-ex loss: 0.642174  [    1/   88]
per-ex loss: 0.660888  [    2/   88]
per-ex loss: 0.736282  [    3/   88]
per-ex loss: 0.486318  [    4/   88]
per-ex loss: 0.758577  [    5/   88]
per-ex loss: 0.711981  [    6/   88]
per-ex loss: 0.814178  [    7/   88]
per-ex loss: 0.479668  [    8/   88]
per-ex loss: 0.555629  [    9/   88]
per-ex loss: 0.767012  [   10/   88]
per-ex loss: 0.484537  [   11/   88]
per-ex loss: 0.553866  [   12/   88]
per-ex loss: 0.517188  [   13/   88]
per-ex loss: 0.711227  [   14/   88]
per-ex loss: 0.713555  [   15/   88]
per-ex loss: 0.748599  [   16/   88]
per-ex loss: 0.504737  [   17/   88]
per-ex loss: 0.568971  [   18/   88]
per-ex loss: 0.798110  [   19/   88]
per-ex loss: 0.738295  [   20/   88]
per-ex loss: 0.570481  [   21/   88]
per-ex loss: 0.694077  [   22/   88]
per-ex loss: 0.706375  [   23/   88]
per-ex loss: 0.512571  [   24/   88]
per-ex loss: 0.733679  [   25/   88]
per-ex loss: 0.588359  [   26/   88]
per-ex loss: 0.740804  [   27/   88]
per-ex loss: 0.690736  [   28/   88]
per-ex loss: 0.558661  [   29/   88]
per-ex loss: 0.777030  [   30/   88]
per-ex loss: 0.637995  [   31/   88]
per-ex loss: 0.638360  [   32/   88]
per-ex loss: 0.582380  [   33/   88]
per-ex loss: 0.531727  [   34/   88]
per-ex loss: 0.609779  [   35/   88]
per-ex loss: 0.700263  [   36/   88]
per-ex loss: 0.707115  [   37/   88]
per-ex loss: 0.604033  [   38/   88]
per-ex loss: 0.712183  [   39/   88]
per-ex loss: 0.763607  [   40/   88]
per-ex loss: 0.753794  [   41/   88]
per-ex loss: 0.497834  [   42/   88]
per-ex loss: 0.569750  [   43/   88]
per-ex loss: 0.554765  [   44/   88]
per-ex loss: 0.712675  [   45/   88]
per-ex loss: 0.623222  [   46/   88]
per-ex loss: 0.604164  [   47/   88]
per-ex loss: 0.700538  [   48/   88]
per-ex loss: 0.454028  [   49/   88]
per-ex loss: 0.579476  [   50/   88]
per-ex loss: 0.670276  [   51/   88]
per-ex loss: 0.668115  [   52/   88]
per-ex loss: 0.610206  [   53/   88]
per-ex loss: 0.551386  [   54/   88]
per-ex loss: 0.521498  [   55/   88]
per-ex loss: 0.578165  [   56/   88]
per-ex loss: 0.544401  [   57/   88]
per-ex loss: 0.516774  [   58/   88]
per-ex loss: 0.555215  [   59/   88]
per-ex loss: 0.570906  [   60/   88]
per-ex loss: 0.514421  [   61/   88]
per-ex loss: 0.748037  [   62/   88]
per-ex loss: 0.735328  [   63/   88]
per-ex loss: 0.474415  [   64/   88]
per-ex loss: 0.759279  [   65/   88]
per-ex loss: 0.541051  [   66/   88]
per-ex loss: 0.706997  [   67/   88]
per-ex loss: 0.551049  [   68/   88]
per-ex loss: 0.661325  [   69/   88]
per-ex loss: 0.502106  [   70/   88]
per-ex loss: 0.499523  [   71/   88]
per-ex loss: 0.777107  [   72/   88]
per-ex loss: 0.647883  [   73/   88]
per-ex loss: 0.565277  [   74/   88]
per-ex loss: 0.660227  [   75/   88]
per-ex loss: 0.558723  [   76/   88]
per-ex loss: 0.578153  [   77/   88]
per-ex loss: 0.526361  [   78/   88]
per-ex loss: 0.787827  [   79/   88]
per-ex loss: 0.537593  [   80/   88]
per-ex loss: 0.721052  [   81/   88]
per-ex loss: 0.596394  [   82/   88]
per-ex loss: 0.545118  [   83/   88]
per-ex loss: 0.791903  [   84/   88]
per-ex loss: 0.624912  [   85/   88]
per-ex loss: 0.527750  [   86/   88]
per-ex loss: 0.805007  [   87/   88]
per-ex loss: 0.733918  [   88/   88]
Train Error: Avg loss: 0.63099923
validation Error: 
 Avg loss: 0.69178682 
 F1: 0.491316 
 Precision: 0.497628 
 Recall: 0.485163
 IoU: 0.325659

test Error: 
 Avg loss: 0.64333772 
 F1: 0.563444 
 Precision: 0.558049 
 Recall: 0.568944
 IoU: 0.392218

We have finished training iteration 66
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_64_.pth
per-ex loss: 0.521732  [    1/   88]
per-ex loss: 0.675422  [    2/   88]
per-ex loss: 0.776836  [    3/   88]
per-ex loss: 0.707228  [    4/   88]
per-ex loss: 0.576004  [    5/   88]
per-ex loss: 0.626330  [    6/   88]
per-ex loss: 0.790154  [    7/   88]
per-ex loss: 0.563777  [    8/   88]
per-ex loss: 0.579084  [    9/   88]
per-ex loss: 0.699190  [   10/   88]
per-ex loss: 0.732643  [   11/   88]
per-ex loss: 0.762449  [   12/   88]
per-ex loss: 0.741529  [   13/   88]
per-ex loss: 0.672522  [   14/   88]
per-ex loss: 0.653023  [   15/   88]
per-ex loss: 0.624810  [   16/   88]
per-ex loss: 0.526552  [   17/   88]
per-ex loss: 0.691693  [   18/   88]
per-ex loss: 0.530534  [   19/   88]
per-ex loss: 0.622277  [   20/   88]
per-ex loss: 0.549959  [   21/   88]
per-ex loss: 0.693540  [   22/   88]
per-ex loss: 0.786076  [   23/   88]
per-ex loss: 0.742694  [   24/   88]
per-ex loss: 0.512990  [   25/   88]
per-ex loss: 0.525281  [   26/   88]
per-ex loss: 0.746327  [   27/   88]
per-ex loss: 0.475654  [   28/   88]
per-ex loss: 0.592674  [   29/   88]
per-ex loss: 0.756198  [   30/   88]
per-ex loss: 0.525965  [   31/   88]
per-ex loss: 0.493285  [   32/   88]
per-ex loss: 0.649894  [   33/   88]
per-ex loss: 0.520592  [   34/   88]
per-ex loss: 0.544840  [   35/   88]
per-ex loss: 0.781767  [   36/   88]
per-ex loss: 0.778234  [   37/   88]
per-ex loss: 0.572059  [   38/   88]
per-ex loss: 0.533527  [   39/   88]
per-ex loss: 0.522275  [   40/   88]
per-ex loss: 0.553902  [   41/   88]
per-ex loss: 0.542182  [   42/   88]
per-ex loss: 0.522526  [   43/   88]
per-ex loss: 0.744330  [   44/   88]
per-ex loss: 0.592131  [   45/   88]
per-ex loss: 0.456153  [   46/   88]
per-ex loss: 0.694254  [   47/   88]
per-ex loss: 0.735531  [   48/   88]
per-ex loss: 0.561340  [   49/   88]
per-ex loss: 0.509975  [   50/   88]
per-ex loss: 0.637375  [   51/   88]
per-ex loss: 0.575744  [   52/   88]
per-ex loss: 0.567852  [   53/   88]
per-ex loss: 0.564726  [   54/   88]
per-ex loss: 0.690713  [   55/   88]
per-ex loss: 0.586410  [   56/   88]
per-ex loss: 0.791840  [   57/   88]
per-ex loss: 0.530537  [   58/   88]
per-ex loss: 0.619862  [   59/   88]
per-ex loss: 0.488967  [   60/   88]
per-ex loss: 0.608258  [   61/   88]
per-ex loss: 0.713816  [   62/   88]
per-ex loss: 0.717268  [   63/   88]
per-ex loss: 0.578098  [   64/   88]
per-ex loss: 0.599910  [   65/   88]
per-ex loss: 0.725950  [   66/   88]
per-ex loss: 0.730676  [   67/   88]
per-ex loss: 0.470204  [   68/   88]
per-ex loss: 0.712290  [   69/   88]
per-ex loss: 0.512999  [   70/   88]
per-ex loss: 0.668865  [   71/   88]
per-ex loss: 0.503260  [   72/   88]
per-ex loss: 0.711393  [   73/   88]
per-ex loss: 0.796527  [   74/   88]
per-ex loss: 0.538998  [   75/   88]
per-ex loss: 0.765701  [   76/   88]
per-ex loss: 0.655139  [   77/   88]
per-ex loss: 0.505584  [   78/   88]
per-ex loss: 0.769019  [   79/   88]
per-ex loss: 0.614982  [   80/   88]
per-ex loss: 0.582303  [   81/   88]
per-ex loss: 0.734547  [   82/   88]
per-ex loss: 0.731709  [   83/   88]
per-ex loss: 0.572880  [   84/   88]
per-ex loss: 0.668579  [   85/   88]
per-ex loss: 0.705683  [   86/   88]
per-ex loss: 0.691773  [   87/   88]
per-ex loss: 0.481286  [   88/   88]
Train Error: Avg loss: 0.62967804
validation Error: 
 Avg loss: 0.68715102 
 F1: 0.499969 
 Precision: 0.563802 
 Recall: 0.449120
 IoU: 0.333306

test Error: 
 Avg loss: 0.64210193 
 F1: 0.566643 
 Precision: 0.620317 
 Recall: 0.521518
 IoU: 0.395326

We have finished training iteration 67
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_65_.pth
per-ex loss: 0.624228  [    1/   88]
per-ex loss: 0.794407  [    2/   88]
per-ex loss: 0.473098  [    3/   88]
per-ex loss: 0.740414  [    4/   88]
per-ex loss: 0.517989  [    5/   88]
per-ex loss: 0.660218  [    6/   88]
per-ex loss: 0.710196  [    7/   88]
per-ex loss: 0.729564  [    8/   88]
per-ex loss: 0.747238  [    9/   88]
per-ex loss: 0.599865  [   10/   88]
per-ex loss: 0.516608  [   11/   88]
per-ex loss: 0.716397  [   12/   88]
per-ex loss: 0.498202  [   13/   88]
per-ex loss: 0.775165  [   14/   88]
per-ex loss: 0.594608  [   15/   88]
per-ex loss: 0.569701  [   16/   88]
per-ex loss: 0.626020  [   17/   88]
per-ex loss: 0.666485  [   18/   88]
per-ex loss: 0.563048  [   19/   88]
per-ex loss: 0.603987  [   20/   88]
per-ex loss: 0.600002  [   21/   88]
per-ex loss: 0.711025  [   22/   88]
per-ex loss: 0.699927  [   23/   88]
per-ex loss: 0.513920  [   24/   88]
per-ex loss: 0.781458  [   25/   88]
per-ex loss: 0.668557  [   26/   88]
per-ex loss: 0.763873  [   27/   88]
per-ex loss: 0.616902  [   28/   88]
per-ex loss: 0.554002  [   29/   88]
per-ex loss: 0.664477  [   30/   88]
per-ex loss: 0.573099  [   31/   88]
per-ex loss: 0.517634  [   32/   88]
per-ex loss: 0.707363  [   33/   88]
per-ex loss: 0.784331  [   34/   88]
per-ex loss: 0.572432  [   35/   88]
per-ex loss: 0.523066  [   36/   88]
per-ex loss: 0.714074  [   37/   88]
per-ex loss: 0.790966  [   38/   88]
per-ex loss: 0.511174  [   39/   88]
per-ex loss: 0.547377  [   40/   88]
per-ex loss: 0.564823  [   41/   88]
per-ex loss: 0.500017  [   42/   88]
per-ex loss: 0.702189  [   43/   88]
per-ex loss: 0.639755  [   44/   88]
per-ex loss: 0.561673  [   45/   88]
per-ex loss: 0.797412  [   46/   88]
per-ex loss: 0.713457  [   47/   88]
per-ex loss: 0.592577  [   48/   88]
per-ex loss: 0.721180  [   49/   88]
per-ex loss: 0.774696  [   50/   88]
per-ex loss: 0.582638  [   51/   88]
per-ex loss: 0.502413  [   52/   88]
per-ex loss: 0.672737  [   53/   88]
per-ex loss: 0.554565  [   54/   88]
per-ex loss: 0.707313  [   55/   88]
per-ex loss: 0.733692  [   56/   88]
per-ex loss: 0.464473  [   57/   88]
per-ex loss: 0.649846  [   58/   88]
per-ex loss: 0.564055  [   59/   88]
per-ex loss: 0.520089  [   60/   88]
per-ex loss: 0.720128  [   61/   88]
per-ex loss: 0.642069  [   62/   88]
per-ex loss: 0.652752  [   63/   88]
per-ex loss: 0.544339  [   64/   88]
per-ex loss: 0.539646  [   65/   88]
per-ex loss: 0.560704  [   66/   88]
per-ex loss: 0.763258  [   67/   88]
per-ex loss: 0.695627  [   68/   88]
per-ex loss: 0.487361  [   69/   88]
per-ex loss: 0.697309  [   70/   88]
per-ex loss: 0.531032  [   71/   88]
per-ex loss: 0.591860  [   72/   88]
per-ex loss: 0.742050  [   73/   88]
per-ex loss: 0.523562  [   74/   88]
per-ex loss: 0.563778  [   75/   88]
per-ex loss: 0.800136  [   76/   88]
per-ex loss: 0.771228  [   77/   88]
per-ex loss: 0.580477  [   78/   88]
per-ex loss: 0.731025  [   79/   88]
per-ex loss: 0.479970  [   80/   88]
per-ex loss: 0.680698  [   81/   88]
per-ex loss: 0.484197  [   82/   88]
per-ex loss: 0.591042  [   83/   88]
per-ex loss: 0.521693  [   84/   88]
per-ex loss: 0.546853  [   85/   88]
per-ex loss: 0.528582  [   86/   88]
per-ex loss: 0.725755  [   87/   88]
per-ex loss: 0.512126  [   88/   88]
Train Error: Avg loss: 0.62809038
validation Error: 
 Avg loss: 0.68505556 
 F1: 0.500688 
 Precision: 0.631261 
 Recall: 0.414874
 IoU: 0.333946

test Error: 
 Avg loss: 0.64195698 
 F1: 0.567024 
 Precision: 0.653658 
 Recall: 0.500667
 IoU: 0.395696

We have finished training iteration 68
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_66_.pth
per-ex loss: 0.553675  [    1/   88]
per-ex loss: 0.760723  [    2/   88]
per-ex loss: 0.702280  [    3/   88]
per-ex loss: 0.756445  [    4/   88]
per-ex loss: 0.570855  [    5/   88]
per-ex loss: 0.625078  [    6/   88]
per-ex loss: 0.542695  [    7/   88]
per-ex loss: 0.754206  [    8/   88]
per-ex loss: 0.785034  [    9/   88]
per-ex loss: 0.759431  [   10/   88]
per-ex loss: 0.642639  [   11/   88]
per-ex loss: 0.725452  [   12/   88]
per-ex loss: 0.559097  [   13/   88]
per-ex loss: 0.465231  [   14/   88]
per-ex loss: 0.702447  [   15/   88]
per-ex loss: 0.609707  [   16/   88]
per-ex loss: 0.580164  [   17/   88]
per-ex loss: 0.715152  [   18/   88]
per-ex loss: 0.543700  [   19/   88]
per-ex loss: 0.732709  [   20/   88]
per-ex loss: 0.574395  [   21/   88]
per-ex loss: 0.720454  [   22/   88]
per-ex loss: 0.741641  [   23/   88]
per-ex loss: 0.464270  [   24/   88]
per-ex loss: 0.717652  [   25/   88]
per-ex loss: 0.594122  [   26/   88]
per-ex loss: 0.596251  [   27/   88]
per-ex loss: 0.583793  [   28/   88]
per-ex loss: 0.542491  [   29/   88]
per-ex loss: 0.490749  [   30/   88]
per-ex loss: 0.697731  [   31/   88]
per-ex loss: 0.512967  [   32/   88]
per-ex loss: 0.491768  [   33/   88]
per-ex loss: 0.482982  [   34/   88]
per-ex loss: 0.526759  [   35/   88]
per-ex loss: 0.711888  [   36/   88]
per-ex loss: 0.559011  [   37/   88]
per-ex loss: 0.778192  [   38/   88]
per-ex loss: 0.684878  [   39/   88]
per-ex loss: 0.690696  [   40/   88]
per-ex loss: 0.657477  [   41/   88]
per-ex loss: 0.794610  [   42/   88]
per-ex loss: 0.527590  [   43/   88]
per-ex loss: 0.739147  [   44/   88]
per-ex loss: 0.792807  [   45/   88]
per-ex loss: 0.601112  [   46/   88]
per-ex loss: 0.566674  [   47/   88]
per-ex loss: 0.682092  [   48/   88]
per-ex loss: 0.726986  [   49/   88]
per-ex loss: 0.508056  [   50/   88]
per-ex loss: 0.518695  [   51/   88]
per-ex loss: 0.574597  [   52/   88]
per-ex loss: 0.592872  [   53/   88]
per-ex loss: 0.737223  [   54/   88]
per-ex loss: 0.566974  [   55/   88]
per-ex loss: 0.698776  [   56/   88]
per-ex loss: 0.525338  [   57/   88]
per-ex loss: 0.578871  [   58/   88]
per-ex loss: 0.670316  [   59/   88]
per-ex loss: 0.491895  [   60/   88]
per-ex loss: 0.545973  [   61/   88]
per-ex loss: 0.719625  [   62/   88]
per-ex loss: 0.735264  [   63/   88]
per-ex loss: 0.765065  [   64/   88]
per-ex loss: 0.595441  [   65/   88]
per-ex loss: 0.608747  [   66/   88]
per-ex loss: 0.682644  [   67/   88]
per-ex loss: 0.591153  [   68/   88]
per-ex loss: 0.500846  [   69/   88]
per-ex loss: 0.654011  [   70/   88]
per-ex loss: 0.566937  [   71/   88]
per-ex loss: 0.563017  [   72/   88]
per-ex loss: 0.792683  [   73/   88]
per-ex loss: 0.519844  [   74/   88]
per-ex loss: 0.702692  [   75/   88]
per-ex loss: 0.556178  [   76/   88]
per-ex loss: 0.708998  [   77/   88]
per-ex loss: 0.623577  [   78/   88]
per-ex loss: 0.521718  [   79/   88]
per-ex loss: 0.774808  [   80/   88]
per-ex loss: 0.566758  [   81/   88]
per-ex loss: 0.552638  [   82/   88]
per-ex loss: 0.564169  [   83/   88]
per-ex loss: 0.658207  [   84/   88]
per-ex loss: 0.709016  [   85/   88]
per-ex loss: 0.804086  [   86/   88]
per-ex loss: 0.649357  [   87/   88]
per-ex loss: 0.572095  [   88/   88]
Train Error: Avg loss: 0.63189850
validation Error: 
 Avg loss: 0.68501263 
 F1: 0.500589 
 Precision: 0.516463 
 Recall: 0.485661
 IoU: 0.333857

test Error: 
 Avg loss: 0.64515839 
 F1: 0.564071 
 Precision: 0.550169 
 Recall: 0.578695
 IoU: 0.392827

We have finished training iteration 69
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_67_.pth
per-ex loss: 0.708282  [    1/   88]
per-ex loss: 0.774433  [    2/   88]
per-ex loss: 0.737178  [    3/   88]
per-ex loss: 0.663915  [    4/   88]
per-ex loss: 0.738894  [    5/   88]
per-ex loss: 0.509873  [    6/   88]
per-ex loss: 0.670606  [    7/   88]
per-ex loss: 0.626477  [    8/   88]
per-ex loss: 0.590267  [    9/   88]
per-ex loss: 0.700763  [   10/   88]
per-ex loss: 0.523765  [   11/   88]
per-ex loss: 0.813143  [   12/   88]
per-ex loss: 0.734287  [   13/   88]
per-ex loss: 0.473373  [   14/   88]
per-ex loss: 0.546293  [   15/   88]
per-ex loss: 0.802419  [   16/   88]
per-ex loss: 0.722743  [   17/   88]
per-ex loss: 0.510279  [   18/   88]
per-ex loss: 0.559551  [   19/   88]
per-ex loss: 0.534816  [   20/   88]
per-ex loss: 0.547713  [   21/   88]
per-ex loss: 0.764642  [   22/   88]
per-ex loss: 0.662427  [   23/   88]
per-ex loss: 0.788418  [   24/   88]
per-ex loss: 0.533375  [   25/   88]
per-ex loss: 0.523765  [   26/   88]
per-ex loss: 0.500893  [   27/   88]
per-ex loss: 0.582183  [   28/   88]
per-ex loss: 0.753583  [   29/   88]
per-ex loss: 0.545905  [   30/   88]
per-ex loss: 0.651453  [   31/   88]
per-ex loss: 0.564417  [   32/   88]
per-ex loss: 0.707017  [   33/   88]
per-ex loss: 0.764031  [   34/   88]
per-ex loss: 0.744613  [   35/   88]
per-ex loss: 0.445840  [   36/   88]
per-ex loss: 0.696152  [   37/   88]
per-ex loss: 0.554133  [   38/   88]
per-ex loss: 0.693505  [   39/   88]
per-ex loss: 0.482183  [   40/   88]
per-ex loss: 0.737030  [   41/   88]
per-ex loss: 0.741410  [   42/   88]
per-ex loss: 0.660621  [   43/   88]
per-ex loss: 0.512212  [   44/   88]
per-ex loss: 0.794030  [   45/   88]
per-ex loss: 0.492976  [   46/   88]
per-ex loss: 0.627429  [   47/   88]
per-ex loss: 0.593786  [   48/   88]
per-ex loss: 0.602584  [   49/   88]
per-ex loss: 0.577757  [   50/   88]
per-ex loss: 0.585948  [   51/   88]
per-ex loss: 0.590945  [   52/   88]
per-ex loss: 0.565179  [   53/   88]
per-ex loss: 0.496297  [   54/   88]
per-ex loss: 0.746298  [   55/   88]
per-ex loss: 0.721937  [   56/   88]
per-ex loss: 0.480467  [   57/   88]
per-ex loss: 0.521808  [   58/   88]
per-ex loss: 0.555566  [   59/   88]
per-ex loss: 0.505917  [   60/   88]
per-ex loss: 0.728734  [   61/   88]
per-ex loss: 0.563077  [   62/   88]
per-ex loss: 0.708088  [   63/   88]
per-ex loss: 0.721487  [   64/   88]
per-ex loss: 0.589428  [   65/   88]
per-ex loss: 0.791803  [   66/   88]
per-ex loss: 0.638688  [   67/   88]
per-ex loss: 0.538352  [   68/   88]
per-ex loss: 0.549705  [   69/   88]
per-ex loss: 0.599434  [   70/   88]
per-ex loss: 0.555964  [   71/   88]
per-ex loss: 0.567470  [   72/   88]
per-ex loss: 0.674076  [   73/   88]
per-ex loss: 0.696112  [   74/   88]
per-ex loss: 0.548892  [   75/   88]
per-ex loss: 0.800488  [   76/   88]
per-ex loss: 0.534735  [   77/   88]
per-ex loss: 0.530135  [   78/   88]
per-ex loss: 0.631152  [   79/   88]
per-ex loss: 0.767495  [   80/   88]
per-ex loss: 0.681559  [   81/   88]
per-ex loss: 0.733841  [   82/   88]
per-ex loss: 0.675180  [   83/   88]
per-ex loss: 0.519626  [   84/   88]
per-ex loss: 0.478246  [   85/   88]
per-ex loss: 0.763368  [   86/   88]
per-ex loss: 0.712415  [   87/   88]
per-ex loss: 0.561762  [   88/   88]
Train Error: Avg loss: 0.62976265
validation Error: 
 Avg loss: 0.68915737 
 F1: 0.497055 
 Precision: 0.562938 
 Recall: 0.444978
 IoU: 0.330721

test Error: 
 Avg loss: 0.64731839 
 F1: 0.557414 
 Precision: 0.599788 
 Recall: 0.520631
 IoU: 0.386399

We have finished training iteration 70
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_62_.pth
per-ex loss: 0.597604  [    1/   88]
per-ex loss: 0.559430  [    2/   88]
per-ex loss: 0.512002  [    3/   88]
per-ex loss: 0.758491  [    4/   88]
per-ex loss: 0.703008  [    5/   88]
per-ex loss: 0.501824  [    6/   88]
per-ex loss: 0.530706  [    7/   88]
per-ex loss: 0.649271  [    8/   88]
per-ex loss: 0.621945  [    9/   88]
per-ex loss: 0.510657  [   10/   88]
per-ex loss: 0.537171  [   11/   88]
per-ex loss: 0.497483  [   12/   88]
per-ex loss: 0.726198  [   13/   88]
per-ex loss: 0.786115  [   14/   88]
per-ex loss: 0.604261  [   15/   88]
per-ex loss: 0.675332  [   16/   88]
per-ex loss: 0.564768  [   17/   88]
per-ex loss: 0.511494  [   18/   88]
per-ex loss: 0.692557  [   19/   88]
per-ex loss: 0.767077  [   20/   88]
per-ex loss: 0.717979  [   21/   88]
per-ex loss: 0.592283  [   22/   88]
per-ex loss: 0.533927  [   23/   88]
per-ex loss: 0.740152  [   24/   88]
per-ex loss: 0.543553  [   25/   88]
per-ex loss: 0.462340  [   26/   88]
per-ex loss: 0.651411  [   27/   88]
per-ex loss: 0.559433  [   28/   88]
per-ex loss: 0.739005  [   29/   88]
per-ex loss: 0.491509  [   30/   88]
per-ex loss: 0.578803  [   31/   88]
per-ex loss: 0.762515  [   32/   88]
per-ex loss: 0.658500  [   33/   88]
per-ex loss: 0.500857  [   34/   88]
per-ex loss: 0.696638  [   35/   88]
per-ex loss: 0.768485  [   36/   88]
per-ex loss: 0.562379  [   37/   88]
per-ex loss: 0.711489  [   38/   88]
per-ex loss: 0.497677  [   39/   88]
per-ex loss: 0.595655  [   40/   88]
per-ex loss: 0.541563  [   41/   88]
per-ex loss: 0.703589  [   42/   88]
per-ex loss: 0.741532  [   43/   88]
per-ex loss: 0.618737  [   44/   88]
per-ex loss: 0.594514  [   45/   88]
per-ex loss: 0.453064  [   46/   88]
per-ex loss: 0.498087  [   47/   88]
per-ex loss: 0.730107  [   48/   88]
per-ex loss: 0.745180  [   49/   88]
per-ex loss: 0.584559  [   50/   88]
per-ex loss: 0.804771  [   51/   88]
per-ex loss: 0.546605  [   52/   88]
per-ex loss: 0.635129  [   53/   88]
per-ex loss: 0.538396  [   54/   88]
per-ex loss: 0.694461  [   55/   88]
per-ex loss: 0.537059  [   56/   88]
per-ex loss: 0.794266  [   57/   88]
per-ex loss: 0.643852  [   58/   88]
per-ex loss: 0.571910  [   59/   88]
per-ex loss: 0.553158  [   60/   88]
per-ex loss: 0.438675  [   61/   88]
per-ex loss: 0.727900  [   62/   88]
per-ex loss: 0.496508  [   63/   88]
per-ex loss: 0.705893  [   64/   88]
per-ex loss: 0.592714  [   65/   88]
per-ex loss: 0.555624  [   66/   88]
per-ex loss: 0.706618  [   67/   88]
per-ex loss: 0.670076  [   68/   88]
per-ex loss: 0.580857  [   69/   88]
per-ex loss: 0.560006  [   70/   88]
per-ex loss: 0.497783  [   71/   88]
per-ex loss: 0.777211  [   72/   88]
per-ex loss: 0.543678  [   73/   88]
per-ex loss: 0.776307  [   74/   88]
per-ex loss: 0.568384  [   75/   88]
per-ex loss: 0.595830  [   76/   88]
per-ex loss: 0.657203  [   77/   88]
per-ex loss: 0.763573  [   78/   88]
per-ex loss: 0.749996  [   79/   88]
per-ex loss: 0.705788  [   80/   88]
per-ex loss: 0.561629  [   81/   88]
per-ex loss: 0.625440  [   82/   88]
per-ex loss: 0.793286  [   83/   88]
per-ex loss: 0.507025  [   84/   88]
per-ex loss: 0.745235  [   85/   88]
per-ex loss: 0.693871  [   86/   88]
per-ex loss: 0.598129  [   87/   88]
per-ex loss: 0.724999  [   88/   88]
Train Error: Avg loss: 0.62639502
validation Error: 
 Avg loss: 0.68733296 
 F1: 0.497781 
 Precision: 0.510107 
 Recall: 0.486037
 IoU: 0.331364

test Error: 
 Avg loss: 0.64645143 
 F1: 0.560279 
 Precision: 0.556256 
 Recall: 0.564362
 IoU: 0.389158

We have finished training iteration 71
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_46_.pth
per-ex loss: 0.715046  [    1/   88]
per-ex loss: 0.788894  [    2/   88]
per-ex loss: 0.501214  [    3/   88]
per-ex loss: 0.672660  [    4/   88]
per-ex loss: 0.677456  [    5/   88]
per-ex loss: 0.704642  [    6/   88]
per-ex loss: 0.569633  [    7/   88]
per-ex loss: 0.634139  [    8/   88]
per-ex loss: 0.615397  [    9/   88]
per-ex loss: 0.531173  [   10/   88]
per-ex loss: 0.607377  [   11/   88]
per-ex loss: 0.658283  [   12/   88]
per-ex loss: 0.574792  [   13/   88]
per-ex loss: 0.747921  [   14/   88]
per-ex loss: 0.504758  [   15/   88]
per-ex loss: 0.745481  [   16/   88]
per-ex loss: 0.552558  [   17/   88]
per-ex loss: 0.574567  [   18/   88]
per-ex loss: 0.511830  [   19/   88]
per-ex loss: 0.491979  [   20/   88]
per-ex loss: 0.817234  [   21/   88]
per-ex loss: 0.538388  [   22/   88]
per-ex loss: 0.857293  [   23/   88]
per-ex loss: 0.740048  [   24/   88]
per-ex loss: 0.691622  [   25/   88]
per-ex loss: 0.658937  [   26/   88]
per-ex loss: 0.747466  [   27/   88]
per-ex loss: 0.585211  [   28/   88]
per-ex loss: 0.576886  [   29/   88]
per-ex loss: 0.550812  [   30/   88]
per-ex loss: 0.654894  [   31/   88]
per-ex loss: 0.572959  [   32/   88]
per-ex loss: 0.745014  [   33/   88]
per-ex loss: 0.537248  [   34/   88]
per-ex loss: 0.574164  [   35/   88]
per-ex loss: 0.738418  [   36/   88]
per-ex loss: 0.758406  [   37/   88]
per-ex loss: 0.521080  [   38/   88]
per-ex loss: 0.628685  [   39/   88]
per-ex loss: 0.576821  [   40/   88]
per-ex loss: 0.546105  [   41/   88]
per-ex loss: 0.720511  [   42/   88]
per-ex loss: 0.537920  [   43/   88]
per-ex loss: 0.520084  [   44/   88]
per-ex loss: 0.476264  [   45/   88]
per-ex loss: 0.547011  [   46/   88]
per-ex loss: 0.536895  [   47/   88]
per-ex loss: 0.501275  [   48/   88]
per-ex loss: 0.798515  [   49/   88]
per-ex loss: 0.837527  [   50/   88]
per-ex loss: 0.773326  [   51/   88]
per-ex loss: 0.747168  [   52/   88]
per-ex loss: 0.585909  [   53/   88]
per-ex loss: 0.740612  [   54/   88]
per-ex loss: 0.519620  [   55/   88]
per-ex loss: 0.587116  [   56/   88]
per-ex loss: 0.764998  [   57/   88]
per-ex loss: 0.543574  [   58/   88]
per-ex loss: 0.709795  [   59/   88]
per-ex loss: 0.596149  [   60/   88]
per-ex loss: 0.697286  [   61/   88]
per-ex loss: 0.766235  [   62/   88]
per-ex loss: 0.578347  [   63/   88]
per-ex loss: 0.672657  [   64/   88]
per-ex loss: 0.694575  [   65/   88]
per-ex loss: 0.729386  [   66/   88]
per-ex loss: 0.690075  [   67/   88]
per-ex loss: 0.551851  [   68/   88]
per-ex loss: 0.704512  [   69/   88]
per-ex loss: 0.631556  [   70/   88]
per-ex loss: 0.383366  [   71/   88]
per-ex loss: 0.534359  [   72/   88]
per-ex loss: 0.524863  [   73/   88]
per-ex loss: 0.667638  [   74/   88]
per-ex loss: 0.548384  [   75/   88]
per-ex loss: 0.514259  [   76/   88]
per-ex loss: 0.494333  [   77/   88]
per-ex loss: 0.586409  [   78/   88]
per-ex loss: 0.637352  [   79/   88]
per-ex loss: 0.626698  [   80/   88]
per-ex loss: 0.709055  [   81/   88]
per-ex loss: 0.728148  [   82/   88]
per-ex loss: 0.755527  [   83/   88]
per-ex loss: 0.596504  [   84/   88]
per-ex loss: 0.700624  [   85/   88]
per-ex loss: 0.459911  [   86/   88]
per-ex loss: 0.591915  [   87/   88]
per-ex loss: 0.768445  [   88/   88]
Train Error: Avg loss: 0.63165940
validation Error: 
 Avg loss: 0.69533246 
 F1: 0.493327 
 Precision: 0.535030 
 Recall: 0.457655
 IoU: 0.327428

test Error: 
 Avg loss: 0.64767593 
 F1: 0.557418 
 Precision: 0.585324 
 Recall: 0.532052
 IoU: 0.386403

We have finished training iteration 72
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_70_.pth
per-ex loss: 0.696360  [    1/   88]
per-ex loss: 0.708016  [    2/   88]
per-ex loss: 0.696452  [    3/   88]
per-ex loss: 0.448731  [    4/   88]
per-ex loss: 0.424119  [    5/   88]
per-ex loss: 0.710016  [    6/   88]
per-ex loss: 0.702289  [    7/   88]
per-ex loss: 0.613523  [    8/   88]
per-ex loss: 0.733911  [    9/   88]
per-ex loss: 0.787937  [   10/   88]
per-ex loss: 0.583687  [   11/   88]
per-ex loss: 0.718744  [   12/   88]
per-ex loss: 0.744130  [   13/   88]
per-ex loss: 0.556917  [   14/   88]
per-ex loss: 0.585510  [   15/   88]
per-ex loss: 0.490141  [   16/   88]
per-ex loss: 0.607746  [   17/   88]
per-ex loss: 0.819471  [   18/   88]
per-ex loss: 0.592265  [   19/   88]
per-ex loss: 0.738714  [   20/   88]
per-ex loss: 0.635383  [   21/   88]
per-ex loss: 0.717266  [   22/   88]
per-ex loss: 0.788309  [   23/   88]
per-ex loss: 0.711743  [   24/   88]
per-ex loss: 0.563546  [   25/   88]
per-ex loss: 0.541090  [   26/   88]
per-ex loss: 0.652252  [   27/   88]
per-ex loss: 0.551707  [   28/   88]
per-ex loss: 0.589137  [   29/   88]
per-ex loss: 0.682535  [   30/   88]
per-ex loss: 0.498079  [   31/   88]
per-ex loss: 0.641583  [   32/   88]
per-ex loss: 0.651191  [   33/   88]
per-ex loss: 0.695349  [   34/   88]
per-ex loss: 0.740634  [   35/   88]
per-ex loss: 0.497601  [   36/   88]
per-ex loss: 0.789430  [   37/   88]
per-ex loss: 0.576556  [   38/   88]
per-ex loss: 0.795364  [   39/   88]
per-ex loss: 0.487940  [   40/   88]
per-ex loss: 0.505348  [   41/   88]
per-ex loss: 0.781799  [   42/   88]
per-ex loss: 0.586100  [   43/   88]
per-ex loss: 0.789778  [   44/   88]
per-ex loss: 0.687045  [   45/   88]
per-ex loss: 0.790223  [   46/   88]
per-ex loss: 0.512992  [   47/   88]
per-ex loss: 0.682394  [   48/   88]
per-ex loss: 0.538859  [   49/   88]
per-ex loss: 0.751658  [   50/   88]
per-ex loss: 0.738951  [   51/   88]
per-ex loss: 0.510108  [   52/   88]
per-ex loss: 0.569024  [   53/   88]
per-ex loss: 0.571404  [   54/   88]
per-ex loss: 0.504288  [   55/   88]
per-ex loss: 0.606383  [   56/   88]
per-ex loss: 0.521885  [   57/   88]
per-ex loss: 0.624951  [   58/   88]
per-ex loss: 0.531431  [   59/   88]
per-ex loss: 0.685468  [   60/   88]
per-ex loss: 0.739603  [   61/   88]
per-ex loss: 0.709319  [   62/   88]
per-ex loss: 0.696915  [   63/   88]
per-ex loss: 0.719111  [   64/   88]
per-ex loss: 0.648350  [   65/   88]
per-ex loss: 0.528944  [   66/   88]
per-ex loss: 0.635871  [   67/   88]
per-ex loss: 0.505250  [   68/   88]
per-ex loss: 0.531617  [   69/   88]
per-ex loss: 0.534071  [   70/   88]
per-ex loss: 0.814271  [   71/   88]
per-ex loss: 0.532475  [   72/   88]
per-ex loss: 0.594475  [   73/   88]
per-ex loss: 0.642637  [   74/   88]
per-ex loss: 0.558746  [   75/   88]
per-ex loss: 0.622796  [   76/   88]
per-ex loss: 0.571247  [   77/   88]
per-ex loss: 0.514621  [   78/   88]
per-ex loss: 0.733090  [   79/   88]
per-ex loss: 0.562510  [   80/   88]
per-ex loss: 0.713893  [   81/   88]
per-ex loss: 0.566328  [   82/   88]
per-ex loss: 0.458350  [   83/   88]
per-ex loss: 0.583509  [   84/   88]
per-ex loss: 0.773977  [   85/   88]
per-ex loss: 0.565412  [   86/   88]
per-ex loss: 0.517036  [   87/   88]
per-ex loss: 0.489402  [   88/   88]
Train Error: Avg loss: 0.62862823
validation Error: 
 Avg loss: 0.68637904 
 F1: 0.501652 
 Precision: 0.635244 
 Recall: 0.414486
 IoU: 0.334804

test Error: 
 Avg loss: 0.64622152 
 F1: 0.561532 
 Precision: 0.660777 
 Recall: 0.488206
 IoU: 0.390368

We have finished training iteration 73
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_71_.pth
per-ex loss: 0.590812  [    1/   88]
per-ex loss: 0.779526  [    2/   88]
per-ex loss: 0.541830  [    3/   88]
per-ex loss: 0.686973  [    4/   88]
per-ex loss: 0.744682  [    5/   88]
per-ex loss: 0.541076  [    6/   88]
per-ex loss: 0.396882  [    7/   88]
per-ex loss: 0.637373  [    8/   88]
per-ex loss: 0.557115  [    9/   88]
per-ex loss: 0.481398  [   10/   88]
per-ex loss: 0.560474  [   11/   88]
per-ex loss: 0.690796  [   12/   88]
per-ex loss: 0.718166  [   13/   88]
per-ex loss: 0.715755  [   14/   88]
per-ex loss: 0.758613  [   15/   88]
per-ex loss: 0.545738  [   16/   88]
per-ex loss: 0.641322  [   17/   88]
per-ex loss: 0.810934  [   18/   88]
per-ex loss: 0.496038  [   19/   88]
per-ex loss: 0.569903  [   20/   88]
per-ex loss: 0.542089  [   21/   88]
per-ex loss: 0.579480  [   22/   88]
per-ex loss: 0.594721  [   23/   88]
per-ex loss: 0.480419  [   24/   88]
per-ex loss: 0.733995  [   25/   88]
per-ex loss: 0.608104  [   26/   88]
per-ex loss: 0.570918  [   27/   88]
per-ex loss: 0.822087  [   28/   88]
per-ex loss: 0.505187  [   29/   88]
per-ex loss: 0.538700  [   30/   88]
per-ex loss: 0.583827  [   31/   88]
per-ex loss: 0.554114  [   32/   88]
per-ex loss: 0.573543  [   33/   88]
per-ex loss: 0.535641  [   34/   88]
per-ex loss: 0.532712  [   35/   88]
per-ex loss: 0.506585  [   36/   88]
per-ex loss: 0.771074  [   37/   88]
per-ex loss: 0.503788  [   38/   88]
per-ex loss: 0.767247  [   39/   88]
per-ex loss: 0.717645  [   40/   88]
per-ex loss: 0.792230  [   41/   88]
per-ex loss: 0.607973  [   42/   88]
per-ex loss: 0.594717  [   43/   88]
per-ex loss: 0.732114  [   44/   88]
per-ex loss: 0.569140  [   45/   88]
per-ex loss: 0.494769  [   46/   88]
per-ex loss: 0.687378  [   47/   88]
per-ex loss: 0.712816  [   48/   88]
per-ex loss: 0.742269  [   49/   88]
per-ex loss: 0.467232  [   50/   88]
per-ex loss: 0.554219  [   51/   88]
per-ex loss: 0.792856  [   52/   88]
per-ex loss: 0.673848  [   53/   88]
per-ex loss: 0.751813  [   54/   88]
per-ex loss: 0.706598  [   55/   88]
per-ex loss: 0.588780  [   56/   88]
per-ex loss: 0.661255  [   57/   88]
per-ex loss: 0.672739  [   58/   88]
per-ex loss: 0.647487  [   59/   88]
per-ex loss: 0.501979  [   60/   88]
per-ex loss: 0.733855  [   61/   88]
per-ex loss: 0.709765  [   62/   88]
per-ex loss: 0.729215  [   63/   88]
per-ex loss: 0.701751  [   64/   88]
per-ex loss: 0.660943  [   65/   88]
per-ex loss: 0.591594  [   66/   88]
per-ex loss: 0.559524  [   67/   88]
per-ex loss: 0.603281  [   68/   88]
per-ex loss: 0.506198  [   69/   88]
per-ex loss: 0.771658  [   70/   88]
per-ex loss: 0.799169  [   71/   88]
per-ex loss: 0.494219  [   72/   88]
per-ex loss: 0.564653  [   73/   88]
per-ex loss: 0.495883  [   74/   88]
per-ex loss: 0.590921  [   75/   88]
per-ex loss: 0.574696  [   76/   88]
per-ex loss: 0.666204  [   77/   88]
per-ex loss: 0.795733  [   78/   88]
per-ex loss: 0.786257  [   79/   88]
per-ex loss: 0.510609  [   80/   88]
per-ex loss: 0.537180  [   81/   88]
per-ex loss: 0.523754  [   82/   88]
per-ex loss: 0.694171  [   83/   88]
per-ex loss: 0.585068  [   84/   88]
per-ex loss: 0.772505  [   85/   88]
per-ex loss: 0.707739  [   86/   88]
per-ex loss: 0.740629  [   87/   88]
per-ex loss: 0.625517  [   88/   88]
Train Error: Avg loss: 0.63036573
validation Error: 
 Avg loss: 0.68817481 
 F1: 0.500309 
 Precision: 0.535224 
 Recall: 0.469670
 IoU: 0.333608

test Error: 
 Avg loss: 0.64161412 
 F1: 0.564799 
 Precision: 0.572139 
 Recall: 0.557645
 IoU: 0.393533

We have finished training iteration 74
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_72_.pth
per-ex loss: 0.521317  [    1/   88]
per-ex loss: 0.745494  [    2/   88]
per-ex loss: 0.491431  [    3/   88]
per-ex loss: 0.465573  [    4/   88]
per-ex loss: 0.581722  [    5/   88]
per-ex loss: 0.504011  [    6/   88]
per-ex loss: 0.536306  [    7/   88]
per-ex loss: 0.604712  [    8/   88]
per-ex loss: 0.513725  [    9/   88]
per-ex loss: 0.804539  [   10/   88]
per-ex loss: 0.646671  [   11/   88]
per-ex loss: 0.484368  [   12/   88]
per-ex loss: 0.715020  [   13/   88]
per-ex loss: 0.652036  [   14/   88]
per-ex loss: 0.598541  [   15/   88]
per-ex loss: 0.794450  [   16/   88]
per-ex loss: 0.635714  [   17/   88]
per-ex loss: 0.706094  [   18/   88]
per-ex loss: 0.665011  [   19/   88]
per-ex loss: 0.741115  [   20/   88]
per-ex loss: 0.546740  [   21/   88]
per-ex loss: 0.592606  [   22/   88]
per-ex loss: 0.575145  [   23/   88]
per-ex loss: 0.559077  [   24/   88]
per-ex loss: 0.529082  [   25/   88]
per-ex loss: 0.726245  [   26/   88]
per-ex loss: 0.766567  [   27/   88]
per-ex loss: 0.797988  [   28/   88]
per-ex loss: 0.706117  [   29/   88]
per-ex loss: 0.650925  [   30/   88]
per-ex loss: 0.696663  [   31/   88]
per-ex loss: 0.752301  [   32/   88]
per-ex loss: 0.664492  [   33/   88]
per-ex loss: 0.559032  [   34/   88]
per-ex loss: 0.465994  [   35/   88]
per-ex loss: 0.685956  [   36/   88]
per-ex loss: 0.740348  [   37/   88]
per-ex loss: 0.710462  [   38/   88]
per-ex loss: 0.623254  [   39/   88]
per-ex loss: 0.636630  [   40/   88]
per-ex loss: 0.596306  [   41/   88]
per-ex loss: 0.607863  [   42/   88]
per-ex loss: 0.719097  [   43/   88]
per-ex loss: 0.810689  [   44/   88]
per-ex loss: 0.775687  [   45/   88]
per-ex loss: 0.580177  [   46/   88]
per-ex loss: 0.594652  [   47/   88]
per-ex loss: 0.720439  [   48/   88]
per-ex loss: 0.515340  [   49/   88]
per-ex loss: 0.570333  [   50/   88]
per-ex loss: 0.728276  [   51/   88]
per-ex loss: 0.671681  [   52/   88]
per-ex loss: 0.546122  [   53/   88]
per-ex loss: 0.556604  [   54/   88]
per-ex loss: 0.612918  [   55/   88]
per-ex loss: 0.759166  [   56/   88]
per-ex loss: 0.630216  [   57/   88]
per-ex loss: 0.562640  [   58/   88]
per-ex loss: 0.602260  [   59/   88]
per-ex loss: 0.495437  [   60/   88]
per-ex loss: 0.666061  [   61/   88]
per-ex loss: 0.534583  [   62/   88]
per-ex loss: 0.715210  [   63/   88]
per-ex loss: 0.574044  [   64/   88]
per-ex loss: 0.737644  [   65/   88]
per-ex loss: 0.495860  [   66/   88]
per-ex loss: 0.511567  [   67/   88]
per-ex loss: 0.513699  [   68/   88]
per-ex loss: 0.538101  [   69/   88]
per-ex loss: 0.525823  [   70/   88]
per-ex loss: 0.553110  [   71/   88]
per-ex loss: 0.743908  [   72/   88]
per-ex loss: 0.709519  [   73/   88]
per-ex loss: 0.499731  [   74/   88]
per-ex loss: 0.704220  [   75/   88]
per-ex loss: 0.626713  [   76/   88]
per-ex loss: 0.698179  [   77/   88]
per-ex loss: 0.530094  [   78/   88]
per-ex loss: 0.746271  [   79/   88]
per-ex loss: 0.598725  [   80/   88]
per-ex loss: 0.591931  [   81/   88]
per-ex loss: 0.793868  [   82/   88]
per-ex loss: 0.770198  [   83/   88]
per-ex loss: 0.526072  [   84/   88]
per-ex loss: 0.734146  [   85/   88]
per-ex loss: 0.571780  [   86/   88]
per-ex loss: 0.376413  [   87/   88]
per-ex loss: 0.588132  [   88/   88]
Train Error: Avg loss: 0.62755655
validation Error: 
 Avg loss: 0.69152405 
 F1: 0.493694 
 Precision: 0.585545 
 Recall: 0.426752
 IoU: 0.327751

test Error: 
 Avg loss: 0.65253273 
 F1: 0.556536 
 Precision: 0.612160 
 Recall: 0.510179
 IoU: 0.385556

We have finished training iteration 75
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_73_.pth
per-ex loss: 0.552109  [    1/   88]
per-ex loss: 0.804326  [    2/   88]
per-ex loss: 0.584946  [    3/   88]
per-ex loss: 0.729283  [    4/   88]
per-ex loss: 0.527304  [    5/   88]
per-ex loss: 0.626971  [    6/   88]
per-ex loss: 0.508241  [    7/   88]
per-ex loss: 0.507347  [    8/   88]
per-ex loss: 0.693280  [    9/   88]
per-ex loss: 0.727938  [   10/   88]
per-ex loss: 0.778303  [   11/   88]
per-ex loss: 0.699183  [   12/   88]
per-ex loss: 0.661100  [   13/   88]
per-ex loss: 0.747294  [   14/   88]
per-ex loss: 0.802616  [   15/   88]
per-ex loss: 0.737457  [   16/   88]
per-ex loss: 0.571461  [   17/   88]
per-ex loss: 0.565182  [   18/   88]
per-ex loss: 0.467894  [   19/   88]
per-ex loss: 0.599263  [   20/   88]
per-ex loss: 0.492207  [   21/   88]
per-ex loss: 0.589897  [   22/   88]
per-ex loss: 0.525023  [   23/   88]
per-ex loss: 0.714229  [   24/   88]
per-ex loss: 0.508664  [   25/   88]
per-ex loss: 0.670784  [   26/   88]
per-ex loss: 0.496137  [   27/   88]
per-ex loss: 0.569209  [   28/   88]
per-ex loss: 0.639234  [   29/   88]
per-ex loss: 0.775724  [   30/   88]
per-ex loss: 0.542418  [   31/   88]
per-ex loss: 0.537027  [   32/   88]
per-ex loss: 0.563681  [   33/   88]
per-ex loss: 0.477264  [   34/   88]
per-ex loss: 0.496112  [   35/   88]
per-ex loss: 0.744250  [   36/   88]
per-ex loss: 0.659970  [   37/   88]
per-ex loss: 0.735282  [   38/   88]
per-ex loss: 0.597016  [   39/   88]
per-ex loss: 0.506918  [   40/   88]
per-ex loss: 0.738077  [   41/   88]
per-ex loss: 0.541773  [   42/   88]
per-ex loss: 0.523287  [   43/   88]
per-ex loss: 0.653956  [   44/   88]
per-ex loss: 0.668597  [   45/   88]
per-ex loss: 0.800492  [   46/   88]
per-ex loss: 0.502709  [   47/   88]
per-ex loss: 0.734468  [   48/   88]
per-ex loss: 0.553799  [   49/   88]
per-ex loss: 0.786320  [   50/   88]
per-ex loss: 0.428966  [   51/   88]
per-ex loss: 0.710868  [   52/   88]
per-ex loss: 0.539819  [   53/   88]
per-ex loss: 0.509202  [   54/   88]
per-ex loss: 0.711434  [   55/   88]
per-ex loss: 0.598207  [   56/   88]
per-ex loss: 0.749633  [   57/   88]
per-ex loss: 0.695675  [   58/   88]
per-ex loss: 0.520494  [   59/   88]
per-ex loss: 0.658882  [   60/   88]
per-ex loss: 0.644397  [   61/   88]
per-ex loss: 0.564716  [   62/   88]
per-ex loss: 0.701863  [   63/   88]
per-ex loss: 0.711508  [   64/   88]
per-ex loss: 0.635643  [   65/   88]
per-ex loss: 0.602030  [   66/   88]
per-ex loss: 0.507850  [   67/   88]
per-ex loss: 0.590733  [   68/   88]
per-ex loss: 0.635700  [   69/   88]
per-ex loss: 0.642699  [   70/   88]
per-ex loss: 0.519401  [   71/   88]
per-ex loss: 0.760587  [   72/   88]
per-ex loss: 0.738493  [   73/   88]
per-ex loss: 0.747156  [   74/   88]
per-ex loss: 0.560882  [   75/   88]
per-ex loss: 0.635136  [   76/   88]
per-ex loss: 0.809478  [   77/   88]
per-ex loss: 0.636175  [   78/   88]
per-ex loss: 0.532519  [   79/   88]
per-ex loss: 0.583459  [   80/   88]
per-ex loss: 0.577995  [   81/   88]
per-ex loss: 0.535435  [   82/   88]
per-ex loss: 0.747368  [   83/   88]
per-ex loss: 0.562761  [   84/   88]
per-ex loss: 0.469680  [   85/   88]
per-ex loss: 0.571163  [   86/   88]
per-ex loss: 0.761131  [   87/   88]
per-ex loss: 0.702651  [   88/   88]
Train Error: Avg loss: 0.62554329
validation Error: 
 Avg loss: 0.69634534 
 F1: 0.487867 
 Precision: 0.574581 
 Recall: 0.423894
 IoU: 0.322635

test Error: 
 Avg loss: 0.64967516 
 F1: 0.554283 
 Precision: 0.645180 
 Recall: 0.485835
 IoU: 0.383396

We have finished training iteration 76
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_74_.pth
per-ex loss: 0.610754  [    1/   88]
per-ex loss: 0.733847  [    2/   88]
per-ex loss: 0.534110  [    3/   88]
per-ex loss: 0.503915  [    4/   88]
per-ex loss: 0.738064  [    5/   88]
per-ex loss: 0.494468  [    6/   88]
per-ex loss: 0.710371  [    7/   88]
per-ex loss: 0.570409  [    8/   88]
per-ex loss: 0.583370  [    9/   88]
per-ex loss: 0.507952  [   10/   88]
per-ex loss: 0.498696  [   11/   88]
per-ex loss: 0.575119  [   12/   88]
per-ex loss: 0.695943  [   13/   88]
per-ex loss: 0.659581  [   14/   88]
per-ex loss: 0.563375  [   15/   88]
per-ex loss: 0.504252  [   16/   88]
per-ex loss: 0.553647  [   17/   88]
per-ex loss: 0.727707  [   18/   88]
per-ex loss: 0.637371  [   19/   88]
per-ex loss: 0.701862  [   20/   88]
per-ex loss: 0.549012  [   21/   88]
per-ex loss: 0.730997  [   22/   88]
per-ex loss: 0.534044  [   23/   88]
per-ex loss: 0.631845  [   24/   88]
per-ex loss: 0.585133  [   25/   88]
per-ex loss: 0.571606  [   26/   88]
per-ex loss: 0.638623  [   27/   88]
per-ex loss: 0.734102  [   28/   88]
per-ex loss: 0.675389  [   29/   88]
per-ex loss: 0.537420  [   30/   88]
per-ex loss: 0.529203  [   31/   88]
per-ex loss: 0.661559  [   32/   88]
per-ex loss: 0.610944  [   33/   88]
per-ex loss: 0.788433  [   34/   88]
per-ex loss: 0.532523  [   35/   88]
per-ex loss: 0.672378  [   36/   88]
per-ex loss: 0.626884  [   37/   88]
per-ex loss: 0.581930  [   38/   88]
per-ex loss: 0.543631  [   39/   88]
per-ex loss: 0.609402  [   40/   88]
per-ex loss: 0.482151  [   41/   88]
per-ex loss: 0.758976  [   42/   88]
per-ex loss: 0.691203  [   43/   88]
per-ex loss: 0.805371  [   44/   88]
per-ex loss: 0.590019  [   45/   88]
per-ex loss: 0.783389  [   46/   88]
per-ex loss: 0.555165  [   47/   88]
per-ex loss: 0.578787  [   48/   88]
per-ex loss: 0.709778  [   49/   88]
per-ex loss: 0.551668  [   50/   88]
per-ex loss: 0.709735  [   51/   88]
per-ex loss: 0.654291  [   52/   88]
per-ex loss: 0.782643  [   53/   88]
per-ex loss: 0.550232  [   54/   88]
per-ex loss: 0.603445  [   55/   88]
per-ex loss: 0.544679  [   56/   88]
per-ex loss: 0.504018  [   57/   88]
per-ex loss: 0.797482  [   58/   88]
per-ex loss: 0.626846  [   59/   88]
per-ex loss: 0.476207  [   60/   88]
per-ex loss: 0.731645  [   61/   88]
per-ex loss: 0.708308  [   62/   88]
per-ex loss: 0.491562  [   63/   88]
per-ex loss: 0.545896  [   64/   88]
per-ex loss: 0.694870  [   65/   88]
per-ex loss: 0.569589  [   66/   88]
per-ex loss: 0.504576  [   67/   88]
per-ex loss: 0.774582  [   68/   88]
per-ex loss: 0.559266  [   69/   88]
per-ex loss: 0.670366  [   70/   88]
per-ex loss: 0.738144  [   71/   88]
per-ex loss: 0.539650  [   72/   88]
per-ex loss: 0.721284  [   73/   88]
per-ex loss: 0.631543  [   74/   88]
per-ex loss: 0.710146  [   75/   88]
per-ex loss: 0.460520  [   76/   88]
per-ex loss: 0.520213  [   77/   88]
per-ex loss: 0.725802  [   78/   88]
per-ex loss: 0.728512  [   79/   88]
per-ex loss: 0.797652  [   80/   88]
per-ex loss: 0.761764  [   81/   88]
per-ex loss: 0.449515  [   82/   88]
per-ex loss: 0.723841  [   83/   88]
per-ex loss: 0.490268  [   84/   88]
per-ex loss: 0.551024  [   85/   88]
per-ex loss: 0.655517  [   86/   88]
per-ex loss: 0.491590  [   87/   88]
per-ex loss: 0.689430  [   88/   88]
Train Error: Avg loss: 0.62321626
validation Error: 
 Avg loss: 0.68502007 
 F1: 0.500304 
 Precision: 0.555364 
 Recall: 0.455177
 IoU: 0.333604

test Error: 
 Avg loss: 0.64760614 
 F1: 0.561602 
 Precision: 0.584687 
 Recall: 0.540270
 IoU: 0.390435

We have finished training iteration 77
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_75_.pth
per-ex loss: 0.686476  [    1/   88]
per-ex loss: 0.782297  [    2/   88]
per-ex loss: 0.691828  [    3/   88]
per-ex loss: 0.732279  [    4/   88]
per-ex loss: 0.515951  [    5/   88]
per-ex loss: 0.603994  [    6/   88]
per-ex loss: 0.523042  [    7/   88]
per-ex loss: 0.521836  [    8/   88]
per-ex loss: 0.671856  [    9/   88]
per-ex loss: 0.490077  [   10/   88]
per-ex loss: 0.516752  [   11/   88]
per-ex loss: 0.644855  [   12/   88]
per-ex loss: 0.639557  [   13/   88]
per-ex loss: 0.617931  [   14/   88]
per-ex loss: 0.527046  [   15/   88]
per-ex loss: 0.579870  [   16/   88]
per-ex loss: 0.552048  [   17/   88]
per-ex loss: 0.797950  [   18/   88]
per-ex loss: 0.794031  [   19/   88]
per-ex loss: 0.720179  [   20/   88]
per-ex loss: 0.566676  [   21/   88]
per-ex loss: 0.625132  [   22/   88]
per-ex loss: 0.621477  [   23/   88]
per-ex loss: 0.570655  [   24/   88]
per-ex loss: 0.737022  [   25/   88]
per-ex loss: 0.756036  [   26/   88]
per-ex loss: 0.420184  [   27/   88]
per-ex loss: 0.743579  [   28/   88]
per-ex loss: 0.531958  [   29/   88]
per-ex loss: 0.683288  [   30/   88]
per-ex loss: 0.759751  [   31/   88]
per-ex loss: 0.739777  [   32/   88]
per-ex loss: 0.640994  [   33/   88]
per-ex loss: 0.506324  [   34/   88]
per-ex loss: 0.689108  [   35/   88]
per-ex loss: 0.705585  [   36/   88]
per-ex loss: 0.715937  [   37/   88]
per-ex loss: 0.706585  [   38/   88]
per-ex loss: 0.796387  [   39/   88]
per-ex loss: 0.733811  [   40/   88]
per-ex loss: 0.662414  [   41/   88]
per-ex loss: 0.615658  [   42/   88]
per-ex loss: 0.650863  [   43/   88]
per-ex loss: 0.745446  [   44/   88]
per-ex loss: 0.571391  [   45/   88]
per-ex loss: 0.472744  [   46/   88]
per-ex loss: 0.688640  [   47/   88]
per-ex loss: 0.594080  [   48/   88]
per-ex loss: 0.490683  [   49/   88]
per-ex loss: 0.730117  [   50/   88]
per-ex loss: 0.589078  [   51/   88]
per-ex loss: 0.904237  [   52/   88]
per-ex loss: 0.758086  [   53/   88]
per-ex loss: 0.773602  [   54/   88]
per-ex loss: 0.780738  [   55/   88]
per-ex loss: 0.562242  [   56/   88]
per-ex loss: 0.587710  [   57/   88]
per-ex loss: 0.597585  [   58/   88]
per-ex loss: 0.469364  [   59/   88]
per-ex loss: 0.496526  [   60/   88]
per-ex loss: 0.536925  [   61/   88]
per-ex loss: 0.565905  [   62/   88]
per-ex loss: 0.667477  [   63/   88]
per-ex loss: 0.476613  [   64/   88]
per-ex loss: 0.734926  [   65/   88]
per-ex loss: 0.587735  [   66/   88]
per-ex loss: 0.672440  [   67/   88]
per-ex loss: 0.499074  [   68/   88]
per-ex loss: 0.718424  [   69/   88]
per-ex loss: 0.789636  [   70/   88]
per-ex loss: 0.525979  [   71/   88]
per-ex loss: 0.732169  [   72/   88]
per-ex loss: 0.581437  [   73/   88]
per-ex loss: 0.556550  [   74/   88]
per-ex loss: 0.493240  [   75/   88]
per-ex loss: 0.558415  [   76/   88]
per-ex loss: 0.527855  [   77/   88]
per-ex loss: 0.706929  [   78/   88]
per-ex loss: 0.517079  [   79/   88]
per-ex loss: 0.546743  [   80/   88]
per-ex loss: 0.484947  [   81/   88]
per-ex loss: 0.680329  [   82/   88]
per-ex loss: 0.580965  [   83/   88]
per-ex loss: 0.524243  [   84/   88]
per-ex loss: 0.560503  [   85/   88]
per-ex loss: 0.592279  [   86/   88]
per-ex loss: 0.510050  [   87/   88]
per-ex loss: 0.530408  [   88/   88]
Train Error: Avg loss: 0.62568863
validation Error: 
 Avg loss: 0.68299551 
 F1: 0.503674 
 Precision: 0.585528 
 Recall: 0.441899
 IoU: 0.336607

test Error: 
 Avg loss: 0.64483251 
 F1: 0.565652 
 Precision: 0.610886 
 Recall: 0.526656
 IoU: 0.394362

We have finished training iteration 78
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_76_.pth
per-ex loss: 0.649094  [    1/   88]
per-ex loss: 0.799741  [    2/   88]
per-ex loss: 0.483203  [    3/   88]
per-ex loss: 0.674554  [    4/   88]
per-ex loss: 0.693530  [    5/   88]
per-ex loss: 0.525848  [    6/   88]
per-ex loss: 0.483927  [    7/   88]
per-ex loss: 0.696129  [    8/   88]
per-ex loss: 0.586125  [    9/   88]
per-ex loss: 0.488420  [   10/   88]
per-ex loss: 0.538190  [   11/   88]
per-ex loss: 0.682593  [   12/   88]
per-ex loss: 0.724918  [   13/   88]
per-ex loss: 0.495045  [   14/   88]
per-ex loss: 0.782731  [   15/   88]
per-ex loss: 0.549251  [   16/   88]
per-ex loss: 0.700218  [   17/   88]
per-ex loss: 0.530385  [   18/   88]
per-ex loss: 0.635087  [   19/   88]
per-ex loss: 0.617740  [   20/   88]
per-ex loss: 0.695939  [   21/   88]
per-ex loss: 0.740948  [   22/   88]
per-ex loss: 0.671042  [   23/   88]
per-ex loss: 0.460022  [   24/   88]
per-ex loss: 0.524421  [   25/   88]
per-ex loss: 0.497473  [   26/   88]
per-ex loss: 0.485445  [   27/   88]
per-ex loss: 0.732186  [   28/   88]
per-ex loss: 0.534447  [   29/   88]
per-ex loss: 0.481494  [   30/   88]
per-ex loss: 0.782093  [   31/   88]
per-ex loss: 0.759564  [   32/   88]
per-ex loss: 0.594779  [   33/   88]
per-ex loss: 0.465652  [   34/   88]
per-ex loss: 0.594596  [   35/   88]
per-ex loss: 0.493738  [   36/   88]
per-ex loss: 0.495148  [   37/   88]
per-ex loss: 0.732444  [   38/   88]
per-ex loss: 0.559708  [   39/   88]
per-ex loss: 0.579461  [   40/   88]
per-ex loss: 0.515761  [   41/   88]
per-ex loss: 0.670284  [   42/   88]
per-ex loss: 0.575001  [   43/   88]
per-ex loss: 0.813508  [   44/   88]
per-ex loss: 0.671223  [   45/   88]
per-ex loss: 0.785501  [   46/   88]
per-ex loss: 0.747750  [   47/   88]
per-ex loss: 0.546353  [   48/   88]
per-ex loss: 0.582836  [   49/   88]
per-ex loss: 0.728985  [   50/   88]
per-ex loss: 0.556651  [   51/   88]
per-ex loss: 0.560539  [   52/   88]
per-ex loss: 0.565842  [   53/   88]
per-ex loss: 0.717350  [   54/   88]
per-ex loss: 0.787998  [   55/   88]
per-ex loss: 0.550897  [   56/   88]
per-ex loss: 0.627344  [   57/   88]
per-ex loss: 0.564623  [   58/   88]
per-ex loss: 0.557661  [   59/   88]
per-ex loss: 0.570397  [   60/   88]
per-ex loss: 0.775766  [   61/   88]
per-ex loss: 0.599329  [   62/   88]
per-ex loss: 0.566416  [   63/   88]
per-ex loss: 0.541000  [   64/   88]
per-ex loss: 0.744450  [   65/   88]
per-ex loss: 0.572995  [   66/   88]
per-ex loss: 0.534208  [   67/   88]
per-ex loss: 0.557428  [   68/   88]
per-ex loss: 0.797596  [   69/   88]
per-ex loss: 0.749826  [   70/   88]
per-ex loss: 0.709092  [   71/   88]
per-ex loss: 0.733842  [   72/   88]
per-ex loss: 0.803752  [   73/   88]
per-ex loss: 0.521335  [   74/   88]
per-ex loss: 0.732588  [   75/   88]
per-ex loss: 0.501289  [   76/   88]
per-ex loss: 0.641663  [   77/   88]
per-ex loss: 0.645204  [   78/   88]
per-ex loss: 0.531713  [   79/   88]
per-ex loss: 0.711699  [   80/   88]
per-ex loss: 0.518680  [   81/   88]
per-ex loss: 0.612516  [   82/   88]
per-ex loss: 0.702863  [   83/   88]
per-ex loss: 0.701254  [   84/   88]
per-ex loss: 0.716913  [   85/   88]
per-ex loss: 0.635339  [   86/   88]
per-ex loss: 0.403232  [   87/   88]
per-ex loss: 0.692224  [   88/   88]
Train Error: Avg loss: 0.62350051
validation Error: 
 Avg loss: 0.68356346 
 F1: 0.502633 
 Precision: 0.561739 
 Recall: 0.454780
 IoU: 0.335678

test Error: 
 Avg loss: 0.64257321 
 F1: 0.563780 
 Precision: 0.610837 
 Recall: 0.523454
 IoU: 0.392544

We have finished training iteration 79
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_68_.pth
per-ex loss: 0.632663  [    1/   88]
per-ex loss: 0.705488  [    2/   88]
per-ex loss: 0.550293  [    3/   88]
per-ex loss: 0.646265  [    4/   88]
per-ex loss: 0.557889  [    5/   88]
per-ex loss: 0.487850  [    6/   88]
per-ex loss: 0.570794  [    7/   88]
per-ex loss: 0.546283  [    8/   88]
per-ex loss: 0.550588  [    9/   88]
per-ex loss: 0.488196  [   10/   88]
per-ex loss: 0.540046  [   11/   88]
per-ex loss: 0.656231  [   12/   88]
per-ex loss: 0.486140  [   13/   88]
per-ex loss: 0.759438  [   14/   88]
per-ex loss: 0.607552  [   15/   88]
per-ex loss: 0.654241  [   16/   88]
per-ex loss: 0.503075  [   17/   88]
per-ex loss: 0.559012  [   18/   88]
per-ex loss: 0.699536  [   19/   88]
per-ex loss: 0.709725  [   20/   88]
per-ex loss: 0.565881  [   21/   88]
per-ex loss: 0.701734  [   22/   88]
per-ex loss: 0.514253  [   23/   88]
per-ex loss: 0.477175  [   24/   88]
per-ex loss: 0.796937  [   25/   88]
per-ex loss: 0.744511  [   26/   88]
per-ex loss: 0.519988  [   27/   88]
per-ex loss: 0.503917  [   28/   88]
per-ex loss: 0.760607  [   29/   88]
per-ex loss: 0.645699  [   30/   88]
per-ex loss: 0.605113  [   31/   88]
per-ex loss: 0.712834  [   32/   88]
per-ex loss: 0.598668  [   33/   88]
per-ex loss: 0.583576  [   34/   88]
per-ex loss: 0.730513  [   35/   88]
per-ex loss: 0.509610  [   36/   88]
per-ex loss: 0.707597  [   37/   88]
per-ex loss: 0.494949  [   38/   88]
per-ex loss: 0.560059  [   39/   88]
per-ex loss: 0.781241  [   40/   88]
per-ex loss: 0.375933  [   41/   88]
per-ex loss: 0.502614  [   42/   88]
per-ex loss: 0.693766  [   43/   88]
per-ex loss: 0.805075  [   44/   88]
per-ex loss: 0.636657  [   45/   88]
per-ex loss: 0.731128  [   46/   88]
per-ex loss: 0.476859  [   47/   88]
per-ex loss: 0.513260  [   48/   88]
per-ex loss: 0.724611  [   49/   88]
per-ex loss: 0.788658  [   50/   88]
per-ex loss: 0.615088  [   51/   88]
per-ex loss: 0.738124  [   52/   88]
per-ex loss: 0.720527  [   53/   88]
per-ex loss: 0.548671  [   54/   88]
per-ex loss: 0.613969  [   55/   88]
per-ex loss: 0.597117  [   56/   88]
per-ex loss: 0.540981  [   57/   88]
per-ex loss: 0.563424  [   58/   88]
per-ex loss: 0.499036  [   59/   88]
per-ex loss: 0.617696  [   60/   88]
per-ex loss: 0.494188  [   61/   88]
per-ex loss: 0.565974  [   62/   88]
per-ex loss: 0.711802  [   63/   88]
per-ex loss: 0.699402  [   64/   88]
per-ex loss: 0.808969  [   65/   88]
per-ex loss: 0.572319  [   66/   88]
per-ex loss: 0.522100  [   67/   88]
per-ex loss: 0.767741  [   68/   88]
per-ex loss: 0.547692  [   69/   88]
per-ex loss: 0.529875  [   70/   88]
per-ex loss: 0.835864  [   71/   88]
per-ex loss: 0.767603  [   72/   88]
per-ex loss: 0.539788  [   73/   88]
per-ex loss: 0.696947  [   74/   88]
per-ex loss: 0.753497  [   75/   88]
per-ex loss: 0.689292  [   76/   88]
per-ex loss: 0.551062  [   77/   88]
per-ex loss: 0.494036  [   78/   88]
per-ex loss: 0.596086  [   79/   88]
per-ex loss: 0.663658  [   80/   88]
per-ex loss: 0.613823  [   81/   88]
per-ex loss: 0.546101  [   82/   88]
per-ex loss: 0.658429  [   83/   88]
per-ex loss: 0.695829  [   84/   88]
per-ex loss: 0.789622  [   85/   88]
per-ex loss: 0.732135  [   86/   88]
per-ex loss: 0.645355  [   87/   88]
per-ex loss: 0.733659  [   88/   88]
Train Error: Avg loss: 0.62445692
validation Error: 
 Avg loss: 0.69499393 
 F1: 0.492375 
 Precision: 0.628890 
 Recall: 0.404556
 IoU: 0.326589

test Error: 
 Avg loss: 0.64398961 
 F1: 0.561052 
 Precision: 0.673738 
 Recall: 0.480659
 IoU: 0.389904

We have finished training iteration 80
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_77_.pth
per-ex loss: 0.580374  [    1/   88]
per-ex loss: 0.551993  [    2/   88]
per-ex loss: 0.730325  [    3/   88]
per-ex loss: 0.561378  [    4/   88]
per-ex loss: 0.761099  [    5/   88]
per-ex loss: 0.595566  [    6/   88]
per-ex loss: 0.570225  [    7/   88]
per-ex loss: 0.537789  [    8/   88]
per-ex loss: 0.491554  [    9/   88]
per-ex loss: 0.798979  [   10/   88]
per-ex loss: 0.479074  [   11/   88]
per-ex loss: 0.524538  [   12/   88]
per-ex loss: 0.783450  [   13/   88]
per-ex loss: 0.616572  [   14/   88]
per-ex loss: 0.577204  [   15/   88]
per-ex loss: 0.701559  [   16/   88]
per-ex loss: 0.672533  [   17/   88]
per-ex loss: 0.655606  [   18/   88]
per-ex loss: 0.686263  [   19/   88]
per-ex loss: 0.484607  [   20/   88]
per-ex loss: 0.619611  [   21/   88]
per-ex loss: 0.760128  [   22/   88]
per-ex loss: 0.537513  [   23/   88]
per-ex loss: 0.738722  [   24/   88]
per-ex loss: 0.570318  [   25/   88]
per-ex loss: 0.565541  [   26/   88]
per-ex loss: 0.746820  [   27/   88]
per-ex loss: 0.637354  [   28/   88]
per-ex loss: 0.618881  [   29/   88]
per-ex loss: 0.568560  [   30/   88]
per-ex loss: 0.518180  [   31/   88]
per-ex loss: 0.683621  [   32/   88]
per-ex loss: 0.613340  [   33/   88]
per-ex loss: 0.544911  [   34/   88]
per-ex loss: 0.780286  [   35/   88]
per-ex loss: 0.588920  [   36/   88]
per-ex loss: 0.744421  [   37/   88]
per-ex loss: 0.754176  [   38/   88]
per-ex loss: 0.496428  [   39/   88]
per-ex loss: 0.658808  [   40/   88]
per-ex loss: 0.547526  [   41/   88]
per-ex loss: 0.730992  [   42/   88]
per-ex loss: 0.716863  [   43/   88]
per-ex loss: 0.568757  [   44/   88]
per-ex loss: 0.396743  [   45/   88]
per-ex loss: 0.575042  [   46/   88]
per-ex loss: 0.542229  [   47/   88]
per-ex loss: 0.492514  [   48/   88]
per-ex loss: 0.485125  [   49/   88]
per-ex loss: 0.572240  [   50/   88]
per-ex loss: 0.690560  [   51/   88]
per-ex loss: 0.625889  [   52/   88]
per-ex loss: 0.528117  [   53/   88]
per-ex loss: 0.568981  [   54/   88]
per-ex loss: 0.600881  [   55/   88]
per-ex loss: 0.715294  [   56/   88]
per-ex loss: 0.528157  [   57/   88]
per-ex loss: 0.749238  [   58/   88]
per-ex loss: 0.467160  [   59/   88]
per-ex loss: 0.514346  [   60/   88]
per-ex loss: 0.731409  [   61/   88]
per-ex loss: 0.742880  [   62/   88]
per-ex loss: 0.788095  [   63/   88]
per-ex loss: 0.580419  [   64/   88]
per-ex loss: 0.736772  [   65/   88]
per-ex loss: 0.711897  [   66/   88]
per-ex loss: 0.526103  [   67/   88]
per-ex loss: 0.734012  [   68/   88]
per-ex loss: 0.589615  [   69/   88]
per-ex loss: 0.642395  [   70/   88]
per-ex loss: 0.456499  [   71/   88]
per-ex loss: 0.717306  [   72/   88]
per-ex loss: 0.642956  [   73/   88]
per-ex loss: 0.701797  [   74/   88]
per-ex loss: 0.779875  [   75/   88]
per-ex loss: 0.660693  [   76/   88]
per-ex loss: 0.658754  [   77/   88]
per-ex loss: 0.705732  [   78/   88]
per-ex loss: 0.523621  [   79/   88]
per-ex loss: 0.570542  [   80/   88]
per-ex loss: 0.547068  [   81/   88]
per-ex loss: 0.492210  [   82/   88]
per-ex loss: 0.763067  [   83/   88]
per-ex loss: 0.786854  [   84/   88]
per-ex loss: 0.804080  [   85/   88]
per-ex loss: 0.543307  [   86/   88]
per-ex loss: 0.555801  [   87/   88]
per-ex loss: 0.488976  [   88/   88]
Train Error: Avg loss: 0.62393881
validation Error: 
 Avg loss: 0.69057569 
 F1: 0.495899 
 Precision: 0.601777 
 Recall: 0.421704
 IoU: 0.329698

test Error: 
 Avg loss: 0.65363682 
 F1: 0.552028 
 Precision: 0.640284 
 Recall: 0.485156
 IoU: 0.381243

We have finished training iteration 81
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_69_.pth
per-ex loss: 0.546965  [    1/   88]
per-ex loss: 0.731798  [    2/   88]
per-ex loss: 0.677290  [    3/   88]
per-ex loss: 0.492724  [    4/   88]
per-ex loss: 0.606498  [    5/   88]
per-ex loss: 0.461009  [    6/   88]
per-ex loss: 0.633176  [    7/   88]
per-ex loss: 0.526135  [    8/   88]
per-ex loss: 0.534128  [    9/   88]
per-ex loss: 0.660738  [   10/   88]
per-ex loss: 0.722975  [   11/   88]
per-ex loss: 0.761977  [   12/   88]
per-ex loss: 0.500392  [   13/   88]
per-ex loss: 0.591743  [   14/   88]
per-ex loss: 0.729496  [   15/   88]
per-ex loss: 0.686527  [   16/   88]
per-ex loss: 0.487389  [   17/   88]
per-ex loss: 0.547369  [   18/   88]
per-ex loss: 0.699237  [   19/   88]
per-ex loss: 0.599040  [   20/   88]
per-ex loss: 0.563713  [   21/   88]
per-ex loss: 0.760997  [   22/   88]
per-ex loss: 0.547457  [   23/   88]
per-ex loss: 0.490395  [   24/   88]
per-ex loss: 0.569596  [   25/   88]
per-ex loss: 0.678128  [   26/   88]
per-ex loss: 0.776457  [   27/   88]
per-ex loss: 0.492117  [   28/   88]
per-ex loss: 0.568797  [   29/   88]
per-ex loss: 0.731282  [   30/   88]
per-ex loss: 0.696233  [   31/   88]
per-ex loss: 0.740326  [   32/   88]
per-ex loss: 0.735026  [   33/   88]
per-ex loss: 0.484420  [   34/   88]
per-ex loss: 0.567357  [   35/   88]
per-ex loss: 0.553048  [   36/   88]
per-ex loss: 0.569081  [   37/   88]
per-ex loss: 0.526841  [   38/   88]
per-ex loss: 0.725661  [   39/   88]
per-ex loss: 0.617258  [   40/   88]
per-ex loss: 0.585592  [   41/   88]
per-ex loss: 0.769044  [   42/   88]
per-ex loss: 0.756872  [   43/   88]
per-ex loss: 0.706017  [   44/   88]
per-ex loss: 0.609063  [   45/   88]
per-ex loss: 0.482068  [   46/   88]
per-ex loss: 0.508307  [   47/   88]
per-ex loss: 0.702182  [   48/   88]
per-ex loss: 0.568658  [   49/   88]
per-ex loss: 0.803255  [   50/   88]
per-ex loss: 0.730322  [   51/   88]
per-ex loss: 0.636495  [   52/   88]
per-ex loss: 0.376981  [   53/   88]
per-ex loss: 0.482617  [   54/   88]
per-ex loss: 0.537701  [   55/   88]
per-ex loss: 0.791137  [   56/   88]
per-ex loss: 0.746270  [   57/   88]
per-ex loss: 0.744812  [   58/   88]
per-ex loss: 0.566812  [   59/   88]
per-ex loss: 0.699297  [   60/   88]
per-ex loss: 0.687262  [   61/   88]
per-ex loss: 0.720580  [   62/   88]
per-ex loss: 0.487741  [   63/   88]
per-ex loss: 0.525528  [   64/   88]
per-ex loss: 0.600734  [   65/   88]
per-ex loss: 0.645644  [   66/   88]
per-ex loss: 0.798928  [   67/   88]
per-ex loss: 0.509673  [   68/   88]
per-ex loss: 0.648955  [   69/   88]
per-ex loss: 0.576943  [   70/   88]
per-ex loss: 0.576135  [   71/   88]
per-ex loss: 0.475193  [   72/   88]
per-ex loss: 0.640870  [   73/   88]
per-ex loss: 0.553961  [   74/   88]
per-ex loss: 0.735688  [   75/   88]
per-ex loss: 0.550789  [   76/   88]
per-ex loss: 0.565430  [   77/   88]
per-ex loss: 0.665646  [   78/   88]
per-ex loss: 0.802191  [   79/   88]
per-ex loss: 0.546271  [   80/   88]
per-ex loss: 0.766978  [   81/   88]
per-ex loss: 0.502932  [   82/   88]
per-ex loss: 0.523393  [   83/   88]
per-ex loss: 0.634479  [   84/   88]
per-ex loss: 0.505982  [   85/   88]
per-ex loss: 0.707214  [   86/   88]
per-ex loss: 0.616751  [   87/   88]
per-ex loss: 0.486286  [   88/   88]
Train Error: Avg loss: 0.61877811
validation Error: 
 Avg loss: 0.68526742 
 F1: 0.502860 
 Precision: 0.597162 
 Recall: 0.434280
 IoU: 0.335881

test Error: 
 Avg loss: 0.64318048 
 F1: 0.562211 
 Precision: 0.632592 
 Recall: 0.505923
 IoU: 0.391025

We have finished training iteration 82
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_80_.pth
per-ex loss: 0.770742  [    1/   88]
per-ex loss: 0.787845  [    2/   88]
per-ex loss: 0.742863  [    3/   88]
per-ex loss: 0.530058  [    4/   88]
per-ex loss: 0.823569  [    5/   88]
per-ex loss: 0.492092  [    6/   88]
per-ex loss: 0.731407  [    7/   88]
per-ex loss: 0.689014  [    8/   88]
per-ex loss: 0.502078  [    9/   88]
per-ex loss: 0.621995  [   10/   88]
per-ex loss: 0.688116  [   11/   88]
per-ex loss: 0.731160  [   12/   88]
per-ex loss: 0.722447  [   13/   88]
per-ex loss: 0.678140  [   14/   88]
per-ex loss: 0.525040  [   15/   88]
per-ex loss: 0.760699  [   16/   88]
per-ex loss: 0.474414  [   17/   88]
per-ex loss: 0.472659  [   18/   88]
per-ex loss: 0.540183  [   19/   88]
per-ex loss: 0.803719  [   20/   88]
per-ex loss: 0.771987  [   21/   88]
per-ex loss: 0.708819  [   22/   88]
per-ex loss: 0.603523  [   23/   88]
per-ex loss: 0.744533  [   24/   88]
per-ex loss: 0.529729  [   25/   88]
per-ex loss: 0.482621  [   26/   88]
per-ex loss: 0.534343  [   27/   88]
per-ex loss: 0.702263  [   28/   88]
per-ex loss: 0.563010  [   29/   88]
per-ex loss: 0.560440  [   30/   88]
per-ex loss: 0.703211  [   31/   88]
per-ex loss: 0.452289  [   32/   88]
per-ex loss: 0.503574  [   33/   88]
per-ex loss: 0.764108  [   34/   88]
per-ex loss: 0.625948  [   35/   88]
per-ex loss: 0.729185  [   36/   88]
per-ex loss: 0.467061  [   37/   88]
per-ex loss: 0.726405  [   38/   88]
per-ex loss: 0.630562  [   39/   88]
per-ex loss: 0.792615  [   40/   88]
per-ex loss: 0.499283  [   41/   88]
per-ex loss: 0.650807  [   42/   88]
per-ex loss: 0.629523  [   43/   88]
per-ex loss: 0.574124  [   44/   88]
per-ex loss: 0.666789  [   45/   88]
per-ex loss: 0.691543  [   46/   88]
per-ex loss: 0.657365  [   47/   88]
per-ex loss: 0.579635  [   48/   88]
per-ex loss: 0.699966  [   49/   88]
per-ex loss: 0.693170  [   50/   88]
per-ex loss: 0.566810  [   51/   88]
per-ex loss: 0.505279  [   52/   88]
per-ex loss: 0.641876  [   53/   88]
per-ex loss: 0.543641  [   54/   88]
per-ex loss: 0.561289  [   55/   88]
per-ex loss: 0.732461  [   56/   88]
per-ex loss: 0.706350  [   57/   88]
per-ex loss: 0.495847  [   58/   88]
per-ex loss: 0.659919  [   59/   88]
per-ex loss: 0.741296  [   60/   88]
per-ex loss: 0.774246  [   61/   88]
per-ex loss: 0.555709  [   62/   88]
per-ex loss: 0.536127  [   63/   88]
per-ex loss: 0.492202  [   64/   88]
per-ex loss: 0.778660  [   65/   88]
per-ex loss: 0.493078  [   66/   88]
per-ex loss: 0.583505  [   67/   88]
per-ex loss: 0.512125  [   68/   88]
per-ex loss: 0.492144  [   69/   88]
per-ex loss: 0.579562  [   70/   88]
per-ex loss: 0.622084  [   71/   88]
per-ex loss: 0.625629  [   72/   88]
per-ex loss: 0.538319  [   73/   88]
per-ex loss: 0.796866  [   74/   88]
per-ex loss: 0.743194  [   75/   88]
per-ex loss: 0.566615  [   76/   88]
per-ex loss: 0.545893  [   77/   88]
per-ex loss: 0.539166  [   78/   88]
per-ex loss: 0.403921  [   79/   88]
per-ex loss: 0.571733  [   80/   88]
per-ex loss: 0.535582  [   81/   88]
per-ex loss: 0.537596  [   82/   88]
per-ex loss: 0.707199  [   83/   88]
per-ex loss: 0.589725  [   84/   88]
per-ex loss: 0.500052  [   85/   88]
per-ex loss: 0.698794  [   86/   88]
per-ex loss: 0.595906  [   87/   88]
per-ex loss: 0.585361  [   88/   88]
Train Error: Avg loss: 0.62139126
validation Error: 
 Avg loss: 0.68448248 
 F1: 0.501824 
 Precision: 0.591564 
 Recall: 0.435724
 IoU: 0.334956

test Error: 
 Avg loss: 0.64619994 
 F1: 0.561790 
 Precision: 0.621007 
 Recall: 0.512884
 IoU: 0.390618

We have finished training iteration 83
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_81_.pth
per-ex loss: 0.772715  [    1/   88]
per-ex loss: 0.531553  [    2/   88]
per-ex loss: 0.787851  [    3/   88]
per-ex loss: 0.722559  [    4/   88]
per-ex loss: 0.639405  [    5/   88]
per-ex loss: 0.496521  [    6/   88]
per-ex loss: 0.699019  [    7/   88]
per-ex loss: 0.556636  [    8/   88]
per-ex loss: 0.436300  [    9/   88]
per-ex loss: 0.731109  [   10/   88]
per-ex loss: 0.596694  [   11/   88]
per-ex loss: 0.614127  [   12/   88]
per-ex loss: 0.729779  [   13/   88]
per-ex loss: 0.683768  [   14/   88]
per-ex loss: 0.627244  [   15/   88]
per-ex loss: 0.682137  [   16/   88]
per-ex loss: 0.653466  [   17/   88]
per-ex loss: 0.675593  [   18/   88]
per-ex loss: 0.520904  [   19/   88]
per-ex loss: 0.576673  [   20/   88]
per-ex loss: 0.711528  [   21/   88]
per-ex loss: 0.717925  [   22/   88]
per-ex loss: 0.798139  [   23/   88]
per-ex loss: 0.733511  [   24/   88]
per-ex loss: 0.557600  [   25/   88]
per-ex loss: 0.490030  [   26/   88]
per-ex loss: 0.563065  [   27/   88]
per-ex loss: 0.600267  [   28/   88]
per-ex loss: 0.780753  [   29/   88]
per-ex loss: 0.655209  [   30/   88]
per-ex loss: 0.620079  [   31/   88]
per-ex loss: 0.450049  [   32/   88]
per-ex loss: 0.744601  [   33/   88]
per-ex loss: 0.626863  [   34/   88]
per-ex loss: 0.563455  [   35/   88]
per-ex loss: 0.574318  [   36/   88]
per-ex loss: 0.587909  [   37/   88]
per-ex loss: 0.642758  [   38/   88]
per-ex loss: 0.776554  [   39/   88]
per-ex loss: 0.599754  [   40/   88]
per-ex loss: 0.484337  [   41/   88]
per-ex loss: 0.769981  [   42/   88]
per-ex loss: 0.693920  [   43/   88]
per-ex loss: 0.701656  [   44/   88]
per-ex loss: 0.548351  [   45/   88]
per-ex loss: 0.715279  [   46/   88]
per-ex loss: 0.714479  [   47/   88]
per-ex loss: 0.489002  [   48/   88]
per-ex loss: 0.543937  [   49/   88]
per-ex loss: 0.566097  [   50/   88]
per-ex loss: 0.489249  [   51/   88]
per-ex loss: 0.525325  [   52/   88]
per-ex loss: 0.496795  [   53/   88]
per-ex loss: 0.695049  [   54/   88]
per-ex loss: 0.538426  [   55/   88]
per-ex loss: 0.505324  [   56/   88]
per-ex loss: 0.564933  [   57/   88]
per-ex loss: 0.734766  [   58/   88]
per-ex loss: 0.766255  [   59/   88]
per-ex loss: 0.525914  [   60/   88]
per-ex loss: 0.594781  [   61/   88]
per-ex loss: 0.548339  [   62/   88]
per-ex loss: 0.561489  [   63/   88]
per-ex loss: 0.652082  [   64/   88]
per-ex loss: 0.568330  [   65/   88]
per-ex loss: 0.769370  [   66/   88]
per-ex loss: 0.559923  [   67/   88]
per-ex loss: 0.499273  [   68/   88]
per-ex loss: 0.514727  [   69/   88]
per-ex loss: 0.763174  [   70/   88]
per-ex loss: 0.634238  [   71/   88]
per-ex loss: 0.524601  [   72/   88]
per-ex loss: 0.685282  [   73/   88]
per-ex loss: 0.744021  [   74/   88]
per-ex loss: 0.718622  [   75/   88]
per-ex loss: 0.821921  [   76/   88]
per-ex loss: 0.656833  [   77/   88]
per-ex loss: 0.671523  [   78/   88]
per-ex loss: 0.554326  [   79/   88]
per-ex loss: 0.608428  [   80/   88]
per-ex loss: 0.458369  [   81/   88]
per-ex loss: 0.712896  [   82/   88]
per-ex loss: 0.558173  [   83/   88]
per-ex loss: 0.480264  [   84/   88]
per-ex loss: 0.504278  [   85/   88]
per-ex loss: 0.556808  [   86/   88]
per-ex loss: 0.566639  [   87/   88]
per-ex loss: 0.501120  [   88/   88]
Train Error: Avg loss: 0.62031048
validation Error: 
 Avg loss: 0.69748291 
 F1: 0.486466 
 Precision: 0.586652 
 Recall: 0.415507
 IoU: 0.321410

test Error: 
 Avg loss: 0.64996998 
 F1: 0.551940 
 Precision: 0.646494 
 Recall: 0.481515
 IoU: 0.381158

We have finished training iteration 84
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_82_.pth
per-ex loss: 0.583688  [    1/   88]
per-ex loss: 0.479668  [    2/   88]
per-ex loss: 0.747794  [    3/   88]
per-ex loss: 0.569008  [    4/   88]
per-ex loss: 0.764361  [    5/   88]
per-ex loss: 0.733723  [    6/   88]
per-ex loss: 0.825167  [    7/   88]
per-ex loss: 0.541531  [    8/   88]
per-ex loss: 0.553779  [    9/   88]
per-ex loss: 0.701784  [   10/   88]
per-ex loss: 0.598372  [   11/   88]
per-ex loss: 0.552562  [   12/   88]
per-ex loss: 0.713315  [   13/   88]
per-ex loss: 0.796234  [   14/   88]
per-ex loss: 0.473591  [   15/   88]
per-ex loss: 0.547573  [   16/   88]
per-ex loss: 0.458203  [   17/   88]
per-ex loss: 0.574388  [   18/   88]
per-ex loss: 0.494556  [   19/   88]
per-ex loss: 0.621754  [   20/   88]
per-ex loss: 0.637008  [   21/   88]
per-ex loss: 0.550009  [   22/   88]
per-ex loss: 0.641317  [   23/   88]
per-ex loss: 0.620579  [   24/   88]
per-ex loss: 0.747197  [   25/   88]
per-ex loss: 0.461526  [   26/   88]
per-ex loss: 0.554479  [   27/   88]
per-ex loss: 0.598010  [   28/   88]
per-ex loss: 0.577067  [   29/   88]
per-ex loss: 0.723213  [   30/   88]
per-ex loss: 0.758985  [   31/   88]
per-ex loss: 0.679908  [   32/   88]
per-ex loss: 0.511086  [   33/   88]
per-ex loss: 0.545253  [   34/   88]
per-ex loss: 0.486282  [   35/   88]
per-ex loss: 0.587740  [   36/   88]
per-ex loss: 0.507411  [   37/   88]
per-ex loss: 0.645514  [   38/   88]
per-ex loss: 0.552995  [   39/   88]
per-ex loss: 0.624974  [   40/   88]
per-ex loss: 0.648811  [   41/   88]
per-ex loss: 0.503298  [   42/   88]
per-ex loss: 0.628562  [   43/   88]
per-ex loss: 0.729164  [   44/   88]
per-ex loss: 0.500576  [   45/   88]
per-ex loss: 0.791912  [   46/   88]
per-ex loss: 0.723207  [   47/   88]
per-ex loss: 0.711885  [   48/   88]
per-ex loss: 0.484160  [   49/   88]
per-ex loss: 0.733441  [   50/   88]
per-ex loss: 0.777205  [   51/   88]
per-ex loss: 0.506789  [   52/   88]
per-ex loss: 0.515777  [   53/   88]
per-ex loss: 0.715679  [   54/   88]
per-ex loss: 0.684634  [   55/   88]
per-ex loss: 0.669540  [   56/   88]
per-ex loss: 0.528143  [   57/   88]
per-ex loss: 0.684299  [   58/   88]
per-ex loss: 0.591577  [   59/   88]
per-ex loss: 0.532881  [   60/   88]
per-ex loss: 0.359800  [   61/   88]
per-ex loss: 0.688101  [   62/   88]
per-ex loss: 0.735385  [   63/   88]
per-ex loss: 0.692508  [   64/   88]
per-ex loss: 0.553159  [   65/   88]
per-ex loss: 0.793528  [   66/   88]
per-ex loss: 0.600599  [   67/   88]
per-ex loss: 0.524939  [   68/   88]
per-ex loss: 0.554058  [   69/   88]
per-ex loss: 0.558786  [   70/   88]
per-ex loss: 0.515564  [   71/   88]
per-ex loss: 0.747380  [   72/   88]
per-ex loss: 0.547803  [   73/   88]
per-ex loss: 0.598112  [   74/   88]
per-ex loss: 0.542261  [   75/   88]
per-ex loss: 0.793810  [   76/   88]
per-ex loss: 0.733857  [   77/   88]
per-ex loss: 0.705181  [   78/   88]
per-ex loss: 0.724719  [   79/   88]
per-ex loss: 0.516711  [   80/   88]
per-ex loss: 0.549143  [   81/   88]
per-ex loss: 0.734081  [   82/   88]
per-ex loss: 0.661049  [   83/   88]
per-ex loss: 0.473862  [   84/   88]
per-ex loss: 0.698983  [   85/   88]
per-ex loss: 0.576775  [   86/   88]
per-ex loss: 0.736904  [   87/   88]
per-ex loss: 0.526874  [   88/   88]
Train Error: Avg loss: 0.61871675
validation Error: 
 Avg loss: 0.68540021 
 F1: 0.498363 
 Precision: 0.612941 
 Recall: 0.419875
 IoU: 0.331880

test Error: 
 Avg loss: 0.64607050 
 F1: 0.561481 
 Precision: 0.642455 
 Recall: 0.498634
 IoU: 0.390319

We have finished training iteration 85
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_83_.pth
per-ex loss: 0.579485  [    1/   88]
per-ex loss: 0.572280  [    2/   88]
per-ex loss: 0.542054  [    3/   88]
per-ex loss: 0.717198  [    4/   88]
per-ex loss: 0.811400  [    5/   88]
per-ex loss: 0.761734  [    6/   88]
per-ex loss: 0.566842  [    7/   88]
per-ex loss: 0.640053  [    8/   88]
per-ex loss: 0.572024  [    9/   88]
per-ex loss: 0.661686  [   10/   88]
per-ex loss: 0.653991  [   11/   88]
per-ex loss: 0.585545  [   12/   88]
per-ex loss: 0.552319  [   13/   88]
per-ex loss: 0.427753  [   14/   88]
per-ex loss: 0.707626  [   15/   88]
per-ex loss: 0.646721  [   16/   88]
per-ex loss: 0.546508  [   17/   88]
per-ex loss: 0.572230  [   18/   88]
per-ex loss: 0.526746  [   19/   88]
per-ex loss: 0.539812  [   20/   88]
per-ex loss: 0.617924  [   21/   88]
per-ex loss: 0.497980  [   22/   88]
per-ex loss: 0.645375  [   23/   88]
per-ex loss: 0.510661  [   24/   88]
per-ex loss: 0.722533  [   25/   88]
per-ex loss: 0.524536  [   26/   88]
per-ex loss: 0.718593  [   27/   88]
per-ex loss: 0.591451  [   28/   88]
per-ex loss: 0.716317  [   29/   88]
per-ex loss: 0.492769  [   30/   88]
per-ex loss: 0.646805  [   31/   88]
per-ex loss: 0.586818  [   32/   88]
per-ex loss: 0.802659  [   33/   88]
per-ex loss: 0.698499  [   34/   88]
per-ex loss: 0.730542  [   35/   88]
per-ex loss: 0.692588  [   36/   88]
per-ex loss: 0.544718  [   37/   88]
per-ex loss: 0.557408  [   38/   88]
per-ex loss: 0.621602  [   39/   88]
per-ex loss: 0.482965  [   40/   88]
per-ex loss: 0.547173  [   41/   88]
per-ex loss: 0.618290  [   42/   88]
per-ex loss: 0.660190  [   43/   88]
per-ex loss: 0.594867  [   44/   88]
per-ex loss: 0.616830  [   45/   88]
per-ex loss: 0.702245  [   46/   88]
per-ex loss: 0.540512  [   47/   88]
per-ex loss: 0.536690  [   48/   88]
per-ex loss: 0.764989  [   49/   88]
per-ex loss: 0.693489  [   50/   88]
per-ex loss: 0.501148  [   51/   88]
per-ex loss: 0.740116  [   52/   88]
per-ex loss: 0.764322  [   53/   88]
per-ex loss: 0.503882  [   54/   88]
per-ex loss: 0.622464  [   55/   88]
per-ex loss: 0.602415  [   56/   88]
per-ex loss: 0.649100  [   57/   88]
per-ex loss: 0.787420  [   58/   88]
per-ex loss: 0.714281  [   59/   88]
per-ex loss: 0.741579  [   60/   88]
per-ex loss: 0.467645  [   61/   88]
per-ex loss: 0.763972  [   62/   88]
per-ex loss: 0.494031  [   63/   88]
per-ex loss: 0.788482  [   64/   88]
per-ex loss: 0.700648  [   65/   88]
per-ex loss: 0.742056  [   66/   88]
per-ex loss: 0.547732  [   67/   88]
per-ex loss: 0.744470  [   68/   88]
per-ex loss: 0.699757  [   69/   88]
per-ex loss: 0.729619  [   70/   88]
per-ex loss: 0.501337  [   71/   88]
per-ex loss: 0.485516  [   72/   88]
per-ex loss: 0.791353  [   73/   88]
per-ex loss: 0.511524  [   74/   88]
per-ex loss: 0.739960  [   75/   88]
per-ex loss: 0.545786  [   76/   88]
per-ex loss: 0.531211  [   77/   88]
per-ex loss: 0.720030  [   78/   88]
per-ex loss: 0.493426  [   79/   88]
per-ex loss: 0.483343  [   80/   88]
per-ex loss: 0.564391  [   81/   88]
per-ex loss: 0.546301  [   82/   88]
per-ex loss: 0.705596  [   83/   88]
per-ex loss: 0.573337  [   84/   88]
per-ex loss: 0.547677  [   85/   88]
per-ex loss: 0.515455  [   86/   88]
per-ex loss: 0.689460  [   87/   88]
per-ex loss: 0.700094  [   88/   88]
Train Error: Avg loss: 0.62287457
validation Error: 
 Avg loss: 0.68317499 
 F1: 0.504658 
 Precision: 0.598490 
 Recall: 0.436260
 IoU: 0.337486

test Error: 
 Avg loss: 0.64048345 
 F1: 0.571885 
 Precision: 0.631841 
 Recall: 0.522322
 IoU: 0.400448

We have finished training iteration 86
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_84_.pth
per-ex loss: 0.746082  [    1/   88]
per-ex loss: 0.746088  [    2/   88]
per-ex loss: 0.745603  [    3/   88]
per-ex loss: 0.493894  [    4/   88]
per-ex loss: 0.701960  [    5/   88]
per-ex loss: 0.713006  [    6/   88]
per-ex loss: 0.754583  [    7/   88]
per-ex loss: 0.596467  [    8/   88]
per-ex loss: 0.495985  [    9/   88]
per-ex loss: 0.705370  [   10/   88]
per-ex loss: 0.479105  [   11/   88]
per-ex loss: 0.759341  [   12/   88]
per-ex loss: 0.770072  [   13/   88]
per-ex loss: 0.675943  [   14/   88]
per-ex loss: 0.634523  [   15/   88]
per-ex loss: 0.588646  [   16/   88]
per-ex loss: 0.386001  [   17/   88]
per-ex loss: 0.471492  [   18/   88]
per-ex loss: 0.490155  [   19/   88]
per-ex loss: 0.738910  [   20/   88]
per-ex loss: 0.656643  [   21/   88]
per-ex loss: 0.586761  [   22/   88]
per-ex loss: 0.676064  [   23/   88]
per-ex loss: 0.707531  [   24/   88]
per-ex loss: 0.726616  [   25/   88]
per-ex loss: 0.489741  [   26/   88]
per-ex loss: 0.559640  [   27/   88]
per-ex loss: 0.482540  [   28/   88]
per-ex loss: 0.560970  [   29/   88]
per-ex loss: 0.810358  [   30/   88]
per-ex loss: 0.618145  [   31/   88]
per-ex loss: 0.718537  [   32/   88]
per-ex loss: 0.685743  [   33/   88]
per-ex loss: 0.544896  [   34/   88]
per-ex loss: 0.685167  [   35/   88]
per-ex loss: 0.479360  [   36/   88]
per-ex loss: 0.693885  [   37/   88]
per-ex loss: 0.573096  [   38/   88]
per-ex loss: 0.535895  [   39/   88]
per-ex loss: 0.584871  [   40/   88]
per-ex loss: 0.703732  [   41/   88]
per-ex loss: 0.526533  [   42/   88]
per-ex loss: 0.636104  [   43/   88]
per-ex loss: 0.625286  [   44/   88]
per-ex loss: 0.556918  [   45/   88]
per-ex loss: 0.502157  [   46/   88]
per-ex loss: 0.526228  [   47/   88]
per-ex loss: 0.659009  [   48/   88]
per-ex loss: 0.528421  [   49/   88]
per-ex loss: 0.660376  [   50/   88]
per-ex loss: 0.756937  [   51/   88]
per-ex loss: 0.545750  [   52/   88]
per-ex loss: 0.547932  [   53/   88]
per-ex loss: 0.667217  [   54/   88]
per-ex loss: 0.526237  [   55/   88]
per-ex loss: 0.694130  [   56/   88]
per-ex loss: 0.515836  [   57/   88]
per-ex loss: 0.564399  [   58/   88]
per-ex loss: 0.481446  [   59/   88]
per-ex loss: 0.746360  [   60/   88]
per-ex loss: 0.532998  [   61/   88]
per-ex loss: 0.801655  [   62/   88]
per-ex loss: 0.767650  [   63/   88]
per-ex loss: 0.595224  [   64/   88]
per-ex loss: 0.580537  [   65/   88]
per-ex loss: 0.553856  [   66/   88]
per-ex loss: 0.509806  [   67/   88]
per-ex loss: 0.596823  [   68/   88]
per-ex loss: 0.552443  [   69/   88]
per-ex loss: 0.524791  [   70/   88]
per-ex loss: 0.759009  [   71/   88]
per-ex loss: 0.620125  [   72/   88]
per-ex loss: 0.578441  [   73/   88]
per-ex loss: 0.690361  [   74/   88]
per-ex loss: 0.502840  [   75/   88]
per-ex loss: 0.630368  [   76/   88]
per-ex loss: 0.502279  [   77/   88]
per-ex loss: 0.743993  [   78/   88]
per-ex loss: 0.569816  [   79/   88]
per-ex loss: 0.620730  [   80/   88]
per-ex loss: 0.452631  [   81/   88]
per-ex loss: 0.634670  [   82/   88]
per-ex loss: 0.792155  [   83/   88]
per-ex loss: 0.495387  [   84/   88]
per-ex loss: 0.570559  [   85/   88]
per-ex loss: 0.792207  [   86/   88]
per-ex loss: 0.728371  [   87/   88]
per-ex loss: 0.717545  [   88/   88]
Train Error: Avg loss: 0.61884047
validation Error: 
 Avg loss: 0.69529937 
 F1: 0.493804 
 Precision: 0.501657 
 Recall: 0.486194
 IoU: 0.327849

test Error: 
 Avg loss: 0.64672446 
 F1: 0.557260 
 Precision: 0.552889 
 Recall: 0.561700
 IoU: 0.386251

We have finished training iteration 87
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_85_.pth
per-ex loss: 0.538491  [    1/   88]
per-ex loss: 0.669453  [    2/   88]
per-ex loss: 0.555155  [    3/   88]
per-ex loss: 0.552795  [    4/   88]
per-ex loss: 0.541611  [    5/   88]
per-ex loss: 0.417052  [    6/   88]
per-ex loss: 0.629784  [    7/   88]
per-ex loss: 0.574109  [    8/   88]
per-ex loss: 0.496018  [    9/   88]
per-ex loss: 0.768875  [   10/   88]
per-ex loss: 0.687371  [   11/   88]
per-ex loss: 0.543080  [   12/   88]
per-ex loss: 0.678540  [   13/   88]
per-ex loss: 0.587976  [   14/   88]
per-ex loss: 0.798252  [   15/   88]
per-ex loss: 0.502994  [   16/   88]
per-ex loss: 0.584688  [   17/   88]
per-ex loss: 0.555089  [   18/   88]
per-ex loss: 0.520634  [   19/   88]
per-ex loss: 0.545085  [   20/   88]
per-ex loss: 0.542183  [   21/   88]
per-ex loss: 0.484778  [   22/   88]
per-ex loss: 0.797974  [   23/   88]
per-ex loss: 0.608461  [   24/   88]
per-ex loss: 0.747385  [   25/   88]
per-ex loss: 0.757835  [   26/   88]
per-ex loss: 0.664204  [   27/   88]
per-ex loss: 0.488867  [   28/   88]
per-ex loss: 0.490282  [   29/   88]
per-ex loss: 0.720910  [   30/   88]
per-ex loss: 0.527609  [   31/   88]
per-ex loss: 0.688844  [   32/   88]
per-ex loss: 0.550464  [   33/   88]
per-ex loss: 0.657752  [   34/   88]
per-ex loss: 0.516670  [   35/   88]
per-ex loss: 0.586162  [   36/   88]
per-ex loss: 0.599635  [   37/   88]
per-ex loss: 0.551175  [   38/   88]
per-ex loss: 0.460657  [   39/   88]
per-ex loss: 0.717647  [   40/   88]
per-ex loss: 0.739944  [   41/   88]
per-ex loss: 0.728589  [   42/   88]
per-ex loss: 0.532691  [   43/   88]
per-ex loss: 0.547635  [   44/   88]
per-ex loss: 0.545902  [   45/   88]
per-ex loss: 0.702134  [   46/   88]
per-ex loss: 0.456240  [   47/   88]
per-ex loss: 0.638809  [   48/   88]
per-ex loss: 0.712467  [   49/   88]
per-ex loss: 0.458637  [   50/   88]
per-ex loss: 0.484733  [   51/   88]
per-ex loss: 0.618881  [   52/   88]
per-ex loss: 0.728320  [   53/   88]
per-ex loss: 0.549344  [   54/   88]
per-ex loss: 0.649629  [   55/   88]
per-ex loss: 0.598986  [   56/   88]
per-ex loss: 0.527571  [   57/   88]
per-ex loss: 0.480921  [   58/   88]
per-ex loss: 0.545559  [   59/   88]
per-ex loss: 0.747629  [   60/   88]
per-ex loss: 0.771474  [   61/   88]
per-ex loss: 0.696541  [   62/   88]
per-ex loss: 0.797236  [   63/   88]
per-ex loss: 0.567562  [   64/   88]
per-ex loss: 0.566608  [   65/   88]
per-ex loss: 0.625368  [   66/   88]
per-ex loss: 0.718352  [   67/   88]
per-ex loss: 0.622747  [   68/   88]
per-ex loss: 0.810944  [   69/   88]
per-ex loss: 0.591112  [   70/   88]
per-ex loss: 0.712447  [   71/   88]
per-ex loss: 0.540576  [   72/   88]
per-ex loss: 0.732325  [   73/   88]
per-ex loss: 0.737422  [   74/   88]
per-ex loss: 0.651417  [   75/   88]
per-ex loss: 0.686496  [   76/   88]
per-ex loss: 0.537601  [   77/   88]
per-ex loss: 0.597203  [   78/   88]
per-ex loss: 0.496334  [   79/   88]
per-ex loss: 0.723716  [   80/   88]
per-ex loss: 0.493926  [   81/   88]
per-ex loss: 0.502152  [   82/   88]
per-ex loss: 0.614104  [   83/   88]
per-ex loss: 0.725972  [   84/   88]
per-ex loss: 0.736668  [   85/   88]
per-ex loss: 0.736832  [   86/   88]
per-ex loss: 0.721259  [   87/   88]
per-ex loss: 0.640408  [   88/   88]
Train Error: Avg loss: 0.61654513
validation Error: 
 Avg loss: 0.68966489 
 F1: 0.496819 
 Precision: 0.518221 
 Recall: 0.477114
 IoU: 0.330511

test Error: 
 Avg loss: 0.64579513 
 F1: 0.561325 
 Precision: 0.567648 
 Recall: 0.555141
 IoU: 0.390168

We have finished training iteration 88
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_79_.pth
per-ex loss: 0.465045  [    1/   88]
per-ex loss: 0.544520  [    2/   88]
per-ex loss: 0.704062  [    3/   88]
per-ex loss: 0.733649  [    4/   88]
per-ex loss: 0.756114  [    5/   88]
per-ex loss: 0.656207  [    6/   88]
per-ex loss: 0.560375  [    7/   88]
per-ex loss: 0.620457  [    8/   88]
per-ex loss: 0.543074  [    9/   88]
per-ex loss: 0.472301  [   10/   88]
per-ex loss: 0.580778  [   11/   88]
per-ex loss: 0.538716  [   12/   88]
per-ex loss: 0.617048  [   13/   88]
per-ex loss: 0.728985  [   14/   88]
per-ex loss: 0.538730  [   15/   88]
per-ex loss: 0.504297  [   16/   88]
per-ex loss: 0.505380  [   17/   88]
per-ex loss: 0.713493  [   18/   88]
per-ex loss: 0.459252  [   19/   88]
per-ex loss: 0.704240  [   20/   88]
per-ex loss: 0.534060  [   21/   88]
per-ex loss: 0.500158  [   22/   88]
per-ex loss: 0.790365  [   23/   88]
per-ex loss: 0.563882  [   24/   88]
per-ex loss: 0.701099  [   25/   88]
per-ex loss: 0.573180  [   26/   88]
per-ex loss: 0.494506  [   27/   88]
per-ex loss: 0.494862  [   28/   88]
per-ex loss: 0.500615  [   29/   88]
per-ex loss: 0.624486  [   30/   88]
per-ex loss: 0.579216  [   31/   88]
per-ex loss: 0.680540  [   32/   88]
per-ex loss: 0.451568  [   33/   88]
per-ex loss: 0.630559  [   34/   88]
per-ex loss: 0.649457  [   35/   88]
per-ex loss: 0.791209  [   36/   88]
per-ex loss: 0.619297  [   37/   88]
per-ex loss: 0.763440  [   38/   88]
per-ex loss: 0.701191  [   39/   88]
per-ex loss: 0.497595  [   40/   88]
per-ex loss: 0.637409  [   41/   88]
per-ex loss: 0.735443  [   42/   88]
per-ex loss: 0.504014  [   43/   88]
per-ex loss: 0.521175  [   44/   88]
per-ex loss: 0.660872  [   45/   88]
per-ex loss: 0.544685  [   46/   88]
per-ex loss: 0.625086  [   47/   88]
per-ex loss: 0.718877  [   48/   88]
per-ex loss: 0.475451  [   49/   88]
per-ex loss: 0.555928  [   50/   88]
per-ex loss: 0.531844  [   51/   88]
per-ex loss: 0.809748  [   52/   88]
per-ex loss: 0.697397  [   53/   88]
per-ex loss: 0.686472  [   54/   88]
per-ex loss: 0.668728  [   55/   88]
per-ex loss: 0.685984  [   56/   88]
per-ex loss: 0.645339  [   57/   88]
per-ex loss: 0.723707  [   58/   88]
per-ex loss: 0.598684  [   59/   88]
per-ex loss: 0.687315  [   60/   88]
per-ex loss: 0.559675  [   61/   88]
per-ex loss: 0.563583  [   62/   88]
per-ex loss: 0.723777  [   63/   88]
per-ex loss: 0.596180  [   64/   88]
per-ex loss: 0.704884  [   65/   88]
per-ex loss: 0.477919  [   66/   88]
per-ex loss: 0.590955  [   67/   88]
per-ex loss: 0.739649  [   68/   88]
per-ex loss: 0.787992  [   69/   88]
per-ex loss: 0.595276  [   70/   88]
per-ex loss: 0.409233  [   71/   88]
per-ex loss: 0.772062  [   72/   88]
per-ex loss: 0.528888  [   73/   88]
per-ex loss: 0.561715  [   74/   88]
per-ex loss: 0.761948  [   75/   88]
per-ex loss: 0.592538  [   76/   88]
per-ex loss: 0.481910  [   77/   88]
per-ex loss: 0.557828  [   78/   88]
per-ex loss: 0.520403  [   79/   88]
per-ex loss: 0.547675  [   80/   88]
per-ex loss: 0.726933  [   81/   88]
per-ex loss: 0.538240  [   82/   88]
per-ex loss: 0.718130  [   83/   88]
per-ex loss: 0.748763  [   84/   88]
per-ex loss: 0.739679  [   85/   88]
per-ex loss: 0.552978  [   86/   88]
per-ex loss: 0.726469  [   87/   88]
per-ex loss: 0.565661  [   88/   88]
Train Error: Avg loss: 0.61583081
validation Error: 
 Avg loss: 0.68643677 
 F1: 0.502654 
 Precision: 0.571594 
 Recall: 0.448553
 IoU: 0.335696

test Error: 
 Avg loss: 0.63954235 
 F1: 0.567318 
 Precision: 0.619816 
 Recall: 0.523019
 IoU: 0.395984

We have finished training iteration 89
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_87_.pth
per-ex loss: 0.589387  [    1/   88]
per-ex loss: 0.625066  [    2/   88]
per-ex loss: 0.499799  [    3/   88]
per-ex loss: 0.450396  [    4/   88]
per-ex loss: 0.735005  [    5/   88]
per-ex loss: 0.656855  [    6/   88]
per-ex loss: 0.703196  [    7/   88]
per-ex loss: 0.682453  [    8/   88]
per-ex loss: 0.657403  [    9/   88]
per-ex loss: 0.606819  [   10/   88]
per-ex loss: 0.496851  [   11/   88]
per-ex loss: 0.529992  [   12/   88]
per-ex loss: 0.563206  [   13/   88]
per-ex loss: 0.516000  [   14/   88]
per-ex loss: 0.705823  [   15/   88]
per-ex loss: 0.510717  [   16/   88]
per-ex loss: 0.780494  [   17/   88]
per-ex loss: 0.622680  [   18/   88]
per-ex loss: 0.747149  [   19/   88]
per-ex loss: 0.732663  [   20/   88]
per-ex loss: 0.693769  [   21/   88]
per-ex loss: 0.623324  [   22/   88]
per-ex loss: 0.583107  [   23/   88]
per-ex loss: 0.535164  [   24/   88]
per-ex loss: 0.530158  [   25/   88]
per-ex loss: 0.483616  [   26/   88]
per-ex loss: 0.681366  [   27/   88]
per-ex loss: 0.688928  [   28/   88]
per-ex loss: 0.472347  [   29/   88]
per-ex loss: 0.723129  [   30/   88]
per-ex loss: 0.505260  [   31/   88]
per-ex loss: 0.697700  [   32/   88]
per-ex loss: 0.637264  [   33/   88]
per-ex loss: 0.699694  [   34/   88]
per-ex loss: 0.536335  [   35/   88]
per-ex loss: 0.546730  [   36/   88]
per-ex loss: 0.620103  [   37/   88]
per-ex loss: 0.553752  [   38/   88]
per-ex loss: 0.570691  [   39/   88]
per-ex loss: 0.634198  [   40/   88]
per-ex loss: 0.520636  [   41/   88]
per-ex loss: 0.572399  [   42/   88]
per-ex loss: 0.546990  [   43/   88]
per-ex loss: 0.554584  [   44/   88]
per-ex loss: 0.802457  [   45/   88]
per-ex loss: 0.732889  [   46/   88]
per-ex loss: 0.549415  [   47/   88]
per-ex loss: 0.586715  [   48/   88]
per-ex loss: 0.591636  [   49/   88]
per-ex loss: 0.488144  [   50/   88]
per-ex loss: 0.742310  [   51/   88]
per-ex loss: 0.703455  [   52/   88]
per-ex loss: 0.572670  [   53/   88]
per-ex loss: 0.496698  [   54/   88]
per-ex loss: 0.491065  [   55/   88]
per-ex loss: 0.724770  [   56/   88]
per-ex loss: 0.554191  [   57/   88]
per-ex loss: 0.621913  [   58/   88]
per-ex loss: 0.725510  [   59/   88]
per-ex loss: 0.517771  [   60/   88]
per-ex loss: 0.765942  [   61/   88]
per-ex loss: 0.789183  [   62/   88]
per-ex loss: 0.612034  [   63/   88]
per-ex loss: 0.497780  [   64/   88]
per-ex loss: 0.708848  [   65/   88]
per-ex loss: 0.654226  [   66/   88]
per-ex loss: 0.460319  [   67/   88]
per-ex loss: 0.611961  [   68/   88]
per-ex loss: 0.517777  [   69/   88]
per-ex loss: 0.566671  [   70/   88]
per-ex loss: 0.484971  [   71/   88]
per-ex loss: 0.804550  [   72/   88]
per-ex loss: 0.710528  [   73/   88]
per-ex loss: 0.747459  [   74/   88]
per-ex loss: 0.549851  [   75/   88]
per-ex loss: 0.799786  [   76/   88]
per-ex loss: 0.751308  [   77/   88]
per-ex loss: 0.536102  [   78/   88]
per-ex loss: 0.745875  [   79/   88]
per-ex loss: 0.557190  [   80/   88]
per-ex loss: 0.503571  [   81/   88]
per-ex loss: 0.478904  [   82/   88]
per-ex loss: 0.719495  [   83/   88]
per-ex loss: 0.442720  [   84/   88]
per-ex loss: 0.639962  [   85/   88]
per-ex loss: 0.795934  [   86/   88]
per-ex loss: 0.627010  [   87/   88]
per-ex loss: 0.776510  [   88/   88]
Train Error: Avg loss: 0.61796902
validation Error: 
 Avg loss: 0.68454784 
 F1: 0.503061 
 Precision: 0.562867 
 Recall: 0.454743
 IoU: 0.336060

test Error: 
 Avg loss: 0.64251273 
 F1: 0.564697 
 Precision: 0.606047 
 Recall: 0.528629
 IoU: 0.393434

We have finished training iteration 90
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_88_.pth
per-ex loss: 0.632328  [    1/   88]
per-ex loss: 0.550605  [    2/   88]
per-ex loss: 0.555743  [    3/   88]
per-ex loss: 0.600142  [    4/   88]
per-ex loss: 0.581019  [    5/   88]
per-ex loss: 0.767613  [    6/   88]
per-ex loss: 0.517348  [    7/   88]
per-ex loss: 0.666538  [    8/   88]
per-ex loss: 0.564763  [    9/   88]
per-ex loss: 0.790693  [   10/   88]
per-ex loss: 0.642255  [   11/   88]
per-ex loss: 0.519777  [   12/   88]
per-ex loss: 0.605448  [   13/   88]
per-ex loss: 0.523691  [   14/   88]
per-ex loss: 0.475400  [   15/   88]
per-ex loss: 0.598307  [   16/   88]
per-ex loss: 0.500365  [   17/   88]
per-ex loss: 0.735851  [   18/   88]
per-ex loss: 0.635323  [   19/   88]
per-ex loss: 0.511745  [   20/   88]
per-ex loss: 0.720086  [   21/   88]
per-ex loss: 0.637957  [   22/   88]
per-ex loss: 0.714153  [   23/   88]
per-ex loss: 0.539818  [   24/   88]
per-ex loss: 0.572008  [   25/   88]
per-ex loss: 0.541012  [   26/   88]
per-ex loss: 0.538588  [   27/   88]
per-ex loss: 0.516988  [   28/   88]
per-ex loss: 0.531937  [   29/   88]
per-ex loss: 0.747059  [   30/   88]
per-ex loss: 0.801546  [   31/   88]
per-ex loss: 0.493591  [   32/   88]
per-ex loss: 0.516993  [   33/   88]
per-ex loss: 0.750215  [   34/   88]
per-ex loss: 0.624859  [   35/   88]
per-ex loss: 0.721967  [   36/   88]
per-ex loss: 0.784366  [   37/   88]
per-ex loss: 0.677494  [   38/   88]
per-ex loss: 0.716780  [   39/   88]
per-ex loss: 0.735909  [   40/   88]
per-ex loss: 0.630836  [   41/   88]
per-ex loss: 0.637615  [   42/   88]
per-ex loss: 0.626492  [   43/   88]
per-ex loss: 0.702358  [   44/   88]
per-ex loss: 0.496215  [   45/   88]
per-ex loss: 0.706329  [   46/   88]
per-ex loss: 0.537752  [   47/   88]
per-ex loss: 0.715978  [   48/   88]
per-ex loss: 0.490818  [   49/   88]
per-ex loss: 0.814570  [   50/   88]
per-ex loss: 0.436413  [   51/   88]
per-ex loss: 0.544233  [   52/   88]
per-ex loss: 0.561036  [   53/   88]
per-ex loss: 0.527537  [   54/   88]
per-ex loss: 0.719638  [   55/   88]
per-ex loss: 0.690945  [   56/   88]
per-ex loss: 0.730463  [   57/   88]
per-ex loss: 0.749370  [   58/   88]
per-ex loss: 0.701918  [   59/   88]
per-ex loss: 0.742837  [   60/   88]
per-ex loss: 0.694223  [   61/   88]
per-ex loss: 0.569304  [   62/   88]
per-ex loss: 0.600029  [   63/   88]
per-ex loss: 0.572116  [   64/   88]
per-ex loss: 0.655640  [   65/   88]
per-ex loss: 0.560081  [   66/   88]
per-ex loss: 0.547540  [   67/   88]
per-ex loss: 0.691281  [   68/   88]
per-ex loss: 0.487352  [   69/   88]
per-ex loss: 0.481084  [   70/   88]
per-ex loss: 0.706311  [   71/   88]
per-ex loss: 0.740716  [   72/   88]
per-ex loss: 0.599751  [   73/   88]
per-ex loss: 0.555232  [   74/   88]
per-ex loss: 0.542328  [   75/   88]
per-ex loss: 0.745871  [   76/   88]
per-ex loss: 0.736599  [   77/   88]
per-ex loss: 0.544605  [   78/   88]
per-ex loss: 0.662350  [   79/   88]
per-ex loss: 0.714441  [   80/   88]
per-ex loss: 0.825878  [   81/   88]
per-ex loss: 0.545215  [   82/   88]
per-ex loss: 0.623110  [   83/   88]
per-ex loss: 0.487232  [   84/   88]
per-ex loss: 0.588620  [   85/   88]
per-ex loss: 0.455433  [   86/   88]
per-ex loss: 0.525287  [   87/   88]
per-ex loss: 0.519794  [   88/   88]
Train Error: Avg loss: 0.62044383
validation Error: 
 Avg loss: 0.68528705 
 F1: 0.502634 
 Precision: 0.558193 
 Recall: 0.457134
 IoU: 0.335679

test Error: 
 Avg loss: 0.64159428 
 F1: 0.566269 
 Precision: 0.605352 
 Recall: 0.531926
 IoU: 0.394962

We have finished training iteration 91
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_89_.pth
per-ex loss: 0.724283  [    1/   88]
per-ex loss: 0.533992  [    2/   88]
per-ex loss: 0.735291  [    3/   88]
per-ex loss: 0.662259  [    4/   88]
per-ex loss: 0.547940  [    5/   88]
per-ex loss: 0.655130  [    6/   88]
per-ex loss: 0.701310  [    7/   88]
per-ex loss: 0.563348  [    8/   88]
per-ex loss: 0.588878  [    9/   88]
per-ex loss: 0.684256  [   10/   88]
per-ex loss: 0.633769  [   11/   88]
per-ex loss: 0.778634  [   12/   88]
per-ex loss: 0.533804  [   13/   88]
per-ex loss: 0.757108  [   14/   88]
per-ex loss: 0.665024  [   15/   88]
per-ex loss: 0.799693  [   16/   88]
per-ex loss: 0.726851  [   17/   88]
per-ex loss: 0.763177  [   18/   88]
per-ex loss: 0.470037  [   19/   88]
per-ex loss: 0.726679  [   20/   88]
per-ex loss: 0.694397  [   21/   88]
per-ex loss: 0.540009  [   22/   88]
per-ex loss: 0.485036  [   23/   88]
per-ex loss: 0.593592  [   24/   88]
per-ex loss: 0.684354  [   25/   88]
per-ex loss: 0.589902  [   26/   88]
per-ex loss: 0.566743  [   27/   88]
per-ex loss: 0.625374  [   28/   88]
per-ex loss: 0.536124  [   29/   88]
per-ex loss: 0.524630  [   30/   88]
per-ex loss: 0.398387  [   31/   88]
per-ex loss: 0.468678  [   32/   88]
per-ex loss: 0.561264  [   33/   88]
per-ex loss: 0.502874  [   34/   88]
per-ex loss: 0.487216  [   35/   88]
per-ex loss: 0.591013  [   36/   88]
per-ex loss: 0.743379  [   37/   88]
per-ex loss: 0.469979  [   38/   88]
per-ex loss: 0.461106  [   39/   88]
per-ex loss: 0.534394  [   40/   88]
per-ex loss: 0.701702  [   41/   88]
per-ex loss: 0.591702  [   42/   88]
per-ex loss: 0.550649  [   43/   88]
per-ex loss: 0.572953  [   44/   88]
per-ex loss: 0.482928  [   45/   88]
per-ex loss: 0.538212  [   46/   88]
per-ex loss: 0.574298  [   47/   88]
per-ex loss: 0.783807  [   48/   88]
per-ex loss: 0.561059  [   49/   88]
per-ex loss: 0.794861  [   50/   88]
per-ex loss: 0.498721  [   51/   88]
per-ex loss: 0.752986  [   52/   88]
per-ex loss: 0.585202  [   53/   88]
per-ex loss: 0.586932  [   54/   88]
per-ex loss: 0.507070  [   55/   88]
per-ex loss: 0.745460  [   56/   88]
per-ex loss: 0.532628  [   57/   88]
per-ex loss: 0.785038  [   58/   88]
per-ex loss: 0.595105  [   59/   88]
per-ex loss: 0.682563  [   60/   88]
per-ex loss: 0.509073  [   61/   88]
per-ex loss: 0.478755  [   62/   88]
per-ex loss: 0.681889  [   63/   88]
per-ex loss: 0.673512  [   64/   88]
per-ex loss: 0.483119  [   65/   88]
per-ex loss: 0.524659  [   66/   88]
per-ex loss: 0.491005  [   67/   88]
per-ex loss: 0.746364  [   68/   88]
per-ex loss: 0.664188  [   69/   88]
per-ex loss: 0.631900  [   70/   88]
per-ex loss: 0.637518  [   71/   88]
per-ex loss: 0.718009  [   72/   88]
per-ex loss: 0.751847  [   73/   88]
per-ex loss: 0.588518  [   74/   88]
per-ex loss: 0.560917  [   75/   88]
per-ex loss: 0.630782  [   76/   88]
per-ex loss: 0.683075  [   77/   88]
per-ex loss: 0.523543  [   78/   88]
per-ex loss: 0.512648  [   79/   88]
per-ex loss: 0.627449  [   80/   88]
per-ex loss: 0.452937  [   81/   88]
per-ex loss: 0.532551  [   82/   88]
per-ex loss: 0.802637  [   83/   88]
per-ex loss: 0.760691  [   84/   88]
per-ex loss: 0.687654  [   85/   88]
per-ex loss: 0.737398  [   86/   88]
per-ex loss: 0.577967  [   87/   88]
per-ex loss: 0.571087  [   88/   88]
Train Error: Avg loss: 0.61335774
validation Error: 
 Avg loss: 0.70032149 
 F1: 0.484175 
 Precision: 0.511612 
 Recall: 0.459532
 IoU: 0.319414

test Error: 
 Avg loss: 0.64547084 
 F1: 0.559004 
 Precision: 0.585126 
 Recall: 0.535115
 IoU: 0.387929

We have finished training iteration 92
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_90_.pth
per-ex loss: 0.783890  [    1/   88]
per-ex loss: 0.488136  [    2/   88]
per-ex loss: 0.574028  [    3/   88]
per-ex loss: 0.529483  [    4/   88]
per-ex loss: 0.697399  [    5/   88]
per-ex loss: 0.493400  [    6/   88]
per-ex loss: 0.732194  [    7/   88]
per-ex loss: 0.571395  [    8/   88]
per-ex loss: 0.606712  [    9/   88]
per-ex loss: 0.490330  [   10/   88]
per-ex loss: 0.493229  [   11/   88]
per-ex loss: 0.553609  [   12/   88]
per-ex loss: 0.506860  [   13/   88]
per-ex loss: 0.577212  [   14/   88]
per-ex loss: 0.782350  [   15/   88]
per-ex loss: 0.533692  [   16/   88]
per-ex loss: 0.562724  [   17/   88]
per-ex loss: 0.469838  [   18/   88]
per-ex loss: 0.744799  [   19/   88]
per-ex loss: 0.597494  [   20/   88]
per-ex loss: 0.512974  [   21/   88]
per-ex loss: 0.552073  [   22/   88]
per-ex loss: 0.468700  [   23/   88]
per-ex loss: 0.533222  [   24/   88]
per-ex loss: 0.543290  [   25/   88]
per-ex loss: 0.659018  [   26/   88]
per-ex loss: 0.525628  [   27/   88]
per-ex loss: 0.629889  [   28/   88]
per-ex loss: 0.504541  [   29/   88]
per-ex loss: 0.656842  [   30/   88]
per-ex loss: 0.511839  [   31/   88]
per-ex loss: 0.773279  [   32/   88]
per-ex loss: 0.623195  [   33/   88]
per-ex loss: 0.357720  [   34/   88]
per-ex loss: 0.709555  [   35/   88]
per-ex loss: 0.777191  [   36/   88]
per-ex loss: 0.577774  [   37/   88]
per-ex loss: 0.534922  [   38/   88]
per-ex loss: 0.564682  [   39/   88]
per-ex loss: 0.615837  [   40/   88]
per-ex loss: 0.786836  [   41/   88]
per-ex loss: 0.711367  [   42/   88]
per-ex loss: 0.787425  [   43/   88]
per-ex loss: 0.636597  [   44/   88]
per-ex loss: 0.470488  [   45/   88]
per-ex loss: 0.672148  [   46/   88]
per-ex loss: 0.539573  [   47/   88]
per-ex loss: 0.665253  [   48/   88]
per-ex loss: 0.568786  [   49/   88]
per-ex loss: 0.590372  [   50/   88]
per-ex loss: 0.758129  [   51/   88]
per-ex loss: 0.551554  [   52/   88]
per-ex loss: 0.644014  [   53/   88]
per-ex loss: 0.793814  [   54/   88]
per-ex loss: 0.613077  [   55/   88]
per-ex loss: 0.624303  [   56/   88]
per-ex loss: 0.694708  [   57/   88]
per-ex loss: 0.720344  [   58/   88]
per-ex loss: 0.476611  [   59/   88]
per-ex loss: 0.558630  [   60/   88]
per-ex loss: 0.692012  [   61/   88]
per-ex loss: 0.689131  [   62/   88]
per-ex loss: 0.785985  [   63/   88]
per-ex loss: 0.729408  [   64/   88]
per-ex loss: 0.712921  [   65/   88]
per-ex loss: 0.567554  [   66/   88]
per-ex loss: 0.744223  [   67/   88]
per-ex loss: 0.764312  [   68/   88]
per-ex loss: 0.732538  [   69/   88]
per-ex loss: 0.689910  [   70/   88]
per-ex loss: 0.637753  [   71/   88]
per-ex loss: 0.503267  [   72/   88]
per-ex loss: 0.505510  [   73/   88]
per-ex loss: 0.559480  [   74/   88]
per-ex loss: 0.519043  [   75/   88]
per-ex loss: 0.702527  [   76/   88]
per-ex loss: 0.504348  [   77/   88]
per-ex loss: 0.706866  [   78/   88]
per-ex loss: 0.620150  [   79/   88]
per-ex loss: 0.747433  [   80/   88]
per-ex loss: 0.723220  [   81/   88]
per-ex loss: 0.570323  [   82/   88]
per-ex loss: 0.702040  [   83/   88]
per-ex loss: 0.635414  [   84/   88]
per-ex loss: 0.450116  [   85/   88]
per-ex loss: 0.522660  [   86/   88]
per-ex loss: 0.560818  [   87/   88]
per-ex loss: 0.525979  [   88/   88]
Train Error: Avg loss: 0.61465813
validation Error: 
 Avg loss: 0.68693871 
 F1: 0.503696 
 Precision: 0.510829 
 Recall: 0.496760
 IoU: 0.336627

test Error: 
 Avg loss: 0.64212945 
 F1: 0.563461 
 Precision: 0.554489 
 Recall: 0.572728
 IoU: 0.392235

We have finished training iteration 93
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_91_.pth
per-ex loss: 0.590238  [    1/   88]
per-ex loss: 0.552482  [    2/   88]
per-ex loss: 0.494855  [    3/   88]
per-ex loss: 0.533913  [    4/   88]
per-ex loss: 0.502914  [    5/   88]
per-ex loss: 0.468742  [    6/   88]
per-ex loss: 0.531551  [    7/   88]
per-ex loss: 0.714083  [    8/   88]
per-ex loss: 0.684513  [    9/   88]
per-ex loss: 0.491852  [   10/   88]
per-ex loss: 0.641002  [   11/   88]
per-ex loss: 0.490287  [   12/   88]
per-ex loss: 0.543414  [   13/   88]
per-ex loss: 0.787260  [   14/   88]
per-ex loss: 0.530708  [   15/   88]
per-ex loss: 0.812429  [   16/   88]
per-ex loss: 0.513841  [   17/   88]
per-ex loss: 0.557410  [   18/   88]
per-ex loss: 0.447312  [   19/   88]
per-ex loss: 0.792955  [   20/   88]
per-ex loss: 0.498597  [   21/   88]
per-ex loss: 0.511272  [   22/   88]
per-ex loss: 0.548336  [   23/   88]
per-ex loss: 0.474082  [   24/   88]
per-ex loss: 0.723492  [   25/   88]
per-ex loss: 0.658486  [   26/   88]
per-ex loss: 0.751507  [   27/   88]
per-ex loss: 0.700487  [   28/   88]
per-ex loss: 0.574618  [   29/   88]
per-ex loss: 0.575005  [   30/   88]
per-ex loss: 0.608676  [   31/   88]
per-ex loss: 0.486558  [   32/   88]
per-ex loss: 0.621202  [   33/   88]
per-ex loss: 0.720663  [   34/   88]
per-ex loss: 0.706611  [   35/   88]
per-ex loss: 0.591909  [   36/   88]
per-ex loss: 0.776564  [   37/   88]
per-ex loss: 0.704436  [   38/   88]
per-ex loss: 0.678867  [   39/   88]
per-ex loss: 0.784379  [   40/   88]
per-ex loss: 0.507782  [   41/   88]
per-ex loss: 0.516586  [   42/   88]
per-ex loss: 0.441831  [   43/   88]
per-ex loss: 0.672124  [   44/   88]
per-ex loss: 0.470058  [   45/   88]
per-ex loss: 0.562017  [   46/   88]
per-ex loss: 0.558885  [   47/   88]
per-ex loss: 0.652951  [   48/   88]
per-ex loss: 0.526161  [   49/   88]
per-ex loss: 0.616115  [   50/   88]
per-ex loss: 0.535888  [   51/   88]
per-ex loss: 0.547748  [   52/   88]
per-ex loss: 0.500382  [   53/   88]
per-ex loss: 0.679859  [   54/   88]
per-ex loss: 0.772301  [   55/   88]
per-ex loss: 0.753314  [   56/   88]
per-ex loss: 0.765562  [   57/   88]
per-ex loss: 0.518197  [   58/   88]
per-ex loss: 0.721017  [   59/   88]
per-ex loss: 0.541681  [   60/   88]
per-ex loss: 0.391687  [   61/   88]
per-ex loss: 0.631334  [   62/   88]
per-ex loss: 0.518358  [   63/   88]
per-ex loss: 0.727584  [   64/   88]
per-ex loss: 0.528123  [   65/   88]
per-ex loss: 0.692754  [   66/   88]
per-ex loss: 0.588387  [   67/   88]
per-ex loss: 0.678001  [   68/   88]
per-ex loss: 0.695716  [   69/   88]
per-ex loss: 0.626860  [   70/   88]
per-ex loss: 0.772414  [   71/   88]
per-ex loss: 0.603614  [   72/   88]
per-ex loss: 0.506225  [   73/   88]
per-ex loss: 0.558364  [   74/   88]
per-ex loss: 0.607816  [   75/   88]
per-ex loss: 0.606091  [   76/   88]
per-ex loss: 0.736382  [   77/   88]
per-ex loss: 0.719227  [   78/   88]
per-ex loss: 0.507145  [   79/   88]
per-ex loss: 0.699592  [   80/   88]
per-ex loss: 0.617338  [   81/   88]
per-ex loss: 0.620452  [   82/   88]
per-ex loss: 0.715721  [   83/   88]
per-ex loss: 0.556809  [   84/   88]
per-ex loss: 0.738118  [   85/   88]
per-ex loss: 0.743905  [   86/   88]
per-ex loss: 0.701522  [   87/   88]
per-ex loss: 0.525768  [   88/   88]
Train Error: Avg loss: 0.61196898
validation Error: 
 Avg loss: 0.71227308 
 F1: 0.466707 
 Precision: 0.471743 
 Recall: 0.461777
 IoU: 0.304382

test Error: 
 Avg loss: 0.66064335 
 F1: 0.542820 
 Precision: 0.562822 
 Recall: 0.524192
 IoU: 0.372514

We have finished training iteration 94
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_92_.pth
per-ex loss: 0.560553  [    1/   88]
per-ex loss: 0.743282  [    2/   88]
per-ex loss: 0.730132  [    3/   88]
per-ex loss: 0.487783  [    4/   88]
per-ex loss: 0.479627  [    5/   88]
per-ex loss: 0.744687  [    6/   88]
per-ex loss: 0.597523  [    7/   88]
per-ex loss: 0.543097  [    8/   88]
per-ex loss: 0.652862  [    9/   88]
per-ex loss: 0.378067  [   10/   88]
per-ex loss: 0.535293  [   11/   88]
per-ex loss: 0.644464  [   12/   88]
per-ex loss: 0.554089  [   13/   88]
per-ex loss: 0.573995  [   14/   88]
per-ex loss: 0.565373  [   15/   88]
per-ex loss: 0.489399  [   16/   88]
per-ex loss: 0.786024  [   17/   88]
per-ex loss: 0.730945  [   18/   88]
per-ex loss: 0.784209  [   19/   88]
per-ex loss: 0.502410  [   20/   88]
per-ex loss: 0.632687  [   21/   88]
per-ex loss: 0.789566  [   22/   88]
per-ex loss: 0.531946  [   23/   88]
per-ex loss: 0.640665  [   24/   88]
per-ex loss: 0.732072  [   25/   88]
per-ex loss: 0.769099  [   26/   88]
per-ex loss: 0.583719  [   27/   88]
per-ex loss: 0.791037  [   28/   88]
per-ex loss: 0.616895  [   29/   88]
per-ex loss: 0.752126  [   30/   88]
per-ex loss: 0.504742  [   31/   88]
per-ex loss: 0.686812  [   32/   88]
per-ex loss: 0.514704  [   33/   88]
per-ex loss: 0.557194  [   34/   88]
per-ex loss: 0.660262  [   35/   88]
per-ex loss: 0.592007  [   36/   88]
per-ex loss: 0.742826  [   37/   88]
per-ex loss: 0.718431  [   38/   88]
per-ex loss: 0.719164  [   39/   88]
per-ex loss: 0.528263  [   40/   88]
per-ex loss: 0.657167  [   41/   88]
per-ex loss: 0.528531  [   42/   88]
per-ex loss: 0.685260  [   43/   88]
per-ex loss: 0.505222  [   44/   88]
per-ex loss: 0.493227  [   45/   88]
per-ex loss: 0.612387  [   46/   88]
per-ex loss: 0.736244  [   47/   88]
per-ex loss: 0.739834  [   48/   88]
per-ex loss: 0.698383  [   49/   88]
per-ex loss: 0.657362  [   50/   88]
per-ex loss: 0.672974  [   51/   88]
per-ex loss: 0.519128  [   52/   88]
per-ex loss: 0.466614  [   53/   88]
per-ex loss: 0.489068  [   54/   88]
per-ex loss: 0.505976  [   55/   88]
per-ex loss: 0.476949  [   56/   88]
per-ex loss: 0.600899  [   57/   88]
per-ex loss: 0.693708  [   58/   88]
per-ex loss: 0.747478  [   59/   88]
per-ex loss: 0.694700  [   60/   88]
per-ex loss: 0.468502  [   61/   88]
per-ex loss: 0.705823  [   62/   88]
per-ex loss: 0.551323  [   63/   88]
per-ex loss: 0.693957  [   64/   88]
per-ex loss: 0.518639  [   65/   88]
per-ex loss: 0.504129  [   66/   88]
per-ex loss: 0.737183  [   67/   88]
per-ex loss: 0.548715  [   68/   88]
per-ex loss: 0.772513  [   69/   88]
per-ex loss: 0.704802  [   70/   88]
per-ex loss: 0.756024  [   71/   88]
per-ex loss: 0.546627  [   72/   88]
per-ex loss: 0.546420  [   73/   88]
per-ex loss: 0.645814  [   74/   88]
per-ex loss: 0.641038  [   75/   88]
per-ex loss: 0.596025  [   76/   88]
per-ex loss: 0.553732  [   77/   88]
per-ex loss: 0.564103  [   78/   88]
per-ex loss: 0.562634  [   79/   88]
per-ex loss: 0.596597  [   80/   88]
per-ex loss: 0.637080  [   81/   88]
per-ex loss: 0.511036  [   82/   88]
per-ex loss: 0.702621  [   83/   88]
per-ex loss: 0.453467  [   84/   88]
per-ex loss: 0.577371  [   85/   88]
per-ex loss: 0.492588  [   86/   88]
per-ex loss: 0.532496  [   87/   88]
per-ex loss: 0.495617  [   88/   88]
Train Error: Avg loss: 0.61302291
validation Error: 
 Avg loss: 0.69262801 
 F1: 0.491306 
 Precision: 0.567899 
 Recall: 0.432918
 IoU: 0.325650

test Error: 
 Avg loss: 0.64959441 
 F1: 0.556157 
 Precision: 0.623376 
 Recall: 0.502023
 IoU: 0.385192

We have finished training iteration 95
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_93_.pth
per-ex loss: 0.640429  [    1/   88]
per-ex loss: 0.494151  [    2/   88]
per-ex loss: 0.769900  [    3/   88]
per-ex loss: 0.545942  [    4/   88]
per-ex loss: 0.800048  [    5/   88]
per-ex loss: 0.556870  [    6/   88]
per-ex loss: 0.755279  [    7/   88]
per-ex loss: 0.790538  [    8/   88]
per-ex loss: 0.629737  [    9/   88]
per-ex loss: 0.711034  [   10/   88]
per-ex loss: 0.759526  [   11/   88]
per-ex loss: 0.711117  [   12/   88]
per-ex loss: 0.558502  [   13/   88]
per-ex loss: 0.681171  [   14/   88]
per-ex loss: 0.765852  [   15/   88]
per-ex loss: 0.459553  [   16/   88]
per-ex loss: 0.485241  [   17/   88]
per-ex loss: 0.593099  [   18/   88]
per-ex loss: 0.722972  [   19/   88]
per-ex loss: 0.683195  [   20/   88]
per-ex loss: 0.703536  [   21/   88]
per-ex loss: 0.525321  [   22/   88]
per-ex loss: 0.497480  [   23/   88]
per-ex loss: 0.477078  [   24/   88]
per-ex loss: 0.684437  [   25/   88]
per-ex loss: 0.720495  [   26/   88]
per-ex loss: 0.481515  [   27/   88]
per-ex loss: 0.486522  [   28/   88]
per-ex loss: 0.808865  [   29/   88]
per-ex loss: 0.663691  [   30/   88]
per-ex loss: 0.564297  [   31/   88]
per-ex loss: 0.615695  [   32/   88]
per-ex loss: 0.707119  [   33/   88]
per-ex loss: 0.507959  [   34/   88]
per-ex loss: 0.461351  [   35/   88]
per-ex loss: 0.802421  [   36/   88]
per-ex loss: 0.749736  [   37/   88]
per-ex loss: 0.472478  [   38/   88]
per-ex loss: 0.561680  [   39/   88]
per-ex loss: 0.527208  [   40/   88]
per-ex loss: 0.577492  [   41/   88]
per-ex loss: 0.549364  [   42/   88]
per-ex loss: 0.621822  [   43/   88]
per-ex loss: 0.579761  [   44/   88]
per-ex loss: 0.611891  [   45/   88]
per-ex loss: 0.521986  [   46/   88]
per-ex loss: 0.556058  [   47/   88]
per-ex loss: 0.555779  [   48/   88]
per-ex loss: 0.560995  [   49/   88]
per-ex loss: 0.729326  [   50/   88]
per-ex loss: 0.598491  [   51/   88]
per-ex loss: 0.690369  [   52/   88]
per-ex loss: 0.535095  [   53/   88]
per-ex loss: 0.495370  [   54/   88]
per-ex loss: 0.648751  [   55/   88]
per-ex loss: 0.737400  [   56/   88]
per-ex loss: 0.513932  [   57/   88]
per-ex loss: 0.576599  [   58/   88]
per-ex loss: 0.677545  [   59/   88]
per-ex loss: 0.488037  [   60/   88]
per-ex loss: 0.728369  [   61/   88]
per-ex loss: 0.713423  [   62/   88]
per-ex loss: 0.588807  [   63/   88]
per-ex loss: 0.691927  [   64/   88]
per-ex loss: 0.525674  [   65/   88]
per-ex loss: 0.487561  [   66/   88]
per-ex loss: 0.657726  [   67/   88]
per-ex loss: 0.724409  [   68/   88]
per-ex loss: 0.553856  [   69/   88]
per-ex loss: 0.618546  [   70/   88]
per-ex loss: 0.780021  [   71/   88]
per-ex loss: 0.523175  [   72/   88]
per-ex loss: 0.652489  [   73/   88]
per-ex loss: 0.458091  [   74/   88]
per-ex loss: 0.616593  [   75/   88]
per-ex loss: 0.744200  [   76/   88]
per-ex loss: 0.584234  [   77/   88]
per-ex loss: 0.702896  [   78/   88]
per-ex loss: 0.712404  [   79/   88]
per-ex loss: 0.518381  [   80/   88]
per-ex loss: 0.620707  [   81/   88]
per-ex loss: 0.562799  [   82/   88]
per-ex loss: 0.591472  [   83/   88]
per-ex loss: 0.542591  [   84/   88]
per-ex loss: 0.448005  [   85/   88]
per-ex loss: 0.578077  [   86/   88]
per-ex loss: 0.350158  [   87/   88]
per-ex loss: 0.534862  [   88/   88]
Train Error: Avg loss: 0.61102903
validation Error: 
 Avg loss: 0.68941204 
 F1: 0.494883 
 Precision: 0.654267 
 Recall: 0.397941
 IoU: 0.328800

test Error: 
 Avg loss: 0.64828431 
 F1: 0.559541 
 Precision: 0.685354 
 Recall: 0.472756
 IoU: 0.388447

We have finished training iteration 96
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_94_.pth
per-ex loss: 0.574329  [    1/   88]
per-ex loss: 0.540301  [    2/   88]
per-ex loss: 0.632176  [    3/   88]
per-ex loss: 0.534764  [    4/   88]
per-ex loss: 0.756332  [    5/   88]
per-ex loss: 0.484602  [    6/   88]
per-ex loss: 0.583618  [    7/   88]
per-ex loss: 0.780553  [    8/   88]
per-ex loss: 0.592630  [    9/   88]
per-ex loss: 0.804298  [   10/   88]
per-ex loss: 0.704044  [   11/   88]
per-ex loss: 0.553966  [   12/   88]
per-ex loss: 0.498698  [   13/   88]
per-ex loss: 0.620130  [   14/   88]
per-ex loss: 0.536151  [   15/   88]
per-ex loss: 0.752724  [   16/   88]
per-ex loss: 0.583443  [   17/   88]
per-ex loss: 0.495585  [   18/   88]
per-ex loss: 0.496873  [   19/   88]
per-ex loss: 0.776122  [   20/   88]
per-ex loss: 0.756665  [   21/   88]
per-ex loss: 0.656359  [   22/   88]
per-ex loss: 0.575950  [   23/   88]
per-ex loss: 0.679841  [   24/   88]
per-ex loss: 0.462668  [   25/   88]
per-ex loss: 0.458328  [   26/   88]
per-ex loss: 0.731767  [   27/   88]
per-ex loss: 0.562682  [   28/   88]
per-ex loss: 0.725745  [   29/   88]
per-ex loss: 0.633416  [   30/   88]
per-ex loss: 0.524396  [   31/   88]
per-ex loss: 0.524901  [   32/   88]
per-ex loss: 0.599244  [   33/   88]
per-ex loss: 0.718830  [   34/   88]
per-ex loss: 0.484336  [   35/   88]
per-ex loss: 0.506774  [   36/   88]
per-ex loss: 0.530952  [   37/   88]
per-ex loss: 0.526539  [   38/   88]
per-ex loss: 0.733276  [   39/   88]
per-ex loss: 0.814523  [   40/   88]
per-ex loss: 0.491140  [   41/   88]
per-ex loss: 0.740132  [   42/   88]
per-ex loss: 0.566968  [   43/   88]
per-ex loss: 0.712829  [   44/   88]
per-ex loss: 0.455725  [   45/   88]
per-ex loss: 0.763702  [   46/   88]
per-ex loss: 0.484616  [   47/   88]
per-ex loss: 0.695340  [   48/   88]
per-ex loss: 0.460717  [   49/   88]
per-ex loss: 0.715843  [   50/   88]
per-ex loss: 0.453716  [   51/   88]
per-ex loss: 0.533583  [   52/   88]
per-ex loss: 0.734540  [   53/   88]
per-ex loss: 0.737288  [   54/   88]
per-ex loss: 0.556198  [   55/   88]
per-ex loss: 0.372609  [   56/   88]
per-ex loss: 0.793038  [   57/   88]
per-ex loss: 0.546102  [   58/   88]
per-ex loss: 0.503526  [   59/   88]
per-ex loss: 0.574898  [   60/   88]
per-ex loss: 0.764225  [   61/   88]
per-ex loss: 0.551988  [   62/   88]
per-ex loss: 0.624929  [   63/   88]
per-ex loss: 0.698757  [   64/   88]
per-ex loss: 0.489858  [   65/   88]
per-ex loss: 0.525441  [   66/   88]
per-ex loss: 0.720201  [   67/   88]
per-ex loss: 0.708661  [   68/   88]
per-ex loss: 0.672170  [   69/   88]
per-ex loss: 0.661279  [   70/   88]
per-ex loss: 0.655198  [   71/   88]
per-ex loss: 0.586617  [   72/   88]
per-ex loss: 0.676464  [   73/   88]
per-ex loss: 0.540114  [   74/   88]
per-ex loss: 0.520789  [   75/   88]
per-ex loss: 0.500321  [   76/   88]
per-ex loss: 0.602719  [   77/   88]
per-ex loss: 0.682229  [   78/   88]
per-ex loss: 0.679387  [   79/   88]
per-ex loss: 0.619718  [   80/   88]
per-ex loss: 0.574767  [   81/   88]
per-ex loss: 0.492540  [   82/   88]
per-ex loss: 0.562783  [   83/   88]
per-ex loss: 0.613262  [   84/   88]
per-ex loss: 0.689770  [   85/   88]
per-ex loss: 0.649584  [   86/   88]
per-ex loss: 0.713418  [   87/   88]
per-ex loss: 0.551210  [   88/   88]
Train Error: Avg loss: 0.61056176
validation Error: 
 Avg loss: 0.69222041 
 F1: 0.494121 
 Precision: 0.508011 
 Recall: 0.480970
 IoU: 0.328128

test Error: 
 Avg loss: 0.64986679 
 F1: 0.552201 
 Precision: 0.555021 
 Recall: 0.549409
 IoU: 0.381407

We have finished training iteration 97
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_95_.pth
per-ex loss: 0.767654  [    1/   88]
per-ex loss: 0.728436  [    2/   88]
per-ex loss: 0.617755  [    3/   88]
per-ex loss: 0.612905  [    4/   88]
per-ex loss: 0.559974  [    5/   88]
per-ex loss: 0.650246  [    6/   88]
per-ex loss: 0.705009  [    7/   88]
per-ex loss: 0.516212  [    8/   88]
per-ex loss: 0.553853  [    9/   88]
per-ex loss: 0.795682  [   10/   88]
per-ex loss: 0.510359  [   11/   88]
per-ex loss: 0.582748  [   12/   88]
per-ex loss: 0.788339  [   13/   88]
per-ex loss: 0.699141  [   14/   88]
per-ex loss: 0.636128  [   15/   88]
per-ex loss: 0.554582  [   16/   88]
per-ex loss: 0.759174  [   17/   88]
per-ex loss: 0.735865  [   18/   88]
per-ex loss: 0.679997  [   19/   88]
per-ex loss: 0.709908  [   20/   88]
per-ex loss: 0.782605  [   21/   88]
per-ex loss: 0.484468  [   22/   88]
per-ex loss: 0.504131  [   23/   88]
per-ex loss: 0.553973  [   24/   88]
per-ex loss: 0.509513  [   25/   88]
per-ex loss: 0.724562  [   26/   88]
per-ex loss: 0.497486  [   27/   88]
per-ex loss: 0.537265  [   28/   88]
per-ex loss: 0.540722  [   29/   88]
per-ex loss: 0.727919  [   30/   88]
per-ex loss: 0.593440  [   31/   88]
per-ex loss: 0.633782  [   32/   88]
per-ex loss: 0.714333  [   33/   88]
per-ex loss: 0.633100  [   34/   88]
per-ex loss: 0.680313  [   35/   88]
per-ex loss: 0.693142  [   36/   88]
per-ex loss: 0.501692  [   37/   88]
per-ex loss: 0.680761  [   38/   88]
per-ex loss: 0.787975  [   39/   88]
per-ex loss: 0.708678  [   40/   88]
per-ex loss: 0.567022  [   41/   88]
per-ex loss: 0.707769  [   42/   88]
per-ex loss: 0.726043  [   43/   88]
per-ex loss: 0.686902  [   44/   88]
per-ex loss: 0.769217  [   45/   88]
per-ex loss: 0.556649  [   46/   88]
per-ex loss: 0.580846  [   47/   88]
per-ex loss: 0.791496  [   48/   88]
per-ex loss: 0.658413  [   49/   88]
per-ex loss: 0.500972  [   50/   88]
per-ex loss: 0.749109  [   51/   88]
per-ex loss: 0.472641  [   52/   88]
per-ex loss: 0.368373  [   53/   88]
per-ex loss: 0.594379  [   54/   88]
per-ex loss: 0.554407  [   55/   88]
per-ex loss: 0.623322  [   56/   88]
per-ex loss: 0.535388  [   57/   88]
per-ex loss: 0.571443  [   58/   88]
per-ex loss: 0.457642  [   59/   88]
per-ex loss: 0.527641  [   60/   88]
per-ex loss: 0.483384  [   61/   88]
per-ex loss: 0.697936  [   62/   88]
per-ex loss: 0.602354  [   63/   88]
per-ex loss: 0.532772  [   64/   88]
per-ex loss: 0.608904  [   65/   88]
per-ex loss: 0.676149  [   66/   88]
per-ex loss: 0.488040  [   67/   88]
per-ex loss: 0.618993  [   68/   88]
per-ex loss: 0.517136  [   69/   88]
per-ex loss: 0.503168  [   70/   88]
per-ex loss: 0.473435  [   71/   88]
per-ex loss: 0.635051  [   72/   88]
per-ex loss: 0.724473  [   73/   88]
per-ex loss: 0.570690  [   74/   88]
per-ex loss: 0.532460  [   75/   88]
per-ex loss: 0.546373  [   76/   88]
per-ex loss: 0.459949  [   77/   88]
per-ex loss: 0.808596  [   78/   88]
per-ex loss: 0.527967  [   79/   88]
per-ex loss: 0.490495  [   80/   88]
per-ex loss: 0.571492  [   81/   88]
per-ex loss: 0.526244  [   82/   88]
per-ex loss: 0.485117  [   83/   88]
per-ex loss: 0.564395  [   84/   88]
per-ex loss: 0.628992  [   85/   88]
per-ex loss: 0.612751  [   86/   88]
per-ex loss: 0.569539  [   87/   88]
per-ex loss: 0.771874  [   88/   88]
Train Error: Avg loss: 0.61229798
validation Error: 
 Avg loss: 0.69596565 
 F1: 0.491350 
 Precision: 0.637140 
 Recall: 0.399855
 IoU: 0.325688

test Error: 
 Avg loss: 0.65657233 
 F1: 0.543791 
 Precision: 0.691964 
 Recall: 0.447883
 IoU: 0.373429

We have finished training iteration 98
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_96_.pth
per-ex loss: 0.767294  [    1/   88]
per-ex loss: 0.602298  [    2/   88]
per-ex loss: 0.536759  [    3/   88]
per-ex loss: 0.594316  [    4/   88]
per-ex loss: 0.711926  [    5/   88]
per-ex loss: 0.547143  [    6/   88]
per-ex loss: 0.693746  [    7/   88]
per-ex loss: 0.470101  [    8/   88]
per-ex loss: 0.557607  [    9/   88]
per-ex loss: 0.699498  [   10/   88]
per-ex loss: 0.473297  [   11/   88]
per-ex loss: 0.712666  [   12/   88]
per-ex loss: 0.748401  [   13/   88]
per-ex loss: 0.558967  [   14/   88]
per-ex loss: 0.616376  [   15/   88]
per-ex loss: 0.485164  [   16/   88]
per-ex loss: 0.588662  [   17/   88]
per-ex loss: 0.643665  [   18/   88]
per-ex loss: 0.598472  [   19/   88]
per-ex loss: 0.694743  [   20/   88]
per-ex loss: 0.611737  [   21/   88]
per-ex loss: 0.669234  [   22/   88]
per-ex loss: 0.480251  [   23/   88]
per-ex loss: 0.521075  [   24/   88]
per-ex loss: 0.537956  [   25/   88]
per-ex loss: 0.732305  [   26/   88]
per-ex loss: 0.596522  [   27/   88]
per-ex loss: 0.775799  [   28/   88]
per-ex loss: 0.500086  [   29/   88]
per-ex loss: 0.496802  [   30/   88]
per-ex loss: 0.544239  [   31/   88]
per-ex loss: 0.479146  [   32/   88]
per-ex loss: 0.537531  [   33/   88]
per-ex loss: 0.533290  [   34/   88]
per-ex loss: 0.723061  [   35/   88]
per-ex loss: 0.522935  [   36/   88]
per-ex loss: 0.719336  [   37/   88]
per-ex loss: 0.741045  [   38/   88]
per-ex loss: 0.549472  [   39/   88]
per-ex loss: 0.622742  [   40/   88]
per-ex loss: 0.470838  [   41/   88]
per-ex loss: 0.484475  [   42/   88]
per-ex loss: 0.549339  [   43/   88]
per-ex loss: 0.557515  [   44/   88]
per-ex loss: 0.794578  [   45/   88]
per-ex loss: 0.491262  [   46/   88]
per-ex loss: 0.630803  [   47/   88]
per-ex loss: 0.786057  [   48/   88]
per-ex loss: 0.546881  [   49/   88]
per-ex loss: 0.544131  [   50/   88]
per-ex loss: 0.537121  [   51/   88]
per-ex loss: 0.786013  [   52/   88]
per-ex loss: 0.445010  [   53/   88]
per-ex loss: 0.765793  [   54/   88]
per-ex loss: 0.764829  [   55/   88]
per-ex loss: 0.525820  [   56/   88]
per-ex loss: 0.516799  [   57/   88]
per-ex loss: 0.486165  [   58/   88]
per-ex loss: 0.378204  [   59/   88]
per-ex loss: 0.543549  [   60/   88]
per-ex loss: 0.636895  [   61/   88]
per-ex loss: 0.671809  [   62/   88]
per-ex loss: 0.697259  [   63/   88]
per-ex loss: 0.562089  [   64/   88]
per-ex loss: 0.638733  [   65/   88]
per-ex loss: 0.710182  [   66/   88]
per-ex loss: 0.759219  [   67/   88]
per-ex loss: 0.600226  [   68/   88]
per-ex loss: 0.698285  [   69/   88]
per-ex loss: 0.764046  [   70/   88]
per-ex loss: 0.478585  [   71/   88]
per-ex loss: 0.725734  [   72/   88]
per-ex loss: 0.720091  [   73/   88]
per-ex loss: 0.533040  [   74/   88]
per-ex loss: 0.513455  [   75/   88]
per-ex loss: 0.713507  [   76/   88]
per-ex loss: 0.557907  [   77/   88]
per-ex loss: 0.588983  [   78/   88]
per-ex loss: 0.629000  [   79/   88]
per-ex loss: 0.750110  [   80/   88]
per-ex loss: 0.676224  [   81/   88]
per-ex loss: 0.528385  [   82/   88]
per-ex loss: 0.659491  [   83/   88]
per-ex loss: 0.633858  [   84/   88]
per-ex loss: 0.544310  [   85/   88]
per-ex loss: 0.561456  [   86/   88]
per-ex loss: 0.701259  [   87/   88]
per-ex loss: 0.713320  [   88/   88]
Train Error: Avg loss: 0.61100349
validation Error: 
 Avg loss: 0.68722238 
 F1: 0.500522 
 Precision: 0.592354 
 Recall: 0.433342
 IoU: 0.333798

test Error: 
 Avg loss: 0.64323353 
 F1: 0.562367 
 Precision: 0.637923 
 Recall: 0.502813
 IoU: 0.391175

We have finished training iteration 99
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_97_.pth
per-ex loss: 0.598063  [    1/   88]
per-ex loss: 0.583639  [    2/   88]
per-ex loss: 0.548133  [    3/   88]
per-ex loss: 0.725607  [    4/   88]
per-ex loss: 0.590137  [    5/   88]
per-ex loss: 0.520929  [    6/   88]
per-ex loss: 0.588245  [    7/   88]
per-ex loss: 0.642121  [    8/   88]
per-ex loss: 0.534585  [    9/   88]
per-ex loss: 0.516803  [   10/   88]
per-ex loss: 0.662115  [   11/   88]
per-ex loss: 0.516255  [   12/   88]
per-ex loss: 0.754740  [   13/   88]
per-ex loss: 0.360233  [   14/   88]
per-ex loss: 0.720313  [   15/   88]
per-ex loss: 0.450006  [   16/   88]
per-ex loss: 0.484091  [   17/   88]
per-ex loss: 0.486069  [   18/   88]
per-ex loss: 0.463142  [   19/   88]
per-ex loss: 0.644578  [   20/   88]
per-ex loss: 0.777644  [   21/   88]
per-ex loss: 0.526849  [   22/   88]
per-ex loss: 0.740100  [   23/   88]
per-ex loss: 0.592730  [   24/   88]
per-ex loss: 0.494379  [   25/   88]
per-ex loss: 0.701060  [   26/   88]
per-ex loss: 0.682201  [   27/   88]
per-ex loss: 0.471094  [   28/   88]
per-ex loss: 0.783537  [   29/   88]
per-ex loss: 0.573561  [   30/   88]
per-ex loss: 0.722323  [   31/   88]
per-ex loss: 0.728801  [   32/   88]
per-ex loss: 0.490183  [   33/   88]
per-ex loss: 0.640042  [   34/   88]
per-ex loss: 0.614275  [   35/   88]
per-ex loss: 0.513512  [   36/   88]
per-ex loss: 0.516713  [   37/   88]
per-ex loss: 0.640401  [   38/   88]
per-ex loss: 0.491861  [   39/   88]
per-ex loss: 0.563356  [   40/   88]
per-ex loss: 0.567421  [   41/   88]
per-ex loss: 0.561787  [   42/   88]
per-ex loss: 0.500189  [   43/   88]
per-ex loss: 0.762307  [   44/   88]
per-ex loss: 0.550347  [   45/   88]
per-ex loss: 0.530236  [   46/   88]
per-ex loss: 0.564412  [   47/   88]
per-ex loss: 0.744082  [   48/   88]
per-ex loss: 0.518928  [   49/   88]
per-ex loss: 0.731108  [   50/   88]
per-ex loss: 0.577128  [   51/   88]
per-ex loss: 0.523494  [   52/   88]
per-ex loss: 0.648191  [   53/   88]
per-ex loss: 0.619325  [   54/   88]
per-ex loss: 0.784645  [   55/   88]
per-ex loss: 0.785029  [   56/   88]
per-ex loss: 0.691043  [   57/   88]
per-ex loss: 0.693419  [   58/   88]
per-ex loss: 0.682127  [   59/   88]
per-ex loss: 0.503392  [   60/   88]
per-ex loss: 0.822558  [   61/   88]
per-ex loss: 0.740390  [   62/   88]
per-ex loss: 0.555081  [   63/   88]
per-ex loss: 0.651687  [   64/   88]
per-ex loss: 0.569276  [   65/   88]
per-ex loss: 0.552600  [   66/   88]
per-ex loss: 0.692182  [   67/   88]
per-ex loss: 0.480815  [   68/   88]
per-ex loss: 0.498266  [   69/   88]
per-ex loss: 0.530682  [   70/   88]
per-ex loss: 0.464366  [   71/   88]
per-ex loss: 0.679989  [   72/   88]
per-ex loss: 0.555657  [   73/   88]
per-ex loss: 0.512421  [   74/   88]
per-ex loss: 0.712655  [   75/   88]
per-ex loss: 0.656980  [   76/   88]
per-ex loss: 0.756845  [   77/   88]
per-ex loss: 0.698814  [   78/   88]
per-ex loss: 0.551431  [   79/   88]
per-ex loss: 0.762865  [   80/   88]
per-ex loss: 0.494142  [   81/   88]
per-ex loss: 0.589460  [   82/   88]
per-ex loss: 0.779298  [   83/   88]
per-ex loss: 0.668642  [   84/   88]
per-ex loss: 0.599289  [   85/   88]
per-ex loss: 0.545843  [   86/   88]
per-ex loss: 0.727173  [   87/   88]
per-ex loss: 0.787368  [   88/   88]
Train Error: Avg loss: 0.61170244
validation Error: 
 Avg loss: 0.70149160 
 F1: 0.483493 
 Precision: 0.493498 
 Recall: 0.473886
 IoU: 0.318820

test Error: 
 Avg loss: 0.65627760 
 F1: 0.548408 
 Precision: 0.546596 
 Recall: 0.550233
 IoU: 0.377798

We have finished training iteration 100
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_98_.pth
per-ex loss: 0.622875  [    1/   88]
per-ex loss: 0.459181  [    2/   88]
per-ex loss: 0.780110  [    3/   88]
per-ex loss: 0.537837  [    4/   88]
per-ex loss: 0.708902  [    5/   88]
per-ex loss: 0.697523  [    6/   88]
per-ex loss: 0.473243  [    7/   88]
per-ex loss: 0.655842  [    8/   88]
per-ex loss: 0.662913  [    9/   88]
per-ex loss: 0.464508  [   10/   88]
per-ex loss: 0.565511  [   11/   88]
per-ex loss: 0.723262  [   12/   88]
per-ex loss: 0.689324  [   13/   88]
per-ex loss: 0.545530  [   14/   88]
per-ex loss: 0.517308  [   15/   88]
per-ex loss: 0.673013  [   16/   88]
per-ex loss: 0.626456  [   17/   88]
per-ex loss: 0.757319  [   18/   88]
per-ex loss: 0.724290  [   19/   88]
per-ex loss: 0.637049  [   20/   88]
per-ex loss: 0.675804  [   21/   88]
per-ex loss: 0.471868  [   22/   88]
per-ex loss: 0.512942  [   23/   88]
per-ex loss: 0.554162  [   24/   88]
per-ex loss: 0.695044  [   25/   88]
per-ex loss: 0.502396  [   26/   88]
per-ex loss: 0.686639  [   27/   88]
per-ex loss: 0.756451  [   28/   88]
per-ex loss: 0.633340  [   29/   88]
per-ex loss: 0.482965  [   30/   88]
per-ex loss: 0.721437  [   31/   88]
per-ex loss: 0.783042  [   32/   88]
per-ex loss: 0.724889  [   33/   88]
per-ex loss: 0.697811  [   34/   88]
per-ex loss: 0.545581  [   35/   88]
per-ex loss: 0.734704  [   36/   88]
per-ex loss: 0.568438  [   37/   88]
per-ex loss: 0.639608  [   38/   88]
per-ex loss: 0.360327  [   39/   88]
per-ex loss: 0.489262  [   40/   88]
per-ex loss: 0.581185  [   41/   88]
per-ex loss: 0.587503  [   42/   88]
per-ex loss: 0.613425  [   43/   88]
per-ex loss: 0.475144  [   44/   88]
per-ex loss: 0.487265  [   45/   88]
per-ex loss: 0.519574  [   46/   88]
per-ex loss: 0.519135  [   47/   88]
per-ex loss: 0.482901  [   48/   88]
per-ex loss: 0.711609  [   49/   88]
per-ex loss: 0.566365  [   50/   88]
per-ex loss: 0.755798  [   51/   88]
per-ex loss: 0.455361  [   52/   88]
per-ex loss: 0.708354  [   53/   88]
per-ex loss: 0.746926  [   54/   88]
per-ex loss: 0.641773  [   55/   88]
per-ex loss: 0.491453  [   56/   88]
per-ex loss: 0.535708  [   57/   88]
per-ex loss: 0.502163  [   58/   88]
per-ex loss: 0.529940  [   59/   88]
per-ex loss: 0.480263  [   60/   88]
per-ex loss: 0.615969  [   61/   88]
per-ex loss: 0.528980  [   62/   88]
per-ex loss: 0.782732  [   63/   88]
per-ex loss: 0.539655  [   64/   88]
per-ex loss: 0.644001  [   65/   88]
per-ex loss: 0.718777  [   66/   88]
per-ex loss: 0.756494  [   67/   88]
per-ex loss: 0.586993  [   68/   88]
per-ex loss: 0.595840  [   69/   88]
per-ex loss: 0.762203  [   70/   88]
per-ex loss: 0.705301  [   71/   88]
per-ex loss: 0.548944  [   72/   88]
per-ex loss: 0.791510  [   73/   88]
per-ex loss: 0.798215  [   74/   88]
per-ex loss: 0.546325  [   75/   88]
per-ex loss: 0.562598  [   76/   88]
per-ex loss: 0.482667  [   77/   88]
per-ex loss: 0.608488  [   78/   88]
per-ex loss: 0.474231  [   79/   88]
per-ex loss: 0.723995  [   80/   88]
per-ex loss: 0.579272  [   81/   88]
per-ex loss: 0.645769  [   82/   88]
per-ex loss: 0.665229  [   83/   88]
per-ex loss: 0.526099  [   84/   88]
per-ex loss: 0.599400  [   85/   88]
per-ex loss: 0.493628  [   86/   88]
per-ex loss: 0.538244  [   87/   88]
per-ex loss: 0.546698  [   88/   88]
Train Error: Avg loss: 0.60816826
validation Error: 
 Avg loss: 0.68568654 
 F1: 0.501843 
 Precision: 0.540284 
 Recall: 0.468509
 IoU: 0.334974

test Error: 
 Avg loss: 0.64426376 
 F1: 0.565309 
 Precision: 0.587434 
 Recall: 0.544791
 IoU: 0.394029

We have finished training iteration 101
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_99_.pth
per-ex loss: 0.614185  [    1/   88]
per-ex loss: 0.541407  [    2/   88]
per-ex loss: 0.406549  [    3/   88]
per-ex loss: 0.472675  [    4/   88]
per-ex loss: 0.525716  [    5/   88]
per-ex loss: 0.518332  [    6/   88]
per-ex loss: 0.619340  [    7/   88]
per-ex loss: 0.531941  [    8/   88]
per-ex loss: 0.700692  [    9/   88]
per-ex loss: 0.707838  [   10/   88]
per-ex loss: 0.483704  [   11/   88]
per-ex loss: 0.699630  [   12/   88]
per-ex loss: 0.489726  [   13/   88]
per-ex loss: 0.467816  [   14/   88]
per-ex loss: 0.687366  [   15/   88]
per-ex loss: 0.582976  [   16/   88]
per-ex loss: 0.649154  [   17/   88]
per-ex loss: 0.716710  [   18/   88]
per-ex loss: 0.542925  [   19/   88]
per-ex loss: 0.591195  [   20/   88]
per-ex loss: 0.494593  [   21/   88]
per-ex loss: 0.731794  [   22/   88]
per-ex loss: 0.519649  [   23/   88]
per-ex loss: 0.735995  [   24/   88]
per-ex loss: 0.694345  [   25/   88]
per-ex loss: 0.530585  [   26/   88]
per-ex loss: 0.509275  [   27/   88]
per-ex loss: 0.486919  [   28/   88]
per-ex loss: 0.794394  [   29/   88]
per-ex loss: 0.774944  [   30/   88]
per-ex loss: 0.512040  [   31/   88]
per-ex loss: 0.564658  [   32/   88]
per-ex loss: 0.575587  [   33/   88]
per-ex loss: 0.547065  [   34/   88]
per-ex loss: 0.707090  [   35/   88]
per-ex loss: 0.453127  [   36/   88]
per-ex loss: 0.619967  [   37/   88]
per-ex loss: 0.469901  [   38/   88]
per-ex loss: 0.714956  [   39/   88]
per-ex loss: 0.533373  [   40/   88]
per-ex loss: 0.536825  [   41/   88]
per-ex loss: 0.756510  [   42/   88]
per-ex loss: 0.709059  [   43/   88]
per-ex loss: 0.513883  [   44/   88]
per-ex loss: 0.506956  [   45/   88]
per-ex loss: 0.770194  [   46/   88]
per-ex loss: 0.730493  [   47/   88]
per-ex loss: 0.662899  [   48/   88]
per-ex loss: 0.739538  [   49/   88]
per-ex loss: 0.490675  [   50/   88]
per-ex loss: 0.698658  [   51/   88]
per-ex loss: 0.461785  [   52/   88]
per-ex loss: 0.723013  [   53/   88]
per-ex loss: 0.621925  [   54/   88]
per-ex loss: 0.520803  [   55/   88]
per-ex loss: 0.591741  [   56/   88]
per-ex loss: 0.654031  [   57/   88]
per-ex loss: 0.653143  [   58/   88]
per-ex loss: 0.728072  [   59/   88]
per-ex loss: 0.777032  [   60/   88]
per-ex loss: 0.796999  [   61/   88]
per-ex loss: 0.532512  [   62/   88]
per-ex loss: 0.501154  [   63/   88]
per-ex loss: 0.737426  [   64/   88]
per-ex loss: 0.532553  [   65/   88]
per-ex loss: 0.708954  [   66/   88]
per-ex loss: 0.514778  [   67/   88]
per-ex loss: 0.742561  [   68/   88]
per-ex loss: 0.559181  [   69/   88]
per-ex loss: 0.663549  [   70/   88]
per-ex loss: 0.554131  [   71/   88]
per-ex loss: 0.611217  [   72/   88]
per-ex loss: 0.741826  [   73/   88]
per-ex loss: 0.628518  [   74/   88]
per-ex loss: 0.759493  [   75/   88]
per-ex loss: 0.536551  [   76/   88]
per-ex loss: 0.583297  [   77/   88]
per-ex loss: 0.498353  [   78/   88]
per-ex loss: 0.484456  [   79/   88]
per-ex loss: 0.688879  [   80/   88]
per-ex loss: 0.456196  [   81/   88]
per-ex loss: 0.686309  [   82/   88]
per-ex loss: 0.644911  [   83/   88]
per-ex loss: 0.474032  [   84/   88]
per-ex loss: 0.593035  [   85/   88]
per-ex loss: 0.607828  [   86/   88]
per-ex loss: 0.591166  [   87/   88]
per-ex loss: 0.536440  [   88/   88]
Train Error: Avg loss: 0.60606452
validation Error: 
 Avg loss: 0.68010468 
 F1: 0.508968 
 Precision: 0.587699 
 Recall: 0.448840
 IoU: 0.341353

test Error: 
 Avg loss: 0.63968304 
 F1: 0.569191 
 Precision: 0.631406 
 Recall: 0.518137
 IoU: 0.397811

We have finished training iteration 102
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_100_.pth
per-ex loss: 0.638002  [    1/   88]
per-ex loss: 0.703725  [    2/   88]
per-ex loss: 0.632669  [    3/   88]
per-ex loss: 0.546315  [    4/   88]
per-ex loss: 0.490966  [    5/   88]
per-ex loss: 0.492805  [    6/   88]
per-ex loss: 0.640743  [    7/   88]
per-ex loss: 0.751977  [    8/   88]
per-ex loss: 0.479585  [    9/   88]
per-ex loss: 0.717247  [   10/   88]
per-ex loss: 0.739807  [   11/   88]
per-ex loss: 0.580004  [   12/   88]
per-ex loss: 0.699054  [   13/   88]
per-ex loss: 0.777744  [   14/   88]
per-ex loss: 0.483227  [   15/   88]
per-ex loss: 0.779344  [   16/   88]
per-ex loss: 0.497853  [   17/   88]
per-ex loss: 0.693345  [   18/   88]
per-ex loss: 0.672184  [   19/   88]
per-ex loss: 0.757778  [   20/   88]
per-ex loss: 0.727352  [   21/   88]
per-ex loss: 0.516824  [   22/   88]
per-ex loss: 0.610538  [   23/   88]
per-ex loss: 0.673996  [   24/   88]
per-ex loss: 0.487843  [   25/   88]
per-ex loss: 0.600978  [   26/   88]
per-ex loss: 0.619172  [   27/   88]
per-ex loss: 0.476016  [   28/   88]
per-ex loss: 0.676328  [   29/   88]
per-ex loss: 0.537572  [   30/   88]
per-ex loss: 0.550940  [   31/   88]
per-ex loss: 0.736416  [   32/   88]
per-ex loss: 0.567728  [   33/   88]
per-ex loss: 0.591334  [   34/   88]
per-ex loss: 0.510726  [   35/   88]
per-ex loss: 0.683098  [   36/   88]
per-ex loss: 0.487051  [   37/   88]
per-ex loss: 0.772725  [   38/   88]
per-ex loss: 0.619624  [   39/   88]
per-ex loss: 0.537170  [   40/   88]
per-ex loss: 0.697548  [   41/   88]
per-ex loss: 0.593657  [   42/   88]
per-ex loss: 0.719210  [   43/   88]
per-ex loss: 0.554659  [   44/   88]
per-ex loss: 0.688858  [   45/   88]
per-ex loss: 0.470558  [   46/   88]
per-ex loss: 0.374080  [   47/   88]
per-ex loss: 0.543368  [   48/   88]
per-ex loss: 0.788550  [   49/   88]
per-ex loss: 0.617366  [   50/   88]
per-ex loss: 0.678597  [   51/   88]
per-ex loss: 0.733980  [   52/   88]
per-ex loss: 0.558648  [   53/   88]
per-ex loss: 0.545924  [   54/   88]
per-ex loss: 0.520299  [   55/   88]
per-ex loss: 0.593175  [   56/   88]
per-ex loss: 0.548846  [   57/   88]
per-ex loss: 0.474151  [   58/   88]
per-ex loss: 0.719750  [   59/   88]
per-ex loss: 0.675232  [   60/   88]
per-ex loss: 0.784116  [   61/   88]
per-ex loss: 0.554398  [   62/   88]
per-ex loss: 0.512568  [   63/   88]
per-ex loss: 0.543360  [   64/   88]
per-ex loss: 0.632360  [   65/   88]
per-ex loss: 0.502522  [   66/   88]
per-ex loss: 0.485187  [   67/   88]
per-ex loss: 0.635948  [   68/   88]
per-ex loss: 0.477091  [   69/   88]
per-ex loss: 0.515687  [   70/   88]
per-ex loss: 0.475762  [   71/   88]
per-ex loss: 0.586283  [   72/   88]
per-ex loss: 0.693199  [   73/   88]
per-ex loss: 0.546432  [   74/   88]
per-ex loss: 0.652565  [   75/   88]
per-ex loss: 0.457052  [   76/   88]
per-ex loss: 0.613148  [   77/   88]
per-ex loss: 0.502360  [   78/   88]
per-ex loss: 0.541651  [   79/   88]
per-ex loss: 0.519850  [   80/   88]
per-ex loss: 0.704173  [   81/   88]
per-ex loss: 0.745664  [   82/   88]
per-ex loss: 0.759403  [   83/   88]
per-ex loss: 0.739879  [   84/   88]
per-ex loss: 0.559037  [   85/   88]
per-ex loss: 0.482909  [   86/   88]
per-ex loss: 0.490149  [   87/   88]
per-ex loss: 0.793876  [   88/   88]
Train Error: Avg loss: 0.60637374
validation Error: 
 Avg loss: 0.68332626 
 F1: 0.501891 
 Precision: 0.557640 
 Recall: 0.456276
 IoU: 0.335017

test Error: 
 Avg loss: 0.64457949 
 F1: 0.563266 
 Precision: 0.601115 
 Recall: 0.529900
 IoU: 0.392046

We have finished training iteration 103
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_101_.pth
per-ex loss: 0.545196  [    1/   88]
per-ex loss: 0.462016  [    2/   88]
per-ex loss: 0.634526  [    3/   88]
per-ex loss: 0.592725  [    4/   88]
per-ex loss: 0.472190  [    5/   88]
per-ex loss: 0.663097  [    6/   88]
per-ex loss: 0.727282  [    7/   88]
per-ex loss: 0.717014  [    8/   88]
per-ex loss: 0.778715  [    9/   88]
per-ex loss: 0.738255  [   10/   88]
per-ex loss: 0.767305  [   11/   88]
per-ex loss: 0.496965  [   12/   88]
per-ex loss: 0.552772  [   13/   88]
per-ex loss: 0.549473  [   14/   88]
per-ex loss: 0.646161  [   15/   88]
per-ex loss: 0.503236  [   16/   88]
per-ex loss: 0.811665  [   17/   88]
per-ex loss: 0.620289  [   18/   88]
per-ex loss: 0.767895  [   19/   88]
per-ex loss: 0.560570  [   20/   88]
per-ex loss: 0.503166  [   21/   88]
per-ex loss: 0.553550  [   22/   88]
per-ex loss: 0.533318  [   23/   88]
per-ex loss: 0.558328  [   24/   88]
per-ex loss: 0.596580  [   25/   88]
per-ex loss: 0.531288  [   26/   88]
per-ex loss: 0.680080  [   27/   88]
per-ex loss: 0.694000  [   28/   88]
per-ex loss: 0.730813  [   29/   88]
per-ex loss: 0.649944  [   30/   88]
per-ex loss: 0.716816  [   31/   88]
per-ex loss: 0.574414  [   32/   88]
per-ex loss: 0.713551  [   33/   88]
per-ex loss: 0.727083  [   34/   88]
per-ex loss: 0.648145  [   35/   88]
per-ex loss: 0.707166  [   36/   88]
per-ex loss: 0.602165  [   37/   88]
per-ex loss: 0.459705  [   38/   88]
per-ex loss: 0.565985  [   39/   88]
per-ex loss: 0.548283  [   40/   88]
per-ex loss: 0.720066  [   41/   88]
per-ex loss: 0.740258  [   42/   88]
per-ex loss: 0.622515  [   43/   88]
per-ex loss: 0.403584  [   44/   88]
per-ex loss: 0.476451  [   45/   88]
per-ex loss: 0.787889  [   46/   88]
per-ex loss: 0.572977  [   47/   88]
per-ex loss: 0.739336  [   48/   88]
per-ex loss: 0.532197  [   49/   88]
per-ex loss: 0.538111  [   50/   88]
per-ex loss: 0.744268  [   51/   88]
per-ex loss: 0.739120  [   52/   88]
per-ex loss: 0.619479  [   53/   88]
per-ex loss: 0.625880  [   54/   88]
per-ex loss: 0.599556  [   55/   88]
per-ex loss: 0.689551  [   56/   88]
per-ex loss: 0.693606  [   57/   88]
per-ex loss: 0.560179  [   58/   88]
per-ex loss: 0.659898  [   59/   88]
per-ex loss: 0.547580  [   60/   88]
per-ex loss: 0.627824  [   61/   88]
per-ex loss: 0.573304  [   62/   88]
per-ex loss: 0.505599  [   63/   88]
per-ex loss: 0.510391  [   64/   88]
per-ex loss: 0.473160  [   65/   88]
per-ex loss: 0.578720  [   66/   88]
per-ex loss: 0.476850  [   67/   88]
per-ex loss: 0.727228  [   68/   88]
per-ex loss: 0.538608  [   69/   88]
per-ex loss: 0.499500  [   70/   88]
per-ex loss: 0.501311  [   71/   88]
per-ex loss: 0.607447  [   72/   88]
per-ex loss: 0.478160  [   73/   88]
per-ex loss: 0.754422  [   74/   88]
per-ex loss: 0.531940  [   75/   88]
per-ex loss: 0.447280  [   76/   88]
per-ex loss: 0.713773  [   77/   88]
per-ex loss: 0.475055  [   78/   88]
per-ex loss: 0.681506  [   79/   88]
per-ex loss: 0.466138  [   80/   88]
per-ex loss: 0.541032  [   81/   88]
per-ex loss: 0.530746  [   82/   88]
per-ex loss: 0.604692  [   83/   88]
per-ex loss: 0.535052  [   84/   88]
per-ex loss: 0.458427  [   85/   88]
per-ex loss: 0.755850  [   86/   88]
per-ex loss: 0.786418  [   87/   88]
per-ex loss: 0.482528  [   88/   88]
Train Error: Avg loss: 0.60655892
validation Error: 
 Avg loss: 0.68755134 
 F1: 0.499083 
 Precision: 0.642470 
 Recall: 0.408021
 IoU: 0.332519

test Error: 
 Avg loss: 0.64546225 
 F1: 0.562472 
 Precision: 0.670336 
 Recall: 0.484509
 IoU: 0.391277

We have finished training iteration 104
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_86_.pth
per-ex loss: 0.501324  [    1/   88]
per-ex loss: 0.531157  [    2/   88]
per-ex loss: 0.737726  [    3/   88]
per-ex loss: 0.540632  [    4/   88]
per-ex loss: 0.369746  [    5/   88]
per-ex loss: 0.587324  [    6/   88]
per-ex loss: 0.506389  [    7/   88]
per-ex loss: 0.582918  [    8/   88]
per-ex loss: 0.539421  [    9/   88]
per-ex loss: 0.711142  [   10/   88]
per-ex loss: 0.556399  [   11/   88]
per-ex loss: 0.532108  [   12/   88]
per-ex loss: 0.498888  [   13/   88]
per-ex loss: 0.808954  [   14/   88]
per-ex loss: 0.538247  [   15/   88]
per-ex loss: 0.546524  [   16/   88]
per-ex loss: 0.458021  [   17/   88]
per-ex loss: 0.735671  [   18/   88]
per-ex loss: 0.773926  [   19/   88]
per-ex loss: 0.752481  [   20/   88]
per-ex loss: 0.777507  [   21/   88]
per-ex loss: 0.476386  [   22/   88]
per-ex loss: 0.516049  [   23/   88]
per-ex loss: 0.522659  [   24/   88]
per-ex loss: 0.511759  [   25/   88]
per-ex loss: 0.659570  [   26/   88]
per-ex loss: 0.769578  [   27/   88]
per-ex loss: 0.565904  [   28/   88]
per-ex loss: 0.458029  [   29/   88]
per-ex loss: 0.527922  [   30/   88]
per-ex loss: 0.584400  [   31/   88]
per-ex loss: 0.654132  [   32/   88]
per-ex loss: 0.745189  [   33/   88]
per-ex loss: 0.612140  [   34/   88]
per-ex loss: 0.740907  [   35/   88]
per-ex loss: 0.605542  [   36/   88]
per-ex loss: 0.657774  [   37/   88]
per-ex loss: 0.689344  [   38/   88]
per-ex loss: 0.616263  [   39/   88]
per-ex loss: 0.579737  [   40/   88]
per-ex loss: 0.696313  [   41/   88]
per-ex loss: 0.485610  [   42/   88]
per-ex loss: 0.709941  [   43/   88]
per-ex loss: 0.573484  [   44/   88]
per-ex loss: 0.504108  [   45/   88]
per-ex loss: 0.785179  [   46/   88]
per-ex loss: 0.464412  [   47/   88]
per-ex loss: 0.690602  [   48/   88]
per-ex loss: 0.557446  [   49/   88]
per-ex loss: 0.648668  [   50/   88]
per-ex loss: 0.556627  [   51/   88]
per-ex loss: 0.537814  [   52/   88]
per-ex loss: 0.517913  [   53/   88]
per-ex loss: 0.697303  [   54/   88]
per-ex loss: 0.634665  [   55/   88]
per-ex loss: 0.724970  [   56/   88]
per-ex loss: 0.476089  [   57/   88]
per-ex loss: 0.692447  [   58/   88]
per-ex loss: 0.477948  [   59/   88]
per-ex loss: 0.550233  [   60/   88]
per-ex loss: 0.467344  [   61/   88]
per-ex loss: 0.598879  [   62/   88]
per-ex loss: 0.809614  [   63/   88]
per-ex loss: 0.493341  [   64/   88]
per-ex loss: 0.644894  [   65/   88]
per-ex loss: 0.778926  [   66/   88]
per-ex loss: 0.510772  [   67/   88]
per-ex loss: 0.506259  [   68/   88]
per-ex loss: 0.739395  [   69/   88]
per-ex loss: 0.582975  [   70/   88]
per-ex loss: 0.536419  [   71/   88]
per-ex loss: 0.791453  [   72/   88]
per-ex loss: 0.491541  [   73/   88]
per-ex loss: 0.621981  [   74/   88]
per-ex loss: 0.572473  [   75/   88]
per-ex loss: 0.575477  [   76/   88]
per-ex loss: 0.699380  [   77/   88]
per-ex loss: 0.746107  [   78/   88]
per-ex loss: 0.536408  [   79/   88]
per-ex loss: 0.511856  [   80/   88]
per-ex loss: 0.736242  [   81/   88]
per-ex loss: 0.487224  [   82/   88]
per-ex loss: 0.454912  [   83/   88]
per-ex loss: 0.690812  [   84/   88]
per-ex loss: 0.688300  [   85/   88]
per-ex loss: 0.580248  [   86/   88]
per-ex loss: 0.727485  [   87/   88]
per-ex loss: 0.692949  [   88/   88]
Train Error: Avg loss: 0.60608214
validation Error: 
 Avg loss: 0.68380909 
 F1: 0.504611 
 Precision: 0.559394 
 Recall: 0.459601
 IoU: 0.337445

test Error: 
 Avg loss: 0.63650004 
 F1: 0.573178 
 Precision: 0.602935 
 Recall: 0.546220
 IoU: 0.401717

We have finished training iteration 105
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_103_.pth
per-ex loss: 0.508227  [    1/   88]
per-ex loss: 0.746467  [    2/   88]
per-ex loss: 0.551025  [    3/   88]
per-ex loss: 0.692249  [    4/   88]
per-ex loss: 0.711024  [    5/   88]
per-ex loss: 0.595363  [    6/   88]
per-ex loss: 0.566496  [    7/   88]
per-ex loss: 0.786923  [    8/   88]
per-ex loss: 0.780981  [    9/   88]
per-ex loss: 0.487515  [   10/   88]
per-ex loss: 0.481544  [   11/   88]
per-ex loss: 0.488313  [   12/   88]
per-ex loss: 0.478246  [   13/   88]
per-ex loss: 0.770157  [   14/   88]
per-ex loss: 0.504523  [   15/   88]
per-ex loss: 0.710004  [   16/   88]
per-ex loss: 0.691842  [   17/   88]
per-ex loss: 0.514690  [   18/   88]
per-ex loss: 0.657254  [   19/   88]
per-ex loss: 0.719089  [   20/   88]
per-ex loss: 0.511076  [   21/   88]
per-ex loss: 0.532181  [   22/   88]
per-ex loss: 0.480598  [   23/   88]
per-ex loss: 0.532300  [   24/   88]
per-ex loss: 0.744197  [   25/   88]
per-ex loss: 0.707434  [   26/   88]
per-ex loss: 0.491230  [   27/   88]
per-ex loss: 0.615854  [   28/   88]
per-ex loss: 0.610575  [   29/   88]
per-ex loss: 0.444283  [   30/   88]
per-ex loss: 0.688823  [   31/   88]
per-ex loss: 0.620528  [   32/   88]
per-ex loss: 0.747485  [   33/   88]
per-ex loss: 0.526756  [   34/   88]
per-ex loss: 0.554933  [   35/   88]
per-ex loss: 0.709288  [   36/   88]
per-ex loss: 0.584191  [   37/   88]
per-ex loss: 0.661157  [   38/   88]
per-ex loss: 0.447488  [   39/   88]
per-ex loss: 0.721060  [   40/   88]
per-ex loss: 0.716591  [   41/   88]
per-ex loss: 0.668788  [   42/   88]
per-ex loss: 0.680143  [   43/   88]
per-ex loss: 0.595348  [   44/   88]
per-ex loss: 0.590832  [   45/   88]
per-ex loss: 0.681970  [   46/   88]
per-ex loss: 0.498111  [   47/   88]
per-ex loss: 0.473934  [   48/   88]
per-ex loss: 0.604035  [   49/   88]
per-ex loss: 0.677857  [   50/   88]
per-ex loss: 0.521608  [   51/   88]
per-ex loss: 0.630485  [   52/   88]
per-ex loss: 0.622479  [   53/   88]
per-ex loss: 0.772635  [   54/   88]
per-ex loss: 0.477562  [   55/   88]
per-ex loss: 0.644759  [   56/   88]
per-ex loss: 0.653004  [   57/   88]
per-ex loss: 0.798509  [   58/   88]
per-ex loss: 0.538659  [   59/   88]
per-ex loss: 0.736963  [   60/   88]
per-ex loss: 0.444951  [   61/   88]
per-ex loss: 0.551649  [   62/   88]
per-ex loss: 0.534462  [   63/   88]
per-ex loss: 0.759874  [   64/   88]
per-ex loss: 0.397705  [   65/   88]
per-ex loss: 0.715639  [   66/   88]
per-ex loss: 0.708932  [   67/   88]
per-ex loss: 0.568780  [   68/   88]
per-ex loss: 0.479697  [   69/   88]
per-ex loss: 0.551971  [   70/   88]
per-ex loss: 0.480287  [   71/   88]
per-ex loss: 0.590141  [   72/   88]
per-ex loss: 0.581494  [   73/   88]
per-ex loss: 0.705108  [   74/   88]
per-ex loss: 0.525163  [   75/   88]
per-ex loss: 0.579095  [   76/   88]
per-ex loss: 0.496991  [   77/   88]
per-ex loss: 0.534670  [   78/   88]
per-ex loss: 0.733157  [   79/   88]
per-ex loss: 0.587137  [   80/   88]
per-ex loss: 0.635495  [   81/   88]
per-ex loss: 0.511426  [   82/   88]
per-ex loss: 0.550538  [   83/   88]
per-ex loss: 0.613122  [   84/   88]
per-ex loss: 0.615954  [   85/   88]
per-ex loss: 0.447019  [   86/   88]
per-ex loss: 0.553200  [   87/   88]
per-ex loss: 0.718781  [   88/   88]
Train Error: Avg loss: 0.60375089
validation Error: 
 Avg loss: 0.69157775 
 F1: 0.497153 
 Precision: 0.599723 
 Recall: 0.424543
 IoU: 0.330807

test Error: 
 Avg loss: 0.64634959 
 F1: 0.557428 
 Precision: 0.651649 
 Recall: 0.487011
 IoU: 0.386412

We have finished training iteration 106
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_104_.pth
per-ex loss: 0.749790  [    1/   88]
per-ex loss: 0.455881  [    2/   88]
per-ex loss: 0.613219  [    3/   88]
per-ex loss: 0.712985  [    4/   88]
per-ex loss: 0.747028  [    5/   88]
per-ex loss: 0.551679  [    6/   88]
per-ex loss: 0.614622  [    7/   88]
per-ex loss: 0.452386  [    8/   88]
per-ex loss: 0.559347  [    9/   88]
per-ex loss: 0.775276  [   10/   88]
per-ex loss: 0.541806  [   11/   88]
per-ex loss: 0.566589  [   12/   88]
per-ex loss: 0.692451  [   13/   88]
per-ex loss: 0.509425  [   14/   88]
per-ex loss: 0.627090  [   15/   88]
per-ex loss: 0.726179  [   16/   88]
per-ex loss: 0.600869  [   17/   88]
per-ex loss: 0.530866  [   18/   88]
per-ex loss: 0.607295  [   19/   88]
per-ex loss: 0.785997  [   20/   88]
per-ex loss: 0.540009  [   21/   88]
per-ex loss: 0.501668  [   22/   88]
per-ex loss: 0.713257  [   23/   88]
per-ex loss: 0.455022  [   24/   88]
per-ex loss: 0.680999  [   25/   88]
per-ex loss: 0.653455  [   26/   88]
per-ex loss: 0.545900  [   27/   88]
per-ex loss: 0.737224  [   28/   88]
per-ex loss: 0.478249  [   29/   88]
per-ex loss: 0.786432  [   30/   88]
per-ex loss: 0.532600  [   31/   88]
per-ex loss: 0.662609  [   32/   88]
per-ex loss: 0.620729  [   33/   88]
per-ex loss: 0.693533  [   34/   88]
per-ex loss: 0.556925  [   35/   88]
per-ex loss: 0.549437  [   36/   88]
per-ex loss: 0.559886  [   37/   88]
per-ex loss: 0.484261  [   38/   88]
per-ex loss: 0.691655  [   39/   88]
per-ex loss: 0.483898  [   40/   88]
per-ex loss: 0.585786  [   41/   88]
per-ex loss: 0.609727  [   42/   88]
per-ex loss: 0.489412  [   43/   88]
per-ex loss: 0.755315  [   44/   88]
per-ex loss: 0.568525  [   45/   88]
per-ex loss: 0.447596  [   46/   88]
per-ex loss: 0.697170  [   47/   88]
per-ex loss: 0.730423  [   48/   88]
per-ex loss: 0.683444  [   49/   88]
per-ex loss: 0.509615  [   50/   88]
per-ex loss: 0.544441  [   51/   88]
per-ex loss: 0.425951  [   52/   88]
per-ex loss: 0.683130  [   53/   88]
per-ex loss: 0.721794  [   54/   88]
per-ex loss: 0.695231  [   55/   88]
per-ex loss: 0.516432  [   56/   88]
per-ex loss: 0.461752  [   57/   88]
per-ex loss: 0.733362  [   58/   88]
per-ex loss: 0.705529  [   59/   88]
per-ex loss: 0.569910  [   60/   88]
per-ex loss: 0.732113  [   61/   88]
per-ex loss: 0.354563  [   62/   88]
per-ex loss: 0.801548  [   63/   88]
per-ex loss: 0.768929  [   64/   88]
per-ex loss: 0.552231  [   65/   88]
per-ex loss: 0.536725  [   66/   88]
per-ex loss: 0.708601  [   67/   88]
per-ex loss: 0.541418  [   68/   88]
per-ex loss: 0.514329  [   69/   88]
per-ex loss: 0.547648  [   70/   88]
per-ex loss: 0.480365  [   71/   88]
per-ex loss: 0.742268  [   72/   88]
per-ex loss: 0.561032  [   73/   88]
per-ex loss: 0.602126  [   74/   88]
per-ex loss: 0.737201  [   75/   88]
per-ex loss: 0.668785  [   76/   88]
per-ex loss: 0.514683  [   77/   88]
per-ex loss: 0.626089  [   78/   88]
per-ex loss: 0.646879  [   79/   88]
per-ex loss: 0.493602  [   80/   88]
per-ex loss: 0.539048  [   81/   88]
per-ex loss: 0.601791  [   82/   88]
per-ex loss: 0.564427  [   83/   88]
per-ex loss: 0.506608  [   84/   88]
per-ex loss: 0.537092  [   85/   88]
per-ex loss: 0.728875  [   86/   88]
per-ex loss: 0.500623  [   87/   88]
per-ex loss: 0.664587  [   88/   88]
Train Error: Avg loss: 0.60519612
validation Error: 
 Avg loss: 0.68641254 
 F1: 0.501932 
 Precision: 0.594953 
 Recall: 0.434065
 IoU: 0.335053

test Error: 
 Avg loss: 0.63987948 
 F1: 0.568067 
 Precision: 0.649139 
 Recall: 0.504997
 IoU: 0.396713

We have finished training iteration 107
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_105_.pth
per-ex loss: 0.727112  [    1/   88]
per-ex loss: 0.684126  [    2/   88]
per-ex loss: 0.785938  [    3/   88]
per-ex loss: 0.668897  [    4/   88]
per-ex loss: 0.733127  [    5/   88]
per-ex loss: 0.518222  [    6/   88]
per-ex loss: 0.561207  [    7/   88]
per-ex loss: 0.529042  [    8/   88]
per-ex loss: 0.475876  [    9/   88]
per-ex loss: 0.614485  [   10/   88]
per-ex loss: 0.567623  [   11/   88]
per-ex loss: 0.576400  [   12/   88]
per-ex loss: 0.627702  [   13/   88]
per-ex loss: 0.561087  [   14/   88]
per-ex loss: 0.685188  [   15/   88]
per-ex loss: 0.560003  [   16/   88]
per-ex loss: 0.489987  [   17/   88]
per-ex loss: 0.690015  [   18/   88]
per-ex loss: 0.774152  [   19/   88]
per-ex loss: 0.713935  [   20/   88]
per-ex loss: 0.496621  [   21/   88]
per-ex loss: 0.584570  [   22/   88]
per-ex loss: 0.729744  [   23/   88]
per-ex loss: 0.698032  [   24/   88]
per-ex loss: 0.544967  [   25/   88]
per-ex loss: 0.726353  [   26/   88]
per-ex loss: 0.684677  [   27/   88]
per-ex loss: 0.504093  [   28/   88]
per-ex loss: 0.676465  [   29/   88]
per-ex loss: 0.556298  [   30/   88]
per-ex loss: 0.496372  [   31/   88]
per-ex loss: 0.724493  [   32/   88]
per-ex loss: 0.499140  [   33/   88]
per-ex loss: 0.578001  [   34/   88]
per-ex loss: 0.549485  [   35/   88]
per-ex loss: 0.605580  [   36/   88]
per-ex loss: 0.715304  [   37/   88]
per-ex loss: 0.552962  [   38/   88]
per-ex loss: 0.487982  [   39/   88]
per-ex loss: 0.519041  [   40/   88]
per-ex loss: 0.647065  [   41/   88]
per-ex loss: 0.448674  [   42/   88]
per-ex loss: 0.576876  [   43/   88]
per-ex loss: 0.483295  [   44/   88]
per-ex loss: 0.717775  [   45/   88]
per-ex loss: 0.533563  [   46/   88]
per-ex loss: 0.780121  [   47/   88]
per-ex loss: 0.594864  [   48/   88]
per-ex loss: 0.769185  [   49/   88]
per-ex loss: 0.757277  [   50/   88]
per-ex loss: 0.550973  [   51/   88]
per-ex loss: 0.508891  [   52/   88]
per-ex loss: 0.471704  [   53/   88]
per-ex loss: 0.479029  [   54/   88]
per-ex loss: 0.546400  [   55/   88]
per-ex loss: 0.421912  [   56/   88]
per-ex loss: 0.529470  [   57/   88]
per-ex loss: 0.555220  [   58/   88]
per-ex loss: 0.762696  [   59/   88]
per-ex loss: 0.635030  [   60/   88]
per-ex loss: 0.724353  [   61/   88]
per-ex loss: 0.473780  [   62/   88]
per-ex loss: 0.526694  [   63/   88]
per-ex loss: 0.652431  [   64/   88]
per-ex loss: 0.445454  [   65/   88]
per-ex loss: 0.605257  [   66/   88]
per-ex loss: 0.591966  [   67/   88]
per-ex loss: 0.744178  [   68/   88]
per-ex loss: 0.719545  [   69/   88]
per-ex loss: 0.624689  [   70/   88]
per-ex loss: 0.656718  [   71/   88]
per-ex loss: 0.617723  [   72/   88]
per-ex loss: 0.681306  [   73/   88]
per-ex loss: 0.630959  [   74/   88]
per-ex loss: 0.617102  [   75/   88]
per-ex loss: 0.491238  [   76/   88]
per-ex loss: 0.456800  [   77/   88]
per-ex loss: 0.491713  [   78/   88]
per-ex loss: 0.712055  [   79/   88]
per-ex loss: 0.349727  [   80/   88]
per-ex loss: 0.719356  [   81/   88]
per-ex loss: 0.704731  [   82/   88]
per-ex loss: 0.544028  [   83/   88]
per-ex loss: 0.521531  [   84/   88]
per-ex loss: 0.615870  [   85/   88]
per-ex loss: 0.788877  [   86/   88]
per-ex loss: 0.488892  [   87/   88]
per-ex loss: 0.632834  [   88/   88]
Train Error: Avg loss: 0.60311514
validation Error: 
 Avg loss: 0.68602320 
 F1: 0.503615 
 Precision: 0.523487 
 Recall: 0.485197
 IoU: 0.336555

test Error: 
 Avg loss: 0.63865116 
 F1: 0.568839 
 Precision: 0.579145 
 Recall: 0.558893
 IoU: 0.397466

We have finished training iteration 108
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_106_.pth
per-ex loss: 0.693929  [    1/   88]
per-ex loss: 0.453308  [    2/   88]
per-ex loss: 0.498816  [    3/   88]
per-ex loss: 0.567463  [    4/   88]
per-ex loss: 0.565360  [    5/   88]
per-ex loss: 0.589977  [    6/   88]
per-ex loss: 0.455555  [    7/   88]
per-ex loss: 0.472897  [    8/   88]
per-ex loss: 0.621347  [    9/   88]
per-ex loss: 0.594485  [   10/   88]
per-ex loss: 0.519356  [   11/   88]
per-ex loss: 0.523380  [   12/   88]
per-ex loss: 0.624571  [   13/   88]
per-ex loss: 0.748603  [   14/   88]
per-ex loss: 0.559471  [   15/   88]
per-ex loss: 0.544671  [   16/   88]
per-ex loss: 0.441337  [   17/   88]
per-ex loss: 0.743677  [   18/   88]
per-ex loss: 0.526532  [   19/   88]
per-ex loss: 0.573779  [   20/   88]
per-ex loss: 0.688026  [   21/   88]
per-ex loss: 0.712540  [   22/   88]
per-ex loss: 0.530617  [   23/   88]
per-ex loss: 0.655558  [   24/   88]
per-ex loss: 0.582172  [   25/   88]
per-ex loss: 0.741808  [   26/   88]
per-ex loss: 0.560992  [   27/   88]
per-ex loss: 0.448547  [   28/   88]
per-ex loss: 0.720440  [   29/   88]
per-ex loss: 0.741842  [   30/   88]
per-ex loss: 0.734032  [   31/   88]
per-ex loss: 0.671460  [   32/   88]
per-ex loss: 0.595051  [   33/   88]
per-ex loss: 0.502190  [   34/   88]
per-ex loss: 0.525613  [   35/   88]
per-ex loss: 0.614072  [   36/   88]
per-ex loss: 0.743911  [   37/   88]
per-ex loss: 0.789030  [   38/   88]
per-ex loss: 0.773709  [   39/   88]
per-ex loss: 0.481313  [   40/   88]
per-ex loss: 0.561392  [   41/   88]
per-ex loss: 0.516091  [   42/   88]
per-ex loss: 0.652270  [   43/   88]
per-ex loss: 0.631003  [   44/   88]
per-ex loss: 0.524531  [   45/   88]
per-ex loss: 0.531275  [   46/   88]
per-ex loss: 0.594964  [   47/   88]
per-ex loss: 0.483930  [   48/   88]
per-ex loss: 0.667292  [   49/   88]
per-ex loss: 0.489281  [   50/   88]
per-ex loss: 0.640975  [   51/   88]
per-ex loss: 0.720992  [   52/   88]
per-ex loss: 0.534451  [   53/   88]
per-ex loss: 0.579154  [   54/   88]
per-ex loss: 0.714123  [   55/   88]
per-ex loss: 0.557825  [   56/   88]
per-ex loss: 0.790567  [   57/   88]
per-ex loss: 0.613342  [   58/   88]
per-ex loss: 0.459992  [   59/   88]
per-ex loss: 0.501195  [   60/   88]
per-ex loss: 0.647326  [   61/   88]
per-ex loss: 0.530793  [   62/   88]
per-ex loss: 0.636527  [   63/   88]
per-ex loss: 0.562405  [   64/   88]
per-ex loss: 0.435887  [   65/   88]
per-ex loss: 0.726443  [   66/   88]
per-ex loss: 0.651260  [   67/   88]
per-ex loss: 0.750183  [   68/   88]
per-ex loss: 0.475708  [   69/   88]
per-ex loss: 0.699225  [   70/   88]
per-ex loss: 0.765915  [   71/   88]
per-ex loss: 0.692257  [   72/   88]
per-ex loss: 0.533286  [   73/   88]
per-ex loss: 0.667878  [   74/   88]
per-ex loss: 0.515370  [   75/   88]
per-ex loss: 0.775617  [   76/   88]
per-ex loss: 0.483701  [   77/   88]
per-ex loss: 0.474762  [   78/   88]
per-ex loss: 0.523599  [   79/   88]
per-ex loss: 0.716891  [   80/   88]
per-ex loss: 0.543283  [   81/   88]
per-ex loss: 0.689849  [   82/   88]
per-ex loss: 0.589525  [   83/   88]
per-ex loss: 0.680430  [   84/   88]
per-ex loss: 0.675534  [   85/   88]
per-ex loss: 0.716089  [   86/   88]
per-ex loss: 0.726967  [   87/   88]
per-ex loss: 0.554041  [   88/   88]
Train Error: Avg loss: 0.60610036
validation Error: 
 Avg loss: 0.68321652 
 F1: 0.507694 
 Precision: 0.547872 
 Recall: 0.473007
 IoU: 0.340208

test Error: 
 Avg loss: 0.63829406 
 F1: 0.566559 
 Precision: 0.588796 
 Recall: 0.545941
 IoU: 0.395244

We have finished training iteration 109
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_107_.pth
per-ex loss: 0.546862  [    1/   88]
per-ex loss: 0.489186  [    2/   88]
per-ex loss: 0.440254  [    3/   88]
per-ex loss: 0.630599  [    4/   88]
per-ex loss: 0.497183  [    5/   88]
per-ex loss: 0.703921  [    6/   88]
per-ex loss: 0.559728  [    7/   88]
per-ex loss: 0.693444  [    8/   88]
per-ex loss: 0.716129  [    9/   88]
per-ex loss: 0.755558  [   10/   88]
per-ex loss: 0.549099  [   11/   88]
per-ex loss: 0.692494  [   12/   88]
per-ex loss: 0.772579  [   13/   88]
per-ex loss: 0.543604  [   14/   88]
per-ex loss: 0.446179  [   15/   88]
per-ex loss: 0.740529  [   16/   88]
per-ex loss: 0.510537  [   17/   88]
per-ex loss: 0.779500  [   18/   88]
per-ex loss: 0.547003  [   19/   88]
per-ex loss: 0.747078  [   20/   88]
per-ex loss: 0.654486  [   21/   88]
per-ex loss: 0.539064  [   22/   88]
per-ex loss: 0.544461  [   23/   88]
per-ex loss: 0.498938  [   24/   88]
per-ex loss: 0.663471  [   25/   88]
per-ex loss: 0.649509  [   26/   88]
per-ex loss: 0.476815  [   27/   88]
per-ex loss: 0.516624  [   28/   88]
per-ex loss: 0.549585  [   29/   88]
per-ex loss: 0.618126  [   30/   88]
per-ex loss: 0.479576  [   31/   88]
per-ex loss: 0.520942  [   32/   88]
per-ex loss: 0.730906  [   33/   88]
per-ex loss: 0.736011  [   34/   88]
per-ex loss: 0.784736  [   35/   88]
per-ex loss: 0.689551  [   36/   88]
per-ex loss: 0.733367  [   37/   88]
per-ex loss: 0.606538  [   38/   88]
per-ex loss: 0.564359  [   39/   88]
per-ex loss: 0.719397  [   40/   88]
per-ex loss: 0.693383  [   41/   88]
per-ex loss: 0.731351  [   42/   88]
per-ex loss: 0.584300  [   43/   88]
per-ex loss: 0.542271  [   44/   88]
per-ex loss: 0.613467  [   45/   88]
per-ex loss: 0.476271  [   46/   88]
per-ex loss: 0.713981  [   47/   88]
per-ex loss: 0.485008  [   48/   88]
per-ex loss: 0.845959  [   49/   88]
per-ex loss: 0.731811  [   50/   88]
per-ex loss: 0.578027  [   51/   88]
per-ex loss: 0.596808  [   52/   88]
per-ex loss: 0.539643  [   53/   88]
per-ex loss: 0.534757  [   54/   88]
per-ex loss: 0.441044  [   55/   88]
per-ex loss: 0.470097  [   56/   88]
per-ex loss: 0.540226  [   57/   88]
per-ex loss: 0.442998  [   58/   88]
per-ex loss: 0.568702  [   59/   88]
per-ex loss: 0.610416  [   60/   88]
per-ex loss: 0.547750  [   61/   88]
per-ex loss: 0.595532  [   62/   88]
per-ex loss: 0.554393  [   63/   88]
per-ex loss: 0.622155  [   64/   88]
per-ex loss: 0.654624  [   65/   88]
per-ex loss: 0.366390  [   66/   88]
per-ex loss: 0.689687  [   67/   88]
per-ex loss: 0.673212  [   68/   88]
per-ex loss: 0.715774  [   69/   88]
per-ex loss: 0.689301  [   70/   88]
per-ex loss: 0.516536  [   71/   88]
per-ex loss: 0.609501  [   72/   88]
per-ex loss: 0.517223  [   73/   88]
per-ex loss: 0.733996  [   74/   88]
per-ex loss: 0.543998  [   75/   88]
per-ex loss: 0.564861  [   76/   88]
per-ex loss: 0.706555  [   77/   88]
per-ex loss: 0.567956  [   78/   88]
per-ex loss: 0.620305  [   79/   88]
per-ex loss: 0.749903  [   80/   88]
per-ex loss: 0.502565  [   81/   88]
per-ex loss: 0.503144  [   82/   88]
per-ex loss: 0.777370  [   83/   88]
per-ex loss: 0.769775  [   84/   88]
per-ex loss: 0.638495  [   85/   88]
per-ex loss: 0.559936  [   86/   88]
per-ex loss: 0.530169  [   87/   88]
per-ex loss: 0.470069  [   88/   88]
Train Error: Avg loss: 0.60647299
validation Error: 
 Avg loss: 0.67908306 
 F1: 0.508131 
 Precision: 0.599638 
 Recall: 0.440855
 IoU: 0.340600

test Error: 
 Avg loss: 0.63864452 
 F1: 0.567527 
 Precision: 0.646371 
 Recall: 0.505826
 IoU: 0.396187

We have finished training iteration 110
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_108_.pth
per-ex loss: 0.569833  [    1/   88]
per-ex loss: 0.434215  [    2/   88]
per-ex loss: 0.562158  [    3/   88]
per-ex loss: 0.468082  [    4/   88]
per-ex loss: 0.758899  [    5/   88]
per-ex loss: 0.716148  [    6/   88]
per-ex loss: 0.522198  [    7/   88]
per-ex loss: 0.628795  [    8/   88]
per-ex loss: 0.704327  [    9/   88]
per-ex loss: 0.483317  [   10/   88]
per-ex loss: 0.580979  [   11/   88]
per-ex loss: 0.583908  [   12/   88]
per-ex loss: 0.688052  [   13/   88]
per-ex loss: 0.651240  [   14/   88]
per-ex loss: 0.614693  [   15/   88]
per-ex loss: 0.715827  [   16/   88]
per-ex loss: 0.582739  [   17/   88]
per-ex loss: 0.686342  [   18/   88]
per-ex loss: 0.697933  [   19/   88]
per-ex loss: 0.512876  [   20/   88]
per-ex loss: 0.548311  [   21/   88]
per-ex loss: 0.720787  [   22/   88]
per-ex loss: 0.618787  [   23/   88]
per-ex loss: 0.573613  [   24/   88]
per-ex loss: 0.606298  [   25/   88]
per-ex loss: 0.539063  [   26/   88]
per-ex loss: 0.580544  [   27/   88]
per-ex loss: 0.611237  [   28/   88]
per-ex loss: 0.669124  [   29/   88]
per-ex loss: 0.612484  [   30/   88]
per-ex loss: 0.791262  [   31/   88]
per-ex loss: 0.605221  [   32/   88]
per-ex loss: 0.486930  [   33/   88]
per-ex loss: 0.723188  [   34/   88]
per-ex loss: 0.717327  [   35/   88]
per-ex loss: 0.376929  [   36/   88]
per-ex loss: 0.472794  [   37/   88]
per-ex loss: 0.693268  [   38/   88]
per-ex loss: 0.517842  [   39/   88]
per-ex loss: 0.508699  [   40/   88]
per-ex loss: 0.545834  [   41/   88]
per-ex loss: 0.784027  [   42/   88]
per-ex loss: 0.521901  [   43/   88]
per-ex loss: 0.532567  [   44/   88]
per-ex loss: 0.554066  [   45/   88]
per-ex loss: 0.572744  [   46/   88]
per-ex loss: 0.772072  [   47/   88]
per-ex loss: 0.642303  [   48/   88]
per-ex loss: 0.524491  [   49/   88]
per-ex loss: 0.560004  [   50/   88]
per-ex loss: 0.669170  [   51/   88]
per-ex loss: 0.767237  [   52/   88]
per-ex loss: 0.457980  [   53/   88]
per-ex loss: 0.527045  [   54/   88]
per-ex loss: 0.716814  [   55/   88]
per-ex loss: 0.621098  [   56/   88]
per-ex loss: 0.484953  [   57/   88]
per-ex loss: 0.553569  [   58/   88]
per-ex loss: 0.537983  [   59/   88]
per-ex loss: 0.722016  [   60/   88]
per-ex loss: 0.690526  [   61/   88]
per-ex loss: 0.484753  [   62/   88]
per-ex loss: 0.478676  [   63/   88]
per-ex loss: 0.476883  [   64/   88]
per-ex loss: 0.495365  [   65/   88]
per-ex loss: 0.636102  [   66/   88]
per-ex loss: 0.756817  [   67/   88]
per-ex loss: 0.542250  [   68/   88]
per-ex loss: 0.599908  [   69/   88]
per-ex loss: 0.687850  [   70/   88]
per-ex loss: 0.437899  [   71/   88]
per-ex loss: 0.612144  [   72/   88]
per-ex loss: 0.549519  [   73/   88]
per-ex loss: 0.759133  [   74/   88]
per-ex loss: 0.490955  [   75/   88]
per-ex loss: 0.550100  [   76/   88]
per-ex loss: 0.699666  [   77/   88]
per-ex loss: 0.718424  [   78/   88]
per-ex loss: 0.499794  [   79/   88]
per-ex loss: 0.778817  [   80/   88]
per-ex loss: 0.500798  [   81/   88]
per-ex loss: 0.673804  [   82/   88]
per-ex loss: 0.448975  [   83/   88]
per-ex loss: 0.662140  [   84/   88]
per-ex loss: 0.731002  [   85/   88]
per-ex loss: 0.729171  [   86/   88]
per-ex loss: 0.526874  [   87/   88]
per-ex loss: 0.753976  [   88/   88]
Train Error: Avg loss: 0.60427806
validation Error: 
 Avg loss: 0.68331408 
 F1: 0.500046 
 Precision: 0.536205 
 Recall: 0.468456
 IoU: 0.333374

test Error: 
 Avg loss: 0.64197015 
 F1: 0.561841 
 Precision: 0.577787 
 Recall: 0.546753
 IoU: 0.390667

We have finished training iteration 111
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_109_.pth
per-ex loss: 0.509661  [    1/   88]
per-ex loss: 0.677494  [    2/   88]
per-ex loss: 0.532561  [    3/   88]
per-ex loss: 0.525302  [    4/   88]
per-ex loss: 0.520670  [    5/   88]
per-ex loss: 0.747518  [    6/   88]
per-ex loss: 0.564486  [    7/   88]
per-ex loss: 0.541072  [    8/   88]
per-ex loss: 0.687669  [    9/   88]
per-ex loss: 0.520722  [   10/   88]
per-ex loss: 0.611489  [   11/   88]
per-ex loss: 0.792417  [   12/   88]
per-ex loss: 0.707329  [   13/   88]
per-ex loss: 0.706521  [   14/   88]
per-ex loss: 0.711370  [   15/   88]
per-ex loss: 0.739016  [   16/   88]
per-ex loss: 0.792871  [   17/   88]
per-ex loss: 0.778260  [   18/   88]
per-ex loss: 0.663127  [   19/   88]
per-ex loss: 0.583960  [   20/   88]
per-ex loss: 0.584083  [   21/   88]
per-ex loss: 0.627797  [   22/   88]
per-ex loss: 0.545364  [   23/   88]
per-ex loss: 0.501198  [   24/   88]
per-ex loss: 0.767922  [   25/   88]
per-ex loss: 0.555744  [   26/   88]
per-ex loss: 0.523104  [   27/   88]
per-ex loss: 0.486837  [   28/   88]
per-ex loss: 0.499349  [   29/   88]
per-ex loss: 0.469167  [   30/   88]
per-ex loss: 0.578688  [   31/   88]
per-ex loss: 0.734057  [   32/   88]
per-ex loss: 0.468707  [   33/   88]
per-ex loss: 0.525409  [   34/   88]
per-ex loss: 0.500433  [   35/   88]
per-ex loss: 0.636003  [   36/   88]
per-ex loss: 0.835714  [   37/   88]
per-ex loss: 0.577884  [   38/   88]
per-ex loss: 0.526599  [   39/   88]
per-ex loss: 0.548818  [   40/   88]
per-ex loss: 0.558874  [   41/   88]
per-ex loss: 0.639304  [   42/   88]
per-ex loss: 0.703566  [   43/   88]
per-ex loss: 0.537399  [   44/   88]
per-ex loss: 0.679083  [   45/   88]
per-ex loss: 0.718041  [   46/   88]
per-ex loss: 0.501434  [   47/   88]
per-ex loss: 0.492710  [   48/   88]
per-ex loss: 0.452352  [   49/   88]
per-ex loss: 0.525025  [   50/   88]
per-ex loss: 0.539469  [   51/   88]
per-ex loss: 0.681062  [   52/   88]
per-ex loss: 0.734613  [   53/   88]
per-ex loss: 0.572451  [   54/   88]
per-ex loss: 0.771709  [   55/   88]
per-ex loss: 0.723680  [   56/   88]
per-ex loss: 0.525035  [   57/   88]
per-ex loss: 0.744110  [   58/   88]
per-ex loss: 0.724216  [   59/   88]
per-ex loss: 0.658999  [   60/   88]
per-ex loss: 0.431824  [   61/   88]
per-ex loss: 0.609462  [   62/   88]
per-ex loss: 0.347577  [   63/   88]
per-ex loss: 0.549142  [   64/   88]
per-ex loss: 0.656901  [   65/   88]
per-ex loss: 0.657813  [   66/   88]
per-ex loss: 0.504686  [   67/   88]
per-ex loss: 0.729085  [   68/   88]
per-ex loss: 0.627829  [   69/   88]
per-ex loss: 0.746486  [   70/   88]
per-ex loss: 0.491695  [   71/   88]
per-ex loss: 0.685678  [   72/   88]
per-ex loss: 0.709298  [   73/   88]
per-ex loss: 0.586647  [   74/   88]
per-ex loss: 0.510307  [   75/   88]
per-ex loss: 0.704679  [   76/   88]
per-ex loss: 0.814078  [   77/   88]
per-ex loss: 0.602287  [   78/   88]
per-ex loss: 0.566849  [   79/   88]
per-ex loss: 0.699647  [   80/   88]
per-ex loss: 0.546282  [   81/   88]
per-ex loss: 0.567306  [   82/   88]
per-ex loss: 0.513065  [   83/   88]
per-ex loss: 0.500969  [   84/   88]
per-ex loss: 0.618493  [   85/   88]
per-ex loss: 0.472081  [   86/   88]
per-ex loss: 0.490885  [   87/   88]
per-ex loss: 0.514877  [   88/   88]
Train Error: Avg loss: 0.60619829
validation Error: 
 Avg loss: 0.70055083 
 F1: 0.479142 
 Precision: 0.641932 
 Recall: 0.382215
 IoU: 0.315048

test Error: 
 Avg loss: 0.66386580 
 F1: 0.535961 
 Precision: 0.675734 
 Recall: 0.444100
 IoU: 0.366083

We have finished training iteration 112
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_78_.pth
per-ex loss: 0.515388  [    1/   88]
per-ex loss: 0.617112  [    2/   88]
per-ex loss: 0.547752  [    3/   88]
per-ex loss: 0.652661  [    4/   88]
per-ex loss: 0.527844  [    5/   88]
per-ex loss: 0.472488  [    6/   88]
per-ex loss: 0.576683  [    7/   88]
per-ex loss: 0.728791  [    8/   88]
per-ex loss: 0.475215  [    9/   88]
per-ex loss: 0.490695  [   10/   88]
per-ex loss: 0.569143  [   11/   88]
per-ex loss: 0.686992  [   12/   88]
per-ex loss: 0.514782  [   13/   88]
per-ex loss: 0.532044  [   14/   88]
per-ex loss: 0.681076  [   15/   88]
per-ex loss: 0.557897  [   16/   88]
per-ex loss: 0.467715  [   17/   88]
per-ex loss: 0.685283  [   18/   88]
per-ex loss: 0.645621  [   19/   88]
per-ex loss: 0.700408  [   20/   88]
per-ex loss: 0.471806  [   21/   88]
per-ex loss: 0.569376  [   22/   88]
per-ex loss: 0.685229  [   23/   88]
per-ex loss: 0.526947  [   24/   88]
per-ex loss: 0.728934  [   25/   88]
per-ex loss: 0.617802  [   26/   88]
per-ex loss: 0.578660  [   27/   88]
per-ex loss: 0.592156  [   28/   88]
per-ex loss: 0.545804  [   29/   88]
per-ex loss: 0.770916  [   30/   88]
per-ex loss: 0.679163  [   31/   88]
per-ex loss: 0.712683  [   32/   88]
per-ex loss: 0.750287  [   33/   88]
per-ex loss: 0.793848  [   34/   88]
per-ex loss: 0.521542  [   35/   88]
per-ex loss: 0.500287  [   36/   88]
per-ex loss: 0.471972  [   37/   88]
per-ex loss: 0.562550  [   38/   88]
per-ex loss: 0.572510  [   39/   88]
per-ex loss: 0.758875  [   40/   88]
per-ex loss: 0.726140  [   41/   88]
per-ex loss: 0.440917  [   42/   88]
per-ex loss: 0.686029  [   43/   88]
per-ex loss: 0.599956  [   44/   88]
per-ex loss: 0.687916  [   45/   88]
per-ex loss: 0.496580  [   46/   88]
per-ex loss: 0.776855  [   47/   88]
per-ex loss: 0.523640  [   48/   88]
per-ex loss: 0.688908  [   49/   88]
per-ex loss: 0.608841  [   50/   88]
per-ex loss: 0.544314  [   51/   88]
per-ex loss: 0.388416  [   52/   88]
per-ex loss: 0.693974  [   53/   88]
per-ex loss: 0.500619  [   54/   88]
per-ex loss: 0.782866  [   55/   88]
per-ex loss: 0.608950  [   56/   88]
per-ex loss: 0.529559  [   57/   88]
per-ex loss: 0.533830  [   58/   88]
per-ex loss: 0.628991  [   59/   88]
per-ex loss: 0.631013  [   60/   88]
per-ex loss: 0.725215  [   61/   88]
per-ex loss: 0.533093  [   62/   88]
per-ex loss: 0.730339  [   63/   88]
per-ex loss: 0.533773  [   64/   88]
per-ex loss: 0.738248  [   65/   88]
per-ex loss: 0.569166  [   66/   88]
per-ex loss: 0.721257  [   67/   88]
per-ex loss: 0.589223  [   68/   88]
per-ex loss: 0.547262  [   69/   88]
per-ex loss: 0.728418  [   70/   88]
per-ex loss: 0.612249  [   71/   88]
per-ex loss: 0.479036  [   72/   88]
per-ex loss: 0.457879  [   73/   88]
per-ex loss: 0.546550  [   74/   88]
per-ex loss: 0.512433  [   75/   88]
per-ex loss: 0.438872  [   76/   88]
per-ex loss: 0.593382  [   77/   88]
per-ex loss: 0.493494  [   78/   88]
per-ex loss: 0.661225  [   79/   88]
per-ex loss: 0.717821  [   80/   88]
per-ex loss: 0.574515  [   81/   88]
per-ex loss: 0.449481  [   82/   88]
per-ex loss: 0.486240  [   83/   88]
per-ex loss: 0.537628  [   84/   88]
per-ex loss: 0.638137  [   85/   88]
per-ex loss: 0.504322  [   86/   88]
per-ex loss: 0.721386  [   87/   88]
per-ex loss: 0.772910  [   88/   88]
Train Error: Avg loss: 0.59941829
validation Error: 
 Avg loss: 0.68604949 
 F1: 0.504215 
 Precision: 0.531531 
 Recall: 0.479570
 IoU: 0.337091

test Error: 
 Avg loss: 0.64297250 
 F1: 0.564631 
 Precision: 0.589913 
 Recall: 0.541427
 IoU: 0.393370

We have finished training iteration 113
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_111_.pth
per-ex loss: 0.673791  [    1/   88]
per-ex loss: 0.673182  [    2/   88]
per-ex loss: 0.460222  [    3/   88]
per-ex loss: 0.496786  [    4/   88]
per-ex loss: 0.577054  [    5/   88]
per-ex loss: 0.495947  [    6/   88]
per-ex loss: 0.607623  [    7/   88]
per-ex loss: 0.544781  [    8/   88]
per-ex loss: 0.734561  [    9/   88]
per-ex loss: 0.486045  [   10/   88]
per-ex loss: 0.511056  [   11/   88]
per-ex loss: 0.718806  [   12/   88]
per-ex loss: 0.580985  [   13/   88]
per-ex loss: 0.502616  [   14/   88]
per-ex loss: 0.426104  [   15/   88]
per-ex loss: 0.562665  [   16/   88]
per-ex loss: 0.673615  [   17/   88]
per-ex loss: 0.599024  [   18/   88]
per-ex loss: 0.470626  [   19/   88]
per-ex loss: 0.679335  [   20/   88]
per-ex loss: 0.495038  [   21/   88]
per-ex loss: 0.574272  [   22/   88]
per-ex loss: 0.539208  [   23/   88]
per-ex loss: 0.589287  [   24/   88]
per-ex loss: 0.460295  [   25/   88]
per-ex loss: 0.774052  [   26/   88]
per-ex loss: 0.541149  [   27/   88]
per-ex loss: 0.548496  [   28/   88]
per-ex loss: 0.744727  [   29/   88]
per-ex loss: 0.433132  [   30/   88]
per-ex loss: 0.651877  [   31/   88]
per-ex loss: 0.442238  [   32/   88]
per-ex loss: 0.766590  [   33/   88]
per-ex loss: 0.614631  [   34/   88]
per-ex loss: 0.529395  [   35/   88]
per-ex loss: 0.614207  [   36/   88]
per-ex loss: 0.502931  [   37/   88]
per-ex loss: 0.538849  [   38/   88]
per-ex loss: 0.610553  [   39/   88]
per-ex loss: 0.787467  [   40/   88]
per-ex loss: 0.555524  [   41/   88]
per-ex loss: 0.682225  [   42/   88]
per-ex loss: 0.664345  [   43/   88]
per-ex loss: 0.535726  [   44/   88]
per-ex loss: 0.737528  [   45/   88]
per-ex loss: 0.630565  [   46/   88]
per-ex loss: 0.519682  [   47/   88]
per-ex loss: 0.481123  [   48/   88]
per-ex loss: 0.718936  [   49/   88]
per-ex loss: 0.619139  [   50/   88]
per-ex loss: 0.699649  [   51/   88]
per-ex loss: 0.664919  [   52/   88]
per-ex loss: 0.749551  [   53/   88]
per-ex loss: 0.607885  [   54/   88]
per-ex loss: 0.697413  [   55/   88]
per-ex loss: 0.527397  [   56/   88]
per-ex loss: 0.711166  [   57/   88]
per-ex loss: 0.495887  [   58/   88]
per-ex loss: 0.556887  [   59/   88]
per-ex loss: 0.615736  [   60/   88]
per-ex loss: 0.694139  [   61/   88]
per-ex loss: 0.486195  [   62/   88]
per-ex loss: 0.689250  [   63/   88]
per-ex loss: 0.765673  [   64/   88]
per-ex loss: 0.480923  [   65/   88]
per-ex loss: 0.753180  [   66/   88]
per-ex loss: 0.354462  [   67/   88]
per-ex loss: 0.648072  [   68/   88]
per-ex loss: 0.566137  [   69/   88]
per-ex loss: 0.788055  [   70/   88]
per-ex loss: 0.521975  [   71/   88]
per-ex loss: 0.723181  [   72/   88]
per-ex loss: 0.537484  [   73/   88]
per-ex loss: 0.480219  [   74/   88]
per-ex loss: 0.540163  [   75/   88]
per-ex loss: 0.519344  [   76/   88]
per-ex loss: 0.671309  [   77/   88]
per-ex loss: 0.726007  [   78/   88]
per-ex loss: 0.444633  [   79/   88]
per-ex loss: 0.542829  [   80/   88]
per-ex loss: 0.718240  [   81/   88]
per-ex loss: 0.573279  [   82/   88]
per-ex loss: 0.706319  [   83/   88]
per-ex loss: 0.764600  [   84/   88]
per-ex loss: 0.613902  [   85/   88]
per-ex loss: 0.521168  [   86/   88]
per-ex loss: 0.552824  [   87/   88]
per-ex loss: 0.472355  [   88/   88]
Train Error: Avg loss: 0.59727742
validation Error: 
 Avg loss: 0.68695689 
 F1: 0.500995 
 Precision: 0.590800 
 Recall: 0.434890
 IoU: 0.334219

test Error: 
 Avg loss: 0.64098532 
 F1: 0.566068 
 Precision: 0.632368 
 Recall: 0.512351
 IoU: 0.394766

We have finished training iteration 114
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_112_.pth
per-ex loss: 0.541763  [    1/   88]
per-ex loss: 0.700922  [    2/   88]
per-ex loss: 0.564721  [    3/   88]
per-ex loss: 0.535771  [    4/   88]
per-ex loss: 0.525936  [    5/   88]
per-ex loss: 0.725500  [    6/   88]
per-ex loss: 0.503188  [    7/   88]
per-ex loss: 0.605458  [    8/   88]
per-ex loss: 0.806352  [    9/   88]
per-ex loss: 0.458820  [   10/   88]
per-ex loss: 0.506536  [   11/   88]
per-ex loss: 0.578940  [   12/   88]
per-ex loss: 0.694195  [   13/   88]
per-ex loss: 0.684152  [   14/   88]
per-ex loss: 0.676378  [   15/   88]
per-ex loss: 0.526699  [   16/   88]
per-ex loss: 0.447593  [   17/   88]
per-ex loss: 0.616469  [   18/   88]
per-ex loss: 0.546791  [   19/   88]
per-ex loss: 0.610726  [   20/   88]
per-ex loss: 0.690520  [   21/   88]
per-ex loss: 0.531178  [   22/   88]
per-ex loss: 0.683579  [   23/   88]
per-ex loss: 0.782317  [   24/   88]
per-ex loss: 0.725157  [   25/   88]
per-ex loss: 0.492912  [   26/   88]
per-ex loss: 0.528916  [   27/   88]
per-ex loss: 0.695332  [   28/   88]
per-ex loss: 0.464588  [   29/   88]
per-ex loss: 0.761047  [   30/   88]
per-ex loss: 0.547959  [   31/   88]
per-ex loss: 0.716827  [   32/   88]
per-ex loss: 0.471866  [   33/   88]
per-ex loss: 0.583559  [   34/   88]
per-ex loss: 0.591408  [   35/   88]
per-ex loss: 0.471373  [   36/   88]
per-ex loss: 0.560209  [   37/   88]
per-ex loss: 0.478447  [   38/   88]
per-ex loss: 0.732725  [   39/   88]
per-ex loss: 0.669630  [   40/   88]
per-ex loss: 0.767290  [   41/   88]
per-ex loss: 0.718782  [   42/   88]
per-ex loss: 0.774652  [   43/   88]
per-ex loss: 0.591154  [   44/   88]
per-ex loss: 0.525200  [   45/   88]
per-ex loss: 0.753749  [   46/   88]
per-ex loss: 0.513515  [   47/   88]
per-ex loss: 0.483009  [   48/   88]
per-ex loss: 0.666644  [   49/   88]
per-ex loss: 0.608736  [   50/   88]
per-ex loss: 0.690776  [   51/   88]
per-ex loss: 0.475827  [   52/   88]
per-ex loss: 0.518397  [   53/   88]
per-ex loss: 0.580481  [   54/   88]
per-ex loss: 0.479705  [   55/   88]
per-ex loss: 0.724940  [   56/   88]
per-ex loss: 0.505906  [   57/   88]
per-ex loss: 0.621236  [   58/   88]
per-ex loss: 0.516859  [   59/   88]
per-ex loss: 0.633198  [   60/   88]
per-ex loss: 0.545371  [   61/   88]
per-ex loss: 0.528796  [   62/   88]
per-ex loss: 0.504581  [   63/   88]
per-ex loss: 0.732062  [   64/   88]
per-ex loss: 0.478308  [   65/   88]
per-ex loss: 0.352431  [   66/   88]
per-ex loss: 0.435815  [   67/   88]
per-ex loss: 0.590134  [   68/   88]
per-ex loss: 0.651732  [   69/   88]
per-ex loss: 0.623716  [   70/   88]
per-ex loss: 0.524835  [   71/   88]
per-ex loss: 0.762291  [   72/   88]
per-ex loss: 0.512916  [   73/   88]
per-ex loss: 0.587961  [   74/   88]
per-ex loss: 0.669345  [   75/   88]
per-ex loss: 0.740540  [   76/   88]
per-ex loss: 0.580802  [   77/   88]
per-ex loss: 0.546301  [   78/   88]
per-ex loss: 0.670281  [   79/   88]
per-ex loss: 0.632164  [   80/   88]
per-ex loss: 0.574605  [   81/   88]
per-ex loss: 0.782936  [   82/   88]
per-ex loss: 0.583393  [   83/   88]
per-ex loss: 0.717481  [   84/   88]
per-ex loss: 0.488555  [   85/   88]
per-ex loss: 0.547704  [   86/   88]
per-ex loss: 0.691694  [   87/   88]
per-ex loss: 0.496562  [   88/   88]
Train Error: Avg loss: 0.59931612
validation Error: 
 Avg loss: 0.68226645 
 F1: 0.507388 
 Precision: 0.527457 
 Recall: 0.488790
 IoU: 0.339933

test Error: 
 Avg loss: 0.63206960 
 F1: 0.574711 
 Precision: 0.579703 
 Recall: 0.569804
 IoU: 0.403224

We have finished training iteration 115
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_113_.pth
per-ex loss: 0.645693  [    1/   88]
per-ex loss: 0.635945  [    2/   88]
per-ex loss: 0.572516  [    3/   88]
per-ex loss: 0.570721  [    4/   88]
per-ex loss: 0.704492  [    5/   88]
per-ex loss: 0.667568  [    6/   88]
per-ex loss: 0.683236  [    7/   88]
per-ex loss: 0.604432  [    8/   88]
per-ex loss: 0.568279  [    9/   88]
per-ex loss: 0.475072  [   10/   88]
per-ex loss: 0.628871  [   11/   88]
per-ex loss: 0.568583  [   12/   88]
per-ex loss: 0.705742  [   13/   88]
per-ex loss: 0.549687  [   14/   88]
per-ex loss: 0.648146  [   15/   88]
per-ex loss: 0.784491  [   16/   88]
per-ex loss: 0.593560  [   17/   88]
per-ex loss: 0.460574  [   18/   88]
per-ex loss: 0.556526  [   19/   88]
per-ex loss: 0.564298  [   20/   88]
per-ex loss: 0.517387  [   21/   88]
per-ex loss: 0.781576  [   22/   88]
per-ex loss: 0.700253  [   23/   88]
per-ex loss: 0.718410  [   24/   88]
per-ex loss: 0.732215  [   25/   88]
per-ex loss: 0.553297  [   26/   88]
per-ex loss: 0.479351  [   27/   88]
per-ex loss: 0.466590  [   28/   88]
per-ex loss: 0.498414  [   29/   88]
per-ex loss: 0.516866  [   30/   88]
per-ex loss: 0.527929  [   31/   88]
per-ex loss: 0.779924  [   32/   88]
per-ex loss: 0.736759  [   33/   88]
per-ex loss: 0.712276  [   34/   88]
per-ex loss: 0.579593  [   35/   88]
per-ex loss: 0.576508  [   36/   88]
per-ex loss: 0.727024  [   37/   88]
per-ex loss: 0.497988  [   38/   88]
per-ex loss: 0.519708  [   39/   88]
per-ex loss: 0.647348  [   40/   88]
per-ex loss: 0.592730  [   41/   88]
per-ex loss: 0.520663  [   42/   88]
per-ex loss: 0.692439  [   43/   88]
per-ex loss: 0.495599  [   44/   88]
per-ex loss: 0.685071  [   45/   88]
per-ex loss: 0.548771  [   46/   88]
per-ex loss: 0.507858  [   47/   88]
per-ex loss: 0.760402  [   48/   88]
per-ex loss: 0.743302  [   49/   88]
per-ex loss: 0.616180  [   50/   88]
per-ex loss: 0.377966  [   51/   88]
per-ex loss: 0.631397  [   52/   88]
per-ex loss: 0.613381  [   53/   88]
per-ex loss: 0.466047  [   54/   88]
per-ex loss: 0.708267  [   55/   88]
per-ex loss: 0.591836  [   56/   88]
per-ex loss: 0.470415  [   57/   88]
per-ex loss: 0.542952  [   58/   88]
per-ex loss: 0.536961  [   59/   88]
per-ex loss: 0.541707  [   60/   88]
per-ex loss: 0.455171  [   61/   88]
per-ex loss: 0.571078  [   62/   88]
per-ex loss: 0.523296  [   63/   88]
per-ex loss: 0.528459  [   64/   88]
per-ex loss: 0.714886  [   65/   88]
per-ex loss: 0.613686  [   66/   88]
per-ex loss: 0.471227  [   67/   88]
per-ex loss: 0.523860  [   68/   88]
per-ex loss: 0.492982  [   69/   88]
per-ex loss: 0.494768  [   70/   88]
per-ex loss: 0.523920  [   71/   88]
per-ex loss: 0.687085  [   72/   88]
per-ex loss: 0.692307  [   73/   88]
per-ex loss: 0.526315  [   74/   88]
per-ex loss: 0.782481  [   75/   88]
per-ex loss: 0.528056  [   76/   88]
per-ex loss: 0.434914  [   77/   88]
per-ex loss: 0.558009  [   78/   88]
per-ex loss: 0.697502  [   79/   88]
per-ex loss: 0.543303  [   80/   88]
per-ex loss: 0.667958  [   81/   88]
per-ex loss: 0.732258  [   82/   88]
per-ex loss: 0.726451  [   83/   88]
per-ex loss: 0.717038  [   84/   88]
per-ex loss: 0.805462  [   85/   88]
per-ex loss: 0.473944  [   86/   88]
per-ex loss: 0.606399  [   87/   88]
per-ex loss: 0.515499  [   88/   88]
Train Error: Avg loss: 0.59897848
validation Error: 
 Avg loss: 0.68764062 
 F1: 0.498760 
 Precision: 0.588838 
 Recall: 0.432585
 IoU: 0.332232

test Error: 
 Avg loss: 0.64400850 
 F1: 0.560053 
 Precision: 0.655266 
 Recall: 0.489000
 IoU: 0.388940

We have finished training iteration 116
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_114_.pth
per-ex loss: 0.713116  [    1/   88]
per-ex loss: 0.509815  [    2/   88]
per-ex loss: 0.473391  [    3/   88]
per-ex loss: 0.561958  [    4/   88]
per-ex loss: 0.516263  [    5/   88]
per-ex loss: 0.558814  [    6/   88]
per-ex loss: 0.502416  [    7/   88]
per-ex loss: 0.593762  [    8/   88]
per-ex loss: 0.502578  [    9/   88]
per-ex loss: 0.488697  [   10/   88]
per-ex loss: 0.717750  [   11/   88]
per-ex loss: 0.381963  [   12/   88]
per-ex loss: 0.555947  [   13/   88]
per-ex loss: 0.800495  [   14/   88]
per-ex loss: 0.517016  [   15/   88]
per-ex loss: 0.713747  [   16/   88]
per-ex loss: 0.697608  [   17/   88]
per-ex loss: 0.621772  [   18/   88]
per-ex loss: 0.611844  [   19/   88]
per-ex loss: 0.608927  [   20/   88]
per-ex loss: 0.494181  [   21/   88]
per-ex loss: 0.547843  [   22/   88]
per-ex loss: 0.694289  [   23/   88]
per-ex loss: 0.689889  [   24/   88]
per-ex loss: 0.475496  [   25/   88]
per-ex loss: 0.441701  [   26/   88]
per-ex loss: 0.774103  [   27/   88]
per-ex loss: 0.487159  [   28/   88]
per-ex loss: 0.455544  [   29/   88]
per-ex loss: 0.712427  [   30/   88]
per-ex loss: 0.686631  [   31/   88]
per-ex loss: 0.703925  [   32/   88]
per-ex loss: 0.542986  [   33/   88]
per-ex loss: 0.529824  [   34/   88]
per-ex loss: 0.561249  [   35/   88]
per-ex loss: 0.698878  [   36/   88]
per-ex loss: 0.597561  [   37/   88]
per-ex loss: 0.609694  [   38/   88]
per-ex loss: 0.689768  [   39/   88]
per-ex loss: 0.504010  [   40/   88]
per-ex loss: 0.521944  [   41/   88]
per-ex loss: 0.608361  [   42/   88]
per-ex loss: 0.618411  [   43/   88]
per-ex loss: 0.748173  [   44/   88]
per-ex loss: 0.460804  [   45/   88]
per-ex loss: 0.742782  [   46/   88]
per-ex loss: 0.755856  [   47/   88]
per-ex loss: 0.769817  [   48/   88]
per-ex loss: 0.719664  [   49/   88]
per-ex loss: 0.538887  [   50/   88]
per-ex loss: 0.556550  [   51/   88]
per-ex loss: 0.759568  [   52/   88]
per-ex loss: 0.502610  [   53/   88]
per-ex loss: 0.479430  [   54/   88]
per-ex loss: 0.587289  [   55/   88]
per-ex loss: 0.628654  [   56/   88]
per-ex loss: 0.736395  [   57/   88]
per-ex loss: 0.573462  [   58/   88]
per-ex loss: 0.536962  [   59/   88]
per-ex loss: 0.582798  [   60/   88]
per-ex loss: 0.466311  [   61/   88]
per-ex loss: 0.604434  [   62/   88]
per-ex loss: 0.457955  [   63/   88]
per-ex loss: 0.544827  [   64/   88]
per-ex loss: 0.676378  [   65/   88]
per-ex loss: 0.774906  [   66/   88]
per-ex loss: 0.543683  [   67/   88]
per-ex loss: 0.520228  [   68/   88]
per-ex loss: 0.532322  [   69/   88]
per-ex loss: 0.532358  [   70/   88]
per-ex loss: 0.552691  [   71/   88]
per-ex loss: 0.681570  [   72/   88]
per-ex loss: 0.654543  [   73/   88]
per-ex loss: 0.483763  [   74/   88]
per-ex loss: 0.606334  [   75/   88]
per-ex loss: 0.766729  [   76/   88]
per-ex loss: 0.676066  [   77/   88]
per-ex loss: 0.699645  [   78/   88]
per-ex loss: 0.532542  [   79/   88]
per-ex loss: 0.636750  [   80/   88]
per-ex loss: 0.721016  [   81/   88]
per-ex loss: 0.593721  [   82/   88]
per-ex loss: 0.464210  [   83/   88]
per-ex loss: 0.716399  [   84/   88]
per-ex loss: 0.705693  [   85/   88]
per-ex loss: 0.693341  [   86/   88]
per-ex loss: 0.491634  [   87/   88]
per-ex loss: 0.534027  [   88/   88]
Train Error: Avg loss: 0.60042644
validation Error: 
 Avg loss: 0.69040541 
 F1: 0.495864 
 Precision: 0.511231 
 Recall: 0.481393
 IoU: 0.329667

test Error: 
 Avg loss: 0.65128922 
 F1: 0.554962 
 Precision: 0.560950 
 Recall: 0.549100
 IoU: 0.384046

We have finished training iteration 117
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_115_.pth
per-ex loss: 0.525504  [    1/   88]
per-ex loss: 0.696138  [    2/   88]
per-ex loss: 0.569165  [    3/   88]
per-ex loss: 0.638113  [    4/   88]
per-ex loss: 0.532426  [    5/   88]
per-ex loss: 0.592222  [    6/   88]
per-ex loss: 0.587114  [    7/   88]
per-ex loss: 0.655157  [    8/   88]
per-ex loss: 0.472825  [    9/   88]
per-ex loss: 0.525493  [   10/   88]
per-ex loss: 0.558545  [   11/   88]
per-ex loss: 0.675281  [   12/   88]
per-ex loss: 0.531182  [   13/   88]
per-ex loss: 0.643103  [   14/   88]
per-ex loss: 0.501398  [   15/   88]
per-ex loss: 0.494115  [   16/   88]
per-ex loss: 0.790850  [   17/   88]
per-ex loss: 0.611792  [   18/   88]
per-ex loss: 0.448375  [   19/   88]
per-ex loss: 0.481113  [   20/   88]
per-ex loss: 0.506237  [   21/   88]
per-ex loss: 0.739254  [   22/   88]
per-ex loss: 0.708772  [   23/   88]
per-ex loss: 0.557812  [   24/   88]
per-ex loss: 0.580108  [   25/   88]
per-ex loss: 0.712175  [   26/   88]
per-ex loss: 0.731003  [   27/   88]
per-ex loss: 0.518538  [   28/   88]
per-ex loss: 0.489530  [   29/   88]
per-ex loss: 0.434861  [   30/   88]
per-ex loss: 0.691577  [   31/   88]
per-ex loss: 0.649288  [   32/   88]
per-ex loss: 0.606834  [   33/   88]
per-ex loss: 0.497579  [   34/   88]
per-ex loss: 0.468492  [   35/   88]
per-ex loss: 0.691070  [   36/   88]
per-ex loss: 0.768259  [   37/   88]
per-ex loss: 0.592317  [   38/   88]
per-ex loss: 0.755633  [   39/   88]
per-ex loss: 0.552860  [   40/   88]
per-ex loss: 0.612638  [   41/   88]
per-ex loss: 0.527755  [   42/   88]
per-ex loss: 0.458564  [   43/   88]
per-ex loss: 0.644414  [   44/   88]
per-ex loss: 0.588503  [   45/   88]
per-ex loss: 0.515501  [   46/   88]
per-ex loss: 0.704455  [   47/   88]
per-ex loss: 0.740211  [   48/   88]
per-ex loss: 0.368870  [   49/   88]
per-ex loss: 0.692065  [   50/   88]
per-ex loss: 0.492367  [   51/   88]
per-ex loss: 0.488694  [   52/   88]
per-ex loss: 0.688935  [   53/   88]
per-ex loss: 0.508858  [   54/   88]
per-ex loss: 0.561538  [   55/   88]
per-ex loss: 0.722310  [   56/   88]
per-ex loss: 0.620612  [   57/   88]
per-ex loss: 0.539689  [   58/   88]
per-ex loss: 0.549039  [   59/   88]
per-ex loss: 0.751212  [   60/   88]
per-ex loss: 0.781398  [   61/   88]
per-ex loss: 0.473179  [   62/   88]
per-ex loss: 0.716117  [   63/   88]
per-ex loss: 0.607126  [   64/   88]
per-ex loss: 0.782075  [   65/   88]
per-ex loss: 0.481764  [   66/   88]
per-ex loss: 0.412146  [   67/   88]
per-ex loss: 0.683534  [   68/   88]
per-ex loss: 0.689015  [   69/   88]
per-ex loss: 0.705777  [   70/   88]
per-ex loss: 0.592080  [   71/   88]
per-ex loss: 0.719049  [   72/   88]
per-ex loss: 0.458245  [   73/   88]
per-ex loss: 0.569397  [   74/   88]
per-ex loss: 0.503303  [   75/   88]
per-ex loss: 0.519942  [   76/   88]
per-ex loss: 0.762180  [   77/   88]
per-ex loss: 0.639117  [   78/   88]
per-ex loss: 0.708723  [   79/   88]
per-ex loss: 0.673367  [   80/   88]
per-ex loss: 0.772629  [   81/   88]
per-ex loss: 0.573821  [   82/   88]
per-ex loss: 0.516935  [   83/   88]
per-ex loss: 0.568435  [   84/   88]
per-ex loss: 0.611023  [   85/   88]
per-ex loss: 0.552987  [   86/   88]
per-ex loss: 0.543887  [   87/   88]
per-ex loss: 0.465687  [   88/   88]
Train Error: Avg loss: 0.59819638
validation Error: 
 Avg loss: 0.68531496 
 F1: 0.502261 
 Precision: 0.599163 
 Recall: 0.432339
 IoU: 0.335346

test Error: 
 Avg loss: 0.64065034 
 F1: 0.564106 
 Precision: 0.633981 
 Recall: 0.508105
 IoU: 0.392861

We have finished training iteration 118
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_116_.pth
per-ex loss: 0.568315  [    1/   88]
per-ex loss: 0.691431  [    2/   88]
per-ex loss: 0.494473  [    3/   88]
per-ex loss: 0.478934  [    4/   88]
per-ex loss: 0.531219  [    5/   88]
per-ex loss: 0.496295  [    6/   88]
per-ex loss: 0.710151  [    7/   88]
per-ex loss: 0.470455  [    8/   88]
per-ex loss: 0.619004  [    9/   88]
per-ex loss: 0.472160  [   10/   88]
per-ex loss: 0.731165  [   11/   88]
per-ex loss: 0.569635  [   12/   88]
per-ex loss: 0.548805  [   13/   88]
per-ex loss: 0.515976  [   14/   88]
per-ex loss: 0.522020  [   15/   88]
per-ex loss: 0.546663  [   16/   88]
per-ex loss: 0.465106  [   17/   88]
per-ex loss: 0.534902  [   18/   88]
per-ex loss: 0.491729  [   19/   88]
per-ex loss: 0.486952  [   20/   88]
per-ex loss: 0.606245  [   21/   88]
per-ex loss: 0.717061  [   22/   88]
per-ex loss: 0.715945  [   23/   88]
per-ex loss: 0.488132  [   24/   88]
per-ex loss: 0.523196  [   25/   88]
per-ex loss: 0.750017  [   26/   88]
per-ex loss: 0.633267  [   27/   88]
per-ex loss: 0.766057  [   28/   88]
per-ex loss: 0.688630  [   29/   88]
per-ex loss: 0.661130  [   30/   88]
per-ex loss: 0.647707  [   31/   88]
per-ex loss: 0.488420  [   32/   88]
per-ex loss: 0.595242  [   33/   88]
per-ex loss: 0.606141  [   34/   88]
per-ex loss: 0.474037  [   35/   88]
per-ex loss: 0.606212  [   36/   88]
per-ex loss: 0.676397  [   37/   88]
per-ex loss: 0.693038  [   38/   88]
per-ex loss: 0.651181  [   39/   88]
per-ex loss: 0.540075  [   40/   88]
per-ex loss: 0.776855  [   41/   88]
per-ex loss: 0.539935  [   42/   88]
per-ex loss: 0.763196  [   43/   88]
per-ex loss: 0.476357  [   44/   88]
per-ex loss: 0.566387  [   45/   88]
per-ex loss: 0.509973  [   46/   88]
per-ex loss: 0.591753  [   47/   88]
per-ex loss: 0.547587  [   48/   88]
per-ex loss: 0.588544  [   49/   88]
per-ex loss: 0.448788  [   50/   88]
per-ex loss: 0.810503  [   51/   88]
per-ex loss: 0.688387  [   52/   88]
per-ex loss: 0.550749  [   53/   88]
per-ex loss: 0.545253  [   54/   88]
per-ex loss: 0.682733  [   55/   88]
per-ex loss: 0.790312  [   56/   88]
per-ex loss: 0.540418  [   57/   88]
per-ex loss: 0.590867  [   58/   88]
per-ex loss: 0.695292  [   59/   88]
per-ex loss: 0.450281  [   60/   88]
per-ex loss: 0.462606  [   61/   88]
per-ex loss: 0.554214  [   62/   88]
per-ex loss: 0.592702  [   63/   88]
per-ex loss: 0.500313  [   64/   88]
per-ex loss: 0.735106  [   65/   88]
per-ex loss: 0.751353  [   66/   88]
per-ex loss: 0.494160  [   67/   88]
per-ex loss: 0.639483  [   68/   88]
per-ex loss: 0.716445  [   69/   88]
per-ex loss: 0.694987  [   70/   88]
per-ex loss: 0.601432  [   71/   88]
per-ex loss: 0.696494  [   72/   88]
per-ex loss: 0.691424  [   73/   88]
per-ex loss: 0.516709  [   74/   88]
per-ex loss: 0.748055  [   75/   88]
per-ex loss: 0.566826  [   76/   88]
per-ex loss: 0.708322  [   77/   88]
per-ex loss: 0.446544  [   78/   88]
per-ex loss: 0.648722  [   79/   88]
per-ex loss: 0.722946  [   80/   88]
per-ex loss: 0.372538  [   81/   88]
per-ex loss: 0.713915  [   82/   88]
per-ex loss: 0.550495  [   83/   88]
per-ex loss: 0.441968  [   84/   88]
per-ex loss: 0.710497  [   85/   88]
per-ex loss: 0.681034  [   86/   88]
per-ex loss: 0.542415  [   87/   88]
per-ex loss: 0.536367  [   88/   88]
Train Error: Avg loss: 0.59847454
validation Error: 
 Avg loss: 0.69308458 
 F1: 0.497795 
 Precision: 0.504071 
 Recall: 0.491674
 IoU: 0.331377

test Error: 
 Avg loss: 0.64522552 
 F1: 0.559034 
 Precision: 0.548798 
 Recall: 0.569660
 IoU: 0.387958

We have finished training iteration 119
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_117_.pth
per-ex loss: 0.545401  [    1/   88]
per-ex loss: 0.475542  [    2/   88]
per-ex loss: 0.492960  [    3/   88]
per-ex loss: 0.533447  [    4/   88]
per-ex loss: 0.545914  [    5/   88]
per-ex loss: 0.701274  [    6/   88]
per-ex loss: 0.615615  [    7/   88]
per-ex loss: 0.575794  [    8/   88]
per-ex loss: 0.627038  [    9/   88]
per-ex loss: 0.716785  [   10/   88]
per-ex loss: 0.483209  [   11/   88]
per-ex loss: 0.481466  [   12/   88]
per-ex loss: 0.538544  [   13/   88]
per-ex loss: 0.701409  [   14/   88]
per-ex loss: 0.516791  [   15/   88]
per-ex loss: 0.523535  [   16/   88]
per-ex loss: 0.675651  [   17/   88]
per-ex loss: 0.746750  [   18/   88]
per-ex loss: 0.711100  [   19/   88]
per-ex loss: 0.368577  [   20/   88]
per-ex loss: 0.598284  [   21/   88]
per-ex loss: 0.717006  [   22/   88]
per-ex loss: 0.541281  [   23/   88]
per-ex loss: 0.682692  [   24/   88]
per-ex loss: 0.603637  [   25/   88]
per-ex loss: 0.625659  [   26/   88]
per-ex loss: 0.456517  [   27/   88]
per-ex loss: 0.678675  [   28/   88]
per-ex loss: 0.524305  [   29/   88]
per-ex loss: 0.573785  [   30/   88]
per-ex loss: 0.485793  [   31/   88]
per-ex loss: 0.637194  [   32/   88]
per-ex loss: 0.789588  [   33/   88]
per-ex loss: 0.566662  [   34/   88]
per-ex loss: 0.638570  [   35/   88]
per-ex loss: 0.553452  [   36/   88]
per-ex loss: 0.506001  [   37/   88]
per-ex loss: 0.785908  [   38/   88]
per-ex loss: 0.714056  [   39/   88]
per-ex loss: 0.487430  [   40/   88]
per-ex loss: 0.749376  [   41/   88]
per-ex loss: 0.479637  [   42/   88]
per-ex loss: 0.457713  [   43/   88]
per-ex loss: 0.535135  [   44/   88]
per-ex loss: 0.481610  [   45/   88]
per-ex loss: 0.769239  [   46/   88]
per-ex loss: 0.627081  [   47/   88]
per-ex loss: 0.768128  [   48/   88]
per-ex loss: 0.475498  [   49/   88]
per-ex loss: 0.772034  [   50/   88]
per-ex loss: 0.555872  [   51/   88]
per-ex loss: 0.491690  [   52/   88]
per-ex loss: 0.620651  [   53/   88]
per-ex loss: 0.539751  [   54/   88]
per-ex loss: 0.677003  [   55/   88]
per-ex loss: 0.604973  [   56/   88]
per-ex loss: 0.691960  [   57/   88]
per-ex loss: 0.624325  [   58/   88]
per-ex loss: 0.775261  [   59/   88]
per-ex loss: 0.578915  [   60/   88]
per-ex loss: 0.620362  [   61/   88]
per-ex loss: 0.560612  [   62/   88]
per-ex loss: 0.738883  [   63/   88]
per-ex loss: 0.516835  [   64/   88]
per-ex loss: 0.557653  [   65/   88]
per-ex loss: 0.455927  [   66/   88]
per-ex loss: 0.569403  [   67/   88]
per-ex loss: 0.559063  [   68/   88]
per-ex loss: 0.701645  [   69/   88]
per-ex loss: 0.523071  [   70/   88]
per-ex loss: 0.686314  [   71/   88]
per-ex loss: 0.537694  [   72/   88]
per-ex loss: 0.542993  [   73/   88]
per-ex loss: 0.651774  [   74/   88]
per-ex loss: 0.504554  [   75/   88]
per-ex loss: 0.429688  [   76/   88]
per-ex loss: 0.460419  [   77/   88]
per-ex loss: 0.607744  [   78/   88]
per-ex loss: 0.443727  [   79/   88]
per-ex loss: 0.689618  [   80/   88]
per-ex loss: 0.703163  [   81/   88]
per-ex loss: 0.520812  [   82/   88]
per-ex loss: 0.735336  [   83/   88]
per-ex loss: 0.625794  [   84/   88]
per-ex loss: 0.726406  [   85/   88]
per-ex loss: 0.484979  [   86/   88]
per-ex loss: 0.683992  [   87/   88]
per-ex loss: 0.774785  [   88/   88]
Train Error: Avg loss: 0.59809539
validation Error: 
 Avg loss: 0.68094646 
 F1: 0.505062 
 Precision: 0.579815 
 Recall: 0.447383
 IoU: 0.337848

test Error: 
 Avg loss: 0.64255759 
 F1: 0.564115 
 Precision: 0.618612 
 Recall: 0.518442
 IoU: 0.392869

We have finished training iteration 120
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_118_.pth
per-ex loss: 0.495953  [    1/   88]
per-ex loss: 0.782491  [    2/   88]
per-ex loss: 0.579868  [    3/   88]
per-ex loss: 0.691772  [    4/   88]
per-ex loss: 0.707054  [    5/   88]
per-ex loss: 0.732606  [    6/   88]
per-ex loss: 0.452603  [    7/   88]
per-ex loss: 0.496790  [    8/   88]
per-ex loss: 0.562040  [    9/   88]
per-ex loss: 0.675174  [   10/   88]
per-ex loss: 0.421122  [   11/   88]
per-ex loss: 0.472150  [   12/   88]
per-ex loss: 0.621058  [   13/   88]
per-ex loss: 0.670998  [   14/   88]
per-ex loss: 0.477379  [   15/   88]
per-ex loss: 0.534738  [   16/   88]
per-ex loss: 0.781849  [   17/   88]
per-ex loss: 0.512298  [   18/   88]
per-ex loss: 0.508155  [   19/   88]
per-ex loss: 0.561308  [   20/   88]
per-ex loss: 0.486117  [   21/   88]
per-ex loss: 0.663260  [   22/   88]
per-ex loss: 0.438889  [   23/   88]
per-ex loss: 0.736822  [   24/   88]
per-ex loss: 0.450620  [   25/   88]
per-ex loss: 0.646417  [   26/   88]
per-ex loss: 0.605428  [   27/   88]
per-ex loss: 0.540577  [   28/   88]
per-ex loss: 0.711390  [   29/   88]
per-ex loss: 0.568532  [   30/   88]
per-ex loss: 0.550275  [   31/   88]
per-ex loss: 0.623774  [   32/   88]
per-ex loss: 0.512170  [   33/   88]
per-ex loss: 0.732495  [   34/   88]
per-ex loss: 0.454654  [   35/   88]
per-ex loss: 0.625127  [   36/   88]
per-ex loss: 0.489547  [   37/   88]
per-ex loss: 0.631250  [   38/   88]
per-ex loss: 0.696991  [   39/   88]
per-ex loss: 0.521217  [   40/   88]
per-ex loss: 0.532816  [   41/   88]
per-ex loss: 0.602141  [   42/   88]
per-ex loss: 0.612111  [   43/   88]
per-ex loss: 0.441610  [   44/   88]
per-ex loss: 0.496221  [   45/   88]
per-ex loss: 0.503782  [   46/   88]
per-ex loss: 0.710763  [   47/   88]
per-ex loss: 0.726423  [   48/   88]
per-ex loss: 0.682359  [   49/   88]
per-ex loss: 0.555155  [   50/   88]
per-ex loss: 0.482663  [   51/   88]
per-ex loss: 0.503592  [   52/   88]
per-ex loss: 0.474639  [   53/   88]
per-ex loss: 0.580378  [   54/   88]
per-ex loss: 0.708468  [   55/   88]
per-ex loss: 0.553703  [   56/   88]
per-ex loss: 0.450362  [   57/   88]
per-ex loss: 0.709717  [   58/   88]
per-ex loss: 0.541578  [   59/   88]
per-ex loss: 0.586536  [   60/   88]
per-ex loss: 0.613901  [   61/   88]
per-ex loss: 0.689874  [   62/   88]
per-ex loss: 0.581715  [   63/   88]
per-ex loss: 0.579112  [   64/   88]
per-ex loss: 0.690370  [   65/   88]
per-ex loss: 0.730378  [   66/   88]
per-ex loss: 0.497820  [   67/   88]
per-ex loss: 0.527229  [   68/   88]
per-ex loss: 0.693974  [   69/   88]
per-ex loss: 0.531120  [   70/   88]
per-ex loss: 0.649042  [   71/   88]
per-ex loss: 0.527892  [   72/   88]
per-ex loss: 0.707958  [   73/   88]
per-ex loss: 0.727206  [   74/   88]
per-ex loss: 0.742231  [   75/   88]
per-ex loss: 0.688723  [   76/   88]
per-ex loss: 0.774348  [   77/   88]
per-ex loss: 0.625109  [   78/   88]
per-ex loss: 0.554752  [   79/   88]
per-ex loss: 0.386711  [   80/   88]
per-ex loss: 0.565954  [   81/   88]
per-ex loss: 0.553845  [   82/   88]
per-ex loss: 0.571793  [   83/   88]
per-ex loss: 0.689733  [   84/   88]
per-ex loss: 0.771606  [   85/   88]
per-ex loss: 0.559123  [   86/   88]
per-ex loss: 0.727809  [   87/   88]
per-ex loss: 0.780548  [   88/   88]
Train Error: Avg loss: 0.59793006
validation Error: 
 Avg loss: 0.68691381 
 F1: 0.503150 
 Precision: 0.598861 
 Recall: 0.433816
 IoU: 0.336139

test Error: 
 Avg loss: 0.64218431 
 F1: 0.568567 
 Precision: 0.642881 
 Recall: 0.509653
 IoU: 0.397201

We have finished training iteration 121
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_119_.pth
per-ex loss: 0.685978  [    1/   88]
per-ex loss: 0.468020  [    2/   88]
per-ex loss: 0.755783  [    3/   88]
per-ex loss: 0.572390  [    4/   88]
per-ex loss: 0.528617  [    5/   88]
per-ex loss: 0.475957  [    6/   88]
per-ex loss: 0.617830  [    7/   88]
per-ex loss: 0.643784  [    8/   88]
per-ex loss: 0.628713  [    9/   88]
per-ex loss: 0.580368  [   10/   88]
per-ex loss: 0.656899  [   11/   88]
per-ex loss: 0.738493  [   12/   88]
per-ex loss: 0.521840  [   13/   88]
per-ex loss: 0.677643  [   14/   88]
per-ex loss: 0.606065  [   15/   88]
per-ex loss: 0.534486  [   16/   88]
per-ex loss: 0.600235  [   17/   88]
per-ex loss: 0.579402  [   18/   88]
per-ex loss: 0.582039  [   19/   88]
per-ex loss: 0.506846  [   20/   88]
per-ex loss: 0.532022  [   21/   88]
per-ex loss: 0.703924  [   22/   88]
per-ex loss: 0.498642  [   23/   88]
per-ex loss: 0.500998  [   24/   88]
per-ex loss: 0.706596  [   25/   88]
per-ex loss: 0.603852  [   26/   88]
per-ex loss: 0.695016  [   27/   88]
per-ex loss: 0.772446  [   28/   88]
per-ex loss: 0.435246  [   29/   88]
per-ex loss: 0.613433  [   30/   88]
per-ex loss: 0.533188  [   31/   88]
per-ex loss: 0.685493  [   32/   88]
per-ex loss: 0.727276  [   33/   88]
per-ex loss: 0.479230  [   34/   88]
per-ex loss: 0.703804  [   35/   88]
per-ex loss: 0.700989  [   36/   88]
per-ex loss: 0.682684  [   37/   88]
per-ex loss: 0.698104  [   38/   88]
per-ex loss: 0.558317  [   39/   88]
per-ex loss: 0.791284  [   40/   88]
per-ex loss: 0.494949  [   41/   88]
per-ex loss: 0.716891  [   42/   88]
per-ex loss: 0.565026  [   43/   88]
per-ex loss: 0.491170  [   44/   88]
per-ex loss: 0.746750  [   45/   88]
per-ex loss: 0.702172  [   46/   88]
per-ex loss: 0.764386  [   47/   88]
per-ex loss: 0.570494  [   48/   88]
per-ex loss: 0.564132  [   49/   88]
per-ex loss: 0.488183  [   50/   88]
per-ex loss: 0.508466  [   51/   88]
per-ex loss: 0.579701  [   52/   88]
per-ex loss: 0.734157  [   53/   88]
per-ex loss: 0.534402  [   54/   88]
per-ex loss: 0.637711  [   55/   88]
per-ex loss: 0.423475  [   56/   88]
per-ex loss: 0.502203  [   57/   88]
per-ex loss: 0.756250  [   58/   88]
per-ex loss: 0.443371  [   59/   88]
per-ex loss: 0.692212  [   60/   88]
per-ex loss: 0.506397  [   61/   88]
per-ex loss: 0.770046  [   62/   88]
per-ex loss: 0.525208  [   63/   88]
per-ex loss: 0.449359  [   64/   88]
per-ex loss: 0.674225  [   65/   88]
per-ex loss: 0.663545  [   66/   88]
per-ex loss: 0.514555  [   67/   88]
per-ex loss: 0.460175  [   68/   88]
per-ex loss: 0.536332  [   69/   88]
per-ex loss: 0.366012  [   70/   88]
per-ex loss: 0.548426  [   71/   88]
per-ex loss: 0.734914  [   72/   88]
per-ex loss: 0.525280  [   73/   88]
per-ex loss: 0.539758  [   74/   88]
per-ex loss: 0.711063  [   75/   88]
per-ex loss: 0.718374  [   76/   88]
per-ex loss: 0.578397  [   77/   88]
per-ex loss: 0.553695  [   78/   88]
per-ex loss: 0.470291  [   79/   88]
per-ex loss: 0.520337  [   80/   88]
per-ex loss: 0.742199  [   81/   88]
per-ex loss: 0.452931  [   82/   88]
per-ex loss: 0.463644  [   83/   88]
per-ex loss: 0.559841  [   84/   88]
per-ex loss: 0.739852  [   85/   88]
per-ex loss: 0.578990  [   86/   88]
per-ex loss: 0.495283  [   87/   88]
per-ex loss: 0.475325  [   88/   88]
Train Error: Avg loss: 0.59516464
validation Error: 
 Avg loss: 0.68450877 
 F1: 0.501400 
 Precision: 0.613748 
 Recall: 0.423819
 IoU: 0.334579

test Error: 
 Avg loss: 0.64398007 
 F1: 0.562398 
 Precision: 0.665665 
 Recall: 0.486869
 IoU: 0.391206

We have finished training iteration 122
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_120_.pth
per-ex loss: 0.632961  [    1/   88]
per-ex loss: 0.575359  [    2/   88]
per-ex loss: 0.595648  [    3/   88]
per-ex loss: 0.493176  [    4/   88]
per-ex loss: 0.737598  [    5/   88]
per-ex loss: 0.315575  [    6/   88]
per-ex loss: 0.728090  [    7/   88]
per-ex loss: 0.468963  [    8/   88]
per-ex loss: 0.570928  [    9/   88]
per-ex loss: 0.551130  [   10/   88]
per-ex loss: 0.472877  [   11/   88]
per-ex loss: 0.676483  [   12/   88]
per-ex loss: 0.765856  [   13/   88]
per-ex loss: 0.540123  [   14/   88]
per-ex loss: 0.746656  [   15/   88]
per-ex loss: 0.524280  [   16/   88]
per-ex loss: 0.590444  [   17/   88]
per-ex loss: 0.570976  [   18/   88]
per-ex loss: 0.707601  [   19/   88]
per-ex loss: 0.538345  [   20/   88]
per-ex loss: 0.474602  [   21/   88]
per-ex loss: 0.553839  [   22/   88]
per-ex loss: 0.715089  [   23/   88]
per-ex loss: 0.642423  [   24/   88]
per-ex loss: 0.499586  [   25/   88]
per-ex loss: 0.715261  [   26/   88]
per-ex loss: 0.774565  [   27/   88]
per-ex loss: 0.633795  [   28/   88]
per-ex loss: 0.574151  [   29/   88]
per-ex loss: 0.722468  [   30/   88]
per-ex loss: 0.479113  [   31/   88]
per-ex loss: 0.637112  [   32/   88]
per-ex loss: 0.687907  [   33/   88]
per-ex loss: 0.580905  [   34/   88]
per-ex loss: 0.655863  [   35/   88]
per-ex loss: 0.402200  [   36/   88]
per-ex loss: 0.530843  [   37/   88]
per-ex loss: 0.710946  [   38/   88]
per-ex loss: 0.477766  [   39/   88]
per-ex loss: 0.481069  [   40/   88]
per-ex loss: 0.751406  [   41/   88]
per-ex loss: 0.471473  [   42/   88]
per-ex loss: 0.685016  [   43/   88]
per-ex loss: 0.784681  [   44/   88]
per-ex loss: 0.776392  [   45/   88]
per-ex loss: 0.601385  [   46/   88]
per-ex loss: 0.712171  [   47/   88]
per-ex loss: 0.505727  [   48/   88]
per-ex loss: 0.462688  [   49/   88]
per-ex loss: 0.457451  [   50/   88]
per-ex loss: 0.528528  [   51/   88]
per-ex loss: 0.580221  [   52/   88]
per-ex loss: 0.704224  [   53/   88]
per-ex loss: 0.663857  [   54/   88]
per-ex loss: 0.704726  [   55/   88]
per-ex loss: 0.437637  [   56/   88]
per-ex loss: 0.517135  [   57/   88]
per-ex loss: 0.450119  [   58/   88]
per-ex loss: 0.460373  [   59/   88]
per-ex loss: 0.703165  [   60/   88]
per-ex loss: 0.662564  [   61/   88]
per-ex loss: 0.542255  [   62/   88]
per-ex loss: 0.466431  [   63/   88]
per-ex loss: 0.556653  [   64/   88]
per-ex loss: 0.685650  [   65/   88]
per-ex loss: 0.522097  [   66/   88]
per-ex loss: 0.614366  [   67/   88]
per-ex loss: 0.499651  [   68/   88]
per-ex loss: 0.669936  [   69/   88]
per-ex loss: 0.505773  [   70/   88]
per-ex loss: 0.495963  [   71/   88]
per-ex loss: 0.518203  [   72/   88]
per-ex loss: 0.438614  [   73/   88]
per-ex loss: 0.566904  [   74/   88]
per-ex loss: 0.657037  [   75/   88]
per-ex loss: 0.546343  [   76/   88]
per-ex loss: 0.592848  [   77/   88]
per-ex loss: 0.565175  [   78/   88]
per-ex loss: 0.527664  [   79/   88]
per-ex loss: 0.652441  [   80/   88]
per-ex loss: 0.490284  [   81/   88]
per-ex loss: 0.676837  [   82/   88]
per-ex loss: 0.526788  [   83/   88]
per-ex loss: 0.646535  [   84/   88]
per-ex loss: 0.533963  [   85/   88]
per-ex loss: 0.465303  [   86/   88]
per-ex loss: 0.682620  [   87/   88]
per-ex loss: 0.773217  [   88/   88]
Train Error: Avg loss: 0.58855751
validation Error: 
 Avg loss: 0.68320803 
 F1: 0.502562 
 Precision: 0.618790 
 Recall: 0.423091
 IoU: 0.335614

test Error: 
 Avg loss: 0.63992656 
 F1: 0.566662 
 Precision: 0.663740 
 Recall: 0.494358
 IoU: 0.395345

We have finished training iteration 123
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_121_.pth
per-ex loss: 0.500454  [    1/   88]
per-ex loss: 0.482963  [    2/   88]
per-ex loss: 0.601665  [    3/   88]
per-ex loss: 0.765564  [    4/   88]
per-ex loss: 0.650175  [    5/   88]
per-ex loss: 0.715814  [    6/   88]
per-ex loss: 0.527637  [    7/   88]
per-ex loss: 0.556509  [    8/   88]
per-ex loss: 0.781783  [    9/   88]
per-ex loss: 0.640312  [   10/   88]
per-ex loss: 0.526723  [   11/   88]
per-ex loss: 0.741719  [   12/   88]
per-ex loss: 0.724460  [   13/   88]
per-ex loss: 0.682402  [   14/   88]
per-ex loss: 0.529789  [   15/   88]
per-ex loss: 0.704769  [   16/   88]
per-ex loss: 0.563682  [   17/   88]
per-ex loss: 0.616669  [   18/   88]
per-ex loss: 0.674703  [   19/   88]
per-ex loss: 0.508583  [   20/   88]
per-ex loss: 0.517308  [   21/   88]
per-ex loss: 0.538281  [   22/   88]
per-ex loss: 0.782798  [   23/   88]
per-ex loss: 0.468771  [   24/   88]
per-ex loss: 0.467963  [   25/   88]
per-ex loss: 0.454409  [   26/   88]
per-ex loss: 0.702758  [   27/   88]
per-ex loss: 0.559724  [   28/   88]
per-ex loss: 0.687646  [   29/   88]
per-ex loss: 0.577353  [   30/   88]
per-ex loss: 0.618198  [   31/   88]
per-ex loss: 0.494045  [   32/   88]
per-ex loss: 0.685071  [   33/   88]
per-ex loss: 0.681298  [   34/   88]
per-ex loss: 0.583344  [   35/   88]
per-ex loss: 0.544274  [   36/   88]
per-ex loss: 0.767256  [   37/   88]
per-ex loss: 0.483885  [   38/   88]
per-ex loss: 0.695250  [   39/   88]
per-ex loss: 0.529117  [   40/   88]
per-ex loss: 0.718720  [   41/   88]
per-ex loss: 0.503742  [   42/   88]
per-ex loss: 0.487807  [   43/   88]
per-ex loss: 0.635280  [   44/   88]
per-ex loss: 0.579417  [   45/   88]
per-ex loss: 0.536462  [   46/   88]
per-ex loss: 0.436141  [   47/   88]
per-ex loss: 0.384545  [   48/   88]
per-ex loss: 0.550350  [   49/   88]
per-ex loss: 0.595393  [   50/   88]
per-ex loss: 0.459055  [   51/   88]
per-ex loss: 0.723280  [   52/   88]
per-ex loss: 0.673421  [   53/   88]
per-ex loss: 0.675773  [   54/   88]
per-ex loss: 0.639174  [   55/   88]
per-ex loss: 0.470781  [   56/   88]
per-ex loss: 0.743206  [   57/   88]
per-ex loss: 0.503242  [   58/   88]
per-ex loss: 0.482176  [   59/   88]
per-ex loss: 0.724234  [   60/   88]
per-ex loss: 0.775605  [   61/   88]
per-ex loss: 0.612961  [   62/   88]
per-ex loss: 0.592106  [   63/   88]
per-ex loss: 0.527959  [   64/   88]
per-ex loss: 0.487120  [   65/   88]
per-ex loss: 0.705452  [   66/   88]
per-ex loss: 0.685291  [   67/   88]
per-ex loss: 0.565194  [   68/   88]
per-ex loss: 0.727838  [   69/   88]
per-ex loss: 0.581428  [   70/   88]
per-ex loss: 0.538797  [   71/   88]
per-ex loss: 0.466182  [   72/   88]
per-ex loss: 0.500483  [   73/   88]
per-ex loss: 0.466321  [   74/   88]
per-ex loss: 0.469974  [   75/   88]
per-ex loss: 0.485924  [   76/   88]
per-ex loss: 0.506275  [   77/   88]
per-ex loss: 0.757070  [   78/   88]
per-ex loss: 0.696354  [   79/   88]
per-ex loss: 0.402387  [   80/   88]
per-ex loss: 0.539673  [   81/   88]
per-ex loss: 0.529477  [   82/   88]
per-ex loss: 0.612938  [   83/   88]
per-ex loss: 0.761599  [   84/   88]
per-ex loss: 0.582855  [   85/   88]
per-ex loss: 0.664443  [   86/   88]
per-ex loss: 0.630450  [   87/   88]
per-ex loss: 0.572985  [   88/   88]
Train Error: Avg loss: 0.59434618
validation Error: 
 Avg loss: 0.68879222 
 F1: 0.498442 
 Precision: 0.552572 
 Recall: 0.453972
 IoU: 0.331950

test Error: 
 Avg loss: 0.64186634 
 F1: 0.562130 
 Precision: 0.620111 
 Recall: 0.514065
 IoU: 0.390947

We have finished training iteration 124
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_122_.pth
per-ex loss: 0.704468  [    1/   88]
per-ex loss: 0.647733  [    2/   88]
per-ex loss: 0.621557  [    3/   88]
per-ex loss: 0.502461  [    4/   88]
per-ex loss: 0.791971  [    5/   88]
per-ex loss: 0.536096  [    6/   88]
per-ex loss: 0.722042  [    7/   88]
per-ex loss: 0.627373  [    8/   88]
per-ex loss: 0.777388  [    9/   88]
per-ex loss: 0.487354  [   10/   88]
per-ex loss: 0.540853  [   11/   88]
per-ex loss: 0.674090  [   12/   88]
per-ex loss: 0.526118  [   13/   88]
per-ex loss: 0.456575  [   14/   88]
per-ex loss: 0.526426  [   15/   88]
per-ex loss: 0.506533  [   16/   88]
per-ex loss: 0.547010  [   17/   88]
per-ex loss: 0.417560  [   18/   88]
per-ex loss: 0.640241  [   19/   88]
per-ex loss: 0.697416  [   20/   88]
per-ex loss: 0.565555  [   21/   88]
per-ex loss: 0.737773  [   22/   88]
per-ex loss: 0.353389  [   23/   88]
per-ex loss: 0.611559  [   24/   88]
per-ex loss: 0.536488  [   25/   88]
per-ex loss: 0.550751  [   26/   88]
per-ex loss: 0.712257  [   27/   88]
per-ex loss: 0.519035  [   28/   88]
per-ex loss: 0.744982  [   29/   88]
per-ex loss: 0.596076  [   30/   88]
per-ex loss: 0.692416  [   31/   88]
per-ex loss: 0.523276  [   32/   88]
per-ex loss: 0.628023  [   33/   88]
per-ex loss: 0.483158  [   34/   88]
per-ex loss: 0.687416  [   35/   88]
per-ex loss: 0.655768  [   36/   88]
per-ex loss: 0.517364  [   37/   88]
per-ex loss: 0.583671  [   38/   88]
per-ex loss: 0.598233  [   39/   88]
per-ex loss: 0.488556  [   40/   88]
per-ex loss: 0.505899  [   41/   88]
per-ex loss: 0.454556  [   42/   88]
per-ex loss: 0.649349  [   43/   88]
per-ex loss: 0.508749  [   44/   88]
per-ex loss: 0.509659  [   45/   88]
per-ex loss: 0.537195  [   46/   88]
per-ex loss: 0.446165  [   47/   88]
per-ex loss: 0.774878  [   48/   88]
per-ex loss: 0.551205  [   49/   88]
per-ex loss: 0.581464  [   50/   88]
per-ex loss: 0.681820  [   51/   88]
per-ex loss: 0.709627  [   52/   88]
per-ex loss: 0.501783  [   53/   88]
per-ex loss: 0.507561  [   54/   88]
per-ex loss: 0.773205  [   55/   88]
per-ex loss: 0.682979  [   56/   88]
per-ex loss: 0.695680  [   57/   88]
per-ex loss: 0.592139  [   58/   88]
per-ex loss: 0.764968  [   59/   88]
per-ex loss: 0.576559  [   60/   88]
per-ex loss: 0.544805  [   61/   88]
per-ex loss: 0.451967  [   62/   88]
per-ex loss: 0.781341  [   63/   88]
per-ex loss: 0.472409  [   64/   88]
per-ex loss: 0.470762  [   65/   88]
per-ex loss: 0.721560  [   66/   88]
per-ex loss: 0.742071  [   67/   88]
per-ex loss: 0.447703  [   68/   88]
per-ex loss: 0.641967  [   69/   88]
per-ex loss: 0.693645  [   70/   88]
per-ex loss: 0.730800  [   71/   88]
per-ex loss: 0.638305  [   72/   88]
per-ex loss: 0.707022  [   73/   88]
per-ex loss: 0.478804  [   74/   88]
per-ex loss: 0.474856  [   75/   88]
per-ex loss: 0.768380  [   76/   88]
per-ex loss: 0.575065  [   77/   88]
per-ex loss: 0.541261  [   78/   88]
per-ex loss: 0.536206  [   79/   88]
per-ex loss: 0.490053  [   80/   88]
per-ex loss: 0.436257  [   81/   88]
per-ex loss: 0.552447  [   82/   88]
per-ex loss: 0.645101  [   83/   88]
per-ex loss: 0.711888  [   84/   88]
per-ex loss: 0.720588  [   85/   88]
per-ex loss: 0.710097  [   86/   88]
per-ex loss: 0.577702  [   87/   88]
per-ex loss: 0.528612  [   88/   88]
Train Error: Avg loss: 0.59697864
validation Error: 
 Avg loss: 0.69363803 
 F1: 0.494097 
 Precision: 0.633115 
 Recall: 0.405138
 IoU: 0.328107

test Error: 
 Avg loss: 0.65266707 
 F1: 0.549369 
 Precision: 0.676494 
 Recall: 0.462463
 IoU: 0.378710

We have finished training iteration 125
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_123_.pth
per-ex loss: 0.772167  [    1/   88]
per-ex loss: 0.503235  [    2/   88]
per-ex loss: 0.783210  [    3/   88]
per-ex loss: 0.681184  [    4/   88]
per-ex loss: 0.626634  [    5/   88]
per-ex loss: 0.487350  [    6/   88]
per-ex loss: 0.785271  [    7/   88]
per-ex loss: 0.515937  [    8/   88]
per-ex loss: 0.756794  [    9/   88]
per-ex loss: 0.490598  [   10/   88]
per-ex loss: 0.394878  [   11/   88]
per-ex loss: 0.558604  [   12/   88]
per-ex loss: 0.531260  [   13/   88]
per-ex loss: 0.524641  [   14/   88]
per-ex loss: 0.783346  [   15/   88]
per-ex loss: 0.718840  [   16/   88]
per-ex loss: 0.492483  [   17/   88]
per-ex loss: 0.582750  [   18/   88]
per-ex loss: 0.522622  [   19/   88]
per-ex loss: 0.702814  [   20/   88]
per-ex loss: 0.545150  [   21/   88]
per-ex loss: 0.695498  [   22/   88]
per-ex loss: 0.611055  [   23/   88]
per-ex loss: 0.439373  [   24/   88]
per-ex loss: 0.464671  [   25/   88]
per-ex loss: 0.453551  [   26/   88]
per-ex loss: 0.604107  [   27/   88]
per-ex loss: 0.455849  [   28/   88]
per-ex loss: 0.524640  [   29/   88]
per-ex loss: 0.548167  [   30/   88]
per-ex loss: 0.566182  [   31/   88]
per-ex loss: 0.602331  [   32/   88]
per-ex loss: 0.610150  [   33/   88]
per-ex loss: 0.464132  [   34/   88]
per-ex loss: 0.720120  [   35/   88]
per-ex loss: 0.722714  [   36/   88]
per-ex loss: 0.661379  [   37/   88]
per-ex loss: 0.571288  [   38/   88]
per-ex loss: 0.529359  [   39/   88]
per-ex loss: 0.686041  [   40/   88]
per-ex loss: 0.709515  [   41/   88]
per-ex loss: 0.693695  [   42/   88]
per-ex loss: 0.551208  [   43/   88]
per-ex loss: 0.539375  [   44/   88]
per-ex loss: 0.515289  [   45/   88]
per-ex loss: 0.600924  [   46/   88]
per-ex loss: 0.569261  [   47/   88]
per-ex loss: 0.699063  [   48/   88]
per-ex loss: 0.677268  [   49/   88]
per-ex loss: 0.666376  [   50/   88]
per-ex loss: 0.599108  [   51/   88]
per-ex loss: 0.490459  [   52/   88]
per-ex loss: 0.527569  [   53/   88]
per-ex loss: 0.700309  [   54/   88]
per-ex loss: 0.429764  [   55/   88]
per-ex loss: 0.624382  [   56/   88]
per-ex loss: 0.529972  [   57/   88]
per-ex loss: 0.526127  [   58/   88]
per-ex loss: 0.720921  [   59/   88]
per-ex loss: 0.727641  [   60/   88]
per-ex loss: 0.471917  [   61/   88]
per-ex loss: 0.744565  [   62/   88]
per-ex loss: 0.707029  [   63/   88]
per-ex loss: 0.597958  [   64/   88]
per-ex loss: 0.569821  [   65/   88]
per-ex loss: 0.577716  [   66/   88]
per-ex loss: 0.492840  [   67/   88]
per-ex loss: 0.484646  [   68/   88]
per-ex loss: 0.462598  [   69/   88]
per-ex loss: 0.517806  [   70/   88]
per-ex loss: 0.521658  [   71/   88]
per-ex loss: 0.687676  [   72/   88]
per-ex loss: 0.647274  [   73/   88]
per-ex loss: 0.667222  [   74/   88]
per-ex loss: 0.539831  [   75/   88]
per-ex loss: 0.558250  [   76/   88]
per-ex loss: 0.476957  [   77/   88]
per-ex loss: 0.702734  [   78/   88]
per-ex loss: 0.638374  [   79/   88]
per-ex loss: 0.697557  [   80/   88]
per-ex loss: 0.521196  [   81/   88]
per-ex loss: 0.437695  [   82/   88]
per-ex loss: 0.475089  [   83/   88]
per-ex loss: 0.780114  [   84/   88]
per-ex loss: 0.467662  [   85/   88]
per-ex loss: 0.685072  [   86/   88]
per-ex loss: 0.583863  [   87/   88]
per-ex loss: 0.722480  [   88/   88]
Train Error: Avg loss: 0.59347955
validation Error: 
 Avg loss: 0.69226468 
 F1: 0.494433 
 Precision: 0.578542 
 Recall: 0.431676
 IoU: 0.328403

test Error: 
 Avg loss: 0.64400260 
 F1: 0.554730 
 Precision: 0.644938 
 Recall: 0.486660
 IoU: 0.383824

We have finished training iteration 126
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_124_.pth
per-ex loss: 0.489578  [    1/   88]
per-ex loss: 0.670415  [    2/   88]
per-ex loss: 0.477320  [    3/   88]
per-ex loss: 0.571656  [    4/   88]
per-ex loss: 0.698025  [    5/   88]
per-ex loss: 0.516409  [    6/   88]
per-ex loss: 0.543608  [    7/   88]
per-ex loss: 0.678262  [    8/   88]
per-ex loss: 0.660068  [    9/   88]
per-ex loss: 0.549399  [   10/   88]
per-ex loss: 0.469686  [   11/   88]
per-ex loss: 0.668656  [   12/   88]
per-ex loss: 0.532778  [   13/   88]
per-ex loss: 0.534559  [   14/   88]
per-ex loss: 0.661735  [   15/   88]
per-ex loss: 0.438882  [   16/   88]
per-ex loss: 0.445457  [   17/   88]
per-ex loss: 0.747841  [   18/   88]
per-ex loss: 0.750451  [   19/   88]
per-ex loss: 0.711494  [   20/   88]
per-ex loss: 0.573237  [   21/   88]
per-ex loss: 0.507774  [   22/   88]
per-ex loss: 0.596441  [   23/   88]
per-ex loss: 0.576782  [   24/   88]
per-ex loss: 0.489610  [   25/   88]
per-ex loss: 0.516091  [   26/   88]
per-ex loss: 0.575942  [   27/   88]
per-ex loss: 0.703069  [   28/   88]
per-ex loss: 0.692779  [   29/   88]
per-ex loss: 0.698301  [   30/   88]
per-ex loss: 0.803317  [   31/   88]
per-ex loss: 0.506667  [   32/   88]
per-ex loss: 0.516773  [   33/   88]
per-ex loss: 0.730444  [   34/   88]
per-ex loss: 0.469749  [   35/   88]
per-ex loss: 0.549116  [   36/   88]
per-ex loss: 0.578840  [   37/   88]
per-ex loss: 0.698696  [   38/   88]
per-ex loss: 0.610708  [   39/   88]
per-ex loss: 0.502244  [   40/   88]
per-ex loss: 0.596845  [   41/   88]
per-ex loss: 0.609112  [   42/   88]
per-ex loss: 0.554947  [   43/   88]
per-ex loss: 0.737405  [   44/   88]
per-ex loss: 0.436065  [   45/   88]
per-ex loss: 0.547676  [   46/   88]
per-ex loss: 0.568250  [   47/   88]
per-ex loss: 0.760220  [   48/   88]
per-ex loss: 0.382830  [   49/   88]
per-ex loss: 0.609299  [   50/   88]
per-ex loss: 0.654982  [   51/   88]
per-ex loss: 0.659354  [   52/   88]
per-ex loss: 0.515993  [   53/   88]
per-ex loss: 0.785127  [   54/   88]
per-ex loss: 0.622045  [   55/   88]
per-ex loss: 0.678882  [   56/   88]
per-ex loss: 0.720766  [   57/   88]
per-ex loss: 0.579675  [   58/   88]
per-ex loss: 0.729970  [   59/   88]
per-ex loss: 0.571508  [   60/   88]
per-ex loss: 0.607961  [   61/   88]
per-ex loss: 0.702870  [   62/   88]
per-ex loss: 0.557269  [   63/   88]
per-ex loss: 0.679264  [   64/   88]
per-ex loss: 0.482780  [   65/   88]
per-ex loss: 0.489846  [   66/   88]
per-ex loss: 0.715127  [   67/   88]
per-ex loss: 0.457239  [   68/   88]
per-ex loss: 0.516671  [   69/   88]
per-ex loss: 0.793843  [   70/   88]
per-ex loss: 0.607012  [   71/   88]
per-ex loss: 0.646232  [   72/   88]
per-ex loss: 0.433323  [   73/   88]
per-ex loss: 0.469631  [   74/   88]
per-ex loss: 0.717046  [   75/   88]
per-ex loss: 0.684439  [   76/   88]
per-ex loss: 0.592148  [   77/   88]
per-ex loss: 0.646362  [   78/   88]
per-ex loss: 0.511072  [   79/   88]
per-ex loss: 0.493303  [   80/   88]
per-ex loss: 0.512531  [   81/   88]
per-ex loss: 0.459376  [   82/   88]
per-ex loss: 0.500321  [   83/   88]
per-ex loss: 0.560498  [   84/   88]
per-ex loss: 0.485028  [   85/   88]
per-ex loss: 0.544791  [   86/   88]
per-ex loss: 0.777747  [   87/   88]
per-ex loss: 0.421265  [   88/   88]
Train Error: Avg loss: 0.59203186
validation Error: 
 Avg loss: 0.69040946 
 F1: 0.496958 
 Precision: 0.606272 
 Recall: 0.421042
 IoU: 0.330635

test Error: 
 Avg loss: 0.64617657 
 F1: 0.557140 
 Precision: 0.654796 
 Recall: 0.484832
 IoU: 0.386136

We have finished training iteration 127
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_125_.pth
per-ex loss: 0.582712  [    1/   88]
per-ex loss: 0.484323  [    2/   88]
per-ex loss: 0.716182  [    3/   88]
per-ex loss: 0.667352  [    4/   88]
per-ex loss: 0.548499  [    5/   88]
per-ex loss: 0.627934  [    6/   88]
per-ex loss: 0.464319  [    7/   88]
per-ex loss: 0.649337  [    8/   88]
per-ex loss: 0.679474  [    9/   88]
per-ex loss: 0.504875  [   10/   88]
per-ex loss: 0.707167  [   11/   88]
per-ex loss: 0.444489  [   12/   88]
per-ex loss: 0.584194  [   13/   88]
per-ex loss: 0.475808  [   14/   88]
per-ex loss: 0.628881  [   15/   88]
per-ex loss: 0.767773  [   16/   88]
per-ex loss: 0.604331  [   17/   88]
per-ex loss: 0.648397  [   18/   88]
per-ex loss: 0.704809  [   19/   88]
per-ex loss: 0.526340  [   20/   88]
per-ex loss: 0.591198  [   21/   88]
per-ex loss: 0.564722  [   22/   88]
per-ex loss: 0.442584  [   23/   88]
per-ex loss: 0.486652  [   24/   88]
per-ex loss: 0.503094  [   25/   88]
per-ex loss: 0.458545  [   26/   88]
per-ex loss: 0.552495  [   27/   88]
per-ex loss: 0.682948  [   28/   88]
per-ex loss: 0.583973  [   29/   88]
per-ex loss: 0.629462  [   30/   88]
per-ex loss: 0.525381  [   31/   88]
per-ex loss: 0.532019  [   32/   88]
per-ex loss: 0.644538  [   33/   88]
per-ex loss: 0.344605  [   34/   88]
per-ex loss: 0.584523  [   35/   88]
per-ex loss: 0.725310  [   36/   88]
per-ex loss: 0.680133  [   37/   88]
per-ex loss: 0.670603  [   38/   88]
per-ex loss: 0.756450  [   39/   88]
per-ex loss: 0.526893  [   40/   88]
per-ex loss: 0.561820  [   41/   88]
per-ex loss: 0.710187  [   42/   88]
per-ex loss: 0.678532  [   43/   88]
per-ex loss: 0.554956  [   44/   88]
per-ex loss: 0.790201  [   45/   88]
per-ex loss: 0.700692  [   46/   88]
per-ex loss: 0.747151  [   47/   88]
per-ex loss: 0.728908  [   48/   88]
per-ex loss: 0.493185  [   49/   88]
per-ex loss: 0.492386  [   50/   88]
per-ex loss: 0.550855  [   51/   88]
per-ex loss: 0.547826  [   52/   88]
per-ex loss: 0.786746  [   53/   88]
per-ex loss: 0.470754  [   54/   88]
per-ex loss: 0.609977  [   55/   88]
per-ex loss: 0.646718  [   56/   88]
per-ex loss: 0.529247  [   57/   88]
per-ex loss: 0.516290  [   58/   88]
per-ex loss: 0.726224  [   59/   88]
per-ex loss: 0.504014  [   60/   88]
per-ex loss: 0.516870  [   61/   88]
per-ex loss: 0.506841  [   62/   88]
per-ex loss: 0.540928  [   63/   88]
per-ex loss: 0.693612  [   64/   88]
per-ex loss: 0.751160  [   65/   88]
per-ex loss: 0.459161  [   66/   88]
per-ex loss: 0.576929  [   67/   88]
per-ex loss: 0.715820  [   68/   88]
per-ex loss: 0.620651  [   69/   88]
per-ex loss: 0.642731  [   70/   88]
per-ex loss: 0.542704  [   71/   88]
per-ex loss: 0.730143  [   72/   88]
per-ex loss: 0.641893  [   73/   88]
per-ex loss: 0.764333  [   74/   88]
per-ex loss: 0.525774  [   75/   88]
per-ex loss: 0.441207  [   76/   88]
per-ex loss: 0.523372  [   77/   88]
per-ex loss: 0.567348  [   78/   88]
per-ex loss: 0.483956  [   79/   88]
per-ex loss: 0.522690  [   80/   88]
per-ex loss: 0.757977  [   81/   88]
per-ex loss: 0.558906  [   82/   88]
per-ex loss: 0.524741  [   83/   88]
per-ex loss: 0.706376  [   84/   88]
per-ex loss: 0.492806  [   85/   88]
per-ex loss: 0.475126  [   86/   88]
per-ex loss: 0.559016  [   87/   88]
per-ex loss: 0.685561  [   88/   88]
Train Error: Avg loss: 0.59521166
validation Error: 
 Avg loss: 0.68650877 
 F1: 0.502826 
 Precision: 0.615895 
 Recall: 0.424834
 IoU: 0.335851

test Error: 
 Avg loss: 0.64204754 
 F1: 0.564336 
 Precision: 0.657204 
 Recall: 0.494465
 IoU: 0.393084

We have finished training iteration 128
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_126_.pth
per-ex loss: 0.487930  [    1/   88]
per-ex loss: 0.617082  [    2/   88]
per-ex loss: 0.469805  [    3/   88]
per-ex loss: 0.695095  [    4/   88]
per-ex loss: 0.522083  [    5/   88]
per-ex loss: 0.643503  [    6/   88]
per-ex loss: 0.611797  [    7/   88]
per-ex loss: 0.723213  [    8/   88]
per-ex loss: 0.507189  [    9/   88]
per-ex loss: 0.594930  [   10/   88]
per-ex loss: 0.441885  [   11/   88]
per-ex loss: 0.678359  [   12/   88]
per-ex loss: 0.494441  [   13/   88]
per-ex loss: 0.473979  [   14/   88]
per-ex loss: 0.545180  [   15/   88]
per-ex loss: 0.617523  [   16/   88]
per-ex loss: 0.468492  [   17/   88]
per-ex loss: 0.530504  [   18/   88]
per-ex loss: 0.670326  [   19/   88]
per-ex loss: 0.436500  [   20/   88]
per-ex loss: 0.702942  [   21/   88]
per-ex loss: 0.775357  [   22/   88]
per-ex loss: 0.709408  [   23/   88]
per-ex loss: 0.772369  [   24/   88]
per-ex loss: 0.629546  [   25/   88]
per-ex loss: 0.545301  [   26/   88]
per-ex loss: 0.474962  [   27/   88]
per-ex loss: 0.529188  [   28/   88]
per-ex loss: 0.717712  [   29/   88]
per-ex loss: 0.432526  [   30/   88]
per-ex loss: 0.592432  [   31/   88]
per-ex loss: 0.735782  [   32/   88]
per-ex loss: 0.701828  [   33/   88]
per-ex loss: 0.664359  [   34/   88]
per-ex loss: 0.520145  [   35/   88]
per-ex loss: 0.527115  [   36/   88]
per-ex loss: 0.744768  [   37/   88]
per-ex loss: 0.475366  [   38/   88]
per-ex loss: 0.476566  [   39/   88]
per-ex loss: 0.704957  [   40/   88]
per-ex loss: 0.716349  [   41/   88]
per-ex loss: 0.485834  [   42/   88]
per-ex loss: 0.770625  [   43/   88]
per-ex loss: 0.477688  [   44/   88]
per-ex loss: 0.584461  [   45/   88]
per-ex loss: 0.536303  [   46/   88]
per-ex loss: 0.452143  [   47/   88]
per-ex loss: 0.664401  [   48/   88]
per-ex loss: 0.543954  [   49/   88]
per-ex loss: 0.527610  [   50/   88]
per-ex loss: 0.692149  [   51/   88]
per-ex loss: 0.556934  [   52/   88]
per-ex loss: 0.761921  [   53/   88]
per-ex loss: 0.565510  [   54/   88]
per-ex loss: 0.596805  [   55/   88]
per-ex loss: 0.568082  [   56/   88]
per-ex loss: 0.481300  [   57/   88]
per-ex loss: 0.646991  [   58/   88]
per-ex loss: 0.490750  [   59/   88]
per-ex loss: 0.599156  [   60/   88]
per-ex loss: 0.552070  [   61/   88]
per-ex loss: 0.586027  [   62/   88]
per-ex loss: 0.481229  [   63/   88]
per-ex loss: 0.505297  [   64/   88]
per-ex loss: 0.561424  [   65/   88]
per-ex loss: 0.641869  [   66/   88]
per-ex loss: 0.509076  [   67/   88]
per-ex loss: 0.680771  [   68/   88]
per-ex loss: 0.431780  [   69/   88]
per-ex loss: 0.669904  [   70/   88]
per-ex loss: 0.517598  [   71/   88]
per-ex loss: 0.334825  [   72/   88]
per-ex loss: 0.728812  [   73/   88]
per-ex loss: 0.694986  [   74/   88]
per-ex loss: 0.521379  [   75/   88]
per-ex loss: 0.481499  [   76/   88]
per-ex loss: 0.545680  [   77/   88]
per-ex loss: 0.624972  [   78/   88]
per-ex loss: 0.673600  [   79/   88]
per-ex loss: 0.759936  [   80/   88]
per-ex loss: 0.546816  [   81/   88]
per-ex loss: 0.589450  [   82/   88]
per-ex loss: 0.753591  [   83/   88]
per-ex loss: 0.496059  [   84/   88]
per-ex loss: 0.511733  [   85/   88]
per-ex loss: 0.565764  [   86/   88]
per-ex loss: 0.676292  [   87/   88]
per-ex loss: 0.683135  [   88/   88]
Train Error: Avg loss: 0.58757938
validation Error: 
 Avg loss: 0.69509667 
 F1: 0.494257 
 Precision: 0.493784 
 Recall: 0.494730
 IoU: 0.328247

test Error: 
 Avg loss: 0.63919131 
 F1: 0.565386 
 Precision: 0.561837 
 Recall: 0.568979
 IoU: 0.394103

We have finished training iteration 129
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_127_.pth
per-ex loss: 0.587222  [    1/   88]
per-ex loss: 0.682852  [    2/   88]
per-ex loss: 0.564661  [    3/   88]
per-ex loss: 0.480953  [    4/   88]
per-ex loss: 0.446530  [    5/   88]
per-ex loss: 0.615474  [    6/   88]
per-ex loss: 0.677649  [    7/   88]
per-ex loss: 0.531150  [    8/   88]
per-ex loss: 0.774593  [    9/   88]
per-ex loss: 0.746200  [   10/   88]
per-ex loss: 0.688536  [   11/   88]
per-ex loss: 0.752587  [   12/   88]
per-ex loss: 0.460037  [   13/   88]
per-ex loss: 0.520748  [   14/   88]
per-ex loss: 0.685781  [   15/   88]
per-ex loss: 0.443778  [   16/   88]
per-ex loss: 0.683565  [   17/   88]
per-ex loss: 0.471984  [   18/   88]
per-ex loss: 0.706459  [   19/   88]
per-ex loss: 0.724046  [   20/   88]
per-ex loss: 0.460321  [   21/   88]
per-ex loss: 0.498919  [   22/   88]
per-ex loss: 0.608994  [   23/   88]
per-ex loss: 0.524201  [   24/   88]
per-ex loss: 0.572630  [   25/   88]
per-ex loss: 0.709442  [   26/   88]
per-ex loss: 0.479137  [   27/   88]
per-ex loss: 0.532933  [   28/   88]
per-ex loss: 0.439986  [   29/   88]
per-ex loss: 0.451506  [   30/   88]
per-ex loss: 0.489083  [   31/   88]
per-ex loss: 0.725596  [   32/   88]
per-ex loss: 0.609260  [   33/   88]
per-ex loss: 0.483330  [   34/   88]
per-ex loss: 0.721514  [   35/   88]
per-ex loss: 0.456028  [   36/   88]
per-ex loss: 0.728488  [   37/   88]
per-ex loss: 0.547316  [   38/   88]
per-ex loss: 0.626226  [   39/   88]
per-ex loss: 0.543912  [   40/   88]
per-ex loss: 0.606304  [   41/   88]
per-ex loss: 0.608511  [   42/   88]
per-ex loss: 0.787352  [   43/   88]
per-ex loss: 0.560758  [   44/   88]
per-ex loss: 0.474311  [   45/   88]
per-ex loss: 0.555578  [   46/   88]
per-ex loss: 0.537230  [   47/   88]
per-ex loss: 0.733972  [   48/   88]
per-ex loss: 0.774623  [   49/   88]
per-ex loss: 0.752465  [   50/   88]
per-ex loss: 0.511936  [   51/   88]
per-ex loss: 0.695138  [   52/   88]
per-ex loss: 0.536642  [   53/   88]
per-ex loss: 0.413938  [   54/   88]
per-ex loss: 0.651105  [   55/   88]
per-ex loss: 0.610982  [   56/   88]
per-ex loss: 0.562388  [   57/   88]
per-ex loss: 0.510975  [   58/   88]
per-ex loss: 0.731738  [   59/   88]
per-ex loss: 0.641408  [   60/   88]
per-ex loss: 0.528959  [   61/   88]
per-ex loss: 0.761176  [   62/   88]
per-ex loss: 0.484317  [   63/   88]
per-ex loss: 0.508973  [   64/   88]
per-ex loss: 0.534771  [   65/   88]
per-ex loss: 0.656736  [   66/   88]
per-ex loss: 0.677296  [   67/   88]
per-ex loss: 0.707006  [   68/   88]
per-ex loss: 0.609563  [   69/   88]
per-ex loss: 0.478657  [   70/   88]
per-ex loss: 0.585519  [   71/   88]
per-ex loss: 0.538367  [   72/   88]
per-ex loss: 0.543445  [   73/   88]
per-ex loss: 0.547012  [   74/   88]
per-ex loss: 0.495151  [   75/   88]
per-ex loss: 0.612016  [   76/   88]
per-ex loss: 0.701023  [   77/   88]
per-ex loss: 0.680956  [   78/   88]
per-ex loss: 0.688835  [   79/   88]
per-ex loss: 0.621684  [   80/   88]
per-ex loss: 0.531850  [   81/   88]
per-ex loss: 0.521827  [   82/   88]
per-ex loss: 0.584890  [   83/   88]
per-ex loss: 0.668546  [   84/   88]
per-ex loss: 0.701327  [   85/   88]
per-ex loss: 0.574501  [   86/   88]
per-ex loss: 0.508374  [   87/   88]
per-ex loss: 0.503699  [   88/   88]
Train Error: Avg loss: 0.59394837
validation Error: 
 Avg loss: 0.69967640 
 F1: 0.483277 
 Precision: 0.476506 
 Recall: 0.490243
 IoU: 0.318632

test Error: 
 Avg loss: 0.64593678 
 F1: 0.560785 
 Precision: 0.552823 
 Recall: 0.568980
 IoU: 0.389647

We have finished training iteration 130
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_128_.pth
per-ex loss: 0.464114  [    1/   88]
per-ex loss: 0.576633  [    2/   88]
per-ex loss: 0.683832  [    3/   88]
per-ex loss: 0.521925  [    4/   88]
per-ex loss: 0.473079  [    5/   88]
per-ex loss: 0.549227  [    6/   88]
per-ex loss: 0.501085  [    7/   88]
per-ex loss: 0.750202  [    8/   88]
per-ex loss: 0.606169  [    9/   88]
per-ex loss: 0.538552  [   10/   88]
per-ex loss: 0.640655  [   11/   88]
per-ex loss: 0.768909  [   12/   88]
per-ex loss: 0.600474  [   13/   88]
per-ex loss: 0.669852  [   14/   88]
per-ex loss: 0.536124  [   15/   88]
per-ex loss: 0.497084  [   16/   88]
per-ex loss: 0.627203  [   17/   88]
per-ex loss: 0.442452  [   18/   88]
per-ex loss: 0.732882  [   19/   88]
per-ex loss: 0.771706  [   20/   88]
per-ex loss: 0.608250  [   21/   88]
per-ex loss: 0.518257  [   22/   88]
per-ex loss: 0.648380  [   23/   88]
per-ex loss: 0.579961  [   24/   88]
per-ex loss: 0.346900  [   25/   88]
per-ex loss: 0.600918  [   26/   88]
per-ex loss: 0.465057  [   27/   88]
per-ex loss: 0.493989  [   28/   88]
per-ex loss: 0.576138  [   29/   88]
per-ex loss: 0.647582  [   30/   88]
per-ex loss: 0.700608  [   31/   88]
per-ex loss: 0.478142  [   32/   88]
per-ex loss: 0.514097  [   33/   88]
per-ex loss: 0.569015  [   34/   88]
per-ex loss: 0.690953  [   35/   88]
per-ex loss: 0.614746  [   36/   88]
per-ex loss: 0.442078  [   37/   88]
per-ex loss: 0.505989  [   38/   88]
per-ex loss: 0.691984  [   39/   88]
per-ex loss: 0.525234  [   40/   88]
per-ex loss: 0.659342  [   41/   88]
per-ex loss: 0.463783  [   42/   88]
per-ex loss: 0.658312  [   43/   88]
per-ex loss: 0.523566  [   44/   88]
per-ex loss: 0.618347  [   45/   88]
per-ex loss: 0.477081  [   46/   88]
per-ex loss: 0.734726  [   47/   88]
per-ex loss: 0.483858  [   48/   88]
per-ex loss: 0.458263  [   49/   88]
per-ex loss: 0.698924  [   50/   88]
per-ex loss: 0.674353  [   51/   88]
per-ex loss: 0.548931  [   52/   88]
per-ex loss: 0.774923  [   53/   88]
per-ex loss: 0.684529  [   54/   88]
per-ex loss: 0.642611  [   55/   88]
per-ex loss: 0.480822  [   56/   88]
per-ex loss: 0.684959  [   57/   88]
per-ex loss: 0.683838  [   58/   88]
per-ex loss: 0.500772  [   59/   88]
per-ex loss: 0.740952  [   60/   88]
per-ex loss: 0.558207  [   61/   88]
per-ex loss: 0.561766  [   62/   88]
per-ex loss: 0.530465  [   63/   88]
per-ex loss: 0.791270  [   64/   88]
per-ex loss: 0.388003  [   65/   88]
per-ex loss: 0.704785  [   66/   88]
per-ex loss: 0.487427  [   67/   88]
per-ex loss: 0.497478  [   68/   88]
per-ex loss: 0.700565  [   69/   88]
per-ex loss: 0.468283  [   70/   88]
per-ex loss: 0.543638  [   71/   88]
per-ex loss: 0.807924  [   72/   88]
per-ex loss: 0.458490  [   73/   88]
per-ex loss: 0.551261  [   74/   88]
per-ex loss: 0.601414  [   75/   88]
per-ex loss: 0.558953  [   76/   88]
per-ex loss: 0.589061  [   77/   88]
per-ex loss: 0.494824  [   78/   88]
per-ex loss: 0.699509  [   79/   88]
per-ex loss: 0.713619  [   80/   88]
per-ex loss: 0.562326  [   81/   88]
per-ex loss: 0.542694  [   82/   88]
per-ex loss: 0.704894  [   83/   88]
per-ex loss: 0.545515  [   84/   88]
per-ex loss: 0.670684  [   85/   88]
per-ex loss: 0.527075  [   86/   88]
per-ex loss: 0.721871  [   87/   88]
per-ex loss: 0.592136  [   88/   88]
Train Error: Avg loss: 0.59019848
validation Error: 
 Avg loss: 0.68430831 
 F1: 0.502873 
 Precision: 0.592081 
 Recall: 0.437027
 IoU: 0.335892

test Error: 
 Avg loss: 0.63520135 
 F1: 0.573797 
 Precision: 0.652462 
 Recall: 0.512060
 IoU: 0.402325

We have finished training iteration 131
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_129_.pth
per-ex loss: 0.344028  [    1/   88]
per-ex loss: 0.528405  [    2/   88]
per-ex loss: 0.776603  [    3/   88]
per-ex loss: 0.603289  [    4/   88]
per-ex loss: 0.500729  [    5/   88]
per-ex loss: 0.586170  [    6/   88]
per-ex loss: 0.516073  [    7/   88]
per-ex loss: 0.694504  [    8/   88]
per-ex loss: 0.507816  [    9/   88]
per-ex loss: 0.705632  [   10/   88]
per-ex loss: 0.710589  [   11/   88]
per-ex loss: 0.499056  [   12/   88]
per-ex loss: 0.563499  [   13/   88]
per-ex loss: 0.495220  [   14/   88]
per-ex loss: 0.652332  [   15/   88]
per-ex loss: 0.694721  [   16/   88]
per-ex loss: 0.533161  [   17/   88]
per-ex loss: 0.580058  [   18/   88]
per-ex loss: 0.510696  [   19/   88]
per-ex loss: 0.636770  [   20/   88]
per-ex loss: 0.540224  [   21/   88]
per-ex loss: 0.557507  [   22/   88]
per-ex loss: 0.433658  [   23/   88]
per-ex loss: 0.674232  [   24/   88]
per-ex loss: 0.429420  [   25/   88]
per-ex loss: 0.694745  [   26/   88]
per-ex loss: 0.744219  [   27/   88]
per-ex loss: 0.516734  [   28/   88]
per-ex loss: 0.551182  [   29/   88]
per-ex loss: 0.667183  [   30/   88]
per-ex loss: 0.483450  [   31/   88]
per-ex loss: 0.450924  [   32/   88]
per-ex loss: 0.670871  [   33/   88]
per-ex loss: 0.466317  [   34/   88]
per-ex loss: 0.694788  [   35/   88]
per-ex loss: 0.488442  [   36/   88]
per-ex loss: 0.622410  [   37/   88]
per-ex loss: 0.608722  [   38/   88]
per-ex loss: 0.551956  [   39/   88]
per-ex loss: 0.678661  [   40/   88]
per-ex loss: 0.581572  [   41/   88]
per-ex loss: 0.556346  [   42/   88]
per-ex loss: 0.724166  [   43/   88]
per-ex loss: 0.564028  [   44/   88]
per-ex loss: 0.591817  [   45/   88]
per-ex loss: 0.485022  [   46/   88]
per-ex loss: 0.690528  [   47/   88]
per-ex loss: 0.498846  [   48/   88]
per-ex loss: 0.585163  [   49/   88]
per-ex loss: 0.467632  [   50/   88]
per-ex loss: 0.485981  [   51/   88]
per-ex loss: 0.765653  [   52/   88]
per-ex loss: 0.548577  [   53/   88]
per-ex loss: 0.592057  [   54/   88]
per-ex loss: 0.454856  [   55/   88]
per-ex loss: 0.467828  [   56/   88]
per-ex loss: 0.550475  [   57/   88]
per-ex loss: 0.708675  [   58/   88]
per-ex loss: 0.684995  [   59/   88]
per-ex loss: 0.723450  [   60/   88]
per-ex loss: 0.564240  [   61/   88]
per-ex loss: 0.534675  [   62/   88]
per-ex loss: 0.734994  [   63/   88]
per-ex loss: 0.539035  [   64/   88]
per-ex loss: 0.511780  [   65/   88]
per-ex loss: 0.468564  [   66/   88]
per-ex loss: 0.565499  [   67/   88]
per-ex loss: 0.775814  [   68/   88]
per-ex loss: 0.718106  [   69/   88]
per-ex loss: 0.434886  [   70/   88]
per-ex loss: 0.669709  [   71/   88]
per-ex loss: 0.748131  [   72/   88]
per-ex loss: 0.523895  [   73/   88]
per-ex loss: 0.475004  [   74/   88]
per-ex loss: 0.479823  [   75/   88]
per-ex loss: 0.491646  [   76/   88]
per-ex loss: 0.497385  [   77/   88]
per-ex loss: 0.722647  [   78/   88]
per-ex loss: 0.530999  [   79/   88]
per-ex loss: 0.764011  [   80/   88]
per-ex loss: 0.652204  [   81/   88]
per-ex loss: 0.536863  [   82/   88]
per-ex loss: 0.614298  [   83/   88]
per-ex loss: 0.780406  [   84/   88]
per-ex loss: 0.605105  [   85/   88]
per-ex loss: 0.718451  [   86/   88]
per-ex loss: 0.504668  [   87/   88]
per-ex loss: 0.723271  [   88/   88]
Train Error: Avg loss: 0.58839516
validation Error: 
 Avg loss: 0.69348138 
 F1: 0.492339 
 Precision: 0.512830 
 Recall: 0.473424
 IoU: 0.326559

test Error: 
 Avg loss: 0.64059437 
 F1: 0.563204 
 Precision: 0.576117 
 Recall: 0.550858
 IoU: 0.391987

We have finished training iteration 132
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_130_.pth
per-ex loss: 0.537718  [    1/   88]
per-ex loss: 0.575123  [    2/   88]
per-ex loss: 0.443688  [    3/   88]
per-ex loss: 0.682600  [    4/   88]
per-ex loss: 0.786563  [    5/   88]
per-ex loss: 0.773684  [    6/   88]
per-ex loss: 0.568936  [    7/   88]
per-ex loss: 0.487036  [    8/   88]
per-ex loss: 0.472246  [    9/   88]
per-ex loss: 0.510683  [   10/   88]
per-ex loss: 0.605508  [   11/   88]
per-ex loss: 0.415966  [   12/   88]
per-ex loss: 0.500743  [   13/   88]
per-ex loss: 0.510810  [   14/   88]
per-ex loss: 0.513713  [   15/   88]
per-ex loss: 0.779175  [   16/   88]
per-ex loss: 0.515415  [   17/   88]
per-ex loss: 0.569346  [   18/   88]
per-ex loss: 0.733825  [   19/   88]
per-ex loss: 0.614280  [   20/   88]
per-ex loss: 0.800722  [   21/   88]
per-ex loss: 0.692006  [   22/   88]
per-ex loss: 0.700061  [   23/   88]
per-ex loss: 0.737547  [   24/   88]
per-ex loss: 0.682685  [   25/   88]
per-ex loss: 0.752802  [   26/   88]
per-ex loss: 0.601529  [   27/   88]
per-ex loss: 0.442127  [   28/   88]
per-ex loss: 0.734877  [   29/   88]
per-ex loss: 0.470958  [   30/   88]
per-ex loss: 0.677721  [   31/   88]
per-ex loss: 0.544580  [   32/   88]
per-ex loss: 0.467754  [   33/   88]
per-ex loss: 0.564912  [   34/   88]
per-ex loss: 0.501315  [   35/   88]
per-ex loss: 0.491045  [   36/   88]
per-ex loss: 0.662231  [   37/   88]
per-ex loss: 0.524886  [   38/   88]
per-ex loss: 0.616128  [   39/   88]
per-ex loss: 0.457765  [   40/   88]
per-ex loss: 0.534042  [   41/   88]
per-ex loss: 0.704403  [   42/   88]
per-ex loss: 0.470554  [   43/   88]
per-ex loss: 0.648081  [   44/   88]
per-ex loss: 0.701618  [   45/   88]
per-ex loss: 0.681710  [   46/   88]
per-ex loss: 0.498489  [   47/   88]
per-ex loss: 0.580381  [   48/   88]
per-ex loss: 0.712370  [   49/   88]
per-ex loss: 0.550323  [   50/   88]
per-ex loss: 0.456126  [   51/   88]
per-ex loss: 0.526731  [   52/   88]
per-ex loss: 0.607353  [   53/   88]
per-ex loss: 0.360310  [   54/   88]
per-ex loss: 0.572163  [   55/   88]
per-ex loss: 0.532930  [   56/   88]
per-ex loss: 0.466526  [   57/   88]
per-ex loss: 0.659786  [   58/   88]
per-ex loss: 0.638057  [   59/   88]
per-ex loss: 0.657567  [   60/   88]
per-ex loss: 0.703385  [   61/   88]
per-ex loss: 0.514195  [   62/   88]
per-ex loss: 0.672508  [   63/   88]
per-ex loss: 0.763923  [   64/   88]
per-ex loss: 0.607974  [   65/   88]
per-ex loss: 0.464934  [   66/   88]
per-ex loss: 0.695843  [   67/   88]
per-ex loss: 0.613817  [   68/   88]
per-ex loss: 0.481969  [   69/   88]
per-ex loss: 0.599470  [   70/   88]
per-ex loss: 0.563778  [   71/   88]
per-ex loss: 0.612727  [   72/   88]
per-ex loss: 0.551140  [   73/   88]
per-ex loss: 0.567483  [   74/   88]
per-ex loss: 0.536643  [   75/   88]
per-ex loss: 0.481223  [   76/   88]
per-ex loss: 0.569234  [   77/   88]
per-ex loss: 0.686522  [   78/   88]
per-ex loss: 0.678589  [   79/   88]
per-ex loss: 0.507296  [   80/   88]
per-ex loss: 0.456259  [   81/   88]
per-ex loss: 0.472165  [   82/   88]
per-ex loss: 0.428196  [   83/   88]
per-ex loss: 0.574255  [   84/   88]
per-ex loss: 0.712723  [   85/   88]
per-ex loss: 0.727018  [   86/   88]
per-ex loss: 0.528303  [   87/   88]
per-ex loss: 0.712862  [   88/   88]
Train Error: Avg loss: 0.58823481
validation Error: 
 Avg loss: 0.68359402 
 F1: 0.504094 
 Precision: 0.616098 
 Recall: 0.426549
 IoU: 0.336982

test Error: 
 Avg loss: 0.64379290 
 F1: 0.562642 
 Precision: 0.665278 
 Recall: 0.487441
 IoU: 0.391442

We have finished training iteration 133
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_131_.pth
per-ex loss: 0.600201  [    1/   88]
per-ex loss: 0.479667  [    2/   88]
per-ex loss: 0.509229  [    3/   88]
per-ex loss: 0.424117  [    4/   88]
per-ex loss: 0.783649  [    5/   88]
per-ex loss: 0.517622  [    6/   88]
per-ex loss: 0.535137  [    7/   88]
per-ex loss: 0.700936  [    8/   88]
per-ex loss: 0.518710  [    9/   88]
per-ex loss: 0.704450  [   10/   88]
per-ex loss: 0.741397  [   11/   88]
per-ex loss: 0.645800  [   12/   88]
per-ex loss: 0.673904  [   13/   88]
per-ex loss: 0.520580  [   14/   88]
per-ex loss: 0.709807  [   15/   88]
per-ex loss: 0.684589  [   16/   88]
per-ex loss: 0.613356  [   17/   88]
per-ex loss: 0.473060  [   18/   88]
per-ex loss: 0.504810  [   19/   88]
per-ex loss: 0.777131  [   20/   88]
per-ex loss: 0.515129  [   21/   88]
per-ex loss: 0.687994  [   22/   88]
per-ex loss: 0.774926  [   23/   88]
per-ex loss: 0.703851  [   24/   88]
per-ex loss: 0.590106  [   25/   88]
per-ex loss: 0.515233  [   26/   88]
per-ex loss: 0.463066  [   27/   88]
per-ex loss: 0.687208  [   28/   88]
per-ex loss: 0.455830  [   29/   88]
per-ex loss: 0.674838  [   30/   88]
per-ex loss: 0.482026  [   31/   88]
per-ex loss: 0.457462  [   32/   88]
per-ex loss: 0.502282  [   33/   88]
per-ex loss: 0.467001  [   34/   88]
per-ex loss: 0.451490  [   35/   88]
per-ex loss: 0.590552  [   36/   88]
per-ex loss: 0.479460  [   37/   88]
per-ex loss: 0.585770  [   38/   88]
per-ex loss: 0.730915  [   39/   88]
per-ex loss: 0.670780  [   40/   88]
per-ex loss: 0.516496  [   41/   88]
per-ex loss: 0.555559  [   42/   88]
per-ex loss: 0.672532  [   43/   88]
per-ex loss: 0.781169  [   44/   88]
per-ex loss: 0.619310  [   45/   88]
per-ex loss: 0.464152  [   46/   88]
per-ex loss: 0.487922  [   47/   88]
per-ex loss: 0.672528  [   48/   88]
per-ex loss: 0.528574  [   49/   88]
per-ex loss: 0.631935  [   50/   88]
per-ex loss: 0.703904  [   51/   88]
per-ex loss: 0.558440  [   52/   88]
per-ex loss: 0.483957  [   53/   88]
per-ex loss: 0.511925  [   54/   88]
per-ex loss: 0.530093  [   55/   88]
per-ex loss: 0.571575  [   56/   88]
per-ex loss: 0.616226  [   57/   88]
per-ex loss: 0.573501  [   58/   88]
per-ex loss: 0.667310  [   59/   88]
per-ex loss: 0.471254  [   60/   88]
per-ex loss: 0.490981  [   61/   88]
per-ex loss: 0.528018  [   62/   88]
per-ex loss: 0.740918  [   63/   88]
per-ex loss: 0.548463  [   64/   88]
per-ex loss: 0.680776  [   65/   88]
per-ex loss: 0.482112  [   66/   88]
per-ex loss: 0.562418  [   67/   88]
per-ex loss: 0.542641  [   68/   88]
per-ex loss: 0.680766  [   69/   88]
per-ex loss: 0.555998  [   70/   88]
per-ex loss: 0.606452  [   71/   88]
per-ex loss: 0.700826  [   72/   88]
per-ex loss: 0.722499  [   73/   88]
per-ex loss: 0.579490  [   74/   88]
per-ex loss: 0.696315  [   75/   88]
per-ex loss: 0.669985  [   76/   88]
per-ex loss: 0.448040  [   77/   88]
per-ex loss: 0.577485  [   78/   88]
per-ex loss: 0.538404  [   79/   88]
per-ex loss: 0.508421  [   80/   88]
per-ex loss: 0.499265  [   81/   88]
per-ex loss: 0.334183  [   82/   88]
per-ex loss: 0.515398  [   83/   88]
per-ex loss: 0.468048  [   84/   88]
per-ex loss: 0.607467  [   85/   88]
per-ex loss: 0.770458  [   86/   88]
per-ex loss: 0.673895  [   87/   88]
per-ex loss: 0.619988  [   88/   88]
Train Error: Avg loss: 0.58604705
validation Error: 
 Avg loss: 0.69204413 
 F1: 0.496159 
 Precision: 0.509909 
 Recall: 0.483131
 IoU: 0.329928

test Error: 
 Avg loss: 0.64147307 
 F1: 0.562344 
 Precision: 0.573533 
 Recall: 0.551584
 IoU: 0.391153

We have finished training iteration 134
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_132_.pth
per-ex loss: 0.538270  [    1/   88]
per-ex loss: 0.544801  [    2/   88]
per-ex loss: 0.408011  [    3/   88]
per-ex loss: 0.465372  [    4/   88]
per-ex loss: 0.451134  [    5/   88]
per-ex loss: 0.505032  [    6/   88]
per-ex loss: 0.523848  [    7/   88]
per-ex loss: 0.597287  [    8/   88]
per-ex loss: 0.700095  [    9/   88]
per-ex loss: 0.435075  [   10/   88]
per-ex loss: 0.791829  [   11/   88]
per-ex loss: 0.697989  [   12/   88]
per-ex loss: 0.667027  [   13/   88]
per-ex loss: 0.680179  [   14/   88]
per-ex loss: 0.684719  [   15/   88]
per-ex loss: 0.511800  [   16/   88]
per-ex loss: 0.592925  [   17/   88]
per-ex loss: 0.536938  [   18/   88]
per-ex loss: 0.513410  [   19/   88]
per-ex loss: 0.698129  [   20/   88]
per-ex loss: 0.570444  [   21/   88]
per-ex loss: 0.574746  [   22/   88]
per-ex loss: 0.474339  [   23/   88]
per-ex loss: 0.574775  [   24/   88]
per-ex loss: 0.482398  [   25/   88]
per-ex loss: 0.676433  [   26/   88]
per-ex loss: 0.777794  [   27/   88]
per-ex loss: 0.482731  [   28/   88]
per-ex loss: 0.449020  [   29/   88]
per-ex loss: 0.692436  [   30/   88]
per-ex loss: 0.773673  [   31/   88]
per-ex loss: 0.515629  [   32/   88]
per-ex loss: 0.458356  [   33/   88]
per-ex loss: 0.734508  [   34/   88]
per-ex loss: 0.585473  [   35/   88]
per-ex loss: 0.668149  [   36/   88]
per-ex loss: 0.709314  [   37/   88]
per-ex loss: 0.705299  [   38/   88]
per-ex loss: 0.592187  [   39/   88]
per-ex loss: 0.479030  [   40/   88]
per-ex loss: 0.642219  [   41/   88]
per-ex loss: 0.564615  [   42/   88]
per-ex loss: 0.585418  [   43/   88]
per-ex loss: 0.665861  [   44/   88]
per-ex loss: 0.626087  [   45/   88]
per-ex loss: 0.666760  [   46/   88]
per-ex loss: 0.452131  [   47/   88]
per-ex loss: 0.598144  [   48/   88]
per-ex loss: 0.599819  [   49/   88]
per-ex loss: 0.732067  [   50/   88]
per-ex loss: 0.622036  [   51/   88]
per-ex loss: 0.713615  [   52/   88]
per-ex loss: 0.539462  [   53/   88]
per-ex loss: 0.513400  [   54/   88]
per-ex loss: 0.470120  [   55/   88]
per-ex loss: 0.551522  [   56/   88]
per-ex loss: 0.437945  [   57/   88]
per-ex loss: 0.558592  [   58/   88]
per-ex loss: 0.436713  [   59/   88]
per-ex loss: 0.499534  [   60/   88]
per-ex loss: 0.631020  [   61/   88]
per-ex loss: 0.523952  [   62/   88]
per-ex loss: 0.465906  [   63/   88]
per-ex loss: 0.672366  [   64/   88]
per-ex loss: 0.750588  [   65/   88]
per-ex loss: 0.524102  [   66/   88]
per-ex loss: 0.688032  [   67/   88]
per-ex loss: 0.567067  [   68/   88]
per-ex loss: 0.721603  [   69/   88]
per-ex loss: 0.447126  [   70/   88]
per-ex loss: 0.343259  [   71/   88]
per-ex loss: 0.529077  [   72/   88]
per-ex loss: 0.535498  [   73/   88]
per-ex loss: 0.706668  [   74/   88]
per-ex loss: 0.489630  [   75/   88]
per-ex loss: 0.773896  [   76/   88]
per-ex loss: 0.732787  [   77/   88]
per-ex loss: 0.620485  [   78/   88]
per-ex loss: 0.778270  [   79/   88]
per-ex loss: 0.610468  [   80/   88]
per-ex loss: 0.544495  [   81/   88]
per-ex loss: 0.682389  [   82/   88]
per-ex loss: 0.513640  [   83/   88]
per-ex loss: 0.501566  [   84/   88]
per-ex loss: 0.755543  [   85/   88]
per-ex loss: 0.549692  [   86/   88]
per-ex loss: 0.510444  [   87/   88]
per-ex loss: 0.501901  [   88/   88]
Train Error: Avg loss: 0.58713791
validation Error: 
 Avg loss: 0.68684781 
 F1: 0.501263 
 Precision: 0.533334 
 Recall: 0.472831
 IoU: 0.334457

test Error: 
 Avg loss: 0.64559120 
 F1: 0.557585 
 Precision: 0.576191 
 Recall: 0.540143
 IoU: 0.386564

We have finished training iteration 135
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_133_.pth
per-ex loss: 0.435258  [    1/   88]
per-ex loss: 0.542891  [    2/   88]
per-ex loss: 0.752169  [    3/   88]
per-ex loss: 0.573749  [    4/   88]
per-ex loss: 0.487355  [    5/   88]
per-ex loss: 0.660365  [    6/   88]
per-ex loss: 0.790237  [    7/   88]
per-ex loss: 0.475531  [    8/   88]
per-ex loss: 0.732816  [    9/   88]
per-ex loss: 0.554770  [   10/   88]
per-ex loss: 0.585479  [   11/   88]
per-ex loss: 0.529328  [   12/   88]
per-ex loss: 0.472153  [   13/   88]
per-ex loss: 0.677685  [   14/   88]
per-ex loss: 0.702256  [   15/   88]
per-ex loss: 0.461899  [   16/   88]
per-ex loss: 0.678176  [   17/   88]
per-ex loss: 0.461622  [   18/   88]
per-ex loss: 0.521382  [   19/   88]
per-ex loss: 0.691462  [   20/   88]
per-ex loss: 0.493625  [   21/   88]
per-ex loss: 0.520278  [   22/   88]
per-ex loss: 0.764832  [   23/   88]
per-ex loss: 0.507254  [   24/   88]
per-ex loss: 0.536716  [   25/   88]
per-ex loss: 0.569823  [   26/   88]
per-ex loss: 0.469235  [   27/   88]
per-ex loss: 0.426929  [   28/   88]
per-ex loss: 0.536379  [   29/   88]
per-ex loss: 0.506769  [   30/   88]
per-ex loss: 0.455845  [   31/   88]
per-ex loss: 0.678712  [   32/   88]
per-ex loss: 0.670455  [   33/   88]
per-ex loss: 0.583321  [   34/   88]
per-ex loss: 0.666018  [   35/   88]
per-ex loss: 0.779371  [   36/   88]
per-ex loss: 0.531479  [   37/   88]
per-ex loss: 0.711937  [   38/   88]
per-ex loss: 0.711037  [   39/   88]
per-ex loss: 0.477375  [   40/   88]
per-ex loss: 0.487599  [   41/   88]
per-ex loss: 0.746072  [   42/   88]
per-ex loss: 0.665292  [   43/   88]
per-ex loss: 0.437018  [   44/   88]
per-ex loss: 0.585498  [   45/   88]
per-ex loss: 0.498454  [   46/   88]
per-ex loss: 0.548622  [   47/   88]
per-ex loss: 0.539565  [   48/   88]
per-ex loss: 0.756679  [   49/   88]
per-ex loss: 0.540398  [   50/   88]
per-ex loss: 0.531744  [   51/   88]
per-ex loss: 0.698072  [   52/   88]
per-ex loss: 0.684577  [   53/   88]
per-ex loss: 0.785361  [   54/   88]
per-ex loss: 0.604938  [   55/   88]
per-ex loss: 0.732637  [   56/   88]
per-ex loss: 0.442307  [   57/   88]
per-ex loss: 0.533393  [   58/   88]
per-ex loss: 0.519567  [   59/   88]
per-ex loss: 0.641866  [   60/   88]
per-ex loss: 0.601130  [   61/   88]
per-ex loss: 0.625690  [   62/   88]
per-ex loss: 0.513774  [   63/   88]
per-ex loss: 0.583493  [   64/   88]
per-ex loss: 0.585508  [   65/   88]
per-ex loss: 0.699147  [   66/   88]
per-ex loss: 0.465271  [   67/   88]
per-ex loss: 0.618833  [   68/   88]
per-ex loss: 0.693476  [   69/   88]
per-ex loss: 0.667198  [   70/   88]
per-ex loss: 0.469003  [   71/   88]
per-ex loss: 0.605980  [   72/   88]
per-ex loss: 0.718087  [   73/   88]
per-ex loss: 0.676254  [   74/   88]
per-ex loss: 0.730537  [   75/   88]
per-ex loss: 0.572571  [   76/   88]
per-ex loss: 0.733153  [   77/   88]
per-ex loss: 0.521554  [   78/   88]
per-ex loss: 0.615429  [   79/   88]
per-ex loss: 0.663647  [   80/   88]
per-ex loss: 0.481196  [   81/   88]
per-ex loss: 0.350167  [   82/   88]
per-ex loss: 0.431657  [   83/   88]
per-ex loss: 0.552857  [   84/   88]
per-ex loss: 0.567530  [   85/   88]
per-ex loss: 0.493361  [   86/   88]
per-ex loss: 0.698541  [   87/   88]
per-ex loss: 0.506370  [   88/   88]
Train Error: Avg loss: 0.58869373
validation Error: 
 Avg loss: 0.69129268 
 F1: 0.495537 
 Precision: 0.606406 
 Recall: 0.418941
 IoU: 0.329378

test Error: 
 Avg loss: 0.64429689 
 F1: 0.560605 
 Precision: 0.653495 
 Recall: 0.490836
 IoU: 0.389473

We have finished training iteration 136
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_134_.pth
per-ex loss: 0.710676  [    1/   88]
per-ex loss: 0.388777  [    2/   88]
per-ex loss: 0.604519  [    3/   88]
per-ex loss: 0.590862  [    4/   88]
per-ex loss: 0.682319  [    5/   88]
per-ex loss: 0.427444  [    6/   88]
per-ex loss: 0.464825  [    7/   88]
per-ex loss: 0.532632  [    8/   88]
per-ex loss: 0.571981  [    9/   88]
per-ex loss: 0.775246  [   10/   88]
per-ex loss: 0.564665  [   11/   88]
per-ex loss: 0.714967  [   12/   88]
per-ex loss: 0.485965  [   13/   88]
per-ex loss: 0.605544  [   14/   88]
per-ex loss: 0.691556  [   15/   88]
per-ex loss: 0.681016  [   16/   88]
per-ex loss: 0.606797  [   17/   88]
per-ex loss: 0.509299  [   18/   88]
per-ex loss: 0.708781  [   19/   88]
per-ex loss: 0.512710  [   20/   88]
per-ex loss: 0.545402  [   21/   88]
per-ex loss: 0.703300  [   22/   88]
per-ex loss: 0.446145  [   23/   88]
per-ex loss: 0.490417  [   24/   88]
per-ex loss: 0.484091  [   25/   88]
per-ex loss: 0.553405  [   26/   88]
per-ex loss: 0.557591  [   27/   88]
per-ex loss: 0.714614  [   28/   88]
per-ex loss: 0.483647  [   29/   88]
per-ex loss: 0.629369  [   30/   88]
per-ex loss: 0.712565  [   31/   88]
per-ex loss: 0.664528  [   32/   88]
per-ex loss: 0.608528  [   33/   88]
per-ex loss: 0.595725  [   34/   88]
per-ex loss: 0.501739  [   35/   88]
per-ex loss: 0.481542  [   36/   88]
per-ex loss: 0.726470  [   37/   88]
per-ex loss: 0.651432  [   38/   88]
per-ex loss: 0.576797  [   39/   88]
per-ex loss: 0.720382  [   40/   88]
per-ex loss: 0.452569  [   41/   88]
per-ex loss: 0.489439  [   42/   88]
per-ex loss: 0.542483  [   43/   88]
per-ex loss: 0.567517  [   44/   88]
per-ex loss: 0.545371  [   45/   88]
per-ex loss: 0.473945  [   46/   88]
per-ex loss: 0.597787  [   47/   88]
per-ex loss: 0.704799  [   48/   88]
per-ex loss: 0.705190  [   49/   88]
per-ex loss: 0.819562  [   50/   88]
per-ex loss: 0.559253  [   51/   88]
per-ex loss: 0.510293  [   52/   88]
per-ex loss: 0.676237  [   53/   88]
per-ex loss: 0.496650  [   54/   88]
per-ex loss: 0.706888  [   55/   88]
per-ex loss: 0.759846  [   56/   88]
per-ex loss: 0.450131  [   57/   88]
per-ex loss: 0.668070  [   58/   88]
per-ex loss: 0.500416  [   59/   88]
per-ex loss: 0.645618  [   60/   88]
per-ex loss: 0.647739  [   61/   88]
per-ex loss: 0.776660  [   62/   88]
per-ex loss: 0.618172  [   63/   88]
per-ex loss: 0.527126  [   64/   88]
per-ex loss: 0.490726  [   65/   88]
per-ex loss: 0.777025  [   66/   88]
per-ex loss: 0.511928  [   67/   88]
per-ex loss: 0.757377  [   68/   88]
per-ex loss: 0.724654  [   69/   88]
per-ex loss: 0.770811  [   70/   88]
per-ex loss: 0.609302  [   71/   88]
per-ex loss: 0.434668  [   72/   88]
per-ex loss: 0.530673  [   73/   88]
per-ex loss: 0.704386  [   74/   88]
per-ex loss: 0.500673  [   75/   88]
per-ex loss: 0.586552  [   76/   88]
per-ex loss: 0.684928  [   77/   88]
per-ex loss: 0.509026  [   78/   88]
per-ex loss: 0.685779  [   79/   88]
per-ex loss: 0.479223  [   80/   88]
per-ex loss: 0.580776  [   81/   88]
per-ex loss: 0.556674  [   82/   88]
per-ex loss: 0.625060  [   83/   88]
per-ex loss: 0.463656  [   84/   88]
per-ex loss: 0.592067  [   85/   88]
per-ex loss: 0.453122  [   86/   88]
per-ex loss: 0.630918  [   87/   88]
per-ex loss: 0.561796  [   88/   88]
Train Error: Avg loss: 0.59479360
validation Error: 
 Avg loss: 0.68645784 
 F1: 0.502026 
 Precision: 0.575370 
 Recall: 0.445267
 IoU: 0.335137

test Error: 
 Avg loss: 0.63772300 
 F1: 0.570909 
 Precision: 0.629652 
 Recall: 0.522192
 IoU: 0.399491

We have finished training iteration 137
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_135_.pth
per-ex loss: 0.707491  [    1/   88]
per-ex loss: 0.536086  [    2/   88]
per-ex loss: 0.525740  [    3/   88]
per-ex loss: 0.694690  [    4/   88]
per-ex loss: 0.617011  [    5/   88]
per-ex loss: 0.555201  [    6/   88]
per-ex loss: 0.685759  [    7/   88]
per-ex loss: 0.553456  [    8/   88]
per-ex loss: 0.683946  [    9/   88]
per-ex loss: 0.716347  [   10/   88]
per-ex loss: 0.685888  [   11/   88]
per-ex loss: 0.670733  [   12/   88]
per-ex loss: 0.469979  [   13/   88]
per-ex loss: 0.606738  [   14/   88]
per-ex loss: 0.601550  [   15/   88]
per-ex loss: 0.503738  [   16/   88]
per-ex loss: 0.541725  [   17/   88]
per-ex loss: 0.667081  [   18/   88]
per-ex loss: 0.576355  [   19/   88]
per-ex loss: 0.657818  [   20/   88]
per-ex loss: 0.708967  [   21/   88]
per-ex loss: 0.511742  [   22/   88]
per-ex loss: 0.679630  [   23/   88]
per-ex loss: 0.548374  [   24/   88]
per-ex loss: 0.570763  [   25/   88]
per-ex loss: 0.358617  [   26/   88]
per-ex loss: 0.572644  [   27/   88]
per-ex loss: 0.599779  [   28/   88]
per-ex loss: 0.576623  [   29/   88]
per-ex loss: 0.482473  [   30/   88]
per-ex loss: 0.547017  [   31/   88]
per-ex loss: 0.505150  [   32/   88]
per-ex loss: 0.481417  [   33/   88]
per-ex loss: 0.700362  [   34/   88]
per-ex loss: 0.510855  [   35/   88]
per-ex loss: 0.678223  [   36/   88]
per-ex loss: 0.444570  [   37/   88]
per-ex loss: 0.458879  [   38/   88]
per-ex loss: 0.546890  [   39/   88]
per-ex loss: 0.562391  [   40/   88]
per-ex loss: 0.492755  [   41/   88]
per-ex loss: 0.501658  [   42/   88]
per-ex loss: 0.477789  [   43/   88]
per-ex loss: 0.552159  [   44/   88]
per-ex loss: 0.470512  [   45/   88]
per-ex loss: 0.549008  [   46/   88]
per-ex loss: 0.619623  [   47/   88]
per-ex loss: 0.456660  [   48/   88]
per-ex loss: 0.493959  [   49/   88]
per-ex loss: 0.757108  [   50/   88]
per-ex loss: 0.603903  [   51/   88]
per-ex loss: 0.483695  [   52/   88]
per-ex loss: 0.649488  [   53/   88]
per-ex loss: 0.532556  [   54/   88]
per-ex loss: 0.538372  [   55/   88]
per-ex loss: 0.475800  [   56/   88]
per-ex loss: 0.770737  [   57/   88]
per-ex loss: 0.544554  [   58/   88]
per-ex loss: 0.745486  [   59/   88]
per-ex loss: 0.490643  [   60/   88]
per-ex loss: 0.442057  [   61/   88]
per-ex loss: 0.425988  [   62/   88]
per-ex loss: 0.606676  [   63/   88]
per-ex loss: 0.417920  [   64/   88]
per-ex loss: 0.532680  [   65/   88]
per-ex loss: 0.688551  [   66/   88]
per-ex loss: 0.590994  [   67/   88]
per-ex loss: 0.654856  [   68/   88]
per-ex loss: 0.596550  [   69/   88]
per-ex loss: 0.714090  [   70/   88]
per-ex loss: 0.690026  [   71/   88]
per-ex loss: 0.774014  [   72/   88]
per-ex loss: 0.495748  [   73/   88]
per-ex loss: 0.503848  [   74/   88]
per-ex loss: 0.472037  [   75/   88]
per-ex loss: 0.473014  [   76/   88]
per-ex loss: 0.462235  [   77/   88]
per-ex loss: 0.609020  [   78/   88]
per-ex loss: 0.711619  [   79/   88]
per-ex loss: 0.531511  [   80/   88]
per-ex loss: 0.742615  [   81/   88]
per-ex loss: 0.723290  [   82/   88]
per-ex loss: 0.700332  [   83/   88]
per-ex loss: 0.782597  [   84/   88]
per-ex loss: 0.491792  [   85/   88]
per-ex loss: 0.751121  [   86/   88]
per-ex loss: 0.763250  [   87/   88]
per-ex loss: 0.718023  [   88/   88]
Train Error: Avg loss: 0.58608650
validation Error: 
 Avg loss: 0.68817848 
 F1: 0.498772 
 Precision: 0.572573 
 Recall: 0.441823
 IoU: 0.332242

test Error: 
 Avg loss: 0.63904482 
 F1: 0.571769 
 Precision: 0.626400 
 Recall: 0.525903
 IoU: 0.400334

We have finished training iteration 138
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_136_.pth
per-ex loss: 0.764728  [    1/   88]
per-ex loss: 0.589720  [    2/   88]
per-ex loss: 0.554550  [    3/   88]
per-ex loss: 0.549710  [    4/   88]
per-ex loss: 0.566171  [    5/   88]
per-ex loss: 0.553194  [    6/   88]
per-ex loss: 0.684850  [    7/   88]
per-ex loss: 0.509359  [    8/   88]
per-ex loss: 0.685743  [    9/   88]
per-ex loss: 0.477866  [   10/   88]
per-ex loss: 0.633584  [   11/   88]
per-ex loss: 0.559525  [   12/   88]
per-ex loss: 0.466164  [   13/   88]
per-ex loss: 0.712696  [   14/   88]
per-ex loss: 0.609431  [   15/   88]
per-ex loss: 0.445259  [   16/   88]
per-ex loss: 0.763144  [   17/   88]
per-ex loss: 0.433219  [   18/   88]
per-ex loss: 0.459013  [   19/   88]
per-ex loss: 0.654922  [   20/   88]
per-ex loss: 0.544679  [   21/   88]
per-ex loss: 0.743797  [   22/   88]
per-ex loss: 0.752755  [   23/   88]
per-ex loss: 0.584981  [   24/   88]
per-ex loss: 0.499954  [   25/   88]
per-ex loss: 0.520726  [   26/   88]
per-ex loss: 0.682548  [   27/   88]
per-ex loss: 0.453793  [   28/   88]
per-ex loss: 0.691749  [   29/   88]
per-ex loss: 0.688214  [   30/   88]
per-ex loss: 0.791482  [   31/   88]
per-ex loss: 0.598276  [   32/   88]
per-ex loss: 0.527384  [   33/   88]
per-ex loss: 0.571612  [   34/   88]
per-ex loss: 0.476842  [   35/   88]
per-ex loss: 0.713952  [   36/   88]
per-ex loss: 0.524956  [   37/   88]
per-ex loss: 0.741911  [   38/   88]
per-ex loss: 0.773059  [   39/   88]
per-ex loss: 0.528832  [   40/   88]
per-ex loss: 0.498628  [   41/   88]
per-ex loss: 0.611995  [   42/   88]
per-ex loss: 0.547783  [   43/   88]
per-ex loss: 0.565950  [   44/   88]
per-ex loss: 0.680356  [   45/   88]
per-ex loss: 0.721166  [   46/   88]
per-ex loss: 0.356963  [   47/   88]
per-ex loss: 0.524981  [   48/   88]
per-ex loss: 0.473961  [   49/   88]
per-ex loss: 0.708210  [   50/   88]
per-ex loss: 0.507788  [   51/   88]
per-ex loss: 0.525297  [   52/   88]
per-ex loss: 0.468828  [   53/   88]
per-ex loss: 0.434481  [   54/   88]
per-ex loss: 0.698392  [   55/   88]
per-ex loss: 0.715981  [   56/   88]
per-ex loss: 0.447291  [   57/   88]
per-ex loss: 0.533985  [   58/   88]
per-ex loss: 0.547690  [   59/   88]
per-ex loss: 0.704896  [   60/   88]
per-ex loss: 0.717940  [   61/   88]
per-ex loss: 0.541340  [   62/   88]
per-ex loss: 0.503460  [   63/   88]
per-ex loss: 0.574702  [   64/   88]
per-ex loss: 0.586429  [   65/   88]
per-ex loss: 0.465042  [   66/   88]
per-ex loss: 0.434259  [   67/   88]
per-ex loss: 0.525502  [   68/   88]
per-ex loss: 0.694332  [   69/   88]
per-ex loss: 0.701305  [   70/   88]
per-ex loss: 0.480802  [   71/   88]
per-ex loss: 0.583063  [   72/   88]
per-ex loss: 0.656316  [   73/   88]
per-ex loss: 0.591219  [   74/   88]
per-ex loss: 0.616161  [   75/   88]
per-ex loss: 0.583827  [   76/   88]
per-ex loss: 0.418538  [   77/   88]
per-ex loss: 0.518206  [   78/   88]
per-ex loss: 0.773039  [   79/   88]
per-ex loss: 0.670441  [   80/   88]
per-ex loss: 0.480363  [   81/   88]
per-ex loss: 0.698515  [   82/   88]
per-ex loss: 0.606043  [   83/   88]
per-ex loss: 0.458251  [   84/   88]
per-ex loss: 0.678928  [   85/   88]
per-ex loss: 0.535595  [   86/   88]
per-ex loss: 0.724956  [   87/   88]
per-ex loss: 0.609104  [   88/   88]
Train Error: Avg loss: 0.58848470
validation Error: 
 Avg loss: 0.68523172 
 F1: 0.502464 
 Precision: 0.524198 
 Recall: 0.482461
 IoU: 0.335527

test Error: 
 Avg loss: 0.64436325 
 F1: 0.562746 
 Precision: 0.557029 
 Recall: 0.568582
 IoU: 0.391543

We have finished training iteration 139
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_137_.pth
per-ex loss: 0.468040  [    1/   88]
per-ex loss: 0.731323  [    2/   88]
per-ex loss: 0.751970  [    3/   88]
per-ex loss: 0.578572  [    4/   88]
per-ex loss: 0.587221  [    5/   88]
per-ex loss: 0.768598  [    6/   88]
per-ex loss: 0.695381  [    7/   88]
per-ex loss: 0.703652  [    8/   88]
per-ex loss: 0.675903  [    9/   88]
per-ex loss: 0.518864  [   10/   88]
per-ex loss: 0.453671  [   11/   88]
per-ex loss: 0.738557  [   12/   88]
per-ex loss: 0.554902  [   13/   88]
per-ex loss: 0.538090  [   14/   88]
per-ex loss: 0.492368  [   15/   88]
per-ex loss: 0.700224  [   16/   88]
per-ex loss: 0.590717  [   17/   88]
per-ex loss: 0.504186  [   18/   88]
per-ex loss: 0.532542  [   19/   88]
per-ex loss: 0.586582  [   20/   88]
per-ex loss: 0.460083  [   21/   88]
per-ex loss: 0.610150  [   22/   88]
per-ex loss: 0.709104  [   23/   88]
per-ex loss: 0.677746  [   24/   88]
per-ex loss: 0.615158  [   25/   88]
per-ex loss: 0.449811  [   26/   88]
per-ex loss: 0.519052  [   27/   88]
per-ex loss: 0.435925  [   28/   88]
per-ex loss: 0.786759  [   29/   88]
per-ex loss: 0.699282  [   30/   88]
per-ex loss: 0.580392  [   31/   88]
per-ex loss: 0.486074  [   32/   88]
per-ex loss: 0.489907  [   33/   88]
per-ex loss: 0.711806  [   34/   88]
per-ex loss: 0.581217  [   35/   88]
per-ex loss: 0.488963  [   36/   88]
per-ex loss: 0.370747  [   37/   88]
per-ex loss: 0.525526  [   38/   88]
per-ex loss: 0.460805  [   39/   88]
per-ex loss: 0.615753  [   40/   88]
per-ex loss: 0.537861  [   41/   88]
per-ex loss: 0.404376  [   42/   88]
per-ex loss: 0.654508  [   43/   88]
per-ex loss: 0.781395  [   44/   88]
per-ex loss: 0.655936  [   45/   88]
per-ex loss: 0.670792  [   46/   88]
per-ex loss: 0.706821  [   47/   88]
per-ex loss: 0.661613  [   48/   88]
per-ex loss: 0.448987  [   49/   88]
per-ex loss: 0.519378  [   50/   88]
per-ex loss: 0.484120  [   51/   88]
per-ex loss: 0.769097  [   52/   88]
per-ex loss: 0.461176  [   53/   88]
per-ex loss: 0.677727  [   54/   88]
per-ex loss: 0.522229  [   55/   88]
per-ex loss: 0.709993  [   56/   88]
per-ex loss: 0.470371  [   57/   88]
per-ex loss: 0.440816  [   58/   88]
per-ex loss: 0.471422  [   59/   88]
per-ex loss: 0.546733  [   60/   88]
per-ex loss: 0.780583  [   61/   88]
per-ex loss: 0.507689  [   62/   88]
per-ex loss: 0.588410  [   63/   88]
per-ex loss: 0.415204  [   64/   88]
per-ex loss: 0.500542  [   65/   88]
per-ex loss: 0.697517  [   66/   88]
per-ex loss: 0.488176  [   67/   88]
per-ex loss: 0.642939  [   68/   88]
per-ex loss: 0.546309  [   69/   88]
per-ex loss: 0.690173  [   70/   88]
per-ex loss: 0.624545  [   71/   88]
per-ex loss: 0.570411  [   72/   88]
per-ex loss: 0.538905  [   73/   88]
per-ex loss: 0.473802  [   74/   88]
per-ex loss: 0.718598  [   75/   88]
per-ex loss: 0.679083  [   76/   88]
per-ex loss: 0.526992  [   77/   88]
per-ex loss: 0.754467  [   78/   88]
per-ex loss: 0.514775  [   79/   88]
per-ex loss: 0.583258  [   80/   88]
per-ex loss: 0.597973  [   81/   88]
per-ex loss: 0.521474  [   82/   88]
per-ex loss: 0.559189  [   83/   88]
per-ex loss: 0.571009  [   84/   88]
per-ex loss: 0.568716  [   85/   88]
per-ex loss: 0.625260  [   86/   88]
per-ex loss: 0.657682  [   87/   88]
per-ex loss: 0.515347  [   88/   88]
Train Error: Avg loss: 0.58522731
validation Error: 
 Avg loss: 0.68766710 
 F1: 0.501487 
 Precision: 0.516629 
 Recall: 0.487206
 IoU: 0.334656

test Error: 
 Avg loss: 0.64087657 
 F1: 0.564064 
 Precision: 0.565393 
 Recall: 0.562742
 IoU: 0.392820

We have finished training iteration 140
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_138_.pth
per-ex loss: 0.606086  [    1/   88]
per-ex loss: 0.465740  [    2/   88]
per-ex loss: 0.565932  [    3/   88]
per-ex loss: 0.498604  [    4/   88]
per-ex loss: 0.708984  [    5/   88]
per-ex loss: 0.700040  [    6/   88]
per-ex loss: 0.651426  [    7/   88]
per-ex loss: 0.530275  [    8/   88]
per-ex loss: 0.663352  [    9/   88]
per-ex loss: 0.426607  [   10/   88]
per-ex loss: 0.609201  [   11/   88]
per-ex loss: 0.490820  [   12/   88]
per-ex loss: 0.495483  [   13/   88]
per-ex loss: 0.515353  [   14/   88]
per-ex loss: 0.535463  [   15/   88]
per-ex loss: 0.689195  [   16/   88]
per-ex loss: 0.347424  [   17/   88]
per-ex loss: 0.680391  [   18/   88]
per-ex loss: 0.467329  [   19/   88]
per-ex loss: 0.523698  [   20/   88]
per-ex loss: 0.513083  [   21/   88]
per-ex loss: 0.472139  [   22/   88]
per-ex loss: 0.513468  [   23/   88]
per-ex loss: 0.784075  [   24/   88]
per-ex loss: 0.675436  [   25/   88]
per-ex loss: 0.753907  [   26/   88]
per-ex loss: 0.518503  [   27/   88]
per-ex loss: 0.767543  [   28/   88]
per-ex loss: 0.555799  [   29/   88]
per-ex loss: 0.392716  [   30/   88]
per-ex loss: 0.576505  [   31/   88]
per-ex loss: 0.625770  [   32/   88]
per-ex loss: 0.472421  [   33/   88]
per-ex loss: 0.432149  [   34/   88]
per-ex loss: 0.451913  [   35/   88]
per-ex loss: 0.528630  [   36/   88]
per-ex loss: 0.707958  [   37/   88]
per-ex loss: 0.545499  [   38/   88]
per-ex loss: 0.464859  [   39/   88]
per-ex loss: 0.558128  [   40/   88]
per-ex loss: 0.553256  [   41/   88]
per-ex loss: 0.732566  [   42/   88]
per-ex loss: 0.612491  [   43/   88]
per-ex loss: 0.506729  [   44/   88]
per-ex loss: 0.710208  [   45/   88]
per-ex loss: 0.487816  [   46/   88]
per-ex loss: 0.566452  [   47/   88]
per-ex loss: 0.640893  [   48/   88]
per-ex loss: 0.690834  [   49/   88]
per-ex loss: 0.631288  [   50/   88]
per-ex loss: 0.523757  [   51/   88]
per-ex loss: 0.615353  [   52/   88]
per-ex loss: 0.630187  [   53/   88]
per-ex loss: 0.458791  [   54/   88]
per-ex loss: 0.536447  [   55/   88]
per-ex loss: 0.516653  [   56/   88]
per-ex loss: 0.616322  [   57/   88]
per-ex loss: 0.579911  [   58/   88]
per-ex loss: 0.629117  [   59/   88]
per-ex loss: 0.440353  [   60/   88]
per-ex loss: 0.548925  [   61/   88]
per-ex loss: 0.708886  [   62/   88]
per-ex loss: 0.644090  [   63/   88]
per-ex loss: 0.667479  [   64/   88]
per-ex loss: 0.540042  [   65/   88]
per-ex loss: 0.447550  [   66/   88]
per-ex loss: 0.756475  [   67/   88]
per-ex loss: 0.482563  [   68/   88]
per-ex loss: 0.532924  [   69/   88]
per-ex loss: 0.486417  [   70/   88]
per-ex loss: 0.552417  [   71/   88]
per-ex loss: 0.446795  [   72/   88]
per-ex loss: 0.573055  [   73/   88]
per-ex loss: 0.679563  [   74/   88]
per-ex loss: 0.742730  [   75/   88]
per-ex loss: 0.693269  [   76/   88]
per-ex loss: 0.540582  [   77/   88]
per-ex loss: 0.737451  [   78/   88]
per-ex loss: 0.719400  [   79/   88]
per-ex loss: 0.722371  [   80/   88]
per-ex loss: 0.646906  [   81/   88]
per-ex loss: 0.758363  [   82/   88]
per-ex loss: 0.676051  [   83/   88]
per-ex loss: 0.755912  [   84/   88]
per-ex loss: 0.593577  [   85/   88]
per-ex loss: 0.453862  [   86/   88]
per-ex loss: 0.729545  [   87/   88]
per-ex loss: 0.498167  [   88/   88]
Train Error: Avg loss: 0.58518975
validation Error: 
 Avg loss: 0.68718485 
 F1: 0.502734 
 Precision: 0.542589 
 Recall: 0.468333
 IoU: 0.335768

test Error: 
 Avg loss: 0.63181148 
 F1: 0.575587 
 Precision: 0.598988 
 Recall: 0.553946
 IoU: 0.404087

We have finished training iteration 141
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_139_.pth
per-ex loss: 0.610250  [    1/   88]
per-ex loss: 0.622497  [    2/   88]
per-ex loss: 0.706426  [    3/   88]
per-ex loss: 0.333897  [    4/   88]
per-ex loss: 0.696093  [    5/   88]
per-ex loss: 0.673992  [    6/   88]
per-ex loss: 0.522227  [    7/   88]
per-ex loss: 0.465780  [    8/   88]
per-ex loss: 0.561928  [    9/   88]
per-ex loss: 0.437495  [   10/   88]
per-ex loss: 0.431318  [   11/   88]
per-ex loss: 0.521101  [   12/   88]
per-ex loss: 0.565706  [   13/   88]
per-ex loss: 0.705484  [   14/   88]
per-ex loss: 0.764394  [   15/   88]
per-ex loss: 0.508925  [   16/   88]
per-ex loss: 0.500375  [   17/   88]
per-ex loss: 0.699288  [   18/   88]
per-ex loss: 0.441848  [   19/   88]
per-ex loss: 0.585836  [   20/   88]
per-ex loss: 0.524266  [   21/   88]
per-ex loss: 0.700860  [   22/   88]
per-ex loss: 0.606425  [   23/   88]
per-ex loss: 0.486977  [   24/   88]
per-ex loss: 0.518906  [   25/   88]
per-ex loss: 0.658911  [   26/   88]
per-ex loss: 0.625801  [   27/   88]
per-ex loss: 0.780300  [   28/   88]
per-ex loss: 0.569033  [   29/   88]
per-ex loss: 0.648663  [   30/   88]
per-ex loss: 0.564174  [   31/   88]
per-ex loss: 0.479442  [   32/   88]
per-ex loss: 0.515625  [   33/   88]
per-ex loss: 0.546284  [   34/   88]
per-ex loss: 0.700525  [   35/   88]
per-ex loss: 0.685013  [   36/   88]
per-ex loss: 0.414499  [   37/   88]
per-ex loss: 0.473386  [   38/   88]
per-ex loss: 0.647308  [   39/   88]
per-ex loss: 0.733696  [   40/   88]
per-ex loss: 0.490834  [   41/   88]
per-ex loss: 0.772391  [   42/   88]
per-ex loss: 0.642255  [   43/   88]
per-ex loss: 0.505754  [   44/   88]
per-ex loss: 0.553116  [   45/   88]
per-ex loss: 0.602982  [   46/   88]
per-ex loss: 0.719638  [   47/   88]
per-ex loss: 0.710342  [   48/   88]
per-ex loss: 0.463908  [   49/   88]
per-ex loss: 0.688200  [   50/   88]
per-ex loss: 0.527558  [   51/   88]
per-ex loss: 0.468629  [   52/   88]
per-ex loss: 0.664459  [   53/   88]
per-ex loss: 0.792064  [   54/   88]
per-ex loss: 0.629249  [   55/   88]
per-ex loss: 0.694123  [   56/   88]
per-ex loss: 0.508344  [   57/   88]
per-ex loss: 0.578435  [   58/   88]
per-ex loss: 0.505283  [   59/   88]
per-ex loss: 0.463004  [   60/   88]
per-ex loss: 0.480280  [   61/   88]
per-ex loss: 0.668432  [   62/   88]
per-ex loss: 0.571005  [   63/   88]
per-ex loss: 0.730267  [   64/   88]
per-ex loss: 0.490617  [   65/   88]
per-ex loss: 0.597919  [   66/   88]
per-ex loss: 0.709387  [   67/   88]
per-ex loss: 0.550872  [   68/   88]
per-ex loss: 0.539734  [   69/   88]
per-ex loss: 0.472729  [   70/   88]
per-ex loss: 0.511433  [   71/   88]
per-ex loss: 0.507360  [   72/   88]
per-ex loss: 0.519591  [   73/   88]
per-ex loss: 0.482048  [   74/   88]
per-ex loss: 0.746611  [   75/   88]
per-ex loss: 0.550669  [   76/   88]
per-ex loss: 0.709798  [   77/   88]
per-ex loss: 0.661597  [   78/   88]
per-ex loss: 0.762980  [   79/   88]
per-ex loss: 0.644013  [   80/   88]
per-ex loss: 0.518739  [   81/   88]
per-ex loss: 0.578090  [   82/   88]
per-ex loss: 0.568161  [   83/   88]
per-ex loss: 0.498609  [   84/   88]
per-ex loss: 0.676519  [   85/   88]
per-ex loss: 0.706488  [   86/   88]
per-ex loss: 0.436993  [   87/   88]
per-ex loss: 0.464827  [   88/   88]
Train Error: Avg loss: 0.58603735
validation Error: 
 Avg loss: 0.67969824 
 F1: 0.507995 
 Precision: 0.541024 
 Recall: 0.478766
 IoU: 0.340478

test Error: 
 Avg loss: 0.63584646 
 F1: 0.569861 
 Precision: 0.582850 
 Recall: 0.557439
 IoU: 0.398466

We have finished training iteration 142
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_140_.pth
per-ex loss: 0.586447  [    1/   88]
per-ex loss: 0.644505  [    2/   88]
per-ex loss: 0.755084  [    3/   88]
per-ex loss: 0.765929  [    4/   88]
per-ex loss: 0.474829  [    5/   88]
per-ex loss: 0.677478  [    6/   88]
per-ex loss: 0.531091  [    7/   88]
per-ex loss: 0.544284  [    8/   88]
per-ex loss: 0.455848  [    9/   88]
per-ex loss: 0.557724  [   10/   88]
per-ex loss: 0.691986  [   11/   88]
per-ex loss: 0.567869  [   12/   88]
per-ex loss: 0.570932  [   13/   88]
per-ex loss: 0.651384  [   14/   88]
per-ex loss: 0.616704  [   15/   88]
per-ex loss: 0.476631  [   16/   88]
per-ex loss: 0.490444  [   17/   88]
per-ex loss: 0.689798  [   18/   88]
per-ex loss: 0.659747  [   19/   88]
per-ex loss: 0.451998  [   20/   88]
per-ex loss: 0.496008  [   21/   88]
per-ex loss: 0.604089  [   22/   88]
per-ex loss: 0.573727  [   23/   88]
per-ex loss: 0.696949  [   24/   88]
per-ex loss: 0.638727  [   25/   88]
per-ex loss: 0.518383  [   26/   88]
per-ex loss: 0.759348  [   27/   88]
per-ex loss: 0.684652  [   28/   88]
per-ex loss: 0.696943  [   29/   88]
per-ex loss: 0.478152  [   30/   88]
per-ex loss: 0.522480  [   31/   88]
per-ex loss: 0.524254  [   32/   88]
per-ex loss: 0.696655  [   33/   88]
per-ex loss: 0.420631  [   34/   88]
per-ex loss: 0.577111  [   35/   88]
per-ex loss: 0.541775  [   36/   88]
per-ex loss: 0.762922  [   37/   88]
per-ex loss: 0.601582  [   38/   88]
per-ex loss: 0.459817  [   39/   88]
per-ex loss: 0.605434  [   40/   88]
per-ex loss: 0.538084  [   41/   88]
per-ex loss: 0.498922  [   42/   88]
per-ex loss: 0.359699  [   43/   88]
per-ex loss: 0.554224  [   44/   88]
per-ex loss: 0.713791  [   45/   88]
per-ex loss: 0.678938  [   46/   88]
per-ex loss: 0.545078  [   47/   88]
per-ex loss: 0.491393  [   48/   88]
per-ex loss: 0.443383  [   49/   88]
per-ex loss: 0.496865  [   50/   88]
per-ex loss: 0.497558  [   51/   88]
per-ex loss: 0.431965  [   52/   88]
per-ex loss: 0.717986  [   53/   88]
per-ex loss: 0.586123  [   54/   88]
per-ex loss: 0.783458  [   55/   88]
per-ex loss: 0.702824  [   56/   88]
per-ex loss: 0.464067  [   57/   88]
per-ex loss: 0.465759  [   58/   88]
per-ex loss: 0.455558  [   59/   88]
per-ex loss: 0.695301  [   60/   88]
per-ex loss: 0.606575  [   61/   88]
per-ex loss: 0.701043  [   62/   88]
per-ex loss: 0.548355  [   63/   88]
per-ex loss: 0.730562  [   64/   88]
per-ex loss: 0.554315  [   65/   88]
per-ex loss: 0.760849  [   66/   88]
per-ex loss: 0.514103  [   67/   88]
per-ex loss: 0.728165  [   68/   88]
per-ex loss: 0.457640  [   69/   88]
per-ex loss: 0.693655  [   70/   88]
per-ex loss: 0.662556  [   71/   88]
per-ex loss: 0.703304  [   72/   88]
per-ex loss: 0.519223  [   73/   88]
per-ex loss: 0.577650  [   74/   88]
per-ex loss: 0.492227  [   75/   88]
per-ex loss: 0.630424  [   76/   88]
per-ex loss: 0.782351  [   77/   88]
per-ex loss: 0.607125  [   78/   88]
per-ex loss: 0.480975  [   79/   88]
per-ex loss: 0.474485  [   80/   88]
per-ex loss: 0.554260  [   81/   88]
per-ex loss: 0.547788  [   82/   88]
per-ex loss: 0.486812  [   83/   88]
per-ex loss: 0.533918  [   84/   88]
per-ex loss: 0.684274  [   85/   88]
per-ex loss: 0.419339  [   86/   88]
per-ex loss: 0.630131  [   87/   88]
per-ex loss: 0.677391  [   88/   88]
Train Error: Avg loss: 0.58637301
validation Error: 
 Avg loss: 0.69487332 
 F1: 0.489445 
 Precision: 0.512457 
 Recall: 0.468411
 IoU: 0.324017

test Error: 
 Avg loss: 0.64068927 
 F1: 0.565373 
 Precision: 0.584858 
 Recall: 0.547143
 IoU: 0.394090

We have finished training iteration 143
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_141_.pth
per-ex loss: 0.628299  [    1/   88]
per-ex loss: 0.545554  [    2/   88]
per-ex loss: 0.521668  [    3/   88]
per-ex loss: 0.602872  [    4/   88]
per-ex loss: 0.515749  [    5/   88]
per-ex loss: 0.476895  [    6/   88]
per-ex loss: 0.776687  [    7/   88]
per-ex loss: 0.732477  [    8/   88]
per-ex loss: 0.578273  [    9/   88]
per-ex loss: 0.470236  [   10/   88]
per-ex loss: 0.655018  [   11/   88]
per-ex loss: 0.597287  [   12/   88]
per-ex loss: 0.764751  [   13/   88]
per-ex loss: 0.678603  [   14/   88]
per-ex loss: 0.447797  [   15/   88]
per-ex loss: 0.568403  [   16/   88]
per-ex loss: 0.762738  [   17/   88]
per-ex loss: 0.552571  [   18/   88]
per-ex loss: 0.765882  [   19/   88]
per-ex loss: 0.611243  [   20/   88]
per-ex loss: 0.541921  [   21/   88]
per-ex loss: 0.430369  [   22/   88]
per-ex loss: 0.531013  [   23/   88]
per-ex loss: 0.718718  [   24/   88]
per-ex loss: 0.686058  [   25/   88]
per-ex loss: 0.493414  [   26/   88]
per-ex loss: 0.494072  [   27/   88]
per-ex loss: 0.584659  [   28/   88]
per-ex loss: 0.542564  [   29/   88]
per-ex loss: 0.571075  [   30/   88]
per-ex loss: 0.470102  [   31/   88]
per-ex loss: 0.441850  [   32/   88]
per-ex loss: 0.704670  [   33/   88]
per-ex loss: 0.495334  [   34/   88]
per-ex loss: 0.630056  [   35/   88]
per-ex loss: 0.422685  [   36/   88]
per-ex loss: 0.627082  [   37/   88]
per-ex loss: 0.695683  [   38/   88]
per-ex loss: 0.670712  [   39/   88]
per-ex loss: 0.673830  [   40/   88]
per-ex loss: 0.562648  [   41/   88]
per-ex loss: 0.594843  [   42/   88]
per-ex loss: 0.623665  [   43/   88]
per-ex loss: 0.721992  [   44/   88]
per-ex loss: 0.506993  [   45/   88]
per-ex loss: 0.581633  [   46/   88]
per-ex loss: 0.532469  [   47/   88]
per-ex loss: 0.701110  [   48/   88]
per-ex loss: 0.490299  [   49/   88]
per-ex loss: 0.711360  [   50/   88]
per-ex loss: 0.695334  [   51/   88]
per-ex loss: 0.698696  [   52/   88]
per-ex loss: 0.522692  [   53/   88]
per-ex loss: 0.509309  [   54/   88]
per-ex loss: 0.525040  [   55/   88]
per-ex loss: 0.659308  [   56/   88]
per-ex loss: 0.331009  [   57/   88]
per-ex loss: 0.507422  [   58/   88]
per-ex loss: 0.554407  [   59/   88]
per-ex loss: 0.517325  [   60/   88]
per-ex loss: 0.595333  [   61/   88]
per-ex loss: 0.769215  [   62/   88]
per-ex loss: 0.720793  [   63/   88]
per-ex loss: 0.411102  [   64/   88]
per-ex loss: 0.681132  [   65/   88]
per-ex loss: 0.477684  [   66/   88]
per-ex loss: 0.519803  [   67/   88]
per-ex loss: 0.545071  [   68/   88]
per-ex loss: 0.474632  [   69/   88]
per-ex loss: 0.572251  [   70/   88]
per-ex loss: 0.514743  [   71/   88]
per-ex loss: 0.599192  [   72/   88]
per-ex loss: 0.869410  [   73/   88]
per-ex loss: 0.461658  [   74/   88]
per-ex loss: 0.541030  [   75/   88]
per-ex loss: 0.515705  [   76/   88]
per-ex loss: 0.707936  [   77/   88]
per-ex loss: 0.552534  [   78/   88]
per-ex loss: 0.435576  [   79/   88]
per-ex loss: 0.533739  [   80/   88]
per-ex loss: 0.445740  [   81/   88]
per-ex loss: 0.771653  [   82/   88]
per-ex loss: 0.678742  [   83/   88]
per-ex loss: 0.507562  [   84/   88]
per-ex loss: 0.522729  [   85/   88]
per-ex loss: 0.530908  [   86/   88]
per-ex loss: 0.576403  [   87/   88]
per-ex loss: 0.703231  [   88/   88]
Train Error: Avg loss: 0.58479468
validation Error: 
 Avg loss: 0.68539447 
 F1: 0.498731 
 Precision: 0.607943 
 Recall: 0.422781
 IoU: 0.332206

test Error: 
 Avg loss: 0.64403253 
 F1: 0.560922 
 Precision: 0.665089 
 Recall: 0.484967
 IoU: 0.389779

We have finished training iteration 144
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_102_.pth
per-ex loss: 0.613567  [    1/   88]
per-ex loss: 0.678658  [    2/   88]
per-ex loss: 0.681508  [    3/   88]
per-ex loss: 0.679557  [    4/   88]
per-ex loss: 0.438119  [    5/   88]
per-ex loss: 0.576598  [    6/   88]
per-ex loss: 0.537354  [    7/   88]
per-ex loss: 0.672045  [    8/   88]
per-ex loss: 0.603867  [    9/   88]
per-ex loss: 0.772362  [   10/   88]
per-ex loss: 0.665107  [   11/   88]
per-ex loss: 0.524613  [   12/   88]
per-ex loss: 0.516628  [   13/   88]
per-ex loss: 0.455183  [   14/   88]
per-ex loss: 0.512203  [   15/   88]
per-ex loss: 0.690818  [   16/   88]
per-ex loss: 0.710796  [   17/   88]
per-ex loss: 0.585940  [   18/   88]
per-ex loss: 0.771559  [   19/   88]
per-ex loss: 0.476621  [   20/   88]
per-ex loss: 0.697893  [   21/   88]
per-ex loss: 0.599422  [   22/   88]
per-ex loss: 0.447564  [   23/   88]
per-ex loss: 0.577897  [   24/   88]
per-ex loss: 0.545552  [   25/   88]
per-ex loss: 0.609362  [   26/   88]
per-ex loss: 0.547998  [   27/   88]
per-ex loss: 0.665875  [   28/   88]
per-ex loss: 0.698951  [   29/   88]
per-ex loss: 0.464906  [   30/   88]
per-ex loss: 0.571194  [   31/   88]
per-ex loss: 0.499647  [   32/   88]
per-ex loss: 0.413984  [   33/   88]
per-ex loss: 0.656124  [   34/   88]
per-ex loss: 0.709492  [   35/   88]
per-ex loss: 0.339526  [   36/   88]
per-ex loss: 0.449368  [   37/   88]
per-ex loss: 0.723887  [   38/   88]
per-ex loss: 0.643735  [   39/   88]
per-ex loss: 0.568168  [   40/   88]
per-ex loss: 0.563653  [   41/   88]
per-ex loss: 0.478491  [   42/   88]
per-ex loss: 0.452016  [   43/   88]
per-ex loss: 0.483796  [   44/   88]
per-ex loss: 0.552163  [   45/   88]
per-ex loss: 0.530196  [   46/   88]
per-ex loss: 0.768860  [   47/   88]
per-ex loss: 0.532663  [   48/   88]
per-ex loss: 0.494870  [   49/   88]
per-ex loss: 0.537833  [   50/   88]
per-ex loss: 0.617518  [   51/   88]
per-ex loss: 0.531903  [   52/   88]
per-ex loss: 0.495899  [   53/   88]
per-ex loss: 0.426112  [   54/   88]
per-ex loss: 0.599101  [   55/   88]
per-ex loss: 0.679713  [   56/   88]
per-ex loss: 0.450845  [   57/   88]
per-ex loss: 0.659093  [   58/   88]
per-ex loss: 0.474252  [   59/   88]
per-ex loss: 0.615548  [   60/   88]
per-ex loss: 0.733762  [   61/   88]
per-ex loss: 0.718880  [   62/   88]
per-ex loss: 0.514171  [   63/   88]
per-ex loss: 0.580684  [   64/   88]
per-ex loss: 0.755456  [   65/   88]
per-ex loss: 0.679760  [   66/   88]
per-ex loss: 0.653146  [   67/   88]
per-ex loss: 0.580769  [   68/   88]
per-ex loss: 0.675383  [   69/   88]
per-ex loss: 0.512686  [   70/   88]
per-ex loss: 0.489519  [   71/   88]
per-ex loss: 0.580660  [   72/   88]
per-ex loss: 0.647255  [   73/   88]
per-ex loss: 0.465467  [   74/   88]
per-ex loss: 0.497599  [   75/   88]
per-ex loss: 0.683377  [   76/   88]
per-ex loss: 0.471207  [   77/   88]
per-ex loss: 0.706655  [   78/   88]
per-ex loss: 0.470546  [   79/   88]
per-ex loss: 0.536034  [   80/   88]
per-ex loss: 0.632272  [   81/   88]
per-ex loss: 0.528929  [   82/   88]
per-ex loss: 0.771969  [   83/   88]
per-ex loss: 0.475643  [   84/   88]
per-ex loss: 0.782995  [   85/   88]
per-ex loss: 0.494647  [   86/   88]
per-ex loss: 0.683955  [   87/   88]
per-ex loss: 0.468810  [   88/   88]
Train Error: Avg loss: 0.58330012
validation Error: 
 Avg loss: 0.68883943 
 F1: 0.499723 
 Precision: 0.597835 
 Recall: 0.429274
 IoU: 0.333087

test Error: 
 Avg loss: 0.63984210 
 F1: 0.566877 
 Precision: 0.664544 
 Recall: 0.494239
 IoU: 0.395553

We have finished training iteration 145
Deleting model ./unet_att_j_nzo_train/saved_model_wrapper/models/UNet_143_.pth
slurmstepd: error: *** STEP 16870.0 ON aga2 CANCELLED AT 2025-01-14T21:12:20 DUE TO TIME LIMIT ***
