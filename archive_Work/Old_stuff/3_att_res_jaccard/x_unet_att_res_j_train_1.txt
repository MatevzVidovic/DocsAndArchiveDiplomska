unet_att_main_just_train.py do_log: False
Log file name: log_14_13-59-15_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(sd='unet_att_res_j_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_att_res_j.yaml', yo=None, ntibp=None, ptp=None, map=None)
YAML: {'path_to_data': './Data/vein_and_sclera_data', 'batch_size': 1, 'learning_rate': 1e-05, 'num_of_dataloader_workers': 7, 'train_epoch_size_limit': 400, 'num_epochs_per_training_iteration': 1, 'cleanup_k': 3, 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': True, 'loss_fn_name': 'JACCARD', 'alphas': [], 'dataset_option': 'aug_tf', 'zero_out_non_sclera': True, 'add_sclera_to_img': False, 'add_bcosfire_to_img': True, 'add_coye_to_img': True, 'model_type': 'res_att', 'model': '64_2_6', 'input_width': 2048, 'input_height': 1024, 'input_channels': 5, 'output_channels': 2, 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq'}
Validation phase: False
Namespace(sd='unet_att_res_j_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_att_res_j.yaml', yo=None, ntibp=None, ptp=None, map=None)
Device: cuda
dataset_aug_tf.py do_log: False
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 88
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 88
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 88
val dataloader num of batches: 27
test dataloader num of batches: 12
Created new model instance.
per-ex loss: 0.964714  [    1/   88]
per-ex loss: 0.985732  [    2/   88]
per-ex loss: 0.960914  [    3/   88]
per-ex loss: 0.963550  [    4/   88]
per-ex loss: 0.912500  [    5/   88]
per-ex loss: 0.932103  [    6/   88]
per-ex loss: 0.978893  [    7/   88]
per-ex loss: 0.896179  [    8/   88]
per-ex loss: 0.961576  [    9/   88]
per-ex loss: 0.881473  [   10/   88]
per-ex loss: 0.942940  [   11/   88]
per-ex loss: 0.927226  [   12/   88]
per-ex loss: 0.915843  [   13/   88]
per-ex loss: 0.921653  [   14/   88]
per-ex loss: 0.968442  [   15/   88]
per-ex loss: 0.861258  [   16/   88]
per-ex loss: 0.960914  [   17/   88]
per-ex loss: 0.949701  [   18/   88]
per-ex loss: 0.865159  [   19/   88]
per-ex loss: 0.859440  [   20/   88]
per-ex loss: 0.966616  [   21/   88]
per-ex loss: 0.959513  [   22/   88]
per-ex loss: 0.863080  [   23/   88]
per-ex loss: 0.803276  [   24/   88]
per-ex loss: 0.798048  [   25/   88]
per-ex loss: 0.816891  [   26/   88]
per-ex loss: 0.942729  [   27/   88]
per-ex loss: 0.937609  [   28/   88]
per-ex loss: 0.819240  [   29/   88]
per-ex loss: 0.886459  [   30/   88]
per-ex loss: 0.774472  [   31/   88]
per-ex loss: 0.882103  [   32/   88]
per-ex loss: 0.906056  [   33/   88]
per-ex loss: 0.826166  [   34/   88]
per-ex loss: 0.928915  [   35/   88]
per-ex loss: 0.706887  [   36/   88]
per-ex loss: 0.903490  [   37/   88]
per-ex loss: 0.922680  [   38/   88]
per-ex loss: 0.760081  [   39/   88]
per-ex loss: 0.864835  [   40/   88]
per-ex loss: 0.737887  [   41/   88]
per-ex loss: 0.860683  [   42/   88]
per-ex loss: 0.910145  [   43/   88]
per-ex loss: 0.913317  [   44/   88]
per-ex loss: 0.853327  [   45/   88]
per-ex loss: 0.878659  [   46/   88]
per-ex loss: 0.841533  [   47/   88]
per-ex loss: 0.830393  [   48/   88]
per-ex loss: 0.871247  [   49/   88]
per-ex loss: 0.909633  [   50/   88]
per-ex loss: 0.692316  [   51/   88]
per-ex loss: 0.777285  [   52/   88]
per-ex loss: 0.893489  [   53/   88]
per-ex loss: 0.808091  [   54/   88]
per-ex loss: 0.781254  [   55/   88]
per-ex loss: 0.924968  [   56/   88]
per-ex loss: 0.692769  [   57/   88]
per-ex loss: 0.754323  [   58/   88]
per-ex loss: 0.744074  [   59/   88]
per-ex loss: 0.687379  [   60/   88]
per-ex loss: 0.732824  [   61/   88]
per-ex loss: 0.802659  [   62/   88]
per-ex loss: 0.713339  [   63/   88]
per-ex loss: 0.625347  [   64/   88]
per-ex loss: 0.811193  [   65/   88]
per-ex loss: 0.742147  [   66/   88]
per-ex loss: 0.734442  [   67/   88]
per-ex loss: 0.734095  [   68/   88]
per-ex loss: 0.832320  [   69/   88]
per-ex loss: 0.792134  [   70/   88]
per-ex loss: 0.892220  [   71/   88]
per-ex loss: 0.661739  [   72/   88]
per-ex loss: 0.701159  [   73/   88]
per-ex loss: 0.680404  [   74/   88]
per-ex loss: 0.709867  [   75/   88]
per-ex loss: 0.728878  [   76/   88]
per-ex loss: 0.732582  [   77/   88]
per-ex loss: 0.781948  [   78/   88]
per-ex loss: 0.740154  [   79/   88]
per-ex loss: 0.789550  [   80/   88]
per-ex loss: 0.814135  [   81/   88]
per-ex loss: 0.783605  [   82/   88]
per-ex loss: 0.815313  [   83/   88]
per-ex loss: 0.787090  [   84/   88]
per-ex loss: 0.911424  [   85/   88]
per-ex loss: 0.715691  [   86/   88]
per-ex loss: 0.853192  [   87/   88]
per-ex loss: 0.876445  [   88/   88]
Train Error: Avg loss: 0.83793211
validation Error: 
 Avg loss: 0.77545083 
 F1: 0.400825 
 Precision: 0.570754 
 Recall: 0.308868
 IoU: 0.250645

test Error: 
 Avg loss: 0.73770589 
 F1: 0.479235 
 Precision: 0.562567 
 Recall: 0.417406
 IoU: 0.315128

We have finished training iteration 1
per-ex loss: 0.730310  [    1/   88]
per-ex loss: 0.842519  [    2/   88]
per-ex loss: 0.634532  [    3/   88]
per-ex loss: 0.798059  [    4/   88]
per-ex loss: 0.848630  [    5/   88]
per-ex loss: 0.681959  [    6/   88]
per-ex loss: 0.717796  [    7/   88]
per-ex loss: 0.739535  [    8/   88]
per-ex loss: 0.845499  [    9/   88]
per-ex loss: 0.766611  [   10/   88]
per-ex loss: 0.879845  [   11/   88]
per-ex loss: 0.667096  [   12/   88]
per-ex loss: 0.841979  [   13/   88]
per-ex loss: 0.880960  [   14/   88]
per-ex loss: 0.617248  [   15/   88]
per-ex loss: 0.851356  [   16/   88]
per-ex loss: 0.731353  [   17/   88]
per-ex loss: 0.878101  [   18/   88]
per-ex loss: 0.660424  [   19/   88]
per-ex loss: 0.571103  [   20/   88]
per-ex loss: 0.830124  [   21/   88]
per-ex loss: 0.765333  [   22/   88]
per-ex loss: 0.741185  [   23/   88]
per-ex loss: 0.836706  [   24/   88]
per-ex loss: 0.870841  [   25/   88]
per-ex loss: 0.680045  [   26/   88]
per-ex loss: 0.693325  [   27/   88]
per-ex loss: 0.802545  [   28/   88]
per-ex loss: 0.700065  [   29/   88]
per-ex loss: 0.660771  [   30/   88]
per-ex loss: 0.674862  [   31/   88]
per-ex loss: 0.816430  [   32/   88]
per-ex loss: 0.701549  [   33/   88]
per-ex loss: 0.881325  [   34/   88]
per-ex loss: 0.824131  [   35/   88]
per-ex loss: 0.704532  [   36/   88]
per-ex loss: 0.703137  [   37/   88]
per-ex loss: 0.760065  [   38/   88]
per-ex loss: 0.675191  [   39/   88]
per-ex loss: 0.723102  [   40/   88]
per-ex loss: 0.866075  [   41/   88]
per-ex loss: 0.802357  [   42/   88]
per-ex loss: 0.634303  [   43/   88]
per-ex loss: 0.781084  [   44/   88]
per-ex loss: 0.729010  [   45/   88]
per-ex loss: 0.722306  [   46/   88]
per-ex loss: 0.712777  [   47/   88]
per-ex loss: 0.590584  [   48/   88]
per-ex loss: 0.761642  [   49/   88]
per-ex loss: 0.627514  [   50/   88]
per-ex loss: 0.701530  [   51/   88]
per-ex loss: 0.823307  [   52/   88]
per-ex loss: 0.850743  [   53/   88]
per-ex loss: 0.898658  [   54/   88]
per-ex loss: 0.701295  [   55/   88]
per-ex loss: 0.609427  [   56/   88]
per-ex loss: 0.559972  [   57/   88]
per-ex loss: 0.849755  [   58/   88]
per-ex loss: 0.838802  [   59/   88]
per-ex loss: 0.661954  [   60/   88]
per-ex loss: 0.620768  [   61/   88]
per-ex loss: 0.624474  [   62/   88]
per-ex loss: 0.775523  [   63/   88]
per-ex loss: 0.766779  [   64/   88]
per-ex loss: 0.607945  [   65/   88]
per-ex loss: 0.851891  [   66/   88]
per-ex loss: 0.704200  [   67/   88]
per-ex loss: 0.778636  [   68/   88]
per-ex loss: 0.793554  [   69/   88]
per-ex loss: 0.864107  [   70/   88]
per-ex loss: 0.656296  [   71/   88]
per-ex loss: 0.769030  [   72/   88]
per-ex loss: 0.774054  [   73/   88]
per-ex loss: 0.586018  [   74/   88]
per-ex loss: 0.741360  [   75/   88]
per-ex loss: 0.649295  [   76/   88]
per-ex loss: 0.824838  [   77/   88]
per-ex loss: 0.792821  [   78/   88]
per-ex loss: 0.809997  [   79/   88]
per-ex loss: 0.651909  [   80/   88]
per-ex loss: 0.799755  [   81/   88]
per-ex loss: 0.774871  [   82/   88]
per-ex loss: 0.854293  [   83/   88]
per-ex loss: 0.768334  [   84/   88]
per-ex loss: 0.617385  [   85/   88]
per-ex loss: 0.664532  [   86/   88]
per-ex loss: 0.786995  [   87/   88]
per-ex loss: 0.760375  [   88/   88]
Train Error: Avg loss: 0.74571936
validation Error: 
 Avg loss: 0.76270018 
 F1: 0.412944 
 Precision: 0.462645 
 Recall: 0.372886
 IoU: 0.260195

test Error: 
 Avg loss: 0.71420464 
 F1: 0.502751 
 Precision: 0.524317 
 Recall: 0.482888
 IoU: 0.335783

We have finished training iteration 2
per-ex loss: 0.751334  [    1/   88]
per-ex loss: 0.766147  [    2/   88]
per-ex loss: 0.684536  [    3/   88]
per-ex loss: 0.652751  [    4/   88]
per-ex loss: 0.669770  [    5/   88]
per-ex loss: 0.841354  [    6/   88]
per-ex loss: 0.774464  [    7/   88]
per-ex loss: 0.826824  [    8/   88]
per-ex loss: 0.868485  [    9/   88]
per-ex loss: 0.802533  [   10/   88]
per-ex loss: 0.845685  [   11/   88]
per-ex loss: 0.749966  [   12/   88]
per-ex loss: 0.675628  [   13/   88]
per-ex loss: 0.873242  [   14/   88]
per-ex loss: 0.749569  [   15/   88]
per-ex loss: 0.731516  [   16/   88]
per-ex loss: 0.824656  [   17/   88]
per-ex loss: 0.860133  [   18/   88]
per-ex loss: 0.602376  [   19/   88]
per-ex loss: 0.872458  [   20/   88]
per-ex loss: 0.681870  [   21/   88]
per-ex loss: 0.761392  [   22/   88]
per-ex loss: 0.770690  [   23/   88]
per-ex loss: 0.640569  [   24/   88]
per-ex loss: 0.636982  [   25/   88]
per-ex loss: 0.842274  [   26/   88]
per-ex loss: 0.794511  [   27/   88]
per-ex loss: 0.644908  [   28/   88]
per-ex loss: 0.746557  [   29/   88]
per-ex loss: 0.842739  [   30/   88]
per-ex loss: 0.578957  [   31/   88]
per-ex loss: 0.633756  [   32/   88]
per-ex loss: 0.620004  [   33/   88]
per-ex loss: 0.743913  [   34/   88]
per-ex loss: 0.799473  [   35/   88]
per-ex loss: 0.823588  [   36/   88]
per-ex loss: 0.687623  [   37/   88]
per-ex loss: 0.865155  [   38/   88]
per-ex loss: 0.873841  [   39/   88]
per-ex loss: 0.684580  [   40/   88]
per-ex loss: 0.794175  [   41/   88]
per-ex loss: 0.834544  [   42/   88]
per-ex loss: 0.673165  [   43/   88]
per-ex loss: 0.797236  [   44/   88]
per-ex loss: 0.671752  [   45/   88]
per-ex loss: 0.734896  [   46/   88]
per-ex loss: 0.809401  [   47/   88]
per-ex loss: 0.726038  [   48/   88]
per-ex loss: 0.671794  [   49/   88]
per-ex loss: 0.719405  [   50/   88]
per-ex loss: 0.610086  [   51/   88]
per-ex loss: 0.700864  [   52/   88]
per-ex loss: 0.850997  [   53/   88]
per-ex loss: 0.810906  [   54/   88]
per-ex loss: 0.623943  [   55/   88]
per-ex loss: 0.713443  [   56/   88]
per-ex loss: 0.670503  [   57/   88]
per-ex loss: 0.587711  [   58/   88]
per-ex loss: 0.663140  [   59/   88]
per-ex loss: 0.632089  [   60/   88]
per-ex loss: 0.833184  [   61/   88]
per-ex loss: 0.607263  [   62/   88]
per-ex loss: 0.541529  [   63/   88]
per-ex loss: 0.711719  [   64/   88]
per-ex loss: 0.698429  [   65/   88]
per-ex loss: 0.765363  [   66/   88]
per-ex loss: 0.836651  [   67/   88]
per-ex loss: 0.854001  [   68/   88]
per-ex loss: 0.597772  [   69/   88]
per-ex loss: 0.614990  [   70/   88]
per-ex loss: 0.608922  [   71/   88]
per-ex loss: 0.792207  [   72/   88]
per-ex loss: 0.571218  [   73/   88]
per-ex loss: 0.787927  [   74/   88]
per-ex loss: 0.818648  [   75/   88]
per-ex loss: 0.615640  [   76/   88]
per-ex loss: 0.635442  [   77/   88]
per-ex loss: 0.611402  [   78/   88]
per-ex loss: 0.611315  [   79/   88]
per-ex loss: 0.775674  [   80/   88]
per-ex loss: 0.628929  [   81/   88]
per-ex loss: 0.802507  [   82/   88]
per-ex loss: 0.757583  [   83/   88]
per-ex loss: 0.673982  [   84/   88]
per-ex loss: 0.821801  [   85/   88]
per-ex loss: 0.751258  [   86/   88]
per-ex loss: 0.629477  [   87/   88]
per-ex loss: 0.859594  [   88/   88]
Train Error: Avg loss: 0.72962869
validation Error: 
 Avg loss: 0.74444241 
 F1: 0.439667 
 Precision: 0.542167 
 Recall: 0.369761
 IoU: 0.281777

test Error: 
 Avg loss: 0.69434997 
 F1: 0.526523 
 Precision: 0.551272 
 Recall: 0.503901
 IoU: 0.357334

We have finished training iteration 3
per-ex loss: 0.701031  [    1/   88]
per-ex loss: 0.717298  [    2/   88]
per-ex loss: 0.841079  [    3/   88]
per-ex loss: 0.758286  [    4/   88]
per-ex loss: 0.569258  [    5/   88]
per-ex loss: 0.622736  [    6/   88]
per-ex loss: 0.742194  [    7/   88]
per-ex loss: 0.860954  [    8/   88]
per-ex loss: 0.626317  [    9/   88]
per-ex loss: 0.681699  [   10/   88]
per-ex loss: 0.811274  [   11/   88]
per-ex loss: 0.665415  [   12/   88]
per-ex loss: 0.597305  [   13/   88]
per-ex loss: 0.801400  [   14/   88]
per-ex loss: 0.668673  [   15/   88]
per-ex loss: 0.653412  [   16/   88]
per-ex loss: 0.736741  [   17/   88]
per-ex loss: 0.769942  [   18/   88]
per-ex loss: 0.682322  [   19/   88]
per-ex loss: 0.634120  [   20/   88]
per-ex loss: 0.891958  [   21/   88]
per-ex loss: 0.683467  [   22/   88]
per-ex loss: 0.685597  [   23/   88]
per-ex loss: 0.743338  [   24/   88]
per-ex loss: 0.563014  [   25/   88]
per-ex loss: 0.630055  [   26/   88]
per-ex loss: 0.651015  [   27/   88]
per-ex loss: 0.813086  [   28/   88]
per-ex loss: 0.670570  [   29/   88]
per-ex loss: 0.800799  [   30/   88]
per-ex loss: 0.709864  [   31/   88]
per-ex loss: 0.749567  [   32/   88]
per-ex loss: 0.795807  [   33/   88]
per-ex loss: 0.805649  [   34/   88]
per-ex loss: 0.666185  [   35/   88]
per-ex loss: 0.810429  [   36/   88]
per-ex loss: 0.817962  [   37/   88]
per-ex loss: 0.653564  [   38/   88]
per-ex loss: 0.828699  [   39/   88]
per-ex loss: 0.866085  [   40/   88]
per-ex loss: 0.744240  [   41/   88]
per-ex loss: 0.639790  [   42/   88]
per-ex loss: 0.798986  [   43/   88]
per-ex loss: 0.620390  [   44/   88]
per-ex loss: 0.814776  [   45/   88]
per-ex loss: 0.670221  [   46/   88]
per-ex loss: 0.666957  [   47/   88]
per-ex loss: 0.766640  [   48/   88]
per-ex loss: 0.748714  [   49/   88]
per-ex loss: 0.829892  [   50/   88]
per-ex loss: 0.618617  [   51/   88]
per-ex loss: 0.622367  [   52/   88]
per-ex loss: 0.861248  [   53/   88]
per-ex loss: 0.834858  [   54/   88]
per-ex loss: 0.752902  [   55/   88]
per-ex loss: 0.572721  [   56/   88]
per-ex loss: 0.672033  [   57/   88]
per-ex loss: 0.829708  [   58/   88]
per-ex loss: 0.626988  [   59/   88]
per-ex loss: 0.556897  [   60/   88]
per-ex loss: 0.654323  [   61/   88]
per-ex loss: 0.782503  [   62/   88]
per-ex loss: 0.584560  [   63/   88]
per-ex loss: 0.813345  [   64/   88]
per-ex loss: 0.778761  [   65/   88]
per-ex loss: 0.618341  [   66/   88]
per-ex loss: 0.774876  [   67/   88]
per-ex loss: 0.729008  [   68/   88]
per-ex loss: 0.752586  [   69/   88]
per-ex loss: 0.678455  [   70/   88]
per-ex loss: 0.690322  [   71/   88]
per-ex loss: 0.574188  [   72/   88]
per-ex loss: 0.757442  [   73/   88]
per-ex loss: 0.770987  [   74/   88]
per-ex loss: 0.656776  [   75/   88]
per-ex loss: 0.833398  [   76/   88]
per-ex loss: 0.625394  [   77/   88]
per-ex loss: 0.630443  [   78/   88]
per-ex loss: 0.596654  [   79/   88]
per-ex loss: 0.684871  [   80/   88]
per-ex loss: 0.827757  [   81/   88]
per-ex loss: 0.573471  [   82/   88]
per-ex loss: 0.798846  [   83/   88]
per-ex loss: 0.902286  [   84/   88]
per-ex loss: 0.780983  [   85/   88]
per-ex loss: 0.806312  [   86/   88]
per-ex loss: 0.741472  [   87/   88]
per-ex loss: 0.588881  [   88/   88]
Train Error: Avg loss: 0.71857223
validation Error: 
 Avg loss: 0.74108564 
 F1: 0.429075 
 Precision: 0.535701 
 Recall: 0.357849
 IoU: 0.273135

test Error: 
 Avg loss: 0.68937175 
 F1: 0.524555 
 Precision: 0.569498 
 Recall: 0.486187
 IoU: 0.355523

We have finished training iteration 4
per-ex loss: 0.528017  [    1/   88]
per-ex loss: 0.600922  [    2/   88]
per-ex loss: 0.799089  [    3/   88]
per-ex loss: 0.814343  [    4/   88]
per-ex loss: 0.771356  [    5/   88]
per-ex loss: 0.693723  [    6/   88]
per-ex loss: 0.782848  [    7/   88]
per-ex loss: 0.789533  [    8/   88]
per-ex loss: 0.583947  [    9/   88]
per-ex loss: 0.673712  [   10/   88]
per-ex loss: 0.793081  [   11/   88]
per-ex loss: 0.669866  [   12/   88]
per-ex loss: 0.704704  [   13/   88]
per-ex loss: 0.624222  [   14/   88]
per-ex loss: 0.725390  [   15/   88]
per-ex loss: 0.846415  [   16/   88]
per-ex loss: 0.667050  [   17/   88]
per-ex loss: 0.839927  [   18/   88]
per-ex loss: 0.669773  [   19/   88]
per-ex loss: 0.597873  [   20/   88]
per-ex loss: 0.769886  [   21/   88]
per-ex loss: 0.773697  [   22/   88]
per-ex loss: 0.840601  [   23/   88]
per-ex loss: 0.594505  [   24/   88]
per-ex loss: 0.822336  [   25/   88]
per-ex loss: 0.604398  [   26/   88]
per-ex loss: 0.639176  [   27/   88]
per-ex loss: 0.669386  [   28/   88]
per-ex loss: 0.742044  [   29/   88]
per-ex loss: 0.813376  [   30/   88]
per-ex loss: 0.809843  [   31/   88]
per-ex loss: 0.711212  [   32/   88]
per-ex loss: 0.671794  [   33/   88]
per-ex loss: 0.602158  [   34/   88]
per-ex loss: 0.758324  [   35/   88]
per-ex loss: 0.677968  [   36/   88]
per-ex loss: 0.683939  [   37/   88]
per-ex loss: 0.556400  [   38/   88]
per-ex loss: 0.647093  [   39/   88]
per-ex loss: 0.804477  [   40/   88]
per-ex loss: 0.582273  [   41/   88]
per-ex loss: 0.559332  [   42/   88]
per-ex loss: 0.822284  [   43/   88]
per-ex loss: 0.620924  [   44/   88]
per-ex loss: 0.781252  [   45/   88]
per-ex loss: 0.650772  [   46/   88]
per-ex loss: 0.909043  [   47/   88]
per-ex loss: 0.689745  [   48/   88]
per-ex loss: 0.592187  [   49/   88]
per-ex loss: 0.775507  [   50/   88]
per-ex loss: 0.745966  [   51/   88]
per-ex loss: 0.680038  [   52/   88]
per-ex loss: 0.651202  [   53/   88]
per-ex loss: 0.604604  [   54/   88]
per-ex loss: 0.791064  [   55/   88]
per-ex loss: 0.799423  [   56/   88]
per-ex loss: 0.832693  [   57/   88]
per-ex loss: 0.618045  [   58/   88]
per-ex loss: 0.750485  [   59/   88]
per-ex loss: 0.670837  [   60/   88]
per-ex loss: 0.782454  [   61/   88]
per-ex loss: 0.623383  [   62/   88]
per-ex loss: 0.789656  [   63/   88]
per-ex loss: 0.590898  [   64/   88]
per-ex loss: 0.839993  [   65/   88]
per-ex loss: 0.867145  [   66/   88]
per-ex loss: 0.553595  [   67/   88]
per-ex loss: 0.601010  [   68/   88]
per-ex loss: 0.757740  [   69/   88]
per-ex loss: 0.840393  [   70/   88]
per-ex loss: 0.624225  [   71/   88]
per-ex loss: 0.751270  [   72/   88]
per-ex loss: 0.675933  [   73/   88]
per-ex loss: 0.610706  [   74/   88]
per-ex loss: 0.649048  [   75/   88]
per-ex loss: 0.646227  [   76/   88]
per-ex loss: 0.809642  [   77/   88]
per-ex loss: 0.589849  [   78/   88]
per-ex loss: 0.767953  [   79/   88]
per-ex loss: 0.764512  [   80/   88]
per-ex loss: 0.780370  [   81/   88]
per-ex loss: 0.677355  [   82/   88]
per-ex loss: 0.765379  [   83/   88]
per-ex loss: 0.632345  [   84/   88]
per-ex loss: 0.859742  [   85/   88]
per-ex loss: 0.770765  [   86/   88]
per-ex loss: 0.756217  [   87/   88]
per-ex loss: 0.684121  [   88/   88]
Train Error: Avg loss: 0.71088644
validation Error: 
 Avg loss: 0.72549628 
 F1: 0.450004 
 Precision: 0.539585 
 Recall: 0.385931
 IoU: 0.290326

test Error: 
 Avg loss: 0.67469522 
 F1: 0.540547 
 Precision: 0.587944 
 Recall: 0.500222
 IoU: 0.370377

We have finished training iteration 5
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_1_.pth
per-ex loss: 0.782101  [    1/   88]
per-ex loss: 0.659125  [    2/   88]
per-ex loss: 0.707446  [    3/   88]
per-ex loss: 0.721361  [    4/   88]
per-ex loss: 0.709885  [    5/   88]
per-ex loss: 0.649675  [    6/   88]
per-ex loss: 0.735801  [    7/   88]
per-ex loss: 0.763479  [    8/   88]
per-ex loss: 0.781425  [    9/   88]
per-ex loss: 0.713105  [   10/   88]
per-ex loss: 0.740313  [   11/   88]
per-ex loss: 0.609169  [   12/   88]
per-ex loss: 0.714553  [   13/   88]
per-ex loss: 0.605710  [   14/   88]
per-ex loss: 0.639876  [   15/   88]
per-ex loss: 0.804881  [   16/   88]
per-ex loss: 0.757201  [   17/   88]
per-ex loss: 0.654557  [   18/   88]
per-ex loss: 0.667462  [   19/   88]
per-ex loss: 0.772187  [   20/   88]
per-ex loss: 0.654870  [   21/   88]
per-ex loss: 0.597854  [   22/   88]
per-ex loss: 0.839014  [   23/   88]
per-ex loss: 0.648852  [   24/   88]
per-ex loss: 0.807448  [   25/   88]
per-ex loss: 0.817931  [   26/   88]
per-ex loss: 0.660013  [   27/   88]
per-ex loss: 0.812622  [   28/   88]
per-ex loss: 0.603927  [   29/   88]
per-ex loss: 0.598617  [   30/   88]
per-ex loss: 0.741124  [   31/   88]
per-ex loss: 0.751873  [   32/   88]
per-ex loss: 0.614353  [   33/   88]
per-ex loss: 0.823620  [   34/   88]
per-ex loss: 0.824648  [   35/   88]
per-ex loss: 0.571712  [   36/   88]
per-ex loss: 0.789987  [   37/   88]
per-ex loss: 0.605438  [   38/   88]
per-ex loss: 0.701613  [   39/   88]
per-ex loss: 0.712644  [   40/   88]
per-ex loss: 0.612011  [   41/   88]
per-ex loss: 0.550579  [   42/   88]
per-ex loss: 0.751697  [   43/   88]
per-ex loss: 0.629883  [   44/   88]
per-ex loss: 0.577098  [   45/   88]
per-ex loss: 0.573913  [   46/   88]
per-ex loss: 0.580901  [   47/   88]
per-ex loss: 0.750376  [   48/   88]
per-ex loss: 0.773895  [   49/   88]
per-ex loss: 0.617124  [   50/   88]
per-ex loss: 0.773033  [   51/   88]
per-ex loss: 0.627622  [   52/   88]
per-ex loss: 0.820251  [   53/   88]
per-ex loss: 0.620886  [   54/   88]
per-ex loss: 0.535694  [   55/   88]
per-ex loss: 0.681460  [   56/   88]
per-ex loss: 0.718390  [   57/   88]
per-ex loss: 0.851503  [   58/   88]
per-ex loss: 0.854228  [   59/   88]
per-ex loss: 0.608388  [   60/   88]
per-ex loss: 0.729831  [   61/   88]
per-ex loss: 0.652628  [   62/   88]
per-ex loss: 0.847577  [   63/   88]
per-ex loss: 0.805962  [   64/   88]
per-ex loss: 0.839242  [   65/   88]
per-ex loss: 0.737168  [   66/   88]
per-ex loss: 0.751267  [   67/   88]
per-ex loss: 0.814650  [   68/   88]
per-ex loss: 0.586148  [   69/   88]
per-ex loss: 0.746070  [   70/   88]
per-ex loss: 0.739120  [   71/   88]
per-ex loss: 0.764841  [   72/   88]
per-ex loss: 0.814188  [   73/   88]
per-ex loss: 0.660495  [   74/   88]
per-ex loss: 0.660160  [   75/   88]
per-ex loss: 0.605929  [   76/   88]
per-ex loss: 0.774090  [   77/   88]
per-ex loss: 0.842323  [   78/   88]
per-ex loss: 0.764938  [   79/   88]
per-ex loss: 0.740551  [   80/   88]
per-ex loss: 0.850554  [   81/   88]
per-ex loss: 0.668686  [   82/   88]
per-ex loss: 0.664935  [   83/   88]
per-ex loss: 0.546527  [   84/   88]
per-ex loss: 0.546112  [   85/   88]
per-ex loss: 0.719486  [   86/   88]
per-ex loss: 0.598395  [   87/   88]
per-ex loss: 0.645574  [   88/   88]
Train Error: Avg loss: 0.70447554
validation Error: 
 Avg loss: 0.75342279 
 F1: 0.405176 
 Precision: 0.555332 
 Recall: 0.318938
 IoU: 0.254056

test Error: 
 Avg loss: 0.69570679 
 F1: 0.512380 
 Precision: 0.631265 
 Recall: 0.431178
 IoU: 0.344430

We have finished training iteration 6
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_2_.pth
per-ex loss: 0.630943  [    1/   88]
per-ex loss: 0.638077  [    2/   88]
per-ex loss: 0.662250  [    3/   88]
per-ex loss: 0.778165  [    4/   88]
per-ex loss: 0.717765  [    5/   88]
per-ex loss: 0.776519  [    6/   88]
per-ex loss: 0.607738  [    7/   88]
per-ex loss: 0.546766  [    8/   88]
per-ex loss: 0.594499  [    9/   88]
per-ex loss: 0.646920  [   10/   88]
per-ex loss: 0.590738  [   11/   88]
per-ex loss: 0.828544  [   12/   88]
per-ex loss: 0.821655  [   13/   88]
per-ex loss: 0.801315  [   14/   88]
per-ex loss: 0.543243  [   15/   88]
per-ex loss: 0.667181  [   16/   88]
per-ex loss: 0.772318  [   17/   88]
per-ex loss: 0.810306  [   18/   88]
per-ex loss: 0.616814  [   19/   88]
per-ex loss: 0.803677  [   20/   88]
per-ex loss: 0.733532  [   21/   88]
per-ex loss: 0.658142  [   22/   88]
per-ex loss: 0.846605  [   23/   88]
per-ex loss: 0.670269  [   24/   88]
per-ex loss: 0.612579  [   25/   88]
per-ex loss: 0.631576  [   26/   88]
per-ex loss: 0.570715  [   27/   88]
per-ex loss: 0.832848  [   28/   88]
per-ex loss: 0.719354  [   29/   88]
per-ex loss: 0.747386  [   30/   88]
per-ex loss: 0.752034  [   31/   88]
per-ex loss: 0.585566  [   32/   88]
per-ex loss: 0.577517  [   33/   88]
per-ex loss: 0.616624  [   34/   88]
per-ex loss: 0.773038  [   35/   88]
per-ex loss: 0.828123  [   36/   88]
per-ex loss: 0.733223  [   37/   88]
per-ex loss: 0.550887  [   38/   88]
per-ex loss: 0.844963  [   39/   88]
per-ex loss: 0.745804  [   40/   88]
per-ex loss: 0.668642  [   41/   88]
per-ex loss: 0.810344  [   42/   88]
per-ex loss: 0.764558  [   43/   88]
per-ex loss: 0.576871  [   44/   88]
per-ex loss: 0.619310  [   45/   88]
per-ex loss: 0.775588  [   46/   88]
per-ex loss: 0.624176  [   47/   88]
per-ex loss: 0.818091  [   48/   88]
per-ex loss: 0.755834  [   49/   88]
per-ex loss: 0.666842  [   50/   88]
per-ex loss: 0.755020  [   51/   88]
per-ex loss: 0.592571  [   52/   88]
per-ex loss: 0.748276  [   53/   88]
per-ex loss: 0.813222  [   54/   88]
per-ex loss: 0.677868  [   55/   88]
per-ex loss: 0.650647  [   56/   88]
per-ex loss: 0.615945  [   57/   88]
per-ex loss: 0.827673  [   58/   88]
per-ex loss: 0.800913  [   59/   88]
per-ex loss: 0.750891  [   60/   88]
per-ex loss: 0.708528  [   61/   88]
per-ex loss: 0.834375  [   62/   88]
per-ex loss: 0.635152  [   63/   88]
per-ex loss: 0.800860  [   64/   88]
per-ex loss: 0.671224  [   65/   88]
per-ex loss: 0.814414  [   66/   88]
per-ex loss: 0.669816  [   67/   88]
per-ex loss: 0.784208  [   68/   88]
per-ex loss: 0.663059  [   69/   88]
per-ex loss: 0.637715  [   70/   88]
per-ex loss: 0.616478  [   71/   88]
per-ex loss: 0.636990  [   72/   88]
per-ex loss: 0.621099  [   73/   88]
per-ex loss: 0.560024  [   74/   88]
per-ex loss: 0.718576  [   75/   88]
per-ex loss: 0.858264  [   76/   88]
per-ex loss: 0.755017  [   77/   88]
per-ex loss: 0.841271  [   78/   88]
per-ex loss: 0.764507  [   79/   88]
per-ex loss: 0.595706  [   80/   88]
per-ex loss: 0.596565  [   81/   88]
per-ex loss: 0.825024  [   82/   88]
per-ex loss: 0.623207  [   83/   88]
per-ex loss: 0.736086  [   84/   88]
per-ex loss: 0.662550  [   85/   88]
per-ex loss: 0.640567  [   86/   88]
per-ex loss: 0.685790  [   87/   88]
per-ex loss: 0.798784  [   88/   88]
Train Error: Avg loss: 0.70403812
validation Error: 
 Avg loss: 0.73075593 
 F1: 0.445974 
 Precision: 0.588916 
 Recall: 0.358869
 IoU: 0.286980

test Error: 
 Avg loss: 0.68270613 
 F1: 0.527985 
 Precision: 0.625758 
 Recall: 0.456637
 IoU: 0.358682

We have finished training iteration 7
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_3_.pth
per-ex loss: 0.852427  [    1/   88]
per-ex loss: 0.598090  [    2/   88]
per-ex loss: 0.671932  [    3/   88]
per-ex loss: 0.627006  [    4/   88]
per-ex loss: 0.764144  [    5/   88]
per-ex loss: 0.776750  [    6/   88]
per-ex loss: 0.772618  [    7/   88]
per-ex loss: 0.724198  [    8/   88]
per-ex loss: 0.842299  [    9/   88]
per-ex loss: 0.796367  [   10/   88]
per-ex loss: 0.633147  [   11/   88]
per-ex loss: 0.832241  [   12/   88]
per-ex loss: 0.607627  [   13/   88]
per-ex loss: 0.580980  [   14/   88]
per-ex loss: 0.794802  [   15/   88]
per-ex loss: 0.798894  [   16/   88]
per-ex loss: 0.718423  [   17/   88]
per-ex loss: 0.590144  [   18/   88]
per-ex loss: 0.737104  [   19/   88]
per-ex loss: 0.587001  [   20/   88]
per-ex loss: 0.537873  [   21/   88]
per-ex loss: 0.829943  [   22/   88]
per-ex loss: 0.547976  [   23/   88]
per-ex loss: 0.641440  [   24/   88]
per-ex loss: 0.541645  [   25/   88]
per-ex loss: 0.646700  [   26/   88]
per-ex loss: 0.576068  [   27/   88]
per-ex loss: 0.572212  [   28/   88]
per-ex loss: 0.546157  [   29/   88]
per-ex loss: 0.648631  [   30/   88]
per-ex loss: 0.797001  [   31/   88]
per-ex loss: 0.819144  [   32/   88]
per-ex loss: 0.766138  [   33/   88]
per-ex loss: 0.743326  [   34/   88]
per-ex loss: 0.679209  [   35/   88]
per-ex loss: 0.688216  [   36/   88]
per-ex loss: 0.813603  [   37/   88]
per-ex loss: 0.752127  [   38/   88]
per-ex loss: 0.664518  [   39/   88]
per-ex loss: 0.595838  [   40/   88]
per-ex loss: 0.699864  [   41/   88]
per-ex loss: 0.742250  [   42/   88]
per-ex loss: 0.612511  [   43/   88]
per-ex loss: 0.664000  [   44/   88]
per-ex loss: 0.780426  [   45/   88]
per-ex loss: 0.541201  [   46/   88]
per-ex loss: 0.646407  [   47/   88]
per-ex loss: 0.617787  [   48/   88]
per-ex loss: 0.660173  [   49/   88]
per-ex loss: 0.808039  [   50/   88]
per-ex loss: 0.607647  [   51/   88]
per-ex loss: 0.836948  [   52/   88]
per-ex loss: 0.521255  [   53/   88]
per-ex loss: 0.841533  [   54/   88]
per-ex loss: 0.628895  [   55/   88]
per-ex loss: 0.659517  [   56/   88]
per-ex loss: 0.749369  [   57/   88]
per-ex loss: 0.814384  [   58/   88]
per-ex loss: 0.845238  [   59/   88]
per-ex loss: 0.584409  [   60/   88]
per-ex loss: 0.705585  [   61/   88]
per-ex loss: 0.555368  [   62/   88]
per-ex loss: 0.741039  [   63/   88]
per-ex loss: 0.579460  [   64/   88]
per-ex loss: 0.656207  [   65/   88]
per-ex loss: 0.596667  [   66/   88]
per-ex loss: 0.780211  [   67/   88]
per-ex loss: 0.775257  [   68/   88]
per-ex loss: 0.797130  [   69/   88]
per-ex loss: 0.597950  [   70/   88]
per-ex loss: 0.627524  [   71/   88]
per-ex loss: 0.799608  [   72/   88]
per-ex loss: 0.767555  [   73/   88]
per-ex loss: 0.735082  [   74/   88]
per-ex loss: 0.681247  [   75/   88]
per-ex loss: 0.778039  [   76/   88]
per-ex loss: 0.632914  [   77/   88]
per-ex loss: 0.673909  [   78/   88]
per-ex loss: 0.740473  [   79/   88]
per-ex loss: 0.775664  [   80/   88]
per-ex loss: 0.753517  [   81/   88]
per-ex loss: 0.772146  [   82/   88]
per-ex loss: 0.815385  [   83/   88]
per-ex loss: 0.616973  [   84/   88]
per-ex loss: 0.771737  [   85/   88]
per-ex loss: 0.626478  [   86/   88]
per-ex loss: 0.632103  [   87/   88]
per-ex loss: 0.843491  [   88/   88]
Train Error: Avg loss: 0.69777874
validation Error: 
 Avg loss: 0.71862360 
 F1: 0.462384 
 Precision: 0.555179 
 Recall: 0.396167
 IoU: 0.300715

test Error: 
 Avg loss: 0.67166589 
 F1: 0.543187 
 Precision: 0.598962 
 Recall: 0.496915
 IoU: 0.372860

We have finished training iteration 8
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_6_.pth
per-ex loss: 0.774099  [    1/   88]
per-ex loss: 0.835086  [    2/   88]
per-ex loss: 0.645825  [    3/   88]
per-ex loss: 0.752840  [    4/   88]
per-ex loss: 0.589531  [    5/   88]
per-ex loss: 0.725667  [    6/   88]
per-ex loss: 0.699530  [    7/   88]
per-ex loss: 0.827469  [    8/   88]
per-ex loss: 0.727752  [    9/   88]
per-ex loss: 0.636561  [   10/   88]
per-ex loss: 0.596594  [   11/   88]
per-ex loss: 0.677058  [   12/   88]
per-ex loss: 0.592907  [   13/   88]
per-ex loss: 0.791323  [   14/   88]
per-ex loss: 0.779230  [   15/   88]
per-ex loss: 0.598310  [   16/   88]
per-ex loss: 0.587050  [   17/   88]
per-ex loss: 0.837875  [   18/   88]
per-ex loss: 0.650723  [   19/   88]
per-ex loss: 0.784283  [   20/   88]
per-ex loss: 0.612640  [   21/   88]
per-ex loss: 0.758034  [   22/   88]
per-ex loss: 0.583031  [   23/   88]
per-ex loss: 0.711074  [   24/   88]
per-ex loss: 0.653326  [   25/   88]
per-ex loss: 0.672864  [   26/   88]
per-ex loss: 0.703650  [   27/   88]
per-ex loss: 0.834934  [   28/   88]
per-ex loss: 0.604499  [   29/   88]
per-ex loss: 0.775343  [   30/   88]
per-ex loss: 0.664666  [   31/   88]
per-ex loss: 0.815795  [   32/   88]
per-ex loss: 0.610675  [   33/   88]
per-ex loss: 0.658873  [   34/   88]
per-ex loss: 0.844262  [   35/   88]
per-ex loss: 0.709695  [   36/   88]
per-ex loss: 0.725603  [   37/   88]
per-ex loss: 0.802598  [   38/   88]
per-ex loss: 0.795456  [   39/   88]
per-ex loss: 0.612926  [   40/   88]
per-ex loss: 0.836747  [   41/   88]
per-ex loss: 0.683519  [   42/   88]
per-ex loss: 0.761961  [   43/   88]
per-ex loss: 0.607189  [   44/   88]
per-ex loss: 0.630313  [   45/   88]
per-ex loss: 0.629001  [   46/   88]
per-ex loss: 0.596160  [   47/   88]
per-ex loss: 0.621261  [   48/   88]
per-ex loss: 0.796238  [   49/   88]
per-ex loss: 0.643460  [   50/   88]
per-ex loss: 0.592816  [   51/   88]
per-ex loss: 0.648494  [   52/   88]
per-ex loss: 0.648542  [   53/   88]
per-ex loss: 0.531486  [   54/   88]
per-ex loss: 0.747408  [   55/   88]
per-ex loss: 0.830793  [   56/   88]
per-ex loss: 0.659051  [   57/   88]
per-ex loss: 0.674000  [   58/   88]
per-ex loss: 0.751090  [   59/   88]
per-ex loss: 0.824507  [   60/   88]
per-ex loss: 0.771621  [   61/   88]
per-ex loss: 0.592832  [   62/   88]
per-ex loss: 0.632521  [   63/   88]
per-ex loss: 0.764215  [   64/   88]
per-ex loss: 0.755729  [   65/   88]
per-ex loss: 0.734282  [   66/   88]
per-ex loss: 0.745127  [   67/   88]
per-ex loss: 0.659931  [   68/   88]
per-ex loss: 0.830085  [   69/   88]
per-ex loss: 0.626899  [   70/   88]
per-ex loss: 0.601794  [   71/   88]
per-ex loss: 0.773557  [   72/   88]
per-ex loss: 0.595765  [   73/   88]
per-ex loss: 0.671012  [   74/   88]
per-ex loss: 0.574478  [   75/   88]
per-ex loss: 0.770407  [   76/   88]
per-ex loss: 0.579507  [   77/   88]
per-ex loss: 0.808240  [   78/   88]
per-ex loss: 0.544190  [   79/   88]
per-ex loss: 0.781078  [   80/   88]
per-ex loss: 0.530565  [   81/   88]
per-ex loss: 0.659991  [   82/   88]
per-ex loss: 0.744912  [   83/   88]
per-ex loss: 0.683161  [   84/   88]
per-ex loss: 0.777689  [   85/   88]
per-ex loss: 0.607380  [   86/   88]
per-ex loss: 0.576988  [   87/   88]
per-ex loss: 0.787394  [   88/   88]
Train Error: Avg loss: 0.69494363
validation Error: 
 Avg loss: 0.74793272 
 F1: 0.422554 
 Precision: 0.533038 
 Recall: 0.350007
 IoU: 0.267872

test Error: 
 Avg loss: 0.69264476 
 F1: 0.513281 
 Precision: 0.627208 
 Recall: 0.434379
 IoU: 0.345244

We have finished training iteration 9
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_4_.pth
per-ex loss: 0.633182  [    1/   88]
per-ex loss: 0.586423  [    2/   88]
per-ex loss: 0.519807  [    3/   88]
per-ex loss: 0.634999  [    4/   88]
per-ex loss: 0.759813  [    5/   88]
per-ex loss: 0.753673  [    6/   88]
per-ex loss: 0.721257  [    7/   88]
per-ex loss: 0.575067  [    8/   88]
per-ex loss: 0.603892  [    9/   88]
per-ex loss: 0.557485  [   10/   88]
per-ex loss: 0.606900  [   11/   88]
per-ex loss: 0.755950  [   12/   88]
per-ex loss: 0.765170  [   13/   88]
per-ex loss: 0.721144  [   14/   88]
per-ex loss: 0.630092  [   15/   88]
per-ex loss: 0.550342  [   16/   88]
per-ex loss: 0.831107  [   17/   88]
per-ex loss: 0.721364  [   18/   88]
per-ex loss: 0.655206  [   19/   88]
per-ex loss: 0.682747  [   20/   88]
per-ex loss: 0.569033  [   21/   88]
per-ex loss: 0.676320  [   22/   88]
per-ex loss: 0.809136  [   23/   88]
per-ex loss: 0.596442  [   24/   88]
per-ex loss: 0.782887  [   25/   88]
per-ex loss: 0.796901  [   26/   88]
per-ex loss: 0.810678  [   27/   88]
per-ex loss: 0.660488  [   28/   88]
per-ex loss: 0.745218  [   29/   88]
per-ex loss: 0.812027  [   30/   88]
per-ex loss: 0.819829  [   31/   88]
per-ex loss: 0.665743  [   32/   88]
per-ex loss: 0.746118  [   33/   88]
per-ex loss: 0.812509  [   34/   88]
per-ex loss: 0.717255  [   35/   88]
per-ex loss: 0.581751  [   36/   88]
per-ex loss: 0.546978  [   37/   88]
per-ex loss: 0.792176  [   38/   88]
per-ex loss: 0.835741  [   39/   88]
per-ex loss: 0.776319  [   40/   88]
per-ex loss: 0.725395  [   41/   88]
per-ex loss: 0.637551  [   42/   88]
per-ex loss: 0.723531  [   43/   88]
per-ex loss: 0.660179  [   44/   88]
per-ex loss: 0.651027  [   45/   88]
per-ex loss: 0.598833  [   46/   88]
per-ex loss: 0.759564  [   47/   88]
per-ex loss: 0.806713  [   48/   88]
per-ex loss: 0.601213  [   49/   88]
per-ex loss: 0.588112  [   50/   88]
per-ex loss: 0.824916  [   51/   88]
per-ex loss: 0.617022  [   52/   88]
per-ex loss: 0.682182  [   53/   88]
per-ex loss: 0.554308  [   54/   88]
per-ex loss: 0.633110  [   55/   88]
per-ex loss: 0.550948  [   56/   88]
per-ex loss: 0.557598  [   57/   88]
per-ex loss: 0.659354  [   58/   88]
per-ex loss: 0.623674  [   59/   88]
per-ex loss: 0.768710  [   60/   88]
per-ex loss: 0.826849  [   61/   88]
per-ex loss: 0.790283  [   62/   88]
per-ex loss: 0.799159  [   63/   88]
per-ex loss: 0.867431  [   64/   88]
per-ex loss: 0.595739  [   65/   88]
per-ex loss: 0.731191  [   66/   88]
per-ex loss: 0.836938  [   67/   88]
per-ex loss: 0.548523  [   68/   88]
per-ex loss: 0.701146  [   69/   88]
per-ex loss: 0.736060  [   70/   88]
per-ex loss: 0.560604  [   71/   88]
per-ex loss: 0.741219  [   72/   88]
per-ex loss: 0.569181  [   73/   88]
per-ex loss: 0.628321  [   74/   88]
per-ex loss: 0.744503  [   75/   88]
per-ex loss: 0.648507  [   76/   88]
per-ex loss: 0.772737  [   77/   88]
per-ex loss: 0.832100  [   78/   88]
per-ex loss: 0.859290  [   79/   88]
per-ex loss: 0.659828  [   80/   88]
per-ex loss: 0.606034  [   81/   88]
per-ex loss: 0.681189  [   82/   88]
per-ex loss: 0.830982  [   83/   88]
per-ex loss: 0.677577  [   84/   88]
per-ex loss: 0.707067  [   85/   88]
per-ex loss: 0.837130  [   86/   88]
per-ex loss: 0.800332  [   87/   88]
per-ex loss: 0.595977  [   88/   88]
Train Error: Avg loss: 0.69578417
validation Error: 
 Avg loss: 0.73196072 
 F1: 0.445383 
 Precision: 0.439389 
 Recall: 0.451543
 IoU: 0.286491

test Error: 
 Avg loss: 0.66855434 
 F1: 0.546983 
 Precision: 0.536628 
 Recall: 0.557746
 IoU: 0.376446

We have finished training iteration 10
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_7_.pth
per-ex loss: 0.597701  [    1/   88]
per-ex loss: 0.702345  [    2/   88]
per-ex loss: 0.783499  [    3/   88]
per-ex loss: 0.662279  [    4/   88]
per-ex loss: 0.786288  [    5/   88]
per-ex loss: 0.757460  [    6/   88]
per-ex loss: 0.670552  [    7/   88]
per-ex loss: 0.628868  [    8/   88]
per-ex loss: 0.701080  [    9/   88]
per-ex loss: 0.807731  [   10/   88]
per-ex loss: 0.648783  [   11/   88]
per-ex loss: 0.766237  [   12/   88]
per-ex loss: 0.534261  [   13/   88]
per-ex loss: 0.584706  [   14/   88]
per-ex loss: 0.631889  [   15/   88]
per-ex loss: 0.756727  [   16/   88]
per-ex loss: 0.727528  [   17/   88]
per-ex loss: 0.625839  [   18/   88]
per-ex loss: 0.820552  [   19/   88]
per-ex loss: 0.535565  [   20/   88]
per-ex loss: 0.621480  [   21/   88]
per-ex loss: 0.836321  [   22/   88]
per-ex loss: 0.792490  [   23/   88]
per-ex loss: 0.813005  [   24/   88]
per-ex loss: 0.804677  [   25/   88]
per-ex loss: 0.806240  [   26/   88]
per-ex loss: 0.823084  [   27/   88]
per-ex loss: 0.657643  [   28/   88]
per-ex loss: 0.760920  [   29/   88]
per-ex loss: 0.831446  [   30/   88]
per-ex loss: 0.575638  [   31/   88]
per-ex loss: 0.600996  [   32/   88]
per-ex loss: 0.584374  [   33/   88]
per-ex loss: 0.626094  [   34/   88]
per-ex loss: 0.555215  [   35/   88]
per-ex loss: 0.715694  [   36/   88]
per-ex loss: 0.673736  [   37/   88]
per-ex loss: 0.678839  [   38/   88]
per-ex loss: 0.583277  [   39/   88]
per-ex loss: 0.762324  [   40/   88]
per-ex loss: 0.656400  [   41/   88]
per-ex loss: 0.810950  [   42/   88]
per-ex loss: 0.791813  [   43/   88]
per-ex loss: 0.827493  [   44/   88]
per-ex loss: 0.837499  [   45/   88]
per-ex loss: 0.601280  [   46/   88]
per-ex loss: 0.861170  [   47/   88]
per-ex loss: 0.665581  [   48/   88]
per-ex loss: 0.537248  [   49/   88]
per-ex loss: 0.635420  [   50/   88]
per-ex loss: 0.571049  [   51/   88]
per-ex loss: 0.657866  [   52/   88]
per-ex loss: 0.590194  [   53/   88]
per-ex loss: 0.615326  [   54/   88]
per-ex loss: 0.743446  [   55/   88]
per-ex loss: 0.761271  [   56/   88]
per-ex loss: 0.673629  [   57/   88]
per-ex loss: 0.645888  [   58/   88]
per-ex loss: 0.651831  [   59/   88]
per-ex loss: 0.615014  [   60/   88]
per-ex loss: 0.566378  [   61/   88]
per-ex loss: 0.684362  [   62/   88]
per-ex loss: 0.693874  [   63/   88]
per-ex loss: 0.806852  [   64/   88]
per-ex loss: 0.772232  [   65/   88]
per-ex loss: 0.841207  [   66/   88]
per-ex loss: 0.776334  [   67/   88]
per-ex loss: 0.600657  [   68/   88]
per-ex loss: 0.789813  [   69/   88]
per-ex loss: 0.588393  [   70/   88]
per-ex loss: 0.706666  [   71/   88]
per-ex loss: 0.652526  [   72/   88]
per-ex loss: 0.619605  [   73/   88]
per-ex loss: 0.567529  [   74/   88]
per-ex loss: 0.629932  [   75/   88]
per-ex loss: 0.732753  [   76/   88]
per-ex loss: 0.598949  [   77/   88]
per-ex loss: 0.759422  [   78/   88]
per-ex loss: 0.772183  [   79/   88]
per-ex loss: 0.832614  [   80/   88]
per-ex loss: 0.741451  [   81/   88]
per-ex loss: 0.780551  [   82/   88]
per-ex loss: 0.604656  [   83/   88]
per-ex loss: 0.734285  [   84/   88]
per-ex loss: 0.744421  [   85/   88]
per-ex loss: 0.545723  [   86/   88]
per-ex loss: 0.553836  [   87/   88]
per-ex loss: 0.592839  [   88/   88]
Train Error: Avg loss: 0.69174762
validation Error: 
 Avg loss: 0.71620743 
 F1: 0.463355 
 Precision: 0.562933 
 Recall: 0.393711
 IoU: 0.301537

test Error: 
 Avg loss: 0.66838558 
 F1: 0.541869 
 Precision: 0.610718 
 Recall: 0.486970
 IoU: 0.371619

We have finished training iteration 11
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_9_.pth
per-ex loss: 0.739995  [    1/   88]
per-ex loss: 0.782816  [    2/   88]
per-ex loss: 0.835127  [    3/   88]
per-ex loss: 0.545860  [    4/   88]
per-ex loss: 0.592186  [    5/   88]
per-ex loss: 0.658700  [    6/   88]
per-ex loss: 0.670022  [    7/   88]
per-ex loss: 0.774680  [    8/   88]
per-ex loss: 0.732331  [    9/   88]
per-ex loss: 0.607307  [   10/   88]
per-ex loss: 0.618692  [   11/   88]
per-ex loss: 0.609222  [   12/   88]
per-ex loss: 0.766255  [   13/   88]
per-ex loss: 0.585597  [   14/   88]
per-ex loss: 0.549809  [   15/   88]
per-ex loss: 0.626576  [   16/   88]
per-ex loss: 0.685122  [   17/   88]
per-ex loss: 0.793842  [   18/   88]
per-ex loss: 0.848867  [   19/   88]
per-ex loss: 0.712004  [   20/   88]
per-ex loss: 0.698503  [   21/   88]
per-ex loss: 0.554215  [   22/   88]
per-ex loss: 0.679735  [   23/   88]
per-ex loss: 0.662071  [   24/   88]
per-ex loss: 0.751888  [   25/   88]
per-ex loss: 0.778810  [   26/   88]
per-ex loss: 0.635843  [   27/   88]
per-ex loss: 0.788416  [   28/   88]
per-ex loss: 0.582621  [   29/   88]
per-ex loss: 0.826958  [   30/   88]
per-ex loss: 0.811788  [   31/   88]
per-ex loss: 0.808081  [   32/   88]
per-ex loss: 0.821764  [   33/   88]
per-ex loss: 0.643979  [   34/   88]
per-ex loss: 0.762640  [   35/   88]
per-ex loss: 0.734007  [   36/   88]
per-ex loss: 0.569699  [   37/   88]
per-ex loss: 0.602132  [   38/   88]
per-ex loss: 0.534873  [   39/   88]
per-ex loss: 0.829988  [   40/   88]
per-ex loss: 0.650643  [   41/   88]
per-ex loss: 0.614696  [   42/   88]
per-ex loss: 0.679977  [   43/   88]
per-ex loss: 0.583693  [   44/   88]
per-ex loss: 0.766735  [   45/   88]
per-ex loss: 0.657089  [   46/   88]
per-ex loss: 0.837995  [   47/   88]
per-ex loss: 0.556816  [   48/   88]
per-ex loss: 0.835338  [   49/   88]
per-ex loss: 0.724840  [   50/   88]
per-ex loss: 0.642692  [   51/   88]
per-ex loss: 0.592879  [   52/   88]
per-ex loss: 0.798874  [   53/   88]
per-ex loss: 0.611812  [   54/   88]
per-ex loss: 0.744786  [   55/   88]
per-ex loss: 0.744325  [   56/   88]
per-ex loss: 0.718905  [   57/   88]
per-ex loss: 0.569005  [   58/   88]
per-ex loss: 0.782217  [   59/   88]
per-ex loss: 0.641016  [   60/   88]
per-ex loss: 0.655010  [   61/   88]
per-ex loss: 0.571525  [   62/   88]
per-ex loss: 0.830397  [   63/   88]
per-ex loss: 0.560231  [   64/   88]
per-ex loss: 0.852914  [   65/   88]
per-ex loss: 0.581825  [   66/   88]
per-ex loss: 0.704050  [   67/   88]
per-ex loss: 0.767088  [   68/   88]
per-ex loss: 0.593762  [   69/   88]
per-ex loss: 0.626705  [   70/   88]
per-ex loss: 0.711035  [   71/   88]
per-ex loss: 0.640594  [   72/   88]
per-ex loss: 0.849546  [   73/   88]
per-ex loss: 0.753706  [   74/   88]
per-ex loss: 0.601812  [   75/   88]
per-ex loss: 0.595151  [   76/   88]
per-ex loss: 0.761132  [   77/   88]
per-ex loss: 0.568746  [   78/   88]
per-ex loss: 0.645672  [   79/   88]
per-ex loss: 0.660782  [   80/   88]
per-ex loss: 0.738469  [   81/   88]
per-ex loss: 0.732842  [   82/   88]
per-ex loss: 0.665828  [   83/   88]
per-ex loss: 0.605183  [   84/   88]
per-ex loss: 0.627536  [   85/   88]
per-ex loss: 0.522762  [   86/   88]
per-ex loss: 0.813575  [   87/   88]
per-ex loss: 0.811922  [   88/   88]
Train Error: Avg loss: 0.68883129
validation Error: 
 Avg loss: 0.73019341 
 F1: 0.439112 
 Precision: 0.622049 
 Recall: 0.339321
 IoU: 0.281322

test Error: 
 Avg loss: 0.67496680 
 F1: 0.533782 
 Precision: 0.667620 
 Recall: 0.444644
 IoU: 0.364054

We have finished training iteration 12
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_10_.pth
per-ex loss: 0.505791  [    1/   88]
per-ex loss: 0.607658  [    2/   88]
per-ex loss: 0.777474  [    3/   88]
per-ex loss: 0.796511  [    4/   88]
per-ex loss: 0.709416  [    5/   88]
per-ex loss: 0.626095  [    6/   88]
per-ex loss: 0.657244  [    7/   88]
per-ex loss: 0.817322  [    8/   88]
per-ex loss: 0.647281  [    9/   88]
per-ex loss: 0.742654  [   10/   88]
per-ex loss: 0.721376  [   11/   88]
per-ex loss: 0.655469  [   12/   88]
per-ex loss: 0.769040  [   13/   88]
per-ex loss: 0.534761  [   14/   88]
per-ex loss: 0.618402  [   15/   88]
per-ex loss: 0.758058  [   16/   88]
per-ex loss: 0.810541  [   17/   88]
per-ex loss: 0.765377  [   18/   88]
per-ex loss: 0.837737  [   19/   88]
per-ex loss: 0.720137  [   20/   88]
per-ex loss: 0.808807  [   21/   88]
per-ex loss: 0.664598  [   22/   88]
per-ex loss: 0.656102  [   23/   88]
per-ex loss: 0.563744  [   24/   88]
per-ex loss: 0.786750  [   25/   88]
per-ex loss: 0.753271  [   26/   88]
per-ex loss: 0.744595  [   27/   88]
per-ex loss: 0.642875  [   28/   88]
per-ex loss: 0.712734  [   29/   88]
per-ex loss: 0.722939  [   30/   88]
per-ex loss: 0.620596  [   31/   88]
per-ex loss: 0.786195  [   32/   88]
per-ex loss: 0.649399  [   33/   88]
per-ex loss: 0.646207  [   34/   88]
per-ex loss: 0.695971  [   35/   88]
per-ex loss: 0.590490  [   36/   88]
per-ex loss: 0.655679  [   37/   88]
per-ex loss: 0.754618  [   38/   88]
per-ex loss: 0.648429  [   39/   88]
per-ex loss: 0.584913  [   40/   88]
per-ex loss: 0.610661  [   41/   88]
per-ex loss: 0.561097  [   42/   88]
per-ex loss: 0.804968  [   43/   88]
per-ex loss: 0.853715  [   44/   88]
per-ex loss: 0.768912  [   45/   88]
per-ex loss: 0.538603  [   46/   88]
per-ex loss: 0.594397  [   47/   88]
per-ex loss: 0.722169  [   48/   88]
per-ex loss: 0.768590  [   49/   88]
per-ex loss: 0.578266  [   50/   88]
per-ex loss: 0.555771  [   51/   88]
per-ex loss: 0.592514  [   52/   88]
per-ex loss: 0.561327  [   53/   88]
per-ex loss: 0.634151  [   54/   88]
per-ex loss: 0.573570  [   55/   88]
per-ex loss: 0.593131  [   56/   88]
per-ex loss: 0.845268  [   57/   88]
per-ex loss: 0.637978  [   58/   88]
per-ex loss: 0.528370  [   59/   88]
per-ex loss: 0.538629  [   60/   88]
per-ex loss: 0.634702  [   61/   88]
per-ex loss: 0.727716  [   62/   88]
per-ex loss: 0.804393  [   63/   88]
per-ex loss: 0.825353  [   64/   88]
per-ex loss: 0.780485  [   65/   88]
per-ex loss: 0.547843  [   66/   88]
per-ex loss: 0.653505  [   67/   88]
per-ex loss: 0.660466  [   68/   88]
per-ex loss: 0.802314  [   69/   88]
per-ex loss: 0.849688  [   70/   88]
per-ex loss: 0.582779  [   71/   88]
per-ex loss: 0.702144  [   72/   88]
per-ex loss: 0.615448  [   73/   88]
per-ex loss: 0.781046  [   74/   88]
per-ex loss: 0.749656  [   75/   88]
per-ex loss: 0.846307  [   76/   88]
per-ex loss: 0.738811  [   77/   88]
per-ex loss: 0.565182  [   78/   88]
per-ex loss: 0.607334  [   79/   88]
per-ex loss: 0.593128  [   80/   88]
per-ex loss: 0.661613  [   81/   88]
per-ex loss: 0.589750  [   82/   88]
per-ex loss: 0.582391  [   83/   88]
per-ex loss: 0.729573  [   84/   88]
per-ex loss: 0.583634  [   85/   88]
per-ex loss: 0.857326  [   86/   88]
per-ex loss: 0.827565  [   87/   88]
per-ex loss: 0.767414  [   88/   88]
Train Error: Avg loss: 0.68519219
validation Error: 
 Avg loss: 0.71387460 
 F1: 0.468003 
 Precision: 0.536222 
 Recall: 0.415183
 IoU: 0.305486

test Error: 
 Avg loss: 0.66474087 
 F1: 0.551207 
 Precision: 0.584226 
 Recall: 0.521722
 IoU: 0.380460

We have finished training iteration 13
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_5_.pth
per-ex loss: 0.756144  [    1/   88]
per-ex loss: 0.619020  [    2/   88]
per-ex loss: 0.547648  [    3/   88]
per-ex loss: 0.607913  [    4/   88]
per-ex loss: 0.849236  [    5/   88]
per-ex loss: 0.752497  [    6/   88]
per-ex loss: 0.637863  [    7/   88]
per-ex loss: 0.666588  [    8/   88]
per-ex loss: 0.732788  [    9/   88]
per-ex loss: 0.709976  [   10/   88]
per-ex loss: 0.823409  [   11/   88]
per-ex loss: 0.713114  [   12/   88]
per-ex loss: 0.550423  [   13/   88]
per-ex loss: 0.815018  [   14/   88]
per-ex loss: 0.787262  [   15/   88]
per-ex loss: 0.742381  [   16/   88]
per-ex loss: 0.766188  [   17/   88]
per-ex loss: 0.643628  [   18/   88]
per-ex loss: 0.642470  [   19/   88]
per-ex loss: 0.763760  [   20/   88]
per-ex loss: 0.579461  [   21/   88]
per-ex loss: 0.787043  [   22/   88]
per-ex loss: 0.602407  [   23/   88]
per-ex loss: 0.581329  [   24/   88]
per-ex loss: 0.777500  [   25/   88]
per-ex loss: 0.722272  [   26/   88]
per-ex loss: 0.804201  [   27/   88]
per-ex loss: 0.604521  [   28/   88]
per-ex loss: 0.635469  [   29/   88]
per-ex loss: 0.651730  [   30/   88]
per-ex loss: 0.788355  [   31/   88]
per-ex loss: 0.535714  [   32/   88]
per-ex loss: 0.608508  [   33/   88]
per-ex loss: 0.808224  [   34/   88]
per-ex loss: 0.843717  [   35/   88]
per-ex loss: 0.739234  [   36/   88]
per-ex loss: 0.777015  [   37/   88]
per-ex loss: 0.588021  [   38/   88]
per-ex loss: 0.752193  [   39/   88]
per-ex loss: 0.571407  [   40/   88]
per-ex loss: 0.799306  [   41/   88]
per-ex loss: 0.561333  [   42/   88]
per-ex loss: 0.547667  [   43/   88]
per-ex loss: 0.650479  [   44/   88]
per-ex loss: 0.637405  [   45/   88]
per-ex loss: 0.558132  [   46/   88]
per-ex loss: 0.609642  [   47/   88]
per-ex loss: 0.630034  [   48/   88]
per-ex loss: 0.704429  [   49/   88]
per-ex loss: 0.653305  [   50/   88]
per-ex loss: 0.640060  [   51/   88]
per-ex loss: 0.732204  [   52/   88]
per-ex loss: 0.835329  [   53/   88]
per-ex loss: 0.735961  [   54/   88]
per-ex loss: 0.782288  [   55/   88]
per-ex loss: 0.528597  [   56/   88]
per-ex loss: 0.736837  [   57/   88]
per-ex loss: 0.622344  [   58/   88]
per-ex loss: 0.559107  [   59/   88]
per-ex loss: 0.839783  [   60/   88]
per-ex loss: 0.756886  [   61/   88]
per-ex loss: 0.568032  [   62/   88]
per-ex loss: 0.792652  [   63/   88]
per-ex loss: 0.636823  [   64/   88]
per-ex loss: 0.768708  [   65/   88]
per-ex loss: 0.667932  [   66/   88]
per-ex loss: 0.572754  [   67/   88]
per-ex loss: 0.515028  [   68/   88]
per-ex loss: 0.842264  [   69/   88]
per-ex loss: 0.565836  [   70/   88]
per-ex loss: 0.623905  [   71/   88]
per-ex loss: 0.668189  [   72/   88]
per-ex loss: 0.727276  [   73/   88]
per-ex loss: 0.562855  [   74/   88]
per-ex loss: 0.636281  [   75/   88]
per-ex loss: 0.594017  [   76/   88]
per-ex loss: 0.678873  [   77/   88]
per-ex loss: 0.758227  [   78/   88]
per-ex loss: 0.683970  [   79/   88]
per-ex loss: 0.649283  [   80/   88]
per-ex loss: 0.793702  [   81/   88]
per-ex loss: 0.829921  [   82/   88]
per-ex loss: 0.612746  [   83/   88]
per-ex loss: 0.629184  [   84/   88]
per-ex loss: 0.755280  [   85/   88]
per-ex loss: 0.580504  [   86/   88]
per-ex loss: 0.810392  [   87/   88]
per-ex loss: 0.580721  [   88/   88]
Train Error: Avg loss: 0.68309241
validation Error: 
 Avg loss: 0.72144070 
 F1: 0.458390 
 Precision: 0.489957 
 Recall: 0.430645
 IoU: 0.297345

test Error: 
 Avg loss: 0.66725952 
 F1: 0.548061 
 Precision: 0.557297 
 Recall: 0.539126
 IoU: 0.377469

We have finished training iteration 14
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_12_.pth
per-ex loss: 0.780765  [    1/   88]
per-ex loss: 0.513716  [    2/   88]
per-ex loss: 0.615563  [    3/   88]
per-ex loss: 0.622706  [    4/   88]
per-ex loss: 0.768670  [    5/   88]
per-ex loss: 0.769345  [    6/   88]
per-ex loss: 0.802714  [    7/   88]
per-ex loss: 0.585751  [    8/   88]
per-ex loss: 0.577065  [    9/   88]
per-ex loss: 0.826350  [   10/   88]
per-ex loss: 0.642958  [   11/   88]
per-ex loss: 0.622945  [   12/   88]
per-ex loss: 0.543232  [   13/   88]
per-ex loss: 0.578586  [   14/   88]
per-ex loss: 0.804399  [   15/   88]
per-ex loss: 0.755292  [   16/   88]
per-ex loss: 0.826527  [   17/   88]
per-ex loss: 0.817702  [   18/   88]
per-ex loss: 0.782813  [   19/   88]
per-ex loss: 0.596209  [   20/   88]
per-ex loss: 0.563997  [   21/   88]
per-ex loss: 0.814471  [   22/   88]
per-ex loss: 0.540634  [   23/   88]
per-ex loss: 0.733234  [   24/   88]
per-ex loss: 0.844884  [   25/   88]
per-ex loss: 0.629792  [   26/   88]
per-ex loss: 0.550201  [   27/   88]
per-ex loss: 0.560923  [   28/   88]
per-ex loss: 0.656800  [   29/   88]
per-ex loss: 0.592600  [   30/   88]
per-ex loss: 0.645653  [   31/   88]
per-ex loss: 0.611243  [   32/   88]
per-ex loss: 0.562074  [   33/   88]
per-ex loss: 0.745918  [   34/   88]
per-ex loss: 0.617222  [   35/   88]
per-ex loss: 0.811083  [   36/   88]
per-ex loss: 0.590989  [   37/   88]
per-ex loss: 0.647577  [   38/   88]
per-ex loss: 0.660104  [   39/   88]
per-ex loss: 0.603154  [   40/   88]
per-ex loss: 0.714002  [   41/   88]
per-ex loss: 0.650563  [   42/   88]
per-ex loss: 0.826713  [   43/   88]
per-ex loss: 0.604791  [   44/   88]
per-ex loss: 0.548349  [   45/   88]
per-ex loss: 0.697249  [   46/   88]
per-ex loss: 0.622931  [   47/   88]
per-ex loss: 0.601515  [   48/   88]
per-ex loss: 0.731157  [   49/   88]
per-ex loss: 0.823800  [   50/   88]
per-ex loss: 0.823163  [   51/   88]
per-ex loss: 0.812964  [   52/   88]
per-ex loss: 0.767008  [   53/   88]
per-ex loss: 0.582153  [   54/   88]
per-ex loss: 0.754657  [   55/   88]
per-ex loss: 0.762064  [   56/   88]
per-ex loss: 0.670444  [   57/   88]
per-ex loss: 0.739895  [   58/   88]
per-ex loss: 0.590581  [   59/   88]
per-ex loss: 0.607901  [   60/   88]
per-ex loss: 0.632835  [   61/   88]
per-ex loss: 0.755112  [   62/   88]
per-ex loss: 0.764908  [   63/   88]
per-ex loss: 0.766809  [   64/   88]
per-ex loss: 0.571317  [   65/   88]
per-ex loss: 0.751466  [   66/   88]
per-ex loss: 0.564899  [   67/   88]
per-ex loss: 0.737356  [   68/   88]
per-ex loss: 0.764837  [   69/   88]
per-ex loss: 0.765925  [   70/   88]
per-ex loss: 0.658190  [   71/   88]
per-ex loss: 0.795658  [   72/   88]
per-ex loss: 0.595364  [   73/   88]
per-ex loss: 0.606029  [   74/   88]
per-ex loss: 0.775483  [   75/   88]
per-ex loss: 0.660378  [   76/   88]
per-ex loss: 0.569265  [   77/   88]
per-ex loss: 0.646732  [   78/   88]
per-ex loss: 0.594196  [   79/   88]
per-ex loss: 0.718541  [   80/   88]
per-ex loss: 0.720631  [   81/   88]
per-ex loss: 0.722900  [   82/   88]
per-ex loss: 0.748654  [   83/   88]
per-ex loss: 0.635633  [   84/   88]
per-ex loss: 0.545131  [   85/   88]
per-ex loss: 0.713873  [   86/   88]
per-ex loss: 0.739007  [   87/   88]
per-ex loss: 0.723927  [   88/   88]
Train Error: Avg loss: 0.68173655
validation Error: 
 Avg loss: 0.71476942 
 F1: 0.468886 
 Precision: 0.506468 
 Recall: 0.436497
 IoU: 0.306239

test Error: 
 Avg loss: 0.66164435 
 F1: 0.552019 
 Precision: 0.580446 
 Recall: 0.526247
 IoU: 0.381234

We have finished training iteration 15
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_8_.pth
per-ex loss: 0.728454  [    1/   88]
per-ex loss: 0.646482  [    2/   88]
per-ex loss: 0.753823  [    3/   88]
per-ex loss: 0.704974  [    4/   88]
per-ex loss: 0.698958  [    5/   88]
per-ex loss: 0.622786  [    6/   88]
per-ex loss: 0.538998  [    7/   88]
per-ex loss: 0.578203  [    8/   88]
per-ex loss: 0.780722  [    9/   88]
per-ex loss: 0.625805  [   10/   88]
per-ex loss: 0.746879  [   11/   88]
per-ex loss: 0.553632  [   12/   88]
per-ex loss: 0.592618  [   13/   88]
per-ex loss: 0.700603  [   14/   88]
per-ex loss: 0.835733  [   15/   88]
per-ex loss: 0.713467  [   16/   88]
per-ex loss: 0.640585  [   17/   88]
per-ex loss: 0.764484  [   18/   88]
per-ex loss: 0.660997  [   19/   88]
per-ex loss: 0.794328  [   20/   88]
per-ex loss: 0.719211  [   21/   88]
per-ex loss: 0.568432  [   22/   88]
per-ex loss: 0.586176  [   23/   88]
per-ex loss: 0.814058  [   24/   88]
per-ex loss: 0.538211  [   25/   88]
per-ex loss: 0.807423  [   26/   88]
per-ex loss: 0.818877  [   27/   88]
per-ex loss: 0.654652  [   28/   88]
per-ex loss: 0.746670  [   29/   88]
per-ex loss: 0.588285  [   30/   88]
per-ex loss: 0.789375  [   31/   88]
per-ex loss: 0.667641  [   32/   88]
per-ex loss: 0.830746  [   33/   88]
per-ex loss: 0.715121  [   34/   88]
per-ex loss: 0.635985  [   35/   88]
per-ex loss: 0.511524  [   36/   88]
per-ex loss: 0.599117  [   37/   88]
per-ex loss: 0.772929  [   38/   88]
per-ex loss: 0.560005  [   39/   88]
per-ex loss: 0.608782  [   40/   88]
per-ex loss: 0.798947  [   41/   88]
per-ex loss: 0.637914  [   42/   88]
per-ex loss: 0.602872  [   43/   88]
per-ex loss: 0.825447  [   44/   88]
per-ex loss: 0.728866  [   45/   88]
per-ex loss: 0.609922  [   46/   88]
per-ex loss: 0.629307  [   47/   88]
per-ex loss: 0.532549  [   48/   88]
per-ex loss: 0.643670  [   49/   88]
per-ex loss: 0.618941  [   50/   88]
per-ex loss: 0.642363  [   51/   88]
per-ex loss: 0.730432  [   52/   88]
per-ex loss: 0.743192  [   53/   88]
per-ex loss: 0.824925  [   54/   88]
per-ex loss: 0.645150  [   55/   88]
per-ex loss: 0.754098  [   56/   88]
per-ex loss: 0.605474  [   57/   88]
per-ex loss: 0.641557  [   58/   88]
per-ex loss: 0.584282  [   59/   88]
per-ex loss: 0.767934  [   60/   88]
per-ex loss: 0.592842  [   61/   88]
per-ex loss: 0.781144  [   62/   88]
per-ex loss: 0.596364  [   63/   88]
per-ex loss: 0.520805  [   64/   88]
per-ex loss: 0.792721  [   65/   88]
per-ex loss: 0.576111  [   66/   88]
per-ex loss: 0.788900  [   67/   88]
per-ex loss: 0.795996  [   68/   88]
per-ex loss: 0.797031  [   69/   88]
per-ex loss: 0.753644  [   70/   88]
per-ex loss: 0.861597  [   71/   88]
per-ex loss: 0.634022  [   72/   88]
per-ex loss: 0.633348  [   73/   88]
per-ex loss: 0.533057  [   74/   88]
per-ex loss: 0.564794  [   75/   88]
per-ex loss: 0.561784  [   76/   88]
per-ex loss: 0.637010  [   77/   88]
per-ex loss: 0.802008  [   78/   88]
per-ex loss: 0.737333  [   79/   88]
per-ex loss: 0.847015  [   80/   88]
per-ex loss: 0.581651  [   81/   88]
per-ex loss: 0.763805  [   82/   88]
per-ex loss: 0.736479  [   83/   88]
per-ex loss: 0.727052  [   84/   88]
per-ex loss: 0.687145  [   85/   88]
per-ex loss: 0.622811  [   86/   88]
per-ex loss: 0.720717  [   87/   88]
per-ex loss: 0.599263  [   88/   88]
Train Error: Avg loss: 0.68215950
validation Error: 
 Avg loss: 0.74894337 
 F1: 0.417957 
 Precision: 0.385618 
 Recall: 0.456217
 IoU: 0.264188

test Error: 
 Avg loss: 0.68250681 
 F1: 0.525202 
 Precision: 0.494159 
 Recall: 0.560406
 IoU: 0.356118

We have finished training iteration 16
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_14_.pth
per-ex loss: 0.590729  [    1/   88]
per-ex loss: 0.874895  [    2/   88]
per-ex loss: 0.754323  [    3/   88]
per-ex loss: 0.752779  [    4/   88]
per-ex loss: 0.712280  [    5/   88]
per-ex loss: 0.575327  [    6/   88]
per-ex loss: 0.817722  [    7/   88]
per-ex loss: 0.600037  [    8/   88]
per-ex loss: 0.609318  [    9/   88]
per-ex loss: 0.582248  [   10/   88]
per-ex loss: 0.644057  [   11/   88]
per-ex loss: 0.650610  [   12/   88]
per-ex loss: 0.720393  [   13/   88]
per-ex loss: 0.704580  [   14/   88]
per-ex loss: 0.562072  [   15/   88]
per-ex loss: 0.550883  [   16/   88]
per-ex loss: 0.567753  [   17/   88]
per-ex loss: 0.639878  [   18/   88]
per-ex loss: 0.652797  [   19/   88]
per-ex loss: 0.775436  [   20/   88]
per-ex loss: 0.806264  [   21/   88]
per-ex loss: 0.809410  [   22/   88]
per-ex loss: 0.765923  [   23/   88]
per-ex loss: 0.836324  [   24/   88]
per-ex loss: 0.628457  [   25/   88]
per-ex loss: 0.760768  [   26/   88]
per-ex loss: 0.752821  [   27/   88]
per-ex loss: 0.588967  [   28/   88]
per-ex loss: 0.732699  [   29/   88]
per-ex loss: 0.637988  [   30/   88]
per-ex loss: 0.627867  [   31/   88]
per-ex loss: 0.763402  [   32/   88]
per-ex loss: 0.693318  [   33/   88]
per-ex loss: 0.759699  [   34/   88]
per-ex loss: 0.805615  [   35/   88]
per-ex loss: 0.735850  [   36/   88]
per-ex loss: 0.654013  [   37/   88]
per-ex loss: 0.734827  [   38/   88]
per-ex loss: 0.802122  [   39/   88]
per-ex loss: 0.631686  [   40/   88]
per-ex loss: 0.652290  [   41/   88]
per-ex loss: 0.699096  [   42/   88]
per-ex loss: 0.795932  [   43/   88]
per-ex loss: 0.636457  [   44/   88]
per-ex loss: 0.530637  [   45/   88]
per-ex loss: 0.581594  [   46/   88]
per-ex loss: 0.640648  [   47/   88]
per-ex loss: 0.769555  [   48/   88]
per-ex loss: 0.753955  [   49/   88]
per-ex loss: 0.588109  [   50/   88]
per-ex loss: 0.604453  [   51/   88]
per-ex loss: 0.548168  [   52/   88]
per-ex loss: 0.732129  [   53/   88]
per-ex loss: 0.817245  [   54/   88]
per-ex loss: 0.780401  [   55/   88]
per-ex loss: 0.756904  [   56/   88]
per-ex loss: 0.530433  [   57/   88]
per-ex loss: 0.839828  [   58/   88]
per-ex loss: 0.783745  [   59/   88]
per-ex loss: 0.616900  [   60/   88]
per-ex loss: 0.718713  [   61/   88]
per-ex loss: 0.696629  [   62/   88]
per-ex loss: 0.542119  [   63/   88]
per-ex loss: 0.572301  [   64/   88]
per-ex loss: 0.724457  [   65/   88]
per-ex loss: 0.558898  [   66/   88]
per-ex loss: 0.606461  [   67/   88]
per-ex loss: 0.563545  [   68/   88]
per-ex loss: 0.612827  [   69/   88]
per-ex loss: 0.717213  [   70/   88]
per-ex loss: 0.647379  [   71/   88]
per-ex loss: 0.754906  [   72/   88]
per-ex loss: 0.679511  [   73/   88]
per-ex loss: 0.844941  [   74/   88]
per-ex loss: 0.686638  [   75/   88]
per-ex loss: 0.797489  [   76/   88]
per-ex loss: 0.607409  [   77/   88]
per-ex loss: 0.616375  [   78/   88]
per-ex loss: 0.805823  [   79/   88]
per-ex loss: 0.560553  [   80/   88]
per-ex loss: 0.748229  [   81/   88]
per-ex loss: 0.636385  [   82/   88]
per-ex loss: 0.581632  [   83/   88]
per-ex loss: 0.578594  [   84/   88]
per-ex loss: 0.569862  [   85/   88]
per-ex loss: 0.564868  [   86/   88]
per-ex loss: 0.785313  [   87/   88]
per-ex loss: 0.582011  [   88/   88]
Train Error: Avg loss: 0.68051931
validation Error: 
 Avg loss: 0.73550621 
 F1: 0.440655 
 Precision: 0.455002 
 Recall: 0.427185
 IoU: 0.282590

test Error: 
 Avg loss: 0.67164366 
 F1: 0.538379 
 Precision: 0.559710 
 Recall: 0.518614
 IoU: 0.368344

We have finished training iteration 17
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_11_.pth
per-ex loss: 0.513786  [    1/   88]
per-ex loss: 0.646247  [    2/   88]
per-ex loss: 0.735615  [    3/   88]
per-ex loss: 0.566304  [    4/   88]
per-ex loss: 0.568997  [    5/   88]
per-ex loss: 0.542606  [    6/   88]
per-ex loss: 0.801950  [    7/   88]
per-ex loss: 0.798884  [    8/   88]
per-ex loss: 0.523753  [    9/   88]
per-ex loss: 0.548934  [   10/   88]
per-ex loss: 0.651084  [   11/   88]
per-ex loss: 0.639671  [   12/   88]
per-ex loss: 0.736555  [   13/   88]
per-ex loss: 0.795128  [   14/   88]
per-ex loss: 0.565223  [   15/   88]
per-ex loss: 0.510298  [   16/   88]
per-ex loss: 0.635906  [   17/   88]
per-ex loss: 0.525955  [   18/   88]
per-ex loss: 0.792384  [   19/   88]
per-ex loss: 0.582534  [   20/   88]
per-ex loss: 0.676394  [   21/   88]
per-ex loss: 0.667438  [   22/   88]
per-ex loss: 0.613190  [   23/   88]
per-ex loss: 0.609272  [   24/   88]
per-ex loss: 0.786697  [   25/   88]
per-ex loss: 0.829048  [   26/   88]
per-ex loss: 0.615377  [   27/   88]
per-ex loss: 0.745160  [   28/   88]
per-ex loss: 0.850460  [   29/   88]
per-ex loss: 0.720051  [   30/   88]
per-ex loss: 0.830738  [   31/   88]
per-ex loss: 0.844990  [   32/   88]
per-ex loss: 0.724154  [   33/   88]
per-ex loss: 0.577127  [   34/   88]
per-ex loss: 0.765296  [   35/   88]
per-ex loss: 0.598527  [   36/   88]
per-ex loss: 0.766040  [   37/   88]
per-ex loss: 0.754946  [   38/   88]
per-ex loss: 0.636741  [   39/   88]
per-ex loss: 0.744875  [   40/   88]
per-ex loss: 0.688898  [   41/   88]
per-ex loss: 0.761724  [   42/   88]
per-ex loss: 0.746878  [   43/   88]
per-ex loss: 0.593424  [   44/   88]
per-ex loss: 0.834068  [   45/   88]
per-ex loss: 0.768947  [   46/   88]
per-ex loss: 0.717702  [   47/   88]
per-ex loss: 0.559182  [   48/   88]
per-ex loss: 0.562823  [   49/   88]
per-ex loss: 0.705042  [   50/   88]
per-ex loss: 0.606567  [   51/   88]
per-ex loss: 0.617548  [   52/   88]
per-ex loss: 0.827064  [   53/   88]
per-ex loss: 0.638431  [   54/   88]
per-ex loss: 0.726862  [   55/   88]
per-ex loss: 0.502736  [   56/   88]
per-ex loss: 0.757799  [   57/   88]
per-ex loss: 0.645889  [   58/   88]
per-ex loss: 0.575752  [   59/   88]
per-ex loss: 0.795357  [   60/   88]
per-ex loss: 0.602595  [   61/   88]
per-ex loss: 0.774154  [   62/   88]
per-ex loss: 0.624574  [   63/   88]
per-ex loss: 0.709089  [   64/   88]
per-ex loss: 0.588733  [   65/   88]
per-ex loss: 0.718141  [   66/   88]
per-ex loss: 0.802630  [   67/   88]
per-ex loss: 0.767494  [   68/   88]
per-ex loss: 0.583086  [   69/   88]
per-ex loss: 0.626062  [   70/   88]
per-ex loss: 0.581554  [   71/   88]
per-ex loss: 0.598677  [   72/   88]
per-ex loss: 0.588523  [   73/   88]
per-ex loss: 0.753927  [   74/   88]
per-ex loss: 0.712310  [   75/   88]
per-ex loss: 0.786056  [   76/   88]
per-ex loss: 0.565580  [   77/   88]
per-ex loss: 0.769679  [   78/   88]
per-ex loss: 0.738405  [   79/   88]
per-ex loss: 0.574276  [   80/   88]
per-ex loss: 0.637130  [   81/   88]
per-ex loss: 0.653594  [   82/   88]
per-ex loss: 0.732358  [   83/   88]
per-ex loss: 0.823640  [   84/   88]
per-ex loss: 0.646580  [   85/   88]
per-ex loss: 0.655436  [   86/   88]
per-ex loss: 0.638363  [   87/   88]
per-ex loss: 0.566773  [   88/   88]
Train Error: Avg loss: 0.67602779
validation Error: 
 Avg loss: 0.72891946 
 F1: 0.448227 
 Precision: 0.423780 
 Recall: 0.475667
 IoU: 0.288848

test Error: 
 Avg loss: 0.66760534 
 F1: 0.544420 
 Precision: 0.515167 
 Recall: 0.577196
 IoU: 0.374023

We have finished training iteration 18
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_16_.pth
per-ex loss: 0.593166  [    1/   88]
per-ex loss: 0.583588  [    2/   88]
per-ex loss: 0.829661  [    3/   88]
per-ex loss: 0.667666  [    4/   88]
per-ex loss: 0.657233  [    5/   88]
per-ex loss: 0.644198  [    6/   88]
per-ex loss: 0.624643  [    7/   88]
per-ex loss: 0.792225  [    8/   88]
per-ex loss: 0.746528  [    9/   88]
per-ex loss: 0.597346  [   10/   88]
per-ex loss: 0.737579  [   11/   88]
per-ex loss: 0.644445  [   12/   88]
per-ex loss: 0.578961  [   13/   88]
per-ex loss: 0.844407  [   14/   88]
per-ex loss: 0.817431  [   15/   88]
per-ex loss: 0.693299  [   16/   88]
per-ex loss: 0.613142  [   17/   88]
per-ex loss: 0.737314  [   18/   88]
per-ex loss: 0.737655  [   19/   88]
per-ex loss: 0.540399  [   20/   88]
per-ex loss: 0.606263  [   21/   88]
per-ex loss: 0.580095  [   22/   88]
per-ex loss: 0.745723  [   23/   88]
per-ex loss: 0.847470  [   24/   88]
per-ex loss: 0.780848  [   25/   88]
per-ex loss: 0.743207  [   26/   88]
per-ex loss: 0.770136  [   27/   88]
per-ex loss: 0.641004  [   28/   88]
per-ex loss: 0.733934  [   29/   88]
per-ex loss: 0.605164  [   30/   88]
per-ex loss: 0.720822  [   31/   88]
per-ex loss: 0.732094  [   32/   88]
per-ex loss: 0.599161  [   33/   88]
per-ex loss: 0.763698  [   34/   88]
per-ex loss: 0.637724  [   35/   88]
per-ex loss: 0.638001  [   36/   88]
per-ex loss: 0.619829  [   37/   88]
per-ex loss: 0.571732  [   38/   88]
per-ex loss: 0.531467  [   39/   88]
per-ex loss: 0.574249  [   40/   88]
per-ex loss: 0.558155  [   41/   88]
per-ex loss: 0.545814  [   42/   88]
per-ex loss: 0.768284  [   43/   88]
per-ex loss: 0.699272  [   44/   88]
per-ex loss: 0.736059  [   45/   88]
per-ex loss: 0.573385  [   46/   88]
per-ex loss: 0.609523  [   47/   88]
per-ex loss: 0.747847  [   48/   88]
per-ex loss: 0.566506  [   49/   88]
per-ex loss: 0.567425  [   50/   88]
per-ex loss: 0.578256  [   51/   88]
per-ex loss: 0.739506  [   52/   88]
per-ex loss: 0.831371  [   53/   88]
per-ex loss: 0.825122  [   54/   88]
per-ex loss: 0.598333  [   55/   88]
per-ex loss: 0.599085  [   56/   88]
per-ex loss: 0.579025  [   57/   88]
per-ex loss: 0.758912  [   58/   88]
per-ex loss: 0.667726  [   59/   88]
per-ex loss: 0.821424  [   60/   88]
per-ex loss: 0.587310  [   61/   88]
per-ex loss: 0.783561  [   62/   88]
per-ex loss: 0.641414  [   63/   88]
per-ex loss: 0.800234  [   64/   88]
per-ex loss: 0.593568  [   65/   88]
per-ex loss: 0.824778  [   66/   88]
per-ex loss: 0.559105  [   67/   88]
per-ex loss: 0.789441  [   68/   88]
per-ex loss: 0.571906  [   69/   88]
per-ex loss: 0.820077  [   70/   88]
per-ex loss: 0.577600  [   71/   88]
per-ex loss: 0.711183  [   72/   88]
per-ex loss: 0.809777  [   73/   88]
per-ex loss: 0.689380  [   74/   88]
per-ex loss: 0.554740  [   75/   88]
per-ex loss: 0.512292  [   76/   88]
per-ex loss: 0.709798  [   77/   88]
per-ex loss: 0.740166  [   78/   88]
per-ex loss: 0.722565  [   79/   88]
per-ex loss: 0.754682  [   80/   88]
per-ex loss: 0.785485  [   81/   88]
per-ex loss: 0.769661  [   82/   88]
per-ex loss: 0.565799  [   83/   88]
per-ex loss: 0.731045  [   84/   88]
per-ex loss: 0.766855  [   85/   88]
per-ex loss: 0.634223  [   86/   88]
per-ex loss: 0.576383  [   87/   88]
per-ex loss: 0.618992  [   88/   88]
Train Error: Avg loss: 0.67836994
validation Error: 
 Avg loss: 0.70597904 
 F1: 0.484159 
 Precision: 0.515731 
 Recall: 0.456230
 IoU: 0.319400

test Error: 
 Avg loss: 0.65417880 
 F1: 0.561729 
 Precision: 0.572627 
 Recall: 0.551237
 IoU: 0.390558

We have finished training iteration 19
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_17_.pth
per-ex loss: 0.655065  [    1/   88]
per-ex loss: 0.824405  [    2/   88]
per-ex loss: 0.552539  [    3/   88]
per-ex loss: 0.744384  [    4/   88]
per-ex loss: 0.531355  [    5/   88]
per-ex loss: 0.618837  [    6/   88]
per-ex loss: 0.728877  [    7/   88]
per-ex loss: 0.813078  [    8/   88]
per-ex loss: 0.844619  [    9/   88]
per-ex loss: 0.557059  [   10/   88]
per-ex loss: 0.751052  [   11/   88]
per-ex loss: 0.788677  [   12/   88]
per-ex loss: 0.721551  [   13/   88]
per-ex loss: 0.561095  [   14/   88]
per-ex loss: 0.743636  [   15/   88]
per-ex loss: 0.831101  [   16/   88]
per-ex loss: 0.591399  [   17/   88]
per-ex loss: 0.820662  [   18/   88]
per-ex loss: 0.756846  [   19/   88]
per-ex loss: 0.623694  [   20/   88]
per-ex loss: 0.601776  [   21/   88]
per-ex loss: 0.630124  [   22/   88]
per-ex loss: 0.616974  [   23/   88]
per-ex loss: 0.794180  [   24/   88]
per-ex loss: 0.558453  [   25/   88]
per-ex loss: 0.669044  [   26/   88]
per-ex loss: 0.812652  [   27/   88]
per-ex loss: 0.574757  [   28/   88]
per-ex loss: 0.756070  [   29/   88]
per-ex loss: 0.565199  [   30/   88]
per-ex loss: 0.649275  [   31/   88]
per-ex loss: 0.660709  [   32/   88]
per-ex loss: 0.633867  [   33/   88]
per-ex loss: 0.721603  [   34/   88]
per-ex loss: 0.767417  [   35/   88]
per-ex loss: 0.724074  [   36/   88]
per-ex loss: 0.590442  [   37/   88]
per-ex loss: 0.562505  [   38/   88]
per-ex loss: 0.801232  [   39/   88]
per-ex loss: 0.748554  [   40/   88]
per-ex loss: 0.649971  [   41/   88]
per-ex loss: 0.722097  [   42/   88]
per-ex loss: 0.822806  [   43/   88]
per-ex loss: 0.565223  [   44/   88]
per-ex loss: 0.551015  [   45/   88]
per-ex loss: 0.552533  [   46/   88]
per-ex loss: 0.606118  [   47/   88]
per-ex loss: 0.789237  [   48/   88]
per-ex loss: 0.732868  [   49/   88]
per-ex loss: 0.769124  [   50/   88]
per-ex loss: 0.713255  [   51/   88]
per-ex loss: 0.501692  [   52/   88]
per-ex loss: 0.758981  [   53/   88]
per-ex loss: 0.597370  [   54/   88]
per-ex loss: 0.577512  [   55/   88]
per-ex loss: 0.765733  [   56/   88]
per-ex loss: 0.636844  [   57/   88]
per-ex loss: 0.621432  [   58/   88]
per-ex loss: 0.791724  [   59/   88]
per-ex loss: 0.642918  [   60/   88]
per-ex loss: 0.577207  [   61/   88]
per-ex loss: 0.587628  [   62/   88]
per-ex loss: 0.771006  [   63/   88]
per-ex loss: 0.542945  [   64/   88]
per-ex loss: 0.567651  [   65/   88]
per-ex loss: 0.742283  [   66/   88]
per-ex loss: 0.585905  [   67/   88]
per-ex loss: 0.755945  [   68/   88]
per-ex loss: 0.831194  [   69/   88]
per-ex loss: 0.715574  [   70/   88]
per-ex loss: 0.652471  [   71/   88]
per-ex loss: 0.559452  [   72/   88]
per-ex loss: 0.729482  [   73/   88]
per-ex loss: 0.568321  [   74/   88]
per-ex loss: 0.649381  [   75/   88]
per-ex loss: 0.735515  [   76/   88]
per-ex loss: 0.672394  [   77/   88]
per-ex loss: 0.649793  [   78/   88]
per-ex loss: 0.628048  [   79/   88]
per-ex loss: 0.603333  [   80/   88]
per-ex loss: 0.742676  [   81/   88]
per-ex loss: 0.795234  [   82/   88]
per-ex loss: 0.589265  [   83/   88]
per-ex loss: 0.653419  [   84/   88]
per-ex loss: 0.547382  [   85/   88]
per-ex loss: 0.781493  [   86/   88]
per-ex loss: 0.567861  [   87/   88]
per-ex loss: 0.583630  [   88/   88]
Train Error: Avg loss: 0.67333845
validation Error: 
 Avg loss: 0.74519876 
 F1: 0.426193 
 Precision: 0.450872 
 Recall: 0.404074
 IoU: 0.270803

test Error: 
 Avg loss: 0.66942251 
 F1: 0.535464 
 Precision: 0.595934 
 Recall: 0.486136
 IoU: 0.365620

We have finished training iteration 20
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_18_.pth
per-ex loss: 0.739624  [    1/   88]
per-ex loss: 0.814254  [    2/   88]
per-ex loss: 0.740958  [    3/   88]
per-ex loss: 0.730379  [    4/   88]
per-ex loss: 0.624852  [    5/   88]
per-ex loss: 0.580974  [    6/   88]
per-ex loss: 0.590988  [    7/   88]
per-ex loss: 0.755861  [    8/   88]
per-ex loss: 0.583315  [    9/   88]
per-ex loss: 0.587951  [   10/   88]
per-ex loss: 0.704600  [   11/   88]
per-ex loss: 0.646084  [   12/   88]
per-ex loss: 0.529391  [   13/   88]
per-ex loss: 0.765649  [   14/   88]
per-ex loss: 0.752170  [   15/   88]
per-ex loss: 0.737518  [   16/   88]
per-ex loss: 0.589510  [   17/   88]
per-ex loss: 0.572062  [   18/   88]
per-ex loss: 0.602620  [   19/   88]
per-ex loss: 0.718675  [   20/   88]
per-ex loss: 0.628914  [   21/   88]
per-ex loss: 0.743278  [   22/   88]
per-ex loss: 0.837771  [   23/   88]
per-ex loss: 0.602566  [   24/   88]
per-ex loss: 0.641134  [   25/   88]
per-ex loss: 0.507772  [   26/   88]
per-ex loss: 0.642641  [   27/   88]
per-ex loss: 0.833272  [   28/   88]
per-ex loss: 0.728570  [   29/   88]
per-ex loss: 0.567471  [   30/   88]
per-ex loss: 0.817032  [   31/   88]
per-ex loss: 0.736091  [   32/   88]
per-ex loss: 0.802179  [   33/   88]
per-ex loss: 0.591114  [   34/   88]
per-ex loss: 0.561144  [   35/   88]
per-ex loss: 0.607302  [   36/   88]
per-ex loss: 0.770220  [   37/   88]
per-ex loss: 0.789527  [   38/   88]
per-ex loss: 0.790113  [   39/   88]
per-ex loss: 0.576025  [   40/   88]
per-ex loss: 0.665679  [   41/   88]
per-ex loss: 0.655594  [   42/   88]
per-ex loss: 0.610088  [   43/   88]
per-ex loss: 0.802866  [   44/   88]
per-ex loss: 0.702696  [   45/   88]
per-ex loss: 0.701200  [   46/   88]
per-ex loss: 0.553657  [   47/   88]
per-ex loss: 0.620354  [   48/   88]
per-ex loss: 0.574741  [   49/   88]
per-ex loss: 0.549936  [   50/   88]
per-ex loss: 0.748215  [   51/   88]
per-ex loss: 0.601421  [   52/   88]
per-ex loss: 0.647887  [   53/   88]
per-ex loss: 0.552088  [   54/   88]
per-ex loss: 0.632392  [   55/   88]
per-ex loss: 0.624829  [   56/   88]
per-ex loss: 0.624427  [   57/   88]
per-ex loss: 0.615468  [   58/   88]
per-ex loss: 0.550120  [   59/   88]
per-ex loss: 0.723511  [   60/   88]
per-ex loss: 0.521791  [   61/   88]
per-ex loss: 0.735709  [   62/   88]
per-ex loss: 0.601328  [   63/   88]
per-ex loss: 0.733247  [   64/   88]
per-ex loss: 0.775103  [   65/   88]
per-ex loss: 0.698937  [   66/   88]
per-ex loss: 0.749357  [   67/   88]
per-ex loss: 0.763427  [   68/   88]
per-ex loss: 0.725256  [   69/   88]
per-ex loss: 0.590563  [   70/   88]
per-ex loss: 0.604811  [   71/   88]
per-ex loss: 0.780496  [   72/   88]
per-ex loss: 0.787808  [   73/   88]
per-ex loss: 0.772152  [   74/   88]
per-ex loss: 0.777292  [   75/   88]
per-ex loss: 0.785562  [   76/   88]
per-ex loss: 0.573100  [   77/   88]
per-ex loss: 0.814908  [   78/   88]
per-ex loss: 0.708564  [   79/   88]
per-ex loss: 0.772594  [   80/   88]
per-ex loss: 0.593564  [   81/   88]
per-ex loss: 0.525094  [   82/   88]
per-ex loss: 0.757537  [   83/   88]
per-ex loss: 0.523550  [   84/   88]
per-ex loss: 0.554639  [   85/   88]
per-ex loss: 0.601052  [   86/   88]
per-ex loss: 0.659088  [   87/   88]
per-ex loss: 0.832138  [   88/   88]
Train Error: Avg loss: 0.67292507
validation Error: 
 Avg loss: 0.71720729 
 F1: 0.459985 
 Precision: 0.683811 
 Recall: 0.346551
 IoU: 0.298688

test Error: 
 Avg loss: 0.67312464 
 F1: 0.532817 
 Precision: 0.713346 
 Recall: 0.425208
 IoU: 0.363156

We have finished training iteration 21
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_15_.pth
per-ex loss: 0.571214  [    1/   88]
per-ex loss: 0.586088  [    2/   88]
per-ex loss: 0.747755  [    3/   88]
per-ex loss: 0.705473  [    4/   88]
per-ex loss: 0.836805  [    5/   88]
per-ex loss: 0.730046  [    6/   88]
per-ex loss: 0.544910  [    7/   88]
per-ex loss: 0.681154  [    8/   88]
per-ex loss: 0.617510  [    9/   88]
per-ex loss: 0.589603  [   10/   88]
per-ex loss: 0.847141  [   11/   88]
per-ex loss: 0.518707  [   12/   88]
per-ex loss: 0.834230  [   13/   88]
per-ex loss: 0.544886  [   14/   88]
per-ex loss: 0.790383  [   15/   88]
per-ex loss: 0.736010  [   16/   88]
per-ex loss: 0.774526  [   17/   88]
per-ex loss: 0.788909  [   18/   88]
per-ex loss: 0.731060  [   19/   88]
per-ex loss: 0.588884  [   20/   88]
per-ex loss: 0.782165  [   21/   88]
per-ex loss: 0.528895  [   22/   88]
per-ex loss: 0.586498  [   23/   88]
per-ex loss: 0.597409  [   24/   88]
per-ex loss: 0.570920  [   25/   88]
per-ex loss: 0.816249  [   26/   88]
per-ex loss: 0.596315  [   27/   88]
per-ex loss: 0.637485  [   28/   88]
per-ex loss: 0.800364  [   29/   88]
per-ex loss: 0.562940  [   30/   88]
per-ex loss: 0.756953  [   31/   88]
per-ex loss: 0.583880  [   32/   88]
per-ex loss: 0.607269  [   33/   88]
per-ex loss: 0.839325  [   34/   88]
per-ex loss: 0.775642  [   35/   88]
per-ex loss: 0.800871  [   36/   88]
per-ex loss: 0.555051  [   37/   88]
per-ex loss: 0.606880  [   38/   88]
per-ex loss: 0.744185  [   39/   88]
per-ex loss: 0.592267  [   40/   88]
per-ex loss: 0.742349  [   41/   88]
per-ex loss: 0.563989  [   42/   88]
per-ex loss: 0.603062  [   43/   88]
per-ex loss: 0.602934  [   44/   88]
per-ex loss: 0.661390  [   45/   88]
per-ex loss: 0.706007  [   46/   88]
per-ex loss: 0.600806  [   47/   88]
per-ex loss: 0.488707  [   48/   88]
per-ex loss: 0.721523  [   49/   88]
per-ex loss: 0.715777  [   50/   88]
per-ex loss: 0.564890  [   51/   88]
per-ex loss: 0.730343  [   52/   88]
per-ex loss: 0.626646  [   53/   88]
per-ex loss: 0.628686  [   54/   88]
per-ex loss: 0.534719  [   55/   88]
per-ex loss: 0.650504  [   56/   88]
per-ex loss: 0.749200  [   57/   88]
per-ex loss: 0.636504  [   58/   88]
per-ex loss: 0.566425  [   59/   88]
per-ex loss: 0.717507  [   60/   88]
per-ex loss: 0.759192  [   61/   88]
per-ex loss: 0.730597  [   62/   88]
per-ex loss: 0.618974  [   63/   88]
per-ex loss: 0.711621  [   64/   88]
per-ex loss: 0.732528  [   65/   88]
per-ex loss: 0.642926  [   66/   88]
per-ex loss: 0.753157  [   67/   88]
per-ex loss: 0.707316  [   68/   88]
per-ex loss: 0.815349  [   69/   88]
per-ex loss: 0.775940  [   70/   88]
per-ex loss: 0.566644  [   71/   88]
per-ex loss: 0.604173  [   72/   88]
per-ex loss: 0.602932  [   73/   88]
per-ex loss: 0.813758  [   74/   88]
per-ex loss: 0.626536  [   75/   88]
per-ex loss: 0.587916  [   76/   88]
per-ex loss: 0.656262  [   77/   88]
per-ex loss: 0.756285  [   78/   88]
per-ex loss: 0.782424  [   79/   88]
per-ex loss: 0.584348  [   80/   88]
per-ex loss: 0.555258  [   81/   88]
per-ex loss: 0.785528  [   82/   88]
per-ex loss: 0.801273  [   83/   88]
per-ex loss: 0.530367  [   84/   88]
per-ex loss: 0.545742  [   85/   88]
per-ex loss: 0.757757  [   86/   88]
per-ex loss: 0.628220  [   87/   88]
per-ex loss: 0.748058  [   88/   88]
Train Error: Avg loss: 0.67158987
validation Error: 
 Avg loss: 0.71316190 
 F1: 0.465388 
 Precision: 0.500589 
 Recall: 0.434813
 IoU: 0.303261

test Error: 
 Avg loss: 0.65213601 
 F1: 0.562281 
 Precision: 0.588726 
 Recall: 0.538111
 IoU: 0.391093

We have finished training iteration 22
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_20_.pth
per-ex loss: 0.560767  [    1/   88]
per-ex loss: 0.767912  [    2/   88]
per-ex loss: 0.598196  [    3/   88]
per-ex loss: 0.813890  [    4/   88]
per-ex loss: 0.614837  [    5/   88]
per-ex loss: 0.580823  [    6/   88]
per-ex loss: 0.767467  [    7/   88]
per-ex loss: 0.631886  [    8/   88]
per-ex loss: 0.526972  [    9/   88]
per-ex loss: 0.801919  [   10/   88]
per-ex loss: 0.570433  [   11/   88]
per-ex loss: 0.545616  [   12/   88]
per-ex loss: 0.626659  [   13/   88]
per-ex loss: 0.601488  [   14/   88]
per-ex loss: 0.628337  [   15/   88]
per-ex loss: 0.764967  [   16/   88]
per-ex loss: 0.706161  [   17/   88]
per-ex loss: 0.648464  [   18/   88]
per-ex loss: 0.548917  [   19/   88]
per-ex loss: 0.756509  [   20/   88]
per-ex loss: 0.528392  [   21/   88]
per-ex loss: 0.709683  [   22/   88]
per-ex loss: 0.838735  [   23/   88]
per-ex loss: 0.787745  [   24/   88]
per-ex loss: 0.623861  [   25/   88]
per-ex loss: 0.635325  [   26/   88]
per-ex loss: 0.615626  [   27/   88]
per-ex loss: 0.574157  [   28/   88]
per-ex loss: 0.556851  [   29/   88]
per-ex loss: 0.538126  [   30/   88]
per-ex loss: 0.569904  [   31/   88]
per-ex loss: 0.754850  [   32/   88]
per-ex loss: 0.637968  [   33/   88]
per-ex loss: 0.797872  [   34/   88]
per-ex loss: 0.726031  [   35/   88]
per-ex loss: 0.739050  [   36/   88]
per-ex loss: 0.596303  [   37/   88]
per-ex loss: 0.580891  [   38/   88]
per-ex loss: 0.625098  [   39/   88]
per-ex loss: 0.542545  [   40/   88]
per-ex loss: 0.551565  [   41/   88]
per-ex loss: 0.821102  [   42/   88]
per-ex loss: 0.674883  [   43/   88]
per-ex loss: 0.673303  [   44/   88]
per-ex loss: 0.705680  [   45/   88]
per-ex loss: 0.742823  [   46/   88]
per-ex loss: 0.745118  [   47/   88]
per-ex loss: 0.741280  [   48/   88]
per-ex loss: 0.797246  [   49/   88]
per-ex loss: 0.792473  [   50/   88]
per-ex loss: 0.583533  [   51/   88]
per-ex loss: 0.686611  [   52/   88]
per-ex loss: 0.822228  [   53/   88]
per-ex loss: 0.573120  [   54/   88]
per-ex loss: 0.760701  [   55/   88]
per-ex loss: 0.721913  [   56/   88]
per-ex loss: 0.614318  [   57/   88]
per-ex loss: 0.736957  [   58/   88]
per-ex loss: 0.562331  [   59/   88]
per-ex loss: 0.724993  [   60/   88]
per-ex loss: 0.716072  [   61/   88]
per-ex loss: 0.570421  [   62/   88]
per-ex loss: 0.828606  [   63/   88]
per-ex loss: 0.804189  [   64/   88]
per-ex loss: 0.739547  [   65/   88]
per-ex loss: 0.497636  [   66/   88]
per-ex loss: 0.658732  [   67/   88]
per-ex loss: 0.588278  [   68/   88]
per-ex loss: 0.726967  [   69/   88]
per-ex loss: 0.581389  [   70/   88]
per-ex loss: 0.790101  [   71/   88]
per-ex loss: 0.592044  [   72/   88]
per-ex loss: 0.840892  [   73/   88]
per-ex loss: 0.501658  [   74/   88]
per-ex loss: 0.540210  [   75/   88]
per-ex loss: 0.782631  [   76/   88]
per-ex loss: 0.625399  [   77/   88]
per-ex loss: 0.658334  [   78/   88]
per-ex loss: 0.762136  [   79/   88]
per-ex loss: 0.558248  [   80/   88]
per-ex loss: 0.710640  [   81/   88]
per-ex loss: 0.709530  [   82/   88]
per-ex loss: 0.587244  [   83/   88]
per-ex loss: 0.763088  [   84/   88]
per-ex loss: 0.620064  [   85/   88]
per-ex loss: 0.798238  [   86/   88]
per-ex loss: 0.629565  [   87/   88]
per-ex loss: 0.551105  [   88/   88]
Train Error: Avg loss: 0.66825434
validation Error: 
 Avg loss: 0.70366654 
 F1: 0.483392 
 Precision: 0.501102 
 Recall: 0.466891
 IoU: 0.318732

test Error: 
 Avg loss: 0.65554893 
 F1: 0.557375 
 Precision: 0.542693 
 Recall: 0.572873
 IoU: 0.386362

We have finished training iteration 23
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_21_.pth
per-ex loss: 0.625507  [    1/   88]
per-ex loss: 0.664103  [    2/   88]
per-ex loss: 0.808537  [    3/   88]
per-ex loss: 0.777551  [    4/   88]
per-ex loss: 0.726599  [    5/   88]
per-ex loss: 0.599895  [    6/   88]
per-ex loss: 0.738574  [    7/   88]
per-ex loss: 0.612186  [    8/   88]
per-ex loss: 0.763876  [    9/   88]
per-ex loss: 0.752638  [   10/   88]
per-ex loss: 0.811527  [   11/   88]
per-ex loss: 0.554901  [   12/   88]
per-ex loss: 0.648257  [   13/   88]
per-ex loss: 0.589021  [   14/   88]
per-ex loss: 0.803643  [   15/   88]
per-ex loss: 0.809962  [   16/   88]
per-ex loss: 0.667903  [   17/   88]
per-ex loss: 0.626434  [   18/   88]
per-ex loss: 0.715098  [   19/   88]
per-ex loss: 0.536315  [   20/   88]
per-ex loss: 0.619236  [   21/   88]
per-ex loss: 0.628524  [   22/   88]
per-ex loss: 0.544484  [   23/   88]
per-ex loss: 0.727293  [   24/   88]
per-ex loss: 0.825097  [   25/   88]
per-ex loss: 0.707297  [   26/   88]
per-ex loss: 0.580592  [   27/   88]
per-ex loss: 0.736637  [   28/   88]
per-ex loss: 0.581749  [   29/   88]
per-ex loss: 0.593291  [   30/   88]
per-ex loss: 0.608056  [   31/   88]
per-ex loss: 0.735078  [   32/   88]
per-ex loss: 0.608128  [   33/   88]
per-ex loss: 0.597306  [   34/   88]
per-ex loss: 0.583986  [   35/   88]
per-ex loss: 0.724058  [   36/   88]
per-ex loss: 0.560666  [   37/   88]
per-ex loss: 0.616458  [   38/   88]
per-ex loss: 0.738875  [   39/   88]
per-ex loss: 0.717056  [   40/   88]
per-ex loss: 0.622826  [   41/   88]
per-ex loss: 0.727907  [   42/   88]
per-ex loss: 0.530297  [   43/   88]
per-ex loss: 0.739399  [   44/   88]
per-ex loss: 0.734600  [   45/   88]
per-ex loss: 0.538539  [   46/   88]
per-ex loss: 0.700032  [   47/   88]
per-ex loss: 0.833919  [   48/   88]
per-ex loss: 0.637804  [   49/   88]
per-ex loss: 0.774861  [   50/   88]
per-ex loss: 0.564084  [   51/   88]
per-ex loss: 0.576900  [   52/   88]
per-ex loss: 0.566147  [   53/   88]
per-ex loss: 0.703399  [   54/   88]
per-ex loss: 0.790000  [   55/   88]
per-ex loss: 0.762000  [   56/   88]
per-ex loss: 0.778317  [   57/   88]
per-ex loss: 0.705000  [   58/   88]
per-ex loss: 0.545553  [   59/   88]
per-ex loss: 0.627488  [   60/   88]
per-ex loss: 0.549133  [   61/   88]
per-ex loss: 0.711697  [   62/   88]
per-ex loss: 0.592652  [   63/   88]
per-ex loss: 0.720893  [   64/   88]
per-ex loss: 0.631194  [   65/   88]
per-ex loss: 0.715505  [   66/   88]
per-ex loss: 0.787982  [   67/   88]
per-ex loss: 0.745088  [   68/   88]
per-ex loss: 0.804066  [   69/   88]
per-ex loss: 0.591529  [   70/   88]
per-ex loss: 0.594221  [   71/   88]
per-ex loss: 0.476260  [   72/   88]
per-ex loss: 0.774382  [   73/   88]
per-ex loss: 0.570493  [   74/   88]
per-ex loss: 0.797134  [   75/   88]
per-ex loss: 0.601732  [   76/   88]
per-ex loss: 0.535926  [   77/   88]
per-ex loss: 0.563891  [   78/   88]
per-ex loss: 0.645759  [   79/   88]
per-ex loss: 0.730605  [   80/   88]
per-ex loss: 0.548745  [   81/   88]
per-ex loss: 0.584536  [   82/   88]
per-ex loss: 0.593369  [   83/   88]
per-ex loss: 0.525063  [   84/   88]
per-ex loss: 0.559862  [   85/   88]
per-ex loss: 0.844806  [   86/   88]
per-ex loss: 0.744049  [   87/   88]
per-ex loss: 0.526032  [   88/   88]
Train Error: Avg loss: 0.66354621
validation Error: 
 Avg loss: 0.72180945 
 F1: 0.457049 
 Precision: 0.655437 
 Recall: 0.350853
 IoU: 0.296217

test Error: 
 Avg loss: 0.67102527 
 F1: 0.535549 
 Precision: 0.701835 
 Recall: 0.432966
 IoU: 0.365699

We have finished training iteration 24
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_13_.pth
per-ex loss: 0.782029  [    1/   88]
per-ex loss: 0.583281  [    2/   88]
per-ex loss: 0.525695  [    3/   88]
per-ex loss: 0.784664  [    4/   88]
per-ex loss: 0.631424  [    5/   88]
per-ex loss: 0.742174  [    6/   88]
per-ex loss: 0.719516  [    7/   88]
per-ex loss: 0.743469  [    8/   88]
per-ex loss: 0.637821  [    9/   88]
per-ex loss: 0.567790  [   10/   88]
per-ex loss: 0.759751  [   11/   88]
per-ex loss: 0.618658  [   12/   88]
per-ex loss: 0.780892  [   13/   88]
per-ex loss: 0.711989  [   14/   88]
per-ex loss: 0.598086  [   15/   88]
per-ex loss: 0.660822  [   16/   88]
per-ex loss: 0.608754  [   17/   88]
per-ex loss: 0.741319  [   18/   88]
per-ex loss: 0.760568  [   19/   88]
per-ex loss: 0.813405  [   20/   88]
per-ex loss: 0.766488  [   21/   88]
per-ex loss: 0.613373  [   22/   88]
per-ex loss: 0.546079  [   23/   88]
per-ex loss: 0.723720  [   24/   88]
per-ex loss: 0.607528  [   25/   88]
per-ex loss: 0.517776  [   26/   88]
per-ex loss: 0.726822  [   27/   88]
per-ex loss: 0.563842  [   28/   88]
per-ex loss: 0.705186  [   29/   88]
per-ex loss: 0.695332  [   30/   88]
per-ex loss: 0.725904  [   31/   88]
per-ex loss: 0.586158  [   32/   88]
per-ex loss: 0.566870  [   33/   88]
per-ex loss: 0.612049  [   34/   88]
per-ex loss: 0.587428  [   35/   88]
per-ex loss: 0.561817  [   36/   88]
per-ex loss: 0.639597  [   37/   88]
per-ex loss: 0.732967  [   38/   88]
per-ex loss: 0.789816  [   39/   88]
per-ex loss: 0.593857  [   40/   88]
per-ex loss: 0.827493  [   41/   88]
per-ex loss: 0.585390  [   42/   88]
per-ex loss: 0.609599  [   43/   88]
per-ex loss: 0.638999  [   44/   88]
per-ex loss: 0.819559  [   45/   88]
per-ex loss: 0.758065  [   46/   88]
per-ex loss: 0.553300  [   47/   88]
per-ex loss: 0.629562  [   48/   88]
per-ex loss: 0.624144  [   49/   88]
per-ex loss: 0.775956  [   50/   88]
per-ex loss: 0.810366  [   51/   88]
per-ex loss: 0.590595  [   52/   88]
per-ex loss: 0.533270  [   53/   88]
per-ex loss: 0.843197  [   54/   88]
per-ex loss: 0.537803  [   55/   88]
per-ex loss: 0.763226  [   56/   88]
per-ex loss: 0.732864  [   57/   88]
per-ex loss: 0.831370  [   58/   88]
per-ex loss: 0.746244  [   59/   88]
per-ex loss: 0.755872  [   60/   88]
per-ex loss: 0.709471  [   61/   88]
per-ex loss: 0.574114  [   62/   88]
per-ex loss: 0.538527  [   63/   88]
per-ex loss: 0.774106  [   64/   88]
per-ex loss: 0.828833  [   65/   88]
per-ex loss: 0.701229  [   66/   88]
per-ex loss: 0.745398  [   67/   88]
per-ex loss: 0.632685  [   68/   88]
per-ex loss: 0.532020  [   69/   88]
per-ex loss: 0.819487  [   70/   88]
per-ex loss: 0.767516  [   71/   88]
per-ex loss: 0.605547  [   72/   88]
per-ex loss: 0.647840  [   73/   88]
per-ex loss: 0.611714  [   74/   88]
per-ex loss: 0.588309  [   75/   88]
per-ex loss: 0.547960  [   76/   88]
per-ex loss: 0.739360  [   77/   88]
per-ex loss: 0.583702  [   78/   88]
per-ex loss: 0.493785  [   79/   88]
per-ex loss: 0.600201  [   80/   88]
per-ex loss: 0.794228  [   81/   88]
per-ex loss: 0.589901  [   82/   88]
per-ex loss: 0.662361  [   83/   88]
per-ex loss: 0.818148  [   84/   88]
per-ex loss: 0.503095  [   85/   88]
per-ex loss: 0.779527  [   86/   88]
per-ex loss: 0.613949  [   87/   88]
per-ex loss: 0.570559  [   88/   88]
Train Error: Avg loss: 0.67017230
validation Error: 
 Avg loss: 0.71693734 
 F1: 0.460235 
 Precision: 0.652383 
 Recall: 0.355522
 IoU: 0.298899

test Error: 
 Avg loss: 0.66841340 
 F1: 0.540548 
 Precision: 0.676876 
 Recall: 0.449929
 IoU: 0.370377

We have finished training iteration 25
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_22_.pth
per-ex loss: 0.708024  [    1/   88]
per-ex loss: 0.802540  [    2/   88]
per-ex loss: 0.659613  [    3/   88]
per-ex loss: 0.587621  [    4/   88]
per-ex loss: 0.493169  [    5/   88]
per-ex loss: 0.570624  [    6/   88]
per-ex loss: 0.732469  [    7/   88]
per-ex loss: 0.577899  [    8/   88]
per-ex loss: 0.791639  [    9/   88]
per-ex loss: 0.697150  [   10/   88]
per-ex loss: 0.606796  [   11/   88]
per-ex loss: 0.728974  [   12/   88]
per-ex loss: 0.632782  [   13/   88]
per-ex loss: 0.786961  [   14/   88]
per-ex loss: 0.738937  [   15/   88]
per-ex loss: 0.637049  [   16/   88]
per-ex loss: 0.642564  [   17/   88]
per-ex loss: 0.759005  [   18/   88]
per-ex loss: 0.721997  [   19/   88]
per-ex loss: 0.772855  [   20/   88]
per-ex loss: 0.530307  [   21/   88]
per-ex loss: 0.579961  [   22/   88]
per-ex loss: 0.657735  [   23/   88]
per-ex loss: 0.720100  [   24/   88]
per-ex loss: 0.562570  [   25/   88]
per-ex loss: 0.780721  [   26/   88]
per-ex loss: 0.619420  [   27/   88]
per-ex loss: 0.725102  [   28/   88]
per-ex loss: 0.728305  [   29/   88]
per-ex loss: 0.565270  [   30/   88]
per-ex loss: 0.570612  [   31/   88]
per-ex loss: 0.701536  [   32/   88]
per-ex loss: 0.706913  [   33/   88]
per-ex loss: 0.523346  [   34/   88]
per-ex loss: 0.551087  [   35/   88]
per-ex loss: 0.749076  [   36/   88]
per-ex loss: 0.601426  [   37/   88]
per-ex loss: 0.558319  [   38/   88]
per-ex loss: 0.598724  [   39/   88]
per-ex loss: 0.835615  [   40/   88]
per-ex loss: 0.572071  [   41/   88]
per-ex loss: 0.600965  [   42/   88]
per-ex loss: 0.531788  [   43/   88]
per-ex loss: 0.531973  [   44/   88]
per-ex loss: 0.762639  [   45/   88]
per-ex loss: 0.511892  [   46/   88]
per-ex loss: 0.625601  [   47/   88]
per-ex loss: 0.542589  [   48/   88]
per-ex loss: 0.580573  [   49/   88]
per-ex loss: 0.731135  [   50/   88]
per-ex loss: 0.628510  [   51/   88]
per-ex loss: 0.822976  [   52/   88]
per-ex loss: 0.640891  [   53/   88]
per-ex loss: 0.829255  [   54/   88]
per-ex loss: 0.810644  [   55/   88]
per-ex loss: 0.560814  [   56/   88]
per-ex loss: 0.632970  [   57/   88]
per-ex loss: 0.746097  [   58/   88]
per-ex loss: 0.811164  [   59/   88]
per-ex loss: 0.618276  [   60/   88]
per-ex loss: 0.697015  [   61/   88]
per-ex loss: 0.674445  [   62/   88]
per-ex loss: 0.602961  [   63/   88]
per-ex loss: 0.729667  [   64/   88]
per-ex loss: 0.773238  [   65/   88]
per-ex loss: 0.566894  [   66/   88]
per-ex loss: 0.608548  [   67/   88]
per-ex loss: 0.806627  [   68/   88]
per-ex loss: 0.767747  [   69/   88]
per-ex loss: 0.579224  [   70/   88]
per-ex loss: 0.827163  [   71/   88]
per-ex loss: 0.639896  [   72/   88]
per-ex loss: 0.746139  [   73/   88]
per-ex loss: 0.771582  [   74/   88]
per-ex loss: 0.818202  [   75/   88]
per-ex loss: 0.652246  [   76/   88]
per-ex loss: 0.787077  [   77/   88]
per-ex loss: 0.705186  [   78/   88]
per-ex loss: 0.549181  [   79/   88]
per-ex loss: 0.521381  [   80/   88]
per-ex loss: 0.603395  [   81/   88]
per-ex loss: 0.788473  [   82/   88]
per-ex loss: 0.558344  [   83/   88]
per-ex loss: 0.814653  [   84/   88]
per-ex loss: 0.753562  [   85/   88]
per-ex loss: 0.695308  [   86/   88]
per-ex loss: 0.596264  [   87/   88]
per-ex loss: 0.579193  [   88/   88]
Train Error: Avg loss: 0.66842329
validation Error: 
 Avg loss: 0.70440750 
 F1: 0.485003 
 Precision: 0.504001 
 Recall: 0.467385
 IoU: 0.320134

test Error: 
 Avg loss: 0.64920654 
 F1: 0.564835 
 Precision: 0.567656 
 Recall: 0.562042
 IoU: 0.393568

We have finished training iteration 26
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_24_.pth
per-ex loss: 0.803963  [    1/   88]
per-ex loss: 0.695797  [    2/   88]
per-ex loss: 0.716956  [    3/   88]
per-ex loss: 0.703489  [    4/   88]
per-ex loss: 0.702892  [    5/   88]
per-ex loss: 0.765464  [    6/   88]
per-ex loss: 0.641956  [    7/   88]
per-ex loss: 0.736803  [    8/   88]
per-ex loss: 0.789914  [    9/   88]
per-ex loss: 0.610751  [   10/   88]
per-ex loss: 0.750694  [   11/   88]
per-ex loss: 0.795214  [   12/   88]
per-ex loss: 0.602587  [   13/   88]
per-ex loss: 0.605611  [   14/   88]
per-ex loss: 0.552874  [   15/   88]
per-ex loss: 0.814642  [   16/   88]
per-ex loss: 0.761123  [   17/   88]
per-ex loss: 0.548910  [   18/   88]
per-ex loss: 0.721079  [   19/   88]
per-ex loss: 0.691431  [   20/   88]
per-ex loss: 0.572671  [   21/   88]
per-ex loss: 0.552573  [   22/   88]
per-ex loss: 0.750387  [   23/   88]
per-ex loss: 0.580986  [   24/   88]
per-ex loss: 0.561324  [   25/   88]
per-ex loss: 0.531333  [   26/   88]
per-ex loss: 0.777447  [   27/   88]
per-ex loss: 0.607307  [   28/   88]
per-ex loss: 0.637059  [   29/   88]
per-ex loss: 0.651275  [   30/   88]
per-ex loss: 0.497941  [   31/   88]
per-ex loss: 0.557577  [   32/   88]
per-ex loss: 0.812647  [   33/   88]
per-ex loss: 0.612272  [   34/   88]
per-ex loss: 0.614731  [   35/   88]
per-ex loss: 0.768352  [   36/   88]
per-ex loss: 0.641516  [   37/   88]
per-ex loss: 0.860953  [   38/   88]
per-ex loss: 0.611091  [   39/   88]
per-ex loss: 0.491598  [   40/   88]
per-ex loss: 0.750513  [   41/   88]
per-ex loss: 0.573719  [   42/   88]
per-ex loss: 0.753619  [   43/   88]
per-ex loss: 0.604150  [   44/   88]
per-ex loss: 0.693210  [   45/   88]
per-ex loss: 0.777401  [   46/   88]
per-ex loss: 0.633243  [   47/   88]
per-ex loss: 0.608323  [   48/   88]
per-ex loss: 0.783440  [   49/   88]
per-ex loss: 0.614417  [   50/   88]
per-ex loss: 0.627516  [   51/   88]
per-ex loss: 0.814097  [   52/   88]
per-ex loss: 0.705494  [   53/   88]
per-ex loss: 0.776013  [   54/   88]
per-ex loss: 0.655989  [   55/   88]
per-ex loss: 0.573980  [   56/   88]
per-ex loss: 0.537807  [   57/   88]
per-ex loss: 0.551353  [   58/   88]
per-ex loss: 0.560126  [   59/   88]
per-ex loss: 0.837277  [   60/   88]
per-ex loss: 0.610938  [   61/   88]
per-ex loss: 0.577427  [   62/   88]
per-ex loss: 0.591419  [   63/   88]
per-ex loss: 0.796770  [   64/   88]
per-ex loss: 0.748570  [   65/   88]
per-ex loss: 0.764084  [   66/   88]
per-ex loss: 0.717327  [   67/   88]
per-ex loss: 0.780637  [   68/   88]
per-ex loss: 0.559436  [   69/   88]
per-ex loss: 0.603928  [   70/   88]
per-ex loss: 0.577364  [   71/   88]
per-ex loss: 0.821737  [   72/   88]
per-ex loss: 0.708389  [   73/   88]
per-ex loss: 0.509731  [   74/   88]
per-ex loss: 0.751513  [   75/   88]
per-ex loss: 0.687272  [   76/   88]
per-ex loss: 0.585091  [   77/   88]
per-ex loss: 0.711063  [   78/   88]
per-ex loss: 0.622276  [   79/   88]
per-ex loss: 0.746778  [   80/   88]
per-ex loss: 0.559268  [   81/   88]
per-ex loss: 0.561248  [   82/   88]
per-ex loss: 0.531654  [   83/   88]
per-ex loss: 0.728006  [   84/   88]
per-ex loss: 0.645364  [   85/   88]
per-ex loss: 0.565398  [   86/   88]
per-ex loss: 0.759562  [   87/   88]
per-ex loss: 0.597208  [   88/   88]
Train Error: Avg loss: 0.66539017
validation Error: 
 Avg loss: 0.69721151 
 F1: 0.491771 
 Precision: 0.528932 
 Recall: 0.459489
 IoU: 0.326058

test Error: 
 Avg loss: 0.65487986 
 F1: 0.557889 
 Precision: 0.572645 
 Recall: 0.543875
 IoU: 0.386856

We have finished training iteration 27
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_25_.pth
per-ex loss: 0.639811  [    1/   88]
per-ex loss: 0.728091  [    2/   88]
per-ex loss: 0.705531  [    3/   88]
per-ex loss: 0.724205  [    4/   88]
per-ex loss: 0.620705  [    5/   88]
per-ex loss: 0.706212  [    6/   88]
per-ex loss: 0.590722  [    7/   88]
per-ex loss: 0.566767  [    8/   88]
per-ex loss: 0.797296  [    9/   88]
per-ex loss: 0.649895  [   10/   88]
per-ex loss: 0.776242  [   11/   88]
per-ex loss: 0.694695  [   12/   88]
per-ex loss: 0.575922  [   13/   88]
per-ex loss: 0.696442  [   14/   88]
per-ex loss: 0.601800  [   15/   88]
per-ex loss: 0.630539  [   16/   88]
per-ex loss: 0.781604  [   17/   88]
per-ex loss: 0.734044  [   18/   88]
per-ex loss: 0.632760  [   19/   88]
per-ex loss: 0.587652  [   20/   88]
per-ex loss: 0.607155  [   21/   88]
per-ex loss: 0.547344  [   22/   88]
per-ex loss: 0.594421  [   23/   88]
per-ex loss: 0.733217  [   24/   88]
per-ex loss: 0.568901  [   25/   88]
per-ex loss: 0.643636  [   26/   88]
per-ex loss: 0.839187  [   27/   88]
per-ex loss: 0.633444  [   28/   88]
per-ex loss: 0.517183  [   29/   88]
per-ex loss: 0.612997  [   30/   88]
per-ex loss: 0.567512  [   31/   88]
per-ex loss: 0.705147  [   32/   88]
per-ex loss: 0.493693  [   33/   88]
per-ex loss: 0.576433  [   34/   88]
per-ex loss: 0.766346  [   35/   88]
per-ex loss: 0.604410  [   36/   88]
per-ex loss: 0.765025  [   37/   88]
per-ex loss: 0.832388  [   38/   88]
per-ex loss: 0.536875  [   39/   88]
per-ex loss: 0.545316  [   40/   88]
per-ex loss: 0.783336  [   41/   88]
per-ex loss: 0.696741  [   42/   88]
per-ex loss: 0.481717  [   43/   88]
per-ex loss: 0.594815  [   44/   88]
per-ex loss: 0.718760  [   45/   88]
per-ex loss: 0.682525  [   46/   88]
per-ex loss: 0.671274  [   47/   88]
per-ex loss: 0.759430  [   48/   88]
per-ex loss: 0.557048  [   49/   88]
per-ex loss: 0.601210  [   50/   88]
per-ex loss: 0.768137  [   51/   88]
per-ex loss: 0.610858  [   52/   88]
per-ex loss: 0.609941  [   53/   88]
per-ex loss: 0.649503  [   54/   88]
per-ex loss: 0.620758  [   55/   88]
per-ex loss: 0.538445  [   56/   88]
per-ex loss: 0.573733  [   57/   88]
per-ex loss: 0.784876  [   58/   88]
per-ex loss: 0.593778  [   59/   88]
per-ex loss: 0.590148  [   60/   88]
per-ex loss: 0.733771  [   61/   88]
per-ex loss: 0.597199  [   62/   88]
per-ex loss: 0.684542  [   63/   88]
per-ex loss: 0.616466  [   64/   88]
per-ex loss: 0.714745  [   65/   88]
per-ex loss: 0.843378  [   66/   88]
per-ex loss: 0.753487  [   67/   88]
per-ex loss: 0.814859  [   68/   88]
per-ex loss: 0.627461  [   69/   88]
per-ex loss: 0.751561  [   70/   88]
per-ex loss: 0.608476  [   71/   88]
per-ex loss: 0.747911  [   72/   88]
per-ex loss: 0.758827  [   73/   88]
per-ex loss: 0.803395  [   74/   88]
per-ex loss: 0.544358  [   75/   88]
per-ex loss: 0.810159  [   76/   88]
per-ex loss: 0.557158  [   77/   88]
per-ex loss: 0.738743  [   78/   88]
per-ex loss: 0.542755  [   79/   88]
per-ex loss: 0.527206  [   80/   88]
per-ex loss: 0.784228  [   81/   88]
per-ex loss: 0.828459  [   82/   88]
per-ex loss: 0.772019  [   83/   88]
per-ex loss: 0.559400  [   84/   88]
per-ex loss: 0.768070  [   85/   88]
per-ex loss: 0.629818  [   86/   88]
per-ex loss: 0.818833  [   87/   88]
per-ex loss: 0.489012  [   88/   88]
Train Error: Avg loss: 0.66414653
validation Error: 
 Avg loss: 0.70016687 
 F1: 0.487542 
 Precision: 0.574747 
 Recall: 0.423313
 IoU: 0.322350

test Error: 
 Avg loss: 0.65153173 
 F1: 0.560895 
 Precision: 0.617109 
 Recall: 0.514068
 IoU: 0.389753

We have finished training iteration 28
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_19_.pth
per-ex loss: 0.486444  [    1/   88]
per-ex loss: 0.708216  [    2/   88]
per-ex loss: 0.538355  [    3/   88]
per-ex loss: 0.568093  [    4/   88]
per-ex loss: 0.572257  [    5/   88]
per-ex loss: 0.621625  [    6/   88]
per-ex loss: 0.647261  [    7/   88]
per-ex loss: 0.588811  [    8/   88]
per-ex loss: 0.536777  [    9/   88]
per-ex loss: 0.795075  [   10/   88]
per-ex loss: 0.801097  [   11/   88]
per-ex loss: 0.717231  [   12/   88]
per-ex loss: 0.839625  [   13/   88]
per-ex loss: 0.717488  [   14/   88]
per-ex loss: 0.558923  [   15/   88]
per-ex loss: 0.569672  [   16/   88]
per-ex loss: 0.556790  [   17/   88]
per-ex loss: 0.542442  [   18/   88]
per-ex loss: 0.582523  [   19/   88]
per-ex loss: 0.779062  [   20/   88]
per-ex loss: 0.763741  [   21/   88]
per-ex loss: 0.810735  [   22/   88]
per-ex loss: 0.813087  [   23/   88]
per-ex loss: 0.558805  [   24/   88]
per-ex loss: 0.641913  [   25/   88]
per-ex loss: 0.578789  [   26/   88]
per-ex loss: 0.741594  [   27/   88]
per-ex loss: 0.598491  [   28/   88]
per-ex loss: 0.579379  [   29/   88]
per-ex loss: 0.637876  [   30/   88]
per-ex loss: 0.713568  [   31/   88]
per-ex loss: 0.650027  [   32/   88]
per-ex loss: 0.731489  [   33/   88]
per-ex loss: 0.623015  [   34/   88]
per-ex loss: 0.564894  [   35/   88]
per-ex loss: 0.736079  [   36/   88]
per-ex loss: 0.555743  [   37/   88]
per-ex loss: 0.744386  [   38/   88]
per-ex loss: 0.770065  [   39/   88]
per-ex loss: 0.774284  [   40/   88]
per-ex loss: 0.811307  [   41/   88]
per-ex loss: 0.596604  [   42/   88]
per-ex loss: 0.533421  [   43/   88]
per-ex loss: 0.780322  [   44/   88]
per-ex loss: 0.788976  [   45/   88]
per-ex loss: 0.695602  [   46/   88]
per-ex loss: 0.712057  [   47/   88]
per-ex loss: 0.561407  [   48/   88]
per-ex loss: 0.611159  [   49/   88]
per-ex loss: 0.799824  [   50/   88]
per-ex loss: 0.563080  [   51/   88]
per-ex loss: 0.862749  [   52/   88]
per-ex loss: 0.663460  [   53/   88]
per-ex loss: 0.721382  [   54/   88]
per-ex loss: 0.825996  [   55/   88]
per-ex loss: 0.691959  [   56/   88]
per-ex loss: 0.632016  [   57/   88]
per-ex loss: 0.586427  [   58/   88]
per-ex loss: 0.552552  [   59/   88]
per-ex loss: 0.492337  [   60/   88]
per-ex loss: 0.666091  [   61/   88]
per-ex loss: 0.746631  [   62/   88]
per-ex loss: 0.517841  [   63/   88]
per-ex loss: 0.761018  [   64/   88]
per-ex loss: 0.728862  [   65/   88]
per-ex loss: 0.735169  [   66/   88]
per-ex loss: 0.596125  [   67/   88]
per-ex loss: 0.545768  [   68/   88]
per-ex loss: 0.758078  [   69/   88]
per-ex loss: 0.560107  [   70/   88]
per-ex loss: 0.625426  [   71/   88]
per-ex loss: 0.597022  [   72/   88]
per-ex loss: 0.683221  [   73/   88]
per-ex loss: 0.614740  [   74/   88]
per-ex loss: 0.748625  [   75/   88]
per-ex loss: 0.875221  [   76/   88]
per-ex loss: 0.785842  [   77/   88]
per-ex loss: 0.702132  [   78/   88]
per-ex loss: 0.546993  [   79/   88]
per-ex loss: 0.567474  [   80/   88]
per-ex loss: 0.561606  [   81/   88]
per-ex loss: 0.607403  [   82/   88]
per-ex loss: 0.602104  [   83/   88]
per-ex loss: 0.810504  [   84/   88]
per-ex loss: 0.631637  [   85/   88]
per-ex loss: 0.580528  [   86/   88]
per-ex loss: 0.711842  [   87/   88]
per-ex loss: 0.812404  [   88/   88]
Train Error: Avg loss: 0.66453157
validation Error: 
 Avg loss: 0.70383162 
 F1: 0.483222 
 Precision: 0.606658 
 Recall: 0.401525
 IoU: 0.318585

test Error: 
 Avg loss: 0.65685072 
 F1: 0.553993 
 Precision: 0.631401 
 Recall: 0.493492
 IoU: 0.383119

We have finished training iteration 29
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_26_.pth
per-ex loss: 0.700498  [    1/   88]
per-ex loss: 0.727211  [    2/   88]
per-ex loss: 0.829028  [    3/   88]
per-ex loss: 0.614065  [    4/   88]
per-ex loss: 0.640745  [    5/   88]
per-ex loss: 0.802299  [    6/   88]
per-ex loss: 0.651287  [    7/   88]
per-ex loss: 0.603758  [    8/   88]
per-ex loss: 0.692189  [    9/   88]
per-ex loss: 0.746799  [   10/   88]
per-ex loss: 0.838055  [   11/   88]
per-ex loss: 0.730144  [   12/   88]
per-ex loss: 0.619847  [   13/   88]
per-ex loss: 0.763258  [   14/   88]
per-ex loss: 0.616991  [   15/   88]
per-ex loss: 0.749688  [   16/   88]
per-ex loss: 0.569352  [   17/   88]
per-ex loss: 0.739715  [   18/   88]
per-ex loss: 0.631317  [   19/   88]
per-ex loss: 0.829494  [   20/   88]
per-ex loss: 0.559952  [   21/   88]
per-ex loss: 0.549392  [   22/   88]
per-ex loss: 0.762136  [   23/   88]
per-ex loss: 0.716538  [   24/   88]
per-ex loss: 0.607993  [   25/   88]
per-ex loss: 0.725429  [   26/   88]
per-ex loss: 0.563567  [   27/   88]
per-ex loss: 0.785543  [   28/   88]
per-ex loss: 0.520937  [   29/   88]
per-ex loss: 0.769865  [   30/   88]
per-ex loss: 0.583169  [   31/   88]
per-ex loss: 0.567450  [   32/   88]
per-ex loss: 0.601456  [   33/   88]
per-ex loss: 0.618277  [   34/   88]
per-ex loss: 0.595469  [   35/   88]
per-ex loss: 0.713998  [   36/   88]
per-ex loss: 0.760191  [   37/   88]
per-ex loss: 0.621286  [   38/   88]
per-ex loss: 0.802856  [   39/   88]
per-ex loss: 0.693192  [   40/   88]
per-ex loss: 0.800964  [   41/   88]
per-ex loss: 0.748471  [   42/   88]
per-ex loss: 0.588974  [   43/   88]
per-ex loss: 0.592436  [   44/   88]
per-ex loss: 0.736028  [   45/   88]
per-ex loss: 0.618825  [   46/   88]
per-ex loss: 0.578122  [   47/   88]
per-ex loss: 0.591262  [   48/   88]
per-ex loss: 0.729841  [   49/   88]
per-ex loss: 0.597117  [   50/   88]
per-ex loss: 0.578449  [   51/   88]
per-ex loss: 0.728189  [   52/   88]
per-ex loss: 0.812951  [   53/   88]
per-ex loss: 0.741486  [   54/   88]
per-ex loss: 0.703193  [   55/   88]
per-ex loss: 0.539699  [   56/   88]
per-ex loss: 0.580739  [   57/   88]
per-ex loss: 0.528746  [   58/   88]
per-ex loss: 0.831857  [   59/   88]
per-ex loss: 0.492213  [   60/   88]
per-ex loss: 0.796679  [   61/   88]
per-ex loss: 0.540573  [   62/   88]
per-ex loss: 0.701589  [   63/   88]
per-ex loss: 0.584506  [   64/   88]
per-ex loss: 0.558324  [   65/   88]
per-ex loss: 0.546316  [   66/   88]
per-ex loss: 0.517596  [   67/   88]
per-ex loss: 0.537934  [   68/   88]
per-ex loss: 0.648701  [   69/   88]
per-ex loss: 0.518710  [   70/   88]
per-ex loss: 0.566435  [   71/   88]
per-ex loss: 0.789598  [   72/   88]
per-ex loss: 0.612310  [   73/   88]
per-ex loss: 0.583213  [   74/   88]
per-ex loss: 0.604936  [   75/   88]
per-ex loss: 0.724682  [   76/   88]
per-ex loss: 0.793092  [   77/   88]
per-ex loss: 0.740803  [   78/   88]
per-ex loss: 0.730013  [   79/   88]
per-ex loss: 0.686965  [   80/   88]
per-ex loss: 0.815656  [   81/   88]
per-ex loss: 0.720085  [   82/   88]
per-ex loss: 0.616427  [   83/   88]
per-ex loss: 0.618942  [   84/   88]
per-ex loss: 0.559395  [   85/   88]
per-ex loss: 0.773559  [   86/   88]
per-ex loss: 0.563960  [   87/   88]
per-ex loss: 0.565387  [   88/   88]
Train Error: Avg loss: 0.66307252
validation Error: 
 Avg loss: 0.71100997 
 F1: 0.471136 
 Precision: 0.506348 
 Recall: 0.440502
 IoU: 0.308160

test Error: 
 Avg loss: 0.65094071 
 F1: 0.562319 
 Precision: 0.598140 
 Recall: 0.530545
 IoU: 0.391129

We have finished training iteration 30
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_23_.pth
per-ex loss: 0.525243  [    1/   88]
per-ex loss: 0.772686  [    2/   88]
per-ex loss: 0.774559  [    3/   88]
per-ex loss: 0.626843  [    4/   88]
per-ex loss: 0.580243  [    5/   88]
per-ex loss: 0.715326  [    6/   88]
per-ex loss: 0.559063  [    7/   88]
per-ex loss: 0.720258  [    8/   88]
per-ex loss: 0.546026  [    9/   88]
per-ex loss: 0.725426  [   10/   88]
per-ex loss: 0.578691  [   11/   88]
per-ex loss: 0.773754  [   12/   88]
per-ex loss: 0.585398  [   13/   88]
per-ex loss: 0.593716  [   14/   88]
per-ex loss: 0.504223  [   15/   88]
per-ex loss: 0.822232  [   16/   88]
per-ex loss: 0.795657  [   17/   88]
per-ex loss: 0.549065  [   18/   88]
per-ex loss: 0.567226  [   19/   88]
per-ex loss: 0.549306  [   20/   88]
per-ex loss: 0.609124  [   21/   88]
per-ex loss: 0.581843  [   22/   88]
per-ex loss: 0.497409  [   23/   88]
per-ex loss: 0.805038  [   24/   88]
per-ex loss: 0.834629  [   25/   88]
per-ex loss: 0.714233  [   26/   88]
per-ex loss: 0.634648  [   27/   88]
per-ex loss: 0.753423  [   28/   88]
per-ex loss: 0.695479  [   29/   88]
per-ex loss: 0.690982  [   30/   88]
per-ex loss: 0.787226  [   31/   88]
per-ex loss: 0.609366  [   32/   88]
per-ex loss: 0.609879  [   33/   88]
per-ex loss: 0.745674  [   34/   88]
per-ex loss: 0.741104  [   35/   88]
per-ex loss: 0.812892  [   36/   88]
per-ex loss: 0.629763  [   37/   88]
per-ex loss: 0.597875  [   38/   88]
per-ex loss: 0.820544  [   39/   88]
per-ex loss: 0.769790  [   40/   88]
per-ex loss: 0.565587  [   41/   88]
per-ex loss: 0.535349  [   42/   88]
per-ex loss: 0.616671  [   43/   88]
per-ex loss: 0.592047  [   44/   88]
per-ex loss: 0.618390  [   45/   88]
per-ex loss: 0.534363  [   46/   88]
per-ex loss: 0.629931  [   47/   88]
per-ex loss: 0.532474  [   48/   88]
per-ex loss: 0.710512  [   49/   88]
per-ex loss: 0.744003  [   50/   88]
per-ex loss: 0.808548  [   51/   88]
per-ex loss: 0.700597  [   52/   88]
per-ex loss: 0.568920  [   53/   88]
per-ex loss: 0.741374  [   54/   88]
per-ex loss: 0.532570  [   55/   88]
per-ex loss: 0.817320  [   56/   88]
per-ex loss: 0.595621  [   57/   88]
per-ex loss: 0.637109  [   58/   88]
per-ex loss: 0.663618  [   59/   88]
per-ex loss: 0.545138  [   60/   88]
per-ex loss: 0.575558  [   61/   88]
per-ex loss: 0.676565  [   62/   88]
per-ex loss: 0.731306  [   63/   88]
per-ex loss: 0.555424  [   64/   88]
per-ex loss: 0.534851  [   65/   88]
per-ex loss: 0.620746  [   66/   88]
per-ex loss: 0.714591  [   67/   88]
per-ex loss: 0.766649  [   68/   88]
per-ex loss: 0.585519  [   69/   88]
per-ex loss: 0.722899  [   70/   88]
per-ex loss: 0.757852  [   71/   88]
per-ex loss: 0.715162  [   72/   88]
per-ex loss: 0.562084  [   73/   88]
per-ex loss: 0.723029  [   74/   88]
per-ex loss: 0.530437  [   75/   88]
per-ex loss: 0.817023  [   76/   88]
per-ex loss: 0.588482  [   77/   88]
per-ex loss: 0.810143  [   78/   88]
per-ex loss: 0.739904  [   79/   88]
per-ex loss: 0.665719  [   80/   88]
per-ex loss: 0.627373  [   81/   88]
per-ex loss: 0.748318  [   82/   88]
per-ex loss: 0.563033  [   83/   88]
per-ex loss: 0.615430  [   84/   88]
per-ex loss: 0.741824  [   85/   88]
per-ex loss: 0.521159  [   86/   88]
per-ex loss: 0.763083  [   87/   88]
per-ex loss: 0.730389  [   88/   88]
Train Error: Avg loss: 0.66136999
validation Error: 
 Avg loss: 0.70274479 
 F1: 0.481308 
 Precision: 0.533445 
 Recall: 0.438455
 IoU: 0.316923

test Error: 
 Avg loss: 0.65512021 
 F1: 0.557160 
 Precision: 0.584340 
 Recall: 0.532397
 IoU: 0.386155

We have finished training iteration 31
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_29_.pth
per-ex loss: 0.787239  [    1/   88]
per-ex loss: 0.538564  [    2/   88]
per-ex loss: 0.493970  [    3/   88]
per-ex loss: 0.568583  [    4/   88]
per-ex loss: 0.555156  [    5/   88]
per-ex loss: 0.509687  [    6/   88]
per-ex loss: 0.731985  [    7/   88]
per-ex loss: 0.799370  [    8/   88]
per-ex loss: 0.588690  [    9/   88]
per-ex loss: 0.582140  [   10/   88]
per-ex loss: 0.787824  [   11/   88]
per-ex loss: 0.759397  [   12/   88]
per-ex loss: 0.758102  [   13/   88]
per-ex loss: 0.757519  [   14/   88]
per-ex loss: 0.551896  [   15/   88]
per-ex loss: 0.591531  [   16/   88]
per-ex loss: 0.772921  [   17/   88]
per-ex loss: 0.636758  [   18/   88]
per-ex loss: 0.789734  [   19/   88]
per-ex loss: 0.557240  [   20/   88]
per-ex loss: 0.553481  [   21/   88]
per-ex loss: 0.819061  [   22/   88]
per-ex loss: 0.753944  [   23/   88]
per-ex loss: 0.721382  [   24/   88]
per-ex loss: 0.600113  [   25/   88]
per-ex loss: 0.558635  [   26/   88]
per-ex loss: 0.533248  [   27/   88]
per-ex loss: 0.554590  [   28/   88]
per-ex loss: 0.720507  [   29/   88]
per-ex loss: 0.575765  [   30/   88]
per-ex loss: 0.514741  [   31/   88]
per-ex loss: 0.612266  [   32/   88]
per-ex loss: 0.694586  [   33/   88]
per-ex loss: 0.744079  [   34/   88]
per-ex loss: 0.748941  [   35/   88]
per-ex loss: 0.633190  [   36/   88]
per-ex loss: 0.740897  [   37/   88]
per-ex loss: 0.748196  [   38/   88]
per-ex loss: 0.710005  [   39/   88]
per-ex loss: 0.575399  [   40/   88]
per-ex loss: 0.718290  [   41/   88]
per-ex loss: 0.621998  [   42/   88]
per-ex loss: 0.609779  [   43/   88]
per-ex loss: 0.539437  [   44/   88]
per-ex loss: 0.655963  [   45/   88]
per-ex loss: 0.776141  [   46/   88]
per-ex loss: 0.771154  [   47/   88]
per-ex loss: 0.819272  [   48/   88]
per-ex loss: 0.710831  [   49/   88]
per-ex loss: 0.593140  [   50/   88]
per-ex loss: 0.842882  [   51/   88]
per-ex loss: 0.587036  [   52/   88]
per-ex loss: 0.703649  [   53/   88]
per-ex loss: 0.635297  [   54/   88]
per-ex loss: 0.739533  [   55/   88]
per-ex loss: 0.786504  [   56/   88]
per-ex loss: 0.801553  [   57/   88]
per-ex loss: 0.593010  [   58/   88]
per-ex loss: 0.741939  [   59/   88]
per-ex loss: 0.529106  [   60/   88]
per-ex loss: 0.824320  [   61/   88]
per-ex loss: 0.696744  [   62/   88]
per-ex loss: 0.564920  [   63/   88]
per-ex loss: 0.658428  [   64/   88]
per-ex loss: 0.612394  [   65/   88]
per-ex loss: 0.761779  [   66/   88]
per-ex loss: 0.565811  [   67/   88]
per-ex loss: 0.818263  [   68/   88]
per-ex loss: 0.583546  [   69/   88]
per-ex loss: 0.696212  [   70/   88]
per-ex loss: 0.687718  [   71/   88]
per-ex loss: 0.599434  [   72/   88]
per-ex loss: 0.667922  [   73/   88]
per-ex loss: 0.610113  [   74/   88]
per-ex loss: 0.600478  [   75/   88]
per-ex loss: 0.717594  [   76/   88]
per-ex loss: 0.691264  [   77/   88]
per-ex loss: 0.590290  [   78/   88]
per-ex loss: 0.644598  [   79/   88]
per-ex loss: 0.537429  [   80/   88]
per-ex loss: 0.542581  [   81/   88]
per-ex loss: 0.607062  [   82/   88]
per-ex loss: 0.540400  [   83/   88]
per-ex loss: 0.761225  [   84/   88]
per-ex loss: 0.632317  [   85/   88]
per-ex loss: 0.521861  [   86/   88]
per-ex loss: 0.569083  [   87/   88]
per-ex loss: 0.709389  [   88/   88]
Train Error: Avg loss: 0.65932982
validation Error: 
 Avg loss: 0.73594684 
 F1: 0.436312 
 Precision: 0.498832 
 Recall: 0.387718
 IoU: 0.279028

test Error: 
 Avg loss: 0.67448266 
 F1: 0.526075 
 Precision: 0.626395 
 Recall: 0.453452
 IoU: 0.356921

We have finished training iteration 32
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_30_.pth
per-ex loss: 0.759040  [    1/   88]
per-ex loss: 0.548663  [    2/   88]
per-ex loss: 0.765156  [    3/   88]
per-ex loss: 0.680010  [    4/   88]
per-ex loss: 0.561628  [    5/   88]
per-ex loss: 0.715338  [    6/   88]
per-ex loss: 0.750245  [    7/   88]
per-ex loss: 0.639550  [    8/   88]
per-ex loss: 0.737186  [    9/   88]
per-ex loss: 0.787913  [   10/   88]
per-ex loss: 0.812681  [   11/   88]
per-ex loss: 0.524536  [   12/   88]
per-ex loss: 0.529588  [   13/   88]
per-ex loss: 0.645218  [   14/   88]
per-ex loss: 0.747446  [   15/   88]
per-ex loss: 0.614380  [   16/   88]
per-ex loss: 0.600681  [   17/   88]
per-ex loss: 0.561087  [   18/   88]
per-ex loss: 0.519231  [   19/   88]
per-ex loss: 0.557261  [   20/   88]
per-ex loss: 0.720595  [   21/   88]
per-ex loss: 0.764622  [   22/   88]
per-ex loss: 0.715336  [   23/   88]
per-ex loss: 0.838228  [   24/   88]
per-ex loss: 0.817019  [   25/   88]
per-ex loss: 0.530926  [   26/   88]
per-ex loss: 0.590306  [   27/   88]
per-ex loss: 0.725154  [   28/   88]
per-ex loss: 0.510060  [   29/   88]
per-ex loss: 0.792437  [   30/   88]
per-ex loss: 0.774402  [   31/   88]
per-ex loss: 0.617019  [   32/   88]
per-ex loss: 0.590475  [   33/   88]
per-ex loss: 0.738465  [   34/   88]
per-ex loss: 0.590460  [   35/   88]
per-ex loss: 0.775824  [   36/   88]
per-ex loss: 0.771370  [   37/   88]
per-ex loss: 0.548124  [   38/   88]
per-ex loss: 0.630075  [   39/   88]
per-ex loss: 0.692952  [   40/   88]
per-ex loss: 0.659104  [   41/   88]
per-ex loss: 0.626296  [   42/   88]
per-ex loss: 0.785328  [   43/   88]
per-ex loss: 0.577434  [   44/   88]
per-ex loss: 0.631404  [   45/   88]
per-ex loss: 0.737600  [   46/   88]
per-ex loss: 0.680878  [   47/   88]
per-ex loss: 0.701788  [   48/   88]
per-ex loss: 0.746082  [   49/   88]
per-ex loss: 0.600321  [   50/   88]
per-ex loss: 0.782414  [   51/   88]
per-ex loss: 0.649764  [   52/   88]
per-ex loss: 0.588839  [   53/   88]
per-ex loss: 0.703179  [   54/   88]
per-ex loss: 0.572520  [   55/   88]
per-ex loss: 0.700935  [   56/   88]
per-ex loss: 0.792293  [   57/   88]
per-ex loss: 0.715334  [   58/   88]
per-ex loss: 0.689674  [   59/   88]
per-ex loss: 0.645142  [   60/   88]
per-ex loss: 0.600412  [   61/   88]
per-ex loss: 0.578330  [   62/   88]
per-ex loss: 0.558843  [   63/   88]
per-ex loss: 0.614109  [   64/   88]
per-ex loss: 0.624111  [   65/   88]
per-ex loss: 0.564987  [   66/   88]
per-ex loss: 0.478549  [   67/   88]
per-ex loss: 0.550162  [   68/   88]
per-ex loss: 0.534229  [   69/   88]
per-ex loss: 0.524008  [   70/   88]
per-ex loss: 0.532384  [   71/   88]
per-ex loss: 0.827645  [   72/   88]
per-ex loss: 0.636969  [   73/   88]
per-ex loss: 0.627412  [   74/   88]
per-ex loss: 0.569206  [   75/   88]
per-ex loss: 0.630316  [   76/   88]
per-ex loss: 0.528556  [   77/   88]
per-ex loss: 0.817201  [   78/   88]
per-ex loss: 0.789861  [   79/   88]
per-ex loss: 0.761148  [   80/   88]
per-ex loss: 0.730536  [   81/   88]
per-ex loss: 0.601746  [   82/   88]
per-ex loss: 0.782986  [   83/   88]
per-ex loss: 0.577606  [   84/   88]
per-ex loss: 0.556155  [   85/   88]
per-ex loss: 0.760558  [   86/   88]
per-ex loss: 0.703703  [   87/   88]
per-ex loss: 0.561984  [   88/   88]
Train Error: Avg loss: 0.65909918
validation Error: 
 Avg loss: 0.71795717 
 F1: 0.464753 
 Precision: 0.447594 
 Recall: 0.483280
 IoU: 0.302722

test Error: 
 Avg loss: 0.65418151 
 F1: 0.558695 
 Precision: 0.540794 
 Recall: 0.577820
 IoU: 0.387631

We have finished training iteration 33
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_31_.pth
per-ex loss: 0.695454  [    1/   88]
per-ex loss: 0.557060  [    2/   88]
per-ex loss: 0.816476  [    3/   88]
per-ex loss: 0.600642  [    4/   88]
per-ex loss: 0.636370  [    5/   88]
per-ex loss: 0.648631  [    6/   88]
per-ex loss: 0.671185  [    7/   88]
per-ex loss: 0.768108  [    8/   88]
per-ex loss: 0.677340  [    9/   88]
per-ex loss: 0.612743  [   10/   88]
per-ex loss: 0.706990  [   11/   88]
per-ex loss: 0.774629  [   12/   88]
per-ex loss: 0.772014  [   13/   88]
per-ex loss: 0.568447  [   14/   88]
per-ex loss: 0.588172  [   15/   88]
per-ex loss: 0.549002  [   16/   88]
per-ex loss: 0.542879  [   17/   88]
per-ex loss: 0.528030  [   18/   88]
per-ex loss: 0.536085  [   19/   88]
per-ex loss: 0.576661  [   20/   88]
per-ex loss: 0.656963  [   21/   88]
per-ex loss: 0.673014  [   22/   88]
per-ex loss: 0.589162  [   23/   88]
per-ex loss: 0.530766  [   24/   88]
per-ex loss: 0.568555  [   25/   88]
per-ex loss: 0.602175  [   26/   88]
per-ex loss: 0.731206  [   27/   88]
per-ex loss: 0.602999  [   28/   88]
per-ex loss: 0.673729  [   29/   88]
per-ex loss: 0.613864  [   30/   88]
per-ex loss: 0.764113  [   31/   88]
per-ex loss: 0.750826  [   32/   88]
per-ex loss: 0.722848  [   33/   88]
per-ex loss: 0.627073  [   34/   88]
per-ex loss: 0.739417  [   35/   88]
per-ex loss: 0.756598  [   36/   88]
per-ex loss: 0.591628  [   37/   88]
per-ex loss: 0.528539  [   38/   88]
per-ex loss: 0.799445  [   39/   88]
per-ex loss: 0.754261  [   40/   88]
per-ex loss: 0.558427  [   41/   88]
per-ex loss: 0.722698  [   42/   88]
per-ex loss: 0.812443  [   43/   88]
per-ex loss: 0.599017  [   44/   88]
per-ex loss: 0.704569  [   45/   88]
per-ex loss: 0.701207  [   46/   88]
per-ex loss: 0.584776  [   47/   88]
per-ex loss: 0.767011  [   48/   88]
per-ex loss: 0.779508  [   49/   88]
per-ex loss: 0.614518  [   50/   88]
per-ex loss: 0.555667  [   51/   88]
per-ex loss: 0.621790  [   52/   88]
per-ex loss: 0.703061  [   53/   88]
per-ex loss: 0.583820  [   54/   88]
per-ex loss: 0.741381  [   55/   88]
per-ex loss: 0.721421  [   56/   88]
per-ex loss: 0.791088  [   57/   88]
per-ex loss: 0.479821  [   58/   88]
per-ex loss: 0.814282  [   59/   88]
per-ex loss: 0.584481  [   60/   88]
per-ex loss: 0.762335  [   61/   88]
per-ex loss: 0.542656  [   62/   88]
per-ex loss: 0.588286  [   63/   88]
per-ex loss: 0.771548  [   64/   88]
per-ex loss: 0.532456  [   65/   88]
per-ex loss: 0.552160  [   66/   88]
per-ex loss: 0.719682  [   67/   88]
per-ex loss: 0.583658  [   68/   88]
per-ex loss: 0.790736  [   69/   88]
per-ex loss: 0.689282  [   70/   88]
per-ex loss: 0.551809  [   71/   88]
per-ex loss: 0.621028  [   72/   88]
per-ex loss: 0.538537  [   73/   88]
per-ex loss: 0.613888  [   74/   88]
per-ex loss: 0.523459  [   75/   88]
per-ex loss: 0.576861  [   76/   88]
per-ex loss: 0.753976  [   77/   88]
per-ex loss: 0.809556  [   78/   88]
per-ex loss: 0.516671  [   79/   88]
per-ex loss: 0.520685  [   80/   88]
per-ex loss: 0.779266  [   81/   88]
per-ex loss: 0.730029  [   82/   88]
per-ex loss: 0.684683  [   83/   88]
per-ex loss: 0.766993  [   84/   88]
per-ex loss: 0.825111  [   85/   88]
per-ex loss: 0.800739  [   86/   88]
per-ex loss: 0.608177  [   87/   88]
per-ex loss: 0.491498  [   88/   88]
Train Error: Avg loss: 0.65671420
validation Error: 
 Avg loss: 0.70459897 
 F1: 0.479126 
 Precision: 0.642330 
 Recall: 0.382053
 IoU: 0.315033

test Error: 
 Avg loss: 0.65906351 
 F1: 0.550241 
 Precision: 0.689600 
 Recall: 0.457739
 IoU: 0.379540

We have finished training iteration 34
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_32_.pth
per-ex loss: 0.761885  [    1/   88]
per-ex loss: 0.541898  [    2/   88]
per-ex loss: 0.738963  [    3/   88]
per-ex loss: 0.528058  [    4/   88]
per-ex loss: 0.635460  [    5/   88]
per-ex loss: 0.732519  [    6/   88]
per-ex loss: 0.638853  [    7/   88]
per-ex loss: 0.630653  [    8/   88]
per-ex loss: 0.763643  [    9/   88]
per-ex loss: 0.739433  [   10/   88]
per-ex loss: 0.719593  [   11/   88]
per-ex loss: 0.756357  [   12/   88]
per-ex loss: 0.769139  [   13/   88]
per-ex loss: 0.553856  [   14/   88]
per-ex loss: 0.700699  [   15/   88]
per-ex loss: 0.542115  [   16/   88]
per-ex loss: 0.575011  [   17/   88]
per-ex loss: 0.646049  [   18/   88]
per-ex loss: 0.756579  [   19/   88]
per-ex loss: 0.608837  [   20/   88]
per-ex loss: 0.703382  [   21/   88]
per-ex loss: 0.598513  [   22/   88]
per-ex loss: 0.623040  [   23/   88]
per-ex loss: 0.549667  [   24/   88]
per-ex loss: 0.604863  [   25/   88]
per-ex loss: 0.726479  [   26/   88]
per-ex loss: 0.556942  [   27/   88]
per-ex loss: 0.808573  [   28/   88]
per-ex loss: 0.585173  [   29/   88]
per-ex loss: 0.566509  [   30/   88]
per-ex loss: 0.774011  [   31/   88]
per-ex loss: 0.515519  [   32/   88]
per-ex loss: 0.616259  [   33/   88]
per-ex loss: 0.717632  [   34/   88]
per-ex loss: 0.547683  [   35/   88]
per-ex loss: 0.621681  [   36/   88]
per-ex loss: 0.559421  [   37/   88]
per-ex loss: 0.723989  [   38/   88]
per-ex loss: 0.545337  [   39/   88]
per-ex loss: 0.717798  [   40/   88]
per-ex loss: 0.709752  [   41/   88]
per-ex loss: 0.601341  [   42/   88]
per-ex loss: 0.523378  [   43/   88]
per-ex loss: 0.722024  [   44/   88]
per-ex loss: 0.574244  [   45/   88]
per-ex loss: 0.582301  [   46/   88]
per-ex loss: 0.799733  [   47/   88]
per-ex loss: 0.544220  [   48/   88]
per-ex loss: 0.522331  [   49/   88]
per-ex loss: 0.591412  [   50/   88]
per-ex loss: 0.727534  [   51/   88]
per-ex loss: 0.604263  [   52/   88]
per-ex loss: 0.712703  [   53/   88]
per-ex loss: 0.819827  [   54/   88]
per-ex loss: 0.555273  [   55/   88]
per-ex loss: 0.681547  [   56/   88]
per-ex loss: 0.556998  [   57/   88]
per-ex loss: 0.539168  [   58/   88]
per-ex loss: 0.515715  [   59/   88]
per-ex loss: 0.576714  [   60/   88]
per-ex loss: 0.495020  [   61/   88]
per-ex loss: 0.790838  [   62/   88]
per-ex loss: 0.797915  [   63/   88]
per-ex loss: 0.825656  [   64/   88]
per-ex loss: 0.717279  [   65/   88]
per-ex loss: 0.701471  [   66/   88]
per-ex loss: 0.754258  [   67/   88]
per-ex loss: 0.802245  [   68/   88]
per-ex loss: 0.778154  [   69/   88]
per-ex loss: 0.701997  [   70/   88]
per-ex loss: 0.822035  [   71/   88]
per-ex loss: 0.482666  [   72/   88]
per-ex loss: 0.729410  [   73/   88]
per-ex loss: 0.749433  [   74/   88]
per-ex loss: 0.815094  [   75/   88]
per-ex loss: 0.584770  [   76/   88]
per-ex loss: 0.555587  [   77/   88]
per-ex loss: 0.619008  [   78/   88]
per-ex loss: 0.814865  [   79/   88]
per-ex loss: 0.581774  [   80/   88]
per-ex loss: 0.605700  [   81/   88]
per-ex loss: 0.517811  [   82/   88]
per-ex loss: 0.749646  [   83/   88]
per-ex loss: 0.601745  [   84/   88]
per-ex loss: 0.609370  [   85/   88]
per-ex loss: 0.741586  [   86/   88]
per-ex loss: 0.647646  [   87/   88]
per-ex loss: 0.615819  [   88/   88]
Train Error: Avg loss: 0.65535624
validation Error: 
 Avg loss: 0.70208832 
 F1: 0.482197 
 Precision: 0.593333 
 Recall: 0.406127
 IoU: 0.317694

test Error: 
 Avg loss: 0.65635671 
 F1: 0.553569 
 Precision: 0.652260 
 Recall: 0.480819
 IoU: 0.382714

We have finished training iteration 35
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_33_.pth
per-ex loss: 0.725546  [    1/   88]
per-ex loss: 0.585882  [    2/   88]
per-ex loss: 0.811756  [    3/   88]
per-ex loss: 0.558816  [    4/   88]
per-ex loss: 0.792259  [    5/   88]
per-ex loss: 0.797521  [    6/   88]
per-ex loss: 0.812512  [    7/   88]
per-ex loss: 0.628723  [    8/   88]
per-ex loss: 0.600387  [    9/   88]
per-ex loss: 0.753751  [   10/   88]
per-ex loss: 0.581024  [   11/   88]
per-ex loss: 0.631253  [   12/   88]
per-ex loss: 0.720536  [   13/   88]
per-ex loss: 0.724079  [   14/   88]
per-ex loss: 0.575370  [   15/   88]
per-ex loss: 0.557839  [   16/   88]
per-ex loss: 0.549019  [   17/   88]
per-ex loss: 0.785993  [   18/   88]
per-ex loss: 0.730681  [   19/   88]
per-ex loss: 0.552745  [   20/   88]
per-ex loss: 0.738243  [   21/   88]
per-ex loss: 0.788938  [   22/   88]
per-ex loss: 0.795340  [   23/   88]
per-ex loss: 0.490470  [   24/   88]
per-ex loss: 0.647421  [   25/   88]
per-ex loss: 0.687217  [   26/   88]
per-ex loss: 0.524614  [   27/   88]
per-ex loss: 0.534600  [   28/   88]
per-ex loss: 0.764143  [   29/   88]
per-ex loss: 0.522709  [   30/   88]
per-ex loss: 0.719059  [   31/   88]
per-ex loss: 0.731370  [   32/   88]
per-ex loss: 0.569367  [   33/   88]
per-ex loss: 0.745177  [   34/   88]
per-ex loss: 0.625150  [   35/   88]
per-ex loss: 0.628150  [   36/   88]
per-ex loss: 0.560369  [   37/   88]
per-ex loss: 0.808505  [   38/   88]
per-ex loss: 0.563685  [   39/   88]
per-ex loss: 0.571408  [   40/   88]
per-ex loss: 0.774072  [   41/   88]
per-ex loss: 0.557338  [   42/   88]
per-ex loss: 0.599460  [   43/   88]
per-ex loss: 0.729995  [   44/   88]
per-ex loss: 0.710494  [   45/   88]
per-ex loss: 0.533780  [   46/   88]
per-ex loss: 0.616266  [   47/   88]
per-ex loss: 0.655130  [   48/   88]
per-ex loss: 0.682643  [   49/   88]
per-ex loss: 0.542998  [   50/   88]
per-ex loss: 0.691570  [   51/   88]
per-ex loss: 0.742484  [   52/   88]
per-ex loss: 0.702815  [   53/   88]
per-ex loss: 0.548089  [   54/   88]
per-ex loss: 0.512245  [   55/   88]
per-ex loss: 0.612627  [   56/   88]
per-ex loss: 0.579456  [   57/   88]
per-ex loss: 0.734390  [   58/   88]
per-ex loss: 0.704018  [   59/   88]
per-ex loss: 0.646788  [   60/   88]
per-ex loss: 0.699785  [   61/   88]
per-ex loss: 0.614703  [   62/   88]
per-ex loss: 0.619598  [   63/   88]
per-ex loss: 0.618786  [   64/   88]
per-ex loss: 0.741412  [   65/   88]
per-ex loss: 0.813889  [   66/   88]
per-ex loss: 0.523192  [   67/   88]
per-ex loss: 0.505759  [   68/   88]
per-ex loss: 0.714059  [   69/   88]
per-ex loss: 0.706614  [   70/   88]
per-ex loss: 0.749046  [   71/   88]
per-ex loss: 0.564000  [   72/   88]
per-ex loss: 0.556077  [   73/   88]
per-ex loss: 0.624570  [   74/   88]
per-ex loss: 0.735602  [   75/   88]
per-ex loss: 0.801571  [   76/   88]
per-ex loss: 0.662171  [   77/   88]
per-ex loss: 0.728355  [   78/   88]
per-ex loss: 0.597333  [   79/   88]
per-ex loss: 0.611543  [   80/   88]
per-ex loss: 0.665965  [   81/   88]
per-ex loss: 0.554526  [   82/   88]
per-ex loss: 0.781159  [   83/   88]
per-ex loss: 0.561611  [   84/   88]
per-ex loss: 0.589511  [   85/   88]
per-ex loss: 0.534263  [   86/   88]
per-ex loss: 0.584958  [   87/   88]
per-ex loss: 0.838392  [   88/   88]
Train Error: Avg loss: 0.65530380
validation Error: 
 Avg loss: 0.72261681 
 F1: 0.456766 
 Precision: 0.704059 
 Recall: 0.338035
 IoU: 0.295980

test Error: 
 Avg loss: 0.68335953 
 F1: 0.516246 
 Precision: 0.742529 
 Recall: 0.395668
 IoU: 0.347932

We have finished training iteration 36
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_34_.pth
per-ex loss: 0.513729  [    1/   88]
per-ex loss: 0.747587  [    2/   88]
per-ex loss: 0.618574  [    3/   88]
per-ex loss: 0.822297  [    4/   88]
per-ex loss: 0.699326  [    5/   88]
per-ex loss: 0.637893  [    6/   88]
per-ex loss: 0.558912  [    7/   88]
per-ex loss: 0.591959  [    8/   88]
per-ex loss: 0.711176  [    9/   88]
per-ex loss: 0.511784  [   10/   88]
per-ex loss: 0.735231  [   11/   88]
per-ex loss: 0.786960  [   12/   88]
per-ex loss: 0.815279  [   13/   88]
per-ex loss: 0.485457  [   14/   88]
per-ex loss: 0.563756  [   15/   88]
per-ex loss: 0.771844  [   16/   88]
per-ex loss: 0.599079  [   17/   88]
per-ex loss: 0.747260  [   18/   88]
per-ex loss: 0.535199  [   19/   88]
per-ex loss: 0.566868  [   20/   88]
per-ex loss: 0.575796  [   21/   88]
per-ex loss: 0.700698  [   22/   88]
per-ex loss: 0.535612  [   23/   88]
per-ex loss: 0.624937  [   24/   88]
per-ex loss: 0.533563  [   25/   88]
per-ex loss: 0.590378  [   26/   88]
per-ex loss: 0.728181  [   27/   88]
per-ex loss: 0.696309  [   28/   88]
per-ex loss: 0.802085  [   29/   88]
per-ex loss: 0.712890  [   30/   88]
per-ex loss: 0.721601  [   31/   88]
per-ex loss: 0.525087  [   32/   88]
per-ex loss: 0.617323  [   33/   88]
per-ex loss: 0.653229  [   34/   88]
per-ex loss: 0.581748  [   35/   88]
per-ex loss: 0.573711  [   36/   88]
per-ex loss: 0.579454  [   37/   88]
per-ex loss: 0.808852  [   38/   88]
per-ex loss: 0.811399  [   39/   88]
per-ex loss: 0.764877  [   40/   88]
per-ex loss: 0.818864  [   41/   88]
per-ex loss: 0.586548  [   42/   88]
per-ex loss: 0.757912  [   43/   88]
per-ex loss: 0.554568  [   44/   88]
per-ex loss: 0.708582  [   45/   88]
per-ex loss: 0.769106  [   46/   88]
per-ex loss: 0.791533  [   47/   88]
per-ex loss: 0.719618  [   48/   88]
per-ex loss: 0.720896  [   49/   88]
per-ex loss: 0.488962  [   50/   88]
per-ex loss: 0.597285  [   51/   88]
per-ex loss: 0.731451  [   52/   88]
per-ex loss: 0.598314  [   53/   88]
per-ex loss: 0.822971  [   54/   88]
per-ex loss: 0.564127  [   55/   88]
per-ex loss: 0.780137  [   56/   88]
per-ex loss: 0.561808  [   57/   88]
per-ex loss: 0.576149  [   58/   88]
per-ex loss: 0.740028  [   59/   88]
per-ex loss: 0.586926  [   60/   88]
per-ex loss: 0.527199  [   61/   88]
per-ex loss: 0.507808  [   62/   88]
per-ex loss: 0.540607  [   63/   88]
per-ex loss: 0.737140  [   64/   88]
per-ex loss: 0.704922  [   65/   88]
per-ex loss: 0.591293  [   66/   88]
per-ex loss: 0.638320  [   67/   88]
per-ex loss: 0.593547  [   68/   88]
per-ex loss: 0.534109  [   69/   88]
per-ex loss: 0.617217  [   70/   88]
per-ex loss: 0.543357  [   71/   88]
per-ex loss: 0.813151  [   72/   88]
per-ex loss: 0.620161  [   73/   88]
per-ex loss: 0.825220  [   74/   88]
per-ex loss: 0.709168  [   75/   88]
per-ex loss: 0.551420  [   76/   88]
per-ex loss: 0.562064  [   77/   88]
per-ex loss: 0.611994  [   78/   88]
per-ex loss: 0.558274  [   79/   88]
per-ex loss: 0.603271  [   80/   88]
per-ex loss: 0.685757  [   81/   88]
per-ex loss: 0.714072  [   82/   88]
per-ex loss: 0.641623  [   83/   88]
per-ex loss: 0.757111  [   84/   88]
per-ex loss: 0.744213  [   85/   88]
per-ex loss: 0.782379  [   86/   88]
per-ex loss: 0.589277  [   87/   88]
per-ex loss: 0.727087  [   88/   88]
Train Error: Avg loss: 0.65528919
validation Error: 
 Avg loss: 0.70566103 
 F1: 0.474515 
 Precision: 0.467649 
 Recall: 0.481584
 IoU: 0.311058

test Error: 
 Avg loss: 0.64676929 
 F1: 0.562889 
 Precision: 0.551817 
 Recall: 0.574414
 IoU: 0.391681

We have finished training iteration 37
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_35_.pth
per-ex loss: 0.548657  [    1/   88]
per-ex loss: 0.599518  [    2/   88]
per-ex loss: 0.784498  [    3/   88]
per-ex loss: 0.820092  [    4/   88]
per-ex loss: 0.614434  [    5/   88]
per-ex loss: 0.735234  [    6/   88]
per-ex loss: 0.579423  [    7/   88]
per-ex loss: 0.791283  [    8/   88]
per-ex loss: 0.779535  [    9/   88]
per-ex loss: 0.702003  [   10/   88]
per-ex loss: 0.543684  [   11/   88]
per-ex loss: 0.560979  [   12/   88]
per-ex loss: 0.560445  [   13/   88]
per-ex loss: 0.567968  [   14/   88]
per-ex loss: 0.794499  [   15/   88]
per-ex loss: 0.567342  [   16/   88]
per-ex loss: 0.486663  [   17/   88]
per-ex loss: 0.718481  [   18/   88]
per-ex loss: 0.590763  [   19/   88]
per-ex loss: 0.553127  [   20/   88]
per-ex loss: 0.531965  [   21/   88]
per-ex loss: 0.771912  [   22/   88]
per-ex loss: 0.733404  [   23/   88]
per-ex loss: 0.621839  [   24/   88]
per-ex loss: 0.623641  [   25/   88]
per-ex loss: 0.573457  [   26/   88]
per-ex loss: 0.604007  [   27/   88]
per-ex loss: 0.508718  [   28/   88]
per-ex loss: 0.721938  [   29/   88]
per-ex loss: 0.714720  [   30/   88]
per-ex loss: 0.763929  [   31/   88]
per-ex loss: 0.689258  [   32/   88]
per-ex loss: 0.813965  [   33/   88]
per-ex loss: 0.785067  [   34/   88]
per-ex loss: 0.581937  [   35/   88]
per-ex loss: 0.802669  [   36/   88]
per-ex loss: 0.614564  [   37/   88]
per-ex loss: 0.555938  [   38/   88]
per-ex loss: 0.514272  [   39/   88]
per-ex loss: 0.545003  [   40/   88]
per-ex loss: 0.688911  [   41/   88]
per-ex loss: 0.605414  [   42/   88]
per-ex loss: 0.760117  [   43/   88]
per-ex loss: 0.629108  [   44/   88]
per-ex loss: 0.784305  [   45/   88]
per-ex loss: 0.599551  [   46/   88]
per-ex loss: 0.706752  [   47/   88]
per-ex loss: 0.791994  [   48/   88]
per-ex loss: 0.740596  [   49/   88]
per-ex loss: 0.717083  [   50/   88]
per-ex loss: 0.814249  [   51/   88]
per-ex loss: 0.549516  [   52/   88]
per-ex loss: 0.697503  [   53/   88]
per-ex loss: 0.567379  [   54/   88]
per-ex loss: 0.575479  [   55/   88]
per-ex loss: 0.735168  [   56/   88]
per-ex loss: 0.695009  [   57/   88]
per-ex loss: 0.754408  [   58/   88]
per-ex loss: 0.625221  [   59/   88]
per-ex loss: 0.522489  [   60/   88]
per-ex loss: 0.521327  [   61/   88]
per-ex loss: 0.528117  [   62/   88]
per-ex loss: 0.531596  [   63/   88]
per-ex loss: 0.761299  [   64/   88]
per-ex loss: 0.729672  [   65/   88]
per-ex loss: 0.726194  [   66/   88]
per-ex loss: 0.517675  [   67/   88]
per-ex loss: 0.639958  [   68/   88]
per-ex loss: 0.731861  [   69/   88]
per-ex loss: 0.764432  [   70/   88]
per-ex loss: 0.546803  [   71/   88]
per-ex loss: 0.631344  [   72/   88]
per-ex loss: 0.816115  [   73/   88]
per-ex loss: 0.628256  [   74/   88]
per-ex loss: 0.638513  [   75/   88]
per-ex loss: 0.696210  [   76/   88]
per-ex loss: 0.649123  [   77/   88]
per-ex loss: 0.694918  [   78/   88]
per-ex loss: 0.668938  [   79/   88]
per-ex loss: 0.737450  [   80/   88]
per-ex loss: 0.590905  [   81/   88]
per-ex loss: 0.658883  [   82/   88]
per-ex loss: 0.732000  [   83/   88]
per-ex loss: 0.570975  [   84/   88]
per-ex loss: 0.542397  [   85/   88]
per-ex loss: 0.574175  [   86/   88]
per-ex loss: 0.556529  [   87/   88]
per-ex loss: 0.581890  [   88/   88]
Train Error: Avg loss: 0.65339361
validation Error: 
 Avg loss: 0.72727655 
 F1: 0.459042 
 Precision: 0.623326 
 Recall: 0.363292
 IoU: 0.297894

test Error: 
 Avg loss: 0.67396410 
 F1: 0.524002 
 Precision: 0.723182 
 Recall: 0.410846
 IoU: 0.355015

We have finished training iteration 38
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_36_.pth
per-ex loss: 0.552264  [    1/   88]
per-ex loss: 0.787923  [    2/   88]
per-ex loss: 0.508444  [    3/   88]
per-ex loss: 0.564220  [    4/   88]
per-ex loss: 0.645505  [    5/   88]
per-ex loss: 0.753098  [    6/   88]
per-ex loss: 0.586625  [    7/   88]
per-ex loss: 0.707019  [    8/   88]
per-ex loss: 0.543138  [    9/   88]
per-ex loss: 0.581490  [   10/   88]
per-ex loss: 0.548047  [   11/   88]
per-ex loss: 0.793269  [   12/   88]
per-ex loss: 0.677634  [   13/   88]
per-ex loss: 0.573773  [   14/   88]
per-ex loss: 0.677386  [   15/   88]
per-ex loss: 0.720681  [   16/   88]
per-ex loss: 0.560613  [   17/   88]
per-ex loss: 0.741174  [   18/   88]
per-ex loss: 0.516617  [   19/   88]
per-ex loss: 0.808964  [   20/   88]
per-ex loss: 0.766979  [   21/   88]
per-ex loss: 0.723211  [   22/   88]
per-ex loss: 0.713159  [   23/   88]
per-ex loss: 0.523080  [   24/   88]
per-ex loss: 0.728816  [   25/   88]
per-ex loss: 0.708023  [   26/   88]
per-ex loss: 0.610276  [   27/   88]
per-ex loss: 0.746811  [   28/   88]
per-ex loss: 0.546975  [   29/   88]
per-ex loss: 0.651891  [   30/   88]
per-ex loss: 0.704244  [   31/   88]
per-ex loss: 0.535192  [   32/   88]
per-ex loss: 0.695053  [   33/   88]
per-ex loss: 0.774366  [   34/   88]
per-ex loss: 0.594281  [   35/   88]
per-ex loss: 0.554683  [   36/   88]
per-ex loss: 0.693623  [   37/   88]
per-ex loss: 0.533526  [   38/   88]
per-ex loss: 0.632690  [   39/   88]
per-ex loss: 0.720759  [   40/   88]
per-ex loss: 0.514096  [   41/   88]
per-ex loss: 0.593050  [   42/   88]
per-ex loss: 0.572881  [   43/   88]
per-ex loss: 0.762239  [   44/   88]
per-ex loss: 0.727054  [   45/   88]
per-ex loss: 0.716028  [   46/   88]
per-ex loss: 0.552271  [   47/   88]
per-ex loss: 0.537468  [   48/   88]
per-ex loss: 0.546714  [   49/   88]
per-ex loss: 0.578276  [   50/   88]
per-ex loss: 0.617438  [   51/   88]
per-ex loss: 0.601309  [   52/   88]
per-ex loss: 0.601358  [   53/   88]
per-ex loss: 0.694643  [   54/   88]
per-ex loss: 0.591823  [   55/   88]
per-ex loss: 0.724566  [   56/   88]
per-ex loss: 0.778760  [   57/   88]
per-ex loss: 0.487069  [   58/   88]
per-ex loss: 0.710365  [   59/   88]
per-ex loss: 0.552450  [   60/   88]
per-ex loss: 0.795631  [   61/   88]
per-ex loss: 0.616373  [   62/   88]
per-ex loss: 0.624586  [   63/   88]
per-ex loss: 0.732280  [   64/   88]
per-ex loss: 0.776390  [   65/   88]
per-ex loss: 0.536306  [   66/   88]
per-ex loss: 0.568782  [   67/   88]
per-ex loss: 0.807150  [   68/   88]
per-ex loss: 0.735064  [   69/   88]
per-ex loss: 0.664933  [   70/   88]
per-ex loss: 0.560043  [   71/   88]
per-ex loss: 0.639630  [   72/   88]
per-ex loss: 0.745407  [   73/   88]
per-ex loss: 0.525060  [   74/   88]
per-ex loss: 0.788933  [   75/   88]
per-ex loss: 0.587644  [   76/   88]
per-ex loss: 0.814009  [   77/   88]
per-ex loss: 0.791329  [   78/   88]
per-ex loss: 0.546385  [   79/   88]
per-ex loss: 0.675190  [   80/   88]
per-ex loss: 0.628044  [   81/   88]
per-ex loss: 0.608114  [   82/   88]
per-ex loss: 0.610982  [   83/   88]
per-ex loss: 0.781810  [   84/   88]
per-ex loss: 0.809194  [   85/   88]
per-ex loss: 0.682661  [   86/   88]
per-ex loss: 0.568759  [   87/   88]
per-ex loss: 0.808646  [   88/   88]
Train Error: Avg loss: 0.65334901
validation Error: 
 Avg loss: 0.73912486 
 F1: 0.429181 
 Precision: 0.413913 
 Recall: 0.445620
 IoU: 0.273221

test Error: 
 Avg loss: 0.65562314 
 F1: 0.554919 
 Precision: 0.568461 
 Recall: 0.542007
 IoU: 0.384005

We have finished training iteration 39
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_37_.pth
per-ex loss: 0.552379  [    1/   88]
per-ex loss: 0.738997  [    2/   88]
per-ex loss: 0.525512  [    3/   88]
per-ex loss: 0.685044  [    4/   88]
per-ex loss: 0.798657  [    5/   88]
per-ex loss: 0.703126  [    6/   88]
per-ex loss: 0.567151  [    7/   88]
per-ex loss: 0.589247  [    8/   88]
per-ex loss: 0.746556  [    9/   88]
per-ex loss: 0.602002  [   10/   88]
per-ex loss: 0.743329  [   11/   88]
per-ex loss: 0.612888  [   12/   88]
per-ex loss: 0.596162  [   13/   88]
per-ex loss: 0.749952  [   14/   88]
per-ex loss: 0.734731  [   15/   88]
per-ex loss: 0.688529  [   16/   88]
per-ex loss: 0.768463  [   17/   88]
per-ex loss: 0.613534  [   18/   88]
per-ex loss: 0.752892  [   19/   88]
per-ex loss: 0.651666  [   20/   88]
per-ex loss: 0.550305  [   21/   88]
per-ex loss: 0.681787  [   22/   88]
per-ex loss: 0.582036  [   23/   88]
per-ex loss: 0.824994  [   24/   88]
per-ex loss: 0.792769  [   25/   88]
per-ex loss: 0.702652  [   26/   88]
per-ex loss: 0.542799  [   27/   88]
per-ex loss: 0.763294  [   28/   88]
per-ex loss: 0.619312  [   29/   88]
per-ex loss: 0.568118  [   30/   88]
per-ex loss: 0.551626  [   31/   88]
per-ex loss: 0.564601  [   32/   88]
per-ex loss: 0.718455  [   33/   88]
per-ex loss: 0.553906  [   34/   88]
per-ex loss: 0.791035  [   35/   88]
per-ex loss: 0.806739  [   36/   88]
per-ex loss: 0.505485  [   37/   88]
per-ex loss: 0.589837  [   38/   88]
per-ex loss: 0.565525  [   39/   88]
per-ex loss: 0.814137  [   40/   88]
per-ex loss: 0.693380  [   41/   88]
per-ex loss: 0.554446  [   42/   88]
per-ex loss: 0.719461  [   43/   88]
per-ex loss: 0.558693  [   44/   88]
per-ex loss: 0.691870  [   45/   88]
per-ex loss: 0.467737  [   46/   88]
per-ex loss: 0.585786  [   47/   88]
per-ex loss: 0.719227  [   48/   88]
per-ex loss: 0.797952  [   49/   88]
per-ex loss: 0.610488  [   50/   88]
per-ex loss: 0.771187  [   51/   88]
per-ex loss: 0.705357  [   52/   88]
per-ex loss: 0.526785  [   53/   88]
per-ex loss: 0.643855  [   54/   88]
per-ex loss: 0.634809  [   55/   88]
per-ex loss: 0.773941  [   56/   88]
per-ex loss: 0.604143  [   57/   88]
per-ex loss: 0.617833  [   58/   88]
per-ex loss: 0.630424  [   59/   88]
per-ex loss: 0.577751  [   60/   88]
per-ex loss: 0.756384  [   61/   88]
per-ex loss: 0.532646  [   62/   88]
per-ex loss: 0.527604  [   63/   88]
per-ex loss: 0.525457  [   64/   88]
per-ex loss: 0.544089  [   65/   88]
per-ex loss: 0.559510  [   66/   88]
per-ex loss: 0.577547  [   67/   88]
per-ex loss: 0.593180  [   68/   88]
per-ex loss: 0.746662  [   69/   88]
per-ex loss: 0.508156  [   70/   88]
per-ex loss: 0.742656  [   71/   88]
per-ex loss: 0.729609  [   72/   88]
per-ex loss: 0.583885  [   73/   88]
per-ex loss: 0.612987  [   74/   88]
per-ex loss: 0.496021  [   75/   88]
per-ex loss: 0.625950  [   76/   88]
per-ex loss: 0.692715  [   77/   88]
per-ex loss: 0.753049  [   78/   88]
per-ex loss: 0.715401  [   79/   88]
per-ex loss: 0.573588  [   80/   88]
per-ex loss: 0.743483  [   81/   88]
per-ex loss: 0.824338  [   82/   88]
per-ex loss: 0.812796  [   83/   88]
per-ex loss: 0.576174  [   84/   88]
per-ex loss: 0.705316  [   85/   88]
per-ex loss: 0.590208  [   86/   88]
per-ex loss: 0.518646  [   87/   88]
per-ex loss: 0.777403  [   88/   88]
Train Error: Avg loss: 0.65160015
validation Error: 
 Avg loss: 0.70093322 
 F1: 0.485467 
 Precision: 0.603801 
 Recall: 0.405916
 IoU: 0.320539

test Error: 
 Avg loss: 0.65495908 
 F1: 0.553605 
 Precision: 0.655007 
 Recall: 0.479391
 IoU: 0.382749

We have finished training iteration 40
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_38_.pth
per-ex loss: 0.698683  [    1/   88]
per-ex loss: 0.602152  [    2/   88]
per-ex loss: 0.576176  [    3/   88]
per-ex loss: 0.587578  [    4/   88]
per-ex loss: 0.729681  [    5/   88]
per-ex loss: 0.752576  [    6/   88]
per-ex loss: 0.689218  [    7/   88]
per-ex loss: 0.548314  [    8/   88]
per-ex loss: 0.826959  [    9/   88]
per-ex loss: 0.764520  [   10/   88]
per-ex loss: 0.556704  [   11/   88]
per-ex loss: 0.737379  [   12/   88]
per-ex loss: 0.591256  [   13/   88]
per-ex loss: 0.719586  [   14/   88]
per-ex loss: 0.683428  [   15/   88]
per-ex loss: 0.770059  [   16/   88]
per-ex loss: 0.814576  [   17/   88]
per-ex loss: 0.576392  [   18/   88]
per-ex loss: 0.719710  [   19/   88]
per-ex loss: 0.626912  [   20/   88]
per-ex loss: 0.538585  [   21/   88]
per-ex loss: 0.764602  [   22/   88]
per-ex loss: 0.691534  [   23/   88]
per-ex loss: 0.723215  [   24/   88]
per-ex loss: 0.545891  [   25/   88]
per-ex loss: 0.597050  [   26/   88]
per-ex loss: 0.591921  [   27/   88]
per-ex loss: 0.534702  [   28/   88]
per-ex loss: 0.602335  [   29/   88]
per-ex loss: 0.535057  [   30/   88]
per-ex loss: 0.813326  [   31/   88]
per-ex loss: 0.749894  [   32/   88]
per-ex loss: 0.766863  [   33/   88]
per-ex loss: 0.614198  [   34/   88]
per-ex loss: 0.534007  [   35/   88]
per-ex loss: 0.614585  [   36/   88]
per-ex loss: 0.575344  [   37/   88]
per-ex loss: 0.780858  [   38/   88]
per-ex loss: 0.520387  [   39/   88]
per-ex loss: 0.639686  [   40/   88]
per-ex loss: 0.719616  [   41/   88]
per-ex loss: 0.712903  [   42/   88]
per-ex loss: 0.617470  [   43/   88]
per-ex loss: 0.581123  [   44/   88]
per-ex loss: 0.805105  [   45/   88]
per-ex loss: 0.579198  [   46/   88]
per-ex loss: 0.721334  [   47/   88]
per-ex loss: 0.792342  [   48/   88]
per-ex loss: 0.745211  [   49/   88]
per-ex loss: 0.549552  [   50/   88]
per-ex loss: 0.756460  [   51/   88]
per-ex loss: 0.638542  [   52/   88]
per-ex loss: 0.554947  [   53/   88]
per-ex loss: 0.564356  [   54/   88]
per-ex loss: 0.692438  [   55/   88]
per-ex loss: 0.741003  [   56/   88]
per-ex loss: 0.593830  [   57/   88]
per-ex loss: 0.568351  [   58/   88]
per-ex loss: 0.540028  [   59/   88]
per-ex loss: 0.705503  [   60/   88]
per-ex loss: 0.697737  [   61/   88]
per-ex loss: 0.564752  [   62/   88]
per-ex loss: 0.696153  [   63/   88]
per-ex loss: 0.793665  [   64/   88]
per-ex loss: 0.542771  [   65/   88]
per-ex loss: 0.752108  [   66/   88]
per-ex loss: 0.730763  [   67/   88]
per-ex loss: 0.493494  [   68/   88]
per-ex loss: 0.611742  [   69/   88]
per-ex loss: 0.491242  [   70/   88]
per-ex loss: 0.819700  [   71/   88]
per-ex loss: 0.557224  [   72/   88]
per-ex loss: 0.802111  [   73/   88]
per-ex loss: 0.788843  [   74/   88]
per-ex loss: 0.554048  [   75/   88]
per-ex loss: 0.548672  [   76/   88]
per-ex loss: 0.512167  [   77/   88]
per-ex loss: 0.524420  [   78/   88]
per-ex loss: 0.757310  [   79/   88]
per-ex loss: 0.553475  [   80/   88]
per-ex loss: 0.620755  [   81/   88]
per-ex loss: 0.665266  [   82/   88]
per-ex loss: 0.605714  [   83/   88]
per-ex loss: 0.757222  [   84/   88]
per-ex loss: 0.531943  [   85/   88]
per-ex loss: 0.594970  [   86/   88]
per-ex loss: 0.675685  [   87/   88]
per-ex loss: 0.596859  [   88/   88]
Train Error: Avg loss: 0.65143204
validation Error: 
 Avg loss: 0.70964206 
 F1: 0.469778 
 Precision: 0.680565 
 Recall: 0.358685
 IoU: 0.307000

test Error: 
 Avg loss: 0.67093594 
 F1: 0.534140 
 Precision: 0.708982 
 Recall: 0.428475
 IoU: 0.364387

We have finished training iteration 41
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_39_.pth
per-ex loss: 0.643975  [    1/   88]
per-ex loss: 0.524542  [    2/   88]
per-ex loss: 0.689028  [    3/   88]
per-ex loss: 0.583567  [    4/   88]
per-ex loss: 0.614558  [    5/   88]
per-ex loss: 0.556259  [    6/   88]
per-ex loss: 0.750390  [    7/   88]
per-ex loss: 0.607483  [    8/   88]
per-ex loss: 0.540327  [    9/   88]
per-ex loss: 0.724860  [   10/   88]
per-ex loss: 0.556512  [   11/   88]
per-ex loss: 0.824949  [   12/   88]
per-ex loss: 0.704155  [   13/   88]
per-ex loss: 0.615442  [   14/   88]
per-ex loss: 0.531099  [   15/   88]
per-ex loss: 0.647240  [   16/   88]
per-ex loss: 0.790951  [   17/   88]
per-ex loss: 0.809548  [   18/   88]
per-ex loss: 0.605671  [   19/   88]
per-ex loss: 0.714000  [   20/   88]
per-ex loss: 0.699768  [   21/   88]
per-ex loss: 0.538940  [   22/   88]
per-ex loss: 0.866247  [   23/   88]
per-ex loss: 0.742890  [   24/   88]
per-ex loss: 0.550325  [   25/   88]
per-ex loss: 0.667262  [   26/   88]
per-ex loss: 0.620739  [   27/   88]
per-ex loss: 0.614537  [   28/   88]
per-ex loss: 0.620123  [   29/   88]
per-ex loss: 0.560632  [   30/   88]
per-ex loss: 0.774241  [   31/   88]
per-ex loss: 0.496118  [   32/   88]
per-ex loss: 0.521985  [   33/   88]
per-ex loss: 0.570032  [   34/   88]
per-ex loss: 0.587974  [   35/   88]
per-ex loss: 0.558953  [   36/   88]
per-ex loss: 0.507810  [   37/   88]
per-ex loss: 0.562812  [   38/   88]
per-ex loss: 0.739497  [   39/   88]
per-ex loss: 0.544186  [   40/   88]
per-ex loss: 0.738811  [   41/   88]
per-ex loss: 0.574822  [   42/   88]
per-ex loss: 0.803695  [   43/   88]
per-ex loss: 0.622434  [   44/   88]
per-ex loss: 0.620160  [   45/   88]
per-ex loss: 0.768351  [   46/   88]
per-ex loss: 0.754325  [   47/   88]
per-ex loss: 0.794303  [   48/   88]
per-ex loss: 0.822943  [   49/   88]
per-ex loss: 0.490288  [   50/   88]
per-ex loss: 0.594807  [   51/   88]
per-ex loss: 0.620932  [   52/   88]
per-ex loss: 0.738691  [   53/   88]
per-ex loss: 0.759248  [   54/   88]
per-ex loss: 0.576480  [   55/   88]
per-ex loss: 0.711143  [   56/   88]
per-ex loss: 0.783864  [   57/   88]
per-ex loss: 0.524348  [   58/   88]
per-ex loss: 0.686145  [   59/   88]
per-ex loss: 0.821504  [   60/   88]
per-ex loss: 0.463863  [   61/   88]
per-ex loss: 0.636832  [   62/   88]
per-ex loss: 0.547472  [   63/   88]
per-ex loss: 0.832348  [   64/   88]
per-ex loss: 0.513831  [   65/   88]
per-ex loss: 0.577195  [   66/   88]
per-ex loss: 0.569134  [   67/   88]
per-ex loss: 0.534003  [   68/   88]
per-ex loss: 0.623448  [   69/   88]
per-ex loss: 0.555155  [   70/   88]
per-ex loss: 0.568274  [   71/   88]
per-ex loss: 0.767288  [   72/   88]
per-ex loss: 0.721940  [   73/   88]
per-ex loss: 0.624776  [   74/   88]
per-ex loss: 0.760642  [   75/   88]
per-ex loss: 0.777544  [   76/   88]
per-ex loss: 0.689535  [   77/   88]
per-ex loss: 0.754464  [   78/   88]
per-ex loss: 0.699076  [   79/   88]
per-ex loss: 0.522616  [   80/   88]
per-ex loss: 0.732376  [   81/   88]
per-ex loss: 0.657735  [   82/   88]
per-ex loss: 0.778207  [   83/   88]
per-ex loss: 0.633886  [   84/   88]
per-ex loss: 0.696190  [   85/   88]
per-ex loss: 0.744865  [   86/   88]
per-ex loss: 0.745638  [   87/   88]
per-ex loss: 0.701791  [   88/   88]
Train Error: Avg loss: 0.65473913
validation Error: 
 Avg loss: 0.73074822 
 F1: 0.434763 
 Precision: 0.396948 
 Recall: 0.480541
 IoU: 0.277762

test Error: 
 Avg loss: 0.65583855 
 F1: 0.548440 
 Precision: 0.523540 
 Recall: 0.575827
 IoU: 0.377828

We have finished training iteration 42
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_40_.pth
per-ex loss: 0.572809  [    1/   88]
per-ex loss: 0.585625  [    2/   88]
per-ex loss: 0.676860  [    3/   88]
per-ex loss: 0.712534  [    4/   88]
per-ex loss: 0.652994  [    5/   88]
per-ex loss: 0.524830  [    6/   88]
per-ex loss: 0.751696  [    7/   88]
per-ex loss: 0.555218  [    8/   88]
per-ex loss: 0.566740  [    9/   88]
per-ex loss: 0.811701  [   10/   88]
per-ex loss: 0.548001  [   11/   88]
per-ex loss: 0.634427  [   12/   88]
per-ex loss: 0.743998  [   13/   88]
per-ex loss: 0.702512  [   14/   88]
per-ex loss: 0.809709  [   15/   88]
per-ex loss: 0.510318  [   16/   88]
per-ex loss: 0.774155  [   17/   88]
per-ex loss: 0.671646  [   18/   88]
per-ex loss: 0.676209  [   19/   88]
per-ex loss: 0.551252  [   20/   88]
per-ex loss: 0.711948  [   21/   88]
per-ex loss: 0.529415  [   22/   88]
per-ex loss: 0.732584  [   23/   88]
per-ex loss: 0.576781  [   24/   88]
per-ex loss: 0.609035  [   25/   88]
per-ex loss: 0.554941  [   26/   88]
per-ex loss: 0.486489  [   27/   88]
per-ex loss: 0.569911  [   28/   88]
per-ex loss: 0.779931  [   29/   88]
per-ex loss: 0.776766  [   30/   88]
per-ex loss: 0.755139  [   31/   88]
per-ex loss: 0.674045  [   32/   88]
per-ex loss: 0.715375  [   33/   88]
per-ex loss: 0.713399  [   34/   88]
per-ex loss: 0.602165  [   35/   88]
per-ex loss: 0.554227  [   36/   88]
per-ex loss: 0.587573  [   37/   88]
per-ex loss: 0.671959  [   38/   88]
per-ex loss: 0.552556  [   39/   88]
per-ex loss: 0.600165  [   40/   88]
per-ex loss: 0.551550  [   41/   88]
per-ex loss: 0.549806  [   42/   88]
per-ex loss: 0.635832  [   43/   88]
per-ex loss: 0.574726  [   44/   88]
per-ex loss: 0.708188  [   45/   88]
per-ex loss: 0.569367  [   46/   88]
per-ex loss: 0.601617  [   47/   88]
per-ex loss: 0.470579  [   48/   88]
per-ex loss: 0.623494  [   49/   88]
per-ex loss: 0.587927  [   50/   88]
per-ex loss: 0.739990  [   51/   88]
per-ex loss: 0.809119  [   52/   88]
per-ex loss: 0.540230  [   53/   88]
per-ex loss: 0.561463  [   54/   88]
per-ex loss: 0.757938  [   55/   88]
per-ex loss: 0.721694  [   56/   88]
per-ex loss: 0.518216  [   57/   88]
per-ex loss: 0.742631  [   58/   88]
per-ex loss: 0.556365  [   59/   88]
per-ex loss: 0.606909  [   60/   88]
per-ex loss: 0.581664  [   61/   88]
per-ex loss: 0.616728  [   62/   88]
per-ex loss: 0.814968  [   63/   88]
per-ex loss: 0.577236  [   64/   88]
per-ex loss: 0.756660  [   65/   88]
per-ex loss: 0.585169  [   66/   88]
per-ex loss: 0.777180  [   67/   88]
per-ex loss: 0.566446  [   68/   88]
per-ex loss: 0.812846  [   69/   88]
per-ex loss: 0.727015  [   70/   88]
per-ex loss: 0.538574  [   71/   88]
per-ex loss: 0.804490  [   72/   88]
per-ex loss: 0.810686  [   73/   88]
per-ex loss: 0.732049  [   74/   88]
per-ex loss: 0.729259  [   75/   88]
per-ex loss: 0.533913  [   76/   88]
per-ex loss: 0.598986  [   77/   88]
per-ex loss: 0.643768  [   78/   88]
per-ex loss: 0.579699  [   79/   88]
per-ex loss: 0.822029  [   80/   88]
per-ex loss: 0.607421  [   81/   88]
per-ex loss: 0.570259  [   82/   88]
per-ex loss: 0.805957  [   83/   88]
per-ex loss: 0.701877  [   84/   88]
per-ex loss: 0.777744  [   85/   88]
per-ex loss: 0.519821  [   86/   88]
per-ex loss: 0.681269  [   87/   88]
per-ex loss: 0.721250  [   88/   88]
Train Error: Avg loss: 0.65011635
validation Error: 
 Avg loss: 0.69476555 
 F1: 0.493587 
 Precision: 0.561469 
 Recall: 0.440349
 IoU: 0.327657

test Error: 
 Avg loss: 0.64481019 
 F1: 0.566017 
 Precision: 0.631696 
 Recall: 0.512710
 IoU: 0.394717

We have finished training iteration 43
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_41_.pth
per-ex loss: 0.531364  [    1/   88]
per-ex loss: 0.794217  [    2/   88]
per-ex loss: 0.732742  [    3/   88]
per-ex loss: 0.549002  [    4/   88]
per-ex loss: 0.718448  [    5/   88]
per-ex loss: 0.771590  [    6/   88]
per-ex loss: 0.534359  [    7/   88]
per-ex loss: 0.588603  [    8/   88]
per-ex loss: 0.600475  [    9/   88]
per-ex loss: 0.621113  [   10/   88]
per-ex loss: 0.747217  [   11/   88]
per-ex loss: 0.549498  [   12/   88]
per-ex loss: 0.678748  [   13/   88]
per-ex loss: 0.576363  [   14/   88]
per-ex loss: 0.670502  [   15/   88]
per-ex loss: 0.738751  [   16/   88]
per-ex loss: 0.798960  [   17/   88]
per-ex loss: 0.774308  [   18/   88]
per-ex loss: 0.769183  [   19/   88]
per-ex loss: 0.740280  [   20/   88]
per-ex loss: 0.574625  [   21/   88]
per-ex loss: 0.555700  [   22/   88]
per-ex loss: 0.692628  [   23/   88]
per-ex loss: 0.579727  [   24/   88]
per-ex loss: 0.638255  [   25/   88]
per-ex loss: 0.582296  [   26/   88]
per-ex loss: 0.763678  [   27/   88]
per-ex loss: 0.546955  [   28/   88]
per-ex loss: 0.529077  [   29/   88]
per-ex loss: 0.594612  [   30/   88]
per-ex loss: 0.544396  [   31/   88]
per-ex loss: 0.597005  [   32/   88]
per-ex loss: 0.723580  [   33/   88]
per-ex loss: 0.613447  [   34/   88]
per-ex loss: 0.769935  [   35/   88]
per-ex loss: 0.595322  [   36/   88]
per-ex loss: 0.667053  [   37/   88]
per-ex loss: 0.544472  [   38/   88]
per-ex loss: 0.639503  [   39/   88]
per-ex loss: 0.690320  [   40/   88]
per-ex loss: 0.561004  [   41/   88]
per-ex loss: 0.687356  [   42/   88]
per-ex loss: 0.708917  [   43/   88]
per-ex loss: 0.756497  [   44/   88]
per-ex loss: 0.537694  [   45/   88]
per-ex loss: 0.538113  [   46/   88]
per-ex loss: 0.810653  [   47/   88]
per-ex loss: 0.818613  [   48/   88]
per-ex loss: 0.730648  [   49/   88]
per-ex loss: 0.563371  [   50/   88]
per-ex loss: 0.757237  [   51/   88]
per-ex loss: 0.724402  [   52/   88]
per-ex loss: 0.527603  [   53/   88]
per-ex loss: 0.544827  [   54/   88]
per-ex loss: 0.719853  [   55/   88]
per-ex loss: 0.783046  [   56/   88]
per-ex loss: 0.720280  [   57/   88]
per-ex loss: 0.575515  [   58/   88]
per-ex loss: 0.523619  [   59/   88]
per-ex loss: 0.732516  [   60/   88]
per-ex loss: 0.608982  [   61/   88]
per-ex loss: 0.747066  [   62/   88]
per-ex loss: 0.532642  [   63/   88]
per-ex loss: 0.469786  [   64/   88]
per-ex loss: 0.673141  [   65/   88]
per-ex loss: 0.613174  [   66/   88]
per-ex loss: 0.520379  [   67/   88]
per-ex loss: 0.570390  [   68/   88]
per-ex loss: 0.595571  [   69/   88]
per-ex loss: 0.616125  [   70/   88]
per-ex loss: 0.581197  [   71/   88]
per-ex loss: 0.813060  [   72/   88]
per-ex loss: 0.737500  [   73/   88]
per-ex loss: 0.684518  [   74/   88]
per-ex loss: 0.670493  [   75/   88]
per-ex loss: 0.579909  [   76/   88]
per-ex loss: 0.623879  [   77/   88]
per-ex loss: 0.801644  [   78/   88]
per-ex loss: 0.709052  [   79/   88]
per-ex loss: 0.618025  [   80/   88]
per-ex loss: 0.588453  [   81/   88]
per-ex loss: 0.721602  [   82/   88]
per-ex loss: 0.697209  [   83/   88]
per-ex loss: 0.582438  [   84/   88]
per-ex loss: 0.688622  [   85/   88]
per-ex loss: 0.822330  [   86/   88]
per-ex loss: 0.542161  [   87/   88]
per-ex loss: 0.534261  [   88/   88]
Train Error: Avg loss: 0.64992819
validation Error: 
 Avg loss: 0.70575543 
 F1: 0.485115 
 Precision: 0.597733 
 Recall: 0.408206
 IoU: 0.320233

test Error: 
 Avg loss: 0.64946393 
 F1: 0.560928 
 Precision: 0.679052 
 Recall: 0.477811
 IoU: 0.389785

We have finished training iteration 44
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_42_.pth
per-ex loss: 0.705802  [    1/   88]
per-ex loss: 0.603466  [    2/   88]
per-ex loss: 0.756539  [    3/   88]
per-ex loss: 0.511403  [    4/   88]
per-ex loss: 0.605783  [    5/   88]
per-ex loss: 0.569628  [    6/   88]
per-ex loss: 0.588706  [    7/   88]
per-ex loss: 0.540001  [    8/   88]
per-ex loss: 0.699036  [    9/   88]
per-ex loss: 0.772721  [   10/   88]
per-ex loss: 0.564328  [   11/   88]
per-ex loss: 0.613621  [   12/   88]
per-ex loss: 0.746867  [   13/   88]
per-ex loss: 0.756901  [   14/   88]
per-ex loss: 0.643974  [   15/   88]
per-ex loss: 0.543614  [   16/   88]
per-ex loss: 0.681730  [   17/   88]
per-ex loss: 0.525051  [   18/   88]
per-ex loss: 0.600588  [   19/   88]
per-ex loss: 0.651134  [   20/   88]
per-ex loss: 0.567896  [   21/   88]
per-ex loss: 0.569007  [   22/   88]
per-ex loss: 0.702790  [   23/   88]
per-ex loss: 0.575534  [   24/   88]
per-ex loss: 0.818808  [   25/   88]
per-ex loss: 0.537979  [   26/   88]
per-ex loss: 0.543732  [   27/   88]
per-ex loss: 0.511106  [   28/   88]
per-ex loss: 0.545237  [   29/   88]
per-ex loss: 0.819220  [   30/   88]
per-ex loss: 0.756098  [   31/   88]
per-ex loss: 0.817721  [   32/   88]
per-ex loss: 0.546882  [   33/   88]
per-ex loss: 0.771726  [   34/   88]
per-ex loss: 0.564760  [   35/   88]
per-ex loss: 0.549889  [   36/   88]
per-ex loss: 0.561911  [   37/   88]
per-ex loss: 0.458337  [   38/   88]
per-ex loss: 0.706555  [   39/   88]
per-ex loss: 0.782215  [   40/   88]
per-ex loss: 0.699880  [   41/   88]
per-ex loss: 0.525109  [   42/   88]
per-ex loss: 0.591542  [   43/   88]
per-ex loss: 0.598743  [   44/   88]
per-ex loss: 0.627030  [   45/   88]
per-ex loss: 0.750994  [   46/   88]
per-ex loss: 0.795163  [   47/   88]
per-ex loss: 0.636492  [   48/   88]
per-ex loss: 0.585970  [   49/   88]
per-ex loss: 0.740206  [   50/   88]
per-ex loss: 0.577346  [   51/   88]
per-ex loss: 0.522614  [   52/   88]
per-ex loss: 0.718289  [   53/   88]
per-ex loss: 0.678543  [   54/   88]
per-ex loss: 0.578733  [   55/   88]
per-ex loss: 0.769635  [   56/   88]
per-ex loss: 0.540113  [   57/   88]
per-ex loss: 0.617837  [   58/   88]
per-ex loss: 0.800534  [   59/   88]
per-ex loss: 0.693759  [   60/   88]
per-ex loss: 0.569924  [   61/   88]
per-ex loss: 0.686789  [   62/   88]
per-ex loss: 0.498408  [   63/   88]
per-ex loss: 0.534571  [   64/   88]
per-ex loss: 0.630379  [   65/   88]
per-ex loss: 0.707695  [   66/   88]
per-ex loss: 0.718869  [   67/   88]
per-ex loss: 0.836503  [   68/   88]
per-ex loss: 0.552468  [   69/   88]
per-ex loss: 0.520883  [   70/   88]
per-ex loss: 0.721527  [   71/   88]
per-ex loss: 0.814662  [   72/   88]
per-ex loss: 0.691341  [   73/   88]
per-ex loss: 0.743000  [   74/   88]
per-ex loss: 0.737728  [   75/   88]
per-ex loss: 0.776841  [   76/   88]
per-ex loss: 0.597896  [   77/   88]
per-ex loss: 0.805107  [   78/   88]
per-ex loss: 0.581601  [   79/   88]
per-ex loss: 0.618998  [   80/   88]
per-ex loss: 0.532263  [   81/   88]
per-ex loss: 0.663480  [   82/   88]
per-ex loss: 0.512375  [   83/   88]
per-ex loss: 0.524375  [   84/   88]
per-ex loss: 0.770168  [   85/   88]
per-ex loss: 0.699099  [   86/   88]
per-ex loss: 0.722614  [   87/   88]
per-ex loss: 0.621831  [   88/   88]
Train Error: Avg loss: 0.64577530
validation Error: 
 Avg loss: 0.70556478 
 F1: 0.478392 
 Precision: 0.476236 
 Recall: 0.480567
 IoU: 0.314399

test Error: 
 Avg loss: 0.65643940 
 F1: 0.556571 
 Precision: 0.531415 
 Recall: 0.584227
 IoU: 0.385590

We have finished training iteration 45
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_28_.pth
per-ex loss: 0.521456  [    1/   88]
per-ex loss: 0.693444  [    2/   88]
per-ex loss: 0.585576  [    3/   88]
per-ex loss: 0.624391  [    4/   88]
per-ex loss: 0.758624  [    5/   88]
per-ex loss: 0.695976  [    6/   88]
per-ex loss: 0.560535  [    7/   88]
per-ex loss: 0.737773  [    8/   88]
per-ex loss: 0.715154  [    9/   88]
per-ex loss: 0.607223  [   10/   88]
per-ex loss: 0.576557  [   11/   88]
per-ex loss: 0.552890  [   12/   88]
per-ex loss: 0.726608  [   13/   88]
per-ex loss: 0.774029  [   14/   88]
per-ex loss: 0.620013  [   15/   88]
per-ex loss: 0.608315  [   16/   88]
per-ex loss: 0.586511  [   17/   88]
per-ex loss: 0.760316  [   18/   88]
per-ex loss: 0.809230  [   19/   88]
per-ex loss: 0.564987  [   20/   88]
per-ex loss: 0.611530  [   21/   88]
per-ex loss: 0.746673  [   22/   88]
per-ex loss: 0.615735  [   23/   88]
per-ex loss: 0.742909  [   24/   88]
per-ex loss: 0.737788  [   25/   88]
per-ex loss: 0.758150  [   26/   88]
per-ex loss: 0.602422  [   27/   88]
per-ex loss: 0.648182  [   28/   88]
per-ex loss: 0.511358  [   29/   88]
per-ex loss: 0.598185  [   30/   88]
per-ex loss: 0.709774  [   31/   88]
per-ex loss: 0.541403  [   32/   88]
per-ex loss: 0.582727  [   33/   88]
per-ex loss: 0.576839  [   34/   88]
per-ex loss: 0.543334  [   35/   88]
per-ex loss: 0.566935  [   36/   88]
per-ex loss: 0.782193  [   37/   88]
per-ex loss: 0.531977  [   38/   88]
per-ex loss: 0.812129  [   39/   88]
per-ex loss: 0.816117  [   40/   88]
per-ex loss: 0.733041  [   41/   88]
per-ex loss: 0.564864  [   42/   88]
per-ex loss: 0.776821  [   43/   88]
per-ex loss: 0.560510  [   44/   88]
per-ex loss: 0.794280  [   45/   88]
per-ex loss: 0.786661  [   46/   88]
per-ex loss: 0.748834  [   47/   88]
per-ex loss: 0.746478  [   48/   88]
per-ex loss: 0.592403  [   49/   88]
per-ex loss: 0.663469  [   50/   88]
per-ex loss: 0.595897  [   51/   88]
per-ex loss: 0.599954  [   52/   88]
per-ex loss: 0.537860  [   53/   88]
per-ex loss: 0.782921  [   54/   88]
per-ex loss: 0.548051  [   55/   88]
per-ex loss: 0.604007  [   56/   88]
per-ex loss: 0.558764  [   57/   88]
per-ex loss: 0.778507  [   58/   88]
per-ex loss: 0.574771  [   59/   88]
per-ex loss: 0.703484  [   60/   88]
per-ex loss: 0.693243  [   61/   88]
per-ex loss: 0.581733  [   62/   88]
per-ex loss: 0.805729  [   63/   88]
per-ex loss: 0.520492  [   64/   88]
per-ex loss: 0.590598  [   65/   88]
per-ex loss: 0.474422  [   66/   88]
per-ex loss: 0.809362  [   67/   88]
per-ex loss: 0.728360  [   68/   88]
per-ex loss: 0.695815  [   69/   88]
per-ex loss: 0.725960  [   70/   88]
per-ex loss: 0.595056  [   71/   88]
per-ex loss: 0.813072  [   72/   88]
per-ex loss: 0.718032  [   73/   88]
per-ex loss: 0.645134  [   74/   88]
per-ex loss: 0.561073  [   75/   88]
per-ex loss: 0.560373  [   76/   88]
per-ex loss: 0.553619  [   77/   88]
per-ex loss: 0.707100  [   78/   88]
per-ex loss: 0.685466  [   79/   88]
per-ex loss: 0.565152  [   80/   88]
per-ex loss: 0.533023  [   81/   88]
per-ex loss: 0.673196  [   82/   88]
per-ex loss: 0.515256  [   83/   88]
per-ex loss: 0.561035  [   84/   88]
per-ex loss: 0.515641  [   85/   88]
per-ex loss: 0.755126  [   86/   88]
per-ex loss: 0.515721  [   87/   88]
per-ex loss: 0.792973  [   88/   88]
Train Error: Avg loss: 0.65067393
validation Error: 
 Avg loss: 0.70161556 
 F1: 0.482758 
 Precision: 0.496082 
 Recall: 0.470131
 IoU: 0.318181

test Error: 
 Avg loss: 0.64483280 
 F1: 0.565842 
 Precision: 0.581189 
 Recall: 0.551285
 IoU: 0.394547

We have finished training iteration 46
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_44_.pth
per-ex loss: 0.630890  [    1/   88]
per-ex loss: 0.542005  [    2/   88]
per-ex loss: 0.571906  [    3/   88]
per-ex loss: 0.445615  [    4/   88]
per-ex loss: 0.681825  [    5/   88]
per-ex loss: 0.603093  [    6/   88]
per-ex loss: 0.515694  [    7/   88]
per-ex loss: 0.675746  [    8/   88]
per-ex loss: 0.550228  [    9/   88]
per-ex loss: 0.609107  [   10/   88]
per-ex loss: 0.753781  [   11/   88]
per-ex loss: 0.545462  [   12/   88]
per-ex loss: 0.727580  [   13/   88]
per-ex loss: 0.528874  [   14/   88]
per-ex loss: 0.788294  [   15/   88]
per-ex loss: 0.607122  [   16/   88]
per-ex loss: 0.575065  [   17/   88]
per-ex loss: 0.685292  [   18/   88]
per-ex loss: 0.594736  [   19/   88]
per-ex loss: 0.741533  [   20/   88]
per-ex loss: 0.581942  [   21/   88]
per-ex loss: 0.554863  [   22/   88]
per-ex loss: 0.637218  [   23/   88]
per-ex loss: 0.551109  [   24/   88]
per-ex loss: 0.537685  [   25/   88]
per-ex loss: 0.591700  [   26/   88]
per-ex loss: 0.753136  [   27/   88]
per-ex loss: 0.714074  [   28/   88]
per-ex loss: 0.552541  [   29/   88]
per-ex loss: 0.750457  [   30/   88]
per-ex loss: 0.539939  [   31/   88]
per-ex loss: 0.608416  [   32/   88]
per-ex loss: 0.823546  [   33/   88]
per-ex loss: 0.577508  [   34/   88]
per-ex loss: 0.683000  [   35/   88]
per-ex loss: 0.865827  [   36/   88]
per-ex loss: 0.691872  [   37/   88]
per-ex loss: 0.768525  [   38/   88]
per-ex loss: 0.787141  [   39/   88]
per-ex loss: 0.678614  [   40/   88]
per-ex loss: 0.775010  [   41/   88]
per-ex loss: 0.606899  [   42/   88]
per-ex loss: 0.561808  [   43/   88]
per-ex loss: 0.791693  [   44/   88]
per-ex loss: 0.567532  [   45/   88]
per-ex loss: 0.597423  [   46/   88]
per-ex loss: 0.659069  [   47/   88]
per-ex loss: 0.575762  [   48/   88]
per-ex loss: 0.709952  [   49/   88]
per-ex loss: 0.510115  [   50/   88]
per-ex loss: 0.528769  [   51/   88]
per-ex loss: 0.720523  [   52/   88]
per-ex loss: 0.545507  [   53/   88]
per-ex loss: 0.564409  [   54/   88]
per-ex loss: 0.717505  [   55/   88]
per-ex loss: 0.719516  [   56/   88]
per-ex loss: 0.518353  [   57/   88]
per-ex loss: 0.709045  [   58/   88]
per-ex loss: 0.580152  [   59/   88]
per-ex loss: 0.740467  [   60/   88]
per-ex loss: 0.593235  [   61/   88]
per-ex loss: 0.579125  [   62/   88]
per-ex loss: 0.743477  [   63/   88]
per-ex loss: 0.542996  [   64/   88]
per-ex loss: 0.474970  [   65/   88]
per-ex loss: 0.767939  [   66/   88]
per-ex loss: 0.716487  [   67/   88]
per-ex loss: 0.553075  [   68/   88]
per-ex loss: 0.583963  [   69/   88]
per-ex loss: 0.610989  [   70/   88]
per-ex loss: 0.650399  [   71/   88]
per-ex loss: 0.500410  [   72/   88]
per-ex loss: 0.800033  [   73/   88]
per-ex loss: 0.609151  [   74/   88]
per-ex loss: 0.755604  [   75/   88]
per-ex loss: 0.714410  [   76/   88]
per-ex loss: 0.699008  [   77/   88]
per-ex loss: 0.798862  [   78/   88]
per-ex loss: 0.756243  [   79/   88]
per-ex loss: 0.747427  [   80/   88]
per-ex loss: 0.584170  [   81/   88]
per-ex loss: 0.539564  [   82/   88]
per-ex loss: 0.584866  [   83/   88]
per-ex loss: 0.704655  [   84/   88]
per-ex loss: 0.795626  [   85/   88]
per-ex loss: 0.686175  [   86/   88]
per-ex loss: 0.813460  [   87/   88]
per-ex loss: 0.548653  [   88/   88]
Train Error: Avg loss: 0.64519817
validation Error: 
 Avg loss: 0.69800394 
 F1: 0.488548 
 Precision: 0.502855 
 Recall: 0.475031
 IoU: 0.323231

test Error: 
 Avg loss: 0.64701022 
 F1: 0.564902 
 Precision: 0.572799 
 Recall: 0.557221
 IoU: 0.393633

We have finished training iteration 47
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_45_.pth
per-ex loss: 0.554200  [    1/   88]
per-ex loss: 0.521822  [    2/   88]
per-ex loss: 0.794343  [    3/   88]
per-ex loss: 0.802730  [    4/   88]
per-ex loss: 0.714746  [    5/   88]
per-ex loss: 0.690950  [    6/   88]
per-ex loss: 0.534792  [    7/   88]
per-ex loss: 0.545563  [    8/   88]
per-ex loss: 0.520731  [    9/   88]
per-ex loss: 0.698898  [   10/   88]
per-ex loss: 0.602865  [   11/   88]
per-ex loss: 0.547250  [   12/   88]
per-ex loss: 0.674158  [   13/   88]
per-ex loss: 0.684851  [   14/   88]
per-ex loss: 0.519039  [   15/   88]
per-ex loss: 0.816743  [   16/   88]
per-ex loss: 0.680728  [   17/   88]
per-ex loss: 0.551585  [   18/   88]
per-ex loss: 0.566448  [   19/   88]
per-ex loss: 0.806838  [   20/   88]
per-ex loss: 0.580331  [   21/   88]
per-ex loss: 0.573278  [   22/   88]
per-ex loss: 0.757310  [   23/   88]
per-ex loss: 0.518305  [   24/   88]
per-ex loss: 0.629805  [   25/   88]
per-ex loss: 0.779545  [   26/   88]
per-ex loss: 0.718614  [   27/   88]
per-ex loss: 0.779001  [   28/   88]
per-ex loss: 0.600577  [   29/   88]
per-ex loss: 0.545811  [   30/   88]
per-ex loss: 0.588667  [   31/   88]
per-ex loss: 0.749497  [   32/   88]
per-ex loss: 0.722375  [   33/   88]
per-ex loss: 0.649208  [   34/   88]
per-ex loss: 0.556841  [   35/   88]
per-ex loss: 0.695711  [   36/   88]
per-ex loss: 0.496283  [   37/   88]
per-ex loss: 0.588410  [   38/   88]
per-ex loss: 0.541294  [   39/   88]
per-ex loss: 0.637987  [   40/   88]
per-ex loss: 0.753363  [   41/   88]
per-ex loss: 0.721847  [   42/   88]
per-ex loss: 0.766180  [   43/   88]
per-ex loss: 0.674574  [   44/   88]
per-ex loss: 0.711280  [   45/   88]
per-ex loss: 0.586386  [   46/   88]
per-ex loss: 0.731074  [   47/   88]
per-ex loss: 0.804947  [   48/   88]
per-ex loss: 0.518865  [   49/   88]
per-ex loss: 0.774094  [   50/   88]
per-ex loss: 0.615896  [   51/   88]
per-ex loss: 0.586785  [   52/   88]
per-ex loss: 0.600480  [   53/   88]
per-ex loss: 0.606614  [   54/   88]
per-ex loss: 0.818166  [   55/   88]
per-ex loss: 0.490878  [   56/   88]
per-ex loss: 0.600215  [   57/   88]
per-ex loss: 0.622550  [   58/   88]
per-ex loss: 0.593552  [   59/   88]
per-ex loss: 0.570478  [   60/   88]
per-ex loss: 0.675122  [   61/   88]
per-ex loss: 0.530030  [   62/   88]
per-ex loss: 0.586313  [   63/   88]
per-ex loss: 0.772588  [   64/   88]
per-ex loss: 0.735946  [   65/   88]
per-ex loss: 0.536247  [   66/   88]
per-ex loss: 0.716343  [   67/   88]
per-ex loss: 0.488046  [   68/   88]
per-ex loss: 0.822157  [   69/   88]
per-ex loss: 0.729783  [   70/   88]
per-ex loss: 0.676486  [   71/   88]
per-ex loss: 0.584853  [   72/   88]
per-ex loss: 0.554729  [   73/   88]
per-ex loss: 0.521553  [   74/   88]
per-ex loss: 0.714036  [   75/   88]
per-ex loss: 0.769187  [   76/   88]
per-ex loss: 0.770788  [   77/   88]
per-ex loss: 0.765101  [   78/   88]
per-ex loss: 0.588650  [   79/   88]
per-ex loss: 0.755330  [   80/   88]
per-ex loss: 0.533650  [   81/   88]
per-ex loss: 0.772588  [   82/   88]
per-ex loss: 0.532504  [   83/   88]
per-ex loss: 0.736628  [   84/   88]
per-ex loss: 0.730409  [   85/   88]
per-ex loss: 0.627187  [   86/   88]
per-ex loss: 0.545547  [   87/   88]
per-ex loss: 0.603373  [   88/   88]
Train Error: Avg loss: 0.64842654
validation Error: 
 Avg loss: 0.69291492 
 F1: 0.493970 
 Precision: 0.541650 
 Recall: 0.454006
 IoU: 0.327995

test Error: 
 Avg loss: 0.64098441 
 F1: 0.571785 
 Precision: 0.603818 
 Recall: 0.542980
 IoU: 0.400350

We have finished training iteration 48
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_46_.pth
per-ex loss: 0.522011  [    1/   88]
per-ex loss: 0.595692  [    2/   88]
per-ex loss: 0.548193  [    3/   88]
per-ex loss: 0.752385  [    4/   88]
per-ex loss: 0.585257  [    5/   88]
per-ex loss: 0.754593  [    6/   88]
per-ex loss: 0.688512  [    7/   88]
per-ex loss: 0.874332  [    8/   88]
per-ex loss: 0.554358  [    9/   88]
per-ex loss: 0.772946  [   10/   88]
per-ex loss: 0.549773  [   11/   88]
per-ex loss: 0.607129  [   12/   88]
per-ex loss: 0.521273  [   13/   88]
per-ex loss: 0.722984  [   14/   88]
per-ex loss: 0.600781  [   15/   88]
per-ex loss: 0.735996  [   16/   88]
per-ex loss: 0.607590  [   17/   88]
per-ex loss: 0.582810  [   18/   88]
per-ex loss: 0.739322  [   19/   88]
per-ex loss: 0.551015  [   20/   88]
per-ex loss: 0.605086  [   21/   88]
per-ex loss: 0.740399  [   22/   88]
per-ex loss: 0.764622  [   23/   88]
per-ex loss: 0.773539  [   24/   88]
per-ex loss: 0.578894  [   25/   88]
per-ex loss: 0.799923  [   26/   88]
per-ex loss: 0.768062  [   27/   88]
per-ex loss: 0.629871  [   28/   88]
per-ex loss: 0.537142  [   29/   88]
per-ex loss: 0.665172  [   30/   88]
per-ex loss: 0.700018  [   31/   88]
per-ex loss: 0.538555  [   32/   88]
per-ex loss: 0.508765  [   33/   88]
per-ex loss: 0.667911  [   34/   88]
per-ex loss: 0.695283  [   35/   88]
per-ex loss: 0.712974  [   36/   88]
per-ex loss: 0.718779  [   37/   88]
per-ex loss: 0.622265  [   38/   88]
per-ex loss: 0.656998  [   39/   88]
per-ex loss: 0.607962  [   40/   88]
per-ex loss: 0.596895  [   41/   88]
per-ex loss: 0.561178  [   42/   88]
per-ex loss: 0.618907  [   43/   88]
per-ex loss: 0.814170  [   44/   88]
per-ex loss: 0.687386  [   45/   88]
per-ex loss: 0.553608  [   46/   88]
per-ex loss: 0.549242  [   47/   88]
per-ex loss: 0.519728  [   48/   88]
per-ex loss: 0.549083  [   49/   88]
per-ex loss: 0.651767  [   50/   88]
per-ex loss: 0.827593  [   51/   88]
per-ex loss: 0.823481  [   52/   88]
per-ex loss: 0.575484  [   53/   88]
per-ex loss: 0.542801  [   54/   88]
per-ex loss: 0.544069  [   55/   88]
per-ex loss: 0.628090  [   56/   88]
per-ex loss: 0.794223  [   57/   88]
per-ex loss: 0.510196  [   58/   88]
per-ex loss: 0.559106  [   59/   88]
per-ex loss: 0.734586  [   60/   88]
per-ex loss: 0.517093  [   61/   88]
per-ex loss: 0.709861  [   62/   88]
per-ex loss: 0.783843  [   63/   88]
per-ex loss: 0.526340  [   64/   88]
per-ex loss: 0.524108  [   65/   88]
per-ex loss: 0.461262  [   66/   88]
per-ex loss: 0.768298  [   67/   88]
per-ex loss: 0.567429  [   68/   88]
per-ex loss: 0.571166  [   69/   88]
per-ex loss: 0.792702  [   70/   88]
per-ex loss: 0.584943  [   71/   88]
per-ex loss: 0.660631  [   72/   88]
per-ex loss: 0.514683  [   73/   88]
per-ex loss: 0.711935  [   74/   88]
per-ex loss: 0.675887  [   75/   88]
per-ex loss: 0.745286  [   76/   88]
per-ex loss: 0.561557  [   77/   88]
per-ex loss: 0.597310  [   78/   88]
per-ex loss: 0.442724  [   79/   88]
per-ex loss: 0.567423  [   80/   88]
per-ex loss: 0.733609  [   81/   88]
per-ex loss: 0.581100  [   82/   88]
per-ex loss: 0.789455  [   83/   88]
per-ex loss: 0.727698  [   84/   88]
per-ex loss: 0.621076  [   85/   88]
per-ex loss: 0.729300  [   86/   88]
per-ex loss: 0.803408  [   87/   88]
per-ex loss: 0.735645  [   88/   88]
Train Error: Avg loss: 0.64552884
validation Error: 
 Avg loss: 0.69642556 
 F1: 0.491821 
 Precision: 0.502889 
 Recall: 0.481228
 IoU: 0.326102

test Error: 
 Avg loss: 0.64167148 
 F1: 0.568352 
 Precision: 0.565942 
 Recall: 0.570781
 IoU: 0.396991

We have finished training iteration 49
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_47_.pth
per-ex loss: 0.618759  [    1/   88]
per-ex loss: 0.742855  [    2/   88]
per-ex loss: 0.748289  [    3/   88]
per-ex loss: 0.777940  [    4/   88]
per-ex loss: 0.677617  [    5/   88]
per-ex loss: 0.580287  [    6/   88]
per-ex loss: 0.726582  [    7/   88]
per-ex loss: 0.594221  [    8/   88]
per-ex loss: 0.784082  [    9/   88]
per-ex loss: 0.679250  [   10/   88]
per-ex loss: 0.698918  [   11/   88]
per-ex loss: 0.751896  [   12/   88]
per-ex loss: 0.624962  [   13/   88]
per-ex loss: 0.688180  [   14/   88]
per-ex loss: 0.731963  [   15/   88]
per-ex loss: 0.764505  [   16/   88]
per-ex loss: 0.760343  [   17/   88]
per-ex loss: 0.548069  [   18/   88]
per-ex loss: 0.594002  [   19/   88]
per-ex loss: 0.601979  [   20/   88]
per-ex loss: 0.565491  [   21/   88]
per-ex loss: 0.730018  [   22/   88]
per-ex loss: 0.522254  [   23/   88]
per-ex loss: 0.596655  [   24/   88]
per-ex loss: 0.725370  [   25/   88]
per-ex loss: 0.566787  [   26/   88]
per-ex loss: 0.779593  [   27/   88]
per-ex loss: 0.799933  [   28/   88]
per-ex loss: 0.708433  [   29/   88]
per-ex loss: 0.571276  [   30/   88]
per-ex loss: 0.690226  [   31/   88]
per-ex loss: 0.538557  [   32/   88]
per-ex loss: 0.578766  [   33/   88]
per-ex loss: 0.747517  [   34/   88]
per-ex loss: 0.561041  [   35/   88]
per-ex loss: 0.541100  [   36/   88]
per-ex loss: 0.773592  [   37/   88]
per-ex loss: 0.721897  [   38/   88]
per-ex loss: 0.706770  [   39/   88]
per-ex loss: 0.523384  [   40/   88]
per-ex loss: 0.461621  [   41/   88]
per-ex loss: 0.705693  [   42/   88]
per-ex loss: 0.752022  [   43/   88]
per-ex loss: 0.820671  [   44/   88]
per-ex loss: 0.820800  [   45/   88]
per-ex loss: 0.503638  [   46/   88]
per-ex loss: 0.568860  [   47/   88]
per-ex loss: 0.797729  [   48/   88]
per-ex loss: 0.561693  [   49/   88]
per-ex loss: 0.680686  [   50/   88]
per-ex loss: 0.600061  [   51/   88]
per-ex loss: 0.791542  [   52/   88]
per-ex loss: 0.706371  [   53/   88]
per-ex loss: 0.532038  [   54/   88]
per-ex loss: 0.536947  [   55/   88]
per-ex loss: 0.602045  [   56/   88]
per-ex loss: 0.576987  [   57/   88]
per-ex loss: 0.755316  [   58/   88]
per-ex loss: 0.745160  [   59/   88]
per-ex loss: 0.737776  [   60/   88]
per-ex loss: 0.626022  [   61/   88]
per-ex loss: 0.570707  [   62/   88]
per-ex loss: 0.520387  [   63/   88]
per-ex loss: 0.477335  [   64/   88]
per-ex loss: 0.732790  [   65/   88]
per-ex loss: 0.602068  [   66/   88]
per-ex loss: 0.611363  [   67/   88]
per-ex loss: 0.550235  [   68/   88]
per-ex loss: 0.552182  [   69/   88]
per-ex loss: 0.523069  [   70/   88]
per-ex loss: 0.527599  [   71/   88]
per-ex loss: 0.585608  [   72/   88]
per-ex loss: 0.574949  [   73/   88]
per-ex loss: 0.515336  [   74/   88]
per-ex loss: 0.711562  [   75/   88]
per-ex loss: 0.820133  [   76/   88]
per-ex loss: 0.590390  [   77/   88]
per-ex loss: 0.747367  [   78/   88]
per-ex loss: 0.565983  [   79/   88]
per-ex loss: 0.745899  [   80/   88]
per-ex loss: 0.537420  [   81/   88]
per-ex loss: 0.625553  [   82/   88]
per-ex loss: 0.815148  [   83/   88]
per-ex loss: 0.557034  [   84/   88]
per-ex loss: 0.590910  [   85/   88]
per-ex loss: 0.577305  [   86/   88]
per-ex loss: 0.585603  [   87/   88]
per-ex loss: 0.683386  [   88/   88]
Train Error: Avg loss: 0.64800403
validation Error: 
 Avg loss: 0.69418797 
 F1: 0.494653 
 Precision: 0.540373 
 Recall: 0.456067
 IoU: 0.328598

test Error: 
 Avg loss: 0.64059563 
 F1: 0.571796 
 Precision: 0.607800 
 Recall: 0.539819
 IoU: 0.400360

We have finished training iteration 50
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_27_.pth
per-ex loss: 0.572751  [    1/   88]
per-ex loss: 0.715975  [    2/   88]
per-ex loss: 0.740091  [    3/   88]
per-ex loss: 0.780640  [    4/   88]
per-ex loss: 0.517879  [    5/   88]
per-ex loss: 0.695428  [    6/   88]
per-ex loss: 0.544246  [    7/   88]
per-ex loss: 0.578766  [    8/   88]
per-ex loss: 0.564299  [    9/   88]
per-ex loss: 0.624811  [   10/   88]
per-ex loss: 0.534730  [   11/   88]
per-ex loss: 0.716230  [   12/   88]
per-ex loss: 0.762716  [   13/   88]
per-ex loss: 0.744707  [   14/   88]
per-ex loss: 0.522273  [   15/   88]
per-ex loss: 0.781000  [   16/   88]
per-ex loss: 0.797917  [   17/   88]
per-ex loss: 0.711591  [   18/   88]
per-ex loss: 0.732114  [   19/   88]
per-ex loss: 0.550284  [   20/   88]
per-ex loss: 0.795105  [   21/   88]
per-ex loss: 0.766642  [   22/   88]
per-ex loss: 0.605626  [   23/   88]
per-ex loss: 0.686274  [   24/   88]
per-ex loss: 0.539952  [   25/   88]
per-ex loss: 0.507070  [   26/   88]
per-ex loss: 0.572387  [   27/   88]
per-ex loss: 0.525338  [   28/   88]
per-ex loss: 0.807680  [   29/   88]
per-ex loss: 0.546950  [   30/   88]
per-ex loss: 0.663987  [   31/   88]
per-ex loss: 0.663190  [   32/   88]
per-ex loss: 0.553874  [   33/   88]
per-ex loss: 0.512963  [   34/   88]
per-ex loss: 0.826731  [   35/   88]
per-ex loss: 0.727599  [   36/   88]
per-ex loss: 0.619806  [   37/   88]
per-ex loss: 0.631825  [   38/   88]
per-ex loss: 0.568732  [   39/   88]
per-ex loss: 0.702896  [   40/   88]
per-ex loss: 0.726895  [   41/   88]
per-ex loss: 0.687352  [   42/   88]
per-ex loss: 0.548621  [   43/   88]
per-ex loss: 0.560020  [   44/   88]
per-ex loss: 0.527954  [   45/   88]
per-ex loss: 0.733787  [   46/   88]
per-ex loss: 0.762349  [   47/   88]
per-ex loss: 0.510615  [   48/   88]
per-ex loss: 0.755762  [   49/   88]
per-ex loss: 0.686334  [   50/   88]
per-ex loss: 0.563610  [   51/   88]
per-ex loss: 0.576365  [   52/   88]
per-ex loss: 0.745190  [   53/   88]
per-ex loss: 0.598902  [   54/   88]
per-ex loss: 0.555707  [   55/   88]
per-ex loss: 0.807951  [   56/   88]
per-ex loss: 0.762998  [   57/   88]
per-ex loss: 0.534711  [   58/   88]
per-ex loss: 0.563851  [   59/   88]
per-ex loss: 0.736078  [   60/   88]
per-ex loss: 0.722610  [   61/   88]
per-ex loss: 0.532423  [   62/   88]
per-ex loss: 0.606678  [   63/   88]
per-ex loss: 0.446182  [   64/   88]
per-ex loss: 0.613399  [   65/   88]
per-ex loss: 0.576533  [   66/   88]
per-ex loss: 0.792483  [   67/   88]
per-ex loss: 0.604084  [   68/   88]
per-ex loss: 0.575293  [   69/   88]
per-ex loss: 0.490577  [   70/   88]
per-ex loss: 0.692698  [   71/   88]
per-ex loss: 0.609903  [   72/   88]
per-ex loss: 0.663599  [   73/   88]
per-ex loss: 0.683218  [   74/   88]
per-ex loss: 0.509350  [   75/   88]
per-ex loss: 0.640237  [   76/   88]
per-ex loss: 0.700155  [   77/   88]
per-ex loss: 0.718626  [   78/   88]
per-ex loss: 0.706802  [   79/   88]
per-ex loss: 0.526870  [   80/   88]
per-ex loss: 0.767539  [   81/   88]
per-ex loss: 0.585257  [   82/   88]
per-ex loss: 0.570413  [   83/   88]
per-ex loss: 0.479570  [   84/   88]
per-ex loss: 0.623255  [   85/   88]
per-ex loss: 0.582460  [   86/   88]
per-ex loss: 0.566706  [   87/   88]
per-ex loss: 0.751403  [   88/   88]
Train Error: Avg loss: 0.63973242
validation Error: 
 Avg loss: 0.70601441 
 F1: 0.485879 
 Precision: 0.526910 
 Recall: 0.450777
 IoU: 0.320899

test Error: 
 Avg loss: 0.64312070 
 F1: 0.567118 
 Precision: 0.617091 
 Recall: 0.524633
 IoU: 0.395789

We have finished training iteration 51
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_49_.pth
per-ex loss: 0.573485  [    1/   88]
per-ex loss: 0.729618  [    2/   88]
per-ex loss: 0.541564  [    3/   88]
per-ex loss: 0.698209  [    4/   88]
per-ex loss: 0.678491  [    5/   88]
per-ex loss: 0.583730  [    6/   88]
per-ex loss: 0.533162  [    7/   88]
per-ex loss: 0.560581  [    8/   88]
per-ex loss: 0.748731  [    9/   88]
per-ex loss: 0.504559  [   10/   88]
per-ex loss: 0.557208  [   11/   88]
per-ex loss: 0.564453  [   12/   88]
per-ex loss: 0.742308  [   13/   88]
per-ex loss: 0.522606  [   14/   88]
per-ex loss: 0.681174  [   15/   88]
per-ex loss: 0.553479  [   16/   88]
per-ex loss: 0.776313  [   17/   88]
per-ex loss: 0.626416  [   18/   88]
per-ex loss: 0.540590  [   19/   88]
per-ex loss: 0.714149  [   20/   88]
per-ex loss: 0.442354  [   21/   88]
per-ex loss: 0.556581  [   22/   88]
per-ex loss: 0.710378  [   23/   88]
per-ex loss: 0.744570  [   24/   88]
per-ex loss: 0.744084  [   25/   88]
per-ex loss: 0.521099  [   26/   88]
per-ex loss: 0.540872  [   27/   88]
per-ex loss: 0.511719  [   28/   88]
per-ex loss: 0.561707  [   29/   88]
per-ex loss: 0.807173  [   30/   88]
per-ex loss: 0.542702  [   31/   88]
per-ex loss: 0.591272  [   32/   88]
per-ex loss: 0.492247  [   33/   88]
per-ex loss: 0.598360  [   34/   88]
per-ex loss: 0.622075  [   35/   88]
per-ex loss: 0.550753  [   36/   88]
per-ex loss: 0.765770  [   37/   88]
per-ex loss: 0.834791  [   38/   88]
per-ex loss: 0.597898  [   39/   88]
per-ex loss: 0.673848  [   40/   88]
per-ex loss: 0.566988  [   41/   88]
per-ex loss: 0.496107  [   42/   88]
per-ex loss: 0.526160  [   43/   88]
per-ex loss: 0.577465  [   44/   88]
per-ex loss: 0.570468  [   45/   88]
per-ex loss: 0.730280  [   46/   88]
per-ex loss: 0.626646  [   47/   88]
per-ex loss: 0.763135  [   48/   88]
per-ex loss: 0.518757  [   49/   88]
per-ex loss: 0.566228  [   50/   88]
per-ex loss: 0.600681  [   51/   88]
per-ex loss: 0.570597  [   52/   88]
per-ex loss: 0.564812  [   53/   88]
per-ex loss: 0.713141  [   54/   88]
per-ex loss: 0.699849  [   55/   88]
per-ex loss: 0.790349  [   56/   88]
per-ex loss: 0.681625  [   57/   88]
per-ex loss: 0.814694  [   58/   88]
per-ex loss: 0.713673  [   59/   88]
per-ex loss: 0.753373  [   60/   88]
per-ex loss: 0.553076  [   61/   88]
per-ex loss: 0.668591  [   62/   88]
per-ex loss: 0.755139  [   63/   88]
per-ex loss: 0.762008  [   64/   88]
per-ex loss: 0.697327  [   65/   88]
per-ex loss: 0.689975  [   66/   88]
per-ex loss: 0.786201  [   67/   88]
per-ex loss: 0.561419  [   68/   88]
per-ex loss: 0.535393  [   69/   88]
per-ex loss: 0.789032  [   70/   88]
per-ex loss: 0.644966  [   71/   88]
per-ex loss: 0.765639  [   72/   88]
per-ex loss: 0.777419  [   73/   88]
per-ex loss: 0.521419  [   74/   88]
per-ex loss: 0.783933  [   75/   88]
per-ex loss: 0.611829  [   76/   88]
per-ex loss: 0.577344  [   77/   88]
per-ex loss: 0.809770  [   78/   88]
per-ex loss: 0.549436  [   79/   88]
per-ex loss: 0.677467  [   80/   88]
per-ex loss: 0.602444  [   81/   88]
per-ex loss: 0.811775  [   82/   88]
per-ex loss: 0.678226  [   83/   88]
per-ex loss: 0.582024  [   84/   88]
per-ex loss: 0.728590  [   85/   88]
per-ex loss: 0.514476  [   86/   88]
per-ex loss: 0.749263  [   87/   88]
per-ex loss: 0.693881  [   88/   88]
Train Error: Avg loss: 0.64277468
validation Error: 
 Avg loss: 0.70663836 
 F1: 0.473990 
 Precision: 0.511608 
 Recall: 0.441526
 IoU: 0.310608

test Error: 
 Avg loss: 0.65064959 
 F1: 0.555402 
 Precision: 0.606003 
 Recall: 0.512601
 IoU: 0.384468

We have finished training iteration 52
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_43_.pth
per-ex loss: 0.527612  [    1/   88]
per-ex loss: 0.787472  [    2/   88]
per-ex loss: 0.568619  [    3/   88]
per-ex loss: 0.736875  [    4/   88]
per-ex loss: 0.543929  [    5/   88]
per-ex loss: 0.534883  [    6/   88]
per-ex loss: 0.641390  [    7/   88]
per-ex loss: 0.791752  [    8/   88]
per-ex loss: 0.732705  [    9/   88]
per-ex loss: 0.539553  [   10/   88]
per-ex loss: 0.522546  [   11/   88]
per-ex loss: 0.720822  [   12/   88]
per-ex loss: 0.421234  [   13/   88]
per-ex loss: 0.593641  [   14/   88]
per-ex loss: 0.692127  [   15/   88]
per-ex loss: 0.696537  [   16/   88]
per-ex loss: 0.716420  [   17/   88]
per-ex loss: 0.828077  [   18/   88]
per-ex loss: 0.766580  [   19/   88]
per-ex loss: 0.551348  [   20/   88]
per-ex loss: 0.706995  [   21/   88]
per-ex loss: 0.536752  [   22/   88]
per-ex loss: 0.609866  [   23/   88]
per-ex loss: 0.756299  [   24/   88]
per-ex loss: 0.602236  [   25/   88]
per-ex loss: 0.612620  [   26/   88]
per-ex loss: 0.581333  [   27/   88]
per-ex loss: 0.702769  [   28/   88]
per-ex loss: 0.709210  [   29/   88]
per-ex loss: 0.570769  [   30/   88]
per-ex loss: 0.565164  [   31/   88]
per-ex loss: 0.805625  [   32/   88]
per-ex loss: 0.581183  [   33/   88]
per-ex loss: 0.540385  [   34/   88]
per-ex loss: 0.532685  [   35/   88]
per-ex loss: 0.541869  [   36/   88]
per-ex loss: 0.579563  [   37/   88]
per-ex loss: 0.509376  [   38/   88]
per-ex loss: 0.504232  [   39/   88]
per-ex loss: 0.681488  [   40/   88]
per-ex loss: 0.563068  [   41/   88]
per-ex loss: 0.804625  [   42/   88]
per-ex loss: 0.555856  [   43/   88]
per-ex loss: 0.578268  [   44/   88]
per-ex loss: 0.741680  [   45/   88]
per-ex loss: 0.650697  [   46/   88]
per-ex loss: 0.722349  [   47/   88]
per-ex loss: 0.530348  [   48/   88]
per-ex loss: 0.653796  [   49/   88]
per-ex loss: 0.555893  [   50/   88]
per-ex loss: 0.778316  [   51/   88]
per-ex loss: 0.775130  [   52/   88]
per-ex loss: 0.594926  [   53/   88]
per-ex loss: 0.669451  [   54/   88]
per-ex loss: 0.675407  [   55/   88]
per-ex loss: 0.758482  [   56/   88]
per-ex loss: 0.576090  [   57/   88]
per-ex loss: 0.568551  [   58/   88]
per-ex loss: 0.754102  [   59/   88]
per-ex loss: 0.582018  [   60/   88]
per-ex loss: 0.706745  [   61/   88]
per-ex loss: 0.613130  [   62/   88]
per-ex loss: 0.738881  [   63/   88]
per-ex loss: 0.697822  [   64/   88]
per-ex loss: 0.618928  [   65/   88]
per-ex loss: 0.572787  [   66/   88]
per-ex loss: 0.591545  [   67/   88]
per-ex loss: 0.578651  [   68/   88]
per-ex loss: 0.541650  [   69/   88]
per-ex loss: 0.760786  [   70/   88]
per-ex loss: 0.625931  [   71/   88]
per-ex loss: 0.715576  [   72/   88]
per-ex loss: 0.521236  [   73/   88]
per-ex loss: 0.614523  [   74/   88]
per-ex loss: 0.773814  [   75/   88]
per-ex loss: 0.815125  [   76/   88]
per-ex loss: 0.709899  [   77/   88]
per-ex loss: 0.576431  [   78/   88]
per-ex loss: 0.567561  [   79/   88]
per-ex loss: 0.496009  [   80/   88]
per-ex loss: 0.705070  [   81/   88]
per-ex loss: 0.758147  [   82/   88]
per-ex loss: 0.774685  [   83/   88]
per-ex loss: 0.462031  [   84/   88]
per-ex loss: 0.714015  [   85/   88]
per-ex loss: 0.644214  [   86/   88]
per-ex loss: 0.596930  [   87/   88]
per-ex loss: 0.794177  [   88/   88]
Train Error: Avg loss: 0.64227154
validation Error: 
 Avg loss: 0.71265667 
 F1: 0.471354 
 Precision: 0.520107 
 Recall: 0.430958
 IoU: 0.308348

test Error: 
 Avg loss: 0.65818394 
 F1: 0.546622 
 Precision: 0.619495 
 Recall: 0.489089
 IoU: 0.376105

We have finished training iteration 53
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_51_.pth
per-ex loss: 0.695970  [    1/   88]
per-ex loss: 0.716873  [    2/   88]
per-ex loss: 0.699806  [    3/   88]
per-ex loss: 0.720557  [    4/   88]
per-ex loss: 0.688092  [    5/   88]
per-ex loss: 0.613557  [    6/   88]
per-ex loss: 0.778057  [    7/   88]
per-ex loss: 0.551098  [    8/   88]
per-ex loss: 0.730049  [    9/   88]
per-ex loss: 0.562471  [   10/   88]
per-ex loss: 0.623596  [   11/   88]
per-ex loss: 0.792063  [   12/   88]
per-ex loss: 0.514756  [   13/   88]
per-ex loss: 0.512650  [   14/   88]
per-ex loss: 0.530907  [   15/   88]
per-ex loss: 0.817778  [   16/   88]
per-ex loss: 0.758422  [   17/   88]
per-ex loss: 0.490681  [   18/   88]
per-ex loss: 0.742526  [   19/   88]
per-ex loss: 0.771722  [   20/   88]
per-ex loss: 0.716188  [   21/   88]
per-ex loss: 0.678032  [   22/   88]
per-ex loss: 0.526354  [   23/   88]
per-ex loss: 0.555079  [   24/   88]
per-ex loss: 0.558748  [   25/   88]
per-ex loss: 0.584181  [   26/   88]
per-ex loss: 0.577822  [   27/   88]
per-ex loss: 0.703719  [   28/   88]
per-ex loss: 0.733207  [   29/   88]
per-ex loss: 0.583068  [   30/   88]
per-ex loss: 0.603042  [   31/   88]
per-ex loss: 0.560027  [   32/   88]
per-ex loss: 0.521621  [   33/   88]
per-ex loss: 0.650248  [   34/   88]
per-ex loss: 0.466257  [   35/   88]
per-ex loss: 0.593182  [   36/   88]
per-ex loss: 0.645553  [   37/   88]
per-ex loss: 0.623626  [   38/   88]
per-ex loss: 0.749750  [   39/   88]
per-ex loss: 0.558053  [   40/   88]
per-ex loss: 0.677827  [   41/   88]
per-ex loss: 0.615403  [   42/   88]
per-ex loss: 0.785970  [   43/   88]
per-ex loss: 0.566253  [   44/   88]
per-ex loss: 0.645329  [   45/   88]
per-ex loss: 0.734929  [   46/   88]
per-ex loss: 0.756762  [   47/   88]
per-ex loss: 0.589790  [   48/   88]
per-ex loss: 0.671291  [   49/   88]
per-ex loss: 0.782377  [   50/   88]
per-ex loss: 0.813474  [   51/   88]
per-ex loss: 0.620666  [   52/   88]
per-ex loss: 0.638822  [   53/   88]
per-ex loss: 0.588021  [   54/   88]
per-ex loss: 0.519748  [   55/   88]
per-ex loss: 0.607322  [   56/   88]
per-ex loss: 0.805804  [   57/   88]
per-ex loss: 0.689845  [   58/   88]
per-ex loss: 0.583936  [   59/   88]
per-ex loss: 0.556079  [   60/   88]
per-ex loss: 0.763903  [   61/   88]
per-ex loss: 0.789812  [   62/   88]
per-ex loss: 0.514039  [   63/   88]
per-ex loss: 0.780583  [   64/   88]
per-ex loss: 0.563300  [   65/   88]
per-ex loss: 0.756493  [   66/   88]
per-ex loss: 0.689988  [   67/   88]
per-ex loss: 0.535885  [   68/   88]
per-ex loss: 0.618051  [   69/   88]
per-ex loss: 0.789569  [   70/   88]
per-ex loss: 0.555324  [   71/   88]
per-ex loss: 0.721597  [   72/   88]
per-ex loss: 0.463426  [   73/   88]
per-ex loss: 0.703274  [   74/   88]
per-ex loss: 0.531295  [   75/   88]
per-ex loss: 0.550567  [   76/   88]
per-ex loss: 0.539962  [   77/   88]
per-ex loss: 0.540702  [   78/   88]
per-ex loss: 0.725562  [   79/   88]
per-ex loss: 0.580689  [   80/   88]
per-ex loss: 0.716733  [   81/   88]
per-ex loss: 0.717459  [   82/   88]
per-ex loss: 0.530933  [   83/   88]
per-ex loss: 0.702071  [   84/   88]
per-ex loss: 0.542289  [   85/   88]
per-ex loss: 0.572877  [   86/   88]
per-ex loss: 0.582557  [   87/   88]
per-ex loss: 0.617357  [   88/   88]
Train Error: Avg loss: 0.64142422
validation Error: 
 Avg loss: 0.68993522 
 F1: 0.498335 
 Precision: 0.615265 
 Recall: 0.418752
 IoU: 0.331855

test Error: 
 Avg loss: 0.64966221 
 F1: 0.560326 
 Precision: 0.639761 
 Recall: 0.498439
 IoU: 0.389203

We have finished training iteration 54
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_52_.pth
per-ex loss: 0.568252  [    1/   88]
per-ex loss: 0.552814  [    2/   88]
per-ex loss: 0.676242  [    3/   88]
per-ex loss: 0.613284  [    4/   88]
per-ex loss: 0.602218  [    5/   88]
per-ex loss: 0.505730  [    6/   88]
per-ex loss: 0.715435  [    7/   88]
per-ex loss: 0.807537  [    8/   88]
per-ex loss: 0.684237  [    9/   88]
per-ex loss: 0.659143  [   10/   88]
per-ex loss: 0.597755  [   11/   88]
per-ex loss: 0.581586  [   12/   88]
per-ex loss: 0.793621  [   13/   88]
per-ex loss: 0.541050  [   14/   88]
per-ex loss: 0.532817  [   15/   88]
per-ex loss: 0.635238  [   16/   88]
per-ex loss: 0.504471  [   17/   88]
per-ex loss: 0.728029  [   18/   88]
per-ex loss: 0.600813  [   19/   88]
per-ex loss: 0.713641  [   20/   88]
per-ex loss: 0.762032  [   21/   88]
per-ex loss: 0.466325  [   22/   88]
per-ex loss: 0.763958  [   23/   88]
per-ex loss: 0.533116  [   24/   88]
per-ex loss: 0.764262  [   25/   88]
per-ex loss: 0.544032  [   26/   88]
per-ex loss: 0.630632  [   27/   88]
per-ex loss: 0.730675  [   28/   88]
per-ex loss: 0.539758  [   29/   88]
per-ex loss: 0.739554  [   30/   88]
per-ex loss: 0.747481  [   31/   88]
per-ex loss: 0.568334  [   32/   88]
per-ex loss: 0.722984  [   33/   88]
per-ex loss: 0.687237  [   34/   88]
per-ex loss: 0.799768  [   35/   88]
per-ex loss: 0.662565  [   36/   88]
per-ex loss: 0.801000  [   37/   88]
per-ex loss: 0.740040  [   38/   88]
per-ex loss: 0.601381  [   39/   88]
per-ex loss: 0.554318  [   40/   88]
per-ex loss: 0.438965  [   41/   88]
per-ex loss: 0.587899  [   42/   88]
per-ex loss: 0.593658  [   43/   88]
per-ex loss: 0.564724  [   44/   88]
per-ex loss: 0.697573  [   45/   88]
per-ex loss: 0.522786  [   46/   88]
per-ex loss: 0.679132  [   47/   88]
per-ex loss: 0.556879  [   48/   88]
per-ex loss: 0.514111  [   49/   88]
per-ex loss: 0.774817  [   50/   88]
per-ex loss: 0.533114  [   51/   88]
per-ex loss: 0.553702  [   52/   88]
per-ex loss: 0.503021  [   53/   88]
per-ex loss: 0.547306  [   54/   88]
per-ex loss: 0.743129  [   55/   88]
per-ex loss: 0.559388  [   56/   88]
per-ex loss: 0.565358  [   57/   88]
per-ex loss: 0.596641  [   58/   88]
per-ex loss: 0.707528  [   59/   88]
per-ex loss: 0.700341  [   60/   88]
per-ex loss: 0.724251  [   61/   88]
per-ex loss: 0.774606  [   62/   88]
per-ex loss: 0.542654  [   63/   88]
per-ex loss: 0.804128  [   64/   88]
per-ex loss: 0.695156  [   65/   88]
per-ex loss: 0.609967  [   66/   88]
per-ex loss: 0.623363  [   67/   88]
per-ex loss: 0.574657  [   68/   88]
per-ex loss: 0.718949  [   69/   88]
per-ex loss: 0.496477  [   70/   88]
per-ex loss: 0.652835  [   71/   88]
per-ex loss: 0.598895  [   72/   88]
per-ex loss: 0.664464  [   73/   88]
per-ex loss: 0.560979  [   74/   88]
per-ex loss: 0.515977  [   75/   88]
per-ex loss: 0.744305  [   76/   88]
per-ex loss: 0.766814  [   77/   88]
per-ex loss: 0.738006  [   78/   88]
per-ex loss: 0.812145  [   79/   88]
per-ex loss: 0.703829  [   80/   88]
per-ex loss: 0.539541  [   81/   88]
per-ex loss: 0.824356  [   82/   88]
per-ex loss: 0.755553  [   83/   88]
per-ex loss: 0.783202  [   84/   88]
per-ex loss: 0.530596  [   85/   88]
per-ex loss: 0.546615  [   86/   88]
per-ex loss: 0.562828  [   87/   88]
per-ex loss: 0.559258  [   88/   88]
Train Error: Avg loss: 0.64020277
validation Error: 
 Avg loss: 0.68960842 
 F1: 0.498126 
 Precision: 0.541295 
 Recall: 0.461334
 IoU: 0.331670

test Error: 
 Avg loss: 0.63781522 
 F1: 0.570557 
 Precision: 0.604095 
 Recall: 0.540548
 IoU: 0.399147

We have finished training iteration 55
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_53_.pth
per-ex loss: 0.736998  [    1/   88]
per-ex loss: 0.757470  [    2/   88]
per-ex loss: 0.724303  [    3/   88]
per-ex loss: 0.664166  [    4/   88]
per-ex loss: 0.747856  [    5/   88]
per-ex loss: 0.739079  [    6/   88]
per-ex loss: 0.774205  [    7/   88]
per-ex loss: 0.605148  [    8/   88]
per-ex loss: 0.577786  [    9/   88]
per-ex loss: 0.772329  [   10/   88]
per-ex loss: 0.802768  [   11/   88]
per-ex loss: 0.534640  [   12/   88]
per-ex loss: 0.716738  [   13/   88]
per-ex loss: 0.792558  [   14/   88]
per-ex loss: 0.602480  [   15/   88]
per-ex loss: 0.525905  [   16/   88]
per-ex loss: 0.712540  [   17/   88]
per-ex loss: 0.796800  [   18/   88]
per-ex loss: 0.762376  [   19/   88]
per-ex loss: 0.850052  [   20/   88]
per-ex loss: 0.800317  [   21/   88]
per-ex loss: 0.585144  [   22/   88]
per-ex loss: 0.661032  [   23/   88]
per-ex loss: 0.416515  [   24/   88]
per-ex loss: 0.665796  [   25/   88]
per-ex loss: 0.546499  [   26/   88]
per-ex loss: 0.717134  [   27/   88]
per-ex loss: 0.611506  [   28/   88]
per-ex loss: 0.628804  [   29/   88]
per-ex loss: 0.591780  [   30/   88]
per-ex loss: 0.702571  [   31/   88]
per-ex loss: 0.516012  [   32/   88]
per-ex loss: 0.524104  [   33/   88]
per-ex loss: 0.696629  [   34/   88]
per-ex loss: 0.559369  [   35/   88]
per-ex loss: 0.567997  [   36/   88]
per-ex loss: 0.672964  [   37/   88]
per-ex loss: 0.509712  [   38/   88]
per-ex loss: 0.661026  [   39/   88]
per-ex loss: 0.705491  [   40/   88]
per-ex loss: 0.736908  [   41/   88]
per-ex loss: 0.620036  [   42/   88]
per-ex loss: 0.718022  [   43/   88]
per-ex loss: 0.650166  [   44/   88]
per-ex loss: 0.531348  [   45/   88]
per-ex loss: 0.608806  [   46/   88]
per-ex loss: 0.694661  [   47/   88]
per-ex loss: 0.633634  [   48/   88]
per-ex loss: 0.529905  [   49/   88]
per-ex loss: 0.569811  [   50/   88]
per-ex loss: 0.506785  [   51/   88]
per-ex loss: 0.670102  [   52/   88]
per-ex loss: 0.606827  [   53/   88]
per-ex loss: 0.505682  [   54/   88]
per-ex loss: 0.665681  [   55/   88]
per-ex loss: 0.757396  [   56/   88]
per-ex loss: 0.719245  [   57/   88]
per-ex loss: 0.545588  [   58/   88]
per-ex loss: 0.792856  [   59/   88]
per-ex loss: 0.759593  [   60/   88]
per-ex loss: 0.534439  [   61/   88]
per-ex loss: 0.528733  [   62/   88]
per-ex loss: 0.758301  [   63/   88]
per-ex loss: 0.524612  [   64/   88]
per-ex loss: 0.586872  [   65/   88]
per-ex loss: 0.646635  [   66/   88]
per-ex loss: 0.723801  [   67/   88]
per-ex loss: 0.646076  [   68/   88]
per-ex loss: 0.508138  [   69/   88]
per-ex loss: 0.544832  [   70/   88]
per-ex loss: 0.477141  [   71/   88]
per-ex loss: 0.747689  [   72/   88]
per-ex loss: 0.558867  [   73/   88]
per-ex loss: 0.586699  [   74/   88]
per-ex loss: 0.559603  [   75/   88]
per-ex loss: 0.553463  [   76/   88]
per-ex loss: 0.738840  [   77/   88]
per-ex loss: 0.574000  [   78/   88]
per-ex loss: 0.575961  [   79/   88]
per-ex loss: 0.735124  [   80/   88]
per-ex loss: 0.563828  [   81/   88]
per-ex loss: 0.476005  [   82/   88]
per-ex loss: 0.805916  [   83/   88]
per-ex loss: 0.520454  [   84/   88]
per-ex loss: 0.601534  [   85/   88]
per-ex loss: 0.585243  [   86/   88]
per-ex loss: 0.706882  [   87/   88]
per-ex loss: 0.530153  [   88/   88]
Train Error: Avg loss: 0.63931244
validation Error: 
 Avg loss: 0.70678156 
 F1: 0.474199 
 Precision: 0.498960 
 Recall: 0.451779
 IoU: 0.310787

test Error: 
 Avg loss: 0.65196787 
 F1: 0.557370 
 Precision: 0.581360 
 Recall: 0.535281
 IoU: 0.386356

We have finished training iteration 56
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_50_.pth
per-ex loss: 0.574982  [    1/   88]
per-ex loss: 0.520648  [    2/   88]
per-ex loss: 0.695905  [    3/   88]
per-ex loss: 0.593951  [    4/   88]
per-ex loss: 0.595586  [    5/   88]
per-ex loss: 0.615132  [    6/   88]
per-ex loss: 0.575005  [    7/   88]
per-ex loss: 0.720079  [    8/   88]
per-ex loss: 0.808578  [    9/   88]
per-ex loss: 0.545259  [   10/   88]
per-ex loss: 0.618238  [   11/   88]
per-ex loss: 0.557204  [   12/   88]
per-ex loss: 0.602866  [   13/   88]
per-ex loss: 0.545172  [   14/   88]
per-ex loss: 0.801242  [   15/   88]
per-ex loss: 0.596302  [   16/   88]
per-ex loss: 0.530620  [   17/   88]
per-ex loss: 0.747793  [   18/   88]
per-ex loss: 0.713267  [   19/   88]
per-ex loss: 0.489391  [   20/   88]
per-ex loss: 0.515141  [   21/   88]
per-ex loss: 0.685615  [   22/   88]
per-ex loss: 0.558922  [   23/   88]
per-ex loss: 0.551389  [   24/   88]
per-ex loss: 0.807331  [   25/   88]
per-ex loss: 0.551264  [   26/   88]
per-ex loss: 0.756495  [   27/   88]
per-ex loss: 0.747420  [   28/   88]
per-ex loss: 0.673023  [   29/   88]
per-ex loss: 0.609254  [   30/   88]
per-ex loss: 0.585237  [   31/   88]
per-ex loss: 0.535471  [   32/   88]
per-ex loss: 0.684885  [   33/   88]
per-ex loss: 0.628533  [   34/   88]
per-ex loss: 0.750507  [   35/   88]
per-ex loss: 0.509689  [   36/   88]
per-ex loss: 0.730813  [   37/   88]
per-ex loss: 0.515631  [   38/   88]
per-ex loss: 0.579379  [   39/   88]
per-ex loss: 0.622056  [   40/   88]
per-ex loss: 0.780272  [   41/   88]
per-ex loss: 0.529549  [   42/   88]
per-ex loss: 0.543010  [   43/   88]
per-ex loss: 0.805340  [   44/   88]
per-ex loss: 0.554327  [   45/   88]
per-ex loss: 0.808597  [   46/   88]
per-ex loss: 0.561525  [   47/   88]
per-ex loss: 0.716571  [   48/   88]
per-ex loss: 0.540902  [   49/   88]
per-ex loss: 0.622823  [   50/   88]
per-ex loss: 0.585848  [   51/   88]
per-ex loss: 0.584267  [   52/   88]
per-ex loss: 0.783311  [   53/   88]
per-ex loss: 0.602147  [   54/   88]
per-ex loss: 0.599914  [   55/   88]
per-ex loss: 0.697428  [   56/   88]
per-ex loss: 0.705625  [   57/   88]
per-ex loss: 0.760557  [   58/   88]
per-ex loss: 0.713835  [   59/   88]
per-ex loss: 0.603526  [   60/   88]
per-ex loss: 0.521980  [   61/   88]
per-ex loss: 0.560570  [   62/   88]
per-ex loss: 0.741352  [   63/   88]
per-ex loss: 0.702717  [   64/   88]
per-ex loss: 0.699153  [   65/   88]
per-ex loss: 0.733504  [   66/   88]
per-ex loss: 0.744777  [   67/   88]
per-ex loss: 0.477511  [   68/   88]
per-ex loss: 0.663445  [   69/   88]
per-ex loss: 0.661396  [   70/   88]
per-ex loss: 0.508534  [   71/   88]
per-ex loss: 0.800725  [   72/   88]
per-ex loss: 0.425774  [   73/   88]
per-ex loss: 0.520581  [   74/   88]
per-ex loss: 0.592866  [   75/   88]
per-ex loss: 0.772993  [   76/   88]
per-ex loss: 0.675092  [   77/   88]
per-ex loss: 0.717760  [   78/   88]
per-ex loss: 0.682162  [   79/   88]
per-ex loss: 0.520882  [   80/   88]
per-ex loss: 0.816838  [   81/   88]
per-ex loss: 0.563764  [   82/   88]
per-ex loss: 0.743947  [   83/   88]
per-ex loss: 0.517629  [   84/   88]
per-ex loss: 0.522206  [   85/   88]
per-ex loss: 0.718712  [   86/   88]
per-ex loss: 0.731804  [   87/   88]
per-ex loss: 0.789640  [   88/   88]
Train Error: Avg loss: 0.63946548
validation Error: 
 Avg loss: 0.69115251 
 F1: 0.495962 
 Precision: 0.589978 
 Recall: 0.427792
 IoU: 0.329754

test Error: 
 Avg loss: 0.65060564 
 F1: 0.558639 
 Precision: 0.634350 
 Recall: 0.499073
 IoU: 0.387577

We have finished training iteration 57
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_48_.pth
per-ex loss: 0.590402  [    1/   88]
per-ex loss: 0.511874  [    2/   88]
per-ex loss: 0.709530  [    3/   88]
per-ex loss: 0.797071  [    4/   88]
per-ex loss: 0.535958  [    5/   88]
per-ex loss: 0.496211  [    6/   88]
per-ex loss: 0.610526  [    7/   88]
per-ex loss: 0.760195  [    8/   88]
per-ex loss: 0.741916  [    9/   88]
per-ex loss: 0.765899  [   10/   88]
per-ex loss: 0.783235  [   11/   88]
per-ex loss: 0.721030  [   12/   88]
per-ex loss: 0.523542  [   13/   88]
per-ex loss: 0.552913  [   14/   88]
per-ex loss: 0.515318  [   15/   88]
per-ex loss: 0.574181  [   16/   88]
per-ex loss: 0.676880  [   17/   88]
per-ex loss: 0.547126  [   18/   88]
per-ex loss: 0.567877  [   19/   88]
per-ex loss: 0.505802  [   20/   88]
per-ex loss: 0.546120  [   21/   88]
per-ex loss: 0.470887  [   22/   88]
per-ex loss: 0.804089  [   23/   88]
per-ex loss: 0.555745  [   24/   88]
per-ex loss: 0.613770  [   25/   88]
per-ex loss: 0.495658  [   26/   88]
per-ex loss: 0.567182  [   27/   88]
per-ex loss: 0.486425  [   28/   88]
per-ex loss: 0.502123  [   29/   88]
per-ex loss: 0.436886  [   30/   88]
per-ex loss: 0.651412  [   31/   88]
per-ex loss: 0.713943  [   32/   88]
per-ex loss: 0.741513  [   33/   88]
per-ex loss: 0.758689  [   34/   88]
per-ex loss: 0.605777  [   35/   88]
per-ex loss: 0.703205  [   36/   88]
per-ex loss: 0.668007  [   37/   88]
per-ex loss: 0.572122  [   38/   88]
per-ex loss: 0.620159  [   39/   88]
per-ex loss: 0.526933  [   40/   88]
per-ex loss: 0.867296  [   41/   88]
per-ex loss: 0.699150  [   42/   88]
per-ex loss: 0.768655  [   43/   88]
per-ex loss: 0.724723  [   44/   88]
per-ex loss: 0.819752  [   45/   88]
per-ex loss: 0.539359  [   46/   88]
per-ex loss: 0.720755  [   47/   88]
per-ex loss: 0.726452  [   48/   88]
per-ex loss: 0.551756  [   49/   88]
per-ex loss: 0.743590  [   50/   88]
per-ex loss: 0.611484  [   51/   88]
per-ex loss: 0.564499  [   52/   88]
per-ex loss: 0.753165  [   53/   88]
per-ex loss: 0.531317  [   54/   88]
per-ex loss: 0.539888  [   55/   88]
per-ex loss: 0.549167  [   56/   88]
per-ex loss: 0.562132  [   57/   88]
per-ex loss: 0.843780  [   58/   88]
per-ex loss: 0.636593  [   59/   88]
per-ex loss: 0.710654  [   60/   88]
per-ex loss: 0.685709  [   61/   88]
per-ex loss: 0.804327  [   62/   88]
per-ex loss: 0.498577  [   63/   88]
per-ex loss: 0.704323  [   64/   88]
per-ex loss: 0.618263  [   65/   88]
per-ex loss: 0.737343  [   66/   88]
per-ex loss: 0.749753  [   67/   88]
per-ex loss: 0.530898  [   68/   88]
per-ex loss: 0.551366  [   69/   88]
per-ex loss: 0.677452  [   70/   88]
per-ex loss: 0.633921  [   71/   88]
per-ex loss: 0.684392  [   72/   88]
per-ex loss: 0.538100  [   73/   88]
per-ex loss: 0.632095  [   74/   88]
per-ex loss: 0.518883  [   75/   88]
per-ex loss: 0.587331  [   76/   88]
per-ex loss: 0.573555  [   77/   88]
per-ex loss: 0.719771  [   78/   88]
per-ex loss: 0.593823  [   79/   88]
per-ex loss: 0.617520  [   80/   88]
per-ex loss: 0.775958  [   81/   88]
per-ex loss: 0.520245  [   82/   88]
per-ex loss: 0.718383  [   83/   88]
per-ex loss: 0.581146  [   84/   88]
per-ex loss: 0.791779  [   85/   88]
per-ex loss: 0.708725  [   86/   88]
per-ex loss: 0.559308  [   87/   88]
per-ex loss: 0.749515  [   88/   88]
Train Error: Avg loss: 0.63698591
validation Error: 
 Avg loss: 0.71051049 
 F1: 0.468539 
 Precision: 0.509467 
 Recall: 0.433698
 IoU: 0.305942

test Error: 
 Avg loss: 0.65764542 
 F1: 0.547838 
 Precision: 0.602364 
 Recall: 0.502364
 IoU: 0.377257

We have finished training iteration 58
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_56_.pth
per-ex loss: 0.688316  [    1/   88]
per-ex loss: 0.624997  [    2/   88]
per-ex loss: 0.781209  [    3/   88]
per-ex loss: 0.553148  [    4/   88]
per-ex loss: 0.499870  [    5/   88]
per-ex loss: 0.749826  [    6/   88]
per-ex loss: 0.651504  [    7/   88]
per-ex loss: 0.678201  [    8/   88]
per-ex loss: 0.728209  [    9/   88]
per-ex loss: 0.755898  [   10/   88]
per-ex loss: 0.746738  [   11/   88]
per-ex loss: 0.528234  [   12/   88]
per-ex loss: 0.517976  [   13/   88]
per-ex loss: 0.482874  [   14/   88]
per-ex loss: 0.527825  [   15/   88]
per-ex loss: 0.602532  [   16/   88]
per-ex loss: 0.595679  [   17/   88]
per-ex loss: 0.623608  [   18/   88]
per-ex loss: 0.726374  [   19/   88]
per-ex loss: 0.741994  [   20/   88]
per-ex loss: 0.717483  [   21/   88]
per-ex loss: 0.566668  [   22/   88]
per-ex loss: 0.516913  [   23/   88]
per-ex loss: 0.635859  [   24/   88]
per-ex loss: 0.726059  [   25/   88]
per-ex loss: 0.790285  [   26/   88]
per-ex loss: 0.538142  [   27/   88]
per-ex loss: 0.696279  [   28/   88]
per-ex loss: 0.759794  [   29/   88]
per-ex loss: 0.534588  [   30/   88]
per-ex loss: 0.593066  [   31/   88]
per-ex loss: 0.558274  [   32/   88]
per-ex loss: 0.682373  [   33/   88]
per-ex loss: 0.803128  [   34/   88]
per-ex loss: 0.753799  [   35/   88]
per-ex loss: 0.619194  [   36/   88]
per-ex loss: 0.546861  [   37/   88]
per-ex loss: 0.580657  [   38/   88]
per-ex loss: 0.504526  [   39/   88]
per-ex loss: 0.661763  [   40/   88]
per-ex loss: 0.488351  [   41/   88]
per-ex loss: 0.565870  [   42/   88]
per-ex loss: 0.724789  [   43/   88]
per-ex loss: 0.619395  [   44/   88]
per-ex loss: 0.777073  [   45/   88]
per-ex loss: 0.607855  [   46/   88]
per-ex loss: 0.794879  [   47/   88]
per-ex loss: 0.803948  [   48/   88]
per-ex loss: 0.563137  [   49/   88]
per-ex loss: 0.666472  [   50/   88]
per-ex loss: 0.432290  [   51/   88]
per-ex loss: 0.797404  [   52/   88]
per-ex loss: 0.716598  [   53/   88]
per-ex loss: 0.696902  [   54/   88]
per-ex loss: 0.702186  [   55/   88]
per-ex loss: 0.555052  [   56/   88]
per-ex loss: 0.584882  [   57/   88]
per-ex loss: 0.806206  [   58/   88]
per-ex loss: 0.521795  [   59/   88]
per-ex loss: 0.698802  [   60/   88]
per-ex loss: 0.554649  [   61/   88]
per-ex loss: 0.758287  [   62/   88]
per-ex loss: 0.520160  [   63/   88]
per-ex loss: 0.538030  [   64/   88]
per-ex loss: 0.599960  [   65/   88]
per-ex loss: 0.718491  [   66/   88]
per-ex loss: 0.532201  [   67/   88]
per-ex loss: 0.691721  [   68/   88]
per-ex loss: 0.552134  [   69/   88]
per-ex loss: 0.677471  [   70/   88]
per-ex loss: 0.791554  [   71/   88]
per-ex loss: 0.540873  [   72/   88]
per-ex loss: 0.549501  [   73/   88]
per-ex loss: 0.706019  [   74/   88]
per-ex loss: 0.689378  [   75/   88]
per-ex loss: 0.750486  [   76/   88]
per-ex loss: 0.618228  [   77/   88]
per-ex loss: 0.527415  [   78/   88]
per-ex loss: 0.747908  [   79/   88]
per-ex loss: 0.530496  [   80/   88]
per-ex loss: 0.555332  [   81/   88]
per-ex loss: 0.806550  [   82/   88]
per-ex loss: 0.692157  [   83/   88]
per-ex loss: 0.539600  [   84/   88]
per-ex loss: 0.584285  [   85/   88]
per-ex loss: 0.548784  [   86/   88]
per-ex loss: 0.596047  [   87/   88]
per-ex loss: 0.624627  [   88/   88]
Train Error: Avg loss: 0.63928356
validation Error: 
 Avg loss: 0.68940736 
 F1: 0.498669 
 Precision: 0.541984 
 Recall: 0.461764
 IoU: 0.332151

test Error: 
 Avg loss: 0.64326668 
 F1: 0.567076 
 Precision: 0.598878 
 Recall: 0.538480
 IoU: 0.395747

We have finished training iteration 59
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_57_.pth
per-ex loss: 0.737131  [    1/   88]
per-ex loss: 0.686692  [    2/   88]
per-ex loss: 0.702492  [    3/   88]
per-ex loss: 0.714690  [    4/   88]
per-ex loss: 0.523940  [    5/   88]
per-ex loss: 0.519421  [    6/   88]
per-ex loss: 0.804784  [    7/   88]
per-ex loss: 0.593554  [    8/   88]
per-ex loss: 0.648144  [    9/   88]
per-ex loss: 0.688391  [   10/   88]
per-ex loss: 0.537278  [   11/   88]
per-ex loss: 0.596867  [   12/   88]
per-ex loss: 0.587491  [   13/   88]
per-ex loss: 0.595651  [   14/   88]
per-ex loss: 0.510422  [   15/   88]
per-ex loss: 0.665736  [   16/   88]
per-ex loss: 0.755047  [   17/   88]
per-ex loss: 0.577438  [   18/   88]
per-ex loss: 0.552306  [   19/   88]
per-ex loss: 0.513564  [   20/   88]
per-ex loss: 0.687530  [   21/   88]
per-ex loss: 0.708621  [   22/   88]
per-ex loss: 0.567325  [   23/   88]
per-ex loss: 0.729429  [   24/   88]
per-ex loss: 0.529497  [   25/   88]
per-ex loss: 0.575647  [   26/   88]
per-ex loss: 0.728840  [   27/   88]
per-ex loss: 0.505728  [   28/   88]
per-ex loss: 0.540753  [   29/   88]
per-ex loss: 0.530191  [   30/   88]
per-ex loss: 0.732415  [   31/   88]
per-ex loss: 0.725275  [   32/   88]
per-ex loss: 0.793717  [   33/   88]
per-ex loss: 0.581562  [   34/   88]
per-ex loss: 0.575777  [   35/   88]
per-ex loss: 0.743306  [   36/   88]
per-ex loss: 0.619759  [   37/   88]
per-ex loss: 0.512741  [   38/   88]
per-ex loss: 0.794557  [   39/   88]
per-ex loss: 0.510291  [   40/   88]
per-ex loss: 0.629597  [   41/   88]
per-ex loss: 0.526648  [   42/   88]
per-ex loss: 0.750554  [   43/   88]
per-ex loss: 0.492574  [   44/   88]
per-ex loss: 0.570767  [   45/   88]
per-ex loss: 0.690436  [   46/   88]
per-ex loss: 0.756563  [   47/   88]
per-ex loss: 0.596878  [   48/   88]
per-ex loss: 0.724384  [   49/   88]
per-ex loss: 0.747690  [   50/   88]
per-ex loss: 0.626765  [   51/   88]
per-ex loss: 0.520873  [   52/   88]
per-ex loss: 0.763818  [   53/   88]
per-ex loss: 0.696035  [   54/   88]
per-ex loss: 0.552718  [   55/   88]
per-ex loss: 0.680900  [   56/   88]
per-ex loss: 0.544222  [   57/   88]
per-ex loss: 0.784366  [   58/   88]
per-ex loss: 0.750320  [   59/   88]
per-ex loss: 0.583819  [   60/   88]
per-ex loss: 0.835166  [   61/   88]
per-ex loss: 0.594761  [   62/   88]
per-ex loss: 0.785183  [   63/   88]
per-ex loss: 0.804389  [   64/   88]
per-ex loss: 0.682811  [   65/   88]
per-ex loss: 0.706305  [   66/   88]
per-ex loss: 0.764735  [   67/   88]
per-ex loss: 0.732985  [   68/   88]
per-ex loss: 0.497945  [   69/   88]
per-ex loss: 0.702037  [   70/   88]
per-ex loss: 0.622830  [   71/   88]
per-ex loss: 0.616185  [   72/   88]
per-ex loss: 0.547122  [   73/   88]
per-ex loss: 0.514315  [   74/   88]
per-ex loss: 0.551416  [   75/   88]
per-ex loss: 0.501559  [   76/   88]
per-ex loss: 0.568558  [   77/   88]
per-ex loss: 0.517920  [   78/   88]
per-ex loss: 0.472999  [   79/   88]
per-ex loss: 0.541693  [   80/   88]
per-ex loss: 0.539820  [   81/   88]
per-ex loss: 0.811621  [   82/   88]
per-ex loss: 0.548204  [   83/   88]
per-ex loss: 0.749164  [   84/   88]
per-ex loss: 0.579222  [   85/   88]
per-ex loss: 0.652738  [   86/   88]
per-ex loss: 0.714326  [   87/   88]
per-ex loss: 0.542980  [   88/   88]
Train Error: Avg loss: 0.63514682
validation Error: 
 Avg loss: 0.68790246 
 F1: 0.501202 
 Precision: 0.599538 
 Recall: 0.430578
 IoU: 0.334402

test Error: 
 Avg loss: 0.64195552 
 F1: 0.569167 
 Precision: 0.648243 
 Recall: 0.507286
 IoU: 0.397787

We have finished training iteration 60
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_58_.pth
per-ex loss: 0.470797  [    1/   88]
per-ex loss: 0.762043  [    2/   88]
per-ex loss: 0.557440  [    3/   88]
per-ex loss: 0.741836  [    4/   88]
per-ex loss: 0.542967  [    5/   88]
per-ex loss: 0.614385  [    6/   88]
per-ex loss: 0.564218  [    7/   88]
per-ex loss: 0.579849  [    8/   88]
per-ex loss: 0.801844  [    9/   88]
per-ex loss: 0.541975  [   10/   88]
per-ex loss: 0.513611  [   11/   88]
per-ex loss: 0.434923  [   12/   88]
per-ex loss: 0.729880  [   13/   88]
per-ex loss: 0.458162  [   14/   88]
per-ex loss: 0.719910  [   15/   88]
per-ex loss: 0.674576  [   16/   88]
per-ex loss: 0.603385  [   17/   88]
per-ex loss: 0.638673  [   18/   88]
per-ex loss: 0.563390  [   19/   88]
per-ex loss: 0.571377  [   20/   88]
per-ex loss: 0.551449  [   21/   88]
per-ex loss: 0.572173  [   22/   88]
per-ex loss: 0.591237  [   23/   88]
per-ex loss: 0.563665  [   24/   88]
per-ex loss: 0.717833  [   25/   88]
per-ex loss: 0.791717  [   26/   88]
per-ex loss: 0.786260  [   27/   88]
per-ex loss: 0.579191  [   28/   88]
per-ex loss: 0.776058  [   29/   88]
per-ex loss: 0.741011  [   30/   88]
per-ex loss: 0.566059  [   31/   88]
per-ex loss: 0.791644  [   32/   88]
per-ex loss: 0.724474  [   33/   88]
per-ex loss: 0.508519  [   34/   88]
per-ex loss: 0.611156  [   35/   88]
per-ex loss: 0.519929  [   36/   88]
per-ex loss: 0.513351  [   37/   88]
per-ex loss: 0.807326  [   38/   88]
per-ex loss: 0.716054  [   39/   88]
per-ex loss: 0.516492  [   40/   88]
per-ex loss: 0.580928  [   41/   88]
per-ex loss: 0.534612  [   42/   88]
per-ex loss: 0.579889  [   43/   88]
per-ex loss: 0.759916  [   44/   88]
per-ex loss: 0.693460  [   45/   88]
per-ex loss: 0.585383  [   46/   88]
per-ex loss: 0.698684  [   47/   88]
per-ex loss: 0.729037  [   48/   88]
per-ex loss: 0.603682  [   49/   88]
per-ex loss: 0.671240  [   50/   88]
per-ex loss: 0.497874  [   51/   88]
per-ex loss: 0.614663  [   52/   88]
per-ex loss: 0.519597  [   53/   88]
per-ex loss: 0.666979  [   54/   88]
per-ex loss: 0.630551  [   55/   88]
per-ex loss: 0.749356  [   56/   88]
per-ex loss: 0.741986  [   57/   88]
per-ex loss: 0.723352  [   58/   88]
per-ex loss: 0.699861  [   59/   88]
per-ex loss: 0.710007  [   60/   88]
per-ex loss: 0.617335  [   61/   88]
per-ex loss: 0.539581  [   62/   88]
per-ex loss: 0.729986  [   63/   88]
per-ex loss: 0.540992  [   64/   88]
per-ex loss: 0.827526  [   65/   88]
per-ex loss: 0.674559  [   66/   88]
per-ex loss: 0.547309  [   67/   88]
per-ex loss: 0.806529  [   68/   88]
per-ex loss: 0.547174  [   69/   88]
per-ex loss: 0.541462  [   70/   88]
per-ex loss: 0.518627  [   71/   88]
per-ex loss: 0.540380  [   72/   88]
per-ex loss: 0.788012  [   73/   88]
per-ex loss: 0.570820  [   74/   88]
per-ex loss: 0.630002  [   75/   88]
per-ex loss: 0.818395  [   76/   88]
per-ex loss: 0.742447  [   77/   88]
per-ex loss: 0.663997  [   78/   88]
per-ex loss: 0.615935  [   79/   88]
per-ex loss: 0.511790  [   80/   88]
per-ex loss: 0.511202  [   81/   88]
per-ex loss: 0.563718  [   82/   88]
per-ex loss: 0.833128  [   83/   88]
per-ex loss: 0.746811  [   84/   88]
per-ex loss: 0.539463  [   85/   88]
per-ex loss: 0.765097  [   86/   88]
per-ex loss: 0.749338  [   87/   88]
per-ex loss: 0.717514  [   88/   88]
Train Error: Avg loss: 0.63887529
validation Error: 
 Avg loss: 0.71140476 
 F1: 0.469521 
 Precision: 0.485129 
 Recall: 0.454887
 IoU: 0.306781

test Error: 
 Avg loss: 0.64961614 
 F1: 0.560960 
 Precision: 0.586129 
 Recall: 0.537865
 IoU: 0.389816

We have finished training iteration 61
Deleting model ./unet_att_res_j_train/saved_model_wrapper/models/UNet_54_.pth
slurmstepd: error: *** STEP 16863.0 ON aga2 CANCELLED AT 2025-01-14T16:09:16 ***
