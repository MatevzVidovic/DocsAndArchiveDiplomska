unet_att_main_just_train.py do_log: False
Log file name: log_14_13-58-44_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(sd='unet_att_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_att.yaml', yo=None, ntibp=None, ptp=None, map=None)
YAML: {'path_to_data': './Data/vein_and_sclera_data', 'batch_size': 1, 'learning_rate': 1e-05, 'num_of_dataloader_workers': 7, 'train_epoch_size_limit': 400, 'num_epochs_per_training_iteration': 1, 'cleanup_k': 3, 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': True, 'loss_fn_name': 'MCDL', 'alphas': [], 'dataset_option': 'aug_tf', 'zero_out_non_sclera': True, 'add_sclera_to_img': False, 'add_bcosfire_to_img': True, 'add_coye_to_img': True, 'model_type': 'att', 'model': '64_2_6', 'input_width': 2048, 'input_height': 1024, 'input_channels': 5, 'output_channels': 2, 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq'}
Validation phase: False
Namespace(sd='unet_att_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_att.yaml', yo=None, ntibp=None, ptp=None, map=None)
Device: cuda
dataset_aug_tf.py do_log: False
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 88
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 88
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 88
val dataloader num of batches: 27
test dataloader num of batches: 12
Created new model instance.
per-ex loss: 0.927575  [    1/   88]
per-ex loss: 0.878031  [    2/   88]
per-ex loss: 0.945686  [    3/   88]
per-ex loss: 0.914541  [    4/   88]
per-ex loss: 0.839143  [    5/   88]
per-ex loss: 0.787771  [    6/   88]
per-ex loss: 0.929306  [    7/   88]
per-ex loss: 0.937344  [    8/   88]
per-ex loss: 0.761444  [    9/   88]
per-ex loss: 0.815912  [   10/   88]
per-ex loss: 0.781798  [   11/   88]
per-ex loss: 0.899761  [   12/   88]
per-ex loss: 0.866078  [   13/   88]
per-ex loss: 0.778198  [   14/   88]
per-ex loss: 0.835042  [   15/   88]
per-ex loss: 0.801230  [   16/   88]
per-ex loss: 0.938312  [   17/   88]
per-ex loss: 0.782146  [   18/   88]
per-ex loss: 0.749314  [   19/   88]
per-ex loss: 0.805396  [   20/   88]
per-ex loss: 0.903676  [   21/   88]
per-ex loss: 0.798242  [   22/   88]
per-ex loss: 0.890445  [   23/   88]
per-ex loss: 0.870912  [   24/   88]
per-ex loss: 0.846314  [   25/   88]
per-ex loss: 0.735368  [   26/   88]
per-ex loss: 0.803831  [   27/   88]
per-ex loss: 0.887224  [   28/   88]
per-ex loss: 0.820244  [   29/   88]
per-ex loss: 0.753673  [   30/   88]
per-ex loss: 0.759373  [   31/   88]
per-ex loss: 0.898482  [   32/   88]
per-ex loss: 0.786024  [   33/   88]
per-ex loss: 0.799795  [   34/   88]
per-ex loss: 0.690892  [   35/   88]
per-ex loss: 0.893997  [   36/   88]
per-ex loss: 0.581335  [   37/   88]
per-ex loss: 0.861378  [   38/   88]
per-ex loss: 0.600685  [   39/   88]
per-ex loss: 0.553298  [   40/   88]
per-ex loss: 0.908185  [   41/   88]
per-ex loss: 0.776300  [   42/   88]
per-ex loss: 0.693389  [   43/   88]
per-ex loss: 0.758330  [   44/   88]
per-ex loss: 0.775423  [   45/   88]
per-ex loss: 0.820202  [   46/   88]
per-ex loss: 0.495507  [   47/   88]
per-ex loss: 0.538721  [   48/   88]
per-ex loss: 0.679687  [   49/   88]
per-ex loss: 0.755652  [   50/   88]
per-ex loss: 0.756273  [   51/   88]
per-ex loss: 0.706965  [   52/   88]
per-ex loss: 0.719367  [   53/   88]
per-ex loss: 0.736736  [   54/   88]
per-ex loss: 0.872916  [   55/   88]
per-ex loss: 0.759324  [   56/   88]
per-ex loss: 0.535611  [   57/   88]
per-ex loss: 0.789425  [   58/   88]
per-ex loss: 0.692629  [   59/   88]
per-ex loss: 0.582155  [   60/   88]
per-ex loss: 0.823666  [   61/   88]
per-ex loss: 0.740733  [   62/   88]
per-ex loss: 0.633394  [   63/   88]
per-ex loss: 0.781840  [   64/   88]
per-ex loss: 0.834858  [   65/   88]
per-ex loss: 0.543157  [   66/   88]
per-ex loss: 0.799691  [   67/   88]
per-ex loss: 0.560960  [   68/   88]
per-ex loss: 0.766136  [   69/   88]
per-ex loss: 0.582586  [   70/   88]
per-ex loss: 0.592962  [   71/   88]
per-ex loss: 0.672137  [   72/   88]
per-ex loss: 0.630983  [   73/   88]
per-ex loss: 0.582062  [   74/   88]
per-ex loss: 0.571767  [   75/   88]
per-ex loss: 0.456532  [   76/   88]
per-ex loss: 0.776634  [   77/   88]
per-ex loss: 0.810257  [   78/   88]
per-ex loss: 0.748914  [   79/   88]
per-ex loss: 0.807555  [   80/   88]
per-ex loss: 0.705195  [   81/   88]
per-ex loss: 0.477574  [   82/   88]
per-ex loss: 0.418835  [   83/   88]
per-ex loss: 0.773005  [   84/   88]
per-ex loss: 0.502401  [   85/   88]
per-ex loss: 0.524916  [   86/   88]
per-ex loss: 0.680654  [   87/   88]
per-ex loss: 0.599736  [   88/   88]
Train Error: Avg loss: 0.74387679
validation Error: 
 Avg loss: 0.65922757 
 F1: 0.375204 
 Precision: 0.701118 
 Recall: 0.256138
 IoU: 0.230924

test Error: 
 Avg loss: 0.60255099 
 F1: 0.477486 
 Precision: 0.677979 
 Recall: 0.368509
 IoU: 0.313617

We have finished training iteration 1
per-ex loss: 0.736194  [    1/   88]
per-ex loss: 0.764610  [    2/   88]
per-ex loss: 0.784483  [    3/   88]
per-ex loss: 0.563753  [    4/   88]
per-ex loss: 0.485857  [    5/   88]
per-ex loss: 0.780123  [    6/   88]
per-ex loss: 0.727232  [    7/   88]
per-ex loss: 0.588797  [    8/   88]
per-ex loss: 0.753389  [    9/   88]
per-ex loss: 0.564650  [   10/   88]
per-ex loss: 0.445110  [   11/   88]
per-ex loss: 0.704807  [   12/   88]
per-ex loss: 0.619775  [   13/   88]
per-ex loss: 0.570108  [   14/   88]
per-ex loss: 0.705166  [   15/   88]
per-ex loss: 0.453515  [   16/   88]
per-ex loss: 0.500874  [   17/   88]
per-ex loss: 0.796809  [   18/   88]
per-ex loss: 0.479788  [   19/   88]
per-ex loss: 0.584097  [   20/   88]
per-ex loss: 0.542793  [   21/   88]
per-ex loss: 0.624562  [   22/   88]
per-ex loss: 0.575214  [   23/   88]
per-ex loss: 0.534244  [   24/   88]
per-ex loss: 0.802351  [   25/   88]
per-ex loss: 0.419608  [   26/   88]
per-ex loss: 0.765542  [   27/   88]
per-ex loss: 0.564292  [   28/   88]
per-ex loss: 0.702612  [   29/   88]
per-ex loss: 0.649906  [   30/   88]
per-ex loss: 0.653247  [   31/   88]
per-ex loss: 0.731785  [   32/   88]
per-ex loss: 0.618266  [   33/   88]
per-ex loss: 0.541833  [   34/   88]
per-ex loss: 0.762922  [   35/   88]
per-ex loss: 0.619631  [   36/   88]
per-ex loss: 0.496298  [   37/   88]
per-ex loss: 0.601201  [   38/   88]
per-ex loss: 0.674387  [   39/   88]
per-ex loss: 0.497442  [   40/   88]
per-ex loss: 0.779471  [   41/   88]
per-ex loss: 0.520630  [   42/   88]
per-ex loss: 0.732419  [   43/   88]
per-ex loss: 0.775359  [   44/   88]
per-ex loss: 0.477279  [   45/   88]
per-ex loss: 0.546413  [   46/   88]
per-ex loss: 0.542660  [   47/   88]
per-ex loss: 0.673207  [   48/   88]
per-ex loss: 0.476515  [   49/   88]
per-ex loss: 0.685437  [   50/   88]
per-ex loss: 0.415946  [   51/   88]
per-ex loss: 0.718249  [   52/   88]
per-ex loss: 0.536141  [   53/   88]
per-ex loss: 0.694612  [   54/   88]
per-ex loss: 0.506808  [   55/   88]
per-ex loss: 0.620193  [   56/   88]
per-ex loss: 0.459112  [   57/   88]
per-ex loss: 0.628210  [   58/   88]
per-ex loss: 0.759196  [   59/   88]
per-ex loss: 0.626954  [   60/   88]
per-ex loss: 0.493474  [   61/   88]
per-ex loss: 0.755147  [   62/   88]
per-ex loss: 0.551943  [   63/   88]
per-ex loss: 0.642254  [   64/   88]
per-ex loss: 0.596191  [   65/   88]
per-ex loss: 0.680122  [   66/   88]
per-ex loss: 0.723526  [   67/   88]
per-ex loss: 0.489722  [   68/   88]
per-ex loss: 0.795812  [   69/   88]
per-ex loss: 0.665959  [   70/   88]
per-ex loss: 0.451619  [   71/   88]
per-ex loss: 0.507829  [   72/   88]
per-ex loss: 0.464206  [   73/   88]
per-ex loss: 0.619483  [   74/   88]
per-ex loss: 0.446750  [   75/   88]
per-ex loss: 0.671794  [   76/   88]
per-ex loss: 0.698500  [   77/   88]
per-ex loss: 0.740156  [   78/   88]
per-ex loss: 0.450108  [   79/   88]
per-ex loss: 0.535707  [   80/   88]
per-ex loss: 0.495378  [   81/   88]
per-ex loss: 0.686932  [   82/   88]
per-ex loss: 0.784142  [   83/   88]
per-ex loss: 0.461534  [   84/   88]
per-ex loss: 0.439403  [   85/   88]
per-ex loss: 0.608348  [   86/   88]
per-ex loss: 0.687378  [   87/   88]
per-ex loss: 0.512382  [   88/   88]
Train Error: Avg loss: 0.61156688
validation Error: 
 Avg loss: 0.60342927 
 F1: 0.441788 
 Precision: 0.572550 
 Recall: 0.359650
 IoU: 0.283522

test Error: 
 Avg loss: 0.55510382 
 F1: 0.523798 
 Precision: 0.575805 
 Recall: 0.480408
 IoU: 0.354828

We have finished training iteration 2
per-ex loss: 0.461571  [    1/   88]
per-ex loss: 0.710611  [    2/   88]
per-ex loss: 0.419169  [    3/   88]
per-ex loss: 0.767860  [    4/   88]
per-ex loss: 0.563700  [    5/   88]
per-ex loss: 0.628415  [    6/   88]
per-ex loss: 0.637087  [    7/   88]
per-ex loss: 0.544099  [    8/   88]
per-ex loss: 0.443151  [    9/   88]
per-ex loss: 0.486961  [   10/   88]
per-ex loss: 0.608714  [   11/   88]
per-ex loss: 0.452027  [   12/   88]
per-ex loss: 0.641845  [   13/   88]
per-ex loss: 0.651444  [   14/   88]
per-ex loss: 0.375942  [   15/   88]
per-ex loss: 0.786065  [   16/   88]
per-ex loss: 0.426978  [   17/   88]
per-ex loss: 0.739572  [   18/   88]
per-ex loss: 0.628062  [   19/   88]
per-ex loss: 0.503172  [   20/   88]
per-ex loss: 0.671221  [   21/   88]
per-ex loss: 0.633422  [   22/   88]
per-ex loss: 0.507244  [   23/   88]
per-ex loss: 0.780828  [   24/   88]
per-ex loss: 0.528570  [   25/   88]
per-ex loss: 0.432386  [   26/   88]
per-ex loss: 0.430255  [   27/   88]
per-ex loss: 0.457384  [   28/   88]
per-ex loss: 0.700797  [   29/   88]
per-ex loss: 0.540964  [   30/   88]
per-ex loss: 0.677953  [   31/   88]
per-ex loss: 0.397645  [   32/   88]
per-ex loss: 0.467003  [   33/   88]
per-ex loss: 0.545090  [   34/   88]
per-ex loss: 0.502187  [   35/   88]
per-ex loss: 0.653056  [   36/   88]
per-ex loss: 0.734359  [   37/   88]
per-ex loss: 0.441915  [   38/   88]
per-ex loss: 0.571615  [   39/   88]
per-ex loss: 0.732084  [   40/   88]
per-ex loss: 0.622718  [   41/   88]
per-ex loss: 0.709736  [   42/   88]
per-ex loss: 0.530347  [   43/   88]
per-ex loss: 0.492706  [   44/   88]
per-ex loss: 0.600391  [   45/   88]
per-ex loss: 0.496671  [   46/   88]
per-ex loss: 0.621344  [   47/   88]
per-ex loss: 0.535587  [   48/   88]
per-ex loss: 0.538135  [   49/   88]
per-ex loss: 0.803788  [   50/   88]
per-ex loss: 0.713373  [   51/   88]
per-ex loss: 0.684313  [   52/   88]
per-ex loss: 0.557907  [   53/   88]
per-ex loss: 0.727754  [   54/   88]
per-ex loss: 0.507547  [   55/   88]
per-ex loss: 0.522559  [   56/   88]
per-ex loss: 0.634111  [   57/   88]
per-ex loss: 0.764905  [   58/   88]
per-ex loss: 0.759840  [   59/   88]
per-ex loss: 0.698040  [   60/   88]
per-ex loss: 0.782078  [   61/   88]
per-ex loss: 0.511875  [   62/   88]
per-ex loss: 0.456064  [   63/   88]
per-ex loss: 0.560107  [   64/   88]
per-ex loss: 0.480169  [   65/   88]
per-ex loss: 0.512539  [   66/   88]
per-ex loss: 0.480096  [   67/   88]
per-ex loss: 0.716740  [   68/   88]
per-ex loss: 0.568569  [   69/   88]
per-ex loss: 0.507828  [   70/   88]
per-ex loss: 0.654103  [   71/   88]
per-ex loss: 0.683569  [   72/   88]
per-ex loss: 0.540337  [   73/   88]
per-ex loss: 0.655903  [   74/   88]
per-ex loss: 0.517829  [   75/   88]
per-ex loss: 0.462422  [   76/   88]
per-ex loss: 0.666072  [   77/   88]
per-ex loss: 0.744743  [   78/   88]
per-ex loss: 0.507289  [   79/   88]
per-ex loss: 0.464170  [   80/   88]
per-ex loss: 0.524493  [   81/   88]
per-ex loss: 0.607447  [   82/   88]
per-ex loss: 0.634103  [   83/   88]
per-ex loss: 0.579274  [   84/   88]
per-ex loss: 0.709128  [   85/   88]
per-ex loss: 0.420968  [   86/   88]
per-ex loss: 0.724976  [   87/   88]
per-ex loss: 0.647455  [   88/   88]
Train Error: Avg loss: 0.58777885
validation Error: 
 Avg loss: 0.61810388 
 F1: 0.417283 
 Precision: 0.575660 
 Recall: 0.327249
 IoU: 0.263649

test Error: 
 Avg loss: 0.56001657 
 F1: 0.510654 
 Precision: 0.614604 
 Recall: 0.436780
 IoU: 0.342871

We have finished training iteration 3
per-ex loss: 0.555990  [    1/   88]
per-ex loss: 0.727023  [    2/   88]
per-ex loss: 0.414659  [    3/   88]
per-ex loss: 0.479862  [    4/   88]
per-ex loss: 0.473321  [    5/   88]
per-ex loss: 0.519612  [    6/   88]
per-ex loss: 0.430974  [    7/   88]
per-ex loss: 0.442755  [    8/   88]
per-ex loss: 0.717019  [    9/   88]
per-ex loss: 0.605538  [   10/   88]
per-ex loss: 0.653935  [   11/   88]
per-ex loss: 0.708346  [   12/   88]
per-ex loss: 0.510339  [   13/   88]
per-ex loss: 0.567983  [   14/   88]
per-ex loss: 0.721153  [   15/   88]
per-ex loss: 0.718706  [   16/   88]
per-ex loss: 0.764437  [   17/   88]
per-ex loss: 0.681029  [   18/   88]
per-ex loss: 0.727441  [   19/   88]
per-ex loss: 0.709007  [   20/   88]
per-ex loss: 0.592400  [   21/   88]
per-ex loss: 0.474033  [   22/   88]
per-ex loss: 0.662659  [   23/   88]
per-ex loss: 0.530175  [   24/   88]
per-ex loss: 0.715450  [   25/   88]
per-ex loss: 0.522295  [   26/   88]
per-ex loss: 0.664287  [   27/   88]
per-ex loss: 0.519160  [   28/   88]
per-ex loss: 0.660153  [   29/   88]
per-ex loss: 0.752372  [   30/   88]
per-ex loss: 0.601198  [   31/   88]
per-ex loss: 0.624991  [   32/   88]
per-ex loss: 0.496643  [   33/   88]
per-ex loss: 0.639411  [   34/   88]
per-ex loss: 0.433500  [   35/   88]
per-ex loss: 0.481661  [   36/   88]
per-ex loss: 0.528046  [   37/   88]
per-ex loss: 0.461091  [   38/   88]
per-ex loss: 0.461031  [   39/   88]
per-ex loss: 0.677826  [   40/   88]
per-ex loss: 0.432463  [   41/   88]
per-ex loss: 0.637462  [   42/   88]
per-ex loss: 0.640650  [   43/   88]
per-ex loss: 0.432651  [   44/   88]
per-ex loss: 0.586764  [   45/   88]
per-ex loss: 0.501116  [   46/   88]
per-ex loss: 0.458454  [   47/   88]
per-ex loss: 0.421819  [   48/   88]
per-ex loss: 0.718750  [   49/   88]
per-ex loss: 0.606797  [   50/   88]
per-ex loss: 0.568478  [   51/   88]
per-ex loss: 0.671441  [   52/   88]
per-ex loss: 0.544971  [   53/   88]
per-ex loss: 0.555642  [   54/   88]
per-ex loss: 0.623954  [   55/   88]
per-ex loss: 0.623370  [   56/   88]
per-ex loss: 0.686758  [   57/   88]
per-ex loss: 0.508487  [   58/   88]
per-ex loss: 0.386110  [   59/   88]
per-ex loss: 0.506051  [   60/   88]
per-ex loss: 0.583988  [   61/   88]
per-ex loss: 0.517901  [   62/   88]
per-ex loss: 0.442191  [   63/   88]
per-ex loss: 0.546728  [   64/   88]
per-ex loss: 0.480540  [   65/   88]
per-ex loss: 0.487492  [   66/   88]
per-ex loss: 0.631530  [   67/   88]
per-ex loss: 0.466412  [   68/   88]
per-ex loss: 0.472598  [   69/   88]
per-ex loss: 0.526815  [   70/   88]
per-ex loss: 0.680293  [   71/   88]
per-ex loss: 0.785204  [   72/   88]
per-ex loss: 0.415087  [   73/   88]
per-ex loss: 0.746739  [   74/   88]
per-ex loss: 0.577715  [   75/   88]
per-ex loss: 0.705072  [   76/   88]
per-ex loss: 0.465872  [   77/   88]
per-ex loss: 0.718408  [   78/   88]
per-ex loss: 0.554531  [   79/   88]
per-ex loss: 0.410889  [   80/   88]
per-ex loss: 0.432937  [   81/   88]
per-ex loss: 0.768932  [   82/   88]
per-ex loss: 0.661334  [   83/   88]
per-ex loss: 0.528944  [   84/   88]
per-ex loss: 0.507464  [   85/   88]
per-ex loss: 0.419677  [   86/   88]
per-ex loss: 0.629744  [   87/   88]
per-ex loss: 0.671733  [   88/   88]
Train Error: Avg loss: 0.57473260
validation Error: 
 Avg loss: 0.59288904 
 F1: 0.439828 
 Precision: 0.498294 
 Recall: 0.393642
 IoU: 0.281910

test Error: 
 Avg loss: 0.53995058 
 F1: 0.531629 
 Precision: 0.541146 
 Recall: 0.522442
 IoU: 0.362054

We have finished training iteration 4
per-ex loss: 0.660135  [    1/   88]
per-ex loss: 0.397684  [    2/   88]
per-ex loss: 0.421810  [    3/   88]
per-ex loss: 0.430360  [    4/   88]
per-ex loss: 0.481181  [    5/   88]
per-ex loss: 0.476409  [    6/   88]
per-ex loss: 0.498558  [    7/   88]
per-ex loss: 0.422515  [    8/   88]
per-ex loss: 0.687297  [    9/   88]
per-ex loss: 0.552251  [   10/   88]
per-ex loss: 0.494020  [   11/   88]
per-ex loss: 0.416439  [   12/   88]
per-ex loss: 0.738138  [   13/   88]
per-ex loss: 0.664757  [   14/   88]
per-ex loss: 0.558772  [   15/   88]
per-ex loss: 0.499181  [   16/   88]
per-ex loss: 0.523271  [   17/   88]
per-ex loss: 0.538183  [   18/   88]
per-ex loss: 0.768788  [   19/   88]
per-ex loss: 0.624621  [   20/   88]
per-ex loss: 0.660658  [   21/   88]
per-ex loss: 0.417191  [   22/   88]
per-ex loss: 0.471050  [   23/   88]
per-ex loss: 0.726138  [   24/   88]
per-ex loss: 0.668493  [   25/   88]
per-ex loss: 0.661114  [   26/   88]
per-ex loss: 0.731662  [   27/   88]
per-ex loss: 0.565783  [   28/   88]
per-ex loss: 0.491738  [   29/   88]
per-ex loss: 0.710816  [   30/   88]
per-ex loss: 0.620963  [   31/   88]
per-ex loss: 0.481655  [   32/   88]
per-ex loss: 0.644937  [   33/   88]
per-ex loss: 0.475506  [   34/   88]
per-ex loss: 0.584840  [   35/   88]
per-ex loss: 0.652520  [   36/   88]
per-ex loss: 0.445102  [   37/   88]
per-ex loss: 0.561908  [   38/   88]
per-ex loss: 0.660393  [   39/   88]
per-ex loss: 0.486590  [   40/   88]
per-ex loss: 0.545010  [   41/   88]
per-ex loss: 0.749345  [   42/   88]
per-ex loss: 0.542267  [   43/   88]
per-ex loss: 0.741614  [   44/   88]
per-ex loss: 0.720162  [   45/   88]
per-ex loss: 0.565637  [   46/   88]
per-ex loss: 0.507682  [   47/   88]
per-ex loss: 0.525981  [   48/   88]
per-ex loss: 0.544180  [   49/   88]
per-ex loss: 0.636260  [   50/   88]
per-ex loss: 0.686297  [   51/   88]
per-ex loss: 0.755350  [   52/   88]
per-ex loss: 0.641016  [   53/   88]
per-ex loss: 0.648412  [   54/   88]
per-ex loss: 0.690397  [   55/   88]
per-ex loss: 0.662320  [   56/   88]
per-ex loss: 0.757084  [   57/   88]
per-ex loss: 0.472105  [   58/   88]
per-ex loss: 0.602778  [   59/   88]
per-ex loss: 0.444517  [   60/   88]
per-ex loss: 0.599300  [   61/   88]
per-ex loss: 0.601505  [   62/   88]
per-ex loss: 0.581957  [   63/   88]
per-ex loss: 0.472599  [   64/   88]
per-ex loss: 0.455952  [   65/   88]
per-ex loss: 0.416986  [   66/   88]
per-ex loss: 0.655242  [   67/   88]
per-ex loss: 0.474137  [   68/   88]
per-ex loss: 0.758789  [   69/   88]
per-ex loss: 0.656766  [   70/   88]
per-ex loss: 0.531936  [   71/   88]
per-ex loss: 0.482824  [   72/   88]
per-ex loss: 0.406913  [   73/   88]
per-ex loss: 0.521216  [   74/   88]
per-ex loss: 0.452344  [   75/   88]
per-ex loss: 0.512055  [   76/   88]
per-ex loss: 0.626242  [   77/   88]
per-ex loss: 0.461675  [   78/   88]
per-ex loss: 0.699772  [   79/   88]
per-ex loss: 0.452331  [   80/   88]
per-ex loss: 0.420903  [   81/   88]
per-ex loss: 0.528622  [   82/   88]
per-ex loss: 0.498920  [   83/   88]
per-ex loss: 0.754835  [   84/   88]
per-ex loss: 0.653245  [   85/   88]
per-ex loss: 0.441651  [   86/   88]
per-ex loss: 0.463964  [   87/   88]
per-ex loss: 0.466584  [   88/   88]
Train Error: Avg loss: 0.56887622
validation Error: 
 Avg loss: 0.58291979 
 F1: 0.450291 
 Precision: 0.589649 
 Recall: 0.364213
 IoU: 0.290565

test Error: 
 Avg loss: 0.53648930 
 F1: 0.530232 
 Precision: 0.602873 
 Recall: 0.473214
 IoU: 0.360759

We have finished training iteration 5
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_1_.pth
per-ex loss: 0.387933  [    1/   88]
per-ex loss: 0.502038  [    2/   88]
per-ex loss: 0.542492  [    3/   88]
per-ex loss: 0.611989  [    4/   88]
per-ex loss: 0.463890  [    5/   88]
per-ex loss: 0.584079  [    6/   88]
per-ex loss: 0.591296  [    7/   88]
per-ex loss: 0.450628  [    8/   88]
per-ex loss: 0.599948  [    9/   88]
per-ex loss: 0.736359  [   10/   88]
per-ex loss: 0.548342  [   11/   88]
per-ex loss: 0.677680  [   12/   88]
per-ex loss: 0.475883  [   13/   88]
per-ex loss: 0.435471  [   14/   88]
per-ex loss: 0.466542  [   15/   88]
per-ex loss: 0.597058  [   16/   88]
per-ex loss: 0.430309  [   17/   88]
per-ex loss: 0.696604  [   18/   88]
per-ex loss: 0.540281  [   19/   88]
per-ex loss: 0.402327  [   20/   88]
per-ex loss: 0.728217  [   21/   88]
per-ex loss: 0.524659  [   22/   88]
per-ex loss: 0.472686  [   23/   88]
per-ex loss: 0.546846  [   24/   88]
per-ex loss: 0.687998  [   25/   88]
per-ex loss: 0.494216  [   26/   88]
per-ex loss: 0.671246  [   27/   88]
per-ex loss: 0.603856  [   28/   88]
per-ex loss: 0.599111  [   29/   88]
per-ex loss: 0.763385  [   30/   88]
per-ex loss: 0.726112  [   31/   88]
per-ex loss: 0.574706  [   32/   88]
per-ex loss: 0.703786  [   33/   88]
per-ex loss: 0.420906  [   34/   88]
per-ex loss: 0.438164  [   35/   88]
per-ex loss: 0.531142  [   36/   88]
per-ex loss: 0.656302  [   37/   88]
per-ex loss: 0.457968  [   38/   88]
per-ex loss: 0.440543  [   39/   88]
per-ex loss: 0.612522  [   40/   88]
per-ex loss: 0.701791  [   41/   88]
per-ex loss: 0.684238  [   42/   88]
per-ex loss: 0.616212  [   43/   88]
per-ex loss: 0.602657  [   44/   88]
per-ex loss: 0.434969  [   45/   88]
per-ex loss: 0.661249  [   46/   88]
per-ex loss: 0.630191  [   47/   88]
per-ex loss: 0.465928  [   48/   88]
per-ex loss: 0.726738  [   49/   88]
per-ex loss: 0.503247  [   50/   88]
per-ex loss: 0.624424  [   51/   88]
per-ex loss: 0.454423  [   52/   88]
per-ex loss: 0.638990  [   53/   88]
per-ex loss: 0.525169  [   54/   88]
per-ex loss: 0.680573  [   55/   88]
per-ex loss: 0.486946  [   56/   88]
per-ex loss: 0.379103  [   57/   88]
per-ex loss: 0.594156  [   58/   88]
per-ex loss: 0.525545  [   59/   88]
per-ex loss: 0.522112  [   60/   88]
per-ex loss: 0.425723  [   61/   88]
per-ex loss: 0.701458  [   62/   88]
per-ex loss: 0.554018  [   63/   88]
per-ex loss: 0.447687  [   64/   88]
per-ex loss: 0.506587  [   65/   88]
per-ex loss: 0.411181  [   66/   88]
per-ex loss: 0.668169  [   67/   88]
per-ex loss: 0.687666  [   68/   88]
per-ex loss: 0.507288  [   69/   88]
per-ex loss: 0.752944  [   70/   88]
per-ex loss: 0.407202  [   71/   88]
per-ex loss: 0.697319  [   72/   88]
per-ex loss: 0.700002  [   73/   88]
per-ex loss: 0.463965  [   74/   88]
per-ex loss: 0.592819  [   75/   88]
per-ex loss: 0.449559  [   76/   88]
per-ex loss: 0.631757  [   77/   88]
per-ex loss: 0.560914  [   78/   88]
per-ex loss: 0.424318  [   79/   88]
per-ex loss: 0.714447  [   80/   88]
per-ex loss: 0.408874  [   81/   88]
per-ex loss: 0.574716  [   82/   88]
per-ex loss: 0.621943  [   83/   88]
per-ex loss: 0.529791  [   84/   88]
per-ex loss: 0.502144  [   85/   88]
per-ex loss: 0.534779  [   86/   88]
per-ex loss: 0.623976  [   87/   88]
per-ex loss: 0.394039  [   88/   88]
Train Error: Avg loss: 0.56078941
validation Error: 
 Avg loss: 0.59772533 
 F1: 0.426894 
 Precision: 0.525773 
 Recall: 0.359319
 IoU: 0.271370

test Error: 
 Avg loss: 0.53965380 
 F1: 0.520610 
 Precision: 0.587952 
 Recall: 0.467108
 IoU: 0.351908

We have finished training iteration 6
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_3_.pth
per-ex loss: 0.491488  [    1/   88]
per-ex loss: 0.426488  [    2/   88]
per-ex loss: 0.538142  [    3/   88]
per-ex loss: 0.817059  [    4/   88]
per-ex loss: 0.421131  [    5/   88]
per-ex loss: 0.530494  [    6/   88]
per-ex loss: 0.383457  [    7/   88]
per-ex loss: 0.401838  [    8/   88]
per-ex loss: 0.792843  [    9/   88]
per-ex loss: 0.476672  [   10/   88]
per-ex loss: 0.616138  [   11/   88]
per-ex loss: 0.636892  [   12/   88]
per-ex loss: 0.673244  [   13/   88]
per-ex loss: 0.529059  [   14/   88]
per-ex loss: 0.752419  [   15/   88]
per-ex loss: 0.618617  [   16/   88]
per-ex loss: 0.424981  [   17/   88]
per-ex loss: 0.588455  [   18/   88]
per-ex loss: 0.452272  [   19/   88]
per-ex loss: 0.468885  [   20/   88]
per-ex loss: 0.634754  [   21/   88]
per-ex loss: 0.485323  [   22/   88]
per-ex loss: 0.667279  [   23/   88]
per-ex loss: 0.631660  [   24/   88]
per-ex loss: 0.684050  [   25/   88]
per-ex loss: 0.458881  [   26/   88]
per-ex loss: 0.429987  [   27/   88]
per-ex loss: 0.476875  [   28/   88]
per-ex loss: 0.634224  [   29/   88]
per-ex loss: 0.510643  [   30/   88]
per-ex loss: 0.662414  [   31/   88]
per-ex loss: 0.655342  [   32/   88]
per-ex loss: 0.530831  [   33/   88]
per-ex loss: 0.429227  [   34/   88]
per-ex loss: 0.446663  [   35/   88]
per-ex loss: 0.589942  [   36/   88]
per-ex loss: 0.435673  [   37/   88]
per-ex loss: 0.437099  [   38/   88]
per-ex loss: 0.470776  [   39/   88]
per-ex loss: 0.465013  [   40/   88]
per-ex loss: 0.448512  [   41/   88]
per-ex loss: 0.498199  [   42/   88]
per-ex loss: 0.673734  [   43/   88]
per-ex loss: 0.425462  [   44/   88]
per-ex loss: 0.704407  [   45/   88]
per-ex loss: 0.654745  [   46/   88]
per-ex loss: 0.380966  [   47/   88]
per-ex loss: 0.518621  [   48/   88]
per-ex loss: 0.592816  [   49/   88]
per-ex loss: 0.533696  [   50/   88]
per-ex loss: 0.674699  [   51/   88]
per-ex loss: 0.444843  [   52/   88]
per-ex loss: 0.717472  [   53/   88]
per-ex loss: 0.629101  [   54/   88]
per-ex loss: 0.619046  [   55/   88]
per-ex loss: 0.490883  [   56/   88]
per-ex loss: 0.708942  [   57/   88]
per-ex loss: 0.426752  [   58/   88]
per-ex loss: 0.579060  [   59/   88]
per-ex loss: 0.685931  [   60/   88]
per-ex loss: 0.525781  [   61/   88]
per-ex loss: 0.378762  [   62/   88]
per-ex loss: 0.468297  [   63/   88]
per-ex loss: 0.465868  [   64/   88]
per-ex loss: 0.425086  [   65/   88]
per-ex loss: 0.380560  [   66/   88]
per-ex loss: 0.607561  [   67/   88]
per-ex loss: 0.693884  [   68/   88]
per-ex loss: 0.396875  [   69/   88]
per-ex loss: 0.749937  [   70/   88]
per-ex loss: 0.554490  [   71/   88]
per-ex loss: 0.441612  [   72/   88]
per-ex loss: 0.693679  [   73/   88]
per-ex loss: 0.605399  [   74/   88]
per-ex loss: 0.723733  [   75/   88]
per-ex loss: 0.494079  [   76/   88]
per-ex loss: 0.609774  [   77/   88]
per-ex loss: 0.536125  [   78/   88]
per-ex loss: 0.705406  [   79/   88]
per-ex loss: 0.758300  [   80/   88]
per-ex loss: 0.762375  [   81/   88]
per-ex loss: 0.665340  [   82/   88]
per-ex loss: 0.532086  [   83/   88]
per-ex loss: 0.672129  [   84/   88]
per-ex loss: 0.520147  [   85/   88]
per-ex loss: 0.616380  [   86/   88]
per-ex loss: 0.456803  [   87/   88]
per-ex loss: 0.582838  [   88/   88]
Train Error: Avg loss: 0.55918665
validation Error: 
 Avg loss: 0.66711611 
 F1: 0.355745 
 Precision: 0.285017 
 Recall: 0.473162
 IoU: 0.216356

test Error: 
 Avg loss: 0.58556284 
 F1: 0.449058 
 Precision: 0.381305 
 Recall: 0.546094
 IoU: 0.289539

We have finished training iteration 7
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_2_.pth
per-ex loss: 0.504776  [    1/   88]
per-ex loss: 0.487060  [    2/   88]
per-ex loss: 0.584679  [    3/   88]
per-ex loss: 0.621614  [    4/   88]
per-ex loss: 0.473436  [    5/   88]
per-ex loss: 0.698466  [    6/   88]
per-ex loss: 0.439168  [    7/   88]
per-ex loss: 0.510295  [    8/   88]
per-ex loss: 0.456158  [    9/   88]
per-ex loss: 0.554415  [   10/   88]
per-ex loss: 0.687182  [   11/   88]
per-ex loss: 0.584056  [   12/   88]
per-ex loss: 0.464601  [   13/   88]
per-ex loss: 0.699403  [   14/   88]
per-ex loss: 0.524947  [   15/   88]
per-ex loss: 0.547209  [   16/   88]
per-ex loss: 0.584195  [   17/   88]
per-ex loss: 0.667335  [   18/   88]
per-ex loss: 0.555550  [   19/   88]
per-ex loss: 0.520138  [   20/   88]
per-ex loss: 0.470215  [   21/   88]
per-ex loss: 0.387078  [   22/   88]
per-ex loss: 0.493615  [   23/   88]
per-ex loss: 0.562191  [   24/   88]
per-ex loss: 0.402870  [   25/   88]
per-ex loss: 0.677942  [   26/   88]
per-ex loss: 0.683537  [   27/   88]
per-ex loss: 0.483684  [   28/   88]
per-ex loss: 0.745108  [   29/   88]
per-ex loss: 0.599957  [   30/   88]
per-ex loss: 0.673143  [   31/   88]
per-ex loss: 0.725395  [   32/   88]
per-ex loss: 0.739439  [   33/   88]
per-ex loss: 0.537624  [   34/   88]
per-ex loss: 0.439601  [   35/   88]
per-ex loss: 0.476839  [   36/   88]
per-ex loss: 0.731231  [   37/   88]
per-ex loss: 0.740203  [   38/   88]
per-ex loss: 0.524743  [   39/   88]
per-ex loss: 0.490957  [   40/   88]
per-ex loss: 0.440226  [   41/   88]
per-ex loss: 0.439470  [   42/   88]
per-ex loss: 0.436799  [   43/   88]
per-ex loss: 0.374882  [   44/   88]
per-ex loss: 0.439982  [   45/   88]
per-ex loss: 0.726481  [   46/   88]
per-ex loss: 0.487476  [   47/   88]
per-ex loss: 0.507459  [   48/   88]
per-ex loss: 0.640332  [   49/   88]
per-ex loss: 0.602236  [   50/   88]
per-ex loss: 0.411319  [   51/   88]
per-ex loss: 0.393685  [   52/   88]
per-ex loss: 0.468302  [   53/   88]
per-ex loss: 0.672119  [   54/   88]
per-ex loss: 0.675819  [   55/   88]
per-ex loss: 0.387992  [   56/   88]
per-ex loss: 0.537745  [   57/   88]
per-ex loss: 0.407468  [   58/   88]
per-ex loss: 0.764162  [   59/   88]
per-ex loss: 0.680189  [   60/   88]
per-ex loss: 0.717683  [   61/   88]
per-ex loss: 0.478565  [   62/   88]
per-ex loss: 0.633366  [   63/   88]
per-ex loss: 0.588499  [   64/   88]
per-ex loss: 0.663297  [   65/   88]
per-ex loss: 0.607346  [   66/   88]
per-ex loss: 0.600531  [   67/   88]
per-ex loss: 0.674296  [   68/   88]
per-ex loss: 0.658319  [   69/   88]
per-ex loss: 0.442549  [   70/   88]
per-ex loss: 0.666133  [   71/   88]
per-ex loss: 0.505751  [   72/   88]
per-ex loss: 0.667352  [   73/   88]
per-ex loss: 0.547983  [   74/   88]
per-ex loss: 0.623105  [   75/   88]
per-ex loss: 0.479831  [   76/   88]
per-ex loss: 0.416177  [   77/   88]
per-ex loss: 0.429345  [   78/   88]
per-ex loss: 0.540453  [   79/   88]
per-ex loss: 0.447022  [   80/   88]
per-ex loss: 0.543954  [   81/   88]
per-ex loss: 0.615208  [   82/   88]
per-ex loss: 0.518601  [   83/   88]
per-ex loss: 0.426151  [   84/   88]
per-ex loss: 0.635650  [   85/   88]
per-ex loss: 0.637812  [   86/   88]
per-ex loss: 0.410609  [   87/   88]
per-ex loss: 0.412334  [   88/   88]
Train Error: Avg loss: 0.55491045
validation Error: 
 Avg loss: 0.58218121 
 F1: 0.451539 
 Precision: 0.510940 
 Recall: 0.404511
 IoU: 0.291605

test Error: 
 Avg loss: 0.53191045 
 F1: 0.536189 
 Precision: 0.534791 
 Recall: 0.537595
 IoU: 0.366297

We have finished training iteration 8
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_6_.pth
per-ex loss: 0.428044  [    1/   88]
per-ex loss: 0.531926  [    2/   88]
per-ex loss: 0.360108  [    3/   88]
per-ex loss: 0.715422  [    4/   88]
per-ex loss: 0.608951  [    5/   88]
per-ex loss: 0.361192  [    6/   88]
per-ex loss: 0.748406  [    7/   88]
per-ex loss: 0.394977  [    8/   88]
per-ex loss: 0.481283  [    9/   88]
per-ex loss: 0.432714  [   10/   88]
per-ex loss: 0.647151  [   11/   88]
per-ex loss: 0.768444  [   12/   88]
per-ex loss: 0.695463  [   13/   88]
per-ex loss: 0.615122  [   14/   88]
per-ex loss: 0.466840  [   15/   88]
per-ex loss: 0.542775  [   16/   88]
per-ex loss: 0.453261  [   17/   88]
per-ex loss: 0.417204  [   18/   88]
per-ex loss: 0.619107  [   19/   88]
per-ex loss: 0.513808  [   20/   88]
per-ex loss: 0.685722  [   21/   88]
per-ex loss: 0.562374  [   22/   88]
per-ex loss: 0.478696  [   23/   88]
per-ex loss: 0.558611  [   24/   88]
per-ex loss: 0.489038  [   25/   88]
per-ex loss: 0.498596  [   26/   88]
per-ex loss: 0.743692  [   27/   88]
per-ex loss: 0.476534  [   28/   88]
per-ex loss: 0.692551  [   29/   88]
per-ex loss: 0.512738  [   30/   88]
per-ex loss: 0.632840  [   31/   88]
per-ex loss: 0.622104  [   32/   88]
per-ex loss: 0.689872  [   33/   88]
per-ex loss: 0.625913  [   34/   88]
per-ex loss: 0.643510  [   35/   88]
per-ex loss: 0.449346  [   36/   88]
per-ex loss: 0.575110  [   37/   88]
per-ex loss: 0.500147  [   38/   88]
per-ex loss: 0.454735  [   39/   88]
per-ex loss: 0.581610  [   40/   88]
per-ex loss: 0.437547  [   41/   88]
per-ex loss: 0.710104  [   42/   88]
per-ex loss: 0.626833  [   43/   88]
per-ex loss: 0.481590  [   44/   88]
per-ex loss: 0.430470  [   45/   88]
per-ex loss: 0.624865  [   46/   88]
per-ex loss: 0.522191  [   47/   88]
per-ex loss: 0.532751  [   48/   88]
per-ex loss: 0.406199  [   49/   88]
per-ex loss: 0.571689  [   50/   88]
per-ex loss: 0.518528  [   51/   88]
per-ex loss: 0.405171  [   52/   88]
per-ex loss: 0.674218  [   53/   88]
per-ex loss: 0.424174  [   54/   88]
per-ex loss: 0.723976  [   55/   88]
per-ex loss: 0.430101  [   56/   88]
per-ex loss: 0.455538  [   57/   88]
per-ex loss: 0.593109  [   58/   88]
per-ex loss: 0.647914  [   59/   88]
per-ex loss: 0.430705  [   60/   88]
per-ex loss: 0.708991  [   61/   88]
per-ex loss: 0.466263  [   62/   88]
per-ex loss: 0.383155  [   63/   88]
per-ex loss: 0.581359  [   64/   88]
per-ex loss: 0.459397  [   65/   88]
per-ex loss: 0.611577  [   66/   88]
per-ex loss: 0.515570  [   67/   88]
per-ex loss: 0.671327  [   68/   88]
per-ex loss: 0.598518  [   69/   88]
per-ex loss: 0.745155  [   70/   88]
per-ex loss: 0.549173  [   71/   88]
per-ex loss: 0.644557  [   72/   88]
per-ex loss: 0.501919  [   73/   88]
per-ex loss: 0.434852  [   74/   88]
per-ex loss: 0.593902  [   75/   88]
per-ex loss: 0.424269  [   76/   88]
per-ex loss: 0.700529  [   77/   88]
per-ex loss: 0.499690  [   78/   88]
per-ex loss: 0.656288  [   79/   88]
per-ex loss: 0.424446  [   80/   88]
per-ex loss: 0.631048  [   81/   88]
per-ex loss: 0.601581  [   82/   88]
per-ex loss: 0.503817  [   83/   88]
per-ex loss: 0.406395  [   84/   88]
per-ex loss: 0.499071  [   85/   88]
per-ex loss: 0.671099  [   86/   88]
per-ex loss: 0.390623  [   87/   88]
per-ex loss: 0.593390  [   88/   88]
Train Error: Avg loss: 0.54990420
validation Error: 
 Avg loss: 0.57543006 
 F1: 0.454484 
 Precision: 0.587047 
 Recall: 0.370760
 IoU: 0.294066

test Error: 
 Avg loss: 0.52399661 
 F1: 0.542190 
 Precision: 0.611101 
 Recall: 0.487246
 IoU: 0.371921

We have finished training iteration 9
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_7_.pth
per-ex loss: 0.422289  [    1/   88]
per-ex loss: 0.411953  [    2/   88]
per-ex loss: 0.657243  [    3/   88]
per-ex loss: 0.619847  [    4/   88]
per-ex loss: 0.645558  [    5/   88]
per-ex loss: 0.376783  [    6/   88]
per-ex loss: 0.476902  [    7/   88]
per-ex loss: 0.594714  [    8/   88]
per-ex loss: 0.683225  [    9/   88]
per-ex loss: 0.695606  [   10/   88]
per-ex loss: 0.483989  [   11/   88]
per-ex loss: 0.622351  [   12/   88]
per-ex loss: 0.599485  [   13/   88]
per-ex loss: 0.458221  [   14/   88]
per-ex loss: 0.704471  [   15/   88]
per-ex loss: 0.498302  [   16/   88]
per-ex loss: 0.694838  [   17/   88]
per-ex loss: 0.444481  [   18/   88]
per-ex loss: 0.603517  [   19/   88]
per-ex loss: 0.507961  [   20/   88]
per-ex loss: 0.524112  [   21/   88]
per-ex loss: 0.517208  [   22/   88]
per-ex loss: 0.544288  [   23/   88]
per-ex loss: 0.444919  [   24/   88]
per-ex loss: 0.698463  [   25/   88]
per-ex loss: 0.552542  [   26/   88]
per-ex loss: 0.519974  [   27/   88]
per-ex loss: 0.574886  [   28/   88]
per-ex loss: 0.477349  [   29/   88]
per-ex loss: 0.687204  [   30/   88]
per-ex loss: 0.693887  [   31/   88]
per-ex loss: 0.429942  [   32/   88]
per-ex loss: 0.718092  [   33/   88]
per-ex loss: 0.610253  [   34/   88]
per-ex loss: 0.515054  [   35/   88]
per-ex loss: 0.453789  [   36/   88]
per-ex loss: 0.649621  [   37/   88]
per-ex loss: 0.500626  [   38/   88]
per-ex loss: 0.626724  [   39/   88]
per-ex loss: 0.591732  [   40/   88]
per-ex loss: 0.495866  [   41/   88]
per-ex loss: 0.423363  [   42/   88]
per-ex loss: 0.576400  [   43/   88]
per-ex loss: 0.470218  [   44/   88]
per-ex loss: 0.647085  [   45/   88]
per-ex loss: 0.490460  [   46/   88]
per-ex loss: 0.446934  [   47/   88]
per-ex loss: 0.426116  [   48/   88]
per-ex loss: 0.706409  [   49/   88]
per-ex loss: 0.467126  [   50/   88]
per-ex loss: 0.625591  [   51/   88]
per-ex loss: 0.643803  [   52/   88]
per-ex loss: 0.758748  [   53/   88]
per-ex loss: 0.386660  [   54/   88]
per-ex loss: 0.625165  [   55/   88]
per-ex loss: 0.520009  [   56/   88]
per-ex loss: 0.497564  [   57/   88]
per-ex loss: 0.432728  [   58/   88]
per-ex loss: 0.461614  [   59/   88]
per-ex loss: 0.542524  [   60/   88]
per-ex loss: 0.628655  [   61/   88]
per-ex loss: 0.404222  [   62/   88]
per-ex loss: 0.446716  [   63/   88]
per-ex loss: 0.433300  [   64/   88]
per-ex loss: 0.610544  [   65/   88]
per-ex loss: 0.464658  [   66/   88]
per-ex loss: 0.488014  [   67/   88]
per-ex loss: 0.455765  [   68/   88]
per-ex loss: 0.526848  [   69/   88]
per-ex loss: 0.648633  [   70/   88]
per-ex loss: 0.466032  [   71/   88]
per-ex loss: 0.631372  [   72/   88]
per-ex loss: 0.371817  [   73/   88]
per-ex loss: 0.743501  [   74/   88]
per-ex loss: 0.666384  [   75/   88]
per-ex loss: 0.484208  [   76/   88]
per-ex loss: 0.447481  [   77/   88]
per-ex loss: 0.513334  [   78/   88]
per-ex loss: 0.406475  [   79/   88]
per-ex loss: 0.395158  [   80/   88]
per-ex loss: 0.578282  [   81/   88]
per-ex loss: 0.409337  [   82/   88]
per-ex loss: 0.628020  [   83/   88]
per-ex loss: 0.640265  [   84/   88]
per-ex loss: 0.417210  [   85/   88]
per-ex loss: 0.743503  [   86/   88]
per-ex loss: 0.594750  [   87/   88]
per-ex loss: 0.720542  [   88/   88]
Train Error: Avg loss: 0.54706599
validation Error: 
 Avg loss: 0.57001303 
 F1: 0.462509 
 Precision: 0.491819 
 Recall: 0.436497
 IoU: 0.300821

test Error: 
 Avg loss: 0.52139874 
 F1: 0.542529 
 Precision: 0.537832 
 Recall: 0.547309
 IoU: 0.372240

We have finished training iteration 10
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_4_.pth
per-ex loss: 0.454787  [    1/   88]
per-ex loss: 0.454021  [    2/   88]
per-ex loss: 0.424266  [    3/   88]
per-ex loss: 0.571753  [    4/   88]
per-ex loss: 0.705799  [    5/   88]
per-ex loss: 0.583921  [    6/   88]
per-ex loss: 0.707504  [    7/   88]
per-ex loss: 0.428066  [    8/   88]
per-ex loss: 0.566023  [    9/   88]
per-ex loss: 0.453502  [   10/   88]
per-ex loss: 0.658611  [   11/   88]
per-ex loss: 0.494340  [   12/   88]
per-ex loss: 0.415007  [   13/   88]
per-ex loss: 0.583668  [   14/   88]
per-ex loss: 0.415604  [   15/   88]
per-ex loss: 0.688839  [   16/   88]
per-ex loss: 0.606156  [   17/   88]
per-ex loss: 0.613788  [   18/   88]
per-ex loss: 0.479309  [   19/   88]
per-ex loss: 0.496462  [   20/   88]
per-ex loss: 0.491311  [   21/   88]
per-ex loss: 0.468186  [   22/   88]
per-ex loss: 0.420426  [   23/   88]
per-ex loss: 0.410878  [   24/   88]
per-ex loss: 0.625522  [   25/   88]
per-ex loss: 0.646015  [   26/   88]
per-ex loss: 0.724786  [   27/   88]
per-ex loss: 0.441591  [   28/   88]
per-ex loss: 0.566733  [   29/   88]
per-ex loss: 0.649806  [   30/   88]
per-ex loss: 0.629496  [   31/   88]
per-ex loss: 0.485233  [   32/   88]
per-ex loss: 0.447757  [   33/   88]
per-ex loss: 0.657956  [   34/   88]
per-ex loss: 0.503965  [   35/   88]
per-ex loss: 0.502207  [   36/   88]
per-ex loss: 0.414755  [   37/   88]
per-ex loss: 0.488984  [   38/   88]
per-ex loss: 0.506334  [   39/   88]
per-ex loss: 0.574297  [   40/   88]
per-ex loss: 0.508061  [   41/   88]
per-ex loss: 0.555142  [   42/   88]
per-ex loss: 0.662330  [   43/   88]
per-ex loss: 0.602146  [   44/   88]
per-ex loss: 0.625654  [   45/   88]
per-ex loss: 0.706020  [   46/   88]
per-ex loss: 0.366650  [   47/   88]
per-ex loss: 0.407923  [   48/   88]
per-ex loss: 0.602007  [   49/   88]
per-ex loss: 0.490046  [   50/   88]
per-ex loss: 0.580274  [   51/   88]
per-ex loss: 0.440948  [   52/   88]
per-ex loss: 0.402901  [   53/   88]
per-ex loss: 0.574423  [   54/   88]
per-ex loss: 0.618438  [   55/   88]
per-ex loss: 0.373075  [   56/   88]
per-ex loss: 0.469758  [   57/   88]
per-ex loss: 0.381974  [   58/   88]
per-ex loss: 0.508848  [   59/   88]
per-ex loss: 0.558341  [   60/   88]
per-ex loss: 0.401745  [   61/   88]
per-ex loss: 0.779721  [   62/   88]
per-ex loss: 0.400811  [   63/   88]
per-ex loss: 0.666414  [   64/   88]
per-ex loss: 0.458809  [   65/   88]
per-ex loss: 0.445119  [   66/   88]
per-ex loss: 0.426064  [   67/   88]
per-ex loss: 0.386881  [   68/   88]
per-ex loss: 0.667679  [   69/   88]
per-ex loss: 0.481342  [   70/   88]
per-ex loss: 0.704659  [   71/   88]
per-ex loss: 0.654951  [   72/   88]
per-ex loss: 0.663002  [   73/   88]
per-ex loss: 0.547244  [   74/   88]
per-ex loss: 0.698783  [   75/   88]
per-ex loss: 0.518741  [   76/   88]
per-ex loss: 0.645515  [   77/   88]
per-ex loss: 0.422711  [   78/   88]
per-ex loss: 0.519820  [   79/   88]
per-ex loss: 0.454613  [   80/   88]
per-ex loss: 0.727755  [   81/   88]
per-ex loss: 0.512551  [   82/   88]
per-ex loss: 0.543236  [   83/   88]
per-ex loss: 0.632139  [   84/   88]
per-ex loss: 0.532088  [   85/   88]
per-ex loss: 0.690091  [   86/   88]
per-ex loss: 0.722517  [   87/   88]
per-ex loss: 0.620664  [   88/   88]
Train Error: Avg loss: 0.54338961
validation Error: 
 Avg loss: 0.56690995 
 F1: 0.473675 
 Precision: 0.472385 
 Recall: 0.474972
 IoU: 0.310337

test Error: 
 Avg loss: 0.53029808 
 F1: 0.530650 
 Precision: 0.492247 
 Recall: 0.575553
 IoU: 0.361146

We have finished training iteration 11
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_5_.pth
per-ex loss: 0.646542  [    1/   88]
per-ex loss: 0.418677  [    2/   88]
per-ex loss: 0.649050  [    3/   88]
per-ex loss: 0.643826  [    4/   88]
per-ex loss: 0.387070  [    5/   88]
per-ex loss: 0.373248  [    6/   88]
per-ex loss: 0.453428  [    7/   88]
per-ex loss: 0.627437  [    8/   88]
per-ex loss: 0.443472  [    9/   88]
per-ex loss: 0.605543  [   10/   88]
per-ex loss: 0.478648  [   11/   88]
per-ex loss: 0.514937  [   12/   88]
per-ex loss: 0.644904  [   13/   88]
per-ex loss: 0.610029  [   14/   88]
per-ex loss: 0.411471  [   15/   88]
per-ex loss: 0.535687  [   16/   88]
per-ex loss: 0.470023  [   17/   88]
per-ex loss: 0.485430  [   18/   88]
per-ex loss: 0.703100  [   19/   88]
per-ex loss: 0.595450  [   20/   88]
per-ex loss: 0.481014  [   21/   88]
per-ex loss: 0.693374  [   22/   88]
per-ex loss: 0.383121  [   23/   88]
per-ex loss: 0.479327  [   24/   88]
per-ex loss: 0.453716  [   25/   88]
per-ex loss: 0.495552  [   26/   88]
per-ex loss: 0.389651  [   27/   88]
per-ex loss: 0.589522  [   28/   88]
per-ex loss: 0.668680  [   29/   88]
per-ex loss: 0.605406  [   30/   88]
per-ex loss: 0.465953  [   31/   88]
per-ex loss: 0.695673  [   32/   88]
per-ex loss: 0.578928  [   33/   88]
per-ex loss: 0.582344  [   34/   88]
per-ex loss: 0.694969  [   35/   88]
per-ex loss: 0.522536  [   36/   88]
per-ex loss: 0.537863  [   37/   88]
per-ex loss: 0.440738  [   38/   88]
per-ex loss: 0.446900  [   39/   88]
per-ex loss: 0.659575  [   40/   88]
per-ex loss: 0.607926  [   41/   88]
per-ex loss: 0.712621  [   42/   88]
per-ex loss: 0.472691  [   43/   88]
per-ex loss: 0.456785  [   44/   88]
per-ex loss: 0.432011  [   45/   88]
per-ex loss: 0.422606  [   46/   88]
per-ex loss: 0.506836  [   47/   88]
per-ex loss: 0.558468  [   48/   88]
per-ex loss: 0.497462  [   49/   88]
per-ex loss: 0.416174  [   50/   88]
per-ex loss: 0.394598  [   51/   88]
per-ex loss: 0.615772  [   52/   88]
per-ex loss: 0.585973  [   53/   88]
per-ex loss: 0.718715  [   54/   88]
per-ex loss: 0.611533  [   55/   88]
per-ex loss: 0.693404  [   56/   88]
per-ex loss: 0.572574  [   57/   88]
per-ex loss: 0.422483  [   58/   88]
per-ex loss: 0.396729  [   59/   88]
per-ex loss: 0.611291  [   60/   88]
per-ex loss: 0.544892  [   61/   88]
per-ex loss: 0.579311  [   62/   88]
per-ex loss: 0.729120  [   63/   88]
per-ex loss: 0.407077  [   64/   88]
per-ex loss: 0.610095  [   65/   88]
per-ex loss: 0.502136  [   66/   88]
per-ex loss: 0.549820  [   67/   88]
per-ex loss: 0.480611  [   68/   88]
per-ex loss: 0.511832  [   69/   88]
per-ex loss: 0.562679  [   70/   88]
per-ex loss: 0.509535  [   71/   88]
per-ex loss: 0.431798  [   72/   88]
per-ex loss: 0.607940  [   73/   88]
per-ex loss: 0.653523  [   74/   88]
per-ex loss: 0.733301  [   75/   88]
per-ex loss: 0.648477  [   76/   88]
per-ex loss: 0.416005  [   77/   88]
per-ex loss: 0.717783  [   78/   88]
per-ex loss: 0.400186  [   79/   88]
per-ex loss: 0.422507  [   80/   88]
per-ex loss: 0.473950  [   81/   88]
per-ex loss: 0.401352  [   82/   88]
per-ex loss: 0.693653  [   83/   88]
per-ex loss: 0.527380  [   84/   88]
per-ex loss: 0.744294  [   85/   88]
per-ex loss: 0.437613  [   86/   88]
per-ex loss: 0.411457  [   87/   88]
per-ex loss: 0.633879  [   88/   88]
Train Error: Avg loss: 0.54104180
validation Error: 
 Avg loss: 0.57082380 
 F1: 0.458505 
 Precision: 0.529026 
 Recall: 0.404573
 IoU: 0.297441

test Error: 
 Avg loss: 0.51373455 
 F1: 0.550719 
 Precision: 0.590075 
 Recall: 0.516285
 IoU: 0.379995

We have finished training iteration 12
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_8_.pth
per-ex loss: 0.669634  [    1/   88]
per-ex loss: 0.714354  [    2/   88]
per-ex loss: 0.408740  [    3/   88]
per-ex loss: 0.413893  [    4/   88]
per-ex loss: 0.602573  [    5/   88]
per-ex loss: 0.471353  [    6/   88]
per-ex loss: 0.521782  [    7/   88]
per-ex loss: 0.568381  [    8/   88]
per-ex loss: 0.518604  [    9/   88]
per-ex loss: 0.691997  [   10/   88]
per-ex loss: 0.492806  [   11/   88]
per-ex loss: 0.440516  [   12/   88]
per-ex loss: 0.416850  [   13/   88]
per-ex loss: 0.668931  [   14/   88]
per-ex loss: 0.502191  [   15/   88]
per-ex loss: 0.530054  [   16/   88]
per-ex loss: 0.404266  [   17/   88]
per-ex loss: 0.616498  [   18/   88]
per-ex loss: 0.414260  [   19/   88]
per-ex loss: 0.634052  [   20/   88]
per-ex loss: 0.578584  [   21/   88]
per-ex loss: 0.516318  [   22/   88]
per-ex loss: 0.750625  [   23/   88]
per-ex loss: 0.679828  [   24/   88]
per-ex loss: 0.641341  [   25/   88]
per-ex loss: 0.426418  [   26/   88]
per-ex loss: 0.506042  [   27/   88]
per-ex loss: 0.748952  [   28/   88]
per-ex loss: 0.609341  [   29/   88]
per-ex loss: 0.356672  [   30/   88]
per-ex loss: 0.522692  [   31/   88]
per-ex loss: 0.402192  [   32/   88]
per-ex loss: 0.640754  [   33/   88]
per-ex loss: 0.502510  [   34/   88]
per-ex loss: 0.594417  [   35/   88]
per-ex loss: 0.606535  [   36/   88]
per-ex loss: 0.430472  [   37/   88]
per-ex loss: 0.391170  [   38/   88]
per-ex loss: 0.440901  [   39/   88]
per-ex loss: 0.412951  [   40/   88]
per-ex loss: 0.565097  [   41/   88]
per-ex loss: 0.708682  [   42/   88]
per-ex loss: 0.504093  [   43/   88]
per-ex loss: 0.434893  [   44/   88]
per-ex loss: 0.588272  [   45/   88]
per-ex loss: 0.432969  [   46/   88]
per-ex loss: 0.700414  [   47/   88]
per-ex loss: 0.593873  [   48/   88]
per-ex loss: 0.398924  [   49/   88]
per-ex loss: 0.414339  [   50/   88]
per-ex loss: 0.473593  [   51/   88]
per-ex loss: 0.449158  [   52/   88]
per-ex loss: 0.534725  [   53/   88]
per-ex loss: 0.577226  [   54/   88]
per-ex loss: 0.401274  [   55/   88]
per-ex loss: 0.426620  [   56/   88]
per-ex loss: 0.472011  [   57/   88]
per-ex loss: 0.441234  [   58/   88]
per-ex loss: 0.522337  [   59/   88]
per-ex loss: 0.449856  [   60/   88]
per-ex loss: 0.633412  [   61/   88]
per-ex loss: 0.577124  [   62/   88]
per-ex loss: 0.503859  [   63/   88]
per-ex loss: 0.380522  [   64/   88]
per-ex loss: 0.701300  [   65/   88]
per-ex loss: 0.661317  [   66/   88]
per-ex loss: 0.666579  [   67/   88]
per-ex loss: 0.703271  [   68/   88]
per-ex loss: 0.475756  [   69/   88]
per-ex loss: 0.465186  [   70/   88]
per-ex loss: 0.615085  [   71/   88]
per-ex loss: 0.495855  [   72/   88]
per-ex loss: 0.736598  [   73/   88]
per-ex loss: 0.424553  [   74/   88]
per-ex loss: 0.671313  [   75/   88]
per-ex loss: 0.587325  [   76/   88]
per-ex loss: 0.576720  [   77/   88]
per-ex loss: 0.487157  [   78/   88]
per-ex loss: 0.616744  [   79/   88]
per-ex loss: 0.639080  [   80/   88]
per-ex loss: 0.703102  [   81/   88]
per-ex loss: 0.480783  [   82/   88]
per-ex loss: 0.664505  [   83/   88]
per-ex loss: 0.448974  [   84/   88]
per-ex loss: 0.407944  [   85/   88]
per-ex loss: 0.612657  [   86/   88]
per-ex loss: 0.394990  [   87/   88]
per-ex loss: 0.514062  [   88/   88]
Train Error: Avg loss: 0.53858883
validation Error: 
 Avg loss: 0.56386088 
 F1: 0.469095 
 Precision: 0.565673 
 Recall: 0.400686
 IoU: 0.306417

test Error: 
 Avg loss: 0.51751951 
 F1: 0.544695 
 Precision: 0.591152 
 Recall: 0.505009
 IoU: 0.374283

We have finished training iteration 13
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_9_.pth
per-ex loss: 0.425535  [    1/   88]
per-ex loss: 0.662723  [    2/   88]
per-ex loss: 0.610767  [    3/   88]
per-ex loss: 0.701715  [    4/   88]
per-ex loss: 0.479498  [    5/   88]
per-ex loss: 0.626591  [    6/   88]
per-ex loss: 0.756572  [    7/   88]
per-ex loss: 0.655314  [    8/   88]
per-ex loss: 0.413703  [    9/   88]
per-ex loss: 0.608391  [   10/   88]
per-ex loss: 0.654557  [   11/   88]
per-ex loss: 0.400117  [   12/   88]
per-ex loss: 0.371244  [   13/   88]
per-ex loss: 0.665452  [   14/   88]
per-ex loss: 0.691139  [   15/   88]
per-ex loss: 0.707844  [   16/   88]
per-ex loss: 0.483656  [   17/   88]
per-ex loss: 0.555260  [   18/   88]
per-ex loss: 0.458082  [   19/   88]
per-ex loss: 0.503425  [   20/   88]
per-ex loss: 0.460170  [   21/   88]
per-ex loss: 0.597166  [   22/   88]
per-ex loss: 0.625654  [   23/   88]
per-ex loss: 0.711658  [   24/   88]
per-ex loss: 0.693877  [   25/   88]
per-ex loss: 0.617762  [   26/   88]
per-ex loss: 0.458100  [   27/   88]
per-ex loss: 0.709373  [   28/   88]
per-ex loss: 0.513573  [   29/   88]
per-ex loss: 0.461890  [   30/   88]
per-ex loss: 0.512677  [   31/   88]
per-ex loss: 0.555750  [   32/   88]
per-ex loss: 0.728737  [   33/   88]
per-ex loss: 0.588589  [   34/   88]
per-ex loss: 0.411225  [   35/   88]
per-ex loss: 0.579541  [   36/   88]
per-ex loss: 0.458922  [   37/   88]
per-ex loss: 0.441565  [   38/   88]
per-ex loss: 0.603979  [   39/   88]
per-ex loss: 0.634416  [   40/   88]
per-ex loss: 0.436319  [   41/   88]
per-ex loss: 0.442012  [   42/   88]
per-ex loss: 0.457444  [   43/   88]
per-ex loss: 0.705095  [   44/   88]
per-ex loss: 0.683869  [   45/   88]
per-ex loss: 0.340335  [   46/   88]
per-ex loss: 0.451468  [   47/   88]
per-ex loss: 0.439288  [   48/   88]
per-ex loss: 0.446347  [   49/   88]
per-ex loss: 0.483609  [   50/   88]
per-ex loss: 0.647855  [   51/   88]
per-ex loss: 0.439632  [   52/   88]
per-ex loss: 0.710650  [   53/   88]
per-ex loss: 0.630610  [   54/   88]
per-ex loss: 0.694148  [   55/   88]
per-ex loss: 0.513533  [   56/   88]
per-ex loss: 0.413066  [   57/   88]
per-ex loss: 0.504870  [   58/   88]
per-ex loss: 0.397269  [   59/   88]
per-ex loss: 0.454210  [   60/   88]
per-ex loss: 0.357975  [   61/   88]
per-ex loss: 0.633222  [   62/   88]
per-ex loss: 0.411117  [   63/   88]
per-ex loss: 0.417659  [   64/   88]
per-ex loss: 0.540711  [   65/   88]
per-ex loss: 0.428759  [   66/   88]
per-ex loss: 0.626147  [   67/   88]
per-ex loss: 0.437122  [   68/   88]
per-ex loss: 0.672734  [   69/   88]
per-ex loss: 0.466658  [   70/   88]
per-ex loss: 0.578248  [   71/   88]
per-ex loss: 0.435193  [   72/   88]
per-ex loss: 0.566723  [   73/   88]
per-ex loss: 0.418129  [   74/   88]
per-ex loss: 0.428053  [   75/   88]
per-ex loss: 0.465280  [   76/   88]
per-ex loss: 0.383031  [   77/   88]
per-ex loss: 0.461244  [   78/   88]
per-ex loss: 0.488241  [   79/   88]
per-ex loss: 0.524185  [   80/   88]
per-ex loss: 0.614873  [   81/   88]
per-ex loss: 0.489714  [   82/   88]
per-ex loss: 0.591001  [   83/   88]
per-ex loss: 0.583575  [   84/   88]
per-ex loss: 0.648686  [   85/   88]
per-ex loss: 0.642835  [   86/   88]
per-ex loss: 0.516971  [   87/   88]
per-ex loss: 0.458077  [   88/   88]
Train Error: Avg loss: 0.53834096
validation Error: 
 Avg loss: 0.56740442 
 F1: 0.465491 
 Precision: 0.541686 
 Recall: 0.408088
 IoU: 0.303348

test Error: 
 Avg loss: 0.51439794 
 F1: 0.549207 
 Precision: 0.589205 
 Recall: 0.514294
 IoU: 0.378556

We have finished training iteration 14
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_12_.pth
per-ex loss: 0.397933  [    1/   88]
per-ex loss: 0.424173  [    2/   88]
per-ex loss: 0.340347  [    3/   88]
per-ex loss: 0.548970  [    4/   88]
per-ex loss: 0.501103  [    5/   88]
per-ex loss: 0.424310  [    6/   88]
per-ex loss: 0.406494  [    7/   88]
per-ex loss: 0.466539  [    8/   88]
per-ex loss: 0.469694  [    9/   88]
per-ex loss: 0.615465  [   10/   88]
per-ex loss: 0.548338  [   11/   88]
per-ex loss: 0.619406  [   12/   88]
per-ex loss: 0.708217  [   13/   88]
per-ex loss: 0.707279  [   14/   88]
per-ex loss: 0.607583  [   15/   88]
per-ex loss: 0.433834  [   16/   88]
per-ex loss: 0.520350  [   17/   88]
per-ex loss: 0.587221  [   18/   88]
per-ex loss: 0.431809  [   19/   88]
per-ex loss: 0.624919  [   20/   88]
per-ex loss: 0.584137  [   21/   88]
per-ex loss: 0.623154  [   22/   88]
per-ex loss: 0.501213  [   23/   88]
per-ex loss: 0.541396  [   24/   88]
per-ex loss: 0.542374  [   25/   88]
per-ex loss: 0.468706  [   26/   88]
per-ex loss: 0.575289  [   27/   88]
per-ex loss: 0.645086  [   28/   88]
per-ex loss: 0.395015  [   29/   88]
per-ex loss: 0.401746  [   30/   88]
per-ex loss: 0.486851  [   31/   88]
per-ex loss: 0.453560  [   32/   88]
per-ex loss: 0.623206  [   33/   88]
per-ex loss: 0.488603  [   34/   88]
per-ex loss: 0.406721  [   35/   88]
per-ex loss: 0.695035  [   36/   88]
per-ex loss: 0.476242  [   37/   88]
per-ex loss: 0.487497  [   38/   88]
per-ex loss: 0.640414  [   39/   88]
per-ex loss: 0.443930  [   40/   88]
per-ex loss: 0.360672  [   41/   88]
per-ex loss: 0.580625  [   42/   88]
per-ex loss: 0.722592  [   43/   88]
per-ex loss: 0.405887  [   44/   88]
per-ex loss: 0.607114  [   45/   88]
per-ex loss: 0.488646  [   46/   88]
per-ex loss: 0.416505  [   47/   88]
per-ex loss: 0.668454  [   48/   88]
per-ex loss: 0.417135  [   49/   88]
per-ex loss: 0.414202  [   50/   88]
per-ex loss: 0.723053  [   51/   88]
per-ex loss: 0.536234  [   52/   88]
per-ex loss: 0.434959  [   53/   88]
per-ex loss: 0.580813  [   54/   88]
per-ex loss: 0.392943  [   55/   88]
per-ex loss: 0.439099  [   56/   88]
per-ex loss: 0.661815  [   57/   88]
per-ex loss: 0.632574  [   58/   88]
per-ex loss: 0.656230  [   59/   88]
per-ex loss: 0.437527  [   60/   88]
per-ex loss: 0.432397  [   61/   88]
per-ex loss: 0.542656  [   62/   88]
per-ex loss: 0.639448  [   63/   88]
per-ex loss: 0.544965  [   64/   88]
per-ex loss: 0.386381  [   65/   88]
per-ex loss: 0.658985  [   66/   88]
per-ex loss: 0.570561  [   67/   88]
per-ex loss: 0.496952  [   68/   88]
per-ex loss: 0.399297  [   69/   88]
per-ex loss: 0.447169  [   70/   88]
per-ex loss: 0.442517  [   71/   88]
per-ex loss: 0.653849  [   72/   88]
per-ex loss: 0.387295  [   73/   88]
per-ex loss: 0.426226  [   74/   88]
per-ex loss: 0.715166  [   75/   88]
per-ex loss: 0.591999  [   76/   88]
per-ex loss: 0.524222  [   77/   88]
per-ex loss: 0.627815  [   78/   88]
per-ex loss: 0.719320  [   79/   88]
per-ex loss: 0.424134  [   80/   88]
per-ex loss: 0.551095  [   81/   88]
per-ex loss: 0.725850  [   82/   88]
per-ex loss: 0.486754  [   83/   88]
per-ex loss: 0.563785  [   84/   88]
per-ex loss: 0.664204  [   85/   88]
per-ex loss: 0.513909  [   86/   88]
per-ex loss: 0.634978  [   87/   88]
per-ex loss: 0.715721  [   88/   88]
Train Error: Avg loss: 0.53328279
validation Error: 
 Avg loss: 0.62885936 
 F1: 0.387282 
 Precision: 0.298787 
 Recall: 0.550258
 IoU: 0.240142

test Error: 
 Avg loss: 0.55052373 
 F1: 0.499366 
 Precision: 0.400471 
 Recall: 0.663120
 IoU: 0.332770

We have finished training iteration 15
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_10_.pth
per-ex loss: 0.436496  [    1/   88]
per-ex loss: 0.625339  [    2/   88]
per-ex loss: 0.474480  [    3/   88]
per-ex loss: 0.667163  [    4/   88]
per-ex loss: 0.707473  [    5/   88]
per-ex loss: 0.670980  [    6/   88]
per-ex loss: 0.580137  [    7/   88]
per-ex loss: 0.585487  [    8/   88]
per-ex loss: 0.686647  [    9/   88]
per-ex loss: 0.533220  [   10/   88]
per-ex loss: 0.427293  [   11/   88]
per-ex loss: 0.500065  [   12/   88]
per-ex loss: 0.647689  [   13/   88]
per-ex loss: 0.443617  [   14/   88]
per-ex loss: 0.496726  [   15/   88]
per-ex loss: 0.486461  [   16/   88]
per-ex loss: 0.456775  [   17/   88]
per-ex loss: 0.671080  [   18/   88]
per-ex loss: 0.423139  [   19/   88]
per-ex loss: 0.713056  [   20/   88]
per-ex loss: 0.598450  [   21/   88]
per-ex loss: 0.646405  [   22/   88]
per-ex loss: 0.648863  [   23/   88]
per-ex loss: 0.406761  [   24/   88]
per-ex loss: 0.723668  [   25/   88]
per-ex loss: 0.502573  [   26/   88]
per-ex loss: 0.447520  [   27/   88]
per-ex loss: 0.381539  [   28/   88]
per-ex loss: 0.471764  [   29/   88]
per-ex loss: 0.673330  [   30/   88]
per-ex loss: 0.428321  [   31/   88]
per-ex loss: 0.396024  [   32/   88]
per-ex loss: 0.656741  [   33/   88]
per-ex loss: 0.558048  [   34/   88]
per-ex loss: 0.506589  [   35/   88]
per-ex loss: 0.397763  [   36/   88]
per-ex loss: 0.562141  [   37/   88]
per-ex loss: 0.425581  [   38/   88]
per-ex loss: 0.641935  [   39/   88]
per-ex loss: 0.488729  [   40/   88]
per-ex loss: 0.466922  [   41/   88]
per-ex loss: 0.392403  [   42/   88]
per-ex loss: 0.451509  [   43/   88]
per-ex loss: 0.548033  [   44/   88]
per-ex loss: 0.582959  [   45/   88]
per-ex loss: 0.391671  [   46/   88]
per-ex loss: 0.383033  [   47/   88]
per-ex loss: 0.677584  [   48/   88]
per-ex loss: 0.597743  [   49/   88]
per-ex loss: 0.373741  [   50/   88]
per-ex loss: 0.437966  [   51/   88]
per-ex loss: 0.615003  [   52/   88]
per-ex loss: 0.406589  [   53/   88]
per-ex loss: 0.580479  [   54/   88]
per-ex loss: 0.459745  [   55/   88]
per-ex loss: 0.347076  [   56/   88]
per-ex loss: 0.583260  [   57/   88]
per-ex loss: 0.562870  [   58/   88]
per-ex loss: 0.684618  [   59/   88]
per-ex loss: 0.637148  [   60/   88]
per-ex loss: 0.599391  [   61/   88]
per-ex loss: 0.657845  [   62/   88]
per-ex loss: 0.437982  [   63/   88]
per-ex loss: 0.490431  [   64/   88]
per-ex loss: 0.399597  [   65/   88]
per-ex loss: 0.468455  [   66/   88]
per-ex loss: 0.634282  [   67/   88]
per-ex loss: 0.482057  [   68/   88]
per-ex loss: 0.678360  [   69/   88]
per-ex loss: 0.542627  [   70/   88]
per-ex loss: 0.501393  [   71/   88]
per-ex loss: 0.433142  [   72/   88]
per-ex loss: 0.453373  [   73/   88]
per-ex loss: 0.567018  [   74/   88]
per-ex loss: 0.566470  [   75/   88]
per-ex loss: 0.418786  [   76/   88]
per-ex loss: 0.469030  [   77/   88]
per-ex loss: 0.492817  [   78/   88]
per-ex loss: 0.617655  [   79/   88]
per-ex loss: 0.454816  [   80/   88]
per-ex loss: 0.614160  [   81/   88]
per-ex loss: 0.481369  [   82/   88]
per-ex loss: 0.396289  [   83/   88]
per-ex loss: 0.686669  [   84/   88]
per-ex loss: 0.627111  [   85/   88]
per-ex loss: 0.406092  [   86/   88]
per-ex loss: 0.439098  [   87/   88]
per-ex loss: 0.440869  [   88/   88]
Train Error: Avg loss: 0.52765458
validation Error: 
 Avg loss: 0.57036779 
 F1: 0.457165 
 Precision: 0.564887 
 Recall: 0.383947
 IoU: 0.296315

test Error: 
 Avg loss: 0.51103494 
 F1: 0.549144 
 Precision: 0.633120 
 Recall: 0.484836
 IoU: 0.378496

We have finished training iteration 16
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_14_.pth
per-ex loss: 0.664546  [    1/   88]
per-ex loss: 0.582963  [    2/   88]
per-ex loss: 0.637253  [    3/   88]
per-ex loss: 0.412926  [    4/   88]
per-ex loss: 0.711705  [    5/   88]
per-ex loss: 0.405931  [    6/   88]
per-ex loss: 0.475955  [    7/   88]
per-ex loss: 0.448872  [    8/   88]
per-ex loss: 0.434480  [    9/   88]
per-ex loss: 0.430292  [   10/   88]
per-ex loss: 0.475775  [   11/   88]
per-ex loss: 0.612176  [   12/   88]
per-ex loss: 0.457675  [   13/   88]
per-ex loss: 0.538738  [   14/   88]
per-ex loss: 0.613981  [   15/   88]
per-ex loss: 0.711022  [   16/   88]
per-ex loss: 0.636587  [   17/   88]
per-ex loss: 0.612464  [   18/   88]
per-ex loss: 0.436470  [   19/   88]
per-ex loss: 0.732454  [   20/   88]
per-ex loss: 0.429871  [   21/   88]
per-ex loss: 0.689754  [   22/   88]
per-ex loss: 0.596740  [   23/   88]
per-ex loss: 0.492974  [   24/   88]
per-ex loss: 0.491344  [   25/   88]
per-ex loss: 0.613289  [   26/   88]
per-ex loss: 0.412379  [   27/   88]
per-ex loss: 0.518207  [   28/   88]
per-ex loss: 0.436979  [   29/   88]
per-ex loss: 0.448559  [   30/   88]
per-ex loss: 0.484798  [   31/   88]
per-ex loss: 0.416867  [   32/   88]
per-ex loss: 0.659183  [   33/   88]
per-ex loss: 0.727032  [   34/   88]
per-ex loss: 0.368510  [   35/   88]
per-ex loss: 0.452046  [   36/   88]
per-ex loss: 0.505582  [   37/   88]
per-ex loss: 0.475564  [   38/   88]
per-ex loss: 0.576366  [   39/   88]
per-ex loss: 0.651861  [   40/   88]
per-ex loss: 0.396042  [   41/   88]
per-ex loss: 0.698707  [   42/   88]
per-ex loss: 0.685067  [   43/   88]
per-ex loss: 0.660247  [   44/   88]
per-ex loss: 0.719450  [   45/   88]
per-ex loss: 0.485594  [   46/   88]
per-ex loss: 0.472903  [   47/   88]
per-ex loss: 0.643108  [   48/   88]
per-ex loss: 0.562110  [   49/   88]
per-ex loss: 0.603506  [   50/   88]
per-ex loss: 0.563384  [   51/   88]
per-ex loss: 0.606342  [   52/   88]
per-ex loss: 0.451078  [   53/   88]
per-ex loss: 0.428665  [   54/   88]
per-ex loss: 0.705591  [   55/   88]
per-ex loss: 0.402491  [   56/   88]
per-ex loss: 0.420529  [   57/   88]
per-ex loss: 0.436770  [   58/   88]
per-ex loss: 0.574755  [   59/   88]
per-ex loss: 0.636770  [   60/   88]
per-ex loss: 0.421232  [   61/   88]
per-ex loss: 0.430534  [   62/   88]
per-ex loss: 0.403801  [   63/   88]
per-ex loss: 0.548239  [   64/   88]
per-ex loss: 0.570063  [   65/   88]
per-ex loss: 0.468896  [   66/   88]
per-ex loss: 0.523586  [   67/   88]
per-ex loss: 0.402029  [   68/   88]
per-ex loss: 0.407535  [   69/   88]
per-ex loss: 0.591002  [   70/   88]
per-ex loss: 0.621381  [   71/   88]
per-ex loss: 0.661464  [   72/   88]
per-ex loss: 0.571112  [   73/   88]
per-ex loss: 0.352993  [   74/   88]
per-ex loss: 0.400399  [   75/   88]
per-ex loss: 0.663698  [   76/   88]
per-ex loss: 0.444112  [   77/   88]
per-ex loss: 0.361933  [   78/   88]
per-ex loss: 0.595970  [   79/   88]
per-ex loss: 0.639737  [   80/   88]
per-ex loss: 0.445481  [   81/   88]
per-ex loss: 0.407064  [   82/   88]
per-ex loss: 0.464710  [   83/   88]
per-ex loss: 0.581565  [   84/   88]
per-ex loss: 0.497561  [   85/   88]
per-ex loss: 0.386845  [   86/   88]
per-ex loss: 0.685085  [   87/   88]
per-ex loss: 0.616958  [   88/   88]
Train Error: Avg loss: 0.53179842
validation Error: 
 Avg loss: 0.58329562 
 F1: 0.438264 
 Precision: 0.426513 
 Recall: 0.450681
 IoU: 0.280626

test Error: 
 Avg loss: 0.51186391 
 F1: 0.541678 
 Precision: 0.535090 
 Recall: 0.548431
 IoU: 0.371439

We have finished training iteration 17
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_15_.pth
per-ex loss: 0.392727  [    1/   88]
per-ex loss: 0.395124  [    2/   88]
per-ex loss: 0.428876  [    3/   88]
per-ex loss: 0.666752  [    4/   88]
per-ex loss: 0.668161  [    5/   88]
per-ex loss: 0.575889  [    6/   88]
per-ex loss: 0.375582  [    7/   88]
per-ex loss: 0.436812  [    8/   88]
per-ex loss: 0.604877  [    9/   88]
per-ex loss: 0.408035  [   10/   88]
per-ex loss: 0.607946  [   11/   88]
per-ex loss: 0.453446  [   12/   88]
per-ex loss: 0.646730  [   13/   88]
per-ex loss: 0.588533  [   14/   88]
per-ex loss: 0.565586  [   15/   88]
per-ex loss: 0.378720  [   16/   88]
per-ex loss: 0.637542  [   17/   88]
per-ex loss: 0.572591  [   18/   88]
per-ex loss: 0.684666  [   19/   88]
per-ex loss: 0.559042  [   20/   88]
per-ex loss: 0.412323  [   21/   88]
per-ex loss: 0.476572  [   22/   88]
per-ex loss: 0.567531  [   23/   88]
per-ex loss: 0.484106  [   24/   88]
per-ex loss: 0.488045  [   25/   88]
per-ex loss: 0.622357  [   26/   88]
per-ex loss: 0.385522  [   27/   88]
per-ex loss: 0.510129  [   28/   88]
per-ex loss: 0.467888  [   29/   88]
per-ex loss: 0.631365  [   30/   88]
per-ex loss: 0.358313  [   31/   88]
per-ex loss: 0.433150  [   32/   88]
per-ex loss: 0.442402  [   33/   88]
per-ex loss: 0.444584  [   34/   88]
per-ex loss: 0.484839  [   35/   88]
per-ex loss: 0.418784  [   36/   88]
per-ex loss: 0.485758  [   37/   88]
per-ex loss: 0.479996  [   38/   88]
per-ex loss: 0.588183  [   39/   88]
per-ex loss: 0.576226  [   40/   88]
per-ex loss: 0.661204  [   41/   88]
per-ex loss: 0.416376  [   42/   88]
per-ex loss: 0.392978  [   43/   88]
per-ex loss: 0.421793  [   44/   88]
per-ex loss: 0.443118  [   45/   88]
per-ex loss: 0.487442  [   46/   88]
per-ex loss: 0.690687  [   47/   88]
per-ex loss: 0.487391  [   48/   88]
per-ex loss: 0.469716  [   49/   88]
per-ex loss: 0.634052  [   50/   88]
per-ex loss: 0.732262  [   51/   88]
per-ex loss: 0.575835  [   52/   88]
per-ex loss: 0.430501  [   53/   88]
per-ex loss: 0.509361  [   54/   88]
per-ex loss: 0.359526  [   55/   88]
per-ex loss: 0.610137  [   56/   88]
per-ex loss: 0.695045  [   57/   88]
per-ex loss: 0.561932  [   58/   88]
per-ex loss: 0.425287  [   59/   88]
per-ex loss: 0.437953  [   60/   88]
per-ex loss: 0.545171  [   61/   88]
per-ex loss: 0.400319  [   62/   88]
per-ex loss: 0.392039  [   63/   88]
per-ex loss: 0.423276  [   64/   88]
per-ex loss: 0.486454  [   65/   88]
per-ex loss: 0.411128  [   66/   88]
per-ex loss: 0.586337  [   67/   88]
per-ex loss: 0.521124  [   68/   88]
per-ex loss: 0.694561  [   69/   88]
per-ex loss: 0.625362  [   70/   88]
per-ex loss: 0.687242  [   71/   88]
per-ex loss: 0.480317  [   72/   88]
per-ex loss: 0.473392  [   73/   88]
per-ex loss: 0.432668  [   74/   88]
per-ex loss: 0.673366  [   75/   88]
per-ex loss: 0.496412  [   76/   88]
per-ex loss: 0.576197  [   77/   88]
per-ex loss: 0.397791  [   78/   88]
per-ex loss: 0.721504  [   79/   88]
per-ex loss: 0.594989  [   80/   88]
per-ex loss: 0.687241  [   81/   88]
per-ex loss: 0.401873  [   82/   88]
per-ex loss: 0.712328  [   83/   88]
per-ex loss: 0.719038  [   84/   88]
per-ex loss: 0.560710  [   85/   88]
per-ex loss: 0.600987  [   86/   88]
per-ex loss: 0.516920  [   87/   88]
per-ex loss: 0.572165  [   88/   88]
Train Error: Avg loss: 0.52544566
validation Error: 
 Avg loss: 0.57809484 
 F1: 0.449153 
 Precision: 0.415024 
 Recall: 0.489398
 IoU: 0.289618

test Error: 
 Avg loss: 0.51312323 
 F1: 0.541531 
 Precision: 0.513194 
 Recall: 0.573182
 IoU: 0.371301

We have finished training iteration 18
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_16_.pth
per-ex loss: 0.621574  [    1/   88]
per-ex loss: 0.468271  [    2/   88]
per-ex loss: 0.423613  [    3/   88]
per-ex loss: 0.649690  [    4/   88]
per-ex loss: 0.443947  [    5/   88]
per-ex loss: 0.704279  [    6/   88]
per-ex loss: 0.582513  [    7/   88]
per-ex loss: 0.575808  [    8/   88]
per-ex loss: 0.463300  [    9/   88]
per-ex loss: 0.425416  [   10/   88]
per-ex loss: 0.537008  [   11/   88]
per-ex loss: 0.685672  [   12/   88]
per-ex loss: 0.602125  [   13/   88]
per-ex loss: 0.432712  [   14/   88]
per-ex loss: 0.413187  [   15/   88]
per-ex loss: 0.509801  [   16/   88]
per-ex loss: 0.413047  [   17/   88]
per-ex loss: 0.425501  [   18/   88]
per-ex loss: 0.560391  [   19/   88]
per-ex loss: 0.430236  [   20/   88]
per-ex loss: 0.705049  [   21/   88]
per-ex loss: 0.640554  [   22/   88]
per-ex loss: 0.679443  [   23/   88]
per-ex loss: 0.479641  [   24/   88]
per-ex loss: 0.498326  [   25/   88]
per-ex loss: 0.599780  [   26/   88]
per-ex loss: 0.717449  [   27/   88]
per-ex loss: 0.475859  [   28/   88]
per-ex loss: 0.646956  [   29/   88]
per-ex loss: 0.474083  [   30/   88]
per-ex loss: 0.381231  [   31/   88]
per-ex loss: 0.619069  [   32/   88]
per-ex loss: 0.639615  [   33/   88]
per-ex loss: 0.476185  [   34/   88]
per-ex loss: 0.667906  [   35/   88]
per-ex loss: 0.731620  [   36/   88]
per-ex loss: 0.642442  [   37/   88]
per-ex loss: 0.379175  [   38/   88]
per-ex loss: 0.593395  [   39/   88]
per-ex loss: 0.461299  [   40/   88]
per-ex loss: 0.609330  [   41/   88]
per-ex loss: 0.437436  [   42/   88]
per-ex loss: 0.653796  [   43/   88]
per-ex loss: 0.411920  [   44/   88]
per-ex loss: 0.410387  [   45/   88]
per-ex loss: 0.428673  [   46/   88]
per-ex loss: 0.420980  [   47/   88]
per-ex loss: 0.665680  [   48/   88]
per-ex loss: 0.633465  [   49/   88]
per-ex loss: 0.705542  [   50/   88]
per-ex loss: 0.381693  [   51/   88]
per-ex loss: 0.488903  [   52/   88]
per-ex loss: 0.494770  [   53/   88]
per-ex loss: 0.492578  [   54/   88]
per-ex loss: 0.696867  [   55/   88]
per-ex loss: 0.431843  [   56/   88]
per-ex loss: 0.539337  [   57/   88]
per-ex loss: 0.349169  [   58/   88]
per-ex loss: 0.401747  [   59/   88]
per-ex loss: 0.436400  [   60/   88]
per-ex loss: 0.625749  [   61/   88]
per-ex loss: 0.482270  [   62/   88]
per-ex loss: 0.448261  [   63/   88]
per-ex loss: 0.387431  [   64/   88]
per-ex loss: 0.553176  [   65/   88]
per-ex loss: 0.564804  [   66/   88]
per-ex loss: 0.424273  [   67/   88]
per-ex loss: 0.414438  [   68/   88]
per-ex loss: 0.593340  [   69/   88]
per-ex loss: 0.417762  [   70/   88]
per-ex loss: 0.524914  [   71/   88]
per-ex loss: 0.402864  [   72/   88]
per-ex loss: 0.685100  [   73/   88]
per-ex loss: 0.627741  [   74/   88]
per-ex loss: 0.411614  [   75/   88]
per-ex loss: 0.600954  [   76/   88]
per-ex loss: 0.588656  [   77/   88]
per-ex loss: 0.581614  [   78/   88]
per-ex loss: 0.452799  [   79/   88]
per-ex loss: 0.586553  [   80/   88]
per-ex loss: 0.429393  [   81/   88]
per-ex loss: 0.465636  [   82/   88]
per-ex loss: 0.434453  [   83/   88]
per-ex loss: 0.449358  [   84/   88]
per-ex loss: 0.609607  [   85/   88]
per-ex loss: 0.463410  [   86/   88]
per-ex loss: 0.577943  [   87/   88]
per-ex loss: 0.520157  [   88/   88]
Train Error: Avg loss: 0.52604492
validation Error: 
 Avg loss: 0.56129624 
 F1: 0.478595 
 Precision: 0.471296 
 Recall: 0.486124
 IoU: 0.314575

test Error: 
 Avg loss: 0.51435847 
 F1: 0.547454 
 Precision: 0.503138 
 Recall: 0.600330
 IoU: 0.376893

We have finished training iteration 19
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_17_.pth
per-ex loss: 0.597033  [    1/   88]
per-ex loss: 0.585068  [    2/   88]
per-ex loss: 0.409473  [    3/   88]
per-ex loss: 0.350829  [    4/   88]
per-ex loss: 0.578086  [    5/   88]
per-ex loss: 0.376784  [    6/   88]
per-ex loss: 0.534050  [    7/   88]
per-ex loss: 0.457894  [    8/   88]
per-ex loss: 0.439334  [    9/   88]
per-ex loss: 0.434529  [   10/   88]
per-ex loss: 0.361483  [   11/   88]
per-ex loss: 0.707129  [   12/   88]
per-ex loss: 0.447300  [   13/   88]
per-ex loss: 0.650521  [   14/   88]
per-ex loss: 0.713621  [   15/   88]
per-ex loss: 0.391135  [   16/   88]
per-ex loss: 0.585528  [   17/   88]
per-ex loss: 0.366330  [   18/   88]
per-ex loss: 0.672371  [   19/   88]
per-ex loss: 0.487627  [   20/   88]
per-ex loss: 0.621191  [   21/   88]
per-ex loss: 0.399768  [   22/   88]
per-ex loss: 0.491412  [   23/   88]
per-ex loss: 0.612686  [   24/   88]
per-ex loss: 0.430925  [   25/   88]
per-ex loss: 0.651695  [   26/   88]
per-ex loss: 0.586284  [   27/   88]
per-ex loss: 0.470076  [   28/   88]
per-ex loss: 0.434661  [   29/   88]
per-ex loss: 0.725686  [   30/   88]
per-ex loss: 0.428363  [   31/   88]
per-ex loss: 0.406080  [   32/   88]
per-ex loss: 0.406891  [   33/   88]
per-ex loss: 0.477380  [   34/   88]
per-ex loss: 0.654835  [   35/   88]
per-ex loss: 0.504831  [   36/   88]
per-ex loss: 0.662856  [   37/   88]
per-ex loss: 0.532141  [   38/   88]
per-ex loss: 0.698727  [   39/   88]
per-ex loss: 0.596439  [   40/   88]
per-ex loss: 0.590876  [   41/   88]
per-ex loss: 0.673203  [   42/   88]
per-ex loss: 0.521129  [   43/   88]
per-ex loss: 0.580245  [   44/   88]
per-ex loss: 0.702453  [   45/   88]
per-ex loss: 0.408673  [   46/   88]
per-ex loss: 0.646426  [   47/   88]
per-ex loss: 0.394015  [   48/   88]
per-ex loss: 0.464800  [   49/   88]
per-ex loss: 0.433782  [   50/   88]
per-ex loss: 0.482195  [   51/   88]
per-ex loss: 0.682749  [   52/   88]
per-ex loss: 0.403920  [   53/   88]
per-ex loss: 0.485890  [   54/   88]
per-ex loss: 0.548103  [   55/   88]
per-ex loss: 0.687874  [   56/   88]
per-ex loss: 0.576407  [   57/   88]
per-ex loss: 0.662046  [   58/   88]
per-ex loss: 0.449845  [   59/   88]
per-ex loss: 0.461862  [   60/   88]
per-ex loss: 0.392287  [   61/   88]
per-ex loss: 0.649765  [   62/   88]
per-ex loss: 0.391472  [   63/   88]
per-ex loss: 0.482982  [   64/   88]
per-ex loss: 0.652782  [   65/   88]
per-ex loss: 0.402793  [   66/   88]
per-ex loss: 0.391427  [   67/   88]
per-ex loss: 0.423942  [   68/   88]
per-ex loss: 0.486049  [   69/   88]
per-ex loss: 0.471564  [   70/   88]
per-ex loss: 0.626806  [   71/   88]
per-ex loss: 0.518398  [   72/   88]
per-ex loss: 0.474314  [   73/   88]
per-ex loss: 0.580780  [   74/   88]
per-ex loss: 0.524403  [   75/   88]
per-ex loss: 0.618771  [   76/   88]
per-ex loss: 0.380978  [   77/   88]
per-ex loss: 0.543887  [   78/   88]
per-ex loss: 0.416934  [   79/   88]
per-ex loss: 0.468108  [   80/   88]
per-ex loss: 0.569418  [   81/   88]
per-ex loss: 0.545626  [   82/   88]
per-ex loss: 0.374187  [   83/   88]
per-ex loss: 0.425693  [   84/   88]
per-ex loss: 0.396800  [   85/   88]
per-ex loss: 0.580020  [   86/   88]
per-ex loss: 0.534262  [   87/   88]
per-ex loss: 0.556800  [   88/   88]
Train Error: Avg loss: 0.51905449
validation Error: 
 Avg loss: 0.55618813 
 F1: 0.476650 
 Precision: 0.478899 
 Recall: 0.474422
 IoU: 0.312896

test Error: 
 Avg loss: 0.51145224 
 F1: 0.546931 
 Precision: 0.519360 
 Recall: 0.577593
 IoU: 0.376397

We have finished training iteration 20
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_18_.pth
per-ex loss: 0.571122  [    1/   88]
per-ex loss: 0.593069  [    2/   88]
per-ex loss: 0.453484  [    3/   88]
per-ex loss: 0.390710  [    4/   88]
per-ex loss: 0.569788  [    5/   88]
per-ex loss: 0.641265  [    6/   88]
per-ex loss: 0.559100  [    7/   88]
per-ex loss: 0.716715  [    8/   88]
per-ex loss: 0.564441  [    9/   88]
per-ex loss: 0.583606  [   10/   88]
per-ex loss: 0.494209  [   11/   88]
per-ex loss: 0.420514  [   12/   88]
per-ex loss: 0.583153  [   13/   88]
per-ex loss: 0.508065  [   14/   88]
per-ex loss: 0.376522  [   15/   88]
per-ex loss: 0.464120  [   16/   88]
per-ex loss: 0.470271  [   17/   88]
per-ex loss: 0.715327  [   18/   88]
per-ex loss: 0.371474  [   19/   88]
per-ex loss: 0.563955  [   20/   88]
per-ex loss: 0.628576  [   21/   88]
per-ex loss: 0.430703  [   22/   88]
per-ex loss: 0.438688  [   23/   88]
per-ex loss: 0.478604  [   24/   88]
per-ex loss: 0.379142  [   25/   88]
per-ex loss: 0.803736  [   26/   88]
per-ex loss: 0.467319  [   27/   88]
per-ex loss: 0.584069  [   28/   88]
per-ex loss: 0.716673  [   29/   88]
per-ex loss: 0.597633  [   30/   88]
per-ex loss: 0.472704  [   31/   88]
per-ex loss: 0.482461  [   32/   88]
per-ex loss: 0.467032  [   33/   88]
per-ex loss: 0.376210  [   34/   88]
per-ex loss: 0.697784  [   35/   88]
per-ex loss: 0.401737  [   36/   88]
per-ex loss: 0.590320  [   37/   88]
per-ex loss: 0.464146  [   38/   88]
per-ex loss: 0.530590  [   39/   88]
per-ex loss: 0.469916  [   40/   88]
per-ex loss: 0.563872  [   41/   88]
per-ex loss: 0.432479  [   42/   88]
per-ex loss: 0.644621  [   43/   88]
per-ex loss: 0.472443  [   44/   88]
per-ex loss: 0.558485  [   45/   88]
per-ex loss: 0.463856  [   46/   88]
per-ex loss: 0.377640  [   47/   88]
per-ex loss: 0.374769  [   48/   88]
per-ex loss: 0.434178  [   49/   88]
per-ex loss: 0.453325  [   50/   88]
per-ex loss: 0.577017  [   51/   88]
per-ex loss: 0.425983  [   52/   88]
per-ex loss: 0.634461  [   53/   88]
per-ex loss: 0.436942  [   54/   88]
per-ex loss: 0.363869  [   55/   88]
per-ex loss: 0.686269  [   56/   88]
per-ex loss: 0.675467  [   57/   88]
per-ex loss: 0.366742  [   58/   88]
per-ex loss: 0.400672  [   59/   88]
per-ex loss: 0.742222  [   60/   88]
per-ex loss: 0.339697  [   61/   88]
per-ex loss: 0.486052  [   62/   88]
per-ex loss: 0.576336  [   63/   88]
per-ex loss: 0.438235  [   64/   88]
per-ex loss: 0.452727  [   65/   88]
per-ex loss: 0.483675  [   66/   88]
per-ex loss: 0.419584  [   67/   88]
per-ex loss: 0.587892  [   68/   88]
per-ex loss: 0.431602  [   69/   88]
per-ex loss: 0.543672  [   70/   88]
per-ex loss: 0.616260  [   71/   88]
per-ex loss: 0.432234  [   72/   88]
per-ex loss: 0.636072  [   73/   88]
per-ex loss: 0.454459  [   74/   88]
per-ex loss: 0.633932  [   75/   88]
per-ex loss: 0.683760  [   76/   88]
per-ex loss: 0.650454  [   77/   88]
per-ex loss: 0.407501  [   78/   88]
per-ex loss: 0.618774  [   79/   88]
per-ex loss: 0.620356  [   80/   88]
per-ex loss: 0.554811  [   81/   88]
per-ex loss: 0.692526  [   82/   88]
per-ex loss: 0.672138  [   83/   88]
per-ex loss: 0.464414  [   84/   88]
per-ex loss: 0.523090  [   85/   88]
per-ex loss: 0.388467  [   86/   88]
per-ex loss: 0.606674  [   87/   88]
per-ex loss: 0.403379  [   88/   88]
Train Error: Avg loss: 0.52264821
validation Error: 
 Avg loss: 0.55057778 
 F1: 0.480598 
 Precision: 0.523956 
 Recall: 0.443868
 IoU: 0.316307

test Error: 
 Avg loss: 0.50707106 
 F1: 0.550931 
 Precision: 0.544143 
 Recall: 0.557891
 IoU: 0.380197

We have finished training iteration 21
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_11_.pth
per-ex loss: 0.652952  [    1/   88]
per-ex loss: 0.447962  [    2/   88]
per-ex loss: 0.575451  [    3/   88]
per-ex loss: 0.700309  [    4/   88]
per-ex loss: 0.434911  [    5/   88]
per-ex loss: 0.555359  [    6/   88]
per-ex loss: 0.664306  [    7/   88]
per-ex loss: 0.477760  [    8/   88]
per-ex loss: 0.386147  [    9/   88]
per-ex loss: 0.440976  [   10/   88]
per-ex loss: 0.582675  [   11/   88]
per-ex loss: 0.506427  [   12/   88]
per-ex loss: 0.415385  [   13/   88]
per-ex loss: 0.536341  [   14/   88]
per-ex loss: 0.535310  [   15/   88]
per-ex loss: 0.417140  [   16/   88]
per-ex loss: 0.618274  [   17/   88]
per-ex loss: 0.359965  [   18/   88]
per-ex loss: 0.585564  [   19/   88]
per-ex loss: 0.690552  [   20/   88]
per-ex loss: 0.379676  [   21/   88]
per-ex loss: 0.338577  [   22/   88]
per-ex loss: 0.390547  [   23/   88]
per-ex loss: 0.407865  [   24/   88]
per-ex loss: 0.529841  [   25/   88]
per-ex loss: 0.377929  [   26/   88]
per-ex loss: 0.726926  [   27/   88]
per-ex loss: 0.485458  [   28/   88]
per-ex loss: 0.611993  [   29/   88]
per-ex loss: 0.380104  [   30/   88]
per-ex loss: 0.427734  [   31/   88]
per-ex loss: 0.583623  [   32/   88]
per-ex loss: 0.364349  [   33/   88]
per-ex loss: 0.383165  [   34/   88]
per-ex loss: 0.391321  [   35/   88]
per-ex loss: 0.449016  [   36/   88]
per-ex loss: 0.537511  [   37/   88]
per-ex loss: 0.400365  [   38/   88]
per-ex loss: 0.609150  [   39/   88]
per-ex loss: 0.431090  [   40/   88]
per-ex loss: 0.449774  [   41/   88]
per-ex loss: 0.627158  [   42/   88]
per-ex loss: 0.654124  [   43/   88]
per-ex loss: 0.570614  [   44/   88]
per-ex loss: 0.545227  [   45/   88]
per-ex loss: 0.644280  [   46/   88]
per-ex loss: 0.414424  [   47/   88]
per-ex loss: 0.544652  [   48/   88]
per-ex loss: 0.462155  [   49/   88]
per-ex loss: 0.547342  [   50/   88]
per-ex loss: 0.429662  [   51/   88]
per-ex loss: 0.393987  [   52/   88]
per-ex loss: 0.696781  [   53/   88]
per-ex loss: 0.595169  [   54/   88]
per-ex loss: 0.565974  [   55/   88]
per-ex loss: 0.611249  [   56/   88]
per-ex loss: 0.416654  [   57/   88]
per-ex loss: 0.399088  [   58/   88]
per-ex loss: 0.570862  [   59/   88]
per-ex loss: 0.724349  [   60/   88]
per-ex loss: 0.584359  [   61/   88]
per-ex loss: 0.707121  [   62/   88]
per-ex loss: 0.483913  [   63/   88]
per-ex loss: 0.390235  [   64/   88]
per-ex loss: 0.449042  [   65/   88]
per-ex loss: 0.681224  [   66/   88]
per-ex loss: 0.402232  [   67/   88]
per-ex loss: 0.661443  [   68/   88]
per-ex loss: 0.458480  [   69/   88]
per-ex loss: 0.607043  [   70/   88]
per-ex loss: 0.426601  [   71/   88]
per-ex loss: 0.457263  [   72/   88]
per-ex loss: 0.660840  [   73/   88]
per-ex loss: 0.588082  [   74/   88]
per-ex loss: 0.445942  [   75/   88]
per-ex loss: 0.471320  [   76/   88]
per-ex loss: 0.647853  [   77/   88]
per-ex loss: 0.628751  [   78/   88]
per-ex loss: 0.642478  [   79/   88]
per-ex loss: 0.509025  [   80/   88]
per-ex loss: 0.484254  [   81/   88]
per-ex loss: 0.436063  [   82/   88]
per-ex loss: 0.425568  [   83/   88]
per-ex loss: 0.664139  [   84/   88]
per-ex loss: 0.414327  [   85/   88]
per-ex loss: 0.642709  [   86/   88]
per-ex loss: 0.741777  [   87/   88]
per-ex loss: 0.579010  [   88/   88]
Train Error: Avg loss: 0.52209795
validation Error: 
 Avg loss: 0.56794484 
 F1: 0.465519 
 Precision: 0.648032 
 Recall: 0.363221
 IoU: 0.303372

test Error: 
 Avg loss: 0.52020187 
 F1: 0.539821 
 Precision: 0.683838 
 Recall: 0.445911
 IoU: 0.369695

We have finished training iteration 22
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_13_.pth
per-ex loss: 0.483098  [    1/   88]
per-ex loss: 0.514984  [    2/   88]
per-ex loss: 0.466411  [    3/   88]
per-ex loss: 0.455717  [    4/   88]
per-ex loss: 0.554107  [    5/   88]
per-ex loss: 0.560161  [    6/   88]
per-ex loss: 0.402420  [    7/   88]
per-ex loss: 0.393568  [    8/   88]
per-ex loss: 0.640517  [    9/   88]
per-ex loss: 0.462610  [   10/   88]
per-ex loss: 0.600340  [   11/   88]
per-ex loss: 0.574324  [   12/   88]
per-ex loss: 0.364953  [   13/   88]
per-ex loss: 0.538223  [   14/   88]
per-ex loss: 0.717384  [   15/   88]
per-ex loss: 0.584666  [   16/   88]
per-ex loss: 0.678520  [   17/   88]
per-ex loss: 0.488413  [   18/   88]
per-ex loss: 0.471631  [   19/   88]
per-ex loss: 0.435416  [   20/   88]
per-ex loss: 0.469112  [   21/   88]
per-ex loss: 0.540955  [   22/   88]
per-ex loss: 0.391197  [   23/   88]
per-ex loss: 0.623580  [   24/   88]
per-ex loss: 0.439027  [   25/   88]
per-ex loss: 0.608044  [   26/   88]
per-ex loss: 0.428154  [   27/   88]
per-ex loss: 0.377577  [   28/   88]
per-ex loss: 0.693277  [   29/   88]
per-ex loss: 0.409558  [   30/   88]
per-ex loss: 0.649584  [   31/   88]
per-ex loss: 0.342230  [   32/   88]
per-ex loss: 0.657156  [   33/   88]
per-ex loss: 0.444152  [   34/   88]
per-ex loss: 0.526380  [   35/   88]
per-ex loss: 0.388184  [   36/   88]
per-ex loss: 0.434689  [   37/   88]
per-ex loss: 0.640546  [   38/   88]
per-ex loss: 0.567604  [   39/   88]
per-ex loss: 0.551901  [   40/   88]
per-ex loss: 0.464754  [   41/   88]
per-ex loss: 0.634620  [   42/   88]
per-ex loss: 0.403680  [   43/   88]
per-ex loss: 0.610097  [   44/   88]
per-ex loss: 0.664573  [   45/   88]
per-ex loss: 0.531524  [   46/   88]
per-ex loss: 0.629434  [   47/   88]
per-ex loss: 0.427001  [   48/   88]
per-ex loss: 0.424174  [   49/   88]
per-ex loss: 0.549119  [   50/   88]
per-ex loss: 0.604977  [   51/   88]
per-ex loss: 0.587434  [   52/   88]
per-ex loss: 0.423735  [   53/   88]
per-ex loss: 0.555280  [   54/   88]
per-ex loss: 0.423780  [   55/   88]
per-ex loss: 0.689926  [   56/   88]
per-ex loss: 0.564639  [   57/   88]
per-ex loss: 0.484136  [   58/   88]
per-ex loss: 0.565252  [   59/   88]
per-ex loss: 0.440322  [   60/   88]
per-ex loss: 0.491984  [   61/   88]
per-ex loss: 0.441385  [   62/   88]
per-ex loss: 0.537799  [   63/   88]
per-ex loss: 0.561520  [   64/   88]
per-ex loss: 0.426799  [   65/   88]
per-ex loss: 0.419998  [   66/   88]
per-ex loss: 0.634946  [   67/   88]
per-ex loss: 0.359638  [   68/   88]
per-ex loss: 0.461675  [   69/   88]
per-ex loss: 0.419190  [   70/   88]
per-ex loss: 0.357635  [   71/   88]
per-ex loss: 0.682195  [   72/   88]
per-ex loss: 0.512085  [   73/   88]
per-ex loss: 0.691424  [   74/   88]
per-ex loss: 0.358450  [   75/   88]
per-ex loss: 0.425783  [   76/   88]
per-ex loss: 0.388603  [   77/   88]
per-ex loss: 0.715982  [   78/   88]
per-ex loss: 0.387498  [   79/   88]
per-ex loss: 0.728110  [   80/   88]
per-ex loss: 0.600545  [   81/   88]
per-ex loss: 0.421747  [   82/   88]
per-ex loss: 0.385849  [   83/   88]
per-ex loss: 0.465564  [   84/   88]
per-ex loss: 0.468335  [   85/   88]
per-ex loss: 0.688149  [   86/   88]
per-ex loss: 0.725450  [   87/   88]
per-ex loss: 0.475440  [   88/   88]
Train Error: Avg loss: 0.51684778
validation Error: 
 Avg loss: 0.59752338 
 F1: 0.426625 
 Precision: 0.355989 
 Recall: 0.532231
 IoU: 0.271152

test Error: 
 Avg loss: 0.53392841 
 F1: 0.515237 
 Precision: 0.434484 
 Recall: 0.632860
 IoU: 0.347016

We have finished training iteration 23
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_19_.pth
per-ex loss: 0.560671  [    1/   88]
per-ex loss: 0.348400  [    2/   88]
per-ex loss: 0.685475  [    3/   88]
per-ex loss: 0.503289  [    4/   88]
per-ex loss: 0.439654  [    5/   88]
per-ex loss: 0.605879  [    6/   88]
per-ex loss: 0.395350  [    7/   88]
per-ex loss: 0.639268  [    8/   88]
per-ex loss: 0.679795  [    9/   88]
per-ex loss: 0.427841  [   10/   88]
per-ex loss: 0.507100  [   11/   88]
per-ex loss: 0.410492  [   12/   88]
per-ex loss: 0.428876  [   13/   88]
per-ex loss: 0.443961  [   14/   88]
per-ex loss: 0.477578  [   15/   88]
per-ex loss: 0.423831  [   16/   88]
per-ex loss: 0.408603  [   17/   88]
per-ex loss: 0.615487  [   18/   88]
per-ex loss: 0.609953  [   19/   88]
per-ex loss: 0.484449  [   20/   88]
per-ex loss: 0.357659  [   21/   88]
per-ex loss: 0.365076  [   22/   88]
per-ex loss: 0.430252  [   23/   88]
per-ex loss: 0.686028  [   24/   88]
per-ex loss: 0.409611  [   25/   88]
per-ex loss: 0.652272  [   26/   88]
per-ex loss: 0.436117  [   27/   88]
per-ex loss: 0.421625  [   28/   88]
per-ex loss: 0.572204  [   29/   88]
per-ex loss: 0.714763  [   30/   88]
per-ex loss: 0.608446  [   31/   88]
per-ex loss: 0.433964  [   32/   88]
per-ex loss: 0.382743  [   33/   88]
per-ex loss: 0.576059  [   34/   88]
per-ex loss: 0.601229  [   35/   88]
per-ex loss: 0.652356  [   36/   88]
per-ex loss: 0.601090  [   37/   88]
per-ex loss: 0.574800  [   38/   88]
per-ex loss: 0.594547  [   39/   88]
per-ex loss: 0.341395  [   40/   88]
per-ex loss: 0.644536  [   41/   88]
per-ex loss: 0.524849  [   42/   88]
per-ex loss: 0.473743  [   43/   88]
per-ex loss: 0.545551  [   44/   88]
per-ex loss: 0.575756  [   45/   88]
per-ex loss: 0.454103  [   46/   88]
per-ex loss: 0.363842  [   47/   88]
per-ex loss: 0.399890  [   48/   88]
per-ex loss: 0.648127  [   49/   88]
per-ex loss: 0.555767  [   50/   88]
per-ex loss: 0.600763  [   51/   88]
per-ex loss: 0.406523  [   52/   88]
per-ex loss: 0.417972  [   53/   88]
per-ex loss: 0.625871  [   54/   88]
per-ex loss: 0.422898  [   55/   88]
per-ex loss: 0.474809  [   56/   88]
per-ex loss: 0.414038  [   57/   88]
per-ex loss: 0.477095  [   58/   88]
per-ex loss: 0.474757  [   59/   88]
per-ex loss: 0.444222  [   60/   88]
per-ex loss: 0.457850  [   61/   88]
per-ex loss: 0.710259  [   62/   88]
per-ex loss: 0.440261  [   63/   88]
per-ex loss: 0.375106  [   64/   88]
per-ex loss: 0.468193  [   65/   88]
per-ex loss: 0.676239  [   66/   88]
per-ex loss: 0.429773  [   67/   88]
per-ex loss: 0.658065  [   68/   88]
per-ex loss: 0.604397  [   69/   88]
per-ex loss: 0.516585  [   70/   88]
per-ex loss: 0.704175  [   71/   88]
per-ex loss: 0.561015  [   72/   88]
per-ex loss: 0.409232  [   73/   88]
per-ex loss: 0.397138  [   74/   88]
per-ex loss: 0.500806  [   75/   88]
per-ex loss: 0.401951  [   76/   88]
per-ex loss: 0.510337  [   77/   88]
per-ex loss: 0.708900  [   78/   88]
per-ex loss: 0.454528  [   79/   88]
per-ex loss: 0.569090  [   80/   88]
per-ex loss: 0.499160  [   81/   88]
per-ex loss: 0.617613  [   82/   88]
per-ex loss: 0.550396  [   83/   88]
per-ex loss: 0.566884  [   84/   88]
per-ex loss: 0.642128  [   85/   88]
per-ex loss: 0.509195  [   86/   88]
per-ex loss: 0.552020  [   87/   88]
per-ex loss: 0.592382  [   88/   88]
Train Error: Avg loss: 0.51746564
validation Error: 
 Avg loss: 0.55130724 
 F1: 0.472215 
 Precision: 0.554225 
 Recall: 0.411346
 IoU: 0.309085

test Error: 
 Avg loss: 0.50003942 
 F1: 0.554779 
 Precision: 0.622594 
 Recall: 0.500286
 IoU: 0.383871

We have finished training iteration 24
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_22_.pth
per-ex loss: 0.642257  [    1/   88]
per-ex loss: 0.614255  [    2/   88]
per-ex loss: 0.571091  [    3/   88]
per-ex loss: 0.441025  [    4/   88]
per-ex loss: 0.386319  [    5/   88]
per-ex loss: 0.467592  [    6/   88]
per-ex loss: 0.453395  [    7/   88]
per-ex loss: 0.703572  [    8/   88]
per-ex loss: 0.444401  [    9/   88]
per-ex loss: 0.474829  [   10/   88]
per-ex loss: 0.499641  [   11/   88]
per-ex loss: 0.536815  [   12/   88]
per-ex loss: 0.562827  [   13/   88]
per-ex loss: 0.399261  [   14/   88]
per-ex loss: 0.445895  [   15/   88]
per-ex loss: 0.388852  [   16/   88]
per-ex loss: 0.571769  [   17/   88]
per-ex loss: 0.601998  [   18/   88]
per-ex loss: 0.425282  [   19/   88]
per-ex loss: 0.651192  [   20/   88]
per-ex loss: 0.446782  [   21/   88]
per-ex loss: 0.443307  [   22/   88]
per-ex loss: 0.716472  [   23/   88]
per-ex loss: 0.609509  [   24/   88]
per-ex loss: 0.590822  [   25/   88]
per-ex loss: 0.645629  [   26/   88]
per-ex loss: 0.404642  [   27/   88]
per-ex loss: 0.457603  [   28/   88]
per-ex loss: 0.454431  [   29/   88]
per-ex loss: 0.559696  [   30/   88]
per-ex loss: 0.418124  [   31/   88]
per-ex loss: 0.399655  [   32/   88]
per-ex loss: 0.463913  [   33/   88]
per-ex loss: 0.671017  [   34/   88]
per-ex loss: 0.683323  [   35/   88]
per-ex loss: 0.409488  [   36/   88]
per-ex loss: 0.457423  [   37/   88]
per-ex loss: 0.661937  [   38/   88]
per-ex loss: 0.444087  [   39/   88]
per-ex loss: 0.566441  [   40/   88]
per-ex loss: 0.555789  [   41/   88]
per-ex loss: 0.610658  [   42/   88]
per-ex loss: 0.343062  [   43/   88]
per-ex loss: 0.652201  [   44/   88]
per-ex loss: 0.570779  [   45/   88]
per-ex loss: 0.677979  [   46/   88]
per-ex loss: 0.394317  [   47/   88]
per-ex loss: 0.645755  [   48/   88]
per-ex loss: 0.655770  [   49/   88]
per-ex loss: 0.578294  [   50/   88]
per-ex loss: 0.389245  [   51/   88]
per-ex loss: 0.624633  [   52/   88]
per-ex loss: 0.590824  [   53/   88]
per-ex loss: 0.485181  [   54/   88]
per-ex loss: 0.626593  [   55/   88]
per-ex loss: 0.419628  [   56/   88]
per-ex loss: 0.501653  [   57/   88]
per-ex loss: 0.485353  [   58/   88]
per-ex loss: 0.575274  [   59/   88]
per-ex loss: 0.408933  [   60/   88]
per-ex loss: 0.456305  [   61/   88]
per-ex loss: 0.404803  [   62/   88]
per-ex loss: 0.441005  [   63/   88]
per-ex loss: 0.505719  [   64/   88]
per-ex loss: 0.426205  [   65/   88]
per-ex loss: 0.634054  [   66/   88]
per-ex loss: 0.571749  [   67/   88]
per-ex loss: 0.497001  [   68/   88]
per-ex loss: 0.348889  [   69/   88]
per-ex loss: 0.711417  [   70/   88]
per-ex loss: 0.750650  [   71/   88]
per-ex loss: 0.469457  [   72/   88]
per-ex loss: 0.383518  [   73/   88]
per-ex loss: 0.387831  [   74/   88]
per-ex loss: 0.767547  [   75/   88]
per-ex loss: 0.563785  [   76/   88]
per-ex loss: 0.461656  [   77/   88]
per-ex loss: 0.564519  [   78/   88]
per-ex loss: 0.542850  [   79/   88]
per-ex loss: 0.361632  [   80/   88]
per-ex loss: 0.644488  [   81/   88]
per-ex loss: 0.381508  [   82/   88]
per-ex loss: 0.479491  [   83/   88]
per-ex loss: 0.452196  [   84/   88]
per-ex loss: 0.467686  [   85/   88]
per-ex loss: 0.386675  [   86/   88]
per-ex loss: 0.525764  [   87/   88]
per-ex loss: 0.447369  [   88/   88]
Train Error: Avg loss: 0.51834387
validation Error: 
 Avg loss: 0.55613025 
 F1: 0.470127 
 Precision: 0.499786 
 Recall: 0.443791
 IoU: 0.307298

test Error: 
 Avg loss: 0.49759277 
 F1: 0.560196 
 Precision: 0.579290 
 Recall: 0.542320
 IoU: 0.389077

We have finished training iteration 25
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_23_.pth
per-ex loss: 0.437473  [    1/   88]
per-ex loss: 0.644719  [    2/   88]
per-ex loss: 0.720401  [    3/   88]
per-ex loss: 0.396196  [    4/   88]
per-ex loss: 0.374245  [    5/   88]
per-ex loss: 0.549763  [    6/   88]
per-ex loss: 0.461446  [    7/   88]
per-ex loss: 0.432604  [    8/   88]
per-ex loss: 0.371873  [    9/   88]
per-ex loss: 0.615099  [   10/   88]
per-ex loss: 0.444029  [   11/   88]
per-ex loss: 0.593638  [   12/   88]
per-ex loss: 0.548072  [   13/   88]
per-ex loss: 0.371727  [   14/   88]
per-ex loss: 0.600890  [   15/   88]
per-ex loss: 0.564448  [   16/   88]
per-ex loss: 0.406660  [   17/   88]
per-ex loss: 0.327597  [   18/   88]
per-ex loss: 0.553753  [   19/   88]
per-ex loss: 0.637742  [   20/   88]
per-ex loss: 0.331704  [   21/   88]
per-ex loss: 0.685412  [   22/   88]
per-ex loss: 0.696571  [   23/   88]
per-ex loss: 0.397327  [   24/   88]
per-ex loss: 0.424344  [   25/   88]
per-ex loss: 0.614600  [   26/   88]
per-ex loss: 0.461616  [   27/   88]
per-ex loss: 0.501193  [   28/   88]
per-ex loss: 0.482993  [   29/   88]
per-ex loss: 0.393655  [   30/   88]
per-ex loss: 0.696384  [   31/   88]
per-ex loss: 0.363524  [   32/   88]
per-ex loss: 0.637350  [   33/   88]
per-ex loss: 0.476255  [   34/   88]
per-ex loss: 0.409683  [   35/   88]
per-ex loss: 0.557244  [   36/   88]
per-ex loss: 0.665052  [   37/   88]
per-ex loss: 0.388096  [   38/   88]
per-ex loss: 0.645969  [   39/   88]
per-ex loss: 0.605321  [   40/   88]
per-ex loss: 0.429772  [   41/   88]
per-ex loss: 0.457722  [   42/   88]
per-ex loss: 0.688435  [   43/   88]
per-ex loss: 0.409345  [   44/   88]
per-ex loss: 0.446553  [   45/   88]
per-ex loss: 0.569478  [   46/   88]
per-ex loss: 0.482300  [   47/   88]
per-ex loss: 0.585651  [   48/   88]
per-ex loss: 0.583698  [   49/   88]
per-ex loss: 0.614443  [   50/   88]
per-ex loss: 0.473780  [   51/   88]
per-ex loss: 0.543362  [   52/   88]
per-ex loss: 0.545953  [   53/   88]
per-ex loss: 0.624475  [   54/   88]
per-ex loss: 0.473716  [   55/   88]
per-ex loss: 0.441566  [   56/   88]
per-ex loss: 0.638552  [   57/   88]
per-ex loss: 0.453487  [   58/   88]
per-ex loss: 0.460228  [   59/   88]
per-ex loss: 0.580048  [   60/   88]
per-ex loss: 0.685287  [   61/   88]
per-ex loss: 0.447521  [   62/   88]
per-ex loss: 0.393892  [   63/   88]
per-ex loss: 0.434466  [   64/   88]
per-ex loss: 0.610583  [   65/   88]
per-ex loss: 0.586020  [   66/   88]
per-ex loss: 0.701514  [   67/   88]
per-ex loss: 0.527093  [   68/   88]
per-ex loss: 0.399844  [   69/   88]
per-ex loss: 0.576983  [   70/   88]
per-ex loss: 0.385112  [   71/   88]
per-ex loss: 0.440630  [   72/   88]
per-ex loss: 0.426116  [   73/   88]
per-ex loss: 0.618702  [   74/   88]
per-ex loss: 0.395040  [   75/   88]
per-ex loss: 0.428664  [   76/   88]
per-ex loss: 0.583441  [   77/   88]
per-ex loss: 0.476068  [   78/   88]
per-ex loss: 0.542957  [   79/   88]
per-ex loss: 0.487494  [   80/   88]
per-ex loss: 0.421544  [   81/   88]
per-ex loss: 0.502183  [   82/   88]
per-ex loss: 0.558015  [   83/   88]
per-ex loss: 0.372111  [   84/   88]
per-ex loss: 0.678128  [   85/   88]
per-ex loss: 0.589745  [   86/   88]
per-ex loss: 0.480384  [   87/   88]
per-ex loss: 0.554228  [   88/   88]
Train Error: Avg loss: 0.51469314
validation Error: 
 Avg loss: 0.55916930 
 F1: 0.468980 
 Precision: 0.613620 
 Recall: 0.379520
 IoU: 0.306318

test Error: 
 Avg loss: 0.51561843 
 F1: 0.541839 
 Precision: 0.631168 
 Recall: 0.474661
 IoU: 0.371591

We have finished training iteration 26
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_20_.pth
per-ex loss: 0.551755  [    1/   88]
per-ex loss: 0.807551  [    2/   88]
per-ex loss: 0.689059  [    3/   88]
per-ex loss: 0.454992  [    4/   88]
per-ex loss: 0.623510  [    5/   88]
per-ex loss: 0.438525  [    6/   88]
per-ex loss: 0.526526  [    7/   88]
per-ex loss: 0.575418  [    8/   88]
per-ex loss: 0.567192  [    9/   88]
per-ex loss: 0.540573  [   10/   88]
per-ex loss: 0.367637  [   11/   88]
per-ex loss: 0.567984  [   12/   88]
per-ex loss: 0.544461  [   13/   88]
per-ex loss: 0.414662  [   14/   88]
per-ex loss: 0.423314  [   15/   88]
per-ex loss: 0.625176  [   16/   88]
per-ex loss: 0.650664  [   17/   88]
per-ex loss: 0.436080  [   18/   88]
per-ex loss: 0.461908  [   19/   88]
per-ex loss: 0.595263  [   20/   88]
per-ex loss: 0.419552  [   21/   88]
per-ex loss: 0.409821  [   22/   88]
per-ex loss: 0.722678  [   23/   88]
per-ex loss: 0.461897  [   24/   88]
per-ex loss: 0.409983  [   25/   88]
per-ex loss: 0.404091  [   26/   88]
per-ex loss: 0.362729  [   27/   88]
per-ex loss: 0.450909  [   28/   88]
per-ex loss: 0.439063  [   29/   88]
per-ex loss: 0.617100  [   30/   88]
per-ex loss: 0.492099  [   31/   88]
per-ex loss: 0.387797  [   32/   88]
per-ex loss: 0.595206  [   33/   88]
per-ex loss: 0.423740  [   34/   88]
per-ex loss: 0.436035  [   35/   88]
per-ex loss: 0.579817  [   36/   88]
per-ex loss: 0.589528  [   37/   88]
per-ex loss: 0.433677  [   38/   88]
per-ex loss: 0.692674  [   39/   88]
per-ex loss: 0.328490  [   40/   88]
per-ex loss: 0.430296  [   41/   88]
per-ex loss: 0.399423  [   42/   88]
per-ex loss: 0.411580  [   43/   88]
per-ex loss: 0.665534  [   44/   88]
per-ex loss: 0.510652  [   45/   88]
per-ex loss: 0.591590  [   46/   88]
per-ex loss: 0.370709  [   47/   88]
per-ex loss: 0.616138  [   48/   88]
per-ex loss: 0.445254  [   49/   88]
per-ex loss: 0.421268  [   50/   88]
per-ex loss: 0.385350  [   51/   88]
per-ex loss: 0.401852  [   52/   88]
per-ex loss: 0.600305  [   53/   88]
per-ex loss: 0.450322  [   54/   88]
per-ex loss: 0.384186  [   55/   88]
per-ex loss: 0.632872  [   56/   88]
per-ex loss: 0.679494  [   57/   88]
per-ex loss: 0.441425  [   58/   88]
per-ex loss: 0.488740  [   59/   88]
per-ex loss: 0.425090  [   60/   88]
per-ex loss: 0.567470  [   61/   88]
per-ex loss: 0.471349  [   62/   88]
per-ex loss: 0.372534  [   63/   88]
per-ex loss: 0.588142  [   64/   88]
per-ex loss: 0.622352  [   65/   88]
per-ex loss: 0.415038  [   66/   88]
per-ex loss: 0.446350  [   67/   88]
per-ex loss: 0.698557  [   68/   88]
per-ex loss: 0.677510  [   69/   88]
per-ex loss: 0.557191  [   70/   88]
per-ex loss: 0.374407  [   71/   88]
per-ex loss: 0.555246  [   72/   88]
per-ex loss: 0.461271  [   73/   88]
per-ex loss: 0.694519  [   74/   88]
per-ex loss: 0.386016  [   75/   88]
per-ex loss: 0.418531  [   76/   88]
per-ex loss: 0.644628  [   77/   88]
per-ex loss: 0.530179  [   78/   88]
per-ex loss: 0.585953  [   79/   88]
per-ex loss: 0.636238  [   80/   88]
per-ex loss: 0.694891  [   81/   88]
per-ex loss: 0.473097  [   82/   88]
per-ex loss: 0.393372  [   83/   88]
per-ex loss: 0.454814  [   84/   88]
per-ex loss: 0.411724  [   85/   88]
per-ex loss: 0.616381  [   86/   88]
per-ex loss: 0.464142  [   87/   88]
per-ex loss: 0.567929  [   88/   88]
Train Error: Avg loss: 0.51194374
validation Error: 
 Avg loss: 0.56657402 
 F1: 0.459714 
 Precision: 0.513600 
 Recall: 0.416062
 IoU: 0.298460

test Error: 
 Avg loss: 0.50654515 
 F1: 0.548286 
 Precision: 0.597292 
 Recall: 0.506711
 IoU: 0.377681

We have finished training iteration 27
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_25_.pth
per-ex loss: 0.404074  [    1/   88]
per-ex loss: 0.564198  [    2/   88]
per-ex loss: 0.405036  [    3/   88]
per-ex loss: 0.385772  [    4/   88]
per-ex loss: 0.546767  [    5/   88]
per-ex loss: 0.387865  [    6/   88]
per-ex loss: 0.416155  [    7/   88]
per-ex loss: 0.615795  [    8/   88]
per-ex loss: 0.721038  [    9/   88]
per-ex loss: 0.422792  [   10/   88]
per-ex loss: 0.597212  [   11/   88]
per-ex loss: 0.643152  [   12/   88]
per-ex loss: 0.471661  [   13/   88]
per-ex loss: 0.383761  [   14/   88]
per-ex loss: 0.622987  [   15/   88]
per-ex loss: 0.657609  [   16/   88]
per-ex loss: 0.492671  [   17/   88]
per-ex loss: 0.581064  [   18/   88]
per-ex loss: 0.595544  [   19/   88]
per-ex loss: 0.638628  [   20/   88]
per-ex loss: 0.348134  [   21/   88]
per-ex loss: 0.568016  [   22/   88]
per-ex loss: 0.375019  [   23/   88]
per-ex loss: 0.398778  [   24/   88]
per-ex loss: 0.723164  [   25/   88]
per-ex loss: 0.431416  [   26/   88]
per-ex loss: 0.445500  [   27/   88]
per-ex loss: 0.687539  [   28/   88]
per-ex loss: 0.617702  [   29/   88]
per-ex loss: 0.637997  [   30/   88]
per-ex loss: 0.697032  [   31/   88]
per-ex loss: 0.426789  [   32/   88]
per-ex loss: 0.360964  [   33/   88]
per-ex loss: 0.446658  [   34/   88]
per-ex loss: 0.474721  [   35/   88]
per-ex loss: 0.608147  [   36/   88]
per-ex loss: 0.445564  [   37/   88]
per-ex loss: 0.333039  [   38/   88]
per-ex loss: 0.525792  [   39/   88]
per-ex loss: 0.577768  [   40/   88]
per-ex loss: 0.586289  [   41/   88]
per-ex loss: 0.406335  [   42/   88]
per-ex loss: 0.592826  [   43/   88]
per-ex loss: 0.579524  [   44/   88]
per-ex loss: 0.459306  [   45/   88]
per-ex loss: 0.399795  [   46/   88]
per-ex loss: 0.615668  [   47/   88]
per-ex loss: 0.465663  [   48/   88]
per-ex loss: 0.565616  [   49/   88]
per-ex loss: 0.363273  [   50/   88]
per-ex loss: 0.513284  [   51/   88]
per-ex loss: 0.483391  [   52/   88]
per-ex loss: 0.422734  [   53/   88]
per-ex loss: 0.450337  [   54/   88]
per-ex loss: 0.411763  [   55/   88]
per-ex loss: 0.667626  [   56/   88]
per-ex loss: 0.564279  [   57/   88]
per-ex loss: 0.493258  [   58/   88]
per-ex loss: 0.446080  [   59/   88]
per-ex loss: 0.461504  [   60/   88]
per-ex loss: 0.677169  [   61/   88]
per-ex loss: 0.467327  [   62/   88]
per-ex loss: 0.442404  [   63/   88]
per-ex loss: 0.591681  [   64/   88]
per-ex loss: 0.417580  [   65/   88]
per-ex loss: 0.442806  [   66/   88]
per-ex loss: 0.452915  [   67/   88]
per-ex loss: 0.584205  [   68/   88]
per-ex loss: 0.497993  [   69/   88]
per-ex loss: 0.405424  [   70/   88]
per-ex loss: 0.440084  [   71/   88]
per-ex loss: 0.373641  [   72/   88]
per-ex loss: 0.636768  [   73/   88]
per-ex loss: 0.386758  [   74/   88]
per-ex loss: 0.537691  [   75/   88]
per-ex loss: 0.654070  [   76/   88]
per-ex loss: 0.589434  [   77/   88]
per-ex loss: 0.420342  [   78/   88]
per-ex loss: 0.416691  [   79/   88]
per-ex loss: 0.462751  [   80/   88]
per-ex loss: 0.673829  [   81/   88]
per-ex loss: 0.531679  [   82/   88]
per-ex loss: 0.705317  [   83/   88]
per-ex loss: 0.562911  [   84/   88]
per-ex loss: 0.432809  [   85/   88]
per-ex loss: 0.665692  [   86/   88]
per-ex loss: 0.382180  [   87/   88]
per-ex loss: 0.448076  [   88/   88]
Train Error: Avg loss: 0.51059432
validation Error: 
 Avg loss: 0.54743801 
 F1: 0.479539 
 Precision: 0.508913 
 Recall: 0.453370
 IoU: 0.315390

test Error: 
 Avg loss: 0.49642735 
 F1: 0.559831 
 Precision: 0.552375 
 Recall: 0.567492
 IoU: 0.388726

We have finished training iteration 28
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_26_.pth
per-ex loss: 0.447397  [    1/   88]
per-ex loss: 0.424159  [    2/   88]
per-ex loss: 0.435819  [    3/   88]
per-ex loss: 0.572126  [    4/   88]
per-ex loss: 0.456059  [    5/   88]
per-ex loss: 0.458795  [    6/   88]
per-ex loss: 0.634628  [    7/   88]
per-ex loss: 0.652490  [    8/   88]
per-ex loss: 0.442878  [    9/   88]
per-ex loss: 0.401964  [   10/   88]
per-ex loss: 0.490271  [   11/   88]
per-ex loss: 0.434197  [   12/   88]
per-ex loss: 0.571730  [   13/   88]
per-ex loss: 0.574933  [   14/   88]
per-ex loss: 0.642749  [   15/   88]
per-ex loss: 0.656267  [   16/   88]
per-ex loss: 0.563724  [   17/   88]
per-ex loss: 0.394236  [   18/   88]
per-ex loss: 0.419996  [   19/   88]
per-ex loss: 0.525016  [   20/   88]
per-ex loss: 0.628193  [   21/   88]
per-ex loss: 0.412849  [   22/   88]
per-ex loss: 0.468645  [   23/   88]
per-ex loss: 0.530782  [   24/   88]
per-ex loss: 0.639221  [   25/   88]
per-ex loss: 0.392976  [   26/   88]
per-ex loss: 0.682101  [   27/   88]
per-ex loss: 0.467793  [   28/   88]
per-ex loss: 0.489872  [   29/   88]
per-ex loss: 0.460272  [   30/   88]
per-ex loss: 0.673854  [   31/   88]
per-ex loss: 0.662132  [   32/   88]
per-ex loss: 0.664630  [   33/   88]
per-ex loss: 0.563208  [   34/   88]
per-ex loss: 0.404411  [   35/   88]
per-ex loss: 0.438149  [   36/   88]
per-ex loss: 0.673649  [   37/   88]
per-ex loss: 0.381937  [   38/   88]
per-ex loss: 0.659603  [   39/   88]
per-ex loss: 0.384486  [   40/   88]
per-ex loss: 0.437143  [   41/   88]
per-ex loss: 0.413601  [   42/   88]
per-ex loss: 0.397056  [   43/   88]
per-ex loss: 0.665463  [   44/   88]
per-ex loss: 0.601764  [   45/   88]
per-ex loss: 0.442726  [   46/   88]
per-ex loss: 0.425820  [   47/   88]
per-ex loss: 0.491653  [   48/   88]
per-ex loss: 0.375050  [   49/   88]
per-ex loss: 0.362164  [   50/   88]
per-ex loss: 0.470274  [   51/   88]
per-ex loss: 0.434770  [   52/   88]
per-ex loss: 0.617575  [   53/   88]
per-ex loss: 0.338285  [   54/   88]
per-ex loss: 0.450417  [   55/   88]
per-ex loss: 0.610350  [   56/   88]
per-ex loss: 0.463358  [   57/   88]
per-ex loss: 0.551331  [   58/   88]
per-ex loss: 0.569783  [   59/   88]
per-ex loss: 0.363337  [   60/   88]
per-ex loss: 0.601855  [   61/   88]
per-ex loss: 0.540697  [   62/   88]
per-ex loss: 0.416448  [   63/   88]
per-ex loss: 0.565187  [   64/   88]
per-ex loss: 0.722857  [   65/   88]
per-ex loss: 0.386391  [   66/   88]
per-ex loss: 0.640778  [   67/   88]
per-ex loss: 0.398503  [   68/   88]
per-ex loss: 0.595486  [   69/   88]
per-ex loss: 0.542126  [   70/   88]
per-ex loss: 0.569118  [   71/   88]
per-ex loss: 0.447381  [   72/   88]
per-ex loss: 0.424495  [   73/   88]
per-ex loss: 0.483594  [   74/   88]
per-ex loss: 0.496004  [   75/   88]
per-ex loss: 0.583514  [   76/   88]
per-ex loss: 0.592807  [   77/   88]
per-ex loss: 0.664117  [   78/   88]
per-ex loss: 0.452973  [   79/   88]
per-ex loss: 0.675609  [   80/   88]
per-ex loss: 0.609877  [   81/   88]
per-ex loss: 0.388078  [   82/   88]
per-ex loss: 0.597211  [   83/   88]
per-ex loss: 0.417778  [   84/   88]
per-ex loss: 0.373003  [   85/   88]
per-ex loss: 0.480492  [   86/   88]
per-ex loss: 0.583998  [   87/   88]
per-ex loss: 0.377969  [   88/   88]
Train Error: Avg loss: 0.51123255
validation Error: 
 Avg loss: 0.54200860 
 F1: 0.486717 
 Precision: 0.612756 
 Recall: 0.403683
 IoU: 0.321630

test Error: 
 Avg loss: 0.50069467 
 F1: 0.551458 
 Precision: 0.658540 
 Recall: 0.474329
 IoU: 0.380698

We have finished training iteration 29
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_27_.pth
per-ex loss: 0.712819  [    1/   88]
per-ex loss: 0.419958  [    2/   88]
per-ex loss: 0.432102  [    3/   88]
per-ex loss: 0.620827  [    4/   88]
per-ex loss: 0.379049  [    5/   88]
per-ex loss: 0.417510  [    6/   88]
per-ex loss: 0.410278  [    7/   88]
per-ex loss: 0.472438  [    8/   88]
per-ex loss: 0.723741  [    9/   88]
per-ex loss: 0.688467  [   10/   88]
per-ex loss: 0.638025  [   11/   88]
per-ex loss: 0.441991  [   12/   88]
per-ex loss: 0.414066  [   13/   88]
per-ex loss: 0.555204  [   14/   88]
per-ex loss: 0.636025  [   15/   88]
per-ex loss: 0.463529  [   16/   88]
per-ex loss: 0.663247  [   17/   88]
per-ex loss: 0.578928  [   18/   88]
per-ex loss: 0.657652  [   19/   88]
per-ex loss: 0.574420  [   20/   88]
per-ex loss: 0.406768  [   21/   88]
per-ex loss: 0.630352  [   22/   88]
per-ex loss: 0.564951  [   23/   88]
per-ex loss: 0.515055  [   24/   88]
per-ex loss: 0.426107  [   25/   88]
per-ex loss: 0.429229  [   26/   88]
per-ex loss: 0.433913  [   27/   88]
per-ex loss: 0.527083  [   28/   88]
per-ex loss: 0.397527  [   29/   88]
per-ex loss: 0.339846  [   30/   88]
per-ex loss: 0.573803  [   31/   88]
per-ex loss: 0.426848  [   32/   88]
per-ex loss: 0.451475  [   33/   88]
per-ex loss: 0.687012  [   34/   88]
per-ex loss: 0.623492  [   35/   88]
per-ex loss: 0.389102  [   36/   88]
per-ex loss: 0.361611  [   37/   88]
per-ex loss: 0.525151  [   38/   88]
per-ex loss: 0.553263  [   39/   88]
per-ex loss: 0.580533  [   40/   88]
per-ex loss: 0.340221  [   41/   88]
per-ex loss: 0.393643  [   42/   88]
per-ex loss: 0.541713  [   43/   88]
per-ex loss: 0.625521  [   44/   88]
per-ex loss: 0.387283  [   45/   88]
per-ex loss: 0.524719  [   46/   88]
per-ex loss: 0.481881  [   47/   88]
per-ex loss: 0.367826  [   48/   88]
per-ex loss: 0.635753  [   49/   88]
per-ex loss: 0.449072  [   50/   88]
per-ex loss: 0.661999  [   51/   88]
per-ex loss: 0.453705  [   52/   88]
per-ex loss: 0.525674  [   53/   88]
per-ex loss: 0.562544  [   54/   88]
per-ex loss: 0.447843  [   55/   88]
per-ex loss: 0.373401  [   56/   88]
per-ex loss: 0.581519  [   57/   88]
per-ex loss: 0.401706  [   58/   88]
per-ex loss: 0.376697  [   59/   88]
per-ex loss: 0.596628  [   60/   88]
per-ex loss: 0.441426  [   61/   88]
per-ex loss: 0.679298  [   62/   88]
per-ex loss: 0.596329  [   63/   88]
per-ex loss: 0.567937  [   64/   88]
per-ex loss: 0.394183  [   65/   88]
per-ex loss: 0.539991  [   66/   88]
per-ex loss: 0.652812  [   67/   88]
per-ex loss: 0.641430  [   68/   88]
per-ex loss: 0.413150  [   69/   88]
per-ex loss: 0.374861  [   70/   88]
per-ex loss: 0.420413  [   71/   88]
per-ex loss: 0.430206  [   72/   88]
per-ex loss: 0.455642  [   73/   88]
per-ex loss: 0.659403  [   74/   88]
per-ex loss: 0.575679  [   75/   88]
per-ex loss: 0.450651  [   76/   88]
per-ex loss: 0.714488  [   77/   88]
per-ex loss: 0.598867  [   78/   88]
per-ex loss: 0.412690  [   79/   88]
per-ex loss: 0.454109  [   80/   88]
per-ex loss: 0.418994  [   81/   88]
per-ex loss: 0.467492  [   82/   88]
per-ex loss: 0.684702  [   83/   88]
per-ex loss: 0.528222  [   84/   88]
per-ex loss: 0.564545  [   85/   88]
per-ex loss: 0.406483  [   86/   88]
per-ex loss: 0.479447  [   87/   88]
per-ex loss: 0.465259  [   88/   88]
Train Error: Avg loss: 0.51092559
validation Error: 
 Avg loss: 0.55692509 
 F1: 0.470996 
 Precision: 0.485103 
 Recall: 0.457686
 IoU: 0.308041

test Error: 
 Avg loss: 0.49427479 
 F1: 0.560625 
 Precision: 0.576996 
 Recall: 0.545158
 IoU: 0.389492

We have finished training iteration 30
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_24_.pth
per-ex loss: 0.431966  [    1/   88]
per-ex loss: 0.427379  [    2/   88]
per-ex loss: 0.590920  [    3/   88]
per-ex loss: 0.547230  [    4/   88]
per-ex loss: 0.370220  [    5/   88]
per-ex loss: 0.616530  [    6/   88]
per-ex loss: 0.509911  [    7/   88]
per-ex loss: 0.404123  [    8/   88]
per-ex loss: 0.610991  [    9/   88]
per-ex loss: 0.465302  [   10/   88]
per-ex loss: 0.709432  [   11/   88]
per-ex loss: 0.443717  [   12/   88]
per-ex loss: 0.386223  [   13/   88]
per-ex loss: 0.403105  [   14/   88]
per-ex loss: 0.605523  [   15/   88]
per-ex loss: 0.578918  [   16/   88]
per-ex loss: 0.659484  [   17/   88]
per-ex loss: 0.421412  [   18/   88]
per-ex loss: 0.601590  [   19/   88]
per-ex loss: 0.440470  [   20/   88]
per-ex loss: 0.538516  [   21/   88]
per-ex loss: 0.670500  [   22/   88]
per-ex loss: 0.510284  [   23/   88]
per-ex loss: 0.414798  [   24/   88]
per-ex loss: 0.561142  [   25/   88]
per-ex loss: 0.452113  [   26/   88]
per-ex loss: 0.532917  [   27/   88]
per-ex loss: 0.688167  [   28/   88]
per-ex loss: 0.559137  [   29/   88]
per-ex loss: 0.415258  [   30/   88]
per-ex loss: 0.422613  [   31/   88]
per-ex loss: 0.414084  [   32/   88]
per-ex loss: 0.440147  [   33/   88]
per-ex loss: 0.416841  [   34/   88]
per-ex loss: 0.452912  [   35/   88]
per-ex loss: 0.558905  [   36/   88]
per-ex loss: 0.391028  [   37/   88]
per-ex loss: 0.651469  [   38/   88]
per-ex loss: 0.398475  [   39/   88]
per-ex loss: 0.421504  [   40/   88]
per-ex loss: 0.544697  [   41/   88]
per-ex loss: 0.467818  [   42/   88]
per-ex loss: 0.440971  [   43/   88]
per-ex loss: 0.617306  [   44/   88]
per-ex loss: 0.465821  [   45/   88]
per-ex loss: 0.559091  [   46/   88]
per-ex loss: 0.373488  [   47/   88]
per-ex loss: 0.683127  [   48/   88]
per-ex loss: 0.611589  [   49/   88]
per-ex loss: 0.582028  [   50/   88]
per-ex loss: 0.436711  [   51/   88]
per-ex loss: 0.452302  [   52/   88]
per-ex loss: 0.628144  [   53/   88]
per-ex loss: 0.378298  [   54/   88]
per-ex loss: 0.693639  [   55/   88]
per-ex loss: 0.470553  [   56/   88]
per-ex loss: 0.403647  [   57/   88]
per-ex loss: 0.367255  [   58/   88]
per-ex loss: 0.401187  [   59/   88]
per-ex loss: 0.534148  [   60/   88]
per-ex loss: 0.426203  [   61/   88]
per-ex loss: 0.663729  [   62/   88]
per-ex loss: 0.645209  [   63/   88]
per-ex loss: 0.461047  [   64/   88]
per-ex loss: 0.570359  [   65/   88]
per-ex loss: 0.675699  [   66/   88]
per-ex loss: 0.409186  [   67/   88]
per-ex loss: 0.531101  [   68/   88]
per-ex loss: 0.735300  [   69/   88]
per-ex loss: 0.377360  [   70/   88]
per-ex loss: 0.413324  [   71/   88]
per-ex loss: 0.657047  [   72/   88]
per-ex loss: 0.392977  [   73/   88]
per-ex loss: 0.375749  [   74/   88]
per-ex loss: 0.464673  [   75/   88]
per-ex loss: 0.448783  [   76/   88]
per-ex loss: 0.388191  [   77/   88]
per-ex loss: 0.659604  [   78/   88]
per-ex loss: 0.679143  [   79/   88]
per-ex loss: 0.388341  [   80/   88]
per-ex loss: 0.694261  [   81/   88]
per-ex loss: 0.425563  [   82/   88]
per-ex loss: 0.437131  [   83/   88]
per-ex loss: 0.611455  [   84/   88]
per-ex loss: 0.490545  [   85/   88]
per-ex loss: 0.569041  [   86/   88]
per-ex loss: 0.366093  [   87/   88]
per-ex loss: 0.576107  [   88/   88]
Train Error: Avg loss: 0.50886698
validation Error: 
 Avg loss: 0.54318363 
 F1: 0.489979 
 Precision: 0.533736 
 Recall: 0.452853
 IoU: 0.324485

test Error: 
 Avg loss: 0.49844186 
 F1: 0.559290 
 Precision: 0.557667 
 Recall: 0.560923
 IoU: 0.388205

We have finished training iteration 31
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_21_.pth
per-ex loss: 0.658830  [    1/   88]
per-ex loss: 0.377191  [    2/   88]
per-ex loss: 0.508765  [    3/   88]
per-ex loss: 0.424476  [    4/   88]
per-ex loss: 0.632971  [    5/   88]
per-ex loss: 0.419471  [    6/   88]
per-ex loss: 0.364168  [    7/   88]
per-ex loss: 0.683928  [    8/   88]
per-ex loss: 0.453183  [    9/   88]
per-ex loss: 0.479733  [   10/   88]
per-ex loss: 0.417094  [   11/   88]
per-ex loss: 0.369468  [   12/   88]
per-ex loss: 0.446511  [   13/   88]
per-ex loss: 0.567911  [   14/   88]
per-ex loss: 0.637322  [   15/   88]
per-ex loss: 0.689380  [   16/   88]
per-ex loss: 0.644033  [   17/   88]
per-ex loss: 0.621069  [   18/   88]
per-ex loss: 0.627997  [   19/   88]
per-ex loss: 0.395690  [   20/   88]
per-ex loss: 0.487673  [   21/   88]
per-ex loss: 0.547790  [   22/   88]
per-ex loss: 0.569215  [   23/   88]
per-ex loss: 0.434783  [   24/   88]
per-ex loss: 0.543784  [   25/   88]
per-ex loss: 0.478661  [   26/   88]
per-ex loss: 0.470059  [   27/   88]
per-ex loss: 0.433312  [   28/   88]
per-ex loss: 0.415256  [   29/   88]
per-ex loss: 0.398542  [   30/   88]
per-ex loss: 0.454381  [   31/   88]
per-ex loss: 0.350707  [   32/   88]
per-ex loss: 0.687145  [   33/   88]
per-ex loss: 0.414490  [   34/   88]
per-ex loss: 0.677030  [   35/   88]
per-ex loss: 0.610402  [   36/   88]
per-ex loss: 0.569501  [   37/   88]
per-ex loss: 0.420964  [   38/   88]
per-ex loss: 0.547157  [   39/   88]
per-ex loss: 0.390427  [   40/   88]
per-ex loss: 0.598891  [   41/   88]
per-ex loss: 0.568305  [   42/   88]
per-ex loss: 0.682660  [   43/   88]
per-ex loss: 0.383488  [   44/   88]
per-ex loss: 0.640592  [   45/   88]
per-ex loss: 0.360801  [   46/   88]
per-ex loss: 0.479422  [   47/   88]
per-ex loss: 0.518992  [   48/   88]
per-ex loss: 0.441305  [   49/   88]
per-ex loss: 0.644552  [   50/   88]
per-ex loss: 0.474533  [   51/   88]
per-ex loss: 0.578586  [   52/   88]
per-ex loss: 0.418280  [   53/   88]
per-ex loss: 0.465023  [   54/   88]
per-ex loss: 0.470163  [   55/   88]
per-ex loss: 0.373431  [   56/   88]
per-ex loss: 0.610892  [   57/   88]
per-ex loss: 0.462958  [   58/   88]
per-ex loss: 0.462063  [   59/   88]
per-ex loss: 0.449659  [   60/   88]
per-ex loss: 0.520825  [   61/   88]
per-ex loss: 0.419248  [   62/   88]
per-ex loss: 0.496656  [   63/   88]
per-ex loss: 0.452526  [   64/   88]
per-ex loss: 0.567838  [   65/   88]
per-ex loss: 0.644647  [   66/   88]
per-ex loss: 0.557721  [   67/   88]
per-ex loss: 0.629940  [   68/   88]
per-ex loss: 0.597055  [   69/   88]
per-ex loss: 0.650670  [   70/   88]
per-ex loss: 0.403221  [   71/   88]
per-ex loss: 0.462731  [   72/   88]
per-ex loss: 0.351489  [   73/   88]
per-ex loss: 0.635821  [   74/   88]
per-ex loss: 0.420974  [   75/   88]
per-ex loss: 0.582612  [   76/   88]
per-ex loss: 0.409105  [   77/   88]
per-ex loss: 0.604853  [   78/   88]
per-ex loss: 0.383388  [   79/   88]
per-ex loss: 0.565049  [   80/   88]
per-ex loss: 0.472276  [   81/   88]
per-ex loss: 0.402767  [   82/   88]
per-ex loss: 0.724580  [   83/   88]
per-ex loss: 0.451436  [   84/   88]
per-ex loss: 0.719598  [   85/   88]
per-ex loss: 0.463149  [   86/   88]
per-ex loss: 0.387821  [   87/   88]
per-ex loss: 0.372613  [   88/   88]
Train Error: Avg loss: 0.50858723
validation Error: 
 Avg loss: 0.54589951 
 F1: 0.487422 
 Precision: 0.627849 
 Recall: 0.398329
 IoU: 0.322245

test Error: 
 Avg loss: 0.49914177 
 F1: 0.561187 
 Precision: 0.646999 
 Recall: 0.495471
 IoU: 0.390034

We have finished training iteration 32
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_30_.pth
per-ex loss: 0.423255  [    1/   88]
per-ex loss: 0.403655  [    2/   88]
per-ex loss: 0.657981  [    3/   88]
per-ex loss: 0.367430  [    4/   88]
per-ex loss: 0.382564  [    5/   88]
per-ex loss: 0.420008  [    6/   88]
per-ex loss: 0.462010  [    7/   88]
per-ex loss: 0.560524  [    8/   88]
per-ex loss: 0.458944  [    9/   88]
per-ex loss: 0.415146  [   10/   88]
per-ex loss: 0.594344  [   11/   88]
per-ex loss: 0.368184  [   12/   88]
per-ex loss: 0.356628  [   13/   88]
per-ex loss: 0.700966  [   14/   88]
per-ex loss: 0.653774  [   15/   88]
per-ex loss: 0.592628  [   16/   88]
per-ex loss: 0.548510  [   17/   88]
per-ex loss: 0.556959  [   18/   88]
per-ex loss: 0.563187  [   19/   88]
per-ex loss: 0.625494  [   20/   88]
per-ex loss: 0.488129  [   21/   88]
per-ex loss: 0.471523  [   22/   88]
per-ex loss: 0.430464  [   23/   88]
per-ex loss: 0.459959  [   24/   88]
per-ex loss: 0.546559  [   25/   88]
per-ex loss: 0.568919  [   26/   88]
per-ex loss: 0.622837  [   27/   88]
per-ex loss: 0.462692  [   28/   88]
per-ex loss: 0.617241  [   29/   88]
per-ex loss: 0.679446  [   30/   88]
per-ex loss: 0.462517  [   31/   88]
per-ex loss: 0.678069  [   32/   88]
per-ex loss: 0.685697  [   33/   88]
per-ex loss: 0.417365  [   34/   88]
per-ex loss: 0.655844  [   35/   88]
per-ex loss: 0.396152  [   36/   88]
per-ex loss: 0.526597  [   37/   88]
per-ex loss: 0.573249  [   38/   88]
per-ex loss: 0.413180  [   39/   88]
per-ex loss: 0.523694  [   40/   88]
per-ex loss: 0.549442  [   41/   88]
per-ex loss: 0.474509  [   42/   88]
per-ex loss: 0.390107  [   43/   88]
per-ex loss: 0.374577  [   44/   88]
per-ex loss: 0.377240  [   45/   88]
per-ex loss: 0.675942  [   46/   88]
per-ex loss: 0.667077  [   47/   88]
per-ex loss: 0.439654  [   48/   88]
per-ex loss: 0.594251  [   49/   88]
per-ex loss: 0.660348  [   50/   88]
per-ex loss: 0.458290  [   51/   88]
per-ex loss: 0.391628  [   52/   88]
per-ex loss: 0.416919  [   53/   88]
per-ex loss: 0.562779  [   54/   88]
per-ex loss: 0.537160  [   55/   88]
per-ex loss: 0.606251  [   56/   88]
per-ex loss: 0.333042  [   57/   88]
per-ex loss: 0.427970  [   58/   88]
per-ex loss: 0.451889  [   59/   88]
per-ex loss: 0.620047  [   60/   88]
per-ex loss: 0.441901  [   61/   88]
per-ex loss: 0.682355  [   62/   88]
per-ex loss: 0.462626  [   63/   88]
per-ex loss: 0.418569  [   64/   88]
per-ex loss: 0.468445  [   65/   88]
per-ex loss: 0.594829  [   66/   88]
per-ex loss: 0.534022  [   67/   88]
per-ex loss: 0.429879  [   68/   88]
per-ex loss: 0.414316  [   69/   88]
per-ex loss: 0.437081  [   70/   88]
per-ex loss: 0.413893  [   71/   88]
per-ex loss: 0.437842  [   72/   88]
per-ex loss: 0.657383  [   73/   88]
per-ex loss: 0.452339  [   74/   88]
per-ex loss: 0.467250  [   75/   88]
per-ex loss: 0.399031  [   76/   88]
per-ex loss: 0.378113  [   77/   88]
per-ex loss: 0.330517  [   78/   88]
per-ex loss: 0.463073  [   79/   88]
per-ex loss: 0.729912  [   80/   88]
per-ex loss: 0.499189  [   81/   88]
per-ex loss: 0.390885  [   82/   88]
per-ex loss: 0.354944  [   83/   88]
per-ex loss: 0.667381  [   84/   88]
per-ex loss: 0.700743  [   85/   88]
per-ex loss: 0.617306  [   86/   88]
per-ex loss: 0.586480  [   87/   88]
per-ex loss: 0.563991  [   88/   88]
Train Error: Avg loss: 0.50904251
validation Error: 
 Avg loss: 0.54258152 
 F1: 0.482518 
 Precision: 0.541563 
 Recall: 0.435082
 IoU: 0.317973

test Error: 
 Avg loss: 0.49121559 
 F1: 0.566633 
 Precision: 0.604737 
 Recall: 0.533047
 IoU: 0.395316

We have finished training iteration 33
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_28_.pth
per-ex loss: 0.539542  [    1/   88]
per-ex loss: 0.530418  [    2/   88]
per-ex loss: 0.388679  [    3/   88]
per-ex loss: 0.399903  [    4/   88]
per-ex loss: 0.443743  [    5/   88]
per-ex loss: 0.415348  [    6/   88]
per-ex loss: 0.564906  [    7/   88]
per-ex loss: 0.407064  [    8/   88]
per-ex loss: 0.381315  [    9/   88]
per-ex loss: 0.402061  [   10/   88]
per-ex loss: 0.695495  [   11/   88]
per-ex loss: 0.459587  [   12/   88]
per-ex loss: 0.408622  [   13/   88]
per-ex loss: 0.564066  [   14/   88]
per-ex loss: 0.602306  [   15/   88]
per-ex loss: 0.575113  [   16/   88]
per-ex loss: 0.384986  [   17/   88]
per-ex loss: 0.556157  [   18/   88]
per-ex loss: 0.622202  [   19/   88]
per-ex loss: 0.486907  [   20/   88]
per-ex loss: 0.593606  [   21/   88]
per-ex loss: 0.582051  [   22/   88]
per-ex loss: 0.532770  [   23/   88]
per-ex loss: 0.679534  [   24/   88]
per-ex loss: 0.478207  [   25/   88]
per-ex loss: 0.428436  [   26/   88]
per-ex loss: 0.425723  [   27/   88]
per-ex loss: 0.625836  [   28/   88]
per-ex loss: 0.465412  [   29/   88]
per-ex loss: 0.604773  [   30/   88]
per-ex loss: 0.730972  [   31/   88]
per-ex loss: 0.444061  [   32/   88]
per-ex loss: 0.373680  [   33/   88]
per-ex loss: 0.362298  [   34/   88]
per-ex loss: 0.453039  [   35/   88]
per-ex loss: 0.416976  [   36/   88]
per-ex loss: 0.371883  [   37/   88]
per-ex loss: 0.692309  [   38/   88]
per-ex loss: 0.645423  [   39/   88]
per-ex loss: 0.582716  [   40/   88]
per-ex loss: 0.352776  [   41/   88]
per-ex loss: 0.366028  [   42/   88]
per-ex loss: 0.356174  [   43/   88]
per-ex loss: 0.435138  [   44/   88]
per-ex loss: 0.672111  [   45/   88]
per-ex loss: 0.467459  [   46/   88]
per-ex loss: 0.625605  [   47/   88]
per-ex loss: 0.788749  [   48/   88]
per-ex loss: 0.416949  [   49/   88]
per-ex loss: 0.431613  [   50/   88]
per-ex loss: 0.323379  [   51/   88]
per-ex loss: 0.473874  [   52/   88]
per-ex loss: 0.374132  [   53/   88]
per-ex loss: 0.612908  [   54/   88]
per-ex loss: 0.427170  [   55/   88]
per-ex loss: 0.448685  [   56/   88]
per-ex loss: 0.563078  [   57/   88]
per-ex loss: 0.601635  [   58/   88]
per-ex loss: 0.436330  [   59/   88]
per-ex loss: 0.487465  [   60/   88]
per-ex loss: 0.660194  [   61/   88]
per-ex loss: 0.384983  [   62/   88]
per-ex loss: 0.659031  [   63/   88]
per-ex loss: 0.622228  [   64/   88]
per-ex loss: 0.575969  [   65/   88]
per-ex loss: 0.529947  [   66/   88]
per-ex loss: 0.562447  [   67/   88]
per-ex loss: 0.440403  [   68/   88]
per-ex loss: 0.690201  [   69/   88]
per-ex loss: 0.505702  [   70/   88]
per-ex loss: 0.555826  [   71/   88]
per-ex loss: 0.536715  [   72/   88]
per-ex loss: 0.374639  [   73/   88]
per-ex loss: 0.464503  [   74/   88]
per-ex loss: 0.447966  [   75/   88]
per-ex loss: 0.449519  [   76/   88]
per-ex loss: 0.561143  [   77/   88]
per-ex loss: 0.518010  [   78/   88]
per-ex loss: 0.726241  [   79/   88]
per-ex loss: 0.424763  [   80/   88]
per-ex loss: 0.673445  [   81/   88]
per-ex loss: 0.402002  [   82/   88]
per-ex loss: 0.394704  [   83/   88]
per-ex loss: 0.476841  [   84/   88]
per-ex loss: 0.503055  [   85/   88]
per-ex loss: 0.580993  [   86/   88]
per-ex loss: 0.581555  [   87/   88]
per-ex loss: 0.395931  [   88/   88]
Train Error: Avg loss: 0.50770836
validation Error: 
 Avg loss: 0.53673471 
 F1: 0.495248 
 Precision: 0.533059 
 Recall: 0.462446
 IoU: 0.329123

test Error: 
 Avg loss: 0.49349044 
 F1: 0.560792 
 Precision: 0.565257 
 Recall: 0.556397
 IoU: 0.389653

We have finished training iteration 34
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_32_.pth
per-ex loss: 0.428053  [    1/   88]
per-ex loss: 0.586781  [    2/   88]
per-ex loss: 0.643679  [    3/   88]
per-ex loss: 0.422395  [    4/   88]
per-ex loss: 0.400824  [    5/   88]
per-ex loss: 0.427632  [    6/   88]
per-ex loss: 0.404890  [    7/   88]
per-ex loss: 0.580808  [    8/   88]
per-ex loss: 0.442267  [    9/   88]
per-ex loss: 0.549337  [   10/   88]
per-ex loss: 0.571159  [   11/   88]
per-ex loss: 0.604128  [   12/   88]
per-ex loss: 0.606473  [   13/   88]
per-ex loss: 0.548676  [   14/   88]
per-ex loss: 0.532787  [   15/   88]
per-ex loss: 0.361187  [   16/   88]
per-ex loss: 0.370228  [   17/   88]
per-ex loss: 0.379429  [   18/   88]
per-ex loss: 0.469111  [   19/   88]
per-ex loss: 0.337506  [   20/   88]
per-ex loss: 0.463193  [   21/   88]
per-ex loss: 0.467804  [   22/   88]
per-ex loss: 0.515448  [   23/   88]
per-ex loss: 0.377256  [   24/   88]
per-ex loss: 0.539182  [   25/   88]
per-ex loss: 0.450887  [   26/   88]
per-ex loss: 0.641883  [   27/   88]
per-ex loss: 0.413231  [   28/   88]
per-ex loss: 0.650096  [   29/   88]
per-ex loss: 0.603083  [   30/   88]
per-ex loss: 0.370981  [   31/   88]
per-ex loss: 0.689888  [   32/   88]
per-ex loss: 0.423695  [   33/   88]
per-ex loss: 0.562134  [   34/   88]
per-ex loss: 0.361266  [   35/   88]
per-ex loss: 0.375719  [   36/   88]
per-ex loss: 0.549499  [   37/   88]
per-ex loss: 0.642904  [   38/   88]
per-ex loss: 0.614917  [   39/   88]
per-ex loss: 0.476669  [   40/   88]
per-ex loss: 0.477623  [   41/   88]
per-ex loss: 0.394403  [   42/   88]
per-ex loss: 0.399088  [   43/   88]
per-ex loss: 0.419995  [   44/   88]
per-ex loss: 0.454372  [   45/   88]
per-ex loss: 0.431266  [   46/   88]
per-ex loss: 0.400998  [   47/   88]
per-ex loss: 0.612824  [   48/   88]
per-ex loss: 0.436763  [   49/   88]
per-ex loss: 0.356959  [   50/   88]
per-ex loss: 0.658442  [   51/   88]
per-ex loss: 0.427370  [   52/   88]
per-ex loss: 0.364803  [   53/   88]
per-ex loss: 0.445028  [   54/   88]
per-ex loss: 0.572129  [   55/   88]
per-ex loss: 0.430086  [   56/   88]
per-ex loss: 0.349566  [   57/   88]
per-ex loss: 0.334458  [   58/   88]
per-ex loss: 0.670729  [   59/   88]
per-ex loss: 0.388771  [   60/   88]
per-ex loss: 0.404226  [   61/   88]
per-ex loss: 0.470669  [   62/   88]
per-ex loss: 0.686413  [   63/   88]
per-ex loss: 0.467321  [   64/   88]
per-ex loss: 0.592219  [   65/   88]
per-ex loss: 0.605052  [   66/   88]
per-ex loss: 0.533978  [   67/   88]
per-ex loss: 0.463830  [   68/   88]
per-ex loss: 0.657190  [   69/   88]
per-ex loss: 0.503692  [   70/   88]
per-ex loss: 0.427979  [   71/   88]
per-ex loss: 0.416848  [   72/   88]
per-ex loss: 0.681293  [   73/   88]
per-ex loss: 0.660102  [   74/   88]
per-ex loss: 0.513030  [   75/   88]
per-ex loss: 0.470385  [   76/   88]
per-ex loss: 0.579731  [   77/   88]
per-ex loss: 0.706261  [   78/   88]
per-ex loss: 0.554094  [   79/   88]
per-ex loss: 0.460221  [   80/   88]
per-ex loss: 0.589276  [   81/   88]
per-ex loss: 0.544067  [   82/   88]
per-ex loss: 0.575785  [   83/   88]
per-ex loss: 0.572577  [   84/   88]
per-ex loss: 0.662023  [   85/   88]
per-ex loss: 0.551221  [   86/   88]
per-ex loss: 0.387206  [   87/   88]
per-ex loss: 0.607623  [   88/   88]
Train Error: Avg loss: 0.50258034
validation Error: 
 Avg loss: 0.59190252 
 F1: 0.431739 
 Precision: 0.428653 
 Recall: 0.434869
 IoU: 0.275298

test Error: 
 Avg loss: 0.50609378 
 F1: 0.539408 
 Precision: 0.564026 
 Recall: 0.516849
 IoU: 0.369308

We have finished training iteration 35
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_31_.pth
per-ex loss: 0.411718  [    1/   88]
per-ex loss: 0.610243  [    2/   88]
per-ex loss: 0.673173  [    3/   88]
per-ex loss: 0.472050  [    4/   88]
per-ex loss: 0.550189  [    5/   88]
per-ex loss: 0.452217  [    6/   88]
per-ex loss: 0.679199  [    7/   88]
per-ex loss: 0.423639  [    8/   88]
per-ex loss: 0.369546  [    9/   88]
per-ex loss: 0.604518  [   10/   88]
per-ex loss: 0.378308  [   11/   88]
per-ex loss: 0.407214  [   12/   88]
per-ex loss: 0.584865  [   13/   88]
per-ex loss: 0.543749  [   14/   88]
per-ex loss: 0.621779  [   15/   88]
per-ex loss: 0.405876  [   16/   88]
per-ex loss: 0.445762  [   17/   88]
per-ex loss: 0.590899  [   18/   88]
per-ex loss: 0.469036  [   19/   88]
per-ex loss: 0.430148  [   20/   88]
per-ex loss: 0.411919  [   21/   88]
per-ex loss: 0.397553  [   22/   88]
per-ex loss: 0.662522  [   23/   88]
per-ex loss: 0.550829  [   24/   88]
per-ex loss: 0.632041  [   25/   88]
per-ex loss: 0.438995  [   26/   88]
per-ex loss: 0.404005  [   27/   88]
per-ex loss: 0.609805  [   28/   88]
per-ex loss: 0.572426  [   29/   88]
per-ex loss: 0.561410  [   30/   88]
per-ex loss: 0.365526  [   31/   88]
per-ex loss: 0.447373  [   32/   88]
per-ex loss: 0.692910  [   33/   88]
per-ex loss: 0.372848  [   34/   88]
per-ex loss: 0.503935  [   35/   88]
per-ex loss: 0.411986  [   36/   88]
per-ex loss: 0.709983  [   37/   88]
per-ex loss: 0.354336  [   38/   88]
per-ex loss: 0.557917  [   39/   88]
per-ex loss: 0.509022  [   40/   88]
per-ex loss: 0.463974  [   41/   88]
per-ex loss: 0.473556  [   42/   88]
per-ex loss: 0.573741  [   43/   88]
per-ex loss: 0.544123  [   44/   88]
per-ex loss: 0.454280  [   45/   88]
per-ex loss: 0.394510  [   46/   88]
per-ex loss: 0.414441  [   47/   88]
per-ex loss: 0.574443  [   48/   88]
per-ex loss: 0.597896  [   49/   88]
per-ex loss: 0.564840  [   50/   88]
per-ex loss: 0.421430  [   51/   88]
per-ex loss: 0.691876  [   52/   88]
per-ex loss: 0.678411  [   53/   88]
per-ex loss: 0.622418  [   54/   88]
per-ex loss: 0.313305  [   55/   88]
per-ex loss: 0.461697  [   56/   88]
per-ex loss: 0.393101  [   57/   88]
per-ex loss: 0.506209  [   58/   88]
per-ex loss: 0.489145  [   59/   88]
per-ex loss: 0.359713  [   60/   88]
per-ex loss: 0.357467  [   61/   88]
per-ex loss: 0.426228  [   62/   88]
per-ex loss: 0.593525  [   63/   88]
per-ex loss: 0.598353  [   64/   88]
per-ex loss: 0.565302  [   65/   88]
per-ex loss: 0.378053  [   66/   88]
per-ex loss: 0.604964  [   67/   88]
per-ex loss: 0.569977  [   68/   88]
per-ex loss: 0.407224  [   69/   88]
per-ex loss: 0.459966  [   70/   88]
per-ex loss: 0.563684  [   71/   88]
per-ex loss: 0.472661  [   72/   88]
per-ex loss: 0.360367  [   73/   88]
per-ex loss: 0.669353  [   74/   88]
per-ex loss: 0.407692  [   75/   88]
per-ex loss: 0.424245  [   76/   88]
per-ex loss: 0.702652  [   77/   88]
per-ex loss: 0.437134  [   78/   88]
per-ex loss: 0.434462  [   79/   88]
per-ex loss: 0.364103  [   80/   88]
per-ex loss: 0.405605  [   81/   88]
per-ex loss: 0.357688  [   82/   88]
per-ex loss: 0.337961  [   83/   88]
per-ex loss: 0.569742  [   84/   88]
per-ex loss: 0.662230  [   85/   88]
per-ex loss: 0.402654  [   86/   88]
per-ex loss: 0.602627  [   87/   88]
per-ex loss: 0.545747  [   88/   88]
Train Error: Avg loss: 0.49961641
validation Error: 
 Avg loss: 0.55415040 
 F1: 0.466961 
 Precision: 0.641785 
 Recall: 0.366991
 IoU: 0.304598

test Error: 
 Avg loss: 0.50095216 
 F1: 0.554929 
 Precision: 0.661111 
 Recall: 0.478135
 IoU: 0.384015

We have finished training iteration 36
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_33_.pth
per-ex loss: 0.359282  [    1/   88]
per-ex loss: 0.437904  [    2/   88]
per-ex loss: 0.376198  [    3/   88]
per-ex loss: 0.429255  [    4/   88]
per-ex loss: 0.627792  [    5/   88]
per-ex loss: 0.435411  [    6/   88]
per-ex loss: 0.546845  [    7/   88]
per-ex loss: 0.410814  [    8/   88]
per-ex loss: 0.658345  [    9/   88]
per-ex loss: 0.525151  [   10/   88]
per-ex loss: 0.580911  [   11/   88]
per-ex loss: 0.614061  [   12/   88]
per-ex loss: 0.462273  [   13/   88]
per-ex loss: 0.587988  [   14/   88]
per-ex loss: 0.425881  [   15/   88]
per-ex loss: 0.449466  [   16/   88]
per-ex loss: 0.556908  [   17/   88]
per-ex loss: 0.418174  [   18/   88]
per-ex loss: 0.393724  [   19/   88]
per-ex loss: 0.661279  [   20/   88]
per-ex loss: 0.534613  [   21/   88]
per-ex loss: 0.465286  [   22/   88]
per-ex loss: 0.458514  [   23/   88]
per-ex loss: 0.695191  [   24/   88]
per-ex loss: 0.532740  [   25/   88]
per-ex loss: 0.489876  [   26/   88]
per-ex loss: 0.373981  [   27/   88]
per-ex loss: 0.469110  [   28/   88]
per-ex loss: 0.552206  [   29/   88]
per-ex loss: 0.701392  [   30/   88]
per-ex loss: 0.440879  [   31/   88]
per-ex loss: 0.610534  [   32/   88]
per-ex loss: 0.484644  [   33/   88]
per-ex loss: 0.586595  [   34/   88]
per-ex loss: 0.413125  [   35/   88]
per-ex loss: 0.475132  [   36/   88]
per-ex loss: 0.597608  [   37/   88]
per-ex loss: 0.610216  [   38/   88]
per-ex loss: 0.404413  [   39/   88]
per-ex loss: 0.642363  [   40/   88]
per-ex loss: 0.394494  [   41/   88]
per-ex loss: 0.504289  [   42/   88]
per-ex loss: 0.365386  [   43/   88]
per-ex loss: 0.565089  [   44/   88]
per-ex loss: 0.681476  [   45/   88]
per-ex loss: 0.556843  [   46/   88]
per-ex loss: 0.484289  [   47/   88]
per-ex loss: 0.595407  [   48/   88]
per-ex loss: 0.406410  [   49/   88]
per-ex loss: 0.401394  [   50/   88]
per-ex loss: 0.440437  [   51/   88]
per-ex loss: 0.474888  [   52/   88]
per-ex loss: 0.455869  [   53/   88]
per-ex loss: 0.469073  [   54/   88]
per-ex loss: 0.398113  [   55/   88]
per-ex loss: 0.706204  [   56/   88]
per-ex loss: 0.422415  [   57/   88]
per-ex loss: 0.442131  [   58/   88]
per-ex loss: 0.539293  [   59/   88]
per-ex loss: 0.333553  [   60/   88]
per-ex loss: 0.604618  [   61/   88]
per-ex loss: 0.406723  [   62/   88]
per-ex loss: 0.446918  [   63/   88]
per-ex loss: 0.655738  [   64/   88]
per-ex loss: 0.569103  [   65/   88]
per-ex loss: 0.407790  [   66/   88]
per-ex loss: 0.554406  [   67/   88]
per-ex loss: 0.419583  [   68/   88]
per-ex loss: 0.587859  [   69/   88]
per-ex loss: 0.373755  [   70/   88]
per-ex loss: 0.415343  [   71/   88]
per-ex loss: 0.546006  [   72/   88]
per-ex loss: 0.394451  [   73/   88]
per-ex loss: 0.587622  [   74/   88]
per-ex loss: 0.683679  [   75/   88]
per-ex loss: 0.356922  [   76/   88]
per-ex loss: 0.376878  [   77/   88]
per-ex loss: 0.731624  [   78/   88]
per-ex loss: 0.424417  [   79/   88]
per-ex loss: 0.654578  [   80/   88]
per-ex loss: 0.373206  [   81/   88]
per-ex loss: 0.492342  [   82/   88]
per-ex loss: 0.648267  [   83/   88]
per-ex loss: 0.556314  [   84/   88]
per-ex loss: 0.367799  [   85/   88]
per-ex loss: 0.655428  [   86/   88]
per-ex loss: 0.375923  [   87/   88]
per-ex loss: 0.413714  [   88/   88]
Train Error: Avg loss: 0.50243333
validation Error: 
 Avg loss: 0.54136659 
 F1: 0.487994 
 Precision: 0.575830 
 Recall: 0.423409
 IoU: 0.322746

test Error: 
 Avg loss: 0.49565991 
 F1: 0.561295 
 Precision: 0.613509 
 Recall: 0.517272
 IoU: 0.390139

We have finished training iteration 37
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_35_.pth
per-ex loss: 0.461193  [    1/   88]
per-ex loss: 0.563275  [    2/   88]
per-ex loss: 0.393621  [    3/   88]
per-ex loss: 0.652579  [    4/   88]
per-ex loss: 0.357496  [    5/   88]
per-ex loss: 0.401822  [    6/   88]
per-ex loss: 0.532055  [    7/   88]
per-ex loss: 0.357897  [    8/   88]
per-ex loss: 0.419533  [    9/   88]
per-ex loss: 0.346707  [   10/   88]
per-ex loss: 0.418856  [   11/   88]
per-ex loss: 0.623163  [   12/   88]
per-ex loss: 0.541028  [   13/   88]
per-ex loss: 0.536084  [   14/   88]
per-ex loss: 0.461150  [   15/   88]
per-ex loss: 0.519357  [   16/   88]
per-ex loss: 0.687560  [   17/   88]
per-ex loss: 0.661932  [   18/   88]
per-ex loss: 0.415609  [   19/   88]
per-ex loss: 0.422882  [   20/   88]
per-ex loss: 0.617861  [   21/   88]
per-ex loss: 0.615120  [   22/   88]
per-ex loss: 0.405653  [   23/   88]
per-ex loss: 0.423870  [   24/   88]
per-ex loss: 0.546067  [   25/   88]
per-ex loss: 0.436985  [   26/   88]
per-ex loss: 0.443684  [   27/   88]
per-ex loss: 0.582777  [   28/   88]
per-ex loss: 0.584055  [   29/   88]
per-ex loss: 0.487810  [   30/   88]
per-ex loss: 0.507590  [   31/   88]
per-ex loss: 0.538461  [   32/   88]
per-ex loss: 0.676646  [   33/   88]
per-ex loss: 0.422565  [   34/   88]
per-ex loss: 0.640313  [   35/   88]
per-ex loss: 0.449448  [   36/   88]
per-ex loss: 0.388463  [   37/   88]
per-ex loss: 0.556843  [   38/   88]
per-ex loss: 0.589877  [   39/   88]
per-ex loss: 0.352973  [   40/   88]
per-ex loss: 0.396773  [   41/   88]
per-ex loss: 0.528581  [   42/   88]
per-ex loss: 0.427268  [   43/   88]
per-ex loss: 0.400076  [   44/   88]
per-ex loss: 0.425377  [   45/   88]
per-ex loss: 0.390519  [   46/   88]
per-ex loss: 0.408337  [   47/   88]
per-ex loss: 0.370345  [   48/   88]
per-ex loss: 0.611301  [   49/   88]
per-ex loss: 0.526183  [   50/   88]
per-ex loss: 0.578440  [   51/   88]
per-ex loss: 0.589313  [   52/   88]
per-ex loss: 0.329729  [   53/   88]
per-ex loss: 0.632969  [   54/   88]
per-ex loss: 0.358985  [   55/   88]
per-ex loss: 0.412803  [   56/   88]
per-ex loss: 0.427069  [   57/   88]
per-ex loss: 0.613152  [   58/   88]
per-ex loss: 0.645522  [   59/   88]
per-ex loss: 0.401387  [   60/   88]
per-ex loss: 0.411174  [   61/   88]
per-ex loss: 0.373471  [   62/   88]
per-ex loss: 0.431410  [   63/   88]
per-ex loss: 0.642777  [   64/   88]
per-ex loss: 0.541500  [   65/   88]
per-ex loss: 0.707059  [   66/   88]
per-ex loss: 0.586882  [   67/   88]
per-ex loss: 0.441376  [   68/   88]
per-ex loss: 0.610021  [   69/   88]
per-ex loss: 0.647801  [   70/   88]
per-ex loss: 0.451365  [   71/   88]
per-ex loss: 0.687825  [   72/   88]
per-ex loss: 0.597931  [   73/   88]
per-ex loss: 0.452069  [   74/   88]
per-ex loss: 0.698495  [   75/   88]
per-ex loss: 0.457162  [   76/   88]
per-ex loss: 0.422751  [   77/   88]
per-ex loss: 0.374779  [   78/   88]
per-ex loss: 0.377848  [   79/   88]
per-ex loss: 0.361695  [   80/   88]
per-ex loss: 0.463773  [   81/   88]
per-ex loss: 0.443554  [   82/   88]
per-ex loss: 0.387226  [   83/   88]
per-ex loss: 0.563720  [   84/   88]
per-ex loss: 0.637312  [   85/   88]
per-ex loss: 0.512417  [   86/   88]
per-ex loss: 0.559589  [   87/   88]
per-ex loss: 0.641354  [   88/   88]
Train Error: Avg loss: 0.49999231
validation Error: 
 Avg loss: 0.53977768 
 F1: 0.486377 
 Precision: 0.550637 
 Recall: 0.435549
 IoU: 0.321333

test Error: 
 Avg loss: 0.49009228 
 F1: 0.565122 
 Precision: 0.595428 
 Recall: 0.537752
 IoU: 0.393847

We have finished training iteration 38
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_36_.pth
per-ex loss: 0.439720  [    1/   88]
per-ex loss: 0.627399  [    2/   88]
per-ex loss: 0.416954  [    3/   88]
per-ex loss: 0.367402  [    4/   88]
per-ex loss: 0.690959  [    5/   88]
per-ex loss: 0.703153  [    6/   88]
per-ex loss: 0.442033  [    7/   88]
per-ex loss: 0.372530  [    8/   88]
per-ex loss: 0.513988  [    9/   88]
per-ex loss: 0.511628  [   10/   88]
per-ex loss: 0.599050  [   11/   88]
per-ex loss: 0.461395  [   12/   88]
per-ex loss: 0.452146  [   13/   88]
per-ex loss: 0.356795  [   14/   88]
per-ex loss: 0.399666  [   15/   88]
per-ex loss: 0.569420  [   16/   88]
per-ex loss: 0.447141  [   17/   88]
per-ex loss: 0.376178  [   18/   88]
per-ex loss: 0.405766  [   19/   88]
per-ex loss: 0.572945  [   20/   88]
per-ex loss: 0.429757  [   21/   88]
per-ex loss: 0.577370  [   22/   88]
per-ex loss: 0.644785  [   23/   88]
per-ex loss: 0.588024  [   24/   88]
per-ex loss: 0.504401  [   25/   88]
per-ex loss: 0.371837  [   26/   88]
per-ex loss: 0.377519  [   27/   88]
per-ex loss: 0.405893  [   28/   88]
per-ex loss: 0.523588  [   29/   88]
per-ex loss: 0.375939  [   30/   88]
per-ex loss: 0.671860  [   31/   88]
per-ex loss: 0.555918  [   32/   88]
per-ex loss: 0.488352  [   33/   88]
per-ex loss: 0.445391  [   34/   88]
per-ex loss: 0.587877  [   35/   88]
per-ex loss: 0.437471  [   36/   88]
per-ex loss: 0.527188  [   37/   88]
per-ex loss: 0.423546  [   38/   88]
per-ex loss: 0.553108  [   39/   88]
per-ex loss: 0.349070  [   40/   88]
per-ex loss: 0.617312  [   41/   88]
per-ex loss: 0.537415  [   42/   88]
per-ex loss: 0.666619  [   43/   88]
per-ex loss: 0.589995  [   44/   88]
per-ex loss: 0.578593  [   45/   88]
per-ex loss: 0.741431  [   46/   88]
per-ex loss: 0.373646  [   47/   88]
per-ex loss: 0.648283  [   48/   88]
per-ex loss: 0.394930  [   49/   88]
per-ex loss: 0.414874  [   50/   88]
per-ex loss: 0.667890  [   51/   88]
per-ex loss: 0.477973  [   52/   88]
per-ex loss: 0.405382  [   53/   88]
per-ex loss: 0.592513  [   54/   88]
per-ex loss: 0.369203  [   55/   88]
per-ex loss: 0.384286  [   56/   88]
per-ex loss: 0.475179  [   57/   88]
per-ex loss: 0.557448  [   58/   88]
per-ex loss: 0.413848  [   59/   88]
per-ex loss: 0.572796  [   60/   88]
per-ex loss: 0.423662  [   61/   88]
per-ex loss: 0.404277  [   62/   88]
per-ex loss: 0.517289  [   63/   88]
per-ex loss: 0.669850  [   64/   88]
per-ex loss: 0.666487  [   65/   88]
per-ex loss: 0.358302  [   66/   88]
per-ex loss: 0.538945  [   67/   88]
per-ex loss: 0.453688  [   68/   88]
per-ex loss: 0.430852  [   69/   88]
per-ex loss: 0.536370  [   70/   88]
per-ex loss: 0.610801  [   71/   88]
per-ex loss: 0.327223  [   72/   88]
per-ex loss: 0.589493  [   73/   88]
per-ex loss: 0.560303  [   74/   88]
per-ex loss: 0.430924  [   75/   88]
per-ex loss: 0.470466  [   76/   88]
per-ex loss: 0.460337  [   77/   88]
per-ex loss: 0.605940  [   78/   88]
per-ex loss: 0.580473  [   79/   88]
per-ex loss: 0.424012  [   80/   88]
per-ex loss: 0.604728  [   81/   88]
per-ex loss: 0.417272  [   82/   88]
per-ex loss: 0.433413  [   83/   88]
per-ex loss: 0.401672  [   84/   88]
per-ex loss: 0.657119  [   85/   88]
per-ex loss: 0.454059  [   86/   88]
per-ex loss: 0.412055  [   87/   88]
per-ex loss: 0.357543  [   88/   88]
Train Error: Avg loss: 0.49820835
validation Error: 
 Avg loss: 0.56887414 
 F1: 0.450333 
 Precision: 0.452531 
 Recall: 0.448157
 IoU: 0.290600

test Error: 
 Avg loss: 0.50935178 
 F1: 0.541880 
 Precision: 0.539238 
 Recall: 0.544547
 IoU: 0.371629

We have finished training iteration 39
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_29_.pth
per-ex loss: 0.530273  [    1/   88]
per-ex loss: 0.400044  [    2/   88]
per-ex loss: 0.439033  [    3/   88]
per-ex loss: 0.351369  [    4/   88]
per-ex loss: 0.356268  [    5/   88]
per-ex loss: 0.634724  [    6/   88]
per-ex loss: 0.705043  [    7/   88]
per-ex loss: 0.447296  [    8/   88]
per-ex loss: 0.349522  [    9/   88]
per-ex loss: 0.660538  [   10/   88]
per-ex loss: 0.423653  [   11/   88]
per-ex loss: 0.648347  [   12/   88]
per-ex loss: 0.412327  [   13/   88]
per-ex loss: 0.460516  [   14/   88]
per-ex loss: 0.436641  [   15/   88]
per-ex loss: 0.537159  [   16/   88]
per-ex loss: 0.439279  [   17/   88]
per-ex loss: 0.669392  [   18/   88]
per-ex loss: 0.361599  [   19/   88]
per-ex loss: 0.612177  [   20/   88]
per-ex loss: 0.354592  [   21/   88]
per-ex loss: 0.636542  [   22/   88]
per-ex loss: 0.670235  [   23/   88]
per-ex loss: 0.532979  [   24/   88]
per-ex loss: 0.428607  [   25/   88]
per-ex loss: 0.537062  [   26/   88]
per-ex loss: 0.598483  [   27/   88]
per-ex loss: 0.455052  [   28/   88]
per-ex loss: 0.573727  [   29/   88]
per-ex loss: 0.407982  [   30/   88]
per-ex loss: 0.375932  [   31/   88]
per-ex loss: 0.549021  [   32/   88]
per-ex loss: 0.328998  [   33/   88]
per-ex loss: 0.567313  [   34/   88]
per-ex loss: 0.483290  [   35/   88]
per-ex loss: 0.452888  [   36/   88]
per-ex loss: 0.471670  [   37/   88]
per-ex loss: 0.415845  [   38/   88]
per-ex loss: 0.581330  [   39/   88]
per-ex loss: 0.419318  [   40/   88]
per-ex loss: 0.437289  [   41/   88]
per-ex loss: 0.380327  [   42/   88]
per-ex loss: 0.590492  [   43/   88]
per-ex loss: 0.395741  [   44/   88]
per-ex loss: 0.503595  [   45/   88]
per-ex loss: 0.590119  [   46/   88]
per-ex loss: 0.673336  [   47/   88]
per-ex loss: 0.526624  [   48/   88]
per-ex loss: 0.440198  [   49/   88]
per-ex loss: 0.369340  [   50/   88]
per-ex loss: 0.655551  [   51/   88]
per-ex loss: 0.688547  [   52/   88]
per-ex loss: 0.437001  [   53/   88]
per-ex loss: 0.450022  [   54/   88]
per-ex loss: 0.373727  [   55/   88]
per-ex loss: 0.584339  [   56/   88]
per-ex loss: 0.368082  [   57/   88]
per-ex loss: 0.559918  [   58/   88]
per-ex loss: 0.531766  [   59/   88]
per-ex loss: 0.585275  [   60/   88]
per-ex loss: 0.435526  [   61/   88]
per-ex loss: 0.601102  [   62/   88]
per-ex loss: 0.356651  [   63/   88]
per-ex loss: 0.417325  [   64/   88]
per-ex loss: 0.619792  [   65/   88]
per-ex loss: 0.611997  [   66/   88]
per-ex loss: 0.535439  [   67/   88]
per-ex loss: 0.597463  [   68/   88]
per-ex loss: 0.696915  [   69/   88]
per-ex loss: 0.417829  [   70/   88]
per-ex loss: 0.375494  [   71/   88]
per-ex loss: 0.523335  [   72/   88]
per-ex loss: 0.419040  [   73/   88]
per-ex loss: 0.560085  [   74/   88]
per-ex loss: 0.612254  [   75/   88]
per-ex loss: 0.434509  [   76/   88]
per-ex loss: 0.407417  [   77/   88]
per-ex loss: 0.420988  [   78/   88]
per-ex loss: 0.400532  [   79/   88]
per-ex loss: 0.372140  [   80/   88]
per-ex loss: 0.534786  [   81/   88]
per-ex loss: 0.586699  [   82/   88]
per-ex loss: 0.374203  [   83/   88]
per-ex loss: 0.402059  [   84/   88]
per-ex loss: 0.390199  [   85/   88]
per-ex loss: 0.699414  [   86/   88]
per-ex loss: 0.529400  [   87/   88]
per-ex loss: 0.509751  [   88/   88]
Train Error: Avg loss: 0.49658747
validation Error: 
 Avg loss: 0.53562712 
 F1: 0.492072 
 Precision: 0.599367 
 Recall: 0.417358
 IoU: 0.326323

test Error: 
 Avg loss: 0.49099693 
 F1: 0.565552 
 Precision: 0.636834 
 Recall: 0.508621
 IoU: 0.394264

We have finished training iteration 40
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_37_.pth
per-ex loss: 0.703374  [    1/   88]
per-ex loss: 0.662436  [    2/   88]
per-ex loss: 0.401308  [    3/   88]
per-ex loss: 0.568564  [    4/   88]
per-ex loss: 0.558888  [    5/   88]
per-ex loss: 0.680904  [    6/   88]
per-ex loss: 0.403047  [    7/   88]
per-ex loss: 0.430208  [    8/   88]
per-ex loss: 0.623376  [    9/   88]
per-ex loss: 0.686276  [   10/   88]
per-ex loss: 0.329839  [   11/   88]
per-ex loss: 0.411457  [   12/   88]
per-ex loss: 0.630899  [   13/   88]
per-ex loss: 0.576409  [   14/   88]
per-ex loss: 0.417973  [   15/   88]
per-ex loss: 0.596264  [   16/   88]
per-ex loss: 0.432869  [   17/   88]
per-ex loss: 0.442045  [   18/   88]
per-ex loss: 0.373285  [   19/   88]
per-ex loss: 0.594598  [   20/   88]
per-ex loss: 0.656590  [   21/   88]
per-ex loss: 0.456220  [   22/   88]
per-ex loss: 0.585568  [   23/   88]
per-ex loss: 0.559674  [   24/   88]
per-ex loss: 0.526972  [   25/   88]
per-ex loss: 0.625733  [   26/   88]
per-ex loss: 0.450375  [   27/   88]
per-ex loss: 0.380468  [   28/   88]
per-ex loss: 0.403538  [   29/   88]
per-ex loss: 0.535028  [   30/   88]
per-ex loss: 0.660459  [   31/   88]
per-ex loss: 0.535569  [   32/   88]
per-ex loss: 0.566602  [   33/   88]
per-ex loss: 0.326234  [   34/   88]
per-ex loss: 0.474877  [   35/   88]
per-ex loss: 0.368738  [   36/   88]
per-ex loss: 0.557778  [   37/   88]
per-ex loss: 0.623540  [   38/   88]
per-ex loss: 0.454980  [   39/   88]
per-ex loss: 0.373437  [   40/   88]
per-ex loss: 0.585858  [   41/   88]
per-ex loss: 0.392258  [   42/   88]
per-ex loss: 0.554846  [   43/   88]
per-ex loss: 0.384216  [   44/   88]
per-ex loss: 0.441857  [   45/   88]
per-ex loss: 0.376588  [   46/   88]
per-ex loss: 0.639916  [   47/   88]
per-ex loss: 0.402418  [   48/   88]
per-ex loss: 0.422890  [   49/   88]
per-ex loss: 0.358894  [   50/   88]
per-ex loss: 0.604905  [   51/   88]
per-ex loss: 0.496262  [   52/   88]
per-ex loss: 0.528571  [   53/   88]
per-ex loss: 0.504991  [   54/   88]
per-ex loss: 0.421828  [   55/   88]
per-ex loss: 0.438625  [   56/   88]
per-ex loss: 0.412231  [   57/   88]
per-ex loss: 0.427935  [   58/   88]
per-ex loss: 0.357989  [   59/   88]
per-ex loss: 0.553752  [   60/   88]
per-ex loss: 0.445001  [   61/   88]
per-ex loss: 0.355934  [   62/   88]
per-ex loss: 0.677650  [   63/   88]
per-ex loss: 0.368860  [   64/   88]
per-ex loss: 0.535805  [   65/   88]
per-ex loss: 0.351232  [   66/   88]
per-ex loss: 0.639313  [   67/   88]
per-ex loss: 0.436841  [   68/   88]
per-ex loss: 0.669048  [   69/   88]
per-ex loss: 0.582212  [   70/   88]
per-ex loss: 0.373800  [   71/   88]
per-ex loss: 0.701130  [   72/   88]
per-ex loss: 0.444113  [   73/   88]
per-ex loss: 0.421891  [   74/   88]
per-ex loss: 0.543164  [   75/   88]
per-ex loss: 0.387647  [   76/   88]
per-ex loss: 0.448946  [   77/   88]
per-ex loss: 0.522662  [   78/   88]
per-ex loss: 0.556211  [   79/   88]
per-ex loss: 0.605943  [   80/   88]
per-ex loss: 0.405048  [   81/   88]
per-ex loss: 0.437945  [   82/   88]
per-ex loss: 0.416404  [   83/   88]
per-ex loss: 0.454323  [   84/   88]
per-ex loss: 0.528233  [   85/   88]
per-ex loss: 0.353963  [   86/   88]
per-ex loss: 0.401845  [   87/   88]
per-ex loss: 0.587230  [   88/   88]
Train Error: Avg loss: 0.49551849
validation Error: 
 Avg loss: 0.55655513 
 F1: 0.467404 
 Precision: 0.439621 
 Recall: 0.498936
 IoU: 0.304975

test Error: 
 Avg loss: 0.49082707 
 F1: 0.560693 
 Precision: 0.524150 
 Recall: 0.602712
 IoU: 0.389557

We have finished training iteration 41
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_39_.pth
per-ex loss: 0.535520  [    1/   88]
per-ex loss: 0.466935  [    2/   88]
per-ex loss: 0.666216  [    3/   88]
per-ex loss: 0.369006  [    4/   88]
per-ex loss: 0.346368  [    5/   88]
per-ex loss: 0.587439  [    6/   88]
per-ex loss: 0.457197  [    7/   88]
per-ex loss: 0.517754  [    8/   88]
per-ex loss: 0.599783  [    9/   88]
per-ex loss: 0.395543  [   10/   88]
per-ex loss: 0.666821  [   11/   88]
per-ex loss: 0.362446  [   12/   88]
per-ex loss: 0.408197  [   13/   88]
per-ex loss: 0.411959  [   14/   88]
per-ex loss: 0.398475  [   15/   88]
per-ex loss: 0.442249  [   16/   88]
per-ex loss: 0.456053  [   17/   88]
per-ex loss: 0.357412  [   18/   88]
per-ex loss: 0.604985  [   19/   88]
per-ex loss: 0.675854  [   20/   88]
per-ex loss: 0.365954  [   21/   88]
per-ex loss: 0.427318  [   22/   88]
per-ex loss: 0.422137  [   23/   88]
per-ex loss: 0.348224  [   24/   88]
per-ex loss: 0.449965  [   25/   88]
per-ex loss: 0.368038  [   26/   88]
per-ex loss: 0.609227  [   27/   88]
per-ex loss: 0.466474  [   28/   88]
per-ex loss: 0.420057  [   29/   88]
per-ex loss: 0.415973  [   30/   88]
per-ex loss: 0.604153  [   31/   88]
per-ex loss: 0.387286  [   32/   88]
per-ex loss: 0.511046  [   33/   88]
per-ex loss: 0.480602  [   34/   88]
per-ex loss: 0.529745  [   35/   88]
per-ex loss: 0.398709  [   36/   88]
per-ex loss: 0.541065  [   37/   88]
per-ex loss: 0.406280  [   38/   88]
per-ex loss: 0.688499  [   39/   88]
per-ex loss: 0.593075  [   40/   88]
per-ex loss: 0.461971  [   41/   88]
per-ex loss: 0.497461  [   42/   88]
per-ex loss: 0.356441  [   43/   88]
per-ex loss: 0.422098  [   44/   88]
per-ex loss: 0.422851  [   45/   88]
per-ex loss: 0.567894  [   46/   88]
per-ex loss: 0.587885  [   47/   88]
per-ex loss: 0.359127  [   48/   88]
per-ex loss: 0.450808  [   49/   88]
per-ex loss: 0.454946  [   50/   88]
per-ex loss: 0.376471  [   51/   88]
per-ex loss: 0.406534  [   52/   88]
per-ex loss: 0.393607  [   53/   88]
per-ex loss: 0.577375  [   54/   88]
per-ex loss: 0.548141  [   55/   88]
per-ex loss: 0.506189  [   56/   88]
per-ex loss: 0.392971  [   57/   88]
per-ex loss: 0.315846  [   58/   88]
per-ex loss: 0.763823  [   59/   88]
per-ex loss: 0.642132  [   60/   88]
per-ex loss: 0.490300  [   61/   88]
per-ex loss: 0.406172  [   62/   88]
per-ex loss: 0.622576  [   63/   88]
per-ex loss: 0.545360  [   64/   88]
per-ex loss: 0.582143  [   65/   88]
per-ex loss: 0.363430  [   66/   88]
per-ex loss: 0.545633  [   67/   88]
per-ex loss: 0.429256  [   68/   88]
per-ex loss: 0.391581  [   69/   88]
per-ex loss: 0.423026  [   70/   88]
per-ex loss: 0.661551  [   71/   88]
per-ex loss: 0.516120  [   72/   88]
per-ex loss: 0.584827  [   73/   88]
per-ex loss: 0.675875  [   74/   88]
per-ex loss: 0.566960  [   75/   88]
per-ex loss: 0.619326  [   76/   88]
per-ex loss: 0.381624  [   77/   88]
per-ex loss: 0.531826  [   78/   88]
per-ex loss: 0.558120  [   79/   88]
per-ex loss: 0.561957  [   80/   88]
per-ex loss: 0.630701  [   81/   88]
per-ex loss: 0.407740  [   82/   88]
per-ex loss: 0.691516  [   83/   88]
per-ex loss: 0.621608  [   84/   88]
per-ex loss: 0.620926  [   85/   88]
per-ex loss: 0.671535  [   86/   88]
per-ex loss: 0.382638  [   87/   88]
per-ex loss: 0.449031  [   88/   88]
Train Error: Avg loss: 0.49543144
validation Error: 
 Avg loss: 0.58661226 
 F1: 0.434629 
 Precision: 0.406033 
 Recall: 0.467558
 IoU: 0.277652

test Error: 
 Avg loss: 0.50580072 
 F1: 0.544175 
 Precision: 0.524063 
 Recall: 0.565893
 IoU: 0.373792

We have finished training iteration 42
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_38_.pth
per-ex loss: 0.521819  [    1/   88]
per-ex loss: 0.367965  [    2/   88]
per-ex loss: 0.403022  [    3/   88]
per-ex loss: 0.544593  [    4/   88]
per-ex loss: 0.541579  [    5/   88]
per-ex loss: 0.443066  [    6/   88]
per-ex loss: 0.544297  [    7/   88]
per-ex loss: 0.604937  [    8/   88]
per-ex loss: 0.569165  [    9/   88]
per-ex loss: 0.418480  [   10/   88]
per-ex loss: 0.541978  [   11/   88]
per-ex loss: 0.556442  [   12/   88]
per-ex loss: 0.476554  [   13/   88]
per-ex loss: 0.403852  [   14/   88]
per-ex loss: 0.419091  [   15/   88]
per-ex loss: 0.357045  [   16/   88]
per-ex loss: 0.500007  [   17/   88]
per-ex loss: 0.580791  [   18/   88]
per-ex loss: 0.400123  [   19/   88]
per-ex loss: 0.604233  [   20/   88]
per-ex loss: 0.647639  [   21/   88]
per-ex loss: 0.599769  [   22/   88]
per-ex loss: 0.395201  [   23/   88]
per-ex loss: 0.453442  [   24/   88]
per-ex loss: 0.351515  [   25/   88]
per-ex loss: 0.415507  [   26/   88]
per-ex loss: 0.517485  [   27/   88]
per-ex loss: 0.619621  [   28/   88]
per-ex loss: 0.406506  [   29/   88]
per-ex loss: 0.566808  [   30/   88]
per-ex loss: 0.541787  [   31/   88]
per-ex loss: 0.582728  [   32/   88]
per-ex loss: 0.371362  [   33/   88]
per-ex loss: 0.529690  [   34/   88]
per-ex loss: 0.681190  [   35/   88]
per-ex loss: 0.660253  [   36/   88]
per-ex loss: 0.508919  [   37/   88]
per-ex loss: 0.614072  [   38/   88]
per-ex loss: 0.304646  [   39/   88]
per-ex loss: 0.400604  [   40/   88]
per-ex loss: 0.418827  [   41/   88]
per-ex loss: 0.694938  [   42/   88]
per-ex loss: 0.395364  [   43/   88]
per-ex loss: 0.540106  [   44/   88]
per-ex loss: 0.661223  [   45/   88]
per-ex loss: 0.707471  [   46/   88]
per-ex loss: 0.385595  [   47/   88]
per-ex loss: 0.520262  [   48/   88]
per-ex loss: 0.700950  [   49/   88]
per-ex loss: 0.407989  [   50/   88]
per-ex loss: 0.518848  [   51/   88]
per-ex loss: 0.457331  [   52/   88]
per-ex loss: 0.502476  [   53/   88]
per-ex loss: 0.561676  [   54/   88]
per-ex loss: 0.456927  [   55/   88]
per-ex loss: 0.441319  [   56/   88]
per-ex loss: 0.399382  [   57/   88]
per-ex loss: 0.449719  [   58/   88]
per-ex loss: 0.641578  [   59/   88]
per-ex loss: 0.374360  [   60/   88]
per-ex loss: 0.424348  [   61/   88]
per-ex loss: 0.439830  [   62/   88]
per-ex loss: 0.373702  [   63/   88]
per-ex loss: 0.630886  [   64/   88]
per-ex loss: 0.653519  [   65/   88]
per-ex loss: 0.439661  [   66/   88]
per-ex loss: 0.418328  [   67/   88]
per-ex loss: 0.454349  [   68/   88]
per-ex loss: 0.632464  [   69/   88]
per-ex loss: 0.362582  [   70/   88]
per-ex loss: 0.413357  [   71/   88]
per-ex loss: 0.405099  [   72/   88]
per-ex loss: 0.418619  [   73/   88]
per-ex loss: 0.326960  [   74/   88]
per-ex loss: 0.585376  [   75/   88]
per-ex loss: 0.417752  [   76/   88]
per-ex loss: 0.416115  [   77/   88]
per-ex loss: 0.625784  [   78/   88]
per-ex loss: 0.618830  [   79/   88]
per-ex loss: 0.668232  [   80/   88]
per-ex loss: 0.358005  [   81/   88]
per-ex loss: 0.393466  [   82/   88]
per-ex loss: 0.418569  [   83/   88]
per-ex loss: 0.360938  [   84/   88]
per-ex loss: 0.589734  [   85/   88]
per-ex loss: 0.366822  [   86/   88]
per-ex loss: 0.422372  [   87/   88]
per-ex loss: 0.592710  [   88/   88]
Train Error: Avg loss: 0.49355150
validation Error: 
 Avg loss: 0.55070154 
 F1: 0.473854 
 Precision: 0.580260 
 Recall: 0.400426
 IoU: 0.310491

test Error: 
 Avg loss: 0.49987362 
 F1: 0.554592 
 Precision: 0.620686 
 Recall: 0.501219
 IoU: 0.383692

We have finished training iteration 43
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_41_.pth
per-ex loss: 0.527751  [    1/   88]
per-ex loss: 0.357539  [    2/   88]
per-ex loss: 0.409380  [    3/   88]
per-ex loss: 0.562197  [    4/   88]
per-ex loss: 0.575825  [    5/   88]
per-ex loss: 0.362679  [    6/   88]
per-ex loss: 0.442557  [    7/   88]
per-ex loss: 0.516212  [    8/   88]
per-ex loss: 0.430672  [    9/   88]
per-ex loss: 0.353275  [   10/   88]
per-ex loss: 0.516397  [   11/   88]
per-ex loss: 0.446767  [   12/   88]
per-ex loss: 0.421272  [   13/   88]
per-ex loss: 0.656311  [   14/   88]
per-ex loss: 0.388124  [   15/   88]
per-ex loss: 0.461265  [   16/   88]
per-ex loss: 0.405407  [   17/   88]
per-ex loss: 0.577200  [   18/   88]
per-ex loss: 0.404692  [   19/   88]
per-ex loss: 0.368650  [   20/   88]
per-ex loss: 0.521465  [   21/   88]
per-ex loss: 0.560637  [   22/   88]
per-ex loss: 0.444875  [   23/   88]
per-ex loss: 0.620054  [   24/   88]
per-ex loss: 0.600556  [   25/   88]
per-ex loss: 0.424408  [   26/   88]
per-ex loss: 0.595071  [   27/   88]
per-ex loss: 0.394621  [   28/   88]
per-ex loss: 0.449320  [   29/   88]
per-ex loss: 0.364287  [   30/   88]
per-ex loss: 0.504711  [   31/   88]
per-ex loss: 0.400334  [   32/   88]
per-ex loss: 0.609258  [   33/   88]
per-ex loss: 0.409036  [   34/   88]
per-ex loss: 0.593387  [   35/   88]
per-ex loss: 0.388753  [   36/   88]
per-ex loss: 0.684863  [   37/   88]
per-ex loss: 0.350927  [   38/   88]
per-ex loss: 0.513445  [   39/   88]
per-ex loss: 0.315285  [   40/   88]
per-ex loss: 0.468738  [   41/   88]
per-ex loss: 0.443022  [   42/   88]
per-ex loss: 0.667620  [   43/   88]
per-ex loss: 0.390278  [   44/   88]
per-ex loss: 0.410013  [   45/   88]
per-ex loss: 0.546800  [   46/   88]
per-ex loss: 0.385622  [   47/   88]
per-ex loss: 0.367104  [   48/   88]
per-ex loss: 0.440465  [   49/   88]
per-ex loss: 0.585040  [   50/   88]
per-ex loss: 0.483996  [   51/   88]
per-ex loss: 0.404271  [   52/   88]
per-ex loss: 0.379691  [   53/   88]
per-ex loss: 0.622705  [   54/   88]
per-ex loss: 0.588718  [   55/   88]
per-ex loss: 0.606508  [   56/   88]
per-ex loss: 0.572846  [   57/   88]
per-ex loss: 0.425669  [   58/   88]
per-ex loss: 0.342480  [   59/   88]
per-ex loss: 0.548180  [   60/   88]
per-ex loss: 0.654361  [   61/   88]
per-ex loss: 0.627093  [   62/   88]
per-ex loss: 0.465184  [   63/   88]
per-ex loss: 0.440163  [   64/   88]
per-ex loss: 0.535696  [   65/   88]
per-ex loss: 0.422934  [   66/   88]
per-ex loss: 0.690890  [   67/   88]
per-ex loss: 0.396090  [   68/   88]
per-ex loss: 0.434230  [   69/   88]
per-ex loss: 0.698917  [   70/   88]
per-ex loss: 0.400043  [   71/   88]
per-ex loss: 0.662618  [   72/   88]
per-ex loss: 0.604015  [   73/   88]
per-ex loss: 0.622529  [   74/   88]
per-ex loss: 0.629431  [   75/   88]
per-ex loss: 0.432449  [   76/   88]
per-ex loss: 0.444484  [   77/   88]
per-ex loss: 0.548032  [   78/   88]
per-ex loss: 0.473130  [   79/   88]
per-ex loss: 0.369022  [   80/   88]
per-ex loss: 0.583702  [   81/   88]
per-ex loss: 0.510714  [   82/   88]
per-ex loss: 0.552104  [   83/   88]
per-ex loss: 0.412117  [   84/   88]
per-ex loss: 0.356914  [   85/   88]
per-ex loss: 0.368270  [   86/   88]
per-ex loss: 0.712708  [   87/   88]
per-ex loss: 0.624371  [   88/   88]
Train Error: Avg loss: 0.49183459
validation Error: 
 Avg loss: 0.53637016 
 F1: 0.491610 
 Precision: 0.574619 
 Recall: 0.429557
 IoU: 0.325917

test Error: 
 Avg loss: 0.48937614 
 F1: 0.564519 
 Precision: 0.630913 
 Recall: 0.510768
 IoU: 0.393261

We have finished training iteration 44
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_42_.pth
per-ex loss: 0.709018  [    1/   88]
per-ex loss: 0.396866  [    2/   88]
per-ex loss: 0.410729  [    3/   88]
per-ex loss: 0.562567  [    4/   88]
per-ex loss: 0.375992  [    5/   88]
per-ex loss: 0.363016  [    6/   88]
per-ex loss: 0.531799  [    7/   88]
per-ex loss: 0.399042  [    8/   88]
per-ex loss: 0.579110  [    9/   88]
per-ex loss: 0.565545  [   10/   88]
per-ex loss: 0.421605  [   11/   88]
per-ex loss: 0.591881  [   12/   88]
per-ex loss: 0.415687  [   13/   88]
per-ex loss: 0.538738  [   14/   88]
per-ex loss: 0.623750  [   15/   88]
per-ex loss: 0.552507  [   16/   88]
per-ex loss: 0.582624  [   17/   88]
per-ex loss: 0.398573  [   18/   88]
per-ex loss: 0.499684  [   19/   88]
per-ex loss: 0.400855  [   20/   88]
per-ex loss: 0.421461  [   21/   88]
per-ex loss: 0.459946  [   22/   88]
per-ex loss: 0.419485  [   23/   88]
per-ex loss: 0.517272  [   24/   88]
per-ex loss: 0.390833  [   25/   88]
per-ex loss: 0.437462  [   26/   88]
per-ex loss: 0.448360  [   27/   88]
per-ex loss: 0.577352  [   28/   88]
per-ex loss: 0.688954  [   29/   88]
per-ex loss: 0.413756  [   30/   88]
per-ex loss: 0.429531  [   31/   88]
per-ex loss: 0.683208  [   32/   88]
per-ex loss: 0.398109  [   33/   88]
per-ex loss: 0.422160  [   34/   88]
per-ex loss: 0.636127  [   35/   88]
per-ex loss: 0.627200  [   36/   88]
per-ex loss: 0.392191  [   37/   88]
per-ex loss: 0.516055  [   38/   88]
per-ex loss: 0.624192  [   39/   88]
per-ex loss: 0.432563  [   40/   88]
per-ex loss: 0.447990  [   41/   88]
per-ex loss: 0.486811  [   42/   88]
per-ex loss: 0.422560  [   43/   88]
per-ex loss: 0.352572  [   44/   88]
per-ex loss: 0.421194  [   45/   88]
per-ex loss: 0.377977  [   46/   88]
per-ex loss: 0.591812  [   47/   88]
per-ex loss: 0.456938  [   48/   88]
per-ex loss: 0.493271  [   49/   88]
per-ex loss: 0.548103  [   50/   88]
per-ex loss: 0.521467  [   51/   88]
per-ex loss: 0.334154  [   52/   88]
per-ex loss: 0.410610  [   53/   88]
per-ex loss: 0.616019  [   54/   88]
per-ex loss: 0.313562  [   55/   88]
per-ex loss: 0.439961  [   56/   88]
per-ex loss: 0.690585  [   57/   88]
per-ex loss: 0.362003  [   58/   88]
per-ex loss: 0.609938  [   59/   88]
per-ex loss: 0.365289  [   60/   88]
per-ex loss: 0.672435  [   61/   88]
per-ex loss: 0.537599  [   62/   88]
per-ex loss: 0.465757  [   63/   88]
per-ex loss: 0.570935  [   64/   88]
per-ex loss: 0.638827  [   65/   88]
per-ex loss: 0.563369  [   66/   88]
per-ex loss: 0.495712  [   67/   88]
per-ex loss: 0.336514  [   68/   88]
per-ex loss: 0.443098  [   69/   88]
per-ex loss: 0.612938  [   70/   88]
per-ex loss: 0.597313  [   71/   88]
per-ex loss: 0.420555  [   72/   88]
per-ex loss: 0.425211  [   73/   88]
per-ex loss: 0.455873  [   74/   88]
per-ex loss: 0.641438  [   75/   88]
per-ex loss: 0.626776  [   76/   88]
per-ex loss: 0.378105  [   77/   88]
per-ex loss: 0.354642  [   78/   88]
per-ex loss: 0.697899  [   79/   88]
per-ex loss: 0.353782  [   80/   88]
per-ex loss: 0.675324  [   81/   88]
per-ex loss: 0.411690  [   82/   88]
per-ex loss: 0.587080  [   83/   88]
per-ex loss: 0.381955  [   84/   88]
per-ex loss: 0.450586  [   85/   88]
per-ex loss: 0.538310  [   86/   88]
per-ex loss: 0.340391  [   87/   88]
per-ex loss: 0.643250  [   88/   88]
Train Error: Avg loss: 0.49359076
validation Error: 
 Avg loss: 0.54489895 
 F1: 0.480048 
 Precision: 0.636029 
 Recall: 0.385505
 IoU: 0.315831

test Error: 
 Avg loss: 0.49561460 
 F1: 0.560231 
 Precision: 0.660101 
 Recall: 0.486610
 IoU: 0.389112

We have finished training iteration 45
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_43_.pth
per-ex loss: 0.643245  [    1/   88]
per-ex loss: 0.542521  [    2/   88]
per-ex loss: 0.667283  [    3/   88]
per-ex loss: 0.444417  [    4/   88]
per-ex loss: 0.514736  [    5/   88]
per-ex loss: 0.375827  [    6/   88]
per-ex loss: 0.426768  [    7/   88]
per-ex loss: 0.463102  [    8/   88]
per-ex loss: 0.398216  [    9/   88]
per-ex loss: 0.714823  [   10/   88]
per-ex loss: 0.557599  [   11/   88]
per-ex loss: 0.546889  [   12/   88]
per-ex loss: 0.379321  [   13/   88]
per-ex loss: 0.411577  [   14/   88]
per-ex loss: 0.629629  [   15/   88]
per-ex loss: 0.669509  [   16/   88]
per-ex loss: 0.377103  [   17/   88]
per-ex loss: 0.597744  [   18/   88]
per-ex loss: 0.399465  [   19/   88]
per-ex loss: 0.570483  [   20/   88]
per-ex loss: 0.411754  [   21/   88]
per-ex loss: 0.466304  [   22/   88]
per-ex loss: 0.412376  [   23/   88]
per-ex loss: 0.525386  [   24/   88]
per-ex loss: 0.554884  [   25/   88]
per-ex loss: 0.524950  [   26/   88]
per-ex loss: 0.590092  [   27/   88]
per-ex loss: 0.505052  [   28/   88]
per-ex loss: 0.342516  [   29/   88]
per-ex loss: 0.325691  [   30/   88]
per-ex loss: 0.573810  [   31/   88]
per-ex loss: 0.664958  [   32/   88]
per-ex loss: 0.406134  [   33/   88]
per-ex loss: 0.421416  [   34/   88]
per-ex loss: 0.657913  [   35/   88]
per-ex loss: 0.636911  [   36/   88]
per-ex loss: 0.302130  [   37/   88]
per-ex loss: 0.627562  [   38/   88]
per-ex loss: 0.422293  [   39/   88]
per-ex loss: 0.415108  [   40/   88]
per-ex loss: 0.460218  [   41/   88]
per-ex loss: 0.418848  [   42/   88]
per-ex loss: 0.417695  [   43/   88]
per-ex loss: 0.568316  [   44/   88]
per-ex loss: 0.352210  [   45/   88]
per-ex loss: 0.543994  [   46/   88]
per-ex loss: 0.410049  [   47/   88]
per-ex loss: 0.412420  [   48/   88]
per-ex loss: 0.461608  [   49/   88]
per-ex loss: 0.399693  [   50/   88]
per-ex loss: 0.453421  [   51/   88]
per-ex loss: 0.385190  [   52/   88]
per-ex loss: 0.372908  [   53/   88]
per-ex loss: 0.569702  [   54/   88]
per-ex loss: 0.414757  [   55/   88]
per-ex loss: 0.405123  [   56/   88]
per-ex loss: 0.436765  [   57/   88]
per-ex loss: 0.388910  [   58/   88]
per-ex loss: 0.512359  [   59/   88]
per-ex loss: 0.375547  [   60/   88]
per-ex loss: 0.503651  [   61/   88]
per-ex loss: 0.364586  [   62/   88]
per-ex loss: 0.359863  [   63/   88]
per-ex loss: 0.661347  [   64/   88]
per-ex loss: 0.619107  [   65/   88]
per-ex loss: 0.433026  [   66/   88]
per-ex loss: 0.425548  [   67/   88]
per-ex loss: 0.539462  [   68/   88]
per-ex loss: 0.490197  [   69/   88]
per-ex loss: 0.444450  [   70/   88]
per-ex loss: 0.524430  [   71/   88]
per-ex loss: 0.444388  [   72/   88]
per-ex loss: 0.359247  [   73/   88]
per-ex loss: 0.545650  [   74/   88]
per-ex loss: 0.409419  [   75/   88]
per-ex loss: 0.572373  [   76/   88]
per-ex loss: 0.431531  [   77/   88]
per-ex loss: 0.423360  [   78/   88]
per-ex loss: 0.621383  [   79/   88]
per-ex loss: 0.348404  [   80/   88]
per-ex loss: 0.644414  [   81/   88]
per-ex loss: 0.584790  [   82/   88]
per-ex loss: 0.678544  [   83/   88]
per-ex loss: 0.685273  [   84/   88]
per-ex loss: 0.402090  [   85/   88]
per-ex loss: 0.595325  [   86/   88]
per-ex loss: 0.685420  [   87/   88]
per-ex loss: 0.635871  [   88/   88]
Train Error: Avg loss: 0.49216340
validation Error: 
 Avg loss: 0.55475544 
 F1: 0.464905 
 Precision: 0.482758 
 Recall: 0.448326
 IoU: 0.302851

test Error: 
 Avg loss: 0.49101885 
 F1: 0.561056 
 Precision: 0.568368 
 Recall: 0.553929
 IoU: 0.389908

We have finished training iteration 46
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_34_.pth
per-ex loss: 0.383065  [    1/   88]
per-ex loss: 0.549074  [    2/   88]
per-ex loss: 0.455617  [    3/   88]
per-ex loss: 0.440447  [    4/   88]
per-ex loss: 0.689107  [    5/   88]
per-ex loss: 0.422047  [    6/   88]
per-ex loss: 0.493710  [    7/   88]
per-ex loss: 0.512554  [    8/   88]
per-ex loss: 0.394354  [    9/   88]
per-ex loss: 0.615010  [   10/   88]
per-ex loss: 0.540377  [   11/   88]
per-ex loss: 0.450329  [   12/   88]
per-ex loss: 0.578276  [   13/   88]
per-ex loss: 0.397291  [   14/   88]
per-ex loss: 0.535277  [   15/   88]
per-ex loss: 0.394402  [   16/   88]
per-ex loss: 0.437671  [   17/   88]
per-ex loss: 0.581804  [   18/   88]
per-ex loss: 0.572621  [   19/   88]
per-ex loss: 0.404401  [   20/   88]
per-ex loss: 0.458147  [   21/   88]
per-ex loss: 0.397287  [   22/   88]
per-ex loss: 0.633568  [   23/   88]
per-ex loss: 0.607247  [   24/   88]
per-ex loss: 0.442887  [   25/   88]
per-ex loss: 0.500405  [   26/   88]
per-ex loss: 0.378641  [   27/   88]
per-ex loss: 0.464916  [   28/   88]
per-ex loss: 0.384092  [   29/   88]
per-ex loss: 0.553237  [   30/   88]
per-ex loss: 0.812482  [   31/   88]
per-ex loss: 0.572522  [   32/   88]
per-ex loss: 0.387541  [   33/   88]
per-ex loss: 0.523377  [   34/   88]
per-ex loss: 0.336648  [   35/   88]
per-ex loss: 0.375294  [   36/   88]
per-ex loss: 0.633672  [   37/   88]
per-ex loss: 0.671867  [   38/   88]
per-ex loss: 0.580191  [   39/   88]
per-ex loss: 0.577381  [   40/   88]
per-ex loss: 0.345478  [   41/   88]
per-ex loss: 0.393919  [   42/   88]
per-ex loss: 0.685147  [   43/   88]
per-ex loss: 0.590082  [   44/   88]
per-ex loss: 0.438530  [   45/   88]
per-ex loss: 0.559423  [   46/   88]
per-ex loss: 0.464438  [   47/   88]
per-ex loss: 0.332886  [   48/   88]
per-ex loss: 0.456533  [   49/   88]
per-ex loss: 0.351046  [   50/   88]
per-ex loss: 0.353502  [   51/   88]
per-ex loss: 0.628798  [   52/   88]
per-ex loss: 0.543942  [   53/   88]
per-ex loss: 0.427137  [   54/   88]
per-ex loss: 0.668972  [   55/   88]
per-ex loss: 0.400972  [   56/   88]
per-ex loss: 0.548993  [   57/   88]
per-ex loss: 0.433012  [   58/   88]
per-ex loss: 0.353120  [   59/   88]
per-ex loss: 0.694649  [   60/   88]
per-ex loss: 0.410999  [   61/   88]
per-ex loss: 0.668940  [   62/   88]
per-ex loss: 0.334743  [   63/   88]
per-ex loss: 0.493933  [   64/   88]
per-ex loss: 0.478537  [   65/   88]
per-ex loss: 0.500176  [   66/   88]
per-ex loss: 0.559666  [   67/   88]
per-ex loss: 0.416869  [   68/   88]
per-ex loss: 0.525109  [   69/   88]
per-ex loss: 0.365497  [   70/   88]
per-ex loss: 0.354674  [   71/   88]
per-ex loss: 0.384138  [   72/   88]
per-ex loss: 0.397913  [   73/   88]
per-ex loss: 0.435280  [   74/   88]
per-ex loss: 0.560976  [   75/   88]
per-ex loss: 0.610545  [   76/   88]
per-ex loss: 0.591243  [   77/   88]
per-ex loss: 0.403333  [   78/   88]
per-ex loss: 0.392512  [   79/   88]
per-ex loss: 0.708494  [   80/   88]
per-ex loss: 0.403450  [   81/   88]
per-ex loss: 0.417413  [   82/   88]
per-ex loss: 0.442296  [   83/   88]
per-ex loss: 0.504401  [   84/   88]
per-ex loss: 0.673438  [   85/   88]
per-ex loss: 0.413327  [   86/   88]
per-ex loss: 0.636404  [   87/   88]
per-ex loss: 0.416749  [   88/   88]
Train Error: Avg loss: 0.49216446
validation Error: 
 Avg loss: 0.54208150 
 F1: 0.484617 
 Precision: 0.596449 
 Recall: 0.408100
 IoU: 0.319799

test Error: 
 Avg loss: 0.49663173 
 F1: 0.558948 
 Precision: 0.669018 
 Recall: 0.479979
 IoU: 0.387875

We have finished training iteration 47
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_45_.pth
per-ex loss: 0.393357  [    1/   88]
per-ex loss: 0.335912  [    2/   88]
per-ex loss: 0.396347  [    3/   88]
per-ex loss: 0.580302  [    4/   88]
per-ex loss: 0.390980  [    5/   88]
per-ex loss: 0.346710  [    6/   88]
per-ex loss: 0.638204  [    7/   88]
per-ex loss: 0.653437  [    8/   88]
per-ex loss: 0.600637  [    9/   88]
per-ex loss: 0.396433  [   10/   88]
per-ex loss: 0.544037  [   11/   88]
per-ex loss: 0.355557  [   12/   88]
per-ex loss: 0.504039  [   13/   88]
per-ex loss: 0.659776  [   14/   88]
per-ex loss: 0.684838  [   15/   88]
per-ex loss: 0.637242  [   16/   88]
per-ex loss: 0.379409  [   17/   88]
per-ex loss: 0.493250  [   18/   88]
per-ex loss: 0.359829  [   19/   88]
per-ex loss: 0.477139  [   20/   88]
per-ex loss: 0.412739  [   21/   88]
per-ex loss: 0.637912  [   22/   88]
per-ex loss: 0.461659  [   23/   88]
per-ex loss: 0.544764  [   24/   88]
per-ex loss: 0.433729  [   25/   88]
per-ex loss: 0.526186  [   26/   88]
per-ex loss: 0.646728  [   27/   88]
per-ex loss: 0.629991  [   28/   88]
per-ex loss: 0.569757  [   29/   88]
per-ex loss: 0.437178  [   30/   88]
per-ex loss: 0.438683  [   31/   88]
per-ex loss: 0.335607  [   32/   88]
per-ex loss: 0.507413  [   33/   88]
per-ex loss: 0.587962  [   34/   88]
per-ex loss: 0.571786  [   35/   88]
per-ex loss: 0.596945  [   36/   88]
per-ex loss: 0.412113  [   37/   88]
per-ex loss: 0.544360  [   38/   88]
per-ex loss: 0.684579  [   39/   88]
per-ex loss: 0.462189  [   40/   88]
per-ex loss: 0.406101  [   41/   88]
per-ex loss: 0.609560  [   42/   88]
per-ex loss: 0.470610  [   43/   88]
per-ex loss: 0.434246  [   44/   88]
per-ex loss: 0.385706  [   45/   88]
per-ex loss: 0.560049  [   46/   88]
per-ex loss: 0.580755  [   47/   88]
per-ex loss: 0.405035  [   48/   88]
per-ex loss: 0.421698  [   49/   88]
per-ex loss: 0.440905  [   50/   88]
per-ex loss: 0.414546  [   51/   88]
per-ex loss: 0.418881  [   52/   88]
per-ex loss: 0.432313  [   53/   88]
per-ex loss: 0.535560  [   54/   88]
per-ex loss: 0.604100  [   55/   88]
per-ex loss: 0.381268  [   56/   88]
per-ex loss: 0.537720  [   57/   88]
per-ex loss: 0.457783  [   58/   88]
per-ex loss: 0.508956  [   59/   88]
per-ex loss: 0.496444  [   60/   88]
per-ex loss: 0.399728  [   61/   88]
per-ex loss: 0.351574  [   62/   88]
per-ex loss: 0.621798  [   63/   88]
per-ex loss: 0.441875  [   64/   88]
per-ex loss: 0.599661  [   65/   88]
per-ex loss: 0.405134  [   66/   88]
per-ex loss: 0.627472  [   67/   88]
per-ex loss: 0.361654  [   68/   88]
per-ex loss: 0.416050  [   69/   88]
per-ex loss: 0.350429  [   70/   88]
per-ex loss: 0.412846  [   71/   88]
per-ex loss: 0.405814  [   72/   88]
per-ex loss: 0.343368  [   73/   88]
per-ex loss: 0.458682  [   74/   88]
per-ex loss: 0.610791  [   75/   88]
per-ex loss: 0.502138  [   76/   88]
per-ex loss: 0.569900  [   77/   88]
per-ex loss: 0.689275  [   78/   88]
per-ex loss: 0.573629  [   79/   88]
per-ex loss: 0.430185  [   80/   88]
per-ex loss: 0.518149  [   81/   88]
per-ex loss: 0.399243  [   82/   88]
per-ex loss: 0.536013  [   83/   88]
per-ex loss: 0.377588  [   84/   88]
per-ex loss: 0.455345  [   85/   88]
per-ex loss: 0.625570  [   86/   88]
per-ex loss: 0.401214  [   87/   88]
per-ex loss: 0.389032  [   88/   88]
Train Error: Avg loss: 0.48916035
validation Error: 
 Avg loss: 0.54792559 
 F1: 0.480109 
 Precision: 0.506929 
 Recall: 0.455985
 IoU: 0.315884

test Error: 
 Avg loss: 0.48999716 
 F1: 0.565780 
 Precision: 0.587030 
 Recall: 0.546016
 IoU: 0.394486

We have finished training iteration 48
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_46_.pth
per-ex loss: 0.398497  [    1/   88]
per-ex loss: 0.390004  [    2/   88]
per-ex loss: 0.406645  [    3/   88]
per-ex loss: 0.568188  [    4/   88]
per-ex loss: 0.412735  [    5/   88]
per-ex loss: 0.429921  [    6/   88]
per-ex loss: 0.646144  [    7/   88]
per-ex loss: 0.438164  [    8/   88]
per-ex loss: 0.585015  [    9/   88]
per-ex loss: 0.428834  [   10/   88]
per-ex loss: 0.513046  [   11/   88]
per-ex loss: 0.531719  [   12/   88]
per-ex loss: 0.318771  [   13/   88]
per-ex loss: 0.425524  [   14/   88]
per-ex loss: 0.549247  [   15/   88]
per-ex loss: 0.538597  [   16/   88]
per-ex loss: 0.532040  [   17/   88]
per-ex loss: 0.696199  [   18/   88]
per-ex loss: 0.441546  [   19/   88]
per-ex loss: 0.623114  [   20/   88]
per-ex loss: 0.389701  [   21/   88]
per-ex loss: 0.559889  [   22/   88]
per-ex loss: 0.458007  [   23/   88]
per-ex loss: 0.524912  [   24/   88]
per-ex loss: 0.451837  [   25/   88]
per-ex loss: 0.431238  [   26/   88]
per-ex loss: 0.612327  [   27/   88]
per-ex loss: 0.445462  [   28/   88]
per-ex loss: 0.536616  [   29/   88]
per-ex loss: 0.459935  [   30/   88]
per-ex loss: 0.648241  [   31/   88]
per-ex loss: 0.675111  [   32/   88]
per-ex loss: 0.565685  [   33/   88]
per-ex loss: 0.476195  [   34/   88]
per-ex loss: 0.505816  [   35/   88]
per-ex loss: 0.414455  [   36/   88]
per-ex loss: 0.369133  [   37/   88]
per-ex loss: 0.378221  [   38/   88]
per-ex loss: 0.507019  [   39/   88]
per-ex loss: 0.373223  [   40/   88]
per-ex loss: 0.460100  [   41/   88]
per-ex loss: 0.479517  [   42/   88]
per-ex loss: 0.418516  [   43/   88]
per-ex loss: 0.597930  [   44/   88]
per-ex loss: 0.446327  [   45/   88]
per-ex loss: 0.383098  [   46/   88]
per-ex loss: 0.375147  [   47/   88]
per-ex loss: 0.403227  [   48/   88]
per-ex loss: 0.545682  [   49/   88]
per-ex loss: 0.666115  [   50/   88]
per-ex loss: 0.598711  [   51/   88]
per-ex loss: 0.662352  [   52/   88]
per-ex loss: 0.418607  [   53/   88]
per-ex loss: 0.427776  [   54/   88]
per-ex loss: 0.553393  [   55/   88]
per-ex loss: 0.497711  [   56/   88]
per-ex loss: 0.578133  [   57/   88]
per-ex loss: 0.365671  [   58/   88]
per-ex loss: 0.381139  [   59/   88]
per-ex loss: 0.530891  [   60/   88]
per-ex loss: 0.608736  [   61/   88]
per-ex loss: 0.690588  [   62/   88]
per-ex loss: 0.364139  [   63/   88]
per-ex loss: 0.437403  [   64/   88]
per-ex loss: 0.358539  [   65/   88]
per-ex loss: 0.644793  [   66/   88]
per-ex loss: 0.477881  [   67/   88]
per-ex loss: 0.381697  [   68/   88]
per-ex loss: 0.564249  [   69/   88]
per-ex loss: 0.386784  [   70/   88]
per-ex loss: 0.607567  [   71/   88]
per-ex loss: 0.394602  [   72/   88]
per-ex loss: 0.698978  [   73/   88]
per-ex loss: 0.575386  [   74/   88]
per-ex loss: 0.529628  [   75/   88]
per-ex loss: 0.332998  [   76/   88]
per-ex loss: 0.429117  [   77/   88]
per-ex loss: 0.593270  [   78/   88]
per-ex loss: 0.387252  [   79/   88]
per-ex loss: 0.604504  [   80/   88]
per-ex loss: 0.552600  [   81/   88]
per-ex loss: 0.380450  [   82/   88]
per-ex loss: 0.401288  [   83/   88]
per-ex loss: 0.693239  [   84/   88]
per-ex loss: 0.346761  [   85/   88]
per-ex loss: 0.436012  [   86/   88]
per-ex loss: 0.389260  [   87/   88]
per-ex loss: 0.527551  [   88/   88]
Train Error: Avg loss: 0.49138964
validation Error: 
 Avg loss: 0.54765064 
 F1: 0.474465 
 Precision: 0.570907 
 Recall: 0.405897
 IoU: 0.311015

test Error: 
 Avg loss: 0.49090588 
 F1: 0.562812 
 Precision: 0.647034 
 Recall: 0.497991
 IoU: 0.391607

We have finished training iteration 49
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_47_.pth
per-ex loss: 0.675815  [    1/   88]
per-ex loss: 0.577541  [    2/   88]
per-ex loss: 0.536691  [    3/   88]
per-ex loss: 0.516260  [    4/   88]
per-ex loss: 0.573403  [    5/   88]
per-ex loss: 0.579670  [    6/   88]
per-ex loss: 0.550343  [    7/   88]
per-ex loss: 0.297268  [    8/   88]
per-ex loss: 0.394968  [    9/   88]
per-ex loss: 0.658887  [   10/   88]
per-ex loss: 0.508575  [   11/   88]
per-ex loss: 0.429896  [   12/   88]
per-ex loss: 0.397401  [   13/   88]
per-ex loss: 0.403701  [   14/   88]
per-ex loss: 0.415550  [   15/   88]
per-ex loss: 0.451843  [   16/   88]
per-ex loss: 0.502690  [   17/   88]
per-ex loss: 0.572073  [   18/   88]
per-ex loss: 0.454434  [   19/   88]
per-ex loss: 0.403819  [   20/   88]
per-ex loss: 0.579026  [   21/   88]
per-ex loss: 0.421716  [   22/   88]
per-ex loss: 0.344482  [   23/   88]
per-ex loss: 0.409045  [   24/   88]
per-ex loss: 0.363316  [   25/   88]
per-ex loss: 0.354901  [   26/   88]
per-ex loss: 0.407901  [   27/   88]
per-ex loss: 0.479027  [   28/   88]
per-ex loss: 0.368077  [   29/   88]
per-ex loss: 0.407977  [   30/   88]
per-ex loss: 0.577648  [   31/   88]
per-ex loss: 0.596541  [   32/   88]
per-ex loss: 0.354593  [   33/   88]
per-ex loss: 0.636446  [   34/   88]
per-ex loss: 0.407088  [   35/   88]
per-ex loss: 0.444155  [   36/   88]
per-ex loss: 0.620763  [   37/   88]
per-ex loss: 0.590054  [   38/   88]
per-ex loss: 0.436984  [   39/   88]
per-ex loss: 0.401969  [   40/   88]
per-ex loss: 0.413075  [   41/   88]
per-ex loss: 0.369570  [   42/   88]
per-ex loss: 0.635027  [   43/   88]
per-ex loss: 0.632875  [   44/   88]
per-ex loss: 0.616159  [   45/   88]
per-ex loss: 0.380353  [   46/   88]
per-ex loss: 0.372119  [   47/   88]
per-ex loss: 0.451255  [   48/   88]
per-ex loss: 0.642808  [   49/   88]
per-ex loss: 0.358783  [   50/   88]
per-ex loss: 0.567440  [   51/   88]
per-ex loss: 0.745084  [   52/   88]
per-ex loss: 0.544426  [   53/   88]
per-ex loss: 0.467827  [   54/   88]
per-ex loss: 0.409885  [   55/   88]
per-ex loss: 0.588753  [   56/   88]
per-ex loss: 0.384051  [   57/   88]
per-ex loss: 0.402899  [   58/   88]
per-ex loss: 0.363606  [   59/   88]
per-ex loss: 0.459982  [   60/   88]
per-ex loss: 0.556348  [   61/   88]
per-ex loss: 0.509534  [   62/   88]
per-ex loss: 0.434118  [   63/   88]
per-ex loss: 0.514253  [   64/   88]
per-ex loss: 0.399086  [   65/   88]
per-ex loss: 0.571591  [   66/   88]
per-ex loss: 0.502093  [   67/   88]
per-ex loss: 0.515310  [   68/   88]
per-ex loss: 0.495841  [   69/   88]
per-ex loss: 0.677130  [   70/   88]
per-ex loss: 0.474644  [   71/   88]
per-ex loss: 0.427460  [   72/   88]
per-ex loss: 0.553139  [   73/   88]
per-ex loss: 0.458910  [   74/   88]
per-ex loss: 0.605848  [   75/   88]
per-ex loss: 0.441832  [   76/   88]
per-ex loss: 0.548438  [   77/   88]
per-ex loss: 0.420740  [   78/   88]
per-ex loss: 0.673732  [   79/   88]
per-ex loss: 0.373335  [   80/   88]
per-ex loss: 0.439415  [   81/   88]
per-ex loss: 0.453731  [   82/   88]
per-ex loss: 0.571065  [   83/   88]
per-ex loss: 0.717644  [   84/   88]
per-ex loss: 0.388453  [   85/   88]
per-ex loss: 0.633932  [   86/   88]
per-ex loss: 0.440244  [   87/   88]
per-ex loss: 0.337135  [   88/   88]
Train Error: Avg loss: 0.48913092
validation Error: 
 Avg loss: 0.56225314 
 F1: 0.463703 
 Precision: 0.673274 
 Recall: 0.353628
 IoU: 0.301832

test Error: 
 Avg loss: 0.51251362 
 F1: 0.538919 
 Precision: 0.726953 
 Recall: 0.428169
 IoU: 0.368849

We have finished training iteration 50
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_48_.pth
per-ex loss: 0.392975  [    1/   88]
per-ex loss: 0.591400  [    2/   88]
per-ex loss: 0.578021  [    3/   88]
per-ex loss: 0.415080  [    4/   88]
per-ex loss: 0.446635  [    5/   88]
per-ex loss: 0.596607  [    6/   88]
per-ex loss: 0.449716  [    7/   88]
per-ex loss: 0.425984  [    8/   88]
per-ex loss: 0.535138  [    9/   88]
per-ex loss: 0.392201  [   10/   88]
per-ex loss: 0.340964  [   11/   88]
per-ex loss: 0.439715  [   12/   88]
per-ex loss: 0.440942  [   13/   88]
per-ex loss: 0.654047  [   14/   88]
per-ex loss: 0.554321  [   15/   88]
per-ex loss: 0.417395  [   16/   88]
per-ex loss: 0.684250  [   17/   88]
per-ex loss: 0.390681  [   18/   88]
per-ex loss: 0.578119  [   19/   88]
per-ex loss: 0.412730  [   20/   88]
per-ex loss: 0.451972  [   21/   88]
per-ex loss: 0.535810  [   22/   88]
per-ex loss: 0.447934  [   23/   88]
per-ex loss: 0.437460  [   24/   88]
per-ex loss: 0.485818  [   25/   88]
per-ex loss: 0.391930  [   26/   88]
per-ex loss: 0.645624  [   27/   88]
per-ex loss: 0.598741  [   28/   88]
per-ex loss: 0.455493  [   29/   88]
per-ex loss: 0.434784  [   30/   88]
per-ex loss: 0.590253  [   31/   88]
per-ex loss: 0.410253  [   32/   88]
per-ex loss: 0.577003  [   33/   88]
per-ex loss: 0.407624  [   34/   88]
per-ex loss: 0.378682  [   35/   88]
per-ex loss: 0.610127  [   36/   88]
per-ex loss: 0.438145  [   37/   88]
per-ex loss: 0.536933  [   38/   88]
per-ex loss: 0.461403  [   39/   88]
per-ex loss: 0.576962  [   40/   88]
per-ex loss: 0.368701  [   41/   88]
per-ex loss: 0.500796  [   42/   88]
per-ex loss: 0.598648  [   43/   88]
per-ex loss: 0.400744  [   44/   88]
per-ex loss: 0.458006  [   45/   88]
per-ex loss: 0.544157  [   46/   88]
per-ex loss: 0.340637  [   47/   88]
per-ex loss: 0.418525  [   48/   88]
per-ex loss: 0.568699  [   49/   88]
per-ex loss: 0.357771  [   50/   88]
per-ex loss: 0.631236  [   51/   88]
per-ex loss: 0.573398  [   52/   88]
per-ex loss: 0.654278  [   53/   88]
per-ex loss: 0.652731  [   54/   88]
per-ex loss: 0.359433  [   55/   88]
per-ex loss: 0.613765  [   56/   88]
per-ex loss: 0.533588  [   57/   88]
per-ex loss: 0.431887  [   58/   88]
per-ex loss: 0.536657  [   59/   88]
per-ex loss: 0.471983  [   60/   88]
per-ex loss: 0.661760  [   61/   88]
per-ex loss: 0.319058  [   62/   88]
per-ex loss: 0.364485  [   63/   88]
per-ex loss: 0.372850  [   64/   88]
per-ex loss: 0.750748  [   65/   88]
per-ex loss: 0.437401  [   66/   88]
per-ex loss: 0.389473  [   67/   88]
per-ex loss: 0.385028  [   68/   88]
per-ex loss: 0.604885  [   69/   88]
per-ex loss: 0.630000  [   70/   88]
per-ex loss: 0.365034  [   71/   88]
per-ex loss: 0.520967  [   72/   88]
per-ex loss: 0.644674  [   73/   88]
per-ex loss: 0.352835  [   74/   88]
per-ex loss: 0.711847  [   75/   88]
per-ex loss: 0.313712  [   76/   88]
per-ex loss: 0.429095  [   77/   88]
per-ex loss: 0.390098  [   78/   88]
per-ex loss: 0.594145  [   79/   88]
per-ex loss: 0.433489  [   80/   88]
per-ex loss: 0.409410  [   81/   88]
per-ex loss: 0.496487  [   82/   88]
per-ex loss: 0.431779  [   83/   88]
per-ex loss: 0.683971  [   84/   88]
per-ex loss: 0.523792  [   85/   88]
per-ex loss: 0.414413  [   86/   88]
per-ex loss: 0.359142  [   87/   88]
per-ex loss: 0.573436  [   88/   88]
Train Error: Avg loss: 0.49079001
validation Error: 
 Avg loss: 0.54095927 
 F1: 0.486298 
 Precision: 0.556380 
 Recall: 0.431896
 IoU: 0.321264

test Error: 
 Avg loss: 0.48620248 
 F1: 0.565979 
 Precision: 0.636601 
 Recall: 0.509461
 IoU: 0.394680

We have finished training iteration 51
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_49_.pth
per-ex loss: 0.466727  [    1/   88]
per-ex loss: 0.702042  [    2/   88]
per-ex loss: 0.513419  [    3/   88]
per-ex loss: 0.382723  [    4/   88]
per-ex loss: 0.682792  [    5/   88]
per-ex loss: 0.585414  [    6/   88]
per-ex loss: 0.459147  [    7/   88]
per-ex loss: 0.488134  [    8/   88]
per-ex loss: 0.371027  [    9/   88]
per-ex loss: 0.426053  [   10/   88]
per-ex loss: 0.357217  [   11/   88]
per-ex loss: 0.580559  [   12/   88]
per-ex loss: 0.381640  [   13/   88]
per-ex loss: 0.351502  [   14/   88]
per-ex loss: 0.472646  [   15/   88]
per-ex loss: 0.438740  [   16/   88]
per-ex loss: 0.391886  [   17/   88]
per-ex loss: 0.557624  [   18/   88]
per-ex loss: 0.589898  [   19/   88]
per-ex loss: 0.470001  [   20/   88]
per-ex loss: 0.651048  [   21/   88]
per-ex loss: 0.563962  [   22/   88]
per-ex loss: 0.532160  [   23/   88]
per-ex loss: 0.506259  [   24/   88]
per-ex loss: 0.398907  [   25/   88]
per-ex loss: 0.417945  [   26/   88]
per-ex loss: 0.454301  [   27/   88]
per-ex loss: 0.344501  [   28/   88]
per-ex loss: 0.652109  [   29/   88]
per-ex loss: 0.364213  [   30/   88]
per-ex loss: 0.523336  [   31/   88]
per-ex loss: 0.372793  [   32/   88]
per-ex loss: 0.520649  [   33/   88]
per-ex loss: 0.621797  [   34/   88]
per-ex loss: 0.610114  [   35/   88]
per-ex loss: 0.568230  [   36/   88]
per-ex loss: 0.438197  [   37/   88]
per-ex loss: 0.338976  [   38/   88]
per-ex loss: 0.316965  [   39/   88]
per-ex loss: 0.605203  [   40/   88]
per-ex loss: 0.425151  [   41/   88]
per-ex loss: 0.345671  [   42/   88]
per-ex loss: 0.656505  [   43/   88]
per-ex loss: 0.413429  [   44/   88]
per-ex loss: 0.553039  [   45/   88]
per-ex loss: 0.445328  [   46/   88]
per-ex loss: 0.391317  [   47/   88]
per-ex loss: 0.432946  [   48/   88]
per-ex loss: 0.670063  [   49/   88]
per-ex loss: 0.475077  [   50/   88]
per-ex loss: 0.389452  [   51/   88]
per-ex loss: 0.541335  [   52/   88]
per-ex loss: 0.711702  [   53/   88]
per-ex loss: 0.404515  [   54/   88]
per-ex loss: 0.384256  [   55/   88]
per-ex loss: 0.337636  [   56/   88]
per-ex loss: 0.626350  [   57/   88]
per-ex loss: 0.401029  [   58/   88]
per-ex loss: 0.443000  [   59/   88]
per-ex loss: 0.413295  [   60/   88]
per-ex loss: 0.617823  [   61/   88]
per-ex loss: 0.393606  [   62/   88]
per-ex loss: 0.405731  [   63/   88]
per-ex loss: 0.626489  [   64/   88]
per-ex loss: 0.529933  [   65/   88]
per-ex loss: 0.620631  [   66/   88]
per-ex loss: 0.403807  [   67/   88]
per-ex loss: 0.437078  [   68/   88]
per-ex loss: 0.531116  [   69/   88]
per-ex loss: 0.644255  [   70/   88]
per-ex loss: 0.648257  [   71/   88]
per-ex loss: 0.353455  [   72/   88]
per-ex loss: 0.394873  [   73/   88]
per-ex loss: 0.405174  [   74/   88]
per-ex loss: 0.569737  [   75/   88]
per-ex loss: 0.568508  [   76/   88]
per-ex loss: 0.411266  [   77/   88]
per-ex loss: 0.478769  [   78/   88]
per-ex loss: 0.357816  [   79/   88]
per-ex loss: 0.615730  [   80/   88]
per-ex loss: 0.535173  [   81/   88]
per-ex loss: 0.605437  [   82/   88]
per-ex loss: 0.388183  [   83/   88]
per-ex loss: 0.626290  [   84/   88]
per-ex loss: 0.446992  [   85/   88]
per-ex loss: 0.413511  [   86/   88]
per-ex loss: 0.504466  [   87/   88]
per-ex loss: 0.390155  [   88/   88]
Train Error: Avg loss: 0.48700246
validation Error: 
 Avg loss: 0.53284689 
 F1: 0.500585 
 Precision: 0.520667 
 Recall: 0.481994
 IoU: 0.333853

test Error: 
 Avg loss: 0.49450077 
 F1: 0.558989 
 Precision: 0.549479 
 Recall: 0.568834
 IoU: 0.387914

We have finished training iteration 52
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_50_.pth
per-ex loss: 0.584722  [    1/   88]
per-ex loss: 0.523359  [    2/   88]
per-ex loss: 0.542528  [    3/   88]
per-ex loss: 0.434682  [    4/   88]
per-ex loss: 0.559270  [    5/   88]
per-ex loss: 0.432045  [    6/   88]
per-ex loss: 0.500500  [    7/   88]
per-ex loss: 0.400327  [    8/   88]
per-ex loss: 0.343792  [    9/   88]
per-ex loss: 0.568635  [   10/   88]
per-ex loss: 0.593740  [   11/   88]
per-ex loss: 0.324703  [   12/   88]
per-ex loss: 0.527176  [   13/   88]
per-ex loss: 0.448044  [   14/   88]
per-ex loss: 0.475224  [   15/   88]
per-ex loss: 0.414612  [   16/   88]
per-ex loss: 0.690676  [   17/   88]
per-ex loss: 0.614828  [   18/   88]
per-ex loss: 0.336181  [   19/   88]
per-ex loss: 0.312471  [   20/   88]
per-ex loss: 0.442639  [   21/   88]
per-ex loss: 0.608919  [   22/   88]
per-ex loss: 0.667455  [   23/   88]
per-ex loss: 0.527750  [   24/   88]
per-ex loss: 0.405737  [   25/   88]
per-ex loss: 0.405516  [   26/   88]
per-ex loss: 0.366296  [   27/   88]
per-ex loss: 0.412995  [   28/   88]
per-ex loss: 0.370333  [   29/   88]
per-ex loss: 0.630038  [   30/   88]
per-ex loss: 0.442406  [   31/   88]
per-ex loss: 0.647953  [   32/   88]
per-ex loss: 0.370323  [   33/   88]
per-ex loss: 0.447750  [   34/   88]
per-ex loss: 0.409161  [   35/   88]
per-ex loss: 0.602481  [   36/   88]
per-ex loss: 0.444780  [   37/   88]
per-ex loss: 0.504728  [   38/   88]
per-ex loss: 0.397875  [   39/   88]
per-ex loss: 0.618752  [   40/   88]
per-ex loss: 0.597791  [   41/   88]
per-ex loss: 0.402220  [   42/   88]
per-ex loss: 0.504074  [   43/   88]
per-ex loss: 0.397983  [   44/   88]
per-ex loss: 0.585850  [   45/   88]
per-ex loss: 0.449926  [   46/   88]
per-ex loss: 0.383992  [   47/   88]
per-ex loss: 0.412681  [   48/   88]
per-ex loss: 0.358159  [   49/   88]
per-ex loss: 0.374163  [   50/   88]
per-ex loss: 0.505382  [   51/   88]
per-ex loss: 0.479525  [   52/   88]
per-ex loss: 0.430542  [   53/   88]
per-ex loss: 0.373380  [   54/   88]
per-ex loss: 0.656422  [   55/   88]
per-ex loss: 0.572117  [   56/   88]
per-ex loss: 0.672261  [   57/   88]
per-ex loss: 0.361176  [   58/   88]
per-ex loss: 0.672016  [   59/   88]
per-ex loss: 0.687713  [   60/   88]
per-ex loss: 0.348828  [   61/   88]
per-ex loss: 0.380750  [   62/   88]
per-ex loss: 0.317071  [   63/   88]
per-ex loss: 0.593346  [   64/   88]
per-ex loss: 0.700971  [   65/   88]
per-ex loss: 0.580380  [   66/   88]
per-ex loss: 0.567321  [   67/   88]
per-ex loss: 0.349351  [   68/   88]
per-ex loss: 0.436360  [   69/   88]
per-ex loss: 0.372675  [   70/   88]
per-ex loss: 0.399268  [   71/   88]
per-ex loss: 0.431850  [   72/   88]
per-ex loss: 0.439129  [   73/   88]
per-ex loss: 0.365159  [   74/   88]
per-ex loss: 0.640396  [   75/   88]
per-ex loss: 0.622264  [   76/   88]
per-ex loss: 0.547992  [   77/   88]
per-ex loss: 0.548241  [   78/   88]
per-ex loss: 0.400132  [   79/   88]
per-ex loss: 0.388497  [   80/   88]
per-ex loss: 0.520753  [   81/   88]
per-ex loss: 0.476617  [   82/   88]
per-ex loss: 0.397083  [   83/   88]
per-ex loss: 0.533909  [   84/   88]
per-ex loss: 0.573512  [   85/   88]
per-ex loss: 0.518215  [   86/   88]
per-ex loss: 0.567478  [   87/   88]
per-ex loss: 0.463974  [   88/   88]
Train Error: Avg loss: 0.48534430
validation Error: 
 Avg loss: 0.61067642 
 F1: 0.400876 
 Precision: 0.318582 
 Recall: 0.540494
 IoU: 0.250685

test Error: 
 Avg loss: 0.52273377 
 F1: 0.513335 
 Precision: 0.425175 
 Recall: 0.647617
 IoU: 0.345293

We have finished training iteration 53
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_51_.pth
per-ex loss: 0.492772  [    1/   88]
per-ex loss: 0.376697  [    2/   88]
per-ex loss: 0.402389  [    3/   88]
per-ex loss: 0.396049  [    4/   88]
per-ex loss: 0.436303  [    5/   88]
per-ex loss: 0.420881  [    6/   88]
per-ex loss: 0.449736  [    7/   88]
per-ex loss: 0.437985  [    8/   88]
per-ex loss: 0.487294  [    9/   88]
per-ex loss: 0.417409  [   10/   88]
per-ex loss: 0.396960  [   11/   88]
per-ex loss: 0.393483  [   12/   88]
per-ex loss: 0.596360  [   13/   88]
per-ex loss: 0.657369  [   14/   88]
per-ex loss: 0.459620  [   15/   88]
per-ex loss: 0.401595  [   16/   88]
per-ex loss: 0.504580  [   17/   88]
per-ex loss: 0.421583  [   18/   88]
per-ex loss: 0.380511  [   19/   88]
per-ex loss: 0.619942  [   20/   88]
per-ex loss: 0.391485  [   21/   88]
per-ex loss: 0.389032  [   22/   88]
per-ex loss: 0.322656  [   23/   88]
per-ex loss: 0.421885  [   24/   88]
per-ex loss: 0.590088  [   25/   88]
per-ex loss: 0.437549  [   26/   88]
per-ex loss: 0.557545  [   27/   88]
per-ex loss: 0.652792  [   28/   88]
per-ex loss: 0.412233  [   29/   88]
per-ex loss: 0.414941  [   30/   88]
per-ex loss: 0.394267  [   31/   88]
per-ex loss: 0.407238  [   32/   88]
per-ex loss: 0.660848  [   33/   88]
per-ex loss: 0.694183  [   34/   88]
per-ex loss: 0.554909  [   35/   88]
per-ex loss: 0.357429  [   36/   88]
per-ex loss: 0.349802  [   37/   88]
per-ex loss: 0.540759  [   38/   88]
per-ex loss: 0.362581  [   39/   88]
per-ex loss: 0.667056  [   40/   88]
per-ex loss: 0.596411  [   41/   88]
per-ex loss: 0.568492  [   42/   88]
per-ex loss: 0.649859  [   43/   88]
per-ex loss: 0.599554  [   44/   88]
per-ex loss: 0.535140  [   45/   88]
per-ex loss: 0.402903  [   46/   88]
per-ex loss: 0.427684  [   47/   88]
per-ex loss: 0.604921  [   48/   88]
per-ex loss: 0.528324  [   49/   88]
per-ex loss: 0.502982  [   50/   88]
per-ex loss: 0.576470  [   51/   88]
per-ex loss: 0.420632  [   52/   88]
per-ex loss: 0.353866  [   53/   88]
per-ex loss: 0.407535  [   54/   88]
per-ex loss: 0.687673  [   55/   88]
per-ex loss: 0.546828  [   56/   88]
per-ex loss: 0.666802  [   57/   88]
per-ex loss: 0.595334  [   58/   88]
per-ex loss: 0.567486  [   59/   88]
per-ex loss: 0.494790  [   60/   88]
per-ex loss: 0.615068  [   61/   88]
per-ex loss: 0.362650  [   62/   88]
per-ex loss: 0.555652  [   63/   88]
per-ex loss: 0.480907  [   64/   88]
per-ex loss: 0.521348  [   65/   88]
per-ex loss: 0.477588  [   66/   88]
per-ex loss: 0.535137  [   67/   88]
per-ex loss: 0.349264  [   68/   88]
per-ex loss: 0.573373  [   69/   88]
per-ex loss: 0.430442  [   70/   88]
per-ex loss: 0.367909  [   71/   88]
per-ex loss: 0.542908  [   72/   88]
per-ex loss: 0.546454  [   73/   88]
per-ex loss: 0.432763  [   74/   88]
per-ex loss: 0.622420  [   75/   88]
per-ex loss: 0.344889  [   76/   88]
per-ex loss: 0.470694  [   77/   88]
per-ex loss: 0.437796  [   78/   88]
per-ex loss: 0.632884  [   79/   88]
per-ex loss: 0.427362  [   80/   88]
per-ex loss: 0.403640  [   81/   88]
per-ex loss: 0.583995  [   82/   88]
per-ex loss: 0.338659  [   83/   88]
per-ex loss: 0.412829  [   84/   88]
per-ex loss: 0.403312  [   85/   88]
per-ex loss: 0.371209  [   86/   88]
per-ex loss: 0.616006  [   87/   88]
per-ex loss: 0.375746  [   88/   88]
Train Error: Avg loss: 0.48517484
validation Error: 
 Avg loss: 0.56029111 
 F1: 0.459980 
 Precision: 0.479942 
 Recall: 0.441612
 IoU: 0.298684

test Error: 
 Avg loss: 0.49376090 
 F1: 0.558678 
 Precision: 0.572460 
 Recall: 0.545544
 IoU: 0.387615

We have finished training iteration 54
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_44_.pth
per-ex loss: 0.583173  [    1/   88]
per-ex loss: 0.557515  [    2/   88]
per-ex loss: 0.579624  [    3/   88]
per-ex loss: 0.380873  [    4/   88]
per-ex loss: 0.441293  [    5/   88]
per-ex loss: 0.647638  [    6/   88]
per-ex loss: 0.363582  [    7/   88]
per-ex loss: 0.594818  [    8/   88]
per-ex loss: 0.370395  [    9/   88]
per-ex loss: 0.330999  [   10/   88]
per-ex loss: 0.492114  [   11/   88]
per-ex loss: 0.619878  [   12/   88]
per-ex loss: 0.332530  [   13/   88]
per-ex loss: 0.438652  [   14/   88]
per-ex loss: 0.570731  [   15/   88]
per-ex loss: 0.526910  [   16/   88]
per-ex loss: 0.352275  [   17/   88]
per-ex loss: 0.395254  [   18/   88]
per-ex loss: 0.557101  [   19/   88]
per-ex loss: 0.341939  [   20/   88]
per-ex loss: 0.661817  [   21/   88]
per-ex loss: 0.379709  [   22/   88]
per-ex loss: 0.411566  [   23/   88]
per-ex loss: 0.601503  [   24/   88]
per-ex loss: 0.565740  [   25/   88]
per-ex loss: 0.525021  [   26/   88]
per-ex loss: 0.406396  [   27/   88]
per-ex loss: 0.375208  [   28/   88]
per-ex loss: 0.420053  [   29/   88]
per-ex loss: 0.397480  [   30/   88]
per-ex loss: 0.526205  [   31/   88]
per-ex loss: 0.336410  [   32/   88]
per-ex loss: 0.466894  [   33/   88]
per-ex loss: 0.390065  [   34/   88]
per-ex loss: 0.527607  [   35/   88]
per-ex loss: 0.590593  [   36/   88]
per-ex loss: 0.441637  [   37/   88]
per-ex loss: 0.364326  [   38/   88]
per-ex loss: 0.502560  [   39/   88]
per-ex loss: 0.330238  [   40/   88]
per-ex loss: 0.603309  [   41/   88]
per-ex loss: 0.426793  [   42/   88]
per-ex loss: 0.386980  [   43/   88]
per-ex loss: 0.439632  [   44/   88]
per-ex loss: 0.470742  [   45/   88]
per-ex loss: 0.441769  [   46/   88]
per-ex loss: 0.495038  [   47/   88]
per-ex loss: 0.416530  [   48/   88]
per-ex loss: 0.387550  [   49/   88]
per-ex loss: 0.541095  [   50/   88]
per-ex loss: 0.439142  [   51/   88]
per-ex loss: 0.471924  [   52/   88]
per-ex loss: 0.617759  [   53/   88]
per-ex loss: 0.647705  [   54/   88]
per-ex loss: 0.659001  [   55/   88]
per-ex loss: 0.430802  [   56/   88]
per-ex loss: 0.365420  [   57/   88]
per-ex loss: 0.554125  [   58/   88]
per-ex loss: 0.530650  [   59/   88]
per-ex loss: 0.434099  [   60/   88]
per-ex loss: 0.419941  [   61/   88]
per-ex loss: 0.373386  [   62/   88]
per-ex loss: 0.544446  [   63/   88]
per-ex loss: 0.387187  [   64/   88]
per-ex loss: 0.585496  [   65/   88]
per-ex loss: 0.463567  [   66/   88]
per-ex loss: 0.523243  [   67/   88]
per-ex loss: 0.387941  [   68/   88]
per-ex loss: 0.442272  [   69/   88]
per-ex loss: 0.696407  [   70/   88]
per-ex loss: 0.594483  [   71/   88]
per-ex loss: 0.648555  [   72/   88]
per-ex loss: 0.365357  [   73/   88]
per-ex loss: 0.606981  [   74/   88]
per-ex loss: 0.529038  [   75/   88]
per-ex loss: 0.405594  [   76/   88]
per-ex loss: 0.599854  [   77/   88]
per-ex loss: 0.402962  [   78/   88]
per-ex loss: 0.344269  [   79/   88]
per-ex loss: 0.421907  [   80/   88]
per-ex loss: 0.617833  [   81/   88]
per-ex loss: 0.656361  [   82/   88]
per-ex loss: 0.409410  [   83/   88]
per-ex loss: 0.688160  [   84/   88]
per-ex loss: 0.569762  [   85/   88]
per-ex loss: 0.424885  [   86/   88]
per-ex loss: 0.381917  [   87/   88]
per-ex loss: 0.439105  [   88/   88]
Train Error: Avg loss: 0.48168988
validation Error: 
 Avg loss: 0.54186080 
 F1: 0.485403 
 Precision: 0.595911 
 Recall: 0.409470
 IoU: 0.320483

test Error: 
 Avg loss: 0.48939837 
 F1: 0.562218 
 Precision: 0.661217 
 Recall: 0.489003
 IoU: 0.391031

We have finished training iteration 55
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_53_.pth
per-ex loss: 0.406715  [    1/   88]
per-ex loss: 0.566876  [    2/   88]
per-ex loss: 0.360318  [    3/   88]
per-ex loss: 0.321378  [    4/   88]
per-ex loss: 0.599826  [    5/   88]
per-ex loss: 0.538108  [    6/   88]
per-ex loss: 0.395218  [    7/   88]
per-ex loss: 0.555270  [    8/   88]
per-ex loss: 0.349985  [    9/   88]
per-ex loss: 0.557683  [   10/   88]
per-ex loss: 0.604593  [   11/   88]
per-ex loss: 0.469055  [   12/   88]
per-ex loss: 0.413020  [   13/   88]
per-ex loss: 0.361084  [   14/   88]
per-ex loss: 0.359020  [   15/   88]
per-ex loss: 0.342204  [   16/   88]
per-ex loss: 0.438944  [   17/   88]
per-ex loss: 0.631365  [   18/   88]
per-ex loss: 0.412463  [   19/   88]
per-ex loss: 0.407736  [   20/   88]
per-ex loss: 0.377742  [   21/   88]
per-ex loss: 0.577413  [   22/   88]
per-ex loss: 0.519057  [   23/   88]
per-ex loss: 0.370620  [   24/   88]
per-ex loss: 0.366706  [   25/   88]
per-ex loss: 0.402240  [   26/   88]
per-ex loss: 0.391617  [   27/   88]
per-ex loss: 0.579397  [   28/   88]
per-ex loss: 0.639958  [   29/   88]
per-ex loss: 0.427553  [   30/   88]
per-ex loss: 0.588543  [   31/   88]
per-ex loss: 0.698197  [   32/   88]
per-ex loss: 0.415703  [   33/   88]
per-ex loss: 0.534292  [   34/   88]
per-ex loss: 0.344736  [   35/   88]
per-ex loss: 0.416356  [   36/   88]
per-ex loss: 0.539543  [   37/   88]
per-ex loss: 0.363031  [   38/   88]
per-ex loss: 0.583109  [   39/   88]
per-ex loss: 0.399485  [   40/   88]
per-ex loss: 0.519624  [   41/   88]
per-ex loss: 0.418329  [   42/   88]
per-ex loss: 0.503699  [   43/   88]
per-ex loss: 0.580900  [   44/   88]
per-ex loss: 0.530495  [   45/   88]
per-ex loss: 0.348711  [   46/   88]
per-ex loss: 0.485980  [   47/   88]
per-ex loss: 0.500401  [   48/   88]
per-ex loss: 0.644406  [   49/   88]
per-ex loss: 0.416743  [   50/   88]
per-ex loss: 0.408972  [   51/   88]
per-ex loss: 0.381165  [   52/   88]
per-ex loss: 0.618712  [   53/   88]
per-ex loss: 0.464638  [   54/   88]
per-ex loss: 0.441221  [   55/   88]
per-ex loss: 0.412711  [   56/   88]
per-ex loss: 0.559469  [   57/   88]
per-ex loss: 0.394847  [   58/   88]
per-ex loss: 0.589143  [   59/   88]
per-ex loss: 0.526006  [   60/   88]
per-ex loss: 0.584881  [   61/   88]
per-ex loss: 0.432352  [   62/   88]
per-ex loss: 0.396215  [   63/   88]
per-ex loss: 0.648245  [   64/   88]
per-ex loss: 0.608178  [   65/   88]
per-ex loss: 0.406464  [   66/   88]
per-ex loss: 0.426400  [   67/   88]
per-ex loss: 0.334322  [   68/   88]
per-ex loss: 0.455880  [   69/   88]
per-ex loss: 0.698676  [   70/   88]
per-ex loss: 0.606525  [   71/   88]
per-ex loss: 0.409884  [   72/   88]
per-ex loss: 0.429646  [   73/   88]
per-ex loss: 0.537414  [   74/   88]
per-ex loss: 0.704337  [   75/   88]
per-ex loss: 0.330883  [   76/   88]
per-ex loss: 0.355769  [   77/   88]
per-ex loss: 0.496629  [   78/   88]
per-ex loss: 0.434603  [   79/   88]
per-ex loss: 0.382399  [   80/   88]
per-ex loss: 0.547505  [   81/   88]
per-ex loss: 0.599885  [   82/   88]
per-ex loss: 0.722120  [   83/   88]
per-ex loss: 0.360707  [   84/   88]
per-ex loss: 0.507600  [   85/   88]
per-ex loss: 0.375945  [   86/   88]
per-ex loss: 0.638963  [   87/   88]
per-ex loss: 0.570635  [   88/   88]
Train Error: Avg loss: 0.48153860
validation Error: 
 Avg loss: 0.53649493 
 F1: 0.489931 
 Precision: 0.538221 
 Recall: 0.449594
 IoU: 0.324443

test Error: 
 Avg loss: 0.48650112 
 F1: 0.564892 
 Precision: 0.598496 
 Recall: 0.534860
 IoU: 0.393623

We have finished training iteration 56
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_54_.pth
per-ex loss: 0.549906  [    1/   88]
per-ex loss: 0.476260  [    2/   88]
per-ex loss: 0.386204  [    3/   88]
per-ex loss: 0.367024  [    4/   88]
per-ex loss: 0.419686  [    5/   88]
per-ex loss: 0.566817  [    6/   88]
per-ex loss: 0.499118  [    7/   88]
per-ex loss: 0.519745  [    8/   88]
per-ex loss: 0.576100  [    9/   88]
per-ex loss: 0.631351  [   10/   88]
per-ex loss: 0.412678  [   11/   88]
per-ex loss: 0.420368  [   12/   88]
per-ex loss: 0.392565  [   13/   88]
per-ex loss: 0.651420  [   14/   88]
per-ex loss: 0.387282  [   15/   88]
per-ex loss: 0.372777  [   16/   88]
per-ex loss: 0.538267  [   17/   88]
per-ex loss: 0.589674  [   18/   88]
per-ex loss: 0.589288  [   19/   88]
per-ex loss: 0.359668  [   20/   88]
per-ex loss: 0.483725  [   21/   88]
per-ex loss: 0.398159  [   22/   88]
per-ex loss: 0.583825  [   23/   88]
per-ex loss: 0.335188  [   24/   88]
per-ex loss: 0.383793  [   25/   88]
per-ex loss: 0.519643  [   26/   88]
per-ex loss: 0.358779  [   27/   88]
per-ex loss: 0.648873  [   28/   88]
per-ex loss: 0.362328  [   29/   88]
per-ex loss: 0.495484  [   30/   88]
per-ex loss: 0.400671  [   31/   88]
per-ex loss: 0.509175  [   32/   88]
per-ex loss: 0.460122  [   33/   88]
per-ex loss: 0.670701  [   34/   88]
per-ex loss: 0.662977  [   35/   88]
per-ex loss: 0.573588  [   36/   88]
per-ex loss: 0.424890  [   37/   88]
per-ex loss: 0.431015  [   38/   88]
per-ex loss: 0.410670  [   39/   88]
per-ex loss: 0.443543  [   40/   88]
per-ex loss: 0.627969  [   41/   88]
per-ex loss: 0.436860  [   42/   88]
per-ex loss: 0.329814  [   43/   88]
per-ex loss: 0.607713  [   44/   88]
per-ex loss: 0.512870  [   45/   88]
per-ex loss: 0.574928  [   46/   88]
per-ex loss: 0.348377  [   47/   88]
per-ex loss: 0.486002  [   48/   88]
per-ex loss: 0.565835  [   49/   88]
per-ex loss: 0.637303  [   50/   88]
per-ex loss: 0.641192  [   51/   88]
per-ex loss: 0.317609  [   52/   88]
per-ex loss: 0.339507  [   53/   88]
per-ex loss: 0.446165  [   54/   88]
per-ex loss: 0.403517  [   55/   88]
per-ex loss: 0.363287  [   56/   88]
per-ex loss: 0.417323  [   57/   88]
per-ex loss: 0.394180  [   58/   88]
per-ex loss: 0.662831  [   59/   88]
per-ex loss: 0.605163  [   60/   88]
per-ex loss: 0.403053  [   61/   88]
per-ex loss: 0.433912  [   62/   88]
per-ex loss: 0.645203  [   63/   88]
per-ex loss: 0.501736  [   64/   88]
per-ex loss: 0.504262  [   65/   88]
per-ex loss: 0.609000  [   66/   88]
per-ex loss: 0.429968  [   67/   88]
per-ex loss: 0.461122  [   68/   88]
per-ex loss: 0.513315  [   69/   88]
per-ex loss: 0.619705  [   70/   88]
per-ex loss: 0.432811  [   71/   88]
per-ex loss: 0.385826  [   72/   88]
per-ex loss: 0.388665  [   73/   88]
per-ex loss: 0.418351  [   74/   88]
per-ex loss: 0.339858  [   75/   88]
per-ex loss: 0.421895  [   76/   88]
per-ex loss: 0.402714  [   77/   88]
per-ex loss: 0.669618  [   78/   88]
per-ex loss: 0.418838  [   79/   88]
per-ex loss: 0.393566  [   80/   88]
per-ex loss: 0.540727  [   81/   88]
per-ex loss: 0.609358  [   82/   88]
per-ex loss: 0.547566  [   83/   88]
per-ex loss: 0.575219  [   84/   88]
per-ex loss: 0.534043  [   85/   88]
per-ex loss: 0.393349  [   86/   88]
per-ex loss: 0.399554  [   87/   88]
per-ex loss: 0.367071  [   88/   88]
Train Error: Avg loss: 0.48116014
validation Error: 
 Avg loss: 0.55773484 
 F1: 0.466662 
 Precision: 0.469337 
 Recall: 0.464017
 IoU: 0.304344

test Error: 
 Avg loss: 0.50344375 
 F1: 0.543981 
 Precision: 0.551206 
 Recall: 0.536943
 IoU: 0.373608

We have finished training iteration 57
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_55_.pth
per-ex loss: 0.665035  [    1/   88]
per-ex loss: 0.402172  [    2/   88]
per-ex loss: 0.556865  [    3/   88]
per-ex loss: 0.367941  [    4/   88]
per-ex loss: 0.385815  [    5/   88]
per-ex loss: 0.390903  [    6/   88]
per-ex loss: 0.565318  [    7/   88]
per-ex loss: 0.654824  [    8/   88]
per-ex loss: 0.660667  [    9/   88]
per-ex loss: 0.402894  [   10/   88]
per-ex loss: 0.339672  [   11/   88]
per-ex loss: 0.397822  [   12/   88]
per-ex loss: 0.529029  [   13/   88]
per-ex loss: 0.604577  [   14/   88]
per-ex loss: 0.353238  [   15/   88]
per-ex loss: 0.356173  [   16/   88]
per-ex loss: 0.587324  [   17/   88]
per-ex loss: 0.648192  [   18/   88]
per-ex loss: 0.527633  [   19/   88]
per-ex loss: 0.687826  [   20/   88]
per-ex loss: 0.339362  [   21/   88]
per-ex loss: 0.335539  [   22/   88]
per-ex loss: 0.378323  [   23/   88]
per-ex loss: 0.494134  [   24/   88]
per-ex loss: 0.394437  [   25/   88]
per-ex loss: 0.569712  [   26/   88]
per-ex loss: 0.353001  [   27/   88]
per-ex loss: 0.370154  [   28/   88]
per-ex loss: 0.339886  [   29/   88]
per-ex loss: 0.346206  [   30/   88]
per-ex loss: 0.343402  [   31/   88]
per-ex loss: 0.587114  [   32/   88]
per-ex loss: 0.440009  [   33/   88]
per-ex loss: 0.672616  [   34/   88]
per-ex loss: 0.483952  [   35/   88]
per-ex loss: 0.542202  [   36/   88]
per-ex loss: 0.628712  [   37/   88]
per-ex loss: 0.488219  [   38/   88]
per-ex loss: 0.388349  [   39/   88]
per-ex loss: 0.629465  [   40/   88]
per-ex loss: 0.650050  [   41/   88]
per-ex loss: 0.410817  [   42/   88]
per-ex loss: 0.461954  [   43/   88]
per-ex loss: 0.482574  [   44/   88]
per-ex loss: 0.411088  [   45/   88]
per-ex loss: 0.344881  [   46/   88]
per-ex loss: 0.507549  [   47/   88]
per-ex loss: 0.570701  [   48/   88]
per-ex loss: 0.527246  [   49/   88]
per-ex loss: 0.380615  [   50/   88]
per-ex loss: 0.413508  [   51/   88]
per-ex loss: 0.404869  [   52/   88]
per-ex loss: 0.406594  [   53/   88]
per-ex loss: 0.320959  [   54/   88]
per-ex loss: 0.560588  [   55/   88]
per-ex loss: 0.417241  [   56/   88]
per-ex loss: 0.432928  [   57/   88]
per-ex loss: 0.338741  [   58/   88]
per-ex loss: 0.614476  [   59/   88]
per-ex loss: 0.405469  [   60/   88]
per-ex loss: 0.639499  [   61/   88]
per-ex loss: 0.403568  [   62/   88]
per-ex loss: 0.399652  [   63/   88]
per-ex loss: 0.399177  [   64/   88]
per-ex loss: 0.616411  [   65/   88]
per-ex loss: 0.439950  [   66/   88]
per-ex loss: 0.474036  [   67/   88]
per-ex loss: 0.452421  [   68/   88]
per-ex loss: 0.671537  [   69/   88]
per-ex loss: 0.555752  [   70/   88]
per-ex loss: 0.529800  [   71/   88]
per-ex loss: 0.549137  [   72/   88]
per-ex loss: 0.452788  [   73/   88]
per-ex loss: 0.487786  [   74/   88]
per-ex loss: 0.426629  [   75/   88]
per-ex loss: 0.392623  [   76/   88]
per-ex loss: 0.393911  [   77/   88]
per-ex loss: 0.388500  [   78/   88]
per-ex loss: 0.617438  [   79/   88]
per-ex loss: 0.546291  [   80/   88]
per-ex loss: 0.410704  [   81/   88]
per-ex loss: 0.533637  [   82/   88]
per-ex loss: 0.577854  [   83/   88]
per-ex loss: 0.681298  [   84/   88]
per-ex loss: 0.542484  [   85/   88]
per-ex loss: 0.568453  [   86/   88]
per-ex loss: 0.579093  [   87/   88]
per-ex loss: 0.419976  [   88/   88]
Train Error: Avg loss: 0.48206785
validation Error: 
 Avg loss: 0.54041541 
 F1: 0.492464 
 Precision: 0.565424 
 Recall: 0.436182
 IoU: 0.326669

test Error: 
 Avg loss: 0.48832077 
 F1: 0.566418 
 Precision: 0.643083 
 Recall: 0.506086
 IoU: 0.395107

We have finished training iteration 58
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_56_.pth
per-ex loss: 0.434316  [    1/   88]
per-ex loss: 0.427151  [    2/   88]
per-ex loss: 0.583843  [    3/   88]
per-ex loss: 0.379351  [    4/   88]
per-ex loss: 0.389288  [    5/   88]
per-ex loss: 0.577896  [    6/   88]
per-ex loss: 0.384236  [    7/   88]
per-ex loss: 0.505248  [    8/   88]
per-ex loss: 0.448502  [    9/   88]
per-ex loss: 0.425107  [   10/   88]
per-ex loss: 0.515280  [   11/   88]
per-ex loss: 0.420226  [   12/   88]
per-ex loss: 0.592667  [   13/   88]
per-ex loss: 0.359222  [   14/   88]
per-ex loss: 0.689022  [   15/   88]
per-ex loss: 0.378310  [   16/   88]
per-ex loss: 0.634647  [   17/   88]
per-ex loss: 0.374567  [   18/   88]
per-ex loss: 0.535110  [   19/   88]
per-ex loss: 0.483559  [   20/   88]
per-ex loss: 0.542898  [   21/   88]
per-ex loss: 0.342838  [   22/   88]
per-ex loss: 0.381128  [   23/   88]
per-ex loss: 0.375262  [   24/   88]
per-ex loss: 0.390519  [   25/   88]
per-ex loss: 0.389147  [   26/   88]
per-ex loss: 0.500684  [   27/   88]
per-ex loss: 0.393813  [   28/   88]
per-ex loss: 0.450882  [   29/   88]
per-ex loss: 0.468525  [   30/   88]
per-ex loss: 0.538801  [   31/   88]
per-ex loss: 0.599029  [   32/   88]
per-ex loss: 0.617752  [   33/   88]
per-ex loss: 0.353013  [   34/   88]
per-ex loss: 0.366903  [   35/   88]
per-ex loss: 0.505765  [   36/   88]
per-ex loss: 0.500497  [   37/   88]
per-ex loss: 0.365658  [   38/   88]
per-ex loss: 0.403713  [   39/   88]
per-ex loss: 0.502858  [   40/   88]
per-ex loss: 0.603454  [   41/   88]
per-ex loss: 0.434172  [   42/   88]
per-ex loss: 0.356781  [   43/   88]
per-ex loss: 0.606295  [   44/   88]
per-ex loss: 0.350133  [   45/   88]
per-ex loss: 0.440531  [   46/   88]
per-ex loss: 0.608602  [   47/   88]
per-ex loss: 0.313297  [   48/   88]
per-ex loss: 0.658471  [   49/   88]
per-ex loss: 0.581178  [   50/   88]
per-ex loss: 0.495645  [   51/   88]
per-ex loss: 0.556586  [   52/   88]
per-ex loss: 0.401104  [   53/   88]
per-ex loss: 0.553534  [   54/   88]
per-ex loss: 0.383586  [   55/   88]
per-ex loss: 0.422338  [   56/   88]
per-ex loss: 0.387199  [   57/   88]
per-ex loss: 0.443707  [   58/   88]
per-ex loss: 0.568740  [   59/   88]
per-ex loss: 0.639019  [   60/   88]
per-ex loss: 0.585407  [   61/   88]
per-ex loss: 0.645978  [   62/   88]
per-ex loss: 0.337580  [   63/   88]
per-ex loss: 0.650737  [   64/   88]
per-ex loss: 0.598512  [   65/   88]
per-ex loss: 0.628803  [   66/   88]
per-ex loss: 0.439937  [   67/   88]
per-ex loss: 0.564707  [   68/   88]
per-ex loss: 0.447520  [   69/   88]
per-ex loss: 0.539362  [   70/   88]
per-ex loss: 0.439749  [   71/   88]
per-ex loss: 0.444428  [   72/   88]
per-ex loss: 0.510358  [   73/   88]
per-ex loss: 0.412085  [   74/   88]
per-ex loss: 0.456167  [   75/   88]
per-ex loss: 0.351144  [   76/   88]
per-ex loss: 0.391955  [   77/   88]
per-ex loss: 0.707453  [   78/   88]
per-ex loss: 0.700540  [   79/   88]
per-ex loss: 0.556606  [   80/   88]
per-ex loss: 0.595172  [   81/   88]
per-ex loss: 0.451737  [   82/   88]
per-ex loss: 0.594569  [   83/   88]
per-ex loss: 0.331926  [   84/   88]
per-ex loss: 0.343519  [   85/   88]
per-ex loss: 0.293192  [   86/   88]
per-ex loss: 0.402786  [   87/   88]
per-ex loss: 0.612992  [   88/   88]
Train Error: Avg loss: 0.48143783
validation Error: 
 Avg loss: 0.53812328 
 F1: 0.484765 
 Precision: 0.564440 
 Recall: 0.424801
 IoU: 0.319927

test Error: 
 Avg loss: 0.48566292 
 F1: 0.566819 
 Precision: 0.629296 
 Recall: 0.515627
 IoU: 0.395497

We have finished training iteration 59
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_57_.pth
per-ex loss: 0.491885  [    1/   88]
per-ex loss: 0.494655  [    2/   88]
per-ex loss: 0.386597  [    3/   88]
per-ex loss: 0.362538  [    4/   88]
per-ex loss: 0.561598  [    5/   88]
per-ex loss: 0.392653  [    6/   88]
per-ex loss: 0.381317  [    7/   88]
per-ex loss: 0.339739  [    8/   88]
per-ex loss: 0.420279  [    9/   88]
per-ex loss: 0.333813  [   10/   88]
per-ex loss: 0.606844  [   11/   88]
per-ex loss: 0.609886  [   12/   88]
per-ex loss: 0.427809  [   13/   88]
per-ex loss: 0.563551  [   14/   88]
per-ex loss: 0.286256  [   15/   88]
per-ex loss: 0.570848  [   16/   88]
per-ex loss: 0.479058  [   17/   88]
per-ex loss: 0.660382  [   18/   88]
per-ex loss: 0.368820  [   19/   88]
per-ex loss: 0.491816  [   20/   88]
per-ex loss: 0.379012  [   21/   88]
per-ex loss: 0.400241  [   22/   88]
per-ex loss: 0.573741  [   23/   88]
per-ex loss: 0.644687  [   24/   88]
per-ex loss: 0.506917  [   25/   88]
per-ex loss: 0.368429  [   26/   88]
per-ex loss: 0.432602  [   27/   88]
per-ex loss: 0.508613  [   28/   88]
per-ex loss: 0.632984  [   29/   88]
per-ex loss: 0.691759  [   30/   88]
per-ex loss: 0.461807  [   31/   88]
per-ex loss: 0.416314  [   32/   88]
per-ex loss: 0.427521  [   33/   88]
per-ex loss: 0.634687  [   34/   88]
per-ex loss: 0.440688  [   35/   88]
per-ex loss: 0.313477  [   36/   88]
per-ex loss: 0.429898  [   37/   88]
per-ex loss: 0.422323  [   38/   88]
per-ex loss: 0.404810  [   39/   88]
per-ex loss: 0.525817  [   40/   88]
per-ex loss: 0.598929  [   41/   88]
per-ex loss: 0.497485  [   42/   88]
per-ex loss: 0.408229  [   43/   88]
per-ex loss: 0.389402  [   44/   88]
per-ex loss: 0.679671  [   45/   88]
per-ex loss: 0.335920  [   46/   88]
per-ex loss: 0.619659  [   47/   88]
per-ex loss: 0.352908  [   48/   88]
per-ex loss: 0.544824  [   49/   88]
per-ex loss: 0.580992  [   50/   88]
per-ex loss: 0.422779  [   51/   88]
per-ex loss: 0.408887  [   52/   88]
per-ex loss: 0.539162  [   53/   88]
per-ex loss: 0.463710  [   54/   88]
per-ex loss: 0.612772  [   55/   88]
per-ex loss: 0.672241  [   56/   88]
per-ex loss: 0.522646  [   57/   88]
per-ex loss: 0.548380  [   58/   88]
per-ex loss: 0.408043  [   59/   88]
per-ex loss: 0.441643  [   60/   88]
per-ex loss: 0.370365  [   61/   88]
per-ex loss: 0.409730  [   62/   88]
per-ex loss: 0.595431  [   63/   88]
per-ex loss: 0.447491  [   64/   88]
per-ex loss: 0.603106  [   65/   88]
per-ex loss: 0.407988  [   66/   88]
per-ex loss: 0.474096  [   67/   88]
per-ex loss: 0.650075  [   68/   88]
per-ex loss: 0.476006  [   69/   88]
per-ex loss: 0.350908  [   70/   88]
per-ex loss: 0.355348  [   71/   88]
per-ex loss: 0.401205  [   72/   88]
per-ex loss: 0.413834  [   73/   88]
per-ex loss: 0.604878  [   74/   88]
per-ex loss: 0.422447  [   75/   88]
per-ex loss: 0.439671  [   76/   88]
per-ex loss: 0.506240  [   77/   88]
per-ex loss: 0.554965  [   78/   88]
per-ex loss: 0.412597  [   79/   88]
per-ex loss: 0.580022  [   80/   88]
per-ex loss: 0.339919  [   81/   88]
per-ex loss: 0.596396  [   82/   88]
per-ex loss: 0.397102  [   83/   88]
per-ex loss: 0.329133  [   84/   88]
per-ex loss: 0.575361  [   85/   88]
per-ex loss: 0.370690  [   86/   88]
per-ex loss: 0.407107  [   87/   88]
per-ex loss: 0.582716  [   88/   88]
Train Error: Avg loss: 0.47692923
validation Error: 
 Avg loss: 0.54720274 
 F1: 0.478216 
 Precision: 0.496638 
 Recall: 0.461112
 IoU: 0.314247

test Error: 
 Avg loss: 0.50008525 
 F1: 0.546785 
 Precision: 0.559956 
 Recall: 0.534219
 IoU: 0.376259

We have finished training iteration 60
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_58_.pth
per-ex loss: 0.380629  [    1/   88]
per-ex loss: 0.411974  [    2/   88]
per-ex loss: 0.654619  [    3/   88]
per-ex loss: 0.407544  [    4/   88]
per-ex loss: 0.345664  [    5/   88]
per-ex loss: 0.361578  [    6/   88]
per-ex loss: 0.494779  [    7/   88]
per-ex loss: 0.655598  [    8/   88]
per-ex loss: 0.520928  [    9/   88]
per-ex loss: 0.514956  [   10/   88]
per-ex loss: 0.393247  [   11/   88]
per-ex loss: 0.614065  [   12/   88]
per-ex loss: 0.371141  [   13/   88]
per-ex loss: 0.353175  [   14/   88]
per-ex loss: 0.516896  [   15/   88]
per-ex loss: 0.518388  [   16/   88]
per-ex loss: 0.603023  [   17/   88]
per-ex loss: 0.358176  [   18/   88]
per-ex loss: 0.407092  [   19/   88]
per-ex loss: 0.409161  [   20/   88]
per-ex loss: 0.514606  [   21/   88]
per-ex loss: 0.549850  [   22/   88]
per-ex loss: 0.464495  [   23/   88]
per-ex loss: 0.377468  [   24/   88]
per-ex loss: 0.378118  [   25/   88]
per-ex loss: 0.408180  [   26/   88]
per-ex loss: 0.542476  [   27/   88]
per-ex loss: 0.381209  [   28/   88]
per-ex loss: 0.551505  [   29/   88]
per-ex loss: 0.578467  [   30/   88]
per-ex loss: 0.533368  [   31/   88]
per-ex loss: 0.449453  [   32/   88]
per-ex loss: 0.648372  [   33/   88]
per-ex loss: 0.344531  [   34/   88]
per-ex loss: 0.393982  [   35/   88]
per-ex loss: 0.660339  [   36/   88]
per-ex loss: 0.414345  [   37/   88]
per-ex loss: 0.594961  [   38/   88]
per-ex loss: 0.391737  [   39/   88]
per-ex loss: 0.456779  [   40/   88]
per-ex loss: 0.643587  [   41/   88]
per-ex loss: 0.579749  [   42/   88]
per-ex loss: 0.537522  [   43/   88]
per-ex loss: 0.535588  [   44/   88]
per-ex loss: 0.384996  [   45/   88]
per-ex loss: 0.370578  [   46/   88]
per-ex loss: 0.450900  [   47/   88]
per-ex loss: 0.374791  [   48/   88]
per-ex loss: 0.596804  [   49/   88]
per-ex loss: 0.573806  [   50/   88]
per-ex loss: 0.402543  [   51/   88]
per-ex loss: 0.550266  [   52/   88]
per-ex loss: 0.367773  [   53/   88]
per-ex loss: 0.325110  [   54/   88]
per-ex loss: 0.349334  [   55/   88]
per-ex loss: 0.547344  [   56/   88]
per-ex loss: 0.435444  [   57/   88]
per-ex loss: 0.432423  [   58/   88]
per-ex loss: 0.461460  [   59/   88]
per-ex loss: 0.586897  [   60/   88]
per-ex loss: 0.438923  [   61/   88]
per-ex loss: 0.634356  [   62/   88]
per-ex loss: 0.714389  [   63/   88]
per-ex loss: 0.425308  [   64/   88]
per-ex loss: 0.418874  [   65/   88]
per-ex loss: 0.440442  [   66/   88]
per-ex loss: 0.392357  [   67/   88]
per-ex loss: 0.593216  [   68/   88]
per-ex loss: 0.404327  [   69/   88]
per-ex loss: 0.497473  [   70/   88]
per-ex loss: 0.405198  [   71/   88]
per-ex loss: 0.498720  [   72/   88]
per-ex loss: 0.632077  [   73/   88]
per-ex loss: 0.624921  [   74/   88]
per-ex loss: 0.430278  [   75/   88]
per-ex loss: 0.407382  [   76/   88]
per-ex loss: 0.609558  [   77/   88]
per-ex loss: 0.680663  [   78/   88]
per-ex loss: 0.399556  [   79/   88]
per-ex loss: 0.561687  [   80/   88]
per-ex loss: 0.592879  [   81/   88]
per-ex loss: 0.354123  [   82/   88]
per-ex loss: 0.330902  [   83/   88]
per-ex loss: 0.447157  [   84/   88]
per-ex loss: 0.586269  [   85/   88]
per-ex loss: 0.468878  [   86/   88]
per-ex loss: 0.317471  [   87/   88]
per-ex loss: 0.423511  [   88/   88]
Train Error: Avg loss: 0.47914412
validation Error: 
 Avg loss: 0.54384909 
 F1: 0.484575 
 Precision: 0.523638 
 Recall: 0.450936
 IoU: 0.319762

test Error: 
 Avg loss: 0.48424281 
 F1: 0.571784 
 Precision: 0.604763 
 Recall: 0.542216
 IoU: 0.400349

We have finished training iteration 61
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_59_.pth
per-ex loss: 0.455824  [    1/   88]
per-ex loss: 0.425490  [    2/   88]
per-ex loss: 0.299197  [    3/   88]
per-ex loss: 0.607581  [    4/   88]
per-ex loss: 0.634863  [    5/   88]
per-ex loss: 0.502427  [    6/   88]
per-ex loss: 0.580647  [    7/   88]
per-ex loss: 0.424295  [    8/   88]
per-ex loss: 0.422469  [    9/   88]
per-ex loss: 0.405086  [   10/   88]
per-ex loss: 0.422980  [   11/   88]
per-ex loss: 0.351515  [   12/   88]
per-ex loss: 0.377084  [   13/   88]
per-ex loss: 0.359088  [   14/   88]
per-ex loss: 0.650057  [   15/   88]
per-ex loss: 0.644710  [   16/   88]
per-ex loss: 0.655935  [   17/   88]
per-ex loss: 0.554659  [   18/   88]
per-ex loss: 0.543705  [   19/   88]
per-ex loss: 0.373558  [   20/   88]
per-ex loss: 0.456305  [   21/   88]
per-ex loss: 0.452328  [   22/   88]
per-ex loss: 0.442928  [   23/   88]
per-ex loss: 0.602578  [   24/   88]
per-ex loss: 0.357552  [   25/   88]
per-ex loss: 0.389125  [   26/   88]
per-ex loss: 0.424403  [   27/   88]
per-ex loss: 0.420480  [   28/   88]
per-ex loss: 0.579814  [   29/   88]
per-ex loss: 0.386953  [   30/   88]
per-ex loss: 0.420606  [   31/   88]
per-ex loss: 0.520758  [   32/   88]
per-ex loss: 0.678179  [   33/   88]
per-ex loss: 0.417435  [   34/   88]
per-ex loss: 0.578581  [   35/   88]
per-ex loss: 0.575249  [   36/   88]
per-ex loss: 0.540447  [   37/   88]
per-ex loss: 0.340467  [   38/   88]
per-ex loss: 0.422787  [   39/   88]
per-ex loss: 0.525426  [   40/   88]
per-ex loss: 0.442902  [   41/   88]
per-ex loss: 0.537755  [   42/   88]
per-ex loss: 0.499101  [   43/   88]
per-ex loss: 0.473718  [   44/   88]
per-ex loss: 0.632130  [   45/   88]
per-ex loss: 0.376803  [   46/   88]
per-ex loss: 0.369745  [   47/   88]
per-ex loss: 0.358998  [   48/   88]
per-ex loss: 0.357199  [   49/   88]
per-ex loss: 0.347111  [   50/   88]
per-ex loss: 0.580143  [   51/   88]
per-ex loss: 0.366676  [   52/   88]
per-ex loss: 0.449736  [   53/   88]
per-ex loss: 0.545252  [   54/   88]
per-ex loss: 0.361526  [   55/   88]
per-ex loss: 0.656485  [   56/   88]
per-ex loss: 0.362336  [   57/   88]
per-ex loss: 0.585888  [   58/   88]
per-ex loss: 0.544925  [   59/   88]
per-ex loss: 0.503582  [   60/   88]
per-ex loss: 0.400557  [   61/   88]
per-ex loss: 0.596314  [   62/   88]
per-ex loss: 0.522977  [   63/   88]
per-ex loss: 0.557101  [   64/   88]
per-ex loss: 0.634434  [   65/   88]
per-ex loss: 0.624086  [   66/   88]
per-ex loss: 0.435484  [   67/   88]
per-ex loss: 0.411610  [   68/   88]
per-ex loss: 0.376607  [   69/   88]
per-ex loss: 0.431830  [   70/   88]
per-ex loss: 0.612098  [   71/   88]
per-ex loss: 0.461765  [   72/   88]
per-ex loss: 0.383647  [   73/   88]
per-ex loss: 0.419573  [   74/   88]
per-ex loss: 0.380142  [   75/   88]
per-ex loss: 0.543931  [   76/   88]
per-ex loss: 0.342936  [   77/   88]
per-ex loss: 0.358404  [   78/   88]
per-ex loss: 0.409716  [   79/   88]
per-ex loss: 0.418829  [   80/   88]
per-ex loss: 0.708638  [   81/   88]
per-ex loss: 0.589396  [   82/   88]
per-ex loss: 0.610942  [   83/   88]
per-ex loss: 0.541228  [   84/   88]
per-ex loss: 0.392609  [   85/   88]
per-ex loss: 0.620893  [   86/   88]
per-ex loss: 0.386343  [   87/   88]
per-ex loss: 0.591403  [   88/   88]
Train Error: Avg loss: 0.48112583
validation Error: 
 Avg loss: 0.54559098 
 F1: 0.476651 
 Precision: 0.497321 
 Recall: 0.457631
 IoU: 0.312897

test Error: 
 Avg loss: 0.49160802 
 F1: 0.560979 
 Precision: 0.563314 
 Recall: 0.558664
 IoU: 0.389834

We have finished training iteration 62
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_60_.pth
per-ex loss: 0.608680  [    1/   88]
per-ex loss: 0.655269  [    2/   88]
per-ex loss: 0.335943  [    3/   88]
per-ex loss: 0.331957  [    4/   88]
per-ex loss: 0.620133  [    5/   88]
per-ex loss: 0.418101  [    6/   88]
per-ex loss: 0.347770  [    7/   88]
per-ex loss: 0.403714  [    8/   88]
per-ex loss: 0.383679  [    9/   88]
per-ex loss: 0.427840  [   10/   88]
per-ex loss: 0.516022  [   11/   88]
per-ex loss: 0.402795  [   12/   88]
per-ex loss: 0.435998  [   13/   88]
per-ex loss: 0.514238  [   14/   88]
per-ex loss: 0.385805  [   15/   88]
per-ex loss: 0.340649  [   16/   88]
per-ex loss: 0.434376  [   17/   88]
per-ex loss: 0.446549  [   18/   88]
per-ex loss: 0.365253  [   19/   88]
per-ex loss: 0.626989  [   20/   88]
per-ex loss: 0.336970  [   21/   88]
per-ex loss: 0.349251  [   22/   88]
per-ex loss: 0.386434  [   23/   88]
per-ex loss: 0.501790  [   24/   88]
per-ex loss: 0.562082  [   25/   88]
per-ex loss: 0.518309  [   26/   88]
per-ex loss: 0.594053  [   27/   88]
per-ex loss: 0.407152  [   28/   88]
per-ex loss: 0.536454  [   29/   88]
per-ex loss: 0.515520  [   30/   88]
per-ex loss: 0.636024  [   31/   88]
per-ex loss: 0.537473  [   32/   88]
per-ex loss: 0.585198  [   33/   88]
per-ex loss: 0.365219  [   34/   88]
per-ex loss: 0.641389  [   35/   88]
per-ex loss: 0.728126  [   36/   88]
per-ex loss: 0.499613  [   37/   88]
per-ex loss: 0.590360  [   38/   88]
per-ex loss: 0.466491  [   39/   88]
per-ex loss: 0.422238  [   40/   88]
per-ex loss: 0.409828  [   41/   88]
per-ex loss: 0.399179  [   42/   88]
per-ex loss: 0.400141  [   43/   88]
per-ex loss: 0.392210  [   44/   88]
per-ex loss: 0.583473  [   45/   88]
per-ex loss: 0.381697  [   46/   88]
per-ex loss: 0.504058  [   47/   88]
per-ex loss: 0.445390  [   48/   88]
per-ex loss: 0.506957  [   49/   88]
per-ex loss: 0.624971  [   50/   88]
per-ex loss: 0.439039  [   51/   88]
per-ex loss: 0.535208  [   52/   88]
per-ex loss: 0.505470  [   53/   88]
per-ex loss: 0.375670  [   54/   88]
per-ex loss: 0.607708  [   55/   88]
per-ex loss: 0.382759  [   56/   88]
per-ex loss: 0.678254  [   57/   88]
per-ex loss: 0.408270  [   58/   88]
per-ex loss: 0.385892  [   59/   88]
per-ex loss: 0.310186  [   60/   88]
per-ex loss: 0.598031  [   61/   88]
per-ex loss: 0.436655  [   62/   88]
per-ex loss: 0.481694  [   63/   88]
per-ex loss: 0.512739  [   64/   88]
per-ex loss: 0.558349  [   65/   88]
per-ex loss: 0.341934  [   66/   88]
per-ex loss: 0.588797  [   67/   88]
per-ex loss: 0.606298  [   68/   88]
per-ex loss: 0.453345  [   69/   88]
per-ex loss: 0.566503  [   70/   88]
per-ex loss: 0.443821  [   71/   88]
per-ex loss: 0.372061  [   72/   88]
per-ex loss: 0.536002  [   73/   88]
per-ex loss: 0.405574  [   74/   88]
per-ex loss: 0.382995  [   75/   88]
per-ex loss: 0.534598  [   76/   88]
per-ex loss: 0.578785  [   77/   88]
per-ex loss: 0.441120  [   78/   88]
per-ex loss: 0.354501  [   79/   88]
per-ex loss: 0.428252  [   80/   88]
per-ex loss: 0.413554  [   81/   88]
per-ex loss: 0.675377  [   82/   88]
per-ex loss: 0.625112  [   83/   88]
per-ex loss: 0.375038  [   84/   88]
per-ex loss: 0.648425  [   85/   88]
per-ex loss: 0.327946  [   86/   88]
per-ex loss: 0.674686  [   87/   88]
per-ex loss: 0.385806  [   88/   88]
Train Error: Avg loss: 0.47961666
validation Error: 
 Avg loss: 0.52209269 
 F1: 0.509538 
 Precision: 0.573951 
 Recall: 0.458124
 IoU: 0.341865

test Error: 
 Avg loss: 0.48313017 
 F1: 0.568807 
 Precision: 0.604913 
 Recall: 0.536768
 IoU: 0.397435

We have finished training iteration 63
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_61_.pth
per-ex loss: 0.403652  [    1/   88]
per-ex loss: 0.354715  [    2/   88]
per-ex loss: 0.555142  [    3/   88]
per-ex loss: 0.573254  [    4/   88]
per-ex loss: 0.386552  [    5/   88]
per-ex loss: 0.539324  [    6/   88]
per-ex loss: 0.642293  [    7/   88]
per-ex loss: 0.541057  [    8/   88]
per-ex loss: 0.532611  [    9/   88]
per-ex loss: 0.563243  [   10/   88]
per-ex loss: 0.662964  [   11/   88]
per-ex loss: 0.482525  [   12/   88]
per-ex loss: 0.396919  [   13/   88]
per-ex loss: 0.570822  [   14/   88]
per-ex loss: 0.663919  [   15/   88]
per-ex loss: 0.390911  [   16/   88]
per-ex loss: 0.583423  [   17/   88]
per-ex loss: 0.418115  [   18/   88]
per-ex loss: 0.639540  [   19/   88]
per-ex loss: 0.531340  [   20/   88]
per-ex loss: 0.450697  [   21/   88]
per-ex loss: 0.445024  [   22/   88]
per-ex loss: 0.447947  [   23/   88]
per-ex loss: 0.355769  [   24/   88]
per-ex loss: 0.451396  [   25/   88]
per-ex loss: 0.459291  [   26/   88]
per-ex loss: 0.483036  [   27/   88]
per-ex loss: 0.307872  [   28/   88]
per-ex loss: 0.512991  [   29/   88]
per-ex loss: 0.515043  [   30/   88]
per-ex loss: 0.436724  [   31/   88]
per-ex loss: 0.577989  [   32/   88]
per-ex loss: 0.349710  [   33/   88]
per-ex loss: 0.409131  [   34/   88]
per-ex loss: 0.624077  [   35/   88]
per-ex loss: 0.370788  [   36/   88]
per-ex loss: 0.466822  [   37/   88]
per-ex loss: 0.427510  [   38/   88]
per-ex loss: 0.348081  [   39/   88]
per-ex loss: 0.406478  [   40/   88]
per-ex loss: 0.658768  [   41/   88]
per-ex loss: 0.401694  [   42/   88]
per-ex loss: 0.541592  [   43/   88]
per-ex loss: 0.603772  [   44/   88]
per-ex loss: 0.275603  [   45/   88]
per-ex loss: 0.408948  [   46/   88]
per-ex loss: 0.412048  [   47/   88]
per-ex loss: 0.368651  [   48/   88]
per-ex loss: 0.592632  [   49/   88]
per-ex loss: 0.375522  [   50/   88]
per-ex loss: 0.561334  [   51/   88]
per-ex loss: 0.616780  [   52/   88]
per-ex loss: 0.405249  [   53/   88]
per-ex loss: 0.421153  [   54/   88]
per-ex loss: 0.396588  [   55/   88]
per-ex loss: 0.595357  [   56/   88]
per-ex loss: 0.515719  [   57/   88]
per-ex loss: 0.624519  [   58/   88]
per-ex loss: 0.617077  [   59/   88]
per-ex loss: 0.420632  [   60/   88]
per-ex loss: 0.524568  [   61/   88]
per-ex loss: 0.567375  [   62/   88]
per-ex loss: 0.407034  [   63/   88]
per-ex loss: 0.491351  [   64/   88]
per-ex loss: 0.361419  [   65/   88]
per-ex loss: 0.372351  [   66/   88]
per-ex loss: 0.445969  [   67/   88]
per-ex loss: 0.665253  [   68/   88]
per-ex loss: 0.325553  [   69/   88]
per-ex loss: 0.510418  [   70/   88]
per-ex loss: 0.636994  [   71/   88]
per-ex loss: 0.422682  [   72/   88]
per-ex loss: 0.450745  [   73/   88]
per-ex loss: 0.408960  [   74/   88]
per-ex loss: 0.393990  [   75/   88]
per-ex loss: 0.406044  [   76/   88]
per-ex loss: 0.645848  [   77/   88]
per-ex loss: 0.520995  [   78/   88]
per-ex loss: 0.628923  [   79/   88]
per-ex loss: 0.507503  [   80/   88]
per-ex loss: 0.352275  [   81/   88]
per-ex loss: 0.420562  [   82/   88]
per-ex loss: 0.351775  [   83/   88]
per-ex loss: 0.453568  [   84/   88]
per-ex loss: 0.355947  [   85/   88]
per-ex loss: 0.460391  [   86/   88]
per-ex loss: 0.351935  [   87/   88]
per-ex loss: 0.426217  [   88/   88]
Train Error: Avg loss: 0.47680659
validation Error: 
 Avg loss: 0.53526850 
 F1: 0.495067 
 Precision: 0.557067 
 Recall: 0.445485
 IoU: 0.328962

test Error: 
 Avg loss: 0.48196105 
 F1: 0.571619 
 Precision: 0.622484 
 Recall: 0.528440
 IoU: 0.400187

We have finished training iteration 64
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_62_.pth
per-ex loss: 0.553272  [    1/   88]
per-ex loss: 0.388223  [    2/   88]
per-ex loss: 0.358415  [    3/   88]
per-ex loss: 0.662964  [    4/   88]
per-ex loss: 0.314182  [    5/   88]
per-ex loss: 0.691334  [    6/   88]
per-ex loss: 0.373174  [    7/   88]
per-ex loss: 0.539658  [    8/   88]
per-ex loss: 0.533251  [    9/   88]
per-ex loss: 0.338563  [   10/   88]
per-ex loss: 0.387410  [   11/   88]
per-ex loss: 0.369268  [   12/   88]
per-ex loss: 0.397626  [   13/   88]
per-ex loss: 0.366218  [   14/   88]
per-ex loss: 0.396515  [   15/   88]
per-ex loss: 0.398235  [   16/   88]
per-ex loss: 0.527783  [   17/   88]
per-ex loss: 0.409049  [   18/   88]
per-ex loss: 0.559904  [   19/   88]
per-ex loss: 0.536237  [   20/   88]
per-ex loss: 0.695810  [   21/   88]
per-ex loss: 0.601254  [   22/   88]
per-ex loss: 0.484750  [   23/   88]
per-ex loss: 0.584975  [   24/   88]
per-ex loss: 0.492183  [   25/   88]
per-ex loss: 0.283617  [   26/   88]
per-ex loss: 0.651645  [   27/   88]
per-ex loss: 0.363865  [   28/   88]
per-ex loss: 0.642988  [   29/   88]
per-ex loss: 0.579605  [   30/   88]
per-ex loss: 0.376686  [   31/   88]
per-ex loss: 0.414397  [   32/   88]
per-ex loss: 0.537565  [   33/   88]
per-ex loss: 0.483284  [   34/   88]
per-ex loss: 0.615524  [   35/   88]
per-ex loss: 0.395148  [   36/   88]
per-ex loss: 0.422470  [   37/   88]
per-ex loss: 0.582519  [   38/   88]
per-ex loss: 0.479929  [   39/   88]
per-ex loss: 0.515005  [   40/   88]
per-ex loss: 0.454878  [   41/   88]
per-ex loss: 0.595490  [   42/   88]
per-ex loss: 0.398494  [   43/   88]
per-ex loss: 0.379478  [   44/   88]
per-ex loss: 0.313617  [   45/   88]
per-ex loss: 0.603631  [   46/   88]
per-ex loss: 0.436212  [   47/   88]
per-ex loss: 0.637271  [   48/   88]
per-ex loss: 0.559501  [   49/   88]
per-ex loss: 0.351602  [   50/   88]
per-ex loss: 0.477619  [   51/   88]
per-ex loss: 0.391987  [   52/   88]
per-ex loss: 0.424888  [   53/   88]
per-ex loss: 0.632521  [   54/   88]
per-ex loss: 0.390216  [   55/   88]
per-ex loss: 0.406492  [   56/   88]
per-ex loss: 0.386757  [   57/   88]
per-ex loss: 0.393530  [   58/   88]
per-ex loss: 0.436126  [   59/   88]
per-ex loss: 0.658242  [   60/   88]
per-ex loss: 0.331719  [   61/   88]
per-ex loss: 0.386474  [   62/   88]
per-ex loss: 0.546627  [   63/   88]
per-ex loss: 0.398265  [   64/   88]
per-ex loss: 0.493282  [   65/   88]
per-ex loss: 0.375831  [   66/   88]
per-ex loss: 0.454238  [   67/   88]
per-ex loss: 0.337605  [   68/   88]
per-ex loss: 0.393105  [   69/   88]
per-ex loss: 0.612568  [   70/   88]
per-ex loss: 0.358755  [   71/   88]
per-ex loss: 0.474771  [   72/   88]
per-ex loss: 0.344755  [   73/   88]
per-ex loss: 0.422364  [   74/   88]
per-ex loss: 0.375475  [   75/   88]
per-ex loss: 0.688853  [   76/   88]
per-ex loss: 0.348422  [   77/   88]
per-ex loss: 0.481892  [   78/   88]
per-ex loss: 0.461941  [   79/   88]
per-ex loss: 0.404313  [   80/   88]
per-ex loss: 0.355306  [   81/   88]
per-ex loss: 0.565527  [   82/   88]
per-ex loss: 0.559972  [   83/   88]
per-ex loss: 0.564556  [   84/   88]
per-ex loss: 0.406486  [   85/   88]
per-ex loss: 0.612829  [   86/   88]
per-ex loss: 0.682541  [   87/   88]
per-ex loss: 0.495353  [   88/   88]
Train Error: Avg loss: 0.47235085
validation Error: 
 Avg loss: 0.52990040 
 F1: 0.501482 
 Precision: 0.532340 
 Recall: 0.474006
 IoU: 0.334652

test Error: 
 Avg loss: 0.48108876 
 F1: 0.571758 
 Precision: 0.553900 
 Recall: 0.590805
 IoU: 0.400323

We have finished training iteration 65
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_40_.pth
per-ex loss: 0.367938  [    1/   88]
per-ex loss: 0.438586  [    2/   88]
per-ex loss: 0.405457  [    3/   88]
per-ex loss: 0.476552  [    4/   88]
per-ex loss: 0.435771  [    5/   88]
per-ex loss: 0.561143  [    6/   88]
per-ex loss: 0.426105  [    7/   88]
per-ex loss: 0.522423  [    8/   88]
per-ex loss: 0.521264  [    9/   88]
per-ex loss: 0.682968  [   10/   88]
per-ex loss: 0.427085  [   11/   88]
per-ex loss: 0.380627  [   12/   88]
per-ex loss: 0.596812  [   13/   88]
per-ex loss: 0.364550  [   14/   88]
per-ex loss: 0.388091  [   15/   88]
per-ex loss: 0.490703  [   16/   88]
per-ex loss: 0.569652  [   17/   88]
per-ex loss: 0.436400  [   18/   88]
per-ex loss: 0.323463  [   19/   88]
per-ex loss: 0.348262  [   20/   88]
per-ex loss: 0.360632  [   21/   88]
per-ex loss: 0.397488  [   22/   88]
per-ex loss: 0.487438  [   23/   88]
per-ex loss: 0.417967  [   24/   88]
per-ex loss: 0.641647  [   25/   88]
per-ex loss: 0.666344  [   26/   88]
per-ex loss: 0.619171  [   27/   88]
per-ex loss: 0.580435  [   28/   88]
per-ex loss: 0.500949  [   29/   88]
per-ex loss: 0.403583  [   30/   88]
per-ex loss: 0.589167  [   31/   88]
per-ex loss: 0.348282  [   32/   88]
per-ex loss: 0.357747  [   33/   88]
per-ex loss: 0.600207  [   34/   88]
per-ex loss: 0.641522  [   35/   88]
per-ex loss: 0.531799  [   36/   88]
per-ex loss: 0.482666  [   37/   88]
per-ex loss: 0.425768  [   38/   88]
per-ex loss: 0.445341  [   39/   88]
per-ex loss: 0.412057  [   40/   88]
per-ex loss: 0.550166  [   41/   88]
per-ex loss: 0.395758  [   42/   88]
per-ex loss: 0.293877  [   43/   88]
per-ex loss: 0.367858  [   44/   88]
per-ex loss: 0.390205  [   45/   88]
per-ex loss: 0.503764  [   46/   88]
per-ex loss: 0.625251  [   47/   88]
per-ex loss: 0.423956  [   48/   88]
per-ex loss: 0.544915  [   49/   88]
per-ex loss: 0.583971  [   50/   88]
per-ex loss: 0.471314  [   51/   88]
per-ex loss: 0.657111  [   52/   88]
per-ex loss: 0.326860  [   53/   88]
per-ex loss: 0.457465  [   54/   88]
per-ex loss: 0.629042  [   55/   88]
per-ex loss: 0.335861  [   56/   88]
per-ex loss: 0.354506  [   57/   88]
per-ex loss: 0.485276  [   58/   88]
per-ex loss: 0.569565  [   59/   88]
per-ex loss: 0.580428  [   60/   88]
per-ex loss: 0.420068  [   61/   88]
per-ex loss: 0.363997  [   62/   88]
per-ex loss: 0.615945  [   63/   88]
per-ex loss: 0.443866  [   64/   88]
per-ex loss: 0.537063  [   65/   88]
per-ex loss: 0.400766  [   66/   88]
per-ex loss: 0.381119  [   67/   88]
per-ex loss: 0.592944  [   68/   88]
per-ex loss: 0.392067  [   69/   88]
per-ex loss: 0.385704  [   70/   88]
per-ex loss: 0.478631  [   71/   88]
per-ex loss: 0.444068  [   72/   88]
per-ex loss: 0.562516  [   73/   88]
per-ex loss: 0.521795  [   74/   88]
per-ex loss: 0.416687  [   75/   88]
per-ex loss: 0.548709  [   76/   88]
per-ex loss: 0.636547  [   77/   88]
per-ex loss: 0.648096  [   78/   88]
per-ex loss: 0.419849  [   79/   88]
per-ex loss: 0.668027  [   80/   88]
per-ex loss: 0.573789  [   81/   88]
per-ex loss: 0.345014  [   82/   88]
per-ex loss: 0.361521  [   83/   88]
per-ex loss: 0.406505  [   84/   88]
per-ex loss: 0.359083  [   85/   88]
per-ex loss: 0.381680  [   86/   88]
per-ex loss: 0.612691  [   87/   88]
per-ex loss: 0.477643  [   88/   88]
Train Error: Avg loss: 0.47747276
validation Error: 
 Avg loss: 0.54064232 
 F1: 0.490276 
 Precision: 0.521148 
 Recall: 0.462856
 IoU: 0.324745

test Error: 
 Avg loss: 0.48652730 
 F1: 0.564500 
 Precision: 0.584419 
 Recall: 0.545895
 IoU: 0.393243

We have finished training iteration 66
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_64_.pth
per-ex loss: 0.351720  [    1/   88]
per-ex loss: 0.450737  [    2/   88]
per-ex loss: 0.360585  [    3/   88]
per-ex loss: 0.549739  [    4/   88]
per-ex loss: 0.384534  [    5/   88]
per-ex loss: 0.349601  [    6/   88]
per-ex loss: 0.562516  [    7/   88]
per-ex loss: 0.518127  [    8/   88]
per-ex loss: 0.456416  [    9/   88]
per-ex loss: 0.382246  [   10/   88]
per-ex loss: 0.469756  [   11/   88]
per-ex loss: 0.303177  [   12/   88]
per-ex loss: 0.553285  [   13/   88]
per-ex loss: 0.570731  [   14/   88]
per-ex loss: 0.621043  [   15/   88]
per-ex loss: 0.346584  [   16/   88]
per-ex loss: 0.442268  [   17/   88]
per-ex loss: 0.396469  [   18/   88]
per-ex loss: 0.351641  [   19/   88]
per-ex loss: 0.668144  [   20/   88]
per-ex loss: 0.631649  [   21/   88]
per-ex loss: 0.514420  [   22/   88]
per-ex loss: 0.511614  [   23/   88]
per-ex loss: 0.414020  [   24/   88]
per-ex loss: 0.436177  [   25/   88]
per-ex loss: 0.392848  [   26/   88]
per-ex loss: 0.452751  [   27/   88]
per-ex loss: 0.616171  [   28/   88]
per-ex loss: 0.576418  [   29/   88]
per-ex loss: 0.351640  [   30/   88]
per-ex loss: 0.385460  [   31/   88]
per-ex loss: 0.543371  [   32/   88]
per-ex loss: 0.409265  [   33/   88]
per-ex loss: 0.531156  [   34/   88]
per-ex loss: 0.422430  [   35/   88]
per-ex loss: 0.567651  [   36/   88]
per-ex loss: 0.404959  [   37/   88]
per-ex loss: 0.404692  [   38/   88]
per-ex loss: 0.684343  [   39/   88]
per-ex loss: 0.480423  [   40/   88]
per-ex loss: 0.556468  [   41/   88]
per-ex loss: 0.479129  [   42/   88]
per-ex loss: 0.447753  [   43/   88]
per-ex loss: 0.400434  [   44/   88]
per-ex loss: 0.362114  [   45/   88]
per-ex loss: 0.524762  [   46/   88]
per-ex loss: 0.329004  [   47/   88]
per-ex loss: 0.595149  [   48/   88]
per-ex loss: 0.360680  [   49/   88]
per-ex loss: 0.475958  [   50/   88]
per-ex loss: 0.592342  [   51/   88]
per-ex loss: 0.376918  [   52/   88]
per-ex loss: 0.425263  [   53/   88]
per-ex loss: 0.616053  [   54/   88]
per-ex loss: 0.550555  [   55/   88]
per-ex loss: 0.346241  [   56/   88]
per-ex loss: 0.502194  [   57/   88]
per-ex loss: 0.580364  [   58/   88]
per-ex loss: 0.402269  [   59/   88]
per-ex loss: 0.344904  [   60/   88]
per-ex loss: 0.390990  [   61/   88]
per-ex loss: 0.558963  [   62/   88]
per-ex loss: 0.557812  [   63/   88]
per-ex loss: 0.624267  [   64/   88]
per-ex loss: 0.628052  [   65/   88]
per-ex loss: 0.339284  [   66/   88]
per-ex loss: 0.404242  [   67/   88]
per-ex loss: 0.516700  [   68/   88]
per-ex loss: 0.424215  [   69/   88]
per-ex loss: 0.586305  [   70/   88]
per-ex loss: 0.386800  [   71/   88]
per-ex loss: 0.380646  [   72/   88]
per-ex loss: 0.395572  [   73/   88]
per-ex loss: 0.348708  [   74/   88]
per-ex loss: 0.415417  [   75/   88]
per-ex loss: 0.702570  [   76/   88]
per-ex loss: 0.429323  [   77/   88]
per-ex loss: 0.357137  [   78/   88]
per-ex loss: 0.637620  [   79/   88]
per-ex loss: 0.601944  [   80/   88]
per-ex loss: 0.425200  [   81/   88]
per-ex loss: 0.374818  [   82/   88]
per-ex loss: 0.642154  [   83/   88]
per-ex loss: 0.678216  [   84/   88]
per-ex loss: 0.417446  [   85/   88]
per-ex loss: 0.597841  [   86/   88]
per-ex loss: 0.442573  [   87/   88]
per-ex loss: 0.336389  [   88/   88]
Train Error: Avg loss: 0.47375613
validation Error: 
 Avg loss: 0.53798104 
 F1: 0.485367 
 Precision: 0.544034 
 Recall: 0.438122
 IoU: 0.320452

test Error: 
 Avg loss: 0.48214116 
 F1: 0.571941 
 Precision: 0.608284 
 Recall: 0.539696
 IoU: 0.400502

We have finished training iteration 67
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_52_.pth
per-ex loss: 0.488135  [    1/   88]
per-ex loss: 0.369248  [    2/   88]
per-ex loss: 0.630467  [    3/   88]
per-ex loss: 0.609186  [    4/   88]
per-ex loss: 0.436680  [    5/   88]
per-ex loss: 0.369691  [    6/   88]
per-ex loss: 0.538219  [    7/   88]
per-ex loss: 0.575892  [    8/   88]
per-ex loss: 0.356231  [    9/   88]
per-ex loss: 0.417067  [   10/   88]
per-ex loss: 0.356895  [   11/   88]
per-ex loss: 0.398975  [   12/   88]
per-ex loss: 0.399500  [   13/   88]
per-ex loss: 0.394844  [   14/   88]
per-ex loss: 0.459466  [   15/   88]
per-ex loss: 0.549226  [   16/   88]
per-ex loss: 0.508398  [   17/   88]
per-ex loss: 0.420580  [   18/   88]
per-ex loss: 0.613526  [   19/   88]
per-ex loss: 0.373369  [   20/   88]
per-ex loss: 0.653250  [   21/   88]
per-ex loss: 0.351892  [   22/   88]
per-ex loss: 0.356430  [   23/   88]
per-ex loss: 0.528979  [   24/   88]
per-ex loss: 0.559115  [   25/   88]
per-ex loss: 0.422581  [   26/   88]
per-ex loss: 0.515000  [   27/   88]
per-ex loss: 0.587011  [   28/   88]
per-ex loss: 0.535249  [   29/   88]
per-ex loss: 0.406689  [   30/   88]
per-ex loss: 0.484765  [   31/   88]
per-ex loss: 0.380359  [   32/   88]
per-ex loss: 0.457099  [   33/   88]
per-ex loss: 0.351063  [   34/   88]
per-ex loss: 0.405683  [   35/   88]
per-ex loss: 0.466599  [   36/   88]
per-ex loss: 0.456928  [   37/   88]
per-ex loss: 0.495210  [   38/   88]
per-ex loss: 0.662570  [   39/   88]
per-ex loss: 0.645653  [   40/   88]
per-ex loss: 0.568390  [   41/   88]
per-ex loss: 0.598430  [   42/   88]
per-ex loss: 0.394856  [   43/   88]
per-ex loss: 0.581921  [   44/   88]
per-ex loss: 0.525083  [   45/   88]
per-ex loss: 0.569292  [   46/   88]
per-ex loss: 0.696776  [   47/   88]
per-ex loss: 0.403940  [   48/   88]
per-ex loss: 0.643843  [   49/   88]
per-ex loss: 0.421331  [   50/   88]
per-ex loss: 0.377716  [   51/   88]
per-ex loss: 0.418951  [   52/   88]
per-ex loss: 0.625033  [   53/   88]
per-ex loss: 0.328251  [   54/   88]
per-ex loss: 0.400215  [   55/   88]
per-ex loss: 0.400586  [   56/   88]
per-ex loss: 0.318696  [   57/   88]
per-ex loss: 0.615103  [   58/   88]
per-ex loss: 0.443130  [   59/   88]
per-ex loss: 0.598717  [   60/   88]
per-ex loss: 0.564231  [   61/   88]
per-ex loss: 0.402057  [   62/   88]
per-ex loss: 0.336599  [   63/   88]
per-ex loss: 0.542447  [   64/   88]
per-ex loss: 0.364234  [   65/   88]
per-ex loss: 0.491562  [   66/   88]
per-ex loss: 0.376622  [   67/   88]
per-ex loss: 0.574728  [   68/   88]
per-ex loss: 0.334534  [   69/   88]
per-ex loss: 0.347753  [   70/   88]
per-ex loss: 0.347826  [   71/   88]
per-ex loss: 0.453180  [   72/   88]
per-ex loss: 0.389236  [   73/   88]
per-ex loss: 0.347189  [   74/   88]
per-ex loss: 0.393121  [   75/   88]
per-ex loss: 0.602822  [   76/   88]
per-ex loss: 0.424667  [   77/   88]
per-ex loss: 0.566562  [   78/   88]
per-ex loss: 0.305482  [   79/   88]
per-ex loss: 0.395894  [   80/   88]
per-ex loss: 0.513792  [   81/   88]
per-ex loss: 0.417438  [   82/   88]
per-ex loss: 0.682898  [   83/   88]
per-ex loss: 0.551761  [   84/   88]
per-ex loss: 0.489140  [   85/   88]
per-ex loss: 0.492146  [   86/   88]
per-ex loss: 0.404848  [   87/   88]
per-ex loss: 0.597412  [   88/   88]
Train Error: Avg loss: 0.47304732
validation Error: 
 Avg loss: 0.54762409 
 F1: 0.480403 
 Precision: 0.493426 
 Recall: 0.468050
 IoU: 0.316139

test Error: 
 Avg loss: 0.48316619 
 F1: 0.564894 
 Precision: 0.585957 
 Recall: 0.545292
 IoU: 0.393625

We have finished training iteration 68
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_66_.pth
per-ex loss: 0.475734  [    1/   88]
per-ex loss: 0.574710  [    2/   88]
per-ex loss: 0.406157  [    3/   88]
per-ex loss: 0.343241  [    4/   88]
per-ex loss: 0.394127  [    5/   88]
per-ex loss: 0.375073  [    6/   88]
per-ex loss: 0.412129  [    7/   88]
per-ex loss: 0.432251  [    8/   88]
per-ex loss: 0.497137  [    9/   88]
per-ex loss: 0.611287  [   10/   88]
per-ex loss: 0.423683  [   11/   88]
per-ex loss: 0.603599  [   12/   88]
per-ex loss: 0.540463  [   13/   88]
per-ex loss: 0.582276  [   14/   88]
per-ex loss: 0.317112  [   15/   88]
per-ex loss: 0.453966  [   16/   88]
per-ex loss: 0.477859  [   17/   88]
per-ex loss: 0.624299  [   18/   88]
per-ex loss: 0.603163  [   19/   88]
per-ex loss: 0.392710  [   20/   88]
per-ex loss: 0.417879  [   21/   88]
per-ex loss: 0.405014  [   22/   88]
per-ex loss: 0.565611  [   23/   88]
per-ex loss: 0.360258  [   24/   88]
per-ex loss: 0.336279  [   25/   88]
per-ex loss: 0.668924  [   26/   88]
per-ex loss: 0.396755  [   27/   88]
per-ex loss: 0.419164  [   28/   88]
per-ex loss: 0.497650  [   29/   88]
per-ex loss: 0.528883  [   30/   88]
per-ex loss: 0.354276  [   31/   88]
per-ex loss: 0.365798  [   32/   88]
per-ex loss: 0.535404  [   33/   88]
per-ex loss: 0.576506  [   34/   88]
per-ex loss: 0.297330  [   35/   88]
per-ex loss: 0.690700  [   36/   88]
per-ex loss: 0.440039  [   37/   88]
per-ex loss: 0.380726  [   38/   88]
per-ex loss: 0.552506  [   39/   88]
per-ex loss: 0.527795  [   40/   88]
per-ex loss: 0.566562  [   41/   88]
per-ex loss: 0.594885  [   42/   88]
per-ex loss: 0.514765  [   43/   88]
per-ex loss: 0.636734  [   44/   88]
per-ex loss: 0.529064  [   45/   88]
per-ex loss: 0.571158  [   46/   88]
per-ex loss: 0.385592  [   47/   88]
per-ex loss: 0.599478  [   48/   88]
per-ex loss: 0.387728  [   49/   88]
per-ex loss: 0.398744  [   50/   88]
per-ex loss: 0.453776  [   51/   88]
per-ex loss: 0.593495  [   52/   88]
per-ex loss: 0.384351  [   53/   88]
per-ex loss: 0.385019  [   54/   88]
per-ex loss: 0.332151  [   55/   88]
per-ex loss: 0.348362  [   56/   88]
per-ex loss: 0.447185  [   57/   88]
per-ex loss: 0.405571  [   58/   88]
per-ex loss: 0.584779  [   59/   88]
per-ex loss: 0.649694  [   60/   88]
per-ex loss: 0.457367  [   61/   88]
per-ex loss: 0.390226  [   62/   88]
per-ex loss: 0.410246  [   63/   88]
per-ex loss: 0.485270  [   64/   88]
per-ex loss: 0.419792  [   65/   88]
per-ex loss: 0.367708  [   66/   88]
per-ex loss: 0.399521  [   67/   88]
per-ex loss: 0.446367  [   68/   88]
per-ex loss: 0.376337  [   69/   88]
per-ex loss: 0.373324  [   70/   88]
per-ex loss: 0.444155  [   71/   88]
per-ex loss: 0.331483  [   72/   88]
per-ex loss: 0.528803  [   73/   88]
per-ex loss: 0.382509  [   74/   88]
per-ex loss: 0.502302  [   75/   88]
per-ex loss: 0.621016  [   76/   88]
per-ex loss: 0.658119  [   77/   88]
per-ex loss: 0.487162  [   78/   88]
per-ex loss: 0.547989  [   79/   88]
per-ex loss: 0.563368  [   80/   88]
per-ex loss: 0.627106  [   81/   88]
per-ex loss: 0.639202  [   82/   88]
per-ex loss: 0.427998  [   83/   88]
per-ex loss: 0.661875  [   84/   88]
per-ex loss: 0.372530  [   85/   88]
per-ex loss: 0.528401  [   86/   88]
per-ex loss: 0.340083  [   87/   88]
per-ex loss: 0.437059  [   88/   88]
Train Error: Avg loss: 0.47562367
validation Error: 
 Avg loss: 0.56184241 
 F1: 0.461098 
 Precision: 0.416352 
 Recall: 0.516619
 IoU: 0.299628

test Error: 
 Avg loss: 0.49313074 
 F1: 0.552700 
 Precision: 0.500149 
 Recall: 0.617589
 IoU: 0.381883

We have finished training iteration 69
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_67_.pth
per-ex loss: 0.388457  [    1/   88]
per-ex loss: 0.387415  [    2/   88]
per-ex loss: 0.682962  [    3/   88]
per-ex loss: 0.338581  [    4/   88]
per-ex loss: 0.481150  [    5/   88]
per-ex loss: 0.638161  [    6/   88]
per-ex loss: 0.629816  [    7/   88]
per-ex loss: 0.385329  [    8/   88]
per-ex loss: 0.326864  [    9/   88]
per-ex loss: 0.671133  [   10/   88]
per-ex loss: 0.454871  [   11/   88]
per-ex loss: 0.449400  [   12/   88]
per-ex loss: 0.480092  [   13/   88]
per-ex loss: 0.549862  [   14/   88]
per-ex loss: 0.336284  [   15/   88]
per-ex loss: 0.515860  [   16/   88]
per-ex loss: 0.402443  [   17/   88]
per-ex loss: 0.483116  [   18/   88]
per-ex loss: 0.594837  [   19/   88]
per-ex loss: 0.677366  [   20/   88]
per-ex loss: 0.401351  [   21/   88]
per-ex loss: 0.360451  [   22/   88]
per-ex loss: 0.416908  [   23/   88]
per-ex loss: 0.584798  [   24/   88]
per-ex loss: 0.520954  [   25/   88]
per-ex loss: 0.552384  [   26/   88]
per-ex loss: 0.395883  [   27/   88]
per-ex loss: 0.429748  [   28/   88]
per-ex loss: 0.608609  [   29/   88]
per-ex loss: 0.606881  [   30/   88]
per-ex loss: 0.434219  [   31/   88]
per-ex loss: 0.428171  [   32/   88]
per-ex loss: 0.412052  [   33/   88]
per-ex loss: 0.327904  [   34/   88]
per-ex loss: 0.397534  [   35/   88]
per-ex loss: 0.385322  [   36/   88]
per-ex loss: 0.412967  [   37/   88]
per-ex loss: 0.457712  [   38/   88]
per-ex loss: 0.648990  [   39/   88]
per-ex loss: 0.581804  [   40/   88]
per-ex loss: 0.528982  [   41/   88]
per-ex loss: 0.449074  [   42/   88]
per-ex loss: 0.452750  [   43/   88]
per-ex loss: 0.555094  [   44/   88]
per-ex loss: 0.523618  [   45/   88]
per-ex loss: 0.292422  [   46/   88]
per-ex loss: 0.541785  [   47/   88]
per-ex loss: 0.541918  [   48/   88]
per-ex loss: 0.449628  [   49/   88]
per-ex loss: 0.395456  [   50/   88]
per-ex loss: 0.367669  [   51/   88]
per-ex loss: 0.343475  [   52/   88]
per-ex loss: 0.397144  [   53/   88]
per-ex loss: 0.510097  [   54/   88]
per-ex loss: 0.410480  [   55/   88]
per-ex loss: 0.575270  [   56/   88]
per-ex loss: 0.420172  [   57/   88]
per-ex loss: 0.393650  [   58/   88]
per-ex loss: 0.342732  [   59/   88]
per-ex loss: 0.641476  [   60/   88]
per-ex loss: 0.382437  [   61/   88]
per-ex loss: 0.644957  [   62/   88]
per-ex loss: 0.374395  [   63/   88]
per-ex loss: 0.376513  [   64/   88]
per-ex loss: 0.568561  [   65/   88]
per-ex loss: 0.515855  [   66/   88]
per-ex loss: 0.376647  [   67/   88]
per-ex loss: 0.394833  [   68/   88]
per-ex loss: 0.315612  [   69/   88]
per-ex loss: 0.452396  [   70/   88]
per-ex loss: 0.469008  [   71/   88]
per-ex loss: 0.526991  [   72/   88]
per-ex loss: 0.413819  [   73/   88]
per-ex loss: 0.631617  [   74/   88]
per-ex loss: 0.427546  [   75/   88]
per-ex loss: 0.557807  [   76/   88]
per-ex loss: 0.405532  [   77/   88]
per-ex loss: 0.408274  [   78/   88]
per-ex loss: 0.582188  [   79/   88]
per-ex loss: 0.539331  [   80/   88]
per-ex loss: 0.342514  [   81/   88]
per-ex loss: 0.536636  [   82/   88]
per-ex loss: 0.597223  [   83/   88]
per-ex loss: 0.637708  [   84/   88]
per-ex loss: 0.582895  [   85/   88]
per-ex loss: 0.402154  [   86/   88]
per-ex loss: 0.507677  [   87/   88]
per-ex loss: 0.378964  [   88/   88]
Train Error: Avg loss: 0.47410935
validation Error: 
 Avg loss: 0.53912785 
 F1: 0.492625 
 Precision: 0.505271 
 Recall: 0.480595
 IoU: 0.326809

test Error: 
 Avg loss: 0.48569017 
 F1: 0.565565 
 Precision: 0.564993 
 Recall: 0.566138
 IoU: 0.394277

We have finished training iteration 70
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_68_.pth
per-ex loss: 0.341707  [    1/   88]
per-ex loss: 0.379539  [    2/   88]
per-ex loss: 0.376477  [    3/   88]
per-ex loss: 0.553780  [    4/   88]
per-ex loss: 0.410363  [    5/   88]
per-ex loss: 0.522508  [    6/   88]
per-ex loss: 0.482044  [    7/   88]
per-ex loss: 0.336351  [    8/   88]
per-ex loss: 0.652701  [    9/   88]
per-ex loss: 0.509035  [   10/   88]
per-ex loss: 0.541221  [   11/   88]
per-ex loss: 0.560436  [   12/   88]
per-ex loss: 0.526449  [   13/   88]
per-ex loss: 0.481330  [   14/   88]
per-ex loss: 0.628895  [   15/   88]
per-ex loss: 0.464195  [   16/   88]
per-ex loss: 0.324252  [   17/   88]
per-ex loss: 0.606290  [   18/   88]
per-ex loss: 0.365787  [   19/   88]
per-ex loss: 0.348314  [   20/   88]
per-ex loss: 0.430954  [   21/   88]
per-ex loss: 0.398002  [   22/   88]
per-ex loss: 0.410278  [   23/   88]
per-ex loss: 0.573697  [   24/   88]
per-ex loss: 0.581029  [   25/   88]
per-ex loss: 0.492526  [   26/   88]
per-ex loss: 0.485523  [   27/   88]
per-ex loss: 0.344557  [   28/   88]
per-ex loss: 0.412418  [   29/   88]
per-ex loss: 0.573384  [   30/   88]
per-ex loss: 0.386295  [   31/   88]
per-ex loss: 0.612353  [   32/   88]
per-ex loss: 0.557382  [   33/   88]
per-ex loss: 0.620487  [   34/   88]
per-ex loss: 0.364841  [   35/   88]
per-ex loss: 0.393345  [   36/   88]
per-ex loss: 0.469351  [   37/   88]
per-ex loss: 0.437365  [   38/   88]
per-ex loss: 0.459529  [   39/   88]
per-ex loss: 0.426840  [   40/   88]
per-ex loss: 0.516789  [   41/   88]
per-ex loss: 0.376976  [   42/   88]
per-ex loss: 0.538281  [   43/   88]
per-ex loss: 0.404246  [   44/   88]
per-ex loss: 0.364018  [   45/   88]
per-ex loss: 0.350484  [   46/   88]
per-ex loss: 0.530883  [   47/   88]
per-ex loss: 0.408076  [   48/   88]
per-ex loss: 0.403312  [   49/   88]
per-ex loss: 0.297676  [   50/   88]
per-ex loss: 0.343860  [   51/   88]
per-ex loss: 0.404171  [   52/   88]
per-ex loss: 0.439564  [   53/   88]
per-ex loss: 0.345902  [   54/   88]
per-ex loss: 0.368076  [   55/   88]
per-ex loss: 0.577908  [   56/   88]
per-ex loss: 0.427054  [   57/   88]
per-ex loss: 0.608863  [   58/   88]
per-ex loss: 0.426022  [   59/   88]
per-ex loss: 0.528844  [   60/   88]
per-ex loss: 0.359080  [   61/   88]
per-ex loss: 0.397443  [   62/   88]
per-ex loss: 0.631425  [   63/   88]
per-ex loss: 0.351428  [   64/   88]
per-ex loss: 0.625758  [   65/   88]
per-ex loss: 0.411651  [   66/   88]
per-ex loss: 0.588587  [   67/   88]
per-ex loss: 0.485264  [   68/   88]
per-ex loss: 0.575668  [   69/   88]
per-ex loss: 0.412897  [   70/   88]
per-ex loss: 0.463798  [   71/   88]
per-ex loss: 0.377737  [   72/   88]
per-ex loss: 0.369146  [   73/   88]
per-ex loss: 0.697873  [   74/   88]
per-ex loss: 0.528222  [   75/   88]
per-ex loss: 0.613346  [   76/   88]
per-ex loss: 0.423955  [   77/   88]
per-ex loss: 0.395427  [   78/   88]
per-ex loss: 0.321784  [   79/   88]
per-ex loss: 0.406023  [   80/   88]
per-ex loss: 0.615466  [   81/   88]
per-ex loss: 0.382794  [   82/   88]
per-ex loss: 0.557960  [   83/   88]
per-ex loss: 0.601220  [   84/   88]
per-ex loss: 0.674384  [   85/   88]
per-ex loss: 0.440069  [   86/   88]
per-ex loss: 0.674483  [   87/   88]
per-ex loss: 0.427404  [   88/   88]
Train Error: Avg loss: 0.46946736
validation Error: 
 Avg loss: 0.53232762 
 F1: 0.497039 
 Precision: 0.532238 
 Recall: 0.466207
 IoU: 0.330706

test Error: 
 Avg loss: 0.48182193 
 F1: 0.566430 
 Precision: 0.592840 
 Recall: 0.542274
 IoU: 0.395119

We have finished training iteration 71
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_69_.pth
per-ex loss: 0.560618  [    1/   88]
per-ex loss: 0.398421  [    2/   88]
per-ex loss: 0.480397  [    3/   88]
per-ex loss: 0.510300  [    4/   88]
per-ex loss: 0.441668  [    5/   88]
per-ex loss: 0.534857  [    6/   88]
per-ex loss: 0.362127  [    7/   88]
per-ex loss: 0.557746  [    8/   88]
per-ex loss: 0.523184  [    9/   88]
per-ex loss: 0.355386  [   10/   88]
per-ex loss: 0.579611  [   11/   88]
per-ex loss: 0.382099  [   12/   88]
per-ex loss: 0.662097  [   13/   88]
per-ex loss: 0.472593  [   14/   88]
per-ex loss: 0.510094  [   15/   88]
per-ex loss: 0.366410  [   16/   88]
per-ex loss: 0.379277  [   17/   88]
per-ex loss: 0.603025  [   18/   88]
per-ex loss: 0.398508  [   19/   88]
per-ex loss: 0.601254  [   20/   88]
per-ex loss: 0.411500  [   21/   88]
per-ex loss: 0.333617  [   22/   88]
per-ex loss: 0.515980  [   23/   88]
per-ex loss: 0.382762  [   24/   88]
per-ex loss: 0.408994  [   25/   88]
per-ex loss: 0.443292  [   26/   88]
per-ex loss: 0.454679  [   27/   88]
per-ex loss: 0.381533  [   28/   88]
per-ex loss: 0.355973  [   29/   88]
per-ex loss: 0.600817  [   30/   88]
per-ex loss: 0.353293  [   31/   88]
per-ex loss: 0.345078  [   32/   88]
per-ex loss: 0.389449  [   33/   88]
per-ex loss: 0.458019  [   34/   88]
per-ex loss: 0.506256  [   35/   88]
per-ex loss: 0.405948  [   36/   88]
per-ex loss: 0.372162  [   37/   88]
per-ex loss: 0.402497  [   38/   88]
per-ex loss: 0.516886  [   39/   88]
per-ex loss: 0.412826  [   40/   88]
per-ex loss: 0.568506  [   41/   88]
per-ex loss: 0.612096  [   42/   88]
per-ex loss: 0.557669  [   43/   88]
per-ex loss: 0.378257  [   44/   88]
per-ex loss: 0.550546  [   45/   88]
per-ex loss: 0.335831  [   46/   88]
per-ex loss: 0.505957  [   47/   88]
per-ex loss: 0.375818  [   48/   88]
per-ex loss: 0.417909  [   49/   88]
per-ex loss: 0.594762  [   50/   88]
per-ex loss: 0.370814  [   51/   88]
per-ex loss: 0.408157  [   52/   88]
per-ex loss: 0.311691  [   53/   88]
per-ex loss: 0.385634  [   54/   88]
per-ex loss: 0.486943  [   55/   88]
per-ex loss: 0.558712  [   56/   88]
per-ex loss: 0.352178  [   57/   88]
per-ex loss: 0.682826  [   58/   88]
per-ex loss: 0.623949  [   59/   88]
per-ex loss: 0.345551  [   60/   88]
per-ex loss: 0.538224  [   61/   88]
per-ex loss: 0.355452  [   62/   88]
per-ex loss: 0.340615  [   63/   88]
per-ex loss: 0.428566  [   64/   88]
per-ex loss: 0.521675  [   65/   88]
per-ex loss: 0.581354  [   66/   88]
per-ex loss: 0.328514  [   67/   88]
per-ex loss: 0.615460  [   68/   88]
per-ex loss: 0.586262  [   69/   88]
per-ex loss: 0.468704  [   70/   88]
per-ex loss: 0.677292  [   71/   88]
per-ex loss: 0.410906  [   72/   88]
per-ex loss: 0.419701  [   73/   88]
per-ex loss: 0.457349  [   74/   88]
per-ex loss: 0.391313  [   75/   88]
per-ex loss: 0.488482  [   76/   88]
per-ex loss: 0.641750  [   77/   88]
per-ex loss: 0.559195  [   78/   88]
per-ex loss: 0.308751  [   79/   88]
per-ex loss: 0.623754  [   80/   88]
per-ex loss: 0.387358  [   81/   88]
per-ex loss: 0.577249  [   82/   88]
per-ex loss: 0.632376  [   83/   88]
per-ex loss: 0.433777  [   84/   88]
per-ex loss: 0.688622  [   85/   88]
per-ex loss: 0.559384  [   86/   88]
per-ex loss: 0.395603  [   87/   88]
per-ex loss: 0.416770  [   88/   88]
Train Error: Avg loss: 0.47033519
validation Error: 
 Avg loss: 0.55085991 
 F1: 0.477583 
 Precision: 0.609567 
 Recall: 0.392581
 IoU: 0.313700

test Error: 
 Avg loss: 0.49991232 
 F1: 0.549274 
 Precision: 0.686958 
 Recall: 0.457566
 IoU: 0.378620

We have finished training iteration 72
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_70_.pth
per-ex loss: 0.339048  [    1/   88]
per-ex loss: 0.593479  [    2/   88]
per-ex loss: 0.363632  [    3/   88]
per-ex loss: 0.420785  [    4/   88]
per-ex loss: 0.378874  [    5/   88]
per-ex loss: 0.444988  [    6/   88]
per-ex loss: 0.407362  [    7/   88]
per-ex loss: 0.456128  [    8/   88]
per-ex loss: 0.546410  [    9/   88]
per-ex loss: 0.328792  [   10/   88]
per-ex loss: 0.479103  [   11/   88]
per-ex loss: 0.543357  [   12/   88]
per-ex loss: 0.447128  [   13/   88]
per-ex loss: 0.590362  [   14/   88]
per-ex loss: 0.387999  [   15/   88]
per-ex loss: 0.407586  [   16/   88]
per-ex loss: 0.553197  [   17/   88]
per-ex loss: 0.410120  [   18/   88]
per-ex loss: 0.520328  [   19/   88]
per-ex loss: 0.394664  [   20/   88]
per-ex loss: 0.446757  [   21/   88]
per-ex loss: 0.526008  [   22/   88]
per-ex loss: 0.344032  [   23/   88]
per-ex loss: 0.444944  [   24/   88]
per-ex loss: 0.505439  [   25/   88]
per-ex loss: 0.401263  [   26/   88]
per-ex loss: 0.679929  [   27/   88]
per-ex loss: 0.491984  [   28/   88]
per-ex loss: 0.345120  [   29/   88]
per-ex loss: 0.589252  [   30/   88]
per-ex loss: 0.559141  [   31/   88]
per-ex loss: 0.396173  [   32/   88]
per-ex loss: 0.310333  [   33/   88]
per-ex loss: 0.453109  [   34/   88]
per-ex loss: 0.488588  [   35/   88]
per-ex loss: 0.336157  [   36/   88]
per-ex loss: 0.648086  [   37/   88]
per-ex loss: 0.378788  [   38/   88]
per-ex loss: 0.481642  [   39/   88]
per-ex loss: 0.559074  [   40/   88]
per-ex loss: 0.640056  [   41/   88]
per-ex loss: 0.375845  [   42/   88]
per-ex loss: 0.540105  [   43/   88]
per-ex loss: 0.504289  [   44/   88]
per-ex loss: 0.544057  [   45/   88]
per-ex loss: 0.383038  [   46/   88]
per-ex loss: 0.476142  [   47/   88]
per-ex loss: 0.397820  [   48/   88]
per-ex loss: 0.536052  [   49/   88]
per-ex loss: 0.643521  [   50/   88]
per-ex loss: 0.558270  [   51/   88]
per-ex loss: 0.266229  [   52/   88]
per-ex loss: 0.407708  [   53/   88]
per-ex loss: 0.664349  [   54/   88]
per-ex loss: 0.573082  [   55/   88]
per-ex loss: 0.597190  [   56/   88]
per-ex loss: 0.367204  [   57/   88]
per-ex loss: 0.430651  [   58/   88]
per-ex loss: 0.395156  [   59/   88]
per-ex loss: 0.440633  [   60/   88]
per-ex loss: 0.432103  [   61/   88]
per-ex loss: 0.687379  [   62/   88]
per-ex loss: 0.557756  [   63/   88]
per-ex loss: 0.473836  [   64/   88]
per-ex loss: 0.405913  [   65/   88]
per-ex loss: 0.403792  [   66/   88]
per-ex loss: 0.411978  [   67/   88]
per-ex loss: 0.360591  [   68/   88]
per-ex loss: 0.385664  [   69/   88]
per-ex loss: 0.598357  [   70/   88]
per-ex loss: 0.622844  [   71/   88]
per-ex loss: 0.628515  [   72/   88]
per-ex loss: 0.621906  [   73/   88]
per-ex loss: 0.401220  [   74/   88]
per-ex loss: 0.355192  [   75/   88]
per-ex loss: 0.364158  [   76/   88]
per-ex loss: 0.423315  [   77/   88]
per-ex loss: 0.542837  [   78/   88]
per-ex loss: 0.656889  [   79/   88]
per-ex loss: 0.604000  [   80/   88]
per-ex loss: 0.335860  [   81/   88]
per-ex loss: 0.495404  [   82/   88]
per-ex loss: 0.384164  [   83/   88]
per-ex loss: 0.358234  [   84/   88]
per-ex loss: 0.593511  [   85/   88]
per-ex loss: 0.343727  [   86/   88]
per-ex loss: 0.369895  [   87/   88]
per-ex loss: 0.444092  [   88/   88]
Train Error: Avg loss: 0.47049647
validation Error: 
 Avg loss: 0.53230011 
 F1: 0.498549 
 Precision: 0.551575 
 Recall: 0.454825
 IoU: 0.332045

test Error: 
 Avg loss: 0.47822802 
 F1: 0.574768 
 Precision: 0.611659 
 Recall: 0.542074
 IoU: 0.403280

We have finished training iteration 73
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_71_.pth
per-ex loss: 0.673764  [    1/   88]
per-ex loss: 0.343610  [    2/   88]
per-ex loss: 0.379071  [    3/   88]
per-ex loss: 0.657604  [    4/   88]
per-ex loss: 0.530422  [    5/   88]
per-ex loss: 0.348843  [    6/   88]
per-ex loss: 0.385139  [    7/   88]
per-ex loss: 0.480120  [    8/   88]
per-ex loss: 0.606136  [    9/   88]
per-ex loss: 0.412562  [   10/   88]
per-ex loss: 0.552230  [   11/   88]
per-ex loss: 0.328867  [   12/   88]
per-ex loss: 0.470941  [   13/   88]
per-ex loss: 0.333887  [   14/   88]
per-ex loss: 0.353143  [   15/   88]
per-ex loss: 0.388007  [   16/   88]
per-ex loss: 0.666011  [   17/   88]
per-ex loss: 0.565822  [   18/   88]
per-ex loss: 0.514140  [   19/   88]
per-ex loss: 0.524982  [   20/   88]
per-ex loss: 0.617866  [   21/   88]
per-ex loss: 0.427768  [   22/   88]
per-ex loss: 0.562739  [   23/   88]
per-ex loss: 0.563674  [   24/   88]
per-ex loss: 0.619521  [   25/   88]
per-ex loss: 0.410729  [   26/   88]
per-ex loss: 0.469179  [   27/   88]
per-ex loss: 0.391080  [   28/   88]
per-ex loss: 0.478406  [   29/   88]
per-ex loss: 0.630323  [   30/   88]
per-ex loss: 0.581644  [   31/   88]
per-ex loss: 0.387888  [   32/   88]
per-ex loss: 0.581601  [   33/   88]
per-ex loss: 0.542846  [   34/   88]
per-ex loss: 0.371188  [   35/   88]
per-ex loss: 0.692503  [   36/   88]
per-ex loss: 0.362060  [   37/   88]
per-ex loss: 0.396133  [   38/   88]
per-ex loss: 0.519548  [   39/   88]
per-ex loss: 0.492045  [   40/   88]
per-ex loss: 0.443726  [   41/   88]
per-ex loss: 0.575083  [   42/   88]
per-ex loss: 0.344483  [   43/   88]
per-ex loss: 0.299711  [   44/   88]
per-ex loss: 0.457875  [   45/   88]
per-ex loss: 0.501809  [   46/   88]
per-ex loss: 0.554407  [   47/   88]
per-ex loss: 0.402184  [   48/   88]
per-ex loss: 0.611520  [   49/   88]
per-ex loss: 0.520435  [   50/   88]
per-ex loss: 0.356879  [   51/   88]
per-ex loss: 0.499807  [   52/   88]
per-ex loss: 0.305366  [   53/   88]
per-ex loss: 0.450011  [   54/   88]
per-ex loss: 0.392750  [   55/   88]
per-ex loss: 0.349168  [   56/   88]
per-ex loss: 0.455357  [   57/   88]
per-ex loss: 0.353922  [   58/   88]
per-ex loss: 0.619812  [   59/   88]
per-ex loss: 0.337041  [   60/   88]
per-ex loss: 0.437439  [   61/   88]
per-ex loss: 0.352442  [   62/   88]
per-ex loss: 0.579087  [   63/   88]
per-ex loss: 0.543995  [   64/   88]
per-ex loss: 0.427415  [   65/   88]
per-ex loss: 0.377538  [   66/   88]
per-ex loss: 0.417005  [   67/   88]
per-ex loss: 0.373168  [   68/   88]
per-ex loss: 0.344991  [   69/   88]
per-ex loss: 0.362200  [   70/   88]
per-ex loss: 0.416094  [   71/   88]
per-ex loss: 0.379243  [   72/   88]
per-ex loss: 0.453832  [   73/   88]
per-ex loss: 0.377111  [   74/   88]
per-ex loss: 0.423644  [   75/   88]
per-ex loss: 0.550147  [   76/   88]
per-ex loss: 0.393070  [   77/   88]
per-ex loss: 0.572553  [   78/   88]
per-ex loss: 0.353209  [   79/   88]
per-ex loss: 0.621161  [   80/   88]
per-ex loss: 0.409551  [   81/   88]
per-ex loss: 0.325541  [   82/   88]
per-ex loss: 0.594680  [   83/   88]
per-ex loss: 0.586496  [   84/   88]
per-ex loss: 0.469857  [   85/   88]
per-ex loss: 0.340241  [   86/   88]
per-ex loss: 0.359228  [   87/   88]
per-ex loss: 0.443718  [   88/   88]
Train Error: Avg loss: 0.46284144
validation Error: 
 Avg loss: 0.53191914 
 F1: 0.497598 
 Precision: 0.545094 
 Recall: 0.457716
 IoU: 0.331202

test Error: 
 Avg loss: 0.48243256 
 F1: 0.573115 
 Precision: 0.595644 
 Recall: 0.552229
 IoU: 0.401655

We have finished training iteration 74
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_72_.pth
per-ex loss: 0.571473  [    1/   88]
per-ex loss: 0.359683  [    2/   88]
per-ex loss: 0.382122  [    3/   88]
per-ex loss: 0.443359  [    4/   88]
per-ex loss: 0.492523  [    5/   88]
per-ex loss: 0.416024  [    6/   88]
per-ex loss: 0.396376  [    7/   88]
per-ex loss: 0.582694  [    8/   88]
per-ex loss: 0.389291  [    9/   88]
per-ex loss: 0.383496  [   10/   88]
per-ex loss: 0.676050  [   11/   88]
per-ex loss: 0.578740  [   12/   88]
per-ex loss: 0.325148  [   13/   88]
per-ex loss: 0.540874  [   14/   88]
per-ex loss: 0.599510  [   15/   88]
per-ex loss: 0.368182  [   16/   88]
per-ex loss: 0.336703  [   17/   88]
per-ex loss: 0.392263  [   18/   88]
per-ex loss: 0.517046  [   19/   88]
per-ex loss: 0.547980  [   20/   88]
per-ex loss: 0.421821  [   21/   88]
per-ex loss: 0.427990  [   22/   88]
per-ex loss: 0.656120  [   23/   88]
per-ex loss: 0.351641  [   24/   88]
per-ex loss: 0.361143  [   25/   88]
per-ex loss: 0.315621  [   26/   88]
per-ex loss: 0.401942  [   27/   88]
per-ex loss: 0.643768  [   28/   88]
per-ex loss: 0.578815  [   29/   88]
per-ex loss: 0.535852  [   30/   88]
per-ex loss: 0.538524  [   31/   88]
per-ex loss: 0.573728  [   32/   88]
per-ex loss: 0.387427  [   33/   88]
per-ex loss: 0.456455  [   34/   88]
per-ex loss: 0.541233  [   35/   88]
per-ex loss: 0.432522  [   36/   88]
per-ex loss: 0.580834  [   37/   88]
per-ex loss: 0.499318  [   38/   88]
per-ex loss: 0.492912  [   39/   88]
per-ex loss: 0.341973  [   40/   88]
per-ex loss: 0.623377  [   41/   88]
per-ex loss: 0.482144  [   42/   88]
per-ex loss: 0.432590  [   43/   88]
per-ex loss: 0.338981  [   44/   88]
per-ex loss: 0.514514  [   45/   88]
per-ex loss: 0.422826  [   46/   88]
per-ex loss: 0.521618  [   47/   88]
per-ex loss: 0.468286  [   48/   88]
per-ex loss: 0.586808  [   49/   88]
per-ex loss: 0.343485  [   50/   88]
per-ex loss: 0.318130  [   51/   88]
per-ex loss: 0.367942  [   52/   88]
per-ex loss: 0.505343  [   53/   88]
per-ex loss: 0.392963  [   54/   88]
per-ex loss: 0.466829  [   55/   88]
per-ex loss: 0.649635  [   56/   88]
per-ex loss: 0.535387  [   57/   88]
per-ex loss: 0.417880  [   58/   88]
per-ex loss: 0.394072  [   59/   88]
per-ex loss: 0.335978  [   60/   88]
per-ex loss: 0.356191  [   61/   88]
per-ex loss: 0.367994  [   62/   88]
per-ex loss: 0.345636  [   63/   88]
per-ex loss: 0.684819  [   64/   88]
per-ex loss: 0.549923  [   65/   88]
per-ex loss: 0.375767  [   66/   88]
per-ex loss: 0.405667  [   67/   88]
per-ex loss: 0.347744  [   68/   88]
per-ex loss: 0.305697  [   69/   88]
per-ex loss: 0.374307  [   70/   88]
per-ex loss: 0.440475  [   71/   88]
per-ex loss: 0.614556  [   72/   88]
per-ex loss: 0.317419  [   73/   88]
per-ex loss: 0.658138  [   74/   88]
per-ex loss: 0.378710  [   75/   88]
per-ex loss: 0.573906  [   76/   88]
per-ex loss: 0.646251  [   77/   88]
per-ex loss: 0.640242  [   78/   88]
per-ex loss: 0.396647  [   79/   88]
per-ex loss: 0.427353  [   80/   88]
per-ex loss: 0.469602  [   81/   88]
per-ex loss: 0.465654  [   82/   88]
per-ex loss: 0.374620  [   83/   88]
per-ex loss: 0.587033  [   84/   88]
per-ex loss: 0.578325  [   85/   88]
per-ex loss: 0.434998  [   86/   88]
per-ex loss: 0.625272  [   87/   88]
per-ex loss: 0.402987  [   88/   88]
Train Error: Avg loss: 0.46708973
validation Error: 
 Avg loss: 0.56830899 
 F1: 0.457649 
 Precision: 0.438022 
 Recall: 0.479117
 IoU: 0.296722

test Error: 
 Avg loss: 0.48487931 
 F1: 0.564068 
 Precision: 0.550041 
 Recall: 0.578829
 IoU: 0.392824

We have finished training iteration 75
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_73_.pth
per-ex loss: 0.650121  [    1/   88]
per-ex loss: 0.334301  [    2/   88]
per-ex loss: 0.460746  [    3/   88]
per-ex loss: 0.568823  [    4/   88]
per-ex loss: 0.571519  [    5/   88]
per-ex loss: 0.399063  [    6/   88]
per-ex loss: 0.340305  [    7/   88]
per-ex loss: 0.651063  [    8/   88]
per-ex loss: 0.403784  [    9/   88]
per-ex loss: 0.520319  [   10/   88]
per-ex loss: 0.353394  [   11/   88]
per-ex loss: 0.488558  [   12/   88]
per-ex loss: 0.417607  [   13/   88]
per-ex loss: 0.532682  [   14/   88]
per-ex loss: 0.376824  [   15/   88]
per-ex loss: 0.574450  [   16/   88]
per-ex loss: 0.590638  [   17/   88]
per-ex loss: 0.338100  [   18/   88]
per-ex loss: 0.588746  [   19/   88]
per-ex loss: 0.382113  [   20/   88]
per-ex loss: 0.603096  [   21/   88]
per-ex loss: 0.387405  [   22/   88]
per-ex loss: 0.367697  [   23/   88]
per-ex loss: 0.349001  [   24/   88]
per-ex loss: 0.431295  [   25/   88]
per-ex loss: 0.577097  [   26/   88]
per-ex loss: 0.408505  [   27/   88]
per-ex loss: 0.491349  [   28/   88]
per-ex loss: 0.522931  [   29/   88]
per-ex loss: 0.392798  [   30/   88]
per-ex loss: 0.352665  [   31/   88]
per-ex loss: 0.545160  [   32/   88]
per-ex loss: 0.569007  [   33/   88]
per-ex loss: 0.477373  [   34/   88]
per-ex loss: 0.436979  [   35/   88]
per-ex loss: 0.479961  [   36/   88]
per-ex loss: 0.391015  [   37/   88]
per-ex loss: 0.493025  [   38/   88]
per-ex loss: 0.615315  [   39/   88]
per-ex loss: 0.405129  [   40/   88]
per-ex loss: 0.502735  [   41/   88]
per-ex loss: 0.525511  [   42/   88]
per-ex loss: 0.366165  [   43/   88]
per-ex loss: 0.462300  [   44/   88]
per-ex loss: 0.351675  [   45/   88]
per-ex loss: 0.382508  [   46/   88]
per-ex loss: 0.645570  [   47/   88]
per-ex loss: 0.439153  [   48/   88]
per-ex loss: 0.406136  [   49/   88]
per-ex loss: 0.376064  [   50/   88]
per-ex loss: 0.276730  [   51/   88]
per-ex loss: 0.517552  [   52/   88]
per-ex loss: 0.682982  [   53/   88]
per-ex loss: 0.352444  [   54/   88]
per-ex loss: 0.466101  [   55/   88]
per-ex loss: 0.375742  [   56/   88]
per-ex loss: 0.381562  [   57/   88]
per-ex loss: 0.558217  [   58/   88]
per-ex loss: 0.612916  [   59/   88]
per-ex loss: 0.420890  [   60/   88]
per-ex loss: 0.384515  [   61/   88]
per-ex loss: 0.388035  [   62/   88]
per-ex loss: 0.402439  [   63/   88]
per-ex loss: 0.622089  [   64/   88]
per-ex loss: 0.411003  [   65/   88]
per-ex loss: 0.596890  [   66/   88]
per-ex loss: 0.392939  [   67/   88]
per-ex loss: 0.640868  [   68/   88]
per-ex loss: 0.552043  [   69/   88]
per-ex loss: 0.318319  [   70/   88]
per-ex loss: 0.350973  [   71/   88]
per-ex loss: 0.360490  [   72/   88]
per-ex loss: 0.610697  [   73/   88]
per-ex loss: 0.327680  [   74/   88]
per-ex loss: 0.521920  [   75/   88]
per-ex loss: 0.532914  [   76/   88]
per-ex loss: 0.311780  [   77/   88]
per-ex loss: 0.390684  [   78/   88]
per-ex loss: 0.390054  [   79/   88]
per-ex loss: 0.462484  [   80/   88]
per-ex loss: 0.527548  [   81/   88]
per-ex loss: 0.583261  [   82/   88]
per-ex loss: 0.545658  [   83/   88]
per-ex loss: 0.394814  [   84/   88]
per-ex loss: 0.449631  [   85/   88]
per-ex loss: 0.610799  [   86/   88]
per-ex loss: 0.411665  [   87/   88]
per-ex loss: 0.674732  [   88/   88]
Train Error: Avg loss: 0.46715725
validation Error: 
 Avg loss: 0.56274011 
 F1: 0.457938 
 Precision: 0.463062 
 Recall: 0.452926
 IoU: 0.296965

test Error: 
 Avg loss: 0.49570580 
 F1: 0.552026 
 Precision: 0.555056 
 Recall: 0.549028
 IoU: 0.381240

We have finished training iteration 76
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_74_.pth
per-ex loss: 0.496153  [    1/   88]
per-ex loss: 0.486133  [    2/   88]
per-ex loss: 0.551196  [    3/   88]
per-ex loss: 0.563139  [    4/   88]
per-ex loss: 0.432992  [    5/   88]
per-ex loss: 0.434453  [    6/   88]
per-ex loss: 0.444220  [    7/   88]
per-ex loss: 0.505213  [    8/   88]
per-ex loss: 0.338545  [    9/   88]
per-ex loss: 0.536826  [   10/   88]
per-ex loss: 0.595417  [   11/   88]
per-ex loss: 0.337275  [   12/   88]
per-ex loss: 0.517295  [   13/   88]
per-ex loss: 0.520995  [   14/   88]
per-ex loss: 0.388594  [   15/   88]
per-ex loss: 0.487068  [   16/   88]
per-ex loss: 0.517755  [   17/   88]
per-ex loss: 0.627389  [   18/   88]
per-ex loss: 0.359741  [   19/   88]
per-ex loss: 0.356776  [   20/   88]
per-ex loss: 0.441043  [   21/   88]
per-ex loss: 0.449778  [   22/   88]
per-ex loss: 0.320716  [   23/   88]
per-ex loss: 0.410528  [   24/   88]
per-ex loss: 0.399115  [   25/   88]
per-ex loss: 0.574169  [   26/   88]
per-ex loss: 0.581750  [   27/   88]
per-ex loss: 0.444073  [   28/   88]
per-ex loss: 0.319333  [   29/   88]
per-ex loss: 0.406881  [   30/   88]
per-ex loss: 0.423444  [   31/   88]
per-ex loss: 0.537745  [   32/   88]
per-ex loss: 0.363682  [   33/   88]
per-ex loss: 0.661739  [   34/   88]
per-ex loss: 0.396282  [   35/   88]
per-ex loss: 0.537897  [   36/   88]
per-ex loss: 0.482538  [   37/   88]
per-ex loss: 0.618388  [   38/   88]
per-ex loss: 0.586998  [   39/   88]
per-ex loss: 0.643493  [   40/   88]
per-ex loss: 0.546549  [   41/   88]
per-ex loss: 0.546172  [   42/   88]
per-ex loss: 0.390984  [   43/   88]
per-ex loss: 0.372980  [   44/   88]
per-ex loss: 0.612121  [   45/   88]
per-ex loss: 0.670068  [   46/   88]
per-ex loss: 0.587650  [   47/   88]
per-ex loss: 0.449575  [   48/   88]
per-ex loss: 0.392128  [   49/   88]
per-ex loss: 0.546679  [   50/   88]
per-ex loss: 0.466945  [   51/   88]
per-ex loss: 0.409526  [   52/   88]
per-ex loss: 0.450594  [   53/   88]
per-ex loss: 0.638946  [   54/   88]
per-ex loss: 0.594185  [   55/   88]
per-ex loss: 0.338267  [   56/   88]
per-ex loss: 0.542227  [   57/   88]
per-ex loss: 0.454794  [   58/   88]
per-ex loss: 0.340111  [   59/   88]
per-ex loss: 0.388561  [   60/   88]
per-ex loss: 0.683059  [   61/   88]
per-ex loss: 0.571054  [   62/   88]
per-ex loss: 0.583422  [   63/   88]
per-ex loss: 0.342771  [   64/   88]
per-ex loss: 0.305548  [   65/   88]
per-ex loss: 0.380604  [   66/   88]
per-ex loss: 0.353590  [   67/   88]
per-ex loss: 0.412544  [   68/   88]
per-ex loss: 0.399890  [   69/   88]
per-ex loss: 0.347938  [   70/   88]
per-ex loss: 0.339529  [   71/   88]
per-ex loss: 0.366194  [   72/   88]
per-ex loss: 0.416469  [   73/   88]
per-ex loss: 0.552332  [   74/   88]
per-ex loss: 0.423303  [   75/   88]
per-ex loss: 0.552189  [   76/   88]
per-ex loss: 0.474100  [   77/   88]
per-ex loss: 0.321594  [   78/   88]
per-ex loss: 0.516520  [   79/   88]
per-ex loss: 0.378533  [   80/   88]
per-ex loss: 0.525671  [   81/   88]
per-ex loss: 0.394684  [   82/   88]
per-ex loss: 0.611553  [   83/   88]
per-ex loss: 0.584070  [   84/   88]
per-ex loss: 0.404095  [   85/   88]
per-ex loss: 0.401679  [   86/   88]
per-ex loss: 0.389679  [   87/   88]
per-ex loss: 0.375006  [   88/   88]
Train Error: Avg loss: 0.46867558
validation Error: 
 Avg loss: 0.53339182 
 F1: 0.498458 
 Precision: 0.593693 
 Recall: 0.429553
 IoU: 0.331964

test Error: 
 Avg loss: 0.48236463 
 F1: 0.571600 
 Precision: 0.660588 
 Recall: 0.503740
 IoU: 0.400168

We have finished training iteration 77
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_75_.pth
per-ex loss: 0.432085  [    1/   88]
per-ex loss: 0.484675  [    2/   88]
per-ex loss: 0.540966  [    3/   88]
per-ex loss: 0.427978  [    4/   88]
per-ex loss: 0.375971  [    5/   88]
per-ex loss: 0.386979  [    6/   88]
per-ex loss: 0.659725  [    7/   88]
per-ex loss: 0.588330  [    8/   88]
per-ex loss: 0.327535  [    9/   88]
per-ex loss: 0.341289  [   10/   88]
per-ex loss: 0.563759  [   11/   88]
per-ex loss: 0.369334  [   12/   88]
per-ex loss: 0.352803  [   13/   88]
per-ex loss: 0.400481  [   14/   88]
per-ex loss: 0.496905  [   15/   88]
per-ex loss: 0.386178  [   16/   88]
per-ex loss: 0.575860  [   17/   88]
per-ex loss: 0.373181  [   18/   88]
per-ex loss: 0.455492  [   19/   88]
per-ex loss: 0.411023  [   20/   88]
per-ex loss: 0.487248  [   21/   88]
per-ex loss: 0.571580  [   22/   88]
per-ex loss: 0.445161  [   23/   88]
per-ex loss: 0.575255  [   24/   88]
per-ex loss: 0.675164  [   25/   88]
per-ex loss: 0.356648  [   26/   88]
per-ex loss: 0.590596  [   27/   88]
per-ex loss: 0.441212  [   28/   88]
per-ex loss: 0.506591  [   29/   88]
per-ex loss: 0.361814  [   30/   88]
per-ex loss: 0.609868  [   31/   88]
per-ex loss: 0.475988  [   32/   88]
per-ex loss: 0.393195  [   33/   88]
per-ex loss: 0.360458  [   34/   88]
per-ex loss: 0.470672  [   35/   88]
per-ex loss: 0.478140  [   36/   88]
per-ex loss: 0.617470  [   37/   88]
per-ex loss: 0.364492  [   38/   88]
per-ex loss: 0.402560  [   39/   88]
per-ex loss: 0.336760  [   40/   88]
per-ex loss: 0.510480  [   41/   88]
per-ex loss: 0.330694  [   42/   88]
per-ex loss: 0.364828  [   43/   88]
per-ex loss: 0.526635  [   44/   88]
per-ex loss: 0.311901  [   45/   88]
per-ex loss: 0.414126  [   46/   88]
per-ex loss: 0.387354  [   47/   88]
per-ex loss: 0.526676  [   48/   88]
per-ex loss: 0.396157  [   49/   88]
per-ex loss: 0.573314  [   50/   88]
per-ex loss: 0.426912  [   51/   88]
per-ex loss: 0.607831  [   52/   88]
per-ex loss: 0.408051  [   53/   88]
per-ex loss: 0.399823  [   54/   88]
per-ex loss: 0.406471  [   55/   88]
per-ex loss: 0.422788  [   56/   88]
per-ex loss: 0.450745  [   57/   88]
per-ex loss: 0.297090  [   58/   88]
per-ex loss: 0.597728  [   59/   88]
per-ex loss: 0.530550  [   60/   88]
per-ex loss: 0.493264  [   61/   88]
per-ex loss: 0.550468  [   62/   88]
per-ex loss: 0.365146  [   63/   88]
per-ex loss: 0.551095  [   64/   88]
per-ex loss: 0.610508  [   65/   88]
per-ex loss: 0.370184  [   66/   88]
per-ex loss: 0.533422  [   67/   88]
per-ex loss: 0.556744  [   68/   88]
per-ex loss: 0.423698  [   69/   88]
per-ex loss: 0.342274  [   70/   88]
per-ex loss: 0.567796  [   71/   88]
per-ex loss: 0.431951  [   72/   88]
per-ex loss: 0.522161  [   73/   88]
per-ex loss: 0.539816  [   74/   88]
per-ex loss: 0.384753  [   75/   88]
per-ex loss: 0.424440  [   76/   88]
per-ex loss: 0.579054  [   77/   88]
per-ex loss: 0.663342  [   78/   88]
per-ex loss: 0.395859  [   79/   88]
per-ex loss: 0.693455  [   80/   88]
per-ex loss: 0.363622  [   81/   88]
per-ex loss: 0.339764  [   82/   88]
per-ex loss: 0.626656  [   83/   88]
per-ex loss: 0.602209  [   84/   88]
per-ex loss: 0.599793  [   85/   88]
per-ex loss: 0.347555  [   86/   88]
per-ex loss: 0.347812  [   87/   88]
per-ex loss: 0.431990  [   88/   88]
Train Error: Avg loss: 0.46614096
validation Error: 
 Avg loss: 0.54000681 
 F1: 0.486675 
 Precision: 0.605287 
 Recall: 0.406933
 IoU: 0.321593

test Error: 
 Avg loss: 0.49590900 
 F1: 0.558439 
 Precision: 0.642551 
 Recall: 0.493799
 IoU: 0.387385

We have finished training iteration 78
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_76_.pth
per-ex loss: 0.614780  [    1/   88]
per-ex loss: 0.383057  [    2/   88]
per-ex loss: 0.560870  [    3/   88]
per-ex loss: 0.600692  [    4/   88]
per-ex loss: 0.543151  [    5/   88]
per-ex loss: 0.341790  [    6/   88]
per-ex loss: 0.308674  [    7/   88]
per-ex loss: 0.364525  [    8/   88]
per-ex loss: 0.414440  [    9/   88]
per-ex loss: 0.355608  [   10/   88]
per-ex loss: 0.378464  [   11/   88]
per-ex loss: 0.600797  [   12/   88]
per-ex loss: 0.450607  [   13/   88]
per-ex loss: 0.497676  [   14/   88]
per-ex loss: 0.642330  [   15/   88]
per-ex loss: 0.306323  [   16/   88]
per-ex loss: 0.646692  [   17/   88]
per-ex loss: 0.343884  [   18/   88]
per-ex loss: 0.479612  [   19/   88]
per-ex loss: 0.466263  [   20/   88]
per-ex loss: 0.421603  [   21/   88]
per-ex loss: 0.342357  [   22/   88]
per-ex loss: 0.451089  [   23/   88]
per-ex loss: 0.405033  [   24/   88]
per-ex loss: 0.337351  [   25/   88]
per-ex loss: 0.370341  [   26/   88]
per-ex loss: 0.432049  [   27/   88]
per-ex loss: 0.324582  [   28/   88]
per-ex loss: 0.525921  [   29/   88]
per-ex loss: 0.348696  [   30/   88]
per-ex loss: 0.389445  [   31/   88]
per-ex loss: 0.542632  [   32/   88]
per-ex loss: 0.522297  [   33/   88]
per-ex loss: 0.406144  [   34/   88]
per-ex loss: 0.397847  [   35/   88]
per-ex loss: 0.545543  [   36/   88]
per-ex loss: 0.500852  [   37/   88]
per-ex loss: 0.460740  [   38/   88]
per-ex loss: 0.616140  [   39/   88]
per-ex loss: 0.357842  [   40/   88]
per-ex loss: 0.590906  [   41/   88]
per-ex loss: 0.576501  [   42/   88]
per-ex loss: 0.585286  [   43/   88]
per-ex loss: 0.506878  [   44/   88]
per-ex loss: 0.479253  [   45/   88]
per-ex loss: 0.601495  [   46/   88]
per-ex loss: 0.431665  [   47/   88]
per-ex loss: 0.671101  [   48/   88]
per-ex loss: 0.335752  [   49/   88]
per-ex loss: 0.407175  [   50/   88]
per-ex loss: 0.390649  [   51/   88]
per-ex loss: 0.368221  [   52/   88]
per-ex loss: 0.404172  [   53/   88]
per-ex loss: 0.387573  [   54/   88]
per-ex loss: 0.352064  [   55/   88]
per-ex loss: 0.446902  [   56/   88]
per-ex loss: 0.482194  [   57/   88]
per-ex loss: 0.655928  [   58/   88]
per-ex loss: 0.554886  [   59/   88]
per-ex loss: 0.549918  [   60/   88]
per-ex loss: 0.334782  [   61/   88]
per-ex loss: 0.267191  [   62/   88]
per-ex loss: 0.424429  [   63/   88]
per-ex loss: 0.387241  [   64/   88]
per-ex loss: 0.535937  [   65/   88]
per-ex loss: 0.526575  [   66/   88]
per-ex loss: 0.422302  [   67/   88]
per-ex loss: 0.406162  [   68/   88]
per-ex loss: 0.645351  [   69/   88]
per-ex loss: 0.401306  [   70/   88]
per-ex loss: 0.672340  [   71/   88]
per-ex loss: 0.399998  [   72/   88]
per-ex loss: 0.600152  [   73/   88]
per-ex loss: 0.405009  [   74/   88]
per-ex loss: 0.395337  [   75/   88]
per-ex loss: 0.643181  [   76/   88]
per-ex loss: 0.360025  [   77/   88]
per-ex loss: 0.373370  [   78/   88]
per-ex loss: 0.597085  [   79/   88]
per-ex loss: 0.447759  [   80/   88]
per-ex loss: 0.548672  [   81/   88]
per-ex loss: 0.389782  [   82/   88]
per-ex loss: 0.639726  [   83/   88]
per-ex loss: 0.517741  [   84/   88]
per-ex loss: 0.671994  [   85/   88]
per-ex loss: 0.597139  [   86/   88]
per-ex loss: 0.379853  [   87/   88]
per-ex loss: 0.580849  [   88/   88]
Train Error: Avg loss: 0.46986985
validation Error: 
 Avg loss: 0.55742098 
 F1: 0.461480 
 Precision: 0.445879 
 Recall: 0.478213
 IoU: 0.299951

test Error: 
 Avg loss: 0.49333168 
 F1: 0.552764 
 Precision: 0.531341 
 Recall: 0.575988
 IoU: 0.381945

We have finished training iteration 79
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_77_.pth
per-ex loss: 0.526956  [    1/   88]
per-ex loss: 0.417075  [    2/   88]
per-ex loss: 0.373760  [    3/   88]
per-ex loss: 0.383106  [    4/   88]
per-ex loss: 0.434221  [    5/   88]
per-ex loss: 0.504877  [    6/   88]
per-ex loss: 0.433410  [    7/   88]
per-ex loss: 0.336610  [    8/   88]
per-ex loss: 0.561663  [    9/   88]
per-ex loss: 0.681760  [   10/   88]
per-ex loss: 0.683356  [   11/   88]
per-ex loss: 0.376908  [   12/   88]
per-ex loss: 0.478859  [   13/   88]
per-ex loss: 0.437738  [   14/   88]
per-ex loss: 0.375421  [   15/   88]
per-ex loss: 0.491769  [   16/   88]
per-ex loss: 0.563905  [   17/   88]
per-ex loss: 0.593513  [   18/   88]
per-ex loss: 0.573959  [   19/   88]
per-ex loss: 0.628703  [   20/   88]
per-ex loss: 0.389484  [   21/   88]
per-ex loss: 0.639635  [   22/   88]
per-ex loss: 0.388845  [   23/   88]
per-ex loss: 0.434628  [   24/   88]
per-ex loss: 0.628851  [   25/   88]
per-ex loss: 0.533828  [   26/   88]
per-ex loss: 0.325805  [   27/   88]
per-ex loss: 0.345500  [   28/   88]
per-ex loss: 0.340112  [   29/   88]
per-ex loss: 0.541811  [   30/   88]
per-ex loss: 0.401132  [   31/   88]
per-ex loss: 0.382180  [   32/   88]
per-ex loss: 0.351226  [   33/   88]
per-ex loss: 0.466296  [   34/   88]
per-ex loss: 0.612282  [   35/   88]
per-ex loss: 0.495836  [   36/   88]
per-ex loss: 0.342548  [   37/   88]
per-ex loss: 0.342158  [   38/   88]
per-ex loss: 0.614639  [   39/   88]
per-ex loss: 0.357959  [   40/   88]
per-ex loss: 0.389114  [   41/   88]
per-ex loss: 0.467233  [   42/   88]
per-ex loss: 0.556349  [   43/   88]
per-ex loss: 0.429654  [   44/   88]
per-ex loss: 0.575189  [   45/   88]
per-ex loss: 0.355401  [   46/   88]
per-ex loss: 0.546412  [   47/   88]
per-ex loss: 0.537877  [   48/   88]
per-ex loss: 0.339064  [   49/   88]
per-ex loss: 0.349665  [   50/   88]
per-ex loss: 0.584755  [   51/   88]
per-ex loss: 0.394312  [   52/   88]
per-ex loss: 0.639947  [   53/   88]
per-ex loss: 0.336633  [   54/   88]
per-ex loss: 0.510963  [   55/   88]
per-ex loss: 0.585029  [   56/   88]
per-ex loss: 0.536298  [   57/   88]
per-ex loss: 0.245709  [   58/   88]
per-ex loss: 0.397223  [   59/   88]
per-ex loss: 0.432235  [   60/   88]
per-ex loss: 0.378877  [   61/   88]
per-ex loss: 0.573196  [   62/   88]
per-ex loss: 0.385801  [   63/   88]
per-ex loss: 0.444232  [   64/   88]
per-ex loss: 0.641277  [   65/   88]
per-ex loss: 0.557442  [   66/   88]
per-ex loss: 0.369561  [   67/   88]
per-ex loss: 0.579743  [   68/   88]
per-ex loss: 0.338247  [   69/   88]
per-ex loss: 0.463563  [   70/   88]
per-ex loss: 0.478879  [   71/   88]
per-ex loss: 0.310091  [   72/   88]
per-ex loss: 0.386131  [   73/   88]
per-ex loss: 0.488649  [   74/   88]
per-ex loss: 0.341583  [   75/   88]
per-ex loss: 0.607816  [   76/   88]
per-ex loss: 0.468186  [   77/   88]
per-ex loss: 0.509237  [   78/   88]
per-ex loss: 0.442403  [   79/   88]
per-ex loss: 0.377054  [   80/   88]
per-ex loss: 0.392641  [   81/   88]
per-ex loss: 0.553180  [   82/   88]
per-ex loss: 0.458341  [   83/   88]
per-ex loss: 0.364347  [   84/   88]
per-ex loss: 0.372390  [   85/   88]
per-ex loss: 0.562796  [   86/   88]
per-ex loss: 0.404563  [   87/   88]
per-ex loss: 0.345148  [   88/   88]
Train Error: Avg loss: 0.46132679
validation Error: 
 Avg loss: 0.55960638 
 F1: 0.464698 
 Precision: 0.557333 
 Recall: 0.398468
 IoU: 0.302675

test Error: 
 Avg loss: 0.50202518 
 F1: 0.547435 
 Precision: 0.646120 
 Recall: 0.474900
 IoU: 0.376874

We have finished training iteration 80
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_78_.pth
per-ex loss: 0.677276  [    1/   88]
per-ex loss: 0.541879  [    2/   88]
per-ex loss: 0.369281  [    3/   88]
per-ex loss: 0.399125  [    4/   88]
per-ex loss: 0.385613  [    5/   88]
per-ex loss: 0.383005  [    6/   88]
per-ex loss: 0.363964  [    7/   88]
per-ex loss: 0.542260  [    8/   88]
per-ex loss: 0.358455  [    9/   88]
per-ex loss: 0.456559  [   10/   88]
per-ex loss: 0.539069  [   11/   88]
per-ex loss: 0.499729  [   12/   88]
per-ex loss: 0.493362  [   13/   88]
per-ex loss: 0.629071  [   14/   88]
per-ex loss: 0.413476  [   15/   88]
per-ex loss: 0.422882  [   16/   88]
per-ex loss: 0.381719  [   17/   88]
per-ex loss: 0.431124  [   18/   88]
per-ex loss: 0.436437  [   19/   88]
per-ex loss: 0.506016  [   20/   88]
per-ex loss: 0.559155  [   21/   88]
per-ex loss: 0.673481  [   22/   88]
per-ex loss: 0.542159  [   23/   88]
per-ex loss: 0.361000  [   24/   88]
per-ex loss: 0.349040  [   25/   88]
per-ex loss: 0.360967  [   26/   88]
per-ex loss: 0.574626  [   27/   88]
per-ex loss: 0.482751  [   28/   88]
per-ex loss: 0.568108  [   29/   88]
per-ex loss: 0.383059  [   30/   88]
per-ex loss: 0.407101  [   31/   88]
per-ex loss: 0.373544  [   32/   88]
per-ex loss: 0.491282  [   33/   88]
per-ex loss: 0.645209  [   34/   88]
per-ex loss: 0.658645  [   35/   88]
per-ex loss: 0.479933  [   36/   88]
per-ex loss: 0.367197  [   37/   88]
per-ex loss: 0.553790  [   38/   88]
per-ex loss: 0.525174  [   39/   88]
per-ex loss: 0.379081  [   40/   88]
per-ex loss: 0.363579  [   41/   88]
per-ex loss: 0.377675  [   42/   88]
per-ex loss: 0.407335  [   43/   88]
per-ex loss: 0.457836  [   44/   88]
per-ex loss: 0.390965  [   45/   88]
per-ex loss: 0.290211  [   46/   88]
per-ex loss: 0.398728  [   47/   88]
per-ex loss: 0.579456  [   48/   88]
per-ex loss: 0.618580  [   49/   88]
per-ex loss: 0.421466  [   50/   88]
per-ex loss: 0.385032  [   51/   88]
per-ex loss: 0.454928  [   52/   88]
per-ex loss: 0.614647  [   53/   88]
per-ex loss: 0.568038  [   54/   88]
per-ex loss: 0.338263  [   55/   88]
per-ex loss: 0.438673  [   56/   88]
per-ex loss: 0.477457  [   57/   88]
per-ex loss: 0.378057  [   58/   88]
per-ex loss: 0.429956  [   59/   88]
per-ex loss: 0.348311  [   60/   88]
per-ex loss: 0.479364  [   61/   88]
per-ex loss: 0.388776  [   62/   88]
per-ex loss: 0.564355  [   63/   88]
per-ex loss: 0.572801  [   64/   88]
per-ex loss: 0.340458  [   65/   88]
per-ex loss: 0.511029  [   66/   88]
per-ex loss: 0.665110  [   67/   88]
per-ex loss: 0.714926  [   68/   88]
per-ex loss: 0.341398  [   69/   88]
per-ex loss: 0.603165  [   70/   88]
per-ex loss: 0.563965  [   71/   88]
per-ex loss: 0.402458  [   72/   88]
per-ex loss: 0.340988  [   73/   88]
per-ex loss: 0.438115  [   74/   88]
per-ex loss: 0.604120  [   75/   88]
per-ex loss: 0.638495  [   76/   88]
per-ex loss: 0.331581  [   77/   88]
per-ex loss: 0.433586  [   78/   88]
per-ex loss: 0.676535  [   79/   88]
per-ex loss: 0.616385  [   80/   88]
per-ex loss: 0.490895  [   81/   88]
per-ex loss: 0.395166  [   82/   88]
per-ex loss: 0.338250  [   83/   88]
per-ex loss: 0.359321  [   84/   88]
per-ex loss: 0.413233  [   85/   88]
per-ex loss: 0.328068  [   86/   88]
per-ex loss: 0.552385  [   87/   88]
per-ex loss: 0.692124  [   88/   88]
Train Error: Avg loss: 0.47161198
validation Error: 
 Avg loss: 0.53935681 
 F1: 0.487366 
 Precision: 0.589172 
 Recall: 0.415559
 IoU: 0.322197

test Error: 
 Avg loss: 0.48998912 
 F1: 0.558398 
 Precision: 0.633361 
 Recall: 0.499302
 IoU: 0.387346

We have finished training iteration 81
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_79_.pth
per-ex loss: 0.355534  [    1/   88]
per-ex loss: 0.334761  [    2/   88]
per-ex loss: 0.635088  [    3/   88]
per-ex loss: 0.530709  [    4/   88]
per-ex loss: 0.580929  [    5/   88]
per-ex loss: 0.651428  [    6/   88]
per-ex loss: 0.378579  [    7/   88]
per-ex loss: 0.339425  [    8/   88]
per-ex loss: 0.342924  [    9/   88]
per-ex loss: 0.664771  [   10/   88]
per-ex loss: 0.449635  [   11/   88]
per-ex loss: 0.473899  [   12/   88]
per-ex loss: 0.433187  [   13/   88]
per-ex loss: 0.580024  [   14/   88]
per-ex loss: 0.511073  [   15/   88]
per-ex loss: 0.397299  [   16/   88]
per-ex loss: 0.432666  [   17/   88]
per-ex loss: 0.636652  [   18/   88]
per-ex loss: 0.638342  [   19/   88]
per-ex loss: 0.352338  [   20/   88]
per-ex loss: 0.512965  [   21/   88]
per-ex loss: 0.419897  [   22/   88]
per-ex loss: 0.537872  [   23/   88]
per-ex loss: 0.544852  [   24/   88]
per-ex loss: 0.584440  [   25/   88]
per-ex loss: 0.429203  [   26/   88]
per-ex loss: 0.575449  [   27/   88]
per-ex loss: 0.482051  [   28/   88]
per-ex loss: 0.536766  [   29/   88]
per-ex loss: 0.501015  [   30/   88]
per-ex loss: 0.338121  [   31/   88]
per-ex loss: 0.528128  [   32/   88]
per-ex loss: 0.609038  [   33/   88]
per-ex loss: 0.378707  [   34/   88]
per-ex loss: 0.530365  [   35/   88]
per-ex loss: 0.389191  [   36/   88]
per-ex loss: 0.362997  [   37/   88]
per-ex loss: 0.341589  [   38/   88]
per-ex loss: 0.379284  [   39/   88]
per-ex loss: 0.257126  [   40/   88]
per-ex loss: 0.418261  [   41/   88]
per-ex loss: 0.382243  [   42/   88]
per-ex loss: 0.433446  [   43/   88]
per-ex loss: 0.548377  [   44/   88]
per-ex loss: 0.396304  [   45/   88]
per-ex loss: 0.436850  [   46/   88]
per-ex loss: 0.556388  [   47/   88]
per-ex loss: 0.444507  [   48/   88]
per-ex loss: 0.360827  [   49/   88]
per-ex loss: 0.460171  [   50/   88]
per-ex loss: 0.519720  [   51/   88]
per-ex loss: 0.374577  [   52/   88]
per-ex loss: 0.379034  [   53/   88]
per-ex loss: 0.495234  [   54/   88]
per-ex loss: 0.558745  [   55/   88]
per-ex loss: 0.389234  [   56/   88]
per-ex loss: 0.345700  [   57/   88]
per-ex loss: 0.410761  [   58/   88]
per-ex loss: 0.361177  [   59/   88]
per-ex loss: 0.341323  [   60/   88]
per-ex loss: 0.352898  [   61/   88]
per-ex loss: 0.473925  [   62/   88]
per-ex loss: 0.674297  [   63/   88]
per-ex loss: 0.401432  [   64/   88]
per-ex loss: 0.431931  [   65/   88]
per-ex loss: 0.408770  [   66/   88]
per-ex loss: 0.461807  [   67/   88]
per-ex loss: 0.412512  [   68/   88]
per-ex loss: 0.357629  [   69/   88]
per-ex loss: 0.570393  [   70/   88]
per-ex loss: 0.612836  [   71/   88]
per-ex loss: 0.577665  [   72/   88]
per-ex loss: 0.602203  [   73/   88]
per-ex loss: 0.535134  [   74/   88]
per-ex loss: 0.334688  [   75/   88]
per-ex loss: 0.529591  [   76/   88]
per-ex loss: 0.568738  [   77/   88]
per-ex loss: 0.495009  [   78/   88]
per-ex loss: 0.340241  [   79/   88]
per-ex loss: 0.342693  [   80/   88]
per-ex loss: 0.395269  [   81/   88]
per-ex loss: 0.618517  [   82/   88]
per-ex loss: 0.583209  [   83/   88]
per-ex loss: 0.477476  [   84/   88]
per-ex loss: 0.373483  [   85/   88]
per-ex loss: 0.389505  [   86/   88]
per-ex loss: 0.434333  [   87/   88]
per-ex loss: 0.435655  [   88/   88]
Train Error: Avg loss: 0.46321628
validation Error: 
 Avg loss: 0.52633348 
 F1: 0.499372 
 Precision: 0.572671 
 Recall: 0.442708
 IoU: 0.332776

test Error: 
 Avg loss: 0.48418504 
 F1: 0.567223 
 Precision: 0.606178 
 Recall: 0.532971
 IoU: 0.395890

We have finished training iteration 82
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_80_.pth
per-ex loss: 0.397779  [    1/   88]
per-ex loss: 0.504369  [    2/   88]
per-ex loss: 0.557599  [    3/   88]
per-ex loss: 0.415489  [    4/   88]
per-ex loss: 0.587328  [    5/   88]
per-ex loss: 0.608584  [    6/   88]
per-ex loss: 0.509461  [    7/   88]
per-ex loss: 0.423957  [    8/   88]
per-ex loss: 0.343010  [    9/   88]
per-ex loss: 0.375770  [   10/   88]
per-ex loss: 0.466475  [   11/   88]
per-ex loss: 0.375685  [   12/   88]
per-ex loss: 0.341922  [   13/   88]
per-ex loss: 0.577029  [   14/   88]
per-ex loss: 0.367860  [   15/   88]
per-ex loss: 0.342602  [   16/   88]
per-ex loss: 0.632812  [   17/   88]
per-ex loss: 0.556693  [   18/   88]
per-ex loss: 0.463176  [   19/   88]
per-ex loss: 0.334342  [   20/   88]
per-ex loss: 0.550686  [   21/   88]
per-ex loss: 0.613188  [   22/   88]
per-ex loss: 0.579907  [   23/   88]
per-ex loss: 0.440907  [   24/   88]
per-ex loss: 0.384639  [   25/   88]
per-ex loss: 0.335761  [   26/   88]
per-ex loss: 0.478212  [   27/   88]
per-ex loss: 0.317402  [   28/   88]
per-ex loss: 0.565731  [   29/   88]
per-ex loss: 0.472908  [   30/   88]
per-ex loss: 0.668753  [   31/   88]
per-ex loss: 0.513239  [   32/   88]
per-ex loss: 0.349781  [   33/   88]
per-ex loss: 0.614611  [   34/   88]
per-ex loss: 0.511356  [   35/   88]
per-ex loss: 0.536948  [   36/   88]
per-ex loss: 0.399202  [   37/   88]
per-ex loss: 0.411855  [   38/   88]
per-ex loss: 0.353050  [   39/   88]
per-ex loss: 0.329867  [   40/   88]
per-ex loss: 0.517873  [   41/   88]
per-ex loss: 0.434655  [   42/   88]
per-ex loss: 0.370985  [   43/   88]
per-ex loss: 0.388799  [   44/   88]
per-ex loss: 0.423855  [   45/   88]
per-ex loss: 0.560823  [   46/   88]
per-ex loss: 0.349775  [   47/   88]
per-ex loss: 0.306757  [   48/   88]
per-ex loss: 0.604215  [   49/   88]
per-ex loss: 0.627752  [   50/   88]
per-ex loss: 0.495982  [   51/   88]
per-ex loss: 0.588798  [   52/   88]
per-ex loss: 0.545798  [   53/   88]
per-ex loss: 0.583471  [   54/   88]
per-ex loss: 0.355054  [   55/   88]
per-ex loss: 0.389003  [   56/   88]
per-ex loss: 0.615453  [   57/   88]
per-ex loss: 0.449827  [   58/   88]
per-ex loss: 0.406594  [   59/   88]
per-ex loss: 0.522678  [   60/   88]
per-ex loss: 0.389039  [   61/   88]
per-ex loss: 0.630976  [   62/   88]
per-ex loss: 0.613909  [   63/   88]
per-ex loss: 0.500269  [   64/   88]
per-ex loss: 0.355992  [   65/   88]
per-ex loss: 0.422083  [   66/   88]
per-ex loss: 0.635331  [   67/   88]
per-ex loss: 0.418819  [   68/   88]
per-ex loss: 0.363817  [   69/   88]
per-ex loss: 0.338546  [   70/   88]
per-ex loss: 0.353410  [   71/   88]
per-ex loss: 0.409164  [   72/   88]
per-ex loss: 0.671063  [   73/   88]
per-ex loss: 0.576129  [   74/   88]
per-ex loss: 0.413807  [   75/   88]
per-ex loss: 0.569905  [   76/   88]
per-ex loss: 0.460169  [   77/   88]
per-ex loss: 0.352288  [   78/   88]
per-ex loss: 0.586500  [   79/   88]
per-ex loss: 0.373527  [   80/   88]
per-ex loss: 0.576186  [   81/   88]
per-ex loss: 0.368320  [   82/   88]
per-ex loss: 0.509081  [   83/   88]
per-ex loss: 0.387314  [   84/   88]
per-ex loss: 0.344417  [   85/   88]
per-ex loss: 0.360806  [   86/   88]
per-ex loss: 0.427171  [   87/   88]
per-ex loss: 0.417907  [   88/   88]
Train Error: Avg loss: 0.46561404
validation Error: 
 Avg loss: 0.53938709 
 F1: 0.487354 
 Precision: 0.590451 
 Recall: 0.414908
 IoU: 0.322186

test Error: 
 Avg loss: 0.49059145 
 F1: 0.557658 
 Precision: 0.650322 
 Recall: 0.488108
 IoU: 0.386634

We have finished training iteration 83
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_81_.pth
per-ex loss: 0.333391  [    1/   88]
per-ex loss: 0.494760  [    2/   88]
per-ex loss: 0.649294  [    3/   88]
per-ex loss: 0.464794  [    4/   88]
per-ex loss: 0.419755  [    5/   88]
per-ex loss: 0.410313  [    6/   88]
per-ex loss: 0.382708  [    7/   88]
per-ex loss: 0.345264  [    8/   88]
per-ex loss: 0.583640  [    9/   88]
per-ex loss: 0.360343  [   10/   88]
per-ex loss: 0.573689  [   11/   88]
per-ex loss: 0.378861  [   12/   88]
per-ex loss: 0.583260  [   13/   88]
per-ex loss: 0.339460  [   14/   88]
per-ex loss: 0.582416  [   15/   88]
per-ex loss: 0.530519  [   16/   88]
per-ex loss: 0.567264  [   17/   88]
per-ex loss: 0.401893  [   18/   88]
per-ex loss: 0.431324  [   19/   88]
per-ex loss: 0.384397  [   20/   88]
per-ex loss: 0.409178  [   21/   88]
per-ex loss: 0.341435  [   22/   88]
per-ex loss: 0.342053  [   23/   88]
per-ex loss: 0.380196  [   24/   88]
per-ex loss: 0.581745  [   25/   88]
per-ex loss: 0.492652  [   26/   88]
per-ex loss: 0.536178  [   27/   88]
per-ex loss: 0.482433  [   28/   88]
per-ex loss: 0.547107  [   29/   88]
per-ex loss: 0.413943  [   30/   88]
per-ex loss: 0.379824  [   31/   88]
per-ex loss: 0.451533  [   32/   88]
per-ex loss: 0.390496  [   33/   88]
per-ex loss: 0.348020  [   34/   88]
per-ex loss: 0.446331  [   35/   88]
per-ex loss: 0.694799  [   36/   88]
per-ex loss: 0.631552  [   37/   88]
per-ex loss: 0.426311  [   38/   88]
per-ex loss: 0.394869  [   39/   88]
per-ex loss: 0.564462  [   40/   88]
per-ex loss: 0.574704  [   41/   88]
per-ex loss: 0.517344  [   42/   88]
per-ex loss: 0.327474  [   43/   88]
per-ex loss: 0.557736  [   44/   88]
per-ex loss: 0.573745  [   45/   88]
per-ex loss: 0.670948  [   46/   88]
per-ex loss: 0.485089  [   47/   88]
per-ex loss: 0.525779  [   48/   88]
per-ex loss: 0.385992  [   49/   88]
per-ex loss: 0.404888  [   50/   88]
per-ex loss: 0.370621  [   51/   88]
per-ex loss: 0.555187  [   52/   88]
per-ex loss: 0.400045  [   53/   88]
per-ex loss: 0.378505  [   54/   88]
per-ex loss: 0.649036  [   55/   88]
per-ex loss: 0.369242  [   56/   88]
per-ex loss: 0.429283  [   57/   88]
per-ex loss: 0.398449  [   58/   88]
per-ex loss: 0.446752  [   59/   88]
per-ex loss: 0.434580  [   60/   88]
per-ex loss: 0.505602  [   61/   88]
per-ex loss: 0.359365  [   62/   88]
per-ex loss: 0.631705  [   63/   88]
per-ex loss: 0.405578  [   64/   88]
per-ex loss: 0.650958  [   65/   88]
per-ex loss: 0.519403  [   66/   88]
per-ex loss: 0.569496  [   67/   88]
per-ex loss: 0.396981  [   68/   88]
per-ex loss: 0.557757  [   69/   88]
per-ex loss: 0.513316  [   70/   88]
per-ex loss: 0.349340  [   71/   88]
per-ex loss: 0.399500  [   72/   88]
per-ex loss: 0.408512  [   73/   88]
per-ex loss: 0.594327  [   74/   88]
per-ex loss: 0.488896  [   75/   88]
per-ex loss: 0.438887  [   76/   88]
per-ex loss: 0.322804  [   77/   88]
per-ex loss: 0.408706  [   78/   88]
per-ex loss: 0.610371  [   79/   88]
per-ex loss: 0.357644  [   80/   88]
per-ex loss: 0.315451  [   81/   88]
per-ex loss: 0.554523  [   82/   88]
per-ex loss: 0.478660  [   83/   88]
per-ex loss: 0.289424  [   84/   88]
per-ex loss: 0.497454  [   85/   88]
per-ex loss: 0.615735  [   86/   88]
per-ex loss: 0.367788  [   87/   88]
per-ex loss: 0.335175  [   88/   88]
Train Error: Avg loss: 0.46444567
validation Error: 
 Avg loss: 0.53208060 
 F1: 0.494873 
 Precision: 0.571007 
 Recall: 0.436652
 IoU: 0.328791

test Error: 
 Avg loss: 0.48669332 
 F1: 0.565438 
 Precision: 0.623007 
 Recall: 0.517609
 IoU: 0.394154

We have finished training iteration 84
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_65_.pth
per-ex loss: 0.351196  [    1/   88]
per-ex loss: 0.669161  [    2/   88]
per-ex loss: 0.465526  [    3/   88]
per-ex loss: 0.602696  [    4/   88]
per-ex loss: 0.305317  [    5/   88]
per-ex loss: 0.454499  [    6/   88]
per-ex loss: 0.409338  [    7/   88]
per-ex loss: 0.343889  [    8/   88]
per-ex loss: 0.536162  [    9/   88]
per-ex loss: 0.421651  [   10/   88]
per-ex loss: 0.556063  [   11/   88]
per-ex loss: 0.342996  [   12/   88]
per-ex loss: 0.385590  [   13/   88]
per-ex loss: 0.427216  [   14/   88]
per-ex loss: 0.548334  [   15/   88]
per-ex loss: 0.515364  [   16/   88]
per-ex loss: 0.525865  [   17/   88]
per-ex loss: 0.432402  [   18/   88]
per-ex loss: 0.443566  [   19/   88]
per-ex loss: 0.384668  [   20/   88]
per-ex loss: 0.389361  [   21/   88]
per-ex loss: 0.531657  [   22/   88]
per-ex loss: 0.564006  [   23/   88]
per-ex loss: 0.602535  [   24/   88]
per-ex loss: 0.418577  [   25/   88]
per-ex loss: 0.592608  [   26/   88]
per-ex loss: 0.436862  [   27/   88]
per-ex loss: 0.429621  [   28/   88]
per-ex loss: 0.374576  [   29/   88]
per-ex loss: 0.621718  [   30/   88]
per-ex loss: 0.374272  [   31/   88]
per-ex loss: 0.361700  [   32/   88]
per-ex loss: 0.386319  [   33/   88]
per-ex loss: 0.489471  [   34/   88]
per-ex loss: 0.596305  [   35/   88]
per-ex loss: 0.578239  [   36/   88]
per-ex loss: 0.365055  [   37/   88]
per-ex loss: 0.388591  [   38/   88]
per-ex loss: 0.361685  [   39/   88]
per-ex loss: 0.350482  [   40/   88]
per-ex loss: 0.406093  [   41/   88]
per-ex loss: 0.645875  [   42/   88]
per-ex loss: 0.343533  [   43/   88]
per-ex loss: 0.583072  [   44/   88]
per-ex loss: 0.427905  [   45/   88]
per-ex loss: 0.391355  [   46/   88]
per-ex loss: 0.482482  [   47/   88]
per-ex loss: 0.385784  [   48/   88]
per-ex loss: 0.315045  [   49/   88]
per-ex loss: 0.461260  [   50/   88]
per-ex loss: 0.582338  [   51/   88]
per-ex loss: 0.513917  [   52/   88]
per-ex loss: 0.631038  [   53/   88]
per-ex loss: 0.547832  [   54/   88]
per-ex loss: 0.321137  [   55/   88]
per-ex loss: 0.638576  [   56/   88]
per-ex loss: 0.584298  [   57/   88]
per-ex loss: 0.606463  [   58/   88]
per-ex loss: 0.337582  [   59/   88]
per-ex loss: 0.506329  [   60/   88]
per-ex loss: 0.657923  [   61/   88]
per-ex loss: 0.566490  [   62/   88]
per-ex loss: 0.350748  [   63/   88]
per-ex loss: 0.415532  [   64/   88]
per-ex loss: 0.348443  [   65/   88]
per-ex loss: 0.607378  [   66/   88]
per-ex loss: 0.577383  [   67/   88]
per-ex loss: 0.379339  [   68/   88]
per-ex loss: 0.348502  [   69/   88]
per-ex loss: 0.396191  [   70/   88]
per-ex loss: 0.400340  [   71/   88]
per-ex loss: 0.439398  [   72/   88]
per-ex loss: 0.428395  [   73/   88]
per-ex loss: 0.530743  [   74/   88]
per-ex loss: 0.340569  [   75/   88]
per-ex loss: 0.472128  [   76/   88]
per-ex loss: 0.349084  [   77/   88]
per-ex loss: 0.387767  [   78/   88]
per-ex loss: 0.429488  [   79/   88]
per-ex loss: 0.355071  [   80/   88]
per-ex loss: 0.552169  [   81/   88]
per-ex loss: 0.529145  [   82/   88]
per-ex loss: 0.480555  [   83/   88]
per-ex loss: 0.530903  [   84/   88]
per-ex loss: 0.482866  [   85/   88]
per-ex loss: 0.615773  [   86/   88]
per-ex loss: 0.335510  [   87/   88]
per-ex loss: 0.399556  [   88/   88]
Train Error: Avg loss: 0.46311864
validation Error: 
 Avg loss: 0.53195095 
 F1: 0.502319 
 Precision: 0.544543 
 Recall: 0.466172
 IoU: 0.335398

test Error: 
 Avg loss: 0.48187831 
 F1: 0.569262 
 Precision: 0.585887 
 Recall: 0.553554
 IoU: 0.397880

We have finished training iteration 85
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_83_.pth
per-ex loss: 0.580271  [    1/   88]
per-ex loss: 0.518149  [    2/   88]
per-ex loss: 0.675328  [    3/   88]
per-ex loss: 0.343452  [    4/   88]
per-ex loss: 0.380083  [    5/   88]
per-ex loss: 0.564350  [    6/   88]
per-ex loss: 0.517983  [    7/   88]
per-ex loss: 0.402642  [    8/   88]
per-ex loss: 0.596887  [    9/   88]
per-ex loss: 0.377698  [   10/   88]
per-ex loss: 0.400253  [   11/   88]
per-ex loss: 0.382943  [   12/   88]
per-ex loss: 0.389649  [   13/   88]
per-ex loss: 0.370265  [   14/   88]
per-ex loss: 0.527836  [   15/   88]
per-ex loss: 0.490626  [   16/   88]
per-ex loss: 0.382715  [   17/   88]
per-ex loss: 0.359297  [   18/   88]
per-ex loss: 0.323919  [   19/   88]
per-ex loss: 0.589393  [   20/   88]
per-ex loss: 0.477239  [   21/   88]
per-ex loss: 0.626296  [   22/   88]
per-ex loss: 0.343369  [   23/   88]
per-ex loss: 0.365521  [   24/   88]
per-ex loss: 0.550887  [   25/   88]
per-ex loss: 0.432696  [   26/   88]
per-ex loss: 0.429994  [   27/   88]
per-ex loss: 0.438364  [   28/   88]
per-ex loss: 0.520796  [   29/   88]
per-ex loss: 0.538550  [   30/   88]
per-ex loss: 0.315380  [   31/   88]
per-ex loss: 0.590034  [   32/   88]
per-ex loss: 0.544957  [   33/   88]
per-ex loss: 0.647119  [   34/   88]
per-ex loss: 0.568451  [   35/   88]
per-ex loss: 0.536081  [   36/   88]
per-ex loss: 0.404097  [   37/   88]
per-ex loss: 0.369281  [   38/   88]
per-ex loss: 0.418533  [   39/   88]
per-ex loss: 0.440063  [   40/   88]
per-ex loss: 0.342545  [   41/   88]
per-ex loss: 0.317365  [   42/   88]
per-ex loss: 0.353561  [   43/   88]
per-ex loss: 0.458592  [   44/   88]
per-ex loss: 0.552589  [   45/   88]
per-ex loss: 0.335333  [   46/   88]
per-ex loss: 0.458859  [   47/   88]
per-ex loss: 0.350834  [   48/   88]
per-ex loss: 0.423255  [   49/   88]
per-ex loss: 0.351003  [   50/   88]
per-ex loss: 0.413319  [   51/   88]
per-ex loss: 0.390568  [   52/   88]
per-ex loss: 0.391966  [   53/   88]
per-ex loss: 0.500469  [   54/   88]
per-ex loss: 0.460440  [   55/   88]
per-ex loss: 0.395131  [   56/   88]
per-ex loss: 0.613450  [   57/   88]
per-ex loss: 0.528172  [   58/   88]
per-ex loss: 0.370030  [   59/   88]
per-ex loss: 0.361991  [   60/   88]
per-ex loss: 0.542374  [   61/   88]
per-ex loss: 0.610379  [   62/   88]
per-ex loss: 0.457607  [   63/   88]
per-ex loss: 0.651988  [   64/   88]
per-ex loss: 0.612080  [   65/   88]
per-ex loss: 0.620250  [   66/   88]
per-ex loss: 0.449985  [   67/   88]
per-ex loss: 0.601714  [   68/   88]
per-ex loss: 0.432449  [   69/   88]
per-ex loss: 0.468633  [   70/   88]
per-ex loss: 0.609272  [   71/   88]
per-ex loss: 0.270977  [   72/   88]
per-ex loss: 0.421765  [   73/   88]
per-ex loss: 0.534307  [   74/   88]
per-ex loss: 0.412963  [   75/   88]
per-ex loss: 0.451637  [   76/   88]
per-ex loss: 0.402232  [   77/   88]
per-ex loss: 0.772038  [   78/   88]
per-ex loss: 0.322843  [   79/   88]
per-ex loss: 0.578494  [   80/   88]
per-ex loss: 0.645006  [   81/   88]
per-ex loss: 0.624333  [   82/   88]
per-ex loss: 0.501071  [   83/   88]
per-ex loss: 0.346400  [   84/   88]
per-ex loss: 0.557691  [   85/   88]
per-ex loss: 0.405346  [   86/   88]
per-ex loss: 0.385202  [   87/   88]
per-ex loss: 0.659124  [   88/   88]
Train Error: Avg loss: 0.47101220
validation Error: 
 Avg loss: 0.53384840 
 F1: 0.491361 
 Precision: 0.571260 
 Recall: 0.431070
 IoU: 0.325698

test Error: 
 Avg loss: 0.48756612 
 F1: 0.563646 
 Precision: 0.614459 
 Recall: 0.520595
 IoU: 0.392414

We have finished training iteration 86
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_84_.pth
per-ex loss: 0.352302  [    1/   88]
per-ex loss: 0.376778  [    2/   88]
per-ex loss: 0.346197  [    3/   88]
per-ex loss: 0.308870  [    4/   88]
per-ex loss: 0.392875  [    5/   88]
per-ex loss: 0.331765  [    6/   88]
per-ex loss: 0.376989  [    7/   88]
per-ex loss: 0.449827  [    8/   88]
per-ex loss: 0.600467  [    9/   88]
per-ex loss: 0.597300  [   10/   88]
per-ex loss: 0.612642  [   11/   88]
per-ex loss: 0.345817  [   12/   88]
per-ex loss: 0.382083  [   13/   88]
per-ex loss: 0.398965  [   14/   88]
per-ex loss: 0.254633  [   15/   88]
per-ex loss: 0.629398  [   16/   88]
per-ex loss: 0.341914  [   17/   88]
per-ex loss: 0.593959  [   18/   88]
per-ex loss: 0.520371  [   19/   88]
per-ex loss: 0.348338  [   20/   88]
per-ex loss: 0.659286  [   21/   88]
per-ex loss: 0.593574  [   22/   88]
per-ex loss: 0.477144  [   23/   88]
per-ex loss: 0.537289  [   24/   88]
per-ex loss: 0.598529  [   25/   88]
per-ex loss: 0.586902  [   26/   88]
per-ex loss: 0.355067  [   27/   88]
per-ex loss: 0.551886  [   28/   88]
per-ex loss: 0.403236  [   29/   88]
per-ex loss: 0.464271  [   30/   88]
per-ex loss: 0.407913  [   31/   88]
per-ex loss: 0.490448  [   32/   88]
per-ex loss: 0.520752  [   33/   88]
per-ex loss: 0.550256  [   34/   88]
per-ex loss: 0.544107  [   35/   88]
per-ex loss: 0.520801  [   36/   88]
per-ex loss: 0.405547  [   37/   88]
per-ex loss: 0.485345  [   38/   88]
per-ex loss: 0.424510  [   39/   88]
per-ex loss: 0.466959  [   40/   88]
per-ex loss: 0.482245  [   41/   88]
per-ex loss: 0.652892  [   42/   88]
per-ex loss: 0.448258  [   43/   88]
per-ex loss: 0.649578  [   44/   88]
per-ex loss: 0.591212  [   45/   88]
per-ex loss: 0.592482  [   46/   88]
per-ex loss: 0.397563  [   47/   88]
per-ex loss: 0.540128  [   48/   88]
per-ex loss: 0.391208  [   49/   88]
per-ex loss: 0.479711  [   50/   88]
per-ex loss: 0.351764  [   51/   88]
per-ex loss: 0.374793  [   52/   88]
per-ex loss: 0.644046  [   53/   88]
per-ex loss: 0.426836  [   54/   88]
per-ex loss: 0.563073  [   55/   88]
per-ex loss: 0.413434  [   56/   88]
per-ex loss: 0.412451  [   57/   88]
per-ex loss: 0.627110  [   58/   88]
per-ex loss: 0.373015  [   59/   88]
per-ex loss: 0.416630  [   60/   88]
per-ex loss: 0.566322  [   61/   88]
per-ex loss: 0.299978  [   62/   88]
per-ex loss: 0.430569  [   63/   88]
per-ex loss: 0.385327  [   64/   88]
per-ex loss: 0.571032  [   65/   88]
per-ex loss: 0.425932  [   66/   88]
per-ex loss: 0.371105  [   67/   88]
per-ex loss: 0.527352  [   68/   88]
per-ex loss: 0.451457  [   69/   88]
per-ex loss: 0.328749  [   70/   88]
per-ex loss: 0.411867  [   71/   88]
per-ex loss: 0.338040  [   72/   88]
per-ex loss: 0.599088  [   73/   88]
per-ex loss: 0.371247  [   74/   88]
per-ex loss: 0.393836  [   75/   88]
per-ex loss: 0.361995  [   76/   88]
per-ex loss: 0.390916  [   77/   88]
per-ex loss: 0.328626  [   78/   88]
per-ex loss: 0.349658  [   79/   88]
per-ex loss: 0.377171  [   80/   88]
per-ex loss: 0.470004  [   81/   88]
per-ex loss: 0.528644  [   82/   88]
per-ex loss: 0.309557  [   83/   88]
per-ex loss: 0.375784  [   84/   88]
per-ex loss: 0.351173  [   85/   88]
per-ex loss: 0.581277  [   86/   88]
per-ex loss: 0.446759  [   87/   88]
per-ex loss: 0.494252  [   88/   88]
Train Error: Avg loss: 0.45763020
validation Error: 
 Avg loss: 0.52780057 
 F1: 0.501784 
 Precision: 0.523709 
 Recall: 0.481622
 IoU: 0.334921

test Error: 
 Avg loss: 0.47476080 
 F1: 0.573234 
 Precision: 0.583628 
 Recall: 0.563204
 IoU: 0.401772

We have finished training iteration 87
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_85_.pth
per-ex loss: 0.364653  [    1/   88]
per-ex loss: 0.411064  [    2/   88]
per-ex loss: 0.346147  [    3/   88]
per-ex loss: 0.486875  [    4/   88]
per-ex loss: 0.371148  [    5/   88]
per-ex loss: 0.424957  [    6/   88]
per-ex loss: 0.388500  [    7/   88]
per-ex loss: 0.407622  [    8/   88]
per-ex loss: 0.519902  [    9/   88]
per-ex loss: 0.402584  [   10/   88]
per-ex loss: 0.470392  [   11/   88]
per-ex loss: 0.666888  [   12/   88]
per-ex loss: 0.343516  [   13/   88]
per-ex loss: 0.452206  [   14/   88]
per-ex loss: 0.491378  [   15/   88]
per-ex loss: 0.607637  [   16/   88]
per-ex loss: 0.462829  [   17/   88]
per-ex loss: 0.391565  [   18/   88]
per-ex loss: 0.498983  [   19/   88]
per-ex loss: 0.399911  [   20/   88]
per-ex loss: 0.581039  [   21/   88]
per-ex loss: 0.572716  [   22/   88]
per-ex loss: 0.391866  [   23/   88]
per-ex loss: 0.392136  [   24/   88]
per-ex loss: 0.362183  [   25/   88]
per-ex loss: 0.317727  [   26/   88]
per-ex loss: 0.490458  [   27/   88]
per-ex loss: 0.357771  [   28/   88]
per-ex loss: 0.478133  [   29/   88]
per-ex loss: 0.600965  [   30/   88]
per-ex loss: 0.420480  [   31/   88]
per-ex loss: 0.379597  [   32/   88]
per-ex loss: 0.565351  [   33/   88]
per-ex loss: 0.449315  [   34/   88]
per-ex loss: 0.346441  [   35/   88]
per-ex loss: 0.631903  [   36/   88]
per-ex loss: 0.486487  [   37/   88]
per-ex loss: 0.657464  [   38/   88]
per-ex loss: 0.394970  [   39/   88]
per-ex loss: 0.366675  [   40/   88]
per-ex loss: 0.609626  [   41/   88]
per-ex loss: 0.376565  [   42/   88]
per-ex loss: 0.609119  [   43/   88]
per-ex loss: 0.384179  [   44/   88]
per-ex loss: 0.554300  [   45/   88]
per-ex loss: 0.342064  [   46/   88]
per-ex loss: 0.498679  [   47/   88]
per-ex loss: 0.525947  [   48/   88]
per-ex loss: 0.383453  [   49/   88]
per-ex loss: 0.345693  [   50/   88]
per-ex loss: 0.547773  [   51/   88]
per-ex loss: 0.405096  [   52/   88]
per-ex loss: 0.462819  [   53/   88]
per-ex loss: 0.566329  [   54/   88]
per-ex loss: 0.371519  [   55/   88]
per-ex loss: 0.629318  [   56/   88]
per-ex loss: 0.407319  [   57/   88]
per-ex loss: 0.560297  [   58/   88]
per-ex loss: 0.593166  [   59/   88]
per-ex loss: 0.349791  [   60/   88]
per-ex loss: 0.412446  [   61/   88]
per-ex loss: 0.355575  [   62/   88]
per-ex loss: 0.355840  [   63/   88]
per-ex loss: 0.402024  [   64/   88]
per-ex loss: 0.558814  [   65/   88]
per-ex loss: 0.678320  [   66/   88]
per-ex loss: 0.428757  [   67/   88]
per-ex loss: 0.544740  [   68/   88]
per-ex loss: 0.501750  [   69/   88]
per-ex loss: 0.565924  [   70/   88]
per-ex loss: 0.550075  [   71/   88]
per-ex loss: 0.420106  [   72/   88]
per-ex loss: 0.368346  [   73/   88]
per-ex loss: 0.473923  [   74/   88]
per-ex loss: 0.398592  [   75/   88]
per-ex loss: 0.600825  [   76/   88]
per-ex loss: 0.350606  [   77/   88]
per-ex loss: 0.553388  [   78/   88]
per-ex loss: 0.343303  [   79/   88]
per-ex loss: 0.492704  [   80/   88]
per-ex loss: 0.320371  [   81/   88]
per-ex loss: 0.369645  [   82/   88]
per-ex loss: 0.424003  [   83/   88]
per-ex loss: 0.623002  [   84/   88]
per-ex loss: 0.536111  [   85/   88]
per-ex loss: 0.313546  [   86/   88]
per-ex loss: 0.587525  [   87/   88]
per-ex loss: 0.373256  [   88/   88]
Train Error: Avg loss: 0.46112507
validation Error: 
 Avg loss: 0.53275704 
 F1: 0.499892 
 Precision: 0.558919 
 Recall: 0.452142
 IoU: 0.333238

test Error: 
 Avg loss: 0.48237829 
 F1: 0.570360 
 Precision: 0.617715 
 Recall: 0.529748
 IoU: 0.398953

We have finished training iteration 88
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_86_.pth
per-ex loss: 0.327175  [    1/   88]
per-ex loss: 0.598912  [    2/   88]
per-ex loss: 0.592814  [    3/   88]
per-ex loss: 0.535834  [    4/   88]
per-ex loss: 0.367715  [    5/   88]
per-ex loss: 0.398632  [    6/   88]
per-ex loss: 0.623921  [    7/   88]
per-ex loss: 0.327839  [    8/   88]
per-ex loss: 0.556361  [    9/   88]
per-ex loss: 0.329240  [   10/   88]
per-ex loss: 0.381421  [   11/   88]
per-ex loss: 0.638532  [   12/   88]
per-ex loss: 0.548881  [   13/   88]
per-ex loss: 0.298599  [   14/   88]
per-ex loss: 0.592822  [   15/   88]
per-ex loss: 0.518086  [   16/   88]
per-ex loss: 0.369732  [   17/   88]
per-ex loss: 0.459631  [   18/   88]
per-ex loss: 0.414490  [   19/   88]
per-ex loss: 0.543444  [   20/   88]
per-ex loss: 0.451622  [   21/   88]
per-ex loss: 0.408506  [   22/   88]
per-ex loss: 0.526573  [   23/   88]
per-ex loss: 0.596897  [   24/   88]
per-ex loss: 0.391392  [   25/   88]
per-ex loss: 0.336313  [   26/   88]
per-ex loss: 0.497574  [   27/   88]
per-ex loss: 0.378426  [   28/   88]
per-ex loss: 0.467583  [   29/   88]
per-ex loss: 0.429171  [   30/   88]
per-ex loss: 0.544970  [   31/   88]
per-ex loss: 0.340714  [   32/   88]
per-ex loss: 0.330759  [   33/   88]
per-ex loss: 0.406899  [   34/   88]
per-ex loss: 0.424823  [   35/   88]
per-ex loss: 0.632472  [   36/   88]
per-ex loss: 0.470210  [   37/   88]
per-ex loss: 0.513004  [   38/   88]
per-ex loss: 0.271654  [   39/   88]
per-ex loss: 0.357822  [   40/   88]
per-ex loss: 0.407393  [   41/   88]
per-ex loss: 0.383805  [   42/   88]
per-ex loss: 0.380612  [   43/   88]
per-ex loss: 0.342191  [   44/   88]
per-ex loss: 0.409904  [   45/   88]
per-ex loss: 0.350364  [   46/   88]
per-ex loss: 0.571802  [   47/   88]
per-ex loss: 0.460649  [   48/   88]
per-ex loss: 0.603526  [   49/   88]
per-ex loss: 0.522952  [   50/   88]
per-ex loss: 0.408431  [   51/   88]
per-ex loss: 0.545434  [   52/   88]
per-ex loss: 0.548782  [   53/   88]
per-ex loss: 0.402979  [   54/   88]
per-ex loss: 0.563140  [   55/   88]
per-ex loss: 0.563574  [   56/   88]
per-ex loss: 0.412429  [   57/   88]
per-ex loss: 0.313626  [   58/   88]
per-ex loss: 0.324887  [   59/   88]
per-ex loss: 0.672640  [   60/   88]
per-ex loss: 0.393573  [   61/   88]
per-ex loss: 0.315998  [   62/   88]
per-ex loss: 0.557780  [   63/   88]
per-ex loss: 0.332284  [   64/   88]
per-ex loss: 0.346224  [   65/   88]
per-ex loss: 0.564793  [   66/   88]
per-ex loss: 0.530965  [   67/   88]
per-ex loss: 0.438104  [   68/   88]
per-ex loss: 0.627610  [   69/   88]
per-ex loss: 0.519915  [   70/   88]
per-ex loss: 0.371806  [   71/   88]
per-ex loss: 0.388490  [   72/   88]
per-ex loss: 0.453627  [   73/   88]
per-ex loss: 0.474887  [   74/   88]
per-ex loss: 0.603728  [   75/   88]
per-ex loss: 0.672748  [   76/   88]
per-ex loss: 0.376082  [   77/   88]
per-ex loss: 0.618271  [   78/   88]
per-ex loss: 0.372872  [   79/   88]
per-ex loss: 0.330976  [   80/   88]
per-ex loss: 0.514548  [   81/   88]
per-ex loss: 0.463555  [   82/   88]
per-ex loss: 0.468612  [   83/   88]
per-ex loss: 0.558238  [   84/   88]
per-ex loss: 0.396810  [   85/   88]
per-ex loss: 0.403009  [   86/   88]
per-ex loss: 0.480221  [   87/   88]
per-ex loss: 0.446219  [   88/   88]
Train Error: Avg loss: 0.45923329
validation Error: 
 Avg loss: 0.54676070 
 F1: 0.476360 
 Precision: 0.434206 
 Recall: 0.527579
 IoU: 0.312646

test Error: 
 Avg loss: 0.48985947 
 F1: 0.551247 
 Precision: 0.503437 
 Recall: 0.609091
 IoU: 0.380497

We have finished training iteration 89
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_87_.pth
per-ex loss: 0.558972  [    1/   88]
per-ex loss: 0.486159  [    2/   88]
per-ex loss: 0.389594  [    3/   88]
per-ex loss: 0.626429  [    4/   88]
per-ex loss: 0.544949  [    5/   88]
per-ex loss: 0.629251  [    6/   88]
per-ex loss: 0.340405  [    7/   88]
per-ex loss: 0.431688  [    8/   88]
per-ex loss: 0.395181  [    9/   88]
per-ex loss: 0.596010  [   10/   88]
per-ex loss: 0.455098  [   11/   88]
per-ex loss: 0.417211  [   12/   88]
per-ex loss: 0.356203  [   13/   88]
per-ex loss: 0.543949  [   14/   88]
per-ex loss: 0.391136  [   15/   88]
per-ex loss: 0.525044  [   16/   88]
per-ex loss: 0.335116  [   17/   88]
per-ex loss: 0.478778  [   18/   88]
per-ex loss: 0.434675  [   19/   88]
per-ex loss: 0.532489  [   20/   88]
per-ex loss: 0.557474  [   21/   88]
per-ex loss: 0.617746  [   22/   88]
per-ex loss: 0.490313  [   23/   88]
per-ex loss: 0.376539  [   24/   88]
per-ex loss: 0.471636  [   25/   88]
per-ex loss: 0.321090  [   26/   88]
per-ex loss: 0.380566  [   27/   88]
per-ex loss: 0.369002  [   28/   88]
per-ex loss: 0.397655  [   29/   88]
per-ex loss: 0.361355  [   30/   88]
per-ex loss: 0.587959  [   31/   88]
per-ex loss: 0.561058  [   32/   88]
per-ex loss: 0.500128  [   33/   88]
per-ex loss: 0.455588  [   34/   88]
per-ex loss: 0.303875  [   35/   88]
per-ex loss: 0.333061  [   36/   88]
per-ex loss: 0.567532  [   37/   88]
per-ex loss: 0.455766  [   38/   88]
per-ex loss: 0.633900  [   39/   88]
per-ex loss: 0.473565  [   40/   88]
per-ex loss: 0.621367  [   41/   88]
per-ex loss: 0.546491  [   42/   88]
per-ex loss: 0.545965  [   43/   88]
per-ex loss: 0.404530  [   44/   88]
per-ex loss: 0.448228  [   45/   88]
per-ex loss: 0.378893  [   46/   88]
per-ex loss: 0.351315  [   47/   88]
per-ex loss: 0.329261  [   48/   88]
per-ex loss: 0.393197  [   49/   88]
per-ex loss: 0.396064  [   50/   88]
per-ex loss: 0.525764  [   51/   88]
per-ex loss: 0.468528  [   52/   88]
per-ex loss: 0.436771  [   53/   88]
per-ex loss: 0.391791  [   54/   88]
per-ex loss: 0.374905  [   55/   88]
per-ex loss: 0.332682  [   56/   88]
per-ex loss: 0.448494  [   57/   88]
per-ex loss: 0.362031  [   58/   88]
per-ex loss: 0.343117  [   59/   88]
per-ex loss: 0.370004  [   60/   88]
per-ex loss: 0.633846  [   61/   88]
per-ex loss: 0.580411  [   62/   88]
per-ex loss: 0.342574  [   63/   88]
per-ex loss: 0.369509  [   64/   88]
per-ex loss: 0.532714  [   65/   88]
per-ex loss: 0.511742  [   66/   88]
per-ex loss: 0.350033  [   67/   88]
per-ex loss: 0.403235  [   68/   88]
per-ex loss: 0.437596  [   69/   88]
per-ex loss: 0.533203  [   70/   88]
per-ex loss: 0.531648  [   71/   88]
per-ex loss: 0.366380  [   72/   88]
per-ex loss: 0.432521  [   73/   88]
per-ex loss: 0.398170  [   74/   88]
per-ex loss: 0.319040  [   75/   88]
per-ex loss: 0.579791  [   76/   88]
per-ex loss: 0.350666  [   77/   88]
per-ex loss: 0.455920  [   78/   88]
per-ex loss: 0.657945  [   79/   88]
per-ex loss: 0.251348  [   80/   88]
per-ex loss: 0.345957  [   81/   88]
per-ex loss: 0.571429  [   82/   88]
per-ex loss: 0.383263  [   83/   88]
per-ex loss: 0.423107  [   84/   88]
per-ex loss: 0.490737  [   85/   88]
per-ex loss: 0.552315  [   86/   88]
per-ex loss: 0.663931  [   87/   88]
per-ex loss: 0.339030  [   88/   88]
Train Error: Avg loss: 0.45410915
validation Error: 
 Avg loss: 0.53700246 
 F1: 0.489101 
 Precision: 0.475836 
 Recall: 0.503126
 IoU: 0.323715

test Error: 
 Avg loss: 0.47923008 
 F1: 0.567706 
 Precision: 0.546596 
 Recall: 0.590513
 IoU: 0.396362

We have finished training iteration 90
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_88_.pth
per-ex loss: 0.580471  [    1/   88]
per-ex loss: 0.551768  [    2/   88]
per-ex loss: 0.346714  [    3/   88]
per-ex loss: 0.414086  [    4/   88]
per-ex loss: 0.480290  [    5/   88]
per-ex loss: 0.389932  [    6/   88]
per-ex loss: 0.426992  [    7/   88]
per-ex loss: 0.624474  [    8/   88]
per-ex loss: 0.306709  [    9/   88]
per-ex loss: 0.573473  [   10/   88]
per-ex loss: 0.558976  [   11/   88]
per-ex loss: 0.455250  [   12/   88]
per-ex loss: 0.456202  [   13/   88]
per-ex loss: 0.541552  [   14/   88]
per-ex loss: 0.409952  [   15/   88]
per-ex loss: 0.596387  [   16/   88]
per-ex loss: 0.468115  [   17/   88]
per-ex loss: 0.394917  [   18/   88]
per-ex loss: 0.353925  [   19/   88]
per-ex loss: 0.642332  [   20/   88]
per-ex loss: 0.338954  [   21/   88]
per-ex loss: 0.388318  [   22/   88]
per-ex loss: 0.552713  [   23/   88]
per-ex loss: 0.414637  [   24/   88]
per-ex loss: 0.341674  [   25/   88]
per-ex loss: 0.390442  [   26/   88]
per-ex loss: 0.647993  [   27/   88]
per-ex loss: 0.412229  [   28/   88]
per-ex loss: 0.586512  [   29/   88]
per-ex loss: 0.382576  [   30/   88]
per-ex loss: 0.330988  [   31/   88]
per-ex loss: 0.333652  [   32/   88]
per-ex loss: 0.651268  [   33/   88]
per-ex loss: 0.343336  [   34/   88]
per-ex loss: 0.407721  [   35/   88]
per-ex loss: 0.327014  [   36/   88]
per-ex loss: 0.365118  [   37/   88]
per-ex loss: 0.669000  [   38/   88]
per-ex loss: 0.356294  [   39/   88]
per-ex loss: 0.402460  [   40/   88]
per-ex loss: 0.359519  [   41/   88]
per-ex loss: 0.446751  [   42/   88]
per-ex loss: 0.319831  [   43/   88]
per-ex loss: 0.383394  [   44/   88]
per-ex loss: 0.364079  [   45/   88]
per-ex loss: 0.283820  [   46/   88]
per-ex loss: 0.444252  [   47/   88]
per-ex loss: 0.311828  [   48/   88]
per-ex loss: 0.400166  [   49/   88]
per-ex loss: 0.358011  [   50/   88]
per-ex loss: 0.525446  [   51/   88]
per-ex loss: 0.468640  [   52/   88]
per-ex loss: 0.649056  [   53/   88]
per-ex loss: 0.450637  [   54/   88]
per-ex loss: 0.539128  [   55/   88]
per-ex loss: 0.395847  [   56/   88]
per-ex loss: 0.401547  [   57/   88]
per-ex loss: 0.385594  [   58/   88]
per-ex loss: 0.497457  [   59/   88]
per-ex loss: 0.649970  [   60/   88]
per-ex loss: 0.300628  [   61/   88]
per-ex loss: 0.388444  [   62/   88]
per-ex loss: 0.595055  [   63/   88]
per-ex loss: 0.413979  [   64/   88]
per-ex loss: 0.545968  [   65/   88]
per-ex loss: 0.553904  [   66/   88]
per-ex loss: 0.375656  [   67/   88]
per-ex loss: 0.434161  [   68/   88]
per-ex loss: 0.386799  [   69/   88]
per-ex loss: 0.477767  [   70/   88]
per-ex loss: 0.376247  [   71/   88]
per-ex loss: 0.329529  [   72/   88]
per-ex loss: 0.560120  [   73/   88]
per-ex loss: 0.561316  [   74/   88]
per-ex loss: 0.585893  [   75/   88]
per-ex loss: 0.402357  [   76/   88]
per-ex loss: 0.403135  [   77/   88]
per-ex loss: 0.593006  [   78/   88]
per-ex loss: 0.545533  [   79/   88]
per-ex loss: 0.590278  [   80/   88]
per-ex loss: 0.515606  [   81/   88]
per-ex loss: 0.365870  [   82/   88]
per-ex loss: 0.502747  [   83/   88]
per-ex loss: 0.608042  [   84/   88]
per-ex loss: 0.547367  [   85/   88]
per-ex loss: 0.495911  [   86/   88]
per-ex loss: 0.480292  [   87/   88]
per-ex loss: 0.489955  [   88/   88]
Train Error: Avg loss: 0.45768130
validation Error: 
 Avg loss: 0.53482365 
 F1: 0.495972 
 Precision: 0.480180 
 Recall: 0.512838
 IoU: 0.329763

test Error: 
 Avg loss: 0.48421806 
 F1: 0.563442 
 Precision: 0.529569 
 Recall: 0.601943
 IoU: 0.392216

We have finished training iteration 91
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_89_.pth
per-ex loss: 0.466212  [    1/   88]
per-ex loss: 0.442843  [    2/   88]
per-ex loss: 0.376716  [    3/   88]
per-ex loss: 0.533179  [    4/   88]
per-ex loss: 0.406923  [    5/   88]
per-ex loss: 0.543474  [    6/   88]
per-ex loss: 0.543677  [    7/   88]
per-ex loss: 0.379403  [    8/   88]
per-ex loss: 0.591472  [    9/   88]
per-ex loss: 0.516426  [   10/   88]
per-ex loss: 0.347783  [   11/   88]
per-ex loss: 0.542863  [   12/   88]
per-ex loss: 0.380720  [   13/   88]
per-ex loss: 0.525112  [   14/   88]
per-ex loss: 0.461691  [   15/   88]
per-ex loss: 0.281395  [   16/   88]
per-ex loss: 0.365178  [   17/   88]
per-ex loss: 0.445821  [   18/   88]
per-ex loss: 0.358600  [   19/   88]
per-ex loss: 0.480188  [   20/   88]
per-ex loss: 0.564783  [   21/   88]
per-ex loss: 0.410404  [   22/   88]
per-ex loss: 0.326784  [   23/   88]
per-ex loss: 0.486509  [   24/   88]
per-ex loss: 0.425821  [   25/   88]
per-ex loss: 0.402796  [   26/   88]
per-ex loss: 0.672124  [   27/   88]
per-ex loss: 0.326633  [   28/   88]
per-ex loss: 0.382040  [   29/   88]
per-ex loss: 0.373920  [   30/   88]
per-ex loss: 0.626636  [   31/   88]
per-ex loss: 0.396906  [   32/   88]
per-ex loss: 0.690929  [   33/   88]
per-ex loss: 0.577362  [   34/   88]
per-ex loss: 0.396528  [   35/   88]
per-ex loss: 0.405729  [   36/   88]
per-ex loss: 0.616571  [   37/   88]
per-ex loss: 0.622420  [   38/   88]
per-ex loss: 0.561529  [   39/   88]
per-ex loss: 0.475175  [   40/   88]
per-ex loss: 0.436761  [   41/   88]
per-ex loss: 0.320112  [   42/   88]
per-ex loss: 0.473313  [   43/   88]
per-ex loss: 0.597072  [   44/   88]
per-ex loss: 0.356855  [   45/   88]
per-ex loss: 0.402722  [   46/   88]
per-ex loss: 0.636002  [   47/   88]
per-ex loss: 0.455709  [   48/   88]
per-ex loss: 0.401677  [   49/   88]
per-ex loss: 0.471839  [   50/   88]
per-ex loss: 0.327942  [   51/   88]
per-ex loss: 0.350092  [   52/   88]
per-ex loss: 0.583397  [   53/   88]
per-ex loss: 0.340397  [   54/   88]
per-ex loss: 0.583297  [   55/   88]
per-ex loss: 0.514986  [   56/   88]
per-ex loss: 0.437876  [   57/   88]
per-ex loss: 0.401753  [   58/   88]
per-ex loss: 0.412844  [   59/   88]
per-ex loss: 0.376116  [   60/   88]
per-ex loss: 0.339252  [   61/   88]
per-ex loss: 0.429375  [   62/   88]
per-ex loss: 0.297200  [   63/   88]
per-ex loss: 0.352523  [   64/   88]
per-ex loss: 0.517464  [   65/   88]
per-ex loss: 0.315708  [   66/   88]
per-ex loss: 0.490770  [   67/   88]
per-ex loss: 0.587000  [   68/   88]
per-ex loss: 0.328739  [   69/   88]
per-ex loss: 0.382884  [   70/   88]
per-ex loss: 0.568481  [   71/   88]
per-ex loss: 0.564534  [   72/   88]
per-ex loss: 0.607697  [   73/   88]
per-ex loss: 0.337672  [   74/   88]
per-ex loss: 0.367133  [   75/   88]
per-ex loss: 0.453769  [   76/   88]
per-ex loss: 0.428658  [   77/   88]
per-ex loss: 0.383760  [   78/   88]
per-ex loss: 0.472826  [   79/   88]
per-ex loss: 0.409642  [   80/   88]
per-ex loss: 0.534950  [   81/   88]
per-ex loss: 0.627306  [   82/   88]
per-ex loss: 0.395444  [   83/   88]
per-ex loss: 0.598133  [   84/   88]
per-ex loss: 0.588138  [   85/   88]
per-ex loss: 0.362569  [   86/   88]
per-ex loss: 0.353167  [   87/   88]
per-ex loss: 0.595308  [   88/   88]
Train Error: Avg loss: 0.45797888
validation Error: 
 Avg loss: 0.52528233 
 F1: 0.504106 
 Precision: 0.568295 
 Recall: 0.452945
 IoU: 0.336993

test Error: 
 Avg loss: 0.48593186 
 F1: 0.562107 
 Precision: 0.605502 
 Recall: 0.524516
 IoU: 0.390924

We have finished training iteration 92
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_90_.pth
per-ex loss: 0.325415  [    1/   88]
per-ex loss: 0.366468  [    2/   88]
per-ex loss: 0.439729  [    3/   88]
per-ex loss: 0.360974  [    4/   88]
per-ex loss: 0.525726  [    5/   88]
per-ex loss: 0.470030  [    6/   88]
per-ex loss: 0.606826  [    7/   88]
per-ex loss: 0.640567  [    8/   88]
per-ex loss: 0.445217  [    9/   88]
per-ex loss: 0.410742  [   10/   88]
per-ex loss: 0.480025  [   11/   88]
per-ex loss: 0.406098  [   12/   88]
per-ex loss: 0.492391  [   13/   88]
per-ex loss: 0.403759  [   14/   88]
per-ex loss: 0.682525  [   15/   88]
per-ex loss: 0.351071  [   16/   88]
per-ex loss: 0.362953  [   17/   88]
per-ex loss: 0.431391  [   18/   88]
per-ex loss: 0.504230  [   19/   88]
per-ex loss: 0.585521  [   20/   88]
per-ex loss: 0.386517  [   21/   88]
per-ex loss: 0.629436  [   22/   88]
per-ex loss: 0.385332  [   23/   88]
per-ex loss: 0.397046  [   24/   88]
per-ex loss: 0.379806  [   25/   88]
per-ex loss: 0.546989  [   26/   88]
per-ex loss: 0.350518  [   27/   88]
per-ex loss: 0.318321  [   28/   88]
per-ex loss: 0.522367  [   29/   88]
per-ex loss: 0.272142  [   30/   88]
per-ex loss: 0.448527  [   31/   88]
per-ex loss: 0.510110  [   32/   88]
per-ex loss: 0.477146  [   33/   88]
per-ex loss: 0.474513  [   34/   88]
per-ex loss: 0.481246  [   35/   88]
per-ex loss: 0.558740  [   36/   88]
per-ex loss: 0.406242  [   37/   88]
per-ex loss: 0.519577  [   38/   88]
per-ex loss: 0.461291  [   39/   88]
per-ex loss: 0.358480  [   40/   88]
per-ex loss: 0.561949  [   41/   88]
per-ex loss: 0.453874  [   42/   88]
per-ex loss: 0.514848  [   43/   88]
per-ex loss: 0.411677  [   44/   88]
per-ex loss: 0.411056  [   45/   88]
per-ex loss: 0.423876  [   46/   88]
per-ex loss: 0.624314  [   47/   88]
per-ex loss: 0.642308  [   48/   88]
per-ex loss: 0.338479  [   49/   88]
per-ex loss: 0.407765  [   50/   88]
per-ex loss: 0.336213  [   51/   88]
per-ex loss: 0.390379  [   52/   88]
per-ex loss: 0.450964  [   53/   88]
per-ex loss: 0.394684  [   54/   88]
per-ex loss: 0.341090  [   55/   88]
per-ex loss: 0.335785  [   56/   88]
per-ex loss: 0.411001  [   57/   88]
per-ex loss: 0.607356  [   58/   88]
per-ex loss: 0.530065  [   59/   88]
per-ex loss: 0.380518  [   60/   88]
per-ex loss: 0.423909  [   61/   88]
per-ex loss: 0.452893  [   62/   88]
per-ex loss: 0.380099  [   63/   88]
per-ex loss: 0.567533  [   64/   88]
per-ex loss: 0.409623  [   65/   88]
per-ex loss: 0.368847  [   66/   88]
per-ex loss: 0.559860  [   67/   88]
per-ex loss: 0.569812  [   68/   88]
per-ex loss: 0.365316  [   69/   88]
per-ex loss: 0.323998  [   70/   88]
per-ex loss: 0.330179  [   71/   88]
per-ex loss: 0.610074  [   72/   88]
per-ex loss: 0.555379  [   73/   88]
per-ex loss: 0.632401  [   74/   88]
per-ex loss: 0.385586  [   75/   88]
per-ex loss: 0.601592  [   76/   88]
per-ex loss: 0.347527  [   77/   88]
per-ex loss: 0.596167  [   78/   88]
per-ex loss: 0.462548  [   79/   88]
per-ex loss: 0.381078  [   80/   88]
per-ex loss: 0.389426  [   81/   88]
per-ex loss: 0.469529  [   82/   88]
per-ex loss: 0.537434  [   83/   88]
per-ex loss: 0.617871  [   84/   88]
per-ex loss: 0.356409  [   85/   88]
per-ex loss: 0.448514  [   86/   88]
per-ex loss: 0.571646  [   87/   88]
per-ex loss: 0.484622  [   88/   88]
Train Error: Avg loss: 0.45845545
validation Error: 
 Avg loss: 0.54944622 
 F1: 0.473515 
 Precision: 0.497970 
 Recall: 0.451349
 IoU: 0.310200

test Error: 
 Avg loss: 0.48227266 
 F1: 0.569262 
 Precision: 0.592453 
 Recall: 0.547817
 IoU: 0.397880

We have finished training iteration 93
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_91_.pth
per-ex loss: 0.384276  [    1/   88]
per-ex loss: 0.461939  [    2/   88]
per-ex loss: 0.502985  [    3/   88]
per-ex loss: 0.279123  [    4/   88]
per-ex loss: 0.419072  [    5/   88]
per-ex loss: 0.368507  [    6/   88]
per-ex loss: 0.327769  [    7/   88]
per-ex loss: 0.379863  [    8/   88]
per-ex loss: 0.326420  [    9/   88]
per-ex loss: 0.391506  [   10/   88]
per-ex loss: 0.571101  [   11/   88]
per-ex loss: 0.416984  [   12/   88]
per-ex loss: 0.382871  [   13/   88]
per-ex loss: 0.641548  [   14/   88]
per-ex loss: 0.559053  [   15/   88]
per-ex loss: 0.553178  [   16/   88]
per-ex loss: 0.584873  [   17/   88]
per-ex loss: 0.380473  [   18/   88]
per-ex loss: 0.448970  [   19/   88]
per-ex loss: 0.526588  [   20/   88]
per-ex loss: 0.529616  [   21/   88]
per-ex loss: 0.415652  [   22/   88]
per-ex loss: 0.355082  [   23/   88]
per-ex loss: 0.436533  [   24/   88]
per-ex loss: 0.338569  [   25/   88]
per-ex loss: 0.308265  [   26/   88]
per-ex loss: 0.362709  [   27/   88]
per-ex loss: 0.536229  [   28/   88]
per-ex loss: 0.500648  [   29/   88]
per-ex loss: 0.570035  [   30/   88]
per-ex loss: 0.550324  [   31/   88]
per-ex loss: 0.365890  [   32/   88]
per-ex loss: 0.658793  [   33/   88]
per-ex loss: 0.402182  [   34/   88]
per-ex loss: 0.534645  [   35/   88]
per-ex loss: 0.324840  [   36/   88]
per-ex loss: 0.513291  [   37/   88]
per-ex loss: 0.386592  [   38/   88]
per-ex loss: 0.530380  [   39/   88]
per-ex loss: 0.686183  [   40/   88]
per-ex loss: 0.368960  [   41/   88]
per-ex loss: 0.368155  [   42/   88]
per-ex loss: 0.511961  [   43/   88]
per-ex loss: 0.602546  [   44/   88]
per-ex loss: 0.612090  [   45/   88]
per-ex loss: 0.594085  [   46/   88]
per-ex loss: 0.389406  [   47/   88]
per-ex loss: 0.528980  [   48/   88]
per-ex loss: 0.413551  [   49/   88]
per-ex loss: 0.351332  [   50/   88]
per-ex loss: 0.561526  [   51/   88]
per-ex loss: 0.362467  [   52/   88]
per-ex loss: 0.533817  [   53/   88]
per-ex loss: 0.556534  [   54/   88]
per-ex loss: 0.540708  [   55/   88]
per-ex loss: 0.493093  [   56/   88]
per-ex loss: 0.404219  [   57/   88]
per-ex loss: 0.453017  [   58/   88]
per-ex loss: 0.402119  [   59/   88]
per-ex loss: 0.538969  [   60/   88]
per-ex loss: 0.409337  [   61/   88]
per-ex loss: 0.367467  [   62/   88]
per-ex loss: 0.385631  [   63/   88]
per-ex loss: 0.497303  [   64/   88]
per-ex loss: 0.359203  [   65/   88]
per-ex loss: 0.652615  [   66/   88]
per-ex loss: 0.312135  [   67/   88]
per-ex loss: 0.336189  [   68/   88]
per-ex loss: 0.490953  [   69/   88]
per-ex loss: 0.456935  [   70/   88]
per-ex loss: 0.371074  [   71/   88]
per-ex loss: 0.390566  [   72/   88]
per-ex loss: 0.428971  [   73/   88]
per-ex loss: 0.634732  [   74/   88]
per-ex loss: 0.366046  [   75/   88]
per-ex loss: 0.631626  [   76/   88]
per-ex loss: 0.345332  [   77/   88]
per-ex loss: 0.397227  [   78/   88]
per-ex loss: 0.350010  [   79/   88]
per-ex loss: 0.591415  [   80/   88]
per-ex loss: 0.645775  [   81/   88]
per-ex loss: 0.504512  [   82/   88]
per-ex loss: 0.426659  [   83/   88]
per-ex loss: 0.450140  [   84/   88]
per-ex loss: 0.570505  [   85/   88]
per-ex loss: 0.541110  [   86/   88]
per-ex loss: 0.369410  [   87/   88]
per-ex loss: 0.343572  [   88/   88]
Train Error: Avg loss: 0.45940393
validation Error: 
 Avg loss: 0.53361755 
 F1: 0.491303 
 Precision: 0.526048 
 Recall: 0.460863
 IoU: 0.325647

test Error: 
 Avg loss: 0.47732834 
 F1: 0.574285 
 Precision: 0.580441 
 Recall: 0.568259
 IoU: 0.402805

We have finished training iteration 94
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_82_.pth
per-ex loss: 0.491047  [    1/   88]
per-ex loss: 0.558238  [    2/   88]
per-ex loss: 0.422256  [    3/   88]
per-ex loss: 0.405189  [    4/   88]
per-ex loss: 0.353249  [    5/   88]
per-ex loss: 0.648180  [    6/   88]
per-ex loss: 0.692757  [    7/   88]
per-ex loss: 0.352102  [    8/   88]
per-ex loss: 0.524911  [    9/   88]
per-ex loss: 0.345909  [   10/   88]
per-ex loss: 0.655180  [   11/   88]
per-ex loss: 0.385723  [   12/   88]
per-ex loss: 0.330488  [   13/   88]
per-ex loss: 0.384547  [   14/   88]
per-ex loss: 0.538307  [   15/   88]
per-ex loss: 0.332033  [   16/   88]
per-ex loss: 0.394391  [   17/   88]
per-ex loss: 0.388346  [   18/   88]
per-ex loss: 0.587992  [   19/   88]
per-ex loss: 0.499801  [   20/   88]
per-ex loss: 0.393670  [   21/   88]
per-ex loss: 0.645993  [   22/   88]
per-ex loss: 0.430717  [   23/   88]
per-ex loss: 0.304649  [   24/   88]
per-ex loss: 0.393657  [   25/   88]
per-ex loss: 0.415415  [   26/   88]
per-ex loss: 0.339288  [   27/   88]
per-ex loss: 0.358568  [   28/   88]
per-ex loss: 0.566947  [   29/   88]
per-ex loss: 0.563429  [   30/   88]
per-ex loss: 0.527727  [   31/   88]
per-ex loss: 0.572223  [   32/   88]
per-ex loss: 0.482590  [   33/   88]
per-ex loss: 0.334928  [   34/   88]
per-ex loss: 0.583141  [   35/   88]
per-ex loss: 0.401007  [   36/   88]
per-ex loss: 0.559325  [   37/   88]
per-ex loss: 0.410866  [   38/   88]
per-ex loss: 0.376950  [   39/   88]
per-ex loss: 0.469109  [   40/   88]
per-ex loss: 0.412027  [   41/   88]
per-ex loss: 0.450438  [   42/   88]
per-ex loss: 0.315980  [   43/   88]
per-ex loss: 0.421630  [   44/   88]
per-ex loss: 0.494330  [   45/   88]
per-ex loss: 0.443375  [   46/   88]
per-ex loss: 0.476953  [   47/   88]
per-ex loss: 0.339558  [   48/   88]
per-ex loss: 0.565803  [   49/   88]
per-ex loss: 0.332250  [   50/   88]
per-ex loss: 0.394233  [   51/   88]
per-ex loss: 0.418088  [   52/   88]
per-ex loss: 0.566827  [   53/   88]
per-ex loss: 0.342489  [   54/   88]
per-ex loss: 0.576675  [   55/   88]
per-ex loss: 0.329553  [   56/   88]
per-ex loss: 0.629262  [   57/   88]
per-ex loss: 0.566356  [   58/   88]
per-ex loss: 0.339772  [   59/   88]
per-ex loss: 0.457040  [   60/   88]
per-ex loss: 0.530919  [   61/   88]
per-ex loss: 0.333338  [   62/   88]
per-ex loss: 0.485023  [   63/   88]
per-ex loss: 0.423547  [   64/   88]
per-ex loss: 0.361121  [   65/   88]
per-ex loss: 0.430922  [   66/   88]
per-ex loss: 0.461912  [   67/   88]
per-ex loss: 0.408058  [   68/   88]
per-ex loss: 0.507751  [   69/   88]
per-ex loss: 0.367485  [   70/   88]
per-ex loss: 0.436294  [   71/   88]
per-ex loss: 0.541252  [   72/   88]
per-ex loss: 0.685609  [   73/   88]
per-ex loss: 0.361020  [   74/   88]
per-ex loss: 0.369242  [   75/   88]
per-ex loss: 0.609853  [   76/   88]
per-ex loss: 0.397945  [   77/   88]
per-ex loss: 0.628817  [   78/   88]
per-ex loss: 0.590465  [   79/   88]
per-ex loss: 0.525106  [   80/   88]
per-ex loss: 0.516340  [   81/   88]
per-ex loss: 0.405407  [   82/   88]
per-ex loss: 0.374522  [   83/   88]
per-ex loss: 0.623541  [   84/   88]
per-ex loss: 0.594767  [   85/   88]
per-ex loss: 0.364899  [   86/   88]
per-ex loss: 0.270711  [   87/   88]
per-ex loss: 0.392664  [   88/   88]
Train Error: Avg loss: 0.45786387
validation Error: 
 Avg loss: 0.53218271 
 F1: 0.497088 
 Precision: 0.598288 
 Recall: 0.425171
 IoU: 0.330750

test Error: 
 Avg loss: 0.48128625 
 F1: 0.569222 
 Precision: 0.669827 
 Recall: 0.494892
 IoU: 0.397841

We have finished training iteration 95
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_93_.pth
per-ex loss: 0.448515  [    1/   88]
per-ex loss: 0.384923  [    2/   88]
per-ex loss: 0.622432  [    3/   88]
per-ex loss: 0.384818  [    4/   88]
per-ex loss: 0.372027  [    5/   88]
per-ex loss: 0.336372  [    6/   88]
per-ex loss: 0.509208  [    7/   88]
per-ex loss: 0.389106  [    8/   88]
per-ex loss: 0.550962  [    9/   88]
per-ex loss: 0.497249  [   10/   88]
per-ex loss: 0.570701  [   11/   88]
per-ex loss: 0.394166  [   12/   88]
per-ex loss: 0.449451  [   13/   88]
per-ex loss: 0.531661  [   14/   88]
per-ex loss: 0.483446  [   15/   88]
per-ex loss: 0.365043  [   16/   88]
per-ex loss: 0.342529  [   17/   88]
per-ex loss: 0.554092  [   18/   88]
per-ex loss: 0.379711  [   19/   88]
per-ex loss: 0.593244  [   20/   88]
per-ex loss: 0.566039  [   21/   88]
per-ex loss: 0.429643  [   22/   88]
per-ex loss: 0.536319  [   23/   88]
per-ex loss: 0.329613  [   24/   88]
per-ex loss: 0.445908  [   25/   88]
per-ex loss: 0.507272  [   26/   88]
per-ex loss: 0.344800  [   27/   88]
per-ex loss: 0.368981  [   28/   88]
per-ex loss: 0.340594  [   29/   88]
per-ex loss: 0.609342  [   30/   88]
per-ex loss: 0.479952  [   31/   88]
per-ex loss: 0.382257  [   32/   88]
per-ex loss: 0.333027  [   33/   88]
per-ex loss: 0.393028  [   34/   88]
per-ex loss: 0.545050  [   35/   88]
per-ex loss: 0.421920  [   36/   88]
per-ex loss: 0.461621  [   37/   88]
per-ex loss: 0.376361  [   38/   88]
per-ex loss: 0.625276  [   39/   88]
per-ex loss: 0.388636  [   40/   88]
per-ex loss: 0.434551  [   41/   88]
per-ex loss: 0.349891  [   42/   88]
per-ex loss: 0.650682  [   43/   88]
per-ex loss: 0.620967  [   44/   88]
per-ex loss: 0.571692  [   45/   88]
per-ex loss: 0.349631  [   46/   88]
per-ex loss: 0.420535  [   47/   88]
per-ex loss: 0.519760  [   48/   88]
per-ex loss: 0.441130  [   49/   88]
per-ex loss: 0.576781  [   50/   88]
per-ex loss: 0.314915  [   51/   88]
per-ex loss: 0.342083  [   52/   88]
per-ex loss: 0.535335  [   53/   88]
per-ex loss: 0.506067  [   54/   88]
per-ex loss: 0.639306  [   55/   88]
per-ex loss: 0.561993  [   56/   88]
per-ex loss: 0.404357  [   57/   88]
per-ex loss: 0.341442  [   58/   88]
per-ex loss: 0.293738  [   59/   88]
per-ex loss: 0.452795  [   60/   88]
per-ex loss: 0.366825  [   61/   88]
per-ex loss: 0.665892  [   62/   88]
per-ex loss: 0.373941  [   63/   88]
per-ex loss: 0.312460  [   64/   88]
per-ex loss: 0.313854  [   65/   88]
per-ex loss: 0.460024  [   66/   88]
per-ex loss: 0.372947  [   67/   88]
per-ex loss: 0.408673  [   68/   88]
per-ex loss: 0.341337  [   69/   88]
per-ex loss: 0.466961  [   70/   88]
per-ex loss: 0.520803  [   71/   88]
per-ex loss: 0.350723  [   72/   88]
per-ex loss: 0.399403  [   73/   88]
per-ex loss: 0.603136  [   74/   88]
per-ex loss: 0.311452  [   75/   88]
per-ex loss: 0.388901  [   76/   88]
per-ex loss: 0.597409  [   77/   88]
per-ex loss: 0.549601  [   78/   88]
per-ex loss: 0.351274  [   79/   88]
per-ex loss: 0.432361  [   80/   88]
per-ex loss: 0.378014  [   81/   88]
per-ex loss: 0.557951  [   82/   88]
per-ex loss: 0.644807  [   83/   88]
per-ex loss: 0.543537  [   84/   88]
per-ex loss: 0.350772  [   85/   88]
per-ex loss: 0.354734  [   86/   88]
per-ex loss: 0.520478  [   87/   88]
per-ex loss: 0.569870  [   88/   88]
Train Error: Avg loss: 0.45319420
validation Error: 
 Avg loss: 0.54486982 
 F1: 0.481786 
 Precision: 0.489693 
 Recall: 0.474132
 IoU: 0.317338

test Error: 
 Avg loss: 0.48529988 
 F1: 0.558004 
 Precision: 0.576271 
 Recall: 0.540859
 IoU: 0.386966

We have finished training iteration 96
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_94_.pth
per-ex loss: 0.350169  [    1/   88]
per-ex loss: 0.532189  [    2/   88]
per-ex loss: 0.389423  [    3/   88]
per-ex loss: 0.629142  [    4/   88]
per-ex loss: 0.460724  [    5/   88]
per-ex loss: 0.484109  [    6/   88]
per-ex loss: 0.333859  [    7/   88]
per-ex loss: 0.555338  [    8/   88]
per-ex loss: 0.319400  [    9/   88]
per-ex loss: 0.570932  [   10/   88]
per-ex loss: 0.330719  [   11/   88]
per-ex loss: 0.565283  [   12/   88]
per-ex loss: 0.383385  [   13/   88]
per-ex loss: 0.523293  [   14/   88]
per-ex loss: 0.400366  [   15/   88]
per-ex loss: 0.334101  [   16/   88]
per-ex loss: 0.350541  [   17/   88]
per-ex loss: 0.533937  [   18/   88]
per-ex loss: 0.467630  [   19/   88]
per-ex loss: 0.341880  [   20/   88]
per-ex loss: 0.478155  [   21/   88]
per-ex loss: 0.569350  [   22/   88]
per-ex loss: 0.418822  [   23/   88]
per-ex loss: 0.265595  [   24/   88]
per-ex loss: 0.313350  [   25/   88]
per-ex loss: 0.383211  [   26/   88]
per-ex loss: 0.416106  [   27/   88]
per-ex loss: 0.324796  [   28/   88]
per-ex loss: 0.479095  [   29/   88]
per-ex loss: 0.369321  [   30/   88]
per-ex loss: 0.617956  [   31/   88]
per-ex loss: 0.593292  [   32/   88]
per-ex loss: 0.342052  [   33/   88]
per-ex loss: 0.439829  [   34/   88]
per-ex loss: 0.605994  [   35/   88]
per-ex loss: 0.523406  [   36/   88]
per-ex loss: 0.381368  [   37/   88]
per-ex loss: 0.466573  [   38/   88]
per-ex loss: 0.396644  [   39/   88]
per-ex loss: 0.591202  [   40/   88]
per-ex loss: 0.324838  [   41/   88]
per-ex loss: 0.357203  [   42/   88]
per-ex loss: 0.339220  [   43/   88]
per-ex loss: 0.328582  [   44/   88]
per-ex loss: 0.422804  [   45/   88]
per-ex loss: 0.369216  [   46/   88]
per-ex loss: 0.615056  [   47/   88]
per-ex loss: 0.496242  [   48/   88]
per-ex loss: 0.375050  [   49/   88]
per-ex loss: 0.590597  [   50/   88]
per-ex loss: 0.587534  [   51/   88]
per-ex loss: 0.590078  [   52/   88]
per-ex loss: 0.645259  [   53/   88]
per-ex loss: 0.534029  [   54/   88]
per-ex loss: 0.460727  [   55/   88]
per-ex loss: 0.424630  [   56/   88]
per-ex loss: 0.400937  [   57/   88]
per-ex loss: 0.336921  [   58/   88]
per-ex loss: 0.406173  [   59/   88]
per-ex loss: 0.426829  [   60/   88]
per-ex loss: 0.519010  [   61/   88]
per-ex loss: 0.407779  [   62/   88]
per-ex loss: 0.438136  [   63/   88]
per-ex loss: 0.561679  [   64/   88]
per-ex loss: 0.419824  [   65/   88]
per-ex loss: 0.414688  [   66/   88]
per-ex loss: 0.576988  [   67/   88]
per-ex loss: 0.574180  [   68/   88]
per-ex loss: 0.457827  [   69/   88]
per-ex loss: 0.508726  [   70/   88]
per-ex loss: 0.690594  [   71/   88]
per-ex loss: 0.315576  [   72/   88]
per-ex loss: 0.421050  [   73/   88]
per-ex loss: 0.581941  [   74/   88]
per-ex loss: 0.403933  [   75/   88]
per-ex loss: 0.431876  [   76/   88]
per-ex loss: 0.492892  [   77/   88]
per-ex loss: 0.336768  [   78/   88]
per-ex loss: 0.425628  [   79/   88]
per-ex loss: 0.372528  [   80/   88]
per-ex loss: 0.587852  [   81/   88]
per-ex loss: 0.375969  [   82/   88]
per-ex loss: 0.618165  [   83/   88]
per-ex loss: 0.572835  [   84/   88]
per-ex loss: 0.331862  [   85/   88]
per-ex loss: 0.385507  [   86/   88]
per-ex loss: 0.687016  [   87/   88]
per-ex loss: 0.360852  [   88/   88]
Train Error: Avg loss: 0.45609250
validation Error: 
 Avg loss: 0.53540384 
 F1: 0.491328 
 Precision: 0.602858 
 Recall: 0.414622
 IoU: 0.325669

test Error: 
 Avg loss: 0.48179317 
 F1: 0.571777 
 Precision: 0.657705 
 Recall: 0.505708
 IoU: 0.400342

We have finished training iteration 97
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_95_.pth
per-ex loss: 0.606292  [    1/   88]
per-ex loss: 0.372146  [    2/   88]
per-ex loss: 0.344449  [    3/   88]
per-ex loss: 0.541162  [    4/   88]
per-ex loss: 0.487742  [    5/   88]
per-ex loss: 0.554501  [    6/   88]
per-ex loss: 0.530200  [    7/   88]
per-ex loss: 0.652636  [    8/   88]
per-ex loss: 0.548509  [    9/   88]
per-ex loss: 0.635236  [   10/   88]
per-ex loss: 0.601768  [   11/   88]
per-ex loss: 0.309176  [   12/   88]
per-ex loss: 0.388902  [   13/   88]
per-ex loss: 0.631616  [   14/   88]
per-ex loss: 0.517322  [   15/   88]
per-ex loss: 0.353237  [   16/   88]
per-ex loss: 0.536654  [   17/   88]
per-ex loss: 0.413053  [   18/   88]
per-ex loss: 0.576869  [   19/   88]
per-ex loss: 0.460269  [   20/   88]
per-ex loss: 0.494008  [   21/   88]
per-ex loss: 0.542434  [   22/   88]
per-ex loss: 0.302229  [   23/   88]
per-ex loss: 0.401944  [   24/   88]
per-ex loss: 0.605615  [   25/   88]
per-ex loss: 0.396520  [   26/   88]
per-ex loss: 0.599889  [   27/   88]
per-ex loss: 0.770839  [   28/   88]
per-ex loss: 0.502250  [   29/   88]
per-ex loss: 0.561960  [   30/   88]
per-ex loss: 0.406398  [   31/   88]
per-ex loss: 0.585050  [   32/   88]
per-ex loss: 0.412088  [   33/   88]
per-ex loss: 0.387108  [   34/   88]
per-ex loss: 0.488461  [   35/   88]
per-ex loss: 0.388508  [   36/   88]
per-ex loss: 0.432898  [   37/   88]
per-ex loss: 0.339555  [   38/   88]
per-ex loss: 0.381217  [   39/   88]
per-ex loss: 0.374215  [   40/   88]
per-ex loss: 0.307055  [   41/   88]
per-ex loss: 0.333513  [   42/   88]
per-ex loss: 0.330372  [   43/   88]
per-ex loss: 0.437949  [   44/   88]
per-ex loss: 0.645724  [   45/   88]
per-ex loss: 0.561169  [   46/   88]
per-ex loss: 0.418895  [   47/   88]
per-ex loss: 0.405846  [   48/   88]
per-ex loss: 0.326382  [   49/   88]
per-ex loss: 0.406893  [   50/   88]
per-ex loss: 0.371619  [   51/   88]
per-ex loss: 0.262567  [   52/   88]
per-ex loss: 0.335562  [   53/   88]
per-ex loss: 0.390639  [   54/   88]
per-ex loss: 0.394006  [   55/   88]
per-ex loss: 0.487159  [   56/   88]
per-ex loss: 0.638536  [   57/   88]
per-ex loss: 0.615482  [   58/   88]
per-ex loss: 0.316120  [   59/   88]
per-ex loss: 0.319478  [   60/   88]
per-ex loss: 0.411292  [   61/   88]
per-ex loss: 0.362182  [   62/   88]
per-ex loss: 0.491863  [   63/   88]
per-ex loss: 0.366489  [   64/   88]
per-ex loss: 0.389243  [   65/   88]
per-ex loss: 0.551684  [   66/   88]
per-ex loss: 0.489499  [   67/   88]
per-ex loss: 0.408169  [   68/   88]
per-ex loss: 0.497004  [   69/   88]
per-ex loss: 0.531269  [   70/   88]
per-ex loss: 0.428353  [   71/   88]
per-ex loss: 0.412126  [   72/   88]
per-ex loss: 0.426722  [   73/   88]
per-ex loss: 0.495377  [   74/   88]
per-ex loss: 0.340862  [   75/   88]
per-ex loss: 0.465356  [   76/   88]
per-ex loss: 0.355756  [   77/   88]
per-ex loss: 0.586885  [   78/   88]
per-ex loss: 0.445189  [   79/   88]
per-ex loss: 0.521970  [   80/   88]
per-ex loss: 0.580967  [   81/   88]
per-ex loss: 0.380108  [   82/   88]
per-ex loss: 0.367570  [   83/   88]
per-ex loss: 0.531073  [   84/   88]
per-ex loss: 0.390285  [   85/   88]
per-ex loss: 0.389740  [   86/   88]
per-ex loss: 0.508197  [   87/   88]
per-ex loss: 0.367649  [   88/   88]
Train Error: Avg loss: 0.45719064
validation Error: 
 Avg loss: 0.54551191 
 F1: 0.483395 
 Precision: 0.551122 
 Recall: 0.430492
 IoU: 0.318735

test Error: 
 Avg loss: 0.48728692 
 F1: 0.562197 
 Precision: 0.633115 
 Recall: 0.505566
 IoU: 0.391011

We have finished training iteration 98
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_96_.pth
per-ex loss: 0.421004  [    1/   88]
per-ex loss: 0.506593  [    2/   88]
per-ex loss: 0.236898  [    3/   88]
per-ex loss: 0.438425  [    4/   88]
per-ex loss: 0.402021  [    5/   88]
per-ex loss: 0.590768  [    6/   88]
per-ex loss: 0.593121  [    7/   88]
per-ex loss: 0.388543  [    8/   88]
per-ex loss: 0.553758  [    9/   88]
per-ex loss: 0.342576  [   10/   88]
per-ex loss: 0.515051  [   11/   88]
per-ex loss: 0.452290  [   12/   88]
per-ex loss: 0.379607  [   13/   88]
per-ex loss: 0.357118  [   14/   88]
per-ex loss: 0.498407  [   15/   88]
per-ex loss: 0.423533  [   16/   88]
per-ex loss: 0.368626  [   17/   88]
per-ex loss: 0.395326  [   18/   88]
per-ex loss: 0.567515  [   19/   88]
per-ex loss: 0.474584  [   20/   88]
per-ex loss: 0.373761  [   21/   88]
per-ex loss: 0.575871  [   22/   88]
per-ex loss: 0.326743  [   23/   88]
per-ex loss: 0.611001  [   24/   88]
per-ex loss: 0.558861  [   25/   88]
per-ex loss: 0.501883  [   26/   88]
per-ex loss: 0.511819  [   27/   88]
per-ex loss: 0.362515  [   28/   88]
per-ex loss: 0.390175  [   29/   88]
per-ex loss: 0.538145  [   30/   88]
per-ex loss: 0.420410  [   31/   88]
per-ex loss: 0.635128  [   32/   88]
per-ex loss: 0.505601  [   33/   88]
per-ex loss: 0.299841  [   34/   88]
per-ex loss: 0.626160  [   35/   88]
per-ex loss: 0.536454  [   36/   88]
per-ex loss: 0.428621  [   37/   88]
per-ex loss: 0.599268  [   38/   88]
per-ex loss: 0.414675  [   39/   88]
per-ex loss: 0.553713  [   40/   88]
per-ex loss: 0.585374  [   41/   88]
per-ex loss: 0.567747  [   42/   88]
per-ex loss: 0.547178  [   43/   88]
per-ex loss: 0.371013  [   44/   88]
per-ex loss: 0.519856  [   45/   88]
per-ex loss: 0.323218  [   46/   88]
per-ex loss: 0.376179  [   47/   88]
per-ex loss: 0.430812  [   48/   88]
per-ex loss: 0.503549  [   49/   88]
per-ex loss: 0.384722  [   50/   88]
per-ex loss: 0.350127  [   51/   88]
per-ex loss: 0.356029  [   52/   88]
per-ex loss: 0.633695  [   53/   88]
per-ex loss: 0.559294  [   54/   88]
per-ex loss: 0.408072  [   55/   88]
per-ex loss: 0.484392  [   56/   88]
per-ex loss: 0.476105  [   57/   88]
per-ex loss: 0.481795  [   58/   88]
per-ex loss: 0.418357  [   59/   88]
per-ex loss: 0.655647  [   60/   88]
per-ex loss: 0.638316  [   61/   88]
per-ex loss: 0.310360  [   62/   88]
per-ex loss: 0.344305  [   63/   88]
per-ex loss: 0.422917  [   64/   88]
per-ex loss: 0.545070  [   65/   88]
per-ex loss: 0.355195  [   66/   88]
per-ex loss: 0.416665  [   67/   88]
per-ex loss: 0.658472  [   68/   88]
per-ex loss: 0.587891  [   69/   88]
per-ex loss: 0.394860  [   70/   88]
per-ex loss: 0.400400  [   71/   88]
per-ex loss: 0.447055  [   72/   88]
per-ex loss: 0.544880  [   73/   88]
per-ex loss: 0.363669  [   74/   88]
per-ex loss: 0.422160  [   75/   88]
per-ex loss: 0.468184  [   76/   88]
per-ex loss: 0.356193  [   77/   88]
per-ex loss: 0.297438  [   78/   88]
per-ex loss: 0.446719  [   79/   88]
per-ex loss: 0.383425  [   80/   88]
per-ex loss: 0.318089  [   81/   88]
per-ex loss: 0.359052  [   82/   88]
per-ex loss: 0.420217  [   83/   88]
per-ex loss: 0.334917  [   84/   88]
per-ex loss: 0.395193  [   85/   88]
per-ex loss: 0.332576  [   86/   88]
per-ex loss: 0.342223  [   87/   88]
per-ex loss: 0.681484  [   88/   88]
Train Error: Avg loss: 0.45565336
validation Error: 
 Avg loss: 0.54439753 
 F1: 0.482344 
 Precision: 0.593654 
 Recall: 0.406184
 IoU: 0.317822

test Error: 
 Avg loss: 0.49351282 
 F1: 0.555911 
 Precision: 0.666148 
 Recall: 0.476979
 IoU: 0.384956

We have finished training iteration 99
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_97_.pth
per-ex loss: 0.336141  [    1/   88]
per-ex loss: 0.396210  [    2/   88]
per-ex loss: 0.363199  [    3/   88]
per-ex loss: 0.524476  [    4/   88]
per-ex loss: 0.428767  [    5/   88]
per-ex loss: 0.343696  [    6/   88]
per-ex loss: 0.480710  [    7/   88]
per-ex loss: 0.520587  [    8/   88]
per-ex loss: 0.353402  [    9/   88]
per-ex loss: 0.385166  [   10/   88]
per-ex loss: 0.406648  [   11/   88]
per-ex loss: 0.367539  [   12/   88]
per-ex loss: 0.538212  [   13/   88]
per-ex loss: 0.651804  [   14/   88]
per-ex loss: 0.532168  [   15/   88]
per-ex loss: 0.410322  [   16/   88]
per-ex loss: 0.403198  [   17/   88]
per-ex loss: 0.531609  [   18/   88]
per-ex loss: 0.582691  [   19/   88]
per-ex loss: 0.479833  [   20/   88]
per-ex loss: 0.649732  [   21/   88]
per-ex loss: 0.334719  [   22/   88]
per-ex loss: 0.336090  [   23/   88]
per-ex loss: 0.351975  [   24/   88]
per-ex loss: 0.680654  [   25/   88]
per-ex loss: 0.534015  [   26/   88]
per-ex loss: 0.467086  [   27/   88]
per-ex loss: 0.351405  [   28/   88]
per-ex loss: 0.458382  [   29/   88]
per-ex loss: 0.428781  [   30/   88]
per-ex loss: 0.528157  [   31/   88]
per-ex loss: 0.369603  [   32/   88]
per-ex loss: 0.654055  [   33/   88]
per-ex loss: 0.348026  [   34/   88]
per-ex loss: 0.390558  [   35/   88]
per-ex loss: 0.576389  [   36/   88]
per-ex loss: 0.304361  [   37/   88]
per-ex loss: 0.375646  [   38/   88]
per-ex loss: 0.510631  [   39/   88]
per-ex loss: 0.611779  [   40/   88]
per-ex loss: 0.435905  [   41/   88]
per-ex loss: 0.415728  [   42/   88]
per-ex loss: 0.618955  [   43/   88]
per-ex loss: 0.435448  [   44/   88]
per-ex loss: 0.372430  [   45/   88]
per-ex loss: 0.572983  [   46/   88]
per-ex loss: 0.373716  [   47/   88]
per-ex loss: 0.348336  [   48/   88]
per-ex loss: 0.547025  [   49/   88]
per-ex loss: 0.384592  [   50/   88]
per-ex loss: 0.406291  [   51/   88]
per-ex loss: 0.518453  [   52/   88]
per-ex loss: 0.338690  [   53/   88]
per-ex loss: 0.391350  [   54/   88]
per-ex loss: 0.375904  [   55/   88]
per-ex loss: 0.384684  [   56/   88]
per-ex loss: 0.345676  [   57/   88]
per-ex loss: 0.360594  [   58/   88]
per-ex loss: 0.546579  [   59/   88]
per-ex loss: 0.389031  [   60/   88]
per-ex loss: 0.587690  [   61/   88]
per-ex loss: 0.300271  [   62/   88]
per-ex loss: 0.256690  [   63/   88]
per-ex loss: 0.344113  [   64/   88]
per-ex loss: 0.563765  [   65/   88]
per-ex loss: 0.477161  [   66/   88]
per-ex loss: 0.615390  [   67/   88]
per-ex loss: 0.610030  [   68/   88]
per-ex loss: 0.579021  [   69/   88]
per-ex loss: 0.446242  [   70/   88]
per-ex loss: 0.613133  [   71/   88]
per-ex loss: 0.467328  [   72/   88]
per-ex loss: 0.538142  [   73/   88]
per-ex loss: 0.490026  [   74/   88]
per-ex loss: 0.558451  [   75/   88]
per-ex loss: 0.565246  [   76/   88]
per-ex loss: 0.548162  [   77/   88]
per-ex loss: 0.391842  [   78/   88]
per-ex loss: 0.400289  [   79/   88]
per-ex loss: 0.380513  [   80/   88]
per-ex loss: 0.388376  [   81/   88]
per-ex loss: 0.634764  [   82/   88]
per-ex loss: 0.570889  [   83/   88]
per-ex loss: 0.426758  [   84/   88]
per-ex loss: 0.338485  [   85/   88]
per-ex loss: 0.482998  [   86/   88]
per-ex loss: 0.420803  [   87/   88]
per-ex loss: 0.345100  [   88/   88]
Train Error: Avg loss: 0.45684624
validation Error: 
 Avg loss: 0.53398942 
 F1: 0.495623 
 Precision: 0.581409 
 Recall: 0.431898
 IoU: 0.329454

test Error: 
 Avg loss: 0.48521336 
 F1: 0.564744 
 Precision: 0.633067 
 Recall: 0.509732
 IoU: 0.393479

We have finished training iteration 100
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_98_.pth
per-ex loss: 0.386623  [    1/   88]
per-ex loss: 0.553035  [    2/   88]
per-ex loss: 0.480904  [    3/   88]
per-ex loss: 0.430549  [    4/   88]
per-ex loss: 0.514362  [    5/   88]
per-ex loss: 0.469401  [    6/   88]
per-ex loss: 0.662376  [    7/   88]
per-ex loss: 0.416607  [    8/   88]
per-ex loss: 0.328498  [    9/   88]
per-ex loss: 0.314413  [   10/   88]
per-ex loss: 0.386178  [   11/   88]
per-ex loss: 0.566255  [   12/   88]
per-ex loss: 0.433694  [   13/   88]
per-ex loss: 0.448872  [   14/   88]
per-ex loss: 0.338742  [   15/   88]
per-ex loss: 0.400685  [   16/   88]
per-ex loss: 0.569536  [   17/   88]
per-ex loss: 0.348881  [   18/   88]
per-ex loss: 0.489772  [   19/   88]
per-ex loss: 0.383146  [   20/   88]
per-ex loss: 0.369029  [   21/   88]
per-ex loss: 0.377860  [   22/   88]
per-ex loss: 0.546188  [   23/   88]
per-ex loss: 0.507663  [   24/   88]
per-ex loss: 0.365605  [   25/   88]
per-ex loss: 0.573141  [   26/   88]
per-ex loss: 0.443330  [   27/   88]
per-ex loss: 0.404910  [   28/   88]
per-ex loss: 0.434544  [   29/   88]
per-ex loss: 0.300860  [   30/   88]
per-ex loss: 0.338746  [   31/   88]
per-ex loss: 0.348401  [   32/   88]
per-ex loss: 0.545738  [   33/   88]
per-ex loss: 0.394054  [   34/   88]
per-ex loss: 0.609795  [   35/   88]
per-ex loss: 0.564794  [   36/   88]
per-ex loss: 0.554828  [   37/   88]
per-ex loss: 0.491346  [   38/   88]
per-ex loss: 0.588087  [   39/   88]
per-ex loss: 0.529362  [   40/   88]
per-ex loss: 0.576779  [   41/   88]
per-ex loss: 0.650267  [   42/   88]
per-ex loss: 0.314557  [   43/   88]
per-ex loss: 0.589619  [   44/   88]
per-ex loss: 0.408986  [   45/   88]
per-ex loss: 0.332895  [   46/   88]
per-ex loss: 0.530154  [   47/   88]
per-ex loss: 0.361863  [   48/   88]
per-ex loss: 0.396591  [   49/   88]
per-ex loss: 0.407887  [   50/   88]
per-ex loss: 0.362162  [   51/   88]
per-ex loss: 0.385559  [   52/   88]
per-ex loss: 0.376989  [   53/   88]
per-ex loss: 0.539786  [   54/   88]
per-ex loss: 0.564632  [   55/   88]
per-ex loss: 0.250331  [   56/   88]
per-ex loss: 0.459201  [   57/   88]
per-ex loss: 0.574006  [   58/   88]
per-ex loss: 0.385303  [   59/   88]
per-ex loss: 0.585100  [   60/   88]
per-ex loss: 0.462836  [   61/   88]
per-ex loss: 0.365554  [   62/   88]
per-ex loss: 0.366345  [   63/   88]
per-ex loss: 0.388180  [   64/   88]
per-ex loss: 0.370419  [   65/   88]
per-ex loss: 0.476307  [   66/   88]
per-ex loss: 0.304236  [   67/   88]
per-ex loss: 0.401545  [   68/   88]
per-ex loss: 0.537778  [   69/   88]
per-ex loss: 0.314260  [   70/   88]
per-ex loss: 0.492926  [   71/   88]
per-ex loss: 0.455651  [   72/   88]
per-ex loss: 0.570759  [   73/   88]
per-ex loss: 0.515430  [   74/   88]
per-ex loss: 0.384566  [   75/   88]
per-ex loss: 0.641235  [   76/   88]
per-ex loss: 0.369719  [   77/   88]
per-ex loss: 0.627933  [   78/   88]
per-ex loss: 0.612678  [   79/   88]
per-ex loss: 0.423877  [   80/   88]
per-ex loss: 0.358882  [   81/   88]
per-ex loss: 0.363753  [   82/   88]
per-ex loss: 0.623217  [   83/   88]
per-ex loss: 0.364431  [   84/   88]
per-ex loss: 0.539631  [   85/   88]
per-ex loss: 0.306704  [   86/   88]
per-ex loss: 0.627185  [   87/   88]
per-ex loss: 0.402562  [   88/   88]
Train Error: Avg loss: 0.45377360
validation Error: 
 Avg loss: 0.52916299 
 F1: 0.500877 
 Precision: 0.595059 
 Recall: 0.432434
 IoU: 0.334113

test Error: 
 Avg loss: 0.48549301 
 F1: 0.563873 
 Precision: 0.661990 
 Recall: 0.491086
 IoU: 0.392634

We have finished training iteration 101
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_99_.pth
per-ex loss: 0.581982  [    1/   88]
per-ex loss: 0.361535  [    2/   88]
per-ex loss: 0.428610  [    3/   88]
per-ex loss: 0.571331  [    4/   88]
per-ex loss: 0.436693  [    5/   88]
per-ex loss: 0.556083  [    6/   88]
per-ex loss: 0.554001  [    7/   88]
per-ex loss: 0.527331  [    8/   88]
per-ex loss: 0.409692  [    9/   88]
per-ex loss: 0.561952  [   10/   88]
per-ex loss: 0.334581  [   11/   88]
per-ex loss: 0.419195  [   12/   88]
per-ex loss: 0.666416  [   13/   88]
per-ex loss: 0.339714  [   14/   88]
per-ex loss: 0.355196  [   15/   88]
per-ex loss: 0.380470  [   16/   88]
per-ex loss: 0.630623  [   17/   88]
per-ex loss: 0.326176  [   18/   88]
per-ex loss: 0.395104  [   19/   88]
per-ex loss: 0.331129  [   20/   88]
per-ex loss: 0.368033  [   21/   88]
per-ex loss: 0.584113  [   22/   88]
per-ex loss: 0.587969  [   23/   88]
per-ex loss: 0.579764  [   24/   88]
per-ex loss: 0.563306  [   25/   88]
per-ex loss: 0.656301  [   26/   88]
per-ex loss: 0.337702  [   27/   88]
per-ex loss: 0.557826  [   28/   88]
per-ex loss: 0.427355  [   29/   88]
per-ex loss: 0.331575  [   30/   88]
per-ex loss: 0.351965  [   31/   88]
per-ex loss: 0.400519  [   32/   88]
per-ex loss: 0.473299  [   33/   88]
per-ex loss: 0.553373  [   34/   88]
per-ex loss: 0.468401  [   35/   88]
per-ex loss: 0.335307  [   36/   88]
per-ex loss: 0.403350  [   37/   88]
per-ex loss: 0.364519  [   38/   88]
per-ex loss: 0.501268  [   39/   88]
per-ex loss: 0.482266  [   40/   88]
per-ex loss: 0.368977  [   41/   88]
per-ex loss: 0.374928  [   42/   88]
per-ex loss: 0.308501  [   43/   88]
per-ex loss: 0.606421  [   44/   88]
per-ex loss: 0.644164  [   45/   88]
per-ex loss: 0.392865  [   46/   88]
per-ex loss: 0.639974  [   47/   88]
per-ex loss: 0.470353  [   48/   88]
per-ex loss: 0.426743  [   49/   88]
per-ex loss: 0.616116  [   50/   88]
per-ex loss: 0.549792  [   51/   88]
per-ex loss: 0.480056  [   52/   88]
per-ex loss: 0.409703  [   53/   88]
per-ex loss: 0.370650  [   54/   88]
per-ex loss: 0.420159  [   55/   88]
per-ex loss: 0.429041  [   56/   88]
per-ex loss: 0.500969  [   57/   88]
per-ex loss: 0.646207  [   58/   88]
per-ex loss: 0.373142  [   59/   88]
per-ex loss: 0.432762  [   60/   88]
per-ex loss: 0.529424  [   61/   88]
per-ex loss: 0.371926  [   62/   88]
per-ex loss: 0.322549  [   63/   88]
per-ex loss: 0.354413  [   64/   88]
per-ex loss: 0.346157  [   65/   88]
per-ex loss: 0.458114  [   66/   88]
per-ex loss: 0.389866  [   67/   88]
per-ex loss: 0.553568  [   68/   88]
per-ex loss: 0.542709  [   69/   88]
per-ex loss: 0.417048  [   70/   88]
per-ex loss: 0.388448  [   71/   88]
per-ex loss: 0.417057  [   72/   88]
per-ex loss: 0.537890  [   73/   88]
per-ex loss: 0.457193  [   74/   88]
per-ex loss: 0.501764  [   75/   88]
per-ex loss: 0.377352  [   76/   88]
per-ex loss: 0.376573  [   77/   88]
per-ex loss: 0.350836  [   78/   88]
per-ex loss: 0.396033  [   79/   88]
per-ex loss: 0.480533  [   80/   88]
per-ex loss: 0.319838  [   81/   88]
per-ex loss: 0.311217  [   82/   88]
per-ex loss: 0.331421  [   83/   88]
per-ex loss: 0.590875  [   84/   88]
per-ex loss: 0.331845  [   85/   88]
per-ex loss: 0.554482  [   86/   88]
per-ex loss: 0.477722  [   87/   88]
per-ex loss: 0.540878  [   88/   88]
Train Error: Avg loss: 0.45437816
validation Error: 
 Avg loss: 0.53645771 
 F1: 0.496442 
 Precision: 0.615702 
 Recall: 0.415886
 IoU: 0.330178

test Error: 
 Avg loss: 0.48450069 
 F1: 0.564209 
 Precision: 0.670833 
 Recall: 0.486831
 IoU: 0.392960

We have finished training iteration 102
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_100_.pth
per-ex loss: 0.475982  [    1/   88]
per-ex loss: 0.521179  [    2/   88]
per-ex loss: 0.594410  [    3/   88]
per-ex loss: 0.516130  [    4/   88]
per-ex loss: 0.357476  [    5/   88]
per-ex loss: 0.379755  [    6/   88]
per-ex loss: 0.651377  [    7/   88]
per-ex loss: 0.581925  [    8/   88]
per-ex loss: 0.410852  [    9/   88]
per-ex loss: 0.383493  [   10/   88]
per-ex loss: 0.374072  [   11/   88]
per-ex loss: 0.448962  [   12/   88]
per-ex loss: 0.387666  [   13/   88]
per-ex loss: 0.650886  [   14/   88]
per-ex loss: 0.329258  [   15/   88]
per-ex loss: 0.328924  [   16/   88]
per-ex loss: 0.254186  [   17/   88]
per-ex loss: 0.338204  [   18/   88]
per-ex loss: 0.376103  [   19/   88]
per-ex loss: 0.453594  [   20/   88]
per-ex loss: 0.432317  [   21/   88]
per-ex loss: 0.511533  [   22/   88]
per-ex loss: 0.521917  [   23/   88]
per-ex loss: 0.624318  [   24/   88]
per-ex loss: 0.513325  [   25/   88]
per-ex loss: 0.564306  [   26/   88]
per-ex loss: 0.541593  [   27/   88]
per-ex loss: 0.356454  [   28/   88]
per-ex loss: 0.375933  [   29/   88]
per-ex loss: 0.446142  [   30/   88]
per-ex loss: 0.459096  [   31/   88]
per-ex loss: 0.335024  [   32/   88]
per-ex loss: 0.584070  [   33/   88]
per-ex loss: 0.589855  [   34/   88]
per-ex loss: 0.423944  [   35/   88]
per-ex loss: 0.389444  [   36/   88]
per-ex loss: 0.400671  [   37/   88]
per-ex loss: 0.354717  [   38/   88]
per-ex loss: 0.583512  [   39/   88]
per-ex loss: 0.334422  [   40/   88]
per-ex loss: 0.419555  [   41/   88]
per-ex loss: 0.388257  [   42/   88]
per-ex loss: 0.373292  [   43/   88]
per-ex loss: 0.554529  [   44/   88]
per-ex loss: 0.400986  [   45/   88]
per-ex loss: 0.498026  [   46/   88]
per-ex loss: 0.586614  [   47/   88]
per-ex loss: 0.563767  [   48/   88]
per-ex loss: 0.410051  [   49/   88]
per-ex loss: 0.299630  [   50/   88]
per-ex loss: 0.552601  [   51/   88]
per-ex loss: 0.607092  [   52/   88]
per-ex loss: 0.390576  [   53/   88]
per-ex loss: 0.375195  [   54/   88]
per-ex loss: 0.372335  [   55/   88]
per-ex loss: 0.400690  [   56/   88]
per-ex loss: 0.572165  [   57/   88]
per-ex loss: 0.443937  [   58/   88]
per-ex loss: 0.319910  [   59/   88]
per-ex loss: 0.336559  [   60/   88]
per-ex loss: 0.506329  [   61/   88]
per-ex loss: 0.555540  [   62/   88]
per-ex loss: 0.372581  [   63/   88]
per-ex loss: 0.681633  [   64/   88]
per-ex loss: 0.638093  [   65/   88]
per-ex loss: 0.444555  [   66/   88]
per-ex loss: 0.492598  [   67/   88]
per-ex loss: 0.346673  [   68/   88]
per-ex loss: 0.486976  [   69/   88]
per-ex loss: 0.502903  [   70/   88]
per-ex loss: 0.609879  [   71/   88]
per-ex loss: 0.423923  [   72/   88]
per-ex loss: 0.537729  [   73/   88]
per-ex loss: 0.309697  [   74/   88]
per-ex loss: 0.333143  [   75/   88]
per-ex loss: 0.324678  [   76/   88]
per-ex loss: 0.432463  [   77/   88]
per-ex loss: 0.415980  [   78/   88]
per-ex loss: 0.328701  [   79/   88]
per-ex loss: 0.582130  [   80/   88]
per-ex loss: 0.376711  [   81/   88]
per-ex loss: 0.673536  [   82/   88]
per-ex loss: 0.394764  [   83/   88]
per-ex loss: 0.537877  [   84/   88]
per-ex loss: 0.379373  [   85/   88]
per-ex loss: 0.538159  [   86/   88]
per-ex loss: 0.383881  [   87/   88]
per-ex loss: 0.418633  [   88/   88]
Train Error: Avg loss: 0.45513561
validation Error: 
 Avg loss: 0.54181635 
 F1: 0.487193 
 Precision: 0.495044 
 Recall: 0.479588
 IoU: 0.322046

test Error: 
 Avg loss: 0.48120575 
 F1: 0.567976 
 Precision: 0.575330 
 Recall: 0.560808
 IoU: 0.396625

We have finished training iteration 103
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_101_.pth
per-ex loss: 0.425665  [    1/   88]
per-ex loss: 0.532803  [    2/   88]
per-ex loss: 0.650369  [    3/   88]
per-ex loss: 0.331876  [    4/   88]
per-ex loss: 0.454478  [    5/   88]
per-ex loss: 0.398185  [    6/   88]
per-ex loss: 0.445864  [    7/   88]
per-ex loss: 0.550062  [    8/   88]
per-ex loss: 0.372071  [    9/   88]
per-ex loss: 0.459258  [   10/   88]
per-ex loss: 0.423696  [   11/   88]
per-ex loss: 0.564577  [   12/   88]
per-ex loss: 0.408697  [   13/   88]
per-ex loss: 0.452423  [   14/   88]
per-ex loss: 0.333349  [   15/   88]
per-ex loss: 0.417199  [   16/   88]
per-ex loss: 0.556330  [   17/   88]
per-ex loss: 0.384239  [   18/   88]
per-ex loss: 0.345850  [   19/   88]
per-ex loss: 0.369622  [   20/   88]
per-ex loss: 0.448366  [   21/   88]
per-ex loss: 0.309765  [   22/   88]
per-ex loss: 0.440007  [   23/   88]
per-ex loss: 0.358880  [   24/   88]
per-ex loss: 0.622057  [   25/   88]
per-ex loss: 0.517412  [   26/   88]
per-ex loss: 0.536612  [   27/   88]
per-ex loss: 0.327082  [   28/   88]
per-ex loss: 0.546196  [   29/   88]
per-ex loss: 0.474988  [   30/   88]
per-ex loss: 0.552267  [   31/   88]
per-ex loss: 0.409206  [   32/   88]
per-ex loss: 0.362789  [   33/   88]
per-ex loss: 0.393265  [   34/   88]
per-ex loss: 0.410287  [   35/   88]
per-ex loss: 0.451591  [   36/   88]
per-ex loss: 0.444840  [   37/   88]
per-ex loss: 0.580747  [   38/   88]
per-ex loss: 0.386996  [   39/   88]
per-ex loss: 0.477526  [   40/   88]
per-ex loss: 0.299875  [   41/   88]
per-ex loss: 0.532088  [   42/   88]
per-ex loss: 0.333006  [   43/   88]
per-ex loss: 0.557852  [   44/   88]
per-ex loss: 0.441869  [   45/   88]
per-ex loss: 0.507545  [   46/   88]
per-ex loss: 0.613795  [   47/   88]
per-ex loss: 0.383814  [   48/   88]
per-ex loss: 0.619820  [   49/   88]
per-ex loss: 0.586021  [   50/   88]
per-ex loss: 0.460965  [   51/   88]
per-ex loss: 0.719230  [   52/   88]
per-ex loss: 0.417170  [   53/   88]
per-ex loss: 0.582008  [   54/   88]
per-ex loss: 0.481641  [   55/   88]
per-ex loss: 0.347171  [   56/   88]
per-ex loss: 0.366425  [   57/   88]
per-ex loss: 0.639589  [   58/   88]
per-ex loss: 0.368494  [   59/   88]
per-ex loss: 0.437736  [   60/   88]
per-ex loss: 0.357499  [   61/   88]
per-ex loss: 0.340213  [   62/   88]
per-ex loss: 0.540715  [   63/   88]
per-ex loss: 0.482834  [   64/   88]
per-ex loss: 0.408964  [   65/   88]
per-ex loss: 0.390930  [   66/   88]
per-ex loss: 0.385042  [   67/   88]
per-ex loss: 0.583996  [   68/   88]
per-ex loss: 0.494612  [   69/   88]
per-ex loss: 0.362105  [   70/   88]
per-ex loss: 0.405539  [   71/   88]
per-ex loss: 0.510954  [   72/   88]
per-ex loss: 0.355789  [   73/   88]
per-ex loss: 0.531219  [   74/   88]
per-ex loss: 0.600998  [   75/   88]
per-ex loss: 0.392711  [   76/   88]
per-ex loss: 0.567071  [   77/   88]
per-ex loss: 0.401237  [   78/   88]
per-ex loss: 0.428496  [   79/   88]
per-ex loss: 0.531492  [   80/   88]
per-ex loss: 0.412549  [   81/   88]
per-ex loss: 0.545477  [   82/   88]
per-ex loss: 0.271772  [   83/   88]
per-ex loss: 0.357196  [   84/   88]
per-ex loss: 0.662530  [   85/   88]
per-ex loss: 0.610281  [   86/   88]
per-ex loss: 0.343966  [   87/   88]
per-ex loss: 0.396437  [   88/   88]
Train Error: Avg loss: 0.45791170
validation Error: 
 Avg loss: 0.54904164 
 F1: 0.472733 
 Precision: 0.501063 
 Recall: 0.447435
 IoU: 0.309529

test Error: 
 Avg loss: 0.49304849 
 F1: 0.555007 
 Precision: 0.581057 
 Recall: 0.531192
 IoU: 0.384090

We have finished training iteration 104
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_102_.pth
per-ex loss: 0.331004  [    1/   88]
per-ex loss: 0.328801  [    2/   88]
per-ex loss: 0.600224  [    3/   88]
per-ex loss: 0.648461  [    4/   88]
per-ex loss: 0.475394  [    5/   88]
per-ex loss: 0.482629  [    6/   88]
per-ex loss: 0.368949  [    7/   88]
per-ex loss: 0.525376  [    8/   88]
per-ex loss: 0.502772  [    9/   88]
per-ex loss: 0.576604  [   10/   88]
per-ex loss: 0.600195  [   11/   88]
per-ex loss: 0.382791  [   12/   88]
per-ex loss: 0.528497  [   13/   88]
per-ex loss: 0.302515  [   14/   88]
per-ex loss: 0.535211  [   15/   88]
per-ex loss: 0.530031  [   16/   88]
per-ex loss: 0.386129  [   17/   88]
per-ex loss: 0.373033  [   18/   88]
per-ex loss: 0.379905  [   19/   88]
per-ex loss: 0.368907  [   20/   88]
per-ex loss: 0.411603  [   21/   88]
per-ex loss: 0.408539  [   22/   88]
per-ex loss: 0.604403  [   23/   88]
per-ex loss: 0.579162  [   24/   88]
per-ex loss: 0.398244  [   25/   88]
per-ex loss: 0.391834  [   26/   88]
per-ex loss: 0.369042  [   27/   88]
per-ex loss: 0.658550  [   28/   88]
per-ex loss: 0.352698  [   29/   88]
per-ex loss: 0.301015  [   30/   88]
per-ex loss: 0.559067  [   31/   88]
per-ex loss: 0.361877  [   32/   88]
per-ex loss: 0.333047  [   33/   88]
per-ex loss: 0.350435  [   34/   88]
per-ex loss: 0.432440  [   35/   88]
per-ex loss: 0.402552  [   36/   88]
per-ex loss: 0.435649  [   37/   88]
per-ex loss: 0.462107  [   38/   88]
per-ex loss: 0.560549  [   39/   88]
per-ex loss: 0.652961  [   40/   88]
per-ex loss: 0.332291  [   41/   88]
per-ex loss: 0.384312  [   42/   88]
per-ex loss: 0.431466  [   43/   88]
per-ex loss: 0.579358  [   44/   88]
per-ex loss: 0.566140  [   45/   88]
per-ex loss: 0.252713  [   46/   88]
per-ex loss: 0.359433  [   47/   88]
per-ex loss: 0.397077  [   48/   88]
per-ex loss: 0.496624  [   49/   88]
per-ex loss: 0.374937  [   50/   88]
per-ex loss: 0.333511  [   51/   88]
per-ex loss: 0.380277  [   52/   88]
per-ex loss: 0.395589  [   53/   88]
per-ex loss: 0.492050  [   54/   88]
per-ex loss: 0.332788  [   55/   88]
per-ex loss: 0.449584  [   56/   88]
per-ex loss: 0.659002  [   57/   88]
per-ex loss: 0.605820  [   58/   88]
per-ex loss: 0.563902  [   59/   88]
per-ex loss: 0.485565  [   60/   88]
per-ex loss: 0.310567  [   61/   88]
per-ex loss: 0.545145  [   62/   88]
per-ex loss: 0.463698  [   63/   88]
per-ex loss: 0.378847  [   64/   88]
per-ex loss: 0.522369  [   65/   88]
per-ex loss: 0.518506  [   66/   88]
per-ex loss: 0.570691  [   67/   88]
per-ex loss: 0.361846  [   68/   88]
per-ex loss: 0.363507  [   69/   88]
per-ex loss: 0.347472  [   70/   88]
per-ex loss: 0.474578  [   71/   88]
per-ex loss: 0.417207  [   72/   88]
per-ex loss: 0.394546  [   73/   88]
per-ex loss: 0.665516  [   74/   88]
per-ex loss: 0.353960  [   75/   88]
per-ex loss: 0.403244  [   76/   88]
per-ex loss: 0.431099  [   77/   88]
per-ex loss: 0.530535  [   78/   88]
per-ex loss: 0.570430  [   79/   88]
per-ex loss: 0.491294  [   80/   88]
per-ex loss: 0.416841  [   81/   88]
per-ex loss: 0.406459  [   82/   88]
per-ex loss: 0.314851  [   83/   88]
per-ex loss: 0.593594  [   84/   88]
per-ex loss: 0.397417  [   85/   88]
per-ex loss: 0.493542  [   86/   88]
per-ex loss: 0.457819  [   87/   88]
per-ex loss: 0.610386  [   88/   88]
Train Error: Avg loss: 0.45376826
validation Error: 
 Avg loss: 0.55039876 
 F1: 0.475107 
 Precision: 0.434825 
 Recall: 0.523613
 IoU: 0.311567

test Error: 
 Avg loss: 0.48313864 
 F1: 0.556891 
 Precision: 0.512328 
 Recall: 0.609946
 IoU: 0.385897

We have finished training iteration 105
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_103_.pth
per-ex loss: 0.432360  [    1/   88]
per-ex loss: 0.334290  [    2/   88]
per-ex loss: 0.348694  [    3/   88]
per-ex loss: 0.443643  [    4/   88]
per-ex loss: 0.315371  [    5/   88]
per-ex loss: 0.582199  [    6/   88]
per-ex loss: 0.417426  [    7/   88]
per-ex loss: 0.543838  [    8/   88]
per-ex loss: 0.586043  [    9/   88]
per-ex loss: 0.469676  [   10/   88]
per-ex loss: 0.363860  [   11/   88]
per-ex loss: 0.533268  [   12/   88]
per-ex loss: 0.545512  [   13/   88]
per-ex loss: 0.486207  [   14/   88]
per-ex loss: 0.561152  [   15/   88]
per-ex loss: 0.378295  [   16/   88]
per-ex loss: 0.578558  [   17/   88]
per-ex loss: 0.595848  [   18/   88]
per-ex loss: 0.649453  [   19/   88]
per-ex loss: 0.386535  [   20/   88]
per-ex loss: 0.576803  [   21/   88]
per-ex loss: 0.528253  [   22/   88]
per-ex loss: 0.521552  [   23/   88]
per-ex loss: 0.389104  [   24/   88]
per-ex loss: 0.548992  [   25/   88]
per-ex loss: 0.387396  [   26/   88]
per-ex loss: 0.329534  [   27/   88]
per-ex loss: 0.423612  [   28/   88]
per-ex loss: 0.426851  [   29/   88]
per-ex loss: 0.288271  [   30/   88]
per-ex loss: 0.491832  [   31/   88]
per-ex loss: 0.566354  [   32/   88]
per-ex loss: 0.344046  [   33/   88]
per-ex loss: 0.405305  [   34/   88]
per-ex loss: 0.468901  [   35/   88]
per-ex loss: 0.408362  [   36/   88]
per-ex loss: 0.410462  [   37/   88]
per-ex loss: 0.401605  [   38/   88]
per-ex loss: 0.408852  [   39/   88]
per-ex loss: 0.332469  [   40/   88]
per-ex loss: 0.572783  [   41/   88]
per-ex loss: 0.351883  [   42/   88]
per-ex loss: 0.375802  [   43/   88]
per-ex loss: 0.488049  [   44/   88]
per-ex loss: 0.615218  [   45/   88]
per-ex loss: 0.392944  [   46/   88]
per-ex loss: 0.450021  [   47/   88]
per-ex loss: 0.350567  [   48/   88]
per-ex loss: 0.567653  [   49/   88]
per-ex loss: 0.446833  [   50/   88]
per-ex loss: 0.354549  [   51/   88]
per-ex loss: 0.629768  [   52/   88]
per-ex loss: 0.359252  [   53/   88]
per-ex loss: 0.358973  [   54/   88]
per-ex loss: 0.532890  [   55/   88]
per-ex loss: 0.268440  [   56/   88]
per-ex loss: 0.562987  [   57/   88]
per-ex loss: 0.327344  [   58/   88]
per-ex loss: 0.625444  [   59/   88]
per-ex loss: 0.368328  [   60/   88]
per-ex loss: 0.339171  [   61/   88]
per-ex loss: 0.533178  [   62/   88]
per-ex loss: 0.377664  [   63/   88]
per-ex loss: 0.348763  [   64/   88]
per-ex loss: 0.637141  [   65/   88]
per-ex loss: 0.308274  [   66/   88]
per-ex loss: 0.339224  [   67/   88]
per-ex loss: 0.494920  [   68/   88]
per-ex loss: 0.364353  [   69/   88]
per-ex loss: 0.312760  [   70/   88]
per-ex loss: 0.391910  [   71/   88]
per-ex loss: 0.682947  [   72/   88]
per-ex loss: 0.455868  [   73/   88]
per-ex loss: 0.327991  [   74/   88]
per-ex loss: 0.377904  [   75/   88]
per-ex loss: 0.655127  [   76/   88]
per-ex loss: 0.577586  [   77/   88]
per-ex loss: 0.525050  [   78/   88]
per-ex loss: 0.525375  [   79/   88]
per-ex loss: 0.455177  [   80/   88]
per-ex loss: 0.444144  [   81/   88]
per-ex loss: 0.367075  [   82/   88]
per-ex loss: 0.408594  [   83/   88]
per-ex loss: 0.524912  [   84/   88]
per-ex loss: 0.403038  [   85/   88]
per-ex loss: 0.551909  [   86/   88]
per-ex loss: 0.360302  [   87/   88]
per-ex loss: 0.325752  [   88/   88]
Train Error: Avg loss: 0.45030258
validation Error: 
 Avg loss: 0.52769623 
 F1: 0.499107 
 Precision: 0.557884 
 Recall: 0.451534
 IoU: 0.332540

test Error: 
 Avg loss: 0.48098279 
 F1: 0.568564 
 Precision: 0.607642 
 Recall: 0.534209
 IoU: 0.397198

We have finished training iteration 106
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_104_.pth
per-ex loss: 0.565398  [    1/   88]
per-ex loss: 0.525847  [    2/   88]
per-ex loss: 0.381598  [    3/   88]
per-ex loss: 0.361337  [    4/   88]
per-ex loss: 0.368155  [    5/   88]
per-ex loss: 0.452925  [    6/   88]
per-ex loss: 0.420339  [    7/   88]
per-ex loss: 0.577723  [    8/   88]
per-ex loss: 0.362655  [    9/   88]
per-ex loss: 0.598993  [   10/   88]
per-ex loss: 0.690813  [   11/   88]
per-ex loss: 0.448287  [   12/   88]
per-ex loss: 0.559309  [   13/   88]
per-ex loss: 0.514252  [   14/   88]
per-ex loss: 0.348784  [   15/   88]
per-ex loss: 0.646624  [   16/   88]
per-ex loss: 0.524762  [   17/   88]
per-ex loss: 0.647557  [   18/   88]
per-ex loss: 0.415135  [   19/   88]
per-ex loss: 0.397415  [   20/   88]
per-ex loss: 0.439183  [   21/   88]
per-ex loss: 0.391427  [   22/   88]
per-ex loss: 0.332376  [   23/   88]
per-ex loss: 0.479628  [   24/   88]
per-ex loss: 0.365906  [   25/   88]
per-ex loss: 0.475172  [   26/   88]
per-ex loss: 0.442924  [   27/   88]
per-ex loss: 0.310272  [   28/   88]
per-ex loss: 0.531526  [   29/   88]
per-ex loss: 0.633516  [   30/   88]
per-ex loss: 0.331887  [   31/   88]
per-ex loss: 0.228525  [   32/   88]
per-ex loss: 0.329029  [   33/   88]
per-ex loss: 0.625419  [   34/   88]
per-ex loss: 0.483598  [   35/   88]
per-ex loss: 0.526632  [   36/   88]
per-ex loss: 0.495807  [   37/   88]
per-ex loss: 0.461942  [   38/   88]
per-ex loss: 0.337195  [   39/   88]
per-ex loss: 0.414466  [   40/   88]
per-ex loss: 0.334008  [   41/   88]
per-ex loss: 0.361447  [   42/   88]
per-ex loss: 0.389443  [   43/   88]
per-ex loss: 0.579937  [   44/   88]
per-ex loss: 0.371226  [   45/   88]
per-ex loss: 0.302997  [   46/   88]
per-ex loss: 0.341129  [   47/   88]
per-ex loss: 0.349483  [   48/   88]
per-ex loss: 0.350786  [   49/   88]
per-ex loss: 0.519987  [   50/   88]
per-ex loss: 0.331122  [   51/   88]
per-ex loss: 0.299892  [   52/   88]
per-ex loss: 0.364996  [   53/   88]
per-ex loss: 0.563740  [   54/   88]
per-ex loss: 0.421060  [   55/   88]
per-ex loss: 0.322299  [   56/   88]
per-ex loss: 0.616478  [   57/   88]
per-ex loss: 0.648261  [   58/   88]
per-ex loss: 0.541706  [   59/   88]
per-ex loss: 0.548246  [   60/   88]
per-ex loss: 0.562668  [   61/   88]
per-ex loss: 0.445959  [   62/   88]
per-ex loss: 0.350354  [   63/   88]
per-ex loss: 0.339691  [   64/   88]
per-ex loss: 0.594498  [   65/   88]
per-ex loss: 0.469758  [   66/   88]
per-ex loss: 0.328881  [   67/   88]
per-ex loss: 0.430408  [   68/   88]
per-ex loss: 0.443272  [   69/   88]
per-ex loss: 0.565416  [   70/   88]
per-ex loss: 0.584843  [   71/   88]
per-ex loss: 0.380766  [   72/   88]
per-ex loss: 0.345902  [   73/   88]
per-ex loss: 0.337889  [   74/   88]
per-ex loss: 0.545010  [   75/   88]
per-ex loss: 0.440108  [   76/   88]
per-ex loss: 0.375170  [   77/   88]
per-ex loss: 0.402689  [   78/   88]
per-ex loss: 0.487862  [   79/   88]
per-ex loss: 0.413700  [   80/   88]
per-ex loss: 0.501320  [   81/   88]
per-ex loss: 0.504977  [   82/   88]
per-ex loss: 0.379247  [   83/   88]
per-ex loss: 0.516441  [   84/   88]
per-ex loss: 0.326491  [   85/   88]
per-ex loss: 0.385383  [   86/   88]
per-ex loss: 0.425209  [   87/   88]
per-ex loss: 0.376309  [   88/   88]
Train Error: Avg loss: 0.44616820
validation Error: 
 Avg loss: 0.53151117 
 F1: 0.494124 
 Precision: 0.517193 
 Recall: 0.473026
 IoU: 0.328131

test Error: 
 Avg loss: 0.47962101 
 F1: 0.569228 
 Precision: 0.593701 
 Recall: 0.546692
 IoU: 0.397847

We have finished training iteration 107
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_105_.pth
per-ex loss: 0.346270  [    1/   88]
per-ex loss: 0.353875  [    2/   88]
per-ex loss: 0.373499  [    3/   88]
per-ex loss: 0.317810  [    4/   88]
per-ex loss: 0.387745  [    5/   88]
per-ex loss: 0.466876  [    6/   88]
per-ex loss: 0.334175  [    7/   88]
per-ex loss: 0.678278  [    8/   88]
per-ex loss: 0.471438  [    9/   88]
per-ex loss: 0.370062  [   10/   88]
per-ex loss: 0.594399  [   11/   88]
per-ex loss: 0.616416  [   12/   88]
per-ex loss: 0.528848  [   13/   88]
per-ex loss: 0.353805  [   14/   88]
per-ex loss: 0.420741  [   15/   88]
per-ex loss: 0.417007  [   16/   88]
per-ex loss: 0.322198  [   17/   88]
per-ex loss: 0.367164  [   18/   88]
per-ex loss: 0.549649  [   19/   88]
per-ex loss: 0.529089  [   20/   88]
per-ex loss: 0.436709  [   21/   88]
per-ex loss: 0.377695  [   22/   88]
per-ex loss: 0.443244  [   23/   88]
per-ex loss: 0.520252  [   24/   88]
per-ex loss: 0.461181  [   25/   88]
per-ex loss: 0.517600  [   26/   88]
per-ex loss: 0.387214  [   27/   88]
per-ex loss: 0.489154  [   28/   88]
per-ex loss: 0.398845  [   29/   88]
per-ex loss: 0.412758  [   30/   88]
per-ex loss: 0.439342  [   31/   88]
per-ex loss: 0.375636  [   32/   88]
per-ex loss: 0.380836  [   33/   88]
per-ex loss: 0.329748  [   34/   88]
per-ex loss: 0.535048  [   35/   88]
per-ex loss: 0.313400  [   36/   88]
per-ex loss: 0.551761  [   37/   88]
per-ex loss: 0.324505  [   38/   88]
per-ex loss: 0.427900  [   39/   88]
per-ex loss: 0.406158  [   40/   88]
per-ex loss: 0.530266  [   41/   88]
per-ex loss: 0.465427  [   42/   88]
per-ex loss: 0.470040  [   43/   88]
per-ex loss: 0.324575  [   44/   88]
per-ex loss: 0.654651  [   45/   88]
per-ex loss: 0.455332  [   46/   88]
per-ex loss: 0.435800  [   47/   88]
per-ex loss: 0.403219  [   48/   88]
per-ex loss: 0.252636  [   49/   88]
per-ex loss: 0.537776  [   50/   88]
per-ex loss: 0.523003  [   51/   88]
per-ex loss: 0.388403  [   52/   88]
per-ex loss: 0.349656  [   53/   88]
per-ex loss: 0.623477  [   54/   88]
per-ex loss: 0.340171  [   55/   88]
per-ex loss: 0.396514  [   56/   88]
per-ex loss: 0.333309  [   57/   88]
per-ex loss: 0.595915  [   58/   88]
per-ex loss: 0.581816  [   59/   88]
per-ex loss: 0.664330  [   60/   88]
per-ex loss: 0.616013  [   61/   88]
per-ex loss: 0.550539  [   62/   88]
per-ex loss: 0.525943  [   63/   88]
per-ex loss: 0.391728  [   64/   88]
per-ex loss: 0.298879  [   65/   88]
per-ex loss: 0.408368  [   66/   88]
per-ex loss: 0.592509  [   67/   88]
per-ex loss: 0.567619  [   68/   88]
per-ex loss: 0.564576  [   69/   88]
per-ex loss: 0.320555  [   70/   88]
per-ex loss: 0.382499  [   71/   88]
per-ex loss: 0.410041  [   72/   88]
per-ex loss: 0.379135  [   73/   88]
per-ex loss: 0.558988  [   74/   88]
per-ex loss: 0.353757  [   75/   88]
per-ex loss: 0.579305  [   76/   88]
per-ex loss: 0.433299  [   77/   88]
per-ex loss: 0.618987  [   78/   88]
per-ex loss: 0.327595  [   79/   88]
per-ex loss: 0.383046  [   80/   88]
per-ex loss: 0.515546  [   81/   88]
per-ex loss: 0.610918  [   82/   88]
per-ex loss: 0.326979  [   83/   88]
per-ex loss: 0.416636  [   84/   88]
per-ex loss: 0.384583  [   85/   88]
per-ex loss: 0.311737  [   86/   88]
per-ex loss: 0.327577  [   87/   88]
per-ex loss: 0.432813  [   88/   88]
Train Error: Avg loss: 0.44596380
validation Error: 
 Avg loss: 0.53007801 
 F1: 0.496918 
 Precision: 0.522140 
 Recall: 0.474021
 IoU: 0.330600

test Error: 
 Avg loss: 0.48070606 
 F1: 0.566619 
 Precision: 0.569968 
 Recall: 0.563308
 IoU: 0.395302

We have finished training iteration 108
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_106_.pth
per-ex loss: 0.560113  [    1/   88]
per-ex loss: 0.603051  [    2/   88]
per-ex loss: 0.513435  [    3/   88]
per-ex loss: 0.537123  [    4/   88]
per-ex loss: 0.440275  [    5/   88]
per-ex loss: 0.451192  [    6/   88]
per-ex loss: 0.294485  [    7/   88]
per-ex loss: 0.391055  [    8/   88]
per-ex loss: 0.383678  [    9/   88]
per-ex loss: 0.580509  [   10/   88]
per-ex loss: 0.427076  [   11/   88]
per-ex loss: 0.390747  [   12/   88]
per-ex loss: 0.332199  [   13/   88]
per-ex loss: 0.532276  [   14/   88]
per-ex loss: 0.518700  [   15/   88]
per-ex loss: 0.526614  [   16/   88]
per-ex loss: 0.581068  [   17/   88]
per-ex loss: 0.431262  [   18/   88]
per-ex loss: 0.448034  [   19/   88]
per-ex loss: 0.330374  [   20/   88]
per-ex loss: 0.345681  [   21/   88]
per-ex loss: 0.389226  [   22/   88]
per-ex loss: 0.383554  [   23/   88]
per-ex loss: 0.499106  [   24/   88]
per-ex loss: 0.474878  [   25/   88]
per-ex loss: 0.310720  [   26/   88]
per-ex loss: 0.558309  [   27/   88]
per-ex loss: 0.393867  [   28/   88]
per-ex loss: 0.552135  [   29/   88]
per-ex loss: 0.454667  [   30/   88]
per-ex loss: 0.431854  [   31/   88]
per-ex loss: 0.663870  [   32/   88]
per-ex loss: 0.563156  [   33/   88]
per-ex loss: 0.490679  [   34/   88]
per-ex loss: 0.442080  [   35/   88]
per-ex loss: 0.412760  [   36/   88]
per-ex loss: 0.657247  [   37/   88]
per-ex loss: 0.392660  [   38/   88]
per-ex loss: 0.476784  [   39/   88]
per-ex loss: 0.595595  [   40/   88]
per-ex loss: 0.312924  [   41/   88]
per-ex loss: 0.408331  [   42/   88]
per-ex loss: 0.339057  [   43/   88]
per-ex loss: 0.472502  [   44/   88]
per-ex loss: 0.368944  [   45/   88]
per-ex loss: 0.588187  [   46/   88]
per-ex loss: 0.573538  [   47/   88]
per-ex loss: 0.322664  [   48/   88]
per-ex loss: 0.486915  [   49/   88]
per-ex loss: 0.569358  [   50/   88]
per-ex loss: 0.360276  [   51/   88]
per-ex loss: 0.343184  [   52/   88]
per-ex loss: 0.528084  [   53/   88]
per-ex loss: 0.656806  [   54/   88]
per-ex loss: 0.555484  [   55/   88]
per-ex loss: 0.361212  [   56/   88]
per-ex loss: 0.620088  [   57/   88]
per-ex loss: 0.368595  [   58/   88]
per-ex loss: 0.332480  [   59/   88]
per-ex loss: 0.394709  [   60/   88]
per-ex loss: 0.332505  [   61/   88]
per-ex loss: 0.400598  [   62/   88]
per-ex loss: 0.368350  [   63/   88]
per-ex loss: 0.417901  [   64/   88]
per-ex loss: 0.674937  [   65/   88]
per-ex loss: 0.372429  [   66/   88]
per-ex loss: 0.632902  [   67/   88]
per-ex loss: 0.370856  [   68/   88]
per-ex loss: 0.421577  [   69/   88]
per-ex loss: 0.352374  [   70/   88]
per-ex loss: 0.323274  [   71/   88]
per-ex loss: 0.375521  [   72/   88]
per-ex loss: 0.518159  [   73/   88]
per-ex loss: 0.492216  [   74/   88]
per-ex loss: 0.441654  [   75/   88]
per-ex loss: 0.331742  [   76/   88]
per-ex loss: 0.387514  [   77/   88]
per-ex loss: 0.409237  [   78/   88]
per-ex loss: 0.553915  [   79/   88]
per-ex loss: 0.446523  [   80/   88]
per-ex loss: 0.510568  [   81/   88]
per-ex loss: 0.395266  [   82/   88]
per-ex loss: 0.539101  [   83/   88]
per-ex loss: 0.497112  [   84/   88]
per-ex loss: 0.252902  [   85/   88]
per-ex loss: 0.382112  [   86/   88]
per-ex loss: 0.323305  [   87/   88]
per-ex loss: 0.387300  [   88/   88]
Train Error: Avg loss: 0.45042369
validation Error: 
 Avg loss: 0.54694494 
 F1: 0.480695 
 Precision: 0.640305 
 Recall: 0.384780
 IoU: 0.316392

test Error: 
 Avg loss: 0.51829159 
 F1: 0.528991 
 Precision: 0.692193 
 Recall: 0.428064
 IoU: 0.359611

We have finished training iteration 109
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_107_.pth
per-ex loss: 0.323789  [    1/   88]
per-ex loss: 0.606615  [    2/   88]
per-ex loss: 0.638791  [    3/   88]
per-ex loss: 0.672040  [    4/   88]
per-ex loss: 0.474460  [    5/   88]
per-ex loss: 0.375789  [    6/   88]
per-ex loss: 0.404241  [    7/   88]
per-ex loss: 0.486905  [    8/   88]
per-ex loss: 0.392911  [    9/   88]
per-ex loss: 0.325620  [   10/   88]
per-ex loss: 0.278169  [   11/   88]
per-ex loss: 0.320528  [   12/   88]
per-ex loss: 0.601295  [   13/   88]
per-ex loss: 0.471316  [   14/   88]
per-ex loss: 0.366974  [   15/   88]
per-ex loss: 0.386323  [   16/   88]
per-ex loss: 0.561032  [   17/   88]
per-ex loss: 0.531706  [   18/   88]
per-ex loss: 0.439344  [   19/   88]
per-ex loss: 0.671963  [   20/   88]
per-ex loss: 0.527462  [   21/   88]
per-ex loss: 0.381787  [   22/   88]
per-ex loss: 0.655927  [   23/   88]
per-ex loss: 0.343060  [   24/   88]
per-ex loss: 0.590109  [   25/   88]
per-ex loss: 0.326460  [   26/   88]
per-ex loss: 0.422223  [   27/   88]
per-ex loss: 0.407189  [   28/   88]
per-ex loss: 0.501789  [   29/   88]
per-ex loss: 0.551178  [   30/   88]
per-ex loss: 0.591928  [   31/   88]
per-ex loss: 0.475471  [   32/   88]
per-ex loss: 0.387098  [   33/   88]
per-ex loss: 0.336734  [   34/   88]
per-ex loss: 0.550740  [   35/   88]
per-ex loss: 0.335605  [   36/   88]
per-ex loss: 0.457743  [   37/   88]
per-ex loss: 0.397200  [   38/   88]
per-ex loss: 0.404059  [   39/   88]
per-ex loss: 0.440113  [   40/   88]
per-ex loss: 0.341699  [   41/   88]
per-ex loss: 0.369317  [   42/   88]
per-ex loss: 0.379665  [   43/   88]
per-ex loss: 0.400373  [   44/   88]
per-ex loss: 0.319808  [   45/   88]
per-ex loss: 0.530344  [   46/   88]
per-ex loss: 0.576087  [   47/   88]
per-ex loss: 0.565354  [   48/   88]
per-ex loss: 0.512574  [   49/   88]
per-ex loss: 0.368582  [   50/   88]
per-ex loss: 0.324145  [   51/   88]
per-ex loss: 0.484326  [   52/   88]
per-ex loss: 0.435946  [   53/   88]
per-ex loss: 0.551972  [   54/   88]
per-ex loss: 0.405503  [   55/   88]
per-ex loss: 0.347524  [   56/   88]
per-ex loss: 0.438881  [   57/   88]
per-ex loss: 0.617071  [   58/   88]
per-ex loss: 0.343531  [   59/   88]
per-ex loss: 0.361596  [   60/   88]
per-ex loss: 0.424846  [   61/   88]
per-ex loss: 0.424183  [   62/   88]
per-ex loss: 0.415231  [   63/   88]
per-ex loss: 0.473269  [   64/   88]
per-ex loss: 0.384746  [   65/   88]
per-ex loss: 0.357991  [   66/   88]
per-ex loss: 0.450334  [   67/   88]
per-ex loss: 0.400335  [   68/   88]
per-ex loss: 0.584028  [   69/   88]
per-ex loss: 0.450455  [   70/   88]
per-ex loss: 0.419801  [   71/   88]
per-ex loss: 0.346291  [   72/   88]
per-ex loss: 0.644464  [   73/   88]
per-ex loss: 0.566985  [   74/   88]
per-ex loss: 0.403745  [   75/   88]
per-ex loss: 0.606453  [   76/   88]
per-ex loss: 0.375726  [   77/   88]
per-ex loss: 0.514735  [   78/   88]
per-ex loss: 0.474834  [   79/   88]
per-ex loss: 0.584538  [   80/   88]
per-ex loss: 0.412115  [   81/   88]
per-ex loss: 0.341079  [   82/   88]
per-ex loss: 0.557110  [   83/   88]
per-ex loss: 0.382021  [   84/   88]
per-ex loss: 0.322579  [   85/   88]
per-ex loss: 0.307841  [   86/   88]
per-ex loss: 0.544054  [   87/   88]
per-ex loss: 0.517687  [   88/   88]
Train Error: Avg loss: 0.45199388
validation Error: 
 Avg loss: 0.53037586 
 F1: 0.496962 
 Precision: 0.535682 
 Recall: 0.463463
 IoU: 0.330639

test Error: 
 Avg loss: 0.48763956 
 F1: 0.558079 
 Precision: 0.574337 
 Recall: 0.542716
 IoU: 0.387038

We have finished training iteration 110
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_108_.pth
per-ex loss: 0.642461  [    1/   88]
per-ex loss: 0.450705  [    2/   88]
per-ex loss: 0.565145  [    3/   88]
per-ex loss: 0.408443  [    4/   88]
per-ex loss: 0.304562  [    5/   88]
per-ex loss: 0.521862  [    6/   88]
per-ex loss: 0.374053  [    7/   88]
per-ex loss: 0.336020  [    8/   88]
per-ex loss: 0.371781  [    9/   88]
per-ex loss: 0.342502  [   10/   88]
per-ex loss: 0.460599  [   11/   88]
per-ex loss: 0.319817  [   12/   88]
per-ex loss: 0.448538  [   13/   88]
per-ex loss: 0.647320  [   14/   88]
per-ex loss: 0.406192  [   15/   88]
per-ex loss: 0.680093  [   16/   88]
per-ex loss: 0.572462  [   17/   88]
per-ex loss: 0.387670  [   18/   88]
per-ex loss: 0.310164  [   19/   88]
per-ex loss: 0.412709  [   20/   88]
per-ex loss: 0.577496  [   21/   88]
per-ex loss: 0.372735  [   22/   88]
per-ex loss: 0.458438  [   23/   88]
per-ex loss: 0.358083  [   24/   88]
per-ex loss: 0.393149  [   25/   88]
per-ex loss: 0.483133  [   26/   88]
per-ex loss: 0.564180  [   27/   88]
per-ex loss: 0.377303  [   28/   88]
per-ex loss: 0.379927  [   29/   88]
per-ex loss: 0.377778  [   30/   88]
per-ex loss: 0.588153  [   31/   88]
per-ex loss: 0.406241  [   32/   88]
per-ex loss: 0.417506  [   33/   88]
per-ex loss: 0.313309  [   34/   88]
per-ex loss: 0.492441  [   35/   88]
per-ex loss: 0.528904  [   36/   88]
per-ex loss: 0.519125  [   37/   88]
per-ex loss: 0.348984  [   38/   88]
per-ex loss: 0.373962  [   39/   88]
per-ex loss: 0.423570  [   40/   88]
per-ex loss: 0.560481  [   41/   88]
per-ex loss: 0.583097  [   42/   88]
per-ex loss: 0.223273  [   43/   88]
per-ex loss: 0.327431  [   44/   88]
per-ex loss: 0.423946  [   45/   88]
per-ex loss: 0.421978  [   46/   88]
per-ex loss: 0.511439  [   47/   88]
per-ex loss: 0.565320  [   48/   88]
per-ex loss: 0.332005  [   49/   88]
per-ex loss: 0.385990  [   50/   88]
per-ex loss: 0.370277  [   51/   88]
per-ex loss: 0.526255  [   52/   88]
per-ex loss: 0.368503  [   53/   88]
per-ex loss: 0.507331  [   54/   88]
per-ex loss: 0.319028  [   55/   88]
per-ex loss: 0.597370  [   56/   88]
per-ex loss: 0.458111  [   57/   88]
per-ex loss: 0.372768  [   58/   88]
per-ex loss: 0.351610  [   59/   88]
per-ex loss: 0.562837  [   60/   88]
per-ex loss: 0.524430  [   61/   88]
per-ex loss: 0.514562  [   62/   88]
per-ex loss: 0.369618  [   63/   88]
per-ex loss: 0.470992  [   64/   88]
per-ex loss: 0.363508  [   65/   88]
per-ex loss: 0.334752  [   66/   88]
per-ex loss: 0.338749  [   67/   88]
per-ex loss: 0.590037  [   68/   88]
per-ex loss: 0.475631  [   69/   88]
per-ex loss: 0.363634  [   70/   88]
per-ex loss: 0.441024  [   71/   88]
per-ex loss: 0.306840  [   72/   88]
per-ex loss: 0.331541  [   73/   88]
per-ex loss: 0.375807  [   74/   88]
per-ex loss: 0.380077  [   75/   88]
per-ex loss: 0.559558  [   76/   88]
per-ex loss: 0.329912  [   77/   88]
per-ex loss: 0.606330  [   78/   88]
per-ex loss: 0.633337  [   79/   88]
per-ex loss: 0.531857  [   80/   88]
per-ex loss: 0.478379  [   81/   88]
per-ex loss: 0.420469  [   82/   88]
per-ex loss: 0.338737  [   83/   88]
per-ex loss: 0.582615  [   84/   88]
per-ex loss: 0.617311  [   85/   88]
per-ex loss: 0.436205  [   86/   88]
per-ex loss: 0.537323  [   87/   88]
per-ex loss: 0.400282  [   88/   88]
Train Error: Avg loss: 0.44477367
validation Error: 
 Avg loss: 0.53079153 
 F1: 0.499660 
 Precision: 0.501104 
 Recall: 0.498225
 IoU: 0.333032

test Error: 
 Avg loss: 0.47433325 
 F1: 0.572087 
 Precision: 0.553839 
 Recall: 0.591580
 IoU: 0.400646

We have finished training iteration 111
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_109_.pth
per-ex loss: 0.479389  [    1/   88]
per-ex loss: 0.417996  [    2/   88]
per-ex loss: 0.538416  [    3/   88]
per-ex loss: 0.415358  [    4/   88]
per-ex loss: 0.332727  [    5/   88]
per-ex loss: 0.301404  [    6/   88]
per-ex loss: 0.474563  [    7/   88]
per-ex loss: 0.550195  [    8/   88]
per-ex loss: 0.585621  [    9/   88]
per-ex loss: 0.445318  [   10/   88]
per-ex loss: 0.296678  [   11/   88]
per-ex loss: 0.311577  [   12/   88]
per-ex loss: 0.532494  [   13/   88]
per-ex loss: 0.562162  [   14/   88]
per-ex loss: 0.332607  [   15/   88]
per-ex loss: 0.469907  [   16/   88]
per-ex loss: 0.383439  [   17/   88]
per-ex loss: 0.434153  [   18/   88]
per-ex loss: 0.372500  [   19/   88]
per-ex loss: 0.337585  [   20/   88]
per-ex loss: 0.556549  [   21/   88]
per-ex loss: 0.360740  [   22/   88]
per-ex loss: 0.338123  [   23/   88]
per-ex loss: 0.497042  [   24/   88]
per-ex loss: 0.367367  [   25/   88]
per-ex loss: 0.511141  [   26/   88]
per-ex loss: 0.485464  [   27/   88]
per-ex loss: 0.570788  [   28/   88]
per-ex loss: 0.388176  [   29/   88]
per-ex loss: 0.633221  [   30/   88]
per-ex loss: 0.386153  [   31/   88]
per-ex loss: 0.424806  [   32/   88]
per-ex loss: 0.381791  [   33/   88]
per-ex loss: 0.360612  [   34/   88]
per-ex loss: 0.564822  [   35/   88]
per-ex loss: 0.319473  [   36/   88]
per-ex loss: 0.331430  [   37/   88]
per-ex loss: 0.454962  [   38/   88]
per-ex loss: 0.345268  [   39/   88]
per-ex loss: 0.396470  [   40/   88]
per-ex loss: 0.472303  [   41/   88]
per-ex loss: 0.537659  [   42/   88]
per-ex loss: 0.491394  [   43/   88]
per-ex loss: 0.466211  [   44/   88]
per-ex loss: 0.448064  [   45/   88]
per-ex loss: 0.578544  [   46/   88]
per-ex loss: 0.537138  [   47/   88]
per-ex loss: 0.315601  [   48/   88]
per-ex loss: 0.647078  [   49/   88]
per-ex loss: 0.367990  [   50/   88]
per-ex loss: 0.393087  [   51/   88]
per-ex loss: 0.376121  [   52/   88]
per-ex loss: 0.361746  [   53/   88]
per-ex loss: 0.579715  [   54/   88]
per-ex loss: 0.360928  [   55/   88]
per-ex loss: 0.251430  [   56/   88]
per-ex loss: 0.369110  [   57/   88]
per-ex loss: 0.392341  [   58/   88]
per-ex loss: 0.300550  [   59/   88]
per-ex loss: 0.427174  [   60/   88]
per-ex loss: 0.569358  [   61/   88]
per-ex loss: 0.572735  [   62/   88]
per-ex loss: 0.521664  [   63/   88]
per-ex loss: 0.603256  [   64/   88]
per-ex loss: 0.556875  [   65/   88]
per-ex loss: 0.412826  [   66/   88]
per-ex loss: 0.638050  [   67/   88]
per-ex loss: 0.319049  [   68/   88]
per-ex loss: 0.325334  [   69/   88]
per-ex loss: 0.395827  [   70/   88]
per-ex loss: 0.395020  [   71/   88]
per-ex loss: 0.427020  [   72/   88]
per-ex loss: 0.369050  [   73/   88]
per-ex loss: 0.384929  [   74/   88]
per-ex loss: 0.629989  [   75/   88]
per-ex loss: 0.413878  [   76/   88]
per-ex loss: 0.435791  [   77/   88]
per-ex loss: 0.542033  [   78/   88]
per-ex loss: 0.630680  [   79/   88]
per-ex loss: 0.524333  [   80/   88]
per-ex loss: 0.582948  [   81/   88]
per-ex loss: 0.324750  [   82/   88]
per-ex loss: 0.358737  [   83/   88]
per-ex loss: 0.552481  [   84/   88]
per-ex loss: 0.346122  [   85/   88]
per-ex loss: 0.487339  [   86/   88]
per-ex loss: 0.533590  [   87/   88]
per-ex loss: 0.368832  [   88/   88]
Train Error: Avg loss: 0.44483149
validation Error: 
 Avg loss: 0.52607074 
 F1: 0.503735 
 Precision: 0.530490 
 Recall: 0.479548
 IoU: 0.336661

test Error: 
 Avg loss: 0.47673327 
 F1: 0.568555 
 Precision: 0.581958 
 Recall: 0.555757
 IoU: 0.397190

We have finished training iteration 112
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_110_.pth
per-ex loss: 0.539340  [    1/   88]
per-ex loss: 0.523709  [    2/   88]
per-ex loss: 0.393385  [    3/   88]
per-ex loss: 0.562121  [    4/   88]
per-ex loss: 0.474079  [    5/   88]
per-ex loss: 0.635937  [    6/   88]
per-ex loss: 0.392010  [    7/   88]
per-ex loss: 0.534312  [    8/   88]
per-ex loss: 0.362707  [    9/   88]
per-ex loss: 0.296088  [   10/   88]
per-ex loss: 0.529196  [   11/   88]
per-ex loss: 0.452632  [   12/   88]
per-ex loss: 0.571214  [   13/   88]
per-ex loss: 0.337812  [   14/   88]
per-ex loss: 0.446713  [   15/   88]
per-ex loss: 0.523248  [   16/   88]
per-ex loss: 0.330047  [   17/   88]
per-ex loss: 0.329312  [   18/   88]
per-ex loss: 0.572111  [   19/   88]
per-ex loss: 0.459830  [   20/   88]
per-ex loss: 0.453911  [   21/   88]
per-ex loss: 0.229068  [   22/   88]
per-ex loss: 0.311017  [   23/   88]
per-ex loss: 0.483397  [   24/   88]
per-ex loss: 0.401420  [   25/   88]
per-ex loss: 0.400404  [   26/   88]
per-ex loss: 0.393636  [   27/   88]
per-ex loss: 0.619356  [   28/   88]
per-ex loss: 0.321230  [   29/   88]
per-ex loss: 0.353317  [   30/   88]
per-ex loss: 0.451219  [   31/   88]
per-ex loss: 0.567827  [   32/   88]
per-ex loss: 0.370994  [   33/   88]
per-ex loss: 0.453717  [   34/   88]
per-ex loss: 0.393938  [   35/   88]
per-ex loss: 0.565397  [   36/   88]
per-ex loss: 0.430302  [   37/   88]
per-ex loss: 0.666979  [   38/   88]
per-ex loss: 0.549741  [   39/   88]
per-ex loss: 0.356243  [   40/   88]
per-ex loss: 0.521511  [   41/   88]
per-ex loss: 0.364028  [   42/   88]
per-ex loss: 0.418046  [   43/   88]
per-ex loss: 0.463899  [   44/   88]
per-ex loss: 0.583988  [   45/   88]
per-ex loss: 0.631987  [   46/   88]
per-ex loss: 0.374488  [   47/   88]
per-ex loss: 0.439960  [   48/   88]
per-ex loss: 0.385379  [   49/   88]
per-ex loss: 0.554697  [   50/   88]
per-ex loss: 0.635505  [   51/   88]
per-ex loss: 0.337991  [   52/   88]
per-ex loss: 0.349520  [   53/   88]
per-ex loss: 0.373962  [   54/   88]
per-ex loss: 0.446880  [   55/   88]
per-ex loss: 0.572238  [   56/   88]
per-ex loss: 0.527015  [   57/   88]
per-ex loss: 0.315945  [   58/   88]
per-ex loss: 0.405951  [   59/   88]
per-ex loss: 0.383013  [   60/   88]
per-ex loss: 0.375868  [   61/   88]
per-ex loss: 0.433152  [   62/   88]
per-ex loss: 0.328973  [   63/   88]
per-ex loss: 0.420987  [   64/   88]
per-ex loss: 0.319097  [   65/   88]
per-ex loss: 0.299869  [   66/   88]
per-ex loss: 0.364531  [   67/   88]
per-ex loss: 0.509368  [   68/   88]
per-ex loss: 0.495313  [   69/   88]
per-ex loss: 0.608250  [   70/   88]
per-ex loss: 0.451570  [   71/   88]
per-ex loss: 0.398142  [   72/   88]
per-ex loss: 0.422585  [   73/   88]
per-ex loss: 0.573210  [   74/   88]
per-ex loss: 0.488700  [   75/   88]
per-ex loss: 0.479658  [   76/   88]
per-ex loss: 0.610665  [   77/   88]
per-ex loss: 0.393095  [   78/   88]
per-ex loss: 0.356547  [   79/   88]
per-ex loss: 0.323763  [   80/   88]
per-ex loss: 0.313851  [   81/   88]
per-ex loss: 0.366577  [   82/   88]
per-ex loss: 0.574942  [   83/   88]
per-ex loss: 0.349066  [   84/   88]
per-ex loss: 0.435227  [   85/   88]
per-ex loss: 0.381745  [   86/   88]
per-ex loss: 0.348006  [   87/   88]
per-ex loss: 0.531396  [   88/   88]
Train Error: Avg loss: 0.44408034
validation Error: 
 Avg loss: 0.53269068 
 F1: 0.494873 
 Precision: 0.548583 
 Recall: 0.450742
 IoU: 0.328791

test Error: 
 Avg loss: 0.48417579 
 F1: 0.563607 
 Precision: 0.622607 
 Recall: 0.514822
 IoU: 0.392377

We have finished training iteration 113
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_111_.pth
per-ex loss: 0.586939  [    1/   88]
per-ex loss: 0.356973  [    2/   88]
per-ex loss: 0.437458  [    3/   88]
per-ex loss: 0.514716  [    4/   88]
per-ex loss: 0.335597  [    5/   88]
per-ex loss: 0.380610  [    6/   88]
per-ex loss: 0.357913  [    7/   88]
per-ex loss: 0.364736  [    8/   88]
per-ex loss: 0.557834  [    9/   88]
per-ex loss: 0.561078  [   10/   88]
per-ex loss: 0.359164  [   11/   88]
per-ex loss: 0.462812  [   12/   88]
per-ex loss: 0.386848  [   13/   88]
per-ex loss: 0.407929  [   14/   88]
per-ex loss: 0.448827  [   15/   88]
per-ex loss: 0.384200  [   16/   88]
per-ex loss: 0.449674  [   17/   88]
per-ex loss: 0.647359  [   18/   88]
per-ex loss: 0.358336  [   19/   88]
per-ex loss: 0.519361  [   20/   88]
per-ex loss: 0.308975  [   21/   88]
per-ex loss: 0.392806  [   22/   88]
per-ex loss: 0.479975  [   23/   88]
per-ex loss: 0.632142  [   24/   88]
per-ex loss: 0.562878  [   25/   88]
per-ex loss: 0.342580  [   26/   88]
per-ex loss: 0.371536  [   27/   88]
per-ex loss: 0.443464  [   28/   88]
per-ex loss: 0.326847  [   29/   88]
per-ex loss: 0.382034  [   30/   88]
per-ex loss: 0.470881  [   31/   88]
per-ex loss: 0.453980  [   32/   88]
per-ex loss: 0.412192  [   33/   88]
per-ex loss: 0.336405  [   34/   88]
per-ex loss: 0.601347  [   35/   88]
per-ex loss: 0.399541  [   36/   88]
per-ex loss: 0.630184  [   37/   88]
per-ex loss: 0.551648  [   38/   88]
per-ex loss: 0.297535  [   39/   88]
per-ex loss: 0.541029  [   40/   88]
per-ex loss: 0.297106  [   41/   88]
per-ex loss: 0.456297  [   42/   88]
per-ex loss: 0.387876  [   43/   88]
per-ex loss: 0.448213  [   44/   88]
per-ex loss: 0.315355  [   45/   88]
per-ex loss: 0.375901  [   46/   88]
per-ex loss: 0.365822  [   47/   88]
per-ex loss: 0.309015  [   48/   88]
per-ex loss: 0.382661  [   49/   88]
per-ex loss: 0.522532  [   50/   88]
per-ex loss: 0.379222  [   51/   88]
per-ex loss: 0.551565  [   52/   88]
per-ex loss: 0.623027  [   53/   88]
per-ex loss: 0.359693  [   54/   88]
per-ex loss: 0.549236  [   55/   88]
per-ex loss: 0.536356  [   56/   88]
per-ex loss: 0.433588  [   57/   88]
per-ex loss: 0.337409  [   58/   88]
per-ex loss: 0.326008  [   59/   88]
per-ex loss: 0.604788  [   60/   88]
per-ex loss: 0.568910  [   61/   88]
per-ex loss: 0.630368  [   62/   88]
per-ex loss: 0.307068  [   63/   88]
per-ex loss: 0.409315  [   64/   88]
per-ex loss: 0.606553  [   65/   88]
per-ex loss: 0.263520  [   66/   88]
per-ex loss: 0.523989  [   67/   88]
per-ex loss: 0.540025  [   68/   88]
per-ex loss: 0.355974  [   69/   88]
per-ex loss: 0.403365  [   70/   88]
per-ex loss: 0.484328  [   71/   88]
per-ex loss: 0.417597  [   72/   88]
per-ex loss: 0.419228  [   73/   88]
per-ex loss: 0.357065  [   74/   88]
per-ex loss: 0.363955  [   75/   88]
per-ex loss: 0.592095  [   76/   88]
per-ex loss: 0.543024  [   77/   88]
per-ex loss: 0.335530  [   78/   88]
per-ex loss: 0.520380  [   79/   88]
per-ex loss: 0.500480  [   80/   88]
per-ex loss: 0.428468  [   81/   88]
per-ex loss: 0.547875  [   82/   88]
per-ex loss: 0.336628  [   83/   88]
per-ex loss: 0.609287  [   84/   88]
per-ex loss: 0.342906  [   85/   88]
per-ex loss: 0.377357  [   86/   88]
per-ex loss: 0.345870  [   87/   88]
per-ex loss: 0.582362  [   88/   88]
Train Error: Avg loss: 0.44422198
validation Error: 
 Avg loss: 0.54234786 
 F1: 0.490840 
 Precision: 0.555715 
 Recall: 0.439529
 IoU: 0.325241

test Error: 
 Avg loss: 0.49393054 
 F1: 0.550165 
 Precision: 0.618665 
 Recall: 0.495322
 IoU: 0.379467

We have finished training iteration 114
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_112_.pth
per-ex loss: 0.478475  [    1/   88]
per-ex loss: 0.464158  [    2/   88]
per-ex loss: 0.338000  [    3/   88]
per-ex loss: 0.401885  [    4/   88]
per-ex loss: 0.348290  [    5/   88]
per-ex loss: 0.534634  [    6/   88]
per-ex loss: 0.382457  [    7/   88]
per-ex loss: 0.335385  [    8/   88]
per-ex loss: 0.367198  [    9/   88]
per-ex loss: 0.333416  [   10/   88]
per-ex loss: 0.335223  [   11/   88]
per-ex loss: 0.308285  [   12/   88]
per-ex loss: 0.654633  [   13/   88]
per-ex loss: 0.519125  [   14/   88]
per-ex loss: 0.386442  [   15/   88]
per-ex loss: 0.579964  [   16/   88]
per-ex loss: 0.353564  [   17/   88]
per-ex loss: 0.691635  [   18/   88]
per-ex loss: 0.407066  [   19/   88]
per-ex loss: 0.408786  [   20/   88]
per-ex loss: 0.516168  [   21/   88]
per-ex loss: 0.612642  [   22/   88]
per-ex loss: 0.434043  [   23/   88]
per-ex loss: 0.293044  [   24/   88]
per-ex loss: 0.464601  [   25/   88]
per-ex loss: 0.617132  [   26/   88]
per-ex loss: 0.605031  [   27/   88]
per-ex loss: 0.294451  [   28/   88]
per-ex loss: 0.509797  [   29/   88]
per-ex loss: 0.572364  [   30/   88]
per-ex loss: 0.340840  [   31/   88]
per-ex loss: 0.548133  [   32/   88]
per-ex loss: 0.397982  [   33/   88]
per-ex loss: 0.378612  [   34/   88]
per-ex loss: 0.579790  [   35/   88]
per-ex loss: 0.579814  [   36/   88]
per-ex loss: 0.328317  [   37/   88]
per-ex loss: 0.539930  [   38/   88]
per-ex loss: 0.387571  [   39/   88]
per-ex loss: 0.500822  [   40/   88]
per-ex loss: 0.336014  [   41/   88]
per-ex loss: 0.549197  [   42/   88]
per-ex loss: 0.353857  [   43/   88]
per-ex loss: 0.358044  [   44/   88]
per-ex loss: 0.384633  [   45/   88]
per-ex loss: 0.409529  [   46/   88]
per-ex loss: 0.455459  [   47/   88]
per-ex loss: 0.528426  [   48/   88]
per-ex loss: 0.457132  [   49/   88]
per-ex loss: 0.572772  [   50/   88]
per-ex loss: 0.393591  [   51/   88]
per-ex loss: 0.390463  [   52/   88]
per-ex loss: 0.569145  [   53/   88]
per-ex loss: 0.471660  [   54/   88]
per-ex loss: 0.441904  [   55/   88]
per-ex loss: 0.620670  [   56/   88]
per-ex loss: 0.374634  [   57/   88]
per-ex loss: 0.440911  [   58/   88]
per-ex loss: 0.521934  [   59/   88]
per-ex loss: 0.460221  [   60/   88]
per-ex loss: 0.582128  [   61/   88]
per-ex loss: 0.429908  [   62/   88]
per-ex loss: 0.458843  [   63/   88]
per-ex loss: 0.318134  [   64/   88]
per-ex loss: 0.321334  [   65/   88]
per-ex loss: 0.373858  [   66/   88]
per-ex loss: 0.649020  [   67/   88]
per-ex loss: 0.505964  [   68/   88]
per-ex loss: 0.392257  [   69/   88]
per-ex loss: 0.515867  [   70/   88]
per-ex loss: 0.552660  [   71/   88]
per-ex loss: 0.353017  [   72/   88]
per-ex loss: 0.572341  [   73/   88]
per-ex loss: 0.427613  [   74/   88]
per-ex loss: 0.248583  [   75/   88]
per-ex loss: 0.440736  [   76/   88]
per-ex loss: 0.404528  [   77/   88]
per-ex loss: 0.404826  [   78/   88]
per-ex loss: 0.317988  [   79/   88]
per-ex loss: 0.356530  [   80/   88]
per-ex loss: 0.572401  [   81/   88]
per-ex loss: 0.420292  [   82/   88]
per-ex loss: 0.465062  [   83/   88]
per-ex loss: 0.378866  [   84/   88]
per-ex loss: 0.552678  [   85/   88]
per-ex loss: 0.307802  [   86/   88]
per-ex loss: 0.363166  [   87/   88]
per-ex loss: 0.569220  [   88/   88]
Train Error: Avg loss: 0.44858547
validation Error: 
 Avg loss: 0.53314627 
 F1: 0.498259 
 Precision: 0.544381 
 Recall: 0.459342
 IoU: 0.331788

test Error: 
 Avg loss: 0.47996659 
 F1: 0.569620 
 Precision: 0.621192 
 Recall: 0.525955
 IoU: 0.398230

We have finished training iteration 115
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_113_.pth
per-ex loss: 0.392988  [    1/   88]
per-ex loss: 0.421917  [    2/   88]
per-ex loss: 0.523580  [    3/   88]
per-ex loss: 0.372057  [    4/   88]
per-ex loss: 0.294837  [    5/   88]
per-ex loss: 0.334638  [    6/   88]
per-ex loss: 0.535460  [    7/   88]
per-ex loss: 0.619047  [    8/   88]
per-ex loss: 0.547776  [    9/   88]
per-ex loss: 0.544885  [   10/   88]
per-ex loss: 0.452191  [   11/   88]
per-ex loss: 0.444597  [   12/   88]
per-ex loss: 0.404754  [   13/   88]
per-ex loss: 0.394735  [   14/   88]
per-ex loss: 0.378942  [   15/   88]
per-ex loss: 0.331077  [   16/   88]
per-ex loss: 0.317373  [   17/   88]
per-ex loss: 0.540889  [   18/   88]
per-ex loss: 0.524023  [   19/   88]
per-ex loss: 0.299199  [   20/   88]
per-ex loss: 0.304321  [   21/   88]
per-ex loss: 0.612630  [   22/   88]
per-ex loss: 0.537670  [   23/   88]
per-ex loss: 0.378197  [   24/   88]
per-ex loss: 0.322447  [   25/   88]
per-ex loss: 0.451794  [   26/   88]
per-ex loss: 0.417381  [   27/   88]
per-ex loss: 0.331960  [   28/   88]
per-ex loss: 0.527722  [   29/   88]
per-ex loss: 0.421509  [   30/   88]
per-ex loss: 0.404429  [   31/   88]
per-ex loss: 0.406477  [   32/   88]
per-ex loss: 0.527074  [   33/   88]
per-ex loss: 0.447028  [   34/   88]
per-ex loss: 0.590032  [   35/   88]
per-ex loss: 0.620337  [   36/   88]
per-ex loss: 0.358262  [   37/   88]
per-ex loss: 0.565875  [   38/   88]
per-ex loss: 0.432351  [   39/   88]
per-ex loss: 0.473621  [   40/   88]
per-ex loss: 0.519163  [   41/   88]
per-ex loss: 0.424935  [   42/   88]
per-ex loss: 0.668342  [   43/   88]
per-ex loss: 0.494700  [   44/   88]
per-ex loss: 0.319544  [   45/   88]
per-ex loss: 0.327928  [   46/   88]
per-ex loss: 0.404311  [   47/   88]
per-ex loss: 0.372477  [   48/   88]
per-ex loss: 0.323602  [   49/   88]
per-ex loss: 0.510201  [   50/   88]
per-ex loss: 0.331440  [   51/   88]
per-ex loss: 0.475578  [   52/   88]
per-ex loss: 0.374364  [   53/   88]
per-ex loss: 0.372693  [   54/   88]
per-ex loss: 0.302883  [   55/   88]
per-ex loss: 0.487068  [   56/   88]
per-ex loss: 0.457909  [   57/   88]
per-ex loss: 0.553017  [   58/   88]
per-ex loss: 0.589656  [   59/   88]
per-ex loss: 0.387497  [   60/   88]
per-ex loss: 0.294294  [   61/   88]
per-ex loss: 0.356343  [   62/   88]
per-ex loss: 0.329924  [   63/   88]
per-ex loss: 0.494297  [   64/   88]
per-ex loss: 0.530797  [   65/   88]
per-ex loss: 0.244033  [   66/   88]
per-ex loss: 0.568684  [   67/   88]
per-ex loss: 0.354856  [   68/   88]
per-ex loss: 0.549825  [   69/   88]
per-ex loss: 0.505707  [   70/   88]
per-ex loss: 0.629032  [   71/   88]
per-ex loss: 0.394451  [   72/   88]
per-ex loss: 0.372876  [   73/   88]
per-ex loss: 0.378700  [   74/   88]
per-ex loss: 0.349801  [   75/   88]
per-ex loss: 0.376788  [   76/   88]
per-ex loss: 0.393706  [   77/   88]
per-ex loss: 0.415161  [   78/   88]
per-ex loss: 0.651751  [   79/   88]
per-ex loss: 0.544810  [   80/   88]
per-ex loss: 0.377107  [   81/   88]
per-ex loss: 0.446420  [   82/   88]
per-ex loss: 0.587508  [   83/   88]
per-ex loss: 0.601331  [   84/   88]
per-ex loss: 0.343648  [   85/   88]
per-ex loss: 0.389556  [   86/   88]
per-ex loss: 0.401260  [   87/   88]
per-ex loss: 0.614592  [   88/   88]
Train Error: Avg loss: 0.44318924
validation Error: 
 Avg loss: 0.53745180 
 F1: 0.488954 
 Precision: 0.513366 
 Recall: 0.466758
 IoU: 0.323586

test Error: 
 Avg loss: 0.48925508 
 F1: 0.553736 
 Precision: 0.576200 
 Recall: 0.532958
 IoU: 0.382874

We have finished training iteration 116
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_114_.pth
per-ex loss: 0.331059  [    1/   88]
per-ex loss: 0.360133  [    2/   88]
per-ex loss: 0.329832  [    3/   88]
per-ex loss: 0.389809  [    4/   88]
per-ex loss: 0.340947  [    5/   88]
per-ex loss: 0.375276  [    6/   88]
per-ex loss: 0.381796  [    7/   88]
per-ex loss: 0.445843  [    8/   88]
per-ex loss: 0.362757  [    9/   88]
per-ex loss: 0.661362  [   10/   88]
per-ex loss: 0.338286  [   11/   88]
per-ex loss: 0.307613  [   12/   88]
per-ex loss: 0.430076  [   13/   88]
per-ex loss: 0.530080  [   14/   88]
per-ex loss: 0.435939  [   15/   88]
per-ex loss: 0.492070  [   16/   88]
per-ex loss: 0.582951  [   17/   88]
per-ex loss: 0.604910  [   18/   88]
per-ex loss: 0.604753  [   19/   88]
per-ex loss: 0.366159  [   20/   88]
per-ex loss: 0.324342  [   21/   88]
per-ex loss: 0.578446  [   22/   88]
per-ex loss: 0.402748  [   23/   88]
per-ex loss: 0.573577  [   24/   88]
per-ex loss: 0.398249  [   25/   88]
per-ex loss: 0.491641  [   26/   88]
per-ex loss: 0.481201  [   27/   88]
per-ex loss: 0.575486  [   28/   88]
per-ex loss: 0.410271  [   29/   88]
per-ex loss: 0.559747  [   30/   88]
per-ex loss: 0.533585  [   31/   88]
per-ex loss: 0.367492  [   32/   88]
per-ex loss: 0.367255  [   33/   88]
per-ex loss: 0.568922  [   34/   88]
per-ex loss: 0.383004  [   35/   88]
per-ex loss: 0.398337  [   36/   88]
per-ex loss: 0.494062  [   37/   88]
per-ex loss: 0.340334  [   38/   88]
per-ex loss: 0.551851  [   39/   88]
per-ex loss: 0.497179  [   40/   88]
per-ex loss: 0.512189  [   41/   88]
per-ex loss: 0.357719  [   42/   88]
per-ex loss: 0.313128  [   43/   88]
per-ex loss: 0.353924  [   44/   88]
per-ex loss: 0.528284  [   45/   88]
per-ex loss: 0.526722  [   46/   88]
per-ex loss: 0.356290  [   47/   88]
per-ex loss: 0.326387  [   48/   88]
per-ex loss: 0.452869  [   49/   88]
per-ex loss: 0.450913  [   50/   88]
per-ex loss: 0.332936  [   51/   88]
per-ex loss: 0.341047  [   52/   88]
per-ex loss: 0.513651  [   53/   88]
per-ex loss: 0.378078  [   54/   88]
per-ex loss: 0.560051  [   55/   88]
per-ex loss: 0.325200  [   56/   88]
per-ex loss: 0.483734  [   57/   88]
per-ex loss: 0.417198  [   58/   88]
per-ex loss: 0.305867  [   59/   88]
per-ex loss: 0.529915  [   60/   88]
per-ex loss: 0.338742  [   61/   88]
per-ex loss: 0.371301  [   62/   88]
per-ex loss: 0.517368  [   63/   88]
per-ex loss: 0.622019  [   64/   88]
per-ex loss: 0.419088  [   65/   88]
per-ex loss: 0.311193  [   66/   88]
per-ex loss: 0.455394  [   67/   88]
per-ex loss: 0.459355  [   68/   88]
per-ex loss: 0.305550  [   69/   88]
per-ex loss: 0.289256  [   70/   88]
per-ex loss: 0.374593  [   71/   88]
per-ex loss: 0.404807  [   72/   88]
per-ex loss: 0.535613  [   73/   88]
per-ex loss: 0.633854  [   74/   88]
per-ex loss: 0.651204  [   75/   88]
per-ex loss: 0.380872  [   76/   88]
per-ex loss: 0.379885  [   77/   88]
per-ex loss: 0.602734  [   78/   88]
per-ex loss: 0.403284  [   79/   88]
per-ex loss: 0.301201  [   80/   88]
per-ex loss: 0.553854  [   81/   88]
per-ex loss: 0.413236  [   82/   88]
per-ex loss: 0.573851  [   83/   88]
per-ex loss: 0.599278  [   84/   88]
per-ex loss: 0.408927  [   85/   88]
per-ex loss: 0.448261  [   86/   88]
per-ex loss: 0.630679  [   87/   88]
per-ex loss: 0.401525  [   88/   88]
Train Error: Avg loss: 0.44457281
validation Error: 
 Avg loss: 0.53047519 
 F1: 0.496390 
 Precision: 0.535026 
 Recall: 0.462958
 IoU: 0.330132

test Error: 
 Avg loss: 0.48821433 
 F1: 0.556884 
 Precision: 0.594435 
 Recall: 0.523796
 IoU: 0.385890

We have finished training iteration 117
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_115_.pth
per-ex loss: 0.461680  [    1/   88]
per-ex loss: 0.299417  [    2/   88]
per-ex loss: 0.374845  [    3/   88]
per-ex loss: 0.396000  [    4/   88]
per-ex loss: 0.388932  [    5/   88]
per-ex loss: 0.353457  [    6/   88]
per-ex loss: 0.584866  [    7/   88]
per-ex loss: 0.336443  [    8/   88]
per-ex loss: 0.576543  [    9/   88]
per-ex loss: 0.610971  [   10/   88]
per-ex loss: 0.240183  [   11/   88]
per-ex loss: 0.394544  [   12/   88]
per-ex loss: 0.562250  [   13/   88]
per-ex loss: 0.529045  [   14/   88]
per-ex loss: 0.568307  [   15/   88]
per-ex loss: 0.412895  [   16/   88]
per-ex loss: 0.345804  [   17/   88]
per-ex loss: 0.296693  [   18/   88]
per-ex loss: 0.353939  [   19/   88]
per-ex loss: 0.334859  [   20/   88]
per-ex loss: 0.395374  [   21/   88]
per-ex loss: 0.360536  [   22/   88]
per-ex loss: 0.561407  [   23/   88]
per-ex loss: 0.544381  [   24/   88]
per-ex loss: 0.507705  [   25/   88]
per-ex loss: 0.506859  [   26/   88]
per-ex loss: 0.433846  [   27/   88]
per-ex loss: 0.301867  [   28/   88]
per-ex loss: 0.431657  [   29/   88]
per-ex loss: 0.338883  [   30/   88]
per-ex loss: 0.542535  [   31/   88]
per-ex loss: 0.452717  [   32/   88]
per-ex loss: 0.536958  [   33/   88]
per-ex loss: 0.533219  [   34/   88]
per-ex loss: 0.329530  [   35/   88]
per-ex loss: 0.451964  [   36/   88]
per-ex loss: 0.408553  [   37/   88]
per-ex loss: 0.410836  [   38/   88]
per-ex loss: 0.388002  [   39/   88]
per-ex loss: 0.397983  [   40/   88]
per-ex loss: 0.415273  [   41/   88]
per-ex loss: 0.326194  [   42/   88]
per-ex loss: 0.530641  [   43/   88]
per-ex loss: 0.479656  [   44/   88]
per-ex loss: 0.623645  [   45/   88]
per-ex loss: 0.509969  [   46/   88]
per-ex loss: 0.634636  [   47/   88]
per-ex loss: 0.340347  [   48/   88]
per-ex loss: 0.414570  [   49/   88]
per-ex loss: 0.401432  [   50/   88]
per-ex loss: 0.370786  [   51/   88]
per-ex loss: 0.350494  [   52/   88]
per-ex loss: 0.605877  [   53/   88]
per-ex loss: 0.442605  [   54/   88]
per-ex loss: 0.533103  [   55/   88]
per-ex loss: 0.364755  [   56/   88]
per-ex loss: 0.657426  [   57/   88]
per-ex loss: 0.365646  [   58/   88]
per-ex loss: 0.375059  [   59/   88]
per-ex loss: 0.315757  [   60/   88]
per-ex loss: 0.358078  [   61/   88]
per-ex loss: 0.571268  [   62/   88]
per-ex loss: 0.530981  [   63/   88]
per-ex loss: 0.396777  [   64/   88]
per-ex loss: 0.411235  [   65/   88]
per-ex loss: 0.598176  [   66/   88]
per-ex loss: 0.653617  [   67/   88]
per-ex loss: 0.408289  [   68/   88]
per-ex loss: 0.479747  [   69/   88]
per-ex loss: 0.571674  [   70/   88]
per-ex loss: 0.434234  [   71/   88]
per-ex loss: 0.334181  [   72/   88]
per-ex loss: 0.611229  [   73/   88]
per-ex loss: 0.570839  [   74/   88]
per-ex loss: 0.358925  [   75/   88]
per-ex loss: 0.560421  [   76/   88]
per-ex loss: 0.569580  [   77/   88]
per-ex loss: 0.463560  [   78/   88]
per-ex loss: 0.402596  [   79/   88]
per-ex loss: 0.388209  [   80/   88]
per-ex loss: 0.333648  [   81/   88]
per-ex loss: 0.472025  [   82/   88]
per-ex loss: 0.303970  [   83/   88]
per-ex loss: 0.351605  [   84/   88]
per-ex loss: 0.458760  [   85/   88]
per-ex loss: 0.325977  [   86/   88]
per-ex loss: 0.376167  [   87/   88]
per-ex loss: 0.347007  [   88/   88]
Train Error: Avg loss: 0.44269467
validation Error: 
 Avg loss: 0.53104930 
 F1: 0.500744 
 Precision: 0.612611 
 Recall: 0.423424
 IoU: 0.333995

test Error: 
 Avg loss: 0.48503357 
 F1: 0.564653 
 Precision: 0.672292 
 Recall: 0.486725
 IoU: 0.393392

We have finished training iteration 118
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_116_.pth
per-ex loss: 0.525219  [    1/   88]
per-ex loss: 0.472032  [    2/   88]
per-ex loss: 0.305044  [    3/   88]
per-ex loss: 0.392636  [    4/   88]
per-ex loss: 0.435055  [    5/   88]
per-ex loss: 0.456380  [    6/   88]
per-ex loss: 0.398783  [    7/   88]
per-ex loss: 0.311339  [    8/   88]
per-ex loss: 0.371999  [    9/   88]
per-ex loss: 0.459698  [   10/   88]
per-ex loss: 0.342291  [   11/   88]
per-ex loss: 0.562998  [   12/   88]
per-ex loss: 0.378681  [   13/   88]
per-ex loss: 0.602120  [   14/   88]
per-ex loss: 0.456155  [   15/   88]
per-ex loss: 0.310946  [   16/   88]
per-ex loss: 0.415575  [   17/   88]
per-ex loss: 0.432728  [   18/   88]
per-ex loss: 0.548444  [   19/   88]
per-ex loss: 0.320707  [   20/   88]
per-ex loss: 0.363916  [   21/   88]
per-ex loss: 0.575669  [   22/   88]
per-ex loss: 0.335460  [   23/   88]
per-ex loss: 0.488840  [   24/   88]
per-ex loss: 0.355979  [   25/   88]
per-ex loss: 0.610351  [   26/   88]
per-ex loss: 0.580983  [   27/   88]
per-ex loss: 0.322432  [   28/   88]
per-ex loss: 0.383484  [   29/   88]
per-ex loss: 0.556283  [   30/   88]
per-ex loss: 0.295583  [   31/   88]
per-ex loss: 0.381927  [   32/   88]
per-ex loss: 0.332266  [   33/   88]
per-ex loss: 0.324938  [   34/   88]
per-ex loss: 0.379749  [   35/   88]
per-ex loss: 0.418432  [   36/   88]
per-ex loss: 0.335364  [   37/   88]
per-ex loss: 0.357141  [   38/   88]
per-ex loss: 0.552206  [   39/   88]
per-ex loss: 0.481386  [   40/   88]
per-ex loss: 0.569662  [   41/   88]
per-ex loss: 0.329643  [   42/   88]
per-ex loss: 0.564616  [   43/   88]
per-ex loss: 0.634415  [   44/   88]
per-ex loss: 0.448660  [   45/   88]
per-ex loss: 0.414485  [   46/   88]
per-ex loss: 0.523465  [   47/   88]
per-ex loss: 0.356966  [   48/   88]
per-ex loss: 0.401812  [   49/   88]
per-ex loss: 0.657775  [   50/   88]
per-ex loss: 0.541212  [   51/   88]
per-ex loss: 0.343703  [   52/   88]
per-ex loss: 0.317646  [   53/   88]
per-ex loss: 0.444333  [   54/   88]
per-ex loss: 0.354888  [   55/   88]
per-ex loss: 0.363242  [   56/   88]
per-ex loss: 0.432303  [   57/   88]
per-ex loss: 0.571625  [   58/   88]
per-ex loss: 0.524224  [   59/   88]
per-ex loss: 0.486236  [   60/   88]
per-ex loss: 0.392855  [   61/   88]
per-ex loss: 0.528144  [   62/   88]
per-ex loss: 0.555209  [   63/   88]
per-ex loss: 0.365620  [   64/   88]
per-ex loss: 0.243408  [   65/   88]
per-ex loss: 0.561811  [   66/   88]
per-ex loss: 0.354999  [   67/   88]
per-ex loss: 0.626229  [   68/   88]
per-ex loss: 0.542430  [   69/   88]
per-ex loss: 0.379905  [   70/   88]
per-ex loss: 0.431060  [   71/   88]
per-ex loss: 0.490093  [   72/   88]
per-ex loss: 0.357266  [   73/   88]
per-ex loss: 0.637705  [   74/   88]
per-ex loss: 0.363155  [   75/   88]
per-ex loss: 0.484498  [   76/   88]
per-ex loss: 0.380565  [   77/   88]
per-ex loss: 0.415318  [   78/   88]
per-ex loss: 0.383138  [   79/   88]
per-ex loss: 0.518360  [   80/   88]
per-ex loss: 0.567214  [   81/   88]
per-ex loss: 0.349637  [   82/   88]
per-ex loss: 0.427583  [   83/   88]
per-ex loss: 0.449912  [   84/   88]
per-ex loss: 0.308845  [   85/   88]
per-ex loss: 0.353689  [   86/   88]
per-ex loss: 0.518268  [   87/   88]
per-ex loss: 0.346187  [   88/   88]
Train Error: Avg loss: 0.43801399
validation Error: 
 Avg loss: 0.53834013 
 F1: 0.492506 
 Precision: 0.573518 
 Recall: 0.431548
 IoU: 0.326705

test Error: 
 Avg loss: 0.48197475 
 F1: 0.568552 
 Precision: 0.649491 
 Recall: 0.505551
 IoU: 0.397187

We have finished training iteration 119
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_117_.pth
per-ex loss: 0.289477  [    1/   88]
per-ex loss: 0.581801  [    2/   88]
per-ex loss: 0.518063  [    3/   88]
per-ex loss: 0.383219  [    4/   88]
per-ex loss: 0.531653  [    5/   88]
per-ex loss: 0.483194  [    6/   88]
per-ex loss: 0.380715  [    7/   88]
per-ex loss: 0.444788  [    8/   88]
per-ex loss: 0.479450  [    9/   88]
per-ex loss: 0.554612  [   10/   88]
per-ex loss: 0.394909  [   11/   88]
per-ex loss: 0.354421  [   12/   88]
per-ex loss: 0.530797  [   13/   88]
per-ex loss: 0.349917  [   14/   88]
per-ex loss: 0.429810  [   15/   88]
per-ex loss: 0.337390  [   16/   88]
per-ex loss: 0.349725  [   17/   88]
per-ex loss: 0.308381  [   18/   88]
per-ex loss: 0.339955  [   19/   88]
per-ex loss: 0.501684  [   20/   88]
per-ex loss: 0.445462  [   21/   88]
per-ex loss: 0.424026  [   22/   88]
per-ex loss: 0.450395  [   23/   88]
per-ex loss: 0.632019  [   24/   88]
per-ex loss: 0.549517  [   25/   88]
per-ex loss: 0.554727  [   26/   88]
per-ex loss: 0.408583  [   27/   88]
per-ex loss: 0.350553  [   28/   88]
per-ex loss: 0.529056  [   29/   88]
per-ex loss: 0.454507  [   30/   88]
per-ex loss: 0.521218  [   31/   88]
per-ex loss: 0.547095  [   32/   88]
per-ex loss: 0.547000  [   33/   88]
per-ex loss: 0.275594  [   34/   88]
per-ex loss: 0.382225  [   35/   88]
per-ex loss: 0.537932  [   36/   88]
per-ex loss: 0.320714  [   37/   88]
per-ex loss: 0.384729  [   38/   88]
per-ex loss: 0.644212  [   39/   88]
per-ex loss: 0.327910  [   40/   88]
per-ex loss: 0.388961  [   41/   88]
per-ex loss: 0.425372  [   42/   88]
per-ex loss: 0.325812  [   43/   88]
per-ex loss: 0.362613  [   44/   88]
per-ex loss: 0.531975  [   45/   88]
per-ex loss: 0.436411  [   46/   88]
per-ex loss: 0.427125  [   47/   88]
per-ex loss: 0.484982  [   48/   88]
per-ex loss: 0.341682  [   49/   88]
per-ex loss: 0.410699  [   50/   88]
per-ex loss: 0.618701  [   51/   88]
per-ex loss: 0.325748  [   52/   88]
per-ex loss: 0.360187  [   53/   88]
per-ex loss: 0.381395  [   54/   88]
per-ex loss: 0.336231  [   55/   88]
per-ex loss: 0.609356  [   56/   88]
per-ex loss: 0.343132  [   57/   88]
per-ex loss: 0.427910  [   58/   88]
per-ex loss: 0.563250  [   59/   88]
per-ex loss: 0.608293  [   60/   88]
per-ex loss: 0.302082  [   61/   88]
per-ex loss: 0.413214  [   62/   88]
per-ex loss: 0.358561  [   63/   88]
per-ex loss: 0.342302  [   64/   88]
per-ex loss: 0.397785  [   65/   88]
per-ex loss: 0.591093  [   66/   88]
per-ex loss: 0.321321  [   67/   88]
per-ex loss: 0.402548  [   68/   88]
per-ex loss: 0.578143  [   69/   88]
per-ex loss: 0.372463  [   70/   88]
per-ex loss: 0.396690  [   71/   88]
per-ex loss: 0.312833  [   72/   88]
per-ex loss: 0.484398  [   73/   88]
per-ex loss: 0.445275  [   74/   88]
per-ex loss: 0.347331  [   75/   88]
per-ex loss: 0.566248  [   76/   88]
per-ex loss: 0.358729  [   77/   88]
per-ex loss: 0.577348  [   78/   88]
per-ex loss: 0.634743  [   79/   88]
per-ex loss: 0.378117  [   80/   88]
per-ex loss: 0.465828  [   81/   88]
per-ex loss: 0.299793  [   82/   88]
per-ex loss: 0.525587  [   83/   88]
per-ex loss: 0.438889  [   84/   88]
per-ex loss: 0.351905  [   85/   88]
per-ex loss: 0.641979  [   86/   88]
per-ex loss: 0.444783  [   87/   88]
per-ex loss: 0.363553  [   88/   88]
Train Error: Avg loss: 0.43925921
validation Error: 
 Avg loss: 0.53556086 
 F1: 0.494100 
 Precision: 0.588557 
 Recall: 0.425770
 IoU: 0.328110

test Error: 
 Avg loss: 0.48479115 
 F1: 0.566181 
 Precision: 0.632645 
 Recall: 0.512354
 IoU: 0.394876

We have finished training iteration 120
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_118_.pth
per-ex loss: 0.409227  [    1/   88]
per-ex loss: 0.383107  [    2/   88]
per-ex loss: 0.367845  [    3/   88]
per-ex loss: 0.660568  [    4/   88]
per-ex loss: 0.525881  [    5/   88]
per-ex loss: 0.438801  [    6/   88]
per-ex loss: 0.551062  [    7/   88]
per-ex loss: 0.524437  [    8/   88]
per-ex loss: 0.404111  [    9/   88]
per-ex loss: 0.333248  [   10/   88]
per-ex loss: 0.370601  [   11/   88]
per-ex loss: 0.568213  [   12/   88]
per-ex loss: 0.421424  [   13/   88]
per-ex loss: 0.329603  [   14/   88]
per-ex loss: 0.467461  [   15/   88]
per-ex loss: 0.325553  [   16/   88]
per-ex loss: 0.386655  [   17/   88]
per-ex loss: 0.324290  [   18/   88]
per-ex loss: 0.372525  [   19/   88]
per-ex loss: 0.376975  [   20/   88]
per-ex loss: 0.450579  [   21/   88]
per-ex loss: 0.459534  [   22/   88]
per-ex loss: 0.489418  [   23/   88]
per-ex loss: 0.566609  [   24/   88]
per-ex loss: 0.597697  [   25/   88]
per-ex loss: 0.345378  [   26/   88]
per-ex loss: 0.501934  [   27/   88]
per-ex loss: 0.299562  [   28/   88]
per-ex loss: 0.418244  [   29/   88]
per-ex loss: 0.409470  [   30/   88]
per-ex loss: 0.570741  [   31/   88]
per-ex loss: 0.373140  [   32/   88]
per-ex loss: 0.445414  [   33/   88]
per-ex loss: 0.544965  [   34/   88]
per-ex loss: 0.369825  [   35/   88]
per-ex loss: 0.561242  [   36/   88]
per-ex loss: 0.564276  [   37/   88]
per-ex loss: 0.568667  [   38/   88]
per-ex loss: 0.388213  [   39/   88]
per-ex loss: 0.344067  [   40/   88]
per-ex loss: 0.603642  [   41/   88]
per-ex loss: 0.532761  [   42/   88]
per-ex loss: 0.513794  [   43/   88]
per-ex loss: 0.412313  [   44/   88]
per-ex loss: 0.391906  [   45/   88]
per-ex loss: 0.357970  [   46/   88]
per-ex loss: 0.388745  [   47/   88]
per-ex loss: 0.329096  [   48/   88]
per-ex loss: 0.392196  [   49/   88]
per-ex loss: 0.486941  [   50/   88]
per-ex loss: 0.467427  [   51/   88]
per-ex loss: 0.616010  [   52/   88]
per-ex loss: 0.654681  [   53/   88]
per-ex loss: 0.460577  [   54/   88]
per-ex loss: 0.527550  [   55/   88]
per-ex loss: 0.323956  [   56/   88]
per-ex loss: 0.610150  [   57/   88]
per-ex loss: 0.621983  [   58/   88]
per-ex loss: 0.315669  [   59/   88]
per-ex loss: 0.333864  [   60/   88]
per-ex loss: 0.457922  [   61/   88]
per-ex loss: 0.448953  [   62/   88]
per-ex loss: 0.336285  [   63/   88]
per-ex loss: 0.349629  [   64/   88]
per-ex loss: 0.374621  [   65/   88]
per-ex loss: 0.557989  [   66/   88]
per-ex loss: 0.236627  [   67/   88]
per-ex loss: 0.349077  [   68/   88]
per-ex loss: 0.357764  [   69/   88]
per-ex loss: 0.317478  [   70/   88]
per-ex loss: 0.421106  [   71/   88]
per-ex loss: 0.434424  [   72/   88]
per-ex loss: 0.365959  [   73/   88]
per-ex loss: 0.510705  [   74/   88]
per-ex loss: 0.327564  [   75/   88]
per-ex loss: 0.568953  [   76/   88]
per-ex loss: 0.589449  [   77/   88]
per-ex loss: 0.516186  [   78/   88]
per-ex loss: 0.298986  [   79/   88]
per-ex loss: 0.630858  [   80/   88]
per-ex loss: 0.413112  [   81/   88]
per-ex loss: 0.483026  [   82/   88]
per-ex loss: 0.535571  [   83/   88]
per-ex loss: 0.350693  [   84/   88]
per-ex loss: 0.393795  [   85/   88]
per-ex loss: 0.298426  [   86/   88]
per-ex loss: 0.374206  [   87/   88]
per-ex loss: 0.317766  [   88/   88]
Train Error: Avg loss: 0.44055595
validation Error: 
 Avg loss: 0.55877870 
 F1: 0.466288 
 Precision: 0.469416 
 Recall: 0.463201
 IoU: 0.304026

test Error: 
 Avg loss: 0.50064716 
 F1: 0.543334 
 Precision: 0.555975 
 Recall: 0.531255
 IoU: 0.372998

We have finished training iteration 121
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_119_.pth
per-ex loss: 0.355913  [    1/   88]
per-ex loss: 0.373685  [    2/   88]
per-ex loss: 0.418409  [    3/   88]
per-ex loss: 0.461879  [    4/   88]
per-ex loss: 0.399619  [    5/   88]
per-ex loss: 0.363254  [    6/   88]
per-ex loss: 0.631152  [    7/   88]
per-ex loss: 0.412568  [    8/   88]
per-ex loss: 0.652315  [    9/   88]
per-ex loss: 0.361220  [   10/   88]
per-ex loss: 0.359357  [   11/   88]
per-ex loss: 0.583800  [   12/   88]
per-ex loss: 0.545443  [   13/   88]
per-ex loss: 0.213468  [   14/   88]
per-ex loss: 0.385602  [   15/   88]
per-ex loss: 0.398870  [   16/   88]
per-ex loss: 0.363826  [   17/   88]
per-ex loss: 0.619691  [   18/   88]
per-ex loss: 0.335946  [   19/   88]
per-ex loss: 0.402400  [   20/   88]
per-ex loss: 0.446592  [   21/   88]
per-ex loss: 0.391000  [   22/   88]
per-ex loss: 0.395931  [   23/   88]
per-ex loss: 0.329666  [   24/   88]
per-ex loss: 0.404418  [   25/   88]
per-ex loss: 0.423846  [   26/   88]
per-ex loss: 0.489633  [   27/   88]
per-ex loss: 0.523989  [   28/   88]
per-ex loss: 0.453694  [   29/   88]
per-ex loss: 0.551205  [   30/   88]
per-ex loss: 0.428435  [   31/   88]
per-ex loss: 0.371349  [   32/   88]
per-ex loss: 0.511319  [   33/   88]
per-ex loss: 0.333615  [   34/   88]
per-ex loss: 0.342184  [   35/   88]
per-ex loss: 0.575447  [   36/   88]
per-ex loss: 0.400837  [   37/   88]
per-ex loss: 0.559653  [   38/   88]
per-ex loss: 0.463549  [   39/   88]
per-ex loss: 0.522780  [   40/   88]
per-ex loss: 0.469194  [   41/   88]
per-ex loss: 0.394783  [   42/   88]
per-ex loss: 0.564562  [   43/   88]
per-ex loss: 0.520024  [   44/   88]
per-ex loss: 0.558658  [   45/   88]
per-ex loss: 0.411625  [   46/   88]
per-ex loss: 0.407034  [   47/   88]
per-ex loss: 0.593222  [   48/   88]
per-ex loss: 0.509109  [   49/   88]
per-ex loss: 0.379142  [   50/   88]
per-ex loss: 0.470391  [   51/   88]
per-ex loss: 0.371242  [   52/   88]
per-ex loss: 0.364401  [   53/   88]
per-ex loss: 0.608477  [   54/   88]
per-ex loss: 0.507980  [   55/   88]
per-ex loss: 0.348098  [   56/   88]
per-ex loss: 0.349914  [   57/   88]
per-ex loss: 0.531117  [   58/   88]
per-ex loss: 0.536370  [   59/   88]
per-ex loss: 0.366747  [   60/   88]
per-ex loss: 0.385935  [   61/   88]
per-ex loss: 0.362584  [   62/   88]
per-ex loss: 0.317639  [   63/   88]
per-ex loss: 0.529919  [   64/   88]
per-ex loss: 0.369486  [   65/   88]
per-ex loss: 0.562212  [   66/   88]
per-ex loss: 0.568382  [   67/   88]
per-ex loss: 0.534098  [   68/   88]
per-ex loss: 0.298402  [   69/   88]
per-ex loss: 0.555907  [   70/   88]
per-ex loss: 0.292472  [   71/   88]
per-ex loss: 0.309416  [   72/   88]
per-ex loss: 0.337111  [   73/   88]
per-ex loss: 0.312966  [   74/   88]
per-ex loss: 0.462346  [   75/   88]
per-ex loss: 0.318245  [   76/   88]
per-ex loss: 0.433277  [   77/   88]
per-ex loss: 0.309172  [   78/   88]
per-ex loss: 0.439776  [   79/   88]
per-ex loss: 0.393241  [   80/   88]
per-ex loss: 0.372318  [   81/   88]
per-ex loss: 0.310869  [   82/   88]
per-ex loss: 0.517125  [   83/   88]
per-ex loss: 0.642496  [   84/   88]
per-ex loss: 0.426758  [   85/   88]
per-ex loss: 0.388945  [   86/   88]
per-ex loss: 0.327324  [   87/   88]
per-ex loss: 0.608908  [   88/   88]
Train Error: Avg loss: 0.43757930
validation Error: 
 Avg loss: 0.54767396 
 F1: 0.479986 
 Precision: 0.633437 
 Recall: 0.386384
 IoU: 0.315777

test Error: 
 Avg loss: 0.50765586 
 F1: 0.537965 
 Precision: 0.700534 
 Recall: 0.436637
 IoU: 0.367957

We have finished training iteration 122
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_120_.pth
per-ex loss: 0.660759  [    1/   88]
per-ex loss: 0.343498  [    2/   88]
per-ex loss: 0.628934  [    3/   88]
per-ex loss: 0.358320  [    4/   88]
per-ex loss: 0.522415  [    5/   88]
per-ex loss: 0.318719  [    6/   88]
per-ex loss: 0.527114  [    7/   88]
per-ex loss: 0.395621  [    8/   88]
per-ex loss: 0.365212  [    9/   88]
per-ex loss: 0.421723  [   10/   88]
per-ex loss: 0.498204  [   11/   88]
per-ex loss: 0.570725  [   12/   88]
per-ex loss: 0.412474  [   13/   88]
per-ex loss: 0.560580  [   14/   88]
per-ex loss: 0.388553  [   15/   88]
per-ex loss: 0.540782  [   16/   88]
per-ex loss: 0.299446  [   17/   88]
per-ex loss: 0.561593  [   18/   88]
per-ex loss: 0.441187  [   19/   88]
per-ex loss: 0.486003  [   20/   88]
per-ex loss: 0.327391  [   21/   88]
per-ex loss: 0.486941  [   22/   88]
per-ex loss: 0.324154  [   23/   88]
per-ex loss: 0.430310  [   24/   88]
per-ex loss: 0.360522  [   25/   88]
per-ex loss: 0.322071  [   26/   88]
per-ex loss: 0.558074  [   27/   88]
per-ex loss: 0.622610  [   28/   88]
per-ex loss: 0.309395  [   29/   88]
per-ex loss: 0.363124  [   30/   88]
per-ex loss: 0.511197  [   31/   88]
per-ex loss: 0.325841  [   32/   88]
per-ex loss: 0.410219  [   33/   88]
per-ex loss: 0.295886  [   34/   88]
per-ex loss: 0.363861  [   35/   88]
per-ex loss: 0.337535  [   36/   88]
per-ex loss: 0.407946  [   37/   88]
per-ex loss: 0.383763  [   38/   88]
per-ex loss: 0.444786  [   39/   88]
per-ex loss: 0.358219  [   40/   88]
per-ex loss: 0.526253  [   41/   88]
per-ex loss: 0.542271  [   42/   88]
per-ex loss: 0.424973  [   43/   88]
per-ex loss: 0.388344  [   44/   88]
per-ex loss: 0.510655  [   45/   88]
per-ex loss: 0.338214  [   46/   88]
per-ex loss: 0.519088  [   47/   88]
per-ex loss: 0.325776  [   48/   88]
per-ex loss: 0.556613  [   49/   88]
per-ex loss: 0.353338  [   50/   88]
per-ex loss: 0.375077  [   51/   88]
per-ex loss: 0.589018  [   52/   88]
per-ex loss: 0.509007  [   53/   88]
per-ex loss: 0.460383  [   54/   88]
per-ex loss: 0.552322  [   55/   88]
per-ex loss: 0.487748  [   56/   88]
per-ex loss: 0.441329  [   57/   88]
per-ex loss: 0.446633  [   58/   88]
per-ex loss: 0.342542  [   59/   88]
per-ex loss: 0.617128  [   60/   88]
per-ex loss: 0.410431  [   61/   88]
per-ex loss: 0.395261  [   62/   88]
per-ex loss: 0.557898  [   63/   88]
per-ex loss: 0.586783  [   64/   88]
per-ex loss: 0.347903  [   65/   88]
per-ex loss: 0.372963  [   66/   88]
per-ex loss: 0.559643  [   67/   88]
per-ex loss: 0.306388  [   68/   88]
per-ex loss: 0.599216  [   69/   88]
per-ex loss: 0.300345  [   70/   88]
per-ex loss: 0.336753  [   71/   88]
per-ex loss: 0.633163  [   72/   88]
per-ex loss: 0.405665  [   73/   88]
per-ex loss: 0.439059  [   74/   88]
per-ex loss: 0.300197  [   75/   88]
per-ex loss: 0.358499  [   76/   88]
per-ex loss: 0.473376  [   77/   88]
per-ex loss: 0.400806  [   78/   88]
per-ex loss: 0.340212  [   79/   88]
per-ex loss: 0.433043  [   80/   88]
per-ex loss: 0.549702  [   81/   88]
per-ex loss: 0.439322  [   82/   88]
per-ex loss: 0.308336  [   83/   88]
per-ex loss: 0.445149  [   84/   88]
per-ex loss: 0.356321  [   85/   88]
per-ex loss: 0.495961  [   86/   88]
per-ex loss: 0.540415  [   87/   88]
per-ex loss: 0.407559  [   88/   88]
Train Error: Avg loss: 0.43923625
validation Error: 
 Avg loss: 0.54542668 
 F1: 0.484182 
 Precision: 0.618454 
 Recall: 0.397814
 IoU: 0.319420

test Error: 
 Avg loss: 0.49431501 
 F1: 0.554323 
 Precision: 0.696451 
 Recall: 0.460373
 IoU: 0.383435

We have finished training iteration 123
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_121_.pth
per-ex loss: 0.389360  [    1/   88]
per-ex loss: 0.461298  [    2/   88]
per-ex loss: 0.468210  [    3/   88]
per-ex loss: 0.309613  [    4/   88]
per-ex loss: 0.636560  [    5/   88]
per-ex loss: 0.405148  [    6/   88]
per-ex loss: 0.570062  [    7/   88]
per-ex loss: 0.357426  [    8/   88]
per-ex loss: 0.366315  [    9/   88]
per-ex loss: 0.380004  [   10/   88]
per-ex loss: 0.532292  [   11/   88]
per-ex loss: 0.367586  [   12/   88]
per-ex loss: 0.401286  [   13/   88]
per-ex loss: 0.492418  [   14/   88]
per-ex loss: 0.637892  [   15/   88]
per-ex loss: 0.352053  [   16/   88]
per-ex loss: 0.419967  [   17/   88]
per-ex loss: 0.555612  [   18/   88]
per-ex loss: 0.322310  [   19/   88]
per-ex loss: 0.371092  [   20/   88]
per-ex loss: 0.295935  [   21/   88]
per-ex loss: 0.351566  [   22/   88]
per-ex loss: 0.568193  [   23/   88]
per-ex loss: 0.335110  [   24/   88]
per-ex loss: 0.475819  [   25/   88]
per-ex loss: 0.458315  [   26/   88]
per-ex loss: 0.381926  [   27/   88]
per-ex loss: 0.482198  [   28/   88]
per-ex loss: 0.367810  [   29/   88]
per-ex loss: 0.409306  [   30/   88]
per-ex loss: 0.667889  [   31/   88]
per-ex loss: 0.475458  [   32/   88]
per-ex loss: 0.395102  [   33/   88]
per-ex loss: 0.354343  [   34/   88]
per-ex loss: 0.539943  [   35/   88]
per-ex loss: 0.567964  [   36/   88]
per-ex loss: 0.369017  [   37/   88]
per-ex loss: 0.576375  [   38/   88]
per-ex loss: 0.383679  [   39/   88]
per-ex loss: 0.310013  [   40/   88]
per-ex loss: 0.610163  [   41/   88]
per-ex loss: 0.555493  [   42/   88]
per-ex loss: 0.458809  [   43/   88]
per-ex loss: 0.463546  [   44/   88]
per-ex loss: 0.579136  [   45/   88]
per-ex loss: 0.371573  [   46/   88]
per-ex loss: 0.299580  [   47/   88]
per-ex loss: 0.424942  [   48/   88]
per-ex loss: 0.236489  [   49/   88]
per-ex loss: 0.415025  [   50/   88]
per-ex loss: 0.615285  [   51/   88]
per-ex loss: 0.382657  [   52/   88]
per-ex loss: 0.322199  [   53/   88]
per-ex loss: 0.338190  [   54/   88]
per-ex loss: 0.376457  [   55/   88]
per-ex loss: 0.456566  [   56/   88]
per-ex loss: 0.531738  [   57/   88]
per-ex loss: 0.615828  [   58/   88]
per-ex loss: 0.325451  [   59/   88]
per-ex loss: 0.396485  [   60/   88]
per-ex loss: 0.348177  [   61/   88]
per-ex loss: 0.510592  [   62/   88]
per-ex loss: 0.511928  [   63/   88]
per-ex loss: 0.408932  [   64/   88]
per-ex loss: 0.363373  [   65/   88]
per-ex loss: 0.396562  [   66/   88]
per-ex loss: 0.321046  [   67/   88]
per-ex loss: 0.413936  [   68/   88]
per-ex loss: 0.525935  [   69/   88]
per-ex loss: 0.592317  [   70/   88]
per-ex loss: 0.527049  [   71/   88]
per-ex loss: 0.335121  [   72/   88]
per-ex loss: 0.433165  [   73/   88]
per-ex loss: 0.619535  [   74/   88]
per-ex loss: 0.334423  [   75/   88]
per-ex loss: 0.518517  [   76/   88]
per-ex loss: 0.448087  [   77/   88]
per-ex loss: 0.363846  [   78/   88]
per-ex loss: 0.536870  [   79/   88]
per-ex loss: 0.460551  [   80/   88]
per-ex loss: 0.302034  [   81/   88]
per-ex loss: 0.530074  [   82/   88]
per-ex loss: 0.346375  [   83/   88]
per-ex loss: 0.557580  [   84/   88]
per-ex loss: 0.363457  [   85/   88]
per-ex loss: 0.292746  [   86/   88]
per-ex loss: 0.523406  [   87/   88]
per-ex loss: 0.389753  [   88/   88]
Train Error: Avg loss: 0.43878940
validation Error: 
 Avg loss: 0.52747805 
 F1: 0.501519 
 Precision: 0.561741 
 Recall: 0.452959
 IoU: 0.334685

test Error: 
 Avg loss: 0.48533372 
 F1: 0.562256 
 Precision: 0.611277 
 Recall: 0.520514
 IoU: 0.391068

We have finished training iteration 124
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_122_.pth
per-ex loss: 0.565418  [    1/   88]
per-ex loss: 0.518249  [    2/   88]
per-ex loss: 0.347130  [    3/   88]
per-ex loss: 0.406704  [    4/   88]
per-ex loss: 0.330346  [    5/   88]
per-ex loss: 0.387126  [    6/   88]
per-ex loss: 0.373305  [    7/   88]
per-ex loss: 0.444291  [    8/   88]
per-ex loss: 0.410087  [    9/   88]
per-ex loss: 0.460775  [   10/   88]
per-ex loss: 0.288498  [   11/   88]
per-ex loss: 0.429154  [   12/   88]
per-ex loss: 0.564798  [   13/   88]
per-ex loss: 0.400362  [   14/   88]
per-ex loss: 0.620873  [   15/   88]
per-ex loss: 0.384127  [   16/   88]
per-ex loss: 0.588947  [   17/   88]
per-ex loss: 0.399477  [   18/   88]
per-ex loss: 0.505183  [   19/   88]
per-ex loss: 0.356219  [   20/   88]
per-ex loss: 0.332615  [   21/   88]
per-ex loss: 0.555884  [   22/   88]
per-ex loss: 0.398227  [   23/   88]
per-ex loss: 0.321128  [   24/   88]
per-ex loss: 0.302707  [   25/   88]
per-ex loss: 0.542924  [   26/   88]
per-ex loss: 0.501990  [   27/   88]
per-ex loss: 0.383741  [   28/   88]
per-ex loss: 0.420500  [   29/   88]
per-ex loss: 0.357451  [   30/   88]
per-ex loss: 0.486315  [   31/   88]
per-ex loss: 0.542501  [   32/   88]
per-ex loss: 0.459670  [   33/   88]
per-ex loss: 0.555708  [   34/   88]
per-ex loss: 0.324013  [   35/   88]
per-ex loss: 0.363318  [   36/   88]
per-ex loss: 0.300973  [   37/   88]
per-ex loss: 0.554073  [   38/   88]
per-ex loss: 0.316316  [   39/   88]
per-ex loss: 0.411138  [   40/   88]
per-ex loss: 0.570945  [   41/   88]
per-ex loss: 0.454603  [   42/   88]
per-ex loss: 0.400891  [   43/   88]
per-ex loss: 0.343076  [   44/   88]
per-ex loss: 0.371729  [   45/   88]
per-ex loss: 0.411465  [   46/   88]
per-ex loss: 0.551506  [   47/   88]
per-ex loss: 0.385419  [   48/   88]
per-ex loss: 0.323284  [   49/   88]
per-ex loss: 0.441511  [   50/   88]
per-ex loss: 0.466286  [   51/   88]
per-ex loss: 0.580110  [   52/   88]
per-ex loss: 0.396338  [   53/   88]
per-ex loss: 0.372037  [   54/   88]
per-ex loss: 0.667214  [   55/   88]
per-ex loss: 0.433375  [   56/   88]
per-ex loss: 0.632911  [   57/   88]
per-ex loss: 0.427008  [   58/   88]
per-ex loss: 0.531515  [   59/   88]
per-ex loss: 0.226686  [   60/   88]
per-ex loss: 0.628616  [   61/   88]
per-ex loss: 0.344875  [   62/   88]
per-ex loss: 0.476438  [   63/   88]
per-ex loss: 0.353679  [   64/   88]
per-ex loss: 0.533984  [   65/   88]
per-ex loss: 0.347872  [   66/   88]
per-ex loss: 0.556334  [   67/   88]
per-ex loss: 0.378352  [   68/   88]
per-ex loss: 0.359233  [   69/   88]
per-ex loss: 0.614020  [   70/   88]
per-ex loss: 0.516634  [   71/   88]
per-ex loss: 0.497957  [   72/   88]
per-ex loss: 0.290890  [   73/   88]
per-ex loss: 0.422868  [   74/   88]
per-ex loss: 0.372299  [   75/   88]
per-ex loss: 0.338558  [   76/   88]
per-ex loss: 0.519940  [   77/   88]
per-ex loss: 0.285295  [   78/   88]
per-ex loss: 0.355743  [   79/   88]
per-ex loss: 0.354483  [   80/   88]
per-ex loss: 0.438077  [   81/   88]
per-ex loss: 0.530819  [   82/   88]
per-ex loss: 0.368961  [   83/   88]
per-ex loss: 0.538589  [   84/   88]
per-ex loss: 0.476166  [   85/   88]
per-ex loss: 0.572297  [   86/   88]
per-ex loss: 0.318153  [   87/   88]
per-ex loss: 0.368381  [   88/   88]
Train Error: Avg loss: 0.43590552
validation Error: 
 Avg loss: 0.54040719 
 F1: 0.489920 
 Precision: 0.574615 
 Recall: 0.426985
 IoU: 0.324433

test Error: 
 Avg loss: 0.49121321 
 F1: 0.556707 
 Precision: 0.660153 
 Recall: 0.481289
 IoU: 0.385720

We have finished training iteration 125
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_123_.pth
per-ex loss: 0.559881  [    1/   88]
per-ex loss: 0.656181  [    2/   88]
per-ex loss: 0.548232  [    3/   88]
per-ex loss: 0.321236  [    4/   88]
per-ex loss: 0.304807  [    5/   88]
per-ex loss: 0.414891  [    6/   88]
per-ex loss: 0.521514  [    7/   88]
per-ex loss: 0.569638  [    8/   88]
per-ex loss: 0.553110  [    9/   88]
per-ex loss: 0.356696  [   10/   88]
per-ex loss: 0.479794  [   11/   88]
per-ex loss: 0.374654  [   12/   88]
per-ex loss: 0.303474  [   13/   88]
per-ex loss: 0.401958  [   14/   88]
per-ex loss: 0.607280  [   15/   88]
per-ex loss: 0.390204  [   16/   88]
per-ex loss: 0.408355  [   17/   88]
per-ex loss: 0.297686  [   18/   88]
per-ex loss: 0.447487  [   19/   88]
per-ex loss: 0.534263  [   20/   88]
per-ex loss: 0.560636  [   21/   88]
per-ex loss: 0.337413  [   22/   88]
per-ex loss: 0.386004  [   23/   88]
per-ex loss: 0.518518  [   24/   88]
per-ex loss: 0.430046  [   25/   88]
per-ex loss: 0.364275  [   26/   88]
per-ex loss: 0.447963  [   27/   88]
per-ex loss: 0.536915  [   28/   88]
per-ex loss: 0.355661  [   29/   88]
per-ex loss: 0.423073  [   30/   88]
per-ex loss: 0.212163  [   31/   88]
per-ex loss: 0.295066  [   32/   88]
per-ex loss: 0.500617  [   33/   88]
per-ex loss: 0.591533  [   34/   88]
per-ex loss: 0.383139  [   35/   88]
per-ex loss: 0.414462  [   36/   88]
per-ex loss: 0.385425  [   37/   88]
per-ex loss: 0.320996  [   38/   88]
per-ex loss: 0.411082  [   39/   88]
per-ex loss: 0.538540  [   40/   88]
per-ex loss: 0.324683  [   41/   88]
per-ex loss: 0.290200  [   42/   88]
per-ex loss: 0.390402  [   43/   88]
per-ex loss: 0.443259  [   44/   88]
per-ex loss: 0.361857  [   45/   88]
per-ex loss: 0.305581  [   46/   88]
per-ex loss: 0.439537  [   47/   88]
per-ex loss: 0.565894  [   48/   88]
per-ex loss: 0.352213  [   49/   88]
per-ex loss: 0.497521  [   50/   88]
per-ex loss: 0.353352  [   51/   88]
per-ex loss: 0.422648  [   52/   88]
per-ex loss: 0.344397  [   53/   88]
per-ex loss: 0.401377  [   54/   88]
per-ex loss: 0.337941  [   55/   88]
per-ex loss: 0.351498  [   56/   88]
per-ex loss: 0.346277  [   57/   88]
per-ex loss: 0.339159  [   58/   88]
per-ex loss: 0.408570  [   59/   88]
per-ex loss: 0.548225  [   60/   88]
per-ex loss: 0.299370  [   61/   88]
per-ex loss: 0.550077  [   62/   88]
per-ex loss: 0.405318  [   63/   88]
per-ex loss: 0.387710  [   64/   88]
per-ex loss: 0.469843  [   65/   88]
per-ex loss: 0.564082  [   66/   88]
per-ex loss: 0.615989  [   67/   88]
per-ex loss: 0.518703  [   68/   88]
per-ex loss: 0.625334  [   69/   88]
per-ex loss: 0.391072  [   70/   88]
per-ex loss: 0.302873  [   71/   88]
per-ex loss: 0.504530  [   72/   88]
per-ex loss: 0.438838  [   73/   88]
per-ex loss: 0.395443  [   74/   88]
per-ex loss: 0.543984  [   75/   88]
per-ex loss: 0.352787  [   76/   88]
per-ex loss: 0.424929  [   77/   88]
per-ex loss: 0.358231  [   78/   88]
per-ex loss: 0.350034  [   79/   88]
per-ex loss: 0.554232  [   80/   88]
per-ex loss: 0.607836  [   81/   88]
per-ex loss: 0.460567  [   82/   88]
per-ex loss: 0.408032  [   83/   88]
per-ex loss: 0.345178  [   84/   88]
per-ex loss: 0.334576  [   85/   88]
per-ex loss: 0.473364  [   86/   88]
per-ex loss: 0.379480  [   87/   88]
per-ex loss: 0.666793  [   88/   88]
Train Error: Avg loss: 0.43203029
validation Error: 
 Avg loss: 0.53538162 
 F1: 0.494307 
 Precision: 0.529741 
 Recall: 0.463316
 IoU: 0.328292

test Error: 
 Avg loss: 0.48577179 
 F1: 0.559387 
 Precision: 0.596110 
 Recall: 0.526926
 IoU: 0.388298

We have finished training iteration 126
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_124_.pth
per-ex loss: 0.320755  [    1/   88]
per-ex loss: 0.357437  [    2/   88]
per-ex loss: 0.375113  [    3/   88]
per-ex loss: 0.549181  [    4/   88]
per-ex loss: 0.556867  [    5/   88]
per-ex loss: 0.427290  [    6/   88]
per-ex loss: 0.535941  [    7/   88]
per-ex loss: 0.384293  [    8/   88]
per-ex loss: 0.293128  [    9/   88]
per-ex loss: 0.328002  [   10/   88]
per-ex loss: 0.508161  [   11/   88]
per-ex loss: 0.286845  [   12/   88]
per-ex loss: 0.510241  [   13/   88]
per-ex loss: 0.346760  [   14/   88]
per-ex loss: 0.386568  [   15/   88]
per-ex loss: 0.343548  [   16/   88]
per-ex loss: 0.343811  [   17/   88]
per-ex loss: 0.410654  [   18/   88]
per-ex loss: 0.347159  [   19/   88]
per-ex loss: 0.407929  [   20/   88]
per-ex loss: 0.434846  [   21/   88]
per-ex loss: 0.358562  [   22/   88]
per-ex loss: 0.604613  [   23/   88]
per-ex loss: 0.492619  [   24/   88]
per-ex loss: 0.363359  [   25/   88]
per-ex loss: 0.376323  [   26/   88]
per-ex loss: 0.329338  [   27/   88]
per-ex loss: 0.611502  [   28/   88]
per-ex loss: 0.571792  [   29/   88]
per-ex loss: 0.515166  [   30/   88]
per-ex loss: 0.637719  [   31/   88]
per-ex loss: 0.554081  [   32/   88]
per-ex loss: 0.323963  [   33/   88]
per-ex loss: 0.326883  [   34/   88]
per-ex loss: 0.427977  [   35/   88]
per-ex loss: 0.541670  [   36/   88]
per-ex loss: 0.480406  [   37/   88]
per-ex loss: 0.328612  [   38/   88]
per-ex loss: 0.556072  [   39/   88]
per-ex loss: 0.588642  [   40/   88]
per-ex loss: 0.383255  [   41/   88]
per-ex loss: 0.370872  [   42/   88]
per-ex loss: 0.428361  [   43/   88]
per-ex loss: 0.301121  [   44/   88]
per-ex loss: 0.329912  [   45/   88]
per-ex loss: 0.422944  [   46/   88]
per-ex loss: 0.383047  [   47/   88]
per-ex loss: 0.384187  [   48/   88]
per-ex loss: 0.461412  [   49/   88]
per-ex loss: 0.438427  [   50/   88]
per-ex loss: 0.361403  [   51/   88]
per-ex loss: 0.624685  [   52/   88]
per-ex loss: 0.565043  [   53/   88]
per-ex loss: 0.495023  [   54/   88]
per-ex loss: 0.446314  [   55/   88]
per-ex loss: 0.510506  [   56/   88]
per-ex loss: 0.384378  [   57/   88]
per-ex loss: 0.323964  [   58/   88]
per-ex loss: 0.356712  [   59/   88]
per-ex loss: 0.365005  [   60/   88]
per-ex loss: 0.473304  [   61/   88]
per-ex loss: 0.521019  [   62/   88]
per-ex loss: 0.352154  [   63/   88]
per-ex loss: 0.416391  [   64/   88]
per-ex loss: 0.304424  [   65/   88]
per-ex loss: 0.436216  [   66/   88]
per-ex loss: 0.557279  [   67/   88]
per-ex loss: 0.361525  [   68/   88]
per-ex loss: 0.226319  [   69/   88]
per-ex loss: 0.372921  [   70/   88]
per-ex loss: 0.532387  [   71/   88]
per-ex loss: 0.498625  [   72/   88]
per-ex loss: 0.573700  [   73/   88]
per-ex loss: 0.520111  [   74/   88]
per-ex loss: 0.328373  [   75/   88]
per-ex loss: 0.329442  [   76/   88]
per-ex loss: 0.348190  [   77/   88]
per-ex loss: 0.565527  [   78/   88]
per-ex loss: 0.391681  [   79/   88]
per-ex loss: 0.578600  [   80/   88]
per-ex loss: 0.583742  [   81/   88]
per-ex loss: 0.444640  [   82/   88]
per-ex loss: 0.577048  [   83/   88]
per-ex loss: 0.411971  [   84/   88]
per-ex loss: 0.650736  [   85/   88]
per-ex loss: 0.301926  [   86/   88]
per-ex loss: 0.470779  [   87/   88]
per-ex loss: 0.339126  [   88/   88]
Train Error: Avg loss: 0.43464260
validation Error: 
 Avg loss: 0.53166922 
 F1: 0.498684 
 Precision: 0.528010 
 Recall: 0.472443
 IoU: 0.332164

test Error: 
 Avg loss: 0.48152549 
 F1: 0.562698 
 Precision: 0.593481 
 Recall: 0.534951
 IoU: 0.391496

We have finished training iteration 127
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_125_.pth
per-ex loss: 0.426998  [    1/   88]
per-ex loss: 0.299045  [    2/   88]
per-ex loss: 0.355481  [    3/   88]
per-ex loss: 0.335271  [    4/   88]
per-ex loss: 0.342961  [    5/   88]
per-ex loss: 0.427965  [    6/   88]
per-ex loss: 0.508872  [    7/   88]
per-ex loss: 0.590337  [    8/   88]
per-ex loss: 0.456838  [    9/   88]
per-ex loss: 0.321972  [   10/   88]
per-ex loss: 0.348613  [   11/   88]
per-ex loss: 0.557531  [   12/   88]
per-ex loss: 0.316884  [   13/   88]
per-ex loss: 0.475716  [   14/   88]
per-ex loss: 0.512124  [   15/   88]
per-ex loss: 0.599974  [   16/   88]
per-ex loss: 0.367876  [   17/   88]
per-ex loss: 0.615131  [   18/   88]
per-ex loss: 0.385170  [   19/   88]
per-ex loss: 0.610518  [   20/   88]
per-ex loss: 0.325523  [   21/   88]
per-ex loss: 0.308642  [   22/   88]
per-ex loss: 0.334114  [   23/   88]
per-ex loss: 0.433339  [   24/   88]
per-ex loss: 0.546846  [   25/   88]
per-ex loss: 0.521706  [   26/   88]
per-ex loss: 0.466127  [   27/   88]
per-ex loss: 0.367230  [   28/   88]
per-ex loss: 0.543516  [   29/   88]
per-ex loss: 0.526640  [   30/   88]
per-ex loss: 0.347822  [   31/   88]
per-ex loss: 0.438228  [   32/   88]
per-ex loss: 0.303146  [   33/   88]
per-ex loss: 0.423075  [   34/   88]
per-ex loss: 0.306303  [   35/   88]
per-ex loss: 0.586271  [   36/   88]
per-ex loss: 0.613038  [   37/   88]
per-ex loss: 0.500541  [   38/   88]
per-ex loss: 0.446638  [   39/   88]
per-ex loss: 0.443452  [   40/   88]
per-ex loss: 0.347896  [   41/   88]
per-ex loss: 0.436320  [   42/   88]
per-ex loss: 0.645367  [   43/   88]
per-ex loss: 0.434286  [   44/   88]
per-ex loss: 0.559008  [   45/   88]
per-ex loss: 0.417545  [   46/   88]
per-ex loss: 0.357982  [   47/   88]
per-ex loss: 0.569737  [   48/   88]
per-ex loss: 0.334109  [   49/   88]
per-ex loss: 0.310681  [   50/   88]
per-ex loss: 0.370490  [   51/   88]
per-ex loss: 0.412740  [   52/   88]
per-ex loss: 0.315228  [   53/   88]
per-ex loss: 0.405732  [   54/   88]
per-ex loss: 0.652084  [   55/   88]
per-ex loss: 0.541732  [   56/   88]
per-ex loss: 0.451694  [   57/   88]
per-ex loss: 0.560001  [   58/   88]
per-ex loss: 0.405353  [   59/   88]
per-ex loss: 0.533430  [   60/   88]
per-ex loss: 0.417531  [   61/   88]
per-ex loss: 0.546590  [   62/   88]
per-ex loss: 0.453714  [   63/   88]
per-ex loss: 0.386958  [   64/   88]
per-ex loss: 0.470199  [   65/   88]
per-ex loss: 0.245106  [   66/   88]
per-ex loss: 0.374357  [   67/   88]
per-ex loss: 0.364692  [   68/   88]
per-ex loss: 0.353917  [   69/   88]
per-ex loss: 0.444369  [   70/   88]
per-ex loss: 0.562941  [   71/   88]
per-ex loss: 0.454484  [   72/   88]
per-ex loss: 0.324653  [   73/   88]
per-ex loss: 0.380374  [   74/   88]
per-ex loss: 0.369123  [   75/   88]
per-ex loss: 0.328454  [   76/   88]
per-ex loss: 0.583357  [   77/   88]
per-ex loss: 0.574701  [   78/   88]
per-ex loss: 0.319080  [   79/   88]
per-ex loss: 0.298462  [   80/   88]
per-ex loss: 0.579401  [   81/   88]
per-ex loss: 0.557770  [   82/   88]
per-ex loss: 0.566588  [   83/   88]
per-ex loss: 0.394857  [   84/   88]
per-ex loss: 0.403505  [   85/   88]
per-ex loss: 0.377494  [   86/   88]
per-ex loss: 0.345703  [   87/   88]
per-ex loss: 0.362983  [   88/   88]
Train Error: Avg loss: 0.43795745
validation Error: 
 Avg loss: 0.53242307 
 F1: 0.495535 
 Precision: 0.547459 
 Recall: 0.452607
 IoU: 0.329376

test Error: 
 Avg loss: 0.47942395 
 F1: 0.574075 
 Precision: 0.612854 
 Recall: 0.539911
 IoU: 0.402598

We have finished training iteration 128
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_126_.pth
per-ex loss: 0.321909  [    1/   88]
per-ex loss: 0.567775  [    2/   88]
per-ex loss: 0.553544  [    3/   88]
per-ex loss: 0.434160  [    4/   88]
per-ex loss: 0.330126  [    5/   88]
per-ex loss: 0.449252  [    6/   88]
per-ex loss: 0.379569  [    7/   88]
per-ex loss: 0.304832  [    8/   88]
per-ex loss: 0.548939  [    9/   88]
per-ex loss: 0.359396  [   10/   88]
per-ex loss: 0.378487  [   11/   88]
per-ex loss: 0.439774  [   12/   88]
per-ex loss: 0.327406  [   13/   88]
per-ex loss: 0.465839  [   14/   88]
per-ex loss: 0.555527  [   15/   88]
per-ex loss: 0.537651  [   16/   88]
per-ex loss: 0.416877  [   17/   88]
per-ex loss: 0.536053  [   18/   88]
per-ex loss: 0.342310  [   19/   88]
per-ex loss: 0.301957  [   20/   88]
per-ex loss: 0.352070  [   21/   88]
per-ex loss: 0.324761  [   22/   88]
per-ex loss: 0.391333  [   23/   88]
per-ex loss: 0.640079  [   24/   88]
per-ex loss: 0.319938  [   25/   88]
per-ex loss: 0.368213  [   26/   88]
per-ex loss: 0.354870  [   27/   88]
per-ex loss: 0.552862  [   28/   88]
per-ex loss: 0.416723  [   29/   88]
per-ex loss: 0.388277  [   30/   88]
per-ex loss: 0.374032  [   31/   88]
per-ex loss: 0.490867  [   32/   88]
per-ex loss: 0.417715  [   33/   88]
per-ex loss: 0.428440  [   34/   88]
per-ex loss: 0.313542  [   35/   88]
per-ex loss: 0.416333  [   36/   88]
per-ex loss: 0.543490  [   37/   88]
per-ex loss: 0.347418  [   38/   88]
per-ex loss: 0.600265  [   39/   88]
per-ex loss: 0.424176  [   40/   88]
per-ex loss: 0.397590  [   41/   88]
per-ex loss: 0.492889  [   42/   88]
per-ex loss: 0.443850  [   43/   88]
per-ex loss: 0.425468  [   44/   88]
per-ex loss: 0.388706  [   45/   88]
per-ex loss: 0.487246  [   46/   88]
per-ex loss: 0.466056  [   47/   88]
per-ex loss: 0.462329  [   48/   88]
per-ex loss: 0.352266  [   49/   88]
per-ex loss: 0.560221  [   50/   88]
per-ex loss: 0.532545  [   51/   88]
per-ex loss: 0.538540  [   52/   88]
per-ex loss: 0.627342  [   53/   88]
per-ex loss: 0.603536  [   54/   88]
per-ex loss: 0.254103  [   55/   88]
per-ex loss: 0.356910  [   56/   88]
per-ex loss: 0.497282  [   57/   88]
per-ex loss: 0.411004  [   58/   88]
per-ex loss: 0.613711  [   59/   88]
per-ex loss: 0.415840  [   60/   88]
per-ex loss: 0.364836  [   61/   88]
per-ex loss: 0.374045  [   62/   88]
per-ex loss: 0.309569  [   63/   88]
per-ex loss: 0.313747  [   64/   88]
per-ex loss: 0.497835  [   65/   88]
per-ex loss: 0.428527  [   66/   88]
per-ex loss: 0.355236  [   67/   88]
per-ex loss: 0.329239  [   68/   88]
per-ex loss: 0.347940  [   69/   88]
per-ex loss: 0.318719  [   70/   88]
per-ex loss: 0.493894  [   71/   88]
per-ex loss: 0.406139  [   72/   88]
per-ex loss: 0.645489  [   73/   88]
per-ex loss: 0.576159  [   74/   88]
per-ex loss: 0.338633  [   75/   88]
per-ex loss: 0.533197  [   76/   88]
per-ex loss: 0.573195  [   77/   88]
per-ex loss: 0.394603  [   78/   88]
per-ex loss: 0.603374  [   79/   88]
per-ex loss: 0.377014  [   80/   88]
per-ex loss: 0.405445  [   81/   88]
per-ex loss: 0.554434  [   82/   88]
per-ex loss: 0.456820  [   83/   88]
per-ex loss: 0.442003  [   84/   88]
per-ex loss: 0.374571  [   85/   88]
per-ex loss: 0.305659  [   86/   88]
per-ex loss: 0.604362  [   87/   88]
per-ex loss: 0.521496  [   88/   88]
Train Error: Avg loss: 0.43852732
validation Error: 
 Avg loss: 0.53124562 
 F1: 0.501042 
 Precision: 0.592771 
 Recall: 0.433898
 IoU: 0.334260

test Error: 
 Avg loss: 0.48230353 
 F1: 0.566347 
 Precision: 0.642811 
 Recall: 0.506141
 IoU: 0.395038

We have finished training iteration 129
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_127_.pth
per-ex loss: 0.452662  [    1/   88]
per-ex loss: 0.381013  [    2/   88]
per-ex loss: 0.528547  [    3/   88]
per-ex loss: 0.324184  [    4/   88]
per-ex loss: 0.557937  [    5/   88]
per-ex loss: 0.331418  [    6/   88]
per-ex loss: 0.560716  [    7/   88]
per-ex loss: 0.397774  [    8/   88]
per-ex loss: 0.634921  [    9/   88]
per-ex loss: 0.405770  [   10/   88]
per-ex loss: 0.367135  [   11/   88]
per-ex loss: 0.382240  [   12/   88]
per-ex loss: 0.233890  [   13/   88]
per-ex loss: 0.503088  [   14/   88]
per-ex loss: 0.426728  [   15/   88]
per-ex loss: 0.451023  [   16/   88]
per-ex loss: 0.349644  [   17/   88]
per-ex loss: 0.474243  [   18/   88]
per-ex loss: 0.298157  [   19/   88]
per-ex loss: 0.423289  [   20/   88]
per-ex loss: 0.372398  [   21/   88]
per-ex loss: 0.508526  [   22/   88]
per-ex loss: 0.354485  [   23/   88]
per-ex loss: 0.495837  [   24/   88]
per-ex loss: 0.539204  [   25/   88]
per-ex loss: 0.616131  [   26/   88]
per-ex loss: 0.542368  [   27/   88]
per-ex loss: 0.455667  [   28/   88]
per-ex loss: 0.590203  [   29/   88]
per-ex loss: 0.580346  [   30/   88]
per-ex loss: 0.369611  [   31/   88]
per-ex loss: 0.554312  [   32/   88]
per-ex loss: 0.380277  [   33/   88]
per-ex loss: 0.482598  [   34/   88]
per-ex loss: 0.327482  [   35/   88]
per-ex loss: 0.512150  [   36/   88]
per-ex loss: 0.470587  [   37/   88]
per-ex loss: 0.309441  [   38/   88]
per-ex loss: 0.423187  [   39/   88]
per-ex loss: 0.415527  [   40/   88]
per-ex loss: 0.447078  [   41/   88]
per-ex loss: 0.597494  [   42/   88]
per-ex loss: 0.508229  [   43/   88]
per-ex loss: 0.401760  [   44/   88]
per-ex loss: 0.396826  [   45/   88]
per-ex loss: 0.588473  [   46/   88]
per-ex loss: 0.634911  [   47/   88]
per-ex loss: 0.648145  [   48/   88]
per-ex loss: 0.510570  [   49/   88]
per-ex loss: 0.543911  [   50/   88]
per-ex loss: 0.300587  [   51/   88]
per-ex loss: 0.381693  [   52/   88]
per-ex loss: 0.560205  [   53/   88]
per-ex loss: 0.548779  [   54/   88]
per-ex loss: 0.315187  [   55/   88]
per-ex loss: 0.496968  [   56/   88]
per-ex loss: 0.379254  [   57/   88]
per-ex loss: 0.388506  [   58/   88]
per-ex loss: 0.350742  [   59/   88]
per-ex loss: 0.422110  [   60/   88]
per-ex loss: 0.454638  [   61/   88]
per-ex loss: 0.422792  [   62/   88]
per-ex loss: 0.511225  [   63/   88]
per-ex loss: 0.577946  [   64/   88]
per-ex loss: 0.375017  [   65/   88]
per-ex loss: 0.322590  [   66/   88]
per-ex loss: 0.392509  [   67/   88]
per-ex loss: 0.606144  [   68/   88]
per-ex loss: 0.397785  [   69/   88]
per-ex loss: 0.328274  [   70/   88]
per-ex loss: 0.412652  [   71/   88]
per-ex loss: 0.363788  [   72/   88]
per-ex loss: 0.339352  [   73/   88]
per-ex loss: 0.383197  [   74/   88]
per-ex loss: 0.372672  [   75/   88]
per-ex loss: 0.308349  [   76/   88]
per-ex loss: 0.343057  [   77/   88]
per-ex loss: 0.357132  [   78/   88]
per-ex loss: 0.534406  [   79/   88]
per-ex loss: 0.342139  [   80/   88]
per-ex loss: 0.318971  [   81/   88]
per-ex loss: 0.361504  [   82/   88]
per-ex loss: 0.543608  [   83/   88]
per-ex loss: 0.367699  [   84/   88]
per-ex loss: 0.318660  [   85/   88]
per-ex loss: 0.337456  [   86/   88]
per-ex loss: 0.344998  [   87/   88]
per-ex loss: 0.314899  [   88/   88]
Train Error: Avg loss: 0.43476821
validation Error: 
 Avg loss: 0.54989234 
 F1: 0.479787 
 Precision: 0.565715 
 Recall: 0.416520
 IoU: 0.315605

test Error: 
 Avg loss: 0.49823617 
 F1: 0.548912 
 Precision: 0.657626 
 Recall: 0.471043
 IoU: 0.378276

We have finished training iteration 130
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_128_.pth
per-ex loss: 0.480474  [    1/   88]
per-ex loss: 0.619652  [    2/   88]
per-ex loss: 0.620367  [    3/   88]
per-ex loss: 0.569117  [    4/   88]
per-ex loss: 0.369760  [    5/   88]
per-ex loss: 0.359814  [    6/   88]
per-ex loss: 0.444316  [    7/   88]
per-ex loss: 0.561700  [    8/   88]
per-ex loss: 0.376073  [    9/   88]
per-ex loss: 0.424595  [   10/   88]
per-ex loss: 0.365313  [   11/   88]
per-ex loss: 0.406460  [   12/   88]
per-ex loss: 0.555942  [   13/   88]
per-ex loss: 0.317706  [   14/   88]
per-ex loss: 0.519369  [   15/   88]
per-ex loss: 0.362877  [   16/   88]
per-ex loss: 0.217337  [   17/   88]
per-ex loss: 0.515590  [   18/   88]
per-ex loss: 0.332077  [   19/   88]
per-ex loss: 0.519415  [   20/   88]
per-ex loss: 0.343434  [   21/   88]
per-ex loss: 0.297580  [   22/   88]
per-ex loss: 0.305304  [   23/   88]
per-ex loss: 0.344386  [   24/   88]
per-ex loss: 0.395949  [   25/   88]
per-ex loss: 0.337670  [   26/   88]
per-ex loss: 0.384877  [   27/   88]
per-ex loss: 0.454659  [   28/   88]
per-ex loss: 0.545065  [   29/   88]
per-ex loss: 0.369011  [   30/   88]
per-ex loss: 0.316691  [   31/   88]
per-ex loss: 0.444128  [   32/   88]
per-ex loss: 0.408083  [   33/   88]
per-ex loss: 0.349056  [   34/   88]
per-ex loss: 0.382537  [   35/   88]
per-ex loss: 0.545815  [   36/   88]
per-ex loss: 0.434928  [   37/   88]
per-ex loss: 0.370138  [   38/   88]
per-ex loss: 0.472601  [   39/   88]
per-ex loss: 0.446220  [   40/   88]
per-ex loss: 0.450107  [   41/   88]
per-ex loss: 0.321389  [   42/   88]
per-ex loss: 0.317243  [   43/   88]
per-ex loss: 0.377124  [   44/   88]
per-ex loss: 0.484083  [   45/   88]
per-ex loss: 0.332188  [   46/   88]
per-ex loss: 0.315374  [   47/   88]
per-ex loss: 0.420578  [   48/   88]
per-ex loss: 0.554414  [   49/   88]
per-ex loss: 0.422129  [   50/   88]
per-ex loss: 0.524553  [   51/   88]
per-ex loss: 0.437351  [   52/   88]
per-ex loss: 0.489158  [   53/   88]
per-ex loss: 0.321298  [   54/   88]
per-ex loss: 0.471992  [   55/   88]
per-ex loss: 0.523642  [   56/   88]
per-ex loss: 0.514880  [   57/   88]
per-ex loss: 0.365734  [   58/   88]
per-ex loss: 0.390312  [   59/   88]
per-ex loss: 0.348845  [   60/   88]
per-ex loss: 0.414264  [   61/   88]
per-ex loss: 0.686339  [   62/   88]
per-ex loss: 0.367606  [   63/   88]
per-ex loss: 0.348076  [   64/   88]
per-ex loss: 0.362806  [   65/   88]
per-ex loss: 0.536007  [   66/   88]
per-ex loss: 0.330991  [   67/   88]
per-ex loss: 0.480515  [   68/   88]
per-ex loss: 0.297510  [   69/   88]
per-ex loss: 0.576789  [   70/   88]
per-ex loss: 0.632629  [   71/   88]
per-ex loss: 0.398256  [   72/   88]
per-ex loss: 0.575311  [   73/   88]
per-ex loss: 0.656547  [   74/   88]
per-ex loss: 0.564627  [   75/   88]
per-ex loss: 0.398268  [   76/   88]
per-ex loss: 0.348542  [   77/   88]
per-ex loss: 0.409356  [   78/   88]
per-ex loss: 0.395289  [   79/   88]
per-ex loss: 0.360271  [   80/   88]
per-ex loss: 0.290556  [   81/   88]
per-ex loss: 0.544734  [   82/   88]
per-ex loss: 0.411465  [   83/   88]
per-ex loss: 0.463569  [   84/   88]
per-ex loss: 0.402429  [   85/   88]
per-ex loss: 0.583825  [   86/   88]
per-ex loss: 0.527928  [   87/   88]
per-ex loss: 0.665141  [   88/   88]
Train Error: Avg loss: 0.43520597
validation Error: 
 Avg loss: 0.53536186 
 F1: 0.497221 
 Precision: 0.591409 
 Recall: 0.428913
 IoU: 0.330868

test Error: 
 Avg loss: 0.49055753 
 F1: 0.557876 
 Precision: 0.658316 
 Recall: 0.484027
 IoU: 0.386843

We have finished training iteration 131
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_129_.pth
per-ex loss: 0.553173  [    1/   88]
per-ex loss: 0.402803  [    2/   88]
per-ex loss: 0.549928  [    3/   88]
per-ex loss: 0.516023  [    4/   88]
per-ex loss: 0.249578  [    5/   88]
per-ex loss: 0.528879  [    6/   88]
per-ex loss: 0.409016  [    7/   88]
per-ex loss: 0.360112  [    8/   88]
per-ex loss: 0.495380  [    9/   88]
per-ex loss: 0.330864  [   10/   88]
per-ex loss: 0.329787  [   11/   88]
per-ex loss: 0.361745  [   12/   88]
per-ex loss: 0.337965  [   13/   88]
per-ex loss: 0.445921  [   14/   88]
per-ex loss: 0.516989  [   15/   88]
per-ex loss: 0.301885  [   16/   88]
per-ex loss: 0.347837  [   17/   88]
per-ex loss: 0.425753  [   18/   88]
per-ex loss: 0.626793  [   19/   88]
per-ex loss: 0.418052  [   20/   88]
per-ex loss: 0.401594  [   21/   88]
per-ex loss: 0.409814  [   22/   88]
per-ex loss: 0.538150  [   23/   88]
per-ex loss: 0.447554  [   24/   88]
per-ex loss: 0.292785  [   25/   88]
per-ex loss: 0.497124  [   26/   88]
per-ex loss: 0.627933  [   27/   88]
per-ex loss: 0.330788  [   28/   88]
per-ex loss: 0.634212  [   29/   88]
per-ex loss: 0.431150  [   30/   88]
per-ex loss: 0.338471  [   31/   88]
per-ex loss: 0.395606  [   32/   88]
per-ex loss: 0.535358  [   33/   88]
per-ex loss: 0.435372  [   34/   88]
per-ex loss: 0.323976  [   35/   88]
per-ex loss: 0.353423  [   36/   88]
per-ex loss: 0.450776  [   37/   88]
per-ex loss: 0.294519  [   38/   88]
per-ex loss: 0.549320  [   39/   88]
per-ex loss: 0.448857  [   40/   88]
per-ex loss: 0.527321  [   41/   88]
per-ex loss: 0.372156  [   42/   88]
per-ex loss: 0.516358  [   43/   88]
per-ex loss: 0.562996  [   44/   88]
per-ex loss: 0.533680  [   45/   88]
per-ex loss: 0.424339  [   46/   88]
per-ex loss: 0.408022  [   47/   88]
per-ex loss: 0.402437  [   48/   88]
per-ex loss: 0.355459  [   49/   88]
per-ex loss: 0.603483  [   50/   88]
per-ex loss: 0.345844  [   51/   88]
per-ex loss: 0.391229  [   52/   88]
per-ex loss: 0.300993  [   53/   88]
per-ex loss: 0.411583  [   54/   88]
per-ex loss: 0.437274  [   55/   88]
per-ex loss: 0.362448  [   56/   88]
per-ex loss: 0.330309  [   57/   88]
per-ex loss: 0.323527  [   58/   88]
per-ex loss: 0.537793  [   59/   88]
per-ex loss: 0.431926  [   60/   88]
per-ex loss: 0.374985  [   61/   88]
per-ex loss: 0.344379  [   62/   88]
per-ex loss: 0.357614  [   63/   88]
per-ex loss: 0.344524  [   64/   88]
per-ex loss: 0.326903  [   65/   88]
per-ex loss: 0.412064  [   66/   88]
per-ex loss: 0.553974  [   67/   88]
per-ex loss: 0.319719  [   68/   88]
per-ex loss: 0.309489  [   69/   88]
per-ex loss: 0.328617  [   70/   88]
per-ex loss: 0.542937  [   71/   88]
per-ex loss: 0.536678  [   72/   88]
per-ex loss: 0.354205  [   73/   88]
per-ex loss: 0.359172  [   74/   88]
per-ex loss: 0.349667  [   75/   88]
per-ex loss: 0.568788  [   76/   88]
per-ex loss: 0.521397  [   77/   88]
per-ex loss: 0.548032  [   78/   88]
per-ex loss: 0.613779  [   79/   88]
per-ex loss: 0.496644  [   80/   88]
per-ex loss: 0.390616  [   81/   88]
per-ex loss: 0.446777  [   82/   88]
per-ex loss: 0.527917  [   83/   88]
per-ex loss: 0.387420  [   84/   88]
per-ex loss: 0.465140  [   85/   88]
per-ex loss: 0.617882  [   86/   88]
per-ex loss: 0.338642  [   87/   88]
per-ex loss: 0.359330  [   88/   88]
Train Error: Avg loss: 0.43092883
validation Error: 
 Avg loss: 0.53412523 
 F1: 0.491281 
 Precision: 0.519349 
 Recall: 0.466090
 IoU: 0.325628

test Error: 
 Avg loss: 0.47797002 
 F1: 0.568996 
 Precision: 0.597129 
 Recall: 0.543394
 IoU: 0.397620

We have finished training iteration 132
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_130_.pth
per-ex loss: 0.473416  [    1/   88]
per-ex loss: 0.359531  [    2/   88]
per-ex loss: 0.490865  [    3/   88]
per-ex loss: 0.591038  [    4/   88]
per-ex loss: 0.340554  [    5/   88]
per-ex loss: 0.649444  [    6/   88]
per-ex loss: 0.373844  [    7/   88]
per-ex loss: 0.475488  [    8/   88]
per-ex loss: 0.294486  [    9/   88]
per-ex loss: 0.409166  [   10/   88]
per-ex loss: 0.338340  [   11/   88]
per-ex loss: 0.319985  [   12/   88]
per-ex loss: 0.400383  [   13/   88]
per-ex loss: 0.484777  [   14/   88]
per-ex loss: 0.325108  [   15/   88]
per-ex loss: 0.355618  [   16/   88]
per-ex loss: 0.311821  [   17/   88]
per-ex loss: 0.346109  [   18/   88]
per-ex loss: 0.371589  [   19/   88]
per-ex loss: 0.537203  [   20/   88]
per-ex loss: 0.575085  [   21/   88]
per-ex loss: 0.531165  [   22/   88]
per-ex loss: 0.323658  [   23/   88]
per-ex loss: 0.400299  [   24/   88]
per-ex loss: 0.353034  [   25/   88]
per-ex loss: 0.371884  [   26/   88]
per-ex loss: 0.351729  [   27/   88]
per-ex loss: 0.572046  [   28/   88]
per-ex loss: 0.614322  [   29/   88]
per-ex loss: 0.421754  [   30/   88]
per-ex loss: 0.357970  [   31/   88]
per-ex loss: 0.344586  [   32/   88]
per-ex loss: 0.595810  [   33/   88]
per-ex loss: 0.638188  [   34/   88]
per-ex loss: 0.475619  [   35/   88]
per-ex loss: 0.421354  [   36/   88]
per-ex loss: 0.375597  [   37/   88]
per-ex loss: 0.331663  [   38/   88]
per-ex loss: 0.441058  [   39/   88]
per-ex loss: 0.406824  [   40/   88]
per-ex loss: 0.557984  [   41/   88]
per-ex loss: 0.604421  [   42/   88]
per-ex loss: 0.442193  [   43/   88]
per-ex loss: 0.408206  [   44/   88]
per-ex loss: 0.324526  [   45/   88]
per-ex loss: 0.540311  [   46/   88]
per-ex loss: 0.502931  [   47/   88]
per-ex loss: 0.578678  [   48/   88]
per-ex loss: 0.337294  [   49/   88]
per-ex loss: 0.405753  [   50/   88]
per-ex loss: 0.540952  [   51/   88]
per-ex loss: 0.302143  [   52/   88]
per-ex loss: 0.227004  [   53/   88]
per-ex loss: 0.322742  [   54/   88]
per-ex loss: 0.499709  [   55/   88]
per-ex loss: 0.305959  [   56/   88]
per-ex loss: 0.524062  [   57/   88]
per-ex loss: 0.473882  [   58/   88]
per-ex loss: 0.551844  [   59/   88]
per-ex loss: 0.318680  [   60/   88]
per-ex loss: 0.424150  [   61/   88]
per-ex loss: 0.640753  [   62/   88]
per-ex loss: 0.374741  [   63/   88]
per-ex loss: 0.288758  [   64/   88]
per-ex loss: 0.457932  [   65/   88]
per-ex loss: 0.381630  [   66/   88]
per-ex loss: 0.375572  [   67/   88]
per-ex loss: 0.531985  [   68/   88]
per-ex loss: 0.518645  [   69/   88]
per-ex loss: 0.393096  [   70/   88]
per-ex loss: 0.573934  [   71/   88]
per-ex loss: 0.407528  [   72/   88]
per-ex loss: 0.336769  [   73/   88]
per-ex loss: 0.403043  [   74/   88]
per-ex loss: 0.517825  [   75/   88]
per-ex loss: 0.575450  [   76/   88]
per-ex loss: 0.382756  [   77/   88]
per-ex loss: 0.369415  [   78/   88]
per-ex loss: 0.397386  [   79/   88]
per-ex loss: 0.353136  [   80/   88]
per-ex loss: 0.525623  [   81/   88]
per-ex loss: 0.555463  [   82/   88]
per-ex loss: 0.358811  [   83/   88]
per-ex loss: 0.553372  [   84/   88]
per-ex loss: 0.438206  [   85/   88]
per-ex loss: 0.428148  [   86/   88]
per-ex loss: 0.400972  [   87/   88]
per-ex loss: 0.389959  [   88/   88]
Train Error: Avg loss: 0.43496304
validation Error: 
 Avg loss: 0.53403955 
 F1: 0.495632 
 Precision: 0.560147 
 Recall: 0.444444
 IoU: 0.329462

test Error: 
 Avg loss: 0.48453094 
 F1: 0.563365 
 Precision: 0.619350 
 Recall: 0.516661
 IoU: 0.392142

We have finished training iteration 133
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_131_.pth
per-ex loss: 0.296715  [    1/   88]
per-ex loss: 0.313241  [    2/   88]
per-ex loss: 0.554359  [    3/   88]
per-ex loss: 0.393122  [    4/   88]
per-ex loss: 0.314113  [    5/   88]
per-ex loss: 0.462956  [    6/   88]
per-ex loss: 0.300775  [    7/   88]
per-ex loss: 0.607359  [    8/   88]
per-ex loss: 0.517471  [    9/   88]
per-ex loss: 0.327728  [   10/   88]
per-ex loss: 0.387073  [   11/   88]
per-ex loss: 0.635954  [   12/   88]
per-ex loss: 0.598667  [   13/   88]
per-ex loss: 0.327568  [   14/   88]
per-ex loss: 0.329905  [   15/   88]
per-ex loss: 0.411173  [   16/   88]
per-ex loss: 0.466313  [   17/   88]
per-ex loss: 0.455163  [   18/   88]
per-ex loss: 0.375028  [   19/   88]
per-ex loss: 0.444851  [   20/   88]
per-ex loss: 0.360114  [   21/   88]
per-ex loss: 0.565570  [   22/   88]
per-ex loss: 0.631455  [   23/   88]
per-ex loss: 0.303478  [   24/   88]
per-ex loss: 0.322031  [   25/   88]
per-ex loss: 0.546862  [   26/   88]
per-ex loss: 0.557891  [   27/   88]
per-ex loss: 0.237476  [   28/   88]
per-ex loss: 0.522089  [   29/   88]
per-ex loss: 0.530745  [   30/   88]
per-ex loss: 0.610025  [   31/   88]
per-ex loss: 0.433682  [   32/   88]
per-ex loss: 0.428772  [   33/   88]
per-ex loss: 0.346984  [   34/   88]
per-ex loss: 0.513509  [   35/   88]
per-ex loss: 0.374037  [   36/   88]
per-ex loss: 0.457344  [   37/   88]
per-ex loss: 0.468235  [   38/   88]
per-ex loss: 0.553461  [   39/   88]
per-ex loss: 0.397139  [   40/   88]
per-ex loss: 0.353079  [   41/   88]
per-ex loss: 0.379850  [   42/   88]
per-ex loss: 0.381779  [   43/   88]
per-ex loss: 0.648215  [   44/   88]
per-ex loss: 0.386631  [   45/   88]
per-ex loss: 0.328443  [   46/   88]
per-ex loss: 0.387199  [   47/   88]
per-ex loss: 0.456961  [   48/   88]
per-ex loss: 0.380677  [   49/   88]
per-ex loss: 0.290830  [   50/   88]
per-ex loss: 0.561258  [   51/   88]
per-ex loss: 0.372780  [   52/   88]
per-ex loss: 0.327917  [   53/   88]
per-ex loss: 0.437803  [   54/   88]
per-ex loss: 0.409143  [   55/   88]
per-ex loss: 0.405314  [   56/   88]
per-ex loss: 0.366800  [   57/   88]
per-ex loss: 0.361809  [   58/   88]
per-ex loss: 0.429804  [   59/   88]
per-ex loss: 0.321638  [   60/   88]
per-ex loss: 0.382338  [   61/   88]
per-ex loss: 0.352995  [   62/   88]
per-ex loss: 0.533105  [   63/   88]
per-ex loss: 0.475395  [   64/   88]
per-ex loss: 0.383918  [   65/   88]
per-ex loss: 0.555365  [   66/   88]
per-ex loss: 0.331007  [   67/   88]
per-ex loss: 0.494110  [   68/   88]
per-ex loss: 0.550084  [   69/   88]
per-ex loss: 0.369083  [   70/   88]
per-ex loss: 0.558495  [   71/   88]
per-ex loss: 0.382608  [   72/   88]
per-ex loss: 0.533449  [   73/   88]
per-ex loss: 0.304176  [   74/   88]
per-ex loss: 0.509753  [   75/   88]
per-ex loss: 0.374119  [   76/   88]
per-ex loss: 0.600011  [   77/   88]
per-ex loss: 0.521084  [   78/   88]
per-ex loss: 0.393087  [   79/   88]
per-ex loss: 0.293201  [   80/   88]
per-ex loss: 0.437828  [   81/   88]
per-ex loss: 0.605233  [   82/   88]
per-ex loss: 0.544647  [   83/   88]
per-ex loss: 0.531954  [   84/   88]
per-ex loss: 0.462019  [   85/   88]
per-ex loss: 0.352789  [   86/   88]
per-ex loss: 0.391722  [   87/   88]
per-ex loss: 0.613768  [   88/   88]
Train Error: Avg loss: 0.43756485
validation Error: 
 Avg loss: 0.53490048 
 F1: 0.498460 
 Precision: 0.547329 
 Recall: 0.457603
 IoU: 0.331966

test Error: 
 Avg loss: 0.48359420 
 F1: 0.561689 
 Precision: 0.615074 
 Recall: 0.516831
 IoU: 0.390520

We have finished training iteration 134
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_132_.pth
per-ex loss: 0.392857  [    1/   88]
per-ex loss: 0.657035  [    2/   88]
per-ex loss: 0.398751  [    3/   88]
per-ex loss: 0.551161  [    4/   88]
per-ex loss: 0.543790  [    5/   88]
per-ex loss: 0.527308  [    6/   88]
per-ex loss: 0.375988  [    7/   88]
per-ex loss: 0.630175  [    8/   88]
per-ex loss: 0.332188  [    9/   88]
per-ex loss: 0.337667  [   10/   88]
per-ex loss: 0.378143  [   11/   88]
per-ex loss: 0.410854  [   12/   88]
per-ex loss: 0.516174  [   13/   88]
per-ex loss: 0.294501  [   14/   88]
per-ex loss: 0.539511  [   15/   88]
per-ex loss: 0.532139  [   16/   88]
per-ex loss: 0.515845  [   17/   88]
per-ex loss: 0.559098  [   18/   88]
per-ex loss: 0.378888  [   19/   88]
per-ex loss: 0.437757  [   20/   88]
per-ex loss: 0.538003  [   21/   88]
per-ex loss: 0.324453  [   22/   88]
per-ex loss: 0.344098  [   23/   88]
per-ex loss: 0.451384  [   24/   88]
per-ex loss: 0.438580  [   25/   88]
per-ex loss: 0.398066  [   26/   88]
per-ex loss: 0.557442  [   27/   88]
per-ex loss: 0.358826  [   28/   88]
per-ex loss: 0.285400  [   29/   88]
per-ex loss: 0.375174  [   30/   88]
per-ex loss: 0.541360  [   31/   88]
per-ex loss: 0.294904  [   32/   88]
per-ex loss: 0.549865  [   33/   88]
per-ex loss: 0.389069  [   34/   88]
per-ex loss: 0.389358  [   35/   88]
per-ex loss: 0.423350  [   36/   88]
per-ex loss: 0.373455  [   37/   88]
per-ex loss: 0.361888  [   38/   88]
per-ex loss: 0.545746  [   39/   88]
per-ex loss: 0.442103  [   40/   88]
per-ex loss: 0.390687  [   41/   88]
per-ex loss: 0.616417  [   42/   88]
per-ex loss: 0.334476  [   43/   88]
per-ex loss: 0.414806  [   44/   88]
per-ex loss: 0.573666  [   45/   88]
per-ex loss: 0.474771  [   46/   88]
per-ex loss: 0.586467  [   47/   88]
per-ex loss: 0.353152  [   48/   88]
per-ex loss: 0.310487  [   49/   88]
per-ex loss: 0.449368  [   50/   88]
per-ex loss: 0.333661  [   51/   88]
per-ex loss: 0.554515  [   52/   88]
per-ex loss: 0.417342  [   53/   88]
per-ex loss: 0.371366  [   54/   88]
per-ex loss: 0.516448  [   55/   88]
per-ex loss: 0.364021  [   56/   88]
per-ex loss: 0.359464  [   57/   88]
per-ex loss: 0.536689  [   58/   88]
per-ex loss: 0.330313  [   59/   88]
per-ex loss: 0.283983  [   60/   88]
per-ex loss: 0.323025  [   61/   88]
per-ex loss: 0.496664  [   62/   88]
per-ex loss: 0.560398  [   63/   88]
per-ex loss: 0.443784  [   64/   88]
per-ex loss: 0.518180  [   65/   88]
per-ex loss: 0.404171  [   66/   88]
per-ex loss: 0.303389  [   67/   88]
per-ex loss: 0.312523  [   68/   88]
per-ex loss: 0.356035  [   69/   88]
per-ex loss: 0.602273  [   70/   88]
per-ex loss: 0.636335  [   71/   88]
per-ex loss: 0.390539  [   72/   88]
per-ex loss: 0.531474  [   73/   88]
per-ex loss: 0.319342  [   74/   88]
per-ex loss: 0.376751  [   75/   88]
per-ex loss: 0.500893  [   76/   88]
per-ex loss: 0.339360  [   77/   88]
per-ex loss: 0.373845  [   78/   88]
per-ex loss: 0.493578  [   79/   88]
per-ex loss: 0.307631  [   80/   88]
per-ex loss: 0.229024  [   81/   88]
per-ex loss: 0.400657  [   82/   88]
per-ex loss: 0.444401  [   83/   88]
per-ex loss: 0.485170  [   84/   88]
per-ex loss: 0.384728  [   85/   88]
per-ex loss: 0.616393  [   86/   88]
per-ex loss: 0.423462  [   87/   88]
per-ex loss: 0.369597  [   88/   88]
Train Error: Avg loss: 0.43418265
validation Error: 
 Avg loss: 0.53122716 
 F1: 0.498623 
 Precision: 0.545490 
 Recall: 0.459173
 IoU: 0.332111

test Error: 
 Avg loss: 0.47778702 
 F1: 0.573131 
 Precision: 0.601625 
 Recall: 0.547215
 IoU: 0.401671

We have finished training iteration 135
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_133_.pth
per-ex loss: 0.524956  [    1/   88]
per-ex loss: 0.380934  [    2/   88]
per-ex loss: 0.317951  [    3/   88]
per-ex loss: 0.312274  [    4/   88]
per-ex loss: 0.351709  [    5/   88]
per-ex loss: 0.353018  [    6/   88]
per-ex loss: 0.490640  [    7/   88]
per-ex loss: 0.294878  [    8/   88]
per-ex loss: 0.414175  [    9/   88]
per-ex loss: 0.545425  [   10/   88]
per-ex loss: 0.306757  [   11/   88]
per-ex loss: 0.296545  [   12/   88]
per-ex loss: 0.369280  [   13/   88]
per-ex loss: 0.457678  [   14/   88]
per-ex loss: 0.411774  [   15/   88]
per-ex loss: 0.436964  [   16/   88]
per-ex loss: 0.344100  [   17/   88]
per-ex loss: 0.350874  [   18/   88]
per-ex loss: 0.624207  [   19/   88]
per-ex loss: 0.356271  [   20/   88]
per-ex loss: 0.325659  [   21/   88]
per-ex loss: 0.577850  [   22/   88]
per-ex loss: 0.615174  [   23/   88]
per-ex loss: 0.654619  [   24/   88]
per-ex loss: 0.578447  [   25/   88]
per-ex loss: 0.348735  [   26/   88]
per-ex loss: 0.398695  [   27/   88]
per-ex loss: 0.543322  [   28/   88]
per-ex loss: 0.447511  [   29/   88]
per-ex loss: 0.371470  [   30/   88]
per-ex loss: 0.315991  [   31/   88]
per-ex loss: 0.522593  [   32/   88]
per-ex loss: 0.518875  [   33/   88]
per-ex loss: 0.388859  [   34/   88]
per-ex loss: 0.535230  [   35/   88]
per-ex loss: 0.383554  [   36/   88]
per-ex loss: 0.535391  [   37/   88]
per-ex loss: 0.560067  [   38/   88]
per-ex loss: 0.347993  [   39/   88]
per-ex loss: 0.341588  [   40/   88]
per-ex loss: 0.340805  [   41/   88]
per-ex loss: 0.377407  [   42/   88]
per-ex loss: 0.284940  [   43/   88]
per-ex loss: 0.299278  [   44/   88]
per-ex loss: 0.543877  [   45/   88]
per-ex loss: 0.364880  [   46/   88]
per-ex loss: 0.534514  [   47/   88]
per-ex loss: 0.343037  [   48/   88]
per-ex loss: 0.491235  [   49/   88]
per-ex loss: 0.390183  [   50/   88]
per-ex loss: 0.589375  [   51/   88]
per-ex loss: 0.460451  [   52/   88]
per-ex loss: 0.401622  [   53/   88]
per-ex loss: 0.227644  [   54/   88]
per-ex loss: 0.332746  [   55/   88]
per-ex loss: 0.461931  [   56/   88]
per-ex loss: 0.404935  [   57/   88]
per-ex loss: 0.455103  [   58/   88]
per-ex loss: 0.460616  [   59/   88]
per-ex loss: 0.534684  [   60/   88]
per-ex loss: 0.405466  [   61/   88]
per-ex loss: 0.618566  [   62/   88]
per-ex loss: 0.427033  [   63/   88]
per-ex loss: 0.440911  [   64/   88]
per-ex loss: 0.387691  [   65/   88]
per-ex loss: 0.511590  [   66/   88]
per-ex loss: 0.324408  [   67/   88]
per-ex loss: 0.455897  [   68/   88]
per-ex loss: 0.405233  [   69/   88]
per-ex loss: 0.351318  [   70/   88]
per-ex loss: 0.352573  [   71/   88]
per-ex loss: 0.360679  [   72/   88]
per-ex loss: 0.315036  [   73/   88]
per-ex loss: 0.515631  [   74/   88]
per-ex loss: 0.551112  [   75/   88]
per-ex loss: 0.452192  [   76/   88]
per-ex loss: 0.573505  [   77/   88]
per-ex loss: 0.478603  [   78/   88]
per-ex loss: 0.562108  [   79/   88]
per-ex loss: 0.398199  [   80/   88]
per-ex loss: 0.332706  [   81/   88]
per-ex loss: 0.355280  [   82/   88]
per-ex loss: 0.351683  [   83/   88]
per-ex loss: 0.424024  [   84/   88]
per-ex loss: 0.568547  [   85/   88]
per-ex loss: 0.669211  [   86/   88]
per-ex loss: 0.364989  [   87/   88]
per-ex loss: 0.436974  [   88/   88]
Train Error: Avg loss: 0.43116432
validation Error: 
 Avg loss: 0.53192476 
 F1: 0.501247 
 Precision: 0.597111 
 Recall: 0.431906
 IoU: 0.334443

test Error: 
 Avg loss: 0.48382825 
 F1: 0.564444 
 Precision: 0.642591 
 Recall: 0.503244
 IoU: 0.393189

We have finished training iteration 136
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_134_.pth
per-ex loss: 0.514695  [    1/   88]
per-ex loss: 0.336560  [    2/   88]
per-ex loss: 0.318889  [    3/   88]
per-ex loss: 0.555727  [    4/   88]
per-ex loss: 0.324098  [    5/   88]
per-ex loss: 0.489538  [    6/   88]
per-ex loss: 0.335685  [    7/   88]
per-ex loss: 0.288357  [    8/   88]
per-ex loss: 0.329769  [    9/   88]
per-ex loss: 0.397491  [   10/   88]
per-ex loss: 0.563024  [   11/   88]
per-ex loss: 0.535316  [   12/   88]
per-ex loss: 0.536747  [   13/   88]
per-ex loss: 0.620461  [   14/   88]
per-ex loss: 0.564462  [   15/   88]
per-ex loss: 0.371109  [   16/   88]
per-ex loss: 0.565085  [   17/   88]
per-ex loss: 0.411777  [   18/   88]
per-ex loss: 0.398013  [   19/   88]
per-ex loss: 0.497318  [   20/   88]
per-ex loss: 0.391740  [   21/   88]
per-ex loss: 0.423029  [   22/   88]
per-ex loss: 0.411700  [   23/   88]
per-ex loss: 0.524845  [   24/   88]
per-ex loss: 0.517605  [   25/   88]
per-ex loss: 0.551026  [   26/   88]
per-ex loss: 0.348126  [   27/   88]
per-ex loss: 0.751817  [   28/   88]
per-ex loss: 0.405756  [   29/   88]
per-ex loss: 0.537161  [   30/   88]
per-ex loss: 0.296938  [   31/   88]
per-ex loss: 0.381743  [   32/   88]
per-ex loss: 0.456037  [   33/   88]
per-ex loss: 0.344509  [   34/   88]
per-ex loss: 0.530512  [   35/   88]
per-ex loss: 0.321585  [   36/   88]
per-ex loss: 0.528718  [   37/   88]
per-ex loss: 0.554115  [   38/   88]
per-ex loss: 0.350069  [   39/   88]
per-ex loss: 0.484784  [   40/   88]
per-ex loss: 0.409396  [   41/   88]
per-ex loss: 0.407826  [   42/   88]
per-ex loss: 0.376039  [   43/   88]
per-ex loss: 0.383362  [   44/   88]
per-ex loss: 0.321349  [   45/   88]
per-ex loss: 0.614524  [   46/   88]
per-ex loss: 0.345993  [   47/   88]
per-ex loss: 0.373885  [   48/   88]
per-ex loss: 0.305545  [   49/   88]
per-ex loss: 0.426176  [   50/   88]
per-ex loss: 0.333417  [   51/   88]
per-ex loss: 0.404795  [   52/   88]
per-ex loss: 0.453751  [   53/   88]
per-ex loss: 0.423636  [   54/   88]
per-ex loss: 0.636376  [   55/   88]
per-ex loss: 0.405973  [   56/   88]
per-ex loss: 0.533707  [   57/   88]
per-ex loss: 0.474221  [   58/   88]
per-ex loss: 0.415639  [   59/   88]
per-ex loss: 0.348485  [   60/   88]
per-ex loss: 0.577480  [   61/   88]
per-ex loss: 0.579263  [   62/   88]
per-ex loss: 0.396746  [   63/   88]
per-ex loss: 0.413007  [   64/   88]
per-ex loss: 0.328097  [   65/   88]
per-ex loss: 0.330040  [   66/   88]
per-ex loss: 0.224543  [   67/   88]
per-ex loss: 0.504528  [   68/   88]
per-ex loss: 0.384025  [   69/   88]
per-ex loss: 0.319607  [   70/   88]
per-ex loss: 0.375648  [   71/   88]
per-ex loss: 0.388453  [   72/   88]
per-ex loss: 0.287922  [   73/   88]
per-ex loss: 0.368466  [   74/   88]
per-ex loss: 0.412755  [   75/   88]
per-ex loss: 0.451264  [   76/   88]
per-ex loss: 0.336231  [   77/   88]
per-ex loss: 0.364820  [   78/   88]
per-ex loss: 0.346910  [   79/   88]
per-ex loss: 0.479095  [   80/   88]
per-ex loss: 0.353240  [   81/   88]
per-ex loss: 0.597017  [   82/   88]
per-ex loss: 0.404143  [   83/   88]
per-ex loss: 0.644548  [   84/   88]
per-ex loss: 0.381634  [   85/   88]
per-ex loss: 0.644964  [   86/   88]
per-ex loss: 0.546976  [   87/   88]
per-ex loss: 0.428054  [   88/   88]
Train Error: Avg loss: 0.43556260
validation Error: 
 Avg loss: 0.53147590 
 F1: 0.499241 
 Precision: 0.572886 
 Recall: 0.442374
 IoU: 0.332659

test Error: 
 Avg loss: 0.47949509 
 F1: 0.567460 
 Precision: 0.638780 
 Recall: 0.510466
 IoU: 0.396122

We have finished training iteration 137
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_135_.pth
per-ex loss: 0.452854  [    1/   88]
per-ex loss: 0.307870  [    2/   88]
per-ex loss: 0.343160  [    3/   88]
per-ex loss: 0.402097  [    4/   88]
per-ex loss: 0.360909  [    5/   88]
per-ex loss: 0.354618  [    6/   88]
per-ex loss: 0.519996  [    7/   88]
per-ex loss: 0.412580  [    8/   88]
per-ex loss: 0.390293  [    9/   88]
per-ex loss: 0.431960  [   10/   88]
per-ex loss: 0.288981  [   11/   88]
per-ex loss: 0.388550  [   12/   88]
per-ex loss: 0.320679  [   13/   88]
per-ex loss: 0.355060  [   14/   88]
per-ex loss: 0.303969  [   15/   88]
per-ex loss: 0.306668  [   16/   88]
per-ex loss: 0.316597  [   17/   88]
per-ex loss: 0.324736  [   18/   88]
per-ex loss: 0.335652  [   19/   88]
per-ex loss: 0.326733  [   20/   88]
per-ex loss: 0.430050  [   21/   88]
per-ex loss: 0.311060  [   22/   88]
per-ex loss: 0.372170  [   23/   88]
per-ex loss: 0.352241  [   24/   88]
per-ex loss: 0.422010  [   25/   88]
per-ex loss: 0.577156  [   26/   88]
per-ex loss: 0.370423  [   27/   88]
per-ex loss: 0.363605  [   28/   88]
per-ex loss: 0.312682  [   29/   88]
per-ex loss: 0.312189  [   30/   88]
per-ex loss: 0.334717  [   31/   88]
per-ex loss: 0.364357  [   32/   88]
per-ex loss: 0.379326  [   33/   88]
per-ex loss: 0.429083  [   34/   88]
per-ex loss: 0.530043  [   35/   88]
per-ex loss: 0.522003  [   36/   88]
per-ex loss: 0.642894  [   37/   88]
per-ex loss: 0.530129  [   38/   88]
per-ex loss: 0.566026  [   39/   88]
per-ex loss: 0.326253  [   40/   88]
per-ex loss: 0.371403  [   41/   88]
per-ex loss: 0.358930  [   42/   88]
per-ex loss: 0.421951  [   43/   88]
per-ex loss: 0.449829  [   44/   88]
per-ex loss: 0.636243  [   45/   88]
per-ex loss: 0.468255  [   46/   88]
per-ex loss: 0.516365  [   47/   88]
per-ex loss: 0.406389  [   48/   88]
per-ex loss: 0.415947  [   49/   88]
per-ex loss: 0.633424  [   50/   88]
per-ex loss: 0.355355  [   51/   88]
per-ex loss: 0.545332  [   52/   88]
per-ex loss: 0.488154  [   53/   88]
per-ex loss: 0.404716  [   54/   88]
per-ex loss: 0.419354  [   55/   88]
per-ex loss: 0.317483  [   56/   88]
per-ex loss: 0.548861  [   57/   88]
per-ex loss: 0.639979  [   58/   88]
per-ex loss: 0.513570  [   59/   88]
per-ex loss: 0.568867  [   60/   88]
per-ex loss: 0.214431  [   61/   88]
per-ex loss: 0.469211  [   62/   88]
per-ex loss: 0.416613  [   63/   88]
per-ex loss: 0.582147  [   64/   88]
per-ex loss: 0.537927  [   65/   88]
per-ex loss: 0.561145  [   66/   88]
per-ex loss: 0.438996  [   67/   88]
per-ex loss: 0.479699  [   68/   88]
per-ex loss: 0.637340  [   69/   88]
per-ex loss: 0.608691  [   70/   88]
per-ex loss: 0.368487  [   71/   88]
per-ex loss: 0.399581  [   72/   88]
per-ex loss: 0.546559  [   73/   88]
per-ex loss: 0.384853  [   74/   88]
per-ex loss: 0.473818  [   75/   88]
per-ex loss: 0.428830  [   76/   88]
per-ex loss: 0.544634  [   77/   88]
per-ex loss: 0.349526  [   78/   88]
per-ex loss: 0.424031  [   79/   88]
per-ex loss: 0.654275  [   80/   88]
per-ex loss: 0.513304  [   81/   88]
per-ex loss: 0.517117  [   82/   88]
per-ex loss: 0.555361  [   83/   88]
per-ex loss: 0.300054  [   84/   88]
per-ex loss: 0.387379  [   85/   88]
per-ex loss: 0.422196  [   86/   88]
per-ex loss: 0.416095  [   87/   88]
per-ex loss: 0.609716  [   88/   88]
Train Error: Avg loss: 0.43653185
validation Error: 
 Avg loss: 0.52721211 
 F1: 0.500454 
 Precision: 0.550022 
 Recall: 0.459082
 IoU: 0.333737

test Error: 
 Avg loss: 0.48420455 
 F1: 0.557816 
 Precision: 0.591640 
 Recall: 0.527651
 IoU: 0.386786

We have finished training iteration 138
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_136_.pth
per-ex loss: 0.464248  [    1/   88]
per-ex loss: 0.403399  [    2/   88]
per-ex loss: 0.284610  [    3/   88]
per-ex loss: 0.324362  [    4/   88]
per-ex loss: 0.388689  [    5/   88]
per-ex loss: 0.533260  [    6/   88]
per-ex loss: 0.527751  [    7/   88]
per-ex loss: 0.451031  [    8/   88]
per-ex loss: 0.522063  [    9/   88]
per-ex loss: 0.647088  [   10/   88]
per-ex loss: 0.568879  [   11/   88]
per-ex loss: 0.420181  [   12/   88]
per-ex loss: 0.411453  [   13/   88]
per-ex loss: 0.305157  [   14/   88]
per-ex loss: 0.348760  [   15/   88]
per-ex loss: 0.544784  [   16/   88]
per-ex loss: 0.606472  [   17/   88]
per-ex loss: 0.483219  [   18/   88]
per-ex loss: 0.385867  [   19/   88]
per-ex loss: 0.357690  [   20/   88]
per-ex loss: 0.361671  [   21/   88]
per-ex loss: 0.414414  [   22/   88]
per-ex loss: 0.522198  [   23/   88]
per-ex loss: 0.301260  [   24/   88]
per-ex loss: 0.554491  [   25/   88]
per-ex loss: 0.341226  [   26/   88]
per-ex loss: 0.461135  [   27/   88]
per-ex loss: 0.323162  [   28/   88]
per-ex loss: 0.301862  [   29/   88]
per-ex loss: 0.474291  [   30/   88]
per-ex loss: 0.426467  [   31/   88]
per-ex loss: 0.556692  [   32/   88]
per-ex loss: 0.551804  [   33/   88]
per-ex loss: 0.317715  [   34/   88]
per-ex loss: 0.558814  [   35/   88]
per-ex loss: 0.508803  [   36/   88]
per-ex loss: 0.362111  [   37/   88]
per-ex loss: 0.444002  [   38/   88]
per-ex loss: 0.452138  [   39/   88]
per-ex loss: 0.324453  [   40/   88]
per-ex loss: 0.378540  [   41/   88]
per-ex loss: 0.355996  [   42/   88]
per-ex loss: 0.359265  [   43/   88]
per-ex loss: 0.523918  [   44/   88]
per-ex loss: 0.398511  [   45/   88]
per-ex loss: 0.317138  [   46/   88]
per-ex loss: 0.651159  [   47/   88]
per-ex loss: 0.224137  [   48/   88]
per-ex loss: 0.384206  [   49/   88]
per-ex loss: 0.361387  [   50/   88]
per-ex loss: 0.417549  [   51/   88]
per-ex loss: 0.373976  [   52/   88]
per-ex loss: 0.625736  [   53/   88]
per-ex loss: 0.515719  [   54/   88]
per-ex loss: 0.401979  [   55/   88]
per-ex loss: 0.384086  [   56/   88]
per-ex loss: 0.516991  [   57/   88]
per-ex loss: 0.510645  [   58/   88]
per-ex loss: 0.344623  [   59/   88]
per-ex loss: 0.400796  [   60/   88]
per-ex loss: 0.527099  [   61/   88]
per-ex loss: 0.485649  [   62/   88]
per-ex loss: 0.332203  [   63/   88]
per-ex loss: 0.532297  [   64/   88]
per-ex loss: 0.357258  [   65/   88]
per-ex loss: 0.303327  [   66/   88]
per-ex loss: 0.356303  [   67/   88]
per-ex loss: 0.292861  [   68/   88]
per-ex loss: 0.393744  [   69/   88]
per-ex loss: 0.421639  [   70/   88]
per-ex loss: 0.624786  [   71/   88]
per-ex loss: 0.429886  [   72/   88]
per-ex loss: 0.344061  [   73/   88]
per-ex loss: 0.555500  [   74/   88]
per-ex loss: 0.604406  [   75/   88]
per-ex loss: 0.324620  [   76/   88]
per-ex loss: 0.455039  [   77/   88]
per-ex loss: 0.374613  [   78/   88]
per-ex loss: 0.391924  [   79/   88]
per-ex loss: 0.450937  [   80/   88]
per-ex loss: 0.542571  [   81/   88]
per-ex loss: 0.307767  [   82/   88]
per-ex loss: 0.389013  [   83/   88]
per-ex loss: 0.353747  [   84/   88]
per-ex loss: 0.308100  [   85/   88]
per-ex loss: 0.476099  [   86/   88]
per-ex loss: 0.533797  [   87/   88]
per-ex loss: 0.342858  [   88/   88]
Train Error: Avg loss: 0.42950149
validation Error: 
 Avg loss: 0.53705553 
 F1: 0.493926 
 Precision: 0.628073 
 Recall: 0.406997
 IoU: 0.327956

test Error: 
 Avg loss: 0.48923888 
 F1: 0.556763 
 Precision: 0.682603 
 Recall: 0.470099
 IoU: 0.385774

We have finished training iteration 139
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_137_.pth
per-ex loss: 0.313386  [    1/   88]
per-ex loss: 0.321274  [    2/   88]
per-ex loss: 0.563047  [    3/   88]
per-ex loss: 0.406525  [    4/   88]
per-ex loss: 0.360334  [    5/   88]
per-ex loss: 0.455839  [    6/   88]
per-ex loss: 0.627900  [    7/   88]
per-ex loss: 0.369715  [    8/   88]
per-ex loss: 0.539916  [    9/   88]
per-ex loss: 0.461841  [   10/   88]
per-ex loss: 0.379511  [   11/   88]
per-ex loss: 0.594119  [   12/   88]
per-ex loss: 0.358345  [   13/   88]
per-ex loss: 0.509434  [   14/   88]
per-ex loss: 0.375559  [   15/   88]
per-ex loss: 0.520049  [   16/   88]
per-ex loss: 0.326417  [   17/   88]
per-ex loss: 0.566658  [   18/   88]
per-ex loss: 0.313263  [   19/   88]
per-ex loss: 0.360350  [   20/   88]
per-ex loss: 0.547871  [   21/   88]
per-ex loss: 0.487370  [   22/   88]
per-ex loss: 0.319273  [   23/   88]
per-ex loss: 0.513251  [   24/   88]
per-ex loss: 0.352520  [   25/   88]
per-ex loss: 0.355580  [   26/   88]
per-ex loss: 0.446982  [   27/   88]
per-ex loss: 0.377383  [   28/   88]
per-ex loss: 0.376456  [   29/   88]
per-ex loss: 0.452182  [   30/   88]
per-ex loss: 0.565430  [   31/   88]
per-ex loss: 0.361297  [   32/   88]
per-ex loss: 0.334056  [   33/   88]
per-ex loss: 0.575986  [   34/   88]
per-ex loss: 0.467160  [   35/   88]
per-ex loss: 0.313778  [   36/   88]
per-ex loss: 0.643521  [   37/   88]
per-ex loss: 0.503140  [   38/   88]
per-ex loss: 0.430557  [   39/   88]
per-ex loss: 0.379099  [   40/   88]
per-ex loss: 0.413914  [   41/   88]
per-ex loss: 0.565681  [   42/   88]
per-ex loss: 0.540668  [   43/   88]
per-ex loss: 0.307305  [   44/   88]
per-ex loss: 0.569385  [   45/   88]
per-ex loss: 0.493474  [   46/   88]
per-ex loss: 0.502373  [   47/   88]
per-ex loss: 0.533579  [   48/   88]
per-ex loss: 0.625128  [   49/   88]
per-ex loss: 0.401073  [   50/   88]
per-ex loss: 0.216190  [   51/   88]
per-ex loss: 0.429275  [   52/   88]
per-ex loss: 0.359712  [   53/   88]
per-ex loss: 0.285844  [   54/   88]
per-ex loss: 0.317957  [   55/   88]
per-ex loss: 0.366785  [   56/   88]
per-ex loss: 0.423907  [   57/   88]
per-ex loss: 0.431076  [   58/   88]
per-ex loss: 0.552266  [   59/   88]
per-ex loss: 0.531469  [   60/   88]
per-ex loss: 0.475524  [   61/   88]
per-ex loss: 0.303948  [   62/   88]
per-ex loss: 0.398825  [   63/   88]
per-ex loss: 0.389429  [   64/   88]
per-ex loss: 0.318889  [   65/   88]
per-ex loss: 0.328838  [   66/   88]
per-ex loss: 0.429130  [   67/   88]
per-ex loss: 0.542532  [   68/   88]
per-ex loss: 0.477807  [   69/   88]
per-ex loss: 0.391042  [   70/   88]
per-ex loss: 0.371370  [   71/   88]
per-ex loss: 0.340289  [   72/   88]
per-ex loss: 0.299068  [   73/   88]
per-ex loss: 0.340538  [   74/   88]
per-ex loss: 0.473079  [   75/   88]
per-ex loss: 0.330238  [   76/   88]
per-ex loss: 0.622792  [   77/   88]
per-ex loss: 0.543510  [   78/   88]
per-ex loss: 0.383336  [   79/   88]
per-ex loss: 0.503402  [   80/   88]
per-ex loss: 0.324061  [   81/   88]
per-ex loss: 0.619852  [   82/   88]
per-ex loss: 0.365003  [   83/   88]
per-ex loss: 0.515329  [   84/   88]
per-ex loss: 0.316536  [   85/   88]
per-ex loss: 0.330474  [   86/   88]
per-ex loss: 0.446992  [   87/   88]
per-ex loss: 0.354465  [   88/   88]
Train Error: Avg loss: 0.43101972
validation Error: 
 Avg loss: 0.53983857 
 F1: 0.489877 
 Precision: 0.629412 
 Recall: 0.400982
 IoU: 0.324395

test Error: 
 Avg loss: 0.49112738 
 F1: 0.555746 
 Precision: 0.682915 
 Recall: 0.468504
 IoU: 0.384798

We have finished training iteration 140
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_138_.pth
per-ex loss: 0.338067  [    1/   88]
per-ex loss: 0.559759  [    2/   88]
per-ex loss: 0.410654  [    3/   88]
per-ex loss: 0.359047  [    4/   88]
per-ex loss: 0.318204  [    5/   88]
per-ex loss: 0.535372  [    6/   88]
per-ex loss: 0.529461  [    7/   88]
per-ex loss: 0.553355  [    8/   88]
per-ex loss: 0.543378  [    9/   88]
per-ex loss: 0.470082  [   10/   88]
per-ex loss: 0.500472  [   11/   88]
per-ex loss: 0.439203  [   12/   88]
per-ex loss: 0.348963  [   13/   88]
per-ex loss: 0.339108  [   14/   88]
per-ex loss: 0.305298  [   15/   88]
per-ex loss: 0.363663  [   16/   88]
per-ex loss: 0.540609  [   17/   88]
per-ex loss: 0.350096  [   18/   88]
per-ex loss: 0.528743  [   19/   88]
per-ex loss: 0.350235  [   20/   88]
per-ex loss: 0.455821  [   21/   88]
per-ex loss: 0.435028  [   22/   88]
per-ex loss: 0.437332  [   23/   88]
per-ex loss: 0.627541  [   24/   88]
per-ex loss: 0.610573  [   25/   88]
per-ex loss: 0.347123  [   26/   88]
per-ex loss: 0.472327  [   27/   88]
per-ex loss: 0.278999  [   28/   88]
per-ex loss: 0.279445  [   29/   88]
per-ex loss: 0.358983  [   30/   88]
per-ex loss: 0.555382  [   31/   88]
per-ex loss: 0.414587  [   32/   88]
per-ex loss: 0.497842  [   33/   88]
per-ex loss: 0.504398  [   34/   88]
per-ex loss: 0.367663  [   35/   88]
per-ex loss: 0.298527  [   36/   88]
per-ex loss: 0.521445  [   37/   88]
per-ex loss: 0.542359  [   38/   88]
per-ex loss: 0.378208  [   39/   88]
per-ex loss: 0.384709  [   40/   88]
per-ex loss: 0.331293  [   41/   88]
per-ex loss: 0.554805  [   42/   88]
per-ex loss: 0.353391  [   43/   88]
per-ex loss: 0.317409  [   44/   88]
per-ex loss: 0.342006  [   45/   88]
per-ex loss: 0.436600  [   46/   88]
per-ex loss: 0.417953  [   47/   88]
per-ex loss: 0.463395  [   48/   88]
per-ex loss: 0.432587  [   49/   88]
per-ex loss: 0.359371  [   50/   88]
per-ex loss: 0.322092  [   51/   88]
per-ex loss: 0.513262  [   52/   88]
per-ex loss: 0.406148  [   53/   88]
per-ex loss: 0.528360  [   54/   88]
per-ex loss: 0.395557  [   55/   88]
per-ex loss: 0.364917  [   56/   88]
per-ex loss: 0.638716  [   57/   88]
per-ex loss: 0.467200  [   58/   88]
per-ex loss: 0.559961  [   59/   88]
per-ex loss: 0.365734  [   60/   88]
per-ex loss: 0.405581  [   61/   88]
per-ex loss: 0.628360  [   62/   88]
per-ex loss: 0.327176  [   63/   88]
per-ex loss: 0.584402  [   64/   88]
per-ex loss: 0.572248  [   65/   88]
per-ex loss: 0.407434  [   66/   88]
per-ex loss: 0.323768  [   67/   88]
per-ex loss: 0.542373  [   68/   88]
per-ex loss: 0.390158  [   69/   88]
per-ex loss: 0.308028  [   70/   88]
per-ex loss: 0.321310  [   71/   88]
per-ex loss: 0.380330  [   72/   88]
per-ex loss: 0.390952  [   73/   88]
per-ex loss: 0.264531  [   74/   88]
per-ex loss: 0.530691  [   75/   88]
per-ex loss: 0.637538  [   76/   88]
per-ex loss: 0.386587  [   77/   88]
per-ex loss: 0.404218  [   78/   88]
per-ex loss: 0.391754  [   79/   88]
per-ex loss: 0.330991  [   80/   88]
per-ex loss: 0.286164  [   81/   88]
per-ex loss: 0.380869  [   82/   88]
per-ex loss: 0.348226  [   83/   88]
per-ex loss: 0.389238  [   84/   88]
per-ex loss: 0.406999  [   85/   88]
per-ex loss: 0.362304  [   86/   88]
per-ex loss: 0.474633  [   87/   88]
per-ex loss: 0.415933  [   88/   88]
Train Error: Avg loss: 0.42745013
validation Error: 
 Avg loss: 0.53070766 
 F1: 0.502095 
 Precision: 0.599970 
 Recall: 0.431674
 IoU: 0.335198

test Error: 
 Avg loss: 0.47995859 
 F1: 0.567344 
 Precision: 0.640815 
 Recall: 0.508987
 IoU: 0.396009

We have finished training iteration 141
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_139_.pth
per-ex loss: 0.402668  [    1/   88]
per-ex loss: 0.291969  [    2/   88]
per-ex loss: 0.328191  [    3/   88]
per-ex loss: 0.538650  [    4/   88]
per-ex loss: 0.435961  [    5/   88]
per-ex loss: 0.429132  [    6/   88]
per-ex loss: 0.321569  [    7/   88]
per-ex loss: 0.311907  [    8/   88]
per-ex loss: 0.371392  [    9/   88]
per-ex loss: 0.351108  [   10/   88]
per-ex loss: 0.416402  [   11/   88]
per-ex loss: 0.434435  [   12/   88]
per-ex loss: 0.284382  [   13/   88]
per-ex loss: 0.334792  [   14/   88]
per-ex loss: 0.385988  [   15/   88]
per-ex loss: 0.408343  [   16/   88]
per-ex loss: 0.322841  [   17/   88]
per-ex loss: 0.317490  [   18/   88]
per-ex loss: 0.316362  [   19/   88]
per-ex loss: 0.547057  [   20/   88]
per-ex loss: 0.493657  [   21/   88]
per-ex loss: 0.561921  [   22/   88]
per-ex loss: 0.514755  [   23/   88]
per-ex loss: 0.621141  [   24/   88]
per-ex loss: 0.585619  [   25/   88]
per-ex loss: 0.483304  [   26/   88]
per-ex loss: 0.363920  [   27/   88]
per-ex loss: 0.607654  [   28/   88]
per-ex loss: 0.437236  [   29/   88]
per-ex loss: 0.321584  [   30/   88]
per-ex loss: 0.374945  [   31/   88]
per-ex loss: 0.392007  [   32/   88]
per-ex loss: 0.623662  [   33/   88]
per-ex loss: 0.462588  [   34/   88]
per-ex loss: 0.525008  [   35/   88]
per-ex loss: 0.382353  [   36/   88]
per-ex loss: 0.387037  [   37/   88]
per-ex loss: 0.429276  [   38/   88]
per-ex loss: 0.554709  [   39/   88]
per-ex loss: 0.192826  [   40/   88]
per-ex loss: 0.357208  [   41/   88]
per-ex loss: 0.386018  [   42/   88]
per-ex loss: 0.340031  [   43/   88]
per-ex loss: 0.575033  [   44/   88]
per-ex loss: 0.668979  [   45/   88]
per-ex loss: 0.379818  [   46/   88]
per-ex loss: 0.526563  [   47/   88]
per-ex loss: 0.351893  [   48/   88]
per-ex loss: 0.511189  [   49/   88]
per-ex loss: 0.344980  [   50/   88]
per-ex loss: 0.360993  [   51/   88]
per-ex loss: 0.353353  [   52/   88]
per-ex loss: 0.378311  [   53/   88]
per-ex loss: 0.518579  [   54/   88]
per-ex loss: 0.470783  [   55/   88]
per-ex loss: 0.430021  [   56/   88]
per-ex loss: 0.347110  [   57/   88]
per-ex loss: 0.376390  [   58/   88]
per-ex loss: 0.369553  [   59/   88]
per-ex loss: 0.354164  [   60/   88]
per-ex loss: 0.288948  [   61/   88]
per-ex loss: 0.310911  [   62/   88]
per-ex loss: 0.561137  [   63/   88]
per-ex loss: 0.536482  [   64/   88]
per-ex loss: 0.331987  [   65/   88]
per-ex loss: 0.509653  [   66/   88]
per-ex loss: 0.400072  [   67/   88]
per-ex loss: 0.652473  [   68/   88]
per-ex loss: 0.348198  [   69/   88]
per-ex loss: 0.292723  [   70/   88]
per-ex loss: 0.313126  [   71/   88]
per-ex loss: 0.515269  [   72/   88]
per-ex loss: 0.389857  [   73/   88]
per-ex loss: 0.357083  [   74/   88]
per-ex loss: 0.556727  [   75/   88]
per-ex loss: 0.522635  [   76/   88]
per-ex loss: 0.549203  [   77/   88]
per-ex loss: 0.365762  [   78/   88]
per-ex loss: 0.531650  [   79/   88]
per-ex loss: 0.316498  [   80/   88]
per-ex loss: 0.336806  [   81/   88]
per-ex loss: 0.355380  [   82/   88]
per-ex loss: 0.524603  [   83/   88]
per-ex loss: 0.410991  [   84/   88]
per-ex loss: 0.467453  [   85/   88]
per-ex loss: 0.580815  [   86/   88]
per-ex loss: 0.471228  [   87/   88]
per-ex loss: 0.425208  [   88/   88]
Train Error: Avg loss: 0.42601911
validation Error: 
 Avg loss: 0.53574198 
 F1: 0.495212 
 Precision: 0.579691 
 Recall: 0.432224
 IoU: 0.329091

test Error: 
 Avg loss: 0.48328507 
 F1: 0.563071 
 Precision: 0.633168 
 Recall: 0.506948
 IoU: 0.391857

We have finished training iteration 142
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_140_.pth
per-ex loss: 0.565353  [    1/   88]
per-ex loss: 0.458548  [    2/   88]
per-ex loss: 0.458852  [    3/   88]
per-ex loss: 0.538266  [    4/   88]
per-ex loss: 0.326544  [    5/   88]
per-ex loss: 0.567982  [    6/   88]
per-ex loss: 0.305999  [    7/   88]
per-ex loss: 0.603230  [    8/   88]
per-ex loss: 0.448469  [    9/   88]
per-ex loss: 0.567277  [   10/   88]
per-ex loss: 0.370949  [   11/   88]
per-ex loss: 0.619802  [   12/   88]
per-ex loss: 0.292962  [   13/   88]
per-ex loss: 0.543755  [   14/   88]
per-ex loss: 0.389210  [   15/   88]
per-ex loss: 0.520357  [   16/   88]
per-ex loss: 0.492222  [   17/   88]
per-ex loss: 0.399449  [   18/   88]
per-ex loss: 0.457960  [   19/   88]
per-ex loss: 0.318241  [   20/   88]
per-ex loss: 0.501478  [   21/   88]
per-ex loss: 0.321939  [   22/   88]
per-ex loss: 0.438444  [   23/   88]
per-ex loss: 0.536464  [   24/   88]
per-ex loss: 0.555080  [   25/   88]
per-ex loss: 0.423494  [   26/   88]
per-ex loss: 0.494747  [   27/   88]
per-ex loss: 0.402508  [   28/   88]
per-ex loss: 0.405591  [   29/   88]
per-ex loss: 0.412577  [   30/   88]
per-ex loss: 0.369785  [   31/   88]
per-ex loss: 0.510130  [   32/   88]
per-ex loss: 0.408957  [   33/   88]
per-ex loss: 0.436813  [   34/   88]
per-ex loss: 0.471904  [   35/   88]
per-ex loss: 0.406077  [   36/   88]
per-ex loss: 0.358149  [   37/   88]
per-ex loss: 0.357577  [   38/   88]
per-ex loss: 0.354339  [   39/   88]
per-ex loss: 0.507022  [   40/   88]
per-ex loss: 0.349022  [   41/   88]
per-ex loss: 0.377600  [   42/   88]
per-ex loss: 0.353585  [   43/   88]
per-ex loss: 0.615998  [   44/   88]
per-ex loss: 0.531403  [   45/   88]
per-ex loss: 0.549132  [   46/   88]
per-ex loss: 0.340683  [   47/   88]
per-ex loss: 0.228034  [   48/   88]
per-ex loss: 0.376478  [   49/   88]
per-ex loss: 0.647788  [   50/   88]
per-ex loss: 0.333733  [   51/   88]
per-ex loss: 0.319994  [   52/   88]
per-ex loss: 0.425865  [   53/   88]
per-ex loss: 0.485449  [   54/   88]
per-ex loss: 0.311171  [   55/   88]
per-ex loss: 0.564755  [   56/   88]
per-ex loss: 0.308405  [   57/   88]
per-ex loss: 0.276290  [   58/   88]
per-ex loss: 0.560179  [   59/   88]
per-ex loss: 0.342282  [   60/   88]
per-ex loss: 0.374793  [   61/   88]
per-ex loss: 0.424110  [   62/   88]
per-ex loss: 0.306208  [   63/   88]
per-ex loss: 0.382435  [   64/   88]
per-ex loss: 0.368222  [   65/   88]
per-ex loss: 0.311730  [   66/   88]
per-ex loss: 0.345529  [   67/   88]
per-ex loss: 0.369249  [   68/   88]
per-ex loss: 0.348574  [   69/   88]
per-ex loss: 0.482414  [   70/   88]
per-ex loss: 0.332672  [   71/   88]
per-ex loss: 0.307714  [   72/   88]
per-ex loss: 0.464442  [   73/   88]
per-ex loss: 0.434115  [   74/   88]
per-ex loss: 0.369319  [   75/   88]
per-ex loss: 0.299784  [   76/   88]
per-ex loss: 0.497886  [   77/   88]
per-ex loss: 0.348102  [   78/   88]
per-ex loss: 0.545709  [   79/   88]
per-ex loss: 0.565624  [   80/   88]
per-ex loss: 0.548672  [   81/   88]
per-ex loss: 0.398715  [   82/   88]
per-ex loss: 0.427197  [   83/   88]
per-ex loss: 0.634974  [   84/   88]
per-ex loss: 0.619191  [   85/   88]
per-ex loss: 0.341983  [   86/   88]
per-ex loss: 0.365766  [   87/   88]
per-ex loss: 0.412752  [   88/   88]
Train Error: Avg loss: 0.43004802
validation Error: 
 Avg loss: 0.54241873 
 F1: 0.481939 
 Precision: 0.479874 
 Recall: 0.484022
 IoU: 0.317470

test Error: 
 Avg loss: 0.47736917 
 F1: 0.569613 
 Precision: 0.561569 
 Recall: 0.577890
 IoU: 0.398223

We have finished training iteration 143
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_141_.pth
per-ex loss: 0.373617  [    1/   88]
per-ex loss: 0.552589  [    2/   88]
per-ex loss: 0.438561  [    3/   88]
per-ex loss: 0.320774  [    4/   88]
per-ex loss: 0.398585  [    5/   88]
per-ex loss: 0.337062  [    6/   88]
per-ex loss: 0.334771  [    7/   88]
per-ex loss: 0.607143  [    8/   88]
per-ex loss: 0.396472  [    9/   88]
per-ex loss: 0.533812  [   10/   88]
per-ex loss: 0.440187  [   11/   88]
per-ex loss: 0.611375  [   12/   88]
per-ex loss: 0.359743  [   13/   88]
per-ex loss: 0.345503  [   14/   88]
per-ex loss: 0.500045  [   15/   88]
per-ex loss: 0.488149  [   16/   88]
per-ex loss: 0.544300  [   17/   88]
per-ex loss: 0.338188  [   18/   88]
per-ex loss: 0.386987  [   19/   88]
per-ex loss: 0.538136  [   20/   88]
per-ex loss: 0.531711  [   21/   88]
per-ex loss: 0.593500  [   22/   88]
per-ex loss: 0.368686  [   23/   88]
per-ex loss: 0.486728  [   24/   88]
per-ex loss: 0.316252  [   25/   88]
per-ex loss: 0.409645  [   26/   88]
per-ex loss: 0.348970  [   27/   88]
per-ex loss: 0.639554  [   28/   88]
per-ex loss: 0.361771  [   29/   88]
per-ex loss: 0.417096  [   30/   88]
per-ex loss: 0.471080  [   31/   88]
per-ex loss: 0.351861  [   32/   88]
per-ex loss: 0.283098  [   33/   88]
per-ex loss: 0.325771  [   34/   88]
per-ex loss: 0.539693  [   35/   88]
per-ex loss: 0.505108  [   36/   88]
per-ex loss: 0.317860  [   37/   88]
per-ex loss: 0.407927  [   38/   88]
per-ex loss: 0.532070  [   39/   88]
per-ex loss: 0.449209  [   40/   88]
per-ex loss: 0.395265  [   41/   88]
per-ex loss: 0.320758  [   42/   88]
per-ex loss: 0.382329  [   43/   88]
per-ex loss: 0.511857  [   44/   88]
per-ex loss: 0.413612  [   45/   88]
per-ex loss: 0.376959  [   46/   88]
per-ex loss: 0.537838  [   47/   88]
per-ex loss: 0.345559  [   48/   88]
per-ex loss: 0.629214  [   49/   88]
per-ex loss: 0.343135  [   50/   88]
per-ex loss: 0.570008  [   51/   88]
per-ex loss: 0.639461  [   52/   88]
per-ex loss: 0.562832  [   53/   88]
per-ex loss: 0.285614  [   54/   88]
per-ex loss: 0.387555  [   55/   88]
per-ex loss: 0.311678  [   56/   88]
per-ex loss: 0.405959  [   57/   88]
per-ex loss: 0.298800  [   58/   88]
per-ex loss: 0.471273  [   59/   88]
per-ex loss: 0.285661  [   60/   88]
per-ex loss: 0.383034  [   61/   88]
per-ex loss: 0.519089  [   62/   88]
per-ex loss: 0.533543  [   63/   88]
per-ex loss: 0.355850  [   64/   88]
per-ex loss: 0.542033  [   65/   88]
per-ex loss: 0.485398  [   66/   88]
per-ex loss: 0.428002  [   67/   88]
per-ex loss: 0.314555  [   68/   88]
per-ex loss: 0.571878  [   69/   88]
per-ex loss: 0.489684  [   70/   88]
per-ex loss: 0.385842  [   71/   88]
per-ex loss: 0.226555  [   72/   88]
per-ex loss: 0.288846  [   73/   88]
per-ex loss: 0.448981  [   74/   88]
per-ex loss: 0.381255  [   75/   88]
per-ex loss: 0.359153  [   76/   88]
per-ex loss: 0.345579  [   77/   88]
per-ex loss: 0.360192  [   78/   88]
per-ex loss: 0.625600  [   79/   88]
per-ex loss: 0.563265  [   80/   88]
per-ex loss: 0.370957  [   81/   88]
per-ex loss: 0.542395  [   82/   88]
per-ex loss: 0.374008  [   83/   88]
per-ex loss: 0.344517  [   84/   88]
per-ex loss: 0.527564  [   85/   88]
per-ex loss: 0.446599  [   86/   88]
per-ex loss: 0.331390  [   87/   88]
per-ex loss: 0.399199  [   88/   88]
Train Error: Avg loss: 0.43095361
validation Error: 
 Avg loss: 0.53422957 
 F1: 0.495746 
 Precision: 0.583082 
 Recall: 0.431165
 IoU: 0.329563

test Error: 
 Avg loss: 0.48416990 
 F1: 0.561793 
 Precision: 0.640451 
 Recall: 0.500343
 IoU: 0.390621

We have finished training iteration 144
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_142_.pth
per-ex loss: 0.360566  [    1/   88]
per-ex loss: 0.400745  [    2/   88]
per-ex loss: 0.529345  [    3/   88]
per-ex loss: 0.425394  [    4/   88]
per-ex loss: 0.510951  [    5/   88]
per-ex loss: 0.322160  [    6/   88]
per-ex loss: 0.605092  [    7/   88]
per-ex loss: 0.524050  [    8/   88]
per-ex loss: 0.397127  [    9/   88]
per-ex loss: 0.352267  [   10/   88]
per-ex loss: 0.395207  [   11/   88]
per-ex loss: 0.274922  [   12/   88]
per-ex loss: 0.322385  [   13/   88]
per-ex loss: 0.295747  [   14/   88]
per-ex loss: 0.456677  [   15/   88]
per-ex loss: 0.404804  [   16/   88]
per-ex loss: 0.371048  [   17/   88]
per-ex loss: 0.465609  [   18/   88]
per-ex loss: 0.561918  [   19/   88]
per-ex loss: 0.426550  [   20/   88]
per-ex loss: 0.326140  [   21/   88]
per-ex loss: 0.637322  [   22/   88]
per-ex loss: 0.330372  [   23/   88]
per-ex loss: 0.449935  [   24/   88]
per-ex loss: 0.518209  [   25/   88]
per-ex loss: 0.541113  [   26/   88]
per-ex loss: 0.369563  [   27/   88]
per-ex loss: 0.459721  [   28/   88]
per-ex loss: 0.382886  [   29/   88]
per-ex loss: 0.516108  [   30/   88]
per-ex loss: 0.439118  [   31/   88]
per-ex loss: 0.542318  [   32/   88]
per-ex loss: 0.415999  [   33/   88]
per-ex loss: 0.492656  [   34/   88]
per-ex loss: 0.561563  [   35/   88]
per-ex loss: 0.434085  [   36/   88]
per-ex loss: 0.382637  [   37/   88]
per-ex loss: 0.426110  [   38/   88]
per-ex loss: 0.387640  [   39/   88]
per-ex loss: 0.547713  [   40/   88]
per-ex loss: 0.537892  [   41/   88]
per-ex loss: 0.326260  [   42/   88]
per-ex loss: 0.543373  [   43/   88]
per-ex loss: 0.558644  [   44/   88]
per-ex loss: 0.364068  [   45/   88]
per-ex loss: 0.631924  [   46/   88]
per-ex loss: 0.352423  [   47/   88]
per-ex loss: 0.361853  [   48/   88]
per-ex loss: 0.619768  [   49/   88]
per-ex loss: 0.372248  [   50/   88]
per-ex loss: 0.363823  [   51/   88]
per-ex loss: 0.339921  [   52/   88]
per-ex loss: 0.505169  [   53/   88]
per-ex loss: 0.313238  [   54/   88]
per-ex loss: 0.368779  [   55/   88]
per-ex loss: 0.547860  [   56/   88]
per-ex loss: 0.364336  [   57/   88]
per-ex loss: 0.504968  [   58/   88]
per-ex loss: 0.511222  [   59/   88]
per-ex loss: 0.383966  [   60/   88]
per-ex loss: 0.286590  [   61/   88]
per-ex loss: 0.302924  [   62/   88]
per-ex loss: 0.320930  [   63/   88]
per-ex loss: 0.535817  [   64/   88]
per-ex loss: 0.293574  [   65/   88]
per-ex loss: 0.396023  [   66/   88]
per-ex loss: 0.422619  [   67/   88]
per-ex loss: 0.375376  [   68/   88]
per-ex loss: 0.448248  [   69/   88]
per-ex loss: 0.222757  [   70/   88]
per-ex loss: 0.617748  [   71/   88]
per-ex loss: 0.384819  [   72/   88]
per-ex loss: 0.630844  [   73/   88]
per-ex loss: 0.337252  [   74/   88]
per-ex loss: 0.387333  [   75/   88]
per-ex loss: 0.357685  [   76/   88]
per-ex loss: 0.510760  [   77/   88]
per-ex loss: 0.372328  [   78/   88]
per-ex loss: 0.413186  [   79/   88]
per-ex loss: 0.326646  [   80/   88]
per-ex loss: 0.337950  [   81/   88]
per-ex loss: 0.526084  [   82/   88]
per-ex loss: 0.537137  [   83/   88]
per-ex loss: 0.557123  [   84/   88]
per-ex loss: 0.320043  [   85/   88]
per-ex loss: 0.308981  [   86/   88]
per-ex loss: 0.383838  [   87/   88]
per-ex loss: 0.456840  [   88/   88]
Train Error: Avg loss: 0.42962457
validation Error: 
 Avg loss: 0.53810453 
 F1: 0.491071 
 Precision: 0.644973 
 Recall: 0.396466
 IoU: 0.325443

test Error: 
 Avg loss: 0.49365814 
 F1: 0.554905 
 Precision: 0.681390 
 Recall: 0.468027
 IoU: 0.383992

We have finished training iteration 145
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_143_.pth
per-ex loss: 0.464068  [    1/   88]
per-ex loss: 0.426495  [    2/   88]
per-ex loss: 0.354454  [    3/   88]
per-ex loss: 0.351473  [    4/   88]
per-ex loss: 0.526278  [    5/   88]
per-ex loss: 0.299194  [    6/   88]
per-ex loss: 0.557896  [    7/   88]
per-ex loss: 0.395527  [    8/   88]
per-ex loss: 0.439168  [    9/   88]
per-ex loss: 0.594301  [   10/   88]
per-ex loss: 0.378083  [   11/   88]
per-ex loss: 0.605021  [   12/   88]
per-ex loss: 0.436830  [   13/   88]
per-ex loss: 0.319583  [   14/   88]
per-ex loss: 0.368517  [   15/   88]
per-ex loss: 0.538715  [   16/   88]
per-ex loss: 0.349919  [   17/   88]
per-ex loss: 0.401240  [   18/   88]
per-ex loss: 0.365097  [   19/   88]
per-ex loss: 0.388475  [   20/   88]
per-ex loss: 0.405756  [   21/   88]
per-ex loss: 0.306215  [   22/   88]
per-ex loss: 0.537588  [   23/   88]
per-ex loss: 0.478571  [   24/   88]
per-ex loss: 0.564623  [   25/   88]
per-ex loss: 0.497193  [   26/   88]
per-ex loss: 0.393290  [   27/   88]
per-ex loss: 0.562624  [   28/   88]
per-ex loss: 0.321212  [   29/   88]
per-ex loss: 0.322735  [   30/   88]
per-ex loss: 0.279975  [   31/   88]
per-ex loss: 0.568764  [   32/   88]
per-ex loss: 0.291948  [   33/   88]
per-ex loss: 0.403943  [   34/   88]
per-ex loss: 0.366504  [   35/   88]
per-ex loss: 0.482567  [   36/   88]
per-ex loss: 0.538649  [   37/   88]
per-ex loss: 0.521906  [   38/   88]
per-ex loss: 0.524251  [   39/   88]
per-ex loss: 0.409468  [   40/   88]
per-ex loss: 0.612867  [   41/   88]
per-ex loss: 0.312788  [   42/   88]
per-ex loss: 0.351899  [   43/   88]
per-ex loss: 0.301471  [   44/   88]
per-ex loss: 0.350185  [   45/   88]
per-ex loss: 0.363252  [   46/   88]
per-ex loss: 0.322878  [   47/   88]
per-ex loss: 0.431650  [   48/   88]
per-ex loss: 0.443357  [   49/   88]
per-ex loss: 0.547067  [   50/   88]
per-ex loss: 0.300788  [   51/   88]
per-ex loss: 0.472601  [   52/   88]
per-ex loss: 0.515194  [   53/   88]
per-ex loss: 0.354459  [   54/   88]
per-ex loss: 0.315645  [   55/   88]
per-ex loss: 0.366695  [   56/   88]
per-ex loss: 0.361614  [   57/   88]
per-ex loss: 0.472944  [   58/   88]
per-ex loss: 0.532179  [   59/   88]
per-ex loss: 0.405814  [   60/   88]
per-ex loss: 0.465661  [   61/   88]
per-ex loss: 0.334388  [   62/   88]
per-ex loss: 0.365082  [   63/   88]
per-ex loss: 0.328503  [   64/   88]
per-ex loss: 0.494898  [   65/   88]
per-ex loss: 0.636658  [   66/   88]
per-ex loss: 0.551451  [   67/   88]
per-ex loss: 0.301132  [   68/   88]
per-ex loss: 0.385347  [   69/   88]
per-ex loss: 0.420151  [   70/   88]
per-ex loss: 0.420850  [   71/   88]
per-ex loss: 0.409960  [   72/   88]
per-ex loss: 0.386707  [   73/   88]
per-ex loss: 0.427033  [   74/   88]
per-ex loss: 0.384943  [   75/   88]
per-ex loss: 0.521245  [   76/   88]
per-ex loss: 0.501162  [   77/   88]
per-ex loss: 0.633976  [   78/   88]
per-ex loss: 0.367813  [   79/   88]
per-ex loss: 0.405657  [   80/   88]
per-ex loss: 0.560187  [   81/   88]
per-ex loss: 0.591997  [   82/   88]
per-ex loss: 0.337195  [   83/   88]
per-ex loss: 0.371430  [   84/   88]
per-ex loss: 0.319668  [   85/   88]
per-ex loss: 0.212399  [   86/   88]
per-ex loss: 0.320260  [   87/   88]
per-ex loss: 0.598438  [   88/   88]
Train Error: Avg loss: 0.42645056
validation Error: 
 Avg loss: 0.54070807 
 F1: 0.490685 
 Precision: 0.579985 
 Recall: 0.425215
 IoU: 0.325105

test Error: 
 Avg loss: 0.49733484 
 F1: 0.547937 
 Precision: 0.636930 
 Recall: 0.480764
 IoU: 0.377351

We have finished training iteration 146
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_144_.pth
per-ex loss: 0.530971  [    1/   88]
per-ex loss: 0.424482  [    2/   88]
per-ex loss: 0.403867  [    3/   88]
per-ex loss: 0.329660  [    4/   88]
per-ex loss: 0.357892  [    5/   88]
per-ex loss: 0.337524  [    6/   88]
per-ex loss: 0.625117  [    7/   88]
per-ex loss: 0.426427  [    8/   88]
per-ex loss: 0.359468  [    9/   88]
per-ex loss: 0.393947  [   10/   88]
per-ex loss: 0.376127  [   11/   88]
per-ex loss: 0.597975  [   12/   88]
per-ex loss: 0.547480  [   13/   88]
per-ex loss: 0.389663  [   14/   88]
per-ex loss: 0.448548  [   15/   88]
per-ex loss: 0.403583  [   16/   88]
per-ex loss: 0.367684  [   17/   88]
per-ex loss: 0.394363  [   18/   88]
per-ex loss: 0.618534  [   19/   88]
per-ex loss: 0.409788  [   20/   88]
per-ex loss: 0.658908  [   21/   88]
per-ex loss: 0.520668  [   22/   88]
per-ex loss: 0.557775  [   23/   88]
per-ex loss: 0.489506  [   24/   88]
per-ex loss: 0.601782  [   25/   88]
per-ex loss: 0.528289  [   26/   88]
per-ex loss: 0.353438  [   27/   88]
per-ex loss: 0.329598  [   28/   88]
per-ex loss: 0.352921  [   29/   88]
per-ex loss: 0.315253  [   30/   88]
per-ex loss: 0.447857  [   31/   88]
per-ex loss: 0.620147  [   32/   88]
per-ex loss: 0.519364  [   33/   88]
per-ex loss: 0.378061  [   34/   88]
per-ex loss: 0.354027  [   35/   88]
per-ex loss: 0.318786  [   36/   88]
per-ex loss: 0.509570  [   37/   88]
per-ex loss: 0.358859  [   38/   88]
per-ex loss: 0.312089  [   39/   88]
per-ex loss: 0.439566  [   40/   88]
per-ex loss: 0.283274  [   41/   88]
per-ex loss: 0.348107  [   42/   88]
per-ex loss: 0.580554  [   43/   88]
per-ex loss: 0.386768  [   44/   88]
per-ex loss: 0.344200  [   45/   88]
per-ex loss: 0.286094  [   46/   88]
per-ex loss: 0.345736  [   47/   88]
per-ex loss: 0.557215  [   48/   88]
per-ex loss: 0.374857  [   49/   88]
per-ex loss: 0.428615  [   50/   88]
per-ex loss: 0.474304  [   51/   88]
per-ex loss: 0.351754  [   52/   88]
per-ex loss: 0.282933  [   53/   88]
per-ex loss: 0.397558  [   54/   88]
per-ex loss: 0.549777  [   55/   88]
per-ex loss: 0.372057  [   56/   88]
per-ex loss: 0.473211  [   57/   88]
per-ex loss: 0.362477  [   58/   88]
per-ex loss: 0.557443  [   59/   88]
per-ex loss: 0.419902  [   60/   88]
per-ex loss: 0.383246  [   61/   88]
per-ex loss: 0.219540  [   62/   88]
per-ex loss: 0.508439  [   63/   88]
per-ex loss: 0.510733  [   64/   88]
per-ex loss: 0.358817  [   65/   88]
per-ex loss: 0.367647  [   66/   88]
per-ex loss: 0.521677  [   67/   88]
per-ex loss: 0.320438  [   68/   88]
per-ex loss: 0.468688  [   69/   88]
per-ex loss: 0.459608  [   70/   88]
per-ex loss: 0.393675  [   71/   88]
per-ex loss: 0.327744  [   72/   88]
per-ex loss: 0.289479  [   73/   88]
per-ex loss: 0.303043  [   74/   88]
per-ex loss: 0.411728  [   75/   88]
per-ex loss: 0.563726  [   76/   88]
per-ex loss: 0.463997  [   77/   88]
per-ex loss: 0.524471  [   78/   88]
per-ex loss: 0.307971  [   79/   88]
per-ex loss: 0.346545  [   80/   88]
per-ex loss: 0.489561  [   81/   88]
per-ex loss: 0.389982  [   82/   88]
per-ex loss: 0.310085  [   83/   88]
per-ex loss: 0.455467  [   84/   88]
per-ex loss: 0.541040  [   85/   88]
per-ex loss: 0.563718  [   86/   88]
per-ex loss: 0.597280  [   87/   88]
per-ex loss: 0.333160  [   88/   88]
Train Error: Avg loss: 0.42747621
validation Error: 
 Avg loss: 0.53471028 
 F1: 0.494540 
 Precision: 0.607865 
 Recall: 0.416830
 IoU: 0.328498

test Error: 
 Avg loss: 0.48182613 
 F1: 0.568899 
 Precision: 0.647122 
 Recall: 0.507547
 IoU: 0.397525

We have finished training iteration 147
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_145_.pth
per-ex loss: 0.311809  [    1/   88]
per-ex loss: 0.460588  [    2/   88]
per-ex loss: 0.323235  [    3/   88]
per-ex loss: 0.472227  [    4/   88]
per-ex loss: 0.542604  [    5/   88]
per-ex loss: 0.406287  [    6/   88]
per-ex loss: 0.330339  [    7/   88]
per-ex loss: 0.341706  [    8/   88]
per-ex loss: 0.381513  [    9/   88]
per-ex loss: 0.395847  [   10/   88]
per-ex loss: 0.608225  [   11/   88]
per-ex loss: 0.485637  [   12/   88]
per-ex loss: 0.293667  [   13/   88]
per-ex loss: 0.332585  [   14/   88]
per-ex loss: 0.650147  [   15/   88]
per-ex loss: 0.349682  [   16/   88]
per-ex loss: 0.300489  [   17/   88]
per-ex loss: 0.329527  [   18/   88]
per-ex loss: 0.410186  [   19/   88]
per-ex loss: 0.323192  [   20/   88]
per-ex loss: 0.595779  [   21/   88]
per-ex loss: 0.367163  [   22/   88]
per-ex loss: 0.316735  [   23/   88]
per-ex loss: 0.258253  [   24/   88]
per-ex loss: 0.365063  [   25/   88]
per-ex loss: 0.493971  [   26/   88]
per-ex loss: 0.478108  [   27/   88]
per-ex loss: 0.422983  [   28/   88]
per-ex loss: 0.450522  [   29/   88]
per-ex loss: 0.540758  [   30/   88]
per-ex loss: 0.591114  [   31/   88]
per-ex loss: 0.440729  [   32/   88]
per-ex loss: 0.569529  [   33/   88]
per-ex loss: 0.523551  [   34/   88]
per-ex loss: 0.406004  [   35/   88]
per-ex loss: 0.312271  [   36/   88]
per-ex loss: 0.402212  [   37/   88]
per-ex loss: 0.377876  [   38/   88]
per-ex loss: 0.512267  [   39/   88]
per-ex loss: 0.389526  [   40/   88]
per-ex loss: 0.413728  [   41/   88]
per-ex loss: 0.373897  [   42/   88]
per-ex loss: 0.320335  [   43/   88]
per-ex loss: 0.350729  [   44/   88]
per-ex loss: 0.530403  [   45/   88]
per-ex loss: 0.357780  [   46/   88]
per-ex loss: 0.335005  [   47/   88]
per-ex loss: 0.293873  [   48/   88]
per-ex loss: 0.427270  [   49/   88]
per-ex loss: 0.333851  [   50/   88]
per-ex loss: 0.527491  [   51/   88]
per-ex loss: 0.511660  [   52/   88]
per-ex loss: 0.417587  [   53/   88]
per-ex loss: 0.465054  [   54/   88]
per-ex loss: 0.321290  [   55/   88]
per-ex loss: 0.333807  [   56/   88]
per-ex loss: 0.416612  [   57/   88]
per-ex loss: 0.512238  [   58/   88]
per-ex loss: 0.504738  [   59/   88]
per-ex loss: 0.454644  [   60/   88]
per-ex loss: 0.345380  [   61/   88]
per-ex loss: 0.519844  [   62/   88]
per-ex loss: 0.282407  [   63/   88]
per-ex loss: 0.628492  [   64/   88]
per-ex loss: 0.287511  [   65/   88]
per-ex loss: 0.557454  [   66/   88]
per-ex loss: 0.383529  [   67/   88]
per-ex loss: 0.373444  [   68/   88]
per-ex loss: 0.399763  [   69/   88]
per-ex loss: 0.376143  [   70/   88]
per-ex loss: 0.520939  [   71/   88]
per-ex loss: 0.378347  [   72/   88]
per-ex loss: 0.381674  [   73/   88]
per-ex loss: 0.530970  [   74/   88]
per-ex loss: 0.315130  [   75/   88]
per-ex loss: 0.523002  [   76/   88]
per-ex loss: 0.572821  [   77/   88]
per-ex loss: 0.418944  [   78/   88]
per-ex loss: 0.397067  [   79/   88]
per-ex loss: 0.312391  [   80/   88]
per-ex loss: 0.347085  [   81/   88]
per-ex loss: 0.608218  [   82/   88]
per-ex loss: 0.376912  [   83/   88]
per-ex loss: 0.497462  [   84/   88]
per-ex loss: 0.437019  [   85/   88]
per-ex loss: 0.211516  [   86/   88]
per-ex loss: 0.319686  [   87/   88]
per-ex loss: 0.609311  [   88/   88]
Train Error: Avg loss: 0.42020859
validation Error: 
 Avg loss: 0.54067933 
 F1: 0.484306 
 Precision: 0.483948 
 Recall: 0.484666
 IoU: 0.319528

test Error: 
 Avg loss: 0.48617604 
 F1: 0.556164 
 Precision: 0.555601 
 Recall: 0.556729
 IoU: 0.385199

We have finished training iteration 148
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_146_.pth
per-ex loss: 0.397072  [    1/   88]
per-ex loss: 0.513080  [    2/   88]
per-ex loss: 0.302907  [    3/   88]
per-ex loss: 0.355743  [    4/   88]
per-ex loss: 0.341016  [    5/   88]
per-ex loss: 0.610626  [    6/   88]
per-ex loss: 0.316537  [    7/   88]
per-ex loss: 0.438985  [    8/   88]
per-ex loss: 0.461594  [    9/   88]
per-ex loss: 0.223672  [   10/   88]
per-ex loss: 0.320515  [   11/   88]
per-ex loss: 0.370445  [   12/   88]
per-ex loss: 0.443389  [   13/   88]
per-ex loss: 0.358475  [   14/   88]
per-ex loss: 0.331494  [   15/   88]
per-ex loss: 0.375662  [   16/   88]
per-ex loss: 0.437955  [   17/   88]
per-ex loss: 0.372772  [   18/   88]
per-ex loss: 0.352114  [   19/   88]
per-ex loss: 0.542838  [   20/   88]
per-ex loss: 0.361934  [   21/   88]
per-ex loss: 0.556192  [   22/   88]
per-ex loss: 0.568775  [   23/   88]
per-ex loss: 0.300988  [   24/   88]
per-ex loss: 0.272046  [   25/   88]
per-ex loss: 0.440848  [   26/   88]
per-ex loss: 0.559519  [   27/   88]
per-ex loss: 0.409711  [   28/   88]
per-ex loss: 0.454209  [   29/   88]
per-ex loss: 0.310839  [   30/   88]
per-ex loss: 0.393105  [   31/   88]
per-ex loss: 0.300092  [   32/   88]
per-ex loss: 0.425179  [   33/   88]
per-ex loss: 0.622678  [   34/   88]
per-ex loss: 0.393440  [   35/   88]
per-ex loss: 0.375766  [   36/   88]
per-ex loss: 0.508000  [   37/   88]
per-ex loss: 0.329516  [   38/   88]
per-ex loss: 0.533718  [   39/   88]
per-ex loss: 0.548456  [   40/   88]
per-ex loss: 0.291327  [   41/   88]
per-ex loss: 0.340413  [   42/   88]
per-ex loss: 0.580403  [   43/   88]
per-ex loss: 0.385516  [   44/   88]
per-ex loss: 0.546550  [   45/   88]
per-ex loss: 0.293076  [   46/   88]
per-ex loss: 0.501675  [   47/   88]
per-ex loss: 0.310246  [   48/   88]
per-ex loss: 0.394628  [   49/   88]
per-ex loss: 0.389565  [   50/   88]
per-ex loss: 0.325699  [   51/   88]
per-ex loss: 0.557017  [   52/   88]
per-ex loss: 0.358395  [   53/   88]
per-ex loss: 0.534728  [   54/   88]
per-ex loss: 0.511063  [   55/   88]
per-ex loss: 0.297179  [   56/   88]
per-ex loss: 0.602577  [   57/   88]
per-ex loss: 0.407060  [   58/   88]
per-ex loss: 0.625261  [   59/   88]
per-ex loss: 0.364831  [   60/   88]
per-ex loss: 0.464418  [   61/   88]
per-ex loss: 0.296308  [   62/   88]
per-ex loss: 0.611435  [   63/   88]
per-ex loss: 0.463205  [   64/   88]
per-ex loss: 0.357010  [   65/   88]
per-ex loss: 0.343802  [   66/   88]
per-ex loss: 0.349546  [   67/   88]
per-ex loss: 0.375934  [   68/   88]
per-ex loss: 0.543472  [   69/   88]
per-ex loss: 0.339731  [   70/   88]
per-ex loss: 0.389777  [   71/   88]
per-ex loss: 0.434330  [   72/   88]
per-ex loss: 0.566546  [   73/   88]
per-ex loss: 0.414395  [   74/   88]
per-ex loss: 0.524978  [   75/   88]
per-ex loss: 0.485591  [   76/   88]
per-ex loss: 0.362096  [   77/   88]
per-ex loss: 0.375956  [   78/   88]
per-ex loss: 0.309910  [   79/   88]
per-ex loss: 0.327249  [   80/   88]
per-ex loss: 0.515414  [   81/   88]
per-ex loss: 0.526139  [   82/   88]
per-ex loss: 0.368758  [   83/   88]
per-ex loss: 0.627293  [   84/   88]
per-ex loss: 0.387455  [   85/   88]
per-ex loss: 0.357599  [   86/   88]
per-ex loss: 0.425208  [   87/   88]
per-ex loss: 0.541921  [   88/   88]
Train Error: Avg loss: 0.42312028
validation Error: 
 Avg loss: 0.54060869 
 F1: 0.488343 
 Precision: 0.501306 
 Recall: 0.476034
 IoU: 0.323052

test Error: 
 Avg loss: 0.47881127 
 F1: 0.567438 
 Precision: 0.577910 
 Recall: 0.557339
 IoU: 0.396100

We have finished training iteration 149
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_147_.pth
per-ex loss: 0.319065  [    1/   88]
per-ex loss: 0.418293  [    2/   88]
per-ex loss: 0.527682  [    3/   88]
per-ex loss: 0.616265  [    4/   88]
per-ex loss: 0.318820  [    5/   88]
per-ex loss: 0.471359  [    6/   88]
per-ex loss: 0.382064  [    7/   88]
per-ex loss: 0.417780  [    8/   88]
per-ex loss: 0.345847  [    9/   88]
per-ex loss: 0.360034  [   10/   88]
per-ex loss: 0.520694  [   11/   88]
per-ex loss: 0.386981  [   12/   88]
per-ex loss: 0.395296  [   13/   88]
per-ex loss: 0.546475  [   14/   88]
per-ex loss: 0.395680  [   15/   88]
per-ex loss: 0.326285  [   16/   88]
per-ex loss: 0.559876  [   17/   88]
per-ex loss: 0.547789  [   18/   88]
per-ex loss: 0.528690  [   19/   88]
per-ex loss: 0.367198  [   20/   88]
per-ex loss: 0.350853  [   21/   88]
per-ex loss: 0.431085  [   22/   88]
per-ex loss: 0.397256  [   23/   88]
per-ex loss: 0.553441  [   24/   88]
per-ex loss: 0.393688  [   25/   88]
per-ex loss: 0.441960  [   26/   88]
per-ex loss: 0.349268  [   27/   88]
per-ex loss: 0.537015  [   28/   88]
per-ex loss: 0.552455  [   29/   88]
per-ex loss: 0.551147  [   30/   88]
per-ex loss: 0.473779  [   31/   88]
per-ex loss: 0.292260  [   32/   88]
per-ex loss: 0.442245  [   33/   88]
per-ex loss: 0.295842  [   34/   88]
per-ex loss: 0.602345  [   35/   88]
per-ex loss: 0.382455  [   36/   88]
per-ex loss: 0.361862  [   37/   88]
per-ex loss: 0.502529  [   38/   88]
per-ex loss: 0.322540  [   39/   88]
per-ex loss: 0.373984  [   40/   88]
per-ex loss: 0.300739  [   41/   88]
per-ex loss: 0.520853  [   42/   88]
per-ex loss: 0.330868  [   43/   88]
per-ex loss: 0.463778  [   44/   88]
per-ex loss: 0.436229  [   45/   88]
per-ex loss: 0.450833  [   46/   88]
per-ex loss: 0.393226  [   47/   88]
per-ex loss: 0.340792  [   48/   88]
per-ex loss: 0.285282  [   49/   88]
per-ex loss: 0.359120  [   50/   88]
per-ex loss: 0.445672  [   51/   88]
per-ex loss: 0.352793  [   52/   88]
per-ex loss: 0.355111  [   53/   88]
per-ex loss: 0.477429  [   54/   88]
per-ex loss: 0.320283  [   55/   88]
per-ex loss: 0.522846  [   56/   88]
per-ex loss: 0.519304  [   57/   88]
per-ex loss: 0.554269  [   58/   88]
per-ex loss: 0.346196  [   59/   88]
per-ex loss: 0.544073  [   60/   88]
per-ex loss: 0.417089  [   61/   88]
per-ex loss: 0.372617  [   62/   88]
per-ex loss: 0.352317  [   63/   88]
per-ex loss: 0.453022  [   64/   88]
per-ex loss: 0.471277  [   65/   88]
per-ex loss: 0.632209  [   66/   88]
per-ex loss: 0.285542  [   67/   88]
per-ex loss: 0.316388  [   68/   88]
per-ex loss: 0.329707  [   69/   88]
per-ex loss: 0.350455  [   70/   88]
per-ex loss: 0.636208  [   71/   88]
per-ex loss: 0.362069  [   72/   88]
per-ex loss: 0.419041  [   73/   88]
per-ex loss: 0.385039  [   74/   88]
per-ex loss: 0.279276  [   75/   88]
per-ex loss: 0.338284  [   76/   88]
per-ex loss: 0.304759  [   77/   88]
per-ex loss: 0.219890  [   78/   88]
per-ex loss: 0.609893  [   79/   88]
per-ex loss: 0.559873  [   80/   88]
per-ex loss: 0.439023  [   81/   88]
per-ex loss: 0.513992  [   82/   88]
per-ex loss: 0.638803  [   83/   88]
per-ex loss: 0.466739  [   84/   88]
per-ex loss: 0.401475  [   85/   88]
per-ex loss: 0.414018  [   86/   88]
per-ex loss: 0.514642  [   87/   88]
per-ex loss: 0.571666  [   88/   88]
Train Error: Avg loss: 0.42880903
validation Error: 
 Avg loss: 0.54023153 
 F1: 0.490652 
 Precision: 0.598969 
 Recall: 0.415512
 IoU: 0.325076

test Error: 
 Avg loss: 0.49119328 
 F1: 0.555989 
 Precision: 0.663854 
 Recall: 0.478277
 IoU: 0.385031

We have finished training iteration 150
Deleting model ./unet_att_train/saved_model_wrapper/models/UNet_148_.pth
Max training iterations reached: 150. Train_iter: 150, Initial_train_iter: 0
