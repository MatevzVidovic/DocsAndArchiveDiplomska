/shared/home/matevz.vidovic/Diplomska/Prototip/Delo/model_wrapper.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model = torch.load(self.prev_model_path, map_location=torch.device(device))
unet_original_main.py do_log: True
Log file name: log_07_14-49-31_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(ptd='./Data/vein_and_sclera_data', sd='unet_wzo_train', mti=250, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_original_wzo.yaml', ntibp=None, ptp=None, map=None)
YAML: {'batch_size': 2, 'learning_rate': 1e-05, 'num_of_dataloader_workers': 7, 'train_epoch_size_limit': 400, 'num_epochs_per_training_iteration': 1, 'cleanup_k': 3, 'dataset_option': 'augment', 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': True, 'loss_fn_name': 'MCDL', 'alphas': [], 'model': '64_2_6', 'input_width': 2048, 'input_height': 1024, 'input_channels': 3, 'output_channels': 2, 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq'}
Validation phase: False
Namespace(ptd='./Data/vein_and_sclera_data', sd='unet_wzo_train', mti=250, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_original_wzo.yaml', ntibp=None, ptp=None, map=None)
Device: cuda
dataset_aug.py do_log: True
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 89
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 89
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 45
val dataloader num of batches: 14
test dataloader num of batches: 6
Loaded model path:  ./unet_wzo_train/saved_model_wrapper/models/UNet_69_.pth
Per-example loss in batch: 0.432214  [    2/   89]
Per-example loss in batch: 0.362841  [    4/   89]
Per-example loss in batch: 0.449224  [    6/   89]
Per-example loss in batch: 0.371880  [    8/   89]
Per-example loss in batch: 0.426782  [   10/   89]
Per-example loss in batch: 0.397791  [   12/   89]
Per-example loss in batch: 0.405330  [   14/   89]
Per-example loss in batch: 0.429220  [   16/   89]
Per-example loss in batch: 0.438085  [   18/   89]
Per-example loss in batch: 0.374374  [   20/   89]
Per-example loss in batch: 0.427111  [   22/   89]
Per-example loss in batch: 0.438218  [   24/   89]
Per-example loss in batch: 0.465779  [   26/   89]
Per-example loss in batch: 0.393901  [   28/   89]
Per-example loss in batch: 0.451665  [   30/   89]
Per-example loss in batch: 0.433311  [   32/   89]
Per-example loss in batch: 0.373541  [   34/   89]
Per-example loss in batch: 0.425548  [   36/   89]
Per-example loss in batch: 0.475287  [   38/   89]
Per-example loss in batch: 0.470953  [   40/   89]
Per-example loss in batch: 0.391533  [   42/   89]
Per-example loss in batch: 0.433632  [   44/   89]
Per-example loss in batch: 0.407880  [   46/   89]
Per-example loss in batch: 0.419162  [   48/   89]
Per-example loss in batch: 0.433175  [   50/   89]
Per-example loss in batch: 0.418743  [   52/   89]
Per-example loss in batch: 0.390846  [   54/   89]
Per-example loss in batch: 0.435771  [   56/   89]
Per-example loss in batch: 0.454176  [   58/   89]
Per-example loss in batch: 0.358695  [   60/   89]
Per-example loss in batch: 0.439314  [   62/   89]
Per-example loss in batch: 0.416142  [   64/   89]
Per-example loss in batch: 0.418210  [   66/   89]
Per-example loss in batch: 0.351296  [   68/   89]
Per-example loss in batch: 0.448262  [   70/   89]
Per-example loss in batch: 0.434361  [   72/   89]
Per-example loss in batch: 0.414794  [   74/   89]
Per-example loss in batch: 0.468147  [   76/   89]
Per-example loss in batch: 0.406830  [   78/   89]
Per-example loss in batch: 0.449729  [   80/   89]
Per-example loss in batch: 0.420555  [   82/   89]
Per-example loss in batch: 0.439212  [   84/   89]
Per-example loss in batch: 0.455251  [   86/   89]
Per-example loss in batch: 0.445967  [   88/   89]
Per-example loss in batch: 0.779655  [   89/   89]
Train Error: Avg loss: 0.42661950
validation Error: 
 Avg loss: 0.50791560 
 F1: 0.139125 
 Precision: 0.982744 
 Recall: 0.074861
 IoU: 0.074763

test Error: 
 Avg loss: 0.48876428 
 F1: 0.145334 
 Precision: 0.976198 
 Recall: 0.078511
 IoU: 0.078361

We have finished training iteration 70
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_68_.pth
Per-example loss in batch: 0.406509  [    2/   89]
Per-example loss in batch: 0.429484  [    4/   89]
Per-example loss in batch: 0.365372  [    6/   89]
Per-example loss in batch: 0.354566  [    8/   89]
Per-example loss in batch: 0.439434  [   10/   89]
Per-example loss in batch: 0.474225  [   12/   89]
Per-example loss in batch: 0.385008  [   14/   89]
Per-example loss in batch: 0.354562  [   16/   89]
Per-example loss in batch: 0.383574  [   18/   89]
Per-example loss in batch: 0.438560  [   20/   89]
Per-example loss in batch: 0.398745  [   22/   89]
Per-example loss in batch: 0.424750  [   24/   89]
Per-example loss in batch: 0.356550  [   26/   89]
Per-example loss in batch: 0.445243  [   28/   89]
Per-example loss in batch: 0.421353  [   30/   89]
Per-example loss in batch: 0.441751  [   32/   89]
Per-example loss in batch: 0.444832  [   34/   89]
Per-example loss in batch: 0.442005  [   36/   89]
Per-example loss in batch: 0.431896  [   38/   89]
Per-example loss in batch: 0.419712  [   40/   89]
Per-example loss in batch: 0.423768  [   42/   89]
Per-example loss in batch: 0.433734  [   44/   89]
Per-example loss in batch: 0.391017  [   46/   89]
Per-example loss in batch: 0.472900  [   48/   89]
Per-example loss in batch: 0.382408  [   50/   89]
Per-example loss in batch: 0.419283  [   52/   89]
Per-example loss in batch: 0.411584  [   54/   89]
Per-example loss in batch: 0.393541  [   56/   89]
Per-example loss in batch: 0.467015  [   58/   89]
Per-example loss in batch: 0.431226  [   60/   89]
Per-example loss in batch: 0.445583  [   62/   89]
Per-example loss in batch: 0.423388  [   64/   89]
Per-example loss in batch: 0.426892  [   66/   89]
Per-example loss in batch: 0.440420  [   68/   89]
Per-example loss in batch: 0.433687  [   70/   89]
Per-example loss in batch: 0.389318  [   72/   89]
Per-example loss in batch: 0.440044  [   74/   89]
Per-example loss in batch: 0.427563  [   76/   89]
Per-example loss in batch: 0.461883  [   78/   89]
Per-example loss in batch: 0.438035  [   80/   89]
Per-example loss in batch: 0.459171  [   82/   89]
Per-example loss in batch: 0.470813  [   84/   89]
Per-example loss in batch: 0.452263  [   86/   89]
Per-example loss in batch: 0.409197  [   88/   89]
Per-example loss in batch: 0.901933  [   89/   89]
Train Error: Avg loss: 0.42817593
validation Error: 
 Avg loss: 0.50827371 
 F1: 0.145152 
 Precision: 0.972928 
 Recall: 0.078426
 IoU: 0.078256

test Error: 
 Avg loss: 0.48883681 
 F1: 0.150360 
 Precision: 0.964127 
 Recall: 0.081538
 IoU: 0.081291

We have finished training iteration 71
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_69_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.444489  [    2/   89]
Per-example loss in batch: 0.359254  [    4/   89]
Per-example loss in batch: 0.443101  [    6/   89]
Per-example loss in batch: 0.437315  [    8/   89]
Per-example loss in batch: 0.389430  [   10/   89]
Per-example loss in batch: 0.450995  [   12/   89]
Per-example loss in batch: 0.446924  [   14/   89]
Per-example loss in batch: 0.456414  [   16/   89]
Per-example loss in batch: 0.457909  [   18/   89]
Per-example loss in batch: 0.423056  [   20/   89]
Per-example loss in batch: 0.425344  [   22/   89]
Per-example loss in batch: 0.440480  [   24/   89]
Per-example loss in batch: 0.420792  [   26/   89]
Per-example loss in batch: 0.412639  [   28/   89]
Per-example loss in batch: 0.425686  [   30/   89]
Per-example loss in batch: 0.441144  [   32/   89]
Per-example loss in batch: 0.408420  [   34/   89]
Per-example loss in batch: 0.459259  [   36/   89]
Per-example loss in batch: 0.387656  [   38/   89]
Per-example loss in batch: 0.422828  [   40/   89]
Per-example loss in batch: 0.380580  [   42/   89]
Per-example loss in batch: 0.428139  [   44/   89]
Per-example loss in batch: 0.397338  [   46/   89]
Per-example loss in batch: 0.467733  [   48/   89]
Per-example loss in batch: 0.425955  [   50/   89]
Per-example loss in batch: 0.406945  [   52/   89]
Per-example loss in batch: 0.347374  [   54/   89]
Per-example loss in batch: 0.434011  [   56/   89]
Per-example loss in batch: 0.396122  [   58/   89]
Per-example loss in batch: 0.397565  [   60/   89]
Per-example loss in batch: 0.463091  [   62/   89]
Per-example loss in batch: 0.458525  [   64/   89]
Per-example loss in batch: 0.384352  [   66/   89]
Per-example loss in batch: 0.417204  [   68/   89]
Per-example loss in batch: 0.446541  [   70/   89]
Per-example loss in batch: 0.450194  [   72/   89]
Per-example loss in batch: 0.469938  [   74/   89]
Per-example loss in batch: 0.429014  [   76/   89]
Per-example loss in batch: 0.451449  [   78/   89]
Per-example loss in batch: 0.454452  [   80/   89]
Per-example loss in batch: 0.313380  [   82/   89]
Per-example loss in batch: 0.411285  [   84/   89]
Per-example loss in batch: 0.435152  [   86/   89]
Per-example loss in batch: 0.426066  [   88/   89]
Per-example loss in batch: 0.898843  [   89/   89]
Train Error: Avg loss: 0.42910021
validation Error: 
 Avg loss: 0.50728266 
 F1: 0.149929 
 Precision: 0.961702 
 Recall: 0.081302
 IoU: 0.081040

test Error: 
 Avg loss: 0.48877932 
 F1: 0.153904 
 Precision: 0.948978 
 Recall: 0.083743
 IoU: 0.083367

We have finished training iteration 72
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_70_.pth
Per-example loss in batch: 0.448704  [    2/   89]
Per-example loss in batch: 0.461565  [    4/   89]
Per-example loss in batch: 0.455819  [    6/   89]
Per-example loss in batch: 0.426016  [    8/   89]
Per-example loss in batch: 0.413053  [   10/   89]
Per-example loss in batch: 0.316386  [   12/   89]
Per-example loss in batch: 0.439601  [   14/   89]
Per-example loss in batch: 0.441227  [   16/   89]
Per-example loss in batch: 0.443759  [   18/   89]
Per-example loss in batch: 0.437694  [   20/   89]
Per-example loss in batch: 0.422358  [   22/   89]
Per-example loss in batch: 0.404874  [   24/   89]
Per-example loss in batch: 0.382282  [   26/   89]
Per-example loss in batch: 0.448078  [   28/   89]
Per-example loss in batch: 0.439983  [   30/   89]
Per-example loss in batch: 0.434596  [   32/   89]
Per-example loss in batch: 0.412222  [   34/   89]
Per-example loss in batch: 0.357548  [   36/   89]
Per-example loss in batch: 0.396204  [   38/   89]
Per-example loss in batch: 0.417587  [   40/   89]
Per-example loss in batch: 0.444941  [   42/   89]
Per-example loss in batch: 0.447849  [   44/   89]
Per-example loss in batch: 0.403401  [   46/   89]
Per-example loss in batch: 0.382006  [   48/   89]
Per-example loss in batch: 0.366641  [   50/   89]
Per-example loss in batch: 0.352976  [   52/   89]
Per-example loss in batch: 0.424965  [   54/   89]
Per-example loss in batch: 0.418195  [   56/   89]
Per-example loss in batch: 0.395321  [   58/   89]
Per-example loss in batch: 0.449646  [   60/   89]
Per-example loss in batch: 0.392163  [   62/   89]
Per-example loss in batch: 0.452943  [   64/   89]
Per-example loss in batch: 0.437921  [   66/   89]
Per-example loss in batch: 0.415168  [   68/   89]
Per-example loss in batch: 0.457159  [   70/   89]
Per-example loss in batch: 0.452734  [   72/   89]
Per-example loss in batch: 0.413266  [   74/   89]
Per-example loss in batch: 0.375251  [   76/   89]
Per-example loss in batch: 0.413567  [   78/   89]
Per-example loss in batch: 0.406374  [   80/   89]
Per-example loss in batch: 0.436435  [   82/   89]
Per-example loss in batch: 0.457407  [   84/   89]
Per-example loss in batch: 0.442049  [   86/   89]
Per-example loss in batch: 0.397193  [   88/   89]
Per-example loss in batch: 0.775015  [   89/   89]
Train Error: Avg loss: 0.42298054
validation Error: 
 Avg loss: 0.50813289 
 F1: 0.154614 
 Precision: 0.947389 
 Recall: 0.084176
 IoU: 0.083784

test Error: 
 Avg loss: 0.48897876 
 F1: 0.155898 
 Precision: 0.938585 
 Recall: 0.085009
 IoU: 0.084538

We have finished training iteration 73
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_71_.pth
Per-example loss in batch: 0.404408  [    2/   89]
Per-example loss in batch: 0.403380  [    4/   89]
Per-example loss in batch: 0.418666  [    6/   89]
Per-example loss in batch: 0.466633  [    8/   89]
Per-example loss in batch: 0.319707  [   10/   89]
Per-example loss in batch: 0.395474  [   12/   89]
Per-example loss in batch: 0.467416  [   14/   89]
Per-example loss in batch: 0.370686  [   16/   89]
Per-example loss in batch: 0.402570  [   18/   89]
Per-example loss in batch: 0.426768  [   20/   89]
Per-example loss in batch: 0.411879  [   22/   89]
Per-example loss in batch: 0.463592  [   24/   89]
Per-example loss in batch: 0.446903  [   26/   89]
Per-example loss in batch: 0.422311  [   28/   89]
Per-example loss in batch: 0.448856  [   30/   89]
Per-example loss in batch: 0.343320  [   32/   89]
Per-example loss in batch: 0.408772  [   34/   89]
Per-example loss in batch: 0.420424  [   36/   89]
Per-example loss in batch: 0.419332  [   38/   89]
Per-example loss in batch: 0.377931  [   40/   89]
Per-example loss in batch: 0.425496  [   42/   89]
Per-example loss in batch: 0.409160  [   44/   89]
Per-example loss in batch: 0.407468  [   46/   89]
Per-example loss in batch: 0.466519  [   48/   89]
Per-example loss in batch: 0.456767  [   50/   89]
Per-example loss in batch: 0.378772  [   52/   89]
Per-example loss in batch: 0.376549  [   54/   89]
Per-example loss in batch: 0.426422  [   56/   89]
Per-example loss in batch: 0.424680  [   58/   89]
Per-example loss in batch: 0.429716  [   60/   89]
Per-example loss in batch: 0.428847  [   62/   89]
Per-example loss in batch: 0.435772  [   64/   89]
Per-example loss in batch: 0.446796  [   66/   89]
Per-example loss in batch: 0.437623  [   68/   89]
Per-example loss in batch: 0.432374  [   70/   89]
Per-example loss in batch: 0.453427  [   72/   89]
Per-example loss in batch: 0.312160  [   74/   89]
Per-example loss in batch: 0.442296  [   76/   89]
Per-example loss in batch: 0.437459  [   78/   89]
Per-example loss in batch: 0.438975  [   80/   89]
Per-example loss in batch: 0.452077  [   82/   89]
Per-example loss in batch: 0.395820  [   84/   89]
Per-example loss in batch: 0.351568  [   86/   89]
Per-example loss in batch: 0.364508  [   88/   89]
Per-example loss in batch: 0.837992  [   89/   89]
Train Error: Avg loss: 0.41998369
validation Error: 
 Avg loss: 0.50767998 
 F1: 0.164919 
 Precision: 0.918557 
 Recall: 0.090592
 IoU: 0.089870

test Error: 
 Avg loss: 0.48913491 
 F1: 0.164650 
 Precision: 0.911487 
 Recall: 0.090499
 IoU: 0.089711

We have finished training iteration 74
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_72_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.409935  [    2/   89]
Per-example loss in batch: 0.450679  [    4/   89]
Per-example loss in batch: 0.349528  [    6/   89]
Per-example loss in batch: 0.384254  [    8/   89]
Per-example loss in batch: 0.439940  [   10/   89]
Per-example loss in batch: 0.425532  [   12/   89]
Per-example loss in batch: 0.441711  [   14/   89]
Per-example loss in batch: 0.431340  [   16/   89]
Per-example loss in batch: 0.444283  [   18/   89]
Per-example loss in batch: 0.392871  [   20/   89]
Per-example loss in batch: 0.439203  [   22/   89]
Per-example loss in batch: 0.418367  [   24/   89]
Per-example loss in batch: 0.424896  [   26/   89]
Per-example loss in batch: 0.369411  [   28/   89]
Per-example loss in batch: 0.476023  [   30/   89]
Per-example loss in batch: 0.441431  [   32/   89]
Per-example loss in batch: 0.443911  [   34/   89]
Per-example loss in batch: 0.407652  [   36/   89]
Per-example loss in batch: 0.467834  [   38/   89]
Per-example loss in batch: 0.432551  [   40/   89]
Per-example loss in batch: 0.457643  [   42/   89]
Per-example loss in batch: 0.369819  [   44/   89]
Per-example loss in batch: 0.408156  [   46/   89]
Per-example loss in batch: 0.413752  [   48/   89]
Per-example loss in batch: 0.395145  [   50/   89]
Per-example loss in batch: 0.331346  [   52/   89]
Per-example loss in batch: 0.393580  [   54/   89]
Per-example loss in batch: 0.420901  [   56/   89]
Per-example loss in batch: 0.339326  [   58/   89]
Per-example loss in batch: 0.385279  [   60/   89]
Per-example loss in batch: 0.402718  [   62/   89]
Per-example loss in batch: 0.426237  [   64/   89]
Per-example loss in batch: 0.388630  [   66/   89]
Per-example loss in batch: 0.302287  [   68/   89]
Per-example loss in batch: 0.376506  [   70/   89]
Per-example loss in batch: 0.393554  [   72/   89]
Per-example loss in batch: 0.394905  [   74/   89]
Per-example loss in batch: 0.429559  [   76/   89]
Per-example loss in batch: 0.430206  [   78/   89]
Per-example loss in batch: 0.301108  [   80/   89]
Per-example loss in batch: 0.346348  [   82/   89]
Per-example loss in batch: 0.431152  [   84/   89]
Per-example loss in batch: 0.436891  [   86/   89]
Per-example loss in batch: 0.404863  [   88/   89]
Per-example loss in batch: 0.772823  [   89/   89]
Train Error: Avg loss: 0.41028480
validation Error: 
 Avg loss: 0.50820956 
 F1: 0.185732 
 Precision: 0.826446 
 Recall: 0.104622
 IoU: 0.102373

test Error: 
 Avg loss: 0.48983657 
 F1: 0.183759 
 Precision: 0.811427 
 Recall: 0.103611
 IoU: 0.101175

We have finished training iteration 75
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_73_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.374329  [    2/   89]
Per-example loss in batch: 0.406408  [    4/   89]
Per-example loss in batch: 0.389893  [    6/   89]
Per-example loss in batch: 0.417038  [    8/   89]
Per-example loss in batch: 0.357073  [   10/   89]
Per-example loss in batch: 0.415187  [   12/   89]
Per-example loss in batch: 0.366441  [   14/   89]
Per-example loss in batch: 0.444873  [   16/   89]
Per-example loss in batch: 0.395855  [   18/   89]
Per-example loss in batch: 0.338664  [   20/   89]
Per-example loss in batch: 0.443552  [   22/   89]
Per-example loss in batch: 0.384038  [   24/   89]
Per-example loss in batch: 0.448989  [   26/   89]
Per-example loss in batch: 0.430189  [   28/   89]
Per-example loss in batch: 0.382287  [   30/   89]
Per-example loss in batch: 0.422376  [   32/   89]
Per-example loss in batch: 0.393626  [   34/   89]
Per-example loss in batch: 0.372635  [   36/   89]
Per-example loss in batch: 0.383894  [   38/   89]
Per-example loss in batch: 0.452635  [   40/   89]
Per-example loss in batch: 0.468499  [   42/   89]
Per-example loss in batch: 0.405465  [   44/   89]
Per-example loss in batch: 0.391202  [   46/   89]
Per-example loss in batch: 0.333176  [   48/   89]
Per-example loss in batch: 0.397946  [   50/   89]
Per-example loss in batch: 0.375532  [   52/   89]
Per-example loss in batch: 0.355638  [   54/   89]
Per-example loss in batch: 0.401563  [   56/   89]
Per-example loss in batch: 0.439121  [   58/   89]
Per-example loss in batch: 0.389663  [   60/   89]
Per-example loss in batch: 0.443467  [   62/   89]
Per-example loss in batch: 0.382688  [   64/   89]
Per-example loss in batch: 0.420777  [   66/   89]
Per-example loss in batch: 0.434860  [   68/   89]
Per-example loss in batch: 0.435534  [   70/   89]
Per-example loss in batch: 0.429182  [   72/   89]
Per-example loss in batch: 0.385773  [   74/   89]
Per-example loss in batch: 0.409463  [   76/   89]
Per-example loss in batch: 0.406239  [   78/   89]
Per-example loss in batch: 0.407969  [   80/   89]
Per-example loss in batch: 0.377422  [   82/   89]
Per-example loss in batch: 0.422033  [   84/   89]
Per-example loss in batch: 0.395023  [   86/   89]
Per-example loss in batch: 0.418489  [   88/   89]
Per-example loss in batch: 0.683407  [   89/   89]
Train Error: Avg loss: 0.40648113
validation Error: 
 Avg loss: 0.50933547 
 F1: 0.192148 
 Precision: 0.788219 
 Recall: 0.109410
 IoU: 0.106285

test Error: 
 Avg loss: 0.49029112 
 F1: 0.189869 
 Precision: 0.770500 
 Recall: 0.108275
 IoU: 0.104892

We have finished training iteration 76
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_74_.pth
Per-example loss in batch: 0.447577  [    2/   89]
Per-example loss in batch: 0.446167  [    4/   89]
Per-example loss in batch: 0.394403  [    6/   89]
Per-example loss in batch: 0.437069  [    8/   89]
Per-example loss in batch: 0.414455  [   10/   89]
Per-example loss in batch: 0.417827  [   12/   89]
Per-example loss in batch: 0.283484  [   14/   89]
Per-example loss in batch: 0.399914  [   16/   89]
Per-example loss in batch: 0.392082  [   18/   89]
Per-example loss in batch: 0.435277  [   20/   89]
Per-example loss in batch: 0.451476  [   22/   89]
Per-example loss in batch: 0.435484  [   24/   89]
Per-example loss in batch: 0.389333  [   26/   89]
Per-example loss in batch: 0.361709  [   28/   89]
Per-example loss in batch: 0.390525  [   30/   89]
Per-example loss in batch: 0.420204  [   32/   89]
Per-example loss in batch: 0.420896  [   34/   89]
Per-example loss in batch: 0.445989  [   36/   89]
Per-example loss in batch: 0.434463  [   38/   89]
Per-example loss in batch: 0.338741  [   40/   89]
Per-example loss in batch: 0.419859  [   42/   89]
Per-example loss in batch: 0.397041  [   44/   89]
Per-example loss in batch: 0.419129  [   46/   89]
Per-example loss in batch: 0.399213  [   48/   89]
Per-example loss in batch: 0.433261  [   50/   89]
Per-example loss in batch: 0.420200  [   52/   89]
Per-example loss in batch: 0.369779  [   54/   89]
Per-example loss in batch: 0.418336  [   56/   89]
Per-example loss in batch: 0.353345  [   58/   89]
Per-example loss in batch: 0.422577  [   60/   89]
Per-example loss in batch: 0.386115  [   62/   89]
Per-example loss in batch: 0.427204  [   64/   89]
Per-example loss in batch: 0.362557  [   66/   89]
Per-example loss in batch: 0.446245  [   68/   89]
Per-example loss in batch: 0.394168  [   70/   89]
Per-example loss in batch: 0.371507  [   72/   89]
Per-example loss in batch: 0.354994  [   74/   89]
Per-example loss in batch: 0.427043  [   76/   89]
Per-example loss in batch: 0.371473  [   78/   89]
Per-example loss in batch: 0.418337  [   80/   89]
Per-example loss in batch: 0.370627  [   82/   89]
Per-example loss in batch: 0.421892  [   84/   89]
Per-example loss in batch: 0.358814  [   86/   89]
Per-example loss in batch: 0.440114  [   88/   89]
Per-example loss in batch: 0.588301  [   89/   89]
Train Error: Avg loss: 0.40573156
validation Error: 
 Avg loss: 0.50970816 
 F1: 0.195953 
 Precision: 0.753244 
 Recall: 0.112626
 IoU: 0.108618

test Error: 
 Avg loss: 0.49058017 
 F1: 0.193095 
 Precision: 0.739034 
 Recall: 0.111056
 IoU: 0.106865

We have finished training iteration 77
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_75_.pth
Per-example loss in batch: 0.430401  [    2/   89]
Per-example loss in batch: 0.381333  [    4/   89]
Per-example loss in batch: 0.392116  [    6/   89]
Per-example loss in batch: 0.415575  [    8/   89]
Per-example loss in batch: 0.389418  [   10/   89]
Per-example loss in batch: 0.433847  [   12/   89]
Per-example loss in batch: 0.394298  [   14/   89]
Per-example loss in batch: 0.314132  [   16/   89]
Per-example loss in batch: 0.402158  [   18/   89]
Per-example loss in batch: 0.350113  [   20/   89]
Per-example loss in batch: 0.420459  [   22/   89]
Per-example loss in batch: 0.406041  [   24/   89]
Per-example loss in batch: 0.394551  [   26/   89]
Per-example loss in batch: 0.334712  [   28/   89]
Per-example loss in batch: 0.418167  [   30/   89]
Per-example loss in batch: 0.450409  [   32/   89]
Per-example loss in batch: 0.437655  [   34/   89]
Per-example loss in batch: 0.376117  [   36/   89]
Per-example loss in batch: 0.391106  [   38/   89]
Per-example loss in batch: 0.448585  [   40/   89]
Per-example loss in batch: 0.388834  [   42/   89]
Per-example loss in batch: 0.451777  [   44/   89]
Per-example loss in batch: 0.452079  [   46/   89]
Per-example loss in batch: 0.409195  [   48/   89]
Per-example loss in batch: 0.439851  [   50/   89]
Per-example loss in batch: 0.386961  [   52/   89]
Per-example loss in batch: 0.426523  [   54/   89]
Per-example loss in batch: 0.400842  [   56/   89]
Per-example loss in batch: 0.375732  [   58/   89]
Per-example loss in batch: 0.419683  [   60/   89]
Per-example loss in batch: 0.426006  [   62/   89]
Per-example loss in batch: 0.392436  [   64/   89]
Per-example loss in batch: 0.348934  [   66/   89]
Per-example loss in batch: 0.348979  [   68/   89]
Per-example loss in batch: 0.388467  [   70/   89]
Per-example loss in batch: 0.438579  [   72/   89]
Per-example loss in batch: 0.386376  [   74/   89]
Per-example loss in batch: 0.355019  [   76/   89]
Per-example loss in batch: 0.338769  [   78/   89]
Per-example loss in batch: 0.420549  [   80/   89]
Per-example loss in batch: 0.378454  [   82/   89]
Per-example loss in batch: 0.468959  [   84/   89]
Per-example loss in batch: 0.411327  [   86/   89]
Per-example loss in batch: 0.375784  [   88/   89]
Per-example loss in batch: 0.597252  [   89/   89]
Train Error: Avg loss: 0.40247044
validation Error: 
 Avg loss: 0.51000447 
 F1: 0.205048 
 Precision: 0.701124 
 Recall: 0.120084
 IoU: 0.114236

test Error: 
 Avg loss: 0.49097678 
 F1: 0.202806 
 Precision: 0.681213 
 Recall: 0.119137
 IoU: 0.112846

We have finished training iteration 78
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_76_.pth
Per-example loss in batch: 0.418345  [    2/   89]
Per-example loss in batch: 0.383583  [    4/   89]
Per-example loss in batch: 0.403839  [    6/   89]
Per-example loss in batch: 0.410701  [    8/   89]
Per-example loss in batch: 0.435221  [   10/   89]
Per-example loss in batch: 0.418207  [   12/   89]
Per-example loss in batch: 0.348125  [   14/   89]
Per-example loss in batch: 0.385427  [   16/   89]
Per-example loss in batch: 0.316085  [   18/   89]
Per-example loss in batch: 0.331204  [   20/   89]
Per-example loss in batch: 0.432605  [   22/   89]
Per-example loss in batch: 0.386885  [   24/   89]
Per-example loss in batch: 0.420496  [   26/   89]
Per-example loss in batch: 0.438747  [   28/   89]
Per-example loss in batch: 0.349324  [   30/   89]
Per-example loss in batch: 0.385130  [   32/   89]
Per-example loss in batch: 0.351636  [   34/   89]
Per-example loss in batch: 0.450401  [   36/   89]
Per-example loss in batch: 0.432181  [   38/   89]
Per-example loss in batch: 0.415951  [   40/   89]
Per-example loss in batch: 0.382006  [   42/   89]
Per-example loss in batch: 0.412815  [   44/   89]
Per-example loss in batch: 0.377819  [   46/   89]
Per-example loss in batch: 0.419945  [   48/   89]
Per-example loss in batch: 0.417995  [   50/   89]
Per-example loss in batch: 0.377208  [   52/   89]
Per-example loss in batch: 0.371309  [   54/   89]
Per-example loss in batch: 0.314115  [   56/   89]
Per-example loss in batch: 0.446663  [   58/   89]
Per-example loss in batch: 0.402725  [   60/   89]
Per-example loss in batch: 0.436591  [   62/   89]
Per-example loss in batch: 0.398294  [   64/   89]
Per-example loss in batch: 0.401750  [   66/   89]
Per-example loss in batch: 0.423663  [   68/   89]
Per-example loss in batch: 0.370932  [   70/   89]
Per-example loss in batch: 0.423317  [   72/   89]
Per-example loss in batch: 0.401724  [   74/   89]
Per-example loss in batch: 0.431472  [   76/   89]
Per-example loss in batch: 0.357264  [   78/   89]
Per-example loss in batch: 0.417698  [   80/   89]
Per-example loss in batch: 0.412395  [   82/   89]
Per-example loss in batch: 0.425030  [   84/   89]
Per-example loss in batch: 0.408544  [   86/   89]
Per-example loss in batch: 0.452322  [   88/   89]
Per-example loss in batch: 0.808883  [   89/   89]
Train Error: Avg loss: 0.40454229
validation Error: 
 Avg loss: 0.50964858 
 F1: 0.210404 
 Precision: 0.683618 
 Recall: 0.124336
 IoU: 0.117571

test Error: 
 Avg loss: 0.49111979 
 F1: 0.209374 
 Precision: 0.667585 
 Recall: 0.124157
 IoU: 0.116928

We have finished training iteration 79
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_77_.pth
Per-example loss in batch: 0.421079  [    2/   89]
Per-example loss in batch: 0.418482  [    4/   89]
Per-example loss in batch: 0.412752  [    6/   89]
Per-example loss in batch: 0.389157  [    8/   89]
Per-example loss in batch: 0.425426  [   10/   89]
Per-example loss in batch: 0.384260  [   12/   89]
Per-example loss in batch: 0.414391  [   14/   89]
Per-example loss in batch: 0.386623  [   16/   89]
Per-example loss in batch: 0.414685  [   18/   89]
Per-example loss in batch: 0.304234  [   20/   89]
Per-example loss in batch: 0.379027  [   22/   89]
Per-example loss in batch: 0.410534  [   24/   89]
Per-example loss in batch: 0.416958  [   26/   89]
Per-example loss in batch: 0.365102  [   28/   89]
Per-example loss in batch: 0.411751  [   30/   89]
Per-example loss in batch: 0.305095  [   32/   89]
Per-example loss in batch: 0.425570  [   34/   89]
Per-example loss in batch: 0.444988  [   36/   89]
Per-example loss in batch: 0.366157  [   38/   89]
Per-example loss in batch: 0.340400  [   40/   89]
Per-example loss in batch: 0.407856  [   42/   89]
Per-example loss in batch: 0.363938  [   44/   89]
Per-example loss in batch: 0.356004  [   46/   89]
Per-example loss in batch: 0.397217  [   48/   89]
Per-example loss in batch: 0.411400  [   50/   89]
Per-example loss in batch: 0.389351  [   52/   89]
Per-example loss in batch: 0.452678  [   54/   89]
Per-example loss in batch: 0.411279  [   56/   89]
Per-example loss in batch: 0.394079  [   58/   89]
Per-example loss in batch: 0.399952  [   60/   89]
Per-example loss in batch: 0.345607  [   62/   89]
Per-example loss in batch: 0.410613  [   64/   89]
Per-example loss in batch: 0.379184  [   66/   89]
Per-example loss in batch: 0.437240  [   68/   89]
Per-example loss in batch: 0.383095  [   70/   89]
Per-example loss in batch: 0.442535  [   72/   89]
Per-example loss in batch: 0.356965  [   74/   89]
Per-example loss in batch: 0.440677  [   76/   89]
Per-example loss in batch: 0.360722  [   78/   89]
Per-example loss in batch: 0.364455  [   80/   89]
Per-example loss in batch: 0.431122  [   82/   89]
Per-example loss in batch: 0.396489  [   84/   89]
Per-example loss in batch: 0.375029  [   86/   89]
Per-example loss in batch: 0.426293  [   88/   89]
Per-example loss in batch: 0.850285  [   89/   89]
Train Error: Avg loss: 0.39990101
validation Error: 
 Avg loss: 0.50887720 
 F1: 0.211333 
 Precision: 0.709928 
 Recall: 0.124144
 IoU: 0.118151

test Error: 
 Avg loss: 0.49064405 
 F1: 0.210685 
 Precision: 0.695151 
 Recall: 0.124157
 IoU: 0.117746

We have finished training iteration 80
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_78_.pth
Per-example loss in batch: 0.377407  [    2/   89]
Per-example loss in batch: 0.397050  [    4/   89]
Per-example loss in batch: 0.436609  [    6/   89]
Per-example loss in batch: 0.346080  [    8/   89]
Per-example loss in batch: 0.390009  [   10/   89]
Per-example loss in batch: 0.393779  [   12/   89]
Per-example loss in batch: 0.390641  [   14/   89]
Per-example loss in batch: 0.414918  [   16/   89]
Per-example loss in batch: 0.337895  [   18/   89]
Per-example loss in batch: 0.368244  [   20/   89]
Per-example loss in batch: 0.388783  [   22/   89]
Per-example loss in batch: 0.396993  [   24/   89]
Per-example loss in batch: 0.389305  [   26/   89]
Per-example loss in batch: 0.383884  [   28/   89]
Per-example loss in batch: 0.421816  [   30/   89]
Per-example loss in batch: 0.367233  [   32/   89]
Per-example loss in batch: 0.419570  [   34/   89]
Per-example loss in batch: 0.380357  [   36/   89]
Per-example loss in batch: 0.425841  [   38/   89]
Per-example loss in batch: 0.425306  [   40/   89]
Per-example loss in batch: 0.390922  [   42/   89]
Per-example loss in batch: 0.415098  [   44/   89]
Per-example loss in batch: 0.442566  [   46/   89]
Per-example loss in batch: 0.364978  [   48/   89]
Per-example loss in batch: 0.380940  [   50/   89]
Per-example loss in batch: 0.358191  [   52/   89]
Per-example loss in batch: 0.364388  [   54/   89]
Per-example loss in batch: 0.436258  [   56/   89]
Per-example loss in batch: 0.421246  [   58/   89]
Per-example loss in batch: 0.362037  [   60/   89]
Per-example loss in batch: 0.301219  [   62/   89]
Per-example loss in batch: 0.387944  [   64/   89]
Per-example loss in batch: 0.317791  [   66/   89]
Per-example loss in batch: 0.406854  [   68/   89]
Per-example loss in batch: 0.447472  [   70/   89]
Per-example loss in batch: 0.429282  [   72/   89]
Per-example loss in batch: 0.406807  [   74/   89]
Per-example loss in batch: 0.446716  [   76/   89]
Per-example loss in batch: 0.386833  [   78/   89]
Per-example loss in batch: 0.448387  [   80/   89]
Per-example loss in batch: 0.363664  [   82/   89]
Per-example loss in batch: 0.395951  [   84/   89]
Per-example loss in batch: 0.421196  [   86/   89]
Per-example loss in batch: 0.396049  [   88/   89]
Per-example loss in batch: 0.912378  [   89/   89]
Train Error: Avg loss: 0.40001563
validation Error: 
 Avg loss: 0.51033892 
 F1: 0.216682 
 Precision: 0.683130 
 Recall: 0.128762
 IoU: 0.121505

test Error: 
 Avg loss: 0.49109876 
 F1: 0.216783 
 Precision: 0.674633 
 Recall: 0.129140
 IoU: 0.121568

We have finished training iteration 81
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_79_.pth
Per-example loss in batch: 0.417730  [    2/   89]
Per-example loss in batch: 0.424137  [    4/   89]
Per-example loss in batch: 0.402565  [    6/   89]
Per-example loss in batch: 0.345838  [    8/   89]
Per-example loss in batch: 0.368574  [   10/   89]
Per-example loss in batch: 0.344206  [   12/   89]
Per-example loss in batch: 0.386737  [   14/   89]
Per-example loss in batch: 0.369313  [   16/   89]
Per-example loss in batch: 0.350079  [   18/   89]
Per-example loss in batch: 0.404597  [   20/   89]
Per-example loss in batch: 0.419735  [   22/   89]
Per-example loss in batch: 0.390181  [   24/   89]
Per-example loss in batch: 0.392096  [   26/   89]
Per-example loss in batch: 0.377207  [   28/   89]
Per-example loss in batch: 0.427333  [   30/   89]
Per-example loss in batch: 0.392831  [   32/   89]
Per-example loss in batch: 0.360250  [   34/   89]
Per-example loss in batch: 0.404178  [   36/   89]
Per-example loss in batch: 0.432774  [   38/   89]
Per-example loss in batch: 0.393786  [   40/   89]
Per-example loss in batch: 0.352775  [   42/   89]
Per-example loss in batch: 0.383503  [   44/   89]
Per-example loss in batch: 0.408922  [   46/   89]
Per-example loss in batch: 0.343551  [   48/   89]
Per-example loss in batch: 0.387312  [   50/   89]
Per-example loss in batch: 0.420308  [   52/   89]
Per-example loss in batch: 0.365449  [   54/   89]
Per-example loss in batch: 0.396849  [   56/   89]
Per-example loss in batch: 0.406149  [   58/   89]
Per-example loss in batch: 0.348214  [   60/   89]
Per-example loss in batch: 0.425885  [   62/   89]
Per-example loss in batch: 0.387630  [   64/   89]
Per-example loss in batch: 0.425866  [   66/   89]
Per-example loss in batch: 0.440225  [   68/   89]
Per-example loss in batch: 0.364206  [   70/   89]
Per-example loss in batch: 0.356572  [   72/   89]
Per-example loss in batch: 0.456163  [   74/   89]
Per-example loss in batch: 0.374660  [   76/   89]
Per-example loss in batch: 0.409224  [   78/   89]
Per-example loss in batch: 0.406505  [   80/   89]
Per-example loss in batch: 0.438218  [   82/   89]
Per-example loss in batch: 0.421974  [   84/   89]
Per-example loss in batch: 0.380717  [   86/   89]
Per-example loss in batch: 0.276838  [   88/   89]
Per-example loss in batch: 0.863936  [   89/   89]
Train Error: Avg loss: 0.39581639
validation Error: 
 Avg loss: 0.51119713 
 F1: 0.221495 
 Precision: 0.565373 
 Recall: 0.137725
 IoU: 0.124540

test Error: 
 Avg loss: 0.49218975 
 F1: 0.224181 
 Precision: 0.552358 
 Recall: 0.140628
 IoU: 0.126241

We have finished training iteration 82
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_80_.pth
Per-example loss in batch: 0.408864  [    2/   89]
Per-example loss in batch: 0.382786  [    4/   89]
Per-example loss in batch: 0.349583  [    6/   89]
Per-example loss in batch: 0.363405  [    8/   89]
Per-example loss in batch: 0.407419  [   10/   89]
Per-example loss in batch: 0.401095  [   12/   89]
Per-example loss in batch: 0.445212  [   14/   89]
Per-example loss in batch: 0.347145  [   16/   89]
Per-example loss in batch: 0.385941  [   18/   89]
Per-example loss in batch: 0.439262  [   20/   89]
Per-example loss in batch: 0.373587  [   22/   89]
Per-example loss in batch: 0.410716  [   24/   89]
Per-example loss in batch: 0.411353  [   26/   89]
Per-example loss in batch: 0.419811  [   28/   89]
Per-example loss in batch: 0.449860  [   30/   89]
Per-example loss in batch: 0.428167  [   32/   89]
Per-example loss in batch: 0.372919  [   34/   89]
Per-example loss in batch: 0.320609  [   36/   89]
Per-example loss in batch: 0.425505  [   38/   89]
Per-example loss in batch: 0.416211  [   40/   89]
Per-example loss in batch: 0.373452  [   42/   89]
Per-example loss in batch: 0.322643  [   44/   89]
Per-example loss in batch: 0.396386  [   46/   89]
Per-example loss in batch: 0.348315  [   48/   89]
Per-example loss in batch: 0.381069  [   50/   89]
Per-example loss in batch: 0.415987  [   52/   89]
Per-example loss in batch: 0.427938  [   54/   89]
Per-example loss in batch: 0.363562  [   56/   89]
Per-example loss in batch: 0.387869  [   58/   89]
Per-example loss in batch: 0.342147  [   60/   89]
Per-example loss in batch: 0.369180  [   62/   89]
Per-example loss in batch: 0.387415  [   64/   89]
Per-example loss in batch: 0.435898  [   66/   89]
Per-example loss in batch: 0.395691  [   68/   89]
Per-example loss in batch: 0.342877  [   70/   89]
Per-example loss in batch: 0.400056  [   72/   89]
Per-example loss in batch: 0.404645  [   74/   89]
Per-example loss in batch: 0.399247  [   76/   89]
Per-example loss in batch: 0.385802  [   78/   89]
Per-example loss in batch: 0.393057  [   80/   89]
Per-example loss in batch: 0.366075  [   82/   89]
Per-example loss in batch: 0.353295  [   84/   89]
Per-example loss in batch: 0.392825  [   86/   89]
Per-example loss in batch: 0.363511  [   88/   89]
Per-example loss in batch: 0.915791  [   89/   89]
Train Error: Avg loss: 0.39474800
validation Error: 
 Avg loss: 0.50961366 
 F1: 0.225293 
 Precision: 0.717977 
 Recall: 0.133609
 IoU: 0.126947

test Error: 
 Avg loss: 0.49042438 
 F1: 0.228868 
 Precision: 0.722293 
 Recall: 0.135977
 IoU: 0.129221

We have finished training iteration 83
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_81_.pth
Per-example loss in batch: 0.334761  [    2/   89]
Per-example loss in batch: 0.384362  [    4/   89]
Per-example loss in batch: 0.411948  [    6/   89]
Per-example loss in batch: 0.444444  [    8/   89]
Per-example loss in batch: 0.346989  [   10/   89]
Per-example loss in batch: 0.408689  [   12/   89]
Per-example loss in batch: 0.408843  [   14/   89]
Per-example loss in batch: 0.465125  [   16/   89]
Per-example loss in batch: 0.360709  [   18/   89]
Per-example loss in batch: 0.388044  [   20/   89]
Per-example loss in batch: 0.386558  [   22/   89]
Per-example loss in batch: 0.329786  [   24/   89]
Per-example loss in batch: 0.426553  [   26/   89]
Per-example loss in batch: 0.384117  [   28/   89]
Per-example loss in batch: 0.365905  [   30/   89]
Per-example loss in batch: 0.333867  [   32/   89]
Per-example loss in batch: 0.375492  [   34/   89]
Per-example loss in batch: 0.379847  [   36/   89]
Per-example loss in batch: 0.354222  [   38/   89]
Per-example loss in batch: 0.382138  [   40/   89]
Per-example loss in batch: 0.412239  [   42/   89]
Per-example loss in batch: 0.400086  [   44/   89]
Per-example loss in batch: 0.396771  [   46/   89]
Per-example loss in batch: 0.427137  [   48/   89]
Per-example loss in batch: 0.410017  [   50/   89]
Per-example loss in batch: 0.408690  [   52/   89]
Per-example loss in batch: 0.331078  [   54/   89]
Per-example loss in batch: 0.339666  [   56/   89]
Per-example loss in batch: 0.359892  [   58/   89]
Per-example loss in batch: 0.399810  [   60/   89]
Per-example loss in batch: 0.417764  [   62/   89]
Per-example loss in batch: 0.428947  [   64/   89]
Per-example loss in batch: 0.383020  [   66/   89]
Per-example loss in batch: 0.436121  [   68/   89]
Per-example loss in batch: 0.389784  [   70/   89]
Per-example loss in batch: 0.405399  [   72/   89]
Per-example loss in batch: 0.375299  [   74/   89]
Per-example loss in batch: 0.377597  [   76/   89]
Per-example loss in batch: 0.383885  [   78/   89]
Per-example loss in batch: 0.404601  [   80/   89]
Per-example loss in batch: 0.416100  [   82/   89]
Per-example loss in batch: 0.442890  [   84/   89]
Per-example loss in batch: 0.367049  [   86/   89]
Per-example loss in batch: 0.345428  [   88/   89]
Per-example loss in batch: 0.751565  [   89/   89]
Train Error: Avg loss: 0.39342583
validation Error: 
 Avg loss: 0.51044685 
 F1: 0.226982 
 Precision: 0.663687 
 Recall: 0.136901
 IoU: 0.128020

test Error: 
 Avg loss: 0.49125596 
 F1: 0.233880 
 Precision: 0.661276 
 Recall: 0.142062
 IoU: 0.132426

We have finished training iteration 84
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_82_.pth
Per-example loss in batch: 0.410055  [    2/   89]
Per-example loss in batch: 0.353478  [    4/   89]
Per-example loss in batch: 0.401637  [    6/   89]
Per-example loss in batch: 0.412610  [    8/   89]
Per-example loss in batch: 0.431498  [   10/   89]
Per-example loss in batch: 0.269843  [   12/   89]
Per-example loss in batch: 0.398083  [   14/   89]
Per-example loss in batch: 0.347086  [   16/   89]
Per-example loss in batch: 0.375129  [   18/   89]
Per-example loss in batch: 0.374769  [   20/   89]
Per-example loss in batch: 0.368564  [   22/   89]
Per-example loss in batch: 0.402104  [   24/   89]
Per-example loss in batch: 0.391703  [   26/   89]
Per-example loss in batch: 0.384308  [   28/   89]
Per-example loss in batch: 0.436571  [   30/   89]
Per-example loss in batch: 0.400524  [   32/   89]
Per-example loss in batch: 0.342629  [   34/   89]
Per-example loss in batch: 0.385838  [   36/   89]
Per-example loss in batch: 0.398888  [   38/   89]
Per-example loss in batch: 0.413701  [   40/   89]
Per-example loss in batch: 0.387951  [   42/   89]
Per-example loss in batch: 0.412566  [   44/   89]
Per-example loss in batch: 0.357650  [   46/   89]
Per-example loss in batch: 0.366906  [   48/   89]
Per-example loss in batch: 0.339587  [   50/   89]
Per-example loss in batch: 0.336382  [   52/   89]
Per-example loss in batch: 0.341223  [   54/   89]
Per-example loss in batch: 0.342602  [   56/   89]
Per-example loss in batch: 0.391299  [   58/   89]
Per-example loss in batch: 0.343493  [   60/   89]
Per-example loss in batch: 0.395895  [   62/   89]
Per-example loss in batch: 0.441816  [   64/   89]
Per-example loss in batch: 0.349659  [   66/   89]
Per-example loss in batch: 0.370370  [   68/   89]
Per-example loss in batch: 0.402488  [   70/   89]
Per-example loss in batch: 0.360415  [   72/   89]
Per-example loss in batch: 0.423455  [   74/   89]
Per-example loss in batch: 0.413054  [   76/   89]
Per-example loss in batch: 0.365833  [   78/   89]
Per-example loss in batch: 0.441409  [   80/   89]
Per-example loss in batch: 0.442796  [   82/   89]
Per-example loss in batch: 0.418878  [   84/   89]
Per-example loss in batch: 0.401629  [   86/   89]
Per-example loss in batch: 0.393281  [   88/   89]
Per-example loss in batch: 0.671985  [   89/   89]
Train Error: Avg loss: 0.38821675
validation Error: 
 Avg loss: 0.50926792 
 F1: 0.229305 
 Precision: 0.707266 
 Recall: 0.136834
 IoU: 0.129500

test Error: 
 Avg loss: 0.49056069 
 F1: 0.237037 
 Precision: 0.715669 
 Recall: 0.142041
 IoU: 0.134454

We have finished training iteration 85
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_83_.pth
Per-example loss in batch: 0.392834  [    2/   89]
Per-example loss in batch: 0.362545  [    4/   89]
Per-example loss in batch: 0.417881  [    6/   89]
Per-example loss in batch: 0.430844  [    8/   89]
Per-example loss in batch: 0.345866  [   10/   89]
Per-example loss in batch: 0.356606  [   12/   89]
Per-example loss in batch: 0.377344  [   14/   89]
Per-example loss in batch: 0.363021  [   16/   89]
Per-example loss in batch: 0.419801  [   18/   89]
Per-example loss in batch: 0.373616  [   20/   89]
Per-example loss in batch: 0.322497  [   22/   89]
Per-example loss in batch: 0.390624  [   24/   89]
Per-example loss in batch: 0.441312  [   26/   89]
Per-example loss in batch: 0.387850  [   28/   89]
Per-example loss in batch: 0.362202  [   30/   89]
Per-example loss in batch: 0.448193  [   32/   89]
Per-example loss in batch: 0.349599  [   34/   89]
Per-example loss in batch: 0.366263  [   36/   89]
Per-example loss in batch: 0.375210  [   38/   89]
Per-example loss in batch: 0.315876  [   40/   89]
Per-example loss in batch: 0.382164  [   42/   89]
Per-example loss in batch: 0.382901  [   44/   89]
Per-example loss in batch: 0.373040  [   46/   89]
Per-example loss in batch: 0.407780  [   48/   89]
Per-example loss in batch: 0.425950  [   50/   89]
Per-example loss in batch: 0.339490  [   52/   89]
Per-example loss in batch: 0.419009  [   54/   89]
Per-example loss in batch: 0.386134  [   56/   89]
Per-example loss in batch: 0.350882  [   58/   89]
Per-example loss in batch: 0.412261  [   60/   89]
Per-example loss in batch: 0.430011  [   62/   89]
Per-example loss in batch: 0.404165  [   64/   89]
Per-example loss in batch: 0.427687  [   66/   89]
Per-example loss in batch: 0.344368  [   68/   89]
Per-example loss in batch: 0.384086  [   70/   89]
Per-example loss in batch: 0.379791  [   72/   89]
Per-example loss in batch: 0.440676  [   74/   89]
Per-example loss in batch: 0.438791  [   76/   89]
Per-example loss in batch: 0.351838  [   78/   89]
Per-example loss in batch: 0.345125  [   80/   89]
Per-example loss in batch: 0.420889  [   82/   89]
Per-example loss in batch: 0.335735  [   84/   89]
Per-example loss in batch: 0.417725  [   86/   89]
Per-example loss in batch: 0.360138  [   88/   89]
Per-example loss in batch: 0.859586  [   89/   89]
Train Error: Avg loss: 0.39079574
validation Error: 
 Avg loss: 0.51051320 
 F1: 0.236461 
 Precision: 0.657813 
 Recall: 0.144137
 IoU: 0.134083

test Error: 
 Avg loss: 0.49118476 
 F1: 0.248719 
 Precision: 0.663466 
 Recall: 0.153046
 IoU: 0.142021

We have finished training iteration 86
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_84_.pth
Per-example loss in batch: 0.384817  [    2/   89]
Per-example loss in batch: 0.402655  [    4/   89]
Per-example loss in batch: 0.373057  [    6/   89]
Per-example loss in batch: 0.421551  [    8/   89]
Per-example loss in batch: 0.400782  [   10/   89]
Per-example loss in batch: 0.357729  [   12/   89]
Per-example loss in batch: 0.387752  [   14/   89]
Per-example loss in batch: 0.377237  [   16/   89]
Per-example loss in batch: 0.408573  [   18/   89]
Per-example loss in batch: 0.378269  [   20/   89]
Per-example loss in batch: 0.431455  [   22/   89]
Per-example loss in batch: 0.339802  [   24/   89]
Per-example loss in batch: 0.298348  [   26/   89]
Per-example loss in batch: 0.408249  [   28/   89]
Per-example loss in batch: 0.336555  [   30/   89]
Per-example loss in batch: 0.392399  [   32/   89]
Per-example loss in batch: 0.391057  [   34/   89]
Per-example loss in batch: 0.424873  [   36/   89]
Per-example loss in batch: 0.366319  [   38/   89]
Per-example loss in batch: 0.452082  [   40/   89]
Per-example loss in batch: 0.353848  [   42/   89]
Per-example loss in batch: 0.399442  [   44/   89]
Per-example loss in batch: 0.331430  [   46/   89]
Per-example loss in batch: 0.353984  [   48/   89]
Per-example loss in batch: 0.363921  [   50/   89]
Per-example loss in batch: 0.412454  [   52/   89]
Per-example loss in batch: 0.400809  [   54/   89]
Per-example loss in batch: 0.379701  [   56/   89]
Per-example loss in batch: 0.411769  [   58/   89]
Per-example loss in batch: 0.420126  [   60/   89]
Per-example loss in batch: 0.380605  [   62/   89]
Per-example loss in batch: 0.438016  [   64/   89]
Per-example loss in batch: 0.436237  [   66/   89]
Per-example loss in batch: 0.403712  [   68/   89]
Per-example loss in batch: 0.390354  [   70/   89]
Per-example loss in batch: 0.407505  [   72/   89]
Per-example loss in batch: 0.373696  [   74/   89]
Per-example loss in batch: 0.336181  [   76/   89]
Per-example loss in batch: 0.396335  [   78/   89]
Per-example loss in batch: 0.447340  [   80/   89]
Per-example loss in batch: 0.357795  [   82/   89]
Per-example loss in batch: 0.293430  [   84/   89]
Per-example loss in batch: 0.373111  [   86/   89]
Per-example loss in batch: 0.436544  [   88/   89]
Per-example loss in batch: 0.798548  [   89/   89]
Train Error: Avg loss: 0.39171192
validation Error: 
 Avg loss: 0.51036904 
 F1: 0.234057 
 Precision: 0.642326 
 Recall: 0.143101
 IoU: 0.132540

test Error: 
 Avg loss: 0.49122793 
 F1: 0.245103 
 Precision: 0.641715 
 Recall: 0.151480
 IoU: 0.139668

We have finished training iteration 87
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_85_.pth
Per-example loss in batch: 0.405653  [    2/   89]
Per-example loss in batch: 0.437489  [    4/   89]
Per-example loss in batch: 0.432155  [    6/   89]
Per-example loss in batch: 0.427793  [    8/   89]
Per-example loss in batch: 0.430047  [   10/   89]
Per-example loss in batch: 0.361675  [   12/   89]
Per-example loss in batch: 0.374810  [   14/   89]
Per-example loss in batch: 0.352794  [   16/   89]
Per-example loss in batch: 0.331372  [   18/   89]
Per-example loss in batch: 0.307197  [   20/   89]
Per-example loss in batch: 0.363656  [   22/   89]
Per-example loss in batch: 0.392684  [   24/   89]
Per-example loss in batch: 0.352613  [   26/   89]
Per-example loss in batch: 0.382677  [   28/   89]
Per-example loss in batch: 0.410655  [   30/   89]
Per-example loss in batch: 0.331761  [   32/   89]
Per-example loss in batch: 0.383085  [   34/   89]
Per-example loss in batch: 0.407010  [   36/   89]
Per-example loss in batch: 0.413871  [   38/   89]
Per-example loss in batch: 0.463620  [   40/   89]
Per-example loss in batch: 0.374814  [   42/   89]
Per-example loss in batch: 0.437078  [   44/   89]
Per-example loss in batch: 0.344191  [   46/   89]
Per-example loss in batch: 0.355729  [   48/   89]
Per-example loss in batch: 0.397226  [   50/   89]
Per-example loss in batch: 0.363635  [   52/   89]
Per-example loss in batch: 0.423322  [   54/   89]
Per-example loss in batch: 0.448480  [   56/   89]
Per-example loss in batch: 0.415388  [   58/   89]
Per-example loss in batch: 0.403744  [   60/   89]
Per-example loss in batch: 0.394740  [   62/   89]
Per-example loss in batch: 0.377195  [   64/   89]
Per-example loss in batch: 0.377073  [   66/   89]
Per-example loss in batch: 0.376134  [   68/   89]
Per-example loss in batch: 0.283927  [   70/   89]
Per-example loss in batch: 0.414607  [   72/   89]
Per-example loss in batch: 0.330109  [   74/   89]
Per-example loss in batch: 0.405006  [   76/   89]
Per-example loss in batch: 0.378132  [   78/   89]
Per-example loss in batch: 0.387892  [   80/   89]
Per-example loss in batch: 0.417454  [   82/   89]
Per-example loss in batch: 0.379075  [   84/   89]
Per-example loss in batch: 0.372073  [   86/   89]
Per-example loss in batch: 0.383576  [   88/   89]
Per-example loss in batch: 0.696515  [   89/   89]
Train Error: Avg loss: 0.38992081
validation Error: 
 Avg loss: 0.51036926 
 F1: 0.233771 
 Precision: 0.643370 
 Recall: 0.142835
 IoU: 0.132356

test Error: 
 Avg loss: 0.49119076 
 F1: 0.244529 
 Precision: 0.645488 
 Recall: 0.150835
 IoU: 0.139296

We have finished training iteration 88
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_86_.pth
Per-example loss in batch: 0.361295  [    2/   89]
Per-example loss in batch: 0.336333  [    4/   89]
Per-example loss in batch: 0.418376  [    6/   89]
Per-example loss in batch: 0.425119  [    8/   89]
Per-example loss in batch: 0.380396  [   10/   89]
Per-example loss in batch: 0.317018  [   12/   89]
Per-example loss in batch: 0.417818  [   14/   89]
Per-example loss in batch: 0.457444  [   16/   89]
Per-example loss in batch: 0.407830  [   18/   89]
Per-example loss in batch: 0.404892  [   20/   89]
Per-example loss in batch: 0.395612  [   22/   89]
Per-example loss in batch: 0.386443  [   24/   89]
Per-example loss in batch: 0.422426  [   26/   89]
Per-example loss in batch: 0.333177  [   28/   89]
Per-example loss in batch: 0.388008  [   30/   89]
Per-example loss in batch: 0.357845  [   32/   89]
Per-example loss in batch: 0.371430  [   34/   89]
Per-example loss in batch: 0.403768  [   36/   89]
Per-example loss in batch: 0.380992  [   38/   89]
Per-example loss in batch: 0.415328  [   40/   89]
Per-example loss in batch: 0.436596  [   42/   89]
Per-example loss in batch: 0.359070  [   44/   89]
Per-example loss in batch: 0.431339  [   46/   89]
Per-example loss in batch: 0.404029  [   48/   89]
Per-example loss in batch: 0.404541  [   50/   89]
Per-example loss in batch: 0.417443  [   52/   89]
Per-example loss in batch: 0.396951  [   54/   89]
Per-example loss in batch: 0.427854  [   56/   89]
Per-example loss in batch: 0.346462  [   58/   89]
Per-example loss in batch: 0.374556  [   60/   89]
Per-example loss in batch: 0.348932  [   62/   89]
Per-example loss in batch: 0.371858  [   64/   89]
Per-example loss in batch: 0.414221  [   66/   89]
Per-example loss in batch: 0.407933  [   68/   89]
Per-example loss in batch: 0.362921  [   70/   89]
Per-example loss in batch: 0.422870  [   72/   89]
Per-example loss in batch: 0.333202  [   74/   89]
Per-example loss in batch: 0.358413  [   76/   89]
Per-example loss in batch: 0.390498  [   78/   89]
Per-example loss in batch: 0.291070  [   80/   89]
Per-example loss in batch: 0.358377  [   82/   89]
Per-example loss in batch: 0.379063  [   84/   89]
Per-example loss in batch: 0.438737  [   86/   89]
Per-example loss in batch: 0.412571  [   88/   89]
Per-example loss in batch: 0.801722  [   89/   89]
Train Error: Avg loss: 0.39262737
validation Error: 
 Avg loss: 0.50990114 
 F1: 0.236568 
 Precision: 0.693472 
 Recall: 0.142608
 IoU: 0.134152

test Error: 
 Avg loss: 0.49059553 
 F1: 0.247134 
 Precision: 0.701683 
 Recall: 0.149978
 IoU: 0.140988

We have finished training iteration 89
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_87_.pth
Per-example loss in batch: 0.290783  [    2/   89]
Per-example loss in batch: 0.321767  [    4/   89]
Per-example loss in batch: 0.358444  [    6/   89]
Per-example loss in batch: 0.434478  [    8/   89]
Per-example loss in batch: 0.396479  [   10/   89]
Per-example loss in batch: 0.367043  [   12/   89]
Per-example loss in batch: 0.326027  [   14/   89]
Per-example loss in batch: 0.391380  [   16/   89]
Per-example loss in batch: 0.367514  [   18/   89]
Per-example loss in batch: 0.353970  [   20/   89]
Per-example loss in batch: 0.408756  [   22/   89]
Per-example loss in batch: 0.434612  [   24/   89]
Per-example loss in batch: 0.443246  [   26/   89]
Per-example loss in batch: 0.443206  [   28/   89]
Per-example loss in batch: 0.392964  [   30/   89]
Per-example loss in batch: 0.375262  [   32/   89]
Per-example loss in batch: 0.336003  [   34/   89]
Per-example loss in batch: 0.369593  [   36/   89]
Per-example loss in batch: 0.394676  [   38/   89]
Per-example loss in batch: 0.383209  [   40/   89]
Per-example loss in batch: 0.427853  [   42/   89]
Per-example loss in batch: 0.365842  [   44/   89]
Per-example loss in batch: 0.344998  [   46/   89]
Per-example loss in batch: 0.383868  [   48/   89]
Per-example loss in batch: 0.394294  [   50/   89]
Per-example loss in batch: 0.406062  [   52/   89]
Per-example loss in batch: 0.417817  [   54/   89]
Per-example loss in batch: 0.319325  [   56/   89]
Per-example loss in batch: 0.397036  [   58/   89]
Per-example loss in batch: 0.380450  [   60/   89]
Per-example loss in batch: 0.352783  [   62/   89]
Per-example loss in batch: 0.369740  [   64/   89]
Per-example loss in batch: 0.396188  [   66/   89]
Per-example loss in batch: 0.341382  [   68/   89]
Per-example loss in batch: 0.397839  [   70/   89]
Per-example loss in batch: 0.365166  [   72/   89]
Per-example loss in batch: 0.420507  [   74/   89]
Per-example loss in batch: 0.420902  [   76/   89]
Per-example loss in batch: 0.387111  [   78/   89]
Per-example loss in batch: 0.387414  [   80/   89]
Per-example loss in batch: 0.348540  [   82/   89]
Per-example loss in batch: 0.452814  [   84/   89]
Per-example loss in batch: 0.382888  [   86/   89]
Per-example loss in batch: 0.425586  [   88/   89]
Per-example loss in batch: 0.856942  [   89/   89]
Train Error: Avg loss: 0.38886043
validation Error: 
 Avg loss: 0.51041406 
 F1: 0.239518 
 Precision: 0.641541 
 Recall: 0.147246
 IoU: 0.136052

test Error: 
 Avg loss: 0.49122796 
 F1: 0.253068 
 Precision: 0.646822 
 Recall: 0.157307
 IoU: 0.144864

We have finished training iteration 90
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_88_.pth
Per-example loss in batch: 0.405949  [    2/   89]
Per-example loss in batch: 0.469789  [    4/   89]
Per-example loss in batch: 0.405179  [    6/   89]
Per-example loss in batch: 0.345776  [    8/   89]
Per-example loss in batch: 0.326915  [   10/   89]
Per-example loss in batch: 0.449731  [   12/   89]
Per-example loss in batch: 0.354486  [   14/   89]
Per-example loss in batch: 0.436057  [   16/   89]
Per-example loss in batch: 0.409639  [   18/   89]
Per-example loss in batch: 0.427913  [   20/   89]
Per-example loss in batch: 0.346882  [   22/   89]
Per-example loss in batch: 0.432891  [   24/   89]
Per-example loss in batch: 0.393161  [   26/   89]
Per-example loss in batch: 0.367765  [   28/   89]
Per-example loss in batch: 0.440297  [   30/   89]
Per-example loss in batch: 0.382687  [   32/   89]
Per-example loss in batch: 0.343787  [   34/   89]
Per-example loss in batch: 0.361820  [   36/   89]
Per-example loss in batch: 0.312780  [   38/   89]
Per-example loss in batch: 0.408940  [   40/   89]
Per-example loss in batch: 0.372222  [   42/   89]
Per-example loss in batch: 0.390703  [   44/   89]
Per-example loss in batch: 0.378296  [   46/   89]
Per-example loss in batch: 0.430066  [   48/   89]
Per-example loss in batch: 0.387087  [   50/   89]
Per-example loss in batch: 0.383632  [   52/   89]
Per-example loss in batch: 0.371895  [   54/   89]
Per-example loss in batch: 0.424046  [   56/   89]
Per-example loss in batch: 0.331430  [   58/   89]
Per-example loss in batch: 0.362548  [   60/   89]
Per-example loss in batch: 0.301617  [   62/   89]
Per-example loss in batch: 0.431300  [   64/   89]
Per-example loss in batch: 0.331918  [   66/   89]
Per-example loss in batch: 0.357775  [   68/   89]
Per-example loss in batch: 0.385711  [   70/   89]
Per-example loss in batch: 0.377255  [   72/   89]
Per-example loss in batch: 0.400260  [   74/   89]
Per-example loss in batch: 0.418693  [   76/   89]
Per-example loss in batch: 0.410848  [   78/   89]
Per-example loss in batch: 0.448851  [   80/   89]
Per-example loss in batch: 0.336970  [   82/   89]
Per-example loss in batch: 0.355079  [   84/   89]
Per-example loss in batch: 0.352056  [   86/   89]
Per-example loss in batch: 0.370638  [   88/   89]
Per-example loss in batch: 0.693855  [   89/   89]
Train Error: Avg loss: 0.38832065
validation Error: 
 Avg loss: 0.50901171 
 F1: 0.235302 
 Precision: 0.721319 
 Recall: 0.140580
 IoU: 0.133338

test Error: 
 Avg loss: 0.49026642 
 F1: 0.247041 
 Precision: 0.739969 
 Recall: 0.148271
 IoU: 0.140928

We have finished training iteration 91
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_89_.pth
Per-example loss in batch: 0.379128  [    2/   89]
Per-example loss in batch: 0.418607  [    4/   89]
Per-example loss in batch: 0.408151  [    6/   89]
Per-example loss in batch: 0.416845  [    8/   89]
Per-example loss in batch: 0.410858  [   10/   89]
Per-example loss in batch: 0.373990  [   12/   89]
Per-example loss in batch: 0.384285  [   14/   89]
Per-example loss in batch: 0.373894  [   16/   89]
Per-example loss in batch: 0.405774  [   18/   89]
Per-example loss in batch: 0.323541  [   20/   89]
Per-example loss in batch: 0.422526  [   22/   89]
Per-example loss in batch: 0.452256  [   24/   89]
Per-example loss in batch: 0.363230  [   26/   89]
Per-example loss in batch: 0.402216  [   28/   89]
Per-example loss in batch: 0.405058  [   30/   89]
Per-example loss in batch: 0.350865  [   32/   89]
Per-example loss in batch: 0.383508  [   34/   89]
Per-example loss in batch: 0.327109  [   36/   89]
Per-example loss in batch: 0.369972  [   38/   89]
Per-example loss in batch: 0.365353  [   40/   89]
Per-example loss in batch: 0.360627  [   42/   89]
Per-example loss in batch: 0.450272  [   44/   89]
Per-example loss in batch: 0.341153  [   46/   89]
Per-example loss in batch: 0.317162  [   48/   89]
Per-example loss in batch: 0.375648  [   50/   89]
Per-example loss in batch: 0.343742  [   52/   89]
Per-example loss in batch: 0.352436  [   54/   89]
Per-example loss in batch: 0.428002  [   56/   89]
Per-example loss in batch: 0.444289  [   58/   89]
Per-example loss in batch: 0.395204  [   60/   89]
Per-example loss in batch: 0.374955  [   62/   89]
Per-example loss in batch: 0.383663  [   64/   89]
Per-example loss in batch: 0.372961  [   66/   89]
Per-example loss in batch: 0.392551  [   68/   89]
Per-example loss in batch: 0.447053  [   70/   89]
Per-example loss in batch: 0.404041  [   72/   89]
Per-example loss in batch: 0.426713  [   74/   89]
Per-example loss in batch: 0.441383  [   76/   89]
Per-example loss in batch: 0.329333  [   78/   89]
Per-example loss in batch: 0.344382  [   80/   89]
Per-example loss in batch: 0.398312  [   82/   89]
Per-example loss in batch: 0.337717  [   84/   89]
Per-example loss in batch: 0.315786  [   86/   89]
Per-example loss in batch: 0.355603  [   88/   89]
Per-example loss in batch: 0.830815  [   89/   89]
Train Error: Avg loss: 0.38843965
validation Error: 
 Avg loss: 0.51025811 
 F1: 0.240710 
 Precision: 0.640490 
 Recall: 0.148204
 IoU: 0.136822

test Error: 
 Avg loss: 0.49096242 
 F1: 0.254806 
 Precision: 0.646966 
 Recall: 0.158644
 IoU: 0.146004

We have finished training iteration 92
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_90_.pth
Per-example loss in batch: 0.329949  [    2/   89]
Per-example loss in batch: 0.392853  [    4/   89]
Per-example loss in batch: 0.393233  [    6/   89]
Per-example loss in batch: 0.400229  [    8/   89]
Per-example loss in batch: 0.362859  [   10/   89]
Per-example loss in batch: 0.396863  [   12/   89]
Per-example loss in batch: 0.409903  [   14/   89]
Per-example loss in batch: 0.346136  [   16/   89]
Per-example loss in batch: 0.373704  [   18/   89]
Per-example loss in batch: 0.368731  [   20/   89]
Per-example loss in batch: 0.428157  [   22/   89]
Per-example loss in batch: 0.381429  [   24/   89]
Per-example loss in batch: 0.398124  [   26/   89]
Per-example loss in batch: 0.443131  [   28/   89]
Per-example loss in batch: 0.373436  [   30/   89]
Per-example loss in batch: 0.398949  [   32/   89]
Per-example loss in batch: 0.406060  [   34/   89]
Per-example loss in batch: 0.371216  [   36/   89]
Per-example loss in batch: 0.381854  [   38/   89]
Per-example loss in batch: 0.388881  [   40/   89]
Per-example loss in batch: 0.392659  [   42/   89]
Per-example loss in batch: 0.369074  [   44/   89]
Per-example loss in batch: 0.341003  [   46/   89]
Per-example loss in batch: 0.462582  [   48/   89]
Per-example loss in batch: 0.418354  [   50/   89]
Per-example loss in batch: 0.371585  [   52/   89]
Per-example loss in batch: 0.337079  [   54/   89]
Per-example loss in batch: 0.379224  [   56/   89]
Per-example loss in batch: 0.322626  [   58/   89]
Per-example loss in batch: 0.409020  [   60/   89]
Per-example loss in batch: 0.324172  [   62/   89]
Per-example loss in batch: 0.366117  [   64/   89]
Per-example loss in batch: 0.288450  [   66/   89]
Per-example loss in batch: 0.403035  [   68/   89]
Per-example loss in batch: 0.437406  [   70/   89]
Per-example loss in batch: 0.387544  [   72/   89]
Per-example loss in batch: 0.381193  [   74/   89]
Per-example loss in batch: 0.428693  [   76/   89]
Per-example loss in batch: 0.322731  [   78/   89]
Per-example loss in batch: 0.412377  [   80/   89]
Per-example loss in batch: 0.381517  [   82/   89]
Per-example loss in batch: 0.431650  [   84/   89]
Per-example loss in batch: 0.348624  [   86/   89]
Per-example loss in batch: 0.362946  [   88/   89]
Per-example loss in batch: 0.815886  [   89/   89]
Train Error: Avg loss: 0.38726520
validation Error: 
 Avg loss: 0.51106545 
 F1: 0.244771 
 Precision: 0.586225 
 Recall: 0.154678
 IoU: 0.139453

test Error: 
 Avg loss: 0.49158445 
 F1: 0.259871 
 Precision: 0.588231 
 Recall: 0.166774
 IoU: 0.149340

We have finished training iteration 93
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_91_.pth
Per-example loss in batch: 0.375436  [    2/   89]
Per-example loss in batch: 0.408428  [    4/   89]
Per-example loss in batch: 0.375320  [    6/   89]
Per-example loss in batch: 0.362980  [    8/   89]
Per-example loss in batch: 0.344853  [   10/   89]
Per-example loss in batch: 0.324056  [   12/   89]
Per-example loss in batch: 0.430709  [   14/   89]
Per-example loss in batch: 0.320427  [   16/   89]
Per-example loss in batch: 0.423358  [   18/   89]
Per-example loss in batch: 0.422204  [   20/   89]
Per-example loss in batch: 0.387598  [   22/   89]
Per-example loss in batch: 0.401967  [   24/   89]
Per-example loss in batch: 0.441179  [   26/   89]
Per-example loss in batch: 0.398304  [   28/   89]
Per-example loss in batch: 0.418520  [   30/   89]
Per-example loss in batch: 0.380551  [   32/   89]
Per-example loss in batch: 0.309603  [   34/   89]
Per-example loss in batch: 0.375054  [   36/   89]
Per-example loss in batch: 0.428742  [   38/   89]
Per-example loss in batch: 0.393219  [   40/   89]
Per-example loss in batch: 0.412149  [   42/   89]
Per-example loss in batch: 0.299613  [   44/   89]
Per-example loss in batch: 0.342148  [   46/   89]
Per-example loss in batch: 0.407432  [   48/   89]
Per-example loss in batch: 0.402719  [   50/   89]
Per-example loss in batch: 0.430905  [   52/   89]
Per-example loss in batch: 0.415875  [   54/   89]
Per-example loss in batch: 0.349318  [   56/   89]
Per-example loss in batch: 0.386398  [   58/   89]
Per-example loss in batch: 0.332218  [   60/   89]
Per-example loss in batch: 0.331675  [   62/   89]
Per-example loss in batch: 0.362131  [   64/   89]
Per-example loss in batch: 0.324635  [   66/   89]
Per-example loss in batch: 0.358376  [   68/   89]
Per-example loss in batch: 0.423580  [   70/   89]
Per-example loss in batch: 0.388033  [   72/   89]
Per-example loss in batch: 0.374598  [   74/   89]
Per-example loss in batch: 0.347302  [   76/   89]
Per-example loss in batch: 0.416581  [   78/   89]
Per-example loss in batch: 0.397559  [   80/   89]
Per-example loss in batch: 0.353762  [   82/   89]
Per-example loss in batch: 0.346198  [   84/   89]
Per-example loss in batch: 0.343830  [   86/   89]
Per-example loss in batch: 0.435215  [   88/   89]
Per-example loss in batch: 0.857116  [   89/   89]
Train Error: Avg loss: 0.38501839
validation Error: 
 Avg loss: 0.51040185 
 F1: 0.247012 
 Precision: 0.626148 
 Recall: 0.153853
 IoU: 0.140909

test Error: 
 Avg loss: 0.49085438 
 F1: 0.264239 
 Precision: 0.635690 
 Recall: 0.166783
 IoU: 0.152233

We have finished training iteration 94
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_92_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.380849  [    2/   89]
Per-example loss in batch: 0.320625  [    4/   89]
Per-example loss in batch: 0.388196  [    6/   89]
Per-example loss in batch: 0.388570  [    8/   89]
Per-example loss in batch: 0.352723  [   10/   89]
Per-example loss in batch: 0.376997  [   12/   89]
Per-example loss in batch: 0.375373  [   14/   89]
Per-example loss in batch: 0.379964  [   16/   89]
Per-example loss in batch: 0.415343  [   18/   89]
Per-example loss in batch: 0.445829  [   20/   89]
Per-example loss in batch: 0.406214  [   22/   89]
Per-example loss in batch: 0.350219  [   24/   89]
Per-example loss in batch: 0.397365  [   26/   89]
Per-example loss in batch: 0.388686  [   28/   89]
Per-example loss in batch: 0.351781  [   30/   89]
Per-example loss in batch: 0.388957  [   32/   89]
Per-example loss in batch: 0.372916  [   34/   89]
Per-example loss in batch: 0.423923  [   36/   89]
Per-example loss in batch: 0.373714  [   38/   89]
Per-example loss in batch: 0.336562  [   40/   89]
Per-example loss in batch: 0.411314  [   42/   89]
Per-example loss in batch: 0.407766  [   44/   89]
Per-example loss in batch: 0.372412  [   46/   89]
Per-example loss in batch: 0.405451  [   48/   89]
Per-example loss in batch: 0.347386  [   50/   89]
Per-example loss in batch: 0.397960  [   52/   89]
Per-example loss in batch: 0.453812  [   54/   89]
Per-example loss in batch: 0.316817  [   56/   89]
Per-example loss in batch: 0.305919  [   58/   89]
Per-example loss in batch: 0.289251  [   60/   89]
Per-example loss in batch: 0.377777  [   62/   89]
Per-example loss in batch: 0.443677  [   64/   89]
Per-example loss in batch: 0.416358  [   66/   89]
Per-example loss in batch: 0.366821  [   68/   89]
Per-example loss in batch: 0.387337  [   70/   89]
Per-example loss in batch: 0.347072  [   72/   89]
Per-example loss in batch: 0.393866  [   74/   89]
Per-example loss in batch: 0.320886  [   76/   89]
Per-example loss in batch: 0.440925  [   78/   89]
Per-example loss in batch: 0.433934  [   80/   89]
Per-example loss in batch: 0.431079  [   82/   89]
Per-example loss in batch: 0.343497  [   84/   89]
Per-example loss in batch: 0.413009  [   86/   89]
Per-example loss in batch: 0.337163  [   88/   89]
Per-example loss in batch: 0.851200  [   89/   89]
Train Error: Avg loss: 0.38655937
validation Error: 
 Avg loss: 0.50978874 
 F1: 0.249385 
 Precision: 0.601825 
 Recall: 0.157279
 IoU: 0.142455

test Error: 
 Avg loss: 0.49121294 
 F1: 0.267130 
 Precision: 0.609042 
 Recall: 0.171084
 IoU: 0.154155

We have finished training iteration 95
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_93_.pth
Per-example loss in batch: 0.345768  [    2/   89]
Per-example loss in batch: 0.321167  [    4/   89]
Per-example loss in batch: 0.351265  [    6/   89]
Per-example loss in batch: 0.361704  [    8/   89]
Per-example loss in batch: 0.394396  [   10/   89]
Per-example loss in batch: 0.341760  [   12/   89]
Per-example loss in batch: 0.395833  [   14/   89]
Per-example loss in batch: 0.420922  [   16/   89]
Per-example loss in batch: 0.391650  [   18/   89]
Per-example loss in batch: 0.320827  [   20/   89]
Per-example loss in batch: 0.377077  [   22/   89]
Per-example loss in batch: 0.409509  [   24/   89]
Per-example loss in batch: 0.382629  [   26/   89]
Per-example loss in batch: 0.348920  [   28/   89]
Per-example loss in batch: 0.298748  [   30/   89]
Per-example loss in batch: 0.406663  [   32/   89]
Per-example loss in batch: 0.366586  [   34/   89]
Per-example loss in batch: 0.430320  [   36/   89]
Per-example loss in batch: 0.378175  [   38/   89]
Per-example loss in batch: 0.380963  [   40/   89]
Per-example loss in batch: 0.366571  [   42/   89]
Per-example loss in batch: 0.413288  [   44/   89]
Per-example loss in batch: 0.353256  [   46/   89]
Per-example loss in batch: 0.389591  [   48/   89]
Per-example loss in batch: 0.439089  [   50/   89]
Per-example loss in batch: 0.422330  [   52/   89]
Per-example loss in batch: 0.424243  [   54/   89]
Per-example loss in batch: 0.392776  [   56/   89]
Per-example loss in batch: 0.336138  [   58/   89]
Per-example loss in batch: 0.365916  [   60/   89]
Per-example loss in batch: 0.370488  [   62/   89]
Per-example loss in batch: 0.325330  [   64/   89]
Per-example loss in batch: 0.368988  [   66/   89]
Per-example loss in batch: 0.382126  [   68/   89]
Per-example loss in batch: 0.416481  [   70/   89]
Per-example loss in batch: 0.435368  [   72/   89]
Per-example loss in batch: 0.402557  [   74/   89]
Per-example loss in batch: 0.346448  [   76/   89]
Per-example loss in batch: 0.393386  [   78/   89]
Per-example loss in batch: 0.420496  [   80/   89]
Per-example loss in batch: 0.403571  [   82/   89]
Per-example loss in batch: 0.370043  [   84/   89]
Per-example loss in batch: 0.339348  [   86/   89]
Per-example loss in batch: 0.362658  [   88/   89]
Per-example loss in batch: 0.867263  [   89/   89]
Train Error: Avg loss: 0.38424708
validation Error: 
 Avg loss: 0.51054508 
 F1: 0.245095 
 Precision: 0.618728 
 Recall: 0.152814
 IoU: 0.139663

test Error: 
 Avg loss: 0.49118895 
 F1: 0.261955 
 Precision: 0.628643 
 Recall: 0.165448
 IoU: 0.150718

We have finished training iteration 96
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_94_.pth
Per-example loss in batch: 0.433575  [    2/   89]
Per-example loss in batch: 0.338139  [    4/   89]
Per-example loss in batch: 0.360487  [    6/   89]
Per-example loss in batch: 0.293036  [    8/   89]
Per-example loss in batch: 0.442340  [   10/   89]
Per-example loss in batch: 0.331247  [   12/   89]
Per-example loss in batch: 0.439890  [   14/   89]
Per-example loss in batch: 0.289170  [   16/   89]
Per-example loss in batch: 0.395860  [   18/   89]
Per-example loss in batch: 0.366933  [   20/   89]
Per-example loss in batch: 0.337302  [   22/   89]
Per-example loss in batch: 0.308134  [   24/   89]
Per-example loss in batch: 0.407307  [   26/   89]
Per-example loss in batch: 0.399937  [   28/   89]
Per-example loss in batch: 0.418265  [   30/   89]
Per-example loss in batch: 0.314701  [   32/   89]
Per-example loss in batch: 0.328950  [   34/   89]
Per-example loss in batch: 0.424316  [   36/   89]
Per-example loss in batch: 0.374865  [   38/   89]
Per-example loss in batch: 0.375590  [   40/   89]
Per-example loss in batch: 0.390304  [   42/   89]
Per-example loss in batch: 0.433907  [   44/   89]
Per-example loss in batch: 0.330922  [   46/   89]
Per-example loss in batch: 0.382117  [   48/   89]
Per-example loss in batch: 0.364056  [   50/   89]
Per-example loss in batch: 0.390616  [   52/   89]
Per-example loss in batch: 0.343540  [   54/   89]
Per-example loss in batch: 0.401839  [   56/   89]
Per-example loss in batch: 0.418632  [   58/   89]
Per-example loss in batch: 0.403604  [   60/   89]
Per-example loss in batch: 0.355224  [   62/   89]
Per-example loss in batch: 0.409090  [   64/   89]
Per-example loss in batch: 0.348836  [   66/   89]
Per-example loss in batch: 0.377392  [   68/   89]
Per-example loss in batch: 0.336135  [   70/   89]
Per-example loss in batch: 0.416236  [   72/   89]
Per-example loss in batch: 0.382659  [   74/   89]
Per-example loss in batch: 0.371506  [   76/   89]
Per-example loss in batch: 0.373455  [   78/   89]
Per-example loss in batch: 0.412552  [   80/   89]
Per-example loss in batch: 0.401770  [   82/   89]
Per-example loss in batch: 0.388448  [   84/   89]
Per-example loss in batch: 0.376114  [   86/   89]
Per-example loss in batch: 0.408149  [   88/   89]
Per-example loss in batch: 0.912447  [   89/   89]
Train Error: Avg loss: 0.38322178
validation Error: 
 Avg loss: 0.51042094 
 F1: 0.250221 
 Precision: 0.581611 
 Recall: 0.159399
 IoU: 0.143002

test Error: 
 Avg loss: 0.49136877 
 F1: 0.268146 
 Precision: 0.590111 
 Recall: 0.173490
 IoU: 0.154832

We have finished training iteration 97
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_95_.pth
Per-example loss in batch: 0.414694  [    2/   89]
Per-example loss in batch: 0.369558  [    4/   89]
Per-example loss in batch: 0.319493  [    6/   89]
Per-example loss in batch: 0.434304  [    8/   89]
Per-example loss in batch: 0.393103  [   10/   89]
Per-example loss in batch: 0.382403  [   12/   89]
Per-example loss in batch: 0.407316  [   14/   89]
Per-example loss in batch: 0.439100  [   16/   89]
Per-example loss in batch: 0.343411  [   18/   89]
Per-example loss in batch: 0.417752  [   20/   89]
Per-example loss in batch: 0.391955  [   22/   89]
Per-example loss in batch: 0.364159  [   24/   89]
Per-example loss in batch: 0.406102  [   26/   89]
Per-example loss in batch: 0.376625  [   28/   89]
Per-example loss in batch: 0.436392  [   30/   89]
Per-example loss in batch: 0.354342  [   32/   89]
Per-example loss in batch: 0.366906  [   34/   89]
Per-example loss in batch: 0.432582  [   36/   89]
Per-example loss in batch: 0.421335  [   38/   89]
Per-example loss in batch: 0.398675  [   40/   89]
Per-example loss in batch: 0.376458  [   42/   89]
Per-example loss in batch: 0.346715  [   44/   89]
Per-example loss in batch: 0.325379  [   46/   89]
Per-example loss in batch: 0.357516  [   48/   89]
Per-example loss in batch: 0.415239  [   50/   89]
Per-example loss in batch: 0.413742  [   52/   89]
Per-example loss in batch: 0.408027  [   54/   89]
Per-example loss in batch: 0.341242  [   56/   89]
Per-example loss in batch: 0.337496  [   58/   89]
Per-example loss in batch: 0.403199  [   60/   89]
Per-example loss in batch: 0.363570  [   62/   89]
Per-example loss in batch: 0.380512  [   64/   89]
Per-example loss in batch: 0.327488  [   66/   89]
Per-example loss in batch: 0.372823  [   68/   89]
Per-example loss in batch: 0.393712  [   70/   89]
Per-example loss in batch: 0.344805  [   72/   89]
Per-example loss in batch: 0.375691  [   74/   89]
Per-example loss in batch: 0.338036  [   76/   89]
Per-example loss in batch: 0.374835  [   78/   89]
Per-example loss in batch: 0.405845  [   80/   89]
Per-example loss in batch: 0.396421  [   82/   89]
Per-example loss in batch: 0.331806  [   84/   89]
Per-example loss in batch: 0.355239  [   86/   89]
Per-example loss in batch: 0.403265  [   88/   89]
Per-example loss in batch: 0.905367  [   89/   89]
Train Error: Avg loss: 0.38678538
validation Error: 
 Avg loss: 0.50996558 
 F1: 0.249714 
 Precision: 0.657809 
 Recall: 0.154108
 IoU: 0.142670

test Error: 
 Avg loss: 0.49058375 
 F1: 0.267409 
 Precision: 0.669806 
 Recall: 0.167051
 IoU: 0.154341

We have finished training iteration 98
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_96_.pth
Per-example loss in batch: 0.413118  [    2/   89]
Per-example loss in batch: 0.434341  [    4/   89]
Per-example loss in batch: 0.379398  [    6/   89]
Per-example loss in batch: 0.308491  [    8/   89]
Per-example loss in batch: 0.341927  [   10/   89]
Per-example loss in batch: 0.406565  [   12/   89]
Per-example loss in batch: 0.376030  [   14/   89]
Per-example loss in batch: 0.368829  [   16/   89]
Per-example loss in batch: 0.419778  [   18/   89]
Per-example loss in batch: 0.384964  [   20/   89]
Per-example loss in batch: 0.406482  [   22/   89]
Per-example loss in batch: 0.319821  [   24/   89]
Per-example loss in batch: 0.378783  [   26/   89]
Per-example loss in batch: 0.414306  [   28/   89]
Per-example loss in batch: 0.377616  [   30/   89]
Per-example loss in batch: 0.308843  [   32/   89]
Per-example loss in batch: 0.392584  [   34/   89]
Per-example loss in batch: 0.397766  [   36/   89]
Per-example loss in batch: 0.397634  [   38/   89]
Per-example loss in batch: 0.392423  [   40/   89]
Per-example loss in batch: 0.448168  [   42/   89]
Per-example loss in batch: 0.341938  [   44/   89]
Per-example loss in batch: 0.361716  [   46/   89]
Per-example loss in batch: 0.334474  [   48/   89]
Per-example loss in batch: 0.420746  [   50/   89]
Per-example loss in batch: 0.393826  [   52/   89]
Per-example loss in batch: 0.315931  [   54/   89]
Per-example loss in batch: 0.343481  [   56/   89]
Per-example loss in batch: 0.364479  [   58/   89]
Per-example loss in batch: 0.351100  [   60/   89]
Per-example loss in batch: 0.379145  [   62/   89]
Per-example loss in batch: 0.372984  [   64/   89]
Per-example loss in batch: 0.379792  [   66/   89]
Per-example loss in batch: 0.397202  [   68/   89]
Per-example loss in batch: 0.354423  [   70/   89]
Per-example loss in batch: 0.328117  [   72/   89]
Per-example loss in batch: 0.382575  [   74/   89]
Per-example loss in batch: 0.375909  [   76/   89]
Per-example loss in batch: 0.369255  [   78/   89]
Per-example loss in batch: 0.380220  [   80/   89]
Per-example loss in batch: 0.375062  [   82/   89]
Per-example loss in batch: 0.397484  [   84/   89]
Per-example loss in batch: 0.408653  [   86/   89]
Per-example loss in batch: 0.317538  [   88/   89]
Per-example loss in batch: 0.855673  [   89/   89]
Train Error: Avg loss: 0.38071357
validation Error: 
 Avg loss: 0.51045248 
 F1: 0.257398 
 Precision: 0.592167 
 Recall: 0.164437
 IoU: 0.147709

test Error: 
 Avg loss: 0.49112018 
 F1: 0.270573 
 Precision: 0.582635 
 Recall: 0.176199
 IoU: 0.156452

We have finished training iteration 99
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_97_.pth
Per-example loss in batch: 0.434781  [    2/   89]
Per-example loss in batch: 0.391037  [    4/   89]
Per-example loss in batch: 0.326877  [    6/   89]
Per-example loss in batch: 0.405144  [    8/   89]
Per-example loss in batch: 0.322623  [   10/   89]
Per-example loss in batch: 0.352465  [   12/   89]
Per-example loss in batch: 0.356566  [   14/   89]
Per-example loss in batch: 0.388958  [   16/   89]
Per-example loss in batch: 0.407624  [   18/   89]
Per-example loss in batch: 0.421976  [   20/   89]
Per-example loss in batch: 0.365972  [   22/   89]
Per-example loss in batch: 0.373631  [   24/   89]
Per-example loss in batch: 0.365531  [   26/   89]
Per-example loss in batch: 0.397952  [   28/   89]
Per-example loss in batch: 0.347562  [   30/   89]
Per-example loss in batch: 0.406490  [   32/   89]
Per-example loss in batch: 0.346138  [   34/   89]
Per-example loss in batch: 0.338169  [   36/   89]
Per-example loss in batch: 0.405684  [   38/   89]
Per-example loss in batch: 0.361297  [   40/   89]
Per-example loss in batch: 0.319905  [   42/   89]
Per-example loss in batch: 0.352929  [   44/   89]
Per-example loss in batch: 0.406215  [   46/   89]
Per-example loss in batch: 0.385860  [   48/   89]
Per-example loss in batch: 0.379300  [   50/   89]
Per-example loss in batch: 0.376404  [   52/   89]
Per-example loss in batch: 0.390288  [   54/   89]
Per-example loss in batch: 0.439083  [   56/   89]
Per-example loss in batch: 0.390540  [   58/   89]
Per-example loss in batch: 0.344850  [   60/   89]
Per-example loss in batch: 0.413405  [   62/   89]
Per-example loss in batch: 0.336450  [   64/   89]
Per-example loss in batch: 0.385570  [   66/   89]
Per-example loss in batch: 0.409600  [   68/   89]
Per-example loss in batch: 0.341085  [   70/   89]
Per-example loss in batch: 0.343655  [   72/   89]
Per-example loss in batch: 0.389826  [   74/   89]
Per-example loss in batch: 0.383094  [   76/   89]
Per-example loss in batch: 0.380271  [   78/   89]
Per-example loss in batch: 0.429187  [   80/   89]
Per-example loss in batch: 0.369769  [   82/   89]
Per-example loss in batch: 0.332129  [   84/   89]
Per-example loss in batch: 0.364962  [   86/   89]
Per-example loss in batch: 0.394886  [   88/   89]
Per-example loss in batch: 0.847941  [   89/   89]
Train Error: Avg loss: 0.38201595
validation Error: 
 Avg loss: 0.51030163 
 F1: 0.256394 
 Precision: 0.582856 
 Recall: 0.164344
 IoU: 0.147048

test Error: 
 Avg loss: 0.49123858 
 F1: 0.270241 
 Precision: 0.576011 
 Recall: 0.176531
 IoU: 0.156230

We have finished training iteration 100
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_98_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.429825  [    2/   89]
Per-example loss in batch: 0.360729  [    4/   89]
Per-example loss in batch: 0.360028  [    6/   89]
Per-example loss in batch: 0.351897  [    8/   89]
Per-example loss in batch: 0.390535  [   10/   89]
Per-example loss in batch: 0.268134  [   12/   89]
Per-example loss in batch: 0.399925  [   14/   89]
Per-example loss in batch: 0.377198  [   16/   89]
Per-example loss in batch: 0.407566  [   18/   89]
Per-example loss in batch: 0.388788  [   20/   89]
Per-example loss in batch: 0.361429  [   22/   89]
Per-example loss in batch: 0.356089  [   24/   89]
Per-example loss in batch: 0.448274  [   26/   89]
Per-example loss in batch: 0.357345  [   28/   89]
Per-example loss in batch: 0.334576  [   30/   89]
Per-example loss in batch: 0.387674  [   32/   89]
Per-example loss in batch: 0.419409  [   34/   89]
Per-example loss in batch: 0.376119  [   36/   89]
Per-example loss in batch: 0.397015  [   38/   89]
Per-example loss in batch: 0.361620  [   40/   89]
Per-example loss in batch: 0.395084  [   42/   89]
Per-example loss in batch: 0.405927  [   44/   89]
Per-example loss in batch: 0.374465  [   46/   89]
Per-example loss in batch: 0.414936  [   48/   89]
Per-example loss in batch: 0.360109  [   50/   89]
Per-example loss in batch: 0.316181  [   52/   89]
Per-example loss in batch: 0.428705  [   54/   89]
Per-example loss in batch: 0.407946  [   56/   89]
Per-example loss in batch: 0.357786  [   58/   89]
Per-example loss in batch: 0.424028  [   60/   89]
Per-example loss in batch: 0.341936  [   62/   89]
Per-example loss in batch: 0.325540  [   64/   89]
Per-example loss in batch: 0.416629  [   66/   89]
Per-example loss in batch: 0.358895  [   68/   89]
Per-example loss in batch: 0.358946  [   70/   89]
Per-example loss in batch: 0.400220  [   72/   89]
Per-example loss in batch: 0.354653  [   74/   89]
Per-example loss in batch: 0.394504  [   76/   89]
Per-example loss in batch: 0.389122  [   78/   89]
Per-example loss in batch: 0.423081  [   80/   89]
Per-example loss in batch: 0.363576  [   82/   89]
Per-example loss in batch: 0.329637  [   84/   89]
Per-example loss in batch: 0.324695  [   86/   89]
Per-example loss in batch: 0.424411  [   88/   89]
Per-example loss in batch: 0.861022  [   89/   89]
Train Error: Avg loss: 0.38327411
validation Error: 
 Avg loss: 0.51077772 
 F1: 0.255732 
 Precision: 0.567299 
 Recall: 0.165073
 IoU: 0.146613

test Error: 
 Avg loss: 0.49143728 
 F1: 0.272862 
 Precision: 0.570559 
 Recall: 0.179307
 IoU: 0.157985

We have finished training iteration 101
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_99_.pth
Per-example loss in batch: 0.382437  [    2/   89]
Per-example loss in batch: 0.371974  [    4/   89]
Per-example loss in batch: 0.445450  [    6/   89]
Per-example loss in batch: 0.419257  [    8/   89]
Per-example loss in batch: 0.339820  [   10/   89]
Per-example loss in batch: 0.417237  [   12/   89]
Per-example loss in batch: 0.332039  [   14/   89]
Per-example loss in batch: 0.357028  [   16/   89]
Per-example loss in batch: 0.395318  [   18/   89]
Per-example loss in batch: 0.350192  [   20/   89]
Per-example loss in batch: 0.332458  [   22/   89]
Per-example loss in batch: 0.345914  [   24/   89]
Per-example loss in batch: 0.368931  [   26/   89]
Per-example loss in batch: 0.381875  [   28/   89]
Per-example loss in batch: 0.438485  [   30/   89]
Per-example loss in batch: 0.307780  [   32/   89]
Per-example loss in batch: 0.419856  [   34/   89]
Per-example loss in batch: 0.341601  [   36/   89]
Per-example loss in batch: 0.325502  [   38/   89]
Per-example loss in batch: 0.415422  [   40/   89]
Per-example loss in batch: 0.329095  [   42/   89]
Per-example loss in batch: 0.327016  [   44/   89]
Per-example loss in batch: 0.345530  [   46/   89]
Per-example loss in batch: 0.430532  [   48/   89]
Per-example loss in batch: 0.354039  [   50/   89]
Per-example loss in batch: 0.364369  [   52/   89]
Per-example loss in batch: 0.409501  [   54/   89]
Per-example loss in batch: 0.373217  [   56/   89]
Per-example loss in batch: 0.322248  [   58/   89]
Per-example loss in batch: 0.406325  [   60/   89]
Per-example loss in batch: 0.418689  [   62/   89]
Per-example loss in batch: 0.402816  [   64/   89]
Per-example loss in batch: 0.385604  [   66/   89]
Per-example loss in batch: 0.362421  [   68/   89]
Per-example loss in batch: 0.353156  [   70/   89]
Per-example loss in batch: 0.319940  [   72/   89]
Per-example loss in batch: 0.388176  [   74/   89]
Per-example loss in batch: 0.305208  [   76/   89]
Per-example loss in batch: 0.366680  [   78/   89]
Per-example loss in batch: 0.414444  [   80/   89]
Per-example loss in batch: 0.427222  [   82/   89]
Per-example loss in batch: 0.375699  [   84/   89]
Per-example loss in batch: 0.372062  [   86/   89]
Per-example loss in batch: 0.346505  [   88/   89]
Per-example loss in batch: 0.873009  [   89/   89]
Train Error: Avg loss: 0.37810286
validation Error: 
 Avg loss: 0.50995309 
 F1: 0.258999 
 Precision: 0.630039 
 Recall: 0.163003
 IoU: 0.148764

test Error: 
 Avg loss: 0.49073094 
 F1: 0.277225 
 Precision: 0.635708 
 Recall: 0.177264
 IoU: 0.160917

We have finished training iteration 102
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_100_.pth
Per-example loss in batch: 0.315662  [    2/   89]
Per-example loss in batch: 0.396825  [    4/   89]
Per-example loss in batch: 0.345042  [    6/   89]
Per-example loss in batch: 0.374457  [    8/   89]
Per-example loss in batch: 0.370871  [   10/   89]
Per-example loss in batch: 0.426957  [   12/   89]
Per-example loss in batch: 0.304036  [   14/   89]
Per-example loss in batch: 0.378211  [   16/   89]
Per-example loss in batch: 0.440256  [   18/   89]
Per-example loss in batch: 0.347410  [   20/   89]
Per-example loss in batch: 0.342959  [   22/   89]
Per-example loss in batch: 0.333670  [   24/   89]
Per-example loss in batch: 0.325600  [   26/   89]
Per-example loss in batch: 0.374042  [   28/   89]
Per-example loss in batch: 0.372704  [   30/   89]
Per-example loss in batch: 0.428753  [   32/   89]
Per-example loss in batch: 0.381738  [   34/   89]
Per-example loss in batch: 0.393205  [   36/   89]
Per-example loss in batch: 0.399371  [   38/   89]
Per-example loss in batch: 0.399092  [   40/   89]
Per-example loss in batch: 0.385866  [   42/   89]
Per-example loss in batch: 0.321795  [   44/   89]
Per-example loss in batch: 0.387887  [   46/   89]
Per-example loss in batch: 0.422170  [   48/   89]
Per-example loss in batch: 0.368670  [   50/   89]
Per-example loss in batch: 0.405038  [   52/   89]
Per-example loss in batch: 0.390276  [   54/   89]
Per-example loss in batch: 0.340059  [   56/   89]
Per-example loss in batch: 0.388806  [   58/   89]
Per-example loss in batch: 0.311554  [   60/   89]
Per-example loss in batch: 0.294735  [   62/   89]
Per-example loss in batch: 0.434757  [   64/   89]
Per-example loss in batch: 0.347335  [   66/   89]
Per-example loss in batch: 0.410651  [   68/   89]
Per-example loss in batch: 0.416061  [   70/   89]
Per-example loss in batch: 0.403781  [   72/   89]
Per-example loss in batch: 0.393960  [   74/   89]
Per-example loss in batch: 0.379296  [   76/   89]
Per-example loss in batch: 0.430941  [   78/   89]
Per-example loss in batch: 0.361981  [   80/   89]
Per-example loss in batch: 0.328324  [   82/   89]
Per-example loss in batch: 0.393330  [   84/   89]
Per-example loss in batch: 0.314938  [   86/   89]
Per-example loss in batch: 0.377749  [   88/   89]
Per-example loss in batch: 0.675156  [   89/   89]
Train Error: Avg loss: 0.37749210
validation Error: 
 Avg loss: 0.50988477 
 F1: 0.261643 
 Precision: 0.618498 
 Recall: 0.165915
 IoU: 0.150512

test Error: 
 Avg loss: 0.49066592 
 F1: 0.279584 
 Precision: 0.624974 
 Recall: 0.180069
 IoU: 0.162510

We have finished training iteration 103
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_101_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.337885  [    2/   89]
Per-example loss in batch: 0.298209  [    4/   89]
Per-example loss in batch: 0.389770  [    6/   89]
Per-example loss in batch: 0.436695  [    8/   89]
Per-example loss in batch: 0.410133  [   10/   89]
Per-example loss in batch: 0.376037  [   12/   89]
Per-example loss in batch: 0.421821  [   14/   89]
Per-example loss in batch: 0.392253  [   16/   89]
Per-example loss in batch: 0.322626  [   18/   89]
Per-example loss in batch: 0.364581  [   20/   89]
Per-example loss in batch: 0.380424  [   22/   89]
Per-example loss in batch: 0.345226  [   24/   89]
Per-example loss in batch: 0.366917  [   26/   89]
Per-example loss in batch: 0.353761  [   28/   89]
Per-example loss in batch: 0.365365  [   30/   89]
Per-example loss in batch: 0.309936  [   32/   89]
Per-example loss in batch: 0.321155  [   34/   89]
Per-example loss in batch: 0.397256  [   36/   89]
Per-example loss in batch: 0.402734  [   38/   89]
Per-example loss in batch: 0.441039  [   40/   89]
Per-example loss in batch: 0.429937  [   42/   89]
Per-example loss in batch: 0.380731  [   44/   89]
Per-example loss in batch: 0.428269  [   46/   89]
Per-example loss in batch: 0.421966  [   48/   89]
Per-example loss in batch: 0.419136  [   50/   89]
Per-example loss in batch: 0.408995  [   52/   89]
Per-example loss in batch: 0.401141  [   54/   89]
Per-example loss in batch: 0.349888  [   56/   89]
Per-example loss in batch: 0.332950  [   58/   89]
Per-example loss in batch: 0.410056  [   60/   89]
Per-example loss in batch: 0.287822  [   62/   89]
Per-example loss in batch: 0.362934  [   64/   89]
Per-example loss in batch: 0.379337  [   66/   89]
Per-example loss in batch: 0.370758  [   68/   89]
Per-example loss in batch: 0.353714  [   70/   89]
Per-example loss in batch: 0.390921  [   72/   89]
Per-example loss in batch: 0.307537  [   74/   89]
Per-example loss in batch: 0.372374  [   76/   89]
Per-example loss in batch: 0.364610  [   78/   89]
Per-example loss in batch: 0.324746  [   80/   89]
Per-example loss in batch: 0.362232  [   82/   89]
Per-example loss in batch: 0.449113  [   84/   89]
Per-example loss in batch: 0.357798  [   86/   89]
Per-example loss in batch: 0.333583  [   88/   89]
Per-example loss in batch: 0.682829  [   89/   89]
Train Error: Avg loss: 0.37698391
validation Error: 
 Avg loss: 0.50955732 
 F1: 0.252301 
 Precision: 0.677367 
 Recall: 0.155021
 IoU: 0.144362

test Error: 
 Avg loss: 0.49005142 
 F1: 0.271022 
 Precision: 0.697136 
 Recall: 0.168208
 IoU: 0.156753

We have finished training iteration 104
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_102_.pth
Per-example loss in batch: 0.384186  [    2/   89]
Per-example loss in batch: 0.338183  [    4/   89]
Per-example loss in batch: 0.382083  [    6/   89]
Per-example loss in batch: 0.342298  [    8/   89]
Per-example loss in batch: 0.367820  [   10/   89]
Per-example loss in batch: 0.362761  [   12/   89]
Per-example loss in batch: 0.353962  [   14/   89]
Per-example loss in batch: 0.425280  [   16/   89]
Per-example loss in batch: 0.415265  [   18/   89]
Per-example loss in batch: 0.378883  [   20/   89]
Per-example loss in batch: 0.383671  [   22/   89]
Per-example loss in batch: 0.438434  [   24/   89]
Per-example loss in batch: 0.364244  [   26/   89]
Per-example loss in batch: 0.372633  [   28/   89]
Per-example loss in batch: 0.286412  [   30/   89]
Per-example loss in batch: 0.423411  [   32/   89]
Per-example loss in batch: 0.399966  [   34/   89]
Per-example loss in batch: 0.396947  [   36/   89]
Per-example loss in batch: 0.387110  [   38/   89]
Per-example loss in batch: 0.371543  [   40/   89]
Per-example loss in batch: 0.371886  [   42/   89]
Per-example loss in batch: 0.354307  [   44/   89]
Per-example loss in batch: 0.425136  [   46/   89]
Per-example loss in batch: 0.329099  [   48/   89]
Per-example loss in batch: 0.357130  [   50/   89]
Per-example loss in batch: 0.380592  [   52/   89]
Per-example loss in batch: 0.430706  [   54/   89]
Per-example loss in batch: 0.433433  [   56/   89]
Per-example loss in batch: 0.369247  [   58/   89]
Per-example loss in batch: 0.380280  [   60/   89]
Per-example loss in batch: 0.322036  [   62/   89]
Per-example loss in batch: 0.336410  [   64/   89]
Per-example loss in batch: 0.339936  [   66/   89]
Per-example loss in batch: 0.380697  [   68/   89]
Per-example loss in batch: 0.402181  [   70/   89]
Per-example loss in batch: 0.348156  [   72/   89]
Per-example loss in batch: 0.362217  [   74/   89]
Per-example loss in batch: 0.382657  [   76/   89]
Per-example loss in batch: 0.381549  [   78/   89]
Per-example loss in batch: 0.376567  [   80/   89]
Per-example loss in batch: 0.358699  [   82/   89]
Per-example loss in batch: 0.394323  [   84/   89]
Per-example loss in batch: 0.294732  [   86/   89]
Per-example loss in batch: 0.388613  [   88/   89]
Per-example loss in batch: 0.733752  [   89/   89]
Train Error: Avg loss: 0.37848434
validation Error: 
 Avg loss: 0.51082136 
 F1: 0.249629 
 Precision: 0.581166 
 Recall: 0.158952
 IoU: 0.142615

test Error: 
 Avg loss: 0.49165373 
 F1: 0.264775 
 Precision: 0.577397 
 Recall: 0.171772
 IoU: 0.152588

We have finished training iteration 105
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_103_.pth
Per-example loss in batch: 0.350722  [    2/   89]
Per-example loss in batch: 0.304554  [    4/   89]
Per-example loss in batch: 0.466432  [    6/   89]
Per-example loss in batch: 0.342541  [    8/   89]
Per-example loss in batch: 0.423234  [   10/   89]
Per-example loss in batch: 0.371891  [   12/   89]
Per-example loss in batch: 0.380075  [   14/   89]
Per-example loss in batch: 0.367597  [   16/   89]
Per-example loss in batch: 0.347433  [   18/   89]
Per-example loss in batch: 0.361264  [   20/   89]
Per-example loss in batch: 0.326259  [   22/   89]
Per-example loss in batch: 0.363907  [   24/   89]
Per-example loss in batch: 0.418289  [   26/   89]
Per-example loss in batch: 0.410218  [   28/   89]
Per-example loss in batch: 0.313383  [   30/   89]
Per-example loss in batch: 0.345868  [   32/   89]
Per-example loss in batch: 0.362626  [   34/   89]
Per-example loss in batch: 0.332269  [   36/   89]
Per-example loss in batch: 0.424423  [   38/   89]
Per-example loss in batch: 0.403302  [   40/   89]
Per-example loss in batch: 0.386554  [   42/   89]
Per-example loss in batch: 0.376556  [   44/   89]
Per-example loss in batch: 0.315044  [   46/   89]
Per-example loss in batch: 0.423705  [   48/   89]
Per-example loss in batch: 0.366926  [   50/   89]
Per-example loss in batch: 0.367261  [   52/   89]
Per-example loss in batch: 0.390008  [   54/   89]
Per-example loss in batch: 0.455584  [   56/   89]
Per-example loss in batch: 0.446199  [   58/   89]
Per-example loss in batch: 0.358400  [   60/   89]
Per-example loss in batch: 0.343967  [   62/   89]
Per-example loss in batch: 0.416921  [   64/   89]
Per-example loss in batch: 0.349819  [   66/   89]
Per-example loss in batch: 0.301035  [   68/   89]
Per-example loss in batch: 0.428769  [   70/   89]
Per-example loss in batch: 0.330916  [   72/   89]
Per-example loss in batch: 0.371710  [   74/   89]
Per-example loss in batch: 0.388830  [   76/   89]
Per-example loss in batch: 0.379723  [   78/   89]
Per-example loss in batch: 0.373180  [   80/   89]
Per-example loss in batch: 0.392444  [   82/   89]
Per-example loss in batch: 0.340534  [   84/   89]
Per-example loss in batch: 0.416863  [   86/   89]
Per-example loss in batch: 0.388636  [   88/   89]
Per-example loss in batch: 0.625315  [   89/   89]
Train Error: Avg loss: 0.37839379
validation Error: 
 Avg loss: 0.50983059 
 F1: 0.258801 
 Precision: 0.654529 
 Recall: 0.161287
 IoU: 0.148634

test Error: 
 Avg loss: 0.49030233 
 F1: 0.279038 
 Precision: 0.670059 
 Recall: 0.176209
 IoU: 0.162141

We have finished training iteration 106
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_104_.pth
Per-example loss in batch: 0.380655  [    2/   89]
Per-example loss in batch: 0.339003  [    4/   89]
Per-example loss in batch: 0.324389  [    6/   89]
Per-example loss in batch: 0.408782  [    8/   89]
Per-example loss in batch: 0.394106  [   10/   89]
Per-example loss in batch: 0.370132  [   12/   89]
Per-example loss in batch: 0.334900  [   14/   89]
Per-example loss in batch: 0.382571  [   16/   89]
Per-example loss in batch: 0.335195  [   18/   89]
Per-example loss in batch: 0.372261  [   20/   89]
Per-example loss in batch: 0.363370  [   22/   89]
Per-example loss in batch: 0.407678  [   24/   89]
Per-example loss in batch: 0.434239  [   26/   89]
Per-example loss in batch: 0.425937  [   28/   89]
Per-example loss in batch: 0.410762  [   30/   89]
Per-example loss in batch: 0.326645  [   32/   89]
Per-example loss in batch: 0.355347  [   34/   89]
Per-example loss in batch: 0.419519  [   36/   89]
Per-example loss in batch: 0.371012  [   38/   89]
Per-example loss in batch: 0.343287  [   40/   89]
Per-example loss in batch: 0.428838  [   42/   89]
Per-example loss in batch: 0.406082  [   44/   89]
Per-example loss in batch: 0.397155  [   46/   89]
Per-example loss in batch: 0.366589  [   48/   89]
Per-example loss in batch: 0.370176  [   50/   89]
Per-example loss in batch: 0.335264  [   52/   89]
Per-example loss in batch: 0.390087  [   54/   89]
Per-example loss in batch: 0.461575  [   56/   89]
Per-example loss in batch: 0.354250  [   58/   89]
Per-example loss in batch: 0.370916  [   60/   89]
Per-example loss in batch: 0.376355  [   62/   89]
Per-example loss in batch: 0.376628  [   64/   89]
Per-example loss in batch: 0.374047  [   66/   89]
Per-example loss in batch: 0.296730  [   68/   89]
Per-example loss in batch: 0.417726  [   70/   89]
Per-example loss in batch: 0.288660  [   72/   89]
Per-example loss in batch: 0.356055  [   74/   89]
Per-example loss in batch: 0.360740  [   76/   89]
Per-example loss in batch: 0.403986  [   78/   89]
Per-example loss in batch: 0.323547  [   80/   89]
Per-example loss in batch: 0.400950  [   82/   89]
Per-example loss in batch: 0.398700  [   84/   89]
Per-example loss in batch: 0.273803  [   86/   89]
Per-example loss in batch: 0.407038  [   88/   89]
Per-example loss in batch: 0.609454  [   89/   89]
Train Error: Avg loss: 0.37618909
validation Error: 
 Avg loss: 0.51014912 
 F1: 0.265134 
 Precision: 0.591837 
 Recall: 0.170832
 IoU: 0.152827

test Error: 
 Avg loss: 0.49103346 
 F1: 0.280608 
 Precision: 0.583461 
 Recall: 0.184725
 IoU: 0.163202

We have finished training iteration 107
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_105_.pth
Per-example loss in batch: 0.387977  [    2/   89]
Per-example loss in batch: 0.369771  [    4/   89]
Per-example loss in batch: 0.388337  [    6/   89]
Per-example loss in batch: 0.350764  [    8/   89]
Per-example loss in batch: 0.416811  [   10/   89]
Per-example loss in batch: 0.356483  [   12/   89]
Per-example loss in batch: 0.421362  [   14/   89]
Per-example loss in batch: 0.363316  [   16/   89]
Per-example loss in batch: 0.409121  [   18/   89]
Per-example loss in batch: 0.291101  [   20/   89]
Per-example loss in batch: 0.297304  [   22/   89]
Per-example loss in batch: 0.376826  [   24/   89]
Per-example loss in batch: 0.344951  [   26/   89]
Per-example loss in batch: 0.332164  [   28/   89]
Per-example loss in batch: 0.436539  [   30/   89]
Per-example loss in batch: 0.386772  [   32/   89]
Per-example loss in batch: 0.380298  [   34/   89]
Per-example loss in batch: 0.424754  [   36/   89]
Per-example loss in batch: 0.411576  [   38/   89]
Per-example loss in batch: 0.443679  [   40/   89]
Per-example loss in batch: 0.373539  [   42/   89]
Per-example loss in batch: 0.348216  [   44/   89]
Per-example loss in batch: 0.372406  [   46/   89]
Per-example loss in batch: 0.383201  [   48/   89]
Per-example loss in batch: 0.337548  [   50/   89]
Per-example loss in batch: 0.390171  [   52/   89]
Per-example loss in batch: 0.348893  [   54/   89]
Per-example loss in batch: 0.361135  [   56/   89]
Per-example loss in batch: 0.336218  [   58/   89]
Per-example loss in batch: 0.398554  [   60/   89]
Per-example loss in batch: 0.427298  [   62/   89]
Per-example loss in batch: 0.324098  [   64/   89]
Per-example loss in batch: 0.383460  [   66/   89]
Per-example loss in batch: 0.381536  [   68/   89]
Per-example loss in batch: 0.377295  [   70/   89]
Per-example loss in batch: 0.405644  [   72/   89]
Per-example loss in batch: 0.389459  [   74/   89]
Per-example loss in batch: 0.431378  [   76/   89]
Per-example loss in batch: 0.371800  [   78/   89]
Per-example loss in batch: 0.380832  [   80/   89]
Per-example loss in batch: 0.347106  [   82/   89]
Per-example loss in batch: 0.381911  [   84/   89]
Per-example loss in batch: 0.432046  [   86/   89]
Per-example loss in batch: 0.330846  [   88/   89]
Per-example loss in batch: 0.601979  [   89/   89]
Train Error: Avg loss: 0.37989857
validation Error: 
 Avg loss: 0.51027930 
 F1: 0.259062 
 Precision: 0.625441 
 Recall: 0.163364
 IoU: 0.148806

test Error: 
 Avg loss: 0.49052017 
 F1: 0.277459 
 Precision: 0.630085 
 Recall: 0.177898
 IoU: 0.161075

We have finished training iteration 108
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_106_.pth
Per-example loss in batch: 0.368434  [    2/   89]
Per-example loss in batch: 0.360955  [    4/   89]
Per-example loss in batch: 0.403308  [    6/   89]
Per-example loss in batch: 0.328349  [    8/   89]
Per-example loss in batch: 0.265994  [   10/   89]
Per-example loss in batch: 0.355395  [   12/   89]
Per-example loss in batch: 0.332557  [   14/   89]
Per-example loss in batch: 0.343706  [   16/   89]
Per-example loss in batch: 0.303242  [   18/   89]
Per-example loss in batch: 0.327770  [   20/   89]
Per-example loss in batch: 0.299343  [   22/   89]
Per-example loss in batch: 0.341607  [   24/   89]
Per-example loss in batch: 0.359195  [   26/   89]
Per-example loss in batch: 0.404751  [   28/   89]
Per-example loss in batch: 0.402667  [   30/   89]
Per-example loss in batch: 0.401251  [   32/   89]
Per-example loss in batch: 0.365176  [   34/   89]
Per-example loss in batch: 0.421530  [   36/   89]
Per-example loss in batch: 0.392287  [   38/   89]
Per-example loss in batch: 0.407363  [   40/   89]
Per-example loss in batch: 0.412696  [   42/   89]
Per-example loss in batch: 0.378411  [   44/   89]
Per-example loss in batch: 0.305783  [   46/   89]
Per-example loss in batch: 0.437654  [   48/   89]
Per-example loss in batch: 0.400144  [   50/   89]
Per-example loss in batch: 0.399056  [   52/   89]
Per-example loss in batch: 0.380225  [   54/   89]
Per-example loss in batch: 0.399770  [   56/   89]
Per-example loss in batch: 0.341170  [   58/   89]
Per-example loss in batch: 0.394114  [   60/   89]
Per-example loss in batch: 0.368612  [   62/   89]
Per-example loss in batch: 0.419205  [   64/   89]
Per-example loss in batch: 0.402832  [   66/   89]
Per-example loss in batch: 0.392925  [   68/   89]
Per-example loss in batch: 0.366642  [   70/   89]
Per-example loss in batch: 0.363884  [   72/   89]
Per-example loss in batch: 0.405760  [   74/   89]
Per-example loss in batch: 0.355226  [   76/   89]
Per-example loss in batch: 0.365680  [   78/   89]
Per-example loss in batch: 0.403047  [   80/   89]
Per-example loss in batch: 0.386806  [   82/   89]
Per-example loss in batch: 0.443167  [   84/   89]
Per-example loss in batch: 0.356465  [   86/   89]
Per-example loss in batch: 0.385374  [   88/   89]
Per-example loss in batch: 0.528042  [   89/   89]
Train Error: Avg loss: 0.37558536
validation Error: 
 Avg loss: 0.51075666 
 F1: 0.260302 
 Precision: 0.566700 
 Recall: 0.168953
 IoU: 0.149625

test Error: 
 Avg loss: 0.49149578 
 F1: 0.273825 
 Precision: 0.551584 
 Recall: 0.182117
 IoU: 0.158631

We have finished training iteration 109
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_107_.pth
Per-example loss in batch: 0.414654  [    2/   89]
Per-example loss in batch: 0.392863  [    4/   89]
Per-example loss in batch: 0.357182  [    6/   89]
Per-example loss in batch: 0.304661  [    8/   89]
Per-example loss in batch: 0.365823  [   10/   89]
Per-example loss in batch: 0.303427  [   12/   89]
Per-example loss in batch: 0.326870  [   14/   89]
Per-example loss in batch: 0.393909  [   16/   89]
Per-example loss in batch: 0.330757  [   18/   89]
Per-example loss in batch: 0.345052  [   20/   89]
Per-example loss in batch: 0.427422  [   22/   89]
Per-example loss in batch: 0.399830  [   24/   89]
Per-example loss in batch: 0.370035  [   26/   89]
Per-example loss in batch: 0.335007  [   28/   89]
Per-example loss in batch: 0.308869  [   30/   89]
Per-example loss in batch: 0.385260  [   32/   89]
Per-example loss in batch: 0.291270  [   34/   89]
Per-example loss in batch: 0.311902  [   36/   89]
Per-example loss in batch: 0.404317  [   38/   89]
Per-example loss in batch: 0.416781  [   40/   89]
Per-example loss in batch: 0.345606  [   42/   89]
Per-example loss in batch: 0.380700  [   44/   89]
Per-example loss in batch: 0.308646  [   46/   89]
Per-example loss in batch: 0.349232  [   48/   89]
Per-example loss in batch: 0.343333  [   50/   89]
Per-example loss in batch: 0.431153  [   52/   89]
Per-example loss in batch: 0.359122  [   54/   89]
Per-example loss in batch: 0.367972  [   56/   89]
Per-example loss in batch: 0.372807  [   58/   89]
Per-example loss in batch: 0.435795  [   60/   89]
Per-example loss in batch: 0.411344  [   62/   89]
Per-example loss in batch: 0.371157  [   64/   89]
Per-example loss in batch: 0.374417  [   66/   89]
Per-example loss in batch: 0.399231  [   68/   89]
Per-example loss in batch: 0.328793  [   70/   89]
Per-example loss in batch: 0.330046  [   72/   89]
Per-example loss in batch: 0.409651  [   74/   89]
Per-example loss in batch: 0.385208  [   76/   89]
Per-example loss in batch: 0.335236  [   78/   89]
Per-example loss in batch: 0.377491  [   80/   89]
Per-example loss in batch: 0.372175  [   82/   89]
Per-example loss in batch: 0.383439  [   84/   89]
Per-example loss in batch: 0.387801  [   86/   89]
Per-example loss in batch: 0.337459  [   88/   89]
Per-example loss in batch: 0.664373  [   89/   89]
Train Error: Avg loss: 0.36889650
validation Error: 
 Avg loss: 0.51011422 
 F1: 0.265357 
 Precision: 0.605924 
 Recall: 0.169876
 IoU: 0.152975

test Error: 
 Avg loss: 0.49068858 
 F1: 0.283831 
 Precision: 0.610179 
 Recall: 0.184925
 IoU: 0.165386

We have finished training iteration 110
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_108_.pth
Per-example loss in batch: 0.286302  [    2/   89]
Per-example loss in batch: 0.322613  [    4/   89]
Per-example loss in batch: 0.414786  [    6/   89]
Per-example loss in batch: 0.390878  [    8/   89]
Per-example loss in batch: 0.421553  [   10/   89]
Per-example loss in batch: 0.346713  [   12/   89]
Per-example loss in batch: 0.414904  [   14/   89]
Per-example loss in batch: 0.416242  [   16/   89]
Per-example loss in batch: 0.373014  [   18/   89]
Per-example loss in batch: 0.336310  [   20/   89]
Per-example loss in batch: 0.399664  [   22/   89]
Per-example loss in batch: 0.375723  [   24/   89]
Per-example loss in batch: 0.361172  [   26/   89]
Per-example loss in batch: 0.434799  [   28/   89]
Per-example loss in batch: 0.389882  [   30/   89]
Per-example loss in batch: 0.398583  [   32/   89]
Per-example loss in batch: 0.390525  [   34/   89]
Per-example loss in batch: 0.375274  [   36/   89]
Per-example loss in batch: 0.323135  [   38/   89]
Per-example loss in batch: 0.325219  [   40/   89]
Per-example loss in batch: 0.388910  [   42/   89]
Per-example loss in batch: 0.358229  [   44/   89]
Per-example loss in batch: 0.350732  [   46/   89]
Per-example loss in batch: 0.333389  [   48/   89]
Per-example loss in batch: 0.361351  [   50/   89]
Per-example loss in batch: 0.332702  [   52/   89]
Per-example loss in batch: 0.334741  [   54/   89]
Per-example loss in batch: 0.403626  [   56/   89]
Per-example loss in batch: 0.366913  [   58/   89]
Per-example loss in batch: 0.339790  [   60/   89]
Per-example loss in batch: 0.345605  [   62/   89]
Per-example loss in batch: 0.323744  [   64/   89]
Per-example loss in batch: 0.393209  [   66/   89]
Per-example loss in batch: 0.382215  [   68/   89]
Per-example loss in batch: 0.393067  [   70/   89]
Per-example loss in batch: 0.328891  [   72/   89]
Per-example loss in batch: 0.373021  [   74/   89]
Per-example loss in batch: 0.430400  [   76/   89]
Per-example loss in batch: 0.283606  [   78/   89]
Per-example loss in batch: 0.454074  [   80/   89]
Per-example loss in batch: 0.354776  [   82/   89]
Per-example loss in batch: 0.312305  [   84/   89]
Per-example loss in batch: 0.323227  [   86/   89]
Per-example loss in batch: 0.402415  [   88/   89]
Per-example loss in batch: 0.756347  [   89/   89]
Train Error: Avg loss: 0.37182927
validation Error: 
 Avg loss: 0.50959067 
 F1: 0.268689 
 Precision: 0.630733 
 Recall: 0.170704
 IoU: 0.155194

test Error: 
 Avg loss: 0.49017718 
 F1: 0.286393 
 Precision: 0.628918 
 Recall: 0.185413
 IoU: 0.167129

We have finished training iteration 111
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_109_.pth
Per-example loss in batch: 0.415130  [    2/   89]
Per-example loss in batch: 0.381154  [    4/   89]
Per-example loss in batch: 0.416319  [    6/   89]
Per-example loss in batch: 0.371147  [    8/   89]
Per-example loss in batch: 0.441522  [   10/   89]
Per-example loss in batch: 0.391507  [   12/   89]
Per-example loss in batch: 0.377225  [   14/   89]
Per-example loss in batch: 0.355685  [   16/   89]
Per-example loss in batch: 0.345321  [   18/   89]
Per-example loss in batch: 0.311620  [   20/   89]
Per-example loss in batch: 0.366141  [   22/   89]
Per-example loss in batch: 0.346445  [   24/   89]
Per-example loss in batch: 0.324150  [   26/   89]
Per-example loss in batch: 0.420629  [   28/   89]
Per-example loss in batch: 0.337220  [   30/   89]
Per-example loss in batch: 0.340919  [   32/   89]
Per-example loss in batch: 0.337012  [   34/   89]
Per-example loss in batch: 0.307651  [   36/   89]
Per-example loss in batch: 0.402646  [   38/   89]
Per-example loss in batch: 0.355084  [   40/   89]
Per-example loss in batch: 0.394618  [   42/   89]
Per-example loss in batch: 0.379084  [   44/   89]
Per-example loss in batch: 0.388751  [   46/   89]
Per-example loss in batch: 0.450946  [   48/   89]
Per-example loss in batch: 0.392941  [   50/   89]
Per-example loss in batch: 0.370890  [   52/   89]
Per-example loss in batch: 0.430455  [   54/   89]
Per-example loss in batch: 0.331200  [   56/   89]
Per-example loss in batch: 0.344303  [   58/   89]
Per-example loss in batch: 0.439022  [   60/   89]
Per-example loss in batch: 0.418413  [   62/   89]
Per-example loss in batch: 0.370894  [   64/   89]
Per-example loss in batch: 0.369250  [   66/   89]
Per-example loss in batch: 0.326411  [   68/   89]
Per-example loss in batch: 0.397272  [   70/   89]
Per-example loss in batch: 0.394639  [   72/   89]
Per-example loss in batch: 0.322793  [   74/   89]
Per-example loss in batch: 0.423093  [   76/   89]
Per-example loss in batch: 0.348275  [   78/   89]
Per-example loss in batch: 0.324583  [   80/   89]
Per-example loss in batch: 0.342554  [   82/   89]
Per-example loss in batch: 0.347053  [   84/   89]
Per-example loss in batch: 0.407493  [   86/   89]
Per-example loss in batch: 0.326483  [   88/   89]
Per-example loss in batch: 0.858484  [   89/   89]
Train Error: Avg loss: 0.37786929
validation Error: 
 Avg loss: 0.50948054 
 F1: 0.260076 
 Precision: 0.671046 
 Recall: 0.161294
 IoU: 0.149475

test Error: 
 Avg loss: 0.49008782 
 F1: 0.278654 
 Precision: 0.680500 
 Recall: 0.175197
 IoU: 0.161881

We have finished training iteration 112
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_110_.pth
Per-example loss in batch: 0.374930  [    2/   89]
Per-example loss in batch: 0.352049  [    4/   89]
Per-example loss in batch: 0.336336  [    6/   89]
Per-example loss in batch: 0.399261  [    8/   89]
Per-example loss in batch: 0.413376  [   10/   89]
Per-example loss in batch: 0.352933  [   12/   89]
Per-example loss in batch: 0.399846  [   14/   89]
Per-example loss in batch: 0.289568  [   16/   89]
Per-example loss in batch: 0.375286  [   18/   89]
Per-example loss in batch: 0.406951  [   20/   89]
Per-example loss in batch: 0.436689  [   22/   89]
Per-example loss in batch: 0.359298  [   24/   89]
Per-example loss in batch: 0.321783  [   26/   89]
Per-example loss in batch: 0.354471  [   28/   89]
Per-example loss in batch: 0.398557  [   30/   89]
Per-example loss in batch: 0.327945  [   32/   89]
Per-example loss in batch: 0.367347  [   34/   89]
Per-example loss in batch: 0.363379  [   36/   89]
Per-example loss in batch: 0.389878  [   38/   89]
Per-example loss in batch: 0.325480  [   40/   89]
Per-example loss in batch: 0.385566  [   42/   89]
Per-example loss in batch: 0.342088  [   44/   89]
Per-example loss in batch: 0.273246  [   46/   89]
Per-example loss in batch: 0.432285  [   48/   89]
Per-example loss in batch: 0.343583  [   50/   89]
Per-example loss in batch: 0.341974  [   52/   89]
Per-example loss in batch: 0.377676  [   54/   89]
Per-example loss in batch: 0.350210  [   56/   89]
Per-example loss in batch: 0.401048  [   58/   89]
Per-example loss in batch: 0.355248  [   60/   89]
Per-example loss in batch: 0.408371  [   62/   89]
Per-example loss in batch: 0.376708  [   64/   89]
Per-example loss in batch: 0.330648  [   66/   89]
Per-example loss in batch: 0.377569  [   68/   89]
Per-example loss in batch: 0.388733  [   70/   89]
Per-example loss in batch: 0.336039  [   72/   89]
Per-example loss in batch: 0.395553  [   74/   89]
Per-example loss in batch: 0.406844  [   76/   89]
Per-example loss in batch: 0.406510  [   78/   89]
Per-example loss in batch: 0.399736  [   80/   89]
Per-example loss in batch: 0.326568  [   82/   89]
Per-example loss in batch: 0.395196  [   84/   89]
Per-example loss in batch: 0.376404  [   86/   89]
Per-example loss in batch: 0.369338  [   88/   89]
Per-example loss in batch: 0.806208  [   89/   89]
Train Error: Avg loss: 0.37405858
validation Error: 
 Avg loss: 0.51006240 
 F1: 0.265742 
 Precision: 0.609705 
 Recall: 0.169896
 IoU: 0.153231

test Error: 
 Avg loss: 0.49059784 
 F1: 0.280901 
 Precision: 0.605112 
 Recall: 0.182904
 IoU: 0.163400

We have finished training iteration 113
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_111_.pth
Per-example loss in batch: 0.358629  [    2/   89]
Per-example loss in batch: 0.410776  [    4/   89]
Per-example loss in batch: 0.400695  [    6/   89]
Per-example loss in batch: 0.350869  [    8/   89]
Per-example loss in batch: 0.350008  [   10/   89]
Per-example loss in batch: 0.299024  [   12/   89]
Per-example loss in batch: 0.402503  [   14/   89]
Per-example loss in batch: 0.348837  [   16/   89]
Per-example loss in batch: 0.345899  [   18/   89]
Per-example loss in batch: 0.337294  [   20/   89]
Per-example loss in batch: 0.319074  [   22/   89]
Per-example loss in batch: 0.357642  [   24/   89]
Per-example loss in batch: 0.421551  [   26/   89]
Per-example loss in batch: 0.375352  [   28/   89]
Per-example loss in batch: 0.379965  [   30/   89]
Per-example loss in batch: 0.374597  [   32/   89]
Per-example loss in batch: 0.334875  [   34/   89]
Per-example loss in batch: 0.332574  [   36/   89]
Per-example loss in batch: 0.383075  [   38/   89]
Per-example loss in batch: 0.337542  [   40/   89]
Per-example loss in batch: 0.304593  [   42/   89]
Per-example loss in batch: 0.404567  [   44/   89]
Per-example loss in batch: 0.353292  [   46/   89]
Per-example loss in batch: 0.408157  [   48/   89]
Per-example loss in batch: 0.321410  [   50/   89]
Per-example loss in batch: 0.379294  [   52/   89]
Per-example loss in batch: 0.390623  [   54/   89]
Per-example loss in batch: 0.357008  [   56/   89]
Per-example loss in batch: 0.386611  [   58/   89]
Per-example loss in batch: 0.391732  [   60/   89]
Per-example loss in batch: 0.365822  [   62/   89]
Per-example loss in batch: 0.361511  [   64/   89]
Per-example loss in batch: 0.386993  [   66/   89]
Per-example loss in batch: 0.265937  [   68/   89]
Per-example loss in batch: 0.359285  [   70/   89]
Per-example loss in batch: 0.351332  [   72/   89]
Per-example loss in batch: 0.422525  [   74/   89]
Per-example loss in batch: 0.345185  [   76/   89]
Per-example loss in batch: 0.378561  [   78/   89]
Per-example loss in batch: 0.346286  [   80/   89]
Per-example loss in batch: 0.323942  [   82/   89]
Per-example loss in batch: 0.378492  [   84/   89]
Per-example loss in batch: 0.391248  [   86/   89]
Per-example loss in batch: 0.310609  [   88/   89]
Per-example loss in batch: 0.601850  [   89/   89]
Train Error: Avg loss: 0.36419603
validation Error: 
 Avg loss: 0.50985052 
 F1: 0.271988 
 Precision: 0.607755 
 Recall: 0.175197
 IoU: 0.157399

test Error: 
 Avg loss: 0.49014859 
 F1: 0.288266 
 Precision: 0.608277 
 Recall: 0.188891
 IoU: 0.168406

We have finished training iteration 114
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_112_.pth
Per-example loss in batch: 0.356610  [    2/   89]
Per-example loss in batch: 0.313280  [    4/   89]
Per-example loss in batch: 0.339834  [    6/   89]
Per-example loss in batch: 0.358009  [    8/   89]
Per-example loss in batch: 0.358061  [   10/   89]
Per-example loss in batch: 0.379252  [   12/   89]
Per-example loss in batch: 0.363525  [   14/   89]
Per-example loss in batch: 0.384695  [   16/   89]
Per-example loss in batch: 0.404174  [   18/   89]
Per-example loss in batch: 0.433383  [   20/   89]
Per-example loss in batch: 0.342436  [   22/   89]
Per-example loss in batch: 0.343684  [   24/   89]
Per-example loss in batch: 0.342012  [   26/   89]
Per-example loss in batch: 0.426352  [   28/   89]
Per-example loss in batch: 0.407707  [   30/   89]
Per-example loss in batch: 0.423638  [   32/   89]
Per-example loss in batch: 0.327484  [   34/   89]
Per-example loss in batch: 0.377315  [   36/   89]
Per-example loss in batch: 0.392356  [   38/   89]
Per-example loss in batch: 0.391844  [   40/   89]
Per-example loss in batch: 0.352674  [   42/   89]
Per-example loss in batch: 0.424508  [   44/   89]
Per-example loss in batch: 0.316691  [   46/   89]
Per-example loss in batch: 0.389147  [   48/   89]
Per-example loss in batch: 0.411820  [   50/   89]
Per-example loss in batch: 0.366257  [   52/   89]
Per-example loss in batch: 0.310663  [   54/   89]
Per-example loss in batch: 0.397118  [   56/   89]
Per-example loss in batch: 0.395015  [   58/   89]
Per-example loss in batch: 0.374799  [   60/   89]
Per-example loss in batch: 0.317229  [   62/   89]
Per-example loss in batch: 0.359888  [   64/   89]
Per-example loss in batch: 0.346538  [   66/   89]
Per-example loss in batch: 0.311118  [   68/   89]
Per-example loss in batch: 0.386398  [   70/   89]
Per-example loss in batch: 0.375026  [   72/   89]
Per-example loss in batch: 0.342587  [   74/   89]
Per-example loss in batch: 0.340511  [   76/   89]
Per-example loss in batch: 0.405979  [   78/   89]
Per-example loss in batch: 0.285650  [   80/   89]
Per-example loss in batch: 0.378780  [   82/   89]
Per-example loss in batch: 0.306209  [   84/   89]
Per-example loss in batch: 0.356317  [   86/   89]
Per-example loss in batch: 0.414983  [   88/   89]
Per-example loss in batch: 0.785861  [   89/   89]
Train Error: Avg loss: 0.37133674
validation Error: 
 Avg loss: 0.50956973 
 F1: 0.267219 
 Precision: 0.623298 
 Recall: 0.170064
 IoU: 0.154214

test Error: 
 Avg loss: 0.48970147 
 F1: 0.284607 
 Precision: 0.628608 
 Recall: 0.183944
 IoU: 0.165913

We have finished training iteration 115
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_113_.pth
Per-example loss in batch: 0.305087  [    2/   89]
Per-example loss in batch: 0.426204  [    4/   89]
Per-example loss in batch: 0.326917  [    6/   89]
Per-example loss in batch: 0.320311  [    8/   89]
Per-example loss in batch: 0.382836  [   10/   89]
Per-example loss in batch: 0.377908  [   12/   89]
Per-example loss in batch: 0.396746  [   14/   89]
Per-example loss in batch: 0.311433  [   16/   89]
Per-example loss in batch: 0.399464  [   18/   89]
Per-example loss in batch: 0.333000  [   20/   89]
Per-example loss in batch: 0.352375  [   22/   89]
Per-example loss in batch: 0.332043  [   24/   89]
Per-example loss in batch: 0.403094  [   26/   89]
Per-example loss in batch: 0.371815  [   28/   89]
Per-example loss in batch: 0.375078  [   30/   89]
Per-example loss in batch: 0.390354  [   32/   89]
Per-example loss in batch: 0.334591  [   34/   89]
Per-example loss in batch: 0.412567  [   36/   89]
Per-example loss in batch: 0.420541  [   38/   89]
Per-example loss in batch: 0.368851  [   40/   89]
Per-example loss in batch: 0.312075  [   42/   89]
Per-example loss in batch: 0.335833  [   44/   89]
Per-example loss in batch: 0.356729  [   46/   89]
Per-example loss in batch: 0.287265  [   48/   89]
Per-example loss in batch: 0.336630  [   50/   89]
Per-example loss in batch: 0.413472  [   52/   89]
Per-example loss in batch: 0.355167  [   54/   89]
Per-example loss in batch: 0.390430  [   56/   89]
Per-example loss in batch: 0.389932  [   58/   89]
Per-example loss in batch: 0.397075  [   60/   89]
Per-example loss in batch: 0.348731  [   62/   89]
Per-example loss in batch: 0.385463  [   64/   89]
Per-example loss in batch: 0.323003  [   66/   89]
Per-example loss in batch: 0.356507  [   68/   89]
Per-example loss in batch: 0.413193  [   70/   89]
Per-example loss in batch: 0.345823  [   72/   89]
Per-example loss in batch: 0.380175  [   74/   89]
Per-example loss in batch: 0.369712  [   76/   89]
Per-example loss in batch: 0.374939  [   78/   89]
Per-example loss in batch: 0.377527  [   80/   89]
Per-example loss in batch: 0.268602  [   82/   89]
Per-example loss in batch: 0.269066  [   84/   89]
Per-example loss in batch: 0.426886  [   86/   89]
Per-example loss in batch: 0.435435  [   88/   89]
Per-example loss in batch: 0.896035  [   89/   89]
Train Error: Avg loss: 0.36941343
validation Error: 
 Avg loss: 0.50906244 
 F1: 0.257260 
 Precision: 0.706859 
 Recall: 0.157245
 IoU: 0.147618

test Error: 
 Avg loss: 0.48904498 
 F1: 0.272040 
 Precision: 0.717226 
 Recall: 0.167853
 IoU: 0.157434

We have finished training iteration 116
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_114_.pth
Per-example loss in batch: 0.384064  [    2/   89]
Per-example loss in batch: 0.379577  [    4/   89]
Per-example loss in batch: 0.368260  [    6/   89]
Per-example loss in batch: 0.331739  [    8/   89]
Per-example loss in batch: 0.341516  [   10/   89]
Per-example loss in batch: 0.376804  [   12/   89]
Per-example loss in batch: 0.354671  [   14/   89]
Per-example loss in batch: 0.355583  [   16/   89]
Per-example loss in batch: 0.366496  [   18/   89]
Per-example loss in batch: 0.342709  [   20/   89]
Per-example loss in batch: 0.356465  [   22/   89]
Per-example loss in batch: 0.375434  [   24/   89]
Per-example loss in batch: 0.350176  [   26/   89]
Per-example loss in batch: 0.423208  [   28/   89]
Per-example loss in batch: 0.369874  [   30/   89]
Per-example loss in batch: 0.360644  [   32/   89]
Per-example loss in batch: 0.361832  [   34/   89]
Per-example loss in batch: 0.412498  [   36/   89]
Per-example loss in batch: 0.335080  [   38/   89]
Per-example loss in batch: 0.377742  [   40/   89]
Per-example loss in batch: 0.369490  [   42/   89]
Per-example loss in batch: 0.385863  [   44/   89]
Per-example loss in batch: 0.385857  [   46/   89]
Per-example loss in batch: 0.408945  [   48/   89]
Per-example loss in batch: 0.394181  [   50/   89]
Per-example loss in batch: 0.387601  [   52/   89]
Per-example loss in batch: 0.331688  [   54/   89]
Per-example loss in batch: 0.408608  [   56/   89]
Per-example loss in batch: 0.334139  [   58/   89]
Per-example loss in batch: 0.392265  [   60/   89]
Per-example loss in batch: 0.352322  [   62/   89]
Per-example loss in batch: 0.411689  [   64/   89]
Per-example loss in batch: 0.415563  [   66/   89]
Per-example loss in batch: 0.296906  [   68/   89]
Per-example loss in batch: 0.288166  [   70/   89]
Per-example loss in batch: 0.373685  [   72/   89]
Per-example loss in batch: 0.379667  [   74/   89]
Per-example loss in batch: 0.396321  [   76/   89]
Per-example loss in batch: 0.320102  [   78/   89]
Per-example loss in batch: 0.375113  [   80/   89]
Per-example loss in batch: 0.280195  [   82/   89]
Per-example loss in batch: 0.396383  [   84/   89]
Per-example loss in batch: 0.362352  [   86/   89]
Per-example loss in batch: 0.313267  [   88/   89]
Per-example loss in batch: 0.793251  [   89/   89]
Train Error: Avg loss: 0.37036781
validation Error: 
 Avg loss: 0.50929294 
 F1: 0.269256 
 Precision: 0.627045 
 Recall: 0.171436
 IoU: 0.155572

test Error: 
 Avg loss: 0.49000510 
 F1: 0.284103 
 Precision: 0.617585 
 Recall: 0.184485
 IoU: 0.165571

We have finished training iteration 117
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_115_.pth
Per-example loss in batch: 0.335542  [    2/   89]
Per-example loss in batch: 0.453202  [    4/   89]
Per-example loss in batch: 0.405215  [    6/   89]
Per-example loss in batch: 0.338105  [    8/   89]
Per-example loss in batch: 0.453604  [   10/   89]
Per-example loss in batch: 0.323276  [   12/   89]
Per-example loss in batch: 0.366508  [   14/   89]
Per-example loss in batch: 0.349338  [   16/   89]
Per-example loss in batch: 0.353072  [   18/   89]
Per-example loss in batch: 0.377764  [   20/   89]
Per-example loss in batch: 0.414029  [   22/   89]
Per-example loss in batch: 0.385535  [   24/   89]
Per-example loss in batch: 0.337077  [   26/   89]
Per-example loss in batch: 0.408001  [   28/   89]
Per-example loss in batch: 0.327031  [   30/   89]
Per-example loss in batch: 0.351720  [   32/   89]
Per-example loss in batch: 0.364342  [   34/   89]
Per-example loss in batch: 0.414197  [   36/   89]
Per-example loss in batch: 0.377339  [   38/   89]
Per-example loss in batch: 0.405959  [   40/   89]
Per-example loss in batch: 0.370728  [   42/   89]
Per-example loss in batch: 0.428940  [   44/   89]
Per-example loss in batch: 0.388352  [   46/   89]
Per-example loss in batch: 0.379203  [   48/   89]
Per-example loss in batch: 0.309393  [   50/   89]
Per-example loss in batch: 0.298288  [   52/   89]
Per-example loss in batch: 0.394838  [   54/   89]
Per-example loss in batch: 0.336855  [   56/   89]
Per-example loss in batch: 0.297442  [   58/   89]
Per-example loss in batch: 0.327143  [   60/   89]
Per-example loss in batch: 0.349761  [   62/   89]
Per-example loss in batch: 0.372871  [   64/   89]
Per-example loss in batch: 0.311566  [   66/   89]
Per-example loss in batch: 0.353830  [   68/   89]
Per-example loss in batch: 0.395546  [   70/   89]
Per-example loss in batch: 0.371841  [   72/   89]
Per-example loss in batch: 0.418164  [   74/   89]
Per-example loss in batch: 0.365434  [   76/   89]
Per-example loss in batch: 0.288147  [   78/   89]
Per-example loss in batch: 0.340405  [   80/   89]
Per-example loss in batch: 0.362480  [   82/   89]
Per-example loss in batch: 0.397234  [   84/   89]
Per-example loss in batch: 0.327652  [   86/   89]
Per-example loss in batch: 0.373332  [   88/   89]
Per-example loss in batch: 0.799166  [   89/   89]
Train Error: Avg loss: 0.37078396
validation Error: 
 Avg loss: 0.50991900 
 F1: 0.273225 
 Precision: 0.565520 
 Recall: 0.180125
 IoU: 0.158229

test Error: 
 Avg loss: 0.49015506 
 F1: 0.291099 
 Precision: 0.558488 
 Recall: 0.196852
 IoU: 0.170343

We have finished training iteration 118
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_116_.pth
Per-example loss in batch: 0.339355  [    2/   89]
Per-example loss in batch: 0.356028  [    4/   89]
Per-example loss in batch: 0.356297  [    6/   89]
Per-example loss in batch: 0.436413  [    8/   89]
Per-example loss in batch: 0.325778  [   10/   89]
Per-example loss in batch: 0.403667  [   12/   89]
Per-example loss in batch: 0.303407  [   14/   89]
Per-example loss in batch: 0.327157  [   16/   89]
Per-example loss in batch: 0.370064  [   18/   89]
Per-example loss in batch: 0.427644  [   20/   89]
Per-example loss in batch: 0.351566  [   22/   89]
Per-example loss in batch: 0.389459  [   24/   89]
Per-example loss in batch: 0.358820  [   26/   89]
Per-example loss in batch: 0.353335  [   28/   89]
Per-example loss in batch: 0.357186  [   30/   89]
Per-example loss in batch: 0.355789  [   32/   89]
Per-example loss in batch: 0.346675  [   34/   89]
Per-example loss in batch: 0.418900  [   36/   89]
Per-example loss in batch: 0.362458  [   38/   89]
Per-example loss in batch: 0.411714  [   40/   89]
Per-example loss in batch: 0.384711  [   42/   89]
Per-example loss in batch: 0.362943  [   44/   89]
Per-example loss in batch: 0.344604  [   46/   89]
Per-example loss in batch: 0.354319  [   48/   89]
Per-example loss in batch: 0.364487  [   50/   89]
Per-example loss in batch: 0.384567  [   52/   89]
Per-example loss in batch: 0.388814  [   54/   89]
Per-example loss in batch: 0.392775  [   56/   89]
Per-example loss in batch: 0.377732  [   58/   89]
Per-example loss in batch: 0.350116  [   60/   89]
Per-example loss in batch: 0.348029  [   62/   89]
Per-example loss in batch: 0.327774  [   64/   89]
Per-example loss in batch: 0.396310  [   66/   89]
Per-example loss in batch: 0.391143  [   68/   89]
Per-example loss in batch: 0.277858  [   70/   89]
Per-example loss in batch: 0.313038  [   72/   89]
Per-example loss in batch: 0.400704  [   74/   89]
Per-example loss in batch: 0.321073  [   76/   89]
Per-example loss in batch: 0.390345  [   78/   89]
Per-example loss in batch: 0.366490  [   80/   89]
Per-example loss in batch: 0.365647  [   82/   89]
Per-example loss in batch: 0.451788  [   84/   89]
Per-example loss in batch: 0.368604  [   86/   89]
Per-example loss in batch: 0.392727  [   88/   89]
Per-example loss in batch: 0.622843  [   89/   89]
Train Error: Avg loss: 0.37033103
validation Error: 
 Avg loss: 0.50886086 
 F1: 0.277148 
 Precision: 0.596599 
 Recall: 0.180499
 IoU: 0.160866

test Error: 
 Avg loss: 0.48993284 
 F1: 0.293422 
 Precision: 0.590129 
 Recall: 0.195253
 IoU: 0.171936

We have finished training iteration 119
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_117_.pth
Per-example loss in batch: 0.334750  [    2/   89]
Per-example loss in batch: 0.332733  [    4/   89]
Per-example loss in batch: 0.248931  [    6/   89]
Per-example loss in batch: 0.395802  [    8/   89]
Per-example loss in batch: 0.386118  [   10/   89]
Per-example loss in batch: 0.308659  [   12/   89]
Per-example loss in batch: 0.388609  [   14/   89]
Per-example loss in batch: 0.292504  [   16/   89]
Per-example loss in batch: 0.379740  [   18/   89]
Per-example loss in batch: 0.399302  [   20/   89]
Per-example loss in batch: 0.381917  [   22/   89]
Per-example loss in batch: 0.375031  [   24/   89]
Per-example loss in batch: 0.392334  [   26/   89]
Per-example loss in batch: 0.357354  [   28/   89]
Per-example loss in batch: 0.408540  [   30/   89]
Per-example loss in batch: 0.384099  [   32/   89]
Per-example loss in batch: 0.316718  [   34/   89]
Per-example loss in batch: 0.329372  [   36/   89]
Per-example loss in batch: 0.323296  [   38/   89]
Per-example loss in batch: 0.365169  [   40/   89]
Per-example loss in batch: 0.295309  [   42/   89]
Per-example loss in batch: 0.372862  [   44/   89]
Per-example loss in batch: 0.358500  [   46/   89]
Per-example loss in batch: 0.330898  [   48/   89]
Per-example loss in batch: 0.329106  [   50/   89]
Per-example loss in batch: 0.367926  [   52/   89]
Per-example loss in batch: 0.370242  [   54/   89]
Per-example loss in batch: 0.378633  [   56/   89]
Per-example loss in batch: 0.417242  [   58/   89]
Per-example loss in batch: 0.360526  [   60/   89]
Per-example loss in batch: 0.408470  [   62/   89]
Per-example loss in batch: 0.334728  [   64/   89]
Per-example loss in batch: 0.428489  [   66/   89]
Per-example loss in batch: 0.371538  [   68/   89]
Per-example loss in batch: 0.355009  [   70/   89]
Per-example loss in batch: 0.355526  [   72/   89]
Per-example loss in batch: 0.407394  [   74/   89]
Per-example loss in batch: 0.378730  [   76/   89]
Per-example loss in batch: 0.430120  [   78/   89]
Per-example loss in batch: 0.352206  [   80/   89]
Per-example loss in batch: 0.427886  [   82/   89]
Per-example loss in batch: 0.350356  [   84/   89]
Per-example loss in batch: 0.348789  [   86/   89]
Per-example loss in batch: 0.329353  [   88/   89]
Per-example loss in batch: 0.650192  [   89/   89]
Train Error: Avg loss: 0.36597553
validation Error: 
 Avg loss: 0.50913100 
 F1: 0.275421 
 Precision: 0.597924 
 Recall: 0.178918
 IoU: 0.159703

test Error: 
 Avg loss: 0.48940381 
 F1: 0.296850 
 Precision: 0.596682 
 Recall: 0.197571
 IoU: 0.174295

We have finished training iteration 120
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_118_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.333301  [    2/   89]
Per-example loss in batch: 0.374158  [    4/   89]
Per-example loss in batch: 0.370555  [    6/   89]
Per-example loss in batch: 0.273382  [    8/   89]
Per-example loss in batch: 0.361421  [   10/   89]
Per-example loss in batch: 0.310319  [   12/   89]
Per-example loss in batch: 0.376226  [   14/   89]
Per-example loss in batch: 0.343523  [   16/   89]
Per-example loss in batch: 0.376631  [   18/   89]
Per-example loss in batch: 0.417557  [   20/   89]
Per-example loss in batch: 0.367718  [   22/   89]
Per-example loss in batch: 0.376109  [   24/   89]
Per-example loss in batch: 0.370095  [   26/   89]
Per-example loss in batch: 0.339968  [   28/   89]
Per-example loss in batch: 0.390407  [   30/   89]
Per-example loss in batch: 0.382964  [   32/   89]
Per-example loss in batch: 0.359590  [   34/   89]
Per-example loss in batch: 0.355616  [   36/   89]
Per-example loss in batch: 0.445995  [   38/   89]
Per-example loss in batch: 0.366681  [   40/   89]
Per-example loss in batch: 0.348010  [   42/   89]
Per-example loss in batch: 0.310943  [   44/   89]
Per-example loss in batch: 0.406565  [   46/   89]
Per-example loss in batch: 0.382396  [   48/   89]
Per-example loss in batch: 0.380276  [   50/   89]
Per-example loss in batch: 0.395850  [   52/   89]
Per-example loss in batch: 0.408255  [   54/   89]
Per-example loss in batch: 0.414000  [   56/   89]
Per-example loss in batch: 0.338446  [   58/   89]
Per-example loss in batch: 0.369727  [   60/   89]
Per-example loss in batch: 0.422894  [   62/   89]
Per-example loss in batch: 0.419477  [   64/   89]
Per-example loss in batch: 0.415246  [   66/   89]
Per-example loss in batch: 0.369057  [   68/   89]
Per-example loss in batch: 0.294499  [   70/   89]
Per-example loss in batch: 0.397824  [   72/   89]
Per-example loss in batch: 0.334572  [   74/   89]
Per-example loss in batch: 0.340464  [   76/   89]
Per-example loss in batch: 0.344332  [   78/   89]
Per-example loss in batch: 0.352408  [   80/   89]
Per-example loss in batch: 0.348596  [   82/   89]
Per-example loss in batch: 0.272543  [   84/   89]
Per-example loss in batch: 0.369746  [   86/   89]
Per-example loss in batch: 0.331518  [   88/   89]
Per-example loss in batch: 0.699424  [   89/   89]
Train Error: Avg loss: 0.36875439
validation Error: 
 Avg loss: 0.50933898 
 F1: 0.276984 
 Precision: 0.571402 
 Recall: 0.182797
 IoU: 0.160755

test Error: 
 Avg loss: 0.49000197 
 F1: 0.298881 
 Precision: 0.567232 
 Recall: 0.202894
 IoU: 0.175696

We have finished training iteration 121
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_119_.pth
Per-example loss in batch: 0.363734  [    2/   89]
Per-example loss in batch: 0.406476  [    4/   89]
Per-example loss in batch: 0.285815  [    6/   89]
Per-example loss in batch: 0.329174  [    8/   89]
Per-example loss in batch: 0.390981  [   10/   89]
Per-example loss in batch: 0.348249  [   12/   89]
Per-example loss in batch: 0.383450  [   14/   89]
Per-example loss in batch: 0.443304  [   16/   89]
Per-example loss in batch: 0.301706  [   18/   89]
Per-example loss in batch: 0.335610  [   20/   89]
Per-example loss in batch: 0.327085  [   22/   89]
Per-example loss in batch: 0.368231  [   24/   89]
Per-example loss in batch: 0.372824  [   26/   89]
Per-example loss in batch: 0.380144  [   28/   89]
Per-example loss in batch: 0.303018  [   30/   89]
Per-example loss in batch: 0.398808  [   32/   89]
Per-example loss in batch: 0.305789  [   34/   89]
Per-example loss in batch: 0.345280  [   36/   89]
Per-example loss in batch: 0.406538  [   38/   89]
Per-example loss in batch: 0.360203  [   40/   89]
Per-example loss in batch: 0.384293  [   42/   89]
Per-example loss in batch: 0.352832  [   44/   89]
Per-example loss in batch: 0.326715  [   46/   89]
Per-example loss in batch: 0.422034  [   48/   89]
Per-example loss in batch: 0.379951  [   50/   89]
Per-example loss in batch: 0.348208  [   52/   89]
Per-example loss in batch: 0.357314  [   54/   89]
Per-example loss in batch: 0.376125  [   56/   89]
Per-example loss in batch: 0.375652  [   58/   89]
Per-example loss in batch: 0.304793  [   60/   89]
Per-example loss in batch: 0.372261  [   62/   89]
Per-example loss in batch: 0.350857  [   64/   89]
Per-example loss in batch: 0.389717  [   66/   89]
Per-example loss in batch: 0.377436  [   68/   89]
Per-example loss in batch: 0.279985  [   70/   89]
Per-example loss in batch: 0.321645  [   72/   89]
Per-example loss in batch: 0.428297  [   74/   89]
Per-example loss in batch: 0.309167  [   76/   89]
Per-example loss in batch: 0.386129  [   78/   89]
Per-example loss in batch: 0.360841  [   80/   89]
Per-example loss in batch: 0.414203  [   82/   89]
Per-example loss in batch: 0.377908  [   84/   89]
Per-example loss in batch: 0.325570  [   86/   89]
Per-example loss in batch: 0.323410  [   88/   89]
Per-example loss in batch: 0.887502  [   89/   89]
Train Error: Avg loss: 0.36506759
validation Error: 
 Avg loss: 0.50972315 
 F1: 0.292008 
 Precision: 0.525066 
 Recall: 0.202240
 IoU: 0.170965

test Error: 
 Avg loss: 0.49017340 
 F1: 0.313755 
 Precision: 0.512896 
 Recall: 0.226005
 IoU: 0.186067

We have finished training iteration 122
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_120_.pth
Per-example loss in batch: 0.341196  [    2/   89]
Per-example loss in batch: 0.351314  [    4/   89]
Per-example loss in batch: 0.388481  [    6/   89]
Per-example loss in batch: 0.351563  [    8/   89]
Per-example loss in batch: 0.320392  [   10/   89]
Per-example loss in batch: 0.318031  [   12/   89]
Per-example loss in batch: 0.345615  [   14/   89]
Per-example loss in batch: 0.361870  [   16/   89]
Per-example loss in batch: 0.369243  [   18/   89]
Per-example loss in batch: 0.442443  [   20/   89]
Per-example loss in batch: 0.278977  [   22/   89]
Per-example loss in batch: 0.405224  [   24/   89]
Per-example loss in batch: 0.349788  [   26/   89]
Per-example loss in batch: 0.401031  [   28/   89]
Per-example loss in batch: 0.359853  [   30/   89]
Per-example loss in batch: 0.406499  [   32/   89]
Per-example loss in batch: 0.363522  [   34/   89]
Per-example loss in batch: 0.389796  [   36/   89]
Per-example loss in batch: 0.357701  [   38/   89]
Per-example loss in batch: 0.333439  [   40/   89]
Per-example loss in batch: 0.425503  [   42/   89]
Per-example loss in batch: 0.355467  [   44/   89]
Per-example loss in batch: 0.363136  [   46/   89]
Per-example loss in batch: 0.396919  [   48/   89]
Per-example loss in batch: 0.365155  [   50/   89]
Per-example loss in batch: 0.391333  [   52/   89]
Per-example loss in batch: 0.366007  [   54/   89]
Per-example loss in batch: 0.424936  [   56/   89]
Per-example loss in batch: 0.398612  [   58/   89]
Per-example loss in batch: 0.315968  [   60/   89]
Per-example loss in batch: 0.366311  [   62/   89]
Per-example loss in batch: 0.381415  [   64/   89]
Per-example loss in batch: 0.293250  [   66/   89]
Per-example loss in batch: 0.355475  [   68/   89]
Per-example loss in batch: 0.378944  [   70/   89]
Per-example loss in batch: 0.396023  [   72/   89]
Per-example loss in batch: 0.390605  [   74/   89]
Per-example loss in batch: 0.292788  [   76/   89]
Per-example loss in batch: 0.329289  [   78/   89]
Per-example loss in batch: 0.278092  [   80/   89]
Per-example loss in batch: 0.398548  [   82/   89]
Per-example loss in batch: 0.349835  [   84/   89]
Per-example loss in batch: 0.322410  [   86/   89]
Per-example loss in batch: 0.342637  [   88/   89]
Per-example loss in batch: 0.766908  [   89/   89]
Train Error: Avg loss: 0.36624919
validation Error: 
 Avg loss: 0.50929224 
 F1: 0.279115 
 Precision: 0.590820 
 Recall: 0.182717
 IoU: 0.162193

test Error: 
 Avg loss: 0.48989977 
 F1: 0.304307 
 Precision: 0.584191 
 Recall: 0.205739
 IoU: 0.179459

We have finished training iteration 123
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_121_.pth
Per-example loss in batch: 0.293427  [    2/   89]
Per-example loss in batch: 0.343929  [    4/   89]
Per-example loss in batch: 0.381121  [    6/   89]
Per-example loss in batch: 0.356203  [    8/   89]
Per-example loss in batch: 0.418051  [   10/   89]
Per-example loss in batch: 0.339433  [   12/   89]
Per-example loss in batch: 0.327090  [   14/   89]
Per-example loss in batch: 0.324909  [   16/   89]
Per-example loss in batch: 0.376872  [   18/   89]
Per-example loss in batch: 0.289348  [   20/   89]
Per-example loss in batch: 0.380376  [   22/   89]
Per-example loss in batch: 0.355739  [   24/   89]
Per-example loss in batch: 0.353586  [   26/   89]
Per-example loss in batch: 0.421748  [   28/   89]
Per-example loss in batch: 0.354577  [   30/   89]
Per-example loss in batch: 0.373476  [   32/   89]
Per-example loss in batch: 0.379386  [   34/   89]
Per-example loss in batch: 0.276061  [   36/   89]
Per-example loss in batch: 0.401396  [   38/   89]
Per-example loss in batch: 0.424098  [   40/   89]
Per-example loss in batch: 0.364952  [   42/   89]
Per-example loss in batch: 0.422459  [   44/   89]
Per-example loss in batch: 0.339539  [   46/   89]
Per-example loss in batch: 0.325126  [   48/   89]
Per-example loss in batch: 0.364621  [   50/   89]
Per-example loss in batch: 0.394152  [   52/   89]
Per-example loss in batch: 0.393468  [   54/   89]
Per-example loss in batch: 0.354247  [   56/   89]
Per-example loss in batch: 0.376087  [   58/   89]
Per-example loss in batch: 0.367876  [   60/   89]
Per-example loss in batch: 0.333177  [   62/   89]
Per-example loss in batch: 0.352698  [   64/   89]
Per-example loss in batch: 0.373972  [   66/   89]
Per-example loss in batch: 0.434648  [   68/   89]
Per-example loss in batch: 0.374808  [   70/   89]
Per-example loss in batch: 0.408357  [   72/   89]
Per-example loss in batch: 0.404279  [   74/   89]
Per-example loss in batch: 0.331631  [   76/   89]
Per-example loss in batch: 0.290760  [   78/   89]
Per-example loss in batch: 0.290515  [   80/   89]
Per-example loss in batch: 0.347561  [   82/   89]
Per-example loss in batch: 0.309279  [   84/   89]
Per-example loss in batch: 0.430000  [   86/   89]
Per-example loss in batch: 0.313683  [   88/   89]
Per-example loss in batch: 0.839246  [   89/   89]
Train Error: Avg loss: 0.36603019
validation Error: 
 Avg loss: 0.50894462 
 F1: 0.290599 
 Precision: 0.598420 
 Recall: 0.191892
 IoU: 0.170001

test Error: 
 Avg loss: 0.48890935 
 F1: 0.320438 
 Precision: 0.602515 
 Recall: 0.218257
 IoU: 0.190787

We have finished training iteration 124
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_122_.pth
Per-example loss in batch: 0.326214  [    2/   89]
Per-example loss in batch: 0.329306  [    4/   89]
Per-example loss in batch: 0.412330  [    6/   89]
Per-example loss in batch: 0.397003  [    8/   89]
Per-example loss in batch: 0.406894  [   10/   89]
Per-example loss in batch: 0.359493  [   12/   89]
Per-example loss in batch: 0.270598  [   14/   89]
Per-example loss in batch: 0.317957  [   16/   89]
Per-example loss in batch: 0.291813  [   18/   89]
Per-example loss in batch: 0.341580  [   20/   89]
Per-example loss in batch: 0.367512  [   22/   89]
Per-example loss in batch: 0.377588  [   24/   89]
Per-example loss in batch: 0.402110  [   26/   89]
Per-example loss in batch: 0.374359  [   28/   89]
Per-example loss in batch: 0.410658  [   30/   89]
Per-example loss in batch: 0.286309  [   32/   89]
Per-example loss in batch: 0.295563  [   34/   89]
Per-example loss in batch: 0.319447  [   36/   89]
Per-example loss in batch: 0.342544  [   38/   89]
Per-example loss in batch: 0.366282  [   40/   89]
Per-example loss in batch: 0.329847  [   42/   89]
Per-example loss in batch: 0.387601  [   44/   89]
Per-example loss in batch: 0.295332  [   46/   89]
Per-example loss in batch: 0.364656  [   48/   89]
Per-example loss in batch: 0.372811  [   50/   89]
Per-example loss in batch: 0.392883  [   52/   89]
Per-example loss in batch: 0.409025  [   54/   89]
Per-example loss in batch: 0.359731  [   56/   89]
Per-example loss in batch: 0.386126  [   58/   89]
Per-example loss in batch: 0.344446  [   60/   89]
Per-example loss in batch: 0.386752  [   62/   89]
Per-example loss in batch: 0.370661  [   64/   89]
Per-example loss in batch: 0.345625  [   66/   89]
Per-example loss in batch: 0.356575  [   68/   89]
Per-example loss in batch: 0.335822  [   70/   89]
Per-example loss in batch: 0.335690  [   72/   89]
Per-example loss in batch: 0.327636  [   74/   89]
Per-example loss in batch: 0.347624  [   76/   89]
Per-example loss in batch: 0.306358  [   78/   89]
Per-example loss in batch: 0.401587  [   80/   89]
Per-example loss in batch: 0.400192  [   82/   89]
Per-example loss in batch: 0.380124  [   84/   89]
Per-example loss in batch: 0.423533  [   86/   89]
Per-example loss in batch: 0.394174  [   88/   89]
Per-example loss in batch: 0.854775  [   89/   89]
Train Error: Avg loss: 0.36354513
validation Error: 
 Avg loss: 0.50961195 
 F1: 0.306503 
 Precision: 0.444579 
 Recall: 0.233868
 IoU: 0.180988

test Error: 
 Avg loss: 0.49067341 
 F1: 0.335457 
 Precision: 0.434281 
 Recall: 0.273272
 IoU: 0.201531

We have finished training iteration 125
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_123_.pth
Per-example loss in batch: 0.421883  [    2/   89]
Per-example loss in batch: 0.366463  [    4/   89]
Per-example loss in batch: 0.344453  [    6/   89]
Per-example loss in batch: 0.376171  [    8/   89]
Per-example loss in batch: 0.383650  [   10/   89]
Per-example loss in batch: 0.407967  [   12/   89]
Per-example loss in batch: 0.324934  [   14/   89]
Per-example loss in batch: 0.400024  [   16/   89]
Per-example loss in batch: 0.350726  [   18/   89]
Per-example loss in batch: 0.262685  [   20/   89]
Per-example loss in batch: 0.406641  [   22/   89]
Per-example loss in batch: 0.326106  [   24/   89]
Per-example loss in batch: 0.372145  [   26/   89]
Per-example loss in batch: 0.346297  [   28/   89]
Per-example loss in batch: 0.332521  [   30/   89]
Per-example loss in batch: 0.352516  [   32/   89]
Per-example loss in batch: 0.385381  [   34/   89]
Per-example loss in batch: 0.320201  [   36/   89]
Per-example loss in batch: 0.404105  [   38/   89]
Per-example loss in batch: 0.277224  [   40/   89]
Per-example loss in batch: 0.378691  [   42/   89]
Per-example loss in batch: 0.374255  [   44/   89]
Per-example loss in batch: 0.329839  [   46/   89]
Per-example loss in batch: 0.353877  [   48/   89]
Per-example loss in batch: 0.310236  [   50/   89]
Per-example loss in batch: 0.340199  [   52/   89]
Per-example loss in batch: 0.359801  [   54/   89]
Per-example loss in batch: 0.365860  [   56/   89]
Per-example loss in batch: 0.376821  [   58/   89]
Per-example loss in batch: 0.354443  [   60/   89]
Per-example loss in batch: 0.305032  [   62/   89]
Per-example loss in batch: 0.424585  [   64/   89]
Per-example loss in batch: 0.328894  [   66/   89]
Per-example loss in batch: 0.385929  [   68/   89]
Per-example loss in batch: 0.370709  [   70/   89]
Per-example loss in batch: 0.377470  [   72/   89]
Per-example loss in batch: 0.310100  [   74/   89]
Per-example loss in batch: 0.394891  [   76/   89]
Per-example loss in batch: 0.343915  [   78/   89]
Per-example loss in batch: 0.368999  [   80/   89]
Per-example loss in batch: 0.363234  [   82/   89]
Per-example loss in batch: 0.405821  [   84/   89]
Per-example loss in batch: 0.317663  [   86/   89]
Per-example loss in batch: 0.402322  [   88/   89]
Per-example loss in batch: 0.837582  [   89/   89]
Train Error: Avg loss: 0.36459489
validation Error: 
 Avg loss: 0.50891269 
 F1: 0.303755 
 Precision: 0.502876 
 Recall: 0.217595
 IoU: 0.179075

test Error: 
 Avg loss: 0.49029666 
 F1: 0.331022 
 Precision: 0.485071 
 Recall: 0.251235
 IoU: 0.198338

We have finished training iteration 126
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_124_.pth
Per-example loss in batch: 0.352297  [    2/   89]
Per-example loss in batch: 0.282518  [    4/   89]
Per-example loss in batch: 0.401757  [    6/   89]
Per-example loss in batch: 0.354167  [    8/   89]
Per-example loss in batch: 0.339514  [   10/   89]
Per-example loss in batch: 0.402993  [   12/   89]
Per-example loss in batch: 0.419997  [   14/   89]
Per-example loss in batch: 0.361928  [   16/   89]
Per-example loss in batch: 0.354306  [   18/   89]
Per-example loss in batch: 0.339103  [   20/   89]
Per-example loss in batch: 0.316134  [   22/   89]
Per-example loss in batch: 0.372051  [   24/   89]
Per-example loss in batch: 0.384373  [   26/   89]
Per-example loss in batch: 0.286633  [   28/   89]
Per-example loss in batch: 0.365734  [   30/   89]
Per-example loss in batch: 0.356911  [   32/   89]
Per-example loss in batch: 0.316432  [   34/   89]
Per-example loss in batch: 0.368689  [   36/   89]
Per-example loss in batch: 0.310161  [   38/   89]
Per-example loss in batch: 0.319879  [   40/   89]
Per-example loss in batch: 0.328337  [   42/   89]
Per-example loss in batch: 0.351196  [   44/   89]
Per-example loss in batch: 0.409166  [   46/   89]
Per-example loss in batch: 0.349774  [   48/   89]
Per-example loss in batch: 0.410478  [   50/   89]
Per-example loss in batch: 0.333315  [   52/   89]
Per-example loss in batch: 0.345351  [   54/   89]
Per-example loss in batch: 0.352569  [   56/   89]
Per-example loss in batch: 0.343278  [   58/   89]
Per-example loss in batch: 0.299968  [   60/   89]
Per-example loss in batch: 0.396490  [   62/   89]
Per-example loss in batch: 0.402753  [   64/   89]
Per-example loss in batch: 0.398216  [   66/   89]
Per-example loss in batch: 0.314118  [   68/   89]
Per-example loss in batch: 0.376779  [   70/   89]
Per-example loss in batch: 0.359636  [   72/   89]
Per-example loss in batch: 0.388924  [   74/   89]
Per-example loss in batch: 0.376603  [   76/   89]
Per-example loss in batch: 0.350100  [   78/   89]
Per-example loss in batch: 0.385940  [   80/   89]
Per-example loss in batch: 0.408073  [   82/   89]
Per-example loss in batch: 0.376485  [   84/   89]
Per-example loss in batch: 0.300007  [   86/   89]
Per-example loss in batch: 0.346756  [   88/   89]
Per-example loss in batch: 0.493958  [   89/   89]
Train Error: Avg loss: 0.35858133
validation Error: 
 Avg loss: 0.50946454 
 F1: 0.292957 
 Precision: 0.545640 
 Recall: 0.200231
 IoU: 0.171617

test Error: 
 Avg loss: 0.49005447 
 F1: 0.318540 
 Precision: 0.543487 
 Recall: 0.225292
 IoU: 0.189442

We have finished training iteration 127
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_125_.pth
Per-example loss in batch: 0.266190  [    2/   89]
Per-example loss in batch: 0.337894  [    4/   89]
Per-example loss in batch: 0.324775  [    6/   89]
Per-example loss in batch: 0.422754  [    8/   89]
Per-example loss in batch: 0.320896  [   10/   89]
Per-example loss in batch: 0.351181  [   12/   89]
Per-example loss in batch: 0.395496  [   14/   89]
Per-example loss in batch: 0.354906  [   16/   89]
Per-example loss in batch: 0.313531  [   18/   89]
Per-example loss in batch: 0.323236  [   20/   89]
Per-example loss in batch: 0.410106  [   22/   89]
Per-example loss in batch: 0.381106  [   24/   89]
Per-example loss in batch: 0.390091  [   26/   89]
Per-example loss in batch: 0.359781  [   28/   89]
Per-example loss in batch: 0.426076  [   30/   89]
Per-example loss in batch: 0.352426  [   32/   89]
Per-example loss in batch: 0.403314  [   34/   89]
Per-example loss in batch: 0.335902  [   36/   89]
Per-example loss in batch: 0.341566  [   38/   89]
Per-example loss in batch: 0.267419  [   40/   89]
Per-example loss in batch: 0.329990  [   42/   89]
Per-example loss in batch: 0.359729  [   44/   89]
Per-example loss in batch: 0.276094  [   46/   89]
Per-example loss in batch: 0.408410  [   48/   89]
Per-example loss in batch: 0.348439  [   50/   89]
Per-example loss in batch: 0.392073  [   52/   89]
Per-example loss in batch: 0.324127  [   54/   89]
Per-example loss in batch: 0.331626  [   56/   89]
Per-example loss in batch: 0.336228  [   58/   89]
Per-example loss in batch: 0.358290  [   60/   89]
Per-example loss in batch: 0.342388  [   62/   89]
Per-example loss in batch: 0.357366  [   64/   89]
Per-example loss in batch: 0.308600  [   66/   89]
Per-example loss in batch: 0.412966  [   68/   89]
Per-example loss in batch: 0.414716  [   70/   89]
Per-example loss in batch: 0.330204  [   72/   89]
Per-example loss in batch: 0.336324  [   74/   89]
Per-example loss in batch: 0.347631  [   76/   89]
Per-example loss in batch: 0.360090  [   78/   89]
Per-example loss in batch: 0.359983  [   80/   89]
Per-example loss in batch: 0.383056  [   82/   89]
Per-example loss in batch: 0.259411  [   84/   89]
Per-example loss in batch: 0.385098  [   86/   89]
Per-example loss in batch: 0.349255  [   88/   89]
Per-example loss in batch: 0.670245  [   89/   89]
Train Error: Avg loss: 0.35563739
validation Error: 
 Avg loss: 0.50973963 
 F1: 0.307182 
 Precision: 0.516448 
 Recall: 0.218604
 IoU: 0.181462

test Error: 
 Avg loss: 0.49014389 
 F1: 0.330651 
 Precision: 0.501183 
 Recall: 0.246707
 IoU: 0.198072

We have finished training iteration 128
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_126_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.420492  [    2/   89]
Per-example loss in batch: 0.367654  [    4/   89]
Per-example loss in batch: 0.321232  [    6/   89]
Per-example loss in batch: 0.373729  [    8/   89]
Per-example loss in batch: 0.313019  [   10/   89]
Per-example loss in batch: 0.319406  [   12/   89]
Per-example loss in batch: 0.357778  [   14/   89]
Per-example loss in batch: 0.281938  [   16/   89]
Per-example loss in batch: 0.355517  [   18/   89]
Per-example loss in batch: 0.366634  [   20/   89]
Per-example loss in batch: 0.379687  [   22/   89]
Per-example loss in batch: 0.353883  [   24/   89]
Per-example loss in batch: 0.340217  [   26/   89]
Per-example loss in batch: 0.370220  [   28/   89]
Per-example loss in batch: 0.364010  [   30/   89]
Per-example loss in batch: 0.338093  [   32/   89]
Per-example loss in batch: 0.355949  [   34/   89]
Per-example loss in batch: 0.333638  [   36/   89]
Per-example loss in batch: 0.386003  [   38/   89]
Per-example loss in batch: 0.301474  [   40/   89]
Per-example loss in batch: 0.339883  [   42/   89]
Per-example loss in batch: 0.386932  [   44/   89]
Per-example loss in batch: 0.361948  [   46/   89]
Per-example loss in batch: 0.343851  [   48/   89]
Per-example loss in batch: 0.363993  [   50/   89]
Per-example loss in batch: 0.348029  [   52/   89]
Per-example loss in batch: 0.421258  [   54/   89]
Per-example loss in batch: 0.324849  [   56/   89]
Per-example loss in batch: 0.312153  [   58/   89]
Per-example loss in batch: 0.379745  [   60/   89]
Per-example loss in batch: 0.308255  [   62/   89]
Per-example loss in batch: 0.323795  [   64/   89]
Per-example loss in batch: 0.289480  [   66/   89]
Per-example loss in batch: 0.350047  [   68/   89]
Per-example loss in batch: 0.386319  [   70/   89]
Per-example loss in batch: 0.409332  [   72/   89]
Per-example loss in batch: 0.300282  [   74/   89]
Per-example loss in batch: 0.342246  [   76/   89]
Per-example loss in batch: 0.309974  [   78/   89]
Per-example loss in batch: 0.330565  [   80/   89]
Per-example loss in batch: 0.360288  [   82/   89]
Per-example loss in batch: 0.421084  [   84/   89]
Per-example loss in batch: 0.377182  [   86/   89]
Per-example loss in batch: 0.417099  [   88/   89]
Per-example loss in batch: 0.799001  [   89/   89]
Train Error: Avg loss: 0.35749809
validation Error: 
 Avg loss: 0.51002195 
 F1: 0.295942 
 Precision: 0.523846 
 Recall: 0.206223
 IoU: 0.173669

test Error: 
 Avg loss: 0.49086217 
 F1: 0.309492 
 Precision: 0.482575 
 Recall: 0.227791
 IoU: 0.183076

We have finished training iteration 129
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_127_.pth
Per-example loss in batch: 0.390156  [    2/   89]
Per-example loss in batch: 0.374939  [    4/   89]
Per-example loss in batch: 0.335840  [    6/   89]
Per-example loss in batch: 0.328487  [    8/   89]
Per-example loss in batch: 0.416893  [   10/   89]
Per-example loss in batch: 0.406814  [   12/   89]
Per-example loss in batch: 0.400254  [   14/   89]
Per-example loss in batch: 0.360226  [   16/   89]
Per-example loss in batch: 0.342118  [   18/   89]
Per-example loss in batch: 0.312512  [   20/   89]
Per-example loss in batch: 0.375776  [   22/   89]
Per-example loss in batch: 0.400230  [   24/   89]
Per-example loss in batch: 0.343086  [   26/   89]
Per-example loss in batch: 0.422026  [   28/   89]
Per-example loss in batch: 0.325770  [   30/   89]
Per-example loss in batch: 0.309080  [   32/   89]
Per-example loss in batch: 0.352706  [   34/   89]
Per-example loss in batch: 0.342988  [   36/   89]
Per-example loss in batch: 0.342363  [   38/   89]
Per-example loss in batch: 0.341689  [   40/   89]
Per-example loss in batch: 0.310372  [   42/   89]
Per-example loss in batch: 0.380060  [   44/   89]
Per-example loss in batch: 0.438225  [   46/   89]
Per-example loss in batch: 0.292008  [   48/   89]
Per-example loss in batch: 0.296773  [   50/   89]
Per-example loss in batch: 0.425838  [   52/   89]
Per-example loss in batch: 0.371028  [   54/   89]
Per-example loss in batch: 0.372641  [   56/   89]
Per-example loss in batch: 0.419268  [   58/   89]
Per-example loss in batch: 0.322095  [   60/   89]
Per-example loss in batch: 0.312185  [   62/   89]
Per-example loss in batch: 0.370751  [   64/   89]
Per-example loss in batch: 0.352645  [   66/   89]
Per-example loss in batch: 0.331148  [   68/   89]
Per-example loss in batch: 0.322447  [   70/   89]
Per-example loss in batch: 0.316485  [   72/   89]
Per-example loss in batch: 0.283319  [   74/   89]
Per-example loss in batch: 0.348273  [   76/   89]
Per-example loss in batch: 0.382336  [   78/   89]
Per-example loss in batch: 0.372033  [   80/   89]
Per-example loss in batch: 0.366391  [   82/   89]
Per-example loss in batch: 0.336822  [   84/   89]
Per-example loss in batch: 0.359637  [   86/   89]
Per-example loss in batch: 0.348983  [   88/   89]
Per-example loss in batch: 0.792102  [   89/   89]
Train Error: Avg loss: 0.36071391
validation Error: 
 Avg loss: 0.50941615 
 F1: 0.310138 
 Precision: 0.500732 
 Recall: 0.224635
 IoU: 0.183528

test Error: 
 Avg loss: 0.48977196 
 F1: 0.339322 
 Precision: 0.488744 
 Recall: 0.259872
 IoU: 0.204328

We have finished training iteration 130
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_128_.pth
Per-example loss in batch: 0.426717  [    2/   89]
Per-example loss in batch: 0.390381  [    4/   89]
Per-example loss in batch: 0.356690  [    6/   89]
Per-example loss in batch: 0.303381  [    8/   89]
Per-example loss in batch: 0.387012  [   10/   89]
Per-example loss in batch: 0.285003  [   12/   89]
Per-example loss in batch: 0.358779  [   14/   89]
Per-example loss in batch: 0.338990  [   16/   89]
Per-example loss in batch: 0.357467  [   18/   89]
Per-example loss in batch: 0.366452  [   20/   89]
Per-example loss in batch: 0.318314  [   22/   89]
Per-example loss in batch: 0.379669  [   24/   89]
Per-example loss in batch: 0.398671  [   26/   89]
Per-example loss in batch: 0.374974  [   28/   89]
Per-example loss in batch: 0.318110  [   30/   89]
Per-example loss in batch: 0.342221  [   32/   89]
Per-example loss in batch: 0.320630  [   34/   89]
Per-example loss in batch: 0.400727  [   36/   89]
Per-example loss in batch: 0.341703  [   38/   89]
Per-example loss in batch: 0.290983  [   40/   89]
Per-example loss in batch: 0.307016  [   42/   89]
Per-example loss in batch: 0.363982  [   44/   89]
Per-example loss in batch: 0.333517  [   46/   89]
Per-example loss in batch: 0.395158  [   48/   89]
Per-example loss in batch: 0.349848  [   50/   89]
Per-example loss in batch: 0.405539  [   52/   89]
Per-example loss in batch: 0.304914  [   54/   89]
Per-example loss in batch: 0.401327  [   56/   89]
Per-example loss in batch: 0.322113  [   58/   89]
Per-example loss in batch: 0.396590  [   60/   89]
Per-example loss in batch: 0.262610  [   62/   89]
Per-example loss in batch: 0.264488  [   64/   89]
Per-example loss in batch: 0.342239  [   66/   89]
Per-example loss in batch: 0.286519  [   68/   89]
Per-example loss in batch: 0.307502  [   70/   89]
Per-example loss in batch: 0.354712  [   72/   89]
Per-example loss in batch: 0.386432  [   74/   89]
Per-example loss in batch: 0.372240  [   76/   89]
Per-example loss in batch: 0.369769  [   78/   89]
Per-example loss in batch: 0.420154  [   80/   89]
Per-example loss in batch: 0.374796  [   82/   89]
Per-example loss in batch: 0.356739  [   84/   89]
Per-example loss in batch: 0.382497  [   86/   89]
Per-example loss in batch: 0.433194  [   88/   89]
Per-example loss in batch: 0.813863  [   89/   89]
Train Error: Avg loss: 0.35860000
validation Error: 
 Avg loss: 0.50835030 
 F1: 0.291327 
 Precision: 0.590327 
 Recall: 0.193380
 IoU: 0.170499

test Error: 
 Avg loss: 0.48922105 
 F1: 0.318057 
 Precision: 0.583068 
 Recall: 0.218670
 IoU: 0.189101

We have finished training iteration 131
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_129_.pth
Per-example loss in batch: 0.357606  [    2/   89]
Per-example loss in batch: 0.398695  [    4/   89]
Per-example loss in batch: 0.307837  [    6/   89]
Per-example loss in batch: 0.448796  [    8/   89]
Per-example loss in batch: 0.387710  [   10/   89]
Per-example loss in batch: 0.310253  [   12/   89]
Per-example loss in batch: 0.340201  [   14/   89]
Per-example loss in batch: 0.367619  [   16/   89]
Per-example loss in batch: 0.337440  [   18/   89]
Per-example loss in batch: 0.326563  [   20/   89]
Per-example loss in batch: 0.316395  [   22/   89]
Per-example loss in batch: 0.388498  [   24/   89]
Per-example loss in batch: 0.377421  [   26/   89]
Per-example loss in batch: 0.378392  [   28/   89]
Per-example loss in batch: 0.355328  [   30/   89]
Per-example loss in batch: 0.403535  [   32/   89]
Per-example loss in batch: 0.311861  [   34/   89]
Per-example loss in batch: 0.418452  [   36/   89]
Per-example loss in batch: 0.356869  [   38/   89]
Per-example loss in batch: 0.387450  [   40/   89]
Per-example loss in batch: 0.317050  [   42/   89]
Per-example loss in batch: 0.357197  [   44/   89]
Per-example loss in batch: 0.367970  [   46/   89]
Per-example loss in batch: 0.287915  [   48/   89]
Per-example loss in batch: 0.315584  [   50/   89]
Per-example loss in batch: 0.347364  [   52/   89]
Per-example loss in batch: 0.323577  [   54/   89]
Per-example loss in batch: 0.318220  [   56/   89]
Per-example loss in batch: 0.331733  [   58/   89]
Per-example loss in batch: 0.377101  [   60/   89]
Per-example loss in batch: 0.344544  [   62/   89]
Per-example loss in batch: 0.372950  [   64/   89]
Per-example loss in batch: 0.409746  [   66/   89]
Per-example loss in batch: 0.393501  [   68/   89]
Per-example loss in batch: 0.432418  [   70/   89]
Per-example loss in batch: 0.340085  [   72/   89]
Per-example loss in batch: 0.398724  [   74/   89]
Per-example loss in batch: 0.292170  [   76/   89]
Per-example loss in batch: 0.262801  [   78/   89]
Per-example loss in batch: 0.313181  [   80/   89]
Per-example loss in batch: 0.337117  [   82/   89]
Per-example loss in batch: 0.361804  [   84/   89]
Per-example loss in batch: 0.342522  [   86/   89]
Per-example loss in batch: 0.360811  [   88/   89]
Per-example loss in batch: 0.544288  [   89/   89]
Train Error: Avg loss: 0.35629553
validation Error: 
 Avg loss: 0.50909975 
 F1: 0.311623 
 Precision: 0.509255 
 Recall: 0.224500
 IoU: 0.184570

test Error: 
 Avg loss: 0.48972466 
 F1: 0.341785 
 Precision: 0.501922 
 Recall: 0.259115
 IoU: 0.206116

We have finished training iteration 132
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_130_.pth
Per-example loss in batch: 0.357198  [    2/   89]
Per-example loss in batch: 0.363785  [    4/   89]
Per-example loss in batch: 0.338856  [    6/   89]
Per-example loss in batch: 0.304603  [    8/   89]
Per-example loss in batch: 0.381955  [   10/   89]
Per-example loss in batch: 0.358747  [   12/   89]
Per-example loss in batch: 0.386732  [   14/   89]
Per-example loss in batch: 0.272271  [   16/   89]
Per-example loss in batch: 0.412564  [   18/   89]
Per-example loss in batch: 0.357914  [   20/   89]
Per-example loss in batch: 0.289267  [   22/   89]
Per-example loss in batch: 0.279873  [   24/   89]
Per-example loss in batch: 0.311917  [   26/   89]
Per-example loss in batch: 0.389898  [   28/   89]
Per-example loss in batch: 0.444933  [   30/   89]
Per-example loss in batch: 0.340280  [   32/   89]
Per-example loss in batch: 0.380166  [   34/   89]
Per-example loss in batch: 0.399465  [   36/   89]
Per-example loss in batch: 0.293995  [   38/   89]
Per-example loss in batch: 0.317403  [   40/   89]
Per-example loss in batch: 0.338167  [   42/   89]
Per-example loss in batch: 0.354536  [   44/   89]
Per-example loss in batch: 0.364780  [   46/   89]
Per-example loss in batch: 0.323657  [   48/   89]
Per-example loss in batch: 0.378758  [   50/   89]
Per-example loss in batch: 0.352138  [   52/   89]
Per-example loss in batch: 0.321735  [   54/   89]
Per-example loss in batch: 0.303515  [   56/   89]
Per-example loss in batch: 0.364012  [   58/   89]
Per-example loss in batch: 0.406286  [   60/   89]
Per-example loss in batch: 0.342847  [   62/   89]
Per-example loss in batch: 0.367235  [   64/   89]
Per-example loss in batch: 0.269642  [   66/   89]
Per-example loss in batch: 0.334960  [   68/   89]
Per-example loss in batch: 0.321356  [   70/   89]
Per-example loss in batch: 0.364845  [   72/   89]
Per-example loss in batch: 0.361235  [   74/   89]
Per-example loss in batch: 0.323295  [   76/   89]
Per-example loss in batch: 0.339496  [   78/   89]
Per-example loss in batch: 0.355988  [   80/   89]
Per-example loss in batch: 0.363450  [   82/   89]
Per-example loss in batch: 0.368663  [   84/   89]
Per-example loss in batch: 0.379632  [   86/   89]
Per-example loss in batch: 0.327286  [   88/   89]
Per-example loss in batch: 0.795994  [   89/   89]
Train Error: Avg loss: 0.35297372
validation Error: 
 Avg loss: 0.50940048 
 F1: 0.300983 
 Precision: 0.487890 
 Recall: 0.217616
 IoU: 0.177152

test Error: 
 Avg loss: 0.49053524 
 F1: 0.326760 
 Precision: 0.469743 
 Recall: 0.250508
 IoU: 0.195285

We have finished training iteration 133
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_131_.pth
Per-example loss in batch: 0.363582  [    2/   89]
Per-example loss in batch: 0.316028  [    4/   89]
Per-example loss in batch: 0.329561  [    6/   89]
Per-example loss in batch: 0.424108  [    8/   89]
Per-example loss in batch: 0.335567  [   10/   89]
Per-example loss in batch: 0.298838  [   12/   89]
Per-example loss in batch: 0.271429  [   14/   89]
Per-example loss in batch: 0.361846  [   16/   89]
Per-example loss in batch: 0.371579  [   18/   89]
Per-example loss in batch: 0.341128  [   20/   89]
Per-example loss in batch: 0.282730  [   22/   89]
Per-example loss in batch: 0.366934  [   24/   89]
Per-example loss in batch: 0.330152  [   26/   89]
Per-example loss in batch: 0.379279  [   28/   89]
Per-example loss in batch: 0.269078  [   30/   89]
Per-example loss in batch: 0.372331  [   32/   89]
Per-example loss in batch: 0.306284  [   34/   89]
Per-example loss in batch: 0.393385  [   36/   89]
Per-example loss in batch: 0.311744  [   38/   89]
Per-example loss in batch: 0.407611  [   40/   89]
Per-example loss in batch: 0.333215  [   42/   89]
Per-example loss in batch: 0.276501  [   44/   89]
Per-example loss in batch: 0.344379  [   46/   89]
Per-example loss in batch: 0.421933  [   48/   89]
Per-example loss in batch: 0.351232  [   50/   89]
Per-example loss in batch: 0.351769  [   52/   89]
Per-example loss in batch: 0.363373  [   54/   89]
Per-example loss in batch: 0.354170  [   56/   89]
Per-example loss in batch: 0.371377  [   58/   89]
Per-example loss in batch: 0.301322  [   60/   89]
Per-example loss in batch: 0.375459  [   62/   89]
Per-example loss in batch: 0.342938  [   64/   89]
Per-example loss in batch: 0.391999  [   66/   89]
Per-example loss in batch: 0.313891  [   68/   89]
Per-example loss in batch: 0.334913  [   70/   89]
Per-example loss in batch: 0.402185  [   72/   89]
Per-example loss in batch: 0.360941  [   74/   89]
Per-example loss in batch: 0.401823  [   76/   89]
Per-example loss in batch: 0.310754  [   78/   89]
Per-example loss in batch: 0.362176  [   80/   89]
Per-example loss in batch: 0.318522  [   82/   89]
Per-example loss in batch: 0.314542  [   84/   89]
Per-example loss in batch: 0.364854  [   86/   89]
Per-example loss in batch: 0.428353  [   88/   89]
Per-example loss in batch: 0.790882  [   89/   89]
Train Error: Avg loss: 0.35328664
validation Error: 
 Avg loss: 0.50973183 
 F1: 0.312237 
 Precision: 0.484896 
 Recall: 0.230250
 IoU: 0.185000

test Error: 
 Avg loss: 0.49014615 
 F1: 0.332095 
 Precision: 0.463255 
 Recall: 0.258817
 IoU: 0.199109

We have finished training iteration 134
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_132_.pth
Per-example loss in batch: 0.311487  [    2/   89]
Per-example loss in batch: 0.303971  [    4/   89]
Per-example loss in batch: 0.385098  [    6/   89]
Per-example loss in batch: 0.347505  [    8/   89]
Per-example loss in batch: 0.279906  [   10/   89]
Per-example loss in batch: 0.263153  [   12/   89]
Per-example loss in batch: 0.293270  [   14/   89]
Per-example loss in batch: 0.366442  [   16/   89]
Per-example loss in batch: 0.348645  [   18/   89]
Per-example loss in batch: 0.358622  [   20/   89]
Per-example loss in batch: 0.296614  [   22/   89]
Per-example loss in batch: 0.323030  [   24/   89]
Per-example loss in batch: 0.375849  [   26/   89]
Per-example loss in batch: 0.297515  [   28/   89]
Per-example loss in batch: 0.333169  [   30/   89]
Per-example loss in batch: 0.397772  [   32/   89]
Per-example loss in batch: 0.396060  [   34/   89]
Per-example loss in batch: 0.356314  [   36/   89]
Per-example loss in batch: 0.370231  [   38/   89]
Per-example loss in batch: 0.442998  [   40/   89]
Per-example loss in batch: 0.357499  [   42/   89]
Per-example loss in batch: 0.373756  [   44/   89]
Per-example loss in batch: 0.393123  [   46/   89]
Per-example loss in batch: 0.335648  [   48/   89]
Per-example loss in batch: 0.327309  [   50/   89]
Per-example loss in batch: 0.313234  [   52/   89]
Per-example loss in batch: 0.354494  [   54/   89]
Per-example loss in batch: 0.323780  [   56/   89]
Per-example loss in batch: 0.348985  [   58/   89]
Per-example loss in batch: 0.322870  [   60/   89]
Per-example loss in batch: 0.354209  [   62/   89]
Per-example loss in batch: 0.290990  [   64/   89]
Per-example loss in batch: 0.342895  [   66/   89]
Per-example loss in batch: 0.353602  [   68/   89]
Per-example loss in batch: 0.329908  [   70/   89]
Per-example loss in batch: 0.387816  [   72/   89]
Per-example loss in batch: 0.382175  [   74/   89]
Per-example loss in batch: 0.374025  [   76/   89]
Per-example loss in batch: 0.330977  [   78/   89]
Per-example loss in batch: 0.382627  [   80/   89]
Per-example loss in batch: 0.363629  [   82/   89]
Per-example loss in batch: 0.427979  [   84/   89]
Per-example loss in batch: 0.285646  [   86/   89]
Per-example loss in batch: 0.298448  [   88/   89]
Per-example loss in batch: 0.794325  [   89/   89]
Train Error: Avg loss: 0.35057166
validation Error: 
 Avg loss: 0.50852031 
 F1: 0.326812 
 Precision: 0.500025 
 Recall: 0.242728
 IoU: 0.195323

test Error: 
 Avg loss: 0.48955589 
 F1: 0.344884 
 Precision: 0.499303 
 Recall: 0.263417
 IoU: 0.208374

We have finished training iteration 135
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_133_.pth
Per-example loss in batch: 0.388370  [    2/   89]
Per-example loss in batch: 0.337097  [    4/   89]
Per-example loss in batch: 0.318567  [    6/   89]
Per-example loss in batch: 0.380086  [    8/   89]
Per-example loss in batch: 0.336842  [   10/   89]
Per-example loss in batch: 0.384778  [   12/   89]
Per-example loss in batch: 0.398456  [   14/   89]
Per-example loss in batch: 0.352351  [   16/   89]
Per-example loss in batch: 0.353151  [   18/   89]
Per-example loss in batch: 0.267527  [   20/   89]
Per-example loss in batch: 0.371408  [   22/   89]
Per-example loss in batch: 0.289614  [   24/   89]
Per-example loss in batch: 0.388900  [   26/   89]
Per-example loss in batch: 0.351176  [   28/   89]
Per-example loss in batch: 0.298789  [   30/   89]
Per-example loss in batch: 0.359121  [   32/   89]
Per-example loss in batch: 0.342423  [   34/   89]
Per-example loss in batch: 0.313037  [   36/   89]
Per-example loss in batch: 0.333290  [   38/   89]
Per-example loss in batch: 0.382162  [   40/   89]
Per-example loss in batch: 0.389316  [   42/   89]
Per-example loss in batch: 0.433784  [   44/   89]
Per-example loss in batch: 0.388047  [   46/   89]
Per-example loss in batch: 0.291373  [   48/   89]
Per-example loss in batch: 0.340193  [   50/   89]
Per-example loss in batch: 0.352753  [   52/   89]
Per-example loss in batch: 0.315626  [   54/   89]
Per-example loss in batch: 0.332618  [   56/   89]
Per-example loss in batch: 0.325706  [   58/   89]
Per-example loss in batch: 0.291450  [   60/   89]
Per-example loss in batch: 0.319658  [   62/   89]
Per-example loss in batch: 0.305313  [   64/   89]
Per-example loss in batch: 0.451206  [   66/   89]
Per-example loss in batch: 0.373329  [   68/   89]
Per-example loss in batch: 0.377142  [   70/   89]
Per-example loss in batch: 0.345520  [   72/   89]
Per-example loss in batch: 0.378626  [   74/   89]
Per-example loss in batch: 0.262564  [   76/   89]
Per-example loss in batch: 0.267481  [   78/   89]
Per-example loss in batch: 0.336025  [   80/   89]
Per-example loss in batch: 0.364756  [   82/   89]
Per-example loss in batch: 0.324195  [   84/   89]
Per-example loss in batch: 0.381403  [   86/   89]
Per-example loss in batch: 0.397805  [   88/   89]
Per-example loss in batch: 0.677940  [   89/   89]
Train Error: Avg loss: 0.35128098
validation Error: 
 Avg loss: 0.50937413 
 F1: 0.319321 
 Precision: 0.546417 
 Recall: 0.225571
 IoU: 0.189995

test Error: 
 Avg loss: 0.48980442 
 F1: 0.339930 
 Precision: 0.531153 
 Recall: 0.249946
 IoU: 0.204769

We have finished training iteration 136
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_134_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.325538  [    2/   89]
Per-example loss in batch: 0.420424  [    4/   89]
Per-example loss in batch: 0.314291  [    6/   89]
Per-example loss in batch: 0.341500  [    8/   89]
Per-example loss in batch: 0.397496  [   10/   89]
Per-example loss in batch: 0.417282  [   12/   89]
Per-example loss in batch: 0.323107  [   14/   89]
Per-example loss in batch: 0.337652  [   16/   89]
Per-example loss in batch: 0.286316  [   18/   89]
Per-example loss in batch: 0.298072  [   20/   89]
Per-example loss in batch: 0.378420  [   22/   89]
Per-example loss in batch: 0.400815  [   24/   89]
Per-example loss in batch: 0.408128  [   26/   89]
Per-example loss in batch: 0.325282  [   28/   89]
Per-example loss in batch: 0.357514  [   30/   89]
Per-example loss in batch: 0.345116  [   32/   89]
Per-example loss in batch: 0.406401  [   34/   89]
Per-example loss in batch: 0.284795  [   36/   89]
Per-example loss in batch: 0.347317  [   38/   89]
Per-example loss in batch: 0.389628  [   40/   89]
Per-example loss in batch: 0.291956  [   42/   89]
Per-example loss in batch: 0.342301  [   44/   89]
Per-example loss in batch: 0.352446  [   46/   89]
Per-example loss in batch: 0.349216  [   48/   89]
Per-example loss in batch: 0.358747  [   50/   89]
Per-example loss in batch: 0.268009  [   52/   89]
Per-example loss in batch: 0.285725  [   54/   89]
Per-example loss in batch: 0.358418  [   56/   89]
Per-example loss in batch: 0.401808  [   58/   89]
Per-example loss in batch: 0.326912  [   60/   89]
Per-example loss in batch: 0.300251  [   62/   89]
Per-example loss in batch: 0.379404  [   64/   89]
Per-example loss in batch: 0.435531  [   66/   89]
Per-example loss in batch: 0.369371  [   68/   89]
Per-example loss in batch: 0.317777  [   70/   89]
Per-example loss in batch: 0.311870  [   72/   89]
Per-example loss in batch: 0.266251  [   74/   89]
Per-example loss in batch: 0.342046  [   76/   89]
Per-example loss in batch: 0.321707  [   78/   89]
Per-example loss in batch: 0.294248  [   80/   89]
Per-example loss in batch: 0.318821  [   82/   89]
Per-example loss in batch: 0.365690  [   84/   89]
Per-example loss in batch: 0.342494  [   86/   89]
Per-example loss in batch: 0.324080  [   88/   89]
Per-example loss in batch: 0.514960  [   89/   89]
Train Error: Avg loss: 0.34578993
validation Error: 
 Avg loss: 0.50886492 
 F1: 0.329359 
 Precision: 0.451864 
 Recall: 0.259111
 IoU: 0.197145

test Error: 
 Avg loss: 0.49011103 
 F1: 0.346000 
 Precision: 0.443021 
 Recall: 0.283840
 IoU: 0.209190

We have finished training iteration 137
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_135_.pth
Per-example loss in batch: 0.294182  [    2/   89]
Per-example loss in batch: 0.270653  [    4/   89]
Per-example loss in batch: 0.290005  [    6/   89]
Per-example loss in batch: 0.381328  [    8/   89]
Per-example loss in batch: 0.277422  [   10/   89]
Per-example loss in batch: 0.373769  [   12/   89]
Per-example loss in batch: 0.382893  [   14/   89]
Per-example loss in batch: 0.274125  [   16/   89]
Per-example loss in batch: 0.364903  [   18/   89]
Per-example loss in batch: 0.329207  [   20/   89]
Per-example loss in batch: 0.385671  [   22/   89]
Per-example loss in batch: 0.369526  [   24/   89]
Per-example loss in batch: 0.418022  [   26/   89]
Per-example loss in batch: 0.348449  [   28/   89]
Per-example loss in batch: 0.293718  [   30/   89]
Per-example loss in batch: 0.370925  [   32/   89]
Per-example loss in batch: 0.379288  [   34/   89]
Per-example loss in batch: 0.339469  [   36/   89]
Per-example loss in batch: 0.388458  [   38/   89]
Per-example loss in batch: 0.350703  [   40/   89]
Per-example loss in batch: 0.379325  [   42/   89]
Per-example loss in batch: 0.336487  [   44/   89]
Per-example loss in batch: 0.331827  [   46/   89]
Per-example loss in batch: 0.356715  [   48/   89]
Per-example loss in batch: 0.304243  [   50/   89]
Per-example loss in batch: 0.334176  [   52/   89]
Per-example loss in batch: 0.297675  [   54/   89]
Per-example loss in batch: 0.369158  [   56/   89]
Per-example loss in batch: 0.307452  [   58/   89]
Per-example loss in batch: 0.391074  [   60/   89]
Per-example loss in batch: 0.401486  [   62/   89]
Per-example loss in batch: 0.371675  [   64/   89]
Per-example loss in batch: 0.275336  [   66/   89]
Per-example loss in batch: 0.279040  [   68/   89]
Per-example loss in batch: 0.272829  [   70/   89]
Per-example loss in batch: 0.261108  [   72/   89]
Per-example loss in batch: 0.279708  [   74/   89]
Per-example loss in batch: 0.425620  [   76/   89]
Per-example loss in batch: 0.406566  [   78/   89]
Per-example loss in batch: 0.406523  [   80/   89]
Per-example loss in batch: 0.352359  [   82/   89]
Per-example loss in batch: 0.334709  [   84/   89]
Per-example loss in batch: 0.311554  [   86/   89]
Per-example loss in batch: 0.361346  [   88/   89]
Per-example loss in batch: 0.906896  [   89/   89]
Train Error: Avg loss: 0.34795854
validation Error: 
 Avg loss: 0.50986434 
 F1: 0.323237 
 Precision: 0.489979 
 Recall: 0.241167
 IoU: 0.192774

test Error: 
 Avg loss: 0.49035124 
 F1: 0.343247 
 Precision: 0.475610 
 Recall: 0.268518
 IoU: 0.207181

We have finished training iteration 138
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_136_.pth
Per-example loss in batch: 0.466372  [    2/   89]
Per-example loss in batch: 0.352663  [    4/   89]
Per-example loss in batch: 0.366731  [    6/   89]
Per-example loss in batch: 0.298361  [    8/   89]
Per-example loss in batch: 0.369549  [   10/   89]
Per-example loss in batch: 0.373625  [   12/   89]
Per-example loss in batch: 0.332314  [   14/   89]
Per-example loss in batch: 0.289687  [   16/   89]
Per-example loss in batch: 0.352417  [   18/   89]
Per-example loss in batch: 0.401130  [   20/   89]
Per-example loss in batch: 0.338994  [   22/   89]
Per-example loss in batch: 0.354739  [   24/   89]
Per-example loss in batch: 0.310065  [   26/   89]
Per-example loss in batch: 0.308307  [   28/   89]
Per-example loss in batch: 0.367106  [   30/   89]
Per-example loss in batch: 0.389463  [   32/   89]
Per-example loss in batch: 0.349963  [   34/   89]
Per-example loss in batch: 0.287577  [   36/   89]
Per-example loss in batch: 0.399078  [   38/   89]
Per-example loss in batch: 0.360100  [   40/   89]
Per-example loss in batch: 0.346257  [   42/   89]
Per-example loss in batch: 0.305638  [   44/   89]
Per-example loss in batch: 0.259612  [   46/   89]
Per-example loss in batch: 0.320649  [   48/   89]
Per-example loss in batch: 0.365653  [   50/   89]
Per-example loss in batch: 0.345386  [   52/   89]
Per-example loss in batch: 0.276774  [   54/   89]
Per-example loss in batch: 0.315741  [   56/   89]
Per-example loss in batch: 0.312384  [   58/   89]
Per-example loss in batch: 0.408559  [   60/   89]
Per-example loss in batch: 0.400829  [   62/   89]
Per-example loss in batch: 0.349322  [   64/   89]
Per-example loss in batch: 0.315896  [   66/   89]
Per-example loss in batch: 0.274116  [   68/   89]
Per-example loss in batch: 0.272928  [   70/   89]
Per-example loss in batch: 0.347856  [   72/   89]
Per-example loss in batch: 0.403354  [   74/   89]
Per-example loss in batch: 0.369999  [   76/   89]
Per-example loss in batch: 0.292102  [   78/   89]
Per-example loss in batch: 0.346662  [   80/   89]
Per-example loss in batch: 0.419564  [   82/   89]
Per-example loss in batch: 0.317221  [   84/   89]
Per-example loss in batch: 0.415670  [   86/   89]
Per-example loss in batch: 0.344152  [   88/   89]
Per-example loss in batch: 0.779066  [   89/   89]
Train Error: Avg loss: 0.35020442
validation Error: 
 Avg loss: 0.50990124 
 F1: 0.334787 
 Precision: 0.460822 
 Recall: 0.262887
 IoU: 0.201048

test Error: 
 Avg loss: 0.49040869 
 F1: 0.356165 
 Precision: 0.443611 
 Recall: 0.297517
 IoU: 0.216667

We have finished training iteration 139
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_137_.pth
Per-example loss in batch: 0.379092  [    2/   89]
Per-example loss in batch: 0.290693  [    4/   89]
Per-example loss in batch: 0.290312  [    6/   89]
Per-example loss in batch: 0.246320  [    8/   89]
Per-example loss in batch: 0.391967  [   10/   89]
Per-example loss in batch: 0.332480  [   12/   89]
Per-example loss in batch: 0.381701  [   14/   89]
Per-example loss in batch: 0.318785  [   16/   89]
Per-example loss in batch: 0.317129  [   18/   89]
Per-example loss in batch: 0.339454  [   20/   89]
Per-example loss in batch: 0.387673  [   22/   89]
Per-example loss in batch: 0.271749  [   24/   89]
Per-example loss in batch: 0.360826  [   26/   89]
Per-example loss in batch: 0.395364  [   28/   89]
Per-example loss in batch: 0.368163  [   30/   89]
Per-example loss in batch: 0.361239  [   32/   89]
Per-example loss in batch: 0.291741  [   34/   89]
Per-example loss in batch: 0.352593  [   36/   89]
Per-example loss in batch: 0.376901  [   38/   89]
Per-example loss in batch: 0.351333  [   40/   89]
Per-example loss in batch: 0.342655  [   42/   89]
Per-example loss in batch: 0.306971  [   44/   89]
Per-example loss in batch: 0.343300  [   46/   89]
Per-example loss in batch: 0.364619  [   48/   89]
Per-example loss in batch: 0.284050  [   50/   89]
Per-example loss in batch: 0.440958  [   52/   89]
Per-example loss in batch: 0.334035  [   54/   89]
Per-example loss in batch: 0.311284  [   56/   89]
Per-example loss in batch: 0.287817  [   58/   89]
Per-example loss in batch: 0.367962  [   60/   89]
Per-example loss in batch: 0.326019  [   62/   89]
Per-example loss in batch: 0.396329  [   64/   89]
Per-example loss in batch: 0.398754  [   66/   89]
Per-example loss in batch: 0.342862  [   68/   89]
Per-example loss in batch: 0.386206  [   70/   89]
Per-example loss in batch: 0.316296  [   72/   89]
Per-example loss in batch: 0.313732  [   74/   89]
Per-example loss in batch: 0.296481  [   76/   89]
Per-example loss in batch: 0.424958  [   78/   89]
Per-example loss in batch: 0.322811  [   80/   89]
Per-example loss in batch: 0.286769  [   82/   89]
Per-example loss in batch: 0.343181  [   84/   89]
Per-example loss in batch: 0.348615  [   86/   89]
Per-example loss in batch: 0.305763  [   88/   89]
Per-example loss in batch: 0.801454  [   89/   89]
Train Error: Avg loss: 0.34603748
validation Error: 
 Avg loss: 0.50918406 
 F1: 0.323739 
 Precision: 0.543866 
 Recall: 0.230461
 IoU: 0.193132

test Error: 
 Avg loss: 0.48953258 
 F1: 0.346507 
 Precision: 0.534349 
 Recall: 0.256380
 IoU: 0.209561

We have finished training iteration 140
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_138_.pth
Per-example loss in batch: 0.323875  [    2/   89]
Per-example loss in batch: 0.357547  [    4/   89]
Per-example loss in batch: 0.429977  [    6/   89]
Per-example loss in batch: 0.400052  [    8/   89]
Per-example loss in batch: 0.356908  [   10/   89]
Per-example loss in batch: 0.340147  [   12/   89]
Per-example loss in batch: 0.327551  [   14/   89]
Per-example loss in batch: 0.339016  [   16/   89]
Per-example loss in batch: 0.328181  [   18/   89]
Per-example loss in batch: 0.302400  [   20/   89]
Per-example loss in batch: 0.332549  [   22/   89]
Per-example loss in batch: 0.398136  [   24/   89]
Per-example loss in batch: 0.333786  [   26/   89]
Per-example loss in batch: 0.291742  [   28/   89]
Per-example loss in batch: 0.375468  [   30/   89]
Per-example loss in batch: 0.386358  [   32/   89]
Per-example loss in batch: 0.467246  [   34/   89]
Per-example loss in batch: 0.355061  [   36/   89]
Per-example loss in batch: 0.410348  [   38/   89]
Per-example loss in batch: 0.324610  [   40/   89]
Per-example loss in batch: 0.326188  [   42/   89]
Per-example loss in batch: 0.341335  [   44/   89]
Per-example loss in batch: 0.265679  [   46/   89]
Per-example loss in batch: 0.368636  [   48/   89]
Per-example loss in batch: 0.370950  [   50/   89]
Per-example loss in batch: 0.365262  [   52/   89]
Per-example loss in batch: 0.271578  [   54/   89]
Per-example loss in batch: 0.327579  [   56/   89]
Per-example loss in batch: 0.311448  [   58/   89]
Per-example loss in batch: 0.288752  [   60/   89]
Per-example loss in batch: 0.308062  [   62/   89]
Per-example loss in batch: 0.366091  [   64/   89]
Per-example loss in batch: 0.302841  [   66/   89]
Per-example loss in batch: 0.277199  [   68/   89]
Per-example loss in batch: 0.331033  [   70/   89]
Per-example loss in batch: 0.428538  [   72/   89]
Per-example loss in batch: 0.342777  [   74/   89]
Per-example loss in batch: 0.387604  [   76/   89]
Per-example loss in batch: 0.286670  [   78/   89]
Per-example loss in batch: 0.312405  [   80/   89]
Per-example loss in batch: 0.253719  [   82/   89]
Per-example loss in batch: 0.345669  [   84/   89]
Per-example loss in batch: 0.352998  [   86/   89]
Per-example loss in batch: 0.369191  [   88/   89]
Per-example loss in batch: 0.659595  [   89/   89]
Train Error: Avg loss: 0.34635866
validation Error: 
 Avg loss: 0.50804504 
 F1: 0.334146 
 Precision: 0.540687 
 Recall: 0.241785
 IoU: 0.200586

test Error: 
 Avg loss: 0.48884205 
 F1: 0.360664 
 Precision: 0.541244 
 Recall: 0.270436
 IoU: 0.220006

We have finished training iteration 141
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_139_.pth
Per-example loss in batch: 0.319284  [    2/   89]
Per-example loss in batch: 0.356260  [    4/   89]
Per-example loss in batch: 0.375883  [    6/   89]
Per-example loss in batch: 0.367207  [    8/   89]
Per-example loss in batch: 0.339955  [   10/   89]
Per-example loss in batch: 0.353829  [   12/   89]
Per-example loss in batch: 0.353061  [   14/   89]
Per-example loss in batch: 0.342730  [   16/   89]
Per-example loss in batch: 0.288176  [   18/   89]
Per-example loss in batch: 0.317594  [   20/   89]
Per-example loss in batch: 0.305032  [   22/   89]
Per-example loss in batch: 0.284107  [   24/   89]
Per-example loss in batch: 0.317097  [   26/   89]
Per-example loss in batch: 0.312095  [   28/   89]
Per-example loss in batch: 0.375689  [   30/   89]
Per-example loss in batch: 0.283940  [   32/   89]
Per-example loss in batch: 0.401827  [   34/   89]
Per-example loss in batch: 0.311695  [   36/   89]
Per-example loss in batch: 0.285376  [   38/   89]
Per-example loss in batch: 0.352107  [   40/   89]
Per-example loss in batch: 0.392174  [   42/   89]
Per-example loss in batch: 0.321020  [   44/   89]
Per-example loss in batch: 0.256938  [   46/   89]
Per-example loss in batch: 0.409169  [   48/   89]
Per-example loss in batch: 0.320023  [   50/   89]
Per-example loss in batch: 0.290132  [   52/   89]
Per-example loss in batch: 0.374237  [   54/   89]
Per-example loss in batch: 0.361244  [   56/   89]
Per-example loss in batch: 0.266885  [   58/   89]
Per-example loss in batch: 0.348784  [   60/   89]
Per-example loss in batch: 0.358785  [   62/   89]
Per-example loss in batch: 0.348504  [   64/   89]
Per-example loss in batch: 0.298388  [   66/   89]
Per-example loss in batch: 0.356807  [   68/   89]
Per-example loss in batch: 0.296681  [   70/   89]
Per-example loss in batch: 0.386954  [   72/   89]
Per-example loss in batch: 0.439208  [   74/   89]
Per-example loss in batch: 0.338459  [   76/   89]
Per-example loss in batch: 0.322125  [   78/   89]
Per-example loss in batch: 0.380804  [   80/   89]
Per-example loss in batch: 0.393358  [   82/   89]
Per-example loss in batch: 0.395266  [   84/   89]
Per-example loss in batch: 0.349305  [   86/   89]
Per-example loss in batch: 0.294000  [   88/   89]
Per-example loss in batch: 0.586692  [   89/   89]
Train Error: Avg loss: 0.34237169
validation Error: 
 Avg loss: 0.50934893 
 F1: 0.336754 
 Precision: 0.496800 
 Recall: 0.254701
 IoU: 0.202468

test Error: 
 Avg loss: 0.49007861 
 F1: 0.350969 
 Precision: 0.475134 
 Recall: 0.278254
 IoU: 0.212833

We have finished training iteration 142
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_140_.pth
Per-example loss in batch: 0.349291  [    2/   89]
Per-example loss in batch: 0.411884  [    4/   89]
Per-example loss in batch: 0.332422  [    6/   89]
Per-example loss in batch: 0.348163  [    8/   89]
Per-example loss in batch: 0.450313  [   10/   89]
Per-example loss in batch: 0.307308  [   12/   89]
Per-example loss in batch: 0.390884  [   14/   89]
Per-example loss in batch: 0.365987  [   16/   89]
Per-example loss in batch: 0.337732  [   18/   89]
Per-example loss in batch: 0.364126  [   20/   89]
Per-example loss in batch: 0.320565  [   22/   89]
Per-example loss in batch: 0.349541  [   24/   89]
Per-example loss in batch: 0.396461  [   26/   89]
Per-example loss in batch: 0.334322  [   28/   89]
Per-example loss in batch: 0.346241  [   30/   89]
Per-example loss in batch: 0.397615  [   32/   89]
Per-example loss in batch: 0.383552  [   34/   89]
Per-example loss in batch: 0.360679  [   36/   89]
Per-example loss in batch: 0.330208  [   38/   89]
Per-example loss in batch: 0.256015  [   40/   89]
Per-example loss in batch: 0.341236  [   42/   89]
Per-example loss in batch: 0.323805  [   44/   89]
Per-example loss in batch: 0.320865  [   46/   89]
Per-example loss in batch: 0.302171  [   48/   89]
Per-example loss in batch: 0.310705  [   50/   89]
Per-example loss in batch: 0.373689  [   52/   89]
Per-example loss in batch: 0.291072  [   54/   89]
Per-example loss in batch: 0.335321  [   56/   89]
Per-example loss in batch: 0.266989  [   58/   89]
Per-example loss in batch: 0.328833  [   60/   89]
Per-example loss in batch: 0.267229  [   62/   89]
Per-example loss in batch: 0.329476  [   64/   89]
Per-example loss in batch: 0.400598  [   66/   89]
Per-example loss in batch: 0.286884  [   68/   89]
Per-example loss in batch: 0.408009  [   70/   89]
Per-example loss in batch: 0.342396  [   72/   89]
Per-example loss in batch: 0.333751  [   74/   89]
Per-example loss in batch: 0.333450  [   76/   89]
Per-example loss in batch: 0.322963  [   78/   89]
Per-example loss in batch: 0.350574  [   80/   89]
Per-example loss in batch: 0.349179  [   82/   89]
Per-example loss in batch: 0.355476  [   84/   89]
Per-example loss in batch: 0.392239  [   86/   89]
Per-example loss in batch: 0.264276  [   88/   89]
Per-example loss in batch: 0.620056  [   89/   89]
Train Error: Avg loss: 0.34549502
validation Error: 
 Avg loss: 0.50970623 
 F1: 0.334956 
 Precision: 0.497211 
 Recall: 0.252543
 IoU: 0.201169

test Error: 
 Avg loss: 0.48998357 
 F1: 0.352648 
 Precision: 0.493698 
 Recall: 0.274285
 IoU: 0.214070

We have finished training iteration 143
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_141_.pth
Per-example loss in batch: 0.353188  [    2/   89]
Per-example loss in batch: 0.331526  [    4/   89]
Per-example loss in batch: 0.279393  [    6/   89]
Per-example loss in batch: 0.280971  [    8/   89]
Per-example loss in batch: 0.274361  [   10/   89]
Per-example loss in batch: 0.389848  [   12/   89]
Per-example loss in batch: 0.377409  [   14/   89]
Per-example loss in batch: 0.406023  [   16/   89]
Per-example loss in batch: 0.382560  [   18/   89]
Per-example loss in batch: 0.394493  [   20/   89]
Per-example loss in batch: 0.352285  [   22/   89]
Per-example loss in batch: 0.320212  [   24/   89]
Per-example loss in batch: 0.337477  [   26/   89]
Per-example loss in batch: 0.419699  [   28/   89]
Per-example loss in batch: 0.281713  [   30/   89]
Per-example loss in batch: 0.348407  [   32/   89]
Per-example loss in batch: 0.336858  [   34/   89]
Per-example loss in batch: 0.343054  [   36/   89]
Per-example loss in batch: 0.397754  [   38/   89]
Per-example loss in batch: 0.319503  [   40/   89]
Per-example loss in batch: 0.362841  [   42/   89]
Per-example loss in batch: 0.335933  [   44/   89]
Per-example loss in batch: 0.372964  [   46/   89]
Per-example loss in batch: 0.417848  [   48/   89]
Per-example loss in batch: 0.333465  [   50/   89]
Per-example loss in batch: 0.442997  [   52/   89]
Per-example loss in batch: 0.351243  [   54/   89]
Per-example loss in batch: 0.329678  [   56/   89]
Per-example loss in batch: 0.317684  [   58/   89]
Per-example loss in batch: 0.284355  [   60/   89]
Per-example loss in batch: 0.271828  [   62/   89]
Per-example loss in batch: 0.408014  [   64/   89]
Per-example loss in batch: 0.343845  [   66/   89]
Per-example loss in batch: 0.395954  [   68/   89]
Per-example loss in batch: 0.362931  [   70/   89]
Per-example loss in batch: 0.378153  [   72/   89]
Per-example loss in batch: 0.335283  [   74/   89]
Per-example loss in batch: 0.389726  [   76/   89]
Per-example loss in batch: 0.326482  [   78/   89]
Per-example loss in batch: 0.274397  [   80/   89]
Per-example loss in batch: 0.312953  [   82/   89]
Per-example loss in batch: 0.275582  [   84/   89]
Per-example loss in batch: 0.323454  [   86/   89]
Per-example loss in batch: 0.351262  [   88/   89]
Per-example loss in batch: 0.795540  [   89/   89]
Train Error: Avg loss: 0.35108713
validation Error: 
 Avg loss: 0.50930635 
 F1: 0.337680 
 Precision: 0.483852 
 Recall: 0.259335
 IoU: 0.203138

test Error: 
 Avg loss: 0.48993383 
 F1: 0.356396 
 Precision: 0.472856 
 Recall: 0.285965
 IoU: 0.216838

We have finished training iteration 144
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_142_.pth
Per-example loss in batch: 0.337828  [    2/   89]
Per-example loss in batch: 0.405419  [    4/   89]
Per-example loss in batch: 0.338791  [    6/   89]
Per-example loss in batch: 0.408223  [    8/   89]
Per-example loss in batch: 0.388975  [   10/   89]
Per-example loss in batch: 0.295676  [   12/   89]
Per-example loss in batch: 0.343512  [   14/   89]
Per-example loss in batch: 0.343975  [   16/   89]
Per-example loss in batch: 0.403458  [   18/   89]
Per-example loss in batch: 0.258674  [   20/   89]
Per-example loss in batch: 0.307453  [   22/   89]
Per-example loss in batch: 0.371231  [   24/   89]
Per-example loss in batch: 0.342568  [   26/   89]
Per-example loss in batch: 0.322242  [   28/   89]
Per-example loss in batch: 0.342751  [   30/   89]
Per-example loss in batch: 0.357090  [   32/   89]
Per-example loss in batch: 0.339655  [   34/   89]
Per-example loss in batch: 0.389158  [   36/   89]
Per-example loss in batch: 0.286697  [   38/   89]
Per-example loss in batch: 0.372862  [   40/   89]
Per-example loss in batch: 0.378131  [   42/   89]
Per-example loss in batch: 0.343867  [   44/   89]
Per-example loss in batch: 0.255322  [   46/   89]
Per-example loss in batch: 0.273254  [   48/   89]
Per-example loss in batch: 0.398517  [   50/   89]
Per-example loss in batch: 0.367374  [   52/   89]
Per-example loss in batch: 0.306561  [   54/   89]
Per-example loss in batch: 0.373256  [   56/   89]
Per-example loss in batch: 0.399783  [   58/   89]
Per-example loss in batch: 0.351459  [   60/   89]
Per-example loss in batch: 0.302705  [   62/   89]
Per-example loss in batch: 0.309783  [   64/   89]
Per-example loss in batch: 0.414891  [   66/   89]
Per-example loss in batch: 0.349809  [   68/   89]
Per-example loss in batch: 0.278664  [   70/   89]
Per-example loss in batch: 0.327079  [   72/   89]
Per-example loss in batch: 0.353821  [   74/   89]
Per-example loss in batch: 0.341434  [   76/   89]
Per-example loss in batch: 0.400347  [   78/   89]
Per-example loss in batch: 0.318775  [   80/   89]
Per-example loss in batch: 0.366446  [   82/   89]
Per-example loss in batch: 0.266295  [   84/   89]
Per-example loss in batch: 0.337301  [   86/   89]
Per-example loss in batch: 0.358509  [   88/   89]
Per-example loss in batch: 0.878551  [   89/   89]
Train Error: Avg loss: 0.34986290
validation Error: 
 Avg loss: 0.51099870 
 F1: 0.321621 
 Precision: 0.418093 
 Recall: 0.261322
 IoU: 0.191626

test Error: 
 Avg loss: 0.49207844 
 F1: 0.322662 
 Precision: 0.389720 
 Recall: 0.275292
 IoU: 0.192365

We have finished training iteration 145
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_143_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.258307  [    2/   89]
Per-example loss in batch: 0.333478  [    4/   89]
Per-example loss in batch: 0.305075  [    6/   89]
Per-example loss in batch: 0.260774  [    8/   89]
Per-example loss in batch: 0.343033  [   10/   89]
Per-example loss in batch: 0.401208  [   12/   89]
Per-example loss in batch: 0.338972  [   14/   89]
Per-example loss in batch: 0.313001  [   16/   89]
Per-example loss in batch: 0.336426  [   18/   89]
Per-example loss in batch: 0.330762  [   20/   89]
Per-example loss in batch: 0.382153  [   22/   89]
Per-example loss in batch: 0.326494  [   24/   89]
Per-example loss in batch: 0.381573  [   26/   89]
Per-example loss in batch: 0.382399  [   28/   89]
Per-example loss in batch: 0.410184  [   30/   89]
Per-example loss in batch: 0.291023  [   32/   89]
Per-example loss in batch: 0.353127  [   34/   89]
Per-example loss in batch: 0.396054  [   36/   89]
Per-example loss in batch: 0.313464  [   38/   89]
Per-example loss in batch: 0.348391  [   40/   89]
Per-example loss in batch: 0.305655  [   42/   89]
Per-example loss in batch: 0.406694  [   44/   89]
Per-example loss in batch: 0.371152  [   46/   89]
Per-example loss in batch: 0.354841  [   48/   89]
Per-example loss in batch: 0.378884  [   50/   89]
Per-example loss in batch: 0.321250  [   52/   89]
Per-example loss in batch: 0.333594  [   54/   89]
Per-example loss in batch: 0.366795  [   56/   89]
Per-example loss in batch: 0.328130  [   58/   89]
Per-example loss in batch: 0.363148  [   60/   89]
Per-example loss in batch: 0.306872  [   62/   89]
Per-example loss in batch: 0.401393  [   64/   89]
Per-example loss in batch: 0.370514  [   66/   89]
Per-example loss in batch: 0.334309  [   68/   89]
Per-example loss in batch: 0.380388  [   70/   89]
Per-example loss in batch: 0.353300  [   72/   89]
Per-example loss in batch: 0.322106  [   74/   89]
Per-example loss in batch: 0.302788  [   76/   89]
Per-example loss in batch: 0.359513  [   78/   89]
Per-example loss in batch: 0.337794  [   80/   89]
Per-example loss in batch: 0.385626  [   82/   89]
Per-example loss in batch: 0.367370  [   84/   89]
Per-example loss in batch: 0.325439  [   86/   89]
Per-example loss in batch: 0.307242  [   88/   89]
Per-example loss in batch: 0.572330  [   89/   89]
Train Error: Avg loss: 0.34779460
validation Error: 
 Avg loss: 0.51074737 
 F1: 0.334737 
 Precision: 0.459984 
 Recall: 0.263099
 IoU: 0.201012

test Error: 
 Avg loss: 0.49093499 
 F1: 0.344984 
 Precision: 0.440480 
 Recall: 0.283518
 IoU: 0.208448

We have finished training iteration 146
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_144_.pth
Per-example loss in batch: 0.255642  [    2/   89]
Per-example loss in batch: 0.419953  [    4/   89]
Per-example loss in batch: 0.318609  [    6/   89]
Per-example loss in batch: 0.433724  [    8/   89]
Per-example loss in batch: 0.364468  [   10/   89]
Per-example loss in batch: 0.329561  [   12/   89]
Per-example loss in batch: 0.374215  [   14/   89]
Per-example loss in batch: 0.358166  [   16/   89]
Per-example loss in batch: 0.315765  [   18/   89]
Per-example loss in batch: 0.336819  [   20/   89]
Per-example loss in batch: 0.305487  [   22/   89]
Per-example loss in batch: 0.274998  [   24/   89]
Per-example loss in batch: 0.363204  [   26/   89]
Per-example loss in batch: 0.367232  [   28/   89]
Per-example loss in batch: 0.365129  [   30/   89]
Per-example loss in batch: 0.376100  [   32/   89]
Per-example loss in batch: 0.413719  [   34/   89]
Per-example loss in batch: 0.351248  [   36/   89]
Per-example loss in batch: 0.377055  [   38/   89]
Per-example loss in batch: 0.416100  [   40/   89]
Per-example loss in batch: 0.294024  [   42/   89]
Per-example loss in batch: 0.430856  [   44/   89]
Per-example loss in batch: 0.297722  [   46/   89]
Per-example loss in batch: 0.285741  [   48/   89]
Per-example loss in batch: 0.395461  [   50/   89]
Per-example loss in batch: 0.355583  [   52/   89]
Per-example loss in batch: 0.303611  [   54/   89]
Per-example loss in batch: 0.395347  [   56/   89]
Per-example loss in batch: 0.334078  [   58/   89]
Per-example loss in batch: 0.341886  [   60/   89]
Per-example loss in batch: 0.329033  [   62/   89]
Per-example loss in batch: 0.329608  [   64/   89]
Per-example loss in batch: 0.323807  [   66/   89]
Per-example loss in batch: 0.356694  [   68/   89]
Per-example loss in batch: 0.398086  [   70/   89]
Per-example loss in batch: 0.412984  [   72/   89]
Per-example loss in batch: 0.342268  [   74/   89]
Per-example loss in batch: 0.281847  [   76/   89]
Per-example loss in batch: 0.297527  [   78/   89]
Per-example loss in batch: 0.281216  [   80/   89]
Per-example loss in batch: 0.371799  [   82/   89]
Per-example loss in batch: 0.364458  [   84/   89]
Per-example loss in batch: 0.374120  [   86/   89]
Per-example loss in batch: 0.310065  [   88/   89]
Per-example loss in batch: 0.783452  [   89/   89]
Train Error: Avg loss: 0.35318521
validation Error: 
 Avg loss: 0.50898665 
 F1: 0.332711 
 Precision: 0.562305 
 Recall: 0.236249
 IoU: 0.199552

test Error: 
 Avg loss: 0.48953740 
 F1: 0.349826 
 Precision: 0.559039 
 Recall: 0.254560
 IoU: 0.211993

We have finished training iteration 147
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_145_.pth
Per-example loss in batch: 0.350455  [    2/   89]
Per-example loss in batch: 0.315009  [    4/   89]
Per-example loss in batch: 0.321833  [    6/   89]
Per-example loss in batch: 0.343325  [    8/   89]
Per-example loss in batch: 0.396596  [   10/   89]
Per-example loss in batch: 0.388210  [   12/   89]
Per-example loss in batch: 0.335474  [   14/   89]
Per-example loss in batch: 0.300952  [   16/   89]
Per-example loss in batch: 0.280640  [   18/   89]
Per-example loss in batch: 0.270551  [   20/   89]
Per-example loss in batch: 0.344512  [   22/   89]
Per-example loss in batch: 0.380896  [   24/   89]
Per-example loss in batch: 0.321999  [   26/   89]
Per-example loss in batch: 0.392305  [   28/   89]
Per-example loss in batch: 0.321082  [   30/   89]
Per-example loss in batch: 0.361408  [   32/   89]
Per-example loss in batch: 0.336237  [   34/   89]
Per-example loss in batch: 0.352468  [   36/   89]
Per-example loss in batch: 0.309853  [   38/   89]
Per-example loss in batch: 0.322480  [   40/   89]
Per-example loss in batch: 0.404926  [   42/   89]
Per-example loss in batch: 0.264086  [   44/   89]
Per-example loss in batch: 0.280540  [   46/   89]
Per-example loss in batch: 0.313847  [   48/   89]
Per-example loss in batch: 0.329524  [   50/   89]
Per-example loss in batch: 0.291433  [   52/   89]
Per-example loss in batch: 0.328921  [   54/   89]
Per-example loss in batch: 0.354306  [   56/   89]
Per-example loss in batch: 0.300260  [   58/   89]
Per-example loss in batch: 0.369443  [   60/   89]
Per-example loss in batch: 0.373414  [   62/   89]
Per-example loss in batch: 0.297464  [   64/   89]
Per-example loss in batch: 0.275941  [   66/   89]
Per-example loss in batch: 0.336459  [   68/   89]
Per-example loss in batch: 0.340077  [   70/   89]
Per-example loss in batch: 0.318610  [   72/   89]
Per-example loss in batch: 0.322632  [   74/   89]
Per-example loss in batch: 0.378836  [   76/   89]
Per-example loss in batch: 0.408030  [   78/   89]
Per-example loss in batch: 0.374446  [   80/   89]
Per-example loss in batch: 0.285724  [   82/   89]
Per-example loss in batch: 0.283364  [   84/   89]
Per-example loss in batch: 0.325997  [   86/   89]
Per-example loss in batch: 0.406344  [   88/   89]
Per-example loss in batch: 0.811232  [   89/   89]
Train Error: Avg loss: 0.33969718
validation Error: 
 Avg loss: 0.50972131 
 F1: 0.332062 
 Precision: 0.515983 
 Recall: 0.244802
 IoU: 0.199085

test Error: 
 Avg loss: 0.49012341 
 F1: 0.337593 
 Precision: 0.493304 
 Recall: 0.256598
 IoU: 0.203075

We have finished training iteration 148
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_146_.pth
Per-example loss in batch: 0.413578  [    2/   89]
Per-example loss in batch: 0.338032  [    4/   89]
Per-example loss in batch: 0.256101  [    6/   89]
Per-example loss in batch: 0.347652  [    8/   89]
Per-example loss in batch: 0.378098  [   10/   89]
Per-example loss in batch: 0.403091  [   12/   89]
Per-example loss in batch: 0.297951  [   14/   89]
Per-example loss in batch: 0.396203  [   16/   89]
Per-example loss in batch: 0.336183  [   18/   89]
Per-example loss in batch: 0.407703  [   20/   89]
Per-example loss in batch: 0.330744  [   22/   89]
Per-example loss in batch: 0.335092  [   24/   89]
Per-example loss in batch: 0.379586  [   26/   89]
Per-example loss in batch: 0.319812  [   28/   89]
Per-example loss in batch: 0.364648  [   30/   89]
Per-example loss in batch: 0.299367  [   32/   89]
Per-example loss in batch: 0.348093  [   34/   89]
Per-example loss in batch: 0.336250  [   36/   89]
Per-example loss in batch: 0.315384  [   38/   89]
Per-example loss in batch: 0.394990  [   40/   89]
Per-example loss in batch: 0.277345  [   42/   89]
Per-example loss in batch: 0.413290  [   44/   89]
Per-example loss in batch: 0.328474  [   46/   89]
Per-example loss in batch: 0.284962  [   48/   89]
Per-example loss in batch: 0.332937  [   50/   89]
Per-example loss in batch: 0.312868  [   52/   89]
Per-example loss in batch: 0.296695  [   54/   89]
Per-example loss in batch: 0.340084  [   56/   89]
Per-example loss in batch: 0.330616  [   58/   89]
Per-example loss in batch: 0.365590  [   60/   89]
Per-example loss in batch: 0.256134  [   62/   89]
Per-example loss in batch: 0.354636  [   64/   89]
Per-example loss in batch: 0.344071  [   66/   89]
Per-example loss in batch: 0.350676  [   68/   89]
Per-example loss in batch: 0.408733  [   70/   89]
Per-example loss in batch: 0.358566  [   72/   89]
Per-example loss in batch: 0.276443  [   74/   89]
Per-example loss in batch: 0.309512  [   76/   89]
Per-example loss in batch: 0.383309  [   78/   89]
Per-example loss in batch: 0.435414  [   80/   89]
Per-example loss in batch: 0.338117  [   82/   89]
Per-example loss in batch: 0.351076  [   84/   89]
Per-example loss in batch: 0.375069  [   86/   89]
Per-example loss in batch: 0.291829  [   88/   89]
Per-example loss in batch: 0.733825  [   89/   89]
Train Error: Avg loss: 0.34790818
validation Error: 
 Avg loss: 0.51050764 
 F1: 0.337956 
 Precision: 0.446246 
 Recall: 0.271960
 IoU: 0.203338

test Error: 
 Avg loss: 0.49094715 
 F1: 0.336966 
 Precision: 0.419131 
 Recall: 0.281735
 IoU: 0.202621

We have finished training iteration 149
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_147_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.298569  [    2/   89]
Per-example loss in batch: 0.339786  [    4/   89]
Per-example loss in batch: 0.429623  [    6/   89]
Per-example loss in batch: 0.373895  [    8/   89]
Per-example loss in batch: 0.308613  [   10/   89]
Per-example loss in batch: 0.310704  [   12/   89]
Per-example loss in batch: 0.308006  [   14/   89]
Per-example loss in batch: 0.350549  [   16/   89]
Per-example loss in batch: 0.403399  [   18/   89]
Per-example loss in batch: 0.386611  [   20/   89]
Per-example loss in batch: 0.325831  [   22/   89]
Per-example loss in batch: 0.340159  [   24/   89]
Per-example loss in batch: 0.373220  [   26/   89]
Per-example loss in batch: 0.317142  [   28/   89]
Per-example loss in batch: 0.324235  [   30/   89]
Per-example loss in batch: 0.398435  [   32/   89]
Per-example loss in batch: 0.343760  [   34/   89]
Per-example loss in batch: 0.377966  [   36/   89]
Per-example loss in batch: 0.350417  [   38/   89]
Per-example loss in batch: 0.286024  [   40/   89]
Per-example loss in batch: 0.370912  [   42/   89]
Per-example loss in batch: 0.315867  [   44/   89]
Per-example loss in batch: 0.263927  [   46/   89]
Per-example loss in batch: 0.350581  [   48/   89]
Per-example loss in batch: 0.301186  [   50/   89]
Per-example loss in batch: 0.375704  [   52/   89]
Per-example loss in batch: 0.313701  [   54/   89]
Per-example loss in batch: 0.273027  [   56/   89]
Per-example loss in batch: 0.370117  [   58/   89]
Per-example loss in batch: 0.417624  [   60/   89]
Per-example loss in batch: 0.348586  [   62/   89]
Per-example loss in batch: 0.270278  [   64/   89]
Per-example loss in batch: 0.287754  [   66/   89]
Per-example loss in batch: 0.419339  [   68/   89]
Per-example loss in batch: 0.380912  [   70/   89]
Per-example loss in batch: 0.327363  [   72/   89]
Per-example loss in batch: 0.286067  [   74/   89]
Per-example loss in batch: 0.340387  [   76/   89]
Per-example loss in batch: 0.302108  [   78/   89]
Per-example loss in batch: 0.262456  [   80/   89]
Per-example loss in batch: 0.343143  [   82/   89]
Per-example loss in batch: 0.390199  [   84/   89]
Per-example loss in batch: 0.388797  [   86/   89]
Per-example loss in batch: 0.303599  [   88/   89]
Per-example loss in batch: 0.685203  [   89/   89]
Train Error: Avg loss: 0.34366695
validation Error: 
 Avg loss: 0.50957212 
 F1: 0.344826 
 Precision: 0.489557 
 Recall: 0.266144
 IoU: 0.208332

test Error: 
 Avg loss: 0.49002110 
 F1: 0.363168 
 Precision: 0.490585 
 Recall: 0.288292
 IoU: 0.221872

We have finished training iteration 150
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_148_.pth
Per-example loss in batch: 0.287954  [    2/   89]
Per-example loss in batch: 0.322385  [    4/   89]
Per-example loss in batch: 0.313992  [    6/   89]
Per-example loss in batch: 0.351928  [    8/   89]
Per-example loss in batch: 0.351956  [   10/   89]
Per-example loss in batch: 0.288380  [   12/   89]
Per-example loss in batch: 0.382093  [   14/   89]
Per-example loss in batch: 0.274515  [   16/   89]
Per-example loss in batch: 0.351117  [   18/   89]
Per-example loss in batch: 0.258495  [   20/   89]
Per-example loss in batch: 0.374865  [   22/   89]
Per-example loss in batch: 0.314799  [   24/   89]
Per-example loss in batch: 0.413500  [   26/   89]
Per-example loss in batch: 0.324911  [   28/   89]
Per-example loss in batch: 0.339478  [   30/   89]
Per-example loss in batch: 0.361432  [   32/   89]
Per-example loss in batch: 0.323904  [   34/   89]
Per-example loss in batch: 0.384286  [   36/   89]
Per-example loss in batch: 0.416687  [   38/   89]
Per-example loss in batch: 0.325031  [   40/   89]
Per-example loss in batch: 0.368648  [   42/   89]
Per-example loss in batch: 0.318217  [   44/   89]
Per-example loss in batch: 0.390228  [   46/   89]
Per-example loss in batch: 0.365051  [   48/   89]
Per-example loss in batch: 0.292826  [   50/   89]
Per-example loss in batch: 0.327233  [   52/   89]
Per-example loss in batch: 0.402257  [   54/   89]
Per-example loss in batch: 0.384977  [   56/   89]
Per-example loss in batch: 0.284637  [   58/   89]
Per-example loss in batch: 0.373508  [   60/   89]
Per-example loss in batch: 0.360402  [   62/   89]
Per-example loss in batch: 0.269237  [   64/   89]
Per-example loss in batch: 0.342324  [   66/   89]
Per-example loss in batch: 0.303924  [   68/   89]
Per-example loss in batch: 0.266340  [   70/   89]
Per-example loss in batch: 0.352603  [   72/   89]
Per-example loss in batch: 0.366478  [   74/   89]
Per-example loss in batch: 0.291593  [   76/   89]
Per-example loss in batch: 0.409745  [   78/   89]
Per-example loss in batch: 0.309032  [   80/   89]
Per-example loss in batch: 0.391910  [   82/   89]
Per-example loss in batch: 0.454971  [   84/   89]
Per-example loss in batch: 0.364988  [   86/   89]
Per-example loss in batch: 0.366187  [   88/   89]
Per-example loss in batch: 0.916516  [   89/   89]
Train Error: Avg loss: 0.35005126
validation Error: 
 Avg loss: 0.50963287 
 F1: 0.336652 
 Precision: 0.497751 
 Recall: 0.254335
 IoU: 0.202394

test Error: 
 Avg loss: 0.49020678 
 F1: 0.355155 
 Precision: 0.484140 
 Recall: 0.280440
 IoU: 0.215920

We have finished training iteration 151
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_149_.pth
Per-example loss in batch: 0.400849  [    2/   89]
Per-example loss in batch: 0.341470  [    4/   89]
Per-example loss in batch: 0.303205  [    6/   89]
Per-example loss in batch: 0.298574  [    8/   89]
Per-example loss in batch: 0.447767  [   10/   89]
Per-example loss in batch: 0.411316  [   12/   89]
Per-example loss in batch: 0.396814  [   14/   89]
Per-example loss in batch: 0.387691  [   16/   89]
Per-example loss in batch: 0.317140  [   18/   89]
Per-example loss in batch: 0.396446  [   20/   89]
Per-example loss in batch: 0.264430  [   22/   89]
Per-example loss in batch: 0.365450  [   24/   89]
Per-example loss in batch: 0.347878  [   26/   89]
Per-example loss in batch: 0.297398  [   28/   89]
Per-example loss in batch: 0.346310  [   30/   89]
Per-example loss in batch: 0.295932  [   32/   89]
Per-example loss in batch: 0.338854  [   34/   89]
Per-example loss in batch: 0.395557  [   36/   89]
Per-example loss in batch: 0.306347  [   38/   89]
Per-example loss in batch: 0.341197  [   40/   89]
Per-example loss in batch: 0.331351  [   42/   89]
Per-example loss in batch: 0.323635  [   44/   89]
Per-example loss in batch: 0.401590  [   46/   89]
Per-example loss in batch: 0.303022  [   48/   89]
Per-example loss in batch: 0.325732  [   50/   89]
Per-example loss in batch: 0.379695  [   52/   89]
Per-example loss in batch: 0.328301  [   54/   89]
Per-example loss in batch: 0.375769  [   56/   89]
Per-example loss in batch: 0.327466  [   58/   89]
Per-example loss in batch: 0.360506  [   60/   89]
Per-example loss in batch: 0.334447  [   62/   89]
Per-example loss in batch: 0.297798  [   64/   89]
Per-example loss in batch: 0.304513  [   66/   89]
Per-example loss in batch: 0.345404  [   68/   89]
Per-example loss in batch: 0.317814  [   70/   89]
Per-example loss in batch: 0.310029  [   72/   89]
Per-example loss in batch: 0.342971  [   74/   89]
Per-example loss in batch: 0.322612  [   76/   89]
Per-example loss in batch: 0.422222  [   78/   89]
Per-example loss in batch: 0.319331  [   80/   89]
Per-example loss in batch: 0.371550  [   82/   89]
Per-example loss in batch: 0.352337  [   84/   89]
Per-example loss in batch: 0.283123  [   86/   89]
Per-example loss in batch: 0.334361  [   88/   89]
Per-example loss in batch: 0.556617  [   89/   89]
Train Error: Avg loss: 0.34594410
validation Error: 
 Avg loss: 0.50922854 
 F1: 0.335128 
 Precision: 0.514059 
 Recall: 0.248598
 IoU: 0.201294

test Error: 
 Avg loss: 0.48989026 
 F1: 0.352971 
 Precision: 0.520293 
 Recall: 0.267080
 IoU: 0.214307

We have finished training iteration 152
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_150_.pth
Per-example loss in batch: 0.315456  [    2/   89]
Per-example loss in batch: 0.339325  [    4/   89]
Per-example loss in batch: 0.300367  [    6/   89]
Per-example loss in batch: 0.302389  [    8/   89]
Per-example loss in batch: 0.307708  [   10/   89]
Per-example loss in batch: 0.450770  [   12/   89]
Per-example loss in batch: 0.381529  [   14/   89]
Per-example loss in batch: 0.305292  [   16/   89]
Per-example loss in batch: 0.330225  [   18/   89]
Per-example loss in batch: 0.300671  [   20/   89]
Per-example loss in batch: 0.322045  [   22/   89]
Per-example loss in batch: 0.344648  [   24/   89]
Per-example loss in batch: 0.364787  [   26/   89]
Per-example loss in batch: 0.281723  [   28/   89]
Per-example loss in batch: 0.301461  [   30/   89]
Per-example loss in batch: 0.302262  [   32/   89]
Per-example loss in batch: 0.319537  [   34/   89]
Per-example loss in batch: 0.398975  [   36/   89]
Per-example loss in batch: 0.348989  [   38/   89]
Per-example loss in batch: 0.281063  [   40/   89]
Per-example loss in batch: 0.309842  [   42/   89]
Per-example loss in batch: 0.397361  [   44/   89]
Per-example loss in batch: 0.355329  [   46/   89]
Per-example loss in batch: 0.296645  [   48/   89]
Per-example loss in batch: 0.326565  [   50/   89]
Per-example loss in batch: 0.380531  [   52/   89]
Per-example loss in batch: 0.310568  [   54/   89]
Per-example loss in batch: 0.325808  [   56/   89]
Per-example loss in batch: 0.321411  [   58/   89]
Per-example loss in batch: 0.264987  [   60/   89]
Per-example loss in batch: 0.341092  [   62/   89]
Per-example loss in batch: 0.291524  [   64/   89]
Per-example loss in batch: 0.401833  [   66/   89]
Per-example loss in batch: 0.394194  [   68/   89]
Per-example loss in batch: 0.346922  [   70/   89]
Per-example loss in batch: 0.332706  [   72/   89]
Per-example loss in batch: 0.339181  [   74/   89]
Per-example loss in batch: 0.378903  [   76/   89]
Per-example loss in batch: 0.270076  [   78/   89]
Per-example loss in batch: 0.335131  [   80/   89]
Per-example loss in batch: 0.368975  [   82/   89]
Per-example loss in batch: 0.319432  [   84/   89]
Per-example loss in batch: 0.263866  [   86/   89]
Per-example loss in batch: 0.327406  [   88/   89]
Per-example loss in batch: 0.631288  [   89/   89]
Train Error: Avg loss: 0.33517200
validation Error: 
 Avg loss: 0.50901377 
 F1: 0.340749 
 Precision: 0.548782 
 Recall: 0.247085
 IoU: 0.205363

test Error: 
 Avg loss: 0.48932538 
 F1: 0.357360 
 Precision: 0.534175 
 Recall: 0.268488
 IoU: 0.217552

We have finished training iteration 153
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_151_.pth
Per-example loss in batch: 0.340935  [    2/   89]
Per-example loss in batch: 0.293843  [    4/   89]
Per-example loss in batch: 0.328939  [    6/   89]
Per-example loss in batch: 0.324923  [    8/   89]
Per-example loss in batch: 0.326373  [   10/   89]
Per-example loss in batch: 0.320406  [   12/   89]
Per-example loss in batch: 0.418339  [   14/   89]
Per-example loss in batch: 0.346110  [   16/   89]
Per-example loss in batch: 0.306533  [   18/   89]
Per-example loss in batch: 0.380051  [   20/   89]
Per-example loss in batch: 0.367275  [   22/   89]
Per-example loss in batch: 0.402090  [   24/   89]
Per-example loss in batch: 0.361763  [   26/   89]
Per-example loss in batch: 0.298042  [   28/   89]
Per-example loss in batch: 0.270566  [   30/   89]
Per-example loss in batch: 0.363403  [   32/   89]
Per-example loss in batch: 0.388314  [   34/   89]
Per-example loss in batch: 0.350611  [   36/   89]
Per-example loss in batch: 0.271463  [   38/   89]
Per-example loss in batch: 0.344415  [   40/   89]
Per-example loss in batch: 0.400641  [   42/   89]
Per-example loss in batch: 0.305116  [   44/   89]
Per-example loss in batch: 0.318447  [   46/   89]
Per-example loss in batch: 0.382906  [   48/   89]
Per-example loss in batch: 0.421026  [   50/   89]
Per-example loss in batch: 0.349825  [   52/   89]
Per-example loss in batch: 0.379263  [   54/   89]
Per-example loss in batch: 0.333550  [   56/   89]
Per-example loss in batch: 0.262024  [   58/   89]
Per-example loss in batch: 0.318971  [   60/   89]
Per-example loss in batch: 0.342985  [   62/   89]
Per-example loss in batch: 0.284668  [   64/   89]
Per-example loss in batch: 0.306080  [   66/   89]
Per-example loss in batch: 0.375417  [   68/   89]
Per-example loss in batch: 0.390476  [   70/   89]
Per-example loss in batch: 0.304660  [   72/   89]
Per-example loss in batch: 0.430108  [   74/   89]
Per-example loss in batch: 0.271298  [   76/   89]
Per-example loss in batch: 0.404418  [   78/   89]
Per-example loss in batch: 0.300914  [   80/   89]
Per-example loss in batch: 0.364129  [   82/   89]
Per-example loss in batch: 0.276748  [   84/   89]
Per-example loss in batch: 0.297183  [   86/   89]
Per-example loss in batch: 0.308316  [   88/   89]
Per-example loss in batch: 0.621045  [   89/   89]
Train Error: Avg loss: 0.34256375
validation Error: 
 Avg loss: 0.50910247 
 F1: 0.343221 
 Precision: 0.499612 
 Recall: 0.261397
 IoU: 0.207161

test Error: 
 Avg loss: 0.49008426 
 F1: 0.354899 
 Precision: 0.488333 
 Recall: 0.278736
 IoU: 0.215731

We have finished training iteration 154
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_152_.pth
Per-example loss in batch: 0.334964  [    2/   89]
Per-example loss in batch: 0.329906  [    4/   89]
Per-example loss in batch: 0.329835  [    6/   89]
Per-example loss in batch: 0.352780  [    8/   89]
Per-example loss in batch: 0.395257  [   10/   89]
Per-example loss in batch: 0.295059  [   12/   89]
Per-example loss in batch: 0.289246  [   14/   89]
Per-example loss in batch: 0.287632  [   16/   89]
Per-example loss in batch: 0.393420  [   18/   89]
Per-example loss in batch: 0.374818  [   20/   89]
Per-example loss in batch: 0.465293  [   22/   89]
Per-example loss in batch: 0.299830  [   24/   89]
Per-example loss in batch: 0.391519  [   26/   89]
Per-example loss in batch: 0.295675  [   28/   89]
Per-example loss in batch: 0.359932  [   30/   89]
Per-example loss in batch: 0.345697  [   32/   89]
Per-example loss in batch: 0.412318  [   34/   89]
Per-example loss in batch: 0.333560  [   36/   89]
Per-example loss in batch: 0.366353  [   38/   89]
Per-example loss in batch: 0.377949  [   40/   89]
Per-example loss in batch: 0.456839  [   42/   89]
Per-example loss in batch: 0.457389  [   44/   89]
Per-example loss in batch: 0.303175  [   46/   89]
Per-example loss in batch: 0.406031  [   48/   89]
Per-example loss in batch: 0.303552  [   50/   89]
Per-example loss in batch: 0.265738  [   52/   89]
Per-example loss in batch: 0.342642  [   54/   89]
Per-example loss in batch: 0.362113  [   56/   89]
Per-example loss in batch: 0.387111  [   58/   89]
Per-example loss in batch: 0.340015  [   60/   89]
Per-example loss in batch: 0.366113  [   62/   89]
Per-example loss in batch: 0.359470  [   64/   89]
Per-example loss in batch: 0.253446  [   66/   89]
Per-example loss in batch: 0.354232  [   68/   89]
Per-example loss in batch: 0.273540  [   70/   89]
Per-example loss in batch: 0.277785  [   72/   89]
Per-example loss in batch: 0.371908  [   74/   89]
Per-example loss in batch: 0.336076  [   76/   89]
Per-example loss in batch: 0.341190  [   78/   89]
Per-example loss in batch: 0.251420  [   80/   89]
Per-example loss in batch: 0.254565  [   82/   89]
Per-example loss in batch: 0.336656  [   84/   89]
Per-example loss in batch: 0.321081  [   86/   89]
Per-example loss in batch: 0.326058  [   88/   89]
Per-example loss in batch: 0.845290  [   89/   89]
Train Error: Avg loss: 0.34835582
validation Error: 
 Avg loss: 0.50978208 
 F1: 0.346160 
 Precision: 0.475008 
 Recall: 0.272298
 IoU: 0.209307

test Error: 
 Avg loss: 0.48953732 
 F1: 0.365087 
 Precision: 0.466403 
 Recall: 0.299933
 IoU: 0.223306

We have finished training iteration 155
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_153_.pth
Per-example loss in batch: 0.286360  [    2/   89]
Per-example loss in batch: 0.425861  [    4/   89]
Per-example loss in batch: 0.294170  [    6/   89]
Per-example loss in batch: 0.399644  [    8/   89]
Per-example loss in batch: 0.297509  [   10/   89]
Per-example loss in batch: 0.374261  [   12/   89]
Per-example loss in batch: 0.404337  [   14/   89]
Per-example loss in batch: 0.346764  [   16/   89]
Per-example loss in batch: 0.435982  [   18/   89]
Per-example loss in batch: 0.312481  [   20/   89]
Per-example loss in batch: 0.420203  [   22/   89]
Per-example loss in batch: 0.276188  [   24/   89]
Per-example loss in batch: 0.291569  [   26/   89]
Per-example loss in batch: 0.390765  [   28/   89]
Per-example loss in batch: 0.295542  [   30/   89]
Per-example loss in batch: 0.315604  [   32/   89]
Per-example loss in batch: 0.285387  [   34/   89]
Per-example loss in batch: 0.337948  [   36/   89]
Per-example loss in batch: 0.408953  [   38/   89]
Per-example loss in batch: 0.353502  [   40/   89]
Per-example loss in batch: 0.328756  [   42/   89]
Per-example loss in batch: 0.420097  [   44/   89]
Per-example loss in batch: 0.399623  [   46/   89]
Per-example loss in batch: 0.290913  [   48/   89]
Per-example loss in batch: 0.268840  [   50/   89]
Per-example loss in batch: 0.312749  [   52/   89]
Per-example loss in batch: 0.427295  [   54/   89]
Per-example loss in batch: 0.287638  [   56/   89]
Per-example loss in batch: 0.309818  [   58/   89]
Per-example loss in batch: 0.331609  [   60/   89]
Per-example loss in batch: 0.311583  [   62/   89]
Per-example loss in batch: 0.310536  [   64/   89]
Per-example loss in batch: 0.379932  [   66/   89]
Per-example loss in batch: 0.300471  [   68/   89]
Per-example loss in batch: 0.353898  [   70/   89]
Per-example loss in batch: 0.260592  [   72/   89]
Per-example loss in batch: 0.314610  [   74/   89]
Per-example loss in batch: 0.348746  [   76/   89]
Per-example loss in batch: 0.347339  [   78/   89]
Per-example loss in batch: 0.320777  [   80/   89]
Per-example loss in batch: 0.323249  [   82/   89]
Per-example loss in batch: 0.406678  [   84/   89]
Per-example loss in batch: 0.319609  [   86/   89]
Per-example loss in batch: 0.319919  [   88/   89]
Per-example loss in batch: 0.629421  [   89/   89]
Train Error: Avg loss: 0.34298919
validation Error: 
 Avg loss: 0.50936762 
 F1: 0.348234 
 Precision: 0.524722 
 Recall: 0.260587
 IoU: 0.210825

test Error: 
 Avg loss: 0.48930499 
 F1: 0.368607 
 Precision: 0.514548 
 Recall: 0.287161
 IoU: 0.225947

We have finished training iteration 156
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_154_.pth
Per-example loss in batch: 0.281885  [    2/   89]
Per-example loss in batch: 0.353725  [    4/   89]
Per-example loss in batch: 0.416855  [    6/   89]
Per-example loss in batch: 0.311491  [    8/   89]
Per-example loss in batch: 0.370894  [   10/   89]
Per-example loss in batch: 0.316688  [   12/   89]
Per-example loss in batch: 0.384306  [   14/   89]
Per-example loss in batch: 0.266755  [   16/   89]
Per-example loss in batch: 0.357251  [   18/   89]
Per-example loss in batch: 0.410274  [   20/   89]
Per-example loss in batch: 0.350780  [   22/   89]
Per-example loss in batch: 0.317857  [   24/   89]
Per-example loss in batch: 0.334688  [   26/   89]
Per-example loss in batch: 0.300950  [   28/   89]
Per-example loss in batch: 0.396654  [   30/   89]
Per-example loss in batch: 0.376780  [   32/   89]
Per-example loss in batch: 0.312279  [   34/   89]
Per-example loss in batch: 0.274509  [   36/   89]
Per-example loss in batch: 0.316928  [   38/   89]
Per-example loss in batch: 0.372800  [   40/   89]
Per-example loss in batch: 0.412523  [   42/   89]
Per-example loss in batch: 0.312439  [   44/   89]
Per-example loss in batch: 0.297487  [   46/   89]
Per-example loss in batch: 0.357545  [   48/   89]
Per-example loss in batch: 0.329258  [   50/   89]
Per-example loss in batch: 0.380322  [   52/   89]
Per-example loss in batch: 0.334986  [   54/   89]
Per-example loss in batch: 0.294943  [   56/   89]
Per-example loss in batch: 0.298901  [   58/   89]
Per-example loss in batch: 0.275399  [   60/   89]
Per-example loss in batch: 0.292059  [   62/   89]
Per-example loss in batch: 0.288318  [   64/   89]
Per-example loss in batch: 0.337209  [   66/   89]
Per-example loss in batch: 0.261718  [   68/   89]
Per-example loss in batch: 0.283812  [   70/   89]
Per-example loss in batch: 0.377254  [   72/   89]
Per-example loss in batch: 0.302693  [   74/   89]
Per-example loss in batch: 0.331562  [   76/   89]
Per-example loss in batch: 0.356269  [   78/   89]
Per-example loss in batch: 0.337799  [   80/   89]
Per-example loss in batch: 0.365450  [   82/   89]
Per-example loss in batch: 0.426009  [   84/   89]
Per-example loss in batch: 0.382763  [   86/   89]
Per-example loss in batch: 0.377626  [   88/   89]
Per-example loss in batch: 0.763210  [   89/   89]
Train Error: Avg loss: 0.34202920
validation Error: 
 Avg loss: 0.50850696 
 F1: 0.356630 
 Precision: 0.458491 
 Recall: 0.291801
 IoU: 0.217011

test Error: 
 Avg loss: 0.48846458 
 F1: 0.375753 
 Precision: 0.451927 
 Recall: 0.321555
 IoU: 0.231340

We have finished training iteration 157
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_155_.pth
Per-example loss in batch: 0.337195  [    2/   89]
Per-example loss in batch: 0.312517  [    4/   89]
Per-example loss in batch: 0.325329  [    6/   89]
Per-example loss in batch: 0.345322  [    8/   89]
Per-example loss in batch: 0.369299  [   10/   89]
Per-example loss in batch: 0.349343  [   12/   89]
Per-example loss in batch: 0.304786  [   14/   89]
Per-example loss in batch: 0.317517  [   16/   89]
Per-example loss in batch: 0.391894  [   18/   89]
Per-example loss in batch: 0.340270  [   20/   89]
Per-example loss in batch: 0.330978  [   22/   89]
Per-example loss in batch: 0.439523  [   24/   89]
Per-example loss in batch: 0.319899  [   26/   89]
Per-example loss in batch: 0.310980  [   28/   89]
Per-example loss in batch: 0.363535  [   30/   89]
Per-example loss in batch: 0.320067  [   32/   89]
Per-example loss in batch: 0.364298  [   34/   89]
Per-example loss in batch: 0.312865  [   36/   89]
Per-example loss in batch: 0.308893  [   38/   89]
Per-example loss in batch: 0.272123  [   40/   89]
Per-example loss in batch: 0.303810  [   42/   89]
Per-example loss in batch: 0.448807  [   44/   89]
Per-example loss in batch: 0.382654  [   46/   89]
Per-example loss in batch: 0.303485  [   48/   89]
Per-example loss in batch: 0.342045  [   50/   89]
Per-example loss in batch: 0.429622  [   52/   89]
Per-example loss in batch: 0.390878  [   54/   89]
Per-example loss in batch: 0.355393  [   56/   89]
Per-example loss in batch: 0.281692  [   58/   89]
Per-example loss in batch: 0.359191  [   60/   89]
Per-example loss in batch: 0.286473  [   62/   89]
Per-example loss in batch: 0.314584  [   64/   89]
Per-example loss in batch: 0.296677  [   66/   89]
Per-example loss in batch: 0.319957  [   68/   89]
Per-example loss in batch: 0.264848  [   70/   89]
Per-example loss in batch: 0.302182  [   72/   89]
Per-example loss in batch: 0.406851  [   74/   89]
Per-example loss in batch: 0.379102  [   76/   89]
Per-example loss in batch: 0.289279  [   78/   89]
Per-example loss in batch: 0.331098  [   80/   89]
Per-example loss in batch: 0.355119  [   82/   89]
Per-example loss in batch: 0.355841  [   84/   89]
Per-example loss in batch: 0.252775  [   86/   89]
Per-example loss in batch: 0.352535  [   88/   89]
Per-example loss in batch: 0.522487  [   89/   89]
Train Error: Avg loss: 0.33938819
validation Error: 
 Avg loss: 0.50857333 
 F1: 0.349501 
 Precision: 0.488082 
 Recall: 0.272212
 IoU: 0.211755

test Error: 
 Avg loss: 0.48869204 
 F1: 0.365993 
 Precision: 0.468423 
 Recall: 0.300322
 IoU: 0.223985

We have finished training iteration 158
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_156_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.344623  [    2/   89]
Per-example loss in batch: 0.353099  [    4/   89]
Per-example loss in batch: 0.405777  [    6/   89]
Per-example loss in batch: 0.322990  [    8/   89]
Per-example loss in batch: 0.345483  [   10/   89]
Per-example loss in batch: 0.296153  [   12/   89]
Per-example loss in batch: 0.260085  [   14/   89]
Per-example loss in batch: 0.360174  [   16/   89]
Per-example loss in batch: 0.336594  [   18/   89]
Per-example loss in batch: 0.405553  [   20/   89]
Per-example loss in batch: 0.270319  [   22/   89]
Per-example loss in batch: 0.298595  [   24/   89]
Per-example loss in batch: 0.347122  [   26/   89]
Per-example loss in batch: 0.476398  [   28/   89]
Per-example loss in batch: 0.318484  [   30/   89]
Per-example loss in batch: 0.348105  [   32/   89]
Per-example loss in batch: 0.381191  [   34/   89]
Per-example loss in batch: 0.359249  [   36/   89]
Per-example loss in batch: 0.273262  [   38/   89]
Per-example loss in batch: 0.430191  [   40/   89]
Per-example loss in batch: 0.303087  [   42/   89]
Per-example loss in batch: 0.342393  [   44/   89]
Per-example loss in batch: 0.401377  [   46/   89]
Per-example loss in batch: 0.323062  [   48/   89]
Per-example loss in batch: 0.380116  [   50/   89]
Per-example loss in batch: 0.331383  [   52/   89]
Per-example loss in batch: 0.321193  [   54/   89]
Per-example loss in batch: 0.375769  [   56/   89]
Per-example loss in batch: 0.356002  [   58/   89]
Per-example loss in batch: 0.303591  [   60/   89]
Per-example loss in batch: 0.381764  [   62/   89]
Per-example loss in batch: 0.268570  [   64/   89]
Per-example loss in batch: 0.291428  [   66/   89]
Per-example loss in batch: 0.381037  [   68/   89]
Per-example loss in batch: 0.285096  [   70/   89]
Per-example loss in batch: 0.367648  [   72/   89]
Per-example loss in batch: 0.266778  [   74/   89]
Per-example loss in batch: 0.339572  [   76/   89]
Per-example loss in batch: 0.298292  [   78/   89]
Per-example loss in batch: 0.295009  [   80/   89]
Per-example loss in batch: 0.385781  [   82/   89]
Per-example loss in batch: 0.326170  [   84/   89]
Per-example loss in batch: 0.276574  [   86/   89]
Per-example loss in batch: 0.284838  [   88/   89]
Per-example loss in batch: 0.872990  [   89/   89]
Train Error: Avg loss: 0.34284207
validation Error: 
 Avg loss: 0.50886301 
 F1: 0.349818 
 Precision: 0.432851 
 Recall: 0.293513
 IoU: 0.211987

test Error: 
 Avg loss: 0.48967898 
 F1: 0.367311 
 Precision: 0.413697 
 Recall: 0.330279
 IoU: 0.224973

We have finished training iteration 159
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_157_.pth
Per-example loss in batch: 0.323557  [    2/   89]
Per-example loss in batch: 0.252868  [    4/   89]
Per-example loss in batch: 0.330360  [    6/   89]
Per-example loss in batch: 0.327728  [    8/   89]
Per-example loss in batch: 0.334115  [   10/   89]
Per-example loss in batch: 0.282998  [   12/   89]
Per-example loss in batch: 0.334633  [   14/   89]
Per-example loss in batch: 0.349115  [   16/   89]
Per-example loss in batch: 0.275925  [   18/   89]
Per-example loss in batch: 0.353435  [   20/   89]
Per-example loss in batch: 0.308314  [   22/   89]
Per-example loss in batch: 0.388815  [   24/   89]
Per-example loss in batch: 0.335593  [   26/   89]
Per-example loss in batch: 0.293795  [   28/   89]
Per-example loss in batch: 0.325690  [   30/   89]
Per-example loss in batch: 0.411577  [   32/   89]
Per-example loss in batch: 0.316612  [   34/   89]
Per-example loss in batch: 0.332734  [   36/   89]
Per-example loss in batch: 0.330360  [   38/   89]
Per-example loss in batch: 0.263599  [   40/   89]
Per-example loss in batch: 0.310355  [   42/   89]
Per-example loss in batch: 0.256838  [   44/   89]
Per-example loss in batch: 0.365735  [   46/   89]
Per-example loss in batch: 0.332973  [   48/   89]
Per-example loss in batch: 0.390892  [   50/   89]
Per-example loss in batch: 0.370032  [   52/   89]
Per-example loss in batch: 0.337084  [   54/   89]
Per-example loss in batch: 0.263081  [   56/   89]
Per-example loss in batch: 0.267978  [   58/   89]
Per-example loss in batch: 0.345874  [   60/   89]
Per-example loss in batch: 0.301220  [   62/   89]
Per-example loss in batch: 0.294374  [   64/   89]
Per-example loss in batch: 0.333796  [   66/   89]
Per-example loss in batch: 0.305199  [   68/   89]
Per-example loss in batch: 0.281229  [   70/   89]
Per-example loss in batch: 0.407468  [   72/   89]
Per-example loss in batch: 0.378699  [   74/   89]
Per-example loss in batch: 0.371583  [   76/   89]
Per-example loss in batch: 0.332430  [   78/   89]
Per-example loss in batch: 0.433427  [   80/   89]
Per-example loss in batch: 0.362427  [   82/   89]
Per-example loss in batch: 0.348500  [   84/   89]
Per-example loss in batch: 0.306040  [   86/   89]
Per-example loss in batch: 0.315293  [   88/   89]
Per-example loss in batch: 0.823575  [   89/   89]
Train Error: Avg loss: 0.33474462
validation Error: 
 Avg loss: 0.50939431 
 F1: 0.351117 
 Precision: 0.466428 
 Recall: 0.281520
 IoU: 0.212943

test Error: 
 Avg loss: 0.48938961 
 F1: 0.367627 
 Precision: 0.459744 
 Recall: 0.306262
 IoU: 0.225210

We have finished training iteration 160
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_158_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.318492  [    2/   89]
Per-example loss in batch: 0.383165  [    4/   89]
Per-example loss in batch: 0.308104  [    6/   89]
Per-example loss in batch: 0.348151  [    8/   89]
Per-example loss in batch: 0.345664  [   10/   89]
Per-example loss in batch: 0.342740  [   12/   89]
Per-example loss in batch: 0.261482  [   14/   89]
Per-example loss in batch: 0.371705  [   16/   89]
Per-example loss in batch: 0.322522  [   18/   89]
Per-example loss in batch: 0.372684  [   20/   89]
Per-example loss in batch: 0.297168  [   22/   89]
Per-example loss in batch: 0.349403  [   24/   89]
Per-example loss in batch: 0.316674  [   26/   89]
Per-example loss in batch: 0.329294  [   28/   89]
Per-example loss in batch: 0.319787  [   30/   89]
Per-example loss in batch: 0.412384  [   32/   89]
Per-example loss in batch: 0.306609  [   34/   89]
Per-example loss in batch: 0.397917  [   36/   89]
Per-example loss in batch: 0.406036  [   38/   89]
Per-example loss in batch: 0.315984  [   40/   89]
Per-example loss in batch: 0.322163  [   42/   89]
Per-example loss in batch: 0.288141  [   44/   89]
Per-example loss in batch: 0.383758  [   46/   89]
Per-example loss in batch: 0.315098  [   48/   89]
Per-example loss in batch: 0.310262  [   50/   89]
Per-example loss in batch: 0.265357  [   52/   89]
Per-example loss in batch: 0.297878  [   54/   89]
Per-example loss in batch: 0.301855  [   56/   89]
Per-example loss in batch: 0.393935  [   58/   89]
Per-example loss in batch: 0.317967  [   60/   89]
Per-example loss in batch: 0.243908  [   62/   89]
Per-example loss in batch: 0.356935  [   64/   89]
Per-example loss in batch: 0.292295  [   66/   89]
Per-example loss in batch: 0.261793  [   68/   89]
Per-example loss in batch: 0.326250  [   70/   89]
Per-example loss in batch: 0.411889  [   72/   89]
Per-example loss in batch: 0.395460  [   74/   89]
Per-example loss in batch: 0.349262  [   76/   89]
Per-example loss in batch: 0.355853  [   78/   89]
Per-example loss in batch: 0.368532  [   80/   89]
Per-example loss in batch: 0.350505  [   82/   89]
Per-example loss in batch: 0.343665  [   84/   89]
Per-example loss in batch: 0.376314  [   86/   89]
Per-example loss in batch: 0.261439  [   88/   89]
Per-example loss in batch: 0.723816  [   89/   89]
Train Error: Avg loss: 0.33884004
validation Error: 
 Avg loss: 0.50977924 
 F1: 0.344679 
 Precision: 0.476668 
 Recall: 0.269934
 IoU: 0.208225

test Error: 
 Avg loss: 0.49009530 
 F1: 0.361310 
 Precision: 0.465878 
 Recall: 0.295079
 IoU: 0.220487

We have finished training iteration 161
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_159_.pth
Per-example loss in batch: 0.327632  [    2/   89]
Per-example loss in batch: 0.385326  [    4/   89]
Per-example loss in batch: 0.298581  [    6/   89]
Per-example loss in batch: 0.293972  [    8/   89]
Per-example loss in batch: 0.346565  [   10/   89]
Per-example loss in batch: 0.330006  [   12/   89]
Per-example loss in batch: 0.296488  [   14/   89]
Per-example loss in batch: 0.328894  [   16/   89]
Per-example loss in batch: 0.305945  [   18/   89]
Per-example loss in batch: 0.320028  [   20/   89]
Per-example loss in batch: 0.373651  [   22/   89]
Per-example loss in batch: 0.254842  [   24/   89]
Per-example loss in batch: 0.322618  [   26/   89]
Per-example loss in batch: 0.332345  [   28/   89]
Per-example loss in batch: 0.274289  [   30/   89]
Per-example loss in batch: 0.359666  [   32/   89]
Per-example loss in batch: 0.366847  [   34/   89]
Per-example loss in batch: 0.339348  [   36/   89]
Per-example loss in batch: 0.333313  [   38/   89]
Per-example loss in batch: 0.421461  [   40/   89]
Per-example loss in batch: 0.307674  [   42/   89]
Per-example loss in batch: 0.377519  [   44/   89]
Per-example loss in batch: 0.372891  [   46/   89]
Per-example loss in batch: 0.269787  [   48/   89]
Per-example loss in batch: 0.355245  [   50/   89]
Per-example loss in batch: 0.347475  [   52/   89]
Per-example loss in batch: 0.333414  [   54/   89]
Per-example loss in batch: 0.341221  [   56/   89]
Per-example loss in batch: 0.328099  [   58/   89]
Per-example loss in batch: 0.265518  [   60/   89]
Per-example loss in batch: 0.415410  [   62/   89]
Per-example loss in batch: 0.327176  [   64/   89]
Per-example loss in batch: 0.310524  [   66/   89]
Per-example loss in batch: 0.291328  [   68/   89]
Per-example loss in batch: 0.386818  [   70/   89]
Per-example loss in batch: 0.294552  [   72/   89]
Per-example loss in batch: 0.400030  [   74/   89]
Per-example loss in batch: 0.305859  [   76/   89]
Per-example loss in batch: 0.348211  [   78/   89]
Per-example loss in batch: 0.360803  [   80/   89]
Per-example loss in batch: 0.362881  [   82/   89]
Per-example loss in batch: 0.357965  [   84/   89]
Per-example loss in batch: 0.381902  [   86/   89]
Per-example loss in batch: 0.378224  [   88/   89]
Per-example loss in batch: 0.620895  [   89/   89]
Train Error: Avg loss: 0.34028736
validation Error: 
 Avg loss: 0.50915969 
 F1: 0.352915 
 Precision: 0.459351 
 Recall: 0.286524
 IoU: 0.214266

test Error: 
 Avg loss: 0.48918810 
 F1: 0.365955 
 Precision: 0.446610 
 Recall: 0.309975
 IoU: 0.223956

We have finished training iteration 162
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_160_.pth
Per-example loss in batch: 0.340528  [    2/   89]
Per-example loss in batch: 0.281932  [    4/   89]
Per-example loss in batch: 0.250410  [    6/   89]
Per-example loss in batch: 0.262949  [    8/   89]
Per-example loss in batch: 0.397482  [   10/   89]
Per-example loss in batch: 0.324853  [   12/   89]
Per-example loss in batch: 0.338374  [   14/   89]
Per-example loss in batch: 0.397912  [   16/   89]
Per-example loss in batch: 0.367348  [   18/   89]
Per-example loss in batch: 0.350797  [   20/   89]
Per-example loss in batch: 0.338900  [   22/   89]
Per-example loss in batch: 0.291238  [   24/   89]
Per-example loss in batch: 0.378181  [   26/   89]
Per-example loss in batch: 0.238284  [   28/   89]
Per-example loss in batch: 0.323811  [   30/   89]
Per-example loss in batch: 0.308035  [   32/   89]
Per-example loss in batch: 0.290720  [   34/   89]
Per-example loss in batch: 0.349871  [   36/   89]
Per-example loss in batch: 0.322369  [   38/   89]
Per-example loss in batch: 0.363212  [   40/   89]
Per-example loss in batch: 0.317677  [   42/   89]
Per-example loss in batch: 0.250598  [   44/   89]
Per-example loss in batch: 0.422949  [   46/   89]
Per-example loss in batch: 0.341191  [   48/   89]
Per-example loss in batch: 0.366946  [   50/   89]
Per-example loss in batch: 0.294844  [   52/   89]
Per-example loss in batch: 0.311294  [   54/   89]
Per-example loss in batch: 0.373917  [   56/   89]
Per-example loss in batch: 0.355304  [   58/   89]
Per-example loss in batch: 0.334308  [   60/   89]
Per-example loss in batch: 0.285896  [   62/   89]
Per-example loss in batch: 0.362274  [   64/   89]
Per-example loss in batch: 0.341670  [   66/   89]
Per-example loss in batch: 0.262779  [   68/   89]
Per-example loss in batch: 0.309979  [   70/   89]
Per-example loss in batch: 0.388373  [   72/   89]
Per-example loss in batch: 0.269781  [   74/   89]
Per-example loss in batch: 0.319810  [   76/   89]
Per-example loss in batch: 0.401062  [   78/   89]
Per-example loss in batch: 0.374485  [   80/   89]
Per-example loss in batch: 0.338655  [   82/   89]
Per-example loss in batch: 0.293487  [   84/   89]
Per-example loss in batch: 0.356012  [   86/   89]
Per-example loss in batch: 0.327419  [   88/   89]
Per-example loss in batch: 0.902185  [   89/   89]
Train Error: Avg loss: 0.33638223
validation Error: 
 Avg loss: 0.50845824 
 F1: 0.360935 
 Precision: 0.472613 
 Recall: 0.291947
 IoU: 0.220207

test Error: 
 Avg loss: 0.48813707 
 F1: 0.376054 
 Precision: 0.459600 
 Recall: 0.318210
 IoU: 0.231568

We have finished training iteration 163
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_161_.pth
Per-example loss in batch: 0.364751  [    2/   89]
Per-example loss in batch: 0.330347  [    4/   89]
Per-example loss in batch: 0.362509  [    6/   89]
Per-example loss in batch: 0.374551  [    8/   89]
Per-example loss in batch: 0.418969  [   10/   89]
Per-example loss in batch: 0.318404  [   12/   89]
Per-example loss in batch: 0.290314  [   14/   89]
Per-example loss in batch: 0.301218  [   16/   89]
Per-example loss in batch: 0.439970  [   18/   89]
Per-example loss in batch: 0.363281  [   20/   89]
Per-example loss in batch: 0.349300  [   22/   89]
Per-example loss in batch: 0.434107  [   24/   89]
Per-example loss in batch: 0.318214  [   26/   89]
Per-example loss in batch: 0.339919  [   28/   89]
Per-example loss in batch: 0.268380  [   30/   89]
Per-example loss in batch: 0.282500  [   32/   89]
Per-example loss in batch: 0.318899  [   34/   89]
Per-example loss in batch: 0.292784  [   36/   89]
Per-example loss in batch: 0.258733  [   38/   89]
Per-example loss in batch: 0.326792  [   40/   89]
Per-example loss in batch: 0.285491  [   42/   89]
Per-example loss in batch: 0.333534  [   44/   89]
Per-example loss in batch: 0.326906  [   46/   89]
Per-example loss in batch: 0.272607  [   48/   89]
Per-example loss in batch: 0.401561  [   50/   89]
Per-example loss in batch: 0.331489  [   52/   89]
Per-example loss in batch: 0.402072  [   54/   89]
Per-example loss in batch: 0.285973  [   56/   89]
Per-example loss in batch: 0.389675  [   58/   89]
Per-example loss in batch: 0.329354  [   60/   89]
Per-example loss in batch: 0.289425  [   62/   89]
Per-example loss in batch: 0.285711  [   64/   89]
Per-example loss in batch: 0.394631  [   66/   89]
Per-example loss in batch: 0.286997  [   68/   89]
Per-example loss in batch: 0.389207  [   70/   89]
Per-example loss in batch: 0.329010  [   72/   89]
Per-example loss in batch: 0.279448  [   74/   89]
Per-example loss in batch: 0.330520  [   76/   89]
Per-example loss in batch: 0.329754  [   78/   89]
Per-example loss in batch: 0.350629  [   80/   89]
Per-example loss in batch: 0.345007  [   82/   89]
Per-example loss in batch: 0.335097  [   84/   89]
Per-example loss in batch: 0.347363  [   86/   89]
Per-example loss in batch: 0.290698  [   88/   89]
Per-example loss in batch: 0.688115  [   89/   89]
Train Error: Avg loss: 0.33798106
validation Error: 
 Avg loss: 0.50887958 
 F1: 0.359537 
 Precision: 0.506675 
 Recall: 0.278625
 IoU: 0.219168

test Error: 
 Avg loss: 0.48952629 
 F1: 0.374357 
 Precision: 0.491630 
 Recall: 0.302257
 IoU: 0.230283

We have finished training iteration 164
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_162_.pth
Per-example loss in batch: 0.319856  [    2/   89]
Per-example loss in batch: 0.396756  [    4/   89]
Per-example loss in batch: 0.255047  [    6/   89]
Per-example loss in batch: 0.336993  [    8/   89]
Per-example loss in batch: 0.388357  [   10/   89]
Per-example loss in batch: 0.314756  [   12/   89]
Per-example loss in batch: 0.365982  [   14/   89]
Per-example loss in batch: 0.317984  [   16/   89]
Per-example loss in batch: 0.286379  [   18/   89]
Per-example loss in batch: 0.348458  [   20/   89]
Per-example loss in batch: 0.377143  [   22/   89]
Per-example loss in batch: 0.327230  [   24/   89]
Per-example loss in batch: 0.363849  [   26/   89]
Per-example loss in batch: 0.295971  [   28/   89]
Per-example loss in batch: 0.359017  [   30/   89]
Per-example loss in batch: 0.394023  [   32/   89]
Per-example loss in batch: 0.326949  [   34/   89]
Per-example loss in batch: 0.300238  [   36/   89]
Per-example loss in batch: 0.362347  [   38/   89]
Per-example loss in batch: 0.339335  [   40/   89]
Per-example loss in batch: 0.396406  [   42/   89]
Per-example loss in batch: 0.346904  [   44/   89]
Per-example loss in batch: 0.309591  [   46/   89]
Per-example loss in batch: 0.399918  [   48/   89]
Per-example loss in batch: 0.290069  [   50/   89]
Per-example loss in batch: 0.338770  [   52/   89]
Per-example loss in batch: 0.365553  [   54/   89]
Per-example loss in batch: 0.325823  [   56/   89]
Per-example loss in batch: 0.315111  [   58/   89]
Per-example loss in batch: 0.266383  [   60/   89]
Per-example loss in batch: 0.334184  [   62/   89]
Per-example loss in batch: 0.364280  [   64/   89]
Per-example loss in batch: 0.299167  [   66/   89]
Per-example loss in batch: 0.368407  [   68/   89]
Per-example loss in batch: 0.346853  [   70/   89]
Per-example loss in batch: 0.385303  [   72/   89]
Per-example loss in batch: 0.329025  [   74/   89]
Per-example loss in batch: 0.307607  [   76/   89]
Per-example loss in batch: 0.262505  [   78/   89]
Per-example loss in batch: 0.296033  [   80/   89]
Per-example loss in batch: 0.290893  [   82/   89]
Per-example loss in batch: 0.370615  [   84/   89]
Per-example loss in batch: 0.403801  [   86/   89]
Per-example loss in batch: 0.362114  [   88/   89]
Per-example loss in batch: 0.498850  [   89/   89]
Train Error: Avg loss: 0.33935755
validation Error: 
 Avg loss: 0.50918523 
 F1: 0.358091 
 Precision: 0.493211 
 Recall: 0.281085
 IoU: 0.218094

test Error: 
 Avg loss: 0.48877827 
 F1: 0.372877 
 Precision: 0.480350 
 Recall: 0.304703
 IoU: 0.229163

We have finished training iteration 165
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_163_.pth
Per-example loss in batch: 0.355501  [    2/   89]
Per-example loss in batch: 0.318788  [    4/   89]
Per-example loss in batch: 0.331717  [    6/   89]
Per-example loss in batch: 0.393331  [    8/   89]
Per-example loss in batch: 0.314328  [   10/   89]
Per-example loss in batch: 0.411979  [   12/   89]
Per-example loss in batch: 0.323428  [   14/   89]
Per-example loss in batch: 0.300069  [   16/   89]
Per-example loss in batch: 0.319821  [   18/   89]
Per-example loss in batch: 0.337454  [   20/   89]
Per-example loss in batch: 0.324982  [   22/   89]
Per-example loss in batch: 0.363393  [   24/   89]
Per-example loss in batch: 0.255198  [   26/   89]
Per-example loss in batch: 0.346952  [   28/   89]
Per-example loss in batch: 0.407850  [   30/   89]
Per-example loss in batch: 0.368570  [   32/   89]
Per-example loss in batch: 0.287632  [   34/   89]
Per-example loss in batch: 0.299786  [   36/   89]
Per-example loss in batch: 0.281445  [   38/   89]
Per-example loss in batch: 0.308432  [   40/   89]
Per-example loss in batch: 0.381055  [   42/   89]
Per-example loss in batch: 0.308974  [   44/   89]
Per-example loss in batch: 0.322734  [   46/   89]
Per-example loss in batch: 0.388846  [   48/   89]
Per-example loss in batch: 0.313633  [   50/   89]
Per-example loss in batch: 0.365627  [   52/   89]
Per-example loss in batch: 0.270632  [   54/   89]
Per-example loss in batch: 0.279732  [   56/   89]
Per-example loss in batch: 0.278292  [   58/   89]
Per-example loss in batch: 0.335301  [   60/   89]
Per-example loss in batch: 0.349452  [   62/   89]
Per-example loss in batch: 0.418844  [   64/   89]
Per-example loss in batch: 0.330830  [   66/   89]
Per-example loss in batch: 0.336584  [   68/   89]
Per-example loss in batch: 0.331272  [   70/   89]
Per-example loss in batch: 0.375154  [   72/   89]
Per-example loss in batch: 0.377035  [   74/   89]
Per-example loss in batch: 0.288941  [   76/   89]
Per-example loss in batch: 0.340253  [   78/   89]
Per-example loss in batch: 0.379593  [   80/   89]
Per-example loss in batch: 0.382332  [   82/   89]
Per-example loss in batch: 0.339081  [   84/   89]
Per-example loss in batch: 0.320348  [   86/   89]
Per-example loss in batch: 0.370979  [   88/   89]
Per-example loss in batch: 0.771913  [   89/   89]
Train Error: Avg loss: 0.34207049
validation Error: 
 Avg loss: 0.50929347 
 F1: 0.349395 
 Precision: 0.491915 
 Recall: 0.270907
 IoU: 0.211677

test Error: 
 Avg loss: 0.48911861 
 F1: 0.365046 
 Precision: 0.481102 
 Recall: 0.294100
 IoU: 0.223276

We have finished training iteration 166
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_164_.pth
Per-example loss in batch: 0.278561  [    2/   89]
Per-example loss in batch: 0.343727  [    4/   89]
Per-example loss in batch: 0.325113  [    6/   89]
Per-example loss in batch: 0.403264  [    8/   89]
Per-example loss in batch: 0.372229  [   10/   89]
Per-example loss in batch: 0.273568  [   12/   89]
Per-example loss in batch: 0.388703  [   14/   89]
Per-example loss in batch: 0.344589  [   16/   89]
Per-example loss in batch: 0.268739  [   18/   89]
Per-example loss in batch: 0.317368  [   20/   89]
Per-example loss in batch: 0.349184  [   22/   89]
Per-example loss in batch: 0.369404  [   24/   89]
Per-example loss in batch: 0.303022  [   26/   89]
Per-example loss in batch: 0.399965  [   28/   89]
Per-example loss in batch: 0.365003  [   30/   89]
Per-example loss in batch: 0.321562  [   32/   89]
Per-example loss in batch: 0.285894  [   34/   89]
Per-example loss in batch: 0.388952  [   36/   89]
Per-example loss in batch: 0.287083  [   38/   89]
Per-example loss in batch: 0.433558  [   40/   89]
Per-example loss in batch: 0.338896  [   42/   89]
Per-example loss in batch: 0.271028  [   44/   89]
Per-example loss in batch: 0.334076  [   46/   89]
Per-example loss in batch: 0.344850  [   48/   89]
Per-example loss in batch: 0.369756  [   50/   89]
Per-example loss in batch: 0.387810  [   52/   89]
Per-example loss in batch: 0.313724  [   54/   89]
Per-example loss in batch: 0.378576  [   56/   89]
Per-example loss in batch: 0.337909  [   58/   89]
Per-example loss in batch: 0.383909  [   60/   89]
Per-example loss in batch: 0.340156  [   62/   89]
Per-example loss in batch: 0.359996  [   64/   89]
Per-example loss in batch: 0.349926  [   66/   89]
Per-example loss in batch: 0.323860  [   68/   89]
Per-example loss in batch: 0.264428  [   70/   89]
Per-example loss in batch: 0.324023  [   72/   89]
Per-example loss in batch: 0.335293  [   74/   89]
Per-example loss in batch: 0.307728  [   76/   89]
Per-example loss in batch: 0.318109  [   78/   89]
Per-example loss in batch: 0.336225  [   80/   89]
Per-example loss in batch: 0.390100  [   82/   89]
Per-example loss in batch: 0.362507  [   84/   89]
Per-example loss in batch: 0.288036  [   86/   89]
Per-example loss in batch: 0.349301  [   88/   89]
Per-example loss in batch: 0.521139  [   89/   89]
Train Error: Avg loss: 0.34135464
validation Error: 
 Avg loss: 0.50978355 
 F1: 0.352949 
 Precision: 0.419686 
 Recall: 0.304525
 IoU: 0.214291

test Error: 
 Avg loss: 0.48988078 
 F1: 0.368188 
 Precision: 0.405792 
 Recall: 0.336961
 IoU: 0.225631

We have finished training iteration 167
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_165_.pth
Per-example loss in batch: 0.370001  [    2/   89]
Per-example loss in batch: 0.293217  [    4/   89]
Per-example loss in batch: 0.380116  [    6/   89]
Per-example loss in batch: 0.407265  [    8/   89]
Per-example loss in batch: 0.355224  [   10/   89]
Per-example loss in batch: 0.409364  [   12/   89]
Per-example loss in batch: 0.285273  [   14/   89]
Per-example loss in batch: 0.418845  [   16/   89]
Per-example loss in batch: 0.317870  [   18/   89]
Per-example loss in batch: 0.352277  [   20/   89]
Per-example loss in batch: 0.326392  [   22/   89]
Per-example loss in batch: 0.359253  [   24/   89]
Per-example loss in batch: 0.402735  [   26/   89]
Per-example loss in batch: 0.368333  [   28/   89]
Per-example loss in batch: 0.308840  [   30/   89]
Per-example loss in batch: 0.268978  [   32/   89]
Per-example loss in batch: 0.303245  [   34/   89]
Per-example loss in batch: 0.404828  [   36/   89]
Per-example loss in batch: 0.270884  [   38/   89]
Per-example loss in batch: 0.352823  [   40/   89]
Per-example loss in batch: 0.270965  [   42/   89]
Per-example loss in batch: 0.401583  [   44/   89]
Per-example loss in batch: 0.330576  [   46/   89]
Per-example loss in batch: 0.286687  [   48/   89]
Per-example loss in batch: 0.362653  [   50/   89]
Per-example loss in batch: 0.367131  [   52/   89]
Per-example loss in batch: 0.338742  [   54/   89]
Per-example loss in batch: 0.345751  [   56/   89]
Per-example loss in batch: 0.254891  [   58/   89]
Per-example loss in batch: 0.337198  [   60/   89]
Per-example loss in batch: 0.257216  [   62/   89]
Per-example loss in batch: 0.303299  [   64/   89]
Per-example loss in batch: 0.390071  [   66/   89]
Per-example loss in batch: 0.355853  [   68/   89]
Per-example loss in batch: 0.349499  [   70/   89]
Per-example loss in batch: 0.373537  [   72/   89]
Per-example loss in batch: 0.313296  [   74/   89]
Per-example loss in batch: 0.355712  [   76/   89]
Per-example loss in batch: 0.362431  [   78/   89]
Per-example loss in batch: 0.331927  [   80/   89]
Per-example loss in batch: 0.310905  [   82/   89]
Per-example loss in batch: 0.291876  [   84/   89]
Per-example loss in batch: 0.297941  [   86/   89]
Per-example loss in batch: 0.242518  [   88/   89]
Per-example loss in batch: 0.745640  [   89/   89]
Train Error: Avg loss: 0.34069299
validation Error: 
 Avg loss: 0.51030022 
 F1: 0.347903 
 Precision: 0.451577 
 Recall: 0.282945
 IoU: 0.210583

test Error: 
 Avg loss: 0.49053814 
 F1: 0.354928 
 Precision: 0.433148 
 Recall: 0.300637
 IoU: 0.215752

We have finished training iteration 168
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_166_.pth
Per-example loss in batch: 0.307991  [    2/   89]
Per-example loss in batch: 0.331448  [    4/   89]
Per-example loss in batch: 0.281408  [    6/   89]
Per-example loss in batch: 0.346629  [    8/   89]
Per-example loss in batch: 0.371191  [   10/   89]
Per-example loss in batch: 0.309502  [   12/   89]
Per-example loss in batch: 0.292080  [   14/   89]
Per-example loss in batch: 0.288531  [   16/   89]
Per-example loss in batch: 0.337417  [   18/   89]
Per-example loss in batch: 0.295583  [   20/   89]
Per-example loss in batch: 0.302427  [   22/   89]
Per-example loss in batch: 0.309878  [   24/   89]
Per-example loss in batch: 0.390067  [   26/   89]
Per-example loss in batch: 0.396910  [   28/   89]
Per-example loss in batch: 0.326556  [   30/   89]
Per-example loss in batch: 0.409224  [   32/   89]
Per-example loss in batch: 0.292626  [   34/   89]
Per-example loss in batch: 0.327691  [   36/   89]
Per-example loss in batch: 0.292022  [   38/   89]
Per-example loss in batch: 0.315314  [   40/   89]
Per-example loss in batch: 0.315046  [   42/   89]
Per-example loss in batch: 0.308613  [   44/   89]
Per-example loss in batch: 0.350273  [   46/   89]
Per-example loss in batch: 0.284808  [   48/   89]
Per-example loss in batch: 0.308708  [   50/   89]
Per-example loss in batch: 0.301016  [   52/   89]
Per-example loss in batch: 0.272014  [   54/   89]
Per-example loss in batch: 0.345003  [   56/   89]
Per-example loss in batch: 0.334645  [   58/   89]
Per-example loss in batch: 0.299437  [   60/   89]
Per-example loss in batch: 0.406154  [   62/   89]
Per-example loss in batch: 0.396224  [   64/   89]
Per-example loss in batch: 0.258506  [   66/   89]
Per-example loss in batch: 0.348295  [   68/   89]
Per-example loss in batch: 0.348987  [   70/   89]
Per-example loss in batch: 0.281790  [   72/   89]
Per-example loss in batch: 0.334079  [   74/   89]
Per-example loss in batch: 0.403212  [   76/   89]
Per-example loss in batch: 0.262088  [   78/   89]
Per-example loss in batch: 0.415343  [   80/   89]
Per-example loss in batch: 0.414235  [   82/   89]
Per-example loss in batch: 0.325512  [   84/   89]
Per-example loss in batch: 0.339664  [   86/   89]
Per-example loss in batch: 0.419052  [   88/   89]
Per-example loss in batch: 0.529478  [   89/   89]
Train Error: Avg loss: 0.33397615
validation Error: 
 Avg loss: 0.50827953 
 F1: 0.354625 
 Precision: 0.546102 
 Recall: 0.262564
 IoU: 0.215528

test Error: 
 Avg loss: 0.48838127 
 F1: 0.370408 
 Precision: 0.530926 
 Recall: 0.284418
 IoU: 0.227301

We have finished training iteration 169
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_167_.pth
Per-example loss in batch: 0.336297  [    2/   89]
Per-example loss in batch: 0.289382  [    4/   89]
Per-example loss in batch: 0.265847  [    6/   89]
Per-example loss in batch: 0.359103  [    8/   89]
Per-example loss in batch: 0.329396  [   10/   89]
Per-example loss in batch: 0.366833  [   12/   89]
Per-example loss in batch: 0.377870  [   14/   89]
Per-example loss in batch: 0.390197  [   16/   89]
Per-example loss in batch: 0.321108  [   18/   89]
Per-example loss in batch: 0.371783  [   20/   89]
Per-example loss in batch: 0.327146  [   22/   89]
Per-example loss in batch: 0.384270  [   24/   89]
Per-example loss in batch: 0.382548  [   26/   89]
Per-example loss in batch: 0.333912  [   28/   89]
Per-example loss in batch: 0.332276  [   30/   89]
Per-example loss in batch: 0.392705  [   32/   89]
Per-example loss in batch: 0.266790  [   34/   89]
Per-example loss in batch: 0.364540  [   36/   89]
Per-example loss in batch: 0.315957  [   38/   89]
Per-example loss in batch: 0.403231  [   40/   89]
Per-example loss in batch: 0.266286  [   42/   89]
Per-example loss in batch: 0.356041  [   44/   89]
Per-example loss in batch: 0.429946  [   46/   89]
Per-example loss in batch: 0.327995  [   48/   89]
Per-example loss in batch: 0.340080  [   50/   89]
Per-example loss in batch: 0.272833  [   52/   89]
Per-example loss in batch: 0.278148  [   54/   89]
Per-example loss in batch: 0.361233  [   56/   89]
Per-example loss in batch: 0.352988  [   58/   89]
Per-example loss in batch: 0.239221  [   60/   89]
Per-example loss in batch: 0.319061  [   62/   89]
Per-example loss in batch: 0.312326  [   64/   89]
Per-example loss in batch: 0.318404  [   66/   89]
Per-example loss in batch: 0.344507  [   68/   89]
Per-example loss in batch: 0.422886  [   70/   89]
Per-example loss in batch: 0.288907  [   72/   89]
Per-example loss in batch: 0.315850  [   74/   89]
Per-example loss in batch: 0.242550  [   76/   89]
Per-example loss in batch: 0.361279  [   78/   89]
Per-example loss in batch: 0.321811  [   80/   89]
Per-example loss in batch: 0.287342  [   82/   89]
Per-example loss in batch: 0.341695  [   84/   89]
Per-example loss in batch: 0.316525  [   86/   89]
Per-example loss in batch: 0.298568  [   88/   89]
Per-example loss in batch: 0.813767  [   89/   89]
Train Error: Avg loss: 0.33785524
validation Error: 
 Avg loss: 0.50953922 
 F1: 0.356362 
 Precision: 0.451391 
 Recall: 0.294386
 IoU: 0.216813

test Error: 
 Avg loss: 0.49023381 
 F1: 0.354254 
 Precision: 0.420015 
 Recall: 0.306298
 IoU: 0.215254

We have finished training iteration 170
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_168_.pth
Per-example loss in batch: 0.341926  [    2/   89]
Per-example loss in batch: 0.308845  [    4/   89]
Per-example loss in batch: 0.319360  [    6/   89]
Per-example loss in batch: 0.376152  [    8/   89]
Per-example loss in batch: 0.288277  [   10/   89]
Per-example loss in batch: 0.390159  [   12/   89]
Per-example loss in batch: 0.354556  [   14/   89]
Per-example loss in batch: 0.355518  [   16/   89]
Per-example loss in batch: 0.290743  [   18/   89]
Per-example loss in batch: 0.246184  [   20/   89]
Per-example loss in batch: 0.398279  [   22/   89]
Per-example loss in batch: 0.329594  [   24/   89]
Per-example loss in batch: 0.306205  [   26/   89]
Per-example loss in batch: 0.354795  [   28/   89]
Per-example loss in batch: 0.346237  [   30/   89]
Per-example loss in batch: 0.411075  [   32/   89]
Per-example loss in batch: 0.351216  [   34/   89]
Per-example loss in batch: 0.334502  [   36/   89]
Per-example loss in batch: 0.298116  [   38/   89]
Per-example loss in batch: 0.298685  [   40/   89]
Per-example loss in batch: 0.326786  [   42/   89]
Per-example loss in batch: 0.320672  [   44/   89]
Per-example loss in batch: 0.283870  [   46/   89]
Per-example loss in batch: 0.320906  [   48/   89]
Per-example loss in batch: 0.320957  [   50/   89]
Per-example loss in batch: 0.274190  [   52/   89]
Per-example loss in batch: 0.308669  [   54/   89]
Per-example loss in batch: 0.267162  [   56/   89]
Per-example loss in batch: 0.373397  [   58/   89]
Per-example loss in batch: 0.239370  [   60/   89]
Per-example loss in batch: 0.375229  [   62/   89]
Per-example loss in batch: 0.366450  [   64/   89]
Per-example loss in batch: 0.409602  [   66/   89]
Per-example loss in batch: 0.350832  [   68/   89]
Per-example loss in batch: 0.331758  [   70/   89]
Per-example loss in batch: 0.305526  [   72/   89]
Per-example loss in batch: 0.305352  [   74/   89]
Per-example loss in batch: 0.350642  [   76/   89]
Per-example loss in batch: 0.350560  [   78/   89]
Per-example loss in batch: 0.351005  [   80/   89]
Per-example loss in batch: 0.259031  [   82/   89]
Per-example loss in batch: 0.351106  [   84/   89]
Per-example loss in batch: 0.383285  [   86/   89]
Per-example loss in batch: 0.409867  [   88/   89]
Per-example loss in batch: 0.717706  [   89/   89]
Train Error: Avg loss: 0.33697760
validation Error: 
 Avg loss: 0.51018644 
 F1: 0.356218 
 Precision: 0.446469 
 Recall: 0.296319
 IoU: 0.216707

test Error: 
 Avg loss: 0.49093875 
 F1: 0.359983 
 Precision: 0.421226 
 Recall: 0.314289
 IoU: 0.219500

We have finished training iteration 171
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_169_.pth
Per-example loss in batch: 0.313789  [    2/   89]
Per-example loss in batch: 0.412845  [    4/   89]
Per-example loss in batch: 0.337682  [    6/   89]
Per-example loss in batch: 0.350392  [    8/   89]
Per-example loss in batch: 0.409910  [   10/   89]
Per-example loss in batch: 0.282591  [   12/   89]
Per-example loss in batch: 0.331367  [   14/   89]
Per-example loss in batch: 0.328574  [   16/   89]
Per-example loss in batch: 0.342629  [   18/   89]
Per-example loss in batch: 0.352137  [   20/   89]
Per-example loss in batch: 0.296607  [   22/   89]
Per-example loss in batch: 0.332981  [   24/   89]
Per-example loss in batch: 0.322987  [   26/   89]
Per-example loss in batch: 0.308000  [   28/   89]
Per-example loss in batch: 0.289474  [   30/   89]
Per-example loss in batch: 0.261350  [   32/   89]
Per-example loss in batch: 0.346231  [   34/   89]
Per-example loss in batch: 0.302858  [   36/   89]
Per-example loss in batch: 0.289685  [   38/   89]
Per-example loss in batch: 0.318996  [   40/   89]
Per-example loss in batch: 0.271983  [   42/   89]
Per-example loss in batch: 0.275837  [   44/   89]
Per-example loss in batch: 0.328651  [   46/   89]
Per-example loss in batch: 0.296880  [   48/   89]
Per-example loss in batch: 0.335371  [   50/   89]
Per-example loss in batch: 0.368086  [   52/   89]
Per-example loss in batch: 0.336689  [   54/   89]
Per-example loss in batch: 0.380520  [   56/   89]
Per-example loss in batch: 0.375468  [   58/   89]
Per-example loss in batch: 0.386626  [   60/   89]
Per-example loss in batch: 0.282735  [   62/   89]
Per-example loss in batch: 0.298720  [   64/   89]
Per-example loss in batch: 0.280432  [   66/   89]
Per-example loss in batch: 0.300889  [   68/   89]
Per-example loss in batch: 0.317005  [   70/   89]
Per-example loss in batch: 0.367007  [   72/   89]
Per-example loss in batch: 0.320635  [   74/   89]
Per-example loss in batch: 0.301397  [   76/   89]
Per-example loss in batch: 0.298602  [   78/   89]
Per-example loss in batch: 0.311092  [   80/   89]
Per-example loss in batch: 0.330975  [   82/   89]
Per-example loss in batch: 0.364701  [   84/   89]
Per-example loss in batch: 0.320940  [   86/   89]
Per-example loss in batch: 0.317181  [   88/   89]
Per-example loss in batch: 0.847408  [   89/   89]
Train Error: Avg loss: 0.33085872
validation Error: 
 Avg loss: 0.50932686 
 F1: 0.348492 
 Precision: 0.496152 
 Recall: 0.268564
 IoU: 0.211014

test Error: 
 Avg loss: 0.48870369 
 F1: 0.359600 
 Precision: 0.488504 
 Recall: 0.284523
 IoU: 0.219215

We have finished training iteration 172
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_170_.pth
Per-example loss in batch: 0.341466  [    2/   89]
Per-example loss in batch: 0.259419  [    4/   89]
Per-example loss in batch: 0.342836  [    6/   89]
Per-example loss in batch: 0.338044  [    8/   89]
Per-example loss in batch: 0.290394  [   10/   89]
Per-example loss in batch: 0.393488  [   12/   89]
Per-example loss in batch: 0.384141  [   14/   89]
Per-example loss in batch: 0.306674  [   16/   89]
Per-example loss in batch: 0.344299  [   18/   89]
Per-example loss in batch: 0.265319  [   20/   89]
Per-example loss in batch: 0.336042  [   22/   89]
Per-example loss in batch: 0.341136  [   24/   89]
Per-example loss in batch: 0.307766  [   26/   89]
Per-example loss in batch: 0.257515  [   28/   89]
Per-example loss in batch: 0.330128  [   30/   89]
Per-example loss in batch: 0.439462  [   32/   89]
Per-example loss in batch: 0.327362  [   34/   89]
Per-example loss in batch: 0.254046  [   36/   89]
Per-example loss in batch: 0.375647  [   38/   89]
Per-example loss in batch: 0.399722  [   40/   89]
Per-example loss in batch: 0.314434  [   42/   89]
Per-example loss in batch: 0.280565  [   44/   89]
Per-example loss in batch: 0.240885  [   46/   89]
Per-example loss in batch: 0.408006  [   48/   89]
Per-example loss in batch: 0.325555  [   50/   89]
Per-example loss in batch: 0.285757  [   52/   89]
Per-example loss in batch: 0.359239  [   54/   89]
Per-example loss in batch: 0.278368  [   56/   89]
Per-example loss in batch: 0.365502  [   58/   89]
Per-example loss in batch: 0.317797  [   60/   89]
Per-example loss in batch: 0.289186  [   62/   89]
Per-example loss in batch: 0.313771  [   64/   89]
Per-example loss in batch: 0.388632  [   66/   89]
Per-example loss in batch: 0.318222  [   68/   89]
Per-example loss in batch: 0.273042  [   70/   89]
Per-example loss in batch: 0.324992  [   72/   89]
Per-example loss in batch: 0.373503  [   74/   89]
Per-example loss in batch: 0.337070  [   76/   89]
Per-example loss in batch: 0.269251  [   78/   89]
Per-example loss in batch: 0.337941  [   80/   89]
Per-example loss in batch: 0.311202  [   82/   89]
Per-example loss in batch: 0.340402  [   84/   89]
Per-example loss in batch: 0.323949  [   86/   89]
Per-example loss in batch: 0.336504  [   88/   89]
Per-example loss in batch: 0.930702  [   89/   89]
Train Error: Avg loss: 0.33289950
validation Error: 
 Avg loss: 0.51078021 
 F1: 0.341902 
 Precision: 0.386823 
 Recall: 0.306328
 IoU: 0.206201

test Error: 
 Avg loss: 0.49121165 
 F1: 0.343939 
 Precision: 0.367134 
 Recall: 0.323501
 IoU: 0.207685

We have finished training iteration 173
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_171_.pth
Per-example loss in batch: 0.311647  [    2/   89]
Per-example loss in batch: 0.355512  [    4/   89]
Per-example loss in batch: 0.302826  [    6/   89]
Per-example loss in batch: 0.326650  [    8/   89]
Per-example loss in batch: 0.402842  [   10/   89]
Per-example loss in batch: 0.327284  [   12/   89]
Per-example loss in batch: 0.386026  [   14/   89]
Per-example loss in batch: 0.310219  [   16/   89]
Per-example loss in batch: 0.385238  [   18/   89]
Per-example loss in batch: 0.386164  [   20/   89]
Per-example loss in batch: 0.270915  [   22/   89]
Per-example loss in batch: 0.355043  [   24/   89]
Per-example loss in batch: 0.337706  [   26/   89]
Per-example loss in batch: 0.314918  [   28/   89]
Per-example loss in batch: 0.240718  [   30/   89]
Per-example loss in batch: 0.387751  [   32/   89]
Per-example loss in batch: 0.361492  [   34/   89]
Per-example loss in batch: 0.317065  [   36/   89]
Per-example loss in batch: 0.371415  [   38/   89]
Per-example loss in batch: 0.331930  [   40/   89]
Per-example loss in batch: 0.285522  [   42/   89]
Per-example loss in batch: 0.348133  [   44/   89]
Per-example loss in batch: 0.266706  [   46/   89]
Per-example loss in batch: 0.333144  [   48/   89]
Per-example loss in batch: 0.375733  [   50/   89]
Per-example loss in batch: 0.266356  [   52/   89]
Per-example loss in batch: 0.320844  [   54/   89]
Per-example loss in batch: 0.399503  [   56/   89]
Per-example loss in batch: 0.292716  [   58/   89]
Per-example loss in batch: 0.244787  [   60/   89]
Per-example loss in batch: 0.298890  [   62/   89]
Per-example loss in batch: 0.354196  [   64/   89]
Per-example loss in batch: 0.362558  [   66/   89]
Per-example loss in batch: 0.371366  [   68/   89]
Per-example loss in batch: 0.295422  [   70/   89]
Per-example loss in batch: 0.271664  [   72/   89]
Per-example loss in batch: 0.279193  [   74/   89]
Per-example loss in batch: 0.386723  [   76/   89]
Per-example loss in batch: 0.325222  [   78/   89]
Per-example loss in batch: 0.349466  [   80/   89]
Per-example loss in batch: 0.282696  [   82/   89]
Per-example loss in batch: 0.268715  [   84/   89]
Per-example loss in batch: 0.374551  [   86/   89]
Per-example loss in batch: 0.309379  [   88/   89]
Per-example loss in batch: 0.794759  [   89/   89]
Train Error: Avg loss: 0.33357811
validation Error: 
 Avg loss: 0.50758469 
 F1: 0.366883 
 Precision: 0.534663 
 Recall: 0.279252
 IoU: 0.224652

test Error: 
 Avg loss: 0.48772221 
 F1: 0.379190 
 Precision: 0.516463 
 Recall: 0.299567
 IoU: 0.233951

We have finished training iteration 174
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_172_.pth
Per-example loss in batch: 0.377520  [    2/   89]
Per-example loss in batch: 0.314578  [    4/   89]
Per-example loss in batch: 0.404873  [    6/   89]
Per-example loss in batch: 0.297567  [    8/   89]
Per-example loss in batch: 0.352435  [   10/   89]
Per-example loss in batch: 0.407922  [   12/   89]
Per-example loss in batch: 0.361034  [   14/   89]
Per-example loss in batch: 0.343002  [   16/   89]
Per-example loss in batch: 0.376541  [   18/   89]
Per-example loss in batch: 0.384594  [   20/   89]
Per-example loss in batch: 0.265274  [   22/   89]
Per-example loss in batch: 0.358137  [   24/   89]
Per-example loss in batch: 0.356219  [   26/   89]
Per-example loss in batch: 0.365770  [   28/   89]
Per-example loss in batch: 0.299352  [   30/   89]
Per-example loss in batch: 0.341938  [   32/   89]
Per-example loss in batch: 0.387661  [   34/   89]
Per-example loss in batch: 0.363050  [   36/   89]
Per-example loss in batch: 0.265010  [   38/   89]
Per-example loss in batch: 0.329364  [   40/   89]
Per-example loss in batch: 0.290884  [   42/   89]
Per-example loss in batch: 0.272878  [   44/   89]
Per-example loss in batch: 0.312612  [   46/   89]
Per-example loss in batch: 0.287909  [   48/   89]
Per-example loss in batch: 0.252855  [   50/   89]
Per-example loss in batch: 0.303597  [   52/   89]
Per-example loss in batch: 0.318802  [   54/   89]
Per-example loss in batch: 0.335556  [   56/   89]
Per-example loss in batch: 0.336674  [   58/   89]
Per-example loss in batch: 0.327291  [   60/   89]
Per-example loss in batch: 0.319080  [   62/   89]
Per-example loss in batch: 0.263321  [   64/   89]
Per-example loss in batch: 0.284722  [   66/   89]
Per-example loss in batch: 0.319101  [   68/   89]
Per-example loss in batch: 0.302082  [   70/   89]
Per-example loss in batch: 0.320623  [   72/   89]
Per-example loss in batch: 0.418075  [   74/   89]
Per-example loss in batch: 0.340513  [   76/   89]
Per-example loss in batch: 0.327209  [   78/   89]
Per-example loss in batch: 0.318017  [   80/   89]
Per-example loss in batch: 0.290721  [   82/   89]
Per-example loss in batch: 0.351705  [   84/   89]
Per-example loss in batch: 0.350001  [   86/   89]
Per-example loss in batch: 0.389201  [   88/   89]
Per-example loss in batch: 0.614242  [   89/   89]
Train Error: Avg loss: 0.33466043
validation Error: 
 Avg loss: 0.50926703 
 F1: 0.358205 
 Precision: 0.490091 
 Recall: 0.282250
 IoU: 0.218179

test Error: 
 Avg loss: 0.49032145 
 F1: 0.362610 
 Precision: 0.459906 
 Recall: 0.299293
 IoU: 0.221456

We have finished training iteration 175
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_173_.pth
Per-example loss in batch: 0.408250  [    2/   89]
Per-example loss in batch: 0.309664  [    4/   89]
Per-example loss in batch: 0.336585  [    6/   89]
Per-example loss in batch: 0.309200  [    8/   89]
Per-example loss in batch: 0.313006  [   10/   89]
Per-example loss in batch: 0.417355  [   12/   89]
Per-example loss in batch: 0.304396  [   14/   89]
Per-example loss in batch: 0.306206  [   16/   89]
Per-example loss in batch: 0.400253  [   18/   89]
Per-example loss in batch: 0.386909  [   20/   89]
Per-example loss in batch: 0.243263  [   22/   89]
Per-example loss in batch: 0.334359  [   24/   89]
Per-example loss in batch: 0.340853  [   26/   89]
Per-example loss in batch: 0.409441  [   28/   89]
Per-example loss in batch: 0.374377  [   30/   89]
Per-example loss in batch: 0.249515  [   32/   89]
Per-example loss in batch: 0.266343  [   34/   89]
Per-example loss in batch: 0.313515  [   36/   89]
Per-example loss in batch: 0.270951  [   38/   89]
Per-example loss in batch: 0.336438  [   40/   89]
Per-example loss in batch: 0.318547  [   42/   89]
Per-example loss in batch: 0.372774  [   44/   89]
Per-example loss in batch: 0.282529  [   46/   89]
Per-example loss in batch: 0.391903  [   48/   89]
Per-example loss in batch: 0.342202  [   50/   89]
Per-example loss in batch: 0.279703  [   52/   89]
Per-example loss in batch: 0.306994  [   54/   89]
Per-example loss in batch: 0.297716  [   56/   89]
Per-example loss in batch: 0.355977  [   58/   89]
Per-example loss in batch: 0.289816  [   60/   89]
Per-example loss in batch: 0.390419  [   62/   89]
Per-example loss in batch: 0.388292  [   64/   89]
Per-example loss in batch: 0.254247  [   66/   89]
Per-example loss in batch: 0.322877  [   68/   89]
Per-example loss in batch: 0.372157  [   70/   89]
Per-example loss in batch: 0.386478  [   72/   89]
Per-example loss in batch: 0.309461  [   74/   89]
Per-example loss in batch: 0.306269  [   76/   89]
Per-example loss in batch: 0.318910  [   78/   89]
Per-example loss in batch: 0.281814  [   80/   89]
Per-example loss in batch: 0.329466  [   82/   89]
Per-example loss in batch: 0.276667  [   84/   89]
Per-example loss in batch: 0.322178  [   86/   89]
Per-example loss in batch: 0.322164  [   88/   89]
Per-example loss in batch: 0.707239  [   89/   89]
Train Error: Avg loss: 0.33267547
validation Error: 
 Avg loss: 0.50794394 
 F1: 0.367549 
 Precision: 0.495032 
 Recall: 0.292280
 IoU: 0.225151

test Error: 
 Avg loss: 0.48856143 
 F1: 0.380803 
 Precision: 0.484695 
 Recall: 0.313587
 IoU: 0.235180

We have finished training iteration 176
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_174_.pth
Per-example loss in batch: 0.327854  [    2/   89]
Per-example loss in batch: 0.359101  [    4/   89]
Per-example loss in batch: 0.322657  [    6/   89]
Per-example loss in batch: 0.329918  [    8/   89]
Per-example loss in batch: 0.277743  [   10/   89]
Per-example loss in batch: 0.330672  [   12/   89]
Per-example loss in batch: 0.310441  [   14/   89]
Per-example loss in batch: 0.391610  [   16/   89]
Per-example loss in batch: 0.311755  [   18/   89]
Per-example loss in batch: 0.244413  [   20/   89]
Per-example loss in batch: 0.313287  [   22/   89]
Per-example loss in batch: 0.339503  [   24/   89]
Per-example loss in batch: 0.390045  [   26/   89]
Per-example loss in batch: 0.334152  [   28/   89]
Per-example loss in batch: 0.446547  [   30/   89]
Per-example loss in batch: 0.301751  [   32/   89]
Per-example loss in batch: 0.299545  [   34/   89]
Per-example loss in batch: 0.314125  [   36/   89]
Per-example loss in batch: 0.287744  [   38/   89]
Per-example loss in batch: 0.352049  [   40/   89]
Per-example loss in batch: 0.305806  [   42/   89]
Per-example loss in batch: 0.380904  [   44/   89]
Per-example loss in batch: 0.269064  [   46/   89]
Per-example loss in batch: 0.367562  [   48/   89]
Per-example loss in batch: 0.246428  [   50/   89]
Per-example loss in batch: 0.278782  [   52/   89]
Per-example loss in batch: 0.380861  [   54/   89]
Per-example loss in batch: 0.285420  [   56/   89]
Per-example loss in batch: 0.374058  [   58/   89]
Per-example loss in batch: 0.382896  [   60/   89]
Per-example loss in batch: 0.362887  [   62/   89]
Per-example loss in batch: 0.256354  [   64/   89]
Per-example loss in batch: 0.253226  [   66/   89]
Per-example loss in batch: 0.304877  [   68/   89]
Per-example loss in batch: 0.357247  [   70/   89]
Per-example loss in batch: 0.368976  [   72/   89]
Per-example loss in batch: 0.310548  [   74/   89]
Per-example loss in batch: 0.247790  [   76/   89]
Per-example loss in batch: 0.386492  [   78/   89]
Per-example loss in batch: 0.362102  [   80/   89]
Per-example loss in batch: 0.345685  [   82/   89]
Per-example loss in batch: 0.346975  [   84/   89]
Per-example loss in batch: 0.386410  [   86/   89]
Per-example loss in batch: 0.287175  [   88/   89]
Per-example loss in batch: 0.515609  [   89/   89]
Train Error: Avg loss: 0.33014021
validation Error: 
 Avg loss: 0.50925346 
 F1: 0.366430 
 Precision: 0.475259 
 Recall: 0.298156
 IoU: 0.224313

test Error: 
 Avg loss: 0.48944770 
 F1: 0.376791 
 Precision: 0.458415 
 Recall: 0.319841
 IoU: 0.232127

We have finished training iteration 177
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_175_.pth
Per-example loss in batch: 0.320673  [    2/   89]
Per-example loss in batch: 0.265498  [    4/   89]
Per-example loss in batch: 0.300644  [    6/   89]
Per-example loss in batch: 0.319051  [    8/   89]
Per-example loss in batch: 0.260881  [   10/   89]
Per-example loss in batch: 0.377130  [   12/   89]
Per-example loss in batch: 0.375112  [   14/   89]
Per-example loss in batch: 0.384412  [   16/   89]
Per-example loss in batch: 0.313882  [   18/   89]
Per-example loss in batch: 0.324769  [   20/   89]
Per-example loss in batch: 0.313941  [   22/   89]
Per-example loss in batch: 0.407400  [   24/   89]
Per-example loss in batch: 0.355827  [   26/   89]
Per-example loss in batch: 0.309581  [   28/   89]
Per-example loss in batch: 0.303741  [   30/   89]
Per-example loss in batch: 0.300005  [   32/   89]
Per-example loss in batch: 0.251283  [   34/   89]
Per-example loss in batch: 0.400744  [   36/   89]
Per-example loss in batch: 0.383930  [   38/   89]
Per-example loss in batch: 0.270733  [   40/   89]
Per-example loss in batch: 0.356100  [   42/   89]
Per-example loss in batch: 0.349905  [   44/   89]
Per-example loss in batch: 0.294899  [   46/   89]
Per-example loss in batch: 0.340950  [   48/   89]
Per-example loss in batch: 0.265853  [   50/   89]
Per-example loss in batch: 0.315906  [   52/   89]
Per-example loss in batch: 0.323643  [   54/   89]
Per-example loss in batch: 0.347794  [   56/   89]
Per-example loss in batch: 0.335177  [   58/   89]
Per-example loss in batch: 0.380084  [   60/   89]
Per-example loss in batch: 0.308979  [   62/   89]
Per-example loss in batch: 0.347246  [   64/   89]
Per-example loss in batch: 0.304034  [   66/   89]
Per-example loss in batch: 0.253785  [   68/   89]
Per-example loss in batch: 0.352705  [   70/   89]
Per-example loss in batch: 0.322716  [   72/   89]
Per-example loss in batch: 0.378125  [   74/   89]
Per-example loss in batch: 0.258432  [   76/   89]
Per-example loss in batch: 0.350372  [   78/   89]
Per-example loss in batch: 0.279886  [   80/   89]
Per-example loss in batch: 0.357165  [   82/   89]
Per-example loss in batch: 0.409207  [   84/   89]
Per-example loss in batch: 0.350558  [   86/   89]
Per-example loss in batch: 0.352000  [   88/   89]
Per-example loss in batch: 0.624099  [   89/   89]
Train Error: Avg loss: 0.33228781
validation Error: 
 Avg loss: 0.50937384 
 F1: 0.361017 
 Precision: 0.504886 
 Recall: 0.280957
 IoU: 0.220269

test Error: 
 Avg loss: 0.48937827 
 F1: 0.372617 
 Precision: 0.488884 
 Recall: 0.301026
 IoU: 0.228967

We have finished training iteration 178
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_176_.pth
Per-example loss in batch: 0.300844  [    2/   89]
Per-example loss in batch: 0.322565  [    4/   89]
Per-example loss in batch: 0.322944  [    6/   89]
Per-example loss in batch: 0.349630  [    8/   89]
Per-example loss in batch: 0.313196  [   10/   89]
Per-example loss in batch: 0.356545  [   12/   89]
Per-example loss in batch: 0.369359  [   14/   89]
Per-example loss in batch: 0.306616  [   16/   89]
Per-example loss in batch: 0.326120  [   18/   89]
Per-example loss in batch: 0.310618  [   20/   89]
Per-example loss in batch: 0.365457  [   22/   89]
Per-example loss in batch: 0.335719  [   24/   89]
Per-example loss in batch: 0.293656  [   26/   89]
Per-example loss in batch: 0.340872  [   28/   89]
Per-example loss in batch: 0.260647  [   30/   89]
Per-example loss in batch: 0.339194  [   32/   89]
Per-example loss in batch: 0.304204  [   34/   89]
Per-example loss in batch: 0.283927  [   36/   89]
Per-example loss in batch: 0.332869  [   38/   89]
Per-example loss in batch: 0.318309  [   40/   89]
Per-example loss in batch: 0.366519  [   42/   89]
Per-example loss in batch: 0.330931  [   44/   89]
Per-example loss in batch: 0.334905  [   46/   89]
Per-example loss in batch: 0.307798  [   48/   89]
Per-example loss in batch: 0.366347  [   50/   89]
Per-example loss in batch: 0.366713  [   52/   89]
Per-example loss in batch: 0.322001  [   54/   89]
Per-example loss in batch: 0.439671  [   56/   89]
Per-example loss in batch: 0.275351  [   58/   89]
Per-example loss in batch: 0.313202  [   60/   89]
Per-example loss in batch: 0.341426  [   62/   89]
Per-example loss in batch: 0.281833  [   64/   89]
Per-example loss in batch: 0.411098  [   66/   89]
Per-example loss in batch: 0.276912  [   68/   89]
Per-example loss in batch: 0.365624  [   70/   89]
Per-example loss in batch: 0.369063  [   72/   89]
Per-example loss in batch: 0.367002  [   74/   89]
Per-example loss in batch: 0.275021  [   76/   89]
Per-example loss in batch: 0.325265  [   78/   89]
Per-example loss in batch: 0.312322  [   80/   89]
Per-example loss in batch: 0.387724  [   82/   89]
Per-example loss in batch: 0.299475  [   84/   89]
Per-example loss in batch: 0.323498  [   86/   89]
Per-example loss in batch: 0.331415  [   88/   89]
Per-example loss in batch: 0.788596  [   89/   89]
Train Error: Avg loss: 0.33570124
validation Error: 
 Avg loss: 0.50819002 
 F1: 0.371124 
 Precision: 0.495532 
 Recall: 0.296648
 IoU: 0.227841

test Error: 
 Avg loss: 0.48793407 
 F1: 0.385040 
 Precision: 0.481071 
 Recall: 0.320969
 IoU: 0.238421

We have finished training iteration 179
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_177_.pth
Per-example loss in batch: 0.296980  [    2/   89]
Per-example loss in batch: 0.344809  [    4/   89]
Per-example loss in batch: 0.313144  [    6/   89]
Per-example loss in batch: 0.300651  [    8/   89]
Per-example loss in batch: 0.407683  [   10/   89]
Per-example loss in batch: 0.341585  [   12/   89]
Per-example loss in batch: 0.269693  [   14/   89]
Per-example loss in batch: 0.308311  [   16/   89]
Per-example loss in batch: 0.419114  [   18/   89]
Per-example loss in batch: 0.268936  [   20/   89]
Per-example loss in batch: 0.271531  [   22/   89]
Per-example loss in batch: 0.382309  [   24/   89]
Per-example loss in batch: 0.377887  [   26/   89]
Per-example loss in batch: 0.360690  [   28/   89]
Per-example loss in batch: 0.303494  [   30/   89]
Per-example loss in batch: 0.293169  [   32/   89]
Per-example loss in batch: 0.373462  [   34/   89]
Per-example loss in batch: 0.269796  [   36/   89]
Per-example loss in batch: 0.324360  [   38/   89]
Per-example loss in batch: 0.309742  [   40/   89]
Per-example loss in batch: 0.300168  [   42/   89]
Per-example loss in batch: 0.392766  [   44/   89]
Per-example loss in batch: 0.315618  [   46/   89]
Per-example loss in batch: 0.285249  [   48/   89]
Per-example loss in batch: 0.302917  [   50/   89]
Per-example loss in batch: 0.350429  [   52/   89]
Per-example loss in batch: 0.370197  [   54/   89]
Per-example loss in batch: 0.290819  [   56/   89]
Per-example loss in batch: 0.257353  [   58/   89]
Per-example loss in batch: 0.359285  [   60/   89]
Per-example loss in batch: 0.268537  [   62/   89]
Per-example loss in batch: 0.327769  [   64/   89]
Per-example loss in batch: 0.295950  [   66/   89]
Per-example loss in batch: 0.312574  [   68/   89]
Per-example loss in batch: 0.338919  [   70/   89]
Per-example loss in batch: 0.338434  [   72/   89]
Per-example loss in batch: 0.264166  [   74/   89]
Per-example loss in batch: 0.270112  [   76/   89]
Per-example loss in batch: 0.353415  [   78/   89]
Per-example loss in batch: 0.351679  [   80/   89]
Per-example loss in batch: 0.294099  [   82/   89]
Per-example loss in batch: 0.376081  [   84/   89]
Per-example loss in batch: 0.312275  [   86/   89]
Per-example loss in batch: 0.331899  [   88/   89]
Per-example loss in batch: 0.588107  [   89/   89]
Train Error: Avg loss: 0.32566536
validation Error: 
 Avg loss: 0.50967344 
 F1: 0.359101 
 Precision: 0.465252 
 Recall: 0.292389
 IoU: 0.218844

test Error: 
 Avg loss: 0.48876018 
 F1: 0.368675 
 Precision: 0.454410 
 Recall: 0.310156
 IoU: 0.225997

We have finished training iteration 180
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_178_.pth
Per-example loss in batch: 0.240351  [    2/   89]
Per-example loss in batch: 0.309427  [    4/   89]
Per-example loss in batch: 0.354517  [    6/   89]
Per-example loss in batch: 0.317248  [    8/   89]
Per-example loss in batch: 0.313456  [   10/   89]
Per-example loss in batch: 0.287129  [   12/   89]
Per-example loss in batch: 0.275803  [   14/   89]
Per-example loss in batch: 0.358575  [   16/   89]
Per-example loss in batch: 0.334419  [   18/   89]
Per-example loss in batch: 0.415037  [   20/   89]
Per-example loss in batch: 0.322880  [   22/   89]
Per-example loss in batch: 0.245595  [   24/   89]
Per-example loss in batch: 0.351250  [   26/   89]
Per-example loss in batch: 0.310556  [   28/   89]
Per-example loss in batch: 0.289736  [   30/   89]
Per-example loss in batch: 0.362691  [   32/   89]
Per-example loss in batch: 0.311077  [   34/   89]
Per-example loss in batch: 0.375985  [   36/   89]
Per-example loss in batch: 0.363423  [   38/   89]
Per-example loss in batch: 0.229186  [   40/   89]
Per-example loss in batch: 0.363488  [   42/   89]
Per-example loss in batch: 0.282488  [   44/   89]
Per-example loss in batch: 0.355851  [   46/   89]
Per-example loss in batch: 0.366035  [   48/   89]
Per-example loss in batch: 0.297353  [   50/   89]
Per-example loss in batch: 0.332994  [   52/   89]
Per-example loss in batch: 0.296042  [   54/   89]
Per-example loss in batch: 0.317747  [   56/   89]
Per-example loss in batch: 0.312737  [   58/   89]
Per-example loss in batch: 0.340126  [   60/   89]
Per-example loss in batch: 0.277051  [   62/   89]
Per-example loss in batch: 0.329222  [   64/   89]
Per-example loss in batch: 0.382353  [   66/   89]
Per-example loss in batch: 0.299416  [   68/   89]
Per-example loss in batch: 0.372078  [   70/   89]
Per-example loss in batch: 0.322513  [   72/   89]
Per-example loss in batch: 0.331194  [   74/   89]
Per-example loss in batch: 0.319592  [   76/   89]
Per-example loss in batch: 0.345824  [   78/   89]
Per-example loss in batch: 0.316328  [   80/   89]
Per-example loss in batch: 0.316028  [   82/   89]
Per-example loss in batch: 0.337710  [   84/   89]
Per-example loss in batch: 0.263470  [   86/   89]
Per-example loss in batch: 0.358089  [   88/   89]
Per-example loss in batch: 0.732633  [   89/   89]
Train Error: Avg loss: 0.32742445
validation Error: 
 Avg loss: 0.50971448 
 F1: 0.366272 
 Precision: 0.438305 
 Recall: 0.314573
 IoU: 0.224194

test Error: 
 Avg loss: 0.49010327 
 F1: 0.372992 
 Precision: 0.415638 
 Recall: 0.338283
 IoU: 0.229250

We have finished training iteration 181
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_179_.pth
Per-example loss in batch: 0.256022  [    2/   89]
Per-example loss in batch: 0.338433  [    4/   89]
Per-example loss in batch: 0.285014  [    6/   89]
Per-example loss in batch: 0.327637  [    8/   89]
Per-example loss in batch: 0.380601  [   10/   89]
Per-example loss in batch: 0.346375  [   12/   89]
Per-example loss in batch: 0.270031  [   14/   89]
Per-example loss in batch: 0.319524  [   16/   89]
Per-example loss in batch: 0.379911  [   18/   89]
Per-example loss in batch: 0.336050  [   20/   89]
Per-example loss in batch: 0.261778  [   22/   89]
Per-example loss in batch: 0.350088  [   24/   89]
Per-example loss in batch: 0.358539  [   26/   89]
Per-example loss in batch: 0.297798  [   28/   89]
Per-example loss in batch: 0.383904  [   30/   89]
Per-example loss in batch: 0.385569  [   32/   89]
Per-example loss in batch: 0.301332  [   34/   89]
Per-example loss in batch: 0.327465  [   36/   89]
Per-example loss in batch: 0.359989  [   38/   89]
Per-example loss in batch: 0.326297  [   40/   89]
Per-example loss in batch: 0.344328  [   42/   89]
Per-example loss in batch: 0.312133  [   44/   89]
Per-example loss in batch: 0.325379  [   46/   89]
Per-example loss in batch: 0.336579  [   48/   89]
Per-example loss in batch: 0.274556  [   50/   89]
Per-example loss in batch: 0.354233  [   52/   89]
Per-example loss in batch: 0.346430  [   54/   89]
Per-example loss in batch: 0.295371  [   56/   89]
Per-example loss in batch: 0.412929  [   58/   89]
Per-example loss in batch: 0.388833  [   60/   89]
Per-example loss in batch: 0.304472  [   62/   89]
Per-example loss in batch: 0.309095  [   64/   89]
Per-example loss in batch: 0.340779  [   66/   89]
Per-example loss in batch: 0.282098  [   68/   89]
Per-example loss in batch: 0.409457  [   70/   89]
Per-example loss in batch: 0.424483  [   72/   89]
Per-example loss in batch: 0.273301  [   74/   89]
Per-example loss in batch: 0.331134  [   76/   89]
Per-example loss in batch: 0.281863  [   78/   89]
Per-example loss in batch: 0.274368  [   80/   89]
Per-example loss in batch: 0.265774  [   82/   89]
Per-example loss in batch: 0.308391  [   84/   89]
Per-example loss in batch: 0.296338  [   86/   89]
Per-example loss in batch: 0.353895  [   88/   89]
Per-example loss in batch: 0.805346  [   89/   89]
Train Error: Avg loss: 0.33351128
validation Error: 
 Avg loss: 0.51010080 
 F1: 0.351905 
 Precision: 0.457773 
 Recall: 0.285807
 IoU: 0.213522

test Error: 
 Avg loss: 0.49007655 
 F1: 0.360379 
 Precision: 0.451253 
 Recall: 0.299970
 IoU: 0.219794

We have finished training iteration 182
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_180_.pth
Per-example loss in batch: 0.292025  [    2/   89]
Per-example loss in batch: 0.302646  [    4/   89]
Per-example loss in batch: 0.276325  [    6/   89]
Per-example loss in batch: 0.348921  [    8/   89]
Per-example loss in batch: 0.303295  [   10/   89]
Per-example loss in batch: 0.257446  [   12/   89]
Per-example loss in batch: 0.327891  [   14/   89]
Per-example loss in batch: 0.362058  [   16/   89]
Per-example loss in batch: 0.346301  [   18/   89]
Per-example loss in batch: 0.339051  [   20/   89]
Per-example loss in batch: 0.350470  [   22/   89]
Per-example loss in batch: 0.280096  [   24/   89]
Per-example loss in batch: 0.284668  [   26/   89]
Per-example loss in batch: 0.348497  [   28/   89]
Per-example loss in batch: 0.371134  [   30/   89]
Per-example loss in batch: 0.321745  [   32/   89]
Per-example loss in batch: 0.271698  [   34/   89]
Per-example loss in batch: 0.305730  [   36/   89]
Per-example loss in batch: 0.304928  [   38/   89]
Per-example loss in batch: 0.372745  [   40/   89]
Per-example loss in batch: 0.314468  [   42/   89]
Per-example loss in batch: 0.271059  [   44/   89]
Per-example loss in batch: 0.403465  [   46/   89]
Per-example loss in batch: 0.295972  [   48/   89]
Per-example loss in batch: 0.370554  [   50/   89]
Per-example loss in batch: 0.298133  [   52/   89]
Per-example loss in batch: 0.326798  [   54/   89]
Per-example loss in batch: 0.343503  [   56/   89]
Per-example loss in batch: 0.317643  [   58/   89]
Per-example loss in batch: 0.277324  [   60/   89]
Per-example loss in batch: 0.301804  [   62/   89]
Per-example loss in batch: 0.356874  [   64/   89]
Per-example loss in batch: 0.318402  [   66/   89]
Per-example loss in batch: 0.328732  [   68/   89]
Per-example loss in batch: 0.345603  [   70/   89]
Per-example loss in batch: 0.337148  [   72/   89]
Per-example loss in batch: 0.321887  [   74/   89]
Per-example loss in batch: 0.339393  [   76/   89]
Per-example loss in batch: 0.307978  [   78/   89]
Per-example loss in batch: 0.364052  [   80/   89]
Per-example loss in batch: 0.282502  [   82/   89]
Per-example loss in batch: 0.389995  [   84/   89]
Per-example loss in batch: 0.393912  [   86/   89]
Per-example loss in batch: 0.285826  [   88/   89]
Per-example loss in batch: 0.550375  [   89/   89]
Train Error: Avg loss: 0.32664905
validation Error: 
 Avg loss: 0.50868049 
 F1: 0.364806 
 Precision: 0.467731 
 Recall: 0.299009
 IoU: 0.223097

test Error: 
 Avg loss: 0.48865671 
 F1: 0.378312 
 Precision: 0.456185 
 Recall: 0.323149
 IoU: 0.233283

We have finished training iteration 183
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_181_.pth
Per-example loss in batch: 0.311897  [    2/   89]
Per-example loss in batch: 0.315945  [    4/   89]
Per-example loss in batch: 0.337221  [    6/   89]
Per-example loss in batch: 0.348302  [    8/   89]
Per-example loss in batch: 0.345039  [   10/   89]
Per-example loss in batch: 0.355472  [   12/   89]
Per-example loss in batch: 0.310831  [   14/   89]
Per-example loss in batch: 0.352064  [   16/   89]
Per-example loss in batch: 0.347955  [   18/   89]
Per-example loss in batch: 0.285808  [   20/   89]
Per-example loss in batch: 0.292870  [   22/   89]
Per-example loss in batch: 0.295348  [   24/   89]
Per-example loss in batch: 0.269605  [   26/   89]
Per-example loss in batch: 0.343993  [   28/   89]
Per-example loss in batch: 0.243432  [   30/   89]
Per-example loss in batch: 0.293961  [   32/   89]
Per-example loss in batch: 0.285756  [   34/   89]
Per-example loss in batch: 0.312417  [   36/   89]
Per-example loss in batch: 0.314294  [   38/   89]
Per-example loss in batch: 0.287923  [   40/   89]
Per-example loss in batch: 0.317809  [   42/   89]
Per-example loss in batch: 0.341054  [   44/   89]
Per-example loss in batch: 0.399260  [   46/   89]
Per-example loss in batch: 0.332593  [   48/   89]
Per-example loss in batch: 0.378603  [   50/   89]
Per-example loss in batch: 0.261563  [   52/   89]
Per-example loss in batch: 0.362933  [   54/   89]
Per-example loss in batch: 0.384109  [   56/   89]
Per-example loss in batch: 0.368614  [   58/   89]
Per-example loss in batch: 0.339068  [   60/   89]
Per-example loss in batch: 0.275488  [   62/   89]
Per-example loss in batch: 0.298310  [   64/   89]
Per-example loss in batch: 0.278511  [   66/   89]
Per-example loss in batch: 0.330638  [   68/   89]
Per-example loss in batch: 0.332083  [   70/   89]
Per-example loss in batch: 0.310738  [   72/   89]
Per-example loss in batch: 0.303700  [   74/   89]
Per-example loss in batch: 0.341472  [   76/   89]
Per-example loss in batch: 0.302699  [   78/   89]
Per-example loss in batch: 0.380766  [   80/   89]
Per-example loss in batch: 0.316515  [   82/   89]
Per-example loss in batch: 0.322283  [   84/   89]
Per-example loss in batch: 0.325881  [   86/   89]
Per-example loss in batch: 0.279824  [   88/   89]
Per-example loss in batch: 0.490415  [   89/   89]
Train Error: Avg loss: 0.32314273
validation Error: 
 Avg loss: 0.50850502 
 F1: 0.367142 
 Precision: 0.425041 
 Recall: 0.323126
 IoU: 0.224846

test Error: 
 Avg loss: 0.48897391 
 F1: 0.383256 
 Precision: 0.415944 
 Recall: 0.355331
 IoU: 0.237054

We have finished training iteration 184
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_182_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.308474  [    2/   89]
Per-example loss in batch: 0.312075  [    4/   89]
Per-example loss in batch: 0.370077  [    6/   89]
Per-example loss in batch: 0.248977  [    8/   89]
Per-example loss in batch: 0.301319  [   10/   89]
Per-example loss in batch: 0.315852  [   12/   89]
Per-example loss in batch: 0.323152  [   14/   89]
Per-example loss in batch: 0.394228  [   16/   89]
Per-example loss in batch: 0.311007  [   18/   89]
Per-example loss in batch: 0.295033  [   20/   89]
Per-example loss in batch: 0.315960  [   22/   89]
Per-example loss in batch: 0.286695  [   24/   89]
Per-example loss in batch: 0.262301  [   26/   89]
Per-example loss in batch: 0.393064  [   28/   89]
Per-example loss in batch: 0.261822  [   30/   89]
Per-example loss in batch: 0.285080  [   32/   89]
Per-example loss in batch: 0.354830  [   34/   89]
Per-example loss in batch: 0.298978  [   36/   89]
Per-example loss in batch: 0.358774  [   38/   89]
Per-example loss in batch: 0.312319  [   40/   89]
Per-example loss in batch: 0.392292  [   42/   89]
Per-example loss in batch: 0.331983  [   44/   89]
Per-example loss in batch: 0.242235  [   46/   89]
Per-example loss in batch: 0.358011  [   48/   89]
Per-example loss in batch: 0.371545  [   50/   89]
Per-example loss in batch: 0.276390  [   52/   89]
Per-example loss in batch: 0.371012  [   54/   89]
Per-example loss in batch: 0.344682  [   56/   89]
Per-example loss in batch: 0.301583  [   58/   89]
Per-example loss in batch: 0.344124  [   60/   89]
Per-example loss in batch: 0.333428  [   62/   89]
Per-example loss in batch: 0.323944  [   64/   89]
Per-example loss in batch: 0.311169  [   66/   89]
Per-example loss in batch: 0.357514  [   68/   89]
Per-example loss in batch: 0.294315  [   70/   89]
Per-example loss in batch: 0.362827  [   72/   89]
Per-example loss in batch: 0.401335  [   74/   89]
Per-example loss in batch: 0.424163  [   76/   89]
Per-example loss in batch: 0.287979  [   78/   89]
Per-example loss in batch: 0.263067  [   80/   89]
Per-example loss in batch: 0.259579  [   82/   89]
Per-example loss in batch: 0.320593  [   84/   89]
Per-example loss in batch: 0.317685  [   86/   89]
Per-example loss in batch: 0.263744  [   88/   89]
Per-example loss in batch: 0.599358  [   89/   89]
Train Error: Avg loss: 0.32505385
validation Error: 
 Avg loss: 0.50889146 
 F1: 0.354983 
 Precision: 0.474369 
 Recall: 0.283607
 IoU: 0.215793

test Error: 
 Avg loss: 0.48917335 
 F1: 0.365524 
 Precision: 0.465436 
 Recall: 0.300926
 IoU: 0.223634

We have finished training iteration 185
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_183_.pth
Per-example loss in batch: 0.333573  [    2/   89]
Per-example loss in batch: 0.288003  [    4/   89]
Per-example loss in batch: 0.368640  [    6/   89]
Per-example loss in batch: 0.443847  [    8/   89]
Per-example loss in batch: 0.285233  [   10/   89]
Per-example loss in batch: 0.421641  [   12/   89]
Per-example loss in batch: 0.248482  [   14/   89]
Per-example loss in batch: 0.436275  [   16/   89]
Per-example loss in batch: 0.337255  [   18/   89]
Per-example loss in batch: 0.318758  [   20/   89]
Per-example loss in batch: 0.252158  [   22/   89]
Per-example loss in batch: 0.302540  [   24/   89]
Per-example loss in batch: 0.249616  [   26/   89]
Per-example loss in batch: 0.339956  [   28/   89]
Per-example loss in batch: 0.393987  [   30/   89]
Per-example loss in batch: 0.343962  [   32/   89]
Per-example loss in batch: 0.307098  [   34/   89]
Per-example loss in batch: 0.253988  [   36/   89]
Per-example loss in batch: 0.350763  [   38/   89]
Per-example loss in batch: 0.339648  [   40/   89]
Per-example loss in batch: 0.359419  [   42/   89]
Per-example loss in batch: 0.294542  [   44/   89]
Per-example loss in batch: 0.286013  [   46/   89]
Per-example loss in batch: 0.325418  [   48/   89]
Per-example loss in batch: 0.351812  [   50/   89]
Per-example loss in batch: 0.267821  [   52/   89]
Per-example loss in batch: 0.322118  [   54/   89]
Per-example loss in batch: 0.285947  [   56/   89]
Per-example loss in batch: 0.256891  [   58/   89]
Per-example loss in batch: 0.379442  [   60/   89]
Per-example loss in batch: 0.357946  [   62/   89]
Per-example loss in batch: 0.343234  [   64/   89]
Per-example loss in batch: 0.375833  [   66/   89]
Per-example loss in batch: 0.315123  [   68/   89]
Per-example loss in batch: 0.327492  [   70/   89]
Per-example loss in batch: 0.421624  [   72/   89]
Per-example loss in batch: 0.333875  [   74/   89]
Per-example loss in batch: 0.280251  [   76/   89]
Per-example loss in batch: 0.413407  [   78/   89]
Per-example loss in batch: 0.334767  [   80/   89]
Per-example loss in batch: 0.318750  [   82/   89]
Per-example loss in batch: 0.303348  [   84/   89]
Per-example loss in batch: 0.335214  [   86/   89]
Per-example loss in batch: 0.278470  [   88/   89]
Per-example loss in batch: 0.593097  [   89/   89]
Train Error: Avg loss: 0.33215121
validation Error: 
 Avg loss: 0.50833116 
 F1: 0.356278 
 Precision: 0.481253 
 Recall: 0.282831
 IoU: 0.216751

test Error: 
 Avg loss: 0.48926351 
 F1: 0.365214 
 Precision: 0.465904 
 Recall: 0.300311
 IoU: 0.223402

We have finished training iteration 186
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_184_.pth
Per-example loss in batch: 0.324061  [    2/   89]
Per-example loss in batch: 0.305210  [    4/   89]
Per-example loss in batch: 0.354788  [    6/   89]
Per-example loss in batch: 0.289964  [    8/   89]
Per-example loss in batch: 0.334284  [   10/   89]
Per-example loss in batch: 0.350393  [   12/   89]
Per-example loss in batch: 0.343013  [   14/   89]
Per-example loss in batch: 0.272904  [   16/   89]
Per-example loss in batch: 0.312119  [   18/   89]
Per-example loss in batch: 0.400835  [   20/   89]
Per-example loss in batch: 0.268732  [   22/   89]
Per-example loss in batch: 0.336832  [   24/   89]
Per-example loss in batch: 0.366073  [   26/   89]
Per-example loss in batch: 0.295020  [   28/   89]
Per-example loss in batch: 0.336888  [   30/   89]
Per-example loss in batch: 0.392464  [   32/   89]
Per-example loss in batch: 0.252212  [   34/   89]
Per-example loss in batch: 0.374708  [   36/   89]
Per-example loss in batch: 0.293147  [   38/   89]
Per-example loss in batch: 0.367243  [   40/   89]
Per-example loss in batch: 0.321219  [   42/   89]
Per-example loss in batch: 0.345034  [   44/   89]
Per-example loss in batch: 0.340466  [   46/   89]
Per-example loss in batch: 0.274273  [   48/   89]
Per-example loss in batch: 0.307649  [   50/   89]
Per-example loss in batch: 0.307892  [   52/   89]
Per-example loss in batch: 0.397727  [   54/   89]
Per-example loss in batch: 0.249915  [   56/   89]
Per-example loss in batch: 0.338341  [   58/   89]
Per-example loss in batch: 0.396596  [   60/   89]
Per-example loss in batch: 0.426710  [   62/   89]
Per-example loss in batch: 0.400328  [   64/   89]
Per-example loss in batch: 0.291709  [   66/   89]
Per-example loss in batch: 0.280517  [   68/   89]
Per-example loss in batch: 0.357843  [   70/   89]
Per-example loss in batch: 0.340836  [   72/   89]
Per-example loss in batch: 0.271497  [   74/   89]
Per-example loss in batch: 0.301119  [   76/   89]
Per-example loss in batch: 0.384622  [   78/   89]
Per-example loss in batch: 0.345708  [   80/   89]
Per-example loss in batch: 0.303915  [   82/   89]
Per-example loss in batch: 0.310730  [   84/   89]
Per-example loss in batch: 0.249624  [   86/   89]
Per-example loss in batch: 0.266151  [   88/   89]
Per-example loss in batch: 0.725434  [   89/   89]
Train Error: Avg loss: 0.33132645
validation Error: 
 Avg loss: 0.50879461 
 F1: 0.368615 
 Precision: 0.503966 
 Recall: 0.290575
 IoU: 0.225953

test Error: 
 Avg loss: 0.48831478 
 F1: 0.383447 
 Precision: 0.499202 
 Recall: 0.311270
 IoU: 0.237200

We have finished training iteration 187
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_185_.pth
Per-example loss in batch: 0.449991  [    2/   89]
Per-example loss in batch: 0.264349  [    4/   89]
Per-example loss in batch: 0.385380  [    6/   89]
Per-example loss in batch: 0.256747  [    8/   89]
Per-example loss in batch: 0.261829  [   10/   89]
Per-example loss in batch: 0.330270  [   12/   89]
Per-example loss in batch: 0.412132  [   14/   89]
Per-example loss in batch: 0.354043  [   16/   89]
Per-example loss in batch: 0.331836  [   18/   89]
Per-example loss in batch: 0.370719  [   20/   89]
Per-example loss in batch: 0.248891  [   22/   89]
Per-example loss in batch: 0.309220  [   24/   89]
Per-example loss in batch: 0.327264  [   26/   89]
Per-example loss in batch: 0.320232  [   28/   89]
Per-example loss in batch: 0.281849  [   30/   89]
Per-example loss in batch: 0.355664  [   32/   89]
Per-example loss in batch: 0.329365  [   34/   89]
Per-example loss in batch: 0.307316  [   36/   89]
Per-example loss in batch: 0.360973  [   38/   89]
Per-example loss in batch: 0.336252  [   40/   89]
Per-example loss in batch: 0.272451  [   42/   89]
Per-example loss in batch: 0.244637  [   44/   89]
Per-example loss in batch: 0.355881  [   46/   89]
Per-example loss in batch: 0.295764  [   48/   89]
Per-example loss in batch: 0.362837  [   50/   89]
Per-example loss in batch: 0.286430  [   52/   89]
Per-example loss in batch: 0.365750  [   54/   89]
Per-example loss in batch: 0.335386  [   56/   89]
Per-example loss in batch: 0.398272  [   58/   89]
Per-example loss in batch: 0.284869  [   60/   89]
Per-example loss in batch: 0.321694  [   62/   89]
Per-example loss in batch: 0.351833  [   64/   89]
Per-example loss in batch: 0.246022  [   66/   89]
Per-example loss in batch: 0.332932  [   68/   89]
Per-example loss in batch: 0.344414  [   70/   89]
Per-example loss in batch: 0.367561  [   72/   89]
Per-example loss in batch: 0.296601  [   74/   89]
Per-example loss in batch: 0.311118  [   76/   89]
Per-example loss in batch: 0.419225  [   78/   89]
Per-example loss in batch: 0.431019  [   80/   89]
Per-example loss in batch: 0.350030  [   82/   89]
Per-example loss in batch: 0.305947  [   84/   89]
Per-example loss in batch: 0.327892  [   86/   89]
Per-example loss in batch: 0.344875  [   88/   89]
Per-example loss in batch: 0.556079  [   89/   89]
Train Error: Avg loss: 0.33316411
validation Error: 
 Avg loss: 0.50853474 
 F1: 0.370730 
 Precision: 0.518460 
 Recall: 0.288520
 IoU: 0.227544

test Error: 
 Avg loss: 0.48862947 
 F1: 0.385902 
 Precision: 0.504129 
 Recall: 0.312593
 IoU: 0.239082

We have finished training iteration 188
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_186_.pth
Per-example loss in batch: 0.328200  [    2/   89]
Per-example loss in batch: 0.379399  [    4/   89]
Per-example loss in batch: 0.344518  [    6/   89]
Per-example loss in batch: 0.302487  [    8/   89]
Per-example loss in batch: 0.430316  [   10/   89]
Per-example loss in batch: 0.264549  [   12/   89]
Per-example loss in batch: 0.279422  [   14/   89]
Per-example loss in batch: 0.335467  [   16/   89]
Per-example loss in batch: 0.350181  [   18/   89]
Per-example loss in batch: 0.369741  [   20/   89]
Per-example loss in batch: 0.353111  [   22/   89]
Per-example loss in batch: 0.402723  [   24/   89]
Per-example loss in batch: 0.381355  [   26/   89]
Per-example loss in batch: 0.349411  [   28/   89]
Per-example loss in batch: 0.301378  [   30/   89]
Per-example loss in batch: 0.371032  [   32/   89]
Per-example loss in batch: 0.330046  [   34/   89]
Per-example loss in batch: 0.353911  [   36/   89]
Per-example loss in batch: 0.361737  [   38/   89]
Per-example loss in batch: 0.378187  [   40/   89]
Per-example loss in batch: 0.336439  [   42/   89]
Per-example loss in batch: 0.303613  [   44/   89]
Per-example loss in batch: 0.247365  [   46/   89]
Per-example loss in batch: 0.317812  [   48/   89]
Per-example loss in batch: 0.341855  [   50/   89]
Per-example loss in batch: 0.315744  [   52/   89]
Per-example loss in batch: 0.310753  [   54/   89]
Per-example loss in batch: 0.335991  [   56/   89]
Per-example loss in batch: 0.231782  [   58/   89]
Per-example loss in batch: 0.303235  [   60/   89]
Per-example loss in batch: 0.340835  [   62/   89]
Per-example loss in batch: 0.266126  [   64/   89]
Per-example loss in batch: 0.418342  [   66/   89]
Per-example loss in batch: 0.295068  [   68/   89]
Per-example loss in batch: 0.256147  [   70/   89]
Per-example loss in batch: 0.246992  [   72/   89]
Per-example loss in batch: 0.319313  [   74/   89]
Per-example loss in batch: 0.350452  [   76/   89]
Per-example loss in batch: 0.373214  [   78/   89]
Per-example loss in batch: 0.342329  [   80/   89]
Per-example loss in batch: 0.406268  [   82/   89]
Per-example loss in batch: 0.289609  [   84/   89]
Per-example loss in batch: 0.355861  [   86/   89]
Per-example loss in batch: 0.243664  [   88/   89]
Per-example loss in batch: 0.651453  [   89/   89]
Train Error: Avg loss: 0.33352153
validation Error: 
 Avg loss: 0.50792738 
 F1: 0.368529 
 Precision: 0.538973 
 Recall: 0.279986
 IoU: 0.225888

test Error: 
 Avg loss: 0.48749172 
 F1: 0.386989 
 Precision: 0.525732 
 Recall: 0.306185
 IoU: 0.239917

We have finished training iteration 189
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_187_.pth
Per-example loss in batch: 0.286280  [    2/   89]
Per-example loss in batch: 0.386674  [    4/   89]
Per-example loss in batch: 0.270663  [    6/   89]
Per-example loss in batch: 0.306236  [    8/   89]
Per-example loss in batch: 0.336484  [   10/   89]
Per-example loss in batch: 0.361016  [   12/   89]
Per-example loss in batch: 0.280292  [   14/   89]
Per-example loss in batch: 0.263697  [   16/   89]
Per-example loss in batch: 0.377944  [   18/   89]
Per-example loss in batch: 0.295903  [   20/   89]
Per-example loss in batch: 0.299285  [   22/   89]
Per-example loss in batch: 0.315385  [   24/   89]
Per-example loss in batch: 0.333315  [   26/   89]
Per-example loss in batch: 0.311597  [   28/   89]
Per-example loss in batch: 0.339974  [   30/   89]
Per-example loss in batch: 0.326982  [   32/   89]
Per-example loss in batch: 0.342292  [   34/   89]
Per-example loss in batch: 0.392731  [   36/   89]
Per-example loss in batch: 0.366154  [   38/   89]
Per-example loss in batch: 0.306129  [   40/   89]
Per-example loss in batch: 0.397255  [   42/   89]
Per-example loss in batch: 0.296491  [   44/   89]
Per-example loss in batch: 0.286939  [   46/   89]
Per-example loss in batch: 0.379089  [   48/   89]
Per-example loss in batch: 0.372389  [   50/   89]
Per-example loss in batch: 0.265093  [   52/   89]
Per-example loss in batch: 0.271446  [   54/   89]
Per-example loss in batch: 0.318202  [   56/   89]
Per-example loss in batch: 0.330908  [   58/   89]
Per-example loss in batch: 0.333979  [   60/   89]
Per-example loss in batch: 0.277046  [   62/   89]
Per-example loss in batch: 0.274500  [   64/   89]
Per-example loss in batch: 0.323966  [   66/   89]
Per-example loss in batch: 0.353470  [   68/   89]
Per-example loss in batch: 0.305958  [   70/   89]
Per-example loss in batch: 0.262912  [   72/   89]
Per-example loss in batch: 0.275963  [   74/   89]
Per-example loss in batch: 0.371543  [   76/   89]
Per-example loss in batch: 0.295003  [   78/   89]
Per-example loss in batch: 0.349562  [   80/   89]
Per-example loss in batch: 0.438557  [   82/   89]
Per-example loss in batch: 0.419253  [   84/   89]
Per-example loss in batch: 0.352744  [   86/   89]
Per-example loss in batch: 0.393683  [   88/   89]
Per-example loss in batch: 0.777392  [   89/   89]
Train Error: Avg loss: 0.33334103
validation Error: 
 Avg loss: 0.50813046 
 F1: 0.365423 
 Precision: 0.543079 
 Recall: 0.275349
 IoU: 0.223558

test Error: 
 Avg loss: 0.48764322 
 F1: 0.380628 
 Precision: 0.529680 
 Recall: 0.297041
 IoU: 0.235047

We have finished training iteration 190
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_188_.pth
Per-example loss in batch: 0.279698  [    2/   89]
Per-example loss in batch: 0.285351  [    4/   89]
Per-example loss in batch: 0.386194  [    6/   89]
Per-example loss in batch: 0.419008  [    8/   89]
Per-example loss in batch: 0.305105  [   10/   89]
Per-example loss in batch: 0.287925  [   12/   89]
Per-example loss in batch: 0.241902  [   14/   89]
Per-example loss in batch: 0.362375  [   16/   89]
Per-example loss in batch: 0.464948  [   18/   89]
Per-example loss in batch: 0.307681  [   20/   89]
Per-example loss in batch: 0.343756  [   22/   89]
Per-example loss in batch: 0.303954  [   24/   89]
Per-example loss in batch: 0.347013  [   26/   89]
Per-example loss in batch: 0.358414  [   28/   89]
Per-example loss in batch: 0.374871  [   30/   89]
Per-example loss in batch: 0.405443  [   32/   89]
Per-example loss in batch: 0.321539  [   34/   89]
Per-example loss in batch: 0.322009  [   36/   89]
Per-example loss in batch: 0.268954  [   38/   89]
Per-example loss in batch: 0.280081  [   40/   89]
Per-example loss in batch: 0.320154  [   42/   89]
Per-example loss in batch: 0.370140  [   44/   89]
Per-example loss in batch: 0.316183  [   46/   89]
Per-example loss in batch: 0.287082  [   48/   89]
Per-example loss in batch: 0.398096  [   50/   89]
Per-example loss in batch: 0.290581  [   52/   89]
Per-example loss in batch: 0.329578  [   54/   89]
Per-example loss in batch: 0.336912  [   56/   89]
Per-example loss in batch: 0.379189  [   58/   89]
Per-example loss in batch: 0.258759  [   60/   89]
Per-example loss in batch: 0.293924  [   62/   89]
Per-example loss in batch: 0.302548  [   64/   89]
Per-example loss in batch: 0.357806  [   66/   89]
Per-example loss in batch: 0.374471  [   68/   89]
Per-example loss in batch: 0.272805  [   70/   89]
Per-example loss in batch: 0.325836  [   72/   89]
Per-example loss in batch: 0.373277  [   74/   89]
Per-example loss in batch: 0.252970  [   76/   89]
Per-example loss in batch: 0.387331  [   78/   89]
Per-example loss in batch: 0.301814  [   80/   89]
Per-example loss in batch: 0.246915  [   82/   89]
Per-example loss in batch: 0.312034  [   84/   89]
Per-example loss in batch: 0.281963  [   86/   89]
Per-example loss in batch: 0.350268  [   88/   89]
Per-example loss in batch: 0.749489  [   89/   89]
Train Error: Avg loss: 0.33172131
validation Error: 
 Avg loss: 0.51002221 
 F1: 0.367157 
 Precision: 0.476848 
 Recall: 0.298493
 IoU: 0.224857

test Error: 
 Avg loss: 0.49023016 
 F1: 0.374635 
 Precision: 0.459932 
 Recall: 0.316026
 IoU: 0.230493

We have finished training iteration 191
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_189_.pth
Per-example loss in batch: 0.347895  [    2/   89]
Per-example loss in batch: 0.317360  [    4/   89]
Per-example loss in batch: 0.303402  [    6/   89]
Per-example loss in batch: 0.363338  [    8/   89]
Per-example loss in batch: 0.345848  [   10/   89]
Per-example loss in batch: 0.363252  [   12/   89]
Per-example loss in batch: 0.331784  [   14/   89]
Per-example loss in batch: 0.366794  [   16/   89]
Per-example loss in batch: 0.302201  [   18/   89]
Per-example loss in batch: 0.309101  [   20/   89]
Per-example loss in batch: 0.286647  [   22/   89]
Per-example loss in batch: 0.349871  [   24/   89]
Per-example loss in batch: 0.385147  [   26/   89]
Per-example loss in batch: 0.259737  [   28/   89]
Per-example loss in batch: 0.354762  [   30/   89]
Per-example loss in batch: 0.289868  [   32/   89]
Per-example loss in batch: 0.315271  [   34/   89]
Per-example loss in batch: 0.305377  [   36/   89]
Per-example loss in batch: 0.332759  [   38/   89]
Per-example loss in batch: 0.328014  [   40/   89]
Per-example loss in batch: 0.286576  [   42/   89]
Per-example loss in batch: 0.363958  [   44/   89]
Per-example loss in batch: 0.351716  [   46/   89]
Per-example loss in batch: 0.295304  [   48/   89]
Per-example loss in batch: 0.328766  [   50/   89]
Per-example loss in batch: 0.243628  [   52/   89]
Per-example loss in batch: 0.370920  [   54/   89]
Per-example loss in batch: 0.439487  [   56/   89]
Per-example loss in batch: 0.339575  [   58/   89]
Per-example loss in batch: 0.310768  [   60/   89]
Per-example loss in batch: 0.265924  [   62/   89]
Per-example loss in batch: 0.292488  [   64/   89]
Per-example loss in batch: 0.373271  [   66/   89]
Per-example loss in batch: 0.347014  [   68/   89]
Per-example loss in batch: 0.363435  [   70/   89]
Per-example loss in batch: 0.286615  [   72/   89]
Per-example loss in batch: 0.338738  [   74/   89]
Per-example loss in batch: 0.276617  [   76/   89]
Per-example loss in batch: 0.273279  [   78/   89]
Per-example loss in batch: 0.261334  [   80/   89]
Per-example loss in batch: 0.337661  [   82/   89]
Per-example loss in batch: 0.254842  [   84/   89]
Per-example loss in batch: 0.320191  [   86/   89]
Per-example loss in batch: 0.263263  [   88/   89]
Per-example loss in batch: 0.669519  [   89/   89]
Train Error: Avg loss: 0.32536089
validation Error: 
 Avg loss: 0.50884781 
 F1: 0.364415 
 Precision: 0.491506 
 Recall: 0.289546
 IoU: 0.222804

test Error: 
 Avg loss: 0.48834804 
 F1: 0.374689 
 Precision: 0.477328 
 Recall: 0.308378
 IoU: 0.230533

We have finished training iteration 192
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_190_.pth
Per-example loss in batch: 0.284064  [    2/   89]
Per-example loss in batch: 0.298586  [    4/   89]
Per-example loss in batch: 0.351015  [    6/   89]
Per-example loss in batch: 0.280861  [    8/   89]
Per-example loss in batch: 0.326778  [   10/   89]
Per-example loss in batch: 0.341199  [   12/   89]
Per-example loss in batch: 0.298776  [   14/   89]
Per-example loss in batch: 0.371655  [   16/   89]
Per-example loss in batch: 0.242658  [   18/   89]
Per-example loss in batch: 0.382346  [   20/   89]
Per-example loss in batch: 0.261991  [   22/   89]
Per-example loss in batch: 0.273215  [   24/   89]
Per-example loss in batch: 0.329038  [   26/   89]
Per-example loss in batch: 0.227169  [   28/   89]
Per-example loss in batch: 0.312745  [   30/   89]
Per-example loss in batch: 0.309447  [   32/   89]
Per-example loss in batch: 0.301682  [   34/   89]
Per-example loss in batch: 0.325248  [   36/   89]
Per-example loss in batch: 0.372052  [   38/   89]
Per-example loss in batch: 0.312324  [   40/   89]
Per-example loss in batch: 0.331988  [   42/   89]
Per-example loss in batch: 0.331481  [   44/   89]
Per-example loss in batch: 0.295396  [   46/   89]
Per-example loss in batch: 0.303332  [   48/   89]
Per-example loss in batch: 0.362103  [   50/   89]
Per-example loss in batch: 0.269067  [   52/   89]
Per-example loss in batch: 0.330068  [   54/   89]
Per-example loss in batch: 0.390673  [   56/   89]
Per-example loss in batch: 0.362634  [   58/   89]
Per-example loss in batch: 0.324835  [   60/   89]
Per-example loss in batch: 0.334546  [   62/   89]
Per-example loss in batch: 0.298323  [   64/   89]
Per-example loss in batch: 0.365493  [   66/   89]
Per-example loss in batch: 0.292454  [   68/   89]
Per-example loss in batch: 0.267391  [   70/   89]
Per-example loss in batch: 0.291420  [   72/   89]
Per-example loss in batch: 0.344757  [   74/   89]
Per-example loss in batch: 0.392809  [   76/   89]
Per-example loss in batch: 0.363210  [   78/   89]
Per-example loss in batch: 0.286817  [   80/   89]
Per-example loss in batch: 0.341581  [   82/   89]
Per-example loss in batch: 0.329527  [   84/   89]
Per-example loss in batch: 0.438010  [   86/   89]
Per-example loss in batch: 0.344275  [   88/   89]
Per-example loss in batch: 0.637538  [   89/   89]
Train Error: Avg loss: 0.32615300
validation Error: 
 Avg loss: 0.50902004 
 F1: 0.365714 
 Precision: 0.506251 
 Recall: 0.286250
 IoU: 0.223776

test Error: 
 Avg loss: 0.48921147 
 F1: 0.376158 
 Precision: 0.482300 
 Recall: 0.308307
 IoU: 0.231647

We have finished training iteration 193
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_191_.pth
Per-example loss in batch: 0.305526  [    2/   89]
Per-example loss in batch: 0.361516  [    4/   89]
Per-example loss in batch: 0.357107  [    6/   89]
Per-example loss in batch: 0.285780  [    8/   89]
Per-example loss in batch: 0.335174  [   10/   89]
Per-example loss in batch: 0.265135  [   12/   89]
Per-example loss in batch: 0.397297  [   14/   89]
Per-example loss in batch: 0.305983  [   16/   89]
Per-example loss in batch: 0.347779  [   18/   89]
Per-example loss in batch: 0.324650  [   20/   89]
Per-example loss in batch: 0.360664  [   22/   89]
Per-example loss in batch: 0.252969  [   24/   89]
Per-example loss in batch: 0.329729  [   26/   89]
Per-example loss in batch: 0.326506  [   28/   89]
Per-example loss in batch: 0.292324  [   30/   89]
Per-example loss in batch: 0.313758  [   32/   89]
Per-example loss in batch: 0.296486  [   34/   89]
Per-example loss in batch: 0.267764  [   36/   89]
Per-example loss in batch: 0.318570  [   38/   89]
Per-example loss in batch: 0.386132  [   40/   89]
Per-example loss in batch: 0.291829  [   42/   89]
Per-example loss in batch: 0.285516  [   44/   89]
Per-example loss in batch: 0.337972  [   46/   89]
Per-example loss in batch: 0.377967  [   48/   89]
Per-example loss in batch: 0.313047  [   50/   89]
Per-example loss in batch: 0.332152  [   52/   89]
Per-example loss in batch: 0.371333  [   54/   89]
Per-example loss in batch: 0.355747  [   56/   89]
Per-example loss in batch: 0.339831  [   58/   89]
Per-example loss in batch: 0.381816  [   60/   89]
Per-example loss in batch: 0.316149  [   62/   89]
Per-example loss in batch: 0.330692  [   64/   89]
Per-example loss in batch: 0.401438  [   66/   89]
Per-example loss in batch: 0.302910  [   68/   89]
Per-example loss in batch: 0.278381  [   70/   89]
Per-example loss in batch: 0.397499  [   72/   89]
Per-example loss in batch: 0.323242  [   74/   89]
Per-example loss in batch: 0.287940  [   76/   89]
Per-example loss in batch: 0.372632  [   78/   89]
Per-example loss in batch: 0.322034  [   80/   89]
Per-example loss in batch: 0.317252  [   82/   89]
Per-example loss in batch: 0.263281  [   84/   89]
Per-example loss in batch: 0.316459  [   86/   89]
Per-example loss in batch: 0.286556  [   88/   89]
Per-example loss in batch: 0.809692  [   89/   89]
Train Error: Avg loss: 0.33122179
validation Error: 
 Avg loss: 0.50965682 
 F1: 0.360031 
 Precision: 0.404108 
 Recall: 0.324624
 IoU: 0.219535

test Error: 
 Avg loss: 0.48931345 
 F1: 0.376182 
 Precision: 0.395129 
 Recall: 0.358969
 IoU: 0.231665

We have finished training iteration 194
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_192_.pth
Per-example loss in batch: 0.271901  [    2/   89]
Per-example loss in batch: 0.293873  [    4/   89]
Per-example loss in batch: 0.285162  [    6/   89]
Per-example loss in batch: 0.362204  [    8/   89]
Per-example loss in batch: 0.242884  [   10/   89]
Per-example loss in batch: 0.246605  [   12/   89]
Per-example loss in batch: 0.289350  [   14/   89]
Per-example loss in batch: 0.349932  [   16/   89]
Per-example loss in batch: 0.344695  [   18/   89]
Per-example loss in batch: 0.326864  [   20/   89]
Per-example loss in batch: 0.304786  [   22/   89]
Per-example loss in batch: 0.365465  [   24/   89]
Per-example loss in batch: 0.305172  [   26/   89]
Per-example loss in batch: 0.278787  [   28/   89]
Per-example loss in batch: 0.359192  [   30/   89]
Per-example loss in batch: 0.369496  [   32/   89]
Per-example loss in batch: 0.299826  [   34/   89]
Per-example loss in batch: 0.318102  [   36/   89]
Per-example loss in batch: 0.367278  [   38/   89]
Per-example loss in batch: 0.350407  [   40/   89]
Per-example loss in batch: 0.295833  [   42/   89]
Per-example loss in batch: 0.249085  [   44/   89]
Per-example loss in batch: 0.357086  [   46/   89]
Per-example loss in batch: 0.330540  [   48/   89]
Per-example loss in batch: 0.335146  [   50/   89]
Per-example loss in batch: 0.315841  [   52/   89]
Per-example loss in batch: 0.358266  [   54/   89]
Per-example loss in batch: 0.283735  [   56/   89]
Per-example loss in batch: 0.265793  [   58/   89]
Per-example loss in batch: 0.311472  [   60/   89]
Per-example loss in batch: 0.347121  [   62/   89]
Per-example loss in batch: 0.375143  [   64/   89]
Per-example loss in batch: 0.308865  [   66/   89]
Per-example loss in batch: 0.283392  [   68/   89]
Per-example loss in batch: 0.377300  [   70/   89]
Per-example loss in batch: 0.289028  [   72/   89]
Per-example loss in batch: 0.325553  [   74/   89]
Per-example loss in batch: 0.343985  [   76/   89]
Per-example loss in batch: 0.288259  [   78/   89]
Per-example loss in batch: 0.304776  [   80/   89]
Per-example loss in batch: 0.409958  [   82/   89]
Per-example loss in batch: 0.345003  [   84/   89]
Per-example loss in batch: 0.381940  [   86/   89]
Per-example loss in batch: 0.394478  [   88/   89]
Per-example loss in batch: 0.723745  [   89/   89]
Train Error: Avg loss: 0.32744839
validation Error: 
 Avg loss: 0.50889536 
 F1: 0.357907 
 Precision: 0.477972 
 Recall: 0.286052
 IoU: 0.217958

test Error: 
 Avg loss: 0.48810262 
 F1: 0.364897 
 Precision: 0.461187 
 Recall: 0.301870
 IoU: 0.223164

We have finished training iteration 195
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_193_.pth
Per-example loss in batch: 0.374280  [    2/   89]
Per-example loss in batch: 0.263365  [    4/   89]
Per-example loss in batch: 0.343339  [    6/   89]
Per-example loss in batch: 0.337514  [    8/   89]
Per-example loss in batch: 0.348232  [   10/   89]
Per-example loss in batch: 0.340555  [   12/   89]
Per-example loss in batch: 0.340510  [   14/   89]
Per-example loss in batch: 0.288977  [   16/   89]
Per-example loss in batch: 0.247024  [   18/   89]
Per-example loss in batch: 0.334311  [   20/   89]
Per-example loss in batch: 0.356610  [   22/   89]
Per-example loss in batch: 0.293837  [   24/   89]
Per-example loss in batch: 0.391737  [   26/   89]
Per-example loss in batch: 0.325890  [   28/   89]
Per-example loss in batch: 0.295931  [   30/   89]
Per-example loss in batch: 0.307223  [   32/   89]
Per-example loss in batch: 0.309232  [   34/   89]
Per-example loss in batch: 0.405978  [   36/   89]
Per-example loss in batch: 0.324178  [   38/   89]
Per-example loss in batch: 0.366342  [   40/   89]
Per-example loss in batch: 0.358441  [   42/   89]
Per-example loss in batch: 0.408864  [   44/   89]
Per-example loss in batch: 0.301055  [   46/   89]
Per-example loss in batch: 0.294316  [   48/   89]
Per-example loss in batch: 0.295390  [   50/   89]
Per-example loss in batch: 0.381547  [   52/   89]
Per-example loss in batch: 0.369555  [   54/   89]
Per-example loss in batch: 0.267365  [   56/   89]
Per-example loss in batch: 0.264769  [   58/   89]
Per-example loss in batch: 0.278947  [   60/   89]
Per-example loss in batch: 0.387702  [   62/   89]
Per-example loss in batch: 0.262602  [   64/   89]
Per-example loss in batch: 0.352699  [   66/   89]
Per-example loss in batch: 0.293725  [   68/   89]
Per-example loss in batch: 0.333539  [   70/   89]
Per-example loss in batch: 0.277118  [   72/   89]
Per-example loss in batch: 0.297014  [   74/   89]
Per-example loss in batch: 0.304316  [   76/   89]
Per-example loss in batch: 0.383834  [   78/   89]
Per-example loss in batch: 0.359146  [   80/   89]
Per-example loss in batch: 0.324667  [   82/   89]
Per-example loss in batch: 0.355399  [   84/   89]
Per-example loss in batch: 0.277235  [   86/   89]
Per-example loss in batch: 0.349246  [   88/   89]
Per-example loss in batch: 0.684709  [   89/   89]
Train Error: Avg loss: 0.33069460
validation Error: 
 Avg loss: 0.50948769 
 F1: 0.370362 
 Precision: 0.450111 
 Recall: 0.314619
 IoU: 0.227266

test Error: 
 Avg loss: 0.48958074 
 F1: 0.380757 
 Precision: 0.433997 
 Recall: 0.339153
 IoU: 0.235145

We have finished training iteration 196
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_194_.pth
Per-example loss in batch: 0.291655  [    2/   89]
Per-example loss in batch: 0.329436  [    4/   89]
Per-example loss in batch: 0.410843  [    6/   89]
Per-example loss in batch: 0.310969  [    8/   89]
Per-example loss in batch: 0.314550  [   10/   89]
Per-example loss in batch: 0.317041  [   12/   89]
Per-example loss in batch: 0.332267  [   14/   89]
Per-example loss in batch: 0.358005  [   16/   89]
Per-example loss in batch: 0.347518  [   18/   89]
Per-example loss in batch: 0.342530  [   20/   89]
Per-example loss in batch: 0.289390  [   22/   89]
Per-example loss in batch: 0.303008  [   24/   89]
Per-example loss in batch: 0.288274  [   26/   89]
Per-example loss in batch: 0.234266  [   28/   89]
Per-example loss in batch: 0.292000  [   30/   89]
Per-example loss in batch: 0.280884  [   32/   89]
Per-example loss in batch: 0.320084  [   34/   89]
Per-example loss in batch: 0.292734  [   36/   89]
Per-example loss in batch: 0.242316  [   38/   89]
Per-example loss in batch: 0.332377  [   40/   89]
Per-example loss in batch: 0.282508  [   42/   89]
Per-example loss in batch: 0.299350  [   44/   89]
Per-example loss in batch: 0.372868  [   46/   89]
Per-example loss in batch: 0.316240  [   48/   89]
Per-example loss in batch: 0.279526  [   50/   89]
Per-example loss in batch: 0.406908  [   52/   89]
Per-example loss in batch: 0.313502  [   54/   89]
Per-example loss in batch: 0.351427  [   56/   89]
Per-example loss in batch: 0.308031  [   58/   89]
Per-example loss in batch: 0.255580  [   60/   89]
Per-example loss in batch: 0.307714  [   62/   89]
Per-example loss in batch: 0.311536  [   64/   89]
Per-example loss in batch: 0.349688  [   66/   89]
Per-example loss in batch: 0.375712  [   68/   89]
Per-example loss in batch: 0.314557  [   70/   89]
Per-example loss in batch: 0.350113  [   72/   89]
Per-example loss in batch: 0.342323  [   74/   89]
Per-example loss in batch: 0.267992  [   76/   89]
Per-example loss in batch: 0.360910  [   78/   89]
Per-example loss in batch: 0.248561  [   80/   89]
Per-example loss in batch: 0.278539  [   82/   89]
Per-example loss in batch: 0.296212  [   84/   89]
Per-example loss in batch: 0.287450  [   86/   89]
Per-example loss in batch: 0.354078  [   88/   89]
Per-example loss in batch: 0.782936  [   89/   89]
Train Error: Avg loss: 0.32029085
validation Error: 
 Avg loss: 0.50834684 
 F1: 0.370980 
 Precision: 0.494675 
 Recall: 0.296771
 IoU: 0.227732

test Error: 
 Avg loss: 0.48820246 
 F1: 0.382285 
 Precision: 0.474605 
 Recall: 0.320033
 IoU: 0.236312

We have finished training iteration 197
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_195_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.387430  [    2/   89]
Per-example loss in batch: 0.334109  [    4/   89]
Per-example loss in batch: 0.336118  [    6/   89]
Per-example loss in batch: 0.278927  [    8/   89]
Per-example loss in batch: 0.392573  [   10/   89]
Per-example loss in batch: 0.310399  [   12/   89]
Per-example loss in batch: 0.421087  [   14/   89]
Per-example loss in batch: 0.354369  [   16/   89]
Per-example loss in batch: 0.384361  [   18/   89]
Per-example loss in batch: 0.356622  [   20/   89]
Per-example loss in batch: 0.335668  [   22/   89]
Per-example loss in batch: 0.276576  [   24/   89]
Per-example loss in batch: 0.267126  [   26/   89]
Per-example loss in batch: 0.348508  [   28/   89]
Per-example loss in batch: 0.327045  [   30/   89]
Per-example loss in batch: 0.297967  [   32/   89]
Per-example loss in batch: 0.302672  [   34/   89]
Per-example loss in batch: 0.328479  [   36/   89]
Per-example loss in batch: 0.328092  [   38/   89]
Per-example loss in batch: 0.280360  [   40/   89]
Per-example loss in batch: 0.365968  [   42/   89]
Per-example loss in batch: 0.229903  [   44/   89]
Per-example loss in batch: 0.297917  [   46/   89]
Per-example loss in batch: 0.288349  [   48/   89]
Per-example loss in batch: 0.305877  [   50/   89]
Per-example loss in batch: 0.355435  [   52/   89]
Per-example loss in batch: 0.400696  [   54/   89]
Per-example loss in batch: 0.262057  [   56/   89]
Per-example loss in batch: 0.252331  [   58/   89]
Per-example loss in batch: 0.328737  [   60/   89]
Per-example loss in batch: 0.406328  [   62/   89]
Per-example loss in batch: 0.250651  [   64/   89]
Per-example loss in batch: 0.332707  [   66/   89]
Per-example loss in batch: 0.400729  [   68/   89]
Per-example loss in batch: 0.346478  [   70/   89]
Per-example loss in batch: 0.269879  [   72/   89]
Per-example loss in batch: 0.258309  [   74/   89]
Per-example loss in batch: 0.324696  [   76/   89]
Per-example loss in batch: 0.276530  [   78/   89]
Per-example loss in batch: 0.410629  [   80/   89]
Per-example loss in batch: 0.343027  [   82/   89]
Per-example loss in batch: 0.264852  [   84/   89]
Per-example loss in batch: 0.342307  [   86/   89]
Per-example loss in batch: 0.339525  [   88/   89]
Per-example loss in batch: 0.834047  [   89/   89]
Train Error: Avg loss: 0.33077364
validation Error: 
 Avg loss: 0.50929671 
 F1: 0.368643 
 Precision: 0.474721 
 Recall: 0.301313
 IoU: 0.225973

test Error: 
 Avg loss: 0.48900754 
 F1: 0.378051 
 Precision: 0.455844 
 Recall: 0.322939
 IoU: 0.233084

We have finished training iteration 198
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_196_.pth
Per-example loss in batch: 0.310480  [    2/   89]
Per-example loss in batch: 0.272215  [    4/   89]
Per-example loss in batch: 0.371857  [    6/   89]
Per-example loss in batch: 0.291641  [    8/   89]
Per-example loss in batch: 0.314549  [   10/   89]
Per-example loss in batch: 0.283128  [   12/   89]
Per-example loss in batch: 0.338933  [   14/   89]
Per-example loss in batch: 0.274381  [   16/   89]
Per-example loss in batch: 0.386232  [   18/   89]
Per-example loss in batch: 0.391526  [   20/   89]
Per-example loss in batch: 0.285660  [   22/   89]
Per-example loss in batch: 0.260550  [   24/   89]
Per-example loss in batch: 0.366326  [   26/   89]
Per-example loss in batch: 0.389532  [   28/   89]
Per-example loss in batch: 0.391023  [   30/   89]
Per-example loss in batch: 0.331589  [   32/   89]
Per-example loss in batch: 0.398217  [   34/   89]
Per-example loss in batch: 0.255062  [   36/   89]
Per-example loss in batch: 0.264433  [   38/   89]
Per-example loss in batch: 0.264504  [   40/   89]
Per-example loss in batch: 0.292513  [   42/   89]
Per-example loss in batch: 0.312109  [   44/   89]
Per-example loss in batch: 0.428077  [   46/   89]
Per-example loss in batch: 0.318883  [   48/   89]
Per-example loss in batch: 0.280713  [   50/   89]
Per-example loss in batch: 0.262563  [   52/   89]
Per-example loss in batch: 0.337404  [   54/   89]
Per-example loss in batch: 0.371580  [   56/   89]
Per-example loss in batch: 0.348935  [   58/   89]
Per-example loss in batch: 0.315317  [   60/   89]
Per-example loss in batch: 0.338318  [   62/   89]
Per-example loss in batch: 0.243640  [   64/   89]
Per-example loss in batch: 0.300663  [   66/   89]
Per-example loss in batch: 0.326700  [   68/   89]
Per-example loss in batch: 0.308293  [   70/   89]
Per-example loss in batch: 0.371297  [   72/   89]
Per-example loss in batch: 0.314358  [   74/   89]
Per-example loss in batch: 0.297157  [   76/   89]
Per-example loss in batch: 0.348309  [   78/   89]
Per-example loss in batch: 0.372364  [   80/   89]
Per-example loss in batch: 0.277775  [   82/   89]
Per-example loss in batch: 0.274949  [   84/   89]
Per-example loss in batch: 0.256158  [   86/   89]
Per-example loss in batch: 0.228814  [   88/   89]
Per-example loss in batch: 0.683161  [   89/   89]
Train Error: Avg loss: 0.32157999
validation Error: 
 Avg loss: 0.50839332 
 F1: 0.377096 
 Precision: 0.514960 
 Recall: 0.297460
 IoU: 0.232358

test Error: 
 Avg loss: 0.48828155 
 F1: 0.387018 
 Precision: 0.497335 
 Recall: 0.316756
 IoU: 0.239940

We have finished training iteration 199
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_197_.pth
Per-example loss in batch: 0.321409  [    2/   89]
Per-example loss in batch: 0.335150  [    4/   89]
Per-example loss in batch: 0.291229  [    6/   89]
Per-example loss in batch: 0.363541  [    8/   89]
Per-example loss in batch: 0.310520  [   10/   89]
Per-example loss in batch: 0.339775  [   12/   89]
Per-example loss in batch: 0.417179  [   14/   89]
Per-example loss in batch: 0.333818  [   16/   89]
Per-example loss in batch: 0.295942  [   18/   89]
Per-example loss in batch: 0.256073  [   20/   89]
Per-example loss in batch: 0.355259  [   22/   89]
Per-example loss in batch: 0.300563  [   24/   89]
Per-example loss in batch: 0.326882  [   26/   89]
Per-example loss in batch: 0.360375  [   28/   89]
Per-example loss in batch: 0.352230  [   30/   89]
Per-example loss in batch: 0.301887  [   32/   89]
Per-example loss in batch: 0.383617  [   34/   89]
Per-example loss in batch: 0.362554  [   36/   89]
Per-example loss in batch: 0.300913  [   38/   89]
Per-example loss in batch: 0.377499  [   40/   89]
Per-example loss in batch: 0.368791  [   42/   89]
Per-example loss in batch: 0.379105  [   44/   89]
Per-example loss in batch: 0.268740  [   46/   89]
Per-example loss in batch: 0.366852  [   48/   89]
Per-example loss in batch: 0.385587  [   50/   89]
Per-example loss in batch: 0.332995  [   52/   89]
Per-example loss in batch: 0.334014  [   54/   89]
Per-example loss in batch: 0.279892  [   56/   89]
Per-example loss in batch: 0.339288  [   58/   89]
Per-example loss in batch: 0.323219  [   60/   89]
Per-example loss in batch: 0.358261  [   62/   89]
Per-example loss in batch: 0.235777  [   64/   89]
Per-example loss in batch: 0.347935  [   66/   89]
Per-example loss in batch: 0.304021  [   68/   89]
Per-example loss in batch: 0.270516  [   70/   89]
Per-example loss in batch: 0.329046  [   72/   89]
Per-example loss in batch: 0.356086  [   74/   89]
Per-example loss in batch: 0.283138  [   76/   89]
Per-example loss in batch: 0.284731  [   78/   89]
Per-example loss in batch: 0.352807  [   80/   89]
Per-example loss in batch: 0.365242  [   82/   89]
Per-example loss in batch: 0.279400  [   84/   89]
Per-example loss in batch: 0.371368  [   86/   89]
Per-example loss in batch: 0.353273  [   88/   89]
Per-example loss in batch: 0.704032  [   89/   89]
Train Error: Avg loss: 0.33502275
validation Error: 
 Avg loss: 0.50863514 
 F1: 0.371248 
 Precision: 0.544400 
 Recall: 0.281662
 IoU: 0.227934

test Error: 
 Avg loss: 0.48861024 
 F1: 0.382807 
 Precision: 0.531608 
 Recall: 0.299090
 IoU: 0.236711

We have finished training iteration 200
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_198_.pth
Per-example loss in batch: 0.390629  [    2/   89]
Per-example loss in batch: 0.318232  [    4/   89]
Per-example loss in batch: 0.323780  [    6/   89]
Per-example loss in batch: 0.297691  [    8/   89]
Per-example loss in batch: 0.381129  [   10/   89]
Per-example loss in batch: 0.359650  [   12/   89]
Per-example loss in batch: 0.299196  [   14/   89]
Per-example loss in batch: 0.348781  [   16/   89]
Per-example loss in batch: 0.334142  [   18/   89]
Per-example loss in batch: 0.284182  [   20/   89]
Per-example loss in batch: 0.342139  [   22/   89]
Per-example loss in batch: 0.356972  [   24/   89]
Per-example loss in batch: 0.346056  [   26/   89]
Per-example loss in batch: 0.428068  [   28/   89]
Per-example loss in batch: 0.255642  [   30/   89]
Per-example loss in batch: 0.362801  [   32/   89]
Per-example loss in batch: 0.312909  [   34/   89]
Per-example loss in batch: 0.339294  [   36/   89]
Per-example loss in batch: 0.279608  [   38/   89]
Per-example loss in batch: 0.248926  [   40/   89]
Per-example loss in batch: 0.246596  [   42/   89]
Per-example loss in batch: 0.295184  [   44/   89]
Per-example loss in batch: 0.289611  [   46/   89]
Per-example loss in batch: 0.372196  [   48/   89]
Per-example loss in batch: 0.307330  [   50/   89]
Per-example loss in batch: 0.369913  [   52/   89]
Per-example loss in batch: 0.378850  [   54/   89]
Per-example loss in batch: 0.295822  [   56/   89]
Per-example loss in batch: 0.357011  [   58/   89]
Per-example loss in batch: 0.311061  [   60/   89]
Per-example loss in batch: 0.283463  [   62/   89]
Per-example loss in batch: 0.277943  [   64/   89]
Per-example loss in batch: 0.281437  [   66/   89]
Per-example loss in batch: 0.327453  [   68/   89]
Per-example loss in batch: 0.269017  [   70/   89]
Per-example loss in batch: 0.301885  [   72/   89]
Per-example loss in batch: 0.290865  [   74/   89]
Per-example loss in batch: 0.314145  [   76/   89]
Per-example loss in batch: 0.346110  [   78/   89]
Per-example loss in batch: 0.282203  [   80/   89]
Per-example loss in batch: 0.336853  [   82/   89]
Per-example loss in batch: 0.348631  [   84/   89]
Per-example loss in batch: 0.434111  [   86/   89]
Per-example loss in batch: 0.369453  [   88/   89]
Per-example loss in batch: 0.523373  [   89/   89]
Train Error: Avg loss: 0.32716082
validation Error: 
 Avg loss: 0.50809131 
 F1: 0.370714 
 Precision: 0.489521 
 Recall: 0.298313
 IoU: 0.227532

test Error: 
 Avg loss: 0.48730117 
 F1: 0.385408 
 Precision: 0.478343 
 Recall: 0.322711
 IoU: 0.238703

We have finished training iteration 201
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_199_.pth
Per-example loss in batch: 0.272105  [    2/   89]
Per-example loss in batch: 0.286098  [    4/   89]
Per-example loss in batch: 0.388507  [    6/   89]
Per-example loss in batch: 0.413909  [    8/   89]
Per-example loss in batch: 0.321522  [   10/   89]
Per-example loss in batch: 0.248381  [   12/   89]
Per-example loss in batch: 0.333993  [   14/   89]
Per-example loss in batch: 0.316154  [   16/   89]
Per-example loss in batch: 0.387551  [   18/   89]
Per-example loss in batch: 0.362843  [   20/   89]
Per-example loss in batch: 0.332347  [   22/   89]
Per-example loss in batch: 0.270599  [   24/   89]
Per-example loss in batch: 0.356290  [   26/   89]
Per-example loss in batch: 0.339248  [   28/   89]
Per-example loss in batch: 0.355677  [   30/   89]
Per-example loss in batch: 0.341722  [   32/   89]
Per-example loss in batch: 0.284371  [   34/   89]
Per-example loss in batch: 0.297251  [   36/   89]
Per-example loss in batch: 0.239636  [   38/   89]
Per-example loss in batch: 0.385214  [   40/   89]
Per-example loss in batch: 0.267797  [   42/   89]
Per-example loss in batch: 0.395176  [   44/   89]
Per-example loss in batch: 0.324460  [   46/   89]
Per-example loss in batch: 0.257262  [   48/   89]
Per-example loss in batch: 0.311189  [   50/   89]
Per-example loss in batch: 0.384854  [   52/   89]
Per-example loss in batch: 0.307815  [   54/   89]
Per-example loss in batch: 0.344740  [   56/   89]
Per-example loss in batch: 0.436741  [   58/   89]
Per-example loss in batch: 0.372849  [   60/   89]
Per-example loss in batch: 0.369023  [   62/   89]
Per-example loss in batch: 0.324697  [   64/   89]
Per-example loss in batch: 0.308652  [   66/   89]
Per-example loss in batch: 0.315041  [   68/   89]
Per-example loss in batch: 0.351107  [   70/   89]
Per-example loss in batch: 0.272029  [   72/   89]
Per-example loss in batch: 0.334271  [   74/   89]
Per-example loss in batch: 0.317968  [   76/   89]
Per-example loss in batch: 0.231307  [   78/   89]
Per-example loss in batch: 0.292969  [   80/   89]
Per-example loss in batch: 0.324989  [   82/   89]
Per-example loss in batch: 0.368061  [   84/   89]
Per-example loss in batch: 0.319932  [   86/   89]
Per-example loss in batch: 0.268756  [   88/   89]
Per-example loss in batch: 0.657022  [   89/   89]
Train Error: Avg loss: 0.32951947
validation Error: 
 Avg loss: 0.50807478 
 F1: 0.375560 
 Precision: 0.500635 
 Recall: 0.300488
 IoU: 0.231194

test Error: 
 Avg loss: 0.48841039 
 F1: 0.387403 
 Precision: 0.487000 
 Recall: 0.321627
 IoU: 0.240235

We have finished training iteration 202
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_200_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.250689  [    2/   89]
Per-example loss in batch: 0.245648  [    4/   89]
Per-example loss in batch: 0.286070  [    6/   89]
Per-example loss in batch: 0.345862  [    8/   89]
Per-example loss in batch: 0.294406  [   10/   89]
Per-example loss in batch: 0.275419  [   12/   89]
Per-example loss in batch: 0.311548  [   14/   89]
Per-example loss in batch: 0.276397  [   16/   89]
Per-example loss in batch: 0.358304  [   18/   89]
Per-example loss in batch: 0.376582  [   20/   89]
Per-example loss in batch: 0.313796  [   22/   89]
Per-example loss in batch: 0.247928  [   24/   89]
Per-example loss in batch: 0.319245  [   26/   89]
Per-example loss in batch: 0.406486  [   28/   89]
Per-example loss in batch: 0.283402  [   30/   89]
Per-example loss in batch: 0.272860  [   32/   89]
Per-example loss in batch: 0.364018  [   34/   89]
Per-example loss in batch: 0.239731  [   36/   89]
Per-example loss in batch: 0.404101  [   38/   89]
Per-example loss in batch: 0.311291  [   40/   89]
Per-example loss in batch: 0.298236  [   42/   89]
Per-example loss in batch: 0.333142  [   44/   89]
Per-example loss in batch: 0.302775  [   46/   89]
Per-example loss in batch: 0.390210  [   48/   89]
Per-example loss in batch: 0.348386  [   50/   89]
Per-example loss in batch: 0.306795  [   52/   89]
Per-example loss in batch: 0.304432  [   54/   89]
Per-example loss in batch: 0.362265  [   56/   89]
Per-example loss in batch: 0.338572  [   58/   89]
Per-example loss in batch: 0.347151  [   60/   89]
Per-example loss in batch: 0.336821  [   62/   89]
Per-example loss in batch: 0.323918  [   64/   89]
Per-example loss in batch: 0.276775  [   66/   89]
Per-example loss in batch: 0.301128  [   68/   89]
Per-example loss in batch: 0.255813  [   70/   89]
Per-example loss in batch: 0.375482  [   72/   89]
Per-example loss in batch: 0.370178  [   74/   89]
Per-example loss in batch: 0.369686  [   76/   89]
Per-example loss in batch: 0.338349  [   78/   89]
Per-example loss in batch: 0.308277  [   80/   89]
Per-example loss in batch: 0.311851  [   82/   89]
Per-example loss in batch: 0.304903  [   84/   89]
Per-example loss in batch: 0.276961  [   86/   89]
Per-example loss in batch: 0.339296  [   88/   89]
Per-example loss in batch: 0.630054  [   89/   89]
Train Error: Avg loss: 0.32180261
validation Error: 
 Avg loss: 0.50765445 
 F1: 0.378596 
 Precision: 0.504686 
 Recall: 0.302915
 IoU: 0.233499

test Error: 
 Avg loss: 0.48740496 
 F1: 0.390590 
 Precision: 0.495346 
 Recall: 0.322408
 IoU: 0.242692

We have finished training iteration 203
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_201_.pth
Per-example loss in batch: 0.352722  [    2/   89]
Per-example loss in batch: 0.333740  [    4/   89]
Per-example loss in batch: 0.320082  [    6/   89]
Per-example loss in batch: 0.317163  [    8/   89]
Per-example loss in batch: 0.248375  [   10/   89]
Per-example loss in batch: 0.381473  [   12/   89]
Per-example loss in batch: 0.309523  [   14/   89]
Per-example loss in batch: 0.259332  [   16/   89]
Per-example loss in batch: 0.279144  [   18/   89]
Per-example loss in batch: 0.310571  [   20/   89]
Per-example loss in batch: 0.255389  [   22/   89]
Per-example loss in batch: 0.265174  [   24/   89]
Per-example loss in batch: 0.419896  [   26/   89]
Per-example loss in batch: 0.384981  [   28/   89]
Per-example loss in batch: 0.342036  [   30/   89]
Per-example loss in batch: 0.346693  [   32/   89]
Per-example loss in batch: 0.374129  [   34/   89]
Per-example loss in batch: 0.324373  [   36/   89]
Per-example loss in batch: 0.256905  [   38/   89]
Per-example loss in batch: 0.315883  [   40/   89]
Per-example loss in batch: 0.280939  [   42/   89]
Per-example loss in batch: 0.263806  [   44/   89]
Per-example loss in batch: 0.305541  [   46/   89]
Per-example loss in batch: 0.335203  [   48/   89]
Per-example loss in batch: 0.354514  [   50/   89]
Per-example loss in batch: 0.266972  [   52/   89]
Per-example loss in batch: 0.259100  [   54/   89]
Per-example loss in batch: 0.369605  [   56/   89]
Per-example loss in batch: 0.336476  [   58/   89]
Per-example loss in batch: 0.305229  [   60/   89]
Per-example loss in batch: 0.291814  [   62/   89]
Per-example loss in batch: 0.223518  [   64/   89]
Per-example loss in batch: 0.327217  [   66/   89]
Per-example loss in batch: 0.323391  [   68/   89]
Per-example loss in batch: 0.378319  [   70/   89]
Per-example loss in batch: 0.288145  [   72/   89]
Per-example loss in batch: 0.311219  [   74/   89]
Per-example loss in batch: 0.299933  [   76/   89]
Per-example loss in batch: 0.296644  [   78/   89]
Per-example loss in batch: 0.302229  [   80/   89]
Per-example loss in batch: 0.260163  [   82/   89]
Per-example loss in batch: 0.343101  [   84/   89]
Per-example loss in batch: 0.387949  [   86/   89]
Per-example loss in batch: 0.373019  [   88/   89]
Per-example loss in batch: 0.999850  [   89/   89]
Train Error: Avg loss: 0.32318093
validation Error: 
 Avg loss: 0.50890038 
 F1: 0.371160 
 Precision: 0.506167 
 Recall: 0.293008
 IoU: 0.227868

test Error: 
 Avg loss: 0.48872911 
 F1: 0.384123 
 Precision: 0.499228 
 Recall: 0.312151
 IoU: 0.237718

We have finished training iteration 204
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_202_.pth
Per-example loss in batch: 0.339337  [    2/   89]
Per-example loss in batch: 0.358872  [    4/   89]
Per-example loss in batch: 0.362686  [    6/   89]
Per-example loss in batch: 0.388500  [    8/   89]
Per-example loss in batch: 0.322082  [   10/   89]
Per-example loss in batch: 0.293090  [   12/   89]
Per-example loss in batch: 0.282103  [   14/   89]
Per-example loss in batch: 0.261087  [   16/   89]
Per-example loss in batch: 0.356555  [   18/   89]
Per-example loss in batch: 0.311144  [   20/   89]
Per-example loss in batch: 0.260406  [   22/   89]
Per-example loss in batch: 0.273810  [   24/   89]
Per-example loss in batch: 0.252016  [   26/   89]
Per-example loss in batch: 0.373578  [   28/   89]
Per-example loss in batch: 0.285894  [   30/   89]
Per-example loss in batch: 0.303469  [   32/   89]
Per-example loss in batch: 0.279209  [   34/   89]
Per-example loss in batch: 0.308854  [   36/   89]
Per-example loss in batch: 0.321819  [   38/   89]
Per-example loss in batch: 0.291716  [   40/   89]
Per-example loss in batch: 0.255567  [   42/   89]
Per-example loss in batch: 0.295755  [   44/   89]
Per-example loss in batch: 0.302953  [   46/   89]
Per-example loss in batch: 0.327423  [   48/   89]
Per-example loss in batch: 0.287828  [   50/   89]
Per-example loss in batch: 0.363001  [   52/   89]
Per-example loss in batch: 0.357771  [   54/   89]
Per-example loss in batch: 0.380661  [   56/   89]
Per-example loss in batch: 0.306166  [   58/   89]
Per-example loss in batch: 0.377492  [   60/   89]
Per-example loss in batch: 0.373917  [   62/   89]
Per-example loss in batch: 0.341419  [   64/   89]
Per-example loss in batch: 0.335489  [   66/   89]
Per-example loss in batch: 0.295462  [   68/   89]
Per-example loss in batch: 0.314863  [   70/   89]
Per-example loss in batch: 0.248008  [   72/   89]
Per-example loss in batch: 0.232999  [   74/   89]
Per-example loss in batch: 0.312211  [   76/   89]
Per-example loss in batch: 0.342297  [   78/   89]
Per-example loss in batch: 0.344819  [   80/   89]
Per-example loss in batch: 0.251740  [   82/   89]
Per-example loss in batch: 0.366910  [   84/   89]
Per-example loss in batch: 0.345560  [   86/   89]
Per-example loss in batch: 0.371150  [   88/   89]
Per-example loss in batch: 0.774307  [   89/   89]
Train Error: Avg loss: 0.32235594
validation Error: 
 Avg loss: 0.50892768 
 F1: 0.376499 
 Precision: 0.491458 
 Recall: 0.305126
 IoU: 0.231906

test Error: 
 Avg loss: 0.48881645 
 F1: 0.392831 
 Precision: 0.482575 
 Recall: 0.331233
 IoU: 0.244425

We have finished training iteration 205
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_203_.pth
Per-example loss in batch: 0.248594  [    2/   89]
Per-example loss in batch: 0.351602  [    4/   89]
Per-example loss in batch: 0.341065  [    6/   89]
Per-example loss in batch: 0.335070  [    8/   89]
Per-example loss in batch: 0.299762  [   10/   89]
Per-example loss in batch: 0.265824  [   12/   89]
Per-example loss in batch: 0.399059  [   14/   89]
Per-example loss in batch: 0.325418  [   16/   89]
Per-example loss in batch: 0.361885  [   18/   89]
Per-example loss in batch: 0.235566  [   20/   89]
Per-example loss in batch: 0.262142  [   22/   89]
Per-example loss in batch: 0.328281  [   24/   89]
Per-example loss in batch: 0.233155  [   26/   89]
Per-example loss in batch: 0.327279  [   28/   89]
Per-example loss in batch: 0.309338  [   30/   89]
Per-example loss in batch: 0.243777  [   32/   89]
Per-example loss in batch: 0.280712  [   34/   89]
Per-example loss in batch: 0.298112  [   36/   89]
Per-example loss in batch: 0.305259  [   38/   89]
Per-example loss in batch: 0.285799  [   40/   89]
Per-example loss in batch: 0.379332  [   42/   89]
Per-example loss in batch: 0.311630  [   44/   89]
Per-example loss in batch: 0.304894  [   46/   89]
Per-example loss in batch: 0.356933  [   48/   89]
Per-example loss in batch: 0.327627  [   50/   89]
Per-example loss in batch: 0.348246  [   52/   89]
Per-example loss in batch: 0.335195  [   54/   89]
Per-example loss in batch: 0.291024  [   56/   89]
Per-example loss in batch: 0.378502  [   58/   89]
Per-example loss in batch: 0.348481  [   60/   89]
Per-example loss in batch: 0.364198  [   62/   89]
Per-example loss in batch: 0.256803  [   64/   89]
Per-example loss in batch: 0.416181  [   66/   89]
Per-example loss in batch: 0.297595  [   68/   89]
Per-example loss in batch: 0.340428  [   70/   89]
Per-example loss in batch: 0.337091  [   72/   89]
Per-example loss in batch: 0.300106  [   74/   89]
Per-example loss in batch: 0.280122  [   76/   89]
Per-example loss in batch: 0.445809  [   78/   89]
Per-example loss in batch: 0.326287  [   80/   89]
Per-example loss in batch: 0.344454  [   82/   89]
Per-example loss in batch: 0.320764  [   84/   89]
Per-example loss in batch: 0.237034  [   86/   89]
Per-example loss in batch: 0.264221  [   88/   89]
Per-example loss in batch: 0.785528  [   89/   89]
Train Error: Avg loss: 0.32232402
validation Error: 
 Avg loss: 0.50841205 
 F1: 0.378696 
 Precision: 0.508614 
 Recall: 0.301645
 IoU: 0.233575

test Error: 
 Avg loss: 0.48731500 
 F1: 0.394390 
 Precision: 0.508537 
 Recall: 0.322093
 IoU: 0.245633

We have finished training iteration 206
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_204_.pth
Per-example loss in batch: 0.223863  [    2/   89]
Per-example loss in batch: 0.384438  [    4/   89]
Per-example loss in batch: 0.392628  [    6/   89]
Per-example loss in batch: 0.264802  [    8/   89]
Per-example loss in batch: 0.316656  [   10/   89]
Per-example loss in batch: 0.305514  [   12/   89]
Per-example loss in batch: 0.386179  [   14/   89]
Per-example loss in batch: 0.384615  [   16/   89]
Per-example loss in batch: 0.257560  [   18/   89]
Per-example loss in batch: 0.416862  [   20/   89]
Per-example loss in batch: 0.335330  [   22/   89]
Per-example loss in batch: 0.313733  [   24/   89]
Per-example loss in batch: 0.360857  [   26/   89]
Per-example loss in batch: 0.342130  [   28/   89]
Per-example loss in batch: 0.326415  [   30/   89]
Per-example loss in batch: 0.322241  [   32/   89]
Per-example loss in batch: 0.372673  [   34/   89]
Per-example loss in batch: 0.268390  [   36/   89]
Per-example loss in batch: 0.350737  [   38/   89]
Per-example loss in batch: 0.257128  [   40/   89]
Per-example loss in batch: 0.304838  [   42/   89]
Per-example loss in batch: 0.363559  [   44/   89]
Per-example loss in batch: 0.323149  [   46/   89]
Per-example loss in batch: 0.323077  [   48/   89]
Per-example loss in batch: 0.300803  [   50/   89]
Per-example loss in batch: 0.310738  [   52/   89]
Per-example loss in batch: 0.337327  [   54/   89]
Per-example loss in batch: 0.275384  [   56/   89]
Per-example loss in batch: 0.358024  [   58/   89]
Per-example loss in batch: 0.243466  [   60/   89]
Per-example loss in batch: 0.300107  [   62/   89]
Per-example loss in batch: 0.348325  [   64/   89]
Per-example loss in batch: 0.381418  [   66/   89]
Per-example loss in batch: 0.323991  [   68/   89]
Per-example loss in batch: 0.308958  [   70/   89]
Per-example loss in batch: 0.282241  [   72/   89]
Per-example loss in batch: 0.367577  [   74/   89]
Per-example loss in batch: 0.332311  [   76/   89]
Per-example loss in batch: 0.358330  [   78/   89]
Per-example loss in batch: 0.266427  [   80/   89]
Per-example loss in batch: 0.335007  [   82/   89]
Per-example loss in batch: 0.269284  [   84/   89]
Per-example loss in batch: 0.314736  [   86/   89]
Per-example loss in batch: 0.326775  [   88/   89]
Per-example loss in batch: 0.539030  [   89/   89]
Train Error: Avg loss: 0.32602522
validation Error: 
 Avg loss: 0.50920399 
 F1: 0.373401 
 Precision: 0.457539 
 Recall: 0.315400
 IoU: 0.229559

test Error: 
 Avg loss: 0.48926092 
 F1: 0.385138 
 Precision: 0.457081 
 Recall: 0.332763
 IoU: 0.238496

We have finished training iteration 207
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_205_.pth
Per-example loss in batch: 0.372640  [    2/   89]
Per-example loss in batch: 0.281133  [    4/   89]
Per-example loss in batch: 0.318778  [    6/   89]
Per-example loss in batch: 0.289505  [    8/   89]
Per-example loss in batch: 0.386723  [   10/   89]
Per-example loss in batch: 0.278383  [   12/   89]
Per-example loss in batch: 0.320329  [   14/   89]
Per-example loss in batch: 0.282997  [   16/   89]
Per-example loss in batch: 0.277725  [   18/   89]
Per-example loss in batch: 0.320309  [   20/   89]
Per-example loss in batch: 0.353317  [   22/   89]
Per-example loss in batch: 0.250355  [   24/   89]
Per-example loss in batch: 0.380879  [   26/   89]
Per-example loss in batch: 0.302049  [   28/   89]
Per-example loss in batch: 0.317703  [   30/   89]
Per-example loss in batch: 0.346384  [   32/   89]
Per-example loss in batch: 0.276377  [   34/   89]
Per-example loss in batch: 0.401238  [   36/   89]
Per-example loss in batch: 0.307035  [   38/   89]
Per-example loss in batch: 0.357765  [   40/   89]
Per-example loss in batch: 0.362098  [   42/   89]
Per-example loss in batch: 0.339480  [   44/   89]
Per-example loss in batch: 0.310944  [   46/   89]
Per-example loss in batch: 0.251743  [   48/   89]
Per-example loss in batch: 0.393087  [   50/   89]
Per-example loss in batch: 0.308237  [   52/   89]
Per-example loss in batch: 0.365512  [   54/   89]
Per-example loss in batch: 0.384979  [   56/   89]
Per-example loss in batch: 0.325062  [   58/   89]
Per-example loss in batch: 0.322940  [   60/   89]
Per-example loss in batch: 0.324944  [   62/   89]
Per-example loss in batch: 0.251176  [   64/   89]
Per-example loss in batch: 0.382921  [   66/   89]
Per-example loss in batch: 0.344203  [   68/   89]
Per-example loss in batch: 0.331715  [   70/   89]
Per-example loss in batch: 0.272165  [   72/   89]
Per-example loss in batch: 0.297841  [   74/   89]
Per-example loss in batch: 0.361621  [   76/   89]
Per-example loss in batch: 0.286245  [   78/   89]
Per-example loss in batch: 0.316677  [   80/   89]
Per-example loss in batch: 0.338406  [   82/   89]
Per-example loss in batch: 0.241749  [   84/   89]
Per-example loss in batch: 0.446805  [   86/   89]
Per-example loss in batch: 0.257572  [   88/   89]
Per-example loss in batch: 0.514187  [   89/   89]
Train Error: Avg loss: 0.32577172
validation Error: 
 Avg loss: 0.50802944 
 F1: 0.373177 
 Precision: 0.547147 
 Recall: 0.283148
 IoU: 0.229390

test Error: 
 Avg loss: 0.48775307 
 F1: 0.382727 
 Precision: 0.537257 
 Recall: 0.297235
 IoU: 0.236650

We have finished training iteration 208
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_206_.pth
Per-example loss in batch: 0.294558  [    2/   89]
Per-example loss in batch: 0.362759  [    4/   89]
Per-example loss in batch: 0.387249  [    6/   89]
Per-example loss in batch: 0.279843  [    8/   89]
Per-example loss in batch: 0.250449  [   10/   89]
Per-example loss in batch: 0.311714  [   12/   89]
Per-example loss in batch: 0.293967  [   14/   89]
Per-example loss in batch: 0.239021  [   16/   89]
Per-example loss in batch: 0.404486  [   18/   89]
Per-example loss in batch: 0.432804  [   20/   89]
Per-example loss in batch: 0.299020  [   22/   89]
Per-example loss in batch: 0.293346  [   24/   89]
Per-example loss in batch: 0.325505  [   26/   89]
Per-example loss in batch: 0.314966  [   28/   89]
Per-example loss in batch: 0.281120  [   30/   89]
Per-example loss in batch: 0.309775  [   32/   89]
Per-example loss in batch: 0.292704  [   34/   89]
Per-example loss in batch: 0.352205  [   36/   89]
Per-example loss in batch: 0.352222  [   38/   89]
Per-example loss in batch: 0.302303  [   40/   89]
Per-example loss in batch: 0.275180  [   42/   89]
Per-example loss in batch: 0.407406  [   44/   89]
Per-example loss in batch: 0.285402  [   46/   89]
Per-example loss in batch: 0.328673  [   48/   89]
Per-example loss in batch: 0.362040  [   50/   89]
Per-example loss in batch: 0.292319  [   52/   89]
Per-example loss in batch: 0.405300  [   54/   89]
Per-example loss in batch: 0.335440  [   56/   89]
Per-example loss in batch: 0.340519  [   58/   89]
Per-example loss in batch: 0.403949  [   60/   89]
Per-example loss in batch: 0.309361  [   62/   89]
Per-example loss in batch: 0.355796  [   64/   89]
Per-example loss in batch: 0.324951  [   66/   89]
Per-example loss in batch: 0.280401  [   68/   89]
Per-example loss in batch: 0.358999  [   70/   89]
Per-example loss in batch: 0.288324  [   72/   89]
Per-example loss in batch: 0.265703  [   74/   89]
Per-example loss in batch: 0.350799  [   76/   89]
Per-example loss in batch: 0.357478  [   78/   89]
Per-example loss in batch: 0.313046  [   80/   89]
Per-example loss in batch: 0.319105  [   82/   89]
Per-example loss in batch: 0.272731  [   84/   89]
Per-example loss in batch: 0.336583  [   86/   89]
Per-example loss in batch: 0.378656  [   88/   89]
Per-example loss in batch: 0.772066  [   89/   89]
Train Error: Avg loss: 0.33065642
validation Error: 
 Avg loss: 0.50954461 
 F1: 0.369256 
 Precision: 0.474985 
 Recall: 0.302027
 IoU: 0.226434

test Error: 
 Avg loss: 0.48896553 
 F1: 0.378025 
 Precision: 0.467295 
 Recall: 0.317393
 IoU: 0.233065

We have finished training iteration 209
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_207_.pth
Per-example loss in batch: 0.287716  [    2/   89]
Per-example loss in batch: 0.299941  [    4/   89]
Per-example loss in batch: 0.349263  [    6/   89]
Per-example loss in batch: 0.317683  [    8/   89]
Per-example loss in batch: 0.320589  [   10/   89]
Per-example loss in batch: 0.344302  [   12/   89]
Per-example loss in batch: 0.378680  [   14/   89]
Per-example loss in batch: 0.356556  [   16/   89]
Per-example loss in batch: 0.304597  [   18/   89]
Per-example loss in batch: 0.272601  [   20/   89]
Per-example loss in batch: 0.322555  [   22/   89]
Per-example loss in batch: 0.361642  [   24/   89]
Per-example loss in batch: 0.319618  [   26/   89]
Per-example loss in batch: 0.380760  [   28/   89]
Per-example loss in batch: 0.262745  [   30/   89]
Per-example loss in batch: 0.350938  [   32/   89]
Per-example loss in batch: 0.375591  [   34/   89]
Per-example loss in batch: 0.360354  [   36/   89]
Per-example loss in batch: 0.236185  [   38/   89]
Per-example loss in batch: 0.281654  [   40/   89]
Per-example loss in batch: 0.351368  [   42/   89]
Per-example loss in batch: 0.360590  [   44/   89]
Per-example loss in batch: 0.301384  [   46/   89]
Per-example loss in batch: 0.264668  [   48/   89]
Per-example loss in batch: 0.388178  [   50/   89]
Per-example loss in batch: 0.255201  [   52/   89]
Per-example loss in batch: 0.298118  [   54/   89]
Per-example loss in batch: 0.323157  [   56/   89]
Per-example loss in batch: 0.287707  [   58/   89]
Per-example loss in batch: 0.331947  [   60/   89]
Per-example loss in batch: 0.252280  [   62/   89]
Per-example loss in batch: 0.313157  [   64/   89]
Per-example loss in batch: 0.300362  [   66/   89]
Per-example loss in batch: 0.334652  [   68/   89]
Per-example loss in batch: 0.319307  [   70/   89]
Per-example loss in batch: 0.312604  [   72/   89]
Per-example loss in batch: 0.234788  [   74/   89]
Per-example loss in batch: 0.355420  [   76/   89]
Per-example loss in batch: 0.265340  [   78/   89]
Per-example loss in batch: 0.399299  [   80/   89]
Per-example loss in batch: 0.313137  [   82/   89]
Per-example loss in batch: 0.291730  [   84/   89]
Per-example loss in batch: 0.280539  [   86/   89]
Per-example loss in batch: 0.246338  [   88/   89]
Per-example loss in batch: 0.809801  [   89/   89]
Train Error: Avg loss: 0.32067732
validation Error: 
 Avg loss: 0.50913103 
 F1: 0.376584 
 Precision: 0.463672 
 Recall: 0.317038
 IoU: 0.231970

test Error: 
 Avg loss: 0.48851120 
 F1: 0.391908 
 Precision: 0.456036 
 Recall: 0.343592
 IoU: 0.243710

We have finished training iteration 210
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_208_.pth
Per-example loss in batch: 0.266973  [    2/   89]
Per-example loss in batch: 0.351261  [    4/   89]
Per-example loss in batch: 0.336709  [    6/   89]
Per-example loss in batch: 0.329186  [    8/   89]
Per-example loss in batch: 0.276877  [   10/   89]
Per-example loss in batch: 0.299924  [   12/   89]
Per-example loss in batch: 0.349413  [   14/   89]
Per-example loss in batch: 0.420844  [   16/   89]
Per-example loss in batch: 0.337555  [   18/   89]
Per-example loss in batch: 0.297051  [   20/   89]
Per-example loss in batch: 0.255955  [   22/   89]
Per-example loss in batch: 0.391586  [   24/   89]
Per-example loss in batch: 0.291715  [   26/   89]
Per-example loss in batch: 0.303686  [   28/   89]
Per-example loss in batch: 0.374208  [   30/   89]
Per-example loss in batch: 0.265459  [   32/   89]
Per-example loss in batch: 0.356734  [   34/   89]
Per-example loss in batch: 0.316002  [   36/   89]
Per-example loss in batch: 0.302985  [   38/   89]
Per-example loss in batch: 0.296150  [   40/   89]
Per-example loss in batch: 0.380206  [   42/   89]
Per-example loss in batch: 0.230550  [   44/   89]
Per-example loss in batch: 0.338442  [   46/   89]
Per-example loss in batch: 0.326631  [   48/   89]
Per-example loss in batch: 0.317505  [   50/   89]
Per-example loss in batch: 0.286998  [   52/   89]
Per-example loss in batch: 0.298450  [   54/   89]
Per-example loss in batch: 0.262793  [   56/   89]
Per-example loss in batch: 0.377954  [   58/   89]
Per-example loss in batch: 0.265413  [   60/   89]
Per-example loss in batch: 0.307124  [   62/   89]
Per-example loss in batch: 0.337472  [   64/   89]
Per-example loss in batch: 0.333956  [   66/   89]
Per-example loss in batch: 0.325262  [   68/   89]
Per-example loss in batch: 0.294097  [   70/   89]
Per-example loss in batch: 0.294466  [   72/   89]
Per-example loss in batch: 0.341387  [   74/   89]
Per-example loss in batch: 0.270382  [   76/   89]
Per-example loss in batch: 0.383594  [   78/   89]
Per-example loss in batch: 0.266417  [   80/   89]
Per-example loss in batch: 0.388760  [   82/   89]
Per-example loss in batch: 0.245853  [   84/   89]
Per-example loss in batch: 0.362878  [   86/   89]
Per-example loss in batch: 0.387712  [   88/   89]
Per-example loss in batch: 0.493080  [   89/   89]
Train Error: Avg loss: 0.32114864
validation Error: 
 Avg loss: 0.50773982 
 F1: 0.379741 
 Precision: 0.510568 
 Recall: 0.302285
 IoU: 0.234371

test Error: 
 Avg loss: 0.48834851 
 F1: 0.387558 
 Precision: 0.500431 
 Recall: 0.316232
 IoU: 0.240355

We have finished training iteration 211
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_209_.pth
Per-example loss in batch: 0.318811  [    2/   89]
Per-example loss in batch: 0.329112  [    4/   89]
Per-example loss in batch: 0.306789  [    6/   89]
Per-example loss in batch: 0.252417  [    8/   89]
Per-example loss in batch: 0.373170  [   10/   89]
Per-example loss in batch: 0.386542  [   12/   89]
Per-example loss in batch: 0.272018  [   14/   89]
Per-example loss in batch: 0.408702  [   16/   89]
Per-example loss in batch: 0.364846  [   18/   89]
Per-example loss in batch: 0.338860  [   20/   89]
Per-example loss in batch: 0.276628  [   22/   89]
Per-example loss in batch: 0.297577  [   24/   89]
Per-example loss in batch: 0.339099  [   26/   89]
Per-example loss in batch: 0.346671  [   28/   89]
Per-example loss in batch: 0.283429  [   30/   89]
Per-example loss in batch: 0.306355  [   32/   89]
Per-example loss in batch: 0.284661  [   34/   89]
Per-example loss in batch: 0.325073  [   36/   89]
Per-example loss in batch: 0.302301  [   38/   89]
Per-example loss in batch: 0.400419  [   40/   89]
Per-example loss in batch: 0.331332  [   42/   89]
Per-example loss in batch: 0.291293  [   44/   89]
Per-example loss in batch: 0.361244  [   46/   89]
Per-example loss in batch: 0.293071  [   48/   89]
Per-example loss in batch: 0.307908  [   50/   89]
Per-example loss in batch: 0.289245  [   52/   89]
Per-example loss in batch: 0.373735  [   54/   89]
Per-example loss in batch: 0.304296  [   56/   89]
Per-example loss in batch: 0.297304  [   58/   89]
Per-example loss in batch: 0.269109  [   60/   89]
Per-example loss in batch: 0.302213  [   62/   89]
Per-example loss in batch: 0.309549  [   64/   89]
Per-example loss in batch: 0.236707  [   66/   89]
Per-example loss in batch: 0.314323  [   68/   89]
Per-example loss in batch: 0.251679  [   70/   89]
Per-example loss in batch: 0.309722  [   72/   89]
Per-example loss in batch: 0.347113  [   74/   89]
Per-example loss in batch: 0.280801  [   76/   89]
Per-example loss in batch: 0.366910  [   78/   89]
Per-example loss in batch: 0.249577  [   80/   89]
Per-example loss in batch: 0.322319  [   82/   89]
Per-example loss in batch: 0.296194  [   84/   89]
Per-example loss in batch: 0.390377  [   86/   89]
Per-example loss in batch: 0.334526  [   88/   89]
Per-example loss in batch: 0.828674  [   89/   89]
Train Error: Avg loss: 0.32265990
validation Error: 
 Avg loss: 0.50906718 
 F1: 0.371662 
 Precision: 0.496555 
 Recall: 0.296969
 IoU: 0.228246

test Error: 
 Avg loss: 0.48899066 
 F1: 0.377335 
 Precision: 0.485657 
 Recall: 0.308521
 IoU: 0.232540

We have finished training iteration 212
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_210_.pth
Per-example loss in batch: 0.262982  [    2/   89]
Per-example loss in batch: 0.289467  [    4/   89]
Per-example loss in batch: 0.347136  [    6/   89]
Per-example loss in batch: 0.242505  [    8/   89]
Per-example loss in batch: 0.380607  [   10/   89]
Per-example loss in batch: 0.319685  [   12/   89]
Per-example loss in batch: 0.265082  [   14/   89]
Per-example loss in batch: 0.446107  [   16/   89]
Per-example loss in batch: 0.361782  [   18/   89]
Per-example loss in batch: 0.337581  [   20/   89]
Per-example loss in batch: 0.354654  [   22/   89]
Per-example loss in batch: 0.413604  [   24/   89]
Per-example loss in batch: 0.286755  [   26/   89]
Per-example loss in batch: 0.307411  [   28/   89]
Per-example loss in batch: 0.340288  [   30/   89]
Per-example loss in batch: 0.297529  [   32/   89]
Per-example loss in batch: 0.318493  [   34/   89]
Per-example loss in batch: 0.246591  [   36/   89]
Per-example loss in batch: 0.310013  [   38/   89]
Per-example loss in batch: 0.413827  [   40/   89]
Per-example loss in batch: 0.359008  [   42/   89]
Per-example loss in batch: 0.302049  [   44/   89]
Per-example loss in batch: 0.372528  [   46/   89]
Per-example loss in batch: 0.366917  [   48/   89]
Per-example loss in batch: 0.289328  [   50/   89]
Per-example loss in batch: 0.268528  [   52/   89]
Per-example loss in batch: 0.349890  [   54/   89]
Per-example loss in batch: 0.255602  [   56/   89]
Per-example loss in batch: 0.312098  [   58/   89]
Per-example loss in batch: 0.311257  [   60/   89]
Per-example loss in batch: 0.292970  [   62/   89]
Per-example loss in batch: 0.285077  [   64/   89]
Per-example loss in batch: 0.241930  [   66/   89]
Per-example loss in batch: 0.247282  [   68/   89]
Per-example loss in batch: 0.356971  [   70/   89]
Per-example loss in batch: 0.290961  [   72/   89]
Per-example loss in batch: 0.350867  [   74/   89]
Per-example loss in batch: 0.315031  [   76/   89]
Per-example loss in batch: 0.300618  [   78/   89]
Per-example loss in batch: 0.299526  [   80/   89]
Per-example loss in batch: 0.427773  [   82/   89]
Per-example loss in batch: 0.367550  [   84/   89]
Per-example loss in batch: 0.294218  [   86/   89]
Per-example loss in batch: 0.251851  [   88/   89]
Per-example loss in batch: 0.542787  [   89/   89]
Train Error: Avg loss: 0.32187244
validation Error: 
 Avg loss: 0.50894741 
 F1: 0.378450 
 Precision: 0.498405 
 Recall: 0.305035
 IoU: 0.233388

test Error: 
 Avg loss: 0.48874362 
 F1: 0.389897 
 Precision: 0.487411 
 Recall: 0.324897
 IoU: 0.242157

We have finished training iteration 213
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_211_.pth
Per-example loss in batch: 0.324722  [    2/   89]
Per-example loss in batch: 0.318209  [    4/   89]
Per-example loss in batch: 0.304699  [    6/   89]
Per-example loss in batch: 0.337534  [    8/   89]
Per-example loss in batch: 0.362046  [   10/   89]
Per-example loss in batch: 0.236462  [   12/   89]
Per-example loss in batch: 0.380260  [   14/   89]
Per-example loss in batch: 0.349321  [   16/   89]
Per-example loss in batch: 0.382408  [   18/   89]
Per-example loss in batch: 0.361373  [   20/   89]
Per-example loss in batch: 0.364010  [   22/   89]
Per-example loss in batch: 0.287582  [   24/   89]
Per-example loss in batch: 0.236119  [   26/   89]
Per-example loss in batch: 0.263221  [   28/   89]
Per-example loss in batch: 0.308035  [   30/   89]
Per-example loss in batch: 0.286462  [   32/   89]
Per-example loss in batch: 0.334032  [   34/   89]
Per-example loss in batch: 0.370842  [   36/   89]
Per-example loss in batch: 0.282866  [   38/   89]
Per-example loss in batch: 0.286000  [   40/   89]
Per-example loss in batch: 0.332540  [   42/   89]
Per-example loss in batch: 0.249818  [   44/   89]
Per-example loss in batch: 0.276250  [   46/   89]
Per-example loss in batch: 0.374819  [   48/   89]
Per-example loss in batch: 0.306541  [   50/   89]
Per-example loss in batch: 0.297117  [   52/   89]
Per-example loss in batch: 0.250138  [   54/   89]
Per-example loss in batch: 0.365815  [   56/   89]
Per-example loss in batch: 0.369145  [   58/   89]
Per-example loss in batch: 0.288100  [   60/   89]
Per-example loss in batch: 0.258686  [   62/   89]
Per-example loss in batch: 0.257519  [   64/   89]
Per-example loss in batch: 0.367746  [   66/   89]
Per-example loss in batch: 0.337215  [   68/   89]
Per-example loss in batch: 0.303078  [   70/   89]
Per-example loss in batch: 0.284319  [   72/   89]
Per-example loss in batch: 0.266649  [   74/   89]
Per-example loss in batch: 0.285002  [   76/   89]
Per-example loss in batch: 0.246477  [   78/   89]
Per-example loss in batch: 0.299838  [   80/   89]
Per-example loss in batch: 0.333917  [   82/   89]
Per-example loss in batch: 0.337711  [   84/   89]
Per-example loss in batch: 0.311143  [   86/   89]
Per-example loss in batch: 0.395975  [   88/   89]
Per-example loss in batch: 0.734106  [   89/   89]
Train Error: Avg loss: 0.31772617
validation Error: 
 Avg loss: 0.50881214 
 F1: 0.378949 
 Precision: 0.442309 
 Recall: 0.331468
 IoU: 0.233768

test Error: 
 Avg loss: 0.48733334 
 F1: 0.396642 
 Precision: 0.436760 
 Recall: 0.363274
 IoU: 0.247382

We have finished training iteration 214
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_212_.pth
Per-example loss in batch: 0.335715  [    2/   89]
Per-example loss in batch: 0.288100  [    4/   89]
Per-example loss in batch: 0.308149  [    6/   89]
Per-example loss in batch: 0.263355  [    8/   89]
Per-example loss in batch: 0.307171  [   10/   89]
Per-example loss in batch: 0.388617  [   12/   89]
Per-example loss in batch: 0.403365  [   14/   89]
Per-example loss in batch: 0.232313  [   16/   89]
Per-example loss in batch: 0.327316  [   18/   89]
Per-example loss in batch: 0.310559  [   20/   89]
Per-example loss in batch: 0.348277  [   22/   89]
Per-example loss in batch: 0.242653  [   24/   89]
Per-example loss in batch: 0.309967  [   26/   89]
Per-example loss in batch: 0.378609  [   28/   89]
Per-example loss in batch: 0.279854  [   30/   89]
Per-example loss in batch: 0.348254  [   32/   89]
Per-example loss in batch: 0.329857  [   34/   89]
Per-example loss in batch: 0.298463  [   36/   89]
Per-example loss in batch: 0.307749  [   38/   89]
Per-example loss in batch: 0.320212  [   40/   89]
Per-example loss in batch: 0.385741  [   42/   89]
Per-example loss in batch: 0.361676  [   44/   89]
Per-example loss in batch: 0.279598  [   46/   89]
Per-example loss in batch: 0.275379  [   48/   89]
Per-example loss in batch: 0.269966  [   50/   89]
Per-example loss in batch: 0.275140  [   52/   89]
Per-example loss in batch: 0.316924  [   54/   89]
Per-example loss in batch: 0.316384  [   56/   89]
Per-example loss in batch: 0.308170  [   58/   89]
Per-example loss in batch: 0.306584  [   60/   89]
Per-example loss in batch: 0.350324  [   62/   89]
Per-example loss in batch: 0.283925  [   64/   89]
Per-example loss in batch: 0.404437  [   66/   89]
Per-example loss in batch: 0.340130  [   68/   89]
Per-example loss in batch: 0.296213  [   70/   89]
Per-example loss in batch: 0.303188  [   72/   89]
Per-example loss in batch: 0.324489  [   74/   89]
Per-example loss in batch: 0.315520  [   76/   89]
Per-example loss in batch: 0.322839  [   78/   89]
Per-example loss in batch: 0.313687  [   80/   89]
Per-example loss in batch: 0.284129  [   82/   89]
Per-example loss in batch: 0.400174  [   84/   89]
Per-example loss in batch: 0.379721  [   86/   89]
Per-example loss in batch: 0.242098  [   88/   89]
Per-example loss in batch: 0.727453  [   89/   89]
Train Error: Avg loss: 0.32244315
validation Error: 
 Avg loss: 0.50892138 
 F1: 0.378359 
 Precision: 0.488375 
 Recall: 0.308797
 IoU: 0.233319

test Error: 
 Avg loss: 0.48845029 
 F1: 0.389792 
 Precision: 0.476349 
 Recall: 0.329855
 IoU: 0.242076

We have finished training iteration 215
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_213_.pth
Per-example loss in batch: 0.310199  [    2/   89]
Per-example loss in batch: 0.310683  [    4/   89]
Per-example loss in batch: 0.325949  [    6/   89]
Per-example loss in batch: 0.250542  [    8/   89]
Per-example loss in batch: 0.385246  [   10/   89]
Per-example loss in batch: 0.324388  [   12/   89]
Per-example loss in batch: 0.343168  [   14/   89]
Per-example loss in batch: 0.343243  [   16/   89]
Per-example loss in batch: 0.314355  [   18/   89]
Per-example loss in batch: 0.337812  [   20/   89]
Per-example loss in batch: 0.280587  [   22/   89]
Per-example loss in batch: 0.374870  [   24/   89]
Per-example loss in batch: 0.263813  [   26/   89]
Per-example loss in batch: 0.387098  [   28/   89]
Per-example loss in batch: 0.414054  [   30/   89]
Per-example loss in batch: 0.315764  [   32/   89]
Per-example loss in batch: 0.289213  [   34/   89]
Per-example loss in batch: 0.254340  [   36/   89]
Per-example loss in batch: 0.396191  [   38/   89]
Per-example loss in batch: 0.348760  [   40/   89]
Per-example loss in batch: 0.376144  [   42/   89]
Per-example loss in batch: 0.336934  [   44/   89]
Per-example loss in batch: 0.331599  [   46/   89]
Per-example loss in batch: 0.292371  [   48/   89]
Per-example loss in batch: 0.250863  [   50/   89]
Per-example loss in batch: 0.326823  [   52/   89]
Per-example loss in batch: 0.299007  [   54/   89]
Per-example loss in batch: 0.307734  [   56/   89]
Per-example loss in batch: 0.272415  [   58/   89]
Per-example loss in batch: 0.262633  [   60/   89]
Per-example loss in batch: 0.303775  [   62/   89]
Per-example loss in batch: 0.320906  [   64/   89]
Per-example loss in batch: 0.302715  [   66/   89]
Per-example loss in batch: 0.352256  [   68/   89]
Per-example loss in batch: 0.284965  [   70/   89]
Per-example loss in batch: 0.255911  [   72/   89]
Per-example loss in batch: 0.412377  [   74/   89]
Per-example loss in batch: 0.266288  [   76/   89]
Per-example loss in batch: 0.327988  [   78/   89]
Per-example loss in batch: 0.239130  [   80/   89]
Per-example loss in batch: 0.342490  [   82/   89]
Per-example loss in batch: 0.305300  [   84/   89]
Per-example loss in batch: 0.311043  [   86/   89]
Per-example loss in batch: 0.287968  [   88/   89]
Per-example loss in batch: 0.671745  [   89/   89]
Train Error: Avg loss: 0.32080420
validation Error: 
 Avg loss: 0.50702562 
 F1: 0.371538 
 Precision: 0.565782 
 Recall: 0.276583
 IoU: 0.228153

test Error: 
 Avg loss: 0.48731927 
 F1: 0.382699 
 Precision: 0.551366 
 Recall: 0.293052
 IoU: 0.236628

We have finished training iteration 216
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_214_.pth
Per-example loss in batch: 0.258467  [    2/   89]
Per-example loss in batch: 0.308038  [    4/   89]
Per-example loss in batch: 0.249129  [    6/   89]
Per-example loss in batch: 0.353455  [    8/   89]
Per-example loss in batch: 0.298673  [   10/   89]
Per-example loss in batch: 0.300912  [   12/   89]
Per-example loss in batch: 0.319294  [   14/   89]
Per-example loss in batch: 0.323293  [   16/   89]
Per-example loss in batch: 0.465540  [   18/   89]
Per-example loss in batch: 0.322623  [   20/   89]
Per-example loss in batch: 0.290948  [   22/   89]
Per-example loss in batch: 0.310966  [   24/   89]
Per-example loss in batch: 0.375581  [   26/   89]
Per-example loss in batch: 0.306301  [   28/   89]
Per-example loss in batch: 0.339727  [   30/   89]
Per-example loss in batch: 0.340575  [   32/   89]
Per-example loss in batch: 0.360383  [   34/   89]
Per-example loss in batch: 0.364902  [   36/   89]
Per-example loss in batch: 0.297560  [   38/   89]
Per-example loss in batch: 0.357169  [   40/   89]
Per-example loss in batch: 0.268008  [   42/   89]
Per-example loss in batch: 0.335739  [   44/   89]
Per-example loss in batch: 0.344334  [   46/   89]
Per-example loss in batch: 0.282063  [   48/   89]
Per-example loss in batch: 0.290364  [   50/   89]
Per-example loss in batch: 0.327447  [   52/   89]
Per-example loss in batch: 0.349054  [   54/   89]
Per-example loss in batch: 0.345043  [   56/   89]
Per-example loss in batch: 0.391084  [   58/   89]
Per-example loss in batch: 0.347438  [   60/   89]
Per-example loss in batch: 0.295835  [   62/   89]
Per-example loss in batch: 0.285780  [   64/   89]
Per-example loss in batch: 0.384415  [   66/   89]
Per-example loss in batch: 0.310449  [   68/   89]
Per-example loss in batch: 0.296839  [   70/   89]
Per-example loss in batch: 0.269334  [   72/   89]
Per-example loss in batch: 0.306862  [   74/   89]
Per-example loss in batch: 0.236737  [   76/   89]
Per-example loss in batch: 0.329056  [   78/   89]
Per-example loss in batch: 0.362840  [   80/   89]
Per-example loss in batch: 0.292347  [   82/   89]
Per-example loss in batch: 0.238484  [   84/   89]
Per-example loss in batch: 0.360144  [   86/   89]
Per-example loss in batch: 0.382751  [   88/   89]
Per-example loss in batch: 0.600723  [   89/   89]
Train Error: Avg loss: 0.32531117
validation Error: 
 Avg loss: 0.50768231 
 F1: 0.377782 
 Precision: 0.516996 
 Recall: 0.297635
 IoU: 0.232880

test Error: 
 Avg loss: 0.48716600 
 F1: 0.390633 
 Precision: 0.505826 
 Recall: 0.318175
 IoU: 0.242725

We have finished training iteration 217
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_215_.pth
Per-example loss in batch: 0.341370  [    2/   89]
Per-example loss in batch: 0.360795  [    4/   89]
Per-example loss in batch: 0.304818  [    6/   89]
Per-example loss in batch: 0.321093  [    8/   89]
Per-example loss in batch: 0.348160  [   10/   89]
Per-example loss in batch: 0.287799  [   12/   89]
Per-example loss in batch: 0.263082  [   14/   89]
Per-example loss in batch: 0.305246  [   16/   89]
Per-example loss in batch: 0.347373  [   18/   89]
Per-example loss in batch: 0.329790  [   20/   89]
Per-example loss in batch: 0.303842  [   22/   89]
Per-example loss in batch: 0.345049  [   24/   89]
Per-example loss in batch: 0.338535  [   26/   89]
Per-example loss in batch: 0.316676  [   28/   89]
Per-example loss in batch: 0.370765  [   30/   89]
Per-example loss in batch: 0.337848  [   32/   89]
Per-example loss in batch: 0.394017  [   34/   89]
Per-example loss in batch: 0.303130  [   36/   89]
Per-example loss in batch: 0.274121  [   38/   89]
Per-example loss in batch: 0.282551  [   40/   89]
Per-example loss in batch: 0.402055  [   42/   89]
Per-example loss in batch: 0.340197  [   44/   89]
Per-example loss in batch: 0.309348  [   46/   89]
Per-example loss in batch: 0.271281  [   48/   89]
Per-example loss in batch: 0.323482  [   50/   89]
Per-example loss in batch: 0.345468  [   52/   89]
Per-example loss in batch: 0.324984  [   54/   89]
Per-example loss in batch: 0.405770  [   56/   89]
Per-example loss in batch: 0.360269  [   58/   89]
Per-example loss in batch: 0.367157  [   60/   89]
Per-example loss in batch: 0.299964  [   62/   89]
Per-example loss in batch: 0.334574  [   64/   89]
Per-example loss in batch: 0.357418  [   66/   89]
Per-example loss in batch: 0.376789  [   68/   89]
Per-example loss in batch: 0.281208  [   70/   89]
Per-example loss in batch: 0.308840  [   72/   89]
Per-example loss in batch: 0.233792  [   74/   89]
Per-example loss in batch: 0.299700  [   76/   89]
Per-example loss in batch: 0.261888  [   78/   89]
Per-example loss in batch: 0.368044  [   80/   89]
Per-example loss in batch: 0.346759  [   82/   89]
Per-example loss in batch: 0.337582  [   84/   89]
Per-example loss in batch: 0.284333  [   86/   89]
Per-example loss in batch: 0.358862  [   88/   89]
Per-example loss in batch: 0.728966  [   89/   89]
Train Error: Avg loss: 0.33124277
validation Error: 
 Avg loss: 0.50634833 
 F1: 0.349772 
 Precision: 0.623427 
 Recall: 0.243074
 IoU: 0.211954

test Error: 
 Avg loss: 0.48522710 
 F1: 0.362654 
 Precision: 0.605641 
 Recall: 0.258815
 IoU: 0.221489

We have finished training iteration 218
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_63_.pth
Per-example loss in batch: 0.358651  [    2/   89]
Per-example loss in batch: 0.257525  [    4/   89]
Per-example loss in batch: 0.367129  [    6/   89]
Per-example loss in batch: 0.420216  [    8/   89]
Per-example loss in batch: 0.301155  [   10/   89]
Per-example loss in batch: 0.368336  [   12/   89]
Per-example loss in batch: 0.327259  [   14/   89]
Per-example loss in batch: 0.328145  [   16/   89]
Per-example loss in batch: 0.295537  [   18/   89]
Per-example loss in batch: 0.274275  [   20/   89]
Per-example loss in batch: 0.336448  [   22/   89]
Per-example loss in batch: 0.351191  [   24/   89]
Per-example loss in batch: 0.283930  [   26/   89]
Per-example loss in batch: 0.306673  [   28/   89]
Per-example loss in batch: 0.289969  [   30/   89]
Per-example loss in batch: 0.323064  [   32/   89]
Per-example loss in batch: 0.341735  [   34/   89]
Per-example loss in batch: 0.358573  [   36/   89]
Per-example loss in batch: 0.357207  [   38/   89]
Per-example loss in batch: 0.330940  [   40/   89]
Per-example loss in batch: 0.372981  [   42/   89]
Per-example loss in batch: 0.321549  [   44/   89]
Per-example loss in batch: 0.247039  [   46/   89]
Per-example loss in batch: 0.355246  [   48/   89]
Per-example loss in batch: 0.237969  [   50/   89]
Per-example loss in batch: 0.337950  [   52/   89]
Per-example loss in batch: 0.351417  [   54/   89]
Per-example loss in batch: 0.357228  [   56/   89]
Per-example loss in batch: 0.317608  [   58/   89]
Per-example loss in batch: 0.329185  [   60/   89]
Per-example loss in batch: 0.304490  [   62/   89]
Per-example loss in batch: 0.284311  [   64/   89]
Per-example loss in batch: 0.318438  [   66/   89]
Per-example loss in batch: 0.246853  [   68/   89]
Per-example loss in batch: 0.279134  [   70/   89]
Per-example loss in batch: 0.250299  [   72/   89]
Per-example loss in batch: 0.304722  [   74/   89]
Per-example loss in batch: 0.314179  [   76/   89]
Per-example loss in batch: 0.293037  [   78/   89]
Per-example loss in batch: 0.326677  [   80/   89]
Per-example loss in batch: 0.244174  [   82/   89]
Per-example loss in batch: 0.307126  [   84/   89]
Per-example loss in batch: 0.291027  [   86/   89]
Per-example loss in batch: 0.370438  [   88/   89]
Per-example loss in batch: 0.649001  [   89/   89]
Train Error: Avg loss: 0.32057383
validation Error: 
 Avg loss: 0.50877248 
 F1: 0.366371 
 Precision: 0.426003 
 Recall: 0.321384
 IoU: 0.224268

test Error: 
 Avg loss: 0.48840153 
 F1: 0.380416 
 Precision: 0.420159 
 Recall: 0.347542
 IoU: 0.234885

We have finished training iteration 219
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_217_.pth
Per-example loss in batch: 0.347136  [    2/   89]
Per-example loss in batch: 0.332511  [    4/   89]
Per-example loss in batch: 0.300082  [    6/   89]
Per-example loss in batch: 0.367040  [    8/   89]
Per-example loss in batch: 0.415600  [   10/   89]
Per-example loss in batch: 0.344761  [   12/   89]
Per-example loss in batch: 0.391792  [   14/   89]
Per-example loss in batch: 0.362365  [   16/   89]
Per-example loss in batch: 0.367366  [   18/   89]
Per-example loss in batch: 0.323502  [   20/   89]
Per-example loss in batch: 0.270762  [   22/   89]
Per-example loss in batch: 0.336786  [   24/   89]
Per-example loss in batch: 0.331995  [   26/   89]
Per-example loss in batch: 0.362150  [   28/   89]
Per-example loss in batch: 0.301431  [   30/   89]
Per-example loss in batch: 0.371665  [   32/   89]
Per-example loss in batch: 0.267998  [   34/   89]
Per-example loss in batch: 0.264550  [   36/   89]
Per-example loss in batch: 0.364719  [   38/   89]
Per-example loss in batch: 0.262039  [   40/   89]
Per-example loss in batch: 0.427823  [   42/   89]
Per-example loss in batch: 0.279442  [   44/   89]
Per-example loss in batch: 0.308763  [   46/   89]
Per-example loss in batch: 0.252860  [   48/   89]
Per-example loss in batch: 0.310212  [   50/   89]
Per-example loss in batch: 0.287505  [   52/   89]
Per-example loss in batch: 0.371634  [   54/   89]
Per-example loss in batch: 0.351942  [   56/   89]
Per-example loss in batch: 0.337760  [   58/   89]
Per-example loss in batch: 0.375553  [   60/   89]
Per-example loss in batch: 0.275716  [   62/   89]
Per-example loss in batch: 0.253906  [   64/   89]
Per-example loss in batch: 0.260735  [   66/   89]
Per-example loss in batch: 0.271021  [   68/   89]
Per-example loss in batch: 0.327656  [   70/   89]
Per-example loss in batch: 0.378451  [   72/   89]
Per-example loss in batch: 0.273450  [   74/   89]
Per-example loss in batch: 0.412969  [   76/   89]
Per-example loss in batch: 0.274974  [   78/   89]
Per-example loss in batch: 0.371164  [   80/   89]
Per-example loss in batch: 0.295293  [   82/   89]
Per-example loss in batch: 0.291086  [   84/   89]
Per-example loss in batch: 0.358781  [   86/   89]
Per-example loss in batch: 0.354892  [   88/   89]
Per-example loss in batch: 0.485122  [   89/   89]
Train Error: Avg loss: 0.32881792
validation Error: 
 Avg loss: 0.50954323 
 F1: 0.373483 
 Precision: 0.517629 
 Recall: 0.292133
 IoU: 0.229622

test Error: 
 Avg loss: 0.48917991 
 F1: 0.386052 
 Precision: 0.511942 
 Recall: 0.309857
 IoU: 0.239198

We have finished training iteration 220
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_55_.pth
Per-example loss in batch: 0.409686  [    2/   89]
Per-example loss in batch: 0.392722  [    4/   89]
Per-example loss in batch: 0.296304  [    6/   89]
Per-example loss in batch: 0.363985  [    8/   89]
Per-example loss in batch: 0.358889  [   10/   89]
Per-example loss in batch: 0.324165  [   12/   89]
Per-example loss in batch: 0.327325  [   14/   89]
Per-example loss in batch: 0.334437  [   16/   89]
Per-example loss in batch: 0.383801  [   18/   89]
Per-example loss in batch: 0.300513  [   20/   89]
Per-example loss in batch: 0.290373  [   22/   89]
Per-example loss in batch: 0.340240  [   24/   89]
Per-example loss in batch: 0.366036  [   26/   89]
Per-example loss in batch: 0.291731  [   28/   89]
Per-example loss in batch: 0.357908  [   30/   89]
Per-example loss in batch: 0.342258  [   32/   89]
Per-example loss in batch: 0.334222  [   34/   89]
Per-example loss in batch: 0.334364  [   36/   89]
Per-example loss in batch: 0.351281  [   38/   89]
Per-example loss in batch: 0.322088  [   40/   89]
Per-example loss in batch: 0.319194  [   42/   89]
Per-example loss in batch: 0.311804  [   44/   89]
Per-example loss in batch: 0.363975  [   46/   89]
Per-example loss in batch: 0.311859  [   48/   89]
Per-example loss in batch: 0.325254  [   50/   89]
Per-example loss in batch: 0.341500  [   52/   89]
Per-example loss in batch: 0.316192  [   54/   89]
Per-example loss in batch: 0.329608  [   56/   89]
Per-example loss in batch: 0.324605  [   58/   89]
Per-example loss in batch: 0.288065  [   60/   89]
Per-example loss in batch: 0.230751  [   62/   89]
Per-example loss in batch: 0.284870  [   64/   89]
Per-example loss in batch: 0.337082  [   66/   89]
Per-example loss in batch: 0.321949  [   68/   89]
Per-example loss in batch: 0.245528  [   70/   89]
Per-example loss in batch: 0.292314  [   72/   89]
Per-example loss in batch: 0.238653  [   74/   89]
Per-example loss in batch: 0.303590  [   76/   89]
Per-example loss in batch: 0.366960  [   78/   89]
Per-example loss in batch: 0.371610  [   80/   89]
Per-example loss in batch: 0.290243  [   82/   89]
Per-example loss in batch: 0.332635  [   84/   89]
Per-example loss in batch: 0.266714  [   86/   89]
Per-example loss in batch: 0.405824  [   88/   89]
Per-example loss in batch: 0.637241  [   89/   89]
Train Error: Avg loss: 0.32947705
validation Error: 
 Avg loss: 0.50929357 
 F1: 0.374097 
 Precision: 0.518053 
 Recall: 0.292748
 IoU: 0.230085

test Error: 
 Avg loss: 0.48867466 
 F1: 0.388775 
 Precision: 0.515256 
 Recall: 0.312150
 IoU: 0.241291

We have finished training iteration 221
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_219_.pth
Per-example loss in batch: 0.311944  [    2/   89]
Per-example loss in batch: 0.271105  [    4/   89]
Per-example loss in batch: 0.321583  [    6/   89]
Per-example loss in batch: 0.244060  [    8/   89]
Per-example loss in batch: 0.243774  [   10/   89]
Per-example loss in batch: 0.282012  [   12/   89]
Per-example loss in batch: 0.307839  [   14/   89]
Per-example loss in batch: 0.250240  [   16/   89]
Per-example loss in batch: 0.285071  [   18/   89]
Per-example loss in batch: 0.281752  [   20/   89]
Per-example loss in batch: 0.306997  [   22/   89]
Per-example loss in batch: 0.298487  [   24/   89]
Per-example loss in batch: 0.395541  [   26/   89]
Per-example loss in batch: 0.300350  [   28/   89]
Per-example loss in batch: 0.255964  [   30/   89]
Per-example loss in batch: 0.364656  [   32/   89]
Per-example loss in batch: 0.321413  [   34/   89]
Per-example loss in batch: 0.253305  [   36/   89]
Per-example loss in batch: 0.289109  [   38/   89]
Per-example loss in batch: 0.372402  [   40/   89]
Per-example loss in batch: 0.329040  [   42/   89]
Per-example loss in batch: 0.394083  [   44/   89]
Per-example loss in batch: 0.381285  [   46/   89]
Per-example loss in batch: 0.271934  [   48/   89]
Per-example loss in batch: 0.382509  [   50/   89]
Per-example loss in batch: 0.305979  [   52/   89]
Per-example loss in batch: 0.355602  [   54/   89]
Per-example loss in batch: 0.343323  [   56/   89]
Per-example loss in batch: 0.341969  [   58/   89]
Per-example loss in batch: 0.287986  [   60/   89]
Per-example loss in batch: 0.250381  [   62/   89]
Per-example loss in batch: 0.341295  [   64/   89]
Per-example loss in batch: 0.304216  [   66/   89]
Per-example loss in batch: 0.288138  [   68/   89]
Per-example loss in batch: 0.267683  [   70/   89]
Per-example loss in batch: 0.358149  [   72/   89]
Per-example loss in batch: 0.263268  [   74/   89]
Per-example loss in batch: 0.379601  [   76/   89]
Per-example loss in batch: 0.319625  [   78/   89]
Per-example loss in batch: 0.270617  [   80/   89]
Per-example loss in batch: 0.344403  [   82/   89]
Per-example loss in batch: 0.334202  [   84/   89]
Per-example loss in batch: 0.414946  [   86/   89]
Per-example loss in batch: 0.343885  [   88/   89]
Per-example loss in batch: 0.773595  [   89/   89]
Train Error: Avg loss: 0.31951730
validation Error: 
 Avg loss: 0.50890587 
 F1: 0.374699 
 Precision: 0.532037 
 Recall: 0.289180
 IoU: 0.230541

test Error: 
 Avg loss: 0.48886005 
 F1: 0.381526 
 Precision: 0.522580 
 Recall: 0.300434
 IoU: 0.235732

We have finished training iteration 222
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_220_.pth
Per-example loss in batch: 0.322786  [    2/   89]
Per-example loss in batch: 0.236948  [    4/   89]
Per-example loss in batch: 0.281945  [    6/   89]
Per-example loss in batch: 0.317205  [    8/   89]
Per-example loss in batch: 0.304915  [   10/   89]
Per-example loss in batch: 0.264798  [   12/   89]
Per-example loss in batch: 0.341627  [   14/   89]
Per-example loss in batch: 0.380327  [   16/   89]
Per-example loss in batch: 0.317605  [   18/   89]
Per-example loss in batch: 0.311636  [   20/   89]
Per-example loss in batch: 0.337835  [   22/   89]
Per-example loss in batch: 0.328077  [   24/   89]
Per-example loss in batch: 0.387037  [   26/   89]
Per-example loss in batch: 0.301746  [   28/   89]
Per-example loss in batch: 0.376219  [   30/   89]
Per-example loss in batch: 0.360147  [   32/   89]
Per-example loss in batch: 0.357573  [   34/   89]
Per-example loss in batch: 0.406649  [   36/   89]
Per-example loss in batch: 0.296922  [   38/   89]
Per-example loss in batch: 0.400501  [   40/   89]
Per-example loss in batch: 0.380675  [   42/   89]
Per-example loss in batch: 0.356007  [   44/   89]
Per-example loss in batch: 0.284293  [   46/   89]
Per-example loss in batch: 0.418797  [   48/   89]
Per-example loss in batch: 0.262617  [   50/   89]
Per-example loss in batch: 0.301916  [   52/   89]
Per-example loss in batch: 0.289665  [   54/   89]
Per-example loss in batch: 0.326119  [   56/   89]
Per-example loss in batch: 0.329333  [   58/   89]
Per-example loss in batch: 0.285763  [   60/   89]
Per-example loss in batch: 0.285190  [   62/   89]
Per-example loss in batch: 0.284872  [   64/   89]
Per-example loss in batch: 0.342046  [   66/   89]
Per-example loss in batch: 0.295303  [   68/   89]
Per-example loss in batch: 0.315554  [   70/   89]
Per-example loss in batch: 0.303560  [   72/   89]
Per-example loss in batch: 0.301161  [   74/   89]
Per-example loss in batch: 0.388584  [   76/   89]
Per-example loss in batch: 0.364017  [   78/   89]
Per-example loss in batch: 0.341204  [   80/   89]
Per-example loss in batch: 0.318261  [   82/   89]
Per-example loss in batch: 0.291307  [   84/   89]
Per-example loss in batch: 0.308339  [   86/   89]
Per-example loss in batch: 0.283152  [   88/   89]
Per-example loss in batch: 0.655821  [   89/   89]
Train Error: Avg loss: 0.32849751
validation Error: 
 Avg loss: 0.50894494 
 F1: 0.376566 
 Precision: 0.458178 
 Recall: 0.319631
 IoU: 0.231956

test Error: 
 Avg loss: 0.48835262 
 F1: 0.394815 
 Precision: 0.460627 
 Recall: 0.345458
 IoU: 0.245962

We have finished training iteration 223
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_221_.pth
Per-example loss in batch: 0.296230  [    2/   89]
Per-example loss in batch: 0.289777  [    4/   89]
Per-example loss in batch: 0.391000  [    6/   89]
Per-example loss in batch: 0.336123  [    8/   89]
Per-example loss in batch: 0.254564  [   10/   89]
Per-example loss in batch: 0.315709  [   12/   89]
Per-example loss in batch: 0.289285  [   14/   89]
Per-example loss in batch: 0.282776  [   16/   89]
Per-example loss in batch: 0.385633  [   18/   89]
Per-example loss in batch: 0.306027  [   20/   89]
Per-example loss in batch: 0.271136  [   22/   89]
Per-example loss in batch: 0.303298  [   24/   89]
Per-example loss in batch: 0.272254  [   26/   89]
Per-example loss in batch: 0.316997  [   28/   89]
Per-example loss in batch: 0.326816  [   30/   89]
Per-example loss in batch: 0.276707  [   32/   89]
Per-example loss in batch: 0.353031  [   34/   89]
Per-example loss in batch: 0.339439  [   36/   89]
Per-example loss in batch: 0.402077  [   38/   89]
Per-example loss in batch: 0.334013  [   40/   89]
Per-example loss in batch: 0.244023  [   42/   89]
Per-example loss in batch: 0.243135  [   44/   89]
Per-example loss in batch: 0.260879  [   46/   89]
Per-example loss in batch: 0.365036  [   48/   89]
Per-example loss in batch: 0.360797  [   50/   89]
Per-example loss in batch: 0.365555  [   52/   89]
Per-example loss in batch: 0.328210  [   54/   89]
Per-example loss in batch: 0.381606  [   56/   89]
Per-example loss in batch: 0.302393  [   58/   89]
Per-example loss in batch: 0.328004  [   60/   89]
Per-example loss in batch: 0.375609  [   62/   89]
Per-example loss in batch: 0.238592  [   64/   89]
Per-example loss in batch: 0.358237  [   66/   89]
Per-example loss in batch: 0.310882  [   68/   89]
Per-example loss in batch: 0.437228  [   70/   89]
Per-example loss in batch: 0.389413  [   72/   89]
Per-example loss in batch: 0.246167  [   74/   89]
Per-example loss in batch: 0.359630  [   76/   89]
Per-example loss in batch: 0.338961  [   78/   89]
Per-example loss in batch: 0.275830  [   80/   89]
Per-example loss in batch: 0.343576  [   82/   89]
Per-example loss in batch: 0.348086  [   84/   89]
Per-example loss in batch: 0.299124  [   86/   89]
Per-example loss in batch: 0.312460  [   88/   89]
Per-example loss in batch: 0.840840  [   89/   89]
Train Error: Avg loss: 0.32756734
validation Error: 
 Avg loss: 0.50776229 
 F1: 0.380604 
 Precision: 0.539039 
 Recall: 0.294148
 IoU: 0.235029

test Error: 
 Avg loss: 0.48694248 
 F1: 0.394018 
 Precision: 0.533580 
 Recall: 0.312326
 IoU: 0.245344

We have finished training iteration 224
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_222_.pth
Per-example loss in batch: 0.373564  [    2/   89]
Per-example loss in batch: 0.329187  [    4/   89]
Per-example loss in batch: 0.270424  [    6/   89]
Per-example loss in batch: 0.274549  [    8/   89]
Per-example loss in batch: 0.383037  [   10/   89]
Per-example loss in batch: 0.306089  [   12/   89]
Per-example loss in batch: 0.278171  [   14/   89]
Per-example loss in batch: 0.328024  [   16/   89]
Per-example loss in batch: 0.373091  [   18/   89]
Per-example loss in batch: 0.297336  [   20/   89]
Per-example loss in batch: 0.388819  [   22/   89]
Per-example loss in batch: 0.306047  [   24/   89]
Per-example loss in batch: 0.340818  [   26/   89]
Per-example loss in batch: 0.362576  [   28/   89]
Per-example loss in batch: 0.249824  [   30/   89]
Per-example loss in batch: 0.347392  [   32/   89]
Per-example loss in batch: 0.250219  [   34/   89]
Per-example loss in batch: 0.296785  [   36/   89]
Per-example loss in batch: 0.331017  [   38/   89]
Per-example loss in batch: 0.268773  [   40/   89]
Per-example loss in batch: 0.327767  [   42/   89]
Per-example loss in batch: 0.319485  [   44/   89]
Per-example loss in batch: 0.322429  [   46/   89]
Per-example loss in batch: 0.284543  [   48/   89]
Per-example loss in batch: 0.270107  [   50/   89]
Per-example loss in batch: 0.306345  [   52/   89]
Per-example loss in batch: 0.320252  [   54/   89]
Per-example loss in batch: 0.334428  [   56/   89]
Per-example loss in batch: 0.293342  [   58/   89]
Per-example loss in batch: 0.372269  [   60/   89]
Per-example loss in batch: 0.306216  [   62/   89]
Per-example loss in batch: 0.397704  [   64/   89]
Per-example loss in batch: 0.399356  [   66/   89]
Per-example loss in batch: 0.276063  [   68/   89]
Per-example loss in batch: 0.324170  [   70/   89]
Per-example loss in batch: 0.276293  [   72/   89]
Per-example loss in batch: 0.267995  [   74/   89]
Per-example loss in batch: 0.339115  [   76/   89]
Per-example loss in batch: 0.340910  [   78/   89]
Per-example loss in batch: 0.317567  [   80/   89]
Per-example loss in batch: 0.400165  [   82/   89]
Per-example loss in batch: 0.318196  [   84/   89]
Per-example loss in batch: 0.309168  [   86/   89]
Per-example loss in batch: 0.354924  [   88/   89]
Per-example loss in batch: 0.475372  [   89/   89]
Train Error: Avg loss: 0.32297157
validation Error: 
 Avg loss: 0.50839260 
 F1: 0.375551 
 Precision: 0.532832 
 Recall: 0.289960
 IoU: 0.231186

test Error: 
 Avg loss: 0.48763110 
 F1: 0.385501 
 Precision: 0.523253 
 Recall: 0.305164
 IoU: 0.238775

We have finished training iteration 225
Deleting model ./unet_wzo_train/saved_model_wrapper/models/UNet_223_.pth
slurmstepd: error: *** STEP 16442.0 ON aga1 CANCELLED AT 2025-01-07T16:45:17 ***
