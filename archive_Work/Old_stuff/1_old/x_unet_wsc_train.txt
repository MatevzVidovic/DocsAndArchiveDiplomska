/shared/home/matevz.vidovic/Diplomska/Prototip/Delo/model_wrapper.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model = torch.load(self.prev_model_path, map_location=torch.device(device))
unet_original_main.py do_log: True
Log file name: log_07_14-49-42_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(ptd='./Data/vein_and_sclera_data', sd='unet_wsc_train', mti=250, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_original_wsc.yaml', ntibp=None, ptp=None, map=None)
YAML: {'batch_size': 2, 'learning_rate': 1e-05, 'num_of_dataloader_workers': 7, 'train_epoch_size_limit': 400, 'num_epochs_per_training_iteration': 1, 'cleanup_k': 3, 'dataset_option': 'aug_with_sclera', 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': False, 'loss_fn_name': 'MCDL', 'alphas': [], 'model': '64_2_6', 'input_width': 2048, 'input_height': 1024, 'input_channels': 4, 'output_channels': 2, 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq'}
Validation phase: False
Namespace(ptd='./Data/vein_and_sclera_data', sd='unet_wsc_train', mti=250, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_original_wsc.yaml', ntibp=None, ptp=None, map=None)
Device: cuda
dataset_aug_with_sclera.py do_log: True
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 89
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 89
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 45
val dataloader num of batches: 14
test dataloader num of batches: 6
Loaded model path:  ./unet_wsc_train/saved_model_wrapper/models/UNet_186_.pth
Per-example loss in batch: 0.241814  [    2/   89]
Per-example loss in batch: 0.311415  [    4/   89]
Per-example loss in batch: 0.229873  [    6/   89]
Per-example loss in batch: 0.326124  [    8/   89]
Per-example loss in batch: 0.245054  [   10/   89]
Per-example loss in batch: 0.241667  [   12/   89]
Per-example loss in batch: 0.327039  [   14/   89]
Per-example loss in batch: 0.204513  [   16/   89]
Per-example loss in batch: 0.234828  [   18/   89]
Per-example loss in batch: 0.243028  [   20/   89]
Per-example loss in batch: 0.245590  [   22/   89]
Per-example loss in batch: 0.282699  [   24/   89]
Per-example loss in batch: 0.301568  [   26/   89]
Per-example loss in batch: 0.204868  [   28/   89]
Per-example loss in batch: 0.334500  [   30/   89]
Per-example loss in batch: 0.224024  [   32/   89]
Per-example loss in batch: 0.263127  [   34/   89]
Per-example loss in batch: 0.296236  [   36/   89]
Per-example loss in batch: 0.239218  [   38/   89]
Per-example loss in batch: 0.251748  [   40/   89]
Per-example loss in batch: 0.266437  [   42/   89]
Per-example loss in batch: 0.270964  [   44/   89]
Per-example loss in batch: 0.299770  [   46/   89]
Per-example loss in batch: 0.256930  [   48/   89]
Per-example loss in batch: 0.229448  [   50/   89]
Per-example loss in batch: 0.229272  [   52/   89]
Per-example loss in batch: 0.333057  [   54/   89]
Per-example loss in batch: 0.244301  [   56/   89]
Per-example loss in batch: 0.198220  [   58/   89]
Per-example loss in batch: 0.251698  [   60/   89]
Per-example loss in batch: 0.281602  [   62/   89]
Per-example loss in batch: 0.286246  [   64/   89]
Per-example loss in batch: 0.247837  [   66/   89]
Per-example loss in batch: 0.256247  [   68/   89]
Per-example loss in batch: 0.336753  [   70/   89]
Per-example loss in batch: 0.258885  [   72/   89]
Per-example loss in batch: 0.334470  [   74/   89]
Per-example loss in batch: 0.256379  [   76/   89]
Per-example loss in batch: 0.238836  [   78/   89]
Per-example loss in batch: 0.264405  [   80/   89]
Per-example loss in batch: 0.261828  [   82/   89]
Per-example loss in batch: 0.267212  [   84/   89]
Per-example loss in batch: 0.266879  [   86/   89]
Per-example loss in batch: 0.283456  [   88/   89]
Per-example loss in batch: 0.722642  [   89/   89]
Train Error: Avg loss: 0.27036826
validation Error: 
 Avg loss: 0.28115974 
 F1: 0.471072 
 Precision: 0.540030 
 Recall: 0.417731
 IoU: 0.308106

test Error: 
 Avg loss: 0.26350351 
 F1: 0.503989 
 Precision: 0.584830 
 Recall: 0.442783
 IoU: 0.336889

We have finished training iteration 187
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_185_.pth
Per-example loss in batch: 0.279438  [    2/   89]
Per-example loss in batch: 0.218834  [    4/   89]
Per-example loss in batch: 0.275405  [    6/   89]
Per-example loss in batch: 0.238674  [    8/   89]
Per-example loss in batch: 0.230878  [   10/   89]
Per-example loss in batch: 0.279089  [   12/   89]
Per-example loss in batch: 0.272641  [   14/   89]
Per-example loss in batch: 0.240832  [   16/   89]
Per-example loss in batch: 0.265849  [   18/   89]
Per-example loss in batch: 0.313610  [   20/   89]
Per-example loss in batch: 0.259465  [   22/   89]
Per-example loss in batch: 0.276845  [   24/   89]
Per-example loss in batch: 0.335658  [   26/   89]
Per-example loss in batch: 0.311533  [   28/   89]
Per-example loss in batch: 0.318098  [   30/   89]
Per-example loss in batch: 0.284343  [   32/   89]
Per-example loss in batch: 0.232685  [   34/   89]
Per-example loss in batch: 0.339746  [   36/   89]
Per-example loss in batch: 0.311957  [   38/   89]
Per-example loss in batch: 0.260441  [   40/   89]
Per-example loss in batch: 0.229938  [   42/   89]
Per-example loss in batch: 0.242408  [   44/   89]
Per-example loss in batch: 0.229133  [   46/   89]
Per-example loss in batch: 0.260690  [   48/   89]
Per-example loss in batch: 0.231893  [   50/   89]
Per-example loss in batch: 0.254445  [   52/   89]
Per-example loss in batch: 0.321626  [   54/   89]
Per-example loss in batch: 0.289051  [   56/   89]
Per-example loss in batch: 0.228721  [   58/   89]
Per-example loss in batch: 0.290504  [   60/   89]
Per-example loss in batch: 0.277520  [   62/   89]
Per-example loss in batch: 0.257867  [   64/   89]
Per-example loss in batch: 0.258397  [   66/   89]
Per-example loss in batch: 0.218834  [   68/   89]
Per-example loss in batch: 0.240848  [   70/   89]
Per-example loss in batch: 0.279997  [   72/   89]
Per-example loss in batch: 0.217994  [   74/   89]
Per-example loss in batch: 0.218521  [   76/   89]
Per-example loss in batch: 0.216213  [   78/   89]
Per-example loss in batch: 0.346015  [   80/   89]
Per-example loss in batch: 0.223261  [   82/   89]
Per-example loss in batch: 0.203600  [   84/   89]
Per-example loss in batch: 0.283115  [   86/   89]
Per-example loss in batch: 0.221290  [   88/   89]
Per-example loss in batch: 0.372773  [   89/   89]
Train Error: Avg loss: 0.26459076
validation Error: 
 Avg loss: 0.27819248 
 F1: 0.473113 
 Precision: 0.547592 
 Recall: 0.416468
 IoU: 0.309854

test Error: 
 Avg loss: 0.26197933 
 F1: 0.507708 
 Precision: 0.600298 
 Recall: 0.439863
 IoU: 0.340220

We have finished training iteration 188
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_186_.pth
Per-example loss in batch: 0.225858  [    2/   89]
Per-example loss in batch: 0.344331  [    4/   89]
Per-example loss in batch: 0.280861  [    6/   89]
Per-example loss in batch: 0.294730  [    8/   89]
Per-example loss in batch: 0.320071  [   10/   89]
Per-example loss in batch: 0.296558  [   12/   89]
Per-example loss in batch: 0.245400  [   14/   89]
Per-example loss in batch: 0.223790  [   16/   89]
Per-example loss in batch: 0.232492  [   18/   89]
Per-example loss in batch: 0.216885  [   20/   89]
Per-example loss in batch: 0.251656  [   22/   89]
Per-example loss in batch: 0.254708  [   24/   89]
Per-example loss in batch: 0.249188  [   26/   89]
Per-example loss in batch: 0.247253  [   28/   89]
Per-example loss in batch: 0.257643  [   30/   89]
Per-example loss in batch: 0.217906  [   32/   89]
Per-example loss in batch: 0.280173  [   34/   89]
Per-example loss in batch: 0.217337  [   36/   89]
Per-example loss in batch: 0.230128  [   38/   89]
Per-example loss in batch: 0.313728  [   40/   89]
Per-example loss in batch: 0.215438  [   42/   89]
Per-example loss in batch: 0.302108  [   44/   89]
Per-example loss in batch: 0.263125  [   46/   89]
Per-example loss in batch: 0.267441  [   48/   89]
Per-example loss in batch: 0.278897  [   50/   89]
Per-example loss in batch: 0.219752  [   52/   89]
Per-example loss in batch: 0.266383  [   54/   89]
Per-example loss in batch: 0.287330  [   56/   89]
Per-example loss in batch: 0.210782  [   58/   89]
Per-example loss in batch: 0.237067  [   60/   89]
Per-example loss in batch: 0.325107  [   62/   89]
Per-example loss in batch: 0.203468  [   64/   89]
Per-example loss in batch: 0.288537  [   66/   89]
Per-example loss in batch: 0.271975  [   68/   89]
Per-example loss in batch: 0.295041  [   70/   89]
Per-example loss in batch: 0.267401  [   72/   89]
Per-example loss in batch: 0.271380  [   74/   89]
Per-example loss in batch: 0.230067  [   76/   89]
Per-example loss in batch: 0.267580  [   78/   89]
Per-example loss in batch: 0.220506  [   80/   89]
Per-example loss in batch: 0.261723  [   82/   89]
Per-example loss in batch: 0.287161  [   84/   89]
Per-example loss in batch: 0.260267  [   86/   89]
Per-example loss in batch: 0.296368  [   88/   89]
Per-example loss in batch: 0.439933  [   89/   89]
Train Error: Avg loss: 0.26327113
validation Error: 
 Avg loss: 0.28502599 
 F1: 0.471229 
 Precision: 0.546699 
 Recall: 0.414067
 IoU: 0.308240

test Error: 
 Avg loss: 0.26378762 
 F1: 0.503664 
 Precision: 0.593535 
 Recall: 0.437430
 IoU: 0.336598

We have finished training iteration 189
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_120_.pth
Per-example loss in batch: 0.221083  [    2/   89]
Per-example loss in batch: 0.267150  [    4/   89]
Per-example loss in batch: 0.333822  [    6/   89]
Per-example loss in batch: 0.265515  [    8/   89]
Per-example loss in batch: 0.347562  [   10/   89]
Per-example loss in batch: 0.300435  [   12/   89]
Per-example loss in batch: 0.237595  [   14/   89]
Per-example loss in batch: 0.229460  [   16/   89]
Per-example loss in batch: 0.212871  [   18/   89]
Per-example loss in batch: 0.262284  [   20/   89]
Per-example loss in batch: 0.258645  [   22/   89]
Per-example loss in batch: 0.265316  [   24/   89]
Per-example loss in batch: 0.296513  [   26/   89]
Per-example loss in batch: 0.242337  [   28/   89]
Per-example loss in batch: 0.233701  [   30/   89]
Per-example loss in batch: 0.252535  [   32/   89]
Per-example loss in batch: 0.264384  [   34/   89]
Per-example loss in batch: 0.283821  [   36/   89]
Per-example loss in batch: 0.219139  [   38/   89]
Per-example loss in batch: 0.245447  [   40/   89]
Per-example loss in batch: 0.261427  [   42/   89]
Per-example loss in batch: 0.254577  [   44/   89]
Per-example loss in batch: 0.313332  [   46/   89]
Per-example loss in batch: 0.328441  [   48/   89]
Per-example loss in batch: 0.219610  [   50/   89]
Per-example loss in batch: 0.246053  [   52/   89]
Per-example loss in batch: 0.217588  [   54/   89]
Per-example loss in batch: 0.258975  [   56/   89]
Per-example loss in batch: 0.314456  [   58/   89]
Per-example loss in batch: 0.208205  [   60/   89]
Per-example loss in batch: 0.310923  [   62/   89]
Per-example loss in batch: 0.263922  [   64/   89]
Per-example loss in batch: 0.241898  [   66/   89]
Per-example loss in batch: 0.224040  [   68/   89]
Per-example loss in batch: 0.247260  [   70/   89]
Per-example loss in batch: 0.292836  [   72/   89]
Per-example loss in batch: 0.210150  [   74/   89]
Per-example loss in batch: 0.252827  [   76/   89]
Per-example loss in batch: 0.214984  [   78/   89]
Per-example loss in batch: 0.244981  [   80/   89]
Per-example loss in batch: 0.278518  [   82/   89]
Per-example loss in batch: 0.242763  [   84/   89]
Per-example loss in batch: 0.274007  [   86/   89]
Per-example loss in batch: 0.261376  [   88/   89]
Per-example loss in batch: 0.496051  [   89/   89]
Train Error: Avg loss: 0.26226487
validation Error: 
 Avg loss: 0.28415292 
 F1: 0.473061 
 Precision: 0.560248 
 Recall: 0.409356
 IoU: 0.309810

test Error: 
 Avg loss: 0.26320306 
 F1: 0.504863 
 Precision: 0.609598 
 Recall: 0.430840
 IoU: 0.337670

We have finished training iteration 190
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_187_.pth
Per-example loss in batch: 0.228213  [    2/   89]
Per-example loss in batch: 0.264121  [    4/   89]
Per-example loss in batch: 0.248369  [    6/   89]
Per-example loss in batch: 0.236159  [    8/   89]
Per-example loss in batch: 0.328315  [   10/   89]
Per-example loss in batch: 0.205007  [   12/   89]
Per-example loss in batch: 0.238309  [   14/   89]
Per-example loss in batch: 0.320074  [   16/   89]
Per-example loss in batch: 0.371733  [   18/   89]
Per-example loss in batch: 0.246990  [   20/   89]
Per-example loss in batch: 0.232969  [   22/   89]
Per-example loss in batch: 0.231175  [   24/   89]
Per-example loss in batch: 0.245633  [   26/   89]
Per-example loss in batch: 0.316777  [   28/   89]
Per-example loss in batch: 0.260774  [   30/   89]
Per-example loss in batch: 0.210189  [   32/   89]
Per-example loss in batch: 0.252095  [   34/   89]
Per-example loss in batch: 0.283655  [   36/   89]
Per-example loss in batch: 0.300920  [   38/   89]
Per-example loss in batch: 0.266460  [   40/   89]
Per-example loss in batch: 0.346821  [   42/   89]
Per-example loss in batch: 0.258824  [   44/   89]
Per-example loss in batch: 0.221787  [   46/   89]
Per-example loss in batch: 0.282255  [   48/   89]
Per-example loss in batch: 0.269107  [   50/   89]
Per-example loss in batch: 0.215761  [   52/   89]
Per-example loss in batch: 0.199893  [   54/   89]
Per-example loss in batch: 0.300052  [   56/   89]
Per-example loss in batch: 0.271091  [   58/   89]
Per-example loss in batch: 0.280348  [   60/   89]
Per-example loss in batch: 0.217291  [   62/   89]
Per-example loss in batch: 0.252772  [   64/   89]
Per-example loss in batch: 0.238686  [   66/   89]
Per-example loss in batch: 0.324411  [   68/   89]
Per-example loss in batch: 0.333073  [   70/   89]
Per-example loss in batch: 0.287168  [   72/   89]
Per-example loss in batch: 0.251000  [   74/   89]
Per-example loss in batch: 0.225614  [   76/   89]
Per-example loss in batch: 0.292400  [   78/   89]
Per-example loss in batch: 0.344456  [   80/   89]
Per-example loss in batch: 0.207547  [   82/   89]
Per-example loss in batch: 0.293385  [   84/   89]
Per-example loss in batch: 0.289430  [   86/   89]
Per-example loss in batch: 0.205063  [   88/   89]
Per-example loss in batch: 0.576189  [   89/   89]
Train Error: Avg loss: 0.26930936
validation Error: 
 Avg loss: 0.28347021 
 F1: 0.473765 
 Precision: 0.536513 
 Recall: 0.424157
 IoU: 0.310414

test Error: 
 Avg loss: 0.26147263 
 F1: 0.508313 
 Precision: 0.585800 
 Recall: 0.448930
 IoU: 0.340764

We have finished training iteration 191
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_189_.pth
Per-example loss in batch: 0.249644  [    2/   89]
Per-example loss in batch: 0.308805  [    4/   89]
Per-example loss in batch: 0.216361  [    6/   89]
Per-example loss in batch: 0.245347  [    8/   89]
Per-example loss in batch: 0.298752  [   10/   89]
Per-example loss in batch: 0.251180  [   12/   89]
Per-example loss in batch: 0.297429  [   14/   89]
Per-example loss in batch: 0.234241  [   16/   89]
Per-example loss in batch: 0.229811  [   18/   89]
Per-example loss in batch: 0.311550  [   20/   89]
Per-example loss in batch: 0.324273  [   22/   89]
Per-example loss in batch: 0.260662  [   24/   89]
Per-example loss in batch: 0.198803  [   26/   89]
Per-example loss in batch: 0.202915  [   28/   89]
Per-example loss in batch: 0.228414  [   30/   89]
Per-example loss in batch: 0.290115  [   32/   89]
Per-example loss in batch: 0.244042  [   34/   89]
Per-example loss in batch: 0.277194  [   36/   89]
Per-example loss in batch: 0.227972  [   38/   89]
Per-example loss in batch: 0.210478  [   40/   89]
Per-example loss in batch: 0.230462  [   42/   89]
Per-example loss in batch: 0.271935  [   44/   89]
Per-example loss in batch: 0.225522  [   46/   89]
Per-example loss in batch: 0.279200  [   48/   89]
Per-example loss in batch: 0.299866  [   50/   89]
Per-example loss in batch: 0.352103  [   52/   89]
Per-example loss in batch: 0.280160  [   54/   89]
Per-example loss in batch: 0.254863  [   56/   89]
Per-example loss in batch: 0.216064  [   58/   89]
Per-example loss in batch: 0.226995  [   60/   89]
Per-example loss in batch: 0.279096  [   62/   89]
Per-example loss in batch: 0.235123  [   64/   89]
Per-example loss in batch: 0.295072  [   66/   89]
Per-example loss in batch: 0.288276  [   68/   89]
Per-example loss in batch: 0.286917  [   70/   89]
Per-example loss in batch: 0.244795  [   72/   89]
Per-example loss in batch: 0.233883  [   74/   89]
Per-example loss in batch: 0.335371  [   76/   89]
Per-example loss in batch: 0.294759  [   78/   89]
Per-example loss in batch: 0.265551  [   80/   89]
Per-example loss in batch: 0.204714  [   82/   89]
Per-example loss in batch: 0.239149  [   84/   89]
Per-example loss in batch: 0.341134  [   86/   89]
Per-example loss in batch: 0.252833  [   88/   89]
Per-example loss in batch: 0.587988  [   89/   89]
Train Error: Avg loss: 0.26597362
validation Error: 
 Avg loss: 0.28860979 
 F1: 0.473486 
 Precision: 0.531252 
 Recall: 0.427051
 IoU: 0.310175

test Error: 
 Avg loss: 0.26072134 
 F1: 0.510046 
 Precision: 0.581135 
 Recall: 0.454453
 IoU: 0.342323

We have finished training iteration 192
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_190_.pth
Per-example loss in batch: 0.309132  [    2/   89]
Per-example loss in batch: 0.270450  [    4/   89]
Per-example loss in batch: 0.247944  [    6/   89]
Per-example loss in batch: 0.236455  [    8/   89]
Per-example loss in batch: 0.296708  [   10/   89]
Per-example loss in batch: 0.224643  [   12/   89]
Per-example loss in batch: 0.306122  [   14/   89]
Per-example loss in batch: 0.239743  [   16/   89]
Per-example loss in batch: 0.224714  [   18/   89]
Per-example loss in batch: 0.228221  [   20/   89]
Per-example loss in batch: 0.222388  [   22/   89]
Per-example loss in batch: 0.253508  [   24/   89]
Per-example loss in batch: 0.295328  [   26/   89]
Per-example loss in batch: 0.307625  [   28/   89]
Per-example loss in batch: 0.210710  [   30/   89]
Per-example loss in batch: 0.225862  [   32/   89]
Per-example loss in batch: 0.208753  [   34/   89]
Per-example loss in batch: 0.224725  [   36/   89]
Per-example loss in batch: 0.251759  [   38/   89]
Per-example loss in batch: 0.292182  [   40/   89]
Per-example loss in batch: 0.283908  [   42/   89]
Per-example loss in batch: 0.253710  [   44/   89]
Per-example loss in batch: 0.251094  [   46/   89]
Per-example loss in batch: 0.212490  [   48/   89]
Per-example loss in batch: 0.336673  [   50/   89]
Per-example loss in batch: 0.321820  [   52/   89]
Per-example loss in batch: 0.253023  [   54/   89]
Per-example loss in batch: 0.357646  [   56/   89]
Per-example loss in batch: 0.205734  [   58/   89]
Per-example loss in batch: 0.304637  [   60/   89]
Per-example loss in batch: 0.233532  [   62/   89]
Per-example loss in batch: 0.204823  [   64/   89]
Per-example loss in batch: 0.318530  [   66/   89]
Per-example loss in batch: 0.204417  [   68/   89]
Per-example loss in batch: 0.295716  [   70/   89]
Per-example loss in batch: 0.271695  [   72/   89]
Per-example loss in batch: 0.210059  [   74/   89]
Per-example loss in batch: 0.259231  [   76/   89]
Per-example loss in batch: 0.208737  [   78/   89]
Per-example loss in batch: 0.311584  [   80/   89]
Per-example loss in batch: 0.224807  [   82/   89]
Per-example loss in batch: 0.246089  [   84/   89]
Per-example loss in batch: 0.245027  [   86/   89]
Per-example loss in batch: 0.318504  [   88/   89]
Per-example loss in batch: 0.491311  [   89/   89]
Train Error: Avg loss: 0.26193512
validation Error: 
 Avg loss: 0.28214363 
 F1: 0.472841 
 Precision: 0.553381 
 Recall: 0.412767
 IoU: 0.309622

test Error: 
 Avg loss: 0.26262276 
 F1: 0.506529 
 Precision: 0.603437 
 Recall: 0.436439
 IoU: 0.339162

We have finished training iteration 193
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_191_.pth
Per-example loss in batch: 0.282906  [    2/   89]
Per-example loss in batch: 0.246103  [    4/   89]
Per-example loss in batch: 0.272486  [    6/   89]
Per-example loss in batch: 0.340246  [    8/   89]
Per-example loss in batch: 0.259222  [   10/   89]
Per-example loss in batch: 0.268470  [   12/   89]
Per-example loss in batch: 0.218457  [   14/   89]
Per-example loss in batch: 0.248852  [   16/   89]
Per-example loss in batch: 0.304561  [   18/   89]
Per-example loss in batch: 0.228005  [   20/   89]
Per-example loss in batch: 0.206390  [   22/   89]
Per-example loss in batch: 0.213319  [   24/   89]
Per-example loss in batch: 0.208729  [   26/   89]
Per-example loss in batch: 0.199011  [   28/   89]
Per-example loss in batch: 0.342435  [   30/   89]
Per-example loss in batch: 0.279398  [   32/   89]
Per-example loss in batch: 0.228776  [   34/   89]
Per-example loss in batch: 0.361001  [   36/   89]
Per-example loss in batch: 0.201739  [   38/   89]
Per-example loss in batch: 0.300863  [   40/   89]
Per-example loss in batch: 0.288335  [   42/   89]
Per-example loss in batch: 0.312925  [   44/   89]
Per-example loss in batch: 0.271488  [   46/   89]
Per-example loss in batch: 0.286924  [   48/   89]
Per-example loss in batch: 0.362170  [   50/   89]
Per-example loss in batch: 0.242902  [   52/   89]
Per-example loss in batch: 0.299944  [   54/   89]
Per-example loss in batch: 0.273177  [   56/   89]
Per-example loss in batch: 0.279704  [   58/   89]
Per-example loss in batch: 0.248896  [   60/   89]
Per-example loss in batch: 0.250952  [   62/   89]
Per-example loss in batch: 0.256704  [   64/   89]
Per-example loss in batch: 0.213192  [   66/   89]
Per-example loss in batch: 0.268079  [   68/   89]
Per-example loss in batch: 0.241919  [   70/   89]
Per-example loss in batch: 0.312720  [   72/   89]
Per-example loss in batch: 0.239967  [   74/   89]
Per-example loss in batch: 0.212929  [   76/   89]
Per-example loss in batch: 0.300864  [   78/   89]
Per-example loss in batch: 0.325222  [   80/   89]
Per-example loss in batch: 0.255324  [   82/   89]
Per-example loss in batch: 0.230671  [   84/   89]
Per-example loss in batch: 0.278871  [   86/   89]
Per-example loss in batch: 0.225125  [   88/   89]
Per-example loss in batch: 0.607277  [   89/   89]
Train Error: Avg loss: 0.26951941
validation Error: 
 Avg loss: 0.28597775 
 F1: 0.472959 
 Precision: 0.547676 
 Recall: 0.416182
 IoU: 0.309723

test Error: 
 Avg loss: 0.26239161 
 F1: 0.506573 
 Precision: 0.597124 
 Recall: 0.439870
 IoU: 0.339202

We have finished training iteration 194
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_192_.pth
Per-example loss in batch: 0.291513  [    2/   89]
Per-example loss in batch: 0.242007  [    4/   89]
Per-example loss in batch: 0.274366  [    6/   89]
Per-example loss in batch: 0.262085  [    8/   89]
Per-example loss in batch: 0.271629  [   10/   89]
Per-example loss in batch: 0.254141  [   12/   89]
Per-example loss in batch: 0.305456  [   14/   89]
Per-example loss in batch: 0.276559  [   16/   89]
Per-example loss in batch: 0.251245  [   18/   89]
Per-example loss in batch: 0.329095  [   20/   89]
Per-example loss in batch: 0.304128  [   22/   89]
Per-example loss in batch: 0.243531  [   24/   89]
Per-example loss in batch: 0.319678  [   26/   89]
Per-example loss in batch: 0.294502  [   28/   89]
Per-example loss in batch: 0.300375  [   30/   89]
Per-example loss in batch: 0.250750  [   32/   89]
Per-example loss in batch: 0.304937  [   34/   89]
Per-example loss in batch: 0.234673  [   36/   89]
Per-example loss in batch: 0.299409  [   38/   89]
Per-example loss in batch: 0.270196  [   40/   89]
Per-example loss in batch: 0.241815  [   42/   89]
Per-example loss in batch: 0.241822  [   44/   89]
Per-example loss in batch: 0.204178  [   46/   89]
Per-example loss in batch: 0.266842  [   48/   89]
Per-example loss in batch: 0.319621  [   50/   89]
Per-example loss in batch: 0.216716  [   52/   89]
Per-example loss in batch: 0.229568  [   54/   89]
Per-example loss in batch: 0.206878  [   56/   89]
Per-example loss in batch: 0.315280  [   58/   89]
Per-example loss in batch: 0.258116  [   60/   89]
Per-example loss in batch: 0.333382  [   62/   89]
Per-example loss in batch: 0.244661  [   64/   89]
Per-example loss in batch: 0.203022  [   66/   89]
Per-example loss in batch: 0.322703  [   68/   89]
Per-example loss in batch: 0.223274  [   70/   89]
Per-example loss in batch: 0.344315  [   72/   89]
Per-example loss in batch: 0.203483  [   74/   89]
Per-example loss in batch: 0.222263  [   76/   89]
Per-example loss in batch: 0.308081  [   78/   89]
Per-example loss in batch: 0.219676  [   80/   89]
Per-example loss in batch: 0.340555  [   82/   89]
Per-example loss in batch: 0.267377  [   84/   89]
Per-example loss in batch: 0.231740  [   86/   89]
Per-example loss in batch: 0.248966  [   88/   89]
Per-example loss in batch: 0.530119  [   89/   89]
Train Error: Avg loss: 0.27100388
validation Error: 
 Avg loss: 0.28998928 
 F1: 0.472593 
 Precision: 0.560020 
 Recall: 0.408777
 IoU: 0.309408

test Error: 
 Avg loss: 0.26289649 
 F1: 0.505576 
 Precision: 0.610896 
 Recall: 0.431230
 IoU: 0.338308

We have finished training iteration 195
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_193_.pth
Per-example loss in batch: 0.319142  [    2/   89]
Per-example loss in batch: 0.212540  [    4/   89]
Per-example loss in batch: 0.259388  [    6/   89]
Per-example loss in batch: 0.321709  [    8/   89]
Per-example loss in batch: 0.284232  [   10/   89]
Per-example loss in batch: 0.266884  [   12/   89]
Per-example loss in batch: 0.210981  [   14/   89]
Per-example loss in batch: 0.297209  [   16/   89]
Per-example loss in batch: 0.272316  [   18/   89]
Per-example loss in batch: 0.294148  [   20/   89]
Per-example loss in batch: 0.311105  [   22/   89]
Per-example loss in batch: 0.248129  [   24/   89]
Per-example loss in batch: 0.238402  [   26/   89]
Per-example loss in batch: 0.280731  [   28/   89]
Per-example loss in batch: 0.344183  [   30/   89]
Per-example loss in batch: 0.256537  [   32/   89]
Per-example loss in batch: 0.276011  [   34/   89]
Per-example loss in batch: 0.239817  [   36/   89]
Per-example loss in batch: 0.226298  [   38/   89]
Per-example loss in batch: 0.292013  [   40/   89]
Per-example loss in batch: 0.270693  [   42/   89]
Per-example loss in batch: 0.245072  [   44/   89]
Per-example loss in batch: 0.213099  [   46/   89]
Per-example loss in batch: 0.233395  [   48/   89]
Per-example loss in batch: 0.260278  [   50/   89]
Per-example loss in batch: 0.209392  [   52/   89]
Per-example loss in batch: 0.272923  [   54/   89]
Per-example loss in batch: 0.277098  [   56/   89]
Per-example loss in batch: 0.299730  [   58/   89]
Per-example loss in batch: 0.315175  [   60/   89]
Per-example loss in batch: 0.269823  [   62/   89]
Per-example loss in batch: 0.269372  [   64/   89]
Per-example loss in batch: 0.203215  [   66/   89]
Per-example loss in batch: 0.238646  [   68/   89]
Per-example loss in batch: 0.303456  [   70/   89]
Per-example loss in batch: 0.274545  [   72/   89]
Per-example loss in batch: 0.206796  [   74/   89]
Per-example loss in batch: 0.282290  [   76/   89]
Per-example loss in batch: 0.208885  [   78/   89]
Per-example loss in batch: 0.294088  [   80/   89]
Per-example loss in batch: 0.249220  [   82/   89]
Per-example loss in batch: 0.297347  [   84/   89]
Per-example loss in batch: 0.217988  [   86/   89]
Per-example loss in batch: 0.277944  [   88/   89]
Per-example loss in batch: 0.753770  [   89/   89]
Train Error: Avg loss: 0.27009277
validation Error: 
 Avg loss: 0.28492567 
 F1: 0.471725 
 Precision: 0.552501 
 Recall: 0.411555
 IoU: 0.308665

test Error: 
 Avg loss: 0.26284011 
 F1: 0.505793 
 Precision: 0.600832 
 Recall: 0.436714
 IoU: 0.338503

We have finished training iteration 196
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_194_.pth
Per-example loss in batch: 0.319769  [    2/   89]
Per-example loss in batch: 0.253483  [    4/   89]
Per-example loss in batch: 0.273653  [    6/   89]
Per-example loss in batch: 0.220667  [    8/   89]
Per-example loss in batch: 0.239577  [   10/   89]
Per-example loss in batch: 0.227818  [   12/   89]
Per-example loss in batch: 0.242626  [   14/   89]
Per-example loss in batch: 0.261352  [   16/   89]
Per-example loss in batch: 0.314067  [   18/   89]
Per-example loss in batch: 0.222699  [   20/   89]
Per-example loss in batch: 0.216257  [   22/   89]
Per-example loss in batch: 0.306543  [   24/   89]
Per-example loss in batch: 0.239095  [   26/   89]
Per-example loss in batch: 0.251820  [   28/   89]
Per-example loss in batch: 0.342674  [   30/   89]
Per-example loss in batch: 0.229337  [   32/   89]
Per-example loss in batch: 0.271279  [   34/   89]
Per-example loss in batch: 0.267604  [   36/   89]
Per-example loss in batch: 0.221453  [   38/   89]
Per-example loss in batch: 0.218860  [   40/   89]
Per-example loss in batch: 0.219345  [   42/   89]
Per-example loss in batch: 0.293952  [   44/   89]
Per-example loss in batch: 0.289407  [   46/   89]
Per-example loss in batch: 0.258619  [   48/   89]
Per-example loss in batch: 0.238759  [   50/   89]
Per-example loss in batch: 0.273918  [   52/   89]
Per-example loss in batch: 0.271243  [   54/   89]
Per-example loss in batch: 0.224561  [   56/   89]
Per-example loss in batch: 0.343781  [   58/   89]
Per-example loss in batch: 0.238943  [   60/   89]
Per-example loss in batch: 0.293001  [   62/   89]
Per-example loss in batch: 0.267140  [   64/   89]
Per-example loss in batch: 0.291955  [   66/   89]
Per-example loss in batch: 0.252729  [   68/   89]
Per-example loss in batch: 0.247392  [   70/   89]
Per-example loss in batch: 0.351534  [   72/   89]
Per-example loss in batch: 0.322686  [   74/   89]
Per-example loss in batch: 0.212343  [   76/   89]
Per-example loss in batch: 0.229867  [   78/   89]
Per-example loss in batch: 0.362010  [   80/   89]
Per-example loss in batch: 0.302474  [   82/   89]
Per-example loss in batch: 0.224528  [   84/   89]
Per-example loss in batch: 0.233377  [   86/   89]
Per-example loss in batch: 0.268725  [   88/   89]
Per-example loss in batch: 0.511748  [   89/   89]
Train Error: Avg loss: 0.26761338
validation Error: 
 Avg loss: 0.28746625 
 F1: 0.472387 
 Precision: 0.553058 
 Recall: 0.412255
 IoU: 0.309232

test Error: 
 Avg loss: 0.26195817 
 F1: 0.508012 
 Precision: 0.607337 
 Recall: 0.436608
 IoU: 0.340493

We have finished training iteration 197
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_195_.pth
Per-example loss in batch: 0.252370  [    2/   89]
Per-example loss in batch: 0.215396  [    4/   89]
Per-example loss in batch: 0.260338  [    6/   89]
Per-example loss in batch: 0.327217  [    8/   89]
Per-example loss in batch: 0.240364  [   10/   89]
Per-example loss in batch: 0.211646  [   12/   89]
Per-example loss in batch: 0.342632  [   14/   89]
Per-example loss in batch: 0.275167  [   16/   89]
Per-example loss in batch: 0.301498  [   18/   89]
Per-example loss in batch: 0.290626  [   20/   89]
Per-example loss in batch: 0.302654  [   22/   89]
Per-example loss in batch: 0.284831  [   24/   89]
Per-example loss in batch: 0.259393  [   26/   89]
Per-example loss in batch: 0.231643  [   28/   89]
Per-example loss in batch: 0.202899  [   30/   89]
Per-example loss in batch: 0.283687  [   32/   89]
Per-example loss in batch: 0.277642  [   34/   89]
Per-example loss in batch: 0.335858  [   36/   89]
Per-example loss in batch: 0.371036  [   38/   89]
Per-example loss in batch: 0.248144  [   40/   89]
Per-example loss in batch: 0.230643  [   42/   89]
Per-example loss in batch: 0.294926  [   44/   89]
Per-example loss in batch: 0.249940  [   46/   89]
Per-example loss in batch: 0.208816  [   48/   89]
Per-example loss in batch: 0.229887  [   50/   89]
Per-example loss in batch: 0.205667  [   52/   89]
Per-example loss in batch: 0.287879  [   54/   89]
Per-example loss in batch: 0.261433  [   56/   89]
Per-example loss in batch: 0.205540  [   58/   89]
Per-example loss in batch: 0.252490  [   60/   89]
Per-example loss in batch: 0.248660  [   62/   89]
Per-example loss in batch: 0.262750  [   64/   89]
Per-example loss in batch: 0.253076  [   66/   89]
Per-example loss in batch: 0.260516  [   68/   89]
Per-example loss in batch: 0.301443  [   70/   89]
Per-example loss in batch: 0.302650  [   72/   89]
Per-example loss in batch: 0.228179  [   74/   89]
Per-example loss in batch: 0.256448  [   76/   89]
Per-example loss in batch: 0.330277  [   78/   89]
Per-example loss in batch: 0.237660  [   80/   89]
Per-example loss in batch: 0.248838  [   82/   89]
Per-example loss in batch: 0.229195  [   84/   89]
Per-example loss in batch: 0.215741  [   86/   89]
Per-example loss in batch: 0.269806  [   88/   89]
Per-example loss in batch: 0.480809  [   89/   89]
Train Error: Avg loss: 0.26579557
validation Error: 
 Avg loss: 0.28807969 
 F1: 0.473599 
 Precision: 0.526606 
 Recall: 0.430287
 IoU: 0.310272

test Error: 
 Avg loss: 0.26005811 
 F1: 0.511529 
 Precision: 0.580283 
 Recall: 0.457341
 IoU: 0.343660

We have finished training iteration 198
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_196_.pth
Per-example loss in batch: 0.266946  [    2/   89]
Per-example loss in batch: 0.271834  [    4/   89]
Per-example loss in batch: 0.224975  [    6/   89]
Per-example loss in batch: 0.236536  [    8/   89]
Per-example loss in batch: 0.235334  [   10/   89]
Per-example loss in batch: 0.293189  [   12/   89]
Per-example loss in batch: 0.205281  [   14/   89]
Per-example loss in batch: 0.289209  [   16/   89]
Per-example loss in batch: 0.225266  [   18/   89]
Per-example loss in batch: 0.317686  [   20/   89]
Per-example loss in batch: 0.225443  [   22/   89]
Per-example loss in batch: 0.213068  [   24/   89]
Per-example loss in batch: 0.242718  [   26/   89]
Per-example loss in batch: 0.211959  [   28/   89]
Per-example loss in batch: 0.317728  [   30/   89]
Per-example loss in batch: 0.272640  [   32/   89]
Per-example loss in batch: 0.286621  [   34/   89]
Per-example loss in batch: 0.322835  [   36/   89]
Per-example loss in batch: 0.257773  [   38/   89]
Per-example loss in batch: 0.218254  [   40/   89]
Per-example loss in batch: 0.239565  [   42/   89]
Per-example loss in batch: 0.219204  [   44/   89]
Per-example loss in batch: 0.225987  [   46/   89]
Per-example loss in batch: 0.309928  [   48/   89]
Per-example loss in batch: 0.261359  [   50/   89]
Per-example loss in batch: 0.271062  [   52/   89]
Per-example loss in batch: 0.284086  [   54/   89]
Per-example loss in batch: 0.247466  [   56/   89]
Per-example loss in batch: 0.256482  [   58/   89]
Per-example loss in batch: 0.279775  [   60/   89]
Per-example loss in batch: 0.313013  [   62/   89]
Per-example loss in batch: 0.261677  [   64/   89]
Per-example loss in batch: 0.233149  [   66/   89]
Per-example loss in batch: 0.347267  [   68/   89]
Per-example loss in batch: 0.289983  [   70/   89]
Per-example loss in batch: 0.215913  [   72/   89]
Per-example loss in batch: 0.332337  [   74/   89]
Per-example loss in batch: 0.282006  [   76/   89]
Per-example loss in batch: 0.329489  [   78/   89]
Per-example loss in batch: 0.289742  [   80/   89]
Per-example loss in batch: 0.230832  [   82/   89]
Per-example loss in batch: 0.306498  [   84/   89]
Per-example loss in batch: 0.212750  [   86/   89]
Per-example loss in batch: 0.324640  [   88/   89]
Per-example loss in batch: 0.527335  [   89/   89]
Train Error: Avg loss: 0.26883540
validation Error: 
 Avg loss: 0.28414005 
 F1: 0.474027 
 Precision: 0.553965 
 Recall: 0.414249
 IoU: 0.310639

test Error: 
 Avg loss: 0.26152135 
 F1: 0.508695 
 Precision: 0.608238 
 Recall: 0.437152
 IoU: 0.341108

We have finished training iteration 199
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_197_.pth
Per-example loss in batch: 0.278982  [    2/   89]
Per-example loss in batch: 0.373301  [    4/   89]
Per-example loss in batch: 0.226036  [    6/   89]
Per-example loss in batch: 0.358649  [    8/   89]
Per-example loss in batch: 0.223522  [   10/   89]
Per-example loss in batch: 0.201941  [   12/   89]
Per-example loss in batch: 0.265721  [   14/   89]
Per-example loss in batch: 0.211990  [   16/   89]
Per-example loss in batch: 0.225146  [   18/   89]
Per-example loss in batch: 0.207967  [   20/   89]
Per-example loss in batch: 0.348292  [   22/   89]
Per-example loss in batch: 0.207721  [   24/   89]
Per-example loss in batch: 0.273502  [   26/   89]
Per-example loss in batch: 0.258813  [   28/   89]
Per-example loss in batch: 0.257888  [   30/   89]
Per-example loss in batch: 0.200088  [   32/   89]
Per-example loss in batch: 0.320248  [   34/   89]
Per-example loss in batch: 0.363901  [   36/   89]
Per-example loss in batch: 0.212758  [   38/   89]
Per-example loss in batch: 0.200748  [   40/   89]
Per-example loss in batch: 0.270104  [   42/   89]
Per-example loss in batch: 0.252879  [   44/   89]
Per-example loss in batch: 0.253057  [   46/   89]
Per-example loss in batch: 0.286023  [   48/   89]
Per-example loss in batch: 0.220032  [   50/   89]
Per-example loss in batch: 0.277851  [   52/   89]
Per-example loss in batch: 0.274974  [   54/   89]
Per-example loss in batch: 0.263961  [   56/   89]
Per-example loss in batch: 0.258985  [   58/   89]
Per-example loss in batch: 0.311571  [   60/   89]
Per-example loss in batch: 0.294869  [   62/   89]
Per-example loss in batch: 0.312300  [   64/   89]
Per-example loss in batch: 0.301949  [   66/   89]
Per-example loss in batch: 0.231114  [   68/   89]
Per-example loss in batch: 0.271803  [   70/   89]
Per-example loss in batch: 0.315941  [   72/   89]
Per-example loss in batch: 0.236011  [   74/   89]
Per-example loss in batch: 0.236394  [   76/   89]
Per-example loss in batch: 0.233357  [   78/   89]
Per-example loss in batch: 0.229767  [   80/   89]
Per-example loss in batch: 0.224878  [   82/   89]
Per-example loss in batch: 0.316254  [   84/   89]
Per-example loss in batch: 0.294713  [   86/   89]
Per-example loss in batch: 0.294762  [   88/   89]
Per-example loss in batch: 0.652856  [   89/   89]
Train Error: Avg loss: 0.26982457
validation Error: 
 Avg loss: 0.28753512 
 F1: 0.471476 
 Precision: 0.518465 
 Recall: 0.432296
 IoU: 0.308451

test Error: 
 Avg loss: 0.26159237 
 F1: 0.508065 
 Precision: 0.566908 
 Recall: 0.460288
 IoU: 0.340541

We have finished training iteration 200
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_198_.pth
Per-example loss in batch: 0.224577  [    2/   89]
Per-example loss in batch: 0.265228  [    4/   89]
Per-example loss in batch: 0.280549  [    6/   89]
Per-example loss in batch: 0.247536  [    8/   89]
Per-example loss in batch: 0.231493  [   10/   89]
Per-example loss in batch: 0.264909  [   12/   89]
Per-example loss in batch: 0.211909  [   14/   89]
Per-example loss in batch: 0.322093  [   16/   89]
Per-example loss in batch: 0.258138  [   18/   89]
Per-example loss in batch: 0.205396  [   20/   89]
Per-example loss in batch: 0.310227  [   22/   89]
Per-example loss in batch: 0.335242  [   24/   89]
Per-example loss in batch: 0.282442  [   26/   89]
Per-example loss in batch: 0.275642  [   28/   89]
Per-example loss in batch: 0.203769  [   30/   89]
Per-example loss in batch: 0.260515  [   32/   89]
Per-example loss in batch: 0.331759  [   34/   89]
Per-example loss in batch: 0.267675  [   36/   89]
Per-example loss in batch: 0.251789  [   38/   89]
Per-example loss in batch: 0.262524  [   40/   89]
Per-example loss in batch: 0.200857  [   42/   89]
Per-example loss in batch: 0.357272  [   44/   89]
Per-example loss in batch: 0.207686  [   46/   89]
Per-example loss in batch: 0.219596  [   48/   89]
Per-example loss in batch: 0.302849  [   50/   89]
Per-example loss in batch: 0.343622  [   52/   89]
Per-example loss in batch: 0.324578  [   54/   89]
Per-example loss in batch: 0.268513  [   56/   89]
Per-example loss in batch: 0.322820  [   58/   89]
Per-example loss in batch: 0.250425  [   60/   89]
Per-example loss in batch: 0.356630  [   62/   89]
Per-example loss in batch: 0.401414  [   64/   89]
Per-example loss in batch: 0.290044  [   66/   89]
Per-example loss in batch: 0.295367  [   68/   89]
Per-example loss in batch: 0.229402  [   70/   89]
Per-example loss in batch: 0.304270  [   72/   89]
Per-example loss in batch: 0.226382  [   74/   89]
Per-example loss in batch: 0.273427  [   76/   89]
Per-example loss in batch: 0.253309  [   78/   89]
Per-example loss in batch: 0.225195  [   80/   89]
Per-example loss in batch: 0.269595  [   82/   89]
Per-example loss in batch: 0.278727  [   84/   89]
Per-example loss in batch: 0.211409  [   86/   89]
Per-example loss in batch: 0.245445  [   88/   89]
Per-example loss in batch: 0.404607  [   89/   89]
Train Error: Avg loss: 0.27313600
validation Error: 
 Avg loss: 0.28384017 
 F1: 0.472342 
 Precision: 0.520676 
 Recall: 0.432220
 IoU: 0.309194

test Error: 
 Avg loss: 0.26123559 
 F1: 0.509239 
 Precision: 0.569431 
 Recall: 0.460556
 IoU: 0.341597

We have finished training iteration 201
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_199_.pth
Per-example loss in batch: 0.259332  [    2/   89]
Per-example loss in batch: 0.298104  [    4/   89]
Per-example loss in batch: 0.295514  [    6/   89]
Per-example loss in batch: 0.306927  [    8/   89]
Per-example loss in batch: 0.262378  [   10/   89]
Per-example loss in batch: 0.297796  [   12/   89]
Per-example loss in batch: 0.265729  [   14/   89]
Per-example loss in batch: 0.237096  [   16/   89]
Per-example loss in batch: 0.331723  [   18/   89]
Per-example loss in batch: 0.228107  [   20/   89]
Per-example loss in batch: 0.304752  [   22/   89]
Per-example loss in batch: 0.240743  [   24/   89]
Per-example loss in batch: 0.332807  [   26/   89]
Per-example loss in batch: 0.288848  [   28/   89]
Per-example loss in batch: 0.239509  [   30/   89]
Per-example loss in batch: 0.207479  [   32/   89]
Per-example loss in batch: 0.265604  [   34/   89]
Per-example loss in batch: 0.256191  [   36/   89]
Per-example loss in batch: 0.338242  [   38/   89]
Per-example loss in batch: 0.247223  [   40/   89]
Per-example loss in batch: 0.217504  [   42/   89]
Per-example loss in batch: 0.260600  [   44/   89]
Per-example loss in batch: 0.223335  [   46/   89]
Per-example loss in batch: 0.320976  [   48/   89]
Per-example loss in batch: 0.260558  [   50/   89]
Per-example loss in batch: 0.254003  [   52/   89]
Per-example loss in batch: 0.202327  [   54/   89]
Per-example loss in batch: 0.323128  [   56/   89]
Per-example loss in batch: 0.280306  [   58/   89]
Per-example loss in batch: 0.225910  [   60/   89]
Per-example loss in batch: 0.293633  [   62/   89]
Per-example loss in batch: 0.231194  [   64/   89]
Per-example loss in batch: 0.266035  [   66/   89]
Per-example loss in batch: 0.228501  [   68/   89]
Per-example loss in batch: 0.271517  [   70/   89]
Per-example loss in batch: 0.214564  [   72/   89]
Per-example loss in batch: 0.265254  [   74/   89]
Per-example loss in batch: 0.333422  [   76/   89]
Per-example loss in batch: 0.267753  [   78/   89]
Per-example loss in batch: 0.288480  [   80/   89]
Per-example loss in batch: 0.233054  [   82/   89]
Per-example loss in batch: 0.239029  [   84/   89]
Per-example loss in batch: 0.219787  [   86/   89]
Per-example loss in batch: 0.263826  [   88/   89]
Per-example loss in batch: 0.426167  [   89/   89]
Train Error: Avg loss: 0.26745809
validation Error: 
 Avg loss: 0.27975193 
 F1: 0.474492 
 Precision: 0.548676 
 Recall: 0.417979
 IoU: 0.311039

test Error: 
 Avg loss: 0.26081958 
 F1: 0.510124 
 Precision: 0.603459 
 Recall: 0.441793
 IoU: 0.342394

We have finished training iteration 202
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_200_.pth
Per-example loss in batch: 0.321306  [    2/   89]
Per-example loss in batch: 0.229011  [    4/   89]
Per-example loss in batch: 0.247315  [    6/   89]
Per-example loss in batch: 0.255715  [    8/   89]
Per-example loss in batch: 0.251568  [   10/   89]
Per-example loss in batch: 0.231129  [   12/   89]
Per-example loss in batch: 0.284696  [   14/   89]
Per-example loss in batch: 0.301382  [   16/   89]
Per-example loss in batch: 0.275022  [   18/   89]
Per-example loss in batch: 0.215036  [   20/   89]
Per-example loss in batch: 0.260339  [   22/   89]
Per-example loss in batch: 0.256768  [   24/   89]
Per-example loss in batch: 0.252719  [   26/   89]
Per-example loss in batch: 0.223028  [   28/   89]
Per-example loss in batch: 0.283256  [   30/   89]
Per-example loss in batch: 0.335186  [   32/   89]
Per-example loss in batch: 0.213026  [   34/   89]
Per-example loss in batch: 0.211120  [   36/   89]
Per-example loss in batch: 0.282461  [   38/   89]
Per-example loss in batch: 0.334810  [   40/   89]
Per-example loss in batch: 0.232779  [   42/   89]
Per-example loss in batch: 0.296983  [   44/   89]
Per-example loss in batch: 0.346450  [   46/   89]
Per-example loss in batch: 0.226932  [   48/   89]
Per-example loss in batch: 0.298322  [   50/   89]
Per-example loss in batch: 0.242643  [   52/   89]
Per-example loss in batch: 0.309648  [   54/   89]
Per-example loss in batch: 0.318443  [   56/   89]
Per-example loss in batch: 0.278314  [   58/   89]
Per-example loss in batch: 0.233442  [   60/   89]
Per-example loss in batch: 0.254248  [   62/   89]
Per-example loss in batch: 0.210203  [   64/   89]
Per-example loss in batch: 0.360100  [   66/   89]
Per-example loss in batch: 0.241647  [   68/   89]
Per-example loss in batch: 0.229209  [   70/   89]
Per-example loss in batch: 0.274789  [   72/   89]
Per-example loss in batch: 0.332969  [   74/   89]
Per-example loss in batch: 0.244938  [   76/   89]
Per-example loss in batch: 0.283204  [   78/   89]
Per-example loss in batch: 0.215994  [   80/   89]
Per-example loss in batch: 0.250437  [   82/   89]
Per-example loss in batch: 0.230125  [   84/   89]
Per-example loss in batch: 0.297514  [   86/   89]
Per-example loss in batch: 0.283226  [   88/   89]
Per-example loss in batch: 0.628173  [   89/   89]
Train Error: Avg loss: 0.27127056
validation Error: 
 Avg loss: 0.28292803 
 F1: 0.472085 
 Precision: 0.554714 
 Recall: 0.410880
 IoU: 0.308973

test Error: 
 Avg loss: 0.26233496 
 F1: 0.507300 
 Precision: 0.606485 
 Recall: 0.435996
 IoU: 0.339854

We have finished training iteration 203
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_201_.pth
Per-example loss in batch: 0.202497  [    2/   89]
Per-example loss in batch: 0.340525  [    4/   89]
Per-example loss in batch: 0.234645  [    6/   89]
Per-example loss in batch: 0.256319  [    8/   89]
Per-example loss in batch: 0.287139  [   10/   89]
Per-example loss in batch: 0.296347  [   12/   89]
Per-example loss in batch: 0.272054  [   14/   89]
Per-example loss in batch: 0.288009  [   16/   89]
Per-example loss in batch: 0.203129  [   18/   89]
Per-example loss in batch: 0.275313  [   20/   89]
Per-example loss in batch: 0.229682  [   22/   89]
Per-example loss in batch: 0.242217  [   24/   89]
Per-example loss in batch: 0.288065  [   26/   89]
Per-example loss in batch: 0.214487  [   28/   89]
Per-example loss in batch: 0.248063  [   30/   89]
Per-example loss in batch: 0.331727  [   32/   89]
Per-example loss in batch: 0.244028  [   34/   89]
Per-example loss in batch: 0.214623  [   36/   89]
Per-example loss in batch: 0.312960  [   38/   89]
Per-example loss in batch: 0.217090  [   40/   89]
Per-example loss in batch: 0.258494  [   42/   89]
Per-example loss in batch: 0.287939  [   44/   89]
Per-example loss in batch: 0.303999  [   46/   89]
Per-example loss in batch: 0.238063  [   48/   89]
Per-example loss in batch: 0.250909  [   50/   89]
Per-example loss in batch: 0.265231  [   52/   89]
Per-example loss in batch: 0.248114  [   54/   89]
Per-example loss in batch: 0.278433  [   56/   89]
Per-example loss in batch: 0.292688  [   58/   89]
Per-example loss in batch: 0.256166  [   60/   89]
Per-example loss in batch: 0.275421  [   62/   89]
Per-example loss in batch: 0.242782  [   64/   89]
Per-example loss in batch: 0.231339  [   66/   89]
Per-example loss in batch: 0.320365  [   68/   89]
Per-example loss in batch: 0.250482  [   70/   89]
Per-example loss in batch: 0.227334  [   72/   89]
Per-example loss in batch: 0.269620  [   74/   89]
Per-example loss in batch: 0.246915  [   76/   89]
Per-example loss in batch: 0.204943  [   78/   89]
Per-example loss in batch: 0.278874  [   80/   89]
Per-example loss in batch: 0.213866  [   82/   89]
Per-example loss in batch: 0.238560  [   84/   89]
Per-example loss in batch: 0.304384  [   86/   89]
Per-example loss in batch: 0.297291  [   88/   89]
Per-example loss in batch: 0.642423  [   89/   89]
Train Error: Avg loss: 0.26522109
validation Error: 
 Avg loss: 0.28659348 
 F1: 0.474164 
 Precision: 0.554733 
 Recall: 0.414031
 IoU: 0.310757

test Error: 
 Avg loss: 0.26136666 
 F1: 0.508826 
 Precision: 0.609152 
 Recall: 0.436874
 IoU: 0.341225

We have finished training iteration 204
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_166_.pth
Per-example loss in batch: 0.276378  [    2/   89]
Per-example loss in batch: 0.244784  [    4/   89]
Per-example loss in batch: 0.290987  [    6/   89]
Per-example loss in batch: 0.272594  [    8/   89]
Per-example loss in batch: 0.319368  [   10/   89]
Per-example loss in batch: 0.233217  [   12/   89]
Per-example loss in batch: 0.201995  [   14/   89]
Per-example loss in batch: 0.332834  [   16/   89]
Per-example loss in batch: 0.241639  [   18/   89]
Per-example loss in batch: 0.272354  [   20/   89]
Per-example loss in batch: 0.293032  [   22/   89]
Per-example loss in batch: 0.320542  [   24/   89]
Per-example loss in batch: 0.216324  [   26/   89]
Per-example loss in batch: 0.220941  [   28/   89]
Per-example loss in batch: 0.289055  [   30/   89]
Per-example loss in batch: 0.246612  [   32/   89]
Per-example loss in batch: 0.275807  [   34/   89]
Per-example loss in batch: 0.233808  [   36/   89]
Per-example loss in batch: 0.213017  [   38/   89]
Per-example loss in batch: 0.206393  [   40/   89]
Per-example loss in batch: 0.257181  [   42/   89]
Per-example loss in batch: 0.292921  [   44/   89]
Per-example loss in batch: 0.237170  [   46/   89]
Per-example loss in batch: 0.288280  [   48/   89]
Per-example loss in batch: 0.316447  [   50/   89]
Per-example loss in batch: 0.297590  [   52/   89]
Per-example loss in batch: 0.293402  [   54/   89]
Per-example loss in batch: 0.213595  [   56/   89]
Per-example loss in batch: 0.226040  [   58/   89]
Per-example loss in batch: 0.211436  [   60/   89]
Per-example loss in batch: 0.273740  [   62/   89]
Per-example loss in batch: 0.241579  [   64/   89]
Per-example loss in batch: 0.274046  [   66/   89]
Per-example loss in batch: 0.318825  [   68/   89]
Per-example loss in batch: 0.288356  [   70/   89]
Per-example loss in batch: 0.232927  [   72/   89]
Per-example loss in batch: 0.231140  [   74/   89]
Per-example loss in batch: 0.282887  [   76/   89]
Per-example loss in batch: 0.292024  [   78/   89]
Per-example loss in batch: 0.223798  [   80/   89]
Per-example loss in batch: 0.305532  [   82/   89]
Per-example loss in batch: 0.295369  [   84/   89]
Per-example loss in batch: 0.233201  [   86/   89]
Per-example loss in batch: 0.257645  [   88/   89]
Per-example loss in batch: 0.495598  [   89/   89]
Train Error: Avg loss: 0.26594633
validation Error: 
 Avg loss: 0.27841241 
 F1: 0.473518 
 Precision: 0.545130 
 Recall: 0.418536
 IoU: 0.310202

test Error: 
 Avg loss: 0.26169054 
 F1: 0.507878 
 Precision: 0.596359 
 Recall: 0.442260
 IoU: 0.340373

We have finished training iteration 205
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_203_.pth
Per-example loss in batch: 0.323806  [    2/   89]
Per-example loss in batch: 0.210226  [    4/   89]
Per-example loss in batch: 0.221403  [    6/   89]
Per-example loss in batch: 0.318490  [    8/   89]
Per-example loss in batch: 0.237694  [   10/   89]
Per-example loss in batch: 0.252194  [   12/   89]
Per-example loss in batch: 0.214472  [   14/   89]
Per-example loss in batch: 0.225653  [   16/   89]
Per-example loss in batch: 0.340267  [   18/   89]
Per-example loss in batch: 0.205365  [   20/   89]
Per-example loss in batch: 0.231204  [   22/   89]
Per-example loss in batch: 0.334509  [   24/   89]
Per-example loss in batch: 0.213593  [   26/   89]
Per-example loss in batch: 0.226044  [   28/   89]
Per-example loss in batch: 0.336951  [   30/   89]
Per-example loss in batch: 0.267935  [   32/   89]
Per-example loss in batch: 0.253181  [   34/   89]
Per-example loss in batch: 0.283254  [   36/   89]
Per-example loss in batch: 0.206439  [   38/   89]
Per-example loss in batch: 0.262226  [   40/   89]
Per-example loss in batch: 0.276064  [   42/   89]
Per-example loss in batch: 0.307814  [   44/   89]
Per-example loss in batch: 0.214534  [   46/   89]
Per-example loss in batch: 0.235491  [   48/   89]
Per-example loss in batch: 0.256835  [   50/   89]
Per-example loss in batch: 0.295839  [   52/   89]
Per-example loss in batch: 0.274370  [   54/   89]
Per-example loss in batch: 0.316326  [   56/   89]
Per-example loss in batch: 0.328265  [   58/   89]
Per-example loss in batch: 0.261047  [   60/   89]
Per-example loss in batch: 0.247987  [   62/   89]
Per-example loss in batch: 0.271397  [   64/   89]
Per-example loss in batch: 0.214810  [   66/   89]
Per-example loss in batch: 0.288225  [   68/   89]
Per-example loss in batch: 0.280882  [   70/   89]
Per-example loss in batch: 0.207718  [   72/   89]
Per-example loss in batch: 0.229733  [   74/   89]
Per-example loss in batch: 0.213609  [   76/   89]
Per-example loss in batch: 0.245985  [   78/   89]
Per-example loss in batch: 0.243890  [   80/   89]
Per-example loss in batch: 0.260363  [   82/   89]
Per-example loss in batch: 0.309561  [   84/   89]
Per-example loss in batch: 0.220246  [   86/   89]
Per-example loss in batch: 0.299350  [   88/   89]
Per-example loss in batch: 0.636892  [   89/   89]
Train Error: Avg loss: 0.26480206
validation Error: 
 Avg loss: 0.28034100 
 F1: 0.474615 
 Precision: 0.555147 
 Recall: 0.414488
 IoU: 0.311145

test Error: 
 Avg loss: 0.26148431 
 F1: 0.508463 
 Precision: 0.607600 
 Recall: 0.437138
 IoU: 0.340898

We have finished training iteration 206
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_204_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.208181  [    2/   89]
Per-example loss in batch: 0.303817  [    4/   89]
Per-example loss in batch: 0.250261  [    6/   89]
Per-example loss in batch: 0.241612  [    8/   89]
Per-example loss in batch: 0.216544  [   10/   89]
Per-example loss in batch: 0.255068  [   12/   89]
Per-example loss in batch: 0.291187  [   14/   89]
Per-example loss in batch: 0.309786  [   16/   89]
Per-example loss in batch: 0.268638  [   18/   89]
Per-example loss in batch: 0.265506  [   20/   89]
Per-example loss in batch: 0.295521  [   22/   89]
Per-example loss in batch: 0.236435  [   24/   89]
Per-example loss in batch: 0.316747  [   26/   89]
Per-example loss in batch: 0.319646  [   28/   89]
Per-example loss in batch: 0.228974  [   30/   89]
Per-example loss in batch: 0.207774  [   32/   89]
Per-example loss in batch: 0.260472  [   34/   89]
Per-example loss in batch: 0.335941  [   36/   89]
Per-example loss in batch: 0.322045  [   38/   89]
Per-example loss in batch: 0.302799  [   40/   89]
Per-example loss in batch: 0.209213  [   42/   89]
Per-example loss in batch: 0.255379  [   44/   89]
Per-example loss in batch: 0.213987  [   46/   89]
Per-example loss in batch: 0.343075  [   48/   89]
Per-example loss in batch: 0.228724  [   50/   89]
Per-example loss in batch: 0.207200  [   52/   89]
Per-example loss in batch: 0.202683  [   54/   89]
Per-example loss in batch: 0.229105  [   56/   89]
Per-example loss in batch: 0.327089  [   58/   89]
Per-example loss in batch: 0.288537  [   60/   89]
Per-example loss in batch: 0.223580  [   62/   89]
Per-example loss in batch: 0.346937  [   64/   89]
Per-example loss in batch: 0.201761  [   66/   89]
Per-example loss in batch: 0.278264  [   68/   89]
Per-example loss in batch: 0.227129  [   70/   89]
Per-example loss in batch: 0.311813  [   72/   89]
Per-example loss in batch: 0.317306  [   74/   89]
Per-example loss in batch: 0.259381  [   76/   89]
Per-example loss in batch: 0.202597  [   78/   89]
Per-example loss in batch: 0.313632  [   80/   89]
Per-example loss in batch: 0.272008  [   82/   89]
Per-example loss in batch: 0.332027  [   84/   89]
Per-example loss in batch: 0.295789  [   86/   89]
Per-example loss in batch: 0.228108  [   88/   89]
Per-example loss in batch: 0.655241  [   89/   89]
Train Error: Avg loss: 0.27145841
validation Error: 
 Avg loss: 0.28307096 
 F1: 0.472430 
 Precision: 0.561763 
 Recall: 0.407611
 IoU: 0.309269

test Error: 
 Avg loss: 0.26260414 
 F1: 0.506358 
 Precision: 0.612011 
 Recall: 0.431814
 IoU: 0.339009

We have finished training iteration 207
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_202_.pth
Per-example loss in batch: 0.216400  [    2/   89]
Per-example loss in batch: 0.259472  [    4/   89]
Per-example loss in batch: 0.254782  [    6/   89]
Per-example loss in batch: 0.236539  [    8/   89]
Per-example loss in batch: 0.249767  [   10/   89]
Per-example loss in batch: 0.317270  [   12/   89]
Per-example loss in batch: 0.249901  [   14/   89]
Per-example loss in batch: 0.331806  [   16/   89]
Per-example loss in batch: 0.261547  [   18/   89]
Per-example loss in batch: 0.313468  [   20/   89]
Per-example loss in batch: 0.274435  [   22/   89]
Per-example loss in batch: 0.305167  [   24/   89]
Per-example loss in batch: 0.210212  [   26/   89]
Per-example loss in batch: 0.256673  [   28/   89]
Per-example loss in batch: 0.351239  [   30/   89]
Per-example loss in batch: 0.261527  [   32/   89]
Per-example loss in batch: 0.232571  [   34/   89]
Per-example loss in batch: 0.207582  [   36/   89]
Per-example loss in batch: 0.258393  [   38/   89]
Per-example loss in batch: 0.230716  [   40/   89]
Per-example loss in batch: 0.348601  [   42/   89]
Per-example loss in batch: 0.318352  [   44/   89]
Per-example loss in batch: 0.239815  [   46/   89]
Per-example loss in batch: 0.292317  [   48/   89]
Per-example loss in batch: 0.251654  [   50/   89]
Per-example loss in batch: 0.212906  [   52/   89]
Per-example loss in batch: 0.276275  [   54/   89]
Per-example loss in batch: 0.268533  [   56/   89]
Per-example loss in batch: 0.254327  [   58/   89]
Per-example loss in batch: 0.264337  [   60/   89]
Per-example loss in batch: 0.250201  [   62/   89]
Per-example loss in batch: 0.355727  [   64/   89]
Per-example loss in batch: 0.220350  [   66/   89]
Per-example loss in batch: 0.306637  [   68/   89]
Per-example loss in batch: 0.248422  [   70/   89]
Per-example loss in batch: 0.226457  [   72/   89]
Per-example loss in batch: 0.296422  [   74/   89]
Per-example loss in batch: 0.238693  [   76/   89]
Per-example loss in batch: 0.250615  [   78/   89]
Per-example loss in batch: 0.209855  [   80/   89]
Per-example loss in batch: 0.331172  [   82/   89]
Per-example loss in batch: 0.252205  [   84/   89]
Per-example loss in batch: 0.226988  [   86/   89]
Per-example loss in batch: 0.220133  [   88/   89]
Per-example loss in batch: 0.571043  [   89/   89]
Train Error: Avg loss: 0.26799962
validation Error: 
 Avg loss: 0.28733359 
 F1: 0.473484 
 Precision: 0.521686 
 Recall: 0.433435
 IoU: 0.310173

test Error: 
 Avg loss: 0.26026433 
 F1: 0.510929 
 Precision: 0.574114 
 Recall: 0.460273
 IoU: 0.343119

We have finished training iteration 208
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_206_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.311430  [    2/   89]
Per-example loss in batch: 0.197077  [    4/   89]
Per-example loss in batch: 0.286734  [    6/   89]
Per-example loss in batch: 0.253484  [    8/   89]
Per-example loss in batch: 0.266797  [   10/   89]
Per-example loss in batch: 0.237439  [   12/   89]
Per-example loss in batch: 0.238958  [   14/   89]
Per-example loss in batch: 0.230610  [   16/   89]
Per-example loss in batch: 0.219956  [   18/   89]
Per-example loss in batch: 0.228601  [   20/   89]
Per-example loss in batch: 0.286322  [   22/   89]
Per-example loss in batch: 0.280028  [   24/   89]
Per-example loss in batch: 0.212122  [   26/   89]
Per-example loss in batch: 0.335566  [   28/   89]
Per-example loss in batch: 0.251806  [   30/   89]
Per-example loss in batch: 0.216422  [   32/   89]
Per-example loss in batch: 0.252056  [   34/   89]
Per-example loss in batch: 0.294442  [   36/   89]
Per-example loss in batch: 0.234091  [   38/   89]
Per-example loss in batch: 0.317974  [   40/   89]
Per-example loss in batch: 0.327920  [   42/   89]
Per-example loss in batch: 0.261842  [   44/   89]
Per-example loss in batch: 0.300352  [   46/   89]
Per-example loss in batch: 0.247886  [   48/   89]
Per-example loss in batch: 0.245341  [   50/   89]
Per-example loss in batch: 0.254026  [   52/   89]
Per-example loss in batch: 0.215798  [   54/   89]
Per-example loss in batch: 0.289492  [   56/   89]
Per-example loss in batch: 0.307417  [   58/   89]
Per-example loss in batch: 0.230139  [   60/   89]
Per-example loss in batch: 0.240964  [   62/   89]
Per-example loss in batch: 0.328527  [   64/   89]
Per-example loss in batch: 0.312522  [   66/   89]
Per-example loss in batch: 0.230443  [   68/   89]
Per-example loss in batch: 0.270985  [   70/   89]
Per-example loss in batch: 0.240369  [   72/   89]
Per-example loss in batch: 0.243104  [   74/   89]
Per-example loss in batch: 0.289448  [   76/   89]
Per-example loss in batch: 0.286311  [   78/   89]
Per-example loss in batch: 0.222087  [   80/   89]
Per-example loss in batch: 0.244098  [   82/   89]
Per-example loss in batch: 0.199649  [   84/   89]
Per-example loss in batch: 0.319367  [   86/   89]
Per-example loss in batch: 0.240711  [   88/   89]
Per-example loss in batch: 0.528026  [   89/   89]
Train Error: Avg loss: 0.26437591
validation Error: 
 Avg loss: 0.28119557 
 F1: 0.474599 
 Precision: 0.533188 
 Recall: 0.427611
 IoU: 0.311130

test Error: 
 Avg loss: 0.26028254 
 F1: 0.511040 
 Precision: 0.585923 
 Recall: 0.453128
 IoU: 0.343219

We have finished training iteration 209
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_207_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.341854  [    2/   89]
Per-example loss in batch: 0.294378  [    4/   89]
Per-example loss in batch: 0.236770  [    6/   89]
Per-example loss in batch: 0.277855  [    8/   89]
Per-example loss in batch: 0.312487  [   10/   89]
Per-example loss in batch: 0.209868  [   12/   89]
Per-example loss in batch: 0.238265  [   14/   89]
Per-example loss in batch: 0.221605  [   16/   89]
Per-example loss in batch: 0.287008  [   18/   89]
Per-example loss in batch: 0.245615  [   20/   89]
Per-example loss in batch: 0.238896  [   22/   89]
Per-example loss in batch: 0.255378  [   24/   89]
Per-example loss in batch: 0.279889  [   26/   89]
Per-example loss in batch: 0.283911  [   28/   89]
Per-example loss in batch: 0.235385  [   30/   89]
Per-example loss in batch: 0.258454  [   32/   89]
Per-example loss in batch: 0.254491  [   34/   89]
Per-example loss in batch: 0.297639  [   36/   89]
Per-example loss in batch: 0.246354  [   38/   89]
Per-example loss in batch: 0.227288  [   40/   89]
Per-example loss in batch: 0.275532  [   42/   89]
Per-example loss in batch: 0.275151  [   44/   89]
Per-example loss in batch: 0.321751  [   46/   89]
Per-example loss in batch: 0.218388  [   48/   89]
Per-example loss in batch: 0.268997  [   50/   89]
Per-example loss in batch: 0.246966  [   52/   89]
Per-example loss in batch: 0.198187  [   54/   89]
Per-example loss in batch: 0.238493  [   56/   89]
Per-example loss in batch: 0.317881  [   58/   89]
Per-example loss in batch: 0.229564  [   60/   89]
Per-example loss in batch: 0.334808  [   62/   89]
Per-example loss in batch: 0.230062  [   64/   89]
Per-example loss in batch: 0.279930  [   66/   89]
Per-example loss in batch: 0.238607  [   68/   89]
Per-example loss in batch: 0.293661  [   70/   89]
Per-example loss in batch: 0.295753  [   72/   89]
Per-example loss in batch: 0.217551  [   74/   89]
Per-example loss in batch: 0.252022  [   76/   89]
Per-example loss in batch: 0.238386  [   78/   89]
Per-example loss in batch: 0.365694  [   80/   89]
Per-example loss in batch: 0.314544  [   82/   89]
Per-example loss in batch: 0.249250  [   84/   89]
Per-example loss in batch: 0.252438  [   86/   89]
Per-example loss in batch: 0.209148  [   88/   89]
Per-example loss in batch: 0.654307  [   89/   89]
Train Error: Avg loss: 0.26816426
validation Error: 
 Avg loss: 0.28252554 
 F1: 0.474507 
 Precision: 0.544290 
 Recall: 0.420583
 IoU: 0.311051

test Error: 
 Avg loss: 0.26076309 
 F1: 0.509997 
 Precision: 0.597399 
 Recall: 0.444906
 IoU: 0.342279

We have finished training iteration 210
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_208_.pth
Per-example loss in batch: 0.221821  [    2/   89]
Per-example loss in batch: 0.281503  [    4/   89]
Per-example loss in batch: 0.358463  [    6/   89]
Per-example loss in batch: 0.310286  [    8/   89]
Per-example loss in batch: 0.220916  [   10/   89]
Per-example loss in batch: 0.215725  [   12/   89]
Per-example loss in batch: 0.251538  [   14/   89]
Per-example loss in batch: 0.317090  [   16/   89]
Per-example loss in batch: 0.237585  [   18/   89]
Per-example loss in batch: 0.241058  [   20/   89]
Per-example loss in batch: 0.218492  [   22/   89]
Per-example loss in batch: 0.251388  [   24/   89]
Per-example loss in batch: 0.246851  [   26/   89]
Per-example loss in batch: 0.314927  [   28/   89]
Per-example loss in batch: 0.279499  [   30/   89]
Per-example loss in batch: 0.264838  [   32/   89]
Per-example loss in batch: 0.271447  [   34/   89]
Per-example loss in batch: 0.213635  [   36/   89]
Per-example loss in batch: 0.298118  [   38/   89]
Per-example loss in batch: 0.318062  [   40/   89]
Per-example loss in batch: 0.278994  [   42/   89]
Per-example loss in batch: 0.300800  [   44/   89]
Per-example loss in batch: 0.262428  [   46/   89]
Per-example loss in batch: 0.306196  [   48/   89]
Per-example loss in batch: 0.384041  [   50/   89]
Per-example loss in batch: 0.279481  [   52/   89]
Per-example loss in batch: 0.200504  [   54/   89]
Per-example loss in batch: 0.239671  [   56/   89]
Per-example loss in batch: 0.291547  [   58/   89]
Per-example loss in batch: 0.233866  [   60/   89]
Per-example loss in batch: 0.217720  [   62/   89]
Per-example loss in batch: 0.218913  [   64/   89]
Per-example loss in batch: 0.224049  [   66/   89]
Per-example loss in batch: 0.267056  [   68/   89]
Per-example loss in batch: 0.268811  [   70/   89]
Per-example loss in batch: 0.225997  [   72/   89]
Per-example loss in batch: 0.199887  [   74/   89]
Per-example loss in batch: 0.236693  [   76/   89]
Per-example loss in batch: 0.300644  [   78/   89]
Per-example loss in batch: 0.318564  [   80/   89]
Per-example loss in batch: 0.327462  [   82/   89]
Per-example loss in batch: 0.235517  [   84/   89]
Per-example loss in batch: 0.253573  [   86/   89]
Per-example loss in batch: 0.271395  [   88/   89]
Per-example loss in batch: 0.402290  [   89/   89]
Train Error: Avg loss: 0.26692578
validation Error: 
 Avg loss: 0.28278662 
 F1: 0.473444 
 Precision: 0.560921 
 Recall: 0.409571
 IoU: 0.310139

test Error: 
 Avg loss: 0.26284810 
 F1: 0.505689 
 Precision: 0.612728 
 Recall: 0.430486
 IoU: 0.338409

We have finished training iteration 211
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_209_.pth
Per-example loss in batch: 0.211295  [    2/   89]
Per-example loss in batch: 0.336328  [    4/   89]
Per-example loss in batch: 0.205255  [    6/   89]
Per-example loss in batch: 0.224922  [    8/   89]
Per-example loss in batch: 0.307168  [   10/   89]
Per-example loss in batch: 0.210726  [   12/   89]
Per-example loss in batch: 0.248732  [   14/   89]
Per-example loss in batch: 0.248104  [   16/   89]
Per-example loss in batch: 0.216751  [   18/   89]
Per-example loss in batch: 0.228801  [   20/   89]
Per-example loss in batch: 0.317678  [   22/   89]
Per-example loss in batch: 0.236483  [   24/   89]
Per-example loss in batch: 0.267160  [   26/   89]
Per-example loss in batch: 0.315175  [   28/   89]
Per-example loss in batch: 0.318348  [   30/   89]
Per-example loss in batch: 0.353261  [   32/   89]
Per-example loss in batch: 0.295268  [   34/   89]
Per-example loss in batch: 0.297169  [   36/   89]
Per-example loss in batch: 0.252438  [   38/   89]
Per-example loss in batch: 0.275329  [   40/   89]
Per-example loss in batch: 0.285721  [   42/   89]
Per-example loss in batch: 0.222695  [   44/   89]
Per-example loss in batch: 0.230969  [   46/   89]
Per-example loss in batch: 0.272487  [   48/   89]
Per-example loss in batch: 0.221030  [   50/   89]
Per-example loss in batch: 0.222827  [   52/   89]
Per-example loss in batch: 0.302328  [   54/   89]
Per-example loss in batch: 0.253431  [   56/   89]
Per-example loss in batch: 0.234963  [   58/   89]
Per-example loss in batch: 0.318821  [   60/   89]
Per-example loss in batch: 0.252781  [   62/   89]
Per-example loss in batch: 0.316931  [   64/   89]
Per-example loss in batch: 0.300302  [   66/   89]
Per-example loss in batch: 0.231137  [   68/   89]
Per-example loss in batch: 0.230751  [   70/   89]
Per-example loss in batch: 0.250202  [   72/   89]
Per-example loss in batch: 0.271511  [   74/   89]
Per-example loss in batch: 0.316694  [   76/   89]
Per-example loss in batch: 0.273488  [   78/   89]
Per-example loss in batch: 0.238573  [   80/   89]
Per-example loss in batch: 0.271647  [   82/   89]
Per-example loss in batch: 0.241023  [   84/   89]
Per-example loss in batch: 0.273616  [   86/   89]
Per-example loss in batch: 0.283668  [   88/   89]
Per-example loss in batch: 0.584377  [   89/   89]
Train Error: Avg loss: 0.26912756
validation Error: 
 Avg loss: 0.28096592 
 F1: 0.472422 
 Precision: 0.574550 
 Recall: 0.401121
 IoU: 0.309262

test Error: 
 Avg loss: 0.26366808 
 F1: 0.503890 
 Precision: 0.625180 
 Recall: 0.422016
 IoU: 0.336800

We have finished training iteration 212
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_210_.pth
Per-example loss in batch: 0.233381  [    2/   89]
Per-example loss in batch: 0.259132  [    4/   89]
Per-example loss in batch: 0.242942  [    6/   89]
Per-example loss in batch: 0.340195  [    8/   89]
Per-example loss in batch: 0.203054  [   10/   89]
Per-example loss in batch: 0.248476  [   12/   89]
Per-example loss in batch: 0.311290  [   14/   89]
Per-example loss in batch: 0.237124  [   16/   89]
Per-example loss in batch: 0.305834  [   18/   89]
Per-example loss in batch: 0.252063  [   20/   89]
Per-example loss in batch: 0.304993  [   22/   89]
Per-example loss in batch: 0.269156  [   24/   89]
Per-example loss in batch: 0.306987  [   26/   89]
Per-example loss in batch: 0.239130  [   28/   89]
Per-example loss in batch: 0.297646  [   30/   89]
Per-example loss in batch: 0.297420  [   32/   89]
Per-example loss in batch: 0.227380  [   34/   89]
Per-example loss in batch: 0.298098  [   36/   89]
Per-example loss in batch: 0.339133  [   38/   89]
Per-example loss in batch: 0.232751  [   40/   89]
Per-example loss in batch: 0.238318  [   42/   89]
Per-example loss in batch: 0.230894  [   44/   89]
Per-example loss in batch: 0.343121  [   46/   89]
Per-example loss in batch: 0.295741  [   48/   89]
Per-example loss in batch: 0.240187  [   50/   89]
Per-example loss in batch: 0.314733  [   52/   89]
Per-example loss in batch: 0.299262  [   54/   89]
Per-example loss in batch: 0.294912  [   56/   89]
Per-example loss in batch: 0.258357  [   58/   89]
Per-example loss in batch: 0.223935  [   60/   89]
Per-example loss in batch: 0.203905  [   62/   89]
Per-example loss in batch: 0.261648  [   64/   89]
Per-example loss in batch: 0.307390  [   66/   89]
Per-example loss in batch: 0.260853  [   68/   89]
Per-example loss in batch: 0.304064  [   70/   89]
Per-example loss in batch: 0.257261  [   72/   89]
Per-example loss in batch: 0.286175  [   74/   89]
Per-example loss in batch: 0.299373  [   76/   89]
Per-example loss in batch: 0.208527  [   78/   89]
Per-example loss in batch: 0.202922  [   80/   89]
Per-example loss in batch: 0.277074  [   82/   89]
Per-example loss in batch: 0.226266  [   84/   89]
Per-example loss in batch: 0.294147  [   86/   89]
Per-example loss in batch: 0.208017  [   88/   89]
Per-example loss in batch: 0.571406  [   89/   89]
Train Error: Avg loss: 0.27121279
validation Error: 
 Avg loss: 0.28491988 
 F1: 0.473650 
 Precision: 0.523393 
 Recall: 0.432541
 IoU: 0.310315

test Error: 
 Avg loss: 0.26022681 
 F1: 0.511175 
 Precision: 0.576169 
 Recall: 0.459358
 IoU: 0.343341

We have finished training iteration 213
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_211_.pth
Per-example loss in batch: 0.233362  [    2/   89]
Per-example loss in batch: 0.302694  [    4/   89]
Per-example loss in batch: 0.224935  [    6/   89]
Per-example loss in batch: 0.243569  [    8/   89]
Per-example loss in batch: 0.239860  [   10/   89]
Per-example loss in batch: 0.204560  [   12/   89]
Per-example loss in batch: 0.217885  [   14/   89]
Per-example loss in batch: 0.288387  [   16/   89]
Per-example loss in batch: 0.215695  [   18/   89]
Per-example loss in batch: 0.357434  [   20/   89]
Per-example loss in batch: 0.317366  [   22/   89]
Per-example loss in batch: 0.297110  [   24/   89]
Per-example loss in batch: 0.268491  [   26/   89]
Per-example loss in batch: 0.268813  [   28/   89]
Per-example loss in batch: 0.318992  [   30/   89]
Per-example loss in batch: 0.250926  [   32/   89]
Per-example loss in batch: 0.287800  [   34/   89]
Per-example loss in batch: 0.354265  [   36/   89]
Per-example loss in batch: 0.300560  [   38/   89]
Per-example loss in batch: 0.210112  [   40/   89]
Per-example loss in batch: 0.244554  [   42/   89]
Per-example loss in batch: 0.277559  [   44/   89]
Per-example loss in batch: 0.270349  [   46/   89]
Per-example loss in batch: 0.280200  [   48/   89]
Per-example loss in batch: 0.212568  [   50/   89]
Per-example loss in batch: 0.238371  [   52/   89]
Per-example loss in batch: 0.333249  [   54/   89]
Per-example loss in batch: 0.224255  [   56/   89]
Per-example loss in batch: 0.244103  [   58/   89]
Per-example loss in batch: 0.222188  [   60/   89]
Per-example loss in batch: 0.266595  [   62/   89]
Per-example loss in batch: 0.288584  [   64/   89]
Per-example loss in batch: 0.270016  [   66/   89]
Per-example loss in batch: 0.233545  [   68/   89]
Per-example loss in batch: 0.255893  [   70/   89]
Per-example loss in batch: 0.251400  [   72/   89]
Per-example loss in batch: 0.261016  [   74/   89]
Per-example loss in batch: 0.279779  [   76/   89]
Per-example loss in batch: 0.291955  [   78/   89]
Per-example loss in batch: 0.252779  [   80/   89]
Per-example loss in batch: 0.241293  [   82/   89]
Per-example loss in batch: 0.271489  [   84/   89]
Per-example loss in batch: 0.293878  [   86/   89]
Per-example loss in batch: 0.218780  [   88/   89]
Per-example loss in batch: 0.402287  [   89/   89]
Train Error: Avg loss: 0.26580583
validation Error: 
 Avg loss: 0.28366869 
 F1: 0.472685 
 Precision: 0.557442 
 Recall: 0.410300
 IoU: 0.309487

test Error: 
 Avg loss: 0.26320342 
 F1: 0.505142 
 Precision: 0.604758 
 Recall: 0.433702
 IoU: 0.337920

We have finished training iteration 214
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_212_.pth
Per-example loss in batch: 0.214289  [    2/   89]
Per-example loss in batch: 0.215818  [    4/   89]
Per-example loss in batch: 0.257251  [    6/   89]
Per-example loss in batch: 0.212051  [    8/   89]
Per-example loss in batch: 0.278379  [   10/   89]
Per-example loss in batch: 0.315360  [   12/   89]
Per-example loss in batch: 0.282137  [   14/   89]
Per-example loss in batch: 0.263765  [   16/   89]
Per-example loss in batch: 0.260192  [   18/   89]
Per-example loss in batch: 0.291624  [   20/   89]
Per-example loss in batch: 0.244517  [   22/   89]
Per-example loss in batch: 0.324319  [   24/   89]
Per-example loss in batch: 0.283661  [   26/   89]
Per-example loss in batch: 0.278267  [   28/   89]
Per-example loss in batch: 0.245614  [   30/   89]
Per-example loss in batch: 0.240097  [   32/   89]
Per-example loss in batch: 0.275859  [   34/   89]
Per-example loss in batch: 0.209620  [   36/   89]
Per-example loss in batch: 0.282206  [   38/   89]
Per-example loss in batch: 0.348772  [   40/   89]
Per-example loss in batch: 0.225069  [   42/   89]
Per-example loss in batch: 0.311967  [   44/   89]
Per-example loss in batch: 0.232220  [   46/   89]
Per-example loss in batch: 0.217722  [   48/   89]
Per-example loss in batch: 0.224751  [   50/   89]
Per-example loss in batch: 0.313417  [   52/   89]
Per-example loss in batch: 0.244129  [   54/   89]
Per-example loss in batch: 0.228864  [   56/   89]
Per-example loss in batch: 0.358393  [   58/   89]
Per-example loss in batch: 0.252540  [   60/   89]
Per-example loss in batch: 0.251327  [   62/   89]
Per-example loss in batch: 0.305849  [   64/   89]
Per-example loss in batch: 0.222888  [   66/   89]
Per-example loss in batch: 0.256882  [   68/   89]
Per-example loss in batch: 0.207744  [   70/   89]
Per-example loss in batch: 0.236252  [   72/   89]
Per-example loss in batch: 0.280871  [   74/   89]
Per-example loss in batch: 0.268669  [   76/   89]
Per-example loss in batch: 0.206351  [   78/   89]
Per-example loss in batch: 0.216980  [   80/   89]
Per-example loss in batch: 0.215525  [   82/   89]
Per-example loss in batch: 0.231286  [   84/   89]
Per-example loss in batch: 0.245279  [   86/   89]
Per-example loss in batch: 0.321203  [   88/   89]
Per-example loss in batch: 0.717491  [   89/   89]
Train Error: Avg loss: 0.26424092
validation Error: 
 Avg loss: 0.28409273 
 F1: 0.474445 
 Precision: 0.555807 
 Recall: 0.413861
 IoU: 0.310998

test Error: 
 Avg loss: 0.26171876 
 F1: 0.508088 
 Precision: 0.608723 
 Recall: 0.436006
 IoU: 0.340561

We have finished training iteration 215
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_213_.pth
Per-example loss in batch: 0.302050  [    2/   89]
Per-example loss in batch: 0.198775  [    4/   89]
Per-example loss in batch: 0.270465  [    6/   89]
Per-example loss in batch: 0.299850  [    8/   89]
Per-example loss in batch: 0.286403  [   10/   89]
Per-example loss in batch: 0.265958  [   12/   89]
Per-example loss in batch: 0.222360  [   14/   89]
Per-example loss in batch: 0.225820  [   16/   89]
Per-example loss in batch: 0.272564  [   18/   89]
Per-example loss in batch: 0.207351  [   20/   89]
Per-example loss in batch: 0.264369  [   22/   89]
Per-example loss in batch: 0.252734  [   24/   89]
Per-example loss in batch: 0.251904  [   26/   89]
Per-example loss in batch: 0.197304  [   28/   89]
Per-example loss in batch: 0.378340  [   30/   89]
Per-example loss in batch: 0.246539  [   32/   89]
Per-example loss in batch: 0.230919  [   34/   89]
Per-example loss in batch: 0.207275  [   36/   89]
Per-example loss in batch: 0.316625  [   38/   89]
Per-example loss in batch: 0.268436  [   40/   89]
Per-example loss in batch: 0.236818  [   42/   89]
Per-example loss in batch: 0.287631  [   44/   89]
Per-example loss in batch: 0.222991  [   46/   89]
Per-example loss in batch: 0.297521  [   48/   89]
Per-example loss in batch: 0.259326  [   50/   89]
Per-example loss in batch: 0.281751  [   52/   89]
Per-example loss in batch: 0.253563  [   54/   89]
Per-example loss in batch: 0.292093  [   56/   89]
Per-example loss in batch: 0.213215  [   58/   89]
Per-example loss in batch: 0.261042  [   60/   89]
Per-example loss in batch: 0.272004  [   62/   89]
Per-example loss in batch: 0.214000  [   64/   89]
Per-example loss in batch: 0.300421  [   66/   89]
Per-example loss in batch: 0.267987  [   68/   89]
Per-example loss in batch: 0.348017  [   70/   89]
Per-example loss in batch: 0.327673  [   72/   89]
Per-example loss in batch: 0.273888  [   74/   89]
Per-example loss in batch: 0.338218  [   76/   89]
Per-example loss in batch: 0.278230  [   78/   89]
Per-example loss in batch: 0.236584  [   80/   89]
Per-example loss in batch: 0.279373  [   82/   89]
Per-example loss in batch: 0.225878  [   84/   89]
Per-example loss in batch: 0.306047  [   86/   89]
Per-example loss in batch: 0.214890  [   88/   89]
Per-example loss in batch: 0.377609  [   89/   89]
Train Error: Avg loss: 0.26615743
validation Error: 
 Avg loss: 0.28608621 
 F1: 0.473400 
 Precision: 0.531766 
 Recall: 0.426579
 IoU: 0.310101

test Error: 
 Avg loss: 0.26125219 
 F1: 0.508863 
 Precision: 0.583146 
 Recall: 0.451366
 IoU: 0.341258

We have finished training iteration 216
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_214_.pth
Per-example loss in batch: 0.303495  [    2/   89]
Per-example loss in batch: 0.240671  [    4/   89]
Per-example loss in batch: 0.285255  [    6/   89]
Per-example loss in batch: 0.226002  [    8/   89]
Per-example loss in batch: 0.212674  [   10/   89]
Per-example loss in batch: 0.280576  [   12/   89]
Per-example loss in batch: 0.216852  [   14/   89]
Per-example loss in batch: 0.297812  [   16/   89]
Per-example loss in batch: 0.315966  [   18/   89]
Per-example loss in batch: 0.382285  [   20/   89]
Per-example loss in batch: 0.295495  [   22/   89]
Per-example loss in batch: 0.254069  [   24/   89]
Per-example loss in batch: 0.202719  [   26/   89]
Per-example loss in batch: 0.245579  [   28/   89]
Per-example loss in batch: 0.270724  [   30/   89]
Per-example loss in batch: 0.222904  [   32/   89]
Per-example loss in batch: 0.208406  [   34/   89]
Per-example loss in batch: 0.217268  [   36/   89]
Per-example loss in batch: 0.304471  [   38/   89]
Per-example loss in batch: 0.314514  [   40/   89]
Per-example loss in batch: 0.369393  [   42/   89]
Per-example loss in batch: 0.220274  [   44/   89]
Per-example loss in batch: 0.281134  [   46/   89]
Per-example loss in batch: 0.216179  [   48/   89]
Per-example loss in batch: 0.351733  [   50/   89]
Per-example loss in batch: 0.255971  [   52/   89]
Per-example loss in batch: 0.233187  [   54/   89]
Per-example loss in batch: 0.287261  [   56/   89]
Per-example loss in batch: 0.339159  [   58/   89]
Per-example loss in batch: 0.290237  [   60/   89]
Per-example loss in batch: 0.229398  [   62/   89]
Per-example loss in batch: 0.206311  [   64/   89]
Per-example loss in batch: 0.298857  [   66/   89]
Per-example loss in batch: 0.231189  [   68/   89]
Per-example loss in batch: 0.249626  [   70/   89]
Per-example loss in batch: 0.232783  [   72/   89]
Per-example loss in batch: 0.245981  [   74/   89]
Per-example loss in batch: 0.256618  [   76/   89]
Per-example loss in batch: 0.249586  [   78/   89]
Per-example loss in batch: 0.225506  [   80/   89]
Per-example loss in batch: 0.215873  [   82/   89]
Per-example loss in batch: 0.272614  [   84/   89]
Per-example loss in batch: 0.294222  [   86/   89]
Per-example loss in batch: 0.231885  [   88/   89]
Per-example loss in batch: 0.388673  [   89/   89]
Train Error: Avg loss: 0.26465277
validation Error: 
 Avg loss: 0.28553617 
 F1: 0.472981 
 Precision: 0.525994 
 Recall: 0.429675
 IoU: 0.309741

test Error: 
 Avg loss: 0.26118626 
 F1: 0.509154 
 Precision: 0.578757 
 Recall: 0.454496
 IoU: 0.341520

We have finished training iteration 217
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_215_.pth
Per-example loss in batch: 0.248793  [    2/   89]
Per-example loss in batch: 0.255375  [    4/   89]
Per-example loss in batch: 0.231768  [    6/   89]
Per-example loss in batch: 0.280138  [    8/   89]
Per-example loss in batch: 0.248656  [   10/   89]
Per-example loss in batch: 0.253183  [   12/   89]
Per-example loss in batch: 0.328781  [   14/   89]
Per-example loss in batch: 0.312671  [   16/   89]
Per-example loss in batch: 0.252257  [   18/   89]
Per-example loss in batch: 0.214403  [   20/   89]
Per-example loss in batch: 0.253588  [   22/   89]
Per-example loss in batch: 0.272087  [   24/   89]
Per-example loss in batch: 0.286340  [   26/   89]
Per-example loss in batch: 0.233979  [   28/   89]
Per-example loss in batch: 0.215643  [   30/   89]
Per-example loss in batch: 0.232869  [   32/   89]
Per-example loss in batch: 0.235317  [   34/   89]
Per-example loss in batch: 0.273335  [   36/   89]
Per-example loss in batch: 0.223282  [   38/   89]
Per-example loss in batch: 0.301346  [   40/   89]
Per-example loss in batch: 0.323107  [   42/   89]
Per-example loss in batch: 0.276531  [   44/   89]
Per-example loss in batch: 0.204925  [   46/   89]
Per-example loss in batch: 0.234958  [   48/   89]
Per-example loss in batch: 0.281686  [   50/   89]
Per-example loss in batch: 0.254623  [   52/   89]
Per-example loss in batch: 0.227219  [   54/   89]
Per-example loss in batch: 0.257437  [   56/   89]
Per-example loss in batch: 0.251597  [   58/   89]
Per-example loss in batch: 0.226907  [   60/   89]
Per-example loss in batch: 0.211209  [   62/   89]
Per-example loss in batch: 0.317634  [   64/   89]
Per-example loss in batch: 0.330446  [   66/   89]
Per-example loss in batch: 0.299152  [   68/   89]
Per-example loss in batch: 0.243236  [   70/   89]
Per-example loss in batch: 0.304467  [   72/   89]
Per-example loss in batch: 0.303832  [   74/   89]
Per-example loss in batch: 0.211857  [   76/   89]
Per-example loss in batch: 0.221223  [   78/   89]
Per-example loss in batch: 0.246039  [   80/   89]
Per-example loss in batch: 0.298594  [   82/   89]
Per-example loss in batch: 0.200895  [   84/   89]
Per-example loss in batch: 0.283278  [   86/   89]
Per-example loss in batch: 0.226780  [   88/   89]
Per-example loss in batch: 0.719705  [   89/   89]
Train Error: Avg loss: 0.26407414
validation Error: 
 Avg loss: 0.28473642 
 F1: 0.474173 
 Precision: 0.545258 
 Recall: 0.419485
 IoU: 0.310765

test Error: 
 Avg loss: 0.26174219 
 F1: 0.507632 
 Precision: 0.595458 
 Recall: 0.442383
 IoU: 0.340152

We have finished training iteration 218
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_216_.pth
Per-example loss in batch: 0.241998  [    2/   89]
Per-example loss in batch: 0.247795  [    4/   89]
Per-example loss in batch: 0.340938  [    6/   89]
Per-example loss in batch: 0.254599  [    8/   89]
Per-example loss in batch: 0.246735  [   10/   89]
Per-example loss in batch: 0.345375  [   12/   89]
Per-example loss in batch: 0.231283  [   14/   89]
Per-example loss in batch: 0.262010  [   16/   89]
Per-example loss in batch: 0.296083  [   18/   89]
Per-example loss in batch: 0.220837  [   20/   89]
Per-example loss in batch: 0.289664  [   22/   89]
Per-example loss in batch: 0.301444  [   24/   89]
Per-example loss in batch: 0.278325  [   26/   89]
Per-example loss in batch: 0.215306  [   28/   89]
Per-example loss in batch: 0.286221  [   30/   89]
Per-example loss in batch: 0.215937  [   32/   89]
Per-example loss in batch: 0.273039  [   34/   89]
Per-example loss in batch: 0.208068  [   36/   89]
Per-example loss in batch: 0.236075  [   38/   89]
Per-example loss in batch: 0.319225  [   40/   89]
Per-example loss in batch: 0.335579  [   42/   89]
Per-example loss in batch: 0.359333  [   44/   89]
Per-example loss in batch: 0.319139  [   46/   89]
Per-example loss in batch: 0.316533  [   48/   89]
Per-example loss in batch: 0.282623  [   50/   89]
Per-example loss in batch: 0.268633  [   52/   89]
Per-example loss in batch: 0.250830  [   54/   89]
Per-example loss in batch: 0.209696  [   56/   89]
Per-example loss in batch: 0.231820  [   58/   89]
Per-example loss in batch: 0.307093  [   60/   89]
Per-example loss in batch: 0.203595  [   62/   89]
Per-example loss in batch: 0.241161  [   64/   89]
Per-example loss in batch: 0.234533  [   66/   89]
Per-example loss in batch: 0.199316  [   68/   89]
Per-example loss in batch: 0.243840  [   70/   89]
Per-example loss in batch: 0.227017  [   72/   89]
Per-example loss in batch: 0.209745  [   74/   89]
Per-example loss in batch: 0.275807  [   76/   89]
Per-example loss in batch: 0.222144  [   78/   89]
Per-example loss in batch: 0.251440  [   80/   89]
Per-example loss in batch: 0.300609  [   82/   89]
Per-example loss in batch: 0.241790  [   84/   89]
Per-example loss in batch: 0.291227  [   86/   89]
Per-example loss in batch: 0.234979  [   88/   89]
Per-example loss in batch: 0.733314  [   89/   89]
Train Error: Avg loss: 0.26822685
validation Error: 
 Avg loss: 0.28061341 
 F1: 0.473166 
 Precision: 0.560957 
 Recall: 0.409135
 IoU: 0.309900

test Error: 
 Avg loss: 0.26234880 
 F1: 0.507062 
 Precision: 0.613996 
 Recall: 0.431850
 IoU: 0.339640

We have finished training iteration 219
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_217_.pth
Per-example loss in batch: 0.281580  [    2/   89]
Per-example loss in batch: 0.226861  [    4/   89]
Per-example loss in batch: 0.260723  [    6/   89]
Per-example loss in batch: 0.275378  [    8/   89]
Per-example loss in batch: 0.268939  [   10/   89]
Per-example loss in batch: 0.260416  [   12/   89]
Per-example loss in batch: 0.336761  [   14/   89]
Per-example loss in batch: 0.230447  [   16/   89]
Per-example loss in batch: 0.319204  [   18/   89]
Per-example loss in batch: 0.214243  [   20/   89]
Per-example loss in batch: 0.319274  [   22/   89]
Per-example loss in batch: 0.245718  [   24/   89]
Per-example loss in batch: 0.254260  [   26/   89]
Per-example loss in batch: 0.229247  [   28/   89]
Per-example loss in batch: 0.218676  [   30/   89]
Per-example loss in batch: 0.355909  [   32/   89]
Per-example loss in batch: 0.201613  [   34/   89]
Per-example loss in batch: 0.308720  [   36/   89]
Per-example loss in batch: 0.241553  [   38/   89]
Per-example loss in batch: 0.201420  [   40/   89]
Per-example loss in batch: 0.315418  [   42/   89]
Per-example loss in batch: 0.250244  [   44/   89]
Per-example loss in batch: 0.337282  [   46/   89]
Per-example loss in batch: 0.211559  [   48/   89]
Per-example loss in batch: 0.252496  [   50/   89]
Per-example loss in batch: 0.209464  [   52/   89]
Per-example loss in batch: 0.240023  [   54/   89]
Per-example loss in batch: 0.253319  [   56/   89]
Per-example loss in batch: 0.282969  [   58/   89]
Per-example loss in batch: 0.344678  [   60/   89]
Per-example loss in batch: 0.306327  [   62/   89]
Per-example loss in batch: 0.229553  [   64/   89]
Per-example loss in batch: 0.225704  [   66/   89]
Per-example loss in batch: 0.281936  [   68/   89]
Per-example loss in batch: 0.287288  [   70/   89]
Per-example loss in batch: 0.234952  [   72/   89]
Per-example loss in batch: 0.341198  [   74/   89]
Per-example loss in batch: 0.239644  [   76/   89]
Per-example loss in batch: 0.207760  [   78/   89]
Per-example loss in batch: 0.270480  [   80/   89]
Per-example loss in batch: 0.287475  [   82/   89]
Per-example loss in batch: 0.242720  [   84/   89]
Per-example loss in batch: 0.232771  [   86/   89]
Per-example loss in batch: 0.266647  [   88/   89]
Per-example loss in batch: 0.575724  [   89/   89]
Train Error: Avg loss: 0.26720692
validation Error: 
 Avg loss: 0.29045223 
 F1: 0.471767 
 Precision: 0.519958 
 Recall: 0.431751
 IoU: 0.308701

test Error: 
 Avg loss: 0.26162933 
 F1: 0.508105 
 Precision: 0.567888 
 Recall: 0.459711
 IoU: 0.340577

We have finished training iteration 220
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_218_.pth
Per-example loss in batch: 0.241764  [    2/   89]
Per-example loss in batch: 0.344267  [    4/   89]
Per-example loss in batch: 0.288745  [    6/   89]
Per-example loss in batch: 0.264157  [    8/   89]
Per-example loss in batch: 0.266288  [   10/   89]
Per-example loss in batch: 0.230700  [   12/   89]
Per-example loss in batch: 0.234160  [   14/   89]
Per-example loss in batch: 0.262918  [   16/   89]
Per-example loss in batch: 0.194250  [   18/   89]
Per-example loss in batch: 0.278684  [   20/   89]
Per-example loss in batch: 0.223840  [   22/   89]
Per-example loss in batch: 0.231316  [   24/   89]
Per-example loss in batch: 0.300140  [   26/   89]
Per-example loss in batch: 0.234861  [   28/   89]
Per-example loss in batch: 0.248001  [   30/   89]
Per-example loss in batch: 0.307505  [   32/   89]
Per-example loss in batch: 0.329180  [   34/   89]
Per-example loss in batch: 0.228077  [   36/   89]
Per-example loss in batch: 0.232340  [   38/   89]
Per-example loss in batch: 0.310926  [   40/   89]
Per-example loss in batch: 0.237987  [   42/   89]
Per-example loss in batch: 0.221282  [   44/   89]
Per-example loss in batch: 0.304971  [   46/   89]
Per-example loss in batch: 0.233004  [   48/   89]
Per-example loss in batch: 0.267789  [   50/   89]
Per-example loss in batch: 0.306494  [   52/   89]
Per-example loss in batch: 0.373550  [   54/   89]
Per-example loss in batch: 0.309432  [   56/   89]
Per-example loss in batch: 0.230348  [   58/   89]
Per-example loss in batch: 0.337086  [   60/   89]
Per-example loss in batch: 0.242614  [   62/   89]
Per-example loss in batch: 0.207738  [   64/   89]
Per-example loss in batch: 0.260028  [   66/   89]
Per-example loss in batch: 0.274977  [   68/   89]
Per-example loss in batch: 0.243722  [   70/   89]
Per-example loss in batch: 0.218003  [   72/   89]
Per-example loss in batch: 0.329195  [   74/   89]
Per-example loss in batch: 0.279581  [   76/   89]
Per-example loss in batch: 0.228141  [   78/   89]
Per-example loss in batch: 0.256843  [   80/   89]
Per-example loss in batch: 0.260428  [   82/   89]
Per-example loss in batch: 0.275367  [   84/   89]
Per-example loss in batch: 0.303686  [   86/   89]
Per-example loss in batch: 0.259262  [   88/   89]
Per-example loss in batch: 0.524280  [   89/   89]
Train Error: Avg loss: 0.26911885
validation Error: 
 Avg loss: 0.28693739 
 F1: 0.470382 
 Precision: 0.507528 
 Recall: 0.438304
 IoU: 0.307516

test Error: 
 Avg loss: 0.26196467 
 F1: 0.507769 
 Precision: 0.555086 
 Recall: 0.467885
 IoU: 0.340275

We have finished training iteration 221
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_219_.pth
Per-example loss in batch: 0.223698  [    2/   89]
Per-example loss in batch: 0.213455  [    4/   89]
Per-example loss in batch: 0.201882  [    6/   89]
Per-example loss in batch: 0.325215  [    8/   89]
Per-example loss in batch: 0.258258  [   10/   89]
Per-example loss in batch: 0.226180  [   12/   89]
Per-example loss in batch: 0.324419  [   14/   89]
Per-example loss in batch: 0.246192  [   16/   89]
Per-example loss in batch: 0.313073  [   18/   89]
Per-example loss in batch: 0.292682  [   20/   89]
Per-example loss in batch: 0.287382  [   22/   89]
Per-example loss in batch: 0.219174  [   24/   89]
Per-example loss in batch: 0.262430  [   26/   89]
Per-example loss in batch: 0.322497  [   28/   89]
Per-example loss in batch: 0.242208  [   30/   89]
Per-example loss in batch: 0.249701  [   32/   89]
Per-example loss in batch: 0.290547  [   34/   89]
Per-example loss in batch: 0.303492  [   36/   89]
Per-example loss in batch: 0.217335  [   38/   89]
Per-example loss in batch: 0.293137  [   40/   89]
Per-example loss in batch: 0.300421  [   42/   89]
Per-example loss in batch: 0.239030  [   44/   89]
Per-example loss in batch: 0.207865  [   46/   89]
Per-example loss in batch: 0.267880  [   48/   89]
Per-example loss in batch: 0.271343  [   50/   89]
Per-example loss in batch: 0.265956  [   52/   89]
Per-example loss in batch: 0.268176  [   54/   89]
Per-example loss in batch: 0.267849  [   56/   89]
Per-example loss in batch: 0.220665  [   58/   89]
Per-example loss in batch: 0.230852  [   60/   89]
Per-example loss in batch: 0.290294  [   62/   89]
Per-example loss in batch: 0.208830  [   64/   89]
Per-example loss in batch: 0.321769  [   66/   89]
Per-example loss in batch: 0.238758  [   68/   89]
Per-example loss in batch: 0.218560  [   70/   89]
Per-example loss in batch: 0.311808  [   72/   89]
Per-example loss in batch: 0.267443  [   74/   89]
Per-example loss in batch: 0.274208  [   76/   89]
Per-example loss in batch: 0.219246  [   78/   89]
Per-example loss in batch: 0.243240  [   80/   89]
Per-example loss in batch: 0.274303  [   82/   89]
Per-example loss in batch: 0.256258  [   84/   89]
Per-example loss in batch: 0.232171  [   86/   89]
Per-example loss in batch: 0.259485  [   88/   89]
Per-example loss in batch: 0.431143  [   89/   89]
Train Error: Avg loss: 0.26258291
validation Error: 
 Avg loss: 0.28556203 
 F1: 0.475480 
 Precision: 0.545263 
 Recall: 0.421533
 IoU: 0.311889

test Error: 
 Avg loss: 0.26049542 
 F1: 0.510658 
 Precision: 0.600836 
 Recall: 0.444017
 IoU: 0.342875

We have finished training iteration 222
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_220_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.308422  [    2/   89]
Per-example loss in batch: 0.340633  [    4/   89]
Per-example loss in batch: 0.258547  [    6/   89]
Per-example loss in batch: 0.220654  [    8/   89]
Per-example loss in batch: 0.302688  [   10/   89]
Per-example loss in batch: 0.242059  [   12/   89]
Per-example loss in batch: 0.237698  [   14/   89]
Per-example loss in batch: 0.211988  [   16/   89]
Per-example loss in batch: 0.219945  [   18/   89]
Per-example loss in batch: 0.277485  [   20/   89]
Per-example loss in batch: 0.250639  [   22/   89]
Per-example loss in batch: 0.324496  [   24/   89]
Per-example loss in batch: 0.204071  [   26/   89]
Per-example loss in batch: 0.229413  [   28/   89]
Per-example loss in batch: 0.279838  [   30/   89]
Per-example loss in batch: 0.213067  [   32/   89]
Per-example loss in batch: 0.269422  [   34/   89]
Per-example loss in batch: 0.300877  [   36/   89]
Per-example loss in batch: 0.208543  [   38/   89]
Per-example loss in batch: 0.314013  [   40/   89]
Per-example loss in batch: 0.325312  [   42/   89]
Per-example loss in batch: 0.244643  [   44/   89]
Per-example loss in batch: 0.272270  [   46/   89]
Per-example loss in batch: 0.292840  [   48/   89]
Per-example loss in batch: 0.214358  [   50/   89]
Per-example loss in batch: 0.256723  [   52/   89]
Per-example loss in batch: 0.291522  [   54/   89]
Per-example loss in batch: 0.322199  [   56/   89]
Per-example loss in batch: 0.317532  [   58/   89]
Per-example loss in batch: 0.312209  [   60/   89]
Per-example loss in batch: 0.297990  [   62/   89]
Per-example loss in batch: 0.204414  [   64/   89]
Per-example loss in batch: 0.263702  [   66/   89]
Per-example loss in batch: 0.259477  [   68/   89]
Per-example loss in batch: 0.384308  [   70/   89]
Per-example loss in batch: 0.271339  [   72/   89]
Per-example loss in batch: 0.213274  [   74/   89]
Per-example loss in batch: 0.344874  [   76/   89]
Per-example loss in batch: 0.259613  [   78/   89]
Per-example loss in batch: 0.223103  [   80/   89]
Per-example loss in batch: 0.329654  [   82/   89]
Per-example loss in batch: 0.227462  [   84/   89]
Per-example loss in batch: 0.244138  [   86/   89]
Per-example loss in batch: 0.278215  [   88/   89]
Per-example loss in batch: 0.418730  [   89/   89]
Train Error: Avg loss: 0.27134918
validation Error: 
 Avg loss: 0.28858710 
 F1: 0.475766 
 Precision: 0.552381 
 Recall: 0.417815
 IoU: 0.312134

test Error: 
 Avg loss: 0.26087788 
 F1: 0.509614 
 Precision: 0.605894 
 Recall: 0.439738
 IoU: 0.341935

We have finished training iteration 223
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_221_.pth
Per-example loss in batch: 0.212329  [    2/   89]
Per-example loss in batch: 0.219791  [    4/   89]
Per-example loss in batch: 0.330425  [    6/   89]
Per-example loss in batch: 0.267322  [    8/   89]
Per-example loss in batch: 0.318159  [   10/   89]
Per-example loss in batch: 0.220076  [   12/   89]
Per-example loss in batch: 0.315263  [   14/   89]
Per-example loss in batch: 0.230196  [   16/   89]
Per-example loss in batch: 0.342206  [   18/   89]
Per-example loss in batch: 0.259206  [   20/   89]
Per-example loss in batch: 0.259184  [   22/   89]
Per-example loss in batch: 0.241299  [   24/   89]
Per-example loss in batch: 0.334695  [   26/   89]
Per-example loss in batch: 0.250941  [   28/   89]
Per-example loss in batch: 0.281605  [   30/   89]
Per-example loss in batch: 0.234410  [   32/   89]
Per-example loss in batch: 0.219634  [   34/   89]
Per-example loss in batch: 0.190444  [   36/   89]
Per-example loss in batch: 0.329864  [   38/   89]
Per-example loss in batch: 0.250585  [   40/   89]
Per-example loss in batch: 0.286287  [   42/   89]
Per-example loss in batch: 0.237310  [   44/   89]
Per-example loss in batch: 0.298577  [   46/   89]
Per-example loss in batch: 0.256091  [   48/   89]
Per-example loss in batch: 0.290518  [   50/   89]
Per-example loss in batch: 0.245793  [   52/   89]
Per-example loss in batch: 0.315335  [   54/   89]
Per-example loss in batch: 0.322206  [   56/   89]
Per-example loss in batch: 0.297604  [   58/   89]
Per-example loss in batch: 0.225273  [   60/   89]
Per-example loss in batch: 0.253217  [   62/   89]
Per-example loss in batch: 0.215754  [   64/   89]
Per-example loss in batch: 0.329081  [   66/   89]
Per-example loss in batch: 0.262881  [   68/   89]
Per-example loss in batch: 0.246925  [   70/   89]
Per-example loss in batch: 0.295734  [   72/   89]
Per-example loss in batch: 0.212286  [   74/   89]
Per-example loss in batch: 0.217113  [   76/   89]
Per-example loss in batch: 0.298415  [   78/   89]
Per-example loss in batch: 0.354396  [   80/   89]
Per-example loss in batch: 0.234021  [   82/   89]
Per-example loss in batch: 0.217529  [   84/   89]
Per-example loss in batch: 0.308812  [   86/   89]
Per-example loss in batch: 0.240927  [   88/   89]
Per-example loss in batch: 0.460842  [   89/   89]
Train Error: Avg loss: 0.26966613
validation Error: 
 Avg loss: 0.28171470 
 F1: 0.473162 
 Precision: 0.574639 
 Recall: 0.402146
 IoU: 0.309897

test Error: 
 Avg loss: 0.26297273 
 F1: 0.505538 
 Precision: 0.627178 
 Recall: 0.423417
 IoU: 0.338274

We have finished training iteration 224
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_222_.pth
Per-example loss in batch: 0.245899  [    2/   89]
Per-example loss in batch: 0.259141  [    4/   89]
Per-example loss in batch: 0.244973  [    6/   89]
Per-example loss in batch: 0.237143  [    8/   89]
Per-example loss in batch: 0.245103  [   10/   89]
Per-example loss in batch: 0.221242  [   12/   89]
Per-example loss in batch: 0.301219  [   14/   89]
Per-example loss in batch: 0.220910  [   16/   89]
Per-example loss in batch: 0.252910  [   18/   89]
Per-example loss in batch: 0.247695  [   20/   89]
Per-example loss in batch: 0.249438  [   22/   89]
Per-example loss in batch: 0.265273  [   24/   89]
Per-example loss in batch: 0.251138  [   26/   89]
Per-example loss in batch: 0.232775  [   28/   89]
Per-example loss in batch: 0.239414  [   30/   89]
Per-example loss in batch: 0.223110  [   32/   89]
Per-example loss in batch: 0.288308  [   34/   89]
Per-example loss in batch: 0.263392  [   36/   89]
Per-example loss in batch: 0.227517  [   38/   89]
Per-example loss in batch: 0.291584  [   40/   89]
Per-example loss in batch: 0.275899  [   42/   89]
Per-example loss in batch: 0.210636  [   44/   89]
Per-example loss in batch: 0.257745  [   46/   89]
Per-example loss in batch: 0.293146  [   48/   89]
Per-example loss in batch: 0.275508  [   50/   89]
Per-example loss in batch: 0.258054  [   52/   89]
Per-example loss in batch: 0.282995  [   54/   89]
Per-example loss in batch: 0.254821  [   56/   89]
Per-example loss in batch: 0.230080  [   58/   89]
Per-example loss in batch: 0.254629  [   60/   89]
Per-example loss in batch: 0.329944  [   62/   89]
Per-example loss in batch: 0.319664  [   64/   89]
Per-example loss in batch: 0.285356  [   66/   89]
Per-example loss in batch: 0.263536  [   68/   89]
Per-example loss in batch: 0.220269  [   70/   89]
Per-example loss in batch: 0.238786  [   72/   89]
Per-example loss in batch: 0.225540  [   74/   89]
Per-example loss in batch: 0.216194  [   76/   89]
Per-example loss in batch: 0.252375  [   78/   89]
Per-example loss in batch: 0.301202  [   80/   89]
Per-example loss in batch: 0.362564  [   82/   89]
Per-example loss in batch: 0.219354  [   84/   89]
Per-example loss in batch: 0.222037  [   86/   89]
Per-example loss in batch: 0.239717  [   88/   89]
Per-example loss in batch: 0.724720  [   89/   89]
Train Error: Avg loss: 0.26203579
validation Error: 
 Avg loss: 0.28016675 
 F1: 0.475340 
 Precision: 0.528070 
 Recall: 0.432184
 IoU: 0.311767

test Error: 
 Avg loss: 0.25961428 
 F1: 0.512277 
 Precision: 0.581052 
 Recall: 0.458059
 IoU: 0.344336

We have finished training iteration 225
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_223_.pth
Per-example loss in batch: 0.218332  [    2/   89]
Per-example loss in batch: 0.303530  [    4/   89]
Per-example loss in batch: 0.289348  [    6/   89]
Per-example loss in batch: 0.299995  [    8/   89]
Per-example loss in batch: 0.253766  [   10/   89]
Per-example loss in batch: 0.218823  [   12/   89]
Per-example loss in batch: 0.243385  [   14/   89]
Per-example loss in batch: 0.290428  [   16/   89]
Per-example loss in batch: 0.249934  [   18/   89]
Per-example loss in batch: 0.331513  [   20/   89]
Per-example loss in batch: 0.296199  [   22/   89]
Per-example loss in batch: 0.237151  [   24/   89]
Per-example loss in batch: 0.246022  [   26/   89]
Per-example loss in batch: 0.320526  [   28/   89]
Per-example loss in batch: 0.205419  [   30/   89]
Per-example loss in batch: 0.229358  [   32/   89]
Per-example loss in batch: 0.231003  [   34/   89]
Per-example loss in batch: 0.238935  [   36/   89]
Per-example loss in batch: 0.286492  [   38/   89]
Per-example loss in batch: 0.322445  [   40/   89]
Per-example loss in batch: 0.328977  [   42/   89]
Per-example loss in batch: 0.194719  [   44/   89]
Per-example loss in batch: 0.330439  [   46/   89]
Per-example loss in batch: 0.290249  [   48/   89]
Per-example loss in batch: 0.194669  [   50/   89]
Per-example loss in batch: 0.251544  [   52/   89]
Per-example loss in batch: 0.302444  [   54/   89]
Per-example loss in batch: 0.235578  [   56/   89]
Per-example loss in batch: 0.213625  [   58/   89]
Per-example loss in batch: 0.231333  [   60/   89]
Per-example loss in batch: 0.232548  [   62/   89]
Per-example loss in batch: 0.263405  [   64/   89]
Per-example loss in batch: 0.254848  [   66/   89]
Per-example loss in batch: 0.277543  [   68/   89]
Per-example loss in batch: 0.231155  [   70/   89]
Per-example loss in batch: 0.248749  [   72/   89]
Per-example loss in batch: 0.293765  [   74/   89]
Per-example loss in batch: 0.245699  [   76/   89]
Per-example loss in batch: 0.258038  [   78/   89]
Per-example loss in batch: 0.228048  [   80/   89]
Per-example loss in batch: 0.238901  [   82/   89]
Per-example loss in batch: 0.247904  [   84/   89]
Per-example loss in batch: 0.218901  [   86/   89]
Per-example loss in batch: 0.256007  [   88/   89]
Per-example loss in batch: 0.530411  [   89/   89]
Train Error: Avg loss: 0.26172805
validation Error: 
 Avg loss: 0.28657806 
 F1: 0.474485 
 Precision: 0.544906 
 Recall: 0.420183
 IoU: 0.311033

test Error: 
 Avg loss: 0.26140758 
 F1: 0.508871 
 Precision: 0.600381 
 Recall: 0.441567
 IoU: 0.341265

We have finished training iteration 226
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_224_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.211500  [    2/   89]
Per-example loss in batch: 0.215023  [    4/   89]
Per-example loss in batch: 0.217337  [    6/   89]
Per-example loss in batch: 0.217176  [    8/   89]
Per-example loss in batch: 0.264725  [   10/   89]
Per-example loss in batch: 0.305174  [   12/   89]
Per-example loss in batch: 0.331145  [   14/   89]
Per-example loss in batch: 0.248056  [   16/   89]
Per-example loss in batch: 0.243181  [   18/   89]
Per-example loss in batch: 0.200249  [   20/   89]
Per-example loss in batch: 0.231351  [   22/   89]
Per-example loss in batch: 0.259534  [   24/   89]
Per-example loss in batch: 0.338067  [   26/   89]
Per-example loss in batch: 0.312257  [   28/   89]
Per-example loss in batch: 0.221029  [   30/   89]
Per-example loss in batch: 0.220309  [   32/   89]
Per-example loss in batch: 0.274494  [   34/   89]
Per-example loss in batch: 0.355180  [   36/   89]
Per-example loss in batch: 0.279802  [   38/   89]
Per-example loss in batch: 0.289190  [   40/   89]
Per-example loss in batch: 0.232733  [   42/   89]
Per-example loss in batch: 0.242158  [   44/   89]
Per-example loss in batch: 0.270328  [   46/   89]
Per-example loss in batch: 0.236350  [   48/   89]
Per-example loss in batch: 0.225985  [   50/   89]
Per-example loss in batch: 0.319519  [   52/   89]
Per-example loss in batch: 0.284201  [   54/   89]
Per-example loss in batch: 0.303610  [   56/   89]
Per-example loss in batch: 0.246927  [   58/   89]
Per-example loss in batch: 0.237865  [   60/   89]
Per-example loss in batch: 0.292080  [   62/   89]
Per-example loss in batch: 0.217643  [   64/   89]
Per-example loss in batch: 0.239689  [   66/   89]
Per-example loss in batch: 0.289248  [   68/   89]
Per-example loss in batch: 0.200745  [   70/   89]
Per-example loss in batch: 0.289870  [   72/   89]
Per-example loss in batch: 0.273345  [   74/   89]
Per-example loss in batch: 0.334698  [   76/   89]
Per-example loss in batch: 0.277830  [   78/   89]
Per-example loss in batch: 0.243029  [   80/   89]
Per-example loss in batch: 0.202659  [   82/   89]
Per-example loss in batch: 0.315304  [   84/   89]
Per-example loss in batch: 0.234153  [   86/   89]
Per-example loss in batch: 0.260537  [   88/   89]
Per-example loss in batch: 0.657241  [   89/   89]
Train Error: Avg loss: 0.26593050
validation Error: 
 Avg loss: 0.28279508 
 F1: 0.472840 
 Precision: 0.563782 
 Recall: 0.407162
 IoU: 0.309621

test Error: 
 Avg loss: 0.26366082 
 F1: 0.504022 
 Precision: 0.613362 
 Recall: 0.427767
 IoU: 0.336918

We have finished training iteration 227
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_225_.pth
Per-example loss in batch: 0.226765  [    2/   89]
Per-example loss in batch: 0.307995  [    4/   89]
Per-example loss in batch: 0.223500  [    6/   89]
Per-example loss in batch: 0.201824  [    8/   89]
Per-example loss in batch: 0.257418  [   10/   89]
Per-example loss in batch: 0.215547  [   12/   89]
Per-example loss in batch: 0.302681  [   14/   89]
Per-example loss in batch: 0.328109  [   16/   89]
Per-example loss in batch: 0.275191  [   18/   89]
Per-example loss in batch: 0.267661  [   20/   89]
Per-example loss in batch: 0.275691  [   22/   89]
Per-example loss in batch: 0.358846  [   24/   89]
Per-example loss in batch: 0.244905  [   26/   89]
Per-example loss in batch: 0.219091  [   28/   89]
Per-example loss in batch: 0.250459  [   30/   89]
Per-example loss in batch: 0.337985  [   32/   89]
Per-example loss in batch: 0.249914  [   34/   89]
Per-example loss in batch: 0.262893  [   36/   89]
Per-example loss in batch: 0.239866  [   38/   89]
Per-example loss in batch: 0.259382  [   40/   89]
Per-example loss in batch: 0.223221  [   42/   89]
Per-example loss in batch: 0.260689  [   44/   89]
Per-example loss in batch: 0.250056  [   46/   89]
Per-example loss in batch: 0.279729  [   48/   89]
Per-example loss in batch: 0.262224  [   50/   89]
Per-example loss in batch: 0.228089  [   52/   89]
Per-example loss in batch: 0.251914  [   54/   89]
Per-example loss in batch: 0.277704  [   56/   89]
Per-example loss in batch: 0.320579  [   58/   89]
Per-example loss in batch: 0.334502  [   60/   89]
Per-example loss in batch: 0.284424  [   62/   89]
Per-example loss in batch: 0.352379  [   64/   89]
Per-example loss in batch: 0.209492  [   66/   89]
Per-example loss in batch: 0.236914  [   68/   89]
Per-example loss in batch: 0.297658  [   70/   89]
Per-example loss in batch: 0.261232  [   72/   89]
Per-example loss in batch: 0.232524  [   74/   89]
Per-example loss in batch: 0.260844  [   76/   89]
Per-example loss in batch: 0.213309  [   78/   89]
Per-example loss in batch: 0.203108  [   80/   89]
Per-example loss in batch: 0.281200  [   82/   89]
Per-example loss in batch: 0.248600  [   84/   89]
Per-example loss in batch: 0.311496  [   86/   89]
Per-example loss in batch: 0.277238  [   88/   89]
Per-example loss in batch: 0.710417  [   89/   89]
Train Error: Avg loss: 0.27011364
validation Error: 
 Avg loss: 0.28456233 
 F1: 0.473863 
 Precision: 0.566105 
 Recall: 0.407469
 IoU: 0.310498

test Error: 
 Avg loss: 0.26224577 
 F1: 0.506992 
 Precision: 0.619592 
 Recall: 0.429025
 IoU: 0.339578

We have finished training iteration 228
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_226_.pth
Per-example loss in batch: 0.217053  [    2/   89]
Per-example loss in batch: 0.293659  [    4/   89]
Per-example loss in batch: 0.349336  [    6/   89]
Per-example loss in batch: 0.224366  [    8/   89]
Per-example loss in batch: 0.296419  [   10/   89]
Per-example loss in batch: 0.318413  [   12/   89]
Per-example loss in batch: 0.294395  [   14/   89]
Per-example loss in batch: 0.301393  [   16/   89]
Per-example loss in batch: 0.223766  [   18/   89]
Per-example loss in batch: 0.226881  [   20/   89]
Per-example loss in batch: 0.260135  [   22/   89]
Per-example loss in batch: 0.215599  [   24/   89]
Per-example loss in batch: 0.337737  [   26/   89]
Per-example loss in batch: 0.297559  [   28/   89]
Per-example loss in batch: 0.309112  [   30/   89]
Per-example loss in batch: 0.217992  [   32/   89]
Per-example loss in batch: 0.241185  [   34/   89]
Per-example loss in batch: 0.290910  [   36/   89]
Per-example loss in batch: 0.251634  [   38/   89]
Per-example loss in batch: 0.256771  [   40/   89]
Per-example loss in batch: 0.237937  [   42/   89]
Per-example loss in batch: 0.209318  [   44/   89]
Per-example loss in batch: 0.233576  [   46/   89]
Per-example loss in batch: 0.221576  [   48/   89]
Per-example loss in batch: 0.278441  [   50/   89]
Per-example loss in batch: 0.321985  [   52/   89]
Per-example loss in batch: 0.231601  [   54/   89]
Per-example loss in batch: 0.262444  [   56/   89]
Per-example loss in batch: 0.286345  [   58/   89]
Per-example loss in batch: 0.243671  [   60/   89]
Per-example loss in batch: 0.267833  [   62/   89]
Per-example loss in batch: 0.255044  [   64/   89]
Per-example loss in batch: 0.233595  [   66/   89]
Per-example loss in batch: 0.281494  [   68/   89]
Per-example loss in batch: 0.353317  [   70/   89]
Per-example loss in batch: 0.240374  [   72/   89]
Per-example loss in batch: 0.207939  [   74/   89]
Per-example loss in batch: 0.210096  [   76/   89]
Per-example loss in batch: 0.267187  [   78/   89]
Per-example loss in batch: 0.221591  [   80/   89]
Per-example loss in batch: 0.204285  [   82/   89]
Per-example loss in batch: 0.225335  [   84/   89]
Per-example loss in batch: 0.336469  [   86/   89]
Per-example loss in batch: 0.295045  [   88/   89]
Per-example loss in batch: 0.542302  [   89/   89]
Train Error: Avg loss: 0.26566215
validation Error: 
 Avg loss: 0.28232021 
 F1: 0.471342 
 Precision: 0.566594 
 Recall: 0.403507
 IoU: 0.308337

test Error: 
 Avg loss: 0.26441631 
 F1: 0.502526 
 Precision: 0.614713 
 Recall: 0.424968
 IoU: 0.335582

We have finished training iteration 229
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_227_.pth
Per-example loss in batch: 0.308316  [    2/   89]
Per-example loss in batch: 0.218221  [    4/   89]
Per-example loss in batch: 0.256912  [    6/   89]
Per-example loss in batch: 0.268907  [    8/   89]
Per-example loss in batch: 0.234851  [   10/   89]
Per-example loss in batch: 0.296224  [   12/   89]
Per-example loss in batch: 0.201876  [   14/   89]
Per-example loss in batch: 0.352452  [   16/   89]
Per-example loss in batch: 0.227087  [   18/   89]
Per-example loss in batch: 0.316500  [   20/   89]
Per-example loss in batch: 0.258420  [   22/   89]
Per-example loss in batch: 0.263259  [   24/   89]
Per-example loss in batch: 0.338962  [   26/   89]
Per-example loss in batch: 0.237621  [   28/   89]
Per-example loss in batch: 0.212514  [   30/   89]
Per-example loss in batch: 0.351356  [   32/   89]
Per-example loss in batch: 0.333683  [   34/   89]
Per-example loss in batch: 0.283468  [   36/   89]
Per-example loss in batch: 0.315073  [   38/   89]
Per-example loss in batch: 0.246254  [   40/   89]
Per-example loss in batch: 0.212522  [   42/   89]
Per-example loss in batch: 0.279085  [   44/   89]
Per-example loss in batch: 0.341796  [   46/   89]
Per-example loss in batch: 0.255251  [   48/   89]
Per-example loss in batch: 0.310932  [   50/   89]
Per-example loss in batch: 0.238937  [   52/   89]
Per-example loss in batch: 0.286256  [   54/   89]
Per-example loss in batch: 0.235659  [   56/   89]
Per-example loss in batch: 0.247248  [   58/   89]
Per-example loss in batch: 0.215147  [   60/   89]
Per-example loss in batch: 0.218040  [   62/   89]
Per-example loss in batch: 0.216399  [   64/   89]
Per-example loss in batch: 0.248432  [   66/   89]
Per-example loss in batch: 0.244456  [   68/   89]
Per-example loss in batch: 0.299275  [   70/   89]
Per-example loss in batch: 0.232323  [   72/   89]
Per-example loss in batch: 0.196231  [   74/   89]
Per-example loss in batch: 0.296088  [   76/   89]
Per-example loss in batch: 0.282740  [   78/   89]
Per-example loss in batch: 0.215332  [   80/   89]
Per-example loss in batch: 0.228082  [   82/   89]
Per-example loss in batch: 0.268378  [   84/   89]
Per-example loss in batch: 0.254820  [   86/   89]
Per-example loss in batch: 0.250721  [   88/   89]
Per-example loss in batch: 0.430372  [   89/   89]
Train Error: Avg loss: 0.26542234
validation Error: 
 Avg loss: 0.28255219 
 F1: 0.473355 
 Precision: 0.526766 
 Recall: 0.429778
 IoU: 0.310062

test Error: 
 Avg loss: 0.26103174 
 F1: 0.509564 
 Precision: 0.577454 
 Recall: 0.455958
 IoU: 0.341889

We have finished training iteration 230
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_228_.pth
Per-example loss in batch: 0.247807  [    2/   89]
Per-example loss in batch: 0.300520  [    4/   89]
Per-example loss in batch: 0.223104  [    6/   89]
Per-example loss in batch: 0.272674  [    8/   89]
Per-example loss in batch: 0.202939  [   10/   89]
Per-example loss in batch: 0.274463  [   12/   89]
Per-example loss in batch: 0.319448  [   14/   89]
Per-example loss in batch: 0.328404  [   16/   89]
Per-example loss in batch: 0.239555  [   18/   89]
Per-example loss in batch: 0.249940  [   20/   89]
Per-example loss in batch: 0.229122  [   22/   89]
Per-example loss in batch: 0.216928  [   24/   89]
Per-example loss in batch: 0.253250  [   26/   89]
Per-example loss in batch: 0.231625  [   28/   89]
Per-example loss in batch: 0.322058  [   30/   89]
Per-example loss in batch: 0.276923  [   32/   89]
Per-example loss in batch: 0.281522  [   34/   89]
Per-example loss in batch: 0.264040  [   36/   89]
Per-example loss in batch: 0.353291  [   38/   89]
Per-example loss in batch: 0.290822  [   40/   89]
Per-example loss in batch: 0.231648  [   42/   89]
Per-example loss in batch: 0.220449  [   44/   89]
Per-example loss in batch: 0.295801  [   46/   89]
Per-example loss in batch: 0.236541  [   48/   89]
Per-example loss in batch: 0.213662  [   50/   89]
Per-example loss in batch: 0.295202  [   52/   89]
Per-example loss in batch: 0.210066  [   54/   89]
Per-example loss in batch: 0.221022  [   56/   89]
Per-example loss in batch: 0.311209  [   58/   89]
Per-example loss in batch: 0.219131  [   60/   89]
Per-example loss in batch: 0.264238  [   62/   89]
Per-example loss in batch: 0.319731  [   64/   89]
Per-example loss in batch: 0.256762  [   66/   89]
Per-example loss in batch: 0.236881  [   68/   89]
Per-example loss in batch: 0.290891  [   70/   89]
Per-example loss in batch: 0.217560  [   72/   89]
Per-example loss in batch: 0.338009  [   74/   89]
Per-example loss in batch: 0.386259  [   76/   89]
Per-example loss in batch: 0.240540  [   78/   89]
Per-example loss in batch: 0.251182  [   80/   89]
Per-example loss in batch: 0.222453  [   82/   89]
Per-example loss in batch: 0.268936  [   84/   89]
Per-example loss in batch: 0.276277  [   86/   89]
Per-example loss in batch: 0.304208  [   88/   89]
Per-example loss in batch: 0.624882  [   89/   89]
Train Error: Avg loss: 0.27010193
validation Error: 
 Avg loss: 0.28949267 
 F1: 0.474210 
 Precision: 0.547071 
 Recall: 0.418476
 IoU: 0.310796

test Error: 
 Avg loss: 0.26081092 
 F1: 0.509653 
 Precision: 0.602615 
 Recall: 0.441539
 IoU: 0.341969

We have finished training iteration 231
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_229_.pth
Per-example loss in batch: 0.296216  [    2/   89]
Per-example loss in batch: 0.226544  [    4/   89]
Per-example loss in batch: 0.321712  [    6/   89]
Per-example loss in batch: 0.246725  [    8/   89]
Per-example loss in batch: 0.227337  [   10/   89]
Per-example loss in batch: 0.206722  [   12/   89]
Per-example loss in batch: 0.262594  [   14/   89]
Per-example loss in batch: 0.251388  [   16/   89]
Per-example loss in batch: 0.295243  [   18/   89]
Per-example loss in batch: 0.215864  [   20/   89]
Per-example loss in batch: 0.268866  [   22/   89]
Per-example loss in batch: 0.281780  [   24/   89]
Per-example loss in batch: 0.284984  [   26/   89]
Per-example loss in batch: 0.341534  [   28/   89]
Per-example loss in batch: 0.213170  [   30/   89]
Per-example loss in batch: 0.290635  [   32/   89]
Per-example loss in batch: 0.275152  [   34/   89]
Per-example loss in batch: 0.261712  [   36/   89]
Per-example loss in batch: 0.254863  [   38/   89]
Per-example loss in batch: 0.230470  [   40/   89]
Per-example loss in batch: 0.240607  [   42/   89]
Per-example loss in batch: 0.316166  [   44/   89]
Per-example loss in batch: 0.293583  [   46/   89]
Per-example loss in batch: 0.295016  [   48/   89]
Per-example loss in batch: 0.311686  [   50/   89]
Per-example loss in batch: 0.256188  [   52/   89]
Per-example loss in batch: 0.292238  [   54/   89]
Per-example loss in batch: 0.234110  [   56/   89]
Per-example loss in batch: 0.351940  [   58/   89]
Per-example loss in batch: 0.347962  [   60/   89]
Per-example loss in batch: 0.242029  [   62/   89]
Per-example loss in batch: 0.327931  [   64/   89]
Per-example loss in batch: 0.204147  [   66/   89]
Per-example loss in batch: 0.243420  [   68/   89]
Per-example loss in batch: 0.222611  [   70/   89]
Per-example loss in batch: 0.224651  [   72/   89]
Per-example loss in batch: 0.200942  [   74/   89]
Per-example loss in batch: 0.246693  [   76/   89]
Per-example loss in batch: 0.278255  [   78/   89]
Per-example loss in batch: 0.220862  [   80/   89]
Per-example loss in batch: 0.218009  [   82/   89]
Per-example loss in batch: 0.277534  [   84/   89]
Per-example loss in batch: 0.248515  [   86/   89]
Per-example loss in batch: 0.286303  [   88/   89]
Per-example loss in batch: 0.447315  [   89/   89]
Train Error: Avg loss: 0.26648467
validation Error: 
 Avg loss: 0.29003125 
 F1: 0.473645 
 Precision: 0.528397 
 Recall: 0.429174
 IoU: 0.310311

test Error: 
 Avg loss: 0.26051910 
 F1: 0.510543 
 Precision: 0.581380 
 Recall: 0.455093
 IoU: 0.342771

We have finished training iteration 232
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_230_.pth
Per-example loss in batch: 0.343540  [    2/   89]
Per-example loss in batch: 0.222196  [    4/   89]
Per-example loss in batch: 0.342233  [    6/   89]
Per-example loss in batch: 0.197036  [    8/   89]
Per-example loss in batch: 0.276463  [   10/   89]
Per-example loss in batch: 0.254039  [   12/   89]
Per-example loss in batch: 0.216996  [   14/   89]
Per-example loss in batch: 0.235421  [   16/   89]
Per-example loss in batch: 0.223012  [   18/   89]
Per-example loss in batch: 0.299235  [   20/   89]
Per-example loss in batch: 0.301218  [   22/   89]
Per-example loss in batch: 0.303250  [   24/   89]
Per-example loss in batch: 0.285537  [   26/   89]
Per-example loss in batch: 0.202030  [   28/   89]
Per-example loss in batch: 0.288163  [   30/   89]
Per-example loss in batch: 0.295884  [   32/   89]
Per-example loss in batch: 0.280400  [   34/   89]
Per-example loss in batch: 0.273884  [   36/   89]
Per-example loss in batch: 0.225206  [   38/   89]
Per-example loss in batch: 0.241532  [   40/   89]
Per-example loss in batch: 0.311418  [   42/   89]
Per-example loss in batch: 0.300132  [   44/   89]
Per-example loss in batch: 0.205422  [   46/   89]
Per-example loss in batch: 0.259519  [   48/   89]
Per-example loss in batch: 0.219694  [   50/   89]
Per-example loss in batch: 0.289914  [   52/   89]
Per-example loss in batch: 0.225169  [   54/   89]
Per-example loss in batch: 0.197841  [   56/   89]
Per-example loss in batch: 0.225518  [   58/   89]
Per-example loss in batch: 0.254951  [   60/   89]
Per-example loss in batch: 0.232186  [   62/   89]
Per-example loss in batch: 0.241067  [   64/   89]
Per-example loss in batch: 0.288268  [   66/   89]
Per-example loss in batch: 0.343172  [   68/   89]
Per-example loss in batch: 0.307468  [   70/   89]
Per-example loss in batch: 0.203493  [   72/   89]
Per-example loss in batch: 0.233457  [   74/   89]
Per-example loss in batch: 0.340482  [   76/   89]
Per-example loss in batch: 0.215919  [   78/   89]
Per-example loss in batch: 0.316048  [   80/   89]
Per-example loss in batch: 0.222590  [   82/   89]
Per-example loss in batch: 0.238688  [   84/   89]
Per-example loss in batch: 0.328233  [   86/   89]
Per-example loss in batch: 0.303349  [   88/   89]
Per-example loss in batch: 0.794070  [   89/   89]
Train Error: Avg loss: 0.26984967
validation Error: 
 Avg loss: 0.28104417 
 F1: 0.475679 
 Precision: 0.534296 
 Recall: 0.428651
 IoU: 0.312059

test Error: 
 Avg loss: 0.25919301 
 F1: 0.513412 
 Precision: 0.593259 
 Recall: 0.452509
 IoU: 0.345363

We have finished training iteration 233
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_231_.pth
Per-example loss in batch: 0.208250  [    2/   89]
Per-example loss in batch: 0.213670  [    4/   89]
Per-example loss in batch: 0.308760  [    6/   89]
Per-example loss in batch: 0.241513  [    8/   89]
Per-example loss in batch: 0.227867  [   10/   89]
Per-example loss in batch: 0.225040  [   12/   89]
Per-example loss in batch: 0.252022  [   14/   89]
Per-example loss in batch: 0.215339  [   16/   89]
Per-example loss in batch: 0.314498  [   18/   89]
Per-example loss in batch: 0.284880  [   20/   89]
Per-example loss in batch: 0.271783  [   22/   89]
Per-example loss in batch: 0.320828  [   24/   89]
Per-example loss in batch: 0.284681  [   26/   89]
Per-example loss in batch: 0.339567  [   28/   89]
Per-example loss in batch: 0.284076  [   30/   89]
Per-example loss in batch: 0.216706  [   32/   89]
Per-example loss in batch: 0.325548  [   34/   89]
Per-example loss in batch: 0.265615  [   36/   89]
Per-example loss in batch: 0.222668  [   38/   89]
Per-example loss in batch: 0.258864  [   40/   89]
Per-example loss in batch: 0.232909  [   42/   89]
Per-example loss in batch: 0.257962  [   44/   89]
Per-example loss in batch: 0.237380  [   46/   89]
Per-example loss in batch: 0.242104  [   48/   89]
Per-example loss in batch: 0.333571  [   50/   89]
Per-example loss in batch: 0.233358  [   52/   89]
Per-example loss in batch: 0.226838  [   54/   89]
Per-example loss in batch: 0.274520  [   56/   89]
Per-example loss in batch: 0.302238  [   58/   89]
Per-example loss in batch: 0.258381  [   60/   89]
Per-example loss in batch: 0.256555  [   62/   89]
Per-example loss in batch: 0.223194  [   64/   89]
Per-example loss in batch: 0.251957  [   66/   89]
Per-example loss in batch: 0.218861  [   68/   89]
Per-example loss in batch: 0.287135  [   70/   89]
Per-example loss in batch: 0.205056  [   72/   89]
Per-example loss in batch: 0.296413  [   74/   89]
Per-example loss in batch: 0.217360  [   76/   89]
Per-example loss in batch: 0.223990  [   78/   89]
Per-example loss in batch: 0.307246  [   80/   89]
Per-example loss in batch: 0.278614  [   82/   89]
Per-example loss in batch: 0.215194  [   84/   89]
Per-example loss in batch: 0.278587  [   86/   89]
Per-example loss in batch: 0.228449  [   88/   89]
Per-example loss in batch: 0.712645  [   89/   89]
Train Error: Avg loss: 0.26351388
validation Error: 
 Avg loss: 0.28281054 
 F1: 0.475134 
 Precision: 0.548149 
 Recall: 0.419284
 IoU: 0.311591

test Error: 
 Avg loss: 0.26102395 
 F1: 0.509420 
 Precision: 0.599887 
 Recall: 0.442664
 IoU: 0.341760

We have finished training iteration 234
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_232_.pth
Per-example loss in batch: 0.284396  [    2/   89]
Per-example loss in batch: 0.216710  [    4/   89]
Per-example loss in batch: 0.260439  [    6/   89]
Per-example loss in batch: 0.220516  [    8/   89]
Per-example loss in batch: 0.221033  [   10/   89]
Per-example loss in batch: 0.227423  [   12/   89]
Per-example loss in batch: 0.276091  [   14/   89]
Per-example loss in batch: 0.305343  [   16/   89]
Per-example loss in batch: 0.248611  [   18/   89]
Per-example loss in batch: 0.346274  [   20/   89]
Per-example loss in batch: 0.276766  [   22/   89]
Per-example loss in batch: 0.207943  [   24/   89]
Per-example loss in batch: 0.322464  [   26/   89]
Per-example loss in batch: 0.401586  [   28/   89]
Per-example loss in batch: 0.232742  [   30/   89]
Per-example loss in batch: 0.271362  [   32/   89]
Per-example loss in batch: 0.229460  [   34/   89]
Per-example loss in batch: 0.282757  [   36/   89]
Per-example loss in batch: 0.275954  [   38/   89]
Per-example loss in batch: 0.225796  [   40/   89]
Per-example loss in batch: 0.255798  [   42/   89]
Per-example loss in batch: 0.245278  [   44/   89]
Per-example loss in batch: 0.236930  [   46/   89]
Per-example loss in batch: 0.234041  [   48/   89]
Per-example loss in batch: 0.239031  [   50/   89]
Per-example loss in batch: 0.227662  [   52/   89]
Per-example loss in batch: 0.340760  [   54/   89]
Per-example loss in batch: 0.323542  [   56/   89]
Per-example loss in batch: 0.324829  [   58/   89]
Per-example loss in batch: 0.281604  [   60/   89]
Per-example loss in batch: 0.271196  [   62/   89]
Per-example loss in batch: 0.317063  [   64/   89]
Per-example loss in batch: 0.251386  [   66/   89]
Per-example loss in batch: 0.229724  [   68/   89]
Per-example loss in batch: 0.256183  [   70/   89]
Per-example loss in batch: 0.280525  [   72/   89]
Per-example loss in batch: 0.327572  [   74/   89]
Per-example loss in batch: 0.336483  [   76/   89]
Per-example loss in batch: 0.213157  [   78/   89]
Per-example loss in batch: 0.266556  [   80/   89]
Per-example loss in batch: 0.235996  [   82/   89]
Per-example loss in batch: 0.302846  [   84/   89]
Per-example loss in batch: 0.306458  [   86/   89]
Per-example loss in batch: 0.231447  [   88/   89]
Per-example loss in batch: 0.654242  [   89/   89]
Train Error: Avg loss: 0.27408662
validation Error: 
 Avg loss: 0.28218914 
 F1: 0.473542 
 Precision: 0.558385 
 Recall: 0.411081
 IoU: 0.310223

test Error: 
 Avg loss: 0.26268833 
 F1: 0.505815 
 Precision: 0.606590 
 Recall: 0.433754
 IoU: 0.338522

We have finished training iteration 235
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_233_.pth
Per-example loss in batch: 0.297013  [    2/   89]
Per-example loss in batch: 0.224645  [    4/   89]
Per-example loss in batch: 0.207604  [    6/   89]
Per-example loss in batch: 0.249974  [    8/   89]
Per-example loss in batch: 0.285549  [   10/   89]
Per-example loss in batch: 0.334623  [   12/   89]
Per-example loss in batch: 0.276403  [   14/   89]
Per-example loss in batch: 0.259260  [   16/   89]
Per-example loss in batch: 0.193777  [   18/   89]
Per-example loss in batch: 0.239723  [   20/   89]
Per-example loss in batch: 0.296189  [   22/   89]
Per-example loss in batch: 0.249820  [   24/   89]
Per-example loss in batch: 0.257041  [   26/   89]
Per-example loss in batch: 0.259681  [   28/   89]
Per-example loss in batch: 0.245746  [   30/   89]
Per-example loss in batch: 0.218165  [   32/   89]
Per-example loss in batch: 0.344254  [   34/   89]
Per-example loss in batch: 0.260980  [   36/   89]
Per-example loss in batch: 0.228676  [   38/   89]
Per-example loss in batch: 0.323740  [   40/   89]
Per-example loss in batch: 0.196198  [   42/   89]
Per-example loss in batch: 0.284089  [   44/   89]
Per-example loss in batch: 0.269067  [   46/   89]
Per-example loss in batch: 0.226649  [   48/   89]
Per-example loss in batch: 0.233697  [   50/   89]
Per-example loss in batch: 0.279239  [   52/   89]
Per-example loss in batch: 0.247395  [   54/   89]
Per-example loss in batch: 0.186346  [   56/   89]
Per-example loss in batch: 0.359118  [   58/   89]
Per-example loss in batch: 0.287533  [   60/   89]
Per-example loss in batch: 0.265868  [   62/   89]
Per-example loss in batch: 0.330698  [   64/   89]
Per-example loss in batch: 0.322911  [   66/   89]
Per-example loss in batch: 0.292129  [   68/   89]
Per-example loss in batch: 0.232738  [   70/   89]
Per-example loss in batch: 0.289782  [   72/   89]
Per-example loss in batch: 0.259021  [   74/   89]
Per-example loss in batch: 0.240793  [   76/   89]
Per-example loss in batch: 0.236506  [   78/   89]
Per-example loss in batch: 0.227880  [   80/   89]
Per-example loss in batch: 0.322252  [   82/   89]
Per-example loss in batch: 0.290491  [   84/   89]
Per-example loss in batch: 0.298208  [   86/   89]
Per-example loss in batch: 0.245665  [   88/   89]
Per-example loss in batch: 0.442654  [   89/   89]
Train Error: Avg loss: 0.26738114
validation Error: 
 Avg loss: 0.28383174 
 F1: 0.473965 
 Precision: 0.547014 
 Recall: 0.418129
 IoU: 0.310586

test Error: 
 Avg loss: 0.26115939 
 F1: 0.509047 
 Precision: 0.598834 
 Recall: 0.442674
 IoU: 0.341424

We have finished training iteration 236
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_234_.pth
Per-example loss in batch: 0.272196  [    2/   89]
Per-example loss in batch: 0.258616  [    4/   89]
Per-example loss in batch: 0.328905  [    6/   89]
Per-example loss in batch: 0.209045  [    8/   89]
Per-example loss in batch: 0.298421  [   10/   89]
Per-example loss in batch: 0.209003  [   12/   89]
Per-example loss in batch: 0.256363  [   14/   89]
Per-example loss in batch: 0.210772  [   16/   89]
Per-example loss in batch: 0.328783  [   18/   89]
Per-example loss in batch: 0.340920  [   20/   89]
Per-example loss in batch: 0.260256  [   22/   89]
Per-example loss in batch: 0.254746  [   24/   89]
Per-example loss in batch: 0.301017  [   26/   89]
Per-example loss in batch: 0.251287  [   28/   89]
Per-example loss in batch: 0.267031  [   30/   89]
Per-example loss in batch: 0.228130  [   32/   89]
Per-example loss in batch: 0.268511  [   34/   89]
Per-example loss in batch: 0.285081  [   36/   89]
Per-example loss in batch: 0.205224  [   38/   89]
Per-example loss in batch: 0.240960  [   40/   89]
Per-example loss in batch: 0.274460  [   42/   89]
Per-example loss in batch: 0.289985  [   44/   89]
Per-example loss in batch: 0.309754  [   46/   89]
Per-example loss in batch: 0.276737  [   48/   89]
Per-example loss in batch: 0.269393  [   50/   89]
Per-example loss in batch: 0.337795  [   52/   89]
Per-example loss in batch: 0.219356  [   54/   89]
Per-example loss in batch: 0.336635  [   56/   89]
Per-example loss in batch: 0.255804  [   58/   89]
Per-example loss in batch: 0.264582  [   60/   89]
Per-example loss in batch: 0.256987  [   62/   89]
Per-example loss in batch: 0.215596  [   64/   89]
Per-example loss in batch: 0.372191  [   66/   89]
Per-example loss in batch: 0.280247  [   68/   89]
Per-example loss in batch: 0.198276  [   70/   89]
Per-example loss in batch: 0.286861  [   72/   89]
Per-example loss in batch: 0.224657  [   74/   89]
Per-example loss in batch: 0.200474  [   76/   89]
Per-example loss in batch: 0.343605  [   78/   89]
Per-example loss in batch: 0.264173  [   80/   89]
Per-example loss in batch: 0.240131  [   82/   89]
Per-example loss in batch: 0.329823  [   84/   89]
Per-example loss in batch: 0.346169  [   86/   89]
Per-example loss in batch: 0.265029  [   88/   89]
Per-example loss in batch: 0.666087  [   89/   89]
Train Error: Avg loss: 0.27566360
validation Error: 
 Avg loss: 0.27941274 
 F1: 0.474169 
 Precision: 0.547077 
 Recall: 0.418409
 IoU: 0.310761

test Error: 
 Avg loss: 0.26075364 
 F1: 0.510005 
 Precision: 0.602200 
 Recall: 0.442291
 IoU: 0.342286

We have finished training iteration 237
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_235_.pth
Per-example loss in batch: 0.306164  [    2/   89]
Per-example loss in batch: 0.213367  [    4/   89]
Per-example loss in batch: 0.255581  [    6/   89]
Per-example loss in batch: 0.246489  [    8/   89]
Per-example loss in batch: 0.212782  [   10/   89]
Per-example loss in batch: 0.225030  [   12/   89]
Per-example loss in batch: 0.222564  [   14/   89]
Per-example loss in batch: 0.313254  [   16/   89]
Per-example loss in batch: 0.295511  [   18/   89]
Per-example loss in batch: 0.269594  [   20/   89]
Per-example loss in batch: 0.283582  [   22/   89]
Per-example loss in batch: 0.206662  [   24/   89]
Per-example loss in batch: 0.257283  [   26/   89]
Per-example loss in batch: 0.277536  [   28/   89]
Per-example loss in batch: 0.256350  [   30/   89]
Per-example loss in batch: 0.340218  [   32/   89]
Per-example loss in batch: 0.206442  [   34/   89]
Per-example loss in batch: 0.216570  [   36/   89]
Per-example loss in batch: 0.289471  [   38/   89]
Per-example loss in batch: 0.277563  [   40/   89]
Per-example loss in batch: 0.256498  [   42/   89]
Per-example loss in batch: 0.216106  [   44/   89]
Per-example loss in batch: 0.243386  [   46/   89]
Per-example loss in batch: 0.221054  [   48/   89]
Per-example loss in batch: 0.352728  [   50/   89]
Per-example loss in batch: 0.216581  [   52/   89]
Per-example loss in batch: 0.265990  [   54/   89]
Per-example loss in batch: 0.210538  [   56/   89]
Per-example loss in batch: 0.292125  [   58/   89]
Per-example loss in batch: 0.243402  [   60/   89]
Per-example loss in batch: 0.285525  [   62/   89]
Per-example loss in batch: 0.297965  [   64/   89]
Per-example loss in batch: 0.293104  [   66/   89]
Per-example loss in batch: 0.251786  [   68/   89]
Per-example loss in batch: 0.328040  [   70/   89]
Per-example loss in batch: 0.251845  [   72/   89]
Per-example loss in batch: 0.301015  [   74/   89]
Per-example loss in batch: 0.214129  [   76/   89]
Per-example loss in batch: 0.255074  [   78/   89]
Per-example loss in batch: 0.230353  [   80/   89]
Per-example loss in batch: 0.275828  [   82/   89]
Per-example loss in batch: 0.210324  [   84/   89]
Per-example loss in batch: 0.268483  [   86/   89]
Per-example loss in batch: 0.290556  [   88/   89]
Per-example loss in batch: 0.546944  [   89/   89]
Train Error: Avg loss: 0.26332399
validation Error: 
 Avg loss: 0.28446978 
 F1: 0.474772 
 Precision: 0.535475 
 Recall: 0.426430
 IoU: 0.311279

test Error: 
 Avg loss: 0.26073733 
 F1: 0.509631 
 Precision: 0.584808 
 Recall: 0.451580
 IoU: 0.341949

We have finished training iteration 238
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_236_.pth
Per-example loss in batch: 0.231533  [    2/   89]
Per-example loss in batch: 0.213448  [    4/   89]
Per-example loss in batch: 0.281992  [    6/   89]
Per-example loss in batch: 0.341830  [    8/   89]
Per-example loss in batch: 0.239589  [   10/   89]
Per-example loss in batch: 0.248398  [   12/   89]
Per-example loss in batch: 0.292742  [   14/   89]
Per-example loss in batch: 0.225128  [   16/   89]
Per-example loss in batch: 0.283344  [   18/   89]
Per-example loss in batch: 0.276287  [   20/   89]
Per-example loss in batch: 0.228019  [   22/   89]
Per-example loss in batch: 0.253301  [   24/   89]
Per-example loss in batch: 0.274356  [   26/   89]
Per-example loss in batch: 0.240914  [   28/   89]
Per-example loss in batch: 0.229541  [   30/   89]
Per-example loss in batch: 0.213597  [   32/   89]
Per-example loss in batch: 0.234469  [   34/   89]
Per-example loss in batch: 0.206991  [   36/   89]
Per-example loss in batch: 0.239268  [   38/   89]
Per-example loss in batch: 0.246435  [   40/   89]
Per-example loss in batch: 0.291320  [   42/   89]
Per-example loss in batch: 0.328271  [   44/   89]
Per-example loss in batch: 0.241378  [   46/   89]
Per-example loss in batch: 0.232117  [   48/   89]
Per-example loss in batch: 0.278598  [   50/   89]
Per-example loss in batch: 0.226092  [   52/   89]
Per-example loss in batch: 0.219940  [   54/   89]
Per-example loss in batch: 0.258180  [   56/   89]
Per-example loss in batch: 0.272249  [   58/   89]
Per-example loss in batch: 0.214723  [   60/   89]
Per-example loss in batch: 0.314684  [   62/   89]
Per-example loss in batch: 0.352191  [   64/   89]
Per-example loss in batch: 0.296700  [   66/   89]
Per-example loss in batch: 0.308800  [   68/   89]
Per-example loss in batch: 0.295088  [   70/   89]
Per-example loss in batch: 0.326124  [   72/   89]
Per-example loss in batch: 0.322916  [   74/   89]
Per-example loss in batch: 0.250667  [   76/   89]
Per-example loss in batch: 0.297680  [   78/   89]
Per-example loss in batch: 0.290957  [   80/   89]
Per-example loss in batch: 0.202642  [   82/   89]
Per-example loss in batch: 0.208864  [   84/   89]
Per-example loss in batch: 0.238520  [   86/   89]
Per-example loss in batch: 0.302033  [   88/   89]
Per-example loss in batch: 0.557544  [   89/   89]
Train Error: Avg loss: 0.26630756
validation Error: 
 Avg loss: 0.28212129 
 F1: 0.474048 
 Precision: 0.521878 
 Recall: 0.434249
 IoU: 0.310657

test Error: 
 Avg loss: 0.25997160 
 F1: 0.511368 
 Precision: 0.572540 
 Recall: 0.462007
 IoU: 0.343516

We have finished training iteration 239
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_237_.pth
Per-example loss in batch: 0.283423  [    2/   89]
Per-example loss in batch: 0.204122  [    4/   89]
Per-example loss in batch: 0.336460  [    6/   89]
Per-example loss in batch: 0.283469  [    8/   89]
Per-example loss in batch: 0.308145  [   10/   89]
Per-example loss in batch: 0.228629  [   12/   89]
Per-example loss in batch: 0.216732  [   14/   89]
Per-example loss in batch: 0.247648  [   16/   89]
Per-example loss in batch: 0.234086  [   18/   89]
Per-example loss in batch: 0.235087  [   20/   89]
Per-example loss in batch: 0.234145  [   22/   89]
Per-example loss in batch: 0.273508  [   24/   89]
Per-example loss in batch: 0.254202  [   26/   89]
Per-example loss in batch: 0.255254  [   28/   89]
Per-example loss in batch: 0.361191  [   30/   89]
Per-example loss in batch: 0.306977  [   32/   89]
Per-example loss in batch: 0.235585  [   34/   89]
Per-example loss in batch: 0.222650  [   36/   89]
Per-example loss in batch: 0.231043  [   38/   89]
Per-example loss in batch: 0.258188  [   40/   89]
Per-example loss in batch: 0.237469  [   42/   89]
Per-example loss in batch: 0.299799  [   44/   89]
Per-example loss in batch: 0.281294  [   46/   89]
Per-example loss in batch: 0.248244  [   48/   89]
Per-example loss in batch: 0.194391  [   50/   89]
Per-example loss in batch: 0.247516  [   52/   89]
Per-example loss in batch: 0.295149  [   54/   89]
Per-example loss in batch: 0.275639  [   56/   89]
Per-example loss in batch: 0.276850  [   58/   89]
Per-example loss in batch: 0.277246  [   60/   89]
Per-example loss in batch: 0.284274  [   62/   89]
Per-example loss in batch: 0.205554  [   64/   89]
Per-example loss in batch: 0.236837  [   66/   89]
Per-example loss in batch: 0.231594  [   68/   89]
Per-example loss in batch: 0.272306  [   70/   89]
Per-example loss in batch: 0.319764  [   72/   89]
Per-example loss in batch: 0.228636  [   74/   89]
Per-example loss in batch: 0.244031  [   76/   89]
Per-example loss in batch: 0.306387  [   78/   89]
Per-example loss in batch: 0.238947  [   80/   89]
Per-example loss in batch: 0.343826  [   82/   89]
Per-example loss in batch: 0.259966  [   84/   89]
Per-example loss in batch: 0.204475  [   86/   89]
Per-example loss in batch: 0.318698  [   88/   89]
Per-example loss in batch: 0.650251  [   89/   89]
Train Error: Avg loss: 0.26661936
validation Error: 
 Avg loss: 0.28457257 
 F1: 0.471682 
 Precision: 0.544471 
 Recall: 0.416060
 IoU: 0.308628

test Error: 
 Avg loss: 0.26310300 
 F1: 0.505297 
 Precision: 0.592498 
 Recall: 0.440471
 IoU: 0.338059

We have finished training iteration 240
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_238_.pth
Per-example loss in batch: 0.265137  [    2/   89]
Per-example loss in batch: 0.226312  [    4/   89]
Per-example loss in batch: 0.294456  [    6/   89]
Per-example loss in batch: 0.231148  [    8/   89]
Per-example loss in batch: 0.252867  [   10/   89]
Per-example loss in batch: 0.327878  [   12/   89]
Per-example loss in batch: 0.395496  [   14/   89]
Per-example loss in batch: 0.253449  [   16/   89]
Per-example loss in batch: 0.231553  [   18/   89]
Per-example loss in batch: 0.291355  [   20/   89]
Per-example loss in batch: 0.225796  [   22/   89]
Per-example loss in batch: 0.301116  [   24/   89]
Per-example loss in batch: 0.200094  [   26/   89]
Per-example loss in batch: 0.247108  [   28/   89]
Per-example loss in batch: 0.215684  [   30/   89]
Per-example loss in batch: 0.232981  [   32/   89]
Per-example loss in batch: 0.319510  [   34/   89]
Per-example loss in batch: 0.287110  [   36/   89]
Per-example loss in batch: 0.201281  [   38/   89]
Per-example loss in batch: 0.272575  [   40/   89]
Per-example loss in batch: 0.220711  [   42/   89]
Per-example loss in batch: 0.248483  [   44/   89]
Per-example loss in batch: 0.248505  [   46/   89]
Per-example loss in batch: 0.228515  [   48/   89]
Per-example loss in batch: 0.253043  [   50/   89]
Per-example loss in batch: 0.316997  [   52/   89]
Per-example loss in batch: 0.276680  [   54/   89]
Per-example loss in batch: 0.230412  [   56/   89]
Per-example loss in batch: 0.254616  [   58/   89]
Per-example loss in batch: 0.356180  [   60/   89]
Per-example loss in batch: 0.282832  [   62/   89]
Per-example loss in batch: 0.289763  [   64/   89]
Per-example loss in batch: 0.264709  [   66/   89]
Per-example loss in batch: 0.250629  [   68/   89]
Per-example loss in batch: 0.223856  [   70/   89]
Per-example loss in batch: 0.287721  [   72/   89]
Per-example loss in batch: 0.282415  [   74/   89]
Per-example loss in batch: 0.307801  [   76/   89]
Per-example loss in batch: 0.301806  [   78/   89]
Per-example loss in batch: 0.259393  [   80/   89]
Per-example loss in batch: 0.237349  [   82/   89]
Per-example loss in batch: 0.266184  [   84/   89]
Per-example loss in batch: 0.198672  [   86/   89]
Per-example loss in batch: 0.249077  [   88/   89]
Per-example loss in batch: 0.504094  [   89/   89]
Train Error: Avg loss: 0.26654604
validation Error: 
 Avg loss: 0.28968826 
 F1: 0.474413 
 Precision: 0.529690 
 Recall: 0.429583
 IoU: 0.310971

test Error: 
 Avg loss: 0.25990446 
 F1: 0.511654 
 Precision: 0.584996 
 Recall: 0.454654
 IoU: 0.343774

We have finished training iteration 241
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_239_.pth
Per-example loss in batch: 0.250469  [    2/   89]
Per-example loss in batch: 0.205190  [    4/   89]
Per-example loss in batch: 0.228237  [    6/   89]
Per-example loss in batch: 0.310447  [    8/   89]
Per-example loss in batch: 0.300779  [   10/   89]
Per-example loss in batch: 0.275400  [   12/   89]
Per-example loss in batch: 0.238236  [   14/   89]
Per-example loss in batch: 0.232849  [   16/   89]
Per-example loss in batch: 0.325639  [   18/   89]
Per-example loss in batch: 0.308875  [   20/   89]
Per-example loss in batch: 0.288572  [   22/   89]
Per-example loss in batch: 0.230705  [   24/   89]
Per-example loss in batch: 0.200199  [   26/   89]
Per-example loss in batch: 0.256646  [   28/   89]
Per-example loss in batch: 0.310372  [   30/   89]
Per-example loss in batch: 0.310582  [   32/   89]
Per-example loss in batch: 0.226916  [   34/   89]
Per-example loss in batch: 0.282926  [   36/   89]
Per-example loss in batch: 0.307055  [   38/   89]
Per-example loss in batch: 0.212196  [   40/   89]
Per-example loss in batch: 0.294253  [   42/   89]
Per-example loss in batch: 0.307170  [   44/   89]
Per-example loss in batch: 0.236493  [   46/   89]
Per-example loss in batch: 0.331448  [   48/   89]
Per-example loss in batch: 0.242991  [   50/   89]
Per-example loss in batch: 0.210132  [   52/   89]
Per-example loss in batch: 0.221430  [   54/   89]
Per-example loss in batch: 0.235240  [   56/   89]
Per-example loss in batch: 0.311032  [   58/   89]
Per-example loss in batch: 0.253269  [   60/   89]
Per-example loss in batch: 0.350137  [   62/   89]
Per-example loss in batch: 0.229358  [   64/   89]
Per-example loss in batch: 0.260631  [   66/   89]
Per-example loss in batch: 0.304951  [   68/   89]
Per-example loss in batch: 0.211212  [   70/   89]
Per-example loss in batch: 0.280518  [   72/   89]
Per-example loss in batch: 0.222721  [   74/   89]
Per-example loss in batch: 0.264215  [   76/   89]
Per-example loss in batch: 0.289552  [   78/   89]
Per-example loss in batch: 0.356971  [   80/   89]
Per-example loss in batch: 0.232311  [   82/   89]
Per-example loss in batch: 0.299854  [   84/   89]
Per-example loss in batch: 0.221445  [   86/   89]
Per-example loss in batch: 0.291168  [   88/   89]
Per-example loss in batch: 0.413651  [   89/   89]
Train Error: Avg loss: 0.26893520
validation Error: 
 Avg loss: 0.28540643 
 F1: 0.474092 
 Precision: 0.554351 
 Recall: 0.414133
 IoU: 0.310695

test Error: 
 Avg loss: 0.26157326 
 F1: 0.508399 
 Precision: 0.606524 
 Recall: 0.437602
 IoU: 0.340841

We have finished training iteration 242
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_240_.pth
Per-example loss in batch: 0.212419  [    2/   89]
Per-example loss in batch: 0.271455  [    4/   89]
Per-example loss in batch: 0.263869  [    6/   89]
Per-example loss in batch: 0.240157  [    8/   89]
Per-example loss in batch: 0.273672  [   10/   89]
Per-example loss in batch: 0.228702  [   12/   89]
Per-example loss in batch: 0.231724  [   14/   89]
Per-example loss in batch: 0.205603  [   16/   89]
Per-example loss in batch: 0.306124  [   18/   89]
Per-example loss in batch: 0.295649  [   20/   89]
Per-example loss in batch: 0.255616  [   22/   89]
Per-example loss in batch: 0.251334  [   24/   89]
Per-example loss in batch: 0.216474  [   26/   89]
Per-example loss in batch: 0.227894  [   28/   89]
Per-example loss in batch: 0.245846  [   30/   89]
Per-example loss in batch: 0.274895  [   32/   89]
Per-example loss in batch: 0.297070  [   34/   89]
Per-example loss in batch: 0.320870  [   36/   89]
Per-example loss in batch: 0.275177  [   38/   89]
Per-example loss in batch: 0.248854  [   40/   89]
Per-example loss in batch: 0.289475  [   42/   89]
Per-example loss in batch: 0.265820  [   44/   89]
Per-example loss in batch: 0.199683  [   46/   89]
Per-example loss in batch: 0.247044  [   48/   89]
Per-example loss in batch: 0.243619  [   50/   89]
Per-example loss in batch: 0.325059  [   52/   89]
Per-example loss in batch: 0.295171  [   54/   89]
Per-example loss in batch: 0.333235  [   56/   89]
Per-example loss in batch: 0.235918  [   58/   89]
Per-example loss in batch: 0.260460  [   60/   89]
Per-example loss in batch: 0.234045  [   62/   89]
Per-example loss in batch: 0.284142  [   64/   89]
Per-example loss in batch: 0.262298  [   66/   89]
Per-example loss in batch: 0.218193  [   68/   89]
Per-example loss in batch: 0.222669  [   70/   89]
Per-example loss in batch: 0.292156  [   72/   89]
Per-example loss in batch: 0.265085  [   74/   89]
Per-example loss in batch: 0.244293  [   76/   89]
Per-example loss in batch: 0.251005  [   78/   89]
Per-example loss in batch: 0.315473  [   80/   89]
Per-example loss in batch: 0.223145  [   82/   89]
Per-example loss in batch: 0.312242  [   84/   89]
Per-example loss in batch: 0.275025  [   86/   89]
Per-example loss in batch: 0.252411  [   88/   89]
Per-example loss in batch: 0.452027  [   89/   89]
Train Error: Avg loss: 0.26330524
validation Error: 
 Avg loss: 0.28626041 
 F1: 0.473322 
 Precision: 0.530720 
 Recall: 0.427127
 IoU: 0.310034

test Error: 
 Avg loss: 0.26062404 
 F1: 0.510849 
 Precision: 0.583929 
 Recall: 0.454027
 IoU: 0.343048

We have finished training iteration 243
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_241_.pth
Per-example loss in batch: 0.217353  [    2/   89]
Per-example loss in batch: 0.231367  [    4/   89]
Per-example loss in batch: 0.322712  [    6/   89]
Per-example loss in batch: 0.336769  [    8/   89]
Per-example loss in batch: 0.263462  [   10/   89]
Per-example loss in batch: 0.265221  [   12/   89]
Per-example loss in batch: 0.348107  [   14/   89]
Per-example loss in batch: 0.228225  [   16/   89]
Per-example loss in batch: 0.251963  [   18/   89]
Per-example loss in batch: 0.239498  [   20/   89]
Per-example loss in batch: 0.345165  [   22/   89]
Per-example loss in batch: 0.219304  [   24/   89]
Per-example loss in batch: 0.283124  [   26/   89]
Per-example loss in batch: 0.216709  [   28/   89]
Per-example loss in batch: 0.229900  [   30/   89]
Per-example loss in batch: 0.243429  [   32/   89]
Per-example loss in batch: 0.211281  [   34/   89]
Per-example loss in batch: 0.354978  [   36/   89]
Per-example loss in batch: 0.247193  [   38/   89]
Per-example loss in batch: 0.302344  [   40/   89]
Per-example loss in batch: 0.283928  [   42/   89]
Per-example loss in batch: 0.278156  [   44/   89]
Per-example loss in batch: 0.234833  [   46/   89]
Per-example loss in batch: 0.350822  [   48/   89]
Per-example loss in batch: 0.253508  [   50/   89]
Per-example loss in batch: 0.303553  [   52/   89]
Per-example loss in batch: 0.255297  [   54/   89]
Per-example loss in batch: 0.323747  [   56/   89]
Per-example loss in batch: 0.266899  [   58/   89]
Per-example loss in batch: 0.228469  [   60/   89]
Per-example loss in batch: 0.218142  [   62/   89]
Per-example loss in batch: 0.203787  [   64/   89]
Per-example loss in batch: 0.224142  [   66/   89]
Per-example loss in batch: 0.358050  [   68/   89]
Per-example loss in batch: 0.268535  [   70/   89]
Per-example loss in batch: 0.293442  [   72/   89]
Per-example loss in batch: 0.302548  [   74/   89]
Per-example loss in batch: 0.332217  [   76/   89]
Per-example loss in batch: 0.224362  [   78/   89]
Per-example loss in batch: 0.285459  [   80/   89]
Per-example loss in batch: 0.275428  [   82/   89]
Per-example loss in batch: 0.262414  [   84/   89]
Per-example loss in batch: 0.232375  [   86/   89]
Per-example loss in batch: 0.251918  [   88/   89]
Per-example loss in batch: 0.648514  [   89/   89]
Train Error: Avg loss: 0.27403125
validation Error: 
 Avg loss: 0.28187237 
 F1: 0.472503 
 Precision: 0.553581 
 Recall: 0.412140
 IoU: 0.309332

test Error: 
 Avg loss: 0.26248157 
 F1: 0.506795 
 Precision: 0.603547 
 Recall: 0.436777
 IoU: 0.339400

We have finished training iteration 244
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_242_.pth
Per-example loss in batch: 0.313923  [    2/   89]
Per-example loss in batch: 0.256240  [    4/   89]
Per-example loss in batch: 0.258581  [    6/   89]
Per-example loss in batch: 0.248395  [    8/   89]
Per-example loss in batch: 0.243102  [   10/   89]
Per-example loss in batch: 0.252833  [   12/   89]
Per-example loss in batch: 0.316252  [   14/   89]
Per-example loss in batch: 0.242055  [   16/   89]
Per-example loss in batch: 0.231046  [   18/   89]
Per-example loss in batch: 0.206657  [   20/   89]
Per-example loss in batch: 0.208815  [   22/   89]
Per-example loss in batch: 0.224378  [   24/   89]
Per-example loss in batch: 0.226215  [   26/   89]
Per-example loss in batch: 0.306453  [   28/   89]
Per-example loss in batch: 0.230411  [   30/   89]
Per-example loss in batch: 0.274248  [   32/   89]
Per-example loss in batch: 0.249138  [   34/   89]
Per-example loss in batch: 0.295336  [   36/   89]
Per-example loss in batch: 0.258593  [   38/   89]
Per-example loss in batch: 0.215691  [   40/   89]
Per-example loss in batch: 0.266671  [   42/   89]
Per-example loss in batch: 0.240208  [   44/   89]
Per-example loss in batch: 0.215197  [   46/   89]
Per-example loss in batch: 0.209261  [   48/   89]
Per-example loss in batch: 0.238422  [   50/   89]
Per-example loss in batch: 0.316235  [   52/   89]
Per-example loss in batch: 0.267443  [   54/   89]
Per-example loss in batch: 0.291458  [   56/   89]
Per-example loss in batch: 0.321038  [   58/   89]
Per-example loss in batch: 0.310117  [   60/   89]
Per-example loss in batch: 0.320729  [   62/   89]
Per-example loss in batch: 0.238696  [   64/   89]
Per-example loss in batch: 0.232314  [   66/   89]
Per-example loss in batch: 0.229615  [   68/   89]
Per-example loss in batch: 0.222761  [   70/   89]
Per-example loss in batch: 0.330933  [   72/   89]
Per-example loss in batch: 0.220321  [   74/   89]
Per-example loss in batch: 0.250789  [   76/   89]
Per-example loss in batch: 0.248209  [   78/   89]
Per-example loss in batch: 0.241091  [   80/   89]
Per-example loss in batch: 0.272022  [   82/   89]
Per-example loss in batch: 0.277977  [   84/   89]
Per-example loss in batch: 0.278314  [   86/   89]
Per-example loss in batch: 0.295232  [   88/   89]
Per-example loss in batch: 0.402937  [   89/   89]
Train Error: Avg loss: 0.26055912
validation Error: 
 Avg loss: 0.28950666 
 F1: 0.474324 
 Precision: 0.547474 
 Recall: 0.418418
 IoU: 0.310894

test Error: 
 Avg loss: 0.26163214 
 F1: 0.508292 
 Precision: 0.599712 
 Recall: 0.441057
 IoU: 0.340745

We have finished training iteration 245
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_243_.pth
Per-example loss in batch: 0.237017  [    2/   89]
Per-example loss in batch: 0.228644  [    4/   89]
Per-example loss in batch: 0.250630  [    6/   89]
Per-example loss in batch: 0.304748  [    8/   89]
Per-example loss in batch: 0.224077  [   10/   89]
Per-example loss in batch: 0.236412  [   12/   89]
Per-example loss in batch: 0.236210  [   14/   89]
Per-example loss in batch: 0.304498  [   16/   89]
Per-example loss in batch: 0.223179  [   18/   89]
Per-example loss in batch: 0.350570  [   20/   89]
Per-example loss in batch: 0.227630  [   22/   89]
Per-example loss in batch: 0.208006  [   24/   89]
Per-example loss in batch: 0.244115  [   26/   89]
Per-example loss in batch: 0.346165  [   28/   89]
Per-example loss in batch: 0.234681  [   30/   89]
Per-example loss in batch: 0.264322  [   32/   89]
Per-example loss in batch: 0.300622  [   34/   89]
Per-example loss in batch: 0.240260  [   36/   89]
Per-example loss in batch: 0.265758  [   38/   89]
Per-example loss in batch: 0.271064  [   40/   89]
Per-example loss in batch: 0.234404  [   42/   89]
Per-example loss in batch: 0.219299  [   44/   89]
Per-example loss in batch: 0.340068  [   46/   89]
Per-example loss in batch: 0.258315  [   48/   89]
Per-example loss in batch: 0.232753  [   50/   89]
Per-example loss in batch: 0.239737  [   52/   89]
Per-example loss in batch: 0.313450  [   54/   89]
Per-example loss in batch: 0.280806  [   56/   89]
Per-example loss in batch: 0.231961  [   58/   89]
Per-example loss in batch: 0.249164  [   60/   89]
Per-example loss in batch: 0.332810  [   62/   89]
Per-example loss in batch: 0.233356  [   64/   89]
Per-example loss in batch: 0.254194  [   66/   89]
Per-example loss in batch: 0.266936  [   68/   89]
Per-example loss in batch: 0.263095  [   70/   89]
Per-example loss in batch: 0.227793  [   72/   89]
Per-example loss in batch: 0.262503  [   74/   89]
Per-example loss in batch: 0.213579  [   76/   89]
Per-example loss in batch: 0.243787  [   78/   89]
Per-example loss in batch: 0.233302  [   80/   89]
Per-example loss in batch: 0.285433  [   82/   89]
Per-example loss in batch: 0.248250  [   84/   89]
Per-example loss in batch: 0.281221  [   86/   89]
Per-example loss in batch: 0.345529  [   88/   89]
Per-example loss in batch: 0.497718  [   89/   89]
Train Error: Avg loss: 0.26380250
validation Error: 
 Avg loss: 0.28410400 
 F1: 0.474669 
 Precision: 0.551965 
 Recall: 0.416362
 IoU: 0.311191

test Error: 
 Avg loss: 0.26086296 
 F1: 0.510427 
 Precision: 0.604653 
 Recall: 0.441610
 IoU: 0.342667

We have finished training iteration 246
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_244_.pth
Per-example loss in batch: 0.298943  [    2/   89]
Per-example loss in batch: 0.318797  [    4/   89]
Per-example loss in batch: 0.302386  [    6/   89]
Per-example loss in batch: 0.252117  [    8/   89]
Per-example loss in batch: 0.300974  [   10/   89]
Per-example loss in batch: 0.225404  [   12/   89]
Per-example loss in batch: 0.233505  [   14/   89]
Per-example loss in batch: 0.284817  [   16/   89]
Per-example loss in batch: 0.200557  [   18/   89]
Per-example loss in batch: 0.334181  [   20/   89]
Per-example loss in batch: 0.247539  [   22/   89]
Per-example loss in batch: 0.270234  [   24/   89]
Per-example loss in batch: 0.278649  [   26/   89]
Per-example loss in batch: 0.293428  [   28/   89]
Per-example loss in batch: 0.226380  [   30/   89]
Per-example loss in batch: 0.280911  [   32/   89]
Per-example loss in batch: 0.290150  [   34/   89]
Per-example loss in batch: 0.288549  [   36/   89]
Per-example loss in batch: 0.347855  [   38/   89]
Per-example loss in batch: 0.267248  [   40/   89]
Per-example loss in batch: 0.214369  [   42/   89]
Per-example loss in batch: 0.308659  [   44/   89]
Per-example loss in batch: 0.361172  [   46/   89]
Per-example loss in batch: 0.234576  [   48/   89]
Per-example loss in batch: 0.222012  [   50/   89]
Per-example loss in batch: 0.255848  [   52/   89]
Per-example loss in batch: 0.267552  [   54/   89]
Per-example loss in batch: 0.264680  [   56/   89]
Per-example loss in batch: 0.214651  [   58/   89]
Per-example loss in batch: 0.210236  [   60/   89]
Per-example loss in batch: 0.210020  [   62/   89]
Per-example loss in batch: 0.236141  [   64/   89]
Per-example loss in batch: 0.269234  [   66/   89]
Per-example loss in batch: 0.243814  [   68/   89]
Per-example loss in batch: 0.268423  [   70/   89]
Per-example loss in batch: 0.251011  [   72/   89]
Per-example loss in batch: 0.201636  [   74/   89]
Per-example loss in batch: 0.300536  [   76/   89]
Per-example loss in batch: 0.218139  [   78/   89]
Per-example loss in batch: 0.369713  [   80/   89]
Per-example loss in batch: 0.218783  [   82/   89]
Per-example loss in batch: 0.296970  [   84/   89]
Per-example loss in batch: 0.243449  [   86/   89]
Per-example loss in batch: 0.282549  [   88/   89]
Per-example loss in batch: 0.457924  [   89/   89]
Train Error: Avg loss: 0.26821936
validation Error: 
 Avg loss: 0.28774552 
 F1: 0.474702 
 Precision: 0.534254 
 Recall: 0.427095
 IoU: 0.311219

test Error: 
 Avg loss: 0.25992391 
 F1: 0.511931 
 Precision: 0.588327 
 Recall: 0.453096
 IoU: 0.344024

We have finished training iteration 247
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_245_.pth
Per-example loss in batch: 0.203391  [    2/   89]
Per-example loss in batch: 0.217752  [    4/   89]
Per-example loss in batch: 0.316666  [    6/   89]
Per-example loss in batch: 0.255575  [    8/   89]
Per-example loss in batch: 0.319345  [   10/   89]
Per-example loss in batch: 0.287832  [   12/   89]
Per-example loss in batch: 0.272078  [   14/   89]
Per-example loss in batch: 0.311639  [   16/   89]
Per-example loss in batch: 0.304297  [   18/   89]
Per-example loss in batch: 0.364819  [   20/   89]
Per-example loss in batch: 0.247233  [   22/   89]
Per-example loss in batch: 0.189394  [   24/   89]
Per-example loss in batch: 0.288408  [   26/   89]
Per-example loss in batch: 0.253797  [   28/   89]
Per-example loss in batch: 0.232530  [   30/   89]
Per-example loss in batch: 0.338095  [   32/   89]
Per-example loss in batch: 0.231129  [   34/   89]
Per-example loss in batch: 0.232984  [   36/   89]
Per-example loss in batch: 0.268779  [   38/   89]
Per-example loss in batch: 0.292175  [   40/   89]
Per-example loss in batch: 0.325202  [   42/   89]
Per-example loss in batch: 0.271690  [   44/   89]
Per-example loss in batch: 0.259037  [   46/   89]
Per-example loss in batch: 0.220962  [   48/   89]
Per-example loss in batch: 0.237534  [   50/   89]
Per-example loss in batch: 0.226158  [   52/   89]
Per-example loss in batch: 0.217545  [   54/   89]
Per-example loss in batch: 0.267283  [   56/   89]
Per-example loss in batch: 0.248348  [   58/   89]
Per-example loss in batch: 0.227020  [   60/   89]
Per-example loss in batch: 0.259669  [   62/   89]
Per-example loss in batch: 0.284839  [   64/   89]
Per-example loss in batch: 0.213232  [   66/   89]
Per-example loss in batch: 0.310162  [   68/   89]
Per-example loss in batch: 0.245513  [   70/   89]
Per-example loss in batch: 0.252493  [   72/   89]
Per-example loss in batch: 0.298751  [   74/   89]
Per-example loss in batch: 0.246062  [   76/   89]
Per-example loss in batch: 0.250313  [   78/   89]
Per-example loss in batch: 0.288079  [   80/   89]
Per-example loss in batch: 0.245667  [   82/   89]
Per-example loss in batch: 0.320856  [   84/   89]
Per-example loss in batch: 0.212889  [   86/   89]
Per-example loss in batch: 0.276532  [   88/   89]
Per-example loss in batch: 0.477112  [   89/   89]
Train Error: Avg loss: 0.26679352
validation Error: 
 Avg loss: 0.28305370 
 F1: 0.473968 
 Precision: 0.558354 
 Recall: 0.411740
 IoU: 0.310588

test Error: 
 Avg loss: 0.26151411 
 F1: 0.508919 
 Precision: 0.614730 
 Recall: 0.434185
 IoU: 0.341309

We have finished training iteration 248
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_246_.pth
Per-example loss in batch: 0.290037  [    2/   89]
Per-example loss in batch: 0.239246  [    4/   89]
Per-example loss in batch: 0.239975  [    6/   89]
Per-example loss in batch: 0.225351  [    8/   89]
Per-example loss in batch: 0.207258  [   10/   89]
Per-example loss in batch: 0.254226  [   12/   89]
Per-example loss in batch: 0.243202  [   14/   89]
Per-example loss in batch: 0.276851  [   16/   89]
Per-example loss in batch: 0.263563  [   18/   89]
Per-example loss in batch: 0.221226  [   20/   89]
Per-example loss in batch: 0.296505  [   22/   89]
Per-example loss in batch: 0.224269  [   24/   89]
Per-example loss in batch: 0.270093  [   26/   89]
Per-example loss in batch: 0.198977  [   28/   89]
Per-example loss in batch: 0.275997  [   30/   89]
Per-example loss in batch: 0.240652  [   32/   89]
Per-example loss in batch: 0.285013  [   34/   89]
Per-example loss in batch: 0.267241  [   36/   89]
Per-example loss in batch: 0.232936  [   38/   89]
Per-example loss in batch: 0.280651  [   40/   89]
Per-example loss in batch: 0.328238  [   42/   89]
Per-example loss in batch: 0.215136  [   44/   89]
Per-example loss in batch: 0.190509  [   46/   89]
Per-example loss in batch: 0.224229  [   48/   89]
Per-example loss in batch: 0.223825  [   50/   89]
Per-example loss in batch: 0.309158  [   52/   89]
Per-example loss in batch: 0.235708  [   54/   89]
Per-example loss in batch: 0.237774  [   56/   89]
Per-example loss in batch: 0.275150  [   58/   89]
Per-example loss in batch: 0.298872  [   60/   89]
Per-example loss in batch: 0.274522  [   62/   89]
Per-example loss in batch: 0.255941  [   64/   89]
Per-example loss in batch: 0.274593  [   66/   89]
Per-example loss in batch: 0.324299  [   68/   89]
Per-example loss in batch: 0.276350  [   70/   89]
Per-example loss in batch: 0.238781  [   72/   89]
Per-example loss in batch: 0.292116  [   74/   89]
Per-example loss in batch: 0.326717  [   76/   89]
Per-example loss in batch: 0.275710  [   78/   89]
Per-example loss in batch: 0.320234  [   80/   89]
Per-example loss in batch: 0.242003  [   82/   89]
Per-example loss in batch: 0.306935  [   84/   89]
Per-example loss in batch: 0.289713  [   86/   89]
Per-example loss in batch: 0.255270  [   88/   89]
Per-example loss in batch: 0.425142  [   89/   89]
Train Error: Avg loss: 0.26376691
validation Error: 
 Avg loss: 0.28574020 
 F1: 0.473133 
 Precision: 0.551659 
 Recall: 0.414177
 IoU: 0.309872

test Error: 
 Avg loss: 0.26184156 
 F1: 0.507844 
 Precision: 0.606550 
 Recall: 0.436767
 IoU: 0.340342

We have finished training iteration 249
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_247_.pth
Per-example loss in batch: 0.222968  [    2/   89]
Per-example loss in batch: 0.264030  [    4/   89]
Per-example loss in batch: 0.262021  [    6/   89]
Per-example loss in batch: 0.225759  [    8/   89]
Per-example loss in batch: 0.244804  [   10/   89]
Per-example loss in batch: 0.217910  [   12/   89]
Per-example loss in batch: 0.327375  [   14/   89]
Per-example loss in batch: 0.290386  [   16/   89]
Per-example loss in batch: 0.300655  [   18/   89]
Per-example loss in batch: 0.261546  [   20/   89]
Per-example loss in batch: 0.230378  [   22/   89]
Per-example loss in batch: 0.279703  [   24/   89]
Per-example loss in batch: 0.283373  [   26/   89]
Per-example loss in batch: 0.199933  [   28/   89]
Per-example loss in batch: 0.247862  [   30/   89]
Per-example loss in batch: 0.284719  [   32/   89]
Per-example loss in batch: 0.290548  [   34/   89]
Per-example loss in batch: 0.316622  [   36/   89]
Per-example loss in batch: 0.307791  [   38/   89]
Per-example loss in batch: 0.278928  [   40/   89]
Per-example loss in batch: 0.282595  [   42/   89]
Per-example loss in batch: 0.309260  [   44/   89]
Per-example loss in batch: 0.248875  [   46/   89]
Per-example loss in batch: 0.314287  [   48/   89]
Per-example loss in batch: 0.243993  [   50/   89]
Per-example loss in batch: 0.239369  [   52/   89]
Per-example loss in batch: 0.270376  [   54/   89]
Per-example loss in batch: 0.237586  [   56/   89]
Per-example loss in batch: 0.242933  [   58/   89]
Per-example loss in batch: 0.232425  [   60/   89]
Per-example loss in batch: 0.260971  [   62/   89]
Per-example loss in batch: 0.247942  [   64/   89]
Per-example loss in batch: 0.302791  [   66/   89]
Per-example loss in batch: 0.304835  [   68/   89]
Per-example loss in batch: 0.319449  [   70/   89]
Per-example loss in batch: 0.221565  [   72/   89]
Per-example loss in batch: 0.226324  [   74/   89]
Per-example loss in batch: 0.283245  [   76/   89]
Per-example loss in batch: 0.202976  [   78/   89]
Per-example loss in batch: 0.255664  [   80/   89]
Per-example loss in batch: 0.238707  [   82/   89]
Per-example loss in batch: 0.296069  [   84/   89]
Per-example loss in batch: 0.228155  [   86/   89]
Per-example loss in batch: 0.285332  [   88/   89]
Per-example loss in batch: 0.482918  [   89/   89]
Train Error: Avg loss: 0.26684255
validation Error: 
 Avg loss: 0.28344946 
 F1: 0.472847 
 Precision: 0.518146 
 Recall: 0.434831
 IoU: 0.309626

test Error: 
 Avg loss: 0.25994948 
 F1: 0.512172 
 Precision: 0.572146 
 Recall: 0.463578
 IoU: 0.344241

We have finished training iteration 250
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_248_.pth
Per-example loss in batch: 0.321109  [    2/   89]
Per-example loss in batch: 0.285208  [    4/   89]
Per-example loss in batch: 0.228895  [    6/   89]
Per-example loss in batch: 0.229203  [    8/   89]
Per-example loss in batch: 0.219937  [   10/   89]
Per-example loss in batch: 0.290391  [   12/   89]
Per-example loss in batch: 0.236647  [   14/   89]
Per-example loss in batch: 0.294819  [   16/   89]
Per-example loss in batch: 0.235075  [   18/   89]
Per-example loss in batch: 0.212629  [   20/   89]
Per-example loss in batch: 0.309023  [   22/   89]
Per-example loss in batch: 0.218647  [   24/   89]
Per-example loss in batch: 0.243437  [   26/   89]
Per-example loss in batch: 0.281528  [   28/   89]
Per-example loss in batch: 0.233868  [   30/   89]
Per-example loss in batch: 0.221762  [   32/   89]
Per-example loss in batch: 0.242115  [   34/   89]
Per-example loss in batch: 0.204514  [   36/   89]
Per-example loss in batch: 0.288851  [   38/   89]
Per-example loss in batch: 0.316977  [   40/   89]
Per-example loss in batch: 0.244035  [   42/   89]
Per-example loss in batch: 0.242837  [   44/   89]
Per-example loss in batch: 0.387158  [   46/   89]
Per-example loss in batch: 0.233888  [   48/   89]
Per-example loss in batch: 0.340279  [   50/   89]
Per-example loss in batch: 0.243409  [   52/   89]
Per-example loss in batch: 0.202956  [   54/   89]
Per-example loss in batch: 0.251559  [   56/   89]
Per-example loss in batch: 0.244322  [   58/   89]
Per-example loss in batch: 0.221680  [   60/   89]
Per-example loss in batch: 0.253571  [   62/   89]
Per-example loss in batch: 0.329319  [   64/   89]
Per-example loss in batch: 0.385710  [   66/   89]
Per-example loss in batch: 0.334263  [   68/   89]
Per-example loss in batch: 0.278871  [   70/   89]
Per-example loss in batch: 0.275847  [   72/   89]
Per-example loss in batch: 0.221692  [   74/   89]
Per-example loss in batch: 0.292977  [   76/   89]
Per-example loss in batch: 0.276202  [   78/   89]
Per-example loss in batch: 0.218357  [   80/   89]
Per-example loss in batch: 0.230114  [   82/   89]
Per-example loss in batch: 0.271085  [   84/   89]
Per-example loss in batch: 0.295608  [   86/   89]
Per-example loss in batch: 0.251813  [   88/   89]
Per-example loss in batch: 0.616695  [   89/   89]
Train Error: Avg loss: 0.26855137
validation Error: 
 Avg loss: 0.28635581 
 F1: 0.472881 
 Precision: 0.546906 
 Recall: 0.416506
 IoU: 0.309656

test Error: 
 Avg loss: 0.26159945 
 F1: 0.508961 
 Precision: 0.602231 
 Recall: 0.440707
 IoU: 0.341347

We have finished training iteration 251
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_249_.pth
Per-example loss in batch: 0.278083  [    2/   89]
Per-example loss in batch: 0.231720  [    4/   89]
Per-example loss in batch: 0.274147  [    6/   89]
Per-example loss in batch: 0.295253  [    8/   89]
Per-example loss in batch: 0.308604  [   10/   89]
Per-example loss in batch: 0.232946  [   12/   89]
Per-example loss in batch: 0.215912  [   14/   89]
Per-example loss in batch: 0.242675  [   16/   89]
Per-example loss in batch: 0.267356  [   18/   89]
Per-example loss in batch: 0.284537  [   20/   89]
Per-example loss in batch: 0.296345  [   22/   89]
Per-example loss in batch: 0.273638  [   24/   89]
Per-example loss in batch: 0.211993  [   26/   89]
Per-example loss in batch: 0.264582  [   28/   89]
Per-example loss in batch: 0.215693  [   30/   89]
Per-example loss in batch: 0.352611  [   32/   89]
Per-example loss in batch: 0.235376  [   34/   89]
Per-example loss in batch: 0.295224  [   36/   89]
Per-example loss in batch: 0.231966  [   38/   89]
Per-example loss in batch: 0.250427  [   40/   89]
Per-example loss in batch: 0.220812  [   42/   89]
Per-example loss in batch: 0.238231  [   44/   89]
Per-example loss in batch: 0.306578  [   46/   89]
Per-example loss in batch: 0.380612  [   48/   89]
Per-example loss in batch: 0.243617  [   50/   89]
Per-example loss in batch: 0.227355  [   52/   89]
Per-example loss in batch: 0.221762  [   54/   89]
Per-example loss in batch: 0.252870  [   56/   89]
Per-example loss in batch: 0.280551  [   58/   89]
Per-example loss in batch: 0.216468  [   60/   89]
Per-example loss in batch: 0.255368  [   62/   89]
Per-example loss in batch: 0.226516  [   64/   89]
Per-example loss in batch: 0.297094  [   66/   89]
Per-example loss in batch: 0.299106  [   68/   89]
Per-example loss in batch: 0.307863  [   70/   89]
Per-example loss in batch: 0.255931  [   72/   89]
Per-example loss in batch: 0.270423  [   74/   89]
Per-example loss in batch: 0.220038  [   76/   89]
Per-example loss in batch: 0.331393  [   78/   89]
Per-example loss in batch: 0.273199  [   80/   89]
Per-example loss in batch: 0.202851  [   82/   89]
Per-example loss in batch: 0.200543  [   84/   89]
Per-example loss in batch: 0.209931  [   86/   89]
Per-example loss in batch: 0.277834  [   88/   89]
Per-example loss in batch: 0.543487  [   89/   89]
Train Error: Avg loss: 0.26399499
validation Error: 
 Avg loss: 0.28040750 
 F1: 0.474837 
 Precision: 0.543632 
 Recall: 0.421499
 IoU: 0.311336

test Error: 
 Avg loss: 0.26103899 
 F1: 0.509681 
 Precision: 0.599690 
 Recall: 0.443165
 IoU: 0.341994

We have finished training iteration 252
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_250_.pth
Per-example loss in batch: 0.278045  [    2/   89]
Per-example loss in batch: 0.316415  [    4/   89]
Per-example loss in batch: 0.220232  [    6/   89]
Per-example loss in batch: 0.261099  [    8/   89]
Per-example loss in batch: 0.244875  [   10/   89]
Per-example loss in batch: 0.223116  [   12/   89]
Per-example loss in batch: 0.318325  [   14/   89]
Per-example loss in batch: 0.308513  [   16/   89]
Per-example loss in batch: 0.210524  [   18/   89]
Per-example loss in batch: 0.261320  [   20/   89]
Per-example loss in batch: 0.261720  [   22/   89]
Per-example loss in batch: 0.218593  [   24/   89]
Per-example loss in batch: 0.234953  [   26/   89]
Per-example loss in batch: 0.194768  [   28/   89]
Per-example loss in batch: 0.284145  [   30/   89]
Per-example loss in batch: 0.329622  [   32/   89]
Per-example loss in batch: 0.230956  [   34/   89]
Per-example loss in batch: 0.231150  [   36/   89]
Per-example loss in batch: 0.267275  [   38/   89]
Per-example loss in batch: 0.262163  [   40/   89]
Per-example loss in batch: 0.264692  [   42/   89]
Per-example loss in batch: 0.211159  [   44/   89]
Per-example loss in batch: 0.246255  [   46/   89]
Per-example loss in batch: 0.268617  [   48/   89]
Per-example loss in batch: 0.222047  [   50/   89]
Per-example loss in batch: 0.218701  [   52/   89]
Per-example loss in batch: 0.278761  [   54/   89]
Per-example loss in batch: 0.289286  [   56/   89]
Per-example loss in batch: 0.220642  [   58/   89]
Per-example loss in batch: 0.230962  [   60/   89]
Per-example loss in batch: 0.298593  [   62/   89]
Per-example loss in batch: 0.229431  [   64/   89]
Per-example loss in batch: 0.304534  [   66/   89]
Per-example loss in batch: 0.298391  [   68/   89]
Per-example loss in batch: 0.280536  [   70/   89]
Per-example loss in batch: 0.213750  [   72/   89]
Per-example loss in batch: 0.357101  [   74/   89]
Per-example loss in batch: 0.295424  [   76/   89]
Per-example loss in batch: 0.286209  [   78/   89]
Per-example loss in batch: 0.297420  [   80/   89]
Per-example loss in batch: 0.212818  [   82/   89]
Per-example loss in batch: 0.304512  [   84/   89]
Per-example loss in batch: 0.281643  [   86/   89]
Per-example loss in batch: 0.235768  [   88/   89]
Per-example loss in batch: 0.414011  [   89/   89]
Train Error: Avg loss: 0.26319242
validation Error: 
 Avg loss: 0.28654019 
 F1: 0.472423 
 Precision: 0.539655 
 Recall: 0.420088
 IoU: 0.309263

test Error: 
 Avg loss: 0.26152031 
 F1: 0.508793 
 Precision: 0.593049 
 Recall: 0.445500
 IoU: 0.341196

We have finished training iteration 253
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_251_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.267099  [    2/   89]
Per-example loss in batch: 0.221538  [    4/   89]
Per-example loss in batch: 0.245863  [    6/   89]
Per-example loss in batch: 0.216104  [    8/   89]
Per-example loss in batch: 0.283391  [   10/   89]
Per-example loss in batch: 0.245878  [   12/   89]
Per-example loss in batch: 0.256265  [   14/   89]
Per-example loss in batch: 0.363988  [   16/   89]
Per-example loss in batch: 0.302516  [   18/   89]
Per-example loss in batch: 0.300980  [   20/   89]
Per-example loss in batch: 0.268347  [   22/   89]
Per-example loss in batch: 0.307550  [   24/   89]
Per-example loss in batch: 0.270848  [   26/   89]
Per-example loss in batch: 0.292848  [   28/   89]
Per-example loss in batch: 0.240557  [   30/   89]
Per-example loss in batch: 0.319502  [   32/   89]
Per-example loss in batch: 0.255473  [   34/   89]
Per-example loss in batch: 0.345877  [   36/   89]
Per-example loss in batch: 0.294873  [   38/   89]
Per-example loss in batch: 0.320091  [   40/   89]
Per-example loss in batch: 0.209387  [   42/   89]
Per-example loss in batch: 0.244285  [   44/   89]
Per-example loss in batch: 0.297524  [   46/   89]
Per-example loss in batch: 0.238078  [   48/   89]
Per-example loss in batch: 0.206178  [   50/   89]
Per-example loss in batch: 0.236578  [   52/   89]
Per-example loss in batch: 0.255898  [   54/   89]
Per-example loss in batch: 0.290261  [   56/   89]
Per-example loss in batch: 0.229699  [   58/   89]
Per-example loss in batch: 0.201402  [   60/   89]
Per-example loss in batch: 0.266875  [   62/   89]
Per-example loss in batch: 0.265907  [   64/   89]
Per-example loss in batch: 0.231480  [   66/   89]
Per-example loss in batch: 0.215187  [   68/   89]
Per-example loss in batch: 0.197708  [   70/   89]
Per-example loss in batch: 0.260354  [   72/   89]
Per-example loss in batch: 0.269330  [   74/   89]
Per-example loss in batch: 0.242672  [   76/   89]
Per-example loss in batch: 0.258812  [   78/   89]
Per-example loss in batch: 0.207381  [   80/   89]
Per-example loss in batch: 0.316435  [   82/   89]
Per-example loss in batch: 0.257080  [   84/   89]
Per-example loss in batch: 0.270789  [   86/   89]
Per-example loss in batch: 0.291971  [   88/   89]
Per-example loss in batch: 0.548452  [   89/   89]
Train Error: Avg loss: 0.26640642
validation Error: 
 Avg loss: 0.28838329 
 F1: 0.474595 
 Precision: 0.538678 
 Recall: 0.424138
 IoU: 0.311127

test Error: 
 Avg loss: 0.26065336 
 F1: 0.510083 
 Precision: 0.592866 
 Recall: 0.447586
 IoU: 0.342357

We have finished training iteration 254
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_252_.pth
Per-example loss in batch: 0.292641  [    2/   89]
Per-example loss in batch: 0.211432  [    4/   89]
Per-example loss in batch: 0.213504  [    6/   89]
Per-example loss in batch: 0.213198  [    8/   89]
Per-example loss in batch: 0.255855  [   10/   89]
Per-example loss in batch: 0.203371  [   12/   89]
Per-example loss in batch: 0.247302  [   14/   89]
Per-example loss in batch: 0.260038  [   16/   89]
Per-example loss in batch: 0.232235  [   18/   89]
Per-example loss in batch: 0.374997  [   20/   89]
Per-example loss in batch: 0.282264  [   22/   89]
Per-example loss in batch: 0.247539  [   24/   89]
Per-example loss in batch: 0.249283  [   26/   89]
Per-example loss in batch: 0.319514  [   28/   89]
Per-example loss in batch: 0.256187  [   30/   89]
Per-example loss in batch: 0.217905  [   32/   89]
Per-example loss in batch: 0.269495  [   34/   89]
Per-example loss in batch: 0.263812  [   36/   89]
Per-example loss in batch: 0.321212  [   38/   89]
Per-example loss in batch: 0.206985  [   40/   89]
Per-example loss in batch: 0.241780  [   42/   89]
Per-example loss in batch: 0.298481  [   44/   89]
Per-example loss in batch: 0.283321  [   46/   89]
Per-example loss in batch: 0.281047  [   48/   89]
Per-example loss in batch: 0.260644  [   50/   89]
Per-example loss in batch: 0.269738  [   52/   89]
Per-example loss in batch: 0.246780  [   54/   89]
Per-example loss in batch: 0.226411  [   56/   89]
Per-example loss in batch: 0.253478  [   58/   89]
Per-example loss in batch: 0.219905  [   60/   89]
Per-example loss in batch: 0.309695  [   62/   89]
Per-example loss in batch: 0.214935  [   64/   89]
Per-example loss in batch: 0.267297  [   66/   89]
Per-example loss in batch: 0.218585  [   68/   89]
Per-example loss in batch: 0.293443  [   70/   89]
Per-example loss in batch: 0.235878  [   72/   89]
Per-example loss in batch: 0.243311  [   74/   89]
Per-example loss in batch: 0.258240  [   76/   89]
Per-example loss in batch: 0.274663  [   78/   89]
Per-example loss in batch: 0.245096  [   80/   89]
Per-example loss in batch: 0.265626  [   82/   89]
Per-example loss in batch: 0.312989  [   84/   89]
Per-example loss in batch: 0.314703  [   86/   89]
Per-example loss in batch: 0.331299  [   88/   89]
Per-example loss in batch: 0.518193  [   89/   89]
Train Error: Avg loss: 0.26438679
validation Error: 
 Avg loss: 0.28576395 
 F1: 0.474193 
 Precision: 0.540151 
 Recall: 0.422590
 IoU: 0.310782

test Error: 
 Avg loss: 0.25979622 
 F1: 0.512086 
 Precision: 0.595952 
 Recall: 0.448912
 IoU: 0.344163

We have finished training iteration 255
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_253_.pth
Per-example loss in batch: 0.334972  [    2/   89]
Per-example loss in batch: 0.212110  [    4/   89]
Per-example loss in batch: 0.225335  [    6/   89]
Per-example loss in batch: 0.278726  [    8/   89]
Per-example loss in batch: 0.370144  [   10/   89]
Per-example loss in batch: 0.230975  [   12/   89]
Per-example loss in batch: 0.209391  [   14/   89]
Per-example loss in batch: 0.311097  [   16/   89]
Per-example loss in batch: 0.275199  [   18/   89]
Per-example loss in batch: 0.214682  [   20/   89]
Per-example loss in batch: 0.242070  [   22/   89]
Per-example loss in batch: 0.208963  [   24/   89]
Per-example loss in batch: 0.228342  [   26/   89]
Per-example loss in batch: 0.259069  [   28/   89]
Per-example loss in batch: 0.267155  [   30/   89]
Per-example loss in batch: 0.247554  [   32/   89]
Per-example loss in batch: 0.245219  [   34/   89]
Per-example loss in batch: 0.303312  [   36/   89]
Per-example loss in batch: 0.339582  [   38/   89]
Per-example loss in batch: 0.296987  [   40/   89]
Per-example loss in batch: 0.259007  [   42/   89]
Per-example loss in batch: 0.218335  [   44/   89]
Per-example loss in batch: 0.207360  [   46/   89]
Per-example loss in batch: 0.211440  [   48/   89]
Per-example loss in batch: 0.333564  [   50/   89]
Per-example loss in batch: 0.263206  [   52/   89]
Per-example loss in batch: 0.244931  [   54/   89]
Per-example loss in batch: 0.272853  [   56/   89]
Per-example loss in batch: 0.226825  [   58/   89]
Per-example loss in batch: 0.217148  [   60/   89]
Per-example loss in batch: 0.306903  [   62/   89]
Per-example loss in batch: 0.346144  [   64/   89]
Per-example loss in batch: 0.244302  [   66/   89]
Per-example loss in batch: 0.209434  [   68/   89]
Per-example loss in batch: 0.261270  [   70/   89]
Per-example loss in batch: 0.249937  [   72/   89]
Per-example loss in batch: 0.341642  [   74/   89]
Per-example loss in batch: 0.272187  [   76/   89]
Per-example loss in batch: 0.276154  [   78/   89]
Per-example loss in batch: 0.255455  [   80/   89]
Per-example loss in batch: 0.273184  [   82/   89]
Per-example loss in batch: 0.245369  [   84/   89]
Per-example loss in batch: 0.247575  [   86/   89]
Per-example loss in batch: 0.246281  [   88/   89]
Per-example loss in batch: 0.500167  [   89/   89]
Train Error: Avg loss: 0.26475225
validation Error: 
 Avg loss: 0.28190711 
 F1: 0.474056 
 Precision: 0.538180 
 Recall: 0.423585
 IoU: 0.310664

test Error: 
 Avg loss: 0.26072146 
 F1: 0.510434 
 Precision: 0.591825 
 Recall: 0.448723
 IoU: 0.342673

We have finished training iteration 256
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_254_.pth
Per-example loss in batch: 0.333015  [    2/   89]
Per-example loss in batch: 0.330421  [    4/   89]
Per-example loss in batch: 0.228086  [    6/   89]
Per-example loss in batch: 0.289946  [    8/   89]
Per-example loss in batch: 0.233832  [   10/   89]
Per-example loss in batch: 0.263068  [   12/   89]
Per-example loss in batch: 0.229047  [   14/   89]
Per-example loss in batch: 0.281411  [   16/   89]
Per-example loss in batch: 0.236637  [   18/   89]
Per-example loss in batch: 0.303357  [   20/   89]
Per-example loss in batch: 0.260987  [   22/   89]
Per-example loss in batch: 0.277236  [   24/   89]
Per-example loss in batch: 0.321086  [   26/   89]
Per-example loss in batch: 0.242668  [   28/   89]
Per-example loss in batch: 0.252406  [   30/   89]
Per-example loss in batch: 0.254975  [   32/   89]
Per-example loss in batch: 0.228048  [   34/   89]
Per-example loss in batch: 0.216963  [   36/   89]
Per-example loss in batch: 0.235523  [   38/   89]
Per-example loss in batch: 0.224718  [   40/   89]
Per-example loss in batch: 0.211267  [   42/   89]
Per-example loss in batch: 0.301558  [   44/   89]
Per-example loss in batch: 0.304942  [   46/   89]
Per-example loss in batch: 0.241215  [   48/   89]
Per-example loss in batch: 0.241580  [   50/   89]
Per-example loss in batch: 0.211119  [   52/   89]
Per-example loss in batch: 0.215281  [   54/   89]
Per-example loss in batch: 0.238982  [   56/   89]
Per-example loss in batch: 0.240573  [   58/   89]
Per-example loss in batch: 0.271860  [   60/   89]
Per-example loss in batch: 0.240787  [   62/   89]
Per-example loss in batch: 0.263343  [   64/   89]
Per-example loss in batch: 0.205378  [   66/   89]
Per-example loss in batch: 0.243443  [   68/   89]
Per-example loss in batch: 0.270045  [   70/   89]
Per-example loss in batch: 0.215834  [   72/   89]
Per-example loss in batch: 0.225430  [   74/   89]
Per-example loss in batch: 0.296702  [   76/   89]
Per-example loss in batch: 0.342229  [   78/   89]
Per-example loss in batch: 0.299083  [   80/   89]
Per-example loss in batch: 0.236464  [   82/   89]
Per-example loss in batch: 0.349480  [   84/   89]
Per-example loss in batch: 0.263911  [   86/   89]
Per-example loss in batch: 0.236716  [   88/   89]
Per-example loss in batch: 0.560099  [   89/   89]
Train Error: Avg loss: 0.26271238
validation Error: 
 Avg loss: 0.28311919 
 F1: 0.474538 
 Precision: 0.536368 
 Recall: 0.425490
 IoU: 0.311079

test Error: 
 Avg loss: 0.26006707 
 F1: 0.511605 
 Precision: 0.592236 
 Recall: 0.450299
 IoU: 0.343730

We have finished training iteration 257
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_255_.pth
Per-example loss in batch: 0.283278  [    2/   89]
Per-example loss in batch: 0.232659  [    4/   89]
Per-example loss in batch: 0.321632  [    6/   89]
Per-example loss in batch: 0.281873  [    8/   89]
Per-example loss in batch: 0.260666  [   10/   89]
Per-example loss in batch: 0.245312  [   12/   89]
Per-example loss in batch: 0.322761  [   14/   89]
Per-example loss in batch: 0.228271  [   16/   89]
Per-example loss in batch: 0.255552  [   18/   89]
Per-example loss in batch: 0.245380  [   20/   89]
Per-example loss in batch: 0.240090  [   22/   89]
Per-example loss in batch: 0.278374  [   24/   89]
Per-example loss in batch: 0.343060  [   26/   89]
Per-example loss in batch: 0.295960  [   28/   89]
Per-example loss in batch: 0.312238  [   30/   89]
Per-example loss in batch: 0.245848  [   32/   89]
Per-example loss in batch: 0.220060  [   34/   89]
Per-example loss in batch: 0.317354  [   36/   89]
Per-example loss in batch: 0.308302  [   38/   89]
Per-example loss in batch: 0.214950  [   40/   89]
Per-example loss in batch: 0.312953  [   42/   89]
Per-example loss in batch: 0.338369  [   44/   89]
Per-example loss in batch: 0.257127  [   46/   89]
Per-example loss in batch: 0.203366  [   48/   89]
Per-example loss in batch: 0.233401  [   50/   89]
Per-example loss in batch: 0.203718  [   52/   89]
Per-example loss in batch: 0.363847  [   54/   89]
Per-example loss in batch: 0.283309  [   56/   89]
Per-example loss in batch: 0.268717  [   58/   89]
Per-example loss in batch: 0.224539  [   60/   89]
Per-example loss in batch: 0.202323  [   62/   89]
Per-example loss in batch: 0.241712  [   64/   89]
Per-example loss in batch: 0.347578  [   66/   89]
Per-example loss in batch: 0.230488  [   68/   89]
Per-example loss in batch: 0.238710  [   70/   89]
Per-example loss in batch: 0.225496  [   72/   89]
Per-example loss in batch: 0.261077  [   74/   89]
Per-example loss in batch: 0.200857  [   76/   89]
Per-example loss in batch: 0.292203  [   78/   89]
Per-example loss in batch: 0.300481  [   80/   89]
Per-example loss in batch: 0.304323  [   82/   89]
Per-example loss in batch: 0.206522  [   84/   89]
Per-example loss in batch: 0.314958  [   86/   89]
Per-example loss in batch: 0.207335  [   88/   89]
Per-example loss in batch: 0.524982  [   89/   89]
Train Error: Avg loss: 0.26920263
validation Error: 
 Avg loss: 0.29028567 
 F1: 0.471805 
 Precision: 0.520258 
 Recall: 0.431609
 IoU: 0.308734

test Error: 
 Avg loss: 0.26144091 
 F1: 0.508845 
 Precision: 0.569720 
 Recall: 0.459723
 IoU: 0.341242

We have finished training iteration 258
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_256_.pth
Per-example loss in batch: 0.254568  [    2/   89]
Per-example loss in batch: 0.218329  [    4/   89]
Per-example loss in batch: 0.223102  [    6/   89]
Per-example loss in batch: 0.263556  [    8/   89]
Per-example loss in batch: 0.322057  [   10/   89]
Per-example loss in batch: 0.340034  [   12/   89]
Per-example loss in batch: 0.259257  [   14/   89]
Per-example loss in batch: 0.313309  [   16/   89]
Per-example loss in batch: 0.212889  [   18/   89]
Per-example loss in batch: 0.223570  [   20/   89]
Per-example loss in batch: 0.233299  [   22/   89]
Per-example loss in batch: 0.306845  [   24/   89]
Per-example loss in batch: 0.299231  [   26/   89]
Per-example loss in batch: 0.236657  [   28/   89]
Per-example loss in batch: 0.237793  [   30/   89]
Per-example loss in batch: 0.317304  [   32/   89]
Per-example loss in batch: 0.240085  [   34/   89]
Per-example loss in batch: 0.264445  [   36/   89]
Per-example loss in batch: 0.249528  [   38/   89]
Per-example loss in batch: 0.365079  [   40/   89]
Per-example loss in batch: 0.235520  [   42/   89]
Per-example loss in batch: 0.263777  [   44/   89]
Per-example loss in batch: 0.297115  [   46/   89]
Per-example loss in batch: 0.326445  [   48/   89]
Per-example loss in batch: 0.277207  [   50/   89]
Per-example loss in batch: 0.215900  [   52/   89]
Per-example loss in batch: 0.240962  [   54/   89]
Per-example loss in batch: 0.285028  [   56/   89]
Per-example loss in batch: 0.284223  [   58/   89]
Per-example loss in batch: 0.264539  [   60/   89]
Per-example loss in batch: 0.234506  [   62/   89]
Per-example loss in batch: 0.284159  [   64/   89]
Per-example loss in batch: 0.260293  [   66/   89]
Per-example loss in batch: 0.233989  [   68/   89]
Per-example loss in batch: 0.303264  [   70/   89]
Per-example loss in batch: 0.288517  [   72/   89]
Per-example loss in batch: 0.214514  [   74/   89]
Per-example loss in batch: 0.227827  [   76/   89]
Per-example loss in batch: 0.218898  [   78/   89]
Per-example loss in batch: 0.237848  [   80/   89]
Per-example loss in batch: 0.265905  [   82/   89]
Per-example loss in batch: 0.231186  [   84/   89]
Per-example loss in batch: 0.276922  [   86/   89]
Per-example loss in batch: 0.265626  [   88/   89]
Per-example loss in batch: 0.600911  [   89/   89]
Train Error: Avg loss: 0.26776546
validation Error: 
 Avg loss: 0.28530709 
 F1: 0.473938 
 Precision: 0.546474 
 Recall: 0.418402
 IoU: 0.310563

test Error: 
 Avg loss: 0.26192545 
 F1: 0.507919 
 Precision: 0.601330 
 Recall: 0.439626
 IoU: 0.340409

We have finished training iteration 259
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_257_.pth
Per-example loss in batch: 0.305075  [    2/   89]
Per-example loss in batch: 0.249496  [    4/   89]
Per-example loss in batch: 0.271866  [    6/   89]
Per-example loss in batch: 0.207766  [    8/   89]
Per-example loss in batch: 0.283689  [   10/   89]
Per-example loss in batch: 0.259642  [   12/   89]
Per-example loss in batch: 0.224481  [   14/   89]
Per-example loss in batch: 0.215203  [   16/   89]
Per-example loss in batch: 0.273435  [   18/   89]
Per-example loss in batch: 0.254176  [   20/   89]
Per-example loss in batch: 0.279935  [   22/   89]
Per-example loss in batch: 0.224904  [   24/   89]
Per-example loss in batch: 0.222760  [   26/   89]
Per-example loss in batch: 0.272559  [   28/   89]
Per-example loss in batch: 0.302334  [   30/   89]
Per-example loss in batch: 0.220794  [   32/   89]
Per-example loss in batch: 0.230905  [   34/   89]
Per-example loss in batch: 0.348182  [   36/   89]
Per-example loss in batch: 0.208091  [   38/   89]
Per-example loss in batch: 0.270229  [   40/   89]
Per-example loss in batch: 0.275397  [   42/   89]
Per-example loss in batch: 0.268138  [   44/   89]
Per-example loss in batch: 0.244117  [   46/   89]
Per-example loss in batch: 0.265326  [   48/   89]
Per-example loss in batch: 0.289124  [   50/   89]
Per-example loss in batch: 0.218036  [   52/   89]
Per-example loss in batch: 0.289634  [   54/   89]
Per-example loss in batch: 0.313854  [   56/   89]
Per-example loss in batch: 0.334115  [   58/   89]
Per-example loss in batch: 0.234612  [   60/   89]
Per-example loss in batch: 0.225322  [   62/   89]
Per-example loss in batch: 0.246886  [   64/   89]
Per-example loss in batch: 0.250176  [   66/   89]
Per-example loss in batch: 0.260087  [   68/   89]
Per-example loss in batch: 0.228507  [   70/   89]
Per-example loss in batch: 0.234353  [   72/   89]
Per-example loss in batch: 0.328312  [   74/   89]
Per-example loss in batch: 0.322541  [   76/   89]
Per-example loss in batch: 0.227769  [   78/   89]
Per-example loss in batch: 0.231422  [   80/   89]
Per-example loss in batch: 0.325507  [   82/   89]
Per-example loss in batch: 0.289232  [   84/   89]
Per-example loss in batch: 0.316976  [   86/   89]
Per-example loss in batch: 0.235699  [   88/   89]
Per-example loss in batch: 0.515822  [   89/   89]
Train Error: Avg loss: 0.26603546
validation Error: 
 Avg loss: 0.28487055 
 F1: 0.474329 
 Precision: 0.544402 
 Recall: 0.420238
 IoU: 0.310899

test Error: 
 Avg loss: 0.26097287 
 F1: 0.509399 
 Precision: 0.595839 
 Recall: 0.444862
 IoU: 0.341740

We have finished training iteration 260
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_258_.pth
Per-example loss in batch: 0.208262  [    2/   89]
Per-example loss in batch: 0.245470  [    4/   89]
Per-example loss in batch: 0.306836  [    6/   89]
Per-example loss in batch: 0.370109  [    8/   89]
Per-example loss in batch: 0.371200  [   10/   89]
Per-example loss in batch: 0.305680  [   12/   89]
Per-example loss in batch: 0.235688  [   14/   89]
Per-example loss in batch: 0.272589  [   16/   89]
Per-example loss in batch: 0.298944  [   18/   89]
Per-example loss in batch: 0.234818  [   20/   89]
Per-example loss in batch: 0.275574  [   22/   89]
Per-example loss in batch: 0.325254  [   24/   89]
Per-example loss in batch: 0.251471  [   26/   89]
Per-example loss in batch: 0.235646  [   28/   89]
Per-example loss in batch: 0.326231  [   30/   89]
Per-example loss in batch: 0.226796  [   32/   89]
Per-example loss in batch: 0.269088  [   34/   89]
Per-example loss in batch: 0.276125  [   36/   89]
Per-example loss in batch: 0.275917  [   38/   89]
Per-example loss in batch: 0.243789  [   40/   89]
Per-example loss in batch: 0.233413  [   42/   89]
Per-example loss in batch: 0.247177  [   44/   89]
Per-example loss in batch: 0.218554  [   46/   89]
Per-example loss in batch: 0.238311  [   48/   89]
Per-example loss in batch: 0.203930  [   50/   89]
Per-example loss in batch: 0.295901  [   52/   89]
Per-example loss in batch: 0.266555  [   54/   89]
Per-example loss in batch: 0.264490  [   56/   89]
Per-example loss in batch: 0.248679  [   58/   89]
Per-example loss in batch: 0.228673  [   60/   89]
Per-example loss in batch: 0.274727  [   62/   89]
Per-example loss in batch: 0.207441  [   64/   89]
Per-example loss in batch: 0.264140  [   66/   89]
Per-example loss in batch: 0.339651  [   68/   89]
Per-example loss in batch: 0.224761  [   70/   89]
Per-example loss in batch: 0.231403  [   72/   89]
Per-example loss in batch: 0.232085  [   74/   89]
Per-example loss in batch: 0.342541  [   76/   89]
Per-example loss in batch: 0.236965  [   78/   89]
Per-example loss in batch: 0.238650  [   80/   89]
Per-example loss in batch: 0.248693  [   82/   89]
Per-example loss in batch: 0.343287  [   84/   89]
Per-example loss in batch: 0.233766  [   86/   89]
Per-example loss in batch: 0.223957  [   88/   89]
Per-example loss in batch: 0.605764  [   89/   89]
Train Error: Avg loss: 0.26845217
validation Error: 
 Avg loss: 0.29009006 
 F1: 0.473879 
 Precision: 0.537490 
 Recall: 0.423731
 IoU: 0.310512

test Error: 
 Avg loss: 0.26077055 
 F1: 0.509820 
 Precision: 0.587252 
 Recall: 0.450429
 IoU: 0.342120

We have finished training iteration 261
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_259_.pth
Per-example loss in batch: 0.230324  [    2/   89]
Per-example loss in batch: 0.209374  [    4/   89]
Per-example loss in batch: 0.292324  [    6/   89]
Per-example loss in batch: 0.309318  [    8/   89]
Per-example loss in batch: 0.250381  [   10/   89]
Per-example loss in batch: 0.226526  [   12/   89]
Per-example loss in batch: 0.225064  [   14/   89]
Per-example loss in batch: 0.268114  [   16/   89]
Per-example loss in batch: 0.214146  [   18/   89]
Per-example loss in batch: 0.298409  [   20/   89]
Per-example loss in batch: 0.315903  [   22/   89]
Per-example loss in batch: 0.211953  [   24/   89]
Per-example loss in batch: 0.253368  [   26/   89]
Per-example loss in batch: 0.263626  [   28/   89]
Per-example loss in batch: 0.226186  [   30/   89]
Per-example loss in batch: 0.249341  [   32/   89]
Per-example loss in batch: 0.288535  [   34/   89]
Per-example loss in batch: 0.214555  [   36/   89]
Per-example loss in batch: 0.212075  [   38/   89]
Per-example loss in batch: 0.277278  [   40/   89]
Per-example loss in batch: 0.198414  [   42/   89]
Per-example loss in batch: 0.341230  [   44/   89]
Per-example loss in batch: 0.269143  [   46/   89]
Per-example loss in batch: 0.252974  [   48/   89]
Per-example loss in batch: 0.282348  [   50/   89]
Per-example loss in batch: 0.264480  [   52/   89]
Per-example loss in batch: 0.260524  [   54/   89]
Per-example loss in batch: 0.298030  [   56/   89]
Per-example loss in batch: 0.250313  [   58/   89]
Per-example loss in batch: 0.273981  [   60/   89]
Per-example loss in batch: 0.286951  [   62/   89]
Per-example loss in batch: 0.198236  [   64/   89]
Per-example loss in batch: 0.245888  [   66/   89]
Per-example loss in batch: 0.308686  [   68/   89]
Per-example loss in batch: 0.247157  [   70/   89]
Per-example loss in batch: 0.304436  [   72/   89]
Per-example loss in batch: 0.227011  [   74/   89]
Per-example loss in batch: 0.276469  [   76/   89]
Per-example loss in batch: 0.213315  [   78/   89]
Per-example loss in batch: 0.264890  [   80/   89]
Per-example loss in batch: 0.248910  [   82/   89]
Per-example loss in batch: 0.255192  [   84/   89]
Per-example loss in batch: 0.237845  [   86/   89]
Per-example loss in batch: 0.311381  [   88/   89]
Per-example loss in batch: 0.466027  [   89/   89]
Train Error: Avg loss: 0.26039596
validation Error: 
 Avg loss: 0.28514045 
 F1: 0.473951 
 Precision: 0.537349 
 Recall: 0.423934
 IoU: 0.310574

test Error: 
 Avg loss: 0.26086683 
 F1: 0.509753 
 Precision: 0.589914 
 Recall: 0.448771
 IoU: 0.342059

We have finished training iteration 262
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_260_.pth
Per-example loss in batch: 0.220858  [    2/   89]
Per-example loss in batch: 0.271939  [    4/   89]
Per-example loss in batch: 0.255031  [    6/   89]
Per-example loss in batch: 0.307250  [    8/   89]
Per-example loss in batch: 0.277274  [   10/   89]
Per-example loss in batch: 0.314054  [   12/   89]
Per-example loss in batch: 0.220997  [   14/   89]
Per-example loss in batch: 0.205915  [   16/   89]
Per-example loss in batch: 0.231770  [   18/   89]
Per-example loss in batch: 0.269641  [   20/   89]
Per-example loss in batch: 0.281601  [   22/   89]
Per-example loss in batch: 0.250095  [   24/   89]
Per-example loss in batch: 0.292108  [   26/   89]
Per-example loss in batch: 0.221778  [   28/   89]
Per-example loss in batch: 0.311798  [   30/   89]
Per-example loss in batch: 0.227597  [   32/   89]
Per-example loss in batch: 0.210519  [   34/   89]
Per-example loss in batch: 0.309652  [   36/   89]
Per-example loss in batch: 0.332319  [   38/   89]
Per-example loss in batch: 0.264845  [   40/   89]
Per-example loss in batch: 0.271462  [   42/   89]
Per-example loss in batch: 0.202512  [   44/   89]
Per-example loss in batch: 0.205142  [   46/   89]
Per-example loss in batch: 0.252489  [   48/   89]
Per-example loss in batch: 0.277568  [   50/   89]
Per-example loss in batch: 0.240645  [   52/   89]
Per-example loss in batch: 0.266769  [   54/   89]
Per-example loss in batch: 0.231963  [   56/   89]
Per-example loss in batch: 0.213575  [   58/   89]
Per-example loss in batch: 0.261237  [   60/   89]
Per-example loss in batch: 0.286744  [   62/   89]
Per-example loss in batch: 0.282344  [   64/   89]
Per-example loss in batch: 0.202878  [   66/   89]
Per-example loss in batch: 0.221092  [   68/   89]
Per-example loss in batch: 0.291076  [   70/   89]
Per-example loss in batch: 0.291688  [   72/   89]
Per-example loss in batch: 0.244839  [   74/   89]
Per-example loss in batch: 0.277491  [   76/   89]
Per-example loss in batch: 0.265022  [   78/   89]
Per-example loss in batch: 0.235007  [   80/   89]
Per-example loss in batch: 0.338584  [   82/   89]
Per-example loss in batch: 0.284501  [   84/   89]
Per-example loss in batch: 0.255260  [   86/   89]
Per-example loss in batch: 0.205307  [   88/   89]
Per-example loss in batch: 0.441466  [   89/   89]
Train Error: Avg loss: 0.26074092
validation Error: 
 Avg loss: 0.28430384 
 F1: 0.475016 
 Precision: 0.559780 
 Recall: 0.412547
 IoU: 0.311489

test Error: 
 Avg loss: 0.26155043 
 F1: 0.508577 
 Precision: 0.615037 
 Recall: 0.433535
 IoU: 0.341001

We have finished training iteration 263
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_261_.pth
Per-example loss in batch: 0.252071  [    2/   89]
Per-example loss in batch: 0.201967  [    4/   89]
Per-example loss in batch: 0.299983  [    6/   89]
Per-example loss in batch: 0.245769  [    8/   89]
Per-example loss in batch: 0.256259  [   10/   89]
Per-example loss in batch: 0.272843  [   12/   89]
Per-example loss in batch: 0.209468  [   14/   89]
Per-example loss in batch: 0.247132  [   16/   89]
Per-example loss in batch: 0.213446  [   18/   89]
Per-example loss in batch: 0.205028  [   20/   89]
Per-example loss in batch: 0.274707  [   22/   89]
Per-example loss in batch: 0.237102  [   24/   89]
Per-example loss in batch: 0.201824  [   26/   89]
Per-example loss in batch: 0.230576  [   28/   89]
Per-example loss in batch: 0.333893  [   30/   89]
Per-example loss in batch: 0.292573  [   32/   89]
Per-example loss in batch: 0.198766  [   34/   89]
Per-example loss in batch: 0.311240  [   36/   89]
Per-example loss in batch: 0.253382  [   38/   89]
Per-example loss in batch: 0.315547  [   40/   89]
Per-example loss in batch: 0.287989  [   42/   89]
Per-example loss in batch: 0.257236  [   44/   89]
Per-example loss in batch: 0.187940  [   46/   89]
Per-example loss in batch: 0.252714  [   48/   89]
Per-example loss in batch: 0.351257  [   50/   89]
Per-example loss in batch: 0.240949  [   52/   89]
Per-example loss in batch: 0.299148  [   54/   89]
Per-example loss in batch: 0.312644  [   56/   89]
Per-example loss in batch: 0.277508  [   58/   89]
Per-example loss in batch: 0.265828  [   60/   89]
Per-example loss in batch: 0.223273  [   62/   89]
Per-example loss in batch: 0.262135  [   64/   89]
Per-example loss in batch: 0.338710  [   66/   89]
Per-example loss in batch: 0.210478  [   68/   89]
Per-example loss in batch: 0.222358  [   70/   89]
Per-example loss in batch: 0.315892  [   72/   89]
Per-example loss in batch: 0.230960  [   74/   89]
Per-example loss in batch: 0.280040  [   76/   89]
Per-example loss in batch: 0.251094  [   78/   89]
Per-example loss in batch: 0.283938  [   80/   89]
Per-example loss in batch: 0.247262  [   82/   89]
Per-example loss in batch: 0.231604  [   84/   89]
Per-example loss in batch: 0.317998  [   86/   89]
Per-example loss in batch: 0.319063  [   88/   89]
Per-example loss in batch: 0.629644  [   89/   89]
Train Error: Avg loss: 0.26598688
validation Error: 
 Avg loss: 0.27777085 
 F1: 0.474827 
 Precision: 0.547469 
 Recall: 0.419204
 IoU: 0.311327

test Error: 
 Avg loss: 0.26057402 
 F1: 0.510506 
 Precision: 0.602987 
 Recall: 0.442620
 IoU: 0.342738

We have finished training iteration 264
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_262_.pth
Per-example loss in batch: 0.244266  [    2/   89]
Per-example loss in batch: 0.274470  [    4/   89]
Per-example loss in batch: 0.280471  [    6/   89]
Per-example loss in batch: 0.261917  [    8/   89]
Per-example loss in batch: 0.210199  [   10/   89]
Per-example loss in batch: 0.235453  [   12/   89]
Per-example loss in batch: 0.239438  [   14/   89]
Per-example loss in batch: 0.275584  [   16/   89]
Per-example loss in batch: 0.324444  [   18/   89]
Per-example loss in batch: 0.307097  [   20/   89]
Per-example loss in batch: 0.208616  [   22/   89]
Per-example loss in batch: 0.241532  [   24/   89]
Per-example loss in batch: 0.311539  [   26/   89]
Per-example loss in batch: 0.351444  [   28/   89]
Per-example loss in batch: 0.277403  [   30/   89]
Per-example loss in batch: 0.292815  [   32/   89]
Per-example loss in batch: 0.231413  [   34/   89]
Per-example loss in batch: 0.249551  [   36/   89]
Per-example loss in batch: 0.273913  [   38/   89]
Per-example loss in batch: 0.261797  [   40/   89]
Per-example loss in batch: 0.331967  [   42/   89]
Per-example loss in batch: 0.254859  [   44/   89]
Per-example loss in batch: 0.282154  [   46/   89]
Per-example loss in batch: 0.258077  [   48/   89]
Per-example loss in batch: 0.265847  [   50/   89]
Per-example loss in batch: 0.297640  [   52/   89]
Per-example loss in batch: 0.227520  [   54/   89]
Per-example loss in batch: 0.353074  [   56/   89]
Per-example loss in batch: 0.241424  [   58/   89]
Per-example loss in batch: 0.227153  [   60/   89]
Per-example loss in batch: 0.239535  [   62/   89]
Per-example loss in batch: 0.241945  [   64/   89]
Per-example loss in batch: 0.217794  [   66/   89]
Per-example loss in batch: 0.284734  [   68/   89]
Per-example loss in batch: 0.262873  [   70/   89]
Per-example loss in batch: 0.282472  [   72/   89]
Per-example loss in batch: 0.214401  [   74/   89]
Per-example loss in batch: 0.281778  [   76/   89]
Per-example loss in batch: 0.268945  [   78/   89]
Per-example loss in batch: 0.227329  [   80/   89]
Per-example loss in batch: 0.261748  [   82/   89]
Per-example loss in batch: 0.214560  [   84/   89]
Per-example loss in batch: 0.276284  [   86/   89]
Per-example loss in batch: 0.312602  [   88/   89]
Per-example loss in batch: 0.632593  [   89/   89]
Train Error: Avg loss: 0.26958139
validation Error: 
 Avg loss: 0.28212542 
 F1: 0.473330 
 Precision: 0.544581 
 Recall: 0.418566
 IoU: 0.310040

test Error: 
 Avg loss: 0.26160805 
 F1: 0.508545 
 Precision: 0.595423 
 Recall: 0.443791
 IoU: 0.340972

We have finished training iteration 265
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_263_.pth
Per-example loss in batch: 0.293870  [    2/   89]
Per-example loss in batch: 0.229070  [    4/   89]
Per-example loss in batch: 0.237719  [    6/   89]
Per-example loss in batch: 0.264925  [    8/   89]
Per-example loss in batch: 0.329656  [   10/   89]
Per-example loss in batch: 0.222289  [   12/   89]
Per-example loss in batch: 0.311657  [   14/   89]
Per-example loss in batch: 0.215418  [   16/   89]
Per-example loss in batch: 0.327979  [   18/   89]
Per-example loss in batch: 0.201831  [   20/   89]
Per-example loss in batch: 0.238433  [   22/   89]
Per-example loss in batch: 0.208936  [   24/   89]
Per-example loss in batch: 0.280977  [   26/   89]
Per-example loss in batch: 0.270963  [   28/   89]
Per-example loss in batch: 0.256984  [   30/   89]
Per-example loss in batch: 0.200292  [   32/   89]
Per-example loss in batch: 0.231294  [   34/   89]
Per-example loss in batch: 0.325340  [   36/   89]
Per-example loss in batch: 0.249448  [   38/   89]
Per-example loss in batch: 0.237530  [   40/   89]
Per-example loss in batch: 0.253394  [   42/   89]
Per-example loss in batch: 0.294491  [   44/   89]
Per-example loss in batch: 0.249928  [   46/   89]
Per-example loss in batch: 0.242842  [   48/   89]
Per-example loss in batch: 0.344411  [   50/   89]
Per-example loss in batch: 0.262112  [   52/   89]
Per-example loss in batch: 0.325589  [   54/   89]
Per-example loss in batch: 0.302457  [   56/   89]
Per-example loss in batch: 0.229199  [   58/   89]
Per-example loss in batch: 0.362576  [   60/   89]
Per-example loss in batch: 0.294223  [   62/   89]
Per-example loss in batch: 0.263753  [   64/   89]
Per-example loss in batch: 0.238693  [   66/   89]
Per-example loss in batch: 0.275405  [   68/   89]
Per-example loss in batch: 0.229359  [   70/   89]
Per-example loss in batch: 0.239113  [   72/   89]
Per-example loss in batch: 0.276674  [   74/   89]
Per-example loss in batch: 0.228244  [   76/   89]
Per-example loss in batch: 0.252665  [   78/   89]
Per-example loss in batch: 0.321789  [   80/   89]
Per-example loss in batch: 0.248636  [   82/   89]
Per-example loss in batch: 0.351295  [   84/   89]
Per-example loss in batch: 0.266514  [   86/   89]
Per-example loss in batch: 0.228390  [   88/   89]
Per-example loss in batch: 0.684652  [   89/   89]
Train Error: Avg loss: 0.27098180
validation Error: 
 Avg loss: 0.28588638 
 F1: 0.473341 
 Precision: 0.562364 
 Recall: 0.408651
 IoU: 0.310050

test Error: 
 Avg loss: 0.26217358 
 F1: 0.507292 
 Precision: 0.615286 
 Recall: 0.431548
 IoU: 0.339847

We have finished training iteration 266
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_205_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.245785  [    2/   89]
Per-example loss in batch: 0.229556  [    4/   89]
Per-example loss in batch: 0.294540  [    6/   89]
Per-example loss in batch: 0.334325  [    8/   89]
Per-example loss in batch: 0.267546  [   10/   89]
Per-example loss in batch: 0.242006  [   12/   89]
Per-example loss in batch: 0.216586  [   14/   89]
Per-example loss in batch: 0.329317  [   16/   89]
Per-example loss in batch: 0.294171  [   18/   89]
Per-example loss in batch: 0.252291  [   20/   89]
Per-example loss in batch: 0.234763  [   22/   89]
Per-example loss in batch: 0.308369  [   24/   89]
Per-example loss in batch: 0.285566  [   26/   89]
Per-example loss in batch: 0.259341  [   28/   89]
Per-example loss in batch: 0.349300  [   30/   89]
Per-example loss in batch: 0.296687  [   32/   89]
Per-example loss in batch: 0.228855  [   34/   89]
Per-example loss in batch: 0.254806  [   36/   89]
Per-example loss in batch: 0.252215  [   38/   89]
Per-example loss in batch: 0.262637  [   40/   89]
Per-example loss in batch: 0.264269  [   42/   89]
Per-example loss in batch: 0.298402  [   44/   89]
Per-example loss in batch: 0.237078  [   46/   89]
Per-example loss in batch: 0.220797  [   48/   89]
Per-example loss in batch: 0.245606  [   50/   89]
Per-example loss in batch: 0.220860  [   52/   89]
Per-example loss in batch: 0.199667  [   54/   89]
Per-example loss in batch: 0.341486  [   56/   89]
Per-example loss in batch: 0.226019  [   58/   89]
Per-example loss in batch: 0.257129  [   60/   89]
Per-example loss in batch: 0.324699  [   62/   89]
Per-example loss in batch: 0.226857  [   64/   89]
Per-example loss in batch: 0.232219  [   66/   89]
Per-example loss in batch: 0.224483  [   68/   89]
Per-example loss in batch: 0.288136  [   70/   89]
Per-example loss in batch: 0.229131  [   72/   89]
Per-example loss in batch: 0.267899  [   74/   89]
Per-example loss in batch: 0.306895  [   76/   89]
Per-example loss in batch: 0.275977  [   78/   89]
Per-example loss in batch: 0.226640  [   80/   89]
Per-example loss in batch: 0.196737  [   82/   89]
Per-example loss in batch: 0.257434  [   84/   89]
Per-example loss in batch: 0.232090  [   86/   89]
Per-example loss in batch: 0.261404  [   88/   89]
Per-example loss in batch: 0.413505  [   89/   89]
Train Error: Avg loss: 0.26308602
validation Error: 
 Avg loss: 0.28555445 
 F1: 0.475690 
 Precision: 0.532414 
 Recall: 0.429888
 IoU: 0.312069

test Error: 
 Avg loss: 0.25945245 
 F1: 0.512709 
 Precision: 0.588577 
 Recall: 0.454167
 IoU: 0.344727

We have finished training iteration 267
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_265_.pth
Per-example loss in batch: 0.219521  [    2/   89]
Per-example loss in batch: 0.276385  [    4/   89]
Per-example loss in batch: 0.224368  [    6/   89]
Per-example loss in batch: 0.219606  [    8/   89]
Per-example loss in batch: 0.342130  [   10/   89]
Per-example loss in batch: 0.231641  [   12/   89]
Per-example loss in batch: 0.289375  [   14/   89]
Per-example loss in batch: 0.285046  [   16/   89]
Per-example loss in batch: 0.290431  [   18/   89]
Per-example loss in batch: 0.329780  [   20/   89]
Per-example loss in batch: 0.208618  [   22/   89]
Per-example loss in batch: 0.265685  [   24/   89]
Per-example loss in batch: 0.195910  [   26/   89]
Per-example loss in batch: 0.227089  [   28/   89]
Per-example loss in batch: 0.261170  [   30/   89]
Per-example loss in batch: 0.280130  [   32/   89]
Per-example loss in batch: 0.284868  [   34/   89]
Per-example loss in batch: 0.241977  [   36/   89]
Per-example loss in batch: 0.233769  [   38/   89]
Per-example loss in batch: 0.325653  [   40/   89]
Per-example loss in batch: 0.407515  [   42/   89]
Per-example loss in batch: 0.254456  [   44/   89]
Per-example loss in batch: 0.257681  [   46/   89]
Per-example loss in batch: 0.230522  [   48/   89]
Per-example loss in batch: 0.258620  [   50/   89]
Per-example loss in batch: 0.267609  [   52/   89]
Per-example loss in batch: 0.202509  [   54/   89]
Per-example loss in batch: 0.248085  [   56/   89]
Per-example loss in batch: 0.214926  [   58/   89]
Per-example loss in batch: 0.232747  [   60/   89]
Per-example loss in batch: 0.225433  [   62/   89]
Per-example loss in batch: 0.267909  [   64/   89]
Per-example loss in batch: 0.233103  [   66/   89]
Per-example loss in batch: 0.240955  [   68/   89]
Per-example loss in batch: 0.352378  [   70/   89]
Per-example loss in batch: 0.249365  [   72/   89]
Per-example loss in batch: 0.316073  [   74/   89]
Per-example loss in batch: 0.248381  [   76/   89]
Per-example loss in batch: 0.268598  [   78/   89]
Per-example loss in batch: 0.258966  [   80/   89]
Per-example loss in batch: 0.243465  [   82/   89]
Per-example loss in batch: 0.240314  [   84/   89]
Per-example loss in batch: 0.198881  [   86/   89]
Per-example loss in batch: 0.210269  [   88/   89]
Per-example loss in batch: 0.620537  [   89/   89]
Train Error: Avg loss: 0.26229621
validation Error: 
 Avg loss: 0.28314913 
 F1: 0.474766 
 Precision: 0.542473 
 Recall: 0.422085
 IoU: 0.311274

test Error: 
 Avg loss: 0.26041462 
 F1: 0.511061 
 Precision: 0.598475 
 Recall: 0.445927
 IoU: 0.343238

We have finished training iteration 268
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_266_.pth
Per-example loss in batch: 0.317878  [    2/   89]
Per-example loss in batch: 0.203987  [    4/   89]
Per-example loss in batch: 0.250428  [    6/   89]
Per-example loss in batch: 0.259156  [    8/   89]
Per-example loss in batch: 0.221297  [   10/   89]
Per-example loss in batch: 0.225477  [   12/   89]
Per-example loss in batch: 0.234641  [   14/   89]
Per-example loss in batch: 0.226235  [   16/   89]
Per-example loss in batch: 0.268763  [   18/   89]
Per-example loss in batch: 0.282802  [   20/   89]
Per-example loss in batch: 0.308188  [   22/   89]
Per-example loss in batch: 0.227596  [   24/   89]
Per-example loss in batch: 0.233737  [   26/   89]
Per-example loss in batch: 0.304463  [   28/   89]
Per-example loss in batch: 0.305137  [   30/   89]
Per-example loss in batch: 0.240257  [   32/   89]
Per-example loss in batch: 0.291722  [   34/   89]
Per-example loss in batch: 0.239780  [   36/   89]
Per-example loss in batch: 0.310728  [   38/   89]
Per-example loss in batch: 0.221664  [   40/   89]
Per-example loss in batch: 0.262821  [   42/   89]
Per-example loss in batch: 0.260019  [   44/   89]
Per-example loss in batch: 0.229290  [   46/   89]
Per-example loss in batch: 0.191868  [   48/   89]
Per-example loss in batch: 0.296483  [   50/   89]
Per-example loss in batch: 0.281424  [   52/   89]
Per-example loss in batch: 0.279908  [   54/   89]
Per-example loss in batch: 0.249732  [   56/   89]
Per-example loss in batch: 0.231088  [   58/   89]
Per-example loss in batch: 0.352568  [   60/   89]
Per-example loss in batch: 0.331742  [   62/   89]
Per-example loss in batch: 0.281334  [   64/   89]
Per-example loss in batch: 0.227183  [   66/   89]
Per-example loss in batch: 0.288496  [   68/   89]
Per-example loss in batch: 0.197048  [   70/   89]
Per-example loss in batch: 0.250754  [   72/   89]
Per-example loss in batch: 0.244113  [   74/   89]
Per-example loss in batch: 0.203158  [   76/   89]
Per-example loss in batch: 0.245658  [   78/   89]
Per-example loss in batch: 0.229350  [   80/   89]
Per-example loss in batch: 0.314908  [   82/   89]
Per-example loss in batch: 0.271948  [   84/   89]
Per-example loss in batch: 0.244446  [   86/   89]
Per-example loss in batch: 0.271151  [   88/   89]
Per-example loss in batch: 0.473762  [   89/   89]
Train Error: Avg loss: 0.26173726
validation Error: 
 Avg loss: 0.28417240 
 F1: 0.472050 
 Precision: 0.560037 
 Recall: 0.407957
 IoU: 0.308944

test Error: 
 Avg loss: 0.26339230 
 F1: 0.504794 
 Precision: 0.612505 
 Recall: 0.429300
 IoU: 0.337608

We have finished training iteration 269
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_267_.pth
Per-example loss in batch: 0.307435  [    2/   89]
Per-example loss in batch: 0.210893  [    4/   89]
Per-example loss in batch: 0.205853  [    6/   89]
Per-example loss in batch: 0.314916  [    8/   89]
Per-example loss in batch: 0.297756  [   10/   89]
Per-example loss in batch: 0.228348  [   12/   89]
Per-example loss in batch: 0.238788  [   14/   89]
Per-example loss in batch: 0.309375  [   16/   89]
Per-example loss in batch: 0.220079  [   18/   89]
Per-example loss in batch: 0.251225  [   20/   89]
Per-example loss in batch: 0.278538  [   22/   89]
Per-example loss in batch: 0.290785  [   24/   89]
Per-example loss in batch: 0.206958  [   26/   89]
Per-example loss in batch: 0.244636  [   28/   89]
Per-example loss in batch: 0.239502  [   30/   89]
Per-example loss in batch: 0.233629  [   32/   89]
Per-example loss in batch: 0.242078  [   34/   89]
Per-example loss in batch: 0.351742  [   36/   89]
Per-example loss in batch: 0.254182  [   38/   89]
Per-example loss in batch: 0.218039  [   40/   89]
Per-example loss in batch: 0.284573  [   42/   89]
Per-example loss in batch: 0.338952  [   44/   89]
Per-example loss in batch: 0.224603  [   46/   89]
Per-example loss in batch: 0.217122  [   48/   89]
Per-example loss in batch: 0.268657  [   50/   89]
Per-example loss in batch: 0.259636  [   52/   89]
Per-example loss in batch: 0.317336  [   54/   89]
Per-example loss in batch: 0.223950  [   56/   89]
Per-example loss in batch: 0.212779  [   58/   89]
Per-example loss in batch: 0.282128  [   60/   89]
Per-example loss in batch: 0.212157  [   62/   89]
Per-example loss in batch: 0.289785  [   64/   89]
Per-example loss in batch: 0.286477  [   66/   89]
Per-example loss in batch: 0.256746  [   68/   89]
Per-example loss in batch: 0.242807  [   70/   89]
Per-example loss in batch: 0.278371  [   72/   89]
Per-example loss in batch: 0.268803  [   74/   89]
Per-example loss in batch: 0.255876  [   76/   89]
Per-example loss in batch: 0.306912  [   78/   89]
Per-example loss in batch: 0.263381  [   80/   89]
Per-example loss in batch: 0.321648  [   82/   89]
Per-example loss in batch: 0.296833  [   84/   89]
Per-example loss in batch: 0.269213  [   86/   89]
Per-example loss in batch: 0.221616  [   88/   89]
Per-example loss in batch: 0.489402  [   89/   89]
Train Error: Avg loss: 0.26493980
validation Error: 
 Avg loss: 0.28385954 
 F1: 0.474098 
 Precision: 0.535138 
 Recall: 0.425557
 IoU: 0.310700

test Error: 
 Avg loss: 0.26086922 
 F1: 0.509778 
 Precision: 0.585446 
 Recall: 0.451432
 IoU: 0.342082

We have finished training iteration 270
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_268_.pth
Per-example loss in batch: 0.316315  [    2/   89]
Per-example loss in batch: 0.305592  [    4/   89]
Per-example loss in batch: 0.310489  [    6/   89]
Per-example loss in batch: 0.256323  [    8/   89]
Per-example loss in batch: 0.254475  [   10/   89]
Per-example loss in batch: 0.240738  [   12/   89]
Per-example loss in batch: 0.255480  [   14/   89]
Per-example loss in batch: 0.247848  [   16/   89]
Per-example loss in batch: 0.211962  [   18/   89]
Per-example loss in batch: 0.258992  [   20/   89]
Per-example loss in batch: 0.222357  [   22/   89]
Per-example loss in batch: 0.228463  [   24/   89]
Per-example loss in batch: 0.285018  [   26/   89]
Per-example loss in batch: 0.218989  [   28/   89]
Per-example loss in batch: 0.312935  [   30/   89]
Per-example loss in batch: 0.251250  [   32/   89]
Per-example loss in batch: 0.307808  [   34/   89]
Per-example loss in batch: 0.253901  [   36/   89]
Per-example loss in batch: 0.236774  [   38/   89]
Per-example loss in batch: 0.273917  [   40/   89]
Per-example loss in batch: 0.207595  [   42/   89]
Per-example loss in batch: 0.247819  [   44/   89]
Per-example loss in batch: 0.224406  [   46/   89]
Per-example loss in batch: 0.241183  [   48/   89]
Per-example loss in batch: 0.297820  [   50/   89]
Per-example loss in batch: 0.206115  [   52/   89]
Per-example loss in batch: 0.310799  [   54/   89]
Per-example loss in batch: 0.295210  [   56/   89]
Per-example loss in batch: 0.234367  [   58/   89]
Per-example loss in batch: 0.351774  [   60/   89]
Per-example loss in batch: 0.252576  [   62/   89]
Per-example loss in batch: 0.331122  [   64/   89]
Per-example loss in batch: 0.217369  [   66/   89]
Per-example loss in batch: 0.333165  [   68/   89]
Per-example loss in batch: 0.262745  [   70/   89]
Per-example loss in batch: 0.361234  [   72/   89]
Per-example loss in batch: 0.222458  [   74/   89]
Per-example loss in batch: 0.271149  [   76/   89]
Per-example loss in batch: 0.246460  [   78/   89]
Per-example loss in batch: 0.256932  [   80/   89]
Per-example loss in batch: 0.212525  [   82/   89]
Per-example loss in batch: 0.255030  [   84/   89]
Per-example loss in batch: 0.262709  [   86/   89]
Per-example loss in batch: 0.254762  [   88/   89]
Per-example loss in batch: 0.434157  [   89/   89]
Train Error: Avg loss: 0.26570846
validation Error: 
 Avg loss: 0.28451378 
 F1: 0.475160 
 Precision: 0.540113 
 Recall: 0.424152
 IoU: 0.311613

test Error: 
 Avg loss: 0.26096737 
 F1: 0.509565 
 Precision: 0.592227 
 Recall: 0.447153
 IoU: 0.341891

We have finished training iteration 271
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_269_.pth
Per-example loss in batch: 0.233348  [    2/   89]
Per-example loss in batch: 0.296303  [    4/   89]
Per-example loss in batch: 0.267938  [    6/   89]
Per-example loss in batch: 0.298466  [    8/   89]
Per-example loss in batch: 0.212732  [   10/   89]
Per-example loss in batch: 0.262166  [   12/   89]
Per-example loss in batch: 0.262571  [   14/   89]
Per-example loss in batch: 0.218610  [   16/   89]
Per-example loss in batch: 0.263416  [   18/   89]
Per-example loss in batch: 0.221846  [   20/   89]
Per-example loss in batch: 0.224921  [   22/   89]
Per-example loss in batch: 0.253220  [   24/   89]
Per-example loss in batch: 0.256840  [   26/   89]
Per-example loss in batch: 0.326532  [   28/   89]
Per-example loss in batch: 0.282390  [   30/   89]
Per-example loss in batch: 0.249498  [   32/   89]
Per-example loss in batch: 0.238663  [   34/   89]
Per-example loss in batch: 0.204518  [   36/   89]
Per-example loss in batch: 0.267354  [   38/   89]
Per-example loss in batch: 0.246583  [   40/   89]
Per-example loss in batch: 0.254002  [   42/   89]
Per-example loss in batch: 0.312113  [   44/   89]
Per-example loss in batch: 0.368883  [   46/   89]
Per-example loss in batch: 0.216072  [   48/   89]
Per-example loss in batch: 0.242980  [   50/   89]
Per-example loss in batch: 0.231525  [   52/   89]
Per-example loss in batch: 0.229296  [   54/   89]
Per-example loss in batch: 0.325346  [   56/   89]
Per-example loss in batch: 0.268322  [   58/   89]
Per-example loss in batch: 0.306163  [   60/   89]
Per-example loss in batch: 0.258613  [   62/   89]
Per-example loss in batch: 0.296651  [   64/   89]
Per-example loss in batch: 0.322290  [   66/   89]
Per-example loss in batch: 0.225629  [   68/   89]
Per-example loss in batch: 0.276630  [   70/   89]
Per-example loss in batch: 0.340106  [   72/   89]
Per-example loss in batch: 0.211108  [   74/   89]
Per-example loss in batch: 0.232088  [   76/   89]
Per-example loss in batch: 0.267185  [   78/   89]
Per-example loss in batch: 0.198099  [   80/   89]
Per-example loss in batch: 0.290457  [   82/   89]
Per-example loss in batch: 0.340738  [   84/   89]
Per-example loss in batch: 0.363536  [   86/   89]
Per-example loss in batch: 0.283649  [   88/   89]
Per-example loss in batch: 0.474668  [   89/   89]
Train Error: Avg loss: 0.26936474
validation Error: 
 Avg loss: 0.28506597 
 F1: 0.474309 
 Precision: 0.558685 
 Recall: 0.412076
 IoU: 0.310882

test Error: 
 Avg loss: 0.26167575 
 F1: 0.508122 
 Precision: 0.612785 
 Recall: 0.433996
 IoU: 0.340592

We have finished training iteration 272
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_270_.pth
Per-example loss in batch: 0.204245  [    2/   89]
Per-example loss in batch: 0.337898  [    4/   89]
Per-example loss in batch: 0.225303  [    6/   89]
Per-example loss in batch: 0.336044  [    8/   89]
Per-example loss in batch: 0.291418  [   10/   89]
Per-example loss in batch: 0.213788  [   12/   89]
Per-example loss in batch: 0.259954  [   14/   89]
Per-example loss in batch: 0.242563  [   16/   89]
Per-example loss in batch: 0.293173  [   18/   89]
Per-example loss in batch: 0.303422  [   20/   89]
Per-example loss in batch: 0.211516  [   22/   89]
Per-example loss in batch: 0.331586  [   24/   89]
Per-example loss in batch: 0.225569  [   26/   89]
Per-example loss in batch: 0.243426  [   28/   89]
Per-example loss in batch: 0.224082  [   30/   89]
Per-example loss in batch: 0.310425  [   32/   89]
Per-example loss in batch: 0.242687  [   34/   89]
Per-example loss in batch: 0.265753  [   36/   89]
Per-example loss in batch: 0.208698  [   38/   89]
Per-example loss in batch: 0.245458  [   40/   89]
Per-example loss in batch: 0.227708  [   42/   89]
Per-example loss in batch: 0.299856  [   44/   89]
Per-example loss in batch: 0.243581  [   46/   89]
Per-example loss in batch: 0.268637  [   48/   89]
Per-example loss in batch: 0.293049  [   50/   89]
Per-example loss in batch: 0.228872  [   52/   89]
Per-example loss in batch: 0.298679  [   54/   89]
Per-example loss in batch: 0.208123  [   56/   89]
Per-example loss in batch: 0.273316  [   58/   89]
Per-example loss in batch: 0.228143  [   60/   89]
Per-example loss in batch: 0.334211  [   62/   89]
Per-example loss in batch: 0.207588  [   64/   89]
Per-example loss in batch: 0.243437  [   66/   89]
Per-example loss in batch: 0.303412  [   68/   89]
Per-example loss in batch: 0.281210  [   70/   89]
Per-example loss in batch: 0.253624  [   72/   89]
Per-example loss in batch: 0.296180  [   74/   89]
Per-example loss in batch: 0.285595  [   76/   89]
Per-example loss in batch: 0.239023  [   78/   89]
Per-example loss in batch: 0.266036  [   80/   89]
Per-example loss in batch: 0.238067  [   82/   89]
Per-example loss in batch: 0.289736  [   84/   89]
Per-example loss in batch: 0.265031  [   86/   89]
Per-example loss in batch: 0.237991  [   88/   89]
Per-example loss in batch: 0.571732  [   89/   89]
Train Error: Avg loss: 0.26548257
validation Error: 
 Avg loss: 0.28540135 
 F1: 0.474922 
 Precision: 0.548509 
 Recall: 0.418743
 IoU: 0.311408

test Error: 
 Avg loss: 0.26074624 
 F1: 0.510198 
 Precision: 0.603455 
 Recall: 0.441906
 IoU: 0.342460

We have finished training iteration 273
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_271_.pth
Per-example loss in batch: 0.265810  [    2/   89]
Per-example loss in batch: 0.218183  [    4/   89]
Per-example loss in batch: 0.241389  [    6/   89]
Per-example loss in batch: 0.208511  [    8/   89]
Per-example loss in batch: 0.297389  [   10/   89]
Per-example loss in batch: 0.256146  [   12/   89]
Per-example loss in batch: 0.290775  [   14/   89]
Per-example loss in batch: 0.267812  [   16/   89]
Per-example loss in batch: 0.193679  [   18/   89]
Per-example loss in batch: 0.340836  [   20/   89]
Per-example loss in batch: 0.246875  [   22/   89]
Per-example loss in batch: 0.206668  [   24/   89]
Per-example loss in batch: 0.253452  [   26/   89]
Per-example loss in batch: 0.309213  [   28/   89]
Per-example loss in batch: 0.257271  [   30/   89]
Per-example loss in batch: 0.255479  [   32/   89]
Per-example loss in batch: 0.246159  [   34/   89]
Per-example loss in batch: 0.244651  [   36/   89]
Per-example loss in batch: 0.220678  [   38/   89]
Per-example loss in batch: 0.242049  [   40/   89]
Per-example loss in batch: 0.237839  [   42/   89]
Per-example loss in batch: 0.288834  [   44/   89]
Per-example loss in batch: 0.322431  [   46/   89]
Per-example loss in batch: 0.204320  [   48/   89]
Per-example loss in batch: 0.231726  [   50/   89]
Per-example loss in batch: 0.223472  [   52/   89]
Per-example loss in batch: 0.206727  [   54/   89]
Per-example loss in batch: 0.358661  [   56/   89]
Per-example loss in batch: 0.340008  [   58/   89]
Per-example loss in batch: 0.246903  [   60/   89]
Per-example loss in batch: 0.293624  [   62/   89]
Per-example loss in batch: 0.279924  [   64/   89]
Per-example loss in batch: 0.246117  [   66/   89]
Per-example loss in batch: 0.267500  [   68/   89]
Per-example loss in batch: 0.302056  [   70/   89]
Per-example loss in batch: 0.328327  [   72/   89]
Per-example loss in batch: 0.331224  [   74/   89]
Per-example loss in batch: 0.220391  [   76/   89]
Per-example loss in batch: 0.267062  [   78/   89]
Per-example loss in batch: 0.249074  [   80/   89]
Per-example loss in batch: 0.289509  [   82/   89]
Per-example loss in batch: 0.339776  [   84/   89]
Per-example loss in batch: 0.233792  [   86/   89]
Per-example loss in batch: 0.232892  [   88/   89]
Per-example loss in batch: 0.450886  [   89/   89]
Train Error: Avg loss: 0.26585742
validation Error: 
 Avg loss: 0.28019116 
 F1: 0.474048 
 Precision: 0.540435 
 Recall: 0.422186
 IoU: 0.310657

test Error: 
 Avg loss: 0.26060715 
 F1: 0.510644 
 Precision: 0.595253 
 Recall: 0.447095
 IoU: 0.342863

We have finished training iteration 274
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_272_.pth
Per-example loss in batch: 0.235388  [    2/   89]
Per-example loss in batch: 0.204521  [    4/   89]
Per-example loss in batch: 0.292371  [    6/   89]
Per-example loss in batch: 0.221516  [    8/   89]
Per-example loss in batch: 0.299948  [   10/   89]
Per-example loss in batch: 0.289073  [   12/   89]
Per-example loss in batch: 0.247422  [   14/   89]
Per-example loss in batch: 0.259363  [   16/   89]
Per-example loss in batch: 0.235017  [   18/   89]
Per-example loss in batch: 0.268514  [   20/   89]
Per-example loss in batch: 0.213608  [   22/   89]
Per-example loss in batch: 0.313600  [   24/   89]
Per-example loss in batch: 0.283715  [   26/   89]
Per-example loss in batch: 0.341412  [   28/   89]
Per-example loss in batch: 0.231110  [   30/   89]
Per-example loss in batch: 0.317671  [   32/   89]
Per-example loss in batch: 0.222353  [   34/   89]
Per-example loss in batch: 0.247949  [   36/   89]
Per-example loss in batch: 0.202006  [   38/   89]
Per-example loss in batch: 0.330467  [   40/   89]
Per-example loss in batch: 0.243021  [   42/   89]
Per-example loss in batch: 0.280757  [   44/   89]
Per-example loss in batch: 0.254200  [   46/   89]
Per-example loss in batch: 0.214682  [   48/   89]
Per-example loss in batch: 0.229224  [   50/   89]
Per-example loss in batch: 0.217227  [   52/   89]
Per-example loss in batch: 0.267759  [   54/   89]
Per-example loss in batch: 0.330963  [   56/   89]
Per-example loss in batch: 0.334502  [   58/   89]
Per-example loss in batch: 0.210851  [   60/   89]
Per-example loss in batch: 0.366157  [   62/   89]
Per-example loss in batch: 0.271568  [   64/   89]
Per-example loss in batch: 0.330155  [   66/   89]
Per-example loss in batch: 0.249394  [   68/   89]
Per-example loss in batch: 0.317722  [   70/   89]
Per-example loss in batch: 0.281457  [   72/   89]
Per-example loss in batch: 0.196427  [   74/   89]
Per-example loss in batch: 0.222005  [   76/   89]
Per-example loss in batch: 0.291548  [   78/   89]
Per-example loss in batch: 0.265740  [   80/   89]
Per-example loss in batch: 0.280439  [   82/   89]
Per-example loss in batch: 0.232765  [   84/   89]
Per-example loss in batch: 0.316895  [   86/   89]
Per-example loss in batch: 0.240956  [   88/   89]
Per-example loss in batch: 0.632605  [   89/   89]
Train Error: Avg loss: 0.27010652
validation Error: 
 Avg loss: 0.28640549 
 F1: 0.472643 
 Precision: 0.557259 
 Recall: 0.410337
 IoU: 0.309452

test Error: 
 Avg loss: 0.26275934 
 F1: 0.505789 
 Precision: 0.607871 
 Recall: 0.433063
 IoU: 0.338499

We have finished training iteration 275
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_273_.pth
Per-example loss in batch: 0.235216  [    2/   89]
Per-example loss in batch: 0.225569  [    4/   89]
Per-example loss in batch: 0.255608  [    6/   89]
Per-example loss in batch: 0.254691  [    8/   89]
Per-example loss in batch: 0.246988  [   10/   89]
Per-example loss in batch: 0.269690  [   12/   89]
Per-example loss in batch: 0.316914  [   14/   89]
Per-example loss in batch: 0.276631  [   16/   89]
Per-example loss in batch: 0.278490  [   18/   89]
Per-example loss in batch: 0.307006  [   20/   89]
Per-example loss in batch: 0.317324  [   22/   89]
Per-example loss in batch: 0.249105  [   24/   89]
Per-example loss in batch: 0.264162  [   26/   89]
Per-example loss in batch: 0.270108  [   28/   89]
Per-example loss in batch: 0.267849  [   30/   89]
Per-example loss in batch: 0.221956  [   32/   89]
Per-example loss in batch: 0.273702  [   34/   89]
Per-example loss in batch: 0.242827  [   36/   89]
Per-example loss in batch: 0.216291  [   38/   89]
Per-example loss in batch: 0.313263  [   40/   89]
Per-example loss in batch: 0.234033  [   42/   89]
Per-example loss in batch: 0.251989  [   44/   89]
Per-example loss in batch: 0.220773  [   46/   89]
Per-example loss in batch: 0.238580  [   48/   89]
Per-example loss in batch: 0.232947  [   50/   89]
Per-example loss in batch: 0.244477  [   52/   89]
Per-example loss in batch: 0.274784  [   54/   89]
Per-example loss in batch: 0.338334  [   56/   89]
Per-example loss in batch: 0.248893  [   58/   89]
Per-example loss in batch: 0.298460  [   60/   89]
Per-example loss in batch: 0.242385  [   62/   89]
Per-example loss in batch: 0.229389  [   64/   89]
Per-example loss in batch: 0.279752  [   66/   89]
Per-example loss in batch: 0.240454  [   68/   89]
Per-example loss in batch: 0.208681  [   70/   89]
Per-example loss in batch: 0.288645  [   72/   89]
Per-example loss in batch: 0.294636  [   74/   89]
Per-example loss in batch: 0.262853  [   76/   89]
Per-example loss in batch: 0.226745  [   78/   89]
Per-example loss in batch: 0.240629  [   80/   89]
Per-example loss in batch: 0.271560  [   82/   89]
Per-example loss in batch: 0.249721  [   84/   89]
Per-example loss in batch: 0.248658  [   86/   89]
Per-example loss in batch: 0.355661  [   88/   89]
Per-example loss in batch: 0.414001  [   89/   89]
Train Error: Avg loss: 0.26367257
validation Error: 
 Avg loss: 0.29034250 
 F1: 0.473114 
 Precision: 0.517106 
 Recall: 0.436021
 IoU: 0.309856

test Error: 
 Avg loss: 0.26052078 
 F1: 0.510444 
 Precision: 0.567197 
 Recall: 0.464016
 IoU: 0.342682

We have finished training iteration 276
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_274_.pth
Per-example loss in batch: 0.291410  [    2/   89]
Per-example loss in batch: 0.274453  [    4/   89]
Per-example loss in batch: 0.266679  [    6/   89]
Per-example loss in batch: 0.192435  [    8/   89]
Per-example loss in batch: 0.200592  [   10/   89]
Per-example loss in batch: 0.213100  [   12/   89]
Per-example loss in batch: 0.267503  [   14/   89]
Per-example loss in batch: 0.216501  [   16/   89]
Per-example loss in batch: 0.259392  [   18/   89]
Per-example loss in batch: 0.250899  [   20/   89]
Per-example loss in batch: 0.316660  [   22/   89]
Per-example loss in batch: 0.285590  [   24/   89]
Per-example loss in batch: 0.272543  [   26/   89]
Per-example loss in batch: 0.253097  [   28/   89]
Per-example loss in batch: 0.225363  [   30/   89]
Per-example loss in batch: 0.277473  [   32/   89]
Per-example loss in batch: 0.248394  [   34/   89]
Per-example loss in batch: 0.258228  [   36/   89]
Per-example loss in batch: 0.344347  [   38/   89]
Per-example loss in batch: 0.247355  [   40/   89]
Per-example loss in batch: 0.311394  [   42/   89]
Per-example loss in batch: 0.250391  [   44/   89]
Per-example loss in batch: 0.256811  [   46/   89]
Per-example loss in batch: 0.232482  [   48/   89]
Per-example loss in batch: 0.215750  [   50/   89]
Per-example loss in batch: 0.315984  [   52/   89]
Per-example loss in batch: 0.269133  [   54/   89]
Per-example loss in batch: 0.246027  [   56/   89]
Per-example loss in batch: 0.259333  [   58/   89]
Per-example loss in batch: 0.244537  [   60/   89]
Per-example loss in batch: 0.280095  [   62/   89]
Per-example loss in batch: 0.347622  [   64/   89]
Per-example loss in batch: 0.253544  [   66/   89]
Per-example loss in batch: 0.248138  [   68/   89]
Per-example loss in batch: 0.204414  [   70/   89]
Per-example loss in batch: 0.226009  [   72/   89]
Per-example loss in batch: 0.236833  [   74/   89]
Per-example loss in batch: 0.342761  [   76/   89]
Per-example loss in batch: 0.335158  [   78/   89]
Per-example loss in batch: 0.231888  [   80/   89]
Per-example loss in batch: 0.301127  [   82/   89]
Per-example loss in batch: 0.240773  [   84/   89]
Per-example loss in batch: 0.335346  [   86/   89]
Per-example loss in batch: 0.217641  [   88/   89]
Per-example loss in batch: 0.647196  [   89/   89]
Train Error: Avg loss: 0.26716419
validation Error: 
 Avg loss: 0.28585204 
 F1: 0.473895 
 Precision: 0.559657 
 Recall: 0.410924
 IoU: 0.310526

test Error: 
 Avg loss: 0.26160373 
 F1: 0.508135 
 Precision: 0.611797 
 Recall: 0.434512
 IoU: 0.340604

We have finished training iteration 277
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_275_.pth
Per-example loss in batch: 0.216531  [    2/   89]
Per-example loss in batch: 0.223657  [    4/   89]
Per-example loss in batch: 0.252719  [    6/   89]
Per-example loss in batch: 0.260027  [    8/   89]
Per-example loss in batch: 0.260468  [   10/   89]
Per-example loss in batch: 0.278357  [   12/   89]
Per-example loss in batch: 0.355535  [   14/   89]
Per-example loss in batch: 0.306538  [   16/   89]
Per-example loss in batch: 0.228957  [   18/   89]
Per-example loss in batch: 0.307461  [   20/   89]
Per-example loss in batch: 0.272689  [   22/   89]
Per-example loss in batch: 0.264874  [   24/   89]
Per-example loss in batch: 0.350318  [   26/   89]
Per-example loss in batch: 0.293477  [   28/   89]
Per-example loss in batch: 0.205417  [   30/   89]
Per-example loss in batch: 0.221832  [   32/   89]
Per-example loss in batch: 0.232612  [   34/   89]
Per-example loss in batch: 0.200779  [   36/   89]
Per-example loss in batch: 0.229000  [   38/   89]
Per-example loss in batch: 0.317587  [   40/   89]
Per-example loss in batch: 0.246366  [   42/   89]
Per-example loss in batch: 0.232280  [   44/   89]
Per-example loss in batch: 0.243996  [   46/   89]
Per-example loss in batch: 0.304127  [   48/   89]
Per-example loss in batch: 0.247745  [   50/   89]
Per-example loss in batch: 0.283065  [   52/   89]
Per-example loss in batch: 0.293077  [   54/   89]
Per-example loss in batch: 0.213656  [   56/   89]
Per-example loss in batch: 0.288620  [   58/   89]
Per-example loss in batch: 0.272550  [   60/   89]
Per-example loss in batch: 0.215554  [   62/   89]
Per-example loss in batch: 0.298293  [   64/   89]
Per-example loss in batch: 0.216590  [   66/   89]
Per-example loss in batch: 0.252445  [   68/   89]
Per-example loss in batch: 0.273564  [   70/   89]
Per-example loss in batch: 0.255964  [   72/   89]
Per-example loss in batch: 0.256702  [   74/   89]
Per-example loss in batch: 0.307723  [   76/   89]
Per-example loss in batch: 0.253498  [   78/   89]
Per-example loss in batch: 0.233721  [   80/   89]
Per-example loss in batch: 0.227109  [   82/   89]
Per-example loss in batch: 0.237706  [   84/   89]
Per-example loss in batch: 0.364110  [   86/   89]
Per-example loss in batch: 0.241636  [   88/   89]
Per-example loss in batch: 0.646361  [   89/   89]
Train Error: Avg loss: 0.26656434
validation Error: 
 Avg loss: 0.28109944 
 F1: 0.474623 
 Precision: 0.558299 
 Recall: 0.412760
 IoU: 0.311151

test Error: 
 Avg loss: 0.26100207 
 F1: 0.509706 
 Precision: 0.614446 
 Recall: 0.435474
 IoU: 0.342017

We have finished training iteration 278
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_276_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.233402  [    2/   89]
Per-example loss in batch: 0.299982  [    4/   89]
Per-example loss in batch: 0.238867  [    6/   89]
Per-example loss in batch: 0.293472  [    8/   89]
Per-example loss in batch: 0.320814  [   10/   89]
Per-example loss in batch: 0.219145  [   12/   89]
Per-example loss in batch: 0.279792  [   14/   89]
Per-example loss in batch: 0.337116  [   16/   89]
Per-example loss in batch: 0.295743  [   18/   89]
Per-example loss in batch: 0.231862  [   20/   89]
Per-example loss in batch: 0.295004  [   22/   89]
Per-example loss in batch: 0.255151  [   24/   89]
Per-example loss in batch: 0.226317  [   26/   89]
Per-example loss in batch: 0.264889  [   28/   89]
Per-example loss in batch: 0.204438  [   30/   89]
Per-example loss in batch: 0.241016  [   32/   89]
Per-example loss in batch: 0.297366  [   34/   89]
Per-example loss in batch: 0.313769  [   36/   89]
Per-example loss in batch: 0.278514  [   38/   89]
Per-example loss in batch: 0.243585  [   40/   89]
Per-example loss in batch: 0.235504  [   42/   89]
Per-example loss in batch: 0.228957  [   44/   89]
Per-example loss in batch: 0.280782  [   46/   89]
Per-example loss in batch: 0.245758  [   48/   89]
Per-example loss in batch: 0.287219  [   50/   89]
Per-example loss in batch: 0.228505  [   52/   89]
Per-example loss in batch: 0.211485  [   54/   89]
Per-example loss in batch: 0.256301  [   56/   89]
Per-example loss in batch: 0.232534  [   58/   89]
Per-example loss in batch: 0.247331  [   60/   89]
Per-example loss in batch: 0.268941  [   62/   89]
Per-example loss in batch: 0.230288  [   64/   89]
Per-example loss in batch: 0.229756  [   66/   89]
Per-example loss in batch: 0.213135  [   68/   89]
Per-example loss in batch: 0.248859  [   70/   89]
Per-example loss in batch: 0.267952  [   72/   89]
Per-example loss in batch: 0.239941  [   74/   89]
Per-example loss in batch: 0.330100  [   76/   89]
Per-example loss in batch: 0.305524  [   78/   89]
Per-example loss in batch: 0.331361  [   80/   89]
Per-example loss in batch: 0.341573  [   82/   89]
Per-example loss in batch: 0.247353  [   84/   89]
Per-example loss in batch: 0.302847  [   86/   89]
Per-example loss in batch: 0.226846  [   88/   89]
Per-example loss in batch: 0.454584  [   89/   89]
Train Error: Avg loss: 0.26598617
validation Error: 
 Avg loss: 0.28380494 
 F1: 0.474517 
 Precision: 0.551691 
 Recall: 0.416285
 IoU: 0.311060

test Error: 
 Avg loss: 0.26099536 
 F1: 0.509404 
 Precision: 0.604124 
 Recall: 0.440360
 IoU: 0.341745

We have finished training iteration 279
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_277_.pth
Per-example loss in batch: 0.210951  [    2/   89]
Per-example loss in batch: 0.315250  [    4/   89]
Per-example loss in batch: 0.295286  [    6/   89]
Per-example loss in batch: 0.240619  [    8/   89]
Per-example loss in batch: 0.228070  [   10/   89]
Per-example loss in batch: 0.306254  [   12/   89]
Per-example loss in batch: 0.236801  [   14/   89]
Per-example loss in batch: 0.252914  [   16/   89]
Per-example loss in batch: 0.326493  [   18/   89]
Per-example loss in batch: 0.247348  [   20/   89]
Per-example loss in batch: 0.229435  [   22/   89]
Per-example loss in batch: 0.220819  [   24/   89]
Per-example loss in batch: 0.254010  [   26/   89]
Per-example loss in batch: 0.322855  [   28/   89]
Per-example loss in batch: 0.227781  [   30/   89]
Per-example loss in batch: 0.313918  [   32/   89]
Per-example loss in batch: 0.244407  [   34/   89]
Per-example loss in batch: 0.236494  [   36/   89]
Per-example loss in batch: 0.214391  [   38/   89]
Per-example loss in batch: 0.243085  [   40/   89]
Per-example loss in batch: 0.288216  [   42/   89]
Per-example loss in batch: 0.262699  [   44/   89]
Per-example loss in batch: 0.208134  [   46/   89]
Per-example loss in batch: 0.222109  [   48/   89]
Per-example loss in batch: 0.238800  [   50/   89]
Per-example loss in batch: 0.227276  [   52/   89]
Per-example loss in batch: 0.230012  [   54/   89]
Per-example loss in batch: 0.232162  [   56/   89]
Per-example loss in batch: 0.261215  [   58/   89]
Per-example loss in batch: 0.253627  [   60/   89]
Per-example loss in batch: 0.308782  [   62/   89]
Per-example loss in batch: 0.279222  [   64/   89]
Per-example loss in batch: 0.329380  [   66/   89]
Per-example loss in batch: 0.276520  [   68/   89]
Per-example loss in batch: 0.275806  [   70/   89]
Per-example loss in batch: 0.222205  [   72/   89]
Per-example loss in batch: 0.223502  [   74/   89]
Per-example loss in batch: 0.338021  [   76/   89]
Per-example loss in batch: 0.230189  [   78/   89]
Per-example loss in batch: 0.189930  [   80/   89]
Per-example loss in batch: 0.261033  [   82/   89]
Per-example loss in batch: 0.254715  [   84/   89]
Per-example loss in batch: 0.299662  [   86/   89]
Per-example loss in batch: 0.237231  [   88/   89]
Per-example loss in batch: 0.465552  [   89/   89]
Train Error: Avg loss: 0.25955960
validation Error: 
 Avg loss: 0.28231408 
 F1: 0.473429 
 Precision: 0.549282 
 Recall: 0.415983
 IoU: 0.310125

test Error: 
 Avg loss: 0.26176756 
 F1: 0.508016 
 Precision: 0.603031 
 Recall: 0.438867
 IoU: 0.340497

We have finished training iteration 280
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_278_.pth
Per-example loss in batch: 0.293647  [    2/   89]
Per-example loss in batch: 0.258953  [    4/   89]
Per-example loss in batch: 0.279546  [    6/   89]
Per-example loss in batch: 0.223976  [    8/   89]
Per-example loss in batch: 0.231009  [   10/   89]
Per-example loss in batch: 0.250935  [   12/   89]
Per-example loss in batch: 0.281699  [   14/   89]
Per-example loss in batch: 0.214866  [   16/   89]
Per-example loss in batch: 0.198857  [   18/   89]
Per-example loss in batch: 0.250409  [   20/   89]
Per-example loss in batch: 0.233740  [   22/   89]
Per-example loss in batch: 0.299031  [   24/   89]
Per-example loss in batch: 0.305353  [   26/   89]
Per-example loss in batch: 0.229719  [   28/   89]
Per-example loss in batch: 0.212974  [   30/   89]
Per-example loss in batch: 0.248876  [   32/   89]
Per-example loss in batch: 0.231237  [   34/   89]
Per-example loss in batch: 0.246256  [   36/   89]
Per-example loss in batch: 0.265802  [   38/   89]
Per-example loss in batch: 0.209372  [   40/   89]
Per-example loss in batch: 0.224013  [   42/   89]
Per-example loss in batch: 0.239012  [   44/   89]
Per-example loss in batch: 0.205527  [   46/   89]
Per-example loss in batch: 0.329422  [   48/   89]
Per-example loss in batch: 0.252307  [   50/   89]
Per-example loss in batch: 0.271706  [   52/   89]
Per-example loss in batch: 0.287560  [   54/   89]
Per-example loss in batch: 0.236889  [   56/   89]
Per-example loss in batch: 0.308302  [   58/   89]
Per-example loss in batch: 0.279510  [   60/   89]
Per-example loss in batch: 0.213831  [   62/   89]
Per-example loss in batch: 0.268207  [   64/   89]
Per-example loss in batch: 0.225303  [   66/   89]
Per-example loss in batch: 0.313730  [   68/   89]
Per-example loss in batch: 0.243236  [   70/   89]
Per-example loss in batch: 0.242417  [   72/   89]
Per-example loss in batch: 0.225184  [   74/   89]
Per-example loss in batch: 0.256467  [   76/   89]
Per-example loss in batch: 0.246593  [   78/   89]
Per-example loss in batch: 0.231696  [   80/   89]
Per-example loss in batch: 0.300634  [   82/   89]
Per-example loss in batch: 0.304413  [   84/   89]
Per-example loss in batch: 0.277438  [   86/   89]
Per-example loss in batch: 0.322656  [   88/   89]
Per-example loss in batch: 0.555923  [   89/   89]
Train Error: Avg loss: 0.25955667
validation Error: 
 Avg loss: 0.28485467 
 F1: 0.473701 
 Precision: 0.566217 
 Recall: 0.407173
 IoU: 0.310360

test Error: 
 Avg loss: 0.26234090 
 F1: 0.506935 
 Precision: 0.621464 
 Recall: 0.428050
 IoU: 0.339526

We have finished training iteration 281
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_279_.pth
Per-example loss in batch: 0.237289  [    2/   89]
Per-example loss in batch: 0.265729  [    4/   89]
Per-example loss in batch: 0.208671  [    6/   89]
Per-example loss in batch: 0.230462  [    8/   89]
Per-example loss in batch: 0.188723  [   10/   89]
Per-example loss in batch: 0.217952  [   12/   89]
Per-example loss in batch: 0.232161  [   14/   89]
Per-example loss in batch: 0.322883  [   16/   89]
Per-example loss in batch: 0.283911  [   18/   89]
Per-example loss in batch: 0.262002  [   20/   89]
Per-example loss in batch: 0.221542  [   22/   89]
Per-example loss in batch: 0.268785  [   24/   89]
Per-example loss in batch: 0.240026  [   26/   89]
Per-example loss in batch: 0.227346  [   28/   89]
Per-example loss in batch: 0.195215  [   30/   89]
Per-example loss in batch: 0.289255  [   32/   89]
Per-example loss in batch: 0.280705  [   34/   89]
Per-example loss in batch: 0.266771  [   36/   89]
Per-example loss in batch: 0.274101  [   38/   89]
Per-example loss in batch: 0.249228  [   40/   89]
Per-example loss in batch: 0.248067  [   42/   89]
Per-example loss in batch: 0.275726  [   44/   89]
Per-example loss in batch: 0.308719  [   46/   89]
Per-example loss in batch: 0.239427  [   48/   89]
Per-example loss in batch: 0.216416  [   50/   89]
Per-example loss in batch: 0.230849  [   52/   89]
Per-example loss in batch: 0.240730  [   54/   89]
Per-example loss in batch: 0.225365  [   56/   89]
Per-example loss in batch: 0.235695  [   58/   89]
Per-example loss in batch: 0.295192  [   60/   89]
Per-example loss in batch: 0.239107  [   62/   89]
Per-example loss in batch: 0.218526  [   64/   89]
Per-example loss in batch: 0.317277  [   66/   89]
Per-example loss in batch: 0.349030  [   68/   89]
Per-example loss in batch: 0.243490  [   70/   89]
Per-example loss in batch: 0.321851  [   72/   89]
Per-example loss in batch: 0.234783  [   74/   89]
Per-example loss in batch: 0.243688  [   76/   89]
Per-example loss in batch: 0.272461  [   78/   89]
Per-example loss in batch: 0.305545  [   80/   89]
Per-example loss in batch: 0.238737  [   82/   89]
Per-example loss in batch: 0.365235  [   84/   89]
Per-example loss in batch: 0.231325  [   86/   89]
Per-example loss in batch: 0.316062  [   88/   89]
Per-example loss in batch: 0.580705  [   89/   89]
Train Error: Avg loss: 0.26216663
validation Error: 
 Avg loss: 0.28062395 
 F1: 0.472943 
 Precision: 0.559276 
 Recall: 0.409700
 IoU: 0.309709

test Error: 
 Avg loss: 0.26288210 
 F1: 0.505866 
 Precision: 0.612155 
 Recall: 0.431026
 IoU: 0.338568

We have finished training iteration 282
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_280_.pth
Per-example loss in batch: 0.220930  [    2/   89]
Per-example loss in batch: 0.348553  [    4/   89]
Per-example loss in batch: 0.248445  [    6/   89]
Per-example loss in batch: 0.209809  [    8/   89]
Per-example loss in batch: 0.236995  [   10/   89]
Per-example loss in batch: 0.217376  [   12/   89]
Per-example loss in batch: 0.197844  [   14/   89]
Per-example loss in batch: 0.276798  [   16/   89]
Per-example loss in batch: 0.267237  [   18/   89]
Per-example loss in batch: 0.334712  [   20/   89]
Per-example loss in batch: 0.328422  [   22/   89]
Per-example loss in batch: 0.256034  [   24/   89]
Per-example loss in batch: 0.303194  [   26/   89]
Per-example loss in batch: 0.317858  [   28/   89]
Per-example loss in batch: 0.296978  [   30/   89]
Per-example loss in batch: 0.214927  [   32/   89]
Per-example loss in batch: 0.247591  [   34/   89]
Per-example loss in batch: 0.270654  [   36/   89]
Per-example loss in batch: 0.380403  [   38/   89]
Per-example loss in batch: 0.242337  [   40/   89]
Per-example loss in batch: 0.296818  [   42/   89]
Per-example loss in batch: 0.326720  [   44/   89]
Per-example loss in batch: 0.242992  [   46/   89]
Per-example loss in batch: 0.283297  [   48/   89]
Per-example loss in batch: 0.291073  [   50/   89]
Per-example loss in batch: 0.256613  [   52/   89]
Per-example loss in batch: 0.226373  [   54/   89]
Per-example loss in batch: 0.258310  [   56/   89]
Per-example loss in batch: 0.335789  [   58/   89]
Per-example loss in batch: 0.294992  [   60/   89]
Per-example loss in batch: 0.215174  [   62/   89]
Per-example loss in batch: 0.332957  [   64/   89]
Per-example loss in batch: 0.228870  [   66/   89]
Per-example loss in batch: 0.272996  [   68/   89]
Per-example loss in batch: 0.265460  [   70/   89]
Per-example loss in batch: 0.272278  [   72/   89]
Per-example loss in batch: 0.225389  [   74/   89]
Per-example loss in batch: 0.302877  [   76/   89]
Per-example loss in batch: 0.259402  [   78/   89]
Per-example loss in batch: 0.215329  [   80/   89]
Per-example loss in batch: 0.340968  [   82/   89]
Per-example loss in batch: 0.194079  [   84/   89]
Per-example loss in batch: 0.227505  [   86/   89]
Per-example loss in batch: 0.273312  [   88/   89]
Per-example loss in batch: 0.659221  [   89/   89]
Train Error: Avg loss: 0.27384904
validation Error: 
 Avg loss: 0.28279384 
 F1: 0.474324 
 Precision: 0.557484 
 Recall: 0.412754
 IoU: 0.310895

test Error: 
 Avg loss: 0.26157710 
 F1: 0.508362 
 Precision: 0.612103 
 Recall: 0.434689
 IoU: 0.340807

We have finished training iteration 283
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_281_.pth
Per-example loss in batch: 0.251301  [    2/   89]
Per-example loss in batch: 0.265284  [    4/   89]
Per-example loss in batch: 0.279365  [    6/   89]
Per-example loss in batch: 0.247451  [    8/   89]
Per-example loss in batch: 0.340668  [   10/   89]
Per-example loss in batch: 0.223833  [   12/   89]
Per-example loss in batch: 0.243414  [   14/   89]
Per-example loss in batch: 0.238308  [   16/   89]
Per-example loss in batch: 0.231957  [   18/   89]
Per-example loss in batch: 0.276403  [   20/   89]
Per-example loss in batch: 0.249265  [   22/   89]
Per-example loss in batch: 0.353700  [   24/   89]
Per-example loss in batch: 0.272363  [   26/   89]
Per-example loss in batch: 0.299182  [   28/   89]
Per-example loss in batch: 0.256207  [   30/   89]
Per-example loss in batch: 0.283212  [   32/   89]
Per-example loss in batch: 0.341852  [   34/   89]
Per-example loss in batch: 0.256583  [   36/   89]
Per-example loss in batch: 0.264117  [   38/   89]
Per-example loss in batch: 0.233449  [   40/   89]
Per-example loss in batch: 0.316453  [   42/   89]
Per-example loss in batch: 0.240919  [   44/   89]
Per-example loss in batch: 0.290748  [   46/   89]
Per-example loss in batch: 0.222201  [   48/   89]
Per-example loss in batch: 0.256999  [   50/   89]
Per-example loss in batch: 0.358248  [   52/   89]
Per-example loss in batch: 0.220927  [   54/   89]
Per-example loss in batch: 0.222400  [   56/   89]
Per-example loss in batch: 0.327817  [   58/   89]
Per-example loss in batch: 0.275279  [   60/   89]
Per-example loss in batch: 0.221824  [   62/   89]
Per-example loss in batch: 0.216419  [   64/   89]
Per-example loss in batch: 0.253939  [   66/   89]
Per-example loss in batch: 0.336720  [   68/   89]
Per-example loss in batch: 0.253601  [   70/   89]
Per-example loss in batch: 0.253417  [   72/   89]
Per-example loss in batch: 0.216682  [   74/   89]
Per-example loss in batch: 0.238114  [   76/   89]
Per-example loss in batch: 0.260826  [   78/   89]
Per-example loss in batch: 0.206600  [   80/   89]
Per-example loss in batch: 0.214595  [   82/   89]
Per-example loss in batch: 0.282200  [   84/   89]
Per-example loss in batch: 0.249825  [   86/   89]
Per-example loss in batch: 0.219954  [   88/   89]
Per-example loss in batch: 0.595702  [   89/   89]
Train Error: Avg loss: 0.26657235
validation Error: 
 Avg loss: 0.28514403 
 F1: 0.474168 
 Precision: 0.553617 
 Recall: 0.414661
 IoU: 0.310761

test Error: 
 Avg loss: 0.26130945 
 F1: 0.509222 
 Precision: 0.608588 
 Recall: 0.437749
 IoU: 0.341581

We have finished training iteration 284
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_282_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.226113  [    2/   89]
Per-example loss in batch: 0.242159  [    4/   89]
Per-example loss in batch: 0.260810  [    6/   89]
Per-example loss in batch: 0.285505  [    8/   89]
Per-example loss in batch: 0.235504  [   10/   89]
Per-example loss in batch: 0.223487  [   12/   89]
Per-example loss in batch: 0.263286  [   14/   89]
Per-example loss in batch: 0.275696  [   16/   89]
Per-example loss in batch: 0.353707  [   18/   89]
Per-example loss in batch: 0.277277  [   20/   89]
Per-example loss in batch: 0.275116  [   22/   89]
Per-example loss in batch: 0.364107  [   24/   89]
Per-example loss in batch: 0.280829  [   26/   89]
Per-example loss in batch: 0.244573  [   28/   89]
Per-example loss in batch: 0.229795  [   30/   89]
Per-example loss in batch: 0.302751  [   32/   89]
Per-example loss in batch: 0.212278  [   34/   89]
Per-example loss in batch: 0.281036  [   36/   89]
Per-example loss in batch: 0.210846  [   38/   89]
Per-example loss in batch: 0.210250  [   40/   89]
Per-example loss in batch: 0.232060  [   42/   89]
Per-example loss in batch: 0.293991  [   44/   89]
Per-example loss in batch: 0.259615  [   46/   89]
Per-example loss in batch: 0.290829  [   48/   89]
Per-example loss in batch: 0.241558  [   50/   89]
Per-example loss in batch: 0.255916  [   52/   89]
Per-example loss in batch: 0.244275  [   54/   89]
Per-example loss in batch: 0.269868  [   56/   89]
Per-example loss in batch: 0.249249  [   58/   89]
Per-example loss in batch: 0.204255  [   60/   89]
Per-example loss in batch: 0.225354  [   62/   89]
Per-example loss in batch: 0.224984  [   64/   89]
Per-example loss in batch: 0.320479  [   66/   89]
Per-example loss in batch: 0.208805  [   68/   89]
Per-example loss in batch: 0.340520  [   70/   89]
Per-example loss in batch: 0.310761  [   72/   89]
Per-example loss in batch: 0.264329  [   74/   89]
Per-example loss in batch: 0.228249  [   76/   89]
Per-example loss in batch: 0.197943  [   78/   89]
Per-example loss in batch: 0.294862  [   80/   89]
Per-example loss in batch: 0.275089  [   82/   89]
Per-example loss in batch: 0.325968  [   84/   89]
Per-example loss in batch: 0.299881  [   86/   89]
Per-example loss in batch: 0.284788  [   88/   89]
Per-example loss in batch: 0.612305  [   89/   89]
Train Error: Avg loss: 0.26752596
validation Error: 
 Avg loss: 0.28820063 
 F1: 0.474465 
 Precision: 0.535414 
 Recall: 0.425974
 IoU: 0.311016

test Error: 
 Avg loss: 0.26103066 
 F1: 0.509453 
 Precision: 0.588205 
 Recall: 0.449298
 IoU: 0.341789

We have finished training iteration 285
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_283_.pth
Per-example loss in batch: 0.238600  [    2/   89]
Per-example loss in batch: 0.277209  [    4/   89]
Per-example loss in batch: 0.259376  [    6/   89]
Per-example loss in batch: 0.246807  [    8/   89]
Per-example loss in batch: 0.232291  [   10/   89]
Per-example loss in batch: 0.313114  [   12/   89]
Per-example loss in batch: 0.267707  [   14/   89]
Per-example loss in batch: 0.213011  [   16/   89]
Per-example loss in batch: 0.288800  [   18/   89]
Per-example loss in batch: 0.225972  [   20/   89]
Per-example loss in batch: 0.282077  [   22/   89]
Per-example loss in batch: 0.241505  [   24/   89]
Per-example loss in batch: 0.331148  [   26/   89]
Per-example loss in batch: 0.240866  [   28/   89]
Per-example loss in batch: 0.230730  [   30/   89]
Per-example loss in batch: 0.256041  [   32/   89]
Per-example loss in batch: 0.233983  [   34/   89]
Per-example loss in batch: 0.331133  [   36/   89]
Per-example loss in batch: 0.307557  [   38/   89]
Per-example loss in batch: 0.211409  [   40/   89]
Per-example loss in batch: 0.286956  [   42/   89]
Per-example loss in batch: 0.340858  [   44/   89]
Per-example loss in batch: 0.260603  [   46/   89]
Per-example loss in batch: 0.300170  [   48/   89]
Per-example loss in batch: 0.184934  [   50/   89]
Per-example loss in batch: 0.216090  [   52/   89]
Per-example loss in batch: 0.221824  [   54/   89]
Per-example loss in batch: 0.218386  [   56/   89]
Per-example loss in batch: 0.301092  [   58/   89]
Per-example loss in batch: 0.230247  [   60/   89]
Per-example loss in batch: 0.284641  [   62/   89]
Per-example loss in batch: 0.255347  [   64/   89]
Per-example loss in batch: 0.267951  [   66/   89]
Per-example loss in batch: 0.228913  [   68/   89]
Per-example loss in batch: 0.231821  [   70/   89]
Per-example loss in batch: 0.285239  [   72/   89]
Per-example loss in batch: 0.216798  [   74/   89]
Per-example loss in batch: 0.294487  [   76/   89]
Per-example loss in batch: 0.261377  [   78/   89]
Per-example loss in batch: 0.252828  [   80/   89]
Per-example loss in batch: 0.248427  [   82/   89]
Per-example loss in batch: 0.245432  [   84/   89]
Per-example loss in batch: 0.330694  [   86/   89]
Per-example loss in batch: 0.282625  [   88/   89]
Per-example loss in batch: 0.469583  [   89/   89]
Train Error: Avg loss: 0.26318803
validation Error: 
 Avg loss: 0.29173085 
 F1: 0.471477 
 Precision: 0.541098 
 Recall: 0.417730
 IoU: 0.308453

test Error: 
 Avg loss: 0.26253641 
 F1: 0.506510 
 Precision: 0.590238 
 Recall: 0.443586
 IoU: 0.339146

We have finished training iteration 286
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_284_.pth
Per-example loss in batch: 0.213531  [    2/   89]
Per-example loss in batch: 0.281062  [    4/   89]
Per-example loss in batch: 0.249991  [    6/   89]
Per-example loss in batch: 0.287478  [    8/   89]
Per-example loss in batch: 0.247341  [   10/   89]
Per-example loss in batch: 0.326375  [   12/   89]
Per-example loss in batch: 0.259232  [   14/   89]
Per-example loss in batch: 0.263759  [   16/   89]
Per-example loss in batch: 0.264045  [   18/   89]
Per-example loss in batch: 0.244638  [   20/   89]
Per-example loss in batch: 0.226058  [   22/   89]
Per-example loss in batch: 0.210898  [   24/   89]
Per-example loss in batch: 0.238615  [   26/   89]
Per-example loss in batch: 0.257114  [   28/   89]
Per-example loss in batch: 0.266948  [   30/   89]
Per-example loss in batch: 0.222608  [   32/   89]
Per-example loss in batch: 0.264116  [   34/   89]
Per-example loss in batch: 0.221318  [   36/   89]
Per-example loss in batch: 0.243445  [   38/   89]
Per-example loss in batch: 0.391788  [   40/   89]
Per-example loss in batch: 0.334010  [   42/   89]
Per-example loss in batch: 0.202868  [   44/   89]
Per-example loss in batch: 0.274446  [   46/   89]
Per-example loss in batch: 0.260677  [   48/   89]
Per-example loss in batch: 0.293207  [   50/   89]
Per-example loss in batch: 0.292737  [   52/   89]
Per-example loss in batch: 0.247318  [   54/   89]
Per-example loss in batch: 0.289198  [   56/   89]
Per-example loss in batch: 0.258332  [   58/   89]
Per-example loss in batch: 0.360187  [   60/   89]
Per-example loss in batch: 0.248293  [   62/   89]
Per-example loss in batch: 0.335471  [   64/   89]
Per-example loss in batch: 0.196918  [   66/   89]
Per-example loss in batch: 0.293807  [   68/   89]
Per-example loss in batch: 0.198322  [   70/   89]
Per-example loss in batch: 0.230848  [   72/   89]
Per-example loss in batch: 0.250386  [   74/   89]
Per-example loss in batch: 0.214840  [   76/   89]
Per-example loss in batch: 0.215418  [   78/   89]
Per-example loss in batch: 0.327515  [   80/   89]
Per-example loss in batch: 0.200303  [   82/   89]
Per-example loss in batch: 0.331347  [   84/   89]
Per-example loss in batch: 0.330035  [   86/   89]
Per-example loss in batch: 0.225733  [   88/   89]
Per-example loss in batch: 0.434750  [   89/   89]
Train Error: Avg loss: 0.26539222
validation Error: 
 Avg loss: 0.28463610 
 F1: 0.472995 
 Precision: 0.571415 
 Recall: 0.403498
 IoU: 0.309754

test Error: 
 Avg loss: 0.26364725 
 F1: 0.504368 
 Precision: 0.625547 
 Recall: 0.422518
 IoU: 0.337227

We have finished training iteration 287
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_285_.pth
Per-example loss in batch: 0.231050  [    2/   89]
Per-example loss in batch: 0.210652  [    4/   89]
Per-example loss in batch: 0.203285  [    6/   89]
Per-example loss in batch: 0.252858  [    8/   89]
Per-example loss in batch: 0.316062  [   10/   89]
Per-example loss in batch: 0.228066  [   12/   89]
Per-example loss in batch: 0.211792  [   14/   89]
Per-example loss in batch: 0.221324  [   16/   89]
Per-example loss in batch: 0.362739  [   18/   89]
Per-example loss in batch: 0.262138  [   20/   89]
Per-example loss in batch: 0.252656  [   22/   89]
Per-example loss in batch: 0.286336  [   24/   89]
Per-example loss in batch: 0.266560  [   26/   89]
Per-example loss in batch: 0.219357  [   28/   89]
Per-example loss in batch: 0.245735  [   30/   89]
Per-example loss in batch: 0.276795  [   32/   89]
Per-example loss in batch: 0.228190  [   34/   89]
Per-example loss in batch: 0.204830  [   36/   89]
Per-example loss in batch: 0.323021  [   38/   89]
Per-example loss in batch: 0.281171  [   40/   89]
Per-example loss in batch: 0.294722  [   42/   89]
Per-example loss in batch: 0.235953  [   44/   89]
Per-example loss in batch: 0.312489  [   46/   89]
Per-example loss in batch: 0.329621  [   48/   89]
Per-example loss in batch: 0.262917  [   50/   89]
Per-example loss in batch: 0.255876  [   52/   89]
Per-example loss in batch: 0.219424  [   54/   89]
Per-example loss in batch: 0.255953  [   56/   89]
Per-example loss in batch: 0.329279  [   58/   89]
Per-example loss in batch: 0.306368  [   60/   89]
Per-example loss in batch: 0.211906  [   62/   89]
Per-example loss in batch: 0.227078  [   64/   89]
Per-example loss in batch: 0.195703  [   66/   89]
Per-example loss in batch: 0.259933  [   68/   89]
Per-example loss in batch: 0.321134  [   70/   89]
Per-example loss in batch: 0.335320  [   72/   89]
Per-example loss in batch: 0.296126  [   74/   89]
Per-example loss in batch: 0.331427  [   76/   89]
Per-example loss in batch: 0.311575  [   78/   89]
Per-example loss in batch: 0.275704  [   80/   89]
Per-example loss in batch: 0.253689  [   82/   89]
Per-example loss in batch: 0.210387  [   84/   89]
Per-example loss in batch: 0.340524  [   86/   89]
Per-example loss in batch: 0.338131  [   88/   89]
Per-example loss in batch: 0.449429  [   89/   89]
Train Error: Avg loss: 0.27012512
validation Error: 
 Avg loss: 0.28655991 
 F1: 0.474060 
 Precision: 0.548817 
 Recall: 0.417228
 IoU: 0.310668

test Error: 
 Avg loss: 0.26104681 
 F1: 0.509444 
 Precision: 0.603625 
 Recall: 0.440686
 IoU: 0.341782

We have finished training iteration 288
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_286_.pth
Per-example loss in batch: 0.233372  [    2/   89]
Per-example loss in batch: 0.353924  [    4/   89]
Per-example loss in batch: 0.247594  [    6/   89]
Per-example loss in batch: 0.231303  [    8/   89]
Per-example loss in batch: 0.269745  [   10/   89]
Per-example loss in batch: 0.311635  [   12/   89]
Per-example loss in batch: 0.256118  [   14/   89]
Per-example loss in batch: 0.216947  [   16/   89]
Per-example loss in batch: 0.330668  [   18/   89]
Per-example loss in batch: 0.275453  [   20/   89]
Per-example loss in batch: 0.324974  [   22/   89]
Per-example loss in batch: 0.276628  [   24/   89]
Per-example loss in batch: 0.322273  [   26/   89]
Per-example loss in batch: 0.247859  [   28/   89]
Per-example loss in batch: 0.218973  [   30/   89]
Per-example loss in batch: 0.263648  [   32/   89]
Per-example loss in batch: 0.294644  [   34/   89]
Per-example loss in batch: 0.306759  [   36/   89]
Per-example loss in batch: 0.271089  [   38/   89]
Per-example loss in batch: 0.241807  [   40/   89]
Per-example loss in batch: 0.226238  [   42/   89]
Per-example loss in batch: 0.274381  [   44/   89]
Per-example loss in batch: 0.202238  [   46/   89]
Per-example loss in batch: 0.220372  [   48/   89]
Per-example loss in batch: 0.244678  [   50/   89]
Per-example loss in batch: 0.296819  [   52/   89]
Per-example loss in batch: 0.287837  [   54/   89]
Per-example loss in batch: 0.310875  [   56/   89]
Per-example loss in batch: 0.247285  [   58/   89]
Per-example loss in batch: 0.224078  [   60/   89]
Per-example loss in batch: 0.339563  [   62/   89]
Per-example loss in batch: 0.263329  [   64/   89]
Per-example loss in batch: 0.215370  [   66/   89]
Per-example loss in batch: 0.250405  [   68/   89]
Per-example loss in batch: 0.244287  [   70/   89]
Per-example loss in batch: 0.297616  [   72/   89]
Per-example loss in batch: 0.208221  [   74/   89]
Per-example loss in batch: 0.275131  [   76/   89]
Per-example loss in batch: 0.245891  [   78/   89]
Per-example loss in batch: 0.210092  [   80/   89]
Per-example loss in batch: 0.234688  [   82/   89]
Per-example loss in batch: 0.332575  [   84/   89]
Per-example loss in batch: 0.230873  [   86/   89]
Per-example loss in batch: 0.212096  [   88/   89]
Per-example loss in batch: 0.526122  [   89/   89]
Train Error: Avg loss: 0.26636875
validation Error: 
 Avg loss: 0.27578643 
 F1: 0.474709 
 Precision: 0.512374 
 Recall: 0.442203
 IoU: 0.311225

test Error: 
 Avg loss: 0.25866361 
 F1: 0.514557 
 Precision: 0.569803 
 Recall: 0.469077
 IoU: 0.346400

We have finished training iteration 289
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_287_.pth
Per-example loss in batch: 0.204537  [    2/   89]
Per-example loss in batch: 0.193315  [    4/   89]
Per-example loss in batch: 0.254330  [    6/   89]
Per-example loss in batch: 0.244558  [    8/   89]
Per-example loss in batch: 0.215635  [   10/   89]
Per-example loss in batch: 0.252024  [   12/   89]
Per-example loss in batch: 0.229121  [   14/   89]
Per-example loss in batch: 0.246693  [   16/   89]
Per-example loss in batch: 0.338706  [   18/   89]
Per-example loss in batch: 0.250145  [   20/   89]
Per-example loss in batch: 0.242581  [   22/   89]
Per-example loss in batch: 0.296635  [   24/   89]
Per-example loss in batch: 0.262757  [   26/   89]
Per-example loss in batch: 0.319785  [   28/   89]
Per-example loss in batch: 0.220125  [   30/   89]
Per-example loss in batch: 0.255740  [   32/   89]
Per-example loss in batch: 0.286816  [   34/   89]
Per-example loss in batch: 0.311112  [   36/   89]
Per-example loss in batch: 0.393613  [   38/   89]
Per-example loss in batch: 0.301961  [   40/   89]
Per-example loss in batch: 0.235137  [   42/   89]
Per-example loss in batch: 0.299523  [   44/   89]
Per-example loss in batch: 0.240834  [   46/   89]
Per-example loss in batch: 0.270950  [   48/   89]
Per-example loss in batch: 0.243094  [   50/   89]
Per-example loss in batch: 0.298566  [   52/   89]
Per-example loss in batch: 0.226914  [   54/   89]
Per-example loss in batch: 0.284406  [   56/   89]
Per-example loss in batch: 0.308676  [   58/   89]
Per-example loss in batch: 0.241945  [   60/   89]
Per-example loss in batch: 0.194525  [   62/   89]
Per-example loss in batch: 0.256859  [   64/   89]
Per-example loss in batch: 0.286227  [   66/   89]
Per-example loss in batch: 0.304249  [   68/   89]
Per-example loss in batch: 0.258838  [   70/   89]
Per-example loss in batch: 0.220139  [   72/   89]
Per-example loss in batch: 0.310112  [   74/   89]
Per-example loss in batch: 0.248889  [   76/   89]
Per-example loss in batch: 0.252174  [   78/   89]
Per-example loss in batch: 0.219885  [   80/   89]
Per-example loss in batch: 0.218058  [   82/   89]
Per-example loss in batch: 0.314966  [   84/   89]
Per-example loss in batch: 0.226992  [   86/   89]
Per-example loss in batch: 0.340839  [   88/   89]
Per-example loss in batch: 0.616086  [   89/   89]
Train Error: Avg loss: 0.26811300
validation Error: 
 Avg loss: 0.28300901 
 F1: 0.474108 
 Precision: 0.548125 
 Recall: 0.417702
 IoU: 0.310708

test Error: 
 Avg loss: 0.26092121 
 F1: 0.509776 
 Precision: 0.604351 
 Recall: 0.440795
 IoU: 0.342080

We have finished training iteration 290
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_288_.pth
Per-example loss in batch: 0.318844  [    2/   89]
Per-example loss in batch: 0.317445  [    4/   89]
Per-example loss in batch: 0.230882  [    6/   89]
Per-example loss in batch: 0.361601  [    8/   89]
Per-example loss in batch: 0.329223  [   10/   89]
Per-example loss in batch: 0.241897  [   12/   89]
Per-example loss in batch: 0.279976  [   14/   89]
Per-example loss in batch: 0.324566  [   16/   89]
Per-example loss in batch: 0.302364  [   18/   89]
Per-example loss in batch: 0.228831  [   20/   89]
Per-example loss in batch: 0.257482  [   22/   89]
Per-example loss in batch: 0.263431  [   24/   89]
Per-example loss in batch: 0.255866  [   26/   89]
Per-example loss in batch: 0.305816  [   28/   89]
Per-example loss in batch: 0.219968  [   30/   89]
Per-example loss in batch: 0.252316  [   32/   89]
Per-example loss in batch: 0.272061  [   34/   89]
Per-example loss in batch: 0.291466  [   36/   89]
Per-example loss in batch: 0.257545  [   38/   89]
Per-example loss in batch: 0.300557  [   40/   89]
Per-example loss in batch: 0.266195  [   42/   89]
Per-example loss in batch: 0.235832  [   44/   89]
Per-example loss in batch: 0.233552  [   46/   89]
Per-example loss in batch: 0.297003  [   48/   89]
Per-example loss in batch: 0.280417  [   50/   89]
Per-example loss in batch: 0.258268  [   52/   89]
Per-example loss in batch: 0.318401  [   54/   89]
Per-example loss in batch: 0.298667  [   56/   89]
Per-example loss in batch: 0.273486  [   58/   89]
Per-example loss in batch: 0.215725  [   60/   89]
Per-example loss in batch: 0.334995  [   62/   89]
Per-example loss in batch: 0.234024  [   64/   89]
Per-example loss in batch: 0.304969  [   66/   89]
Per-example loss in batch: 0.227401  [   68/   89]
Per-example loss in batch: 0.230655  [   70/   89]
Per-example loss in batch: 0.225119  [   72/   89]
Per-example loss in batch: 0.200535  [   74/   89]
Per-example loss in batch: 0.267610  [   76/   89]
Per-example loss in batch: 0.232418  [   78/   89]
Per-example loss in batch: 0.235468  [   80/   89]
Per-example loss in batch: 0.245215  [   82/   89]
Per-example loss in batch: 0.210543  [   84/   89]
Per-example loss in batch: 0.280526  [   86/   89]
Per-example loss in batch: 0.239811  [   88/   89]
Per-example loss in batch: 0.712378  [   89/   89]
Train Error: Avg loss: 0.27225086
validation Error: 
 Avg loss: 0.28039349 
 F1: 0.475376 
 Precision: 0.539499 
 Recall: 0.424877
 IoU: 0.311799

test Error: 
 Avg loss: 0.26002980 
 F1: 0.511649 
 Precision: 0.594116 
 Recall: 0.449285
 IoU: 0.343769

We have finished training iteration 291
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_188_.pth
Per-example loss in batch: 0.238419  [    2/   89]
Per-example loss in batch: 0.243186  [    4/   89]
Per-example loss in batch: 0.277547  [    6/   89]
Per-example loss in batch: 0.240101  [    8/   89]
Per-example loss in batch: 0.263701  [   10/   89]
Per-example loss in batch: 0.212082  [   12/   89]
Per-example loss in batch: 0.298206  [   14/   89]
Per-example loss in batch: 0.252030  [   16/   89]
Per-example loss in batch: 0.292071  [   18/   89]
Per-example loss in batch: 0.229293  [   20/   89]
Per-example loss in batch: 0.237438  [   22/   89]
Per-example loss in batch: 0.287816  [   24/   89]
Per-example loss in batch: 0.309979  [   26/   89]
Per-example loss in batch: 0.253178  [   28/   89]
Per-example loss in batch: 0.372080  [   30/   89]
Per-example loss in batch: 0.284453  [   32/   89]
Per-example loss in batch: 0.231059  [   34/   89]
Per-example loss in batch: 0.282050  [   36/   89]
Per-example loss in batch: 0.220871  [   38/   89]
Per-example loss in batch: 0.332706  [   40/   89]
Per-example loss in batch: 0.308316  [   42/   89]
Per-example loss in batch: 0.244487  [   44/   89]
Per-example loss in batch: 0.255254  [   46/   89]
Per-example loss in batch: 0.259260  [   48/   89]
Per-example loss in batch: 0.252897  [   50/   89]
Per-example loss in batch: 0.287947  [   52/   89]
Per-example loss in batch: 0.324902  [   54/   89]
Per-example loss in batch: 0.350001  [   56/   89]
Per-example loss in batch: 0.294609  [   58/   89]
Per-example loss in batch: 0.225125  [   60/   89]
Per-example loss in batch: 0.218623  [   62/   89]
Per-example loss in batch: 0.220200  [   64/   89]
Per-example loss in batch: 0.277174  [   66/   89]
Per-example loss in batch: 0.252184  [   68/   89]
Per-example loss in batch: 0.214847  [   70/   89]
Per-example loss in batch: 0.255471  [   72/   89]
Per-example loss in batch: 0.320327  [   74/   89]
Per-example loss in batch: 0.238207  [   76/   89]
Per-example loss in batch: 0.253081  [   78/   89]
Per-example loss in batch: 0.229758  [   80/   89]
Per-example loss in batch: 0.301254  [   82/   89]
Per-example loss in batch: 0.303261  [   84/   89]
Per-example loss in batch: 0.199013  [   86/   89]
Per-example loss in batch: 0.220360  [   88/   89]
Per-example loss in batch: 0.432653  [   89/   89]
Train Error: Avg loss: 0.26699222
validation Error: 
 Avg loss: 0.28357312 
 F1: 0.474859 
 Precision: 0.547250 
 Recall: 0.419383
 IoU: 0.311355

test Error: 
 Avg loss: 0.26061188 
 F1: 0.510703 
 Precision: 0.602309 
 Recall: 0.443284
 IoU: 0.342916

We have finished training iteration 292
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_290_.pth
Per-example loss in batch: 0.332695  [    2/   89]
Per-example loss in batch: 0.287794  [    4/   89]
Per-example loss in batch: 0.235691  [    6/   89]
Per-example loss in batch: 0.291940  [    8/   89]
Per-example loss in batch: 0.234427  [   10/   89]
Per-example loss in batch: 0.318377  [   12/   89]
Per-example loss in batch: 0.304834  [   14/   89]
Per-example loss in batch: 0.330557  [   16/   89]
Per-example loss in batch: 0.268377  [   18/   89]
Per-example loss in batch: 0.217365  [   20/   89]
Per-example loss in batch: 0.224020  [   22/   89]
Per-example loss in batch: 0.250152  [   24/   89]
Per-example loss in batch: 0.268045  [   26/   89]
Per-example loss in batch: 0.265162  [   28/   89]
Per-example loss in batch: 0.207054  [   30/   89]
Per-example loss in batch: 0.219130  [   32/   89]
Per-example loss in batch: 0.262071  [   34/   89]
Per-example loss in batch: 0.212269  [   36/   89]
Per-example loss in batch: 0.217955  [   38/   89]
Per-example loss in batch: 0.247749  [   40/   89]
Per-example loss in batch: 0.335984  [   42/   89]
Per-example loss in batch: 0.241900  [   44/   89]
Per-example loss in batch: 0.292707  [   46/   89]
Per-example loss in batch: 0.249420  [   48/   89]
Per-example loss in batch: 0.300817  [   50/   89]
Per-example loss in batch: 0.259257  [   52/   89]
Per-example loss in batch: 0.322408  [   54/   89]
Per-example loss in batch: 0.247336  [   56/   89]
Per-example loss in batch: 0.250200  [   58/   89]
Per-example loss in batch: 0.295271  [   60/   89]
Per-example loss in batch: 0.229357  [   62/   89]
Per-example loss in batch: 0.289948  [   64/   89]
Per-example loss in batch: 0.223177  [   66/   89]
Per-example loss in batch: 0.308754  [   68/   89]
Per-example loss in batch: 0.350697  [   70/   89]
Per-example loss in batch: 0.230267  [   72/   89]
Per-example loss in batch: 0.229640  [   74/   89]
Per-example loss in batch: 0.226754  [   76/   89]
Per-example loss in batch: 0.202823  [   78/   89]
Per-example loss in batch: 0.316357  [   80/   89]
Per-example loss in batch: 0.287601  [   82/   89]
Per-example loss in batch: 0.208834  [   84/   89]
Per-example loss in batch: 0.222704  [   86/   89]
Per-example loss in batch: 0.268779  [   88/   89]
Per-example loss in batch: 0.690048  [   89/   89]
Train Error: Avg loss: 0.26812768
validation Error: 
 Avg loss: 0.28194111 
 F1: 0.472875 
 Precision: 0.546286 
 Recall: 0.416858
 IoU: 0.309651

test Error: 
 Avg loss: 0.26209616 
 F1: 0.507238 
 Precision: 0.596674 
 Recall: 0.441119
 IoU: 0.339799

We have finished training iteration 293
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_291_.pth
Per-example loss in batch: 0.218719  [    2/   89]
Per-example loss in batch: 0.288123  [    4/   89]
Per-example loss in batch: 0.303968  [    6/   89]
Per-example loss in batch: 0.239242  [    8/   89]
Per-example loss in batch: 0.254653  [   10/   89]
Per-example loss in batch: 0.294900  [   12/   89]
Per-example loss in batch: 0.246078  [   14/   89]
Per-example loss in batch: 0.272260  [   16/   89]
Per-example loss in batch: 0.215384  [   18/   89]
Per-example loss in batch: 0.293165  [   20/   89]
Per-example loss in batch: 0.256551  [   22/   89]
Per-example loss in batch: 0.273636  [   24/   89]
Per-example loss in batch: 0.341280  [   26/   89]
Per-example loss in batch: 0.209120  [   28/   89]
Per-example loss in batch: 0.241348  [   30/   89]
Per-example loss in batch: 0.320060  [   32/   89]
Per-example loss in batch: 0.212891  [   34/   89]
Per-example loss in batch: 0.254266  [   36/   89]
Per-example loss in batch: 0.217476  [   38/   89]
Per-example loss in batch: 0.230460  [   40/   89]
Per-example loss in batch: 0.228533  [   42/   89]
Per-example loss in batch: 0.212058  [   44/   89]
Per-example loss in batch: 0.307135  [   46/   89]
Per-example loss in batch: 0.311883  [   48/   89]
Per-example loss in batch: 0.302862  [   50/   89]
Per-example loss in batch: 0.345928  [   52/   89]
Per-example loss in batch: 0.234235  [   54/   89]
Per-example loss in batch: 0.250911  [   56/   89]
Per-example loss in batch: 0.345370  [   58/   89]
Per-example loss in batch: 0.227702  [   60/   89]
Per-example loss in batch: 0.226390  [   62/   89]
Per-example loss in batch: 0.210747  [   64/   89]
Per-example loss in batch: 0.234518  [   66/   89]
Per-example loss in batch: 0.264597  [   68/   89]
Per-example loss in batch: 0.280412  [   70/   89]
Per-example loss in batch: 0.314834  [   72/   89]
Per-example loss in batch: 0.331661  [   74/   89]
Per-example loss in batch: 0.254092  [   76/   89]
Per-example loss in batch: 0.292532  [   78/   89]
Per-example loss in batch: 0.221507  [   80/   89]
Per-example loss in batch: 0.344307  [   82/   89]
Per-example loss in batch: 0.311322  [   84/   89]
Per-example loss in batch: 0.304267  [   86/   89]
Per-example loss in batch: 0.223606  [   88/   89]
Per-example loss in batch: 0.472294  [   89/   89]
Train Error: Avg loss: 0.26968850
validation Error: 
 Avg loss: 0.28212575 
 F1: 0.473005 
 Precision: 0.557337 
 Recall: 0.410840
 IoU: 0.309762

test Error: 
 Avg loss: 0.26225415 
 F1: 0.507236 
 Precision: 0.612217 
 Recall: 0.432988
 IoU: 0.339796

We have finished training iteration 294
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_292_.pth
Per-example loss in batch: 0.353205  [    2/   89]
Per-example loss in batch: 0.244333  [    4/   89]
Per-example loss in batch: 0.237490  [    6/   89]
Per-example loss in batch: 0.297569  [    8/   89]
Per-example loss in batch: 0.194392  [   10/   89]
Per-example loss in batch: 0.253580  [   12/   89]
Per-example loss in batch: 0.260340  [   14/   89]
Per-example loss in batch: 0.240057  [   16/   89]
Per-example loss in batch: 0.363364  [   18/   89]
Per-example loss in batch: 0.351071  [   20/   89]
Per-example loss in batch: 0.255187  [   22/   89]
Per-example loss in batch: 0.241946  [   24/   89]
Per-example loss in batch: 0.212783  [   26/   89]
Per-example loss in batch: 0.270470  [   28/   89]
Per-example loss in batch: 0.209623  [   30/   89]
Per-example loss in batch: 0.241527  [   32/   89]
Per-example loss in batch: 0.292598  [   34/   89]
Per-example loss in batch: 0.243188  [   36/   89]
Per-example loss in batch: 0.239043  [   38/   89]
Per-example loss in batch: 0.224760  [   40/   89]
Per-example loss in batch: 0.335050  [   42/   89]
Per-example loss in batch: 0.286903  [   44/   89]
Per-example loss in batch: 0.230539  [   46/   89]
Per-example loss in batch: 0.295556  [   48/   89]
Per-example loss in batch: 0.292335  [   50/   89]
Per-example loss in batch: 0.230625  [   52/   89]
Per-example loss in batch: 0.248839  [   54/   89]
Per-example loss in batch: 0.217782  [   56/   89]
Per-example loss in batch: 0.311830  [   58/   89]
Per-example loss in batch: 0.321723  [   60/   89]
Per-example loss in batch: 0.236462  [   62/   89]
Per-example loss in batch: 0.229251  [   64/   89]
Per-example loss in batch: 0.295173  [   66/   89]
Per-example loss in batch: 0.217353  [   68/   89]
Per-example loss in batch: 0.316967  [   70/   89]
Per-example loss in batch: 0.345658  [   72/   89]
Per-example loss in batch: 0.249246  [   74/   89]
Per-example loss in batch: 0.309663  [   76/   89]
Per-example loss in batch: 0.248363  [   78/   89]
Per-example loss in batch: 0.257459  [   80/   89]
Per-example loss in batch: 0.323756  [   82/   89]
Per-example loss in batch: 0.243179  [   84/   89]
Per-example loss in batch: 0.275324  [   86/   89]
Per-example loss in batch: 0.247246  [   88/   89]
Per-example loss in batch: 0.515067  [   89/   89]
Train Error: Avg loss: 0.27079426
validation Error: 
 Avg loss: 0.28851527 
 F1: 0.472966 
 Precision: 0.583960 
 Recall: 0.397427
 IoU: 0.309729

test Error: 
 Avg loss: 0.26393009 
 F1: 0.503487 
 Precision: 0.637369 
 Recall: 0.416086
 IoU: 0.336440

We have finished training iteration 295
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_293_.pth
Per-example loss in batch: 0.248002  [    2/   89]
Per-example loss in batch: 0.214226  [    4/   89]
Per-example loss in batch: 0.245683  [    6/   89]
Per-example loss in batch: 0.320597  [    8/   89]
Per-example loss in batch: 0.284748  [   10/   89]
Per-example loss in batch: 0.301368  [   12/   89]
Per-example loss in batch: 0.292793  [   14/   89]
Per-example loss in batch: 0.255483  [   16/   89]
Per-example loss in batch: 0.253421  [   18/   89]
Per-example loss in batch: 0.261117  [   20/   89]
Per-example loss in batch: 0.301873  [   22/   89]
Per-example loss in batch: 0.216616  [   24/   89]
Per-example loss in batch: 0.335922  [   26/   89]
Per-example loss in batch: 0.237039  [   28/   89]
Per-example loss in batch: 0.323000  [   30/   89]
Per-example loss in batch: 0.303548  [   32/   89]
Per-example loss in batch: 0.292897  [   34/   89]
Per-example loss in batch: 0.252655  [   36/   89]
Per-example loss in batch: 0.251929  [   38/   89]
Per-example loss in batch: 0.325242  [   40/   89]
Per-example loss in batch: 0.230026  [   42/   89]
Per-example loss in batch: 0.229480  [   44/   89]
Per-example loss in batch: 0.211835  [   46/   89]
Per-example loss in batch: 0.217358  [   48/   89]
Per-example loss in batch: 0.299518  [   50/   89]
Per-example loss in batch: 0.204256  [   52/   89]
Per-example loss in batch: 0.231594  [   54/   89]
Per-example loss in batch: 0.216694  [   56/   89]
Per-example loss in batch: 0.227760  [   58/   89]
Per-example loss in batch: 0.223829  [   60/   89]
Per-example loss in batch: 0.231813  [   62/   89]
Per-example loss in batch: 0.313093  [   64/   89]
Per-example loss in batch: 0.290861  [   66/   89]
Per-example loss in batch: 0.244113  [   68/   89]
Per-example loss in batch: 0.226534  [   70/   89]
Per-example loss in batch: 0.247685  [   72/   89]
Per-example loss in batch: 0.216298  [   74/   89]
Per-example loss in batch: 0.260930  [   76/   89]
Per-example loss in batch: 0.297685  [   78/   89]
Per-example loss in batch: 0.256360  [   80/   89]
Per-example loss in batch: 0.339327  [   82/   89]
Per-example loss in batch: 0.290557  [   84/   89]
Per-example loss in batch: 0.275483  [   86/   89]
Per-example loss in batch: 0.277308  [   88/   89]
Per-example loss in batch: 0.376522  [   89/   89]
Train Error: Avg loss: 0.26442293
validation Error: 
 Avg loss: 0.28975622 
 F1: 0.472575 
 Precision: 0.545834 
 Recall: 0.416653
 IoU: 0.309393

test Error: 
 Avg loss: 0.26213059 
 F1: 0.507386 
 Precision: 0.596700 
 Recall: 0.441328
 IoU: 0.339931

We have finished training iteration 296
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_294_.pth
Per-example loss in batch: 0.245289  [    2/   89]
Per-example loss in batch: 0.221883  [    4/   89]
Per-example loss in batch: 0.197859  [    6/   89]
Per-example loss in batch: 0.235595  [    8/   89]
Per-example loss in batch: 0.293846  [   10/   89]
Per-example loss in batch: 0.224593  [   12/   89]
Per-example loss in batch: 0.306627  [   14/   89]
Per-example loss in batch: 0.269903  [   16/   89]
Per-example loss in batch: 0.299523  [   18/   89]
Per-example loss in batch: 0.271120  [   20/   89]
Per-example loss in batch: 0.264963  [   22/   89]
Per-example loss in batch: 0.245777  [   24/   89]
Per-example loss in batch: 0.245514  [   26/   89]
Per-example loss in batch: 0.287139  [   28/   89]
Per-example loss in batch: 0.248450  [   30/   89]
Per-example loss in batch: 0.324338  [   32/   89]
Per-example loss in batch: 0.236192  [   34/   89]
Per-example loss in batch: 0.288274  [   36/   89]
Per-example loss in batch: 0.226256  [   38/   89]
Per-example loss in batch: 0.253981  [   40/   89]
Per-example loss in batch: 0.287690  [   42/   89]
Per-example loss in batch: 0.253015  [   44/   89]
Per-example loss in batch: 0.323636  [   46/   89]
Per-example loss in batch: 0.219623  [   48/   89]
Per-example loss in batch: 0.233489  [   50/   89]
Per-example loss in batch: 0.254171  [   52/   89]
Per-example loss in batch: 0.279341  [   54/   89]
Per-example loss in batch: 0.236578  [   56/   89]
Per-example loss in batch: 0.224241  [   58/   89]
Per-example loss in batch: 0.272319  [   60/   89]
Per-example loss in batch: 0.318344  [   62/   89]
Per-example loss in batch: 0.288723  [   64/   89]
Per-example loss in batch: 0.230411  [   66/   89]
Per-example loss in batch: 0.225768  [   68/   89]
Per-example loss in batch: 0.325702  [   70/   89]
Per-example loss in batch: 0.237506  [   72/   89]
Per-example loss in batch: 0.294375  [   74/   89]
Per-example loss in batch: 0.264099  [   76/   89]
Per-example loss in batch: 0.271507  [   78/   89]
Per-example loss in batch: 0.304804  [   80/   89]
Per-example loss in batch: 0.307874  [   82/   89]
Per-example loss in batch: 0.216915  [   84/   89]
Per-example loss in batch: 0.205118  [   86/   89]
Per-example loss in batch: 0.289393  [   88/   89]
Per-example loss in batch: 0.707247  [   89/   89]
Train Error: Avg loss: 0.26753679
validation Error: 
 Avg loss: 0.29160271 
 F1: 0.474280 
 Precision: 0.555638 
 Recall: 0.413704
 IoU: 0.310857

test Error: 
 Avg loss: 0.26121625 
 F1: 0.509268 
 Precision: 0.610214 
 Recall: 0.436980
 IoU: 0.341623

We have finished training iteration 297
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_295_.pth
Per-example loss in batch: 0.204490  [    2/   89]
Per-example loss in batch: 0.223649  [    4/   89]
Per-example loss in batch: 0.311444  [    6/   89]
Per-example loss in batch: 0.273737  [    8/   89]
Per-example loss in batch: 0.257120  [   10/   89]
Per-example loss in batch: 0.341849  [   12/   89]
Per-example loss in batch: 0.263727  [   14/   89]
Per-example loss in batch: 0.272658  [   16/   89]
Per-example loss in batch: 0.341859  [   18/   89]
Per-example loss in batch: 0.243585  [   20/   89]
Per-example loss in batch: 0.247915  [   22/   89]
Per-example loss in batch: 0.319401  [   24/   89]
Per-example loss in batch: 0.255106  [   26/   89]
Per-example loss in batch: 0.253445  [   28/   89]
Per-example loss in batch: 0.265628  [   30/   89]
Per-example loss in batch: 0.277294  [   32/   89]
Per-example loss in batch: 0.229971  [   34/   89]
Per-example loss in batch: 0.275387  [   36/   89]
Per-example loss in batch: 0.211303  [   38/   89]
Per-example loss in batch: 0.241838  [   40/   89]
Per-example loss in batch: 0.215625  [   42/   89]
Per-example loss in batch: 0.305016  [   44/   89]
Per-example loss in batch: 0.278454  [   46/   89]
Per-example loss in batch: 0.227128  [   48/   89]
Per-example loss in batch: 0.236011  [   50/   89]
Per-example loss in batch: 0.218125  [   52/   89]
Per-example loss in batch: 0.277938  [   54/   89]
Per-example loss in batch: 0.294333  [   56/   89]
Per-example loss in batch: 0.282932  [   58/   89]
Per-example loss in batch: 0.257175  [   60/   89]
Per-example loss in batch: 0.208099  [   62/   89]
Per-example loss in batch: 0.237471  [   64/   89]
Per-example loss in batch: 0.235195  [   66/   89]
Per-example loss in batch: 0.287730  [   68/   89]
Per-example loss in batch: 0.260083  [   70/   89]
Per-example loss in batch: 0.215479  [   72/   89]
Per-example loss in batch: 0.248107  [   74/   89]
Per-example loss in batch: 0.287220  [   76/   89]
Per-example loss in batch: 0.361561  [   78/   89]
Per-example loss in batch: 0.289845  [   80/   89]
Per-example loss in batch: 0.238984  [   82/   89]
Per-example loss in batch: 0.230207  [   84/   89]
Per-example loss in batch: 0.214598  [   86/   89]
Per-example loss in batch: 0.262038  [   88/   89]
Per-example loss in batch: 0.383902  [   89/   89]
Train Error: Avg loss: 0.26230809
validation Error: 
 Avg loss: 0.28122409 
 F1: 0.475170 
 Precision: 0.545801 
 Recall: 0.420725
 IoU: 0.311622

test Error: 
 Avg loss: 0.26023633 
 F1: 0.511238 
 Precision: 0.602349 
 Recall: 0.444069
 IoU: 0.343398

We have finished training iteration 298
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_296_.pth
Per-example loss in batch: 0.262666  [    2/   89]
Per-example loss in batch: 0.278179  [    4/   89]
Per-example loss in batch: 0.291062  [    6/   89]
Per-example loss in batch: 0.234361  [    8/   89]
Per-example loss in batch: 0.304291  [   10/   89]
Per-example loss in batch: 0.221685  [   12/   89]
Per-example loss in batch: 0.269481  [   14/   89]
Per-example loss in batch: 0.200030  [   16/   89]
Per-example loss in batch: 0.287542  [   18/   89]
Per-example loss in batch: 0.302847  [   20/   89]
Per-example loss in batch: 0.271905  [   22/   89]
Per-example loss in batch: 0.200258  [   24/   89]
Per-example loss in batch: 0.259821  [   26/   89]
Per-example loss in batch: 0.228169  [   28/   89]
Per-example loss in batch: 0.317246  [   30/   89]
Per-example loss in batch: 0.261651  [   32/   89]
Per-example loss in batch: 0.290718  [   34/   89]
Per-example loss in batch: 0.237895  [   36/   89]
Per-example loss in batch: 0.225813  [   38/   89]
Per-example loss in batch: 0.240379  [   40/   89]
Per-example loss in batch: 0.297299  [   42/   89]
Per-example loss in batch: 0.243124  [   44/   89]
Per-example loss in batch: 0.210566  [   46/   89]
Per-example loss in batch: 0.234076  [   48/   89]
Per-example loss in batch: 0.226643  [   50/   89]
Per-example loss in batch: 0.210982  [   52/   89]
Per-example loss in batch: 0.338074  [   54/   89]
Per-example loss in batch: 0.330212  [   56/   89]
Per-example loss in batch: 0.227949  [   58/   89]
Per-example loss in batch: 0.256093  [   60/   89]
Per-example loss in batch: 0.222964  [   62/   89]
Per-example loss in batch: 0.362146  [   64/   89]
Per-example loss in batch: 0.211465  [   66/   89]
Per-example loss in batch: 0.226542  [   68/   89]
Per-example loss in batch: 0.341670  [   70/   89]
Per-example loss in batch: 0.323559  [   72/   89]
Per-example loss in batch: 0.306273  [   74/   89]
Per-example loss in batch: 0.319020  [   76/   89]
Per-example loss in batch: 0.246143  [   78/   89]
Per-example loss in batch: 0.203705  [   80/   89]
Per-example loss in batch: 0.320445  [   82/   89]
Per-example loss in batch: 0.269723  [   84/   89]
Per-example loss in batch: 0.243954  [   86/   89]
Per-example loss in batch: 0.246137  [   88/   89]
Per-example loss in batch: 0.449025  [   89/   89]
Train Error: Avg loss: 0.26582634
validation Error: 
 Avg loss: 0.28653418 
 F1: 0.473550 
 Precision: 0.522249 
 Recall: 0.433158
 IoU: 0.310229

test Error: 
 Avg loss: 0.25957875 
 F1: 0.512511 
 Precision: 0.579513 
 Recall: 0.459397
 IoU: 0.344548

We have finished training iteration 299
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_297_.pth
Per-example loss in batch: 0.239421  [    2/   89]
Per-example loss in batch: 0.236191  [    4/   89]
Per-example loss in batch: 0.208791  [    6/   89]
Per-example loss in batch: 0.244361  [    8/   89]
Per-example loss in batch: 0.276418  [   10/   89]
Per-example loss in batch: 0.202087  [   12/   89]
Per-example loss in batch: 0.264439  [   14/   89]
Per-example loss in batch: 0.215412  [   16/   89]
Per-example loss in batch: 0.307730  [   18/   89]
Per-example loss in batch: 0.334260  [   20/   89]
Per-example loss in batch: 0.242041  [   22/   89]
Per-example loss in batch: 0.197399  [   24/   89]
Per-example loss in batch: 0.316999  [   26/   89]
Per-example loss in batch: 0.252251  [   28/   89]
Per-example loss in batch: 0.318026  [   30/   89]
Per-example loss in batch: 0.222888  [   32/   89]
Per-example loss in batch: 0.282875  [   34/   89]
Per-example loss in batch: 0.312065  [   36/   89]
Per-example loss in batch: 0.321665  [   38/   89]
Per-example loss in batch: 0.260508  [   40/   89]
Per-example loss in batch: 0.242810  [   42/   89]
Per-example loss in batch: 0.249316  [   44/   89]
Per-example loss in batch: 0.266580  [   46/   89]
Per-example loss in batch: 0.198278  [   48/   89]
Per-example loss in batch: 0.271165  [   50/   89]
Per-example loss in batch: 0.234356  [   52/   89]
Per-example loss in batch: 0.242003  [   54/   89]
Per-example loss in batch: 0.243394  [   56/   89]
Per-example loss in batch: 0.317976  [   58/   89]
Per-example loss in batch: 0.255848  [   60/   89]
Per-example loss in batch: 0.269902  [   62/   89]
Per-example loss in batch: 0.276443  [   64/   89]
Per-example loss in batch: 0.271027  [   66/   89]
Per-example loss in batch: 0.339452  [   68/   89]
Per-example loss in batch: 0.314977  [   70/   89]
Per-example loss in batch: 0.218062  [   72/   89]
Per-example loss in batch: 0.249308  [   74/   89]
Per-example loss in batch: 0.255198  [   76/   89]
Per-example loss in batch: 0.281575  [   78/   89]
Per-example loss in batch: 0.267248  [   80/   89]
Per-example loss in batch: 0.286945  [   82/   89]
Per-example loss in batch: 0.319553  [   84/   89]
Per-example loss in batch: 0.244648  [   86/   89]
Per-example loss in batch: 0.213555  [   88/   89]
Per-example loss in batch: 0.616524  [   89/   89]
Train Error: Avg loss: 0.26727433
validation Error: 
 Avg loss: 0.28764246 
 F1: 0.472704 
 Precision: 0.567936 
 Recall: 0.404823
 IoU: 0.309504

test Error: 
 Avg loss: 0.26288780 
 F1: 0.505518 
 Precision: 0.620095 
 Recall: 0.426680
 IoU: 0.338257

We have finished training iteration 300
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_298_.pth
Per-example loss in batch: 0.234573  [    2/   89]
Per-example loss in batch: 0.306434  [    4/   89]
Per-example loss in batch: 0.318891  [    6/   89]
Per-example loss in batch: 0.260481  [    8/   89]
Per-example loss in batch: 0.246193  [   10/   89]
Per-example loss in batch: 0.308492  [   12/   89]
Per-example loss in batch: 0.264507  [   14/   89]
Per-example loss in batch: 0.212677  [   16/   89]
Per-example loss in batch: 0.212048  [   18/   89]
Per-example loss in batch: 0.240385  [   20/   89]
Per-example loss in batch: 0.342869  [   22/   89]
Per-example loss in batch: 0.251863  [   24/   89]
Per-example loss in batch: 0.332827  [   26/   89]
Per-example loss in batch: 0.246645  [   28/   89]
Per-example loss in batch: 0.264762  [   30/   89]
Per-example loss in batch: 0.227944  [   32/   89]
Per-example loss in batch: 0.287766  [   34/   89]
Per-example loss in batch: 0.224508  [   36/   89]
Per-example loss in batch: 0.245383  [   38/   89]
Per-example loss in batch: 0.236294  [   40/   89]
Per-example loss in batch: 0.241190  [   42/   89]
Per-example loss in batch: 0.285745  [   44/   89]
Per-example loss in batch: 0.204933  [   46/   89]
Per-example loss in batch: 0.354304  [   48/   89]
Per-example loss in batch: 0.213587  [   50/   89]
Per-example loss in batch: 0.291018  [   52/   89]
Per-example loss in batch: 0.251343  [   54/   89]
Per-example loss in batch: 0.324629  [   56/   89]
Per-example loss in batch: 0.342759  [   58/   89]
Per-example loss in batch: 0.212748  [   60/   89]
Per-example loss in batch: 0.232238  [   62/   89]
Per-example loss in batch: 0.268750  [   64/   89]
Per-example loss in batch: 0.308923  [   66/   89]
Per-example loss in batch: 0.235163  [   68/   89]
Per-example loss in batch: 0.226474  [   70/   89]
Per-example loss in batch: 0.262460  [   72/   89]
Per-example loss in batch: 0.291368  [   74/   89]
Per-example loss in batch: 0.257502  [   76/   89]
Per-example loss in batch: 0.244089  [   78/   89]
Per-example loss in batch: 0.272569  [   80/   89]
Per-example loss in batch: 0.261794  [   82/   89]
Per-example loss in batch: 0.233078  [   84/   89]
Per-example loss in batch: 0.227767  [   86/   89]
Per-example loss in batch: 0.220379  [   88/   89]
Per-example loss in batch: 0.416329  [   89/   89]
Train Error: Avg loss: 0.26378697
validation Error: 
 Avg loss: 0.27942928 
 F1: 0.474185 
 Precision: 0.536115 
 Recall: 0.425082
 IoU: 0.310775

test Error: 
 Avg loss: 0.26089430 
 F1: 0.509459 
 Precision: 0.589613 
 Recall: 0.448489
 IoU: 0.341794

We have finished training iteration 301
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_299_.pth
Per-example loss in batch: 0.302890  [    2/   89]
Per-example loss in batch: 0.296088  [    4/   89]
Per-example loss in batch: 0.261131  [    6/   89]
Per-example loss in batch: 0.366761  [    8/   89]
Per-example loss in batch: 0.237416  [   10/   89]
Per-example loss in batch: 0.207793  [   12/   89]
Per-example loss in batch: 0.252477  [   14/   89]
Per-example loss in batch: 0.209385  [   16/   89]
Per-example loss in batch: 0.250155  [   18/   89]
Per-example loss in batch: 0.281776  [   20/   89]
Per-example loss in batch: 0.310403  [   22/   89]
Per-example loss in batch: 0.297290  [   24/   89]
Per-example loss in batch: 0.208842  [   26/   89]
Per-example loss in batch: 0.274069  [   28/   89]
Per-example loss in batch: 0.296621  [   30/   89]
Per-example loss in batch: 0.253639  [   32/   89]
Per-example loss in batch: 0.267385  [   34/   89]
Per-example loss in batch: 0.288717  [   36/   89]
Per-example loss in batch: 0.224942  [   38/   89]
Per-example loss in batch: 0.251079  [   40/   89]
Per-example loss in batch: 0.294316  [   42/   89]
Per-example loss in batch: 0.232629  [   44/   89]
Per-example loss in batch: 0.220421  [   46/   89]
Per-example loss in batch: 0.266554  [   48/   89]
Per-example loss in batch: 0.268815  [   50/   89]
Per-example loss in batch: 0.213830  [   52/   89]
Per-example loss in batch: 0.351260  [   54/   89]
Per-example loss in batch: 0.230909  [   56/   89]
Per-example loss in batch: 0.223645  [   58/   89]
Per-example loss in batch: 0.256487  [   60/   89]
Per-example loss in batch: 0.315501  [   62/   89]
Per-example loss in batch: 0.329170  [   64/   89]
Per-example loss in batch: 0.286032  [   66/   89]
Per-example loss in batch: 0.249766  [   68/   89]
Per-example loss in batch: 0.299429  [   70/   89]
Per-example loss in batch: 0.256373  [   72/   89]
Per-example loss in batch: 0.235326  [   74/   89]
Per-example loss in batch: 0.273693  [   76/   89]
Per-example loss in batch: 0.296338  [   78/   89]
Per-example loss in batch: 0.198381  [   80/   89]
Per-example loss in batch: 0.243999  [   82/   89]
Per-example loss in batch: 0.268188  [   84/   89]
Per-example loss in batch: 0.257650  [   86/   89]
Per-example loss in batch: 0.352485  [   88/   89]
Per-example loss in batch: 0.426622  [   89/   89]
Train Error: Avg loss: 0.26906446
validation Error: 
 Avg loss: 0.28618201 
 F1: 0.472508 
 Precision: 0.518294 
 Recall: 0.434155
 IoU: 0.309336

test Error: 
 Avg loss: 0.26023032 
 F1: 0.511646 
 Precision: 0.572933 
 Recall: 0.462203
 IoU: 0.343766

We have finished training iteration 302
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_300_.pth
Per-example loss in batch: 0.254408  [    2/   89]
Per-example loss in batch: 0.268589  [    4/   89]
Per-example loss in batch: 0.301281  [    6/   89]
Per-example loss in batch: 0.344887  [    8/   89]
Per-example loss in batch: 0.297840  [   10/   89]
Per-example loss in batch: 0.344940  [   12/   89]
Per-example loss in batch: 0.271691  [   14/   89]
Per-example loss in batch: 0.257519  [   16/   89]
Per-example loss in batch: 0.260647  [   18/   89]
Per-example loss in batch: 0.202701  [   20/   89]
Per-example loss in batch: 0.222159  [   22/   89]
Per-example loss in batch: 0.205502  [   24/   89]
Per-example loss in batch: 0.275499  [   26/   89]
Per-example loss in batch: 0.290618  [   28/   89]
Per-example loss in batch: 0.235192  [   30/   89]
Per-example loss in batch: 0.272277  [   32/   89]
Per-example loss in batch: 0.297575  [   34/   89]
Per-example loss in batch: 0.382520  [   36/   89]
Per-example loss in batch: 0.239570  [   38/   89]
Per-example loss in batch: 0.205173  [   40/   89]
Per-example loss in batch: 0.200807  [   42/   89]
Per-example loss in batch: 0.318404  [   44/   89]
Per-example loss in batch: 0.204680  [   46/   89]
Per-example loss in batch: 0.234794  [   48/   89]
Per-example loss in batch: 0.302611  [   50/   89]
Per-example loss in batch: 0.210259  [   52/   89]
Per-example loss in batch: 0.199252  [   54/   89]
Per-example loss in batch: 0.219342  [   56/   89]
Per-example loss in batch: 0.337323  [   58/   89]
Per-example loss in batch: 0.292865  [   60/   89]
Per-example loss in batch: 0.216378  [   62/   89]
Per-example loss in batch: 0.224264  [   64/   89]
Per-example loss in batch: 0.294243  [   66/   89]
Per-example loss in batch: 0.321084  [   68/   89]
Per-example loss in batch: 0.196814  [   70/   89]
Per-example loss in batch: 0.281782  [   72/   89]
Per-example loss in batch: 0.268479  [   74/   89]
Per-example loss in batch: 0.274646  [   76/   89]
Per-example loss in batch: 0.242082  [   78/   89]
Per-example loss in batch: 0.281501  [   80/   89]
Per-example loss in batch: 0.228927  [   82/   89]
Per-example loss in batch: 0.203078  [   84/   89]
Per-example loss in batch: 0.251173  [   86/   89]
Per-example loss in batch: 0.299809  [   88/   89]
Per-example loss in batch: 0.586416  [   89/   89]
Train Error: Avg loss: 0.26580664
validation Error: 
 Avg loss: 0.28768654 
 F1: 0.473732 
 Precision: 0.539727 
 Recall: 0.422118
 IoU: 0.310386

test Error: 
 Avg loss: 0.26085024 
 F1: 0.509928 
 Precision: 0.593150 
 Recall: 0.447185
 IoU: 0.342217

We have finished training iteration 303
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_301_.pth
Per-example loss in batch: 0.282398  [    2/   89]
Per-example loss in batch: 0.311313  [    4/   89]
Per-example loss in batch: 0.213125  [    6/   89]
Per-example loss in batch: 0.232316  [    8/   89]
Per-example loss in batch: 0.265033  [   10/   89]
Per-example loss in batch: 0.228610  [   12/   89]
Per-example loss in batch: 0.202064  [   14/   89]
Per-example loss in batch: 0.281307  [   16/   89]
Per-example loss in batch: 0.321201  [   18/   89]
Per-example loss in batch: 0.214755  [   20/   89]
Per-example loss in batch: 0.237944  [   22/   89]
Per-example loss in batch: 0.252846  [   24/   89]
Per-example loss in batch: 0.232885  [   26/   89]
Per-example loss in batch: 0.291123  [   28/   89]
Per-example loss in batch: 0.267896  [   30/   89]
Per-example loss in batch: 0.327904  [   32/   89]
Per-example loss in batch: 0.239221  [   34/   89]
Per-example loss in batch: 0.305663  [   36/   89]
Per-example loss in batch: 0.219746  [   38/   89]
Per-example loss in batch: 0.204543  [   40/   89]
Per-example loss in batch: 0.222118  [   42/   89]
Per-example loss in batch: 0.275704  [   44/   89]
Per-example loss in batch: 0.327020  [   46/   89]
Per-example loss in batch: 0.320127  [   48/   89]
Per-example loss in batch: 0.301839  [   50/   89]
Per-example loss in batch: 0.240605  [   52/   89]
Per-example loss in batch: 0.308688  [   54/   89]
Per-example loss in batch: 0.219743  [   56/   89]
Per-example loss in batch: 0.311767  [   58/   89]
Per-example loss in batch: 0.256819  [   60/   89]
Per-example loss in batch: 0.264931  [   62/   89]
Per-example loss in batch: 0.231447  [   64/   89]
Per-example loss in batch: 0.242329  [   66/   89]
Per-example loss in batch: 0.421424  [   68/   89]
Per-example loss in batch: 0.227901  [   70/   89]
Per-example loss in batch: 0.349563  [   72/   89]
Per-example loss in batch: 0.269828  [   74/   89]
Per-example loss in batch: 0.253504  [   76/   89]
Per-example loss in batch: 0.253808  [   78/   89]
Per-example loss in batch: 0.277693  [   80/   89]
Per-example loss in batch: 0.269360  [   82/   89]
Per-example loss in batch: 0.354406  [   84/   89]
Per-example loss in batch: 0.263804  [   86/   89]
Per-example loss in batch: 0.264290  [   88/   89]
Per-example loss in batch: 0.535334  [   89/   89]
Train Error: Avg loss: 0.27254559
validation Error: 
 Avg loss: 0.29212529 
 F1: 0.469092 
 Precision: 0.526058 
 Recall: 0.423258
 IoU: 0.306414

test Error: 
 Avg loss: 0.26304527 
 F1: 0.505938 
 Precision: 0.574420 
 Recall: 0.452046
 IoU: 0.338633

We have finished training iteration 304
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_302_.pth
Per-example loss in batch: 0.228201  [    2/   89]
Per-example loss in batch: 0.223572  [    4/   89]
Per-example loss in batch: 0.257926  [    6/   89]
Per-example loss in batch: 0.356759  [    8/   89]
Per-example loss in batch: 0.301167  [   10/   89]
Per-example loss in batch: 0.230173  [   12/   89]
Per-example loss in batch: 0.243909  [   14/   89]
Per-example loss in batch: 0.204631  [   16/   89]
Per-example loss in batch: 0.253505  [   18/   89]
Per-example loss in batch: 0.271219  [   20/   89]
Per-example loss in batch: 0.289026  [   22/   89]
Per-example loss in batch: 0.282168  [   24/   89]
Per-example loss in batch: 0.214512  [   26/   89]
Per-example loss in batch: 0.259339  [   28/   89]
Per-example loss in batch: 0.241150  [   30/   89]
Per-example loss in batch: 0.314413  [   32/   89]
Per-example loss in batch: 0.255360  [   34/   89]
Per-example loss in batch: 0.260345  [   36/   89]
Per-example loss in batch: 0.257722  [   38/   89]
Per-example loss in batch: 0.268736  [   40/   89]
Per-example loss in batch: 0.243521  [   42/   89]
Per-example loss in batch: 0.232852  [   44/   89]
Per-example loss in batch: 0.249479  [   46/   89]
Per-example loss in batch: 0.219906  [   48/   89]
Per-example loss in batch: 0.349097  [   50/   89]
Per-example loss in batch: 0.291796  [   52/   89]
Per-example loss in batch: 0.323940  [   54/   89]
Per-example loss in batch: 0.206503  [   56/   89]
Per-example loss in batch: 0.247171  [   58/   89]
Per-example loss in batch: 0.246108  [   60/   89]
Per-example loss in batch: 0.246201  [   62/   89]
Per-example loss in batch: 0.309844  [   64/   89]
Per-example loss in batch: 0.397187  [   66/   89]
Per-example loss in batch: 0.220284  [   68/   89]
Per-example loss in batch: 0.232102  [   70/   89]
Per-example loss in batch: 0.254076  [   72/   89]
Per-example loss in batch: 0.232578  [   74/   89]
Per-example loss in batch: 0.329855  [   76/   89]
Per-example loss in batch: 0.211081  [   78/   89]
Per-example loss in batch: 0.211690  [   80/   89]
Per-example loss in batch: 0.263003  [   82/   89]
Per-example loss in batch: 0.321058  [   84/   89]
Per-example loss in batch: 0.242730  [   86/   89]
Per-example loss in batch: 0.262734  [   88/   89]
Per-example loss in batch: 0.676960  [   89/   89]
Train Error: Avg loss: 0.26735074
validation Error: 
 Avg loss: 0.28722333 
 F1: 0.474806 
 Precision: 0.540769 
 Recall: 0.423187
 IoU: 0.311309

test Error: 
 Avg loss: 0.26038238 
 F1: 0.510735 
 Precision: 0.594785 
 Recall: 0.447499
 IoU: 0.342945

We have finished training iteration 305
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_303_.pth
Per-example loss in batch: 0.216632  [    2/   89]
Per-example loss in batch: 0.224768  [    4/   89]
Per-example loss in batch: 0.233033  [    6/   89]
Per-example loss in batch: 0.271053  [    8/   89]
Per-example loss in batch: 0.237917  [   10/   89]
Per-example loss in batch: 0.309744  [   12/   89]
Per-example loss in batch: 0.287564  [   14/   89]
Per-example loss in batch: 0.302659  [   16/   89]
Per-example loss in batch: 0.329700  [   18/   89]
Per-example loss in batch: 0.223255  [   20/   89]
Per-example loss in batch: 0.307014  [   22/   89]
Per-example loss in batch: 0.282042  [   24/   89]
Per-example loss in batch: 0.217673  [   26/   89]
Per-example loss in batch: 0.241718  [   28/   89]
Per-example loss in batch: 0.251283  [   30/   89]
Per-example loss in batch: 0.248351  [   32/   89]
Per-example loss in batch: 0.264183  [   34/   89]
Per-example loss in batch: 0.228127  [   36/   89]
Per-example loss in batch: 0.284373  [   38/   89]
Per-example loss in batch: 0.246036  [   40/   89]
Per-example loss in batch: 0.271338  [   42/   89]
Per-example loss in batch: 0.284168  [   44/   89]
Per-example loss in batch: 0.298523  [   46/   89]
Per-example loss in batch: 0.321191  [   48/   89]
Per-example loss in batch: 0.228144  [   50/   89]
Per-example loss in batch: 0.349835  [   52/   89]
Per-example loss in batch: 0.231916  [   54/   89]
Per-example loss in batch: 0.279982  [   56/   89]
Per-example loss in batch: 0.264302  [   58/   89]
Per-example loss in batch: 0.299108  [   60/   89]
Per-example loss in batch: 0.228066  [   62/   89]
Per-example loss in batch: 0.235966  [   64/   89]
Per-example loss in batch: 0.279369  [   66/   89]
Per-example loss in batch: 0.243883  [   68/   89]
Per-example loss in batch: 0.264825  [   70/   89]
Per-example loss in batch: 0.315579  [   72/   89]
Per-example loss in batch: 0.222120  [   74/   89]
Per-example loss in batch: 0.230789  [   76/   89]
Per-example loss in batch: 0.296399  [   78/   89]
Per-example loss in batch: 0.279969  [   80/   89]
Per-example loss in batch: 0.232596  [   82/   89]
Per-example loss in batch: 0.340875  [   84/   89]
Per-example loss in batch: 0.233924  [   86/   89]
Per-example loss in batch: 0.213017  [   88/   89]
Per-example loss in batch: 0.416318  [   89/   89]
Train Error: Avg loss: 0.26654309
validation Error: 
 Avg loss: 0.28534489 
 F1: 0.475098 
 Precision: 0.535049 
 Recall: 0.427227
 IoU: 0.311559

test Error: 
 Avg loss: 0.25979824 
 F1: 0.512076 
 Precision: 0.589906 
 Recall: 0.452389
 IoU: 0.344154

We have finished training iteration 306
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_304_.pth
Per-example loss in batch: 0.222744  [    2/   89]
Per-example loss in batch: 0.221077  [    4/   89]
Per-example loss in batch: 0.266468  [    6/   89]
Per-example loss in batch: 0.313608  [    8/   89]
Per-example loss in batch: 0.241511  [   10/   89]
Per-example loss in batch: 0.350228  [   12/   89]
Per-example loss in batch: 0.226953  [   14/   89]
Per-example loss in batch: 0.247317  [   16/   89]
Per-example loss in batch: 0.225833  [   18/   89]
Per-example loss in batch: 0.275591  [   20/   89]
Per-example loss in batch: 0.210418  [   22/   89]
Per-example loss in batch: 0.344879  [   24/   89]
Per-example loss in batch: 0.250040  [   26/   89]
Per-example loss in batch: 0.299162  [   28/   89]
Per-example loss in batch: 0.244675  [   30/   89]
Per-example loss in batch: 0.243079  [   32/   89]
Per-example loss in batch: 0.228894  [   34/   89]
Per-example loss in batch: 0.334349  [   36/   89]
Per-example loss in batch: 0.313966  [   38/   89]
Per-example loss in batch: 0.252792  [   40/   89]
Per-example loss in batch: 0.306453  [   42/   89]
Per-example loss in batch: 0.276478  [   44/   89]
Per-example loss in batch: 0.280678  [   46/   89]
Per-example loss in batch: 0.266241  [   48/   89]
Per-example loss in batch: 0.224639  [   50/   89]
Per-example loss in batch: 0.288254  [   52/   89]
Per-example loss in batch: 0.314353  [   54/   89]
Per-example loss in batch: 0.288946  [   56/   89]
Per-example loss in batch: 0.228788  [   58/   89]
Per-example loss in batch: 0.201293  [   60/   89]
Per-example loss in batch: 0.231589  [   62/   89]
Per-example loss in batch: 0.220655  [   64/   89]
Per-example loss in batch: 0.212947  [   66/   89]
Per-example loss in batch: 0.284453  [   68/   89]
Per-example loss in batch: 0.272200  [   70/   89]
Per-example loss in batch: 0.317771  [   72/   89]
Per-example loss in batch: 0.272507  [   74/   89]
Per-example loss in batch: 0.320443  [   76/   89]
Per-example loss in batch: 0.220655  [   78/   89]
Per-example loss in batch: 0.324014  [   80/   89]
Per-example loss in batch: 0.300191  [   82/   89]
Per-example loss in batch: 0.257416  [   84/   89]
Per-example loss in batch: 0.246575  [   86/   89]
Per-example loss in batch: 0.234402  [   88/   89]
Per-example loss in batch: 0.393185  [   89/   89]
Train Error: Avg loss: 0.26746323
validation Error: 
 Avg loss: 0.27975749 
 F1: 0.475132 
 Precision: 0.543187 
 Recall: 0.422232
 IoU: 0.311589

test Error: 
 Avg loss: 0.26036621 
 F1: 0.510870 
 Precision: 0.597955 
 Recall: 0.445926
 IoU: 0.343066

We have finished training iteration 307
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_305_.pth
Per-example loss in batch: 0.285137  [    2/   89]
Per-example loss in batch: 0.207483  [    4/   89]
Per-example loss in batch: 0.220085  [    6/   89]
Per-example loss in batch: 0.198910  [    8/   89]
Per-example loss in batch: 0.251152  [   10/   89]
Per-example loss in batch: 0.300107  [   12/   89]
Per-example loss in batch: 0.261776  [   14/   89]
Per-example loss in batch: 0.222337  [   16/   89]
Per-example loss in batch: 0.262819  [   18/   89]
Per-example loss in batch: 0.260024  [   20/   89]
Per-example loss in batch: 0.242054  [   22/   89]
Per-example loss in batch: 0.311982  [   24/   89]
Per-example loss in batch: 0.368827  [   26/   89]
Per-example loss in batch: 0.264504  [   28/   89]
Per-example loss in batch: 0.247111  [   30/   89]
Per-example loss in batch: 0.222984  [   32/   89]
Per-example loss in batch: 0.199733  [   34/   89]
Per-example loss in batch: 0.296403  [   36/   89]
Per-example loss in batch: 0.268131  [   38/   89]
Per-example loss in batch: 0.266906  [   40/   89]
Per-example loss in batch: 0.256833  [   42/   89]
Per-example loss in batch: 0.293463  [   44/   89]
Per-example loss in batch: 0.315885  [   46/   89]
Per-example loss in batch: 0.228642  [   48/   89]
Per-example loss in batch: 0.271546  [   50/   89]
Per-example loss in batch: 0.251037  [   52/   89]
Per-example loss in batch: 0.243495  [   54/   89]
Per-example loss in batch: 0.235408  [   56/   89]
Per-example loss in batch: 0.281008  [   58/   89]
Per-example loss in batch: 0.332126  [   60/   89]
Per-example loss in batch: 0.273392  [   62/   89]
Per-example loss in batch: 0.226496  [   64/   89]
Per-example loss in batch: 0.237497  [   66/   89]
Per-example loss in batch: 0.298596  [   68/   89]
Per-example loss in batch: 0.271090  [   70/   89]
Per-example loss in batch: 0.240269  [   72/   89]
Per-example loss in batch: 0.280113  [   74/   89]
Per-example loss in batch: 0.280928  [   76/   89]
Per-example loss in batch: 0.269018  [   78/   89]
Per-example loss in batch: 0.286639  [   80/   89]
Per-example loss in batch: 0.317360  [   82/   89]
Per-example loss in batch: 0.301434  [   84/   89]
Per-example loss in batch: 0.282408  [   86/   89]
Per-example loss in batch: 0.198869  [   88/   89]
Per-example loss in batch: 0.609675  [   89/   89]
Train Error: Avg loss: 0.26824399
validation Error: 
 Avg loss: 0.28026805 
 F1: 0.473345 
 Precision: 0.544351 
 Recall: 0.418726
 IoU: 0.310054

test Error: 
 Avg loss: 0.26162679 
 F1: 0.508426 
 Precision: 0.596975 
 Recall: 0.442753
 IoU: 0.340866

We have finished training iteration 308
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_306_.pth
Per-example loss in batch: 0.276205  [    2/   89]
Per-example loss in batch: 0.211029  [    4/   89]
Per-example loss in batch: 0.265999  [    6/   89]
Per-example loss in batch: 0.223901  [    8/   89]
Per-example loss in batch: 0.273312  [   10/   89]
Per-example loss in batch: 0.305605  [   12/   89]
Per-example loss in batch: 0.310124  [   14/   89]
Per-example loss in batch: 0.235150  [   16/   89]
Per-example loss in batch: 0.247949  [   18/   89]
Per-example loss in batch: 0.252878  [   20/   89]
Per-example loss in batch: 0.240098  [   22/   89]
Per-example loss in batch: 0.227396  [   24/   89]
Per-example loss in batch: 0.274840  [   26/   89]
Per-example loss in batch: 0.206806  [   28/   89]
Per-example loss in batch: 0.272837  [   30/   89]
Per-example loss in batch: 0.201081  [   32/   89]
Per-example loss in batch: 0.292556  [   34/   89]
Per-example loss in batch: 0.238056  [   36/   89]
Per-example loss in batch: 0.266986  [   38/   89]
Per-example loss in batch: 0.244075  [   40/   89]
Per-example loss in batch: 0.277030  [   42/   89]
Per-example loss in batch: 0.284311  [   44/   89]
Per-example loss in batch: 0.236327  [   46/   89]
Per-example loss in batch: 0.244134  [   48/   89]
Per-example loss in batch: 0.295429  [   50/   89]
Per-example loss in batch: 0.346125  [   52/   89]
Per-example loss in batch: 0.322306  [   54/   89]
Per-example loss in batch: 0.308836  [   56/   89]
Per-example loss in batch: 0.280850  [   58/   89]
Per-example loss in batch: 0.254625  [   60/   89]
Per-example loss in batch: 0.310493  [   62/   89]
Per-example loss in batch: 0.269492  [   64/   89]
Per-example loss in batch: 0.286600  [   66/   89]
Per-example loss in batch: 0.216367  [   68/   89]
Per-example loss in batch: 0.209128  [   70/   89]
Per-example loss in batch: 0.248256  [   72/   89]
Per-example loss in batch: 0.241087  [   74/   89]
Per-example loss in batch: 0.322346  [   76/   89]
Per-example loss in batch: 0.289824  [   78/   89]
Per-example loss in batch: 0.265774  [   80/   89]
Per-example loss in batch: 0.304350  [   82/   89]
Per-example loss in batch: 0.278698  [   84/   89]
Per-example loss in batch: 0.226959  [   86/   89]
Per-example loss in batch: 0.367052  [   88/   89]
Per-example loss in batch: 0.445386  [   89/   89]
Train Error: Avg loss: 0.26912303
validation Error: 
 Avg loss: 0.28053006 
 F1: 0.472963 
 Precision: 0.571284 
 Recall: 0.403515
 IoU: 0.309726

test Error: 
 Avg loss: 0.26273802 
 F1: 0.505771 
 Precision: 0.626885 
 Recall: 0.423878
 IoU: 0.338483

We have finished training iteration 309
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_307_.pth
Per-example loss in batch: 0.246118  [    2/   89]
Per-example loss in batch: 0.377237  [    4/   89]
Per-example loss in batch: 0.251659  [    6/   89]
Per-example loss in batch: 0.233250  [    8/   89]
Per-example loss in batch: 0.299410  [   10/   89]
Per-example loss in batch: 0.255855  [   12/   89]
Per-example loss in batch: 0.215568  [   14/   89]
Per-example loss in batch: 0.209900  [   16/   89]
Per-example loss in batch: 0.204873  [   18/   89]
Per-example loss in batch: 0.198366  [   20/   89]
Per-example loss in batch: 0.304675  [   22/   89]
Per-example loss in batch: 0.246969  [   24/   89]
Per-example loss in batch: 0.321678  [   26/   89]
Per-example loss in batch: 0.242483  [   28/   89]
Per-example loss in batch: 0.328126  [   30/   89]
Per-example loss in batch: 0.253866  [   32/   89]
Per-example loss in batch: 0.266451  [   34/   89]
Per-example loss in batch: 0.287036  [   36/   89]
Per-example loss in batch: 0.248852  [   38/   89]
Per-example loss in batch: 0.233062  [   40/   89]
Per-example loss in batch: 0.238317  [   42/   89]
Per-example loss in batch: 0.216802  [   44/   89]
Per-example loss in batch: 0.327000  [   46/   89]
Per-example loss in batch: 0.224044  [   48/   89]
Per-example loss in batch: 0.340989  [   50/   89]
Per-example loss in batch: 0.208395  [   52/   89]
Per-example loss in batch: 0.231727  [   54/   89]
Per-example loss in batch: 0.249365  [   56/   89]
Per-example loss in batch: 0.346624  [   58/   89]
Per-example loss in batch: 0.248833  [   60/   89]
Per-example loss in batch: 0.254821  [   62/   89]
Per-example loss in batch: 0.230267  [   64/   89]
Per-example loss in batch: 0.305940  [   66/   89]
Per-example loss in batch: 0.300346  [   68/   89]
Per-example loss in batch: 0.239478  [   70/   89]
Per-example loss in batch: 0.233872  [   72/   89]
Per-example loss in batch: 0.256134  [   74/   89]
Per-example loss in batch: 0.266702  [   76/   89]
Per-example loss in batch: 0.228955  [   78/   89]
Per-example loss in batch: 0.199767  [   80/   89]
Per-example loss in batch: 0.281073  [   82/   89]
Per-example loss in batch: 0.269439  [   84/   89]
Per-example loss in batch: 0.345738  [   86/   89]
Per-example loss in batch: 0.287267  [   88/   89]
Per-example loss in batch: 0.438154  [   89/   89]
Train Error: Avg loss: 0.26463833
validation Error: 
 Avg loss: 0.29103456 
 F1: 0.473328 
 Precision: 0.519789 
 Recall: 0.434491
 IoU: 0.310039

test Error: 
 Avg loss: 0.26039930 
 F1: 0.511201 
 Precision: 0.572776 
 Recall: 0.461580
 IoU: 0.343365

We have finished training iteration 310
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_308_.pth
Per-example loss in batch: 0.214323  [    2/   89]
Per-example loss in batch: 0.291989  [    4/   89]
Per-example loss in batch: 0.211650  [    6/   89]
Per-example loss in batch: 0.267515  [    8/   89]
Per-example loss in batch: 0.292599  [   10/   89]
Per-example loss in batch: 0.302836  [   12/   89]
Per-example loss in batch: 0.312226  [   14/   89]
Per-example loss in batch: 0.313288  [   16/   89]
Per-example loss in batch: 0.229372  [   18/   89]
Per-example loss in batch: 0.219714  [   20/   89]
Per-example loss in batch: 0.250351  [   22/   89]
Per-example loss in batch: 0.299053  [   24/   89]
Per-example loss in batch: 0.259458  [   26/   89]
Per-example loss in batch: 0.247792  [   28/   89]
Per-example loss in batch: 0.240531  [   30/   89]
Per-example loss in batch: 0.260179  [   32/   89]
Per-example loss in batch: 0.238021  [   34/   89]
Per-example loss in batch: 0.274973  [   36/   89]
Per-example loss in batch: 0.191052  [   38/   89]
Per-example loss in batch: 0.228189  [   40/   89]
Per-example loss in batch: 0.218845  [   42/   89]
Per-example loss in batch: 0.218595  [   44/   89]
Per-example loss in batch: 0.253675  [   46/   89]
Per-example loss in batch: 0.227289  [   48/   89]
Per-example loss in batch: 0.319658  [   50/   89]
Per-example loss in batch: 0.225188  [   52/   89]
Per-example loss in batch: 0.285083  [   54/   89]
Per-example loss in batch: 0.278083  [   56/   89]
Per-example loss in batch: 0.299101  [   58/   89]
Per-example loss in batch: 0.355085  [   60/   89]
Per-example loss in batch: 0.211880  [   62/   89]
Per-example loss in batch: 0.279741  [   64/   89]
Per-example loss in batch: 0.303034  [   66/   89]
Per-example loss in batch: 0.222430  [   68/   89]
Per-example loss in batch: 0.232110  [   70/   89]
Per-example loss in batch: 0.263723  [   72/   89]
Per-example loss in batch: 0.310334  [   74/   89]
Per-example loss in batch: 0.214310  [   76/   89]
Per-example loss in batch: 0.215883  [   78/   89]
Per-example loss in batch: 0.306695  [   80/   89]
Per-example loss in batch: 0.260506  [   82/   89]
Per-example loss in batch: 0.281967  [   84/   89]
Per-example loss in batch: 0.282980  [   86/   89]
Per-example loss in batch: 0.245370  [   88/   89]
Per-example loss in batch: 0.576273  [   89/   89]
Train Error: Avg loss: 0.26392838
validation Error: 
 Avg loss: 0.28722451 
 F1: 0.473856 
 Precision: 0.541438 
 Recall: 0.421273
 IoU: 0.310492

test Error: 
 Avg loss: 0.26096112 
 F1: 0.509667 
 Precision: 0.594986 
 Recall: 0.445748
 IoU: 0.341982

We have finished training iteration 311
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_309_.pth
Per-example loss in batch: 0.396776  [    2/   89]
Per-example loss in batch: 0.251447  [    4/   89]
Per-example loss in batch: 0.224700  [    6/   89]
Per-example loss in batch: 0.306098  [    8/   89]
Per-example loss in batch: 0.233517  [   10/   89]
Per-example loss in batch: 0.298158  [   12/   89]
Per-example loss in batch: 0.275128  [   14/   89]
Per-example loss in batch: 0.221085  [   16/   89]
Per-example loss in batch: 0.319173  [   18/   89]
Per-example loss in batch: 0.278034  [   20/   89]
Per-example loss in batch: 0.248413  [   22/   89]
Per-example loss in batch: 0.275712  [   24/   89]
Per-example loss in batch: 0.314119  [   26/   89]
Per-example loss in batch: 0.225476  [   28/   89]
Per-example loss in batch: 0.250687  [   30/   89]
Per-example loss in batch: 0.313598  [   32/   89]
Per-example loss in batch: 0.253168  [   34/   89]
Per-example loss in batch: 0.317301  [   36/   89]
Per-example loss in batch: 0.280727  [   38/   89]
Per-example loss in batch: 0.275019  [   40/   89]
Per-example loss in batch: 0.235077  [   42/   89]
Per-example loss in batch: 0.264189  [   44/   89]
Per-example loss in batch: 0.260459  [   46/   89]
Per-example loss in batch: 0.252168  [   48/   89]
Per-example loss in batch: 0.234006  [   50/   89]
Per-example loss in batch: 0.308639  [   52/   89]
Per-example loss in batch: 0.215091  [   54/   89]
Per-example loss in batch: 0.245188  [   56/   89]
Per-example loss in batch: 0.198691  [   58/   89]
Per-example loss in batch: 0.244748  [   60/   89]
Per-example loss in batch: 0.309020  [   62/   89]
Per-example loss in batch: 0.326197  [   64/   89]
Per-example loss in batch: 0.291379  [   66/   89]
Per-example loss in batch: 0.241681  [   68/   89]
Per-example loss in batch: 0.235213  [   70/   89]
Per-example loss in batch: 0.249864  [   72/   89]
Per-example loss in batch: 0.343853  [   74/   89]
Per-example loss in batch: 0.291203  [   76/   89]
Per-example loss in batch: 0.218917  [   78/   89]
Per-example loss in batch: 0.268926  [   80/   89]
Per-example loss in batch: 0.210407  [   82/   89]
Per-example loss in batch: 0.243106  [   84/   89]
Per-example loss in batch: 0.276617  [   86/   89]
Per-example loss in batch: 0.208295  [   88/   89]
Per-example loss in batch: 0.434587  [   89/   89]
Train Error: Avg loss: 0.26850710
validation Error: 
 Avg loss: 0.28339845 
 F1: 0.475546 
 Precision: 0.533889 
 Recall: 0.428698
 IoU: 0.311945

test Error: 
 Avg loss: 0.25926441 
 F1: 0.513122 
 Precision: 0.589801 
 Recall: 0.454087
 IoU: 0.345100

We have finished training iteration 312
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_310_.pth
Per-example loss in batch: 0.220391  [    2/   89]
Per-example loss in batch: 0.243877  [    4/   89]
Per-example loss in batch: 0.219363  [    6/   89]
Per-example loss in batch: 0.225893  [    8/   89]
Per-example loss in batch: 0.239028  [   10/   89]
Per-example loss in batch: 0.273255  [   12/   89]
Per-example loss in batch: 0.224788  [   14/   89]
Per-example loss in batch: 0.298998  [   16/   89]
Per-example loss in batch: 0.241694  [   18/   89]
Per-example loss in batch: 0.230314  [   20/   89]
Per-example loss in batch: 0.298773  [   22/   89]
Per-example loss in batch: 0.264820  [   24/   89]
Per-example loss in batch: 0.276496  [   26/   89]
Per-example loss in batch: 0.234472  [   28/   89]
Per-example loss in batch: 0.307632  [   30/   89]
Per-example loss in batch: 0.246735  [   32/   89]
Per-example loss in batch: 0.246235  [   34/   89]
Per-example loss in batch: 0.260267  [   36/   89]
Per-example loss in batch: 0.303426  [   38/   89]
Per-example loss in batch: 0.220852  [   40/   89]
Per-example loss in batch: 0.330868  [   42/   89]
Per-example loss in batch: 0.318471  [   44/   89]
Per-example loss in batch: 0.305559  [   46/   89]
Per-example loss in batch: 0.285624  [   48/   89]
Per-example loss in batch: 0.319396  [   50/   89]
Per-example loss in batch: 0.211742  [   52/   89]
Per-example loss in batch: 0.259990  [   54/   89]
Per-example loss in batch: 0.222351  [   56/   89]
Per-example loss in batch: 0.360021  [   58/   89]
Per-example loss in batch: 0.208434  [   60/   89]
Per-example loss in batch: 0.272339  [   62/   89]
Per-example loss in batch: 0.308641  [   64/   89]
Per-example loss in batch: 0.262566  [   66/   89]
Per-example loss in batch: 0.281816  [   68/   89]
Per-example loss in batch: 0.246408  [   70/   89]
Per-example loss in batch: 0.198549  [   72/   89]
Per-example loss in batch: 0.278785  [   74/   89]
Per-example loss in batch: 0.328346  [   76/   89]
Per-example loss in batch: 0.292559  [   78/   89]
Per-example loss in batch: 0.219912  [   80/   89]
Per-example loss in batch: 0.213324  [   82/   89]
Per-example loss in batch: 0.220309  [   84/   89]
Per-example loss in batch: 0.312827  [   86/   89]
Per-example loss in batch: 0.312870  [   88/   89]
Per-example loss in batch: 0.680781  [   89/   89]
Train Error: Avg loss: 0.26942487
validation Error: 
 Avg loss: 0.28647327 
 F1: 0.474363 
 Precision: 0.537625 
 Recall: 0.424422
 IoU: 0.310928

test Error: 
 Avg loss: 0.26076059 
 F1: 0.509768 
 Precision: 0.591982 
 Recall: 0.447605
 IoU: 0.342073

We have finished training iteration 313
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_311_.pth
Per-example loss in batch: 0.225227  [    2/   89]
Per-example loss in batch: 0.257317  [    4/   89]
Per-example loss in batch: 0.287430  [    6/   89]
Per-example loss in batch: 0.313027  [    8/   89]
Per-example loss in batch: 0.276261  [   10/   89]
Per-example loss in batch: 0.264734  [   12/   89]
Per-example loss in batch: 0.303474  [   14/   89]
Per-example loss in batch: 0.224817  [   16/   89]
Per-example loss in batch: 0.310440  [   18/   89]
Per-example loss in batch: 0.211638  [   20/   89]
Per-example loss in batch: 0.273071  [   22/   89]
Per-example loss in batch: 0.286202  [   24/   89]
Per-example loss in batch: 0.295167  [   26/   89]
Per-example loss in batch: 0.220111  [   28/   89]
Per-example loss in batch: 0.195491  [   30/   89]
Per-example loss in batch: 0.246742  [   32/   89]
Per-example loss in batch: 0.224748  [   34/   89]
Per-example loss in batch: 0.326836  [   36/   89]
Per-example loss in batch: 0.264594  [   38/   89]
Per-example loss in batch: 0.213811  [   40/   89]
Per-example loss in batch: 0.314394  [   42/   89]
Per-example loss in batch: 0.258237  [   44/   89]
Per-example loss in batch: 0.275382  [   46/   89]
Per-example loss in batch: 0.210066  [   48/   89]
Per-example loss in batch: 0.228643  [   50/   89]
Per-example loss in batch: 0.269806  [   52/   89]
Per-example loss in batch: 0.237210  [   54/   89]
Per-example loss in batch: 0.213730  [   56/   89]
Per-example loss in batch: 0.296616  [   58/   89]
Per-example loss in batch: 0.343028  [   60/   89]
Per-example loss in batch: 0.282624  [   62/   89]
Per-example loss in batch: 0.231165  [   64/   89]
Per-example loss in batch: 0.243561  [   66/   89]
Per-example loss in batch: 0.317276  [   68/   89]
Per-example loss in batch: 0.361179  [   70/   89]
Per-example loss in batch: 0.301406  [   72/   89]
Per-example loss in batch: 0.293209  [   74/   89]
Per-example loss in batch: 0.269231  [   76/   89]
Per-example loss in batch: 0.205961  [   78/   89]
Per-example loss in batch: 0.248906  [   80/   89]
Per-example loss in batch: 0.255379  [   82/   89]
Per-example loss in batch: 0.284897  [   84/   89]
Per-example loss in batch: 0.235218  [   86/   89]
Per-example loss in batch: 0.217400  [   88/   89]
Per-example loss in batch: 0.452366  [   89/   89]
Train Error: Avg loss: 0.26610898
validation Error: 
 Avg loss: 0.28729826 
 F1: 0.475222 
 Precision: 0.540781 
 Recall: 0.423840
 IoU: 0.311666

test Error: 
 Avg loss: 0.26021071 
 F1: 0.510753 
 Precision: 0.594566 
 Recall: 0.447650
 IoU: 0.342960

We have finished training iteration 314
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_312_.pth
Per-example loss in batch: 0.219654  [    2/   89]
Per-example loss in batch: 0.235297  [    4/   89]
Per-example loss in batch: 0.274915  [    6/   89]
Per-example loss in batch: 0.225710  [    8/   89]
Per-example loss in batch: 0.220736  [   10/   89]
Per-example loss in batch: 0.253984  [   12/   89]
Per-example loss in batch: 0.210661  [   14/   89]
Per-example loss in batch: 0.194148  [   16/   89]
Per-example loss in batch: 0.333304  [   18/   89]
Per-example loss in batch: 0.227066  [   20/   89]
Per-example loss in batch: 0.216130  [   22/   89]
Per-example loss in batch: 0.270099  [   24/   89]
Per-example loss in batch: 0.352807  [   26/   89]
Per-example loss in batch: 0.206144  [   28/   89]
Per-example loss in batch: 0.286074  [   30/   89]
Per-example loss in batch: 0.258469  [   32/   89]
Per-example loss in batch: 0.284854  [   34/   89]
Per-example loss in batch: 0.252553  [   36/   89]
Per-example loss in batch: 0.284378  [   38/   89]
Per-example loss in batch: 0.278892  [   40/   89]
Per-example loss in batch: 0.317724  [   42/   89]
Per-example loss in batch: 0.246637  [   44/   89]
Per-example loss in batch: 0.240357  [   46/   89]
Per-example loss in batch: 0.238406  [   48/   89]
Per-example loss in batch: 0.238889  [   50/   89]
Per-example loss in batch: 0.239346  [   52/   89]
Per-example loss in batch: 0.290679  [   54/   89]
Per-example loss in batch: 0.207901  [   56/   89]
Per-example loss in batch: 0.284716  [   58/   89]
Per-example loss in batch: 0.268835  [   60/   89]
Per-example loss in batch: 0.244730  [   62/   89]
Per-example loss in batch: 0.337161  [   64/   89]
Per-example loss in batch: 0.247887  [   66/   89]
Per-example loss in batch: 0.314291  [   68/   89]
Per-example loss in batch: 0.208798  [   70/   89]
Per-example loss in batch: 0.293762  [   72/   89]
Per-example loss in batch: 0.362266  [   74/   89]
Per-example loss in batch: 0.309781  [   76/   89]
Per-example loss in batch: 0.249661  [   78/   89]
Per-example loss in batch: 0.210127  [   80/   89]
Per-example loss in batch: 0.227776  [   82/   89]
Per-example loss in batch: 0.228330  [   84/   89]
Per-example loss in batch: 0.261984  [   86/   89]
Per-example loss in batch: 0.234433  [   88/   89]
Per-example loss in batch: 0.563637  [   89/   89]
Train Error: Avg loss: 0.26229591
validation Error: 
 Avg loss: 0.28358142 
 F1: 0.474305 
 Precision: 0.540756 
 Recall: 0.422398
 IoU: 0.310878

test Error: 
 Avg loss: 0.26089352 
 F1: 0.509277 
 Precision: 0.592376 
 Recall: 0.446624
 IoU: 0.341631

We have finished training iteration 315
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_313_.pth
Per-example loss in batch: 0.280578  [    2/   89]
Per-example loss in batch: 0.212349  [    4/   89]
Per-example loss in batch: 0.228856  [    6/   89]
Per-example loss in batch: 0.214038  [    8/   89]
Per-example loss in batch: 0.231664  [   10/   89]
Per-example loss in batch: 0.226607  [   12/   89]
Per-example loss in batch: 0.197981  [   14/   89]
Per-example loss in batch: 0.236701  [   16/   89]
Per-example loss in batch: 0.230091  [   18/   89]
Per-example loss in batch: 0.224819  [   20/   89]
Per-example loss in batch: 0.320358  [   22/   89]
Per-example loss in batch: 0.230052  [   24/   89]
Per-example loss in batch: 0.229086  [   26/   89]
Per-example loss in batch: 0.316555  [   28/   89]
Per-example loss in batch: 0.224889  [   30/   89]
Per-example loss in batch: 0.372825  [   32/   89]
Per-example loss in batch: 0.256186  [   34/   89]
Per-example loss in batch: 0.270087  [   36/   89]
Per-example loss in batch: 0.320655  [   38/   89]
Per-example loss in batch: 0.257243  [   40/   89]
Per-example loss in batch: 0.314547  [   42/   89]
Per-example loss in batch: 0.232338  [   44/   89]
Per-example loss in batch: 0.261613  [   46/   89]
Per-example loss in batch: 0.281565  [   48/   89]
Per-example loss in batch: 0.346015  [   50/   89]
Per-example loss in batch: 0.283208  [   52/   89]
Per-example loss in batch: 0.223193  [   54/   89]
Per-example loss in batch: 0.243937  [   56/   89]
Per-example loss in batch: 0.267382  [   58/   89]
Per-example loss in batch: 0.327151  [   60/   89]
Per-example loss in batch: 0.187588  [   62/   89]
Per-example loss in batch: 0.336194  [   64/   89]
Per-example loss in batch: 0.330857  [   66/   89]
Per-example loss in batch: 0.290999  [   68/   89]
Per-example loss in batch: 0.237947  [   70/   89]
Per-example loss in batch: 0.242134  [   72/   89]
Per-example loss in batch: 0.327699  [   74/   89]
Per-example loss in batch: 0.237939  [   76/   89]
Per-example loss in batch: 0.224857  [   78/   89]
Per-example loss in batch: 0.370027  [   80/   89]
Per-example loss in batch: 0.206888  [   82/   89]
Per-example loss in batch: 0.269817  [   84/   89]
Per-example loss in batch: 0.262054  [   86/   89]
Per-example loss in batch: 0.282998  [   88/   89]
Per-example loss in batch: 0.417428  [   89/   89]
Train Error: Avg loss: 0.26695011
validation Error: 
 Avg loss: 0.28378612 
 F1: 0.473104 
 Precision: 0.541326 
 Recall: 0.420154
 IoU: 0.309847

test Error: 
 Avg loss: 0.26147761 
 F1: 0.508608 
 Precision: 0.596250 
 Recall: 0.443429
 IoU: 0.341029

We have finished training iteration 316
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_314_.pth
Per-example loss in batch: 0.303959  [    2/   89]
Per-example loss in batch: 0.277906  [    4/   89]
Per-example loss in batch: 0.254005  [    6/   89]
Per-example loss in batch: 0.346272  [    8/   89]
Per-example loss in batch: 0.248809  [   10/   89]
Per-example loss in batch: 0.303775  [   12/   89]
Per-example loss in batch: 0.199302  [   14/   89]
Per-example loss in batch: 0.199341  [   16/   89]
Per-example loss in batch: 0.277507  [   18/   89]
Per-example loss in batch: 0.314188  [   20/   89]
Per-example loss in batch: 0.206925  [   22/   89]
Per-example loss in batch: 0.224311  [   24/   89]
Per-example loss in batch: 0.280956  [   26/   89]
Per-example loss in batch: 0.296009  [   28/   89]
Per-example loss in batch: 0.305578  [   30/   89]
Per-example loss in batch: 0.231203  [   32/   89]
Per-example loss in batch: 0.214227  [   34/   89]
Per-example loss in batch: 0.233604  [   36/   89]
Per-example loss in batch: 0.234666  [   38/   89]
Per-example loss in batch: 0.252755  [   40/   89]
Per-example loss in batch: 0.275086  [   42/   89]
Per-example loss in batch: 0.277700  [   44/   89]
Per-example loss in batch: 0.251495  [   46/   89]
Per-example loss in batch: 0.326621  [   48/   89]
Per-example loss in batch: 0.321036  [   50/   89]
Per-example loss in batch: 0.258918  [   52/   89]
Per-example loss in batch: 0.261903  [   54/   89]
Per-example loss in batch: 0.292290  [   56/   89]
Per-example loss in batch: 0.263268  [   58/   89]
Per-example loss in batch: 0.224933  [   60/   89]
Per-example loss in batch: 0.197266  [   62/   89]
Per-example loss in batch: 0.231575  [   64/   89]
Per-example loss in batch: 0.248401  [   66/   89]
Per-example loss in batch: 0.341746  [   68/   89]
Per-example loss in batch: 0.278491  [   70/   89]
Per-example loss in batch: 0.293302  [   72/   89]
Per-example loss in batch: 0.309425  [   74/   89]
Per-example loss in batch: 0.287333  [   76/   89]
Per-example loss in batch: 0.240323  [   78/   89]
Per-example loss in batch: 0.213185  [   80/   89]
Per-example loss in batch: 0.209846  [   82/   89]
Per-example loss in batch: 0.227004  [   84/   89]
Per-example loss in batch: 0.244379  [   86/   89]
Per-example loss in batch: 0.312773  [   88/   89]
Per-example loss in batch: 0.402940  [   89/   89]
Train Error: Avg loss: 0.26505763
validation Error: 
 Avg loss: 0.28378432 
 F1: 0.473583 
 Precision: 0.547490 
 Recall: 0.417256
 IoU: 0.310258

test Error: 
 Avg loss: 0.26101431 
 F1: 0.509325 
 Precision: 0.601177 
 Recall: 0.441821
 IoU: 0.341674

We have finished training iteration 317
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_315_.pth
Per-example loss in batch: 0.232304  [    2/   89]
Per-example loss in batch: 0.249016  [    4/   89]
Per-example loss in batch: 0.238679  [    6/   89]
Per-example loss in batch: 0.333465  [    8/   89]
Per-example loss in batch: 0.265164  [   10/   89]
Per-example loss in batch: 0.262725  [   12/   89]
Per-example loss in batch: 0.222652  [   14/   89]
Per-example loss in batch: 0.271381  [   16/   89]
Per-example loss in batch: 0.286076  [   18/   89]
Per-example loss in batch: 0.213501  [   20/   89]
Per-example loss in batch: 0.217642  [   22/   89]
Per-example loss in batch: 0.216701  [   24/   89]
Per-example loss in batch: 0.351557  [   26/   89]
Per-example loss in batch: 0.285524  [   28/   89]
Per-example loss in batch: 0.326878  [   30/   89]
Per-example loss in batch: 0.306305  [   32/   89]
Per-example loss in batch: 0.226237  [   34/   89]
Per-example loss in batch: 0.302174  [   36/   89]
Per-example loss in batch: 0.322907  [   38/   89]
Per-example loss in batch: 0.201136  [   40/   89]
Per-example loss in batch: 0.284775  [   42/   89]
Per-example loss in batch: 0.315959  [   44/   89]
Per-example loss in batch: 0.307546  [   46/   89]
Per-example loss in batch: 0.260976  [   48/   89]
Per-example loss in batch: 0.287137  [   50/   89]
Per-example loss in batch: 0.323624  [   52/   89]
Per-example loss in batch: 0.223210  [   54/   89]
Per-example loss in batch: 0.265051  [   56/   89]
Per-example loss in batch: 0.359186  [   58/   89]
Per-example loss in batch: 0.259974  [   60/   89]
Per-example loss in batch: 0.260271  [   62/   89]
Per-example loss in batch: 0.219789  [   64/   89]
Per-example loss in batch: 0.249543  [   66/   89]
Per-example loss in batch: 0.199964  [   68/   89]
Per-example loss in batch: 0.258621  [   70/   89]
Per-example loss in batch: 0.239989  [   72/   89]
Per-example loss in batch: 0.256060  [   74/   89]
Per-example loss in batch: 0.211857  [   76/   89]
Per-example loss in batch: 0.251378  [   78/   89]
Per-example loss in batch: 0.190508  [   80/   89]
Per-example loss in batch: 0.212694  [   82/   89]
Per-example loss in batch: 0.328625  [   84/   89]
Per-example loss in batch: 0.257013  [   86/   89]
Per-example loss in batch: 0.253101  [   88/   89]
Per-example loss in batch: 0.723519  [   89/   89]
Train Error: Avg loss: 0.26900298
validation Error: 
 Avg loss: 0.28371517 
 F1: 0.471827 
 Precision: 0.557083 
 Recall: 0.409202
 IoU: 0.308752

test Error: 
 Avg loss: 0.26307257 
 F1: 0.505304 
 Precision: 0.606297 
 Recall: 0.433152
 IoU: 0.338064

We have finished training iteration 318
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_316_.pth
Per-example loss in batch: 0.233860  [    2/   89]
Per-example loss in batch: 0.261336  [    4/   89]
Per-example loss in batch: 0.238836  [    6/   89]
Per-example loss in batch: 0.206170  [    8/   89]
Per-example loss in batch: 0.249328  [   10/   89]
Per-example loss in batch: 0.326443  [   12/   89]
Per-example loss in batch: 0.260764  [   14/   89]
Per-example loss in batch: 0.340834  [   16/   89]
Per-example loss in batch: 0.295875  [   18/   89]
Per-example loss in batch: 0.282973  [   20/   89]
Per-example loss in batch: 0.256424  [   22/   89]
Per-example loss in batch: 0.276114  [   24/   89]
Per-example loss in batch: 0.297500  [   26/   89]
Per-example loss in batch: 0.216101  [   28/   89]
Per-example loss in batch: 0.258887  [   30/   89]
Per-example loss in batch: 0.281211  [   32/   89]
Per-example loss in batch: 0.226080  [   34/   89]
Per-example loss in batch: 0.338242  [   36/   89]
Per-example loss in batch: 0.204768  [   38/   89]
Per-example loss in batch: 0.240297  [   40/   89]
Per-example loss in batch: 0.292226  [   42/   89]
Per-example loss in batch: 0.256763  [   44/   89]
Per-example loss in batch: 0.337710  [   46/   89]
Per-example loss in batch: 0.263158  [   48/   89]
Per-example loss in batch: 0.253179  [   50/   89]
Per-example loss in batch: 0.221558  [   52/   89]
Per-example loss in batch: 0.278681  [   54/   89]
Per-example loss in batch: 0.217955  [   56/   89]
Per-example loss in batch: 0.261515  [   58/   89]
Per-example loss in batch: 0.249392  [   60/   89]
Per-example loss in batch: 0.230079  [   62/   89]
Per-example loss in batch: 0.213770  [   64/   89]
Per-example loss in batch: 0.254146  [   66/   89]
Per-example loss in batch: 0.306714  [   68/   89]
Per-example loss in batch: 0.243018  [   70/   89]
Per-example loss in batch: 0.228856  [   72/   89]
Per-example loss in batch: 0.244405  [   74/   89]
Per-example loss in batch: 0.335224  [   76/   89]
Per-example loss in batch: 0.239651  [   78/   89]
Per-example loss in batch: 0.240942  [   80/   89]
Per-example loss in batch: 0.210301  [   82/   89]
Per-example loss in batch: 0.295918  [   84/   89]
Per-example loss in batch: 0.205277  [   86/   89]
Per-example loss in batch: 0.320916  [   88/   89]
Per-example loss in batch: 0.551715  [   89/   89]
Train Error: Avg loss: 0.26447761
validation Error: 
 Avg loss: 0.28679785 
 F1: 0.472462 
 Precision: 0.567069 
 Recall: 0.404909
 IoU: 0.309296

test Error: 
 Avg loss: 0.26277671 
 F1: 0.506078 
 Precision: 0.620878 
 Recall: 0.427106
 IoU: 0.338758

We have finished training iteration 319
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_317_.pth
Per-example loss in batch: 0.331007  [    2/   89]
Per-example loss in batch: 0.228105  [    4/   89]
Per-example loss in batch: 0.305651  [    6/   89]
Per-example loss in batch: 0.302550  [    8/   89]
Per-example loss in batch: 0.295131  [   10/   89]
Per-example loss in batch: 0.222050  [   12/   89]
Per-example loss in batch: 0.339273  [   14/   89]
Per-example loss in batch: 0.324093  [   16/   89]
Per-example loss in batch: 0.223166  [   18/   89]
Per-example loss in batch: 0.207985  [   20/   89]
Per-example loss in batch: 0.243114  [   22/   89]
Per-example loss in batch: 0.216649  [   24/   89]
Per-example loss in batch: 0.274541  [   26/   89]
Per-example loss in batch: 0.267891  [   28/   89]
Per-example loss in batch: 0.244823  [   30/   89]
Per-example loss in batch: 0.249425  [   32/   89]
Per-example loss in batch: 0.330070  [   34/   89]
Per-example loss in batch: 0.230582  [   36/   89]
Per-example loss in batch: 0.327347  [   38/   89]
Per-example loss in batch: 0.234971  [   40/   89]
Per-example loss in batch: 0.226067  [   42/   89]
Per-example loss in batch: 0.247216  [   44/   89]
Per-example loss in batch: 0.236061  [   46/   89]
Per-example loss in batch: 0.293031  [   48/   89]
Per-example loss in batch: 0.281251  [   50/   89]
Per-example loss in batch: 0.262870  [   52/   89]
Per-example loss in batch: 0.292452  [   54/   89]
Per-example loss in batch: 0.252725  [   56/   89]
Per-example loss in batch: 0.215213  [   58/   89]
Per-example loss in batch: 0.255132  [   60/   89]
Per-example loss in batch: 0.289353  [   62/   89]
Per-example loss in batch: 0.273819  [   64/   89]
Per-example loss in batch: 0.264839  [   66/   89]
Per-example loss in batch: 0.245078  [   68/   89]
Per-example loss in batch: 0.315317  [   70/   89]
Per-example loss in batch: 0.204918  [   72/   89]
Per-example loss in batch: 0.290617  [   74/   89]
Per-example loss in batch: 0.214960  [   76/   89]
Per-example loss in batch: 0.213907  [   78/   89]
Per-example loss in batch: 0.282778  [   80/   89]
Per-example loss in batch: 0.300191  [   82/   89]
Per-example loss in batch: 0.280246  [   84/   89]
Per-example loss in batch: 0.207566  [   86/   89]
Per-example loss in batch: 0.217542  [   88/   89]
Per-example loss in batch: 0.715181  [   89/   89]
Train Error: Avg loss: 0.26784638
validation Error: 
 Avg loss: 0.28233663 
 F1: 0.475286 
 Precision: 0.547370 
 Recall: 0.419978
 IoU: 0.311722

test Error: 
 Avg loss: 0.26049318 
 F1: 0.510510 
 Precision: 0.600250 
 Recall: 0.444113
 IoU: 0.342742

We have finished training iteration 320
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_318_.pth
Per-example loss in batch: 0.284383  [    2/   89]
Per-example loss in batch: 0.273676  [    4/   89]
Per-example loss in batch: 0.263050  [    6/   89]
Per-example loss in batch: 0.269356  [    8/   89]
Per-example loss in batch: 0.325853  [   10/   89]
Per-example loss in batch: 0.241904  [   12/   89]
Per-example loss in batch: 0.295417  [   14/   89]
Per-example loss in batch: 0.207919  [   16/   89]
Per-example loss in batch: 0.215771  [   18/   89]
Per-example loss in batch: 0.227006  [   20/   89]
Per-example loss in batch: 0.275098  [   22/   89]
Per-example loss in batch: 0.217161  [   24/   89]
Per-example loss in batch: 0.253284  [   26/   89]
Per-example loss in batch: 0.242735  [   28/   89]
Per-example loss in batch: 0.240686  [   30/   89]
Per-example loss in batch: 0.206564  [   32/   89]
Per-example loss in batch: 0.267780  [   34/   89]
Per-example loss in batch: 0.255518  [   36/   89]
Per-example loss in batch: 0.249371  [   38/   89]
Per-example loss in batch: 0.259345  [   40/   89]
Per-example loss in batch: 0.252619  [   42/   89]
Per-example loss in batch: 0.328026  [   44/   89]
Per-example loss in batch: 0.301272  [   46/   89]
Per-example loss in batch: 0.232723  [   48/   89]
Per-example loss in batch: 0.335419  [   50/   89]
Per-example loss in batch: 0.297862  [   52/   89]
Per-example loss in batch: 0.304907  [   54/   89]
Per-example loss in batch: 0.251481  [   56/   89]
Per-example loss in batch: 0.254471  [   58/   89]
Per-example loss in batch: 0.258185  [   60/   89]
Per-example loss in batch: 0.296783  [   62/   89]
Per-example loss in batch: 0.223493  [   64/   89]
Per-example loss in batch: 0.303256  [   66/   89]
Per-example loss in batch: 0.206832  [   68/   89]
Per-example loss in batch: 0.231855  [   70/   89]
Per-example loss in batch: 0.230481  [   72/   89]
Per-example loss in batch: 0.219151  [   74/   89]
Per-example loss in batch: 0.242832  [   76/   89]
Per-example loss in batch: 0.297954  [   78/   89]
Per-example loss in batch: 0.221155  [   80/   89]
Per-example loss in batch: 0.292816  [   82/   89]
Per-example loss in batch: 0.342732  [   84/   89]
Per-example loss in batch: 0.213419  [   86/   89]
Per-example loss in batch: 0.259636  [   88/   89]
Per-example loss in batch: 0.382946  [   89/   89]
Train Error: Avg loss: 0.26208339
validation Error: 
 Avg loss: 0.28323474 
 F1: 0.475089 
 Precision: 0.530222 
 Recall: 0.430342
 IoU: 0.311552

test Error: 
 Avg loss: 0.25937392 
 F1: 0.513254 
 Precision: 0.584961 
 Recall: 0.457207
 IoU: 0.345220

We have finished training iteration 321
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_319_.pth
Per-example loss in batch: 0.297813  [    2/   89]
Per-example loss in batch: 0.280418  [    4/   89]
Per-example loss in batch: 0.257987  [    6/   89]
Per-example loss in batch: 0.314684  [    8/   89]
Per-example loss in batch: 0.259344  [   10/   89]
Per-example loss in batch: 0.288308  [   12/   89]
Per-example loss in batch: 0.262714  [   14/   89]
Per-example loss in batch: 0.335825  [   16/   89]
Per-example loss in batch: 0.304040  [   18/   89]
Per-example loss in batch: 0.262848  [   20/   89]
Per-example loss in batch: 0.293481  [   22/   89]
Per-example loss in batch: 0.236558  [   24/   89]
Per-example loss in batch: 0.220795  [   26/   89]
Per-example loss in batch: 0.269751  [   28/   89]
Per-example loss in batch: 0.235022  [   30/   89]
Per-example loss in batch: 0.248958  [   32/   89]
Per-example loss in batch: 0.273250  [   34/   89]
Per-example loss in batch: 0.240772  [   36/   89]
Per-example loss in batch: 0.230425  [   38/   89]
Per-example loss in batch: 0.288303  [   40/   89]
Per-example loss in batch: 0.305868  [   42/   89]
Per-example loss in batch: 0.273194  [   44/   89]
Per-example loss in batch: 0.205161  [   46/   89]
Per-example loss in batch: 0.249628  [   48/   89]
Per-example loss in batch: 0.225188  [   50/   89]
Per-example loss in batch: 0.322514  [   52/   89]
Per-example loss in batch: 0.239454  [   54/   89]
Per-example loss in batch: 0.363467  [   56/   89]
Per-example loss in batch: 0.332388  [   58/   89]
Per-example loss in batch: 0.262183  [   60/   89]
Per-example loss in batch: 0.268542  [   62/   89]
Per-example loss in batch: 0.280788  [   64/   89]
Per-example loss in batch: 0.274439  [   66/   89]
Per-example loss in batch: 0.246305  [   68/   89]
Per-example loss in batch: 0.213760  [   70/   89]
Per-example loss in batch: 0.218116  [   72/   89]
Per-example loss in batch: 0.239405  [   74/   89]
Per-example loss in batch: 0.228350  [   76/   89]
Per-example loss in batch: 0.252668  [   78/   89]
Per-example loss in batch: 0.238687  [   80/   89]
Per-example loss in batch: 0.321135  [   82/   89]
Per-example loss in batch: 0.207585  [   84/   89]
Per-example loss in batch: 0.189376  [   86/   89]
Per-example loss in batch: 0.260019  [   88/   89]
Per-example loss in batch: 0.630250  [   89/   89]
Train Error: Avg loss: 0.26819420
validation Error: 
 Avg loss: 0.28310213 
 F1: 0.474669 
 Precision: 0.552180 
 Recall: 0.416240
 IoU: 0.311191

test Error: 
 Avg loss: 0.26094445 
 F1: 0.509882 
 Precision: 0.604981 
 Recall: 0.440619
 IoU: 0.342175

We have finished training iteration 322
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_320_.pth
Per-example loss in batch: 0.248940  [    2/   89]
Per-example loss in batch: 0.228646  [    4/   89]
Per-example loss in batch: 0.235808  [    6/   89]
Per-example loss in batch: 0.244167  [    8/   89]
Per-example loss in batch: 0.263962  [   10/   89]
Per-example loss in batch: 0.243954  [   12/   89]
Per-example loss in batch: 0.245552  [   14/   89]
Per-example loss in batch: 0.338970  [   16/   89]
Per-example loss in batch: 0.264946  [   18/   89]
Per-example loss in batch: 0.235276  [   20/   89]
Per-example loss in batch: 0.231420  [   22/   89]
Per-example loss in batch: 0.226078  [   24/   89]
Per-example loss in batch: 0.319347  [   26/   89]
Per-example loss in batch: 0.331044  [   28/   89]
Per-example loss in batch: 0.296984  [   30/   89]
Per-example loss in batch: 0.219712  [   32/   89]
Per-example loss in batch: 0.255540  [   34/   89]
Per-example loss in batch: 0.234593  [   36/   89]
Per-example loss in batch: 0.316667  [   38/   89]
Per-example loss in batch: 0.235659  [   40/   89]
Per-example loss in batch: 0.332303  [   42/   89]
Per-example loss in batch: 0.267345  [   44/   89]
Per-example loss in batch: 0.282473  [   46/   89]
Per-example loss in batch: 0.216715  [   48/   89]
Per-example loss in batch: 0.275675  [   50/   89]
Per-example loss in batch: 0.205240  [   52/   89]
Per-example loss in batch: 0.235646  [   54/   89]
Per-example loss in batch: 0.220875  [   56/   89]
Per-example loss in batch: 0.278395  [   58/   89]
Per-example loss in batch: 0.247309  [   60/   89]
Per-example loss in batch: 0.251826  [   62/   89]
Per-example loss in batch: 0.232573  [   64/   89]
Per-example loss in batch: 0.242889  [   66/   89]
Per-example loss in batch: 0.289572  [   68/   89]
Per-example loss in batch: 0.325239  [   70/   89]
Per-example loss in batch: 0.233219  [   72/   89]
Per-example loss in batch: 0.298655  [   74/   89]
Per-example loss in batch: 0.273848  [   76/   89]
Per-example loss in batch: 0.236289  [   78/   89]
Per-example loss in batch: 0.323367  [   80/   89]
Per-example loss in batch: 0.283232  [   82/   89]
Per-example loss in batch: 0.350880  [   84/   89]
Per-example loss in batch: 0.196559  [   86/   89]
Per-example loss in batch: 0.218714  [   88/   89]
Per-example loss in batch: 0.439095  [   89/   89]
Train Error: Avg loss: 0.26417197
validation Error: 
 Avg loss: 0.28469072 
 F1: 0.472585 
 Precision: 0.548845 
 Recall: 0.414932
 IoU: 0.309402

test Error: 
 Avg loss: 0.26279285 
 F1: 0.506281 
 Precision: 0.603302 
 Recall: 0.436143
 IoU: 0.338940

We have finished training iteration 323
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_321_.pth
Per-example loss in batch: 0.257486  [    2/   89]
Per-example loss in batch: 0.358425  [    4/   89]
Per-example loss in batch: 0.234020  [    6/   89]
Per-example loss in batch: 0.250000  [    8/   89]
Per-example loss in batch: 0.265880  [   10/   89]
Per-example loss in batch: 0.347157  [   12/   89]
Per-example loss in batch: 0.314799  [   14/   89]
Per-example loss in batch: 0.189148  [   16/   89]
Per-example loss in batch: 0.292423  [   18/   89]
Per-example loss in batch: 0.316311  [   20/   89]
Per-example loss in batch: 0.271886  [   22/   89]
Per-example loss in batch: 0.278787  [   24/   89]
Per-example loss in batch: 0.230625  [   26/   89]
Per-example loss in batch: 0.234478  [   28/   89]
Per-example loss in batch: 0.266032  [   30/   89]
Per-example loss in batch: 0.304545  [   32/   89]
Per-example loss in batch: 0.322098  [   34/   89]
Per-example loss in batch: 0.280539  [   36/   89]
Per-example loss in batch: 0.237939  [   38/   89]
Per-example loss in batch: 0.295838  [   40/   89]
Per-example loss in batch: 0.236576  [   42/   89]
Per-example loss in batch: 0.254583  [   44/   89]
Per-example loss in batch: 0.268301  [   46/   89]
Per-example loss in batch: 0.314892  [   48/   89]
Per-example loss in batch: 0.270501  [   50/   89]
Per-example loss in batch: 0.274832  [   52/   89]
Per-example loss in batch: 0.244411  [   54/   89]
Per-example loss in batch: 0.245188  [   56/   89]
Per-example loss in batch: 0.272465  [   58/   89]
Per-example loss in batch: 0.272481  [   60/   89]
Per-example loss in batch: 0.265050  [   62/   89]
Per-example loss in batch: 0.280663  [   64/   89]
Per-example loss in batch: 0.219732  [   66/   89]
Per-example loss in batch: 0.220185  [   68/   89]
Per-example loss in batch: 0.253914  [   70/   89]
Per-example loss in batch: 0.208216  [   72/   89]
Per-example loss in batch: 0.256160  [   74/   89]
Per-example loss in batch: 0.242951  [   76/   89]
Per-example loss in batch: 0.213805  [   78/   89]
Per-example loss in batch: 0.253889  [   80/   89]
Per-example loss in batch: 0.235544  [   82/   89]
Per-example loss in batch: 0.367794  [   84/   89]
Per-example loss in batch: 0.240763  [   86/   89]
Per-example loss in batch: 0.261303  [   88/   89]
Per-example loss in batch: 0.494079  [   89/   89]
Train Error: Avg loss: 0.26898107
validation Error: 
 Avg loss: 0.27858384 
 F1: 0.473958 
 Precision: 0.547718 
 Recall: 0.417706
 IoU: 0.310580

test Error: 
 Avg loss: 0.26155858 
 F1: 0.508857 
 Precision: 0.600692 
 Recall: 0.441379
 IoU: 0.341253

We have finished training iteration 324
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_322_.pth
Per-example loss in batch: 0.242871  [    2/   89]
Per-example loss in batch: 0.213866  [    4/   89]
Per-example loss in batch: 0.241486  [    6/   89]
Per-example loss in batch: 0.215481  [    8/   89]
Per-example loss in batch: 0.284951  [   10/   89]
Per-example loss in batch: 0.196586  [   12/   89]
Per-example loss in batch: 0.282195  [   14/   89]
Per-example loss in batch: 0.230067  [   16/   89]
Per-example loss in batch: 0.241673  [   18/   89]
Per-example loss in batch: 0.256526  [   20/   89]
Per-example loss in batch: 0.326958  [   22/   89]
Per-example loss in batch: 0.287477  [   24/   89]
Per-example loss in batch: 0.298677  [   26/   89]
Per-example loss in batch: 0.296040  [   28/   89]
Per-example loss in batch: 0.245148  [   30/   89]
Per-example loss in batch: 0.354179  [   32/   89]
Per-example loss in batch: 0.248488  [   34/   89]
Per-example loss in batch: 0.260671  [   36/   89]
Per-example loss in batch: 0.222864  [   38/   89]
Per-example loss in batch: 0.228666  [   40/   89]
Per-example loss in batch: 0.224904  [   42/   89]
Per-example loss in batch: 0.308823  [   44/   89]
Per-example loss in batch: 0.253881  [   46/   89]
Per-example loss in batch: 0.229178  [   48/   89]
Per-example loss in batch: 0.220208  [   50/   89]
Per-example loss in batch: 0.280151  [   52/   89]
Per-example loss in batch: 0.335222  [   54/   89]
Per-example loss in batch: 0.319614  [   56/   89]
Per-example loss in batch: 0.212145  [   58/   89]
Per-example loss in batch: 0.250610  [   60/   89]
Per-example loss in batch: 0.294133  [   62/   89]
Per-example loss in batch: 0.333885  [   64/   89]
Per-example loss in batch: 0.275245  [   66/   89]
Per-example loss in batch: 0.302186  [   68/   89]
Per-example loss in batch: 0.340540  [   70/   89]
Per-example loss in batch: 0.304592  [   72/   89]
Per-example loss in batch: 0.310337  [   74/   89]
Per-example loss in batch: 0.224177  [   76/   89]
Per-example loss in batch: 0.206504  [   78/   89]
Per-example loss in batch: 0.248133  [   80/   89]
Per-example loss in batch: 0.247004  [   82/   89]
Per-example loss in batch: 0.259221  [   84/   89]
Per-example loss in batch: 0.318088  [   86/   89]
Per-example loss in batch: 0.208687  [   88/   89]
Per-example loss in batch: 0.679064  [   89/   89]
Train Error: Avg loss: 0.27015434
validation Error: 
 Avg loss: 0.28556835 
 F1: 0.473562 
 Precision: 0.557951 
 Recall: 0.411347
 IoU: 0.310240

test Error: 
 Avg loss: 0.26205003 
 F1: 0.507773 
 Precision: 0.609650 
 Recall: 0.435069
 IoU: 0.340278

We have finished training iteration 325
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_323_.pth
Per-example loss in batch: 0.314756  [    2/   89]
Per-example loss in batch: 0.344310  [    4/   89]
Per-example loss in batch: 0.220762  [    6/   89]
Per-example loss in batch: 0.281294  [    8/   89]
Per-example loss in batch: 0.303515  [   10/   89]
Per-example loss in batch: 0.304829  [   12/   89]
Per-example loss in batch: 0.264831  [   14/   89]
Per-example loss in batch: 0.234919  [   16/   89]
Per-example loss in batch: 0.244828  [   18/   89]
Per-example loss in batch: 0.303168  [   20/   89]
Per-example loss in batch: 0.230911  [   22/   89]
Per-example loss in batch: 0.262821  [   24/   89]
Per-example loss in batch: 0.271211  [   26/   89]
Per-example loss in batch: 0.245585  [   28/   89]
Per-example loss in batch: 0.298839  [   30/   89]
Per-example loss in batch: 0.227528  [   32/   89]
Per-example loss in batch: 0.220803  [   34/   89]
Per-example loss in batch: 0.315767  [   36/   89]
Per-example loss in batch: 0.298615  [   38/   89]
Per-example loss in batch: 0.227768  [   40/   89]
Per-example loss in batch: 0.223278  [   42/   89]
Per-example loss in batch: 0.219041  [   44/   89]
Per-example loss in batch: 0.339627  [   46/   89]
Per-example loss in batch: 0.298875  [   48/   89]
Per-example loss in batch: 0.229404  [   50/   89]
Per-example loss in batch: 0.226258  [   52/   89]
Per-example loss in batch: 0.222476  [   54/   89]
Per-example loss in batch: 0.264926  [   56/   89]
Per-example loss in batch: 0.230079  [   58/   89]
Per-example loss in batch: 0.269529  [   60/   89]
Per-example loss in batch: 0.203059  [   62/   89]
Per-example loss in batch: 0.218582  [   64/   89]
Per-example loss in batch: 0.325775  [   66/   89]
Per-example loss in batch: 0.194727  [   68/   89]
Per-example loss in batch: 0.231758  [   70/   89]
Per-example loss in batch: 0.323157  [   72/   89]
Per-example loss in batch: 0.335289  [   74/   89]
Per-example loss in batch: 0.272284  [   76/   89]
Per-example loss in batch: 0.210004  [   78/   89]
Per-example loss in batch: 0.347203  [   80/   89]
Per-example loss in batch: 0.196319  [   82/   89]
Per-example loss in batch: 0.253898  [   84/   89]
Per-example loss in batch: 0.255642  [   86/   89]
Per-example loss in batch: 0.241817  [   88/   89]
Per-example loss in batch: 0.532283  [   89/   89]
Train Error: Avg loss: 0.26553276
validation Error: 
 Avg loss: 0.28548498 
 F1: 0.473401 
 Precision: 0.557389 
 Recall: 0.411410
 IoU: 0.310102

test Error: 
 Avg loss: 0.26171192 
 F1: 0.508299 
 Precision: 0.612505 
 Recall: 0.434395
 IoU: 0.340751

We have finished training iteration 326
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_324_.pth
Per-example loss in batch: 0.248742  [    2/   89]
Per-example loss in batch: 0.319032  [    4/   89]
Per-example loss in batch: 0.196726  [    6/   89]
Per-example loss in batch: 0.223567  [    8/   89]
Per-example loss in batch: 0.297048  [   10/   89]
Per-example loss in batch: 0.211788  [   12/   89]
Per-example loss in batch: 0.253580  [   14/   89]
Per-example loss in batch: 0.249736  [   16/   89]
Per-example loss in batch: 0.251829  [   18/   89]
Per-example loss in batch: 0.212737  [   20/   89]
Per-example loss in batch: 0.252079  [   22/   89]
Per-example loss in batch: 0.207001  [   24/   89]
Per-example loss in batch: 0.308796  [   26/   89]
Per-example loss in batch: 0.239639  [   28/   89]
Per-example loss in batch: 0.313755  [   30/   89]
Per-example loss in batch: 0.256961  [   32/   89]
Per-example loss in batch: 0.292758  [   34/   89]
Per-example loss in batch: 0.289406  [   36/   89]
Per-example loss in batch: 0.226425  [   38/   89]
Per-example loss in batch: 0.222711  [   40/   89]
Per-example loss in batch: 0.203537  [   42/   89]
Per-example loss in batch: 0.248698  [   44/   89]
Per-example loss in batch: 0.253479  [   46/   89]
Per-example loss in batch: 0.344669  [   48/   89]
Per-example loss in batch: 0.255401  [   50/   89]
Per-example loss in batch: 0.220712  [   52/   89]
Per-example loss in batch: 0.239884  [   54/   89]
Per-example loss in batch: 0.267493  [   56/   89]
Per-example loss in batch: 0.198113  [   58/   89]
Per-example loss in batch: 0.221644  [   60/   89]
Per-example loss in batch: 0.316024  [   62/   89]
Per-example loss in batch: 0.271970  [   64/   89]
Per-example loss in batch: 0.226507  [   66/   89]
Per-example loss in batch: 0.269388  [   68/   89]
Per-example loss in batch: 0.333225  [   70/   89]
Per-example loss in batch: 0.295861  [   72/   89]
Per-example loss in batch: 0.291344  [   74/   89]
Per-example loss in batch: 0.306626  [   76/   89]
Per-example loss in batch: 0.277579  [   78/   89]
Per-example loss in batch: 0.232056  [   80/   89]
Per-example loss in batch: 0.323821  [   82/   89]
Per-example loss in batch: 0.271914  [   84/   89]
Per-example loss in batch: 0.311010  [   86/   89]
Per-example loss in batch: 0.279583  [   88/   89]
Per-example loss in batch: 0.520738  [   89/   89]
Train Error: Avg loss: 0.26506120
validation Error: 
 Avg loss: 0.28511392 
 F1: 0.473037 
 Precision: 0.562744 
 Recall: 0.407998
 IoU: 0.309790

test Error: 
 Avg loss: 0.26201350 
 F1: 0.508170 
 Precision: 0.619570 
 Recall: 0.430724
 IoU: 0.340635

We have finished training iteration 327
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_325_.pth
Per-example loss in batch: 0.282747  [    2/   89]
Per-example loss in batch: 0.263591  [    4/   89]
Per-example loss in batch: 0.271639  [    6/   89]
Per-example loss in batch: 0.249435  [    8/   89]
Per-example loss in batch: 0.242883  [   10/   89]
Per-example loss in batch: 0.221178  [   12/   89]
Per-example loss in batch: 0.233687  [   14/   89]
Per-example loss in batch: 0.220259  [   16/   89]
Per-example loss in batch: 0.240098  [   18/   89]
Per-example loss in batch: 0.278863  [   20/   89]
Per-example loss in batch: 0.317242  [   22/   89]
Per-example loss in batch: 0.271591  [   24/   89]
Per-example loss in batch: 0.363964  [   26/   89]
Per-example loss in batch: 0.355747  [   28/   89]
Per-example loss in batch: 0.241056  [   30/   89]
Per-example loss in batch: 0.281927  [   32/   89]
Per-example loss in batch: 0.210370  [   34/   89]
Per-example loss in batch: 0.299481  [   36/   89]
Per-example loss in batch: 0.228281  [   38/   89]
Per-example loss in batch: 0.217179  [   40/   89]
Per-example loss in batch: 0.305836  [   42/   89]
Per-example loss in batch: 0.297896  [   44/   89]
Per-example loss in batch: 0.248610  [   46/   89]
Per-example loss in batch: 0.273430  [   48/   89]
Per-example loss in batch: 0.207769  [   50/   89]
Per-example loss in batch: 0.239434  [   52/   89]
Per-example loss in batch: 0.218321  [   54/   89]
Per-example loss in batch: 0.262870  [   56/   89]
Per-example loss in batch: 0.231986  [   58/   89]
Per-example loss in batch: 0.297907  [   60/   89]
Per-example loss in batch: 0.252575  [   62/   89]
Per-example loss in batch: 0.278713  [   64/   89]
Per-example loss in batch: 0.288052  [   66/   89]
Per-example loss in batch: 0.285115  [   68/   89]
Per-example loss in batch: 0.285140  [   70/   89]
Per-example loss in batch: 0.250382  [   72/   89]
Per-example loss in batch: 0.223649  [   74/   89]
Per-example loss in batch: 0.225967  [   76/   89]
Per-example loss in batch: 0.277246  [   78/   89]
Per-example loss in batch: 0.266327  [   80/   89]
Per-example loss in batch: 0.251053  [   82/   89]
Per-example loss in batch: 0.236811  [   84/   89]
Per-example loss in batch: 0.295999  [   86/   89]
Per-example loss in batch: 0.297830  [   88/   89]
Per-example loss in batch: 0.575273  [   89/   89]
Train Error: Avg loss: 0.26691622
validation Error: 
 Avg loss: 0.27785290 
 F1: 0.474071 
 Precision: 0.547328 
 Recall: 0.418109
 IoU: 0.310677

test Error: 
 Avg loss: 0.26075746 
 F1: 0.510179 
 Precision: 0.602493 
 Recall: 0.442396
 IoU: 0.342443

We have finished training iteration 328
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_326_.pth
Per-example loss in batch: 0.249929  [    2/   89]
Per-example loss in batch: 0.313772  [    4/   89]
Per-example loss in batch: 0.239273  [    6/   89]
Per-example loss in batch: 0.212928  [    8/   89]
Per-example loss in batch: 0.333591  [   10/   89]
Per-example loss in batch: 0.253790  [   12/   89]
Per-example loss in batch: 0.319854  [   14/   89]
Per-example loss in batch: 0.324968  [   16/   89]
Per-example loss in batch: 0.227924  [   18/   89]
Per-example loss in batch: 0.282166  [   20/   89]
Per-example loss in batch: 0.213198  [   22/   89]
Per-example loss in batch: 0.264741  [   24/   89]
Per-example loss in batch: 0.321985  [   26/   89]
Per-example loss in batch: 0.235127  [   28/   89]
Per-example loss in batch: 0.377914  [   30/   89]
Per-example loss in batch: 0.295723  [   32/   89]
Per-example loss in batch: 0.218825  [   34/   89]
Per-example loss in batch: 0.280080  [   36/   89]
Per-example loss in batch: 0.315249  [   38/   89]
Per-example loss in batch: 0.253808  [   40/   89]
Per-example loss in batch: 0.261909  [   42/   89]
Per-example loss in batch: 0.287457  [   44/   89]
Per-example loss in batch: 0.193784  [   46/   89]
Per-example loss in batch: 0.337396  [   48/   89]
Per-example loss in batch: 0.201021  [   50/   89]
Per-example loss in batch: 0.226311  [   52/   89]
Per-example loss in batch: 0.209675  [   54/   89]
Per-example loss in batch: 0.333488  [   56/   89]
Per-example loss in batch: 0.245460  [   58/   89]
Per-example loss in batch: 0.231561  [   60/   89]
Per-example loss in batch: 0.211732  [   62/   89]
Per-example loss in batch: 0.252200  [   64/   89]
Per-example loss in batch: 0.263465  [   66/   89]
Per-example loss in batch: 0.256147  [   68/   89]
Per-example loss in batch: 0.217572  [   70/   89]
Per-example loss in batch: 0.271187  [   72/   89]
Per-example loss in batch: 0.265429  [   74/   89]
Per-example loss in batch: 0.329718  [   76/   89]
Per-example loss in batch: 0.249901  [   78/   89]
Per-example loss in batch: 0.239122  [   80/   89]
Per-example loss in batch: 0.240742  [   82/   89]
Per-example loss in batch: 0.243477  [   84/   89]
Per-example loss in batch: 0.314135  [   86/   89]
Per-example loss in batch: 0.245844  [   88/   89]
Per-example loss in batch: 0.473603  [   89/   89]
Train Error: Avg loss: 0.26742424
validation Error: 
 Avg loss: 0.28400360 
 F1: 0.473214 
 Precision: 0.539454 
 Recall: 0.421461
 IoU: 0.309941

test Error: 
 Avg loss: 0.26093847 
 F1: 0.509952 
 Precision: 0.593766 
 Recall: 0.446872
 IoU: 0.342238

We have finished training iteration 329
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_327_.pth
Per-example loss in batch: 0.305534  [    2/   89]
Per-example loss in batch: 0.280194  [    4/   89]
Per-example loss in batch: 0.269179  [    6/   89]
Per-example loss in batch: 0.199390  [    8/   89]
Per-example loss in batch: 0.199721  [   10/   89]
Per-example loss in batch: 0.251239  [   12/   89]
Per-example loss in batch: 0.224421  [   14/   89]
Per-example loss in batch: 0.209237  [   16/   89]
Per-example loss in batch: 0.236886  [   18/   89]
Per-example loss in batch: 0.326214  [   20/   89]
Per-example loss in batch: 0.300803  [   22/   89]
Per-example loss in batch: 0.215120  [   24/   89]
Per-example loss in batch: 0.295116  [   26/   89]
Per-example loss in batch: 0.318642  [   28/   89]
Per-example loss in batch: 0.229744  [   30/   89]
Per-example loss in batch: 0.267461  [   32/   89]
Per-example loss in batch: 0.269762  [   34/   89]
Per-example loss in batch: 0.284474  [   36/   89]
Per-example loss in batch: 0.235720  [   38/   89]
Per-example loss in batch: 0.229323  [   40/   89]
Per-example loss in batch: 0.275190  [   42/   89]
Per-example loss in batch: 0.327378  [   44/   89]
Per-example loss in batch: 0.304556  [   46/   89]
Per-example loss in batch: 0.231566  [   48/   89]
Per-example loss in batch: 0.244834  [   50/   89]
Per-example loss in batch: 0.297720  [   52/   89]
Per-example loss in batch: 0.209205  [   54/   89]
Per-example loss in batch: 0.260744  [   56/   89]
Per-example loss in batch: 0.259340  [   58/   89]
Per-example loss in batch: 0.237180  [   60/   89]
Per-example loss in batch: 0.203925  [   62/   89]
Per-example loss in batch: 0.243313  [   64/   89]
Per-example loss in batch: 0.319172  [   66/   89]
Per-example loss in batch: 0.271275  [   68/   89]
Per-example loss in batch: 0.349005  [   70/   89]
Per-example loss in batch: 0.218872  [   72/   89]
Per-example loss in batch: 0.238441  [   74/   89]
Per-example loss in batch: 0.279963  [   76/   89]
Per-example loss in batch: 0.249035  [   78/   89]
Per-example loss in batch: 0.231930  [   80/   89]
Per-example loss in batch: 0.318598  [   82/   89]
Per-example loss in batch: 0.290018  [   84/   89]
Per-example loss in batch: 0.256263  [   86/   89]
Per-example loss in batch: 0.292391  [   88/   89]
Per-example loss in batch: 0.614414  [   89/   89]
Train Error: Avg loss: 0.26663595
validation Error: 
 Avg loss: 0.27808410 
 F1: 0.475094 
 Precision: 0.554573 
 Recall: 0.415541
 IoU: 0.311556

test Error: 
 Avg loss: 0.26121986 
 F1: 0.508911 
 Precision: 0.608172 
 Recall: 0.437505
 IoU: 0.341302

We have finished training iteration 330
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_328_.pth
Per-example loss in batch: 0.222527  [    2/   89]
Per-example loss in batch: 0.249058  [    4/   89]
Per-example loss in batch: 0.290691  [    6/   89]
Per-example loss in batch: 0.257547  [    8/   89]
Per-example loss in batch: 0.249929  [   10/   89]
Per-example loss in batch: 0.277768  [   12/   89]
Per-example loss in batch: 0.292623  [   14/   89]
Per-example loss in batch: 0.280463  [   16/   89]
Per-example loss in batch: 0.248045  [   18/   89]
Per-example loss in batch: 0.212341  [   20/   89]
Per-example loss in batch: 0.215363  [   22/   89]
Per-example loss in batch: 0.333248  [   24/   89]
Per-example loss in batch: 0.305661  [   26/   89]
Per-example loss in batch: 0.285200  [   28/   89]
Per-example loss in batch: 0.271473  [   30/   89]
Per-example loss in batch: 0.239838  [   32/   89]
Per-example loss in batch: 0.221881  [   34/   89]
Per-example loss in batch: 0.300467  [   36/   89]
Per-example loss in batch: 0.202893  [   38/   89]
Per-example loss in batch: 0.247636  [   40/   89]
Per-example loss in batch: 0.225183  [   42/   89]
Per-example loss in batch: 0.255969  [   44/   89]
Per-example loss in batch: 0.302101  [   46/   89]
Per-example loss in batch: 0.279435  [   48/   89]
Per-example loss in batch: 0.307567  [   50/   89]
Per-example loss in batch: 0.265543  [   52/   89]
Per-example loss in batch: 0.201564  [   54/   89]
Per-example loss in batch: 0.263343  [   56/   89]
Per-example loss in batch: 0.275323  [   58/   89]
Per-example loss in batch: 0.257778  [   60/   89]
Per-example loss in batch: 0.293632  [   62/   89]
Per-example loss in batch: 0.206134  [   64/   89]
Per-example loss in batch: 0.206496  [   66/   89]
Per-example loss in batch: 0.229970  [   68/   89]
Per-example loss in batch: 0.249871  [   70/   89]
Per-example loss in batch: 0.328901  [   72/   89]
Per-example loss in batch: 0.244240  [   74/   89]
Per-example loss in batch: 0.329216  [   76/   89]
Per-example loss in batch: 0.248653  [   78/   89]
Per-example loss in batch: 0.225533  [   80/   89]
Per-example loss in batch: 0.217320  [   82/   89]
Per-example loss in batch: 0.262946  [   84/   89]
Per-example loss in batch: 0.283732  [   86/   89]
Per-example loss in batch: 0.298374  [   88/   89]
Per-example loss in batch: 0.646890  [   89/   89]
Train Error: Avg loss: 0.26487464
validation Error: 
 Avg loss: 0.28435259 
 F1: 0.475032 
 Precision: 0.554539 
 Recall: 0.415465
 IoU: 0.311503

test Error: 
 Avg loss: 0.26093225 
 F1: 0.509470 
 Precision: 0.609589 
 Recall: 0.437599
 IoU: 0.341805

We have finished training iteration 331
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_329_.pth
Per-example loss in batch: 0.260010  [    2/   89]
Per-example loss in batch: 0.239251  [    4/   89]
Per-example loss in batch: 0.344500  [    6/   89]
Per-example loss in batch: 0.226175  [    8/   89]
Per-example loss in batch: 0.328552  [   10/   89]
Per-example loss in batch: 0.232528  [   12/   89]
Per-example loss in batch: 0.220932  [   14/   89]
Per-example loss in batch: 0.231006  [   16/   89]
Per-example loss in batch: 0.276623  [   18/   89]
Per-example loss in batch: 0.250304  [   20/   89]
Per-example loss in batch: 0.266637  [   22/   89]
Per-example loss in batch: 0.231120  [   24/   89]
Per-example loss in batch: 0.269916  [   26/   89]
Per-example loss in batch: 0.249778  [   28/   89]
Per-example loss in batch: 0.284216  [   30/   89]
Per-example loss in batch: 0.325402  [   32/   89]
Per-example loss in batch: 0.265717  [   34/   89]
Per-example loss in batch: 0.253297  [   36/   89]
Per-example loss in batch: 0.229865  [   38/   89]
Per-example loss in batch: 0.218881  [   40/   89]
Per-example loss in batch: 0.340474  [   42/   89]
Per-example loss in batch: 0.198812  [   44/   89]
Per-example loss in batch: 0.266982  [   46/   89]
Per-example loss in batch: 0.262881  [   48/   89]
Per-example loss in batch: 0.268288  [   50/   89]
Per-example loss in batch: 0.246893  [   52/   89]
Per-example loss in batch: 0.256788  [   54/   89]
Per-example loss in batch: 0.257597  [   56/   89]
Per-example loss in batch: 0.304672  [   58/   89]
Per-example loss in batch: 0.278006  [   60/   89]
Per-example loss in batch: 0.327532  [   62/   89]
Per-example loss in batch: 0.226851  [   64/   89]
Per-example loss in batch: 0.302020  [   66/   89]
Per-example loss in batch: 0.209406  [   68/   89]
Per-example loss in batch: 0.218857  [   70/   89]
Per-example loss in batch: 0.340358  [   72/   89]
Per-example loss in batch: 0.238145  [   74/   89]
Per-example loss in batch: 0.259350  [   76/   89]
Per-example loss in batch: 0.351287  [   78/   89]
Per-example loss in batch: 0.207861  [   80/   89]
Per-example loss in batch: 0.252906  [   82/   89]
Per-example loss in batch: 0.213057  [   84/   89]
Per-example loss in batch: 0.275493  [   86/   89]
Per-example loss in batch: 0.280425  [   88/   89]
Per-example loss in batch: 0.614760  [   89/   89]
Train Error: Avg loss: 0.26734902
validation Error: 
 Avg loss: 0.28832304 
 F1: 0.474356 
 Precision: 0.541916 
 Recall: 0.421774
 IoU: 0.310922

test Error: 
 Avg loss: 0.26054767 
 F1: 0.510780 
 Precision: 0.596486 
 Recall: 0.446609
 IoU: 0.342985

We have finished training iteration 332
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_330_.pth
Per-example loss in batch: 0.231105  [    2/   89]
Per-example loss in batch: 0.319648  [    4/   89]
Per-example loss in batch: 0.288884  [    6/   89]
Per-example loss in batch: 0.265539  [    8/   89]
Per-example loss in batch: 0.247254  [   10/   89]
Per-example loss in batch: 0.211182  [   12/   89]
Per-example loss in batch: 0.237821  [   14/   89]
Per-example loss in batch: 0.271370  [   16/   89]
Per-example loss in batch: 0.256989  [   18/   89]
Per-example loss in batch: 0.297807  [   20/   89]
Per-example loss in batch: 0.289719  [   22/   89]
Per-example loss in batch: 0.294614  [   24/   89]
Per-example loss in batch: 0.238308  [   26/   89]
Per-example loss in batch: 0.296733  [   28/   89]
Per-example loss in batch: 0.315124  [   30/   89]
Per-example loss in batch: 0.257448  [   32/   89]
Per-example loss in batch: 0.266213  [   34/   89]
Per-example loss in batch: 0.285339  [   36/   89]
Per-example loss in batch: 0.201174  [   38/   89]
Per-example loss in batch: 0.262190  [   40/   89]
Per-example loss in batch: 0.260022  [   42/   89]
Per-example loss in batch: 0.219231  [   44/   89]
Per-example loss in batch: 0.271483  [   46/   89]
Per-example loss in batch: 0.234003  [   48/   89]
Per-example loss in batch: 0.213161  [   50/   89]
Per-example loss in batch: 0.233263  [   52/   89]
Per-example loss in batch: 0.310428  [   54/   89]
Per-example loss in batch: 0.297291  [   56/   89]
Per-example loss in batch: 0.262572  [   58/   89]
Per-example loss in batch: 0.252372  [   60/   89]
Per-example loss in batch: 0.284841  [   62/   89]
Per-example loss in batch: 0.254430  [   64/   89]
Per-example loss in batch: 0.231273  [   66/   89]
Per-example loss in batch: 0.289785  [   68/   89]
Per-example loss in batch: 0.278689  [   70/   89]
Per-example loss in batch: 0.316491  [   72/   89]
Per-example loss in batch: 0.259809  [   74/   89]
Per-example loss in batch: 0.296755  [   76/   89]
Per-example loss in batch: 0.251900  [   78/   89]
Per-example loss in batch: 0.278119  [   80/   89]
Per-example loss in batch: 0.383151  [   82/   89]
Per-example loss in batch: 0.279055  [   84/   89]
Per-example loss in batch: 0.302222  [   86/   89]
Per-example loss in batch: 0.229180  [   88/   89]
Per-example loss in batch: 0.381482  [   89/   89]
Train Error: Avg loss: 0.26999389
validation Error: 
 Avg loss: 0.28552707 
 F1: 0.474223 
 Precision: 0.560394 
 Recall: 0.411021
 IoU: 0.310807

test Error: 
 Avg loss: 0.26266083 
 F1: 0.505793 
 Precision: 0.612007 
 Recall: 0.430994
 IoU: 0.338503

We have finished training iteration 333
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_331_.pth
Per-example loss in batch: 0.292430  [    2/   89]
Per-example loss in batch: 0.264811  [    4/   89]
Per-example loss in batch: 0.223389  [    6/   89]
Per-example loss in batch: 0.255748  [    8/   89]
Per-example loss in batch: 0.245113  [   10/   89]
Per-example loss in batch: 0.263828  [   12/   89]
Per-example loss in batch: 0.240294  [   14/   89]
Per-example loss in batch: 0.206201  [   16/   89]
Per-example loss in batch: 0.221740  [   18/   89]
Per-example loss in batch: 0.232076  [   20/   89]
Per-example loss in batch: 0.309118  [   22/   89]
Per-example loss in batch: 0.258897  [   24/   89]
Per-example loss in batch: 0.375942  [   26/   89]
Per-example loss in batch: 0.261656  [   28/   89]
Per-example loss in batch: 0.211295  [   30/   89]
Per-example loss in batch: 0.255925  [   32/   89]
Per-example loss in batch: 0.274509  [   34/   89]
Per-example loss in batch: 0.197478  [   36/   89]
Per-example loss in batch: 0.270736  [   38/   89]
Per-example loss in batch: 0.251553  [   40/   89]
Per-example loss in batch: 0.220575  [   42/   89]
Per-example loss in batch: 0.302039  [   44/   89]
Per-example loss in batch: 0.212324  [   46/   89]
Per-example loss in batch: 0.350388  [   48/   89]
Per-example loss in batch: 0.277340  [   50/   89]
Per-example loss in batch: 0.245652  [   52/   89]
Per-example loss in batch: 0.302994  [   54/   89]
Per-example loss in batch: 0.272017  [   56/   89]
Per-example loss in batch: 0.251934  [   58/   89]
Per-example loss in batch: 0.222669  [   60/   89]
Per-example loss in batch: 0.302599  [   62/   89]
Per-example loss in batch: 0.261573  [   64/   89]
Per-example loss in batch: 0.371542  [   66/   89]
Per-example loss in batch: 0.284242  [   68/   89]
Per-example loss in batch: 0.235697  [   70/   89]
Per-example loss in batch: 0.253741  [   72/   89]
Per-example loss in batch: 0.232625  [   74/   89]
Per-example loss in batch: 0.303329  [   76/   89]
Per-example loss in batch: 0.305386  [   78/   89]
Per-example loss in batch: 0.273468  [   80/   89]
Per-example loss in batch: 0.239019  [   82/   89]
Per-example loss in batch: 0.247769  [   84/   89]
Per-example loss in batch: 0.311360  [   86/   89]
Per-example loss in batch: 0.251099  [   88/   89]
Per-example loss in batch: 0.508802  [   89/   89]
Train Error: Avg loss: 0.26738242
validation Error: 
 Avg loss: 0.28555254 
 F1: 0.474791 
 Precision: 0.546326 
 Recall: 0.419821
 IoU: 0.311296

test Error: 
 Avg loss: 0.26069045 
 F1: 0.510232 
 Precision: 0.601094 
 Recall: 0.443232
 IoU: 0.342491

We have finished training iteration 334
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_332_.pth
Per-example loss in batch: 0.223647  [    2/   89]
Per-example loss in batch: 0.239743  [    4/   89]
Per-example loss in batch: 0.322356  [    6/   89]
Per-example loss in batch: 0.274267  [    8/   89]
Per-example loss in batch: 0.330475  [   10/   89]
Per-example loss in batch: 0.234022  [   12/   89]
Per-example loss in batch: 0.289767  [   14/   89]
Per-example loss in batch: 0.303517  [   16/   89]
Per-example loss in batch: 0.350195  [   18/   89]
Per-example loss in batch: 0.237014  [   20/   89]
Per-example loss in batch: 0.240049  [   22/   89]
Per-example loss in batch: 0.249178  [   24/   89]
Per-example loss in batch: 0.232288  [   26/   89]
Per-example loss in batch: 0.359289  [   28/   89]
Per-example loss in batch: 0.264584  [   30/   89]
Per-example loss in batch: 0.218227  [   32/   89]
Per-example loss in batch: 0.359439  [   34/   89]
Per-example loss in batch: 0.207317  [   36/   89]
Per-example loss in batch: 0.215964  [   38/   89]
Per-example loss in batch: 0.318657  [   40/   89]
Per-example loss in batch: 0.237636  [   42/   89]
Per-example loss in batch: 0.333945  [   44/   89]
Per-example loss in batch: 0.236144  [   46/   89]
Per-example loss in batch: 0.222114  [   48/   89]
Per-example loss in batch: 0.242604  [   50/   89]
Per-example loss in batch: 0.205705  [   52/   89]
Per-example loss in batch: 0.219009  [   54/   89]
Per-example loss in batch: 0.322622  [   56/   89]
Per-example loss in batch: 0.260841  [   58/   89]
Per-example loss in batch: 0.330046  [   60/   89]
Per-example loss in batch: 0.302781  [   62/   89]
Per-example loss in batch: 0.295102  [   64/   89]
Per-example loss in batch: 0.268270  [   66/   89]
Per-example loss in batch: 0.298753  [   68/   89]
Per-example loss in batch: 0.284352  [   70/   89]
Per-example loss in batch: 0.260685  [   72/   89]
Per-example loss in batch: 0.292616  [   74/   89]
Per-example loss in batch: 0.262457  [   76/   89]
Per-example loss in batch: 0.203628  [   78/   89]
Per-example loss in batch: 0.233770  [   80/   89]
Per-example loss in batch: 0.277108  [   82/   89]
Per-example loss in batch: 0.280173  [   84/   89]
Per-example loss in batch: 0.258520  [   86/   89]
Per-example loss in batch: 0.226226  [   88/   89]
Per-example loss in batch: 0.624703  [   89/   89]
Train Error: Avg loss: 0.27275179
validation Error: 
 Avg loss: 0.28571383 
 F1: 0.474720 
 Precision: 0.549725 
 Recall: 0.417726
 IoU: 0.311235

test Error: 
 Avg loss: 0.26112466 
 F1: 0.509211 
 Precision: 0.600526 
 Recall: 0.442001
 IoU: 0.341571

We have finished training iteration 335
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_333_.pth
Per-example loss in batch: 0.341628  [    2/   89]
Per-example loss in batch: 0.315753  [    4/   89]
Per-example loss in batch: 0.255854  [    6/   89]
Per-example loss in batch: 0.245087  [    8/   89]
Per-example loss in batch: 0.261212  [   10/   89]
Per-example loss in batch: 0.232112  [   12/   89]
Per-example loss in batch: 0.318202  [   14/   89]
Per-example loss in batch: 0.258545  [   16/   89]
Per-example loss in batch: 0.217521  [   18/   89]
Per-example loss in batch: 0.240177  [   20/   89]
Per-example loss in batch: 0.303202  [   22/   89]
Per-example loss in batch: 0.288985  [   24/   89]
Per-example loss in batch: 0.254692  [   26/   89]
Per-example loss in batch: 0.204693  [   28/   89]
Per-example loss in batch: 0.250107  [   30/   89]
Per-example loss in batch: 0.254459  [   32/   89]
Per-example loss in batch: 0.303033  [   34/   89]
Per-example loss in batch: 0.243558  [   36/   89]
Per-example loss in batch: 0.195586  [   38/   89]
Per-example loss in batch: 0.316950  [   40/   89]
Per-example loss in batch: 0.267724  [   42/   89]
Per-example loss in batch: 0.212578  [   44/   89]
Per-example loss in batch: 0.264890  [   46/   89]
Per-example loss in batch: 0.301838  [   48/   89]
Per-example loss in batch: 0.228374  [   50/   89]
Per-example loss in batch: 0.221998  [   52/   89]
Per-example loss in batch: 0.243273  [   54/   89]
Per-example loss in batch: 0.232816  [   56/   89]
Per-example loss in batch: 0.260538  [   58/   89]
Per-example loss in batch: 0.276008  [   60/   89]
Per-example loss in batch: 0.308717  [   62/   89]
Per-example loss in batch: 0.290333  [   64/   89]
Per-example loss in batch: 0.262178  [   66/   89]
Per-example loss in batch: 0.256532  [   68/   89]
Per-example loss in batch: 0.280420  [   70/   89]
Per-example loss in batch: 0.255172  [   72/   89]
Per-example loss in batch: 0.208992  [   74/   89]
Per-example loss in batch: 0.234007  [   76/   89]
Per-example loss in batch: 0.244986  [   78/   89]
Per-example loss in batch: 0.221072  [   80/   89]
Per-example loss in batch: 0.254328  [   82/   89]
Per-example loss in batch: 0.242031  [   84/   89]
Per-example loss in batch: 0.283875  [   86/   89]
Per-example loss in batch: 0.304060  [   88/   89]
Per-example loss in batch: 0.428305  [   89/   89]
Train Error: Avg loss: 0.26229774
validation Error: 
 Avg loss: 0.28358485 
 F1: 0.475194 
 Precision: 0.545784 
 Recall: 0.420773
 IoU: 0.311643

test Error: 
 Avg loss: 0.26062928 
 F1: 0.510279 
 Precision: 0.599393 
 Recall: 0.444234
 IoU: 0.342534

We have finished training iteration 336
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_334_.pth
Per-example loss in batch: 0.342525  [    2/   89]
Per-example loss in batch: 0.196374  [    4/   89]
Per-example loss in batch: 0.331685  [    6/   89]
Per-example loss in batch: 0.232685  [    8/   89]
Per-example loss in batch: 0.282370  [   10/   89]
Per-example loss in batch: 0.287021  [   12/   89]
Per-example loss in batch: 0.309750  [   14/   89]
Per-example loss in batch: 0.218788  [   16/   89]
Per-example loss in batch: 0.259024  [   18/   89]
Per-example loss in batch: 0.292404  [   20/   89]
Per-example loss in batch: 0.211553  [   22/   89]
Per-example loss in batch: 0.247383  [   24/   89]
Per-example loss in batch: 0.214080  [   26/   89]
Per-example loss in batch: 0.224990  [   28/   89]
Per-example loss in batch: 0.283669  [   30/   89]
Per-example loss in batch: 0.232573  [   32/   89]
Per-example loss in batch: 0.264758  [   34/   89]
Per-example loss in batch: 0.231916  [   36/   89]
Per-example loss in batch: 0.231974  [   38/   89]
Per-example loss in batch: 0.290135  [   40/   89]
Per-example loss in batch: 0.229178  [   42/   89]
Per-example loss in batch: 0.324086  [   44/   89]
Per-example loss in batch: 0.277752  [   46/   89]
Per-example loss in batch: 0.268719  [   48/   89]
Per-example loss in batch: 0.259508  [   50/   89]
Per-example loss in batch: 0.220084  [   52/   89]
Per-example loss in batch: 0.308496  [   54/   89]
Per-example loss in batch: 0.211544  [   56/   89]
Per-example loss in batch: 0.327371  [   58/   89]
Per-example loss in batch: 0.285488  [   60/   89]
Per-example loss in batch: 0.208203  [   62/   89]
Per-example loss in batch: 0.268199  [   64/   89]
Per-example loss in batch: 0.314021  [   66/   89]
Per-example loss in batch: 0.229376  [   68/   89]
Per-example loss in batch: 0.301136  [   70/   89]
Per-example loss in batch: 0.267795  [   72/   89]
Per-example loss in batch: 0.316250  [   74/   89]
Per-example loss in batch: 0.196970  [   76/   89]
Per-example loss in batch: 0.205268  [   78/   89]
Per-example loss in batch: 0.219668  [   80/   89]
Per-example loss in batch: 0.235227  [   82/   89]
Per-example loss in batch: 0.209192  [   84/   89]
Per-example loss in batch: 0.282056  [   86/   89]
Per-example loss in batch: 0.318052  [   88/   89]
Per-example loss in batch: 0.544330  [   89/   89]
Train Error: Avg loss: 0.26385304
validation Error: 
 Avg loss: 0.28467512 
 F1: 0.473242 
 Precision: 0.539959 
 Recall: 0.421199
 IoU: 0.309965

test Error: 
 Avg loss: 0.26111276 
 F1: 0.509840 
 Precision: 0.594278 
 Recall: 0.446411
 IoU: 0.342137

We have finished training iteration 337
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_335_.pth
Per-example loss in batch: 0.292885  [    2/   89]
Per-example loss in batch: 0.288612  [    4/   89]
Per-example loss in batch: 0.213151  [    6/   89]
Per-example loss in batch: 0.281421  [    8/   89]
Per-example loss in batch: 0.223848  [   10/   89]
Per-example loss in batch: 0.247727  [   12/   89]
Per-example loss in batch: 0.217158  [   14/   89]
Per-example loss in batch: 0.232132  [   16/   89]
Per-example loss in batch: 0.248856  [   18/   89]
Per-example loss in batch: 0.332831  [   20/   89]
Per-example loss in batch: 0.219269  [   22/   89]
Per-example loss in batch: 0.242656  [   24/   89]
Per-example loss in batch: 0.241539  [   26/   89]
Per-example loss in batch: 0.243797  [   28/   89]
Per-example loss in batch: 0.279784  [   30/   89]
Per-example loss in batch: 0.298083  [   32/   89]
Per-example loss in batch: 0.240025  [   34/   89]
Per-example loss in batch: 0.295638  [   36/   89]
Per-example loss in batch: 0.239555  [   38/   89]
Per-example loss in batch: 0.274861  [   40/   89]
Per-example loss in batch: 0.235990  [   42/   89]
Per-example loss in batch: 0.241681  [   44/   89]
Per-example loss in batch: 0.214562  [   46/   89]
Per-example loss in batch: 0.226324  [   48/   89]
Per-example loss in batch: 0.301117  [   50/   89]
Per-example loss in batch: 0.270906  [   52/   89]
Per-example loss in batch: 0.228148  [   54/   89]
Per-example loss in batch: 0.364061  [   56/   89]
Per-example loss in batch: 0.320024  [   58/   89]
Per-example loss in batch: 0.243792  [   60/   89]
Per-example loss in batch: 0.239868  [   62/   89]
Per-example loss in batch: 0.243372  [   64/   89]
Per-example loss in batch: 0.299887  [   66/   89]
Per-example loss in batch: 0.254502  [   68/   89]
Per-example loss in batch: 0.284066  [   70/   89]
Per-example loss in batch: 0.274972  [   72/   89]
Per-example loss in batch: 0.221142  [   74/   89]
Per-example loss in batch: 0.211235  [   76/   89]
Per-example loss in batch: 0.295415  [   78/   89]
Per-example loss in batch: 0.232173  [   80/   89]
Per-example loss in batch: 0.303158  [   82/   89]
Per-example loss in batch: 0.270362  [   84/   89]
Per-example loss in batch: 0.236147  [   86/   89]
Per-example loss in batch: 0.211847  [   88/   89]
Per-example loss in batch: 0.685513  [   89/   89]
Train Error: Avg loss: 0.26340076
validation Error: 
 Avg loss: 0.28355555 
 F1: 0.474581 
 Precision: 0.552404 
 Recall: 0.415978
 IoU: 0.311115

test Error: 
 Avg loss: 0.26080691 
 F1: 0.510016 
 Precision: 0.608636 
 Recall: 0.438900
 IoU: 0.342297

We have finished training iteration 338
Deleting model ./unet_wsc_train/saved_model_wrapper/models/UNet_336_.pth
Per-example loss in batch: 0.233539  [    2/   89]
Per-example loss in batch: 0.282142  [    4/   89]
Per-example loss in batch: 0.248135  [    6/   89]
Per-example loss in batch: 0.235013  [    8/   89]
Per-example loss in batch: 0.256227  [   10/   89]
Per-example loss in batch: 0.247228  [   12/   89]
Per-example loss in batch: 0.224234  [   14/   89]
Per-example loss in batch: 0.250704  [   16/   89]
Per-example loss in batch: 0.210531  [   18/   89]
Per-example loss in batch: 0.212832  [   20/   89]
Per-example loss in batch: 0.274855  [   22/   89]
Per-example loss in batch: 0.314263  [   24/   89]
Per-example loss in batch: 0.244194  [   26/   89]
Per-example loss in batch: 0.270543  [   28/   89]
Per-example loss in batch: 0.237860  [   30/   89]
Per-example loss in batch: 0.238777  [   32/   89]
Per-example loss in batch: 0.250788  [   34/   89]
Per-example loss in batch: 0.292080  [   36/   89]
Per-example loss in batch: 0.308460  [   38/   89]
Per-example loss in batch: 0.331904  [   40/   89]
Per-example loss in batch: 0.230908  [   42/   89]
Per-example loss in batch: 0.262801  [   44/   89]
Per-example loss in batch: 0.237125  [   46/   89]
Per-example loss in batch: 0.300412  [   48/   89]
Per-example loss in batch: 0.314299  [   50/   89]
Per-example loss in batch: 0.241397  [   52/   89]
Per-example loss in batch: 0.307553  [   54/   89]
Per-example loss in batch: 0.229123  [   56/   89]
Per-example loss in batch: 0.262585  [   58/   89]
Per-example loss in batch: 0.297312  [   60/   89]
Per-example loss in batch: 0.234975  [   62/   89]
Per-example loss in batch: 0.253152  [   64/   89]
Per-example loss in batch: 0.245511  [   66/   89]
Per-example loss in batch: 0.282041  [   68/   89]
Per-example loss in batch: 0.293143  [   70/   89]
Per-example loss in batch: 0.259295  [   72/   89]
Per-example loss in batch: 0.270671  [   74/   89]
Per-example loss in batch: 0.287476  [   76/   89]
Per-example loss in batch: 0.269549  [   78/   89]
Per-example loss in batch: 0.226820  [   80/   89]
Per-example loss in batch: 0.334311  [   82/   89]
Per-example loss in batch: 0.215619  [   84/   89]
Per-example loss in batch: 0.305143  [   86/   89]
Per-example loss in batch: 0.271834  [   88/   89]
Per-example loss in batch: 0.613672  [   89/   89]
Train Error: Avg loss: 0.26751017
validation Error: 
 Avg loss: 0.28690614 
 F1: 0.473647 
 Precision: 0.563951 
 Recall: 0.408271
 IoU: 0.310313

slurmstepd: error: *** STEP 16443.0 ON aga2 CANCELLED AT 2025-01-07T16:45:17 ***
