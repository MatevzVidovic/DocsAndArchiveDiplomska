/shared/home/matevz.vidovic/Diplomska/Prototip/Delo/model_wrapper.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model = torch.load(self.prev_model_path, map_location=torch.device(device))
unet_original_main.py do_log: True
Log file name: log_08_13-51-58_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(ptd='./Data/vein_and_sclera_data', sd='unet_bwzo315_train', mti=200, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_original_bwzo315.yaml', ntibp=None, ptp=None, map=None)
YAML: {'batch_size': 1, 'learning_rate': 1e-05, 'num_of_dataloader_workers': 7, 'train_epoch_size_limit': 400, 'num_epochs_per_training_iteration': 1, 'cleanup_k': 3, 'dataset_option': 'aug_bcosfire', 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': True, 'loss_fn_name': 'MCDL', 'alphas': [], 'model': '64_2_6', 'input_width': 3000, 'input_height': 1500, 'input_channels': 4, 'output_channels': 2, 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq'}
Validation phase: False
Namespace(ptd='./Data/vein_and_sclera_data', sd='unet_bwzo315_train', mti=200, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_original_bwzo315.yaml', ntibp=None, ptp=None, map=None)
Device: cuda
dataset_aug_bcosfire.py do_log: False
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 89
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 89
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 89
val dataloader num of batches: 27
test dataloader num of batches: 12
Loaded model path:  ./unet_bwzo315_train/saved_model_wrapper/models/UNet_61_.pth
per-ex loss: 0.379747  [    1/   89]
per-ex loss: 0.359542  [    2/   89]
per-ex loss: 0.384663  [    3/   89]
per-ex loss: 0.632922  [    4/   89]
per-ex loss: 0.583933  [    5/   89]
per-ex loss: 0.499150  [    6/   89]
per-ex loss: 0.403752  [    7/   89]
per-ex loss: 0.483237  [    8/   89]
per-ex loss: 0.558417  [    9/   89]
per-ex loss: 0.479637  [   10/   89]
per-ex loss: 0.553097  [   11/   89]
per-ex loss: 0.435333  [   12/   89]
per-ex loss: 0.632616  [   13/   89]
per-ex loss: 0.533115  [   14/   89]
per-ex loss: 0.429733  [   15/   89]
per-ex loss: 0.405679  [   16/   89]
per-ex loss: 0.339161  [   17/   89]
per-ex loss: 0.489698  [   18/   89]
per-ex loss: 0.500910  [   19/   89]
per-ex loss: 0.400163  [   20/   89]
per-ex loss: 0.348493  [   21/   89]
per-ex loss: 0.381459  [   22/   89]
per-ex loss: 0.497569  [   23/   89]
per-ex loss: 0.636888  [   24/   89]
per-ex loss: 0.376054  [   25/   89]
per-ex loss: 0.515098  [   26/   89]
per-ex loss: 0.496326  [   27/   89]
per-ex loss: 0.345544  [   28/   89]
per-ex loss: 0.999958  [   29/   89]
per-ex loss: 0.359933  [   30/   89]
per-ex loss: 0.595002  [   31/   89]
per-ex loss: 0.505508  [   32/   89]
per-ex loss: 0.549643  [   33/   89]
per-ex loss: 0.536464  [   34/   89]
per-ex loss: 0.299888  [   35/   89]
per-ex loss: 0.389646  [   36/   89]
per-ex loss: 0.652838  [   37/   89]
per-ex loss: 0.388925  [   38/   89]
per-ex loss: 0.342149  [   39/   89]
per-ex loss: 0.356654  [   40/   89]
per-ex loss: 0.320322  [   41/   89]
per-ex loss: 0.571610  [   42/   89]
per-ex loss: 0.333486  [   43/   89]
per-ex loss: 0.423217  [   44/   89]
per-ex loss: 0.385291  [   45/   89]
per-ex loss: 0.494844  [   46/   89]
per-ex loss: 0.437169  [   47/   89]
per-ex loss: 0.410722  [   48/   89]
per-ex loss: 0.302041  [   49/   89]
per-ex loss: 0.560163  [   50/   89]
per-ex loss: 0.333012  [   51/   89]
per-ex loss: 0.389202  [   52/   89]
per-ex loss: 0.379152  [   53/   89]
per-ex loss: 0.425402  [   54/   89]
per-ex loss: 0.432080  [   55/   89]
per-ex loss: 0.272929  [   56/   89]
per-ex loss: 0.555265  [   57/   89]
per-ex loss: 0.441693  [   58/   89]
per-ex loss: 0.353456  [   59/   89]
per-ex loss: 0.363716  [   60/   89]
per-ex loss: 0.450492  [   61/   89]
per-ex loss: 0.366864  [   62/   89]
per-ex loss: 0.434494  [   63/   89]
per-ex loss: 0.546016  [   64/   89]
per-ex loss: 0.593538  [   65/   89]
per-ex loss: 0.357349  [   66/   89]
per-ex loss: 0.618645  [   67/   89]
per-ex loss: 0.346928  [   68/   89]
per-ex loss: 0.367685  [   69/   89]
per-ex loss: 0.346368  [   70/   89]
per-ex loss: 0.498718  [   71/   89]
per-ex loss: 0.595691  [   72/   89]
per-ex loss: 0.331640  [   73/   89]
per-ex loss: 0.566569  [   74/   89]
per-ex loss: 0.679011  [   75/   89]
per-ex loss: 0.379599  [   76/   89]
per-ex loss: 0.557643  [   77/   89]
per-ex loss: 0.539782  [   78/   89]
per-ex loss: 0.411863  [   79/   89]
per-ex loss: 0.654784  [   80/   89]
per-ex loss: 0.502964  [   81/   89]
per-ex loss: 0.529848  [   82/   89]
per-ex loss: 0.397121  [   83/   89]
per-ex loss: 0.518422  [   84/   89]
per-ex loss: 0.595066  [   85/   89]
per-ex loss: 0.363752  [   86/   89]
per-ex loss: 0.579023  [   87/   89]
per-ex loss: 0.317752  [   88/   89]
per-ex loss: 0.341074  [   89/   89]
Train Error: Avg loss: 0.46103390
validation Error: 
 Avg loss: 0.54429967 
 F1: 0.480483 
 Precision: 0.477059 
 Recall: 0.483955
 IoU: 0.316207

test Error: 
 Avg loss: 0.48967418 
 F1: 0.559985 
 Precision: 0.543898 
 Recall: 0.577052
 IoU: 0.388874

We have finished training iteration 62
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_60_.pth
per-ex loss: 0.355649  [    1/   89]
per-ex loss: 0.335053  [    2/   89]
per-ex loss: 0.531779  [    3/   89]
per-ex loss: 0.633393  [    4/   89]
per-ex loss: 0.310954  [    5/   89]
per-ex loss: 0.657180  [    6/   89]
per-ex loss: 0.588580  [    7/   89]
per-ex loss: 0.315765  [    8/   89]
per-ex loss: 0.297220  [    9/   89]
per-ex loss: 0.532966  [   10/   89]
per-ex loss: 0.577422  [   11/   89]
per-ex loss: 0.366373  [   12/   89]
per-ex loss: 0.362311  [   13/   89]
per-ex loss: 0.369806  [   14/   89]
per-ex loss: 0.552483  [   15/   89]
per-ex loss: 0.564872  [   16/   89]
per-ex loss: 0.409954  [   17/   89]
per-ex loss: 0.552698  [   18/   89]
per-ex loss: 0.670438  [   19/   89]
per-ex loss: 0.377084  [   20/   89]
per-ex loss: 0.549766  [   21/   89]
per-ex loss: 0.448769  [   22/   89]
per-ex loss: 0.361291  [   23/   89]
per-ex loss: 0.524694  [   24/   89]
per-ex loss: 0.411337  [   25/   89]
per-ex loss: 0.535419  [   26/   89]
per-ex loss: 0.406154  [   27/   89]
per-ex loss: 0.348174  [   28/   89]
per-ex loss: 0.586132  [   29/   89]
per-ex loss: 0.476478  [   30/   89]
per-ex loss: 0.367159  [   31/   89]
per-ex loss: 0.364202  [   32/   89]
per-ex loss: 0.463823  [   33/   89]
per-ex loss: 0.456817  [   34/   89]
per-ex loss: 0.341235  [   35/   89]
per-ex loss: 0.592114  [   36/   89]
per-ex loss: 0.604219  [   37/   89]
per-ex loss: 0.548913  [   38/   89]
per-ex loss: 0.326232  [   39/   89]
per-ex loss: 0.334664  [   40/   89]
per-ex loss: 0.509541  [   41/   89]
per-ex loss: 0.466523  [   42/   89]
per-ex loss: 0.511925  [   43/   89]
per-ex loss: 0.505915  [   44/   89]
per-ex loss: 0.353582  [   45/   89]
per-ex loss: 0.452176  [   46/   89]
per-ex loss: 0.384794  [   47/   89]
per-ex loss: 0.436653  [   48/   89]
per-ex loss: 0.999884  [   49/   89]
per-ex loss: 0.302180  [   50/   89]
per-ex loss: 0.429568  [   51/   89]
per-ex loss: 0.715075  [   52/   89]
per-ex loss: 0.333308  [   53/   89]
per-ex loss: 0.325519  [   54/   89]
per-ex loss: 0.389350  [   55/   89]
per-ex loss: 0.362004  [   56/   89]
per-ex loss: 0.466834  [   57/   89]
per-ex loss: 0.373474  [   58/   89]
per-ex loss: 0.407186  [   59/   89]
per-ex loss: 0.455494  [   60/   89]
per-ex loss: 0.511487  [   61/   89]
per-ex loss: 0.452952  [   62/   89]
per-ex loss: 0.563968  [   63/   89]
per-ex loss: 0.309396  [   64/   89]
per-ex loss: 0.474791  [   65/   89]
per-ex loss: 0.597424  [   66/   89]
per-ex loss: 0.591604  [   67/   89]
per-ex loss: 0.537447  [   68/   89]
per-ex loss: 0.601005  [   69/   89]
per-ex loss: 0.533448  [   70/   89]
per-ex loss: 0.380215  [   71/   89]
per-ex loss: 0.498802  [   72/   89]
per-ex loss: 0.388595  [   73/   89]
per-ex loss: 0.396044  [   74/   89]
per-ex loss: 0.375867  [   75/   89]
per-ex loss: 0.408342  [   76/   89]
per-ex loss: 0.260274  [   77/   89]
per-ex loss: 0.311486  [   78/   89]
per-ex loss: 0.413632  [   79/   89]
per-ex loss: 0.653188  [   80/   89]
per-ex loss: 0.681497  [   81/   89]
per-ex loss: 0.341871  [   82/   89]
per-ex loss: 0.418879  [   83/   89]
per-ex loss: 0.378039  [   84/   89]
per-ex loss: 0.534568  [   85/   89]
per-ex loss: 0.532282  [   86/   89]
per-ex loss: 0.531754  [   87/   89]
per-ex loss: 0.414740  [   88/   89]
per-ex loss: 0.307957  [   89/   89]
Train Error: Avg loss: 0.46058543
validation Error: 
 Avg loss: 0.54345968 
 F1: 0.481287 
 Precision: 0.538729 
 Recall: 0.434913
 IoU: 0.316904

test Error: 
 Avg loss: 0.49559343 
 F1: 0.557712 
 Precision: 0.611950 
 Recall: 0.512306
 IoU: 0.386686

We have finished training iteration 63
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_61_.pth
per-ex loss: 0.700516  [    1/   89]
per-ex loss: 0.544139  [    2/   89]
per-ex loss: 0.597270  [    3/   89]
per-ex loss: 0.494254  [    4/   89]
per-ex loss: 0.399333  [    5/   89]
per-ex loss: 0.338700  [    6/   89]
per-ex loss: 0.458801  [    7/   89]
per-ex loss: 0.340891  [    8/   89]
per-ex loss: 0.404154  [    9/   89]
per-ex loss: 0.333806  [   10/   89]
per-ex loss: 0.282137  [   11/   89]
per-ex loss: 0.562194  [   12/   89]
per-ex loss: 0.326027  [   13/   89]
per-ex loss: 0.419210  [   14/   89]
per-ex loss: 0.448488  [   15/   89]
per-ex loss: 0.545482  [   16/   89]
per-ex loss: 0.347834  [   17/   89]
per-ex loss: 0.564944  [   18/   89]
per-ex loss: 0.523902  [   19/   89]
per-ex loss: 0.318220  [   20/   89]
per-ex loss: 0.999921  [   21/   89]
per-ex loss: 0.356145  [   22/   89]
per-ex loss: 0.367514  [   23/   89]
per-ex loss: 0.571034  [   24/   89]
per-ex loss: 0.387196  [   25/   89]
per-ex loss: 0.551532  [   26/   89]
per-ex loss: 0.439582  [   27/   89]
per-ex loss: 0.325698  [   28/   89]
per-ex loss: 0.533439  [   29/   89]
per-ex loss: 0.349060  [   30/   89]
per-ex loss: 0.443787  [   31/   89]
per-ex loss: 0.548745  [   32/   89]
per-ex loss: 0.381132  [   33/   89]
per-ex loss: 0.434057  [   34/   89]
per-ex loss: 0.332037  [   35/   89]
per-ex loss: 0.337754  [   36/   89]
per-ex loss: 0.374934  [   37/   89]
per-ex loss: 0.409180  [   38/   89]
per-ex loss: 0.369215  [   39/   89]
per-ex loss: 0.400388  [   40/   89]
per-ex loss: 0.477729  [   41/   89]
per-ex loss: 0.595587  [   42/   89]
per-ex loss: 0.350413  [   43/   89]
per-ex loss: 0.633999  [   44/   89]
per-ex loss: 0.499258  [   45/   89]
per-ex loss: 0.668233  [   46/   89]
per-ex loss: 0.765063  [   47/   89]
per-ex loss: 0.572722  [   48/   89]
per-ex loss: 0.406425  [   49/   89]
per-ex loss: 0.537329  [   50/   89]
per-ex loss: 0.384533  [   51/   89]
per-ex loss: 0.456339  [   52/   89]
per-ex loss: 0.357275  [   53/   89]
per-ex loss: 0.433570  [   54/   89]
per-ex loss: 0.376554  [   55/   89]
per-ex loss: 0.245816  [   56/   89]
per-ex loss: 0.374215  [   57/   89]
per-ex loss: 0.529929  [   58/   89]
per-ex loss: 0.379571  [   59/   89]
per-ex loss: 0.463982  [   60/   89]
per-ex loss: 0.351184  [   61/   89]
per-ex loss: 0.461015  [   62/   89]
per-ex loss: 0.368759  [   63/   89]
per-ex loss: 0.365807  [   64/   89]
per-ex loss: 0.362665  [   65/   89]
per-ex loss: 0.592705  [   66/   89]
per-ex loss: 0.380888  [   67/   89]
per-ex loss: 0.606220  [   68/   89]
per-ex loss: 0.581394  [   69/   89]
per-ex loss: 0.338286  [   70/   89]
per-ex loss: 0.498362  [   71/   89]
per-ex loss: 0.359173  [   72/   89]
per-ex loss: 0.553923  [   73/   89]
per-ex loss: 0.507938  [   74/   89]
per-ex loss: 0.632765  [   75/   89]
per-ex loss: 0.572967  [   76/   89]
per-ex loss: 0.407864  [   77/   89]
per-ex loss: 0.355851  [   78/   89]
per-ex loss: 0.515352  [   79/   89]
per-ex loss: 0.533162  [   80/   89]
per-ex loss: 0.603488  [   81/   89]
per-ex loss: 0.373113  [   82/   89]
per-ex loss: 0.396720  [   83/   89]
per-ex loss: 0.376418  [   84/   89]
per-ex loss: 0.319059  [   85/   89]
per-ex loss: 0.635958  [   86/   89]
per-ex loss: 0.653012  [   87/   89]
per-ex loss: 0.521845  [   88/   89]
per-ex loss: 0.280105  [   89/   89]
Train Error: Avg loss: 0.45929431
validation Error: 
 Avg loss: 0.53135902 
 F1: 0.501944 
 Precision: 0.572880 
 Recall: 0.446640
 IoU: 0.335064

test Error: 
 Avg loss: 0.48906826 
 F1: 0.558361 
 Precision: 0.631887 
 Recall: 0.500163
 IoU: 0.387310

We have finished training iteration 64
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_2_.pth
per-ex loss: 0.332175  [    1/   89]
per-ex loss: 0.572907  [    2/   89]
per-ex loss: 0.486764  [    3/   89]
per-ex loss: 0.356813  [    4/   89]
per-ex loss: 0.531058  [    5/   89]
per-ex loss: 0.336181  [    6/   89]
per-ex loss: 0.352473  [    7/   89]
per-ex loss: 0.466882  [    8/   89]
per-ex loss: 0.587468  [    9/   89]
per-ex loss: 0.511885  [   10/   89]
per-ex loss: 0.602635  [   11/   89]
per-ex loss: 0.643020  [   12/   89]
per-ex loss: 0.384729  [   13/   89]
per-ex loss: 0.385761  [   14/   89]
per-ex loss: 0.362334  [   15/   89]
per-ex loss: 0.618929  [   16/   89]
per-ex loss: 0.409780  [   17/   89]
per-ex loss: 0.328928  [   18/   89]
per-ex loss: 0.336297  [   19/   89]
per-ex loss: 0.393543  [   20/   89]
per-ex loss: 0.597790  [   21/   89]
per-ex loss: 0.343159  [   22/   89]
per-ex loss: 0.377958  [   23/   89]
per-ex loss: 0.391987  [   24/   89]
per-ex loss: 0.564235  [   25/   89]
per-ex loss: 0.596442  [   26/   89]
per-ex loss: 0.521841  [   27/   89]
per-ex loss: 0.365135  [   28/   89]
per-ex loss: 0.379075  [   29/   89]
per-ex loss: 0.414901  [   30/   89]
per-ex loss: 0.525885  [   31/   89]
per-ex loss: 0.387963  [   32/   89]
per-ex loss: 0.492716  [   33/   89]
per-ex loss: 0.325086  [   34/   89]
per-ex loss: 0.398292  [   35/   89]
per-ex loss: 0.452789  [   36/   89]
per-ex loss: 0.340917  [   37/   89]
per-ex loss: 0.476434  [   38/   89]
per-ex loss: 0.577573  [   39/   89]
per-ex loss: 0.560587  [   40/   89]
per-ex loss: 0.417246  [   41/   89]
per-ex loss: 0.596187  [   42/   89]
per-ex loss: 0.613871  [   43/   89]
per-ex loss: 0.592262  [   44/   89]
per-ex loss: 0.533908  [   45/   89]
per-ex loss: 0.488159  [   46/   89]
per-ex loss: 0.999879  [   47/   89]
per-ex loss: 0.304948  [   48/   89]
per-ex loss: 0.378213  [   49/   89]
per-ex loss: 0.577442  [   50/   89]
per-ex loss: 0.505105  [   51/   89]
per-ex loss: 0.328195  [   52/   89]
per-ex loss: 0.417482  [   53/   89]
per-ex loss: 0.501232  [   54/   89]
per-ex loss: 0.411746  [   55/   89]
per-ex loss: 0.366037  [   56/   89]
per-ex loss: 0.390225  [   57/   89]
per-ex loss: 0.617997  [   58/   89]
per-ex loss: 0.364363  [   59/   89]
per-ex loss: 0.645108  [   60/   89]
per-ex loss: 0.446703  [   61/   89]
per-ex loss: 0.353760  [   62/   89]
per-ex loss: 0.567784  [   63/   89]
per-ex loss: 0.356794  [   64/   89]
per-ex loss: 0.402328  [   65/   89]
per-ex loss: 0.233563  [   66/   89]
per-ex loss: 0.580577  [   67/   89]
per-ex loss: 0.431119  [   68/   89]
per-ex loss: 0.365361  [   69/   89]
per-ex loss: 0.492627  [   70/   89]
per-ex loss: 0.368089  [   71/   89]
per-ex loss: 0.296965  [   72/   89]
per-ex loss: 0.406338  [   73/   89]
per-ex loss: 0.552610  [   74/   89]
per-ex loss: 0.551689  [   75/   89]
per-ex loss: 0.652232  [   76/   89]
per-ex loss: 0.330930  [   77/   89]
per-ex loss: 0.376347  [   78/   89]
per-ex loss: 0.576549  [   79/   89]
per-ex loss: 0.540536  [   80/   89]
per-ex loss: 0.524010  [   81/   89]
per-ex loss: 0.527982  [   82/   89]
per-ex loss: 0.298522  [   83/   89]
per-ex loss: 0.360647  [   84/   89]
per-ex loss: 0.448751  [   85/   89]
per-ex loss: 0.510902  [   86/   89]
per-ex loss: 0.472558  [   87/   89]
per-ex loss: 0.657724  [   88/   89]
per-ex loss: 0.321810  [   89/   89]
Train Error: Avg loss: 0.46232282
validation Error: 
 Avg loss: 0.53227939 
 F1: 0.499550 
 Precision: 0.589948 
 Recall: 0.433174
 IoU: 0.332933

test Error: 
 Avg loss: 0.48808138 
 F1: 0.563227 
 Precision: 0.640201 
 Recall: 0.502777
 IoU: 0.392008

We have finished training iteration 65
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_1_.pth
per-ex loss: 0.366947  [    1/   89]
per-ex loss: 0.420288  [    2/   89]
per-ex loss: 0.315973  [    3/   89]
per-ex loss: 0.616832  [    4/   89]
per-ex loss: 0.544284  [    5/   89]
per-ex loss: 0.335848  [    6/   89]
per-ex loss: 0.333881  [    7/   89]
per-ex loss: 0.438777  [    8/   89]
per-ex loss: 0.652971  [    9/   89]
per-ex loss: 0.421216  [   10/   89]
per-ex loss: 0.377453  [   11/   89]
per-ex loss: 0.385286  [   12/   89]
per-ex loss: 0.472758  [   13/   89]
per-ex loss: 0.346032  [   14/   89]
per-ex loss: 0.558090  [   15/   89]
per-ex loss: 0.551325  [   16/   89]
per-ex loss: 0.347023  [   17/   89]
per-ex loss: 0.485917  [   18/   89]
per-ex loss: 0.362797  [   19/   89]
per-ex loss: 0.322788  [   20/   89]
per-ex loss: 0.350647  [   21/   89]
per-ex loss: 0.427154  [   22/   89]
per-ex loss: 0.331794  [   23/   89]
per-ex loss: 0.362331  [   24/   89]
per-ex loss: 0.548035  [   25/   89]
per-ex loss: 0.375858  [   26/   89]
per-ex loss: 0.402828  [   27/   89]
per-ex loss: 0.311687  [   28/   89]
per-ex loss: 0.482106  [   29/   89]
per-ex loss: 0.353851  [   30/   89]
per-ex loss: 0.353254  [   31/   89]
per-ex loss: 0.393285  [   32/   89]
per-ex loss: 0.341971  [   33/   89]
per-ex loss: 0.468871  [   34/   89]
per-ex loss: 0.401751  [   35/   89]
per-ex loss: 0.414527  [   36/   89]
per-ex loss: 0.478352  [   37/   89]
per-ex loss: 0.359930  [   38/   89]
per-ex loss: 0.327504  [   39/   89]
per-ex loss: 0.534403  [   40/   89]
per-ex loss: 0.595821  [   41/   89]
per-ex loss: 0.479257  [   42/   89]
per-ex loss: 0.690456  [   43/   89]
per-ex loss: 0.577963  [   44/   89]
per-ex loss: 0.301304  [   45/   89]
per-ex loss: 0.549259  [   46/   89]
per-ex loss: 0.566047  [   47/   89]
per-ex loss: 0.328505  [   48/   89]
per-ex loss: 0.512852  [   49/   89]
per-ex loss: 0.373482  [   50/   89]
per-ex loss: 0.380005  [   51/   89]
per-ex loss: 0.585806  [   52/   89]
per-ex loss: 0.356868  [   53/   89]
per-ex loss: 0.640560  [   54/   89]
per-ex loss: 0.561372  [   55/   89]
per-ex loss: 0.329413  [   56/   89]
per-ex loss: 0.531221  [   57/   89]
per-ex loss: 0.453118  [   58/   89]
per-ex loss: 0.615742  [   59/   89]
per-ex loss: 0.326228  [   60/   89]
per-ex loss: 0.512799  [   61/   89]
per-ex loss: 0.512175  [   62/   89]
per-ex loss: 0.368689  [   63/   89]
per-ex loss: 0.550337  [   64/   89]
per-ex loss: 0.363241  [   65/   89]
per-ex loss: 0.490617  [   66/   89]
per-ex loss: 0.404243  [   67/   89]
per-ex loss: 0.372607  [   68/   89]
per-ex loss: 0.365565  [   69/   89]
per-ex loss: 0.434952  [   70/   89]
per-ex loss: 0.642980  [   71/   89]
per-ex loss: 0.699058  [   72/   89]
per-ex loss: 0.618650  [   73/   89]
per-ex loss: 0.466989  [   74/   89]
per-ex loss: 0.404688  [   75/   89]
per-ex loss: 0.560078  [   76/   89]
per-ex loss: 0.413792  [   77/   89]
per-ex loss: 0.486834  [   78/   89]
per-ex loss: 0.445636  [   79/   89]
per-ex loss: 0.472736  [   80/   89]
per-ex loss: 0.379879  [   81/   89]
per-ex loss: 0.502274  [   82/   89]
per-ex loss: 0.547609  [   83/   89]
per-ex loss: 0.999920  [   84/   89]
per-ex loss: 0.574025  [   85/   89]
per-ex loss: 0.367475  [   86/   89]
per-ex loss: 0.250696  [   87/   89]
per-ex loss: 0.558756  [   88/   89]
per-ex loss: 0.338504  [   89/   89]
Train Error: Avg loss: 0.45547987
validation Error: 
 Avg loss: 0.57477093 
 F1: 0.458196 
 Precision: 0.434289 
 Recall: 0.484888
 IoU: 0.297181

test Error: 
 Avg loss: 0.53902446 
 F1: 0.510192 
 Precision: 0.450166 
 Recall: 0.588690
 IoU: 0.342455

We have finished training iteration 66
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_62_.pth
per-ex loss: 0.481061  [    1/   89]
per-ex loss: 0.490898  [    2/   89]
per-ex loss: 0.637597  [    3/   89]
per-ex loss: 0.382394  [    4/   89]
per-ex loss: 0.531251  [    5/   89]
per-ex loss: 0.551886  [    6/   89]
per-ex loss: 0.551995  [    7/   89]
per-ex loss: 0.422800  [    8/   89]
per-ex loss: 0.417150  [    9/   89]
per-ex loss: 0.524601  [   10/   89]
per-ex loss: 0.348682  [   11/   89]
per-ex loss: 0.426764  [   12/   89]
per-ex loss: 0.394053  [   13/   89]
per-ex loss: 0.652931  [   14/   89]
per-ex loss: 0.452633  [   15/   89]
per-ex loss: 0.427204  [   16/   89]
per-ex loss: 0.585166  [   17/   89]
per-ex loss: 0.446716  [   18/   89]
per-ex loss: 0.312688  [   19/   89]
per-ex loss: 0.483802  [   20/   89]
per-ex loss: 0.387053  [   21/   89]
per-ex loss: 0.999914  [   22/   89]
per-ex loss: 0.447632  [   23/   89]
per-ex loss: 0.318729  [   24/   89]
per-ex loss: 0.543725  [   25/   89]
per-ex loss: 0.391321  [   26/   89]
per-ex loss: 0.357848  [   27/   89]
per-ex loss: 0.452747  [   28/   89]
per-ex loss: 0.441259  [   29/   89]
per-ex loss: 0.641878  [   30/   89]
per-ex loss: 0.464286  [   31/   89]
per-ex loss: 0.320134  [   32/   89]
per-ex loss: 0.366098  [   33/   89]
per-ex loss: 0.504248  [   34/   89]
per-ex loss: 0.422522  [   35/   89]
per-ex loss: 0.422238  [   36/   89]
per-ex loss: 0.637686  [   37/   89]
per-ex loss: 0.550189  [   38/   89]
per-ex loss: 0.332987  [   39/   89]
per-ex loss: 0.371804  [   40/   89]
per-ex loss: 0.341116  [   41/   89]
per-ex loss: 0.351777  [   42/   89]
per-ex loss: 0.624349  [   43/   89]
per-ex loss: 0.541651  [   44/   89]
per-ex loss: 0.407756  [   45/   89]
per-ex loss: 0.386839  [   46/   89]
per-ex loss: 0.622029  [   47/   89]
per-ex loss: 0.574126  [   48/   89]
per-ex loss: 0.685718  [   49/   89]
per-ex loss: 0.569657  [   50/   89]
per-ex loss: 0.451023  [   51/   89]
per-ex loss: 0.344633  [   52/   89]
per-ex loss: 0.551864  [   53/   89]
per-ex loss: 0.356925  [   54/   89]
per-ex loss: 0.385561  [   55/   89]
per-ex loss: 0.374468  [   56/   89]
per-ex loss: 0.360820  [   57/   89]
per-ex loss: 0.425399  [   58/   89]
per-ex loss: 0.421839  [   59/   89]
per-ex loss: 0.571635  [   60/   89]
per-ex loss: 0.597888  [   61/   89]
per-ex loss: 0.554428  [   62/   89]
per-ex loss: 0.549578  [   63/   89]
per-ex loss: 0.349612  [   64/   89]
per-ex loss: 0.341759  [   65/   89]
per-ex loss: 0.344547  [   66/   89]
per-ex loss: 0.592322  [   67/   89]
per-ex loss: 0.560027  [   68/   89]
per-ex loss: 0.460864  [   69/   89]
per-ex loss: 0.522491  [   70/   89]
per-ex loss: 0.402190  [   71/   89]
per-ex loss: 0.516342  [   72/   89]
per-ex loss: 0.377684  [   73/   89]
per-ex loss: 0.403995  [   74/   89]
per-ex loss: 0.620435  [   75/   89]
per-ex loss: 0.316232  [   76/   89]
per-ex loss: 0.332373  [   77/   89]
per-ex loss: 0.589245  [   78/   89]
per-ex loss: 0.353706  [   79/   89]
per-ex loss: 0.403147  [   80/   89]
per-ex loss: 0.484397  [   81/   89]
per-ex loss: 0.344329  [   82/   89]
per-ex loss: 0.396607  [   83/   89]
per-ex loss: 0.419136  [   84/   89]
per-ex loss: 0.309574  [   85/   89]
per-ex loss: 0.327893  [   86/   89]
per-ex loss: 0.218305  [   87/   89]
per-ex loss: 0.518709  [   88/   89]
per-ex loss: 0.402663  [   89/   89]
Train Error: Avg loss: 0.45905879
validation Error: 
 Avg loss: 0.53746395 
 F1: 0.488356 
 Precision: 0.514303 
 Recall: 0.464901
 IoU: 0.323063

test Error: 
 Avg loss: 0.49069864 
 F1: 0.564130 
 Precision: 0.576477 
 Recall: 0.552301
 IoU: 0.392884

We have finished training iteration 67
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_63_.pth
per-ex loss: 0.428520  [    1/   89]
per-ex loss: 0.428087  [    2/   89]
per-ex loss: 0.621931  [    3/   89]
per-ex loss: 0.574627  [    4/   89]
per-ex loss: 0.455867  [    5/   89]
per-ex loss: 0.316304  [    6/   89]
per-ex loss: 0.543692  [    7/   89]
per-ex loss: 0.400678  [    8/   89]
per-ex loss: 0.314316  [    9/   89]
per-ex loss: 0.325954  [   10/   89]
per-ex loss: 0.362978  [   11/   89]
per-ex loss: 0.528116  [   12/   89]
per-ex loss: 0.512316  [   13/   89]
per-ex loss: 0.566982  [   14/   89]
per-ex loss: 0.356052  [   15/   89]
per-ex loss: 0.552277  [   16/   89]
per-ex loss: 0.381712  [   17/   89]
per-ex loss: 0.432917  [   18/   89]
per-ex loss: 0.403580  [   19/   89]
per-ex loss: 0.443928  [   20/   89]
per-ex loss: 0.616657  [   21/   89]
per-ex loss: 0.323957  [   22/   89]
per-ex loss: 0.999876  [   23/   89]
per-ex loss: 0.385113  [   24/   89]
per-ex loss: 0.350879  [   25/   89]
per-ex loss: 0.362516  [   26/   89]
per-ex loss: 0.366399  [   27/   89]
per-ex loss: 0.660412  [   28/   89]
per-ex loss: 0.411536  [   29/   89]
per-ex loss: 0.463223  [   30/   89]
per-ex loss: 0.288908  [   31/   89]
per-ex loss: 0.435416  [   32/   89]
per-ex loss: 0.571687  [   33/   89]
per-ex loss: 0.477481  [   34/   89]
per-ex loss: 0.513006  [   35/   89]
per-ex loss: 0.350666  [   36/   89]
per-ex loss: 0.355018  [   37/   89]
per-ex loss: 0.522094  [   38/   89]
per-ex loss: 0.557084  [   39/   89]
per-ex loss: 0.280785  [   40/   89]
per-ex loss: 0.345497  [   41/   89]
per-ex loss: 0.460035  [   42/   89]
per-ex loss: 0.450470  [   43/   89]
per-ex loss: 0.636882  [   44/   89]
per-ex loss: 0.569637  [   45/   89]
per-ex loss: 0.361987  [   46/   89]
per-ex loss: 0.358554  [   47/   89]
per-ex loss: 0.543098  [   48/   89]
per-ex loss: 0.441473  [   49/   89]
per-ex loss: 0.617877  [   50/   89]
per-ex loss: 0.573035  [   51/   89]
per-ex loss: 0.393500  [   52/   89]
per-ex loss: 0.530904  [   53/   89]
per-ex loss: 0.465200  [   54/   89]
per-ex loss: 0.509313  [   55/   89]
per-ex loss: 0.545646  [   56/   89]
per-ex loss: 0.560437  [   57/   89]
per-ex loss: 0.550847  [   58/   89]
per-ex loss: 0.400284  [   59/   89]
per-ex loss: 0.326442  [   60/   89]
per-ex loss: 0.542050  [   61/   89]
per-ex loss: 0.482096  [   62/   89]
per-ex loss: 0.325060  [   63/   89]
per-ex loss: 0.541806  [   64/   89]
per-ex loss: 0.389620  [   65/   89]
per-ex loss: 0.326392  [   66/   89]
per-ex loss: 0.434916  [   67/   89]
per-ex loss: 0.661118  [   68/   89]
per-ex loss: 0.612100  [   69/   89]
per-ex loss: 0.331295  [   70/   89]
per-ex loss: 0.564294  [   71/   89]
per-ex loss: 0.415712  [   72/   89]
per-ex loss: 0.344937  [   73/   89]
per-ex loss: 0.331762  [   74/   89]
per-ex loss: 0.528086  [   75/   89]
per-ex loss: 0.544593  [   76/   89]
per-ex loss: 0.449333  [   77/   89]
per-ex loss: 0.376616  [   78/   89]
per-ex loss: 0.403521  [   79/   89]
per-ex loss: 0.381327  [   80/   89]
per-ex loss: 0.346441  [   81/   89]
per-ex loss: 0.362698  [   82/   89]
per-ex loss: 0.435545  [   83/   89]
per-ex loss: 0.236007  [   84/   89]
per-ex loss: 0.624778  [   85/   89]
per-ex loss: 0.368638  [   86/   89]
per-ex loss: 0.364622  [   87/   89]
per-ex loss: 0.477814  [   88/   89]
per-ex loss: 0.400072  [   89/   89]
Train Error: Avg loss: 0.45492037
validation Error: 
 Avg loss: 0.54329171 
 F1: 0.488333 
 Precision: 0.537367 
 Recall: 0.447499
 IoU: 0.323042

test Error: 
 Avg loss: 0.49189413 
 F1: 0.559829 
 Precision: 0.615809 
 Recall: 0.513179
 IoU: 0.388724

We have finished training iteration 68
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_66_.pth
per-ex loss: 0.639439  [    1/   89]
per-ex loss: 0.561618  [    2/   89]
per-ex loss: 0.398938  [    3/   89]
per-ex loss: 0.320506  [    4/   89]
per-ex loss: 0.322877  [    5/   89]
per-ex loss: 0.503972  [    6/   89]
per-ex loss: 0.454001  [    7/   89]
per-ex loss: 0.485666  [    8/   89]
per-ex loss: 0.373713  [    9/   89]
per-ex loss: 0.336551  [   10/   89]
per-ex loss: 0.566110  [   11/   89]
per-ex loss: 0.524272  [   12/   89]
per-ex loss: 0.478271  [   13/   89]
per-ex loss: 0.449715  [   14/   89]
per-ex loss: 0.553955  [   15/   89]
per-ex loss: 0.516956  [   16/   89]
per-ex loss: 0.645868  [   17/   89]
per-ex loss: 0.522871  [   18/   89]
per-ex loss: 0.456580  [   19/   89]
per-ex loss: 0.555989  [   20/   89]
per-ex loss: 0.595981  [   21/   89]
per-ex loss: 0.472171  [   22/   89]
per-ex loss: 0.342403  [   23/   89]
per-ex loss: 0.361052  [   24/   89]
per-ex loss: 0.374740  [   25/   89]
per-ex loss: 0.342806  [   26/   89]
per-ex loss: 0.560487  [   27/   89]
per-ex loss: 0.411078  [   28/   89]
per-ex loss: 0.453349  [   29/   89]
per-ex loss: 0.368011  [   30/   89]
per-ex loss: 0.641102  [   31/   89]
per-ex loss: 0.378706  [   32/   89]
per-ex loss: 0.547946  [   33/   89]
per-ex loss: 0.393819  [   34/   89]
per-ex loss: 0.368328  [   35/   89]
per-ex loss: 0.421583  [   36/   89]
per-ex loss: 0.326408  [   37/   89]
per-ex loss: 0.629405  [   38/   89]
per-ex loss: 0.537396  [   39/   89]
per-ex loss: 0.383926  [   40/   89]
per-ex loss: 0.350542  [   41/   89]
per-ex loss: 0.310190  [   42/   89]
per-ex loss: 0.333263  [   43/   89]
per-ex loss: 0.324658  [   44/   89]
per-ex loss: 0.615222  [   45/   89]
per-ex loss: 0.536940  [   46/   89]
per-ex loss: 0.534857  [   47/   89]
per-ex loss: 0.382623  [   48/   89]
per-ex loss: 0.351678  [   49/   89]
per-ex loss: 0.360170  [   50/   89]
per-ex loss: 0.489166  [   51/   89]
per-ex loss: 0.382565  [   52/   89]
per-ex loss: 0.334856  [   53/   89]
per-ex loss: 0.459260  [   54/   89]
per-ex loss: 0.263108  [   55/   89]
per-ex loss: 0.351746  [   56/   89]
per-ex loss: 0.544619  [   57/   89]
per-ex loss: 0.393342  [   58/   89]
per-ex loss: 0.378687  [   59/   89]
per-ex loss: 0.291571  [   60/   89]
per-ex loss: 0.427113  [   61/   89]
per-ex loss: 0.365495  [   62/   89]
per-ex loss: 0.548095  [   63/   89]
per-ex loss: 0.336531  [   64/   89]
per-ex loss: 0.458591  [   65/   89]
per-ex loss: 0.510488  [   66/   89]
per-ex loss: 0.519248  [   67/   89]
per-ex loss: 0.336792  [   68/   89]
per-ex loss: 0.545676  [   69/   89]
per-ex loss: 0.346364  [   70/   89]
per-ex loss: 0.396923  [   71/   89]
per-ex loss: 0.648538  [   72/   89]
per-ex loss: 0.602220  [   73/   89]
per-ex loss: 0.550198  [   74/   89]
per-ex loss: 0.441481  [   75/   89]
per-ex loss: 0.468734  [   76/   89]
per-ex loss: 0.300191  [   77/   89]
per-ex loss: 0.388129  [   78/   89]
per-ex loss: 0.348937  [   79/   89]
per-ex loss: 0.999929  [   80/   89]
per-ex loss: 0.389678  [   81/   89]
per-ex loss: 0.469040  [   82/   89]
per-ex loss: 0.347483  [   83/   89]
per-ex loss: 0.251217  [   84/   89]
per-ex loss: 0.597495  [   85/   89]
per-ex loss: 0.565509  [   86/   89]
per-ex loss: 0.405364  [   87/   89]
per-ex loss: 0.558075  [   88/   89]
per-ex loss: 0.308939  [   89/   89]
Train Error: Avg loss: 0.44943932
validation Error: 
 Avg loss: 0.55705087 
 F1: 0.477526 
 Precision: 0.491578 
 Recall: 0.464255
 IoU: 0.313652

test Error: 
 Avg loss: 0.52972977 
 F1: 0.528453 
 Precision: 0.500166 
 Recall: 0.560131
 IoU: 0.359114

We have finished training iteration 69
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_67_.pth
per-ex loss: 0.371466  [    1/   89]
per-ex loss: 0.614902  [    2/   89]
per-ex loss: 0.364921  [    3/   89]
per-ex loss: 0.397330  [    4/   89]
per-ex loss: 0.553444  [    5/   89]
per-ex loss: 0.511266  [    6/   89]
per-ex loss: 0.324866  [    7/   89]
per-ex loss: 0.300762  [    8/   89]
per-ex loss: 0.627172  [    9/   89]
per-ex loss: 0.486838  [   10/   89]
per-ex loss: 0.377011  [   11/   89]
per-ex loss: 0.300742  [   12/   89]
per-ex loss: 0.599594  [   13/   89]
per-ex loss: 0.363850  [   14/   89]
per-ex loss: 0.337561  [   15/   89]
per-ex loss: 0.342980  [   16/   89]
per-ex loss: 0.398737  [   17/   89]
per-ex loss: 0.436204  [   18/   89]
per-ex loss: 0.313838  [   19/   89]
per-ex loss: 0.341531  [   20/   89]
per-ex loss: 0.334253  [   21/   89]
per-ex loss: 0.561180  [   22/   89]
per-ex loss: 0.409789  [   23/   89]
per-ex loss: 0.453683  [   24/   89]
per-ex loss: 0.345262  [   25/   89]
per-ex loss: 0.999915  [   26/   89]
per-ex loss: 0.553166  [   27/   89]
per-ex loss: 0.351603  [   28/   89]
per-ex loss: 0.591141  [   29/   89]
per-ex loss: 0.336451  [   30/   89]
per-ex loss: 0.525880  [   31/   89]
per-ex loss: 0.402674  [   32/   89]
per-ex loss: 0.426141  [   33/   89]
per-ex loss: 0.531033  [   34/   89]
per-ex loss: 0.550705  [   35/   89]
per-ex loss: 0.557274  [   36/   89]
per-ex loss: 0.563649  [   37/   89]
per-ex loss: 0.471479  [   38/   89]
per-ex loss: 0.562509  [   39/   89]
per-ex loss: 0.555080  [   40/   89]
per-ex loss: 0.520109  [   41/   89]
per-ex loss: 0.615288  [   42/   89]
per-ex loss: 0.558143  [   43/   89]
per-ex loss: 0.539027  [   44/   89]
per-ex loss: 0.445259  [   45/   89]
per-ex loss: 0.489952  [   46/   89]
per-ex loss: 0.445381  [   47/   89]
per-ex loss: 0.534680  [   48/   89]
per-ex loss: 0.370527  [   49/   89]
per-ex loss: 0.357817  [   50/   89]
per-ex loss: 0.675450  [   51/   89]
per-ex loss: 0.448310  [   52/   89]
per-ex loss: 0.384298  [   53/   89]
per-ex loss: 0.364825  [   54/   89]
per-ex loss: 0.403936  [   55/   89]
per-ex loss: 0.536770  [   56/   89]
per-ex loss: 0.358674  [   57/   89]
per-ex loss: 0.331272  [   58/   89]
per-ex loss: 0.649891  [   59/   89]
per-ex loss: 0.373148  [   60/   89]
per-ex loss: 0.387053  [   61/   89]
per-ex loss: 0.561991  [   62/   89]
per-ex loss: 0.446662  [   63/   89]
per-ex loss: 0.665442  [   64/   89]
per-ex loss: 0.311367  [   65/   89]
per-ex loss: 0.447131  [   66/   89]
per-ex loss: 0.322932  [   67/   89]
per-ex loss: 0.458491  [   68/   89]
per-ex loss: 0.558159  [   69/   89]
per-ex loss: 0.374586  [   70/   89]
per-ex loss: 0.382517  [   71/   89]
per-ex loss: 0.308163  [   72/   89]
per-ex loss: 0.428134  [   73/   89]
per-ex loss: 0.603653  [   74/   89]
per-ex loss: 0.333208  [   75/   89]
per-ex loss: 0.525870  [   76/   89]
per-ex loss: 0.555572  [   77/   89]
per-ex loss: 0.421142  [   78/   89]
per-ex loss: 0.230296  [   79/   89]
per-ex loss: 0.373939  [   80/   89]
per-ex loss: 0.506202  [   81/   89]
per-ex loss: 0.392332  [   82/   89]
per-ex loss: 0.409046  [   83/   89]
per-ex loss: 0.327520  [   84/   89]
per-ex loss: 0.324566  [   85/   89]
per-ex loss: 0.422654  [   86/   89]
per-ex loss: 0.573340  [   87/   89]
per-ex loss: 0.418921  [   88/   89]
per-ex loss: 0.407691  [   89/   89]
Train Error: Avg loss: 0.45313727
validation Error: 
 Avg loss: 0.56358585 
 F1: 0.462488 
 Precision: 0.547052 
 Recall: 0.400568
 IoU: 0.300803

test Error: 
 Avg loss: 0.50428546 
 F1: 0.544776 
 Precision: 0.652385 
 Recall: 0.467640
 IoU: 0.374359

We have finished training iteration 70
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_68_.pth
per-ex loss: 0.312732  [    1/   89]
per-ex loss: 0.532623  [    2/   89]
per-ex loss: 0.619413  [    3/   89]
per-ex loss: 0.351953  [    4/   89]
per-ex loss: 0.533766  [    5/   89]
per-ex loss: 0.423514  [    6/   89]
per-ex loss: 0.391489  [    7/   89]
per-ex loss: 0.599430  [    8/   89]
per-ex loss: 0.362361  [    9/   89]
per-ex loss: 0.537016  [   10/   89]
per-ex loss: 0.558071  [   11/   89]
per-ex loss: 0.522198  [   12/   89]
per-ex loss: 0.607648  [   13/   89]
per-ex loss: 0.503522  [   14/   89]
per-ex loss: 0.247437  [   15/   89]
per-ex loss: 0.376232  [   16/   89]
per-ex loss: 0.548317  [   17/   89]
per-ex loss: 0.524074  [   18/   89]
per-ex loss: 0.328045  [   19/   89]
per-ex loss: 0.495223  [   20/   89]
per-ex loss: 0.527344  [   21/   89]
per-ex loss: 0.305244  [   22/   89]
per-ex loss: 0.327092  [   23/   89]
per-ex loss: 0.366192  [   24/   89]
per-ex loss: 0.405977  [   25/   89]
per-ex loss: 0.500074  [   26/   89]
per-ex loss: 0.353251  [   27/   89]
per-ex loss: 0.327436  [   28/   89]
per-ex loss: 0.599793  [   29/   89]
per-ex loss: 0.353332  [   30/   89]
per-ex loss: 0.647264  [   31/   89]
per-ex loss: 0.682190  [   32/   89]
per-ex loss: 0.552692  [   33/   89]
per-ex loss: 0.570907  [   34/   89]
per-ex loss: 0.515962  [   35/   89]
per-ex loss: 0.401066  [   36/   89]
per-ex loss: 0.431404  [   37/   89]
per-ex loss: 0.502089  [   38/   89]
per-ex loss: 0.541970  [   39/   89]
per-ex loss: 0.383575  [   40/   89]
per-ex loss: 0.483944  [   41/   89]
per-ex loss: 0.438409  [   42/   89]
per-ex loss: 0.299353  [   43/   89]
per-ex loss: 0.403542  [   44/   89]
per-ex loss: 0.322049  [   45/   89]
per-ex loss: 0.395239  [   46/   89]
per-ex loss: 0.612079  [   47/   89]
per-ex loss: 0.577407  [   48/   89]
per-ex loss: 0.330380  [   49/   89]
per-ex loss: 0.344222  [   50/   89]
per-ex loss: 0.394548  [   51/   89]
per-ex loss: 0.564511  [   52/   89]
per-ex loss: 0.417685  [   53/   89]
per-ex loss: 0.413809  [   54/   89]
per-ex loss: 0.420448  [   55/   89]
per-ex loss: 0.403465  [   56/   89]
per-ex loss: 0.326488  [   57/   89]
per-ex loss: 0.392743  [   58/   89]
per-ex loss: 0.549596  [   59/   89]
per-ex loss: 0.535839  [   60/   89]
per-ex loss: 0.298457  [   61/   89]
per-ex loss: 0.337160  [   62/   89]
per-ex loss: 0.642207  [   63/   89]
per-ex loss: 0.445785  [   64/   89]
per-ex loss: 0.582965  [   65/   89]
per-ex loss: 0.465177  [   66/   89]
per-ex loss: 0.491765  [   67/   89]
per-ex loss: 0.546912  [   68/   89]
per-ex loss: 0.539526  [   69/   89]
per-ex loss: 0.390326  [   70/   89]
per-ex loss: 0.372838  [   71/   89]
per-ex loss: 0.632142  [   72/   89]
per-ex loss: 0.389068  [   73/   89]
per-ex loss: 0.387996  [   74/   89]
per-ex loss: 0.431818  [   75/   89]
per-ex loss: 0.999901  [   76/   89]
per-ex loss: 0.472725  [   77/   89]
per-ex loss: 0.308647  [   78/   89]
per-ex loss: 0.339878  [   79/   89]
per-ex loss: 0.320841  [   80/   89]
per-ex loss: 0.435787  [   81/   89]
per-ex loss: 0.358407  [   82/   89]
per-ex loss: 0.380921  [   83/   89]
per-ex loss: 0.400699  [   84/   89]
per-ex loss: 0.424877  [   85/   89]
per-ex loss: 0.328041  [   86/   89]
per-ex loss: 0.634972  [   87/   89]
per-ex loss: 0.346071  [   88/   89]
per-ex loss: 0.622955  [   89/   89]
Train Error: Avg loss: 0.45645548
validation Error: 
 Avg loss: 0.54714774 
 F1: 0.482093 
 Precision: 0.489151 
 Recall: 0.475235
 IoU: 0.317604

test Error: 
 Avg loss: 0.51165393 
 F1: 0.534703 
 Precision: 0.504313 
 Recall: 0.568991
 IoU: 0.364911

We have finished training iteration 71
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_69_.pth
per-ex loss: 0.474190  [    1/   89]
per-ex loss: 0.624285  [    2/   89]
per-ex loss: 0.671819  [    3/   89]
per-ex loss: 0.371596  [    4/   89]
per-ex loss: 0.448961  [    5/   89]
per-ex loss: 0.506570  [    6/   89]
per-ex loss: 0.433731  [    7/   89]
per-ex loss: 0.570516  [    8/   89]
per-ex loss: 0.425219  [    9/   89]
per-ex loss: 0.211403  [   10/   89]
per-ex loss: 0.551234  [   11/   89]
per-ex loss: 0.340545  [   12/   89]
per-ex loss: 0.650298  [   13/   89]
per-ex loss: 0.391356  [   14/   89]
per-ex loss: 0.515915  [   15/   89]
per-ex loss: 0.641339  [   16/   89]
per-ex loss: 0.606905  [   17/   89]
per-ex loss: 0.540400  [   18/   89]
per-ex loss: 0.474151  [   19/   89]
per-ex loss: 0.310463  [   20/   89]
per-ex loss: 0.538611  [   21/   89]
per-ex loss: 0.459665  [   22/   89]
per-ex loss: 0.455983  [   23/   89]
per-ex loss: 0.370260  [   24/   89]
per-ex loss: 0.313082  [   25/   89]
per-ex loss: 0.436383  [   26/   89]
per-ex loss: 0.440047  [   27/   89]
per-ex loss: 0.526502  [   28/   89]
per-ex loss: 0.438848  [   29/   89]
per-ex loss: 0.294645  [   30/   89]
per-ex loss: 0.416981  [   31/   89]
per-ex loss: 0.324401  [   32/   89]
per-ex loss: 0.350318  [   33/   89]
per-ex loss: 0.413017  [   34/   89]
per-ex loss: 0.333832  [   35/   89]
per-ex loss: 0.351168  [   36/   89]
per-ex loss: 0.366628  [   37/   89]
per-ex loss: 0.561596  [   38/   89]
per-ex loss: 0.661894  [   39/   89]
per-ex loss: 0.491818  [   40/   89]
per-ex loss: 0.354760  [   41/   89]
per-ex loss: 0.665704  [   42/   89]
per-ex loss: 0.653984  [   43/   89]
per-ex loss: 0.380862  [   44/   89]
per-ex loss: 0.363346  [   45/   89]
per-ex loss: 0.320678  [   46/   89]
per-ex loss: 0.396863  [   47/   89]
per-ex loss: 0.400787  [   48/   89]
per-ex loss: 0.402073  [   49/   89]
per-ex loss: 0.458590  [   50/   89]
per-ex loss: 0.469221  [   51/   89]
per-ex loss: 0.559114  [   52/   89]
per-ex loss: 0.999930  [   53/   89]
per-ex loss: 0.409952  [   54/   89]
per-ex loss: 0.442224  [   55/   89]
per-ex loss: 0.319187  [   56/   89]
per-ex loss: 0.371941  [   57/   89]
per-ex loss: 0.343229  [   58/   89]
per-ex loss: 0.350755  [   59/   89]
per-ex loss: 0.402967  [   60/   89]
per-ex loss: 0.442501  [   61/   89]
per-ex loss: 0.606929  [   62/   89]
per-ex loss: 0.339464  [   63/   89]
per-ex loss: 0.701641  [   64/   89]
per-ex loss: 0.356144  [   65/   89]
per-ex loss: 0.321786  [   66/   89]
per-ex loss: 0.378598  [   67/   89]
per-ex loss: 0.346027  [   68/   89]
per-ex loss: 0.417645  [   69/   89]
per-ex loss: 0.605093  [   70/   89]
per-ex loss: 0.333407  [   71/   89]
per-ex loss: 0.534995  [   72/   89]
per-ex loss: 0.378976  [   73/   89]
per-ex loss: 0.324310  [   74/   89]
per-ex loss: 0.337999  [   75/   89]
per-ex loss: 0.572716  [   76/   89]
per-ex loss: 0.390164  [   77/   89]
per-ex loss: 0.363998  [   78/   89]
per-ex loss: 0.376771  [   79/   89]
per-ex loss: 0.482870  [   80/   89]
per-ex loss: 0.617130  [   81/   89]
per-ex loss: 0.439152  [   82/   89]
per-ex loss: 0.518885  [   83/   89]
per-ex loss: 0.524305  [   84/   89]
per-ex loss: 0.322298  [   85/   89]
per-ex loss: 0.604861  [   86/   89]
per-ex loss: 0.573952  [   87/   89]
per-ex loss: 0.387644  [   88/   89]
per-ex loss: 0.489993  [   89/   89]
Train Error: Avg loss: 0.45464046
validation Error: 
 Avg loss: 0.56231179 
 F1: 0.462240 
 Precision: 0.400360 
 Recall: 0.546747
 IoU: 0.300593

test Error: 
 Avg loss: 0.50796113 
 F1: 0.535190 
 Precision: 0.455517 
 Recall: 0.648643
 IoU: 0.365365

We have finished training iteration 72
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_70_.pth
per-ex loss: 0.393945  [    1/   89]
per-ex loss: 0.365236  [    2/   89]
per-ex loss: 0.567606  [    3/   89]
per-ex loss: 0.338953  [    4/   89]
per-ex loss: 0.339809  [    5/   89]
per-ex loss: 0.378961  [    6/   89]
per-ex loss: 0.366953  [    7/   89]
per-ex loss: 0.544748  [    8/   89]
per-ex loss: 0.473767  [    9/   89]
per-ex loss: 0.542677  [   10/   89]
per-ex loss: 0.405685  [   11/   89]
per-ex loss: 0.429624  [   12/   89]
per-ex loss: 0.409214  [   13/   89]
per-ex loss: 0.312835  [   14/   89]
per-ex loss: 0.391585  [   15/   89]
per-ex loss: 0.650771  [   16/   89]
per-ex loss: 0.543737  [   17/   89]
per-ex loss: 0.518200  [   18/   89]
per-ex loss: 0.358921  [   19/   89]
per-ex loss: 0.303746  [   20/   89]
per-ex loss: 0.359314  [   21/   89]
per-ex loss: 0.644471  [   22/   89]
per-ex loss: 0.489033  [   23/   89]
per-ex loss: 0.504130  [   24/   89]
per-ex loss: 0.402042  [   25/   89]
per-ex loss: 0.378807  [   26/   89]
per-ex loss: 0.340163  [   27/   89]
per-ex loss: 0.360345  [   28/   89]
per-ex loss: 0.267573  [   29/   89]
per-ex loss: 0.566916  [   30/   89]
per-ex loss: 0.397567  [   31/   89]
per-ex loss: 0.659235  [   32/   89]
per-ex loss: 0.336908  [   33/   89]
per-ex loss: 0.382089  [   34/   89]
per-ex loss: 0.548462  [   35/   89]
per-ex loss: 0.323412  [   36/   89]
per-ex loss: 0.521396  [   37/   89]
per-ex loss: 0.393214  [   38/   89]
per-ex loss: 0.580236  [   39/   89]
per-ex loss: 0.349423  [   40/   89]
per-ex loss: 0.531554  [   41/   89]
per-ex loss: 0.536247  [   42/   89]
per-ex loss: 0.511135  [   43/   89]
per-ex loss: 0.626197  [   44/   89]
per-ex loss: 0.599037  [   45/   89]
per-ex loss: 0.510944  [   46/   89]
per-ex loss: 0.999879  [   47/   89]
per-ex loss: 0.443763  [   48/   89]
per-ex loss: 0.511777  [   49/   89]
per-ex loss: 0.518816  [   50/   89]
per-ex loss: 0.501812  [   51/   89]
per-ex loss: 0.425371  [   52/   89]
per-ex loss: 0.580895  [   53/   89]
per-ex loss: 0.360639  [   54/   89]
per-ex loss: 0.404405  [   55/   89]
per-ex loss: 0.495033  [   56/   89]
per-ex loss: 0.312701  [   57/   89]
per-ex loss: 0.347494  [   58/   89]
per-ex loss: 0.389493  [   59/   89]
per-ex loss: 0.421067  [   60/   89]
per-ex loss: 0.369142  [   61/   89]
per-ex loss: 0.597084  [   62/   89]
per-ex loss: 0.339690  [   63/   89]
per-ex loss: 0.618945  [   64/   89]
per-ex loss: 0.513535  [   65/   89]
per-ex loss: 0.373739  [   66/   89]
per-ex loss: 0.442520  [   67/   89]
per-ex loss: 0.519680  [   68/   89]
per-ex loss: 0.432883  [   69/   89]
per-ex loss: 0.292331  [   70/   89]
per-ex loss: 0.572004  [   71/   89]
per-ex loss: 0.606960  [   72/   89]
per-ex loss: 0.309450  [   73/   89]
per-ex loss: 0.394918  [   74/   89]
per-ex loss: 0.530515  [   75/   89]
per-ex loss: 0.363254  [   76/   89]
per-ex loss: 0.447681  [   77/   89]
per-ex loss: 0.415630  [   78/   89]
per-ex loss: 0.387239  [   79/   89]
per-ex loss: 0.421307  [   80/   89]
per-ex loss: 0.366208  [   81/   89]
per-ex loss: 0.334695  [   82/   89]
per-ex loss: 0.525472  [   83/   89]
per-ex loss: 0.467819  [   84/   89]
per-ex loss: 0.370206  [   85/   89]
per-ex loss: 0.624808  [   86/   89]
per-ex loss: 0.454893  [   87/   89]
per-ex loss: 0.343275  [   88/   89]
per-ex loss: 0.442281  [   89/   89]
Train Error: Avg loss: 0.45334978
validation Error: 
 Avg loss: 0.53434142 
 F1: 0.498191 
 Precision: 0.637193 
 Recall: 0.408975
 IoU: 0.331727

test Error: 
 Avg loss: 0.49772715 
 F1: 0.551531 
 Precision: 0.655506 
 Recall: 0.476025
 IoU: 0.380768

We have finished training iteration 73
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_71_.pth
per-ex loss: 0.498248  [    1/   89]
per-ex loss: 0.333463  [    2/   89]
per-ex loss: 0.344488  [    3/   89]
per-ex loss: 0.413378  [    4/   89]
per-ex loss: 0.338227  [    5/   89]
per-ex loss: 0.580992  [    6/   89]
per-ex loss: 0.419287  [    7/   89]
per-ex loss: 0.386557  [    8/   89]
per-ex loss: 0.513872  [    9/   89]
per-ex loss: 0.381686  [   10/   89]
per-ex loss: 0.491671  [   11/   89]
per-ex loss: 0.624800  [   12/   89]
per-ex loss: 0.539874  [   13/   89]
per-ex loss: 0.575380  [   14/   89]
per-ex loss: 0.367330  [   15/   89]
per-ex loss: 0.651067  [   16/   89]
per-ex loss: 0.405207  [   17/   89]
per-ex loss: 0.332636  [   18/   89]
per-ex loss: 0.362935  [   19/   89]
per-ex loss: 0.413200  [   20/   89]
per-ex loss: 0.530553  [   21/   89]
per-ex loss: 0.383183  [   22/   89]
per-ex loss: 0.523751  [   23/   89]
per-ex loss: 0.381837  [   24/   89]
per-ex loss: 0.229716  [   25/   89]
per-ex loss: 0.446017  [   26/   89]
per-ex loss: 0.294185  [   27/   89]
per-ex loss: 0.547985  [   28/   89]
per-ex loss: 0.354106  [   29/   89]
per-ex loss: 0.348258  [   30/   89]
per-ex loss: 0.292433  [   31/   89]
per-ex loss: 0.268468  [   32/   89]
per-ex loss: 0.340948  [   33/   89]
per-ex loss: 0.366268  [   34/   89]
per-ex loss: 0.383690  [   35/   89]
per-ex loss: 0.547989  [   36/   89]
per-ex loss: 0.354881  [   37/   89]
per-ex loss: 0.478690  [   38/   89]
per-ex loss: 0.388547  [   39/   89]
per-ex loss: 0.629556  [   40/   89]
per-ex loss: 0.436309  [   41/   89]
per-ex loss: 0.549233  [   42/   89]
per-ex loss: 0.314074  [   43/   89]
per-ex loss: 0.334388  [   44/   89]
per-ex loss: 0.378876  [   45/   89]
per-ex loss: 0.471715  [   46/   89]
per-ex loss: 0.335101  [   47/   89]
per-ex loss: 0.584444  [   48/   89]
per-ex loss: 0.607374  [   49/   89]
per-ex loss: 0.316014  [   50/   89]
per-ex loss: 0.419315  [   51/   89]
per-ex loss: 0.567750  [   52/   89]
per-ex loss: 0.435041  [   53/   89]
per-ex loss: 0.385785  [   54/   89]
per-ex loss: 0.999879  [   55/   89]
per-ex loss: 0.554914  [   56/   89]
per-ex loss: 0.456106  [   57/   89]
per-ex loss: 0.332664  [   58/   89]
per-ex loss: 0.372615  [   59/   89]
per-ex loss: 0.399519  [   60/   89]
per-ex loss: 0.269525  [   61/   89]
per-ex loss: 0.364222  [   62/   89]
per-ex loss: 0.512993  [   63/   89]
per-ex loss: 0.448548  [   64/   89]
per-ex loss: 0.602946  [   65/   89]
per-ex loss: 0.315800  [   66/   89]
per-ex loss: 0.376283  [   67/   89]
per-ex loss: 0.455008  [   68/   89]
per-ex loss: 0.569884  [   69/   89]
per-ex loss: 0.383180  [   70/   89]
per-ex loss: 0.561202  [   71/   89]
per-ex loss: 0.613310  [   72/   89]
per-ex loss: 0.398244  [   73/   89]
per-ex loss: 0.445068  [   74/   89]
per-ex loss: 0.512341  [   75/   89]
per-ex loss: 0.294982  [   76/   89]
per-ex loss: 0.516414  [   77/   89]
per-ex loss: 0.530147  [   78/   89]
per-ex loss: 0.570302  [   79/   89]
per-ex loss: 0.486270  [   80/   89]
per-ex loss: 0.343793  [   81/   89]
per-ex loss: 0.416107  [   82/   89]
per-ex loss: 0.382875  [   83/   89]
per-ex loss: 0.572667  [   84/   89]
per-ex loss: 0.567191  [   85/   89]
per-ex loss: 0.522714  [   86/   89]
per-ex loss: 0.358706  [   87/   89]
per-ex loss: 0.529437  [   88/   89]
per-ex loss: 0.339430  [   89/   89]
Train Error: Avg loss: 0.44467487
validation Error: 
 Avg loss: 0.53649034 
 F1: 0.487467 
 Precision: 0.500006 
 Recall: 0.475542
 IoU: 0.322286

test Error: 
 Avg loss: 0.49117415 
 F1: 0.557242 
 Precision: 0.545749 
 Recall: 0.569230
 IoU: 0.386234

We have finished training iteration 74
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_72_.pth
per-ex loss: 0.502138  [    1/   89]
per-ex loss: 0.558664  [    2/   89]
per-ex loss: 0.329815  [    3/   89]
per-ex loss: 0.385555  [    4/   89]
per-ex loss: 0.999958  [    5/   89]
per-ex loss: 0.393850  [    6/   89]
per-ex loss: 0.617398  [    7/   89]
per-ex loss: 0.476033  [    8/   89]
per-ex loss: 0.568820  [    9/   89]
per-ex loss: 0.291895  [   10/   89]
per-ex loss: 0.519071  [   11/   89]
per-ex loss: 0.550063  [   12/   89]
per-ex loss: 0.457970  [   13/   89]
per-ex loss: 0.623201  [   14/   89]
per-ex loss: 0.458559  [   15/   89]
per-ex loss: 0.425947  [   16/   89]
per-ex loss: 0.345727  [   17/   89]
per-ex loss: 0.401024  [   18/   89]
per-ex loss: 0.652173  [   19/   89]
per-ex loss: 0.390579  [   20/   89]
per-ex loss: 0.311014  [   21/   89]
per-ex loss: 0.318217  [   22/   89]
per-ex loss: 0.210659  [   23/   89]
per-ex loss: 0.355031  [   24/   89]
per-ex loss: 0.373083  [   25/   89]
per-ex loss: 0.332588  [   26/   89]
per-ex loss: 0.391337  [   27/   89]
per-ex loss: 0.376923  [   28/   89]
per-ex loss: 0.466532  [   29/   89]
per-ex loss: 0.326279  [   30/   89]
per-ex loss: 0.392800  [   31/   89]
per-ex loss: 0.610946  [   32/   89]
per-ex loss: 0.539200  [   33/   89]
per-ex loss: 0.405868  [   34/   89]
per-ex loss: 0.364206  [   35/   89]
per-ex loss: 0.539904  [   36/   89]
per-ex loss: 0.438970  [   37/   89]
per-ex loss: 0.428376  [   38/   89]
per-ex loss: 0.333795  [   39/   89]
per-ex loss: 0.632508  [   40/   89]
per-ex loss: 0.324732  [   41/   89]
per-ex loss: 0.401163  [   42/   89]
per-ex loss: 0.411537  [   43/   89]
per-ex loss: 0.354151  [   44/   89]
per-ex loss: 0.587467  [   45/   89]
per-ex loss: 0.570040  [   46/   89]
per-ex loss: 0.529985  [   47/   89]
per-ex loss: 0.524029  [   48/   89]
per-ex loss: 0.555124  [   49/   89]
per-ex loss: 0.525330  [   50/   89]
per-ex loss: 0.368542  [   51/   89]
per-ex loss: 0.564206  [   52/   89]
per-ex loss: 0.397046  [   53/   89]
per-ex loss: 0.499725  [   54/   89]
per-ex loss: 0.388702  [   55/   89]
per-ex loss: 0.462118  [   56/   89]
per-ex loss: 0.545328  [   57/   89]
per-ex loss: 0.330306  [   58/   89]
per-ex loss: 0.608539  [   59/   89]
per-ex loss: 0.291301  [   60/   89]
per-ex loss: 0.488915  [   61/   89]
per-ex loss: 0.413960  [   62/   89]
per-ex loss: 0.317717  [   63/   89]
per-ex loss: 0.367626  [   64/   89]
per-ex loss: 0.342015  [   65/   89]
per-ex loss: 0.441569  [   66/   89]
per-ex loss: 0.649038  [   67/   89]
per-ex loss: 0.440729  [   68/   89]
per-ex loss: 0.506285  [   69/   89]
per-ex loss: 0.424798  [   70/   89]
per-ex loss: 0.253591  [   71/   89]
per-ex loss: 0.397459  [   72/   89]
per-ex loss: 0.470989  [   73/   89]
per-ex loss: 0.358697  [   74/   89]
per-ex loss: 0.343783  [   75/   89]
per-ex loss: 0.406962  [   76/   89]
per-ex loss: 0.543324  [   77/   89]
per-ex loss: 0.562545  [   78/   89]
per-ex loss: 0.300075  [   79/   89]
per-ex loss: 0.341663  [   80/   89]
per-ex loss: 0.377948  [   81/   89]
per-ex loss: 0.576127  [   82/   89]
per-ex loss: 0.563183  [   83/   89]
per-ex loss: 0.398136  [   84/   89]
per-ex loss: 0.556776  [   85/   89]
per-ex loss: 0.369051  [   86/   89]
per-ex loss: 0.540069  [   87/   89]
per-ex loss: 0.561953  [   88/   89]
per-ex loss: 0.296726  [   89/   89]
Train Error: Avg loss: 0.44885119
validation Error: 
 Avg loss: 0.53099504 
 F1: 0.493762 
 Precision: 0.544159 
 Recall: 0.451908
 IoU: 0.327811

test Error: 
 Avg loss: 0.47849874 
 F1: 0.573393 
 Precision: 0.615364 
 Recall: 0.536783
 IoU: 0.401928

We have finished training iteration 75
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_73_.pth
per-ex loss: 0.412026  [    1/   89]
per-ex loss: 0.393561  [    2/   89]
per-ex loss: 0.384290  [    3/   89]
per-ex loss: 0.534332  [    4/   89]
per-ex loss: 0.356052  [    5/   89]
per-ex loss: 0.413300  [    6/   89]
per-ex loss: 0.520618  [    7/   89]
per-ex loss: 0.349820  [    8/   89]
per-ex loss: 0.433536  [    9/   89]
per-ex loss: 0.324453  [   10/   89]
per-ex loss: 0.435133  [   11/   89]
per-ex loss: 0.491559  [   12/   89]
per-ex loss: 0.320826  [   13/   89]
per-ex loss: 0.696235  [   14/   89]
per-ex loss: 0.527436  [   15/   89]
per-ex loss: 0.293155  [   16/   89]
per-ex loss: 0.614109  [   17/   89]
per-ex loss: 0.360405  [   18/   89]
per-ex loss: 0.372661  [   19/   89]
per-ex loss: 0.304087  [   20/   89]
per-ex loss: 0.457488  [   21/   89]
per-ex loss: 0.317575  [   22/   89]
per-ex loss: 0.396216  [   23/   89]
per-ex loss: 0.438397  [   24/   89]
per-ex loss: 0.331831  [   25/   89]
per-ex loss: 0.551247  [   26/   89]
per-ex loss: 0.333176  [   27/   89]
per-ex loss: 0.469279  [   28/   89]
per-ex loss: 0.603048  [   29/   89]
per-ex loss: 0.549115  [   30/   89]
per-ex loss: 0.503093  [   31/   89]
per-ex loss: 0.386920  [   32/   89]
per-ex loss: 0.407617  [   33/   89]
per-ex loss: 0.626109  [   34/   89]
per-ex loss: 0.462710  [   35/   89]
per-ex loss: 0.323982  [   36/   89]
per-ex loss: 0.334222  [   37/   89]
per-ex loss: 0.342344  [   38/   89]
per-ex loss: 0.723924  [   39/   89]
per-ex loss: 0.343522  [   40/   89]
per-ex loss: 0.447668  [   41/   89]
per-ex loss: 0.389286  [   42/   89]
per-ex loss: 0.600595  [   43/   89]
per-ex loss: 0.327239  [   44/   89]
per-ex loss: 0.289989  [   45/   89]
per-ex loss: 0.534968  [   46/   89]
per-ex loss: 0.327799  [   47/   89]
per-ex loss: 0.533998  [   48/   89]
per-ex loss: 0.592177  [   49/   89]
per-ex loss: 0.394366  [   50/   89]
per-ex loss: 0.345291  [   51/   89]
per-ex loss: 0.392849  [   52/   89]
per-ex loss: 0.355470  [   53/   89]
per-ex loss: 0.468857  [   54/   89]
per-ex loss: 0.455414  [   55/   89]
per-ex loss: 0.558873  [   56/   89]
per-ex loss: 0.260384  [   57/   89]
per-ex loss: 0.592953  [   58/   89]
per-ex loss: 0.411807  [   59/   89]
per-ex loss: 0.364974  [   60/   89]
per-ex loss: 0.387284  [   61/   89]
per-ex loss: 0.388951  [   62/   89]
per-ex loss: 0.503276  [   63/   89]
per-ex loss: 0.391014  [   64/   89]
per-ex loss: 0.453254  [   65/   89]
per-ex loss: 0.595223  [   66/   89]
per-ex loss: 0.378920  [   67/   89]
per-ex loss: 0.540885  [   68/   89]
per-ex loss: 0.365777  [   69/   89]
per-ex loss: 0.773030  [   70/   89]
per-ex loss: 0.488601  [   71/   89]
per-ex loss: 0.358257  [   72/   89]
per-ex loss: 0.348308  [   73/   89]
per-ex loss: 0.663100  [   74/   89]
per-ex loss: 0.524074  [   75/   89]
per-ex loss: 0.493336  [   76/   89]
per-ex loss: 0.517797  [   77/   89]
per-ex loss: 0.577117  [   78/   89]
per-ex loss: 0.464806  [   79/   89]
per-ex loss: 0.545316  [   80/   89]
per-ex loss: 0.323066  [   81/   89]
per-ex loss: 0.360064  [   82/   89]
per-ex loss: 0.293201  [   83/   89]
per-ex loss: 0.999944  [   84/   89]
per-ex loss: 0.397326  [   85/   89]
per-ex loss: 0.357736  [   86/   89]
per-ex loss: 0.551550  [   87/   89]
per-ex loss: 0.353967  [   88/   89]
per-ex loss: 0.539004  [   89/   89]
Train Error: Avg loss: 0.44964662
validation Error: 
 Avg loss: 0.58183189 
 F1: 0.454044 
 Precision: 0.485520 
 Recall: 0.426400
 IoU: 0.293697

test Error: 
 Avg loss: 0.54059432 
 F1: 0.516182 
 Precision: 0.493226 
 Recall: 0.541380
 IoU: 0.347874

We have finished training iteration 76
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_74_.pth
per-ex loss: 0.999904  [    1/   89]
per-ex loss: 0.504852  [    2/   89]
per-ex loss: 0.417835  [    3/   89]
per-ex loss: 0.438057  [    4/   89]
per-ex loss: 0.539656  [    5/   89]
per-ex loss: 0.226828  [    6/   89]
per-ex loss: 0.326582  [    7/   89]
per-ex loss: 0.395450  [    8/   89]
per-ex loss: 0.393090  [    9/   89]
per-ex loss: 0.487972  [   10/   89]
per-ex loss: 0.308014  [   11/   89]
per-ex loss: 0.392343  [   12/   89]
per-ex loss: 0.415060  [   13/   89]
per-ex loss: 0.382036  [   14/   89]
per-ex loss: 0.333669  [   15/   89]
per-ex loss: 0.321246  [   16/   89]
per-ex loss: 0.359815  [   17/   89]
per-ex loss: 0.373329  [   18/   89]
per-ex loss: 0.719421  [   19/   89]
per-ex loss: 0.378485  [   20/   89]
per-ex loss: 0.510602  [   21/   89]
per-ex loss: 0.332971  [   22/   89]
per-ex loss: 0.473993  [   23/   89]
per-ex loss: 0.460036  [   24/   89]
per-ex loss: 0.657445  [   25/   89]
per-ex loss: 0.538354  [   26/   89]
per-ex loss: 0.347772  [   27/   89]
per-ex loss: 0.336437  [   28/   89]
per-ex loss: 0.516954  [   29/   89]
per-ex loss: 0.599779  [   30/   89]
per-ex loss: 0.446028  [   31/   89]
per-ex loss: 0.331677  [   32/   89]
per-ex loss: 0.302511  [   33/   89]
per-ex loss: 0.330576  [   34/   89]
per-ex loss: 0.474825  [   35/   89]
per-ex loss: 0.446539  [   36/   89]
per-ex loss: 0.348361  [   37/   89]
per-ex loss: 0.351280  [   38/   89]
per-ex loss: 0.346098  [   39/   89]
per-ex loss: 0.524212  [   40/   89]
per-ex loss: 0.497444  [   41/   89]
per-ex loss: 0.302476  [   42/   89]
per-ex loss: 0.349134  [   43/   89]
per-ex loss: 0.283061  [   44/   89]
per-ex loss: 0.584515  [   45/   89]
per-ex loss: 0.624516  [   46/   89]
per-ex loss: 0.375325  [   47/   89]
per-ex loss: 0.547458  [   48/   89]
per-ex loss: 0.533203  [   49/   89]
per-ex loss: 0.331012  [   50/   89]
per-ex loss: 0.542156  [   51/   89]
per-ex loss: 0.431980  [   52/   89]
per-ex loss: 0.427865  [   53/   89]
per-ex loss: 0.351671  [   54/   89]
per-ex loss: 0.452095  [   55/   89]
per-ex loss: 0.373605  [   56/   89]
per-ex loss: 0.385456  [   57/   89]
per-ex loss: 0.644709  [   58/   89]
per-ex loss: 0.260040  [   59/   89]
per-ex loss: 0.536606  [   60/   89]
per-ex loss: 0.287482  [   61/   89]
per-ex loss: 0.365670  [   62/   89]
per-ex loss: 0.556731  [   63/   89]
per-ex loss: 0.546306  [   64/   89]
per-ex loss: 0.429766  [   65/   89]
per-ex loss: 0.533681  [   66/   89]
per-ex loss: 0.512368  [   67/   89]
per-ex loss: 0.387745  [   68/   89]
per-ex loss: 0.623495  [   69/   89]
per-ex loss: 0.519884  [   70/   89]
per-ex loss: 0.446493  [   71/   89]
per-ex loss: 0.578338  [   72/   89]
per-ex loss: 0.535504  [   73/   89]
per-ex loss: 0.478420  [   74/   89]
per-ex loss: 0.470896  [   75/   89]
per-ex loss: 0.491863  [   76/   89]
per-ex loss: 0.388235  [   77/   89]
per-ex loss: 0.493960  [   78/   89]
per-ex loss: 0.381101  [   79/   89]
per-ex loss: 0.310750  [   80/   89]
per-ex loss: 0.363236  [   81/   89]
per-ex loss: 0.335586  [   82/   89]
per-ex loss: 0.564483  [   83/   89]
per-ex loss: 0.419951  [   84/   89]
per-ex loss: 0.399484  [   85/   89]
per-ex loss: 0.315730  [   86/   89]
per-ex loss: 0.339179  [   87/   89]
per-ex loss: 0.567799  [   88/   89]
per-ex loss: 0.619040  [   89/   89]
Train Error: Avg loss: 0.44368084
validation Error: 
 Avg loss: 0.53010454 
 F1: 0.497350 
 Precision: 0.579150 
 Recall: 0.435798
 IoU: 0.330982

test Error: 
 Avg loss: 0.48096877 
 F1: 0.571988 
 Precision: 0.633136 
 Recall: 0.521612
 IoU: 0.400549

We have finished training iteration 77
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_65_.pth
per-ex loss: 0.382540  [    1/   89]
per-ex loss: 0.341089  [    2/   89]
per-ex loss: 0.320052  [    3/   89]
per-ex loss: 0.331742  [    4/   89]
per-ex loss: 0.457465  [    5/   89]
per-ex loss: 0.753792  [    6/   89]
per-ex loss: 0.542540  [    7/   89]
per-ex loss: 0.485575  [    8/   89]
per-ex loss: 0.317050  [    9/   89]
per-ex loss: 0.542331  [   10/   89]
per-ex loss: 0.359753  [   11/   89]
per-ex loss: 0.320016  [   12/   89]
per-ex loss: 0.345980  [   13/   89]
per-ex loss: 0.575276  [   14/   89]
per-ex loss: 0.540988  [   15/   89]
per-ex loss: 0.409233  [   16/   89]
per-ex loss: 0.550547  [   17/   89]
per-ex loss: 0.410909  [   18/   89]
per-ex loss: 0.280481  [   19/   89]
per-ex loss: 0.393579  [   20/   89]
per-ex loss: 0.538627  [   21/   89]
per-ex loss: 0.552406  [   22/   89]
per-ex loss: 0.452646  [   23/   89]
per-ex loss: 0.522697  [   24/   89]
per-ex loss: 0.464533  [   25/   89]
per-ex loss: 0.393303  [   26/   89]
per-ex loss: 0.208544  [   27/   89]
per-ex loss: 0.389215  [   28/   89]
per-ex loss: 0.431347  [   29/   89]
per-ex loss: 0.545184  [   30/   89]
per-ex loss: 0.546088  [   31/   89]
per-ex loss: 0.509475  [   32/   89]
per-ex loss: 0.323879  [   33/   89]
per-ex loss: 0.424730  [   34/   89]
per-ex loss: 0.499756  [   35/   89]
per-ex loss: 0.340465  [   36/   89]
per-ex loss: 0.347476  [   37/   89]
per-ex loss: 0.374160  [   38/   89]
per-ex loss: 0.369305  [   39/   89]
per-ex loss: 0.437921  [   40/   89]
per-ex loss: 0.517250  [   41/   89]
per-ex loss: 0.396526  [   42/   89]
per-ex loss: 0.460779  [   43/   89]
per-ex loss: 0.774632  [   44/   89]
per-ex loss: 0.520855  [   45/   89]
per-ex loss: 0.595799  [   46/   89]
per-ex loss: 0.466330  [   47/   89]
per-ex loss: 0.537442  [   48/   89]
per-ex loss: 0.426135  [   49/   89]
per-ex loss: 0.503692  [   50/   89]
per-ex loss: 0.502257  [   51/   89]
per-ex loss: 0.377806  [   52/   89]
per-ex loss: 0.402811  [   53/   89]
per-ex loss: 0.367515  [   54/   89]
per-ex loss: 0.377081  [   55/   89]
per-ex loss: 0.348175  [   56/   89]
per-ex loss: 0.374851  [   57/   89]
per-ex loss: 0.297509  [   58/   89]
per-ex loss: 0.314033  [   59/   89]
per-ex loss: 0.449293  [   60/   89]
per-ex loss: 0.354499  [   61/   89]
per-ex loss: 0.482433  [   62/   89]
per-ex loss: 0.351994  [   63/   89]
per-ex loss: 0.523760  [   64/   89]
per-ex loss: 0.339717  [   65/   89]
per-ex loss: 0.444059  [   66/   89]
per-ex loss: 0.550731  [   67/   89]
per-ex loss: 0.648487  [   68/   89]
per-ex loss: 0.370559  [   69/   89]
per-ex loss: 0.357885  [   70/   89]
per-ex loss: 0.541350  [   71/   89]
per-ex loss: 0.639369  [   72/   89]
per-ex loss: 0.539214  [   73/   89]
per-ex loss: 0.378536  [   74/   89]
per-ex loss: 0.323015  [   75/   89]
per-ex loss: 0.541274  [   76/   89]
per-ex loss: 0.329250  [   77/   89]
per-ex loss: 0.632721  [   78/   89]
per-ex loss: 0.412368  [   79/   89]
per-ex loss: 0.999918  [   80/   89]
per-ex loss: 0.501054  [   81/   89]
per-ex loss: 0.380276  [   82/   89]
per-ex loss: 0.328703  [   83/   89]
per-ex loss: 0.325584  [   84/   89]
per-ex loss: 0.428693  [   85/   89]
per-ex loss: 0.608677  [   86/   89]
per-ex loss: 0.414573  [   87/   89]
per-ex loss: 0.376391  [   88/   89]
per-ex loss: 0.346604  [   89/   89]
Train Error: Avg loss: 0.44736138
validation Error: 
 Avg loss: 0.57299565 
 F1: 0.463810 
 Precision: 0.582782 
 Recall: 0.385178
 IoU: 0.301923

test Error: 
 Avg loss: 0.53878296 
 F1: 0.526458 
 Precision: 0.593278 
 Recall: 0.473167
 IoU: 0.357274

We have finished training iteration 78
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_76_.pth
per-ex loss: 0.488447  [    1/   89]
per-ex loss: 0.406265  [    2/   89]
per-ex loss: 0.629057  [    3/   89]
per-ex loss: 0.489066  [    4/   89]
per-ex loss: 0.318553  [    5/   89]
per-ex loss: 0.439245  [    6/   89]
per-ex loss: 0.382771  [    7/   89]
per-ex loss: 0.515695  [    8/   89]
per-ex loss: 0.545817  [    9/   89]
per-ex loss: 0.347494  [   10/   89]
per-ex loss: 0.348613  [   11/   89]
per-ex loss: 0.527814  [   12/   89]
per-ex loss: 0.355499  [   13/   89]
per-ex loss: 0.630639  [   14/   89]
per-ex loss: 0.536348  [   15/   89]
per-ex loss: 0.584501  [   16/   89]
per-ex loss: 0.387217  [   17/   89]
per-ex loss: 0.502736  [   18/   89]
per-ex loss: 0.339128  [   19/   89]
per-ex loss: 0.635759  [   20/   89]
per-ex loss: 0.715955  [   21/   89]
per-ex loss: 0.297698  [   22/   89]
per-ex loss: 0.663287  [   23/   89]
per-ex loss: 0.215933  [   24/   89]
per-ex loss: 0.528292  [   25/   89]
per-ex loss: 0.449335  [   26/   89]
per-ex loss: 0.314193  [   27/   89]
per-ex loss: 0.546173  [   28/   89]
per-ex loss: 0.305202  [   29/   89]
per-ex loss: 0.460107  [   30/   89]
per-ex loss: 0.646958  [   31/   89]
per-ex loss: 0.369471  [   32/   89]
per-ex loss: 0.320622  [   33/   89]
per-ex loss: 0.431996  [   34/   89]
per-ex loss: 0.448182  [   35/   89]
per-ex loss: 0.483898  [   36/   89]
per-ex loss: 0.338899  [   37/   89]
per-ex loss: 0.336567  [   38/   89]
per-ex loss: 0.311818  [   39/   89]
per-ex loss: 0.552719  [   40/   89]
per-ex loss: 0.358376  [   41/   89]
per-ex loss: 0.363208  [   42/   89]
per-ex loss: 0.606951  [   43/   89]
per-ex loss: 0.418380  [   44/   89]
per-ex loss: 0.326175  [   45/   89]
per-ex loss: 0.383494  [   46/   89]
per-ex loss: 0.399504  [   47/   89]
per-ex loss: 0.383159  [   48/   89]
per-ex loss: 0.572608  [   49/   89]
per-ex loss: 0.350817  [   50/   89]
per-ex loss: 0.356542  [   51/   89]
per-ex loss: 0.472391  [   52/   89]
per-ex loss: 0.604081  [   53/   89]
per-ex loss: 0.358644  [   54/   89]
per-ex loss: 0.247946  [   55/   89]
per-ex loss: 0.373750  [   56/   89]
per-ex loss: 0.560491  [   57/   89]
per-ex loss: 0.465566  [   58/   89]
per-ex loss: 0.317487  [   59/   89]
per-ex loss: 0.543499  [   60/   89]
per-ex loss: 0.508417  [   61/   89]
per-ex loss: 0.347501  [   62/   89]
per-ex loss: 0.483718  [   63/   89]
per-ex loss: 0.676796  [   64/   89]
per-ex loss: 0.382179  [   65/   89]
per-ex loss: 0.349602  [   66/   89]
per-ex loss: 0.360571  [   67/   89]
per-ex loss: 0.506350  [   68/   89]
per-ex loss: 0.517663  [   69/   89]
per-ex loss: 0.302905  [   70/   89]
per-ex loss: 0.503295  [   71/   89]
per-ex loss: 0.420957  [   72/   89]
per-ex loss: 0.340439  [   73/   89]
per-ex loss: 0.613728  [   74/   89]
per-ex loss: 0.328728  [   75/   89]
per-ex loss: 0.346149  [   76/   89]
per-ex loss: 0.303043  [   77/   89]
per-ex loss: 0.449301  [   78/   89]
per-ex loss: 0.402084  [   79/   89]
per-ex loss: 0.515550  [   80/   89]
per-ex loss: 0.600215  [   81/   89]
per-ex loss: 0.444154  [   82/   89]
per-ex loss: 0.377319  [   83/   89]
per-ex loss: 0.999879  [   84/   89]
per-ex loss: 0.308664  [   85/   89]
per-ex loss: 0.379017  [   86/   89]
per-ex loss: 0.317805  [   87/   89]
per-ex loss: 0.392512  [   88/   89]
per-ex loss: 0.492290  [   89/   89]
Train Error: Avg loss: 0.44474012
validation Error: 
 Avg loss: 0.58590008 
 F1: 0.449575 
 Precision: 0.433974 
 Recall: 0.466340
 IoU: 0.289969

test Error: 
 Avg loss: 0.55155051 
 F1: 0.499365 
 Precision: 0.438002 
 Recall: 0.580725
 IoU: 0.332770

We have finished training iteration 79
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_64_.pth
per-ex loss: 0.566260  [    1/   89]
per-ex loss: 0.370457  [    2/   89]
per-ex loss: 0.324203  [    3/   89]
per-ex loss: 0.608695  [    4/   89]
per-ex loss: 0.999879  [    5/   89]
per-ex loss: 0.549265  [    6/   89]
per-ex loss: 0.339648  [    7/   89]
per-ex loss: 0.430095  [    8/   89]
per-ex loss: 0.433405  [    9/   89]
per-ex loss: 0.525148  [   10/   89]
per-ex loss: 0.553954  [   11/   89]
per-ex loss: 0.481914  [   12/   89]
per-ex loss: 0.397191  [   13/   89]
per-ex loss: 0.391961  [   14/   89]
per-ex loss: 0.613962  [   15/   89]
per-ex loss: 0.515733  [   16/   89]
per-ex loss: 0.484476  [   17/   89]
per-ex loss: 0.386875  [   18/   89]
per-ex loss: 0.370248  [   19/   89]
per-ex loss: 0.311429  [   20/   89]
per-ex loss: 0.526048  [   21/   89]
per-ex loss: 0.429766  [   22/   89]
per-ex loss: 0.351840  [   23/   89]
per-ex loss: 0.537529  [   24/   89]
per-ex loss: 0.359984  [   25/   89]
per-ex loss: 0.406102  [   26/   89]
per-ex loss: 0.535678  [   27/   89]
per-ex loss: 0.501060  [   28/   89]
per-ex loss: 0.355075  [   29/   89]
per-ex loss: 0.436728  [   30/   89]
per-ex loss: 0.482086  [   31/   89]
per-ex loss: 0.307376  [   32/   89]
per-ex loss: 0.423190  [   33/   89]
per-ex loss: 0.261674  [   34/   89]
per-ex loss: 0.264026  [   35/   89]
per-ex loss: 0.356390  [   36/   89]
per-ex loss: 0.516359  [   37/   89]
per-ex loss: 0.475515  [   38/   89]
per-ex loss: 0.531342  [   39/   89]
per-ex loss: 0.448915  [   40/   89]
per-ex loss: 0.538691  [   41/   89]
per-ex loss: 0.585183  [   42/   89]
per-ex loss: 0.367806  [   43/   89]
per-ex loss: 0.505112  [   44/   89]
per-ex loss: 0.594879  [   45/   89]
per-ex loss: 0.374323  [   46/   89]
per-ex loss: 0.316798  [   47/   89]
per-ex loss: 0.360986  [   48/   89]
per-ex loss: 0.473032  [   49/   89]
per-ex loss: 0.325168  [   50/   89]
per-ex loss: 0.407808  [   51/   89]
per-ex loss: 0.313907  [   52/   89]
per-ex loss: 0.520325  [   53/   89]
per-ex loss: 0.374128  [   54/   89]
per-ex loss: 0.343308  [   55/   89]
per-ex loss: 0.538622  [   56/   89]
per-ex loss: 0.314823  [   57/   89]
per-ex loss: 0.390680  [   58/   89]
per-ex loss: 0.411907  [   59/   89]
per-ex loss: 0.370175  [   60/   89]
per-ex loss: 0.570518  [   61/   89]
per-ex loss: 0.537807  [   62/   89]
per-ex loss: 0.383159  [   63/   89]
per-ex loss: 0.550973  [   64/   89]
per-ex loss: 0.556225  [   65/   89]
per-ex loss: 0.638211  [   66/   89]
per-ex loss: 0.358778  [   67/   89]
per-ex loss: 0.447839  [   68/   89]
per-ex loss: 0.518015  [   69/   89]
per-ex loss: 0.344431  [   70/   89]
per-ex loss: 0.302562  [   71/   89]
per-ex loss: 0.295460  [   72/   89]
per-ex loss: 0.374291  [   73/   89]
per-ex loss: 0.472132  [   74/   89]
per-ex loss: 0.332440  [   75/   89]
per-ex loss: 0.353829  [   76/   89]
per-ex loss: 0.625329  [   77/   89]
per-ex loss: 0.423235  [   78/   89]
per-ex loss: 0.318132  [   79/   89]
per-ex loss: 0.705217  [   80/   89]
per-ex loss: 0.516221  [   81/   89]
per-ex loss: 0.669806  [   82/   89]
per-ex loss: 0.320914  [   83/   89]
per-ex loss: 0.300687  [   84/   89]
per-ex loss: 0.359957  [   85/   89]
per-ex loss: 0.340753  [   86/   89]
per-ex loss: 0.345677  [   87/   89]
per-ex loss: 0.383113  [   88/   89]
per-ex loss: 0.529593  [   89/   89]
Train Error: Avg loss: 0.44342070
validation Error: 
 Avg loss: 0.54815216 
 F1: 0.480097 
 Precision: 0.491671 
 Recall: 0.469054
 IoU: 0.315873

test Error: 
 Avg loss: 0.49054496 
 F1: 0.558285 
 Precision: 0.576716 
 Recall: 0.540995
 IoU: 0.387236

We have finished training iteration 80
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_78_.pth
per-ex loss: 0.432552  [    1/   89]
per-ex loss: 0.523818  [    2/   89]
per-ex loss: 0.373771  [    3/   89]
per-ex loss: 0.480755  [    4/   89]
per-ex loss: 0.371287  [    5/   89]
per-ex loss: 0.418927  [    6/   89]
per-ex loss: 0.313492  [    7/   89]
per-ex loss: 0.641577  [    8/   89]
per-ex loss: 0.517291  [    9/   89]
per-ex loss: 0.364378  [   10/   89]
per-ex loss: 0.394617  [   11/   89]
per-ex loss: 0.519605  [   12/   89]
per-ex loss: 0.366203  [   13/   89]
per-ex loss: 0.357376  [   14/   89]
per-ex loss: 0.371848  [   15/   89]
per-ex loss: 0.520460  [   16/   89]
per-ex loss: 0.354885  [   17/   89]
per-ex loss: 0.477171  [   18/   89]
per-ex loss: 0.444245  [   19/   89]
per-ex loss: 0.529562  [   20/   89]
per-ex loss: 0.489041  [   21/   89]
per-ex loss: 0.588145  [   22/   89]
per-ex loss: 0.552008  [   23/   89]
per-ex loss: 0.363627  [   24/   89]
per-ex loss: 0.449284  [   25/   89]
per-ex loss: 0.355200  [   26/   89]
per-ex loss: 0.574224  [   27/   89]
per-ex loss: 0.543838  [   28/   89]
per-ex loss: 0.388085  [   29/   89]
per-ex loss: 0.327030  [   30/   89]
per-ex loss: 0.348379  [   31/   89]
per-ex loss: 0.556757  [   32/   89]
per-ex loss: 0.654748  [   33/   89]
per-ex loss: 0.436453  [   34/   89]
per-ex loss: 0.298786  [   35/   89]
per-ex loss: 0.308723  [   36/   89]
per-ex loss: 0.501171  [   37/   89]
per-ex loss: 0.433714  [   38/   89]
per-ex loss: 0.328566  [   39/   89]
per-ex loss: 0.364833  [   40/   89]
per-ex loss: 0.365302  [   41/   89]
per-ex loss: 0.378106  [   42/   89]
per-ex loss: 0.365381  [   43/   89]
per-ex loss: 0.650813  [   44/   89]
per-ex loss: 0.347041  [   45/   89]
per-ex loss: 0.434429  [   46/   89]
per-ex loss: 0.534383  [   47/   89]
per-ex loss: 0.244574  [   48/   89]
per-ex loss: 0.452014  [   49/   89]
per-ex loss: 0.403102  [   50/   89]
per-ex loss: 0.319168  [   51/   89]
per-ex loss: 0.390718  [   52/   89]
per-ex loss: 0.349420  [   53/   89]
per-ex loss: 0.355288  [   54/   89]
per-ex loss: 0.405828  [   55/   89]
per-ex loss: 0.412396  [   56/   89]
per-ex loss: 0.410968  [   57/   89]
per-ex loss: 0.478797  [   58/   89]
per-ex loss: 0.530758  [   59/   89]
per-ex loss: 0.341070  [   60/   89]
per-ex loss: 0.394728  [   61/   89]
per-ex loss: 0.415364  [   62/   89]
per-ex loss: 0.564105  [   63/   89]
per-ex loss: 0.617058  [   64/   89]
per-ex loss: 0.329934  [   65/   89]
per-ex loss: 0.343803  [   66/   89]
per-ex loss: 0.579031  [   67/   89]
per-ex loss: 0.524881  [   68/   89]
per-ex loss: 0.593737  [   69/   89]
per-ex loss: 0.353536  [   70/   89]
per-ex loss: 0.360723  [   71/   89]
per-ex loss: 0.638321  [   72/   89]
per-ex loss: 0.299890  [   73/   89]
per-ex loss: 0.366246  [   74/   89]
per-ex loss: 0.375114  [   75/   89]
per-ex loss: 0.460115  [   76/   89]
per-ex loss: 0.665794  [   77/   89]
per-ex loss: 0.585048  [   78/   89]
per-ex loss: 0.514370  [   79/   89]
per-ex loss: 0.292683  [   80/   89]
per-ex loss: 0.999879  [   81/   89]
per-ex loss: 0.384336  [   82/   89]
per-ex loss: 0.567429  [   83/   89]
per-ex loss: 0.342127  [   84/   89]
per-ex loss: 0.433637  [   85/   89]
per-ex loss: 0.412960  [   86/   89]
per-ex loss: 0.359648  [   87/   89]
per-ex loss: 0.549267  [   88/   89]
per-ex loss: 0.523773  [   89/   89]
Train Error: Avg loss: 0.44554526
validation Error: 
 Avg loss: 0.56329220 
 F1: 0.471301 
 Precision: 0.576207 
 Recall: 0.398710
 IoU: 0.308302

test Error: 
 Avg loss: 0.52748834 
 F1: 0.526098 
 Precision: 0.574753 
 Recall: 0.485038
 IoU: 0.356942

We have finished training iteration 81
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_79_.pth
per-ex loss: 0.479063  [    1/   89]
per-ex loss: 0.292353  [    2/   89]
per-ex loss: 0.480470  [    3/   89]
per-ex loss: 0.380876  [    4/   89]
per-ex loss: 0.257210  [    5/   89]
per-ex loss: 0.580700  [    6/   89]
per-ex loss: 0.410062  [    7/   89]
per-ex loss: 0.360066  [    8/   89]
per-ex loss: 0.505162  [    9/   89]
per-ex loss: 0.398187  [   10/   89]
per-ex loss: 0.214401  [   11/   89]
per-ex loss: 0.376634  [   12/   89]
per-ex loss: 0.543574  [   13/   89]
per-ex loss: 0.431117  [   14/   89]
per-ex loss: 0.314857  [   15/   89]
per-ex loss: 0.401365  [   16/   89]
per-ex loss: 0.497431  [   17/   89]
per-ex loss: 0.391898  [   18/   89]
per-ex loss: 0.315010  [   19/   89]
per-ex loss: 0.473201  [   20/   89]
per-ex loss: 0.438776  [   21/   89]
per-ex loss: 0.369293  [   22/   89]
per-ex loss: 0.408249  [   23/   89]
per-ex loss: 0.611346  [   24/   89]
per-ex loss: 0.447890  [   25/   89]
per-ex loss: 0.520265  [   26/   89]
per-ex loss: 0.561069  [   27/   89]
per-ex loss: 0.379883  [   28/   89]
per-ex loss: 0.526841  [   29/   89]
per-ex loss: 0.356258  [   30/   89]
per-ex loss: 0.386516  [   31/   89]
per-ex loss: 0.999890  [   32/   89]
per-ex loss: 0.494992  [   33/   89]
per-ex loss: 0.391294  [   34/   89]
per-ex loss: 0.423543  [   35/   89]
per-ex loss: 0.514311  [   36/   89]
per-ex loss: 0.622770  [   37/   89]
per-ex loss: 0.334598  [   38/   89]
per-ex loss: 0.372376  [   39/   89]
per-ex loss: 0.587145  [   40/   89]
per-ex loss: 0.424822  [   41/   89]
per-ex loss: 0.324912  [   42/   89]
per-ex loss: 0.333449  [   43/   89]
per-ex loss: 0.515634  [   44/   89]
per-ex loss: 0.478178  [   45/   89]
per-ex loss: 0.332218  [   46/   89]
per-ex loss: 0.370080  [   47/   89]
per-ex loss: 0.565048  [   48/   89]
per-ex loss: 0.428096  [   49/   89]
per-ex loss: 0.439927  [   50/   89]
per-ex loss: 0.353675  [   51/   89]
per-ex loss: 0.590330  [   52/   89]
per-ex loss: 0.521364  [   53/   89]
per-ex loss: 0.513050  [   54/   89]
per-ex loss: 0.314434  [   55/   89]
per-ex loss: 0.314878  [   56/   89]
per-ex loss: 0.343355  [   57/   89]
per-ex loss: 0.624922  [   58/   89]
per-ex loss: 0.588513  [   59/   89]
per-ex loss: 0.598886  [   60/   89]
per-ex loss: 0.431678  [   61/   89]
per-ex loss: 0.350681  [   62/   89]
per-ex loss: 0.633128  [   63/   89]
per-ex loss: 0.568848  [   64/   89]
per-ex loss: 0.377245  [   65/   89]
per-ex loss: 0.349474  [   66/   89]
per-ex loss: 0.436454  [   67/   89]
per-ex loss: 0.532495  [   68/   89]
per-ex loss: 0.344859  [   69/   89]
per-ex loss: 0.538888  [   70/   89]
per-ex loss: 0.403662  [   71/   89]
per-ex loss: 0.303951  [   72/   89]
per-ex loss: 0.663543  [   73/   89]
per-ex loss: 0.397316  [   74/   89]
per-ex loss: 0.388486  [   75/   89]
per-ex loss: 0.433658  [   76/   89]
per-ex loss: 0.305156  [   77/   89]
per-ex loss: 0.353088  [   78/   89]
per-ex loss: 0.337634  [   79/   89]
per-ex loss: 0.562606  [   80/   89]
per-ex loss: 0.562519  [   81/   89]
per-ex loss: 0.332099  [   82/   89]
per-ex loss: 0.505690  [   83/   89]
per-ex loss: 0.347228  [   84/   89]
per-ex loss: 0.469288  [   85/   89]
per-ex loss: 0.354374  [   86/   89]
per-ex loss: 0.345886  [   87/   89]
per-ex loss: 0.325664  [   88/   89]
per-ex loss: 0.521695  [   89/   89]
Train Error: Avg loss: 0.44161802
validation Error: 
 Avg loss: 0.53750975 
 F1: 0.486077 
 Precision: 0.548733 
 Recall: 0.436264
 IoU: 0.321072

test Error: 
 Avg loss: 0.48488171 
 F1: 0.566007 
 Precision: 0.607646 
 Recall: 0.529709
 IoU: 0.394707

We have finished training iteration 82
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_80_.pth
per-ex loss: 0.632596  [    1/   89]
per-ex loss: 0.326889  [    2/   89]
per-ex loss: 0.302066  [    3/   89]
per-ex loss: 0.330390  [    4/   89]
per-ex loss: 0.365425  [    5/   89]
per-ex loss: 0.313362  [    6/   89]
per-ex loss: 0.469291  [    7/   89]
per-ex loss: 0.539573  [    8/   89]
per-ex loss: 0.321652  [    9/   89]
per-ex loss: 0.387146  [   10/   89]
per-ex loss: 0.468563  [   11/   89]
per-ex loss: 0.393646  [   12/   89]
per-ex loss: 0.321600  [   13/   89]
per-ex loss: 0.628112  [   14/   89]
per-ex loss: 0.350422  [   15/   89]
per-ex loss: 0.280387  [   16/   89]
per-ex loss: 0.282858  [   17/   89]
per-ex loss: 0.522928  [   18/   89]
per-ex loss: 0.388741  [   19/   89]
per-ex loss: 0.525648  [   20/   89]
per-ex loss: 0.505412  [   21/   89]
per-ex loss: 0.475173  [   22/   89]
per-ex loss: 0.347509  [   23/   89]
per-ex loss: 0.368769  [   24/   89]
per-ex loss: 0.550018  [   25/   89]
per-ex loss: 0.409493  [   26/   89]
per-ex loss: 0.527872  [   27/   89]
per-ex loss: 0.331234  [   28/   89]
per-ex loss: 0.364631  [   29/   89]
per-ex loss: 0.624096  [   30/   89]
per-ex loss: 0.314292  [   31/   89]
per-ex loss: 0.437243  [   32/   89]
per-ex loss: 0.428204  [   33/   89]
per-ex loss: 0.537641  [   34/   89]
per-ex loss: 0.557607  [   35/   89]
per-ex loss: 0.278084  [   36/   89]
per-ex loss: 0.662749  [   37/   89]
per-ex loss: 0.505689  [   38/   89]
per-ex loss: 0.442331  [   39/   89]
per-ex loss: 0.312976  [   40/   89]
per-ex loss: 0.353071  [   41/   89]
per-ex loss: 0.427916  [   42/   89]
per-ex loss: 0.530940  [   43/   89]
per-ex loss: 0.447582  [   44/   89]
per-ex loss: 0.594424  [   45/   89]
per-ex loss: 0.716383  [   46/   89]
per-ex loss: 0.374631  [   47/   89]
per-ex loss: 0.213804  [   48/   89]
per-ex loss: 0.550490  [   49/   89]
per-ex loss: 0.367296  [   50/   89]
per-ex loss: 0.424614  [   51/   89]
per-ex loss: 0.387004  [   52/   89]
per-ex loss: 0.394544  [   53/   89]
per-ex loss: 0.403394  [   54/   89]
per-ex loss: 0.419759  [   55/   89]
per-ex loss: 0.530112  [   56/   89]
per-ex loss: 0.418679  [   57/   89]
per-ex loss: 0.405601  [   58/   89]
per-ex loss: 0.353150  [   59/   89]
per-ex loss: 0.424940  [   60/   89]
per-ex loss: 0.354684  [   61/   89]
per-ex loss: 0.404809  [   62/   89]
per-ex loss: 0.475541  [   63/   89]
per-ex loss: 0.403436  [   64/   89]
per-ex loss: 0.349472  [   65/   89]
per-ex loss: 0.530219  [   66/   89]
per-ex loss: 0.408620  [   67/   89]
per-ex loss: 0.653333  [   68/   89]
per-ex loss: 0.629004  [   69/   89]
per-ex loss: 0.362936  [   70/   89]
per-ex loss: 0.519796  [   71/   89]
per-ex loss: 0.404674  [   72/   89]
per-ex loss: 0.366185  [   73/   89]
per-ex loss: 0.582485  [   74/   89]
per-ex loss: 0.999932  [   75/   89]
per-ex loss: 0.470646  [   76/   89]
per-ex loss: 0.346457  [   77/   89]
per-ex loss: 0.321519  [   78/   89]
per-ex loss: 0.329070  [   79/   89]
per-ex loss: 0.470460  [   80/   89]
per-ex loss: 0.550070  [   81/   89]
per-ex loss: 0.586177  [   82/   89]
per-ex loss: 0.426074  [   83/   89]
per-ex loss: 0.380470  [   84/   89]
per-ex loss: 0.324214  [   85/   89]
per-ex loss: 0.373082  [   86/   89]
per-ex loss: 0.526717  [   87/   89]
per-ex loss: 0.422951  [   88/   89]
per-ex loss: 0.597764  [   89/   89]
Train Error: Avg loss: 0.44316233
validation Error: 
 Avg loss: 0.54376132 
 F1: 0.484901 
 Precision: 0.523755 
 Recall: 0.451414
 IoU: 0.320046

test Error: 
 Avg loss: 0.51075973 
 F1: 0.541095 
 Precision: 0.534833 
 Recall: 0.547505
 IoU: 0.370891

We have finished training iteration 83
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_81_.pth
per-ex loss: 0.623066  [    1/   89]
per-ex loss: 0.283824  [    2/   89]
per-ex loss: 0.371065  [    3/   89]
per-ex loss: 0.397309  [    4/   89]
per-ex loss: 0.344759  [    5/   89]
per-ex loss: 0.322725  [    6/   89]
per-ex loss: 0.355369  [    7/   89]
per-ex loss: 0.407698  [    8/   89]
per-ex loss: 0.530198  [    9/   89]
per-ex loss: 0.449746  [   10/   89]
per-ex loss: 0.367591  [   11/   89]
per-ex loss: 0.317047  [   12/   89]
per-ex loss: 0.349433  [   13/   89]
per-ex loss: 0.450284  [   14/   89]
per-ex loss: 0.477777  [   15/   89]
per-ex loss: 0.442935  [   16/   89]
per-ex loss: 0.502080  [   17/   89]
per-ex loss: 0.544705  [   18/   89]
per-ex loss: 0.379275  [   19/   89]
per-ex loss: 0.599758  [   20/   89]
per-ex loss: 0.438388  [   21/   89]
per-ex loss: 0.544398  [   22/   89]
per-ex loss: 0.304738  [   23/   89]
per-ex loss: 0.374234  [   24/   89]
per-ex loss: 0.549772  [   25/   89]
per-ex loss: 0.599132  [   26/   89]
per-ex loss: 0.276169  [   27/   89]
per-ex loss: 0.621931  [   28/   89]
per-ex loss: 0.436339  [   29/   89]
per-ex loss: 0.514449  [   30/   89]
per-ex loss: 0.361023  [   31/   89]
per-ex loss: 0.369972  [   32/   89]
per-ex loss: 0.525063  [   33/   89]
per-ex loss: 0.536572  [   34/   89]
per-ex loss: 0.351921  [   35/   89]
per-ex loss: 0.336927  [   36/   89]
per-ex loss: 0.556223  [   37/   89]
per-ex loss: 0.405197  [   38/   89]
per-ex loss: 0.432628  [   39/   89]
per-ex loss: 0.540722  [   40/   89]
per-ex loss: 0.332129  [   41/   89]
per-ex loss: 0.392581  [   42/   89]
per-ex loss: 0.349366  [   43/   89]
per-ex loss: 0.316649  [   44/   89]
per-ex loss: 0.528689  [   45/   89]
per-ex loss: 0.605416  [   46/   89]
per-ex loss: 0.572299  [   47/   89]
per-ex loss: 0.336006  [   48/   89]
per-ex loss: 0.517378  [   49/   89]
per-ex loss: 0.368707  [   50/   89]
per-ex loss: 0.377674  [   51/   89]
per-ex loss: 0.510878  [   52/   89]
per-ex loss: 0.346238  [   53/   89]
per-ex loss: 0.477247  [   54/   89]
per-ex loss: 0.419986  [   55/   89]
per-ex loss: 0.360339  [   56/   89]
per-ex loss: 0.999879  [   57/   89]
per-ex loss: 0.494511  [   58/   89]
per-ex loss: 0.414688  [   59/   89]
per-ex loss: 0.368411  [   60/   89]
per-ex loss: 0.347854  [   61/   89]
per-ex loss: 0.543983  [   62/   89]
per-ex loss: 0.330269  [   63/   89]
per-ex loss: 0.281401  [   64/   89]
per-ex loss: 0.328276  [   65/   89]
per-ex loss: 0.389908  [   66/   89]
per-ex loss: 0.486312  [   67/   89]
per-ex loss: 0.350984  [   68/   89]
per-ex loss: 0.581692  [   69/   89]
per-ex loss: 0.309021  [   70/   89]
per-ex loss: 0.418468  [   71/   89]
per-ex loss: 0.336123  [   72/   89]
per-ex loss: 0.332717  [   73/   89]
per-ex loss: 0.511533  [   74/   89]
per-ex loss: 0.653730  [   75/   89]
per-ex loss: 0.359602  [   76/   89]
per-ex loss: 0.585209  [   77/   89]
per-ex loss: 0.529861  [   78/   89]
per-ex loss: 0.532040  [   79/   89]
per-ex loss: 0.315579  [   80/   89]
per-ex loss: 0.471049  [   81/   89]
per-ex loss: 0.642786  [   82/   89]
per-ex loss: 0.352884  [   83/   89]
per-ex loss: 0.370874  [   84/   89]
per-ex loss: 0.509508  [   85/   89]
per-ex loss: 0.242930  [   86/   89]
per-ex loss: 0.424586  [   87/   89]
per-ex loss: 0.336982  [   88/   89]
per-ex loss: 0.525964  [   89/   89]
Train Error: Avg loss: 0.43914197
validation Error: 
 Avg loss: 0.53714784 
 F1: 0.492944 
 Precision: 0.597811 
 Recall: 0.419378
 IoU: 0.327091

test Error: 
 Avg loss: 0.49441270 
 F1: 0.555895 
 Precision: 0.656887 
 Recall: 0.481819
 IoU: 0.384941

We have finished training iteration 84
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_82_.pth
per-ex loss: 0.372891  [    1/   89]
per-ex loss: 0.383698  [    2/   89]
per-ex loss: 0.502919  [    3/   89]
per-ex loss: 0.487958  [    4/   89]
per-ex loss: 0.565464  [    5/   89]
per-ex loss: 0.396069  [    6/   89]
per-ex loss: 0.595259  [    7/   89]
per-ex loss: 0.300045  [    8/   89]
per-ex loss: 0.591366  [    9/   89]
per-ex loss: 0.600105  [   10/   89]
per-ex loss: 0.441307  [   11/   89]
per-ex loss: 0.383220  [   12/   89]
per-ex loss: 0.425980  [   13/   89]
per-ex loss: 0.425139  [   14/   89]
per-ex loss: 0.497487  [   15/   89]
per-ex loss: 0.527076  [   16/   89]
per-ex loss: 0.329262  [   17/   89]
per-ex loss: 0.352423  [   18/   89]
per-ex loss: 0.534618  [   19/   89]
per-ex loss: 0.402411  [   20/   89]
per-ex loss: 0.707675  [   21/   89]
per-ex loss: 0.445040  [   22/   89]
per-ex loss: 0.357435  [   23/   89]
per-ex loss: 0.339120  [   24/   89]
per-ex loss: 0.291924  [   25/   89]
per-ex loss: 0.604484  [   26/   89]
per-ex loss: 0.349952  [   27/   89]
per-ex loss: 0.641777  [   28/   89]
per-ex loss: 0.526611  [   29/   89]
per-ex loss: 0.385137  [   30/   89]
per-ex loss: 0.534655  [   31/   89]
per-ex loss: 0.451913  [   32/   89]
per-ex loss: 0.360648  [   33/   89]
per-ex loss: 0.564774  [   34/   89]
per-ex loss: 0.498747  [   35/   89]
per-ex loss: 0.390446  [   36/   89]
per-ex loss: 0.368337  [   37/   89]
per-ex loss: 0.577120  [   38/   89]
per-ex loss: 0.297249  [   39/   89]
per-ex loss: 0.549719  [   40/   89]
per-ex loss: 0.553571  [   41/   89]
per-ex loss: 0.412372  [   42/   89]
per-ex loss: 0.544645  [   43/   89]
per-ex loss: 0.404668  [   44/   89]
per-ex loss: 0.447821  [   45/   89]
per-ex loss: 0.385011  [   46/   89]
per-ex loss: 0.324350  [   47/   89]
per-ex loss: 0.410707  [   48/   89]
per-ex loss: 0.289893  [   49/   89]
per-ex loss: 0.495372  [   50/   89]
per-ex loss: 0.348551  [   51/   89]
per-ex loss: 0.360100  [   52/   89]
per-ex loss: 0.475421  [   53/   89]
per-ex loss: 0.423878  [   54/   89]
per-ex loss: 0.446488  [   55/   89]
per-ex loss: 0.392778  [   56/   89]
per-ex loss: 0.268809  [   57/   89]
per-ex loss: 0.430034  [   58/   89]
per-ex loss: 0.320174  [   59/   89]
per-ex loss: 0.321184  [   60/   89]
per-ex loss: 0.569791  [   61/   89]
per-ex loss: 0.500561  [   62/   89]
per-ex loss: 0.354144  [   63/   89]
per-ex loss: 0.532403  [   64/   89]
per-ex loss: 0.486779  [   65/   89]
per-ex loss: 0.295172  [   66/   89]
per-ex loss: 0.420767  [   67/   89]
per-ex loss: 0.520473  [   68/   89]
per-ex loss: 0.310943  [   69/   89]
per-ex loss: 0.371301  [   70/   89]
per-ex loss: 0.407368  [   71/   89]
per-ex loss: 0.378668  [   72/   89]
per-ex loss: 0.323762  [   73/   89]
per-ex loss: 0.411839  [   74/   89]
per-ex loss: 0.617635  [   75/   89]
per-ex loss: 0.321579  [   76/   89]
per-ex loss: 0.340242  [   77/   89]
per-ex loss: 0.348527  [   78/   89]
per-ex loss: 0.310873  [   79/   89]
per-ex loss: 0.658749  [   80/   89]
per-ex loss: 0.461972  [   81/   89]
per-ex loss: 0.383090  [   82/   89]
per-ex loss: 0.635118  [   83/   89]
per-ex loss: 0.999939  [   84/   89]
per-ex loss: 0.327889  [   85/   89]
per-ex loss: 0.393486  [   86/   89]
per-ex loss: 0.325101  [   87/   89]
per-ex loss: 0.609339  [   88/   89]
per-ex loss: 0.457097  [   89/   89]
Train Error: Avg loss: 0.44368420
validation Error: 
 Avg loss: 0.60294082 
 F1: 0.439975 
 Precision: 0.400629 
 Recall: 0.487890
 IoU: 0.282031

test Error: 
 Avg loss: 0.57726754 
 F1: 0.474251 
 Precision: 0.392170 
 Recall: 0.599786
 IoU: 0.310832

We have finished training iteration 85
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_83_.pth
per-ex loss: 0.434052  [    1/   89]
per-ex loss: 0.571454  [    2/   89]
per-ex loss: 0.338254  [    3/   89]
per-ex loss: 0.331920  [    4/   89]
per-ex loss: 0.520035  [    5/   89]
per-ex loss: 0.344296  [    6/   89]
per-ex loss: 0.349923  [    7/   89]
per-ex loss: 0.508389  [    8/   89]
per-ex loss: 0.301618  [    9/   89]
per-ex loss: 0.312889  [   10/   89]
per-ex loss: 0.390528  [   11/   89]
per-ex loss: 0.465370  [   12/   89]
per-ex loss: 0.999915  [   13/   89]
per-ex loss: 0.581078  [   14/   89]
per-ex loss: 0.509194  [   15/   89]
per-ex loss: 0.372607  [   16/   89]
per-ex loss: 0.408495  [   17/   89]
per-ex loss: 0.572728  [   18/   89]
per-ex loss: 0.361833  [   19/   89]
per-ex loss: 0.400231  [   20/   89]
per-ex loss: 0.283650  [   21/   89]
per-ex loss: 0.327851  [   22/   89]
per-ex loss: 0.290802  [   23/   89]
per-ex loss: 0.455134  [   24/   89]
per-ex loss: 0.333503  [   25/   89]
per-ex loss: 0.316587  [   26/   89]
per-ex loss: 0.330775  [   27/   89]
per-ex loss: 0.381885  [   28/   89]
per-ex loss: 0.508883  [   29/   89]
per-ex loss: 0.607307  [   30/   89]
per-ex loss: 0.507096  [   31/   89]
per-ex loss: 0.354849  [   32/   89]
per-ex loss: 0.552272  [   33/   89]
per-ex loss: 0.550291  [   34/   89]
per-ex loss: 0.364952  [   35/   89]
per-ex loss: 0.397277  [   36/   89]
per-ex loss: 0.549820  [   37/   89]
per-ex loss: 0.293019  [   38/   89]
per-ex loss: 0.573401  [   39/   89]
per-ex loss: 0.379834  [   40/   89]
per-ex loss: 0.232930  [   41/   89]
per-ex loss: 0.346590  [   42/   89]
per-ex loss: 0.451899  [   43/   89]
per-ex loss: 0.440615  [   44/   89]
per-ex loss: 0.439542  [   45/   89]
per-ex loss: 0.313441  [   46/   89]
per-ex loss: 0.483635  [   47/   89]
per-ex loss: 0.358445  [   48/   89]
per-ex loss: 0.314541  [   49/   89]
per-ex loss: 0.696852  [   50/   89]
per-ex loss: 0.412852  [   51/   89]
per-ex loss: 0.658129  [   52/   89]
per-ex loss: 0.358224  [   53/   89]
per-ex loss: 0.455748  [   54/   89]
per-ex loss: 0.370767  [   55/   89]
per-ex loss: 0.433129  [   56/   89]
per-ex loss: 0.339617  [   57/   89]
per-ex loss: 0.412111  [   58/   89]
per-ex loss: 0.384669  [   59/   89]
per-ex loss: 0.591794  [   60/   89]
per-ex loss: 0.322287  [   61/   89]
per-ex loss: 0.371127  [   62/   89]
per-ex loss: 0.515642  [   63/   89]
per-ex loss: 0.349527  [   64/   89]
per-ex loss: 0.501998  [   65/   89]
per-ex loss: 0.535106  [   66/   89]
per-ex loss: 0.516515  [   67/   89]
per-ex loss: 0.530938  [   68/   89]
per-ex loss: 0.309026  [   69/   89]
per-ex loss: 0.518638  [   70/   89]
per-ex loss: 0.326303  [   71/   89]
per-ex loss: 0.448097  [   72/   89]
per-ex loss: 0.697400  [   73/   89]
per-ex loss: 0.675206  [   74/   89]
per-ex loss: 0.351306  [   75/   89]
per-ex loss: 0.516021  [   76/   89]
per-ex loss: 0.509225  [   77/   89]
per-ex loss: 0.593191  [   78/   89]
per-ex loss: 0.638026  [   79/   89]
per-ex loss: 0.406968  [   80/   89]
per-ex loss: 0.536074  [   81/   89]
per-ex loss: 0.378547  [   82/   89]
per-ex loss: 0.461600  [   83/   89]
per-ex loss: 0.356382  [   84/   89]
per-ex loss: 0.517454  [   85/   89]
per-ex loss: 0.485068  [   86/   89]
per-ex loss: 0.345026  [   87/   89]
per-ex loss: 0.332529  [   88/   89]
per-ex loss: 0.347777  [   89/   89]
Train Error: Avg loss: 0.44182616
validation Error: 
 Avg loss: 0.53910751 
 F1: 0.490913 
 Precision: 0.571601 
 Recall: 0.430188
 IoU: 0.325305

test Error: 
 Avg loss: 0.48821379 
 F1: 0.563539 
 Precision: 0.637134 
 Recall: 0.505186
 IoU: 0.392311

We have finished training iteration 86
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_84_.pth
per-ex loss: 0.477895  [    1/   89]
per-ex loss: 0.369240  [    2/   89]
per-ex loss: 0.409556  [    3/   89]
per-ex loss: 0.360909  [    4/   89]
per-ex loss: 0.427893  [    5/   89]
per-ex loss: 0.644138  [    6/   89]
per-ex loss: 0.297227  [    7/   89]
per-ex loss: 0.569173  [    8/   89]
per-ex loss: 0.498246  [    9/   89]
per-ex loss: 0.323277  [   10/   89]
per-ex loss: 0.353376  [   11/   89]
per-ex loss: 0.361632  [   12/   89]
per-ex loss: 0.418086  [   13/   89]
per-ex loss: 0.374622  [   14/   89]
per-ex loss: 0.526876  [   15/   89]
per-ex loss: 0.356697  [   16/   89]
per-ex loss: 0.480160  [   17/   89]
per-ex loss: 0.599338  [   18/   89]
per-ex loss: 0.417320  [   19/   89]
per-ex loss: 0.334203  [   20/   89]
per-ex loss: 0.541824  [   21/   89]
per-ex loss: 0.448286  [   22/   89]
per-ex loss: 0.456009  [   23/   89]
per-ex loss: 0.298730  [   24/   89]
per-ex loss: 0.493962  [   25/   89]
per-ex loss: 0.374444  [   26/   89]
per-ex loss: 0.630834  [   27/   89]
per-ex loss: 0.365826  [   28/   89]
per-ex loss: 0.341594  [   29/   89]
per-ex loss: 0.369888  [   30/   89]
per-ex loss: 0.315904  [   31/   89]
per-ex loss: 0.465790  [   32/   89]
per-ex loss: 0.999925  [   33/   89]
per-ex loss: 0.247651  [   34/   89]
per-ex loss: 0.359647  [   35/   89]
per-ex loss: 0.527276  [   36/   89]
per-ex loss: 0.576446  [   37/   89]
per-ex loss: 0.442454  [   38/   89]
per-ex loss: 0.503525  [   39/   89]
per-ex loss: 0.390318  [   40/   89]
per-ex loss: 0.646255  [   41/   89]
per-ex loss: 0.346776  [   42/   89]
per-ex loss: 0.479679  [   43/   89]
per-ex loss: 0.636080  [   44/   89]
per-ex loss: 0.334195  [   45/   89]
per-ex loss: 0.340538  [   46/   89]
per-ex loss: 0.358617  [   47/   89]
per-ex loss: 0.338126  [   48/   89]
per-ex loss: 0.426437  [   49/   89]
per-ex loss: 0.540320  [   50/   89]
per-ex loss: 0.498035  [   51/   89]
per-ex loss: 0.547472  [   52/   89]
per-ex loss: 0.551400  [   53/   89]
per-ex loss: 0.348962  [   54/   89]
per-ex loss: 0.463772  [   55/   89]
per-ex loss: 0.676359  [   56/   89]
per-ex loss: 0.389901  [   57/   89]
per-ex loss: 0.530602  [   58/   89]
per-ex loss: 0.348295  [   59/   89]
per-ex loss: 0.555638  [   60/   89]
per-ex loss: 0.336964  [   61/   89]
per-ex loss: 0.540980  [   62/   89]
per-ex loss: 0.419322  [   63/   89]
per-ex loss: 0.359134  [   64/   89]
per-ex loss: 0.606725  [   65/   89]
per-ex loss: 0.335224  [   66/   89]
per-ex loss: 0.411195  [   67/   89]
per-ex loss: 0.439485  [   68/   89]
per-ex loss: 0.394353  [   69/   89]
per-ex loss: 0.442353  [   70/   89]
per-ex loss: 0.379230  [   71/   89]
per-ex loss: 0.355825  [   72/   89]
per-ex loss: 0.557491  [   73/   89]
per-ex loss: 0.334168  [   74/   89]
per-ex loss: 0.325191  [   75/   89]
per-ex loss: 0.362297  [   76/   89]
per-ex loss: 0.537417  [   77/   89]
per-ex loss: 0.564007  [   78/   89]
per-ex loss: 0.525236  [   79/   89]
per-ex loss: 0.459995  [   80/   89]
per-ex loss: 0.557443  [   81/   89]
per-ex loss: 0.312382  [   82/   89]
per-ex loss: 0.320826  [   83/   89]
per-ex loss: 0.348938  [   84/   89]
per-ex loss: 0.306809  [   85/   89]
per-ex loss: 0.405229  [   86/   89]
per-ex loss: 0.524855  [   87/   89]
per-ex loss: 0.296092  [   88/   89]
per-ex loss: 0.355399  [   89/   89]
Train Error: Avg loss: 0.44069914
validation Error: 
 Avg loss: 0.53574589 
 F1: 0.484977 
 Precision: 0.519488 
 Recall: 0.454765
 IoU: 0.320112

test Error: 
 Avg loss: 0.48147244 
 F1: 0.565939 
 Precision: 0.587862 
 Recall: 0.545593
 IoU: 0.394641

We have finished training iteration 87
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_85_.pth
per-ex loss: 0.666343  [    1/   89]
per-ex loss: 0.402427  [    2/   89]
per-ex loss: 0.461469  [    3/   89]
per-ex loss: 0.374426  [    4/   89]
per-ex loss: 0.281489  [    5/   89]
per-ex loss: 0.440698  [    6/   89]
per-ex loss: 0.349383  [    7/   89]
per-ex loss: 0.467327  [    8/   89]
per-ex loss: 0.537789  [    9/   89]
per-ex loss: 0.279297  [   10/   89]
per-ex loss: 0.402091  [   11/   89]
per-ex loss: 0.462344  [   12/   89]
per-ex loss: 0.195704  [   13/   89]
per-ex loss: 0.339045  [   14/   89]
per-ex loss: 0.399758  [   15/   89]
per-ex loss: 0.348829  [   16/   89]
per-ex loss: 0.687322  [   17/   89]
per-ex loss: 0.540991  [   18/   89]
per-ex loss: 0.346886  [   19/   89]
per-ex loss: 0.371611  [   20/   89]
per-ex loss: 0.523774  [   21/   89]
per-ex loss: 0.389726  [   22/   89]
per-ex loss: 0.502954  [   23/   89]
per-ex loss: 0.504440  [   24/   89]
per-ex loss: 0.390635  [   25/   89]
per-ex loss: 0.416049  [   26/   89]
per-ex loss: 0.370855  [   27/   89]
per-ex loss: 0.361365  [   28/   89]
per-ex loss: 0.316024  [   29/   89]
per-ex loss: 0.562106  [   30/   89]
per-ex loss: 0.535170  [   31/   89]
per-ex loss: 0.650728  [   32/   89]
per-ex loss: 0.313258  [   33/   89]
per-ex loss: 0.293307  [   34/   89]
per-ex loss: 0.356269  [   35/   89]
per-ex loss: 0.479465  [   36/   89]
per-ex loss: 0.433571  [   37/   89]
per-ex loss: 0.327194  [   38/   89]
per-ex loss: 0.595009  [   39/   89]
per-ex loss: 0.555024  [   40/   89]
per-ex loss: 0.387127  [   41/   89]
per-ex loss: 0.417852  [   42/   89]
per-ex loss: 0.638381  [   43/   89]
per-ex loss: 0.398212  [   44/   89]
per-ex loss: 0.543840  [   45/   89]
per-ex loss: 0.517058  [   46/   89]
per-ex loss: 0.422147  [   47/   89]
per-ex loss: 0.385011  [   48/   89]
per-ex loss: 0.534517  [   49/   89]
per-ex loss: 0.318384  [   50/   89]
per-ex loss: 0.581659  [   51/   89]
per-ex loss: 0.543104  [   52/   89]
per-ex loss: 0.484238  [   53/   89]
per-ex loss: 0.369252  [   54/   89]
per-ex loss: 0.343591  [   55/   89]
per-ex loss: 0.412886  [   56/   89]
per-ex loss: 0.561202  [   57/   89]
per-ex loss: 0.523830  [   58/   89]
per-ex loss: 0.438548  [   59/   89]
per-ex loss: 0.999887  [   60/   89]
per-ex loss: 0.484204  [   61/   89]
per-ex loss: 0.339423  [   62/   89]
per-ex loss: 0.302180  [   63/   89]
per-ex loss: 0.429154  [   64/   89]
per-ex loss: 0.331504  [   65/   89]
per-ex loss: 0.334113  [   66/   89]
per-ex loss: 0.318927  [   67/   89]
per-ex loss: 0.591592  [   68/   89]
per-ex loss: 0.538759  [   69/   89]
per-ex loss: 0.407299  [   70/   89]
per-ex loss: 0.287358  [   71/   89]
per-ex loss: 0.324615  [   72/   89]
per-ex loss: 0.358462  [   73/   89]
per-ex loss: 0.342855  [   74/   89]
per-ex loss: 0.314799  [   75/   89]
per-ex loss: 0.541111  [   76/   89]
per-ex loss: 0.397299  [   77/   89]
per-ex loss: 0.376450  [   78/   89]
per-ex loss: 0.528173  [   79/   89]
per-ex loss: 0.349100  [   80/   89]
per-ex loss: 0.338620  [   81/   89]
per-ex loss: 0.739878  [   82/   89]
per-ex loss: 0.333562  [   83/   89]
per-ex loss: 0.513215  [   84/   89]
per-ex loss: 0.529382  [   85/   89]
per-ex loss: 0.599092  [   86/   89]
per-ex loss: 0.375867  [   87/   89]
per-ex loss: 0.292396  [   88/   89]
per-ex loss: 0.596407  [   89/   89]
Train Error: Avg loss: 0.44122104
validation Error: 
 Avg loss: 0.53294606 
 F1: 0.492541 
 Precision: 0.520791 
 Recall: 0.467197
 IoU: 0.326736

test Error: 
 Avg loss: 0.49200697 
 F1: 0.554835 
 Precision: 0.550409 
 Recall: 0.559332
 IoU: 0.383925

We have finished training iteration 88
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_86_.pth
per-ex loss: 0.374903  [    1/   89]
per-ex loss: 0.403814  [    2/   89]
per-ex loss: 0.547878  [    3/   89]
per-ex loss: 0.563076  [    4/   89]
per-ex loss: 0.389785  [    5/   89]
per-ex loss: 0.626398  [    6/   89]
per-ex loss: 0.599707  [    7/   89]
per-ex loss: 0.512234  [    8/   89]
per-ex loss: 0.607252  [    9/   89]
per-ex loss: 0.393381  [   10/   89]
per-ex loss: 0.342658  [   11/   89]
per-ex loss: 0.434341  [   12/   89]
per-ex loss: 0.651581  [   13/   89]
per-ex loss: 0.355240  [   14/   89]
per-ex loss: 0.333045  [   15/   89]
per-ex loss: 0.274395  [   16/   89]
per-ex loss: 0.326104  [   17/   89]
per-ex loss: 0.282085  [   18/   89]
per-ex loss: 0.470404  [   19/   89]
per-ex loss: 0.426755  [   20/   89]
per-ex loss: 0.446526  [   21/   89]
per-ex loss: 0.375489  [   22/   89]
per-ex loss: 0.430287  [   23/   89]
per-ex loss: 0.424653  [   24/   89]
per-ex loss: 0.205260  [   25/   89]
per-ex loss: 0.338667  [   26/   89]
per-ex loss: 0.528259  [   27/   89]
per-ex loss: 0.332204  [   28/   89]
per-ex loss: 0.448868  [   29/   89]
per-ex loss: 0.619125  [   30/   89]
per-ex loss: 0.630219  [   31/   89]
per-ex loss: 0.362419  [   32/   89]
per-ex loss: 0.511807  [   33/   89]
per-ex loss: 0.505411  [   34/   89]
per-ex loss: 0.593966  [   35/   89]
per-ex loss: 0.488856  [   36/   89]
per-ex loss: 0.378012  [   37/   89]
per-ex loss: 0.620880  [   38/   89]
per-ex loss: 0.505712  [   39/   89]
per-ex loss: 0.426533  [   40/   89]
per-ex loss: 0.372053  [   41/   89]
per-ex loss: 0.361264  [   42/   89]
per-ex loss: 0.534927  [   43/   89]
per-ex loss: 0.511311  [   44/   89]
per-ex loss: 0.383794  [   45/   89]
per-ex loss: 0.331876  [   46/   89]
per-ex loss: 0.423368  [   47/   89]
per-ex loss: 0.511668  [   48/   89]
per-ex loss: 0.374778  [   49/   89]
per-ex loss: 0.603513  [   50/   89]
per-ex loss: 0.590927  [   51/   89]
per-ex loss: 0.404516  [   52/   89]
per-ex loss: 0.459192  [   53/   89]
per-ex loss: 0.320789  [   54/   89]
per-ex loss: 0.555152  [   55/   89]
per-ex loss: 0.382544  [   56/   89]
per-ex loss: 0.387168  [   57/   89]
per-ex loss: 0.525170  [   58/   89]
per-ex loss: 0.546157  [   59/   89]
per-ex loss: 0.360163  [   60/   89]
per-ex loss: 0.397274  [   61/   89]
per-ex loss: 0.467742  [   62/   89]
per-ex loss: 0.542235  [   63/   89]
per-ex loss: 0.528259  [   64/   89]
per-ex loss: 0.353537  [   65/   89]
per-ex loss: 0.464307  [   66/   89]
per-ex loss: 0.369426  [   67/   89]
per-ex loss: 0.310105  [   68/   89]
per-ex loss: 0.477164  [   69/   89]
per-ex loss: 0.499446  [   70/   89]
per-ex loss: 0.999940  [   71/   89]
per-ex loss: 0.321645  [   72/   89]
per-ex loss: 0.388761  [   73/   89]
per-ex loss: 0.327247  [   74/   89]
per-ex loss: 0.377393  [   75/   89]
per-ex loss: 0.266178  [   76/   89]
per-ex loss: 0.361789  [   77/   89]
per-ex loss: 0.342226  [   78/   89]
per-ex loss: 0.356922  [   79/   89]
per-ex loss: 0.351208  [   80/   89]
per-ex loss: 0.363713  [   81/   89]
per-ex loss: 0.571931  [   82/   89]
per-ex loss: 0.380668  [   83/   89]
per-ex loss: 0.281513  [   84/   89]
per-ex loss: 0.395899  [   85/   89]
per-ex loss: 0.497941  [   86/   89]
per-ex loss: 0.320347  [   87/   89]
per-ex loss: 0.490472  [   88/   89]
per-ex loss: 0.324399  [   89/   89]
Train Error: Avg loss: 0.43997985
validation Error: 
 Avg loss: 0.53853410 
 F1: 0.487843 
 Precision: 0.609600 
 Recall: 0.406626
 IoU: 0.322614

test Error: 
 Avg loss: 0.49540719 
 F1: 0.555786 
 Precision: 0.640921 
 Recall: 0.490616
 IoU: 0.384836

We have finished training iteration 89
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_87_.pth
per-ex loss: 0.339107  [    1/   89]
per-ex loss: 0.442330  [    2/   89]
per-ex loss: 0.395674  [    3/   89]
per-ex loss: 0.459879  [    4/   89]
per-ex loss: 0.458379  [    5/   89]
per-ex loss: 0.397370  [    6/   89]
per-ex loss: 0.523332  [    7/   89]
per-ex loss: 0.358762  [    8/   89]
per-ex loss: 0.336540  [    9/   89]
per-ex loss: 0.331627  [   10/   89]
per-ex loss: 0.421693  [   11/   89]
per-ex loss: 0.368570  [   12/   89]
per-ex loss: 0.501538  [   13/   89]
per-ex loss: 0.217584  [   14/   89]
per-ex loss: 0.540214  [   15/   89]
per-ex loss: 0.576593  [   16/   89]
per-ex loss: 0.351191  [   17/   89]
per-ex loss: 0.497519  [   18/   89]
per-ex loss: 0.330105  [   19/   89]
per-ex loss: 0.529670  [   20/   89]
per-ex loss: 0.263842  [   21/   89]
per-ex loss: 0.400314  [   22/   89]
per-ex loss: 0.642851  [   23/   89]
per-ex loss: 0.525690  [   24/   89]
per-ex loss: 0.365390  [   25/   89]
per-ex loss: 0.518418  [   26/   89]
per-ex loss: 0.553555  [   27/   89]
per-ex loss: 0.641764  [   28/   89]
per-ex loss: 0.532207  [   29/   89]
per-ex loss: 0.345338  [   30/   89]
per-ex loss: 0.342424  [   31/   89]
per-ex loss: 0.647475  [   32/   89]
per-ex loss: 0.393272  [   33/   89]
per-ex loss: 0.410017  [   34/   89]
per-ex loss: 0.357601  [   35/   89]
per-ex loss: 0.375334  [   36/   89]
per-ex loss: 0.472607  [   37/   89]
per-ex loss: 0.336593  [   38/   89]
per-ex loss: 0.306511  [   39/   89]
per-ex loss: 0.322870  [   40/   89]
per-ex loss: 0.369713  [   41/   89]
per-ex loss: 0.541990  [   42/   89]
per-ex loss: 0.334968  [   43/   89]
per-ex loss: 0.386231  [   44/   89]
per-ex loss: 0.308112  [   45/   89]
per-ex loss: 0.399941  [   46/   89]
per-ex loss: 0.562254  [   47/   89]
per-ex loss: 0.580645  [   48/   89]
per-ex loss: 0.338691  [   49/   89]
per-ex loss: 0.410109  [   50/   89]
per-ex loss: 0.300428  [   51/   89]
per-ex loss: 0.397664  [   52/   89]
per-ex loss: 0.600744  [   53/   89]
per-ex loss: 0.404969  [   54/   89]
per-ex loss: 0.513067  [   55/   89]
per-ex loss: 0.321860  [   56/   89]
per-ex loss: 0.415746  [   57/   89]
per-ex loss: 0.397212  [   58/   89]
per-ex loss: 0.316919  [   59/   89]
per-ex loss: 0.358434  [   60/   89]
per-ex loss: 0.322217  [   61/   89]
per-ex loss: 0.274281  [   62/   89]
per-ex loss: 0.329816  [   63/   89]
per-ex loss: 0.279371  [   64/   89]
per-ex loss: 0.368586  [   65/   89]
per-ex loss: 0.542580  [   66/   89]
per-ex loss: 0.538283  [   67/   89]
per-ex loss: 0.551161  [   68/   89]
per-ex loss: 0.337647  [   69/   89]
per-ex loss: 0.335480  [   70/   89]
per-ex loss: 0.442653  [   71/   89]
per-ex loss: 0.722814  [   72/   89]
per-ex loss: 0.562418  [   73/   89]
per-ex loss: 0.563188  [   74/   89]
per-ex loss: 0.509409  [   75/   89]
per-ex loss: 0.340807  [   76/   89]
per-ex loss: 0.554422  [   77/   89]
per-ex loss: 0.999894  [   78/   89]
per-ex loss: 0.630858  [   79/   89]
per-ex loss: 0.337597  [   80/   89]
per-ex loss: 0.460140  [   81/   89]
per-ex loss: 0.374299  [   82/   89]
per-ex loss: 0.465871  [   83/   89]
per-ex loss: 0.562841  [   84/   89]
per-ex loss: 0.480703  [   85/   89]
per-ex loss: 0.494561  [   86/   89]
per-ex loss: 0.607405  [   87/   89]
per-ex loss: 0.544899  [   88/   89]
per-ex loss: 0.427782  [   89/   89]
Train Error: Avg loss: 0.44217337
validation Error: 
 Avg loss: 0.54854323 
 F1: 0.484947 
 Precision: 0.540139 
 Recall: 0.439989
 IoU: 0.320086

test Error: 
 Avg loss: 0.51385208 
 F1: 0.538617 
 Precision: 0.544538 
 Recall: 0.532823
 IoU: 0.368567

We have finished training iteration 90
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_88_.pth
per-ex loss: 0.352434  [    1/   89]
per-ex loss: 0.435312  [    2/   89]
per-ex loss: 0.608096  [    3/   89]
per-ex loss: 0.354892  [    4/   89]
per-ex loss: 0.282870  [    5/   89]
per-ex loss: 0.386854  [    6/   89]
per-ex loss: 0.502993  [    7/   89]
per-ex loss: 0.383436  [    8/   89]
per-ex loss: 0.340960  [    9/   89]
per-ex loss: 0.631428  [   10/   89]
per-ex loss: 0.521944  [   11/   89]
per-ex loss: 0.402678  [   12/   89]
per-ex loss: 0.539216  [   13/   89]
per-ex loss: 0.584704  [   14/   89]
per-ex loss: 0.341929  [   15/   89]
per-ex loss: 0.386652  [   16/   89]
per-ex loss: 0.325175  [   17/   89]
per-ex loss: 0.310166  [   18/   89]
per-ex loss: 0.367736  [   19/   89]
per-ex loss: 0.361046  [   20/   89]
per-ex loss: 0.336225  [   21/   89]
per-ex loss: 0.566945  [   22/   89]
per-ex loss: 0.331883  [   23/   89]
per-ex loss: 0.551047  [   24/   89]
per-ex loss: 0.436775  [   25/   89]
per-ex loss: 0.520279  [   26/   89]
per-ex loss: 0.180262  [   27/   89]
per-ex loss: 0.537451  [   28/   89]
per-ex loss: 0.310147  [   29/   89]
per-ex loss: 0.322163  [   30/   89]
per-ex loss: 0.431910  [   31/   89]
per-ex loss: 0.444900  [   32/   89]
per-ex loss: 0.410856  [   33/   89]
per-ex loss: 0.502871  [   34/   89]
per-ex loss: 0.363216  [   35/   89]
per-ex loss: 0.457559  [   36/   89]
per-ex loss: 0.388770  [   37/   89]
per-ex loss: 0.622103  [   38/   89]
per-ex loss: 0.550787  [   39/   89]
per-ex loss: 0.422408  [   40/   89]
per-ex loss: 0.484828  [   41/   89]
per-ex loss: 0.477491  [   42/   89]
per-ex loss: 0.362288  [   43/   89]
per-ex loss: 0.604605  [   44/   89]
per-ex loss: 0.518965  [   45/   89]
per-ex loss: 0.694311  [   46/   89]
per-ex loss: 0.320630  [   47/   89]
per-ex loss: 0.499689  [   48/   89]
per-ex loss: 0.397596  [   49/   89]
per-ex loss: 0.442664  [   50/   89]
per-ex loss: 0.312457  [   51/   89]
per-ex loss: 0.565044  [   52/   89]
per-ex loss: 0.395606  [   53/   89]
per-ex loss: 0.560696  [   54/   89]
per-ex loss: 0.468200  [   55/   89]
per-ex loss: 0.378255  [   56/   89]
per-ex loss: 0.326125  [   57/   89]
per-ex loss: 0.353114  [   58/   89]
per-ex loss: 0.280643  [   59/   89]
per-ex loss: 0.329279  [   60/   89]
per-ex loss: 0.435222  [   61/   89]
per-ex loss: 0.469982  [   62/   89]
per-ex loss: 0.410024  [   63/   89]
per-ex loss: 0.319621  [   64/   89]
per-ex loss: 0.421315  [   65/   89]
per-ex loss: 0.527631  [   66/   89]
per-ex loss: 0.259836  [   67/   89]
per-ex loss: 0.353573  [   68/   89]
per-ex loss: 0.317032  [   69/   89]
per-ex loss: 0.368474  [   70/   89]
per-ex loss: 0.525005  [   71/   89]
per-ex loss: 0.498991  [   72/   89]
per-ex loss: 0.313121  [   73/   89]
per-ex loss: 0.413395  [   74/   89]
per-ex loss: 0.333658  [   75/   89]
per-ex loss: 0.377475  [   76/   89]
per-ex loss: 0.444206  [   77/   89]
per-ex loss: 0.592599  [   78/   89]
per-ex loss: 0.999879  [   79/   89]
per-ex loss: 0.610264  [   80/   89]
per-ex loss: 0.635495  [   81/   89]
per-ex loss: 0.349009  [   82/   89]
per-ex loss: 0.583762  [   83/   89]
per-ex loss: 0.384941  [   84/   89]
per-ex loss: 0.300294  [   85/   89]
per-ex loss: 0.398892  [   86/   89]
per-ex loss: 0.402383  [   87/   89]
per-ex loss: 0.408755  [   88/   89]
per-ex loss: 0.350364  [   89/   89]
Train Error: Avg loss: 0.43468271
validation Error: 
 Avg loss: 0.54804186 
 F1: 0.481684 
 Precision: 0.542344 
 Recall: 0.433228
 IoU: 0.317248

test Error: 
 Avg loss: 0.51249783 
 F1: 0.540959 
 Precision: 0.546757 
 Recall: 0.535283
 IoU: 0.370763

We have finished training iteration 91
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_89_.pth
per-ex loss: 0.366368  [    1/   89]
per-ex loss: 0.359846  [    2/   89]
per-ex loss: 0.600928  [    3/   89]
per-ex loss: 0.298906  [    4/   89]
per-ex loss: 0.330439  [    5/   89]
per-ex loss: 0.605291  [    6/   89]
per-ex loss: 0.528851  [    7/   89]
per-ex loss: 0.480781  [    8/   89]
per-ex loss: 0.368540  [    9/   89]
per-ex loss: 0.263727  [   10/   89]
per-ex loss: 0.543823  [   11/   89]
per-ex loss: 0.366084  [   12/   89]
per-ex loss: 0.565446  [   13/   89]
per-ex loss: 0.328708  [   14/   89]
per-ex loss: 0.394752  [   15/   89]
per-ex loss: 0.370166  [   16/   89]
per-ex loss: 0.547743  [   17/   89]
per-ex loss: 0.350448  [   18/   89]
per-ex loss: 0.391598  [   19/   89]
per-ex loss: 0.417834  [   20/   89]
per-ex loss: 0.621217  [   21/   89]
per-ex loss: 0.357020  [   22/   89]
per-ex loss: 0.380633  [   23/   89]
per-ex loss: 0.569163  [   24/   89]
per-ex loss: 0.442821  [   25/   89]
per-ex loss: 0.354796  [   26/   89]
per-ex loss: 0.404907  [   27/   89]
per-ex loss: 0.338733  [   28/   89]
per-ex loss: 0.314734  [   29/   89]
per-ex loss: 0.325626  [   30/   89]
per-ex loss: 0.387012  [   31/   89]
per-ex loss: 0.327631  [   32/   89]
per-ex loss: 0.314430  [   33/   89]
per-ex loss: 0.534316  [   34/   89]
per-ex loss: 0.325007  [   35/   89]
per-ex loss: 0.569889  [   36/   89]
per-ex loss: 0.334500  [   37/   89]
per-ex loss: 0.349734  [   38/   89]
per-ex loss: 0.536340  [   39/   89]
per-ex loss: 0.577916  [   40/   89]
per-ex loss: 0.559046  [   41/   89]
per-ex loss: 0.500354  [   42/   89]
per-ex loss: 0.441706  [   43/   89]
per-ex loss: 0.508424  [   44/   89]
per-ex loss: 0.681149  [   45/   89]
per-ex loss: 0.348167  [   46/   89]
per-ex loss: 0.460395  [   47/   89]
per-ex loss: 0.292944  [   48/   89]
per-ex loss: 0.413046  [   49/   89]
per-ex loss: 0.622504  [   50/   89]
per-ex loss: 0.457368  [   51/   89]
per-ex loss: 0.602387  [   52/   89]
per-ex loss: 0.426906  [   53/   89]
per-ex loss: 0.354489  [   54/   89]
per-ex loss: 0.335666  [   55/   89]
per-ex loss: 0.471694  [   56/   89]
per-ex loss: 0.457266  [   57/   89]
per-ex loss: 0.279723  [   58/   89]
per-ex loss: 0.373620  [   59/   89]
per-ex loss: 0.356703  [   60/   89]
per-ex loss: 0.596019  [   61/   89]
per-ex loss: 0.536924  [   62/   89]
per-ex loss: 0.318418  [   63/   89]
per-ex loss: 0.999930  [   64/   89]
per-ex loss: 0.509897  [   65/   89]
per-ex loss: 0.683960  [   66/   89]
per-ex loss: 0.518306  [   67/   89]
per-ex loss: 0.282305  [   68/   89]
per-ex loss: 0.402414  [   69/   89]
per-ex loss: 0.546390  [   70/   89]
per-ex loss: 0.475855  [   71/   89]
per-ex loss: 0.402947  [   72/   89]
per-ex loss: 0.362424  [   73/   89]
per-ex loss: 0.378807  [   74/   89]
per-ex loss: 0.581509  [   75/   89]
per-ex loss: 0.310982  [   76/   89]
per-ex loss: 0.395904  [   77/   89]
per-ex loss: 0.374303  [   78/   89]
per-ex loss: 0.545825  [   79/   89]
per-ex loss: 0.354592  [   80/   89]
per-ex loss: 0.199948  [   81/   89]
per-ex loss: 0.326780  [   82/   89]
per-ex loss: 0.358283  [   83/   89]
per-ex loss: 0.512344  [   84/   89]
per-ex loss: 0.387550  [   85/   89]
per-ex loss: 0.258825  [   86/   89]
per-ex loss: 0.421384  [   87/   89]
per-ex loss: 0.500563  [   88/   89]
per-ex loss: 0.519960  [   89/   89]
Train Error: Avg loss: 0.43655740
validation Error: 
 Avg loss: 0.54103664 
 F1: 0.479594 
 Precision: 0.527485 
 Recall: 0.439675
 IoU: 0.315438

test Error: 
 Avg loss: 0.48944604 
 F1: 0.558058 
 Precision: 0.623092 
 Recall: 0.505316
 IoU: 0.387018

We have finished training iteration 92
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_90_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.317395  [    1/   89]
per-ex loss: 0.999916  [    2/   89]
per-ex loss: 0.378036  [    3/   89]
per-ex loss: 0.509795  [    4/   89]
per-ex loss: 0.314945  [    5/   89]
per-ex loss: 0.624402  [    6/   89]
per-ex loss: 0.400551  [    7/   89]
per-ex loss: 0.349387  [    8/   89]
per-ex loss: 0.514275  [    9/   89]
per-ex loss: 0.463511  [   10/   89]
per-ex loss: 0.557681  [   11/   89]
per-ex loss: 0.537305  [   12/   89]
per-ex loss: 0.328013  [   13/   89]
per-ex loss: 0.321860  [   14/   89]
per-ex loss: 0.375421  [   15/   89]
per-ex loss: 0.470262  [   16/   89]
per-ex loss: 0.516710  [   17/   89]
per-ex loss: 0.372808  [   18/   89]
per-ex loss: 0.557123  [   19/   89]
per-ex loss: 0.359796  [   20/   89]
per-ex loss: 0.334069  [   21/   89]
per-ex loss: 0.325227  [   22/   89]
per-ex loss: 0.623827  [   23/   89]
per-ex loss: 0.259099  [   24/   89]
per-ex loss: 0.339594  [   25/   89]
per-ex loss: 0.341096  [   26/   89]
per-ex loss: 0.554403  [   27/   89]
per-ex loss: 0.381031  [   28/   89]
per-ex loss: 0.509810  [   29/   89]
per-ex loss: 0.490854  [   30/   89]
per-ex loss: 0.293338  [   31/   89]
per-ex loss: 0.415037  [   32/   89]
per-ex loss: 0.600019  [   33/   89]
per-ex loss: 0.508566  [   34/   89]
per-ex loss: 0.262801  [   35/   89]
per-ex loss: 0.448679  [   36/   89]
per-ex loss: 0.425656  [   37/   89]
per-ex loss: 0.296448  [   38/   89]
per-ex loss: 0.462003  [   39/   89]
per-ex loss: 0.306324  [   40/   89]
per-ex loss: 0.369576  [   41/   89]
per-ex loss: 0.394130  [   42/   89]
per-ex loss: 0.361935  [   43/   89]
per-ex loss: 0.503536  [   44/   89]
per-ex loss: 0.364593  [   45/   89]
per-ex loss: 0.356628  [   46/   89]
per-ex loss: 0.606980  [   47/   89]
per-ex loss: 0.403418  [   48/   89]
per-ex loss: 0.459702  [   49/   89]
per-ex loss: 0.623902  [   50/   89]
per-ex loss: 0.360919  [   51/   89]
per-ex loss: 0.265234  [   52/   89]
per-ex loss: 0.492126  [   53/   89]
per-ex loss: 0.348093  [   54/   89]
per-ex loss: 0.538708  [   55/   89]
per-ex loss: 0.357818  [   56/   89]
per-ex loss: 0.291723  [   57/   89]
per-ex loss: 0.390452  [   58/   89]
per-ex loss: 0.542740  [   59/   89]
per-ex loss: 0.328910  [   60/   89]
per-ex loss: 0.284081  [   61/   89]
per-ex loss: 0.443068  [   62/   89]
per-ex loss: 0.534875  [   63/   89]
per-ex loss: 0.514857  [   64/   89]
per-ex loss: 0.486996  [   65/   89]
per-ex loss: 0.536017  [   66/   89]
per-ex loss: 0.469063  [   67/   89]
per-ex loss: 0.269653  [   68/   89]
per-ex loss: 0.299125  [   69/   89]
per-ex loss: 0.421756  [   70/   89]
per-ex loss: 0.550196  [   71/   89]
per-ex loss: 0.378083  [   72/   89]
per-ex loss: 0.507068  [   73/   89]
per-ex loss: 0.340457  [   74/   89]
per-ex loss: 0.603648  [   75/   89]
per-ex loss: 0.507117  [   76/   89]
per-ex loss: 0.324422  [   77/   89]
per-ex loss: 0.329652  [   78/   89]
per-ex loss: 0.351653  [   79/   89]
per-ex loss: 0.434920  [   80/   89]
per-ex loss: 0.338996  [   81/   89]
per-ex loss: 0.428996  [   82/   89]
per-ex loss: 0.529125  [   83/   89]
per-ex loss: 0.356139  [   84/   89]
per-ex loss: 0.468643  [   85/   89]
per-ex loss: 0.500375  [   86/   89]
per-ex loss: 0.652887  [   87/   89]
per-ex loss: 0.425409  [   88/   89]
per-ex loss: 0.318051  [   89/   89]
Train Error: Avg loss: 0.43161262
validation Error: 
 Avg loss: 0.53471739 
 F1: 0.497201 
 Precision: 0.534289 
 Recall: 0.464928
 IoU: 0.330850

test Error: 
 Avg loss: 0.48234052 
 F1: 0.566339 
 Precision: 0.598667 
 Recall: 0.537324
 IoU: 0.395030

We have finished training iteration 93
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_91_.pth
per-ex loss: 0.525371  [    1/   89]
per-ex loss: 0.401286  [    2/   89]
per-ex loss: 0.533395  [    3/   89]
per-ex loss: 0.534096  [    4/   89]
per-ex loss: 0.256859  [    5/   89]
per-ex loss: 0.321487  [    6/   89]
per-ex loss: 0.543810  [    7/   89]
per-ex loss: 0.445622  [    8/   89]
per-ex loss: 0.371277  [    9/   89]
per-ex loss: 0.434088  [   10/   89]
per-ex loss: 0.292504  [   11/   89]
per-ex loss: 0.595544  [   12/   89]
per-ex loss: 0.314368  [   13/   89]
per-ex loss: 0.350425  [   14/   89]
per-ex loss: 0.337609  [   15/   89]
per-ex loss: 0.472864  [   16/   89]
per-ex loss: 0.464587  [   17/   89]
per-ex loss: 0.502417  [   18/   89]
per-ex loss: 0.344709  [   19/   89]
per-ex loss: 0.589472  [   20/   89]
per-ex loss: 0.359295  [   21/   89]
per-ex loss: 0.319165  [   22/   89]
per-ex loss: 0.522305  [   23/   89]
per-ex loss: 0.542548  [   24/   89]
per-ex loss: 0.477173  [   25/   89]
per-ex loss: 0.295805  [   26/   89]
per-ex loss: 0.337966  [   27/   89]
per-ex loss: 0.351008  [   28/   89]
per-ex loss: 0.276118  [   29/   89]
per-ex loss: 0.267406  [   30/   89]
per-ex loss: 0.562899  [   31/   89]
per-ex loss: 0.302220  [   32/   89]
per-ex loss: 0.488102  [   33/   89]
per-ex loss: 0.531194  [   34/   89]
per-ex loss: 0.406150  [   35/   89]
per-ex loss: 0.394964  [   36/   89]
per-ex loss: 0.364628  [   37/   89]
per-ex loss: 0.370007  [   38/   89]
per-ex loss: 0.524170  [   39/   89]
per-ex loss: 0.317372  [   40/   89]
per-ex loss: 0.377328  [   41/   89]
per-ex loss: 0.351822  [   42/   89]
per-ex loss: 0.344678  [   43/   89]
per-ex loss: 0.543456  [   44/   89]
per-ex loss: 0.390735  [   45/   89]
per-ex loss: 0.594095  [   46/   89]
per-ex loss: 0.999951  [   47/   89]
per-ex loss: 0.382078  [   48/   89]
per-ex loss: 0.406492  [   49/   89]
per-ex loss: 0.336498  [   50/   89]
per-ex loss: 0.276671  [   51/   89]
per-ex loss: 0.504186  [   52/   89]
per-ex loss: 0.520455  [   53/   89]
per-ex loss: 0.325690  [   54/   89]
per-ex loss: 0.496815  [   55/   89]
per-ex loss: 0.473568  [   56/   89]
per-ex loss: 0.502217  [   57/   89]
per-ex loss: 0.460027  [   58/   89]
per-ex loss: 0.347463  [   59/   89]
per-ex loss: 0.490932  [   60/   89]
per-ex loss: 0.443886  [   61/   89]
per-ex loss: 0.392596  [   62/   89]
per-ex loss: 0.328058  [   63/   89]
per-ex loss: 0.457732  [   64/   89]
per-ex loss: 0.322076  [   65/   89]
per-ex loss: 0.251112  [   66/   89]
per-ex loss: 0.709793  [   67/   89]
per-ex loss: 0.643505  [   68/   89]
per-ex loss: 0.362122  [   69/   89]
per-ex loss: 0.382877  [   70/   89]
per-ex loss: 0.398429  [   71/   89]
per-ex loss: 0.466441  [   72/   89]
per-ex loss: 0.369340  [   73/   89]
per-ex loss: 0.443018  [   74/   89]
per-ex loss: 0.556006  [   75/   89]
per-ex loss: 0.370416  [   76/   89]
per-ex loss: 0.541708  [   77/   89]
per-ex loss: 0.371979  [   78/   89]
per-ex loss: 0.331576  [   79/   89]
per-ex loss: 0.564120  [   80/   89]
per-ex loss: 0.513042  [   81/   89]
per-ex loss: 0.337155  [   82/   89]
per-ex loss: 0.421613  [   83/   89]
per-ex loss: 0.549194  [   84/   89]
per-ex loss: 0.536618  [   85/   89]
per-ex loss: 0.301076  [   86/   89]
per-ex loss: 0.378915  [   87/   89]
per-ex loss: 0.619844  [   88/   89]
per-ex loss: 0.353584  [   89/   89]
Train Error: Avg loss: 0.43241877
validation Error: 
 Avg loss: 0.53683611 
 F1: 0.491564 
 Precision: 0.564509 
 Recall: 0.435314
 IoU: 0.325877

test Error: 
 Avg loss: 0.48222552 
 F1: 0.567554 
 Precision: 0.626332 
 Recall: 0.518862
 IoU: 0.396214

We have finished training iteration 94
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_92_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.403113  [    1/   89]
per-ex loss: 0.535861  [    2/   89]
per-ex loss: 0.444077  [    3/   89]
per-ex loss: 0.363085  [    4/   89]
per-ex loss: 0.298203  [    5/   89]
per-ex loss: 0.391682  [    6/   89]
per-ex loss: 0.999934  [    7/   89]
per-ex loss: 0.312452  [    8/   89]
per-ex loss: 0.308250  [    9/   89]
per-ex loss: 0.397727  [   10/   89]
per-ex loss: 0.311748  [   11/   89]
per-ex loss: 0.376363  [   12/   89]
per-ex loss: 0.241719  [   13/   89]
per-ex loss: 0.452109  [   14/   89]
per-ex loss: 0.321976  [   15/   89]
per-ex loss: 0.325113  [   16/   89]
per-ex loss: 0.315933  [   17/   89]
per-ex loss: 0.337963  [   18/   89]
per-ex loss: 0.382662  [   19/   89]
per-ex loss: 0.485735  [   20/   89]
per-ex loss: 0.404355  [   21/   89]
per-ex loss: 0.323799  [   22/   89]
per-ex loss: 0.600200  [   23/   89]
per-ex loss: 0.239117  [   24/   89]
per-ex loss: 0.552170  [   25/   89]
per-ex loss: 0.349778  [   26/   89]
per-ex loss: 0.485155  [   27/   89]
per-ex loss: 0.346209  [   28/   89]
per-ex loss: 0.586621  [   29/   89]
per-ex loss: 0.245401  [   30/   89]
per-ex loss: 0.418967  [   31/   89]
per-ex loss: 0.535356  [   32/   89]
per-ex loss: 0.346840  [   33/   89]
per-ex loss: 0.280063  [   34/   89]
per-ex loss: 0.340491  [   35/   89]
per-ex loss: 0.516694  [   36/   89]
per-ex loss: 0.473769  [   37/   89]
per-ex loss: 0.337238  [   38/   89]
per-ex loss: 0.554677  [   39/   89]
per-ex loss: 0.342579  [   40/   89]
per-ex loss: 0.553307  [   41/   89]
per-ex loss: 0.381018  [   42/   89]
per-ex loss: 0.529712  [   43/   89]
per-ex loss: 0.297636  [   44/   89]
per-ex loss: 0.332813  [   45/   89]
per-ex loss: 0.534943  [   46/   89]
per-ex loss: 0.426984  [   47/   89]
per-ex loss: 0.370867  [   48/   89]
per-ex loss: 0.396926  [   49/   89]
per-ex loss: 0.630324  [   50/   89]
per-ex loss: 0.346675  [   51/   89]
per-ex loss: 0.419286  [   52/   89]
per-ex loss: 0.314719  [   53/   89]
per-ex loss: 0.408039  [   54/   89]
per-ex loss: 0.333132  [   55/   89]
per-ex loss: 0.530362  [   56/   89]
per-ex loss: 0.313750  [   57/   89]
per-ex loss: 0.548053  [   58/   89]
per-ex loss: 0.550772  [   59/   89]
per-ex loss: 0.407199  [   60/   89]
per-ex loss: 0.527680  [   61/   89]
per-ex loss: 0.617713  [   62/   89]
per-ex loss: 0.429531  [   63/   89]
per-ex loss: 0.397839  [   64/   89]
per-ex loss: 0.637305  [   65/   89]
per-ex loss: 0.373822  [   66/   89]
per-ex loss: 0.375570  [   67/   89]
per-ex loss: 0.406140  [   68/   89]
per-ex loss: 0.600031  [   69/   89]
per-ex loss: 0.402848  [   70/   89]
per-ex loss: 0.506794  [   71/   89]
per-ex loss: 0.277651  [   72/   89]
per-ex loss: 0.348558  [   73/   89]
per-ex loss: 0.364252  [   74/   89]
per-ex loss: 0.522132  [   75/   89]
per-ex loss: 0.550454  [   76/   89]
per-ex loss: 0.519913  [   77/   89]
per-ex loss: 0.346766  [   78/   89]
per-ex loss: 0.461500  [   79/   89]
per-ex loss: 0.529599  [   80/   89]
per-ex loss: 0.458858  [   81/   89]
per-ex loss: 0.371362  [   82/   89]
per-ex loss: 0.311242  [   83/   89]
per-ex loss: 0.426342  [   84/   89]
per-ex loss: 0.572212  [   85/   89]
per-ex loss: 0.647988  [   86/   89]
per-ex loss: 0.205711  [   87/   89]
per-ex loss: 0.515651  [   88/   89]
per-ex loss: 0.396163  [   89/   89]
Train Error: Avg loss: 0.42711598
validation Error: 
 Avg loss: 0.53225480 
 F1: 0.495751 
 Precision: 0.529679 
 Recall: 0.465908
 IoU: 0.329567

test Error: 
 Avg loss: 0.47759692 
 F1: 0.569335 
 Precision: 0.592559 
 Recall: 0.547862
 IoU: 0.397951

We have finished training iteration 95
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_93_.pth
per-ex loss: 0.317391  [    1/   89]
per-ex loss: 0.534847  [    2/   89]
per-ex loss: 0.493316  [    3/   89]
per-ex loss: 0.428490  [    4/   89]
per-ex loss: 0.490716  [    5/   89]
per-ex loss: 0.419184  [    6/   89]
per-ex loss: 0.639635  [    7/   89]
per-ex loss: 0.513970  [    8/   89]
per-ex loss: 0.369953  [    9/   89]
per-ex loss: 0.552989  [   10/   89]
per-ex loss: 0.297193  [   11/   89]
per-ex loss: 0.466723  [   12/   89]
per-ex loss: 0.465635  [   13/   89]
per-ex loss: 0.576238  [   14/   89]
per-ex loss: 0.351392  [   15/   89]
per-ex loss: 0.381030  [   16/   89]
per-ex loss: 0.322147  [   17/   89]
per-ex loss: 0.301282  [   18/   89]
per-ex loss: 0.386619  [   19/   89]
per-ex loss: 0.630530  [   20/   89]
per-ex loss: 0.409059  [   21/   89]
per-ex loss: 0.346798  [   22/   89]
per-ex loss: 0.504621  [   23/   89]
per-ex loss: 0.267819  [   24/   89]
per-ex loss: 0.359614  [   25/   89]
per-ex loss: 0.350765  [   26/   89]
per-ex loss: 0.353805  [   27/   89]
per-ex loss: 0.555771  [   28/   89]
per-ex loss: 0.464928  [   29/   89]
per-ex loss: 0.368080  [   30/   89]
per-ex loss: 0.411447  [   31/   89]
per-ex loss: 0.354708  [   32/   89]
per-ex loss: 0.343194  [   33/   89]
per-ex loss: 0.527579  [   34/   89]
per-ex loss: 0.638488  [   35/   89]
per-ex loss: 0.478284  [   36/   89]
per-ex loss: 0.327225  [   37/   89]
per-ex loss: 0.509257  [   38/   89]
per-ex loss: 0.599856  [   39/   89]
per-ex loss: 0.290420  [   40/   89]
per-ex loss: 0.999901  [   41/   89]
per-ex loss: 0.553729  [   42/   89]
per-ex loss: 0.353822  [   43/   89]
per-ex loss: 0.448962  [   44/   89]
per-ex loss: 0.460902  [   45/   89]
per-ex loss: 0.325431  [   46/   89]
per-ex loss: 0.408863  [   47/   89]
per-ex loss: 0.389364  [   48/   89]
per-ex loss: 0.452545  [   49/   89]
per-ex loss: 0.372179  [   50/   89]
per-ex loss: 0.379255  [   51/   89]
per-ex loss: 0.327263  [   52/   89]
per-ex loss: 0.397342  [   53/   89]
per-ex loss: 0.550646  [   54/   89]
per-ex loss: 0.383919  [   55/   89]
per-ex loss: 0.301134  [   56/   89]
per-ex loss: 0.369575  [   57/   89]
per-ex loss: 0.394517  [   58/   89]
per-ex loss: 0.300487  [   59/   89]
per-ex loss: 0.433213  [   60/   89]
per-ex loss: 0.375313  [   61/   89]
per-ex loss: 0.676964  [   62/   89]
per-ex loss: 0.410239  [   63/   89]
per-ex loss: 0.385983  [   64/   89]
per-ex loss: 0.336137  [   65/   89]
per-ex loss: 0.197554  [   66/   89]
per-ex loss: 0.501830  [   67/   89]
per-ex loss: 0.314952  [   68/   89]
per-ex loss: 0.338872  [   69/   89]
per-ex loss: 0.331702  [   70/   89]
per-ex loss: 0.380792  [   71/   89]
per-ex loss: 0.484398  [   72/   89]
per-ex loss: 0.519774  [   73/   89]
per-ex loss: 0.589997  [   74/   89]
per-ex loss: 0.643374  [   75/   89]
per-ex loss: 0.538236  [   76/   89]
per-ex loss: 0.336542  [   77/   89]
per-ex loss: 0.519246  [   78/   89]
per-ex loss: 0.410004  [   79/   89]
per-ex loss: 0.343255  [   80/   89]
per-ex loss: 0.298353  [   81/   89]
per-ex loss: 0.381075  [   82/   89]
per-ex loss: 0.336371  [   83/   89]
per-ex loss: 0.614455  [   84/   89]
per-ex loss: 0.418123  [   85/   89]
per-ex loss: 0.478112  [   86/   89]
per-ex loss: 0.297702  [   87/   89]
per-ex loss: 0.363276  [   88/   89]
per-ex loss: 0.508057  [   89/   89]
Train Error: Avg loss: 0.43072737
validation Error: 
 Avg loss: 0.53557359 
 F1: 0.486490 
 Precision: 0.510121 
 Recall: 0.464951
 IoU: 0.321431

test Error: 
 Avg loss: 0.47980661 
 F1: 0.565434 
 Precision: 0.575697 
 Recall: 0.555530
 IoU: 0.394150

We have finished training iteration 96
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_94_.pth
per-ex loss: 0.331244  [    1/   89]
per-ex loss: 0.478752  [    2/   89]
per-ex loss: 0.319662  [    3/   89]
per-ex loss: 0.519940  [    4/   89]
per-ex loss: 0.370289  [    5/   89]
per-ex loss: 0.343499  [    6/   89]
per-ex loss: 0.406188  [    7/   89]
per-ex loss: 0.331262  [    8/   89]
per-ex loss: 0.400422  [    9/   89]
per-ex loss: 0.302035  [   10/   89]
per-ex loss: 0.350735  [   11/   89]
per-ex loss: 0.487456  [   12/   89]
per-ex loss: 0.408101  [   13/   89]
per-ex loss: 0.413106  [   14/   89]
per-ex loss: 0.536667  [   15/   89]
per-ex loss: 0.449493  [   16/   89]
per-ex loss: 0.317861  [   17/   89]
per-ex loss: 0.230176  [   18/   89]
per-ex loss: 0.383660  [   19/   89]
per-ex loss: 0.578042  [   20/   89]
per-ex loss: 0.627648  [   21/   89]
per-ex loss: 0.407975  [   22/   89]
per-ex loss: 0.486366  [   23/   89]
per-ex loss: 0.415530  [   24/   89]
per-ex loss: 0.314103  [   25/   89]
per-ex loss: 0.334508  [   26/   89]
per-ex loss: 0.408290  [   27/   89]
per-ex loss: 0.630242  [   28/   89]
per-ex loss: 0.291097  [   29/   89]
per-ex loss: 0.494694  [   30/   89]
per-ex loss: 0.384945  [   31/   89]
per-ex loss: 0.295018  [   32/   89]
per-ex loss: 0.365724  [   33/   89]
per-ex loss: 0.628935  [   34/   89]
per-ex loss: 0.530851  [   35/   89]
per-ex loss: 0.454880  [   36/   89]
per-ex loss: 0.261142  [   37/   89]
per-ex loss: 0.329367  [   38/   89]
per-ex loss: 0.289302  [   39/   89]
per-ex loss: 0.530409  [   40/   89]
per-ex loss: 0.371051  [   41/   89]
per-ex loss: 0.559663  [   42/   89]
per-ex loss: 0.524394  [   43/   89]
per-ex loss: 0.518450  [   44/   89]
per-ex loss: 0.310906  [   45/   89]
per-ex loss: 0.514088  [   46/   89]
per-ex loss: 0.291101  [   47/   89]
per-ex loss: 0.512779  [   48/   89]
per-ex loss: 0.396942  [   49/   89]
per-ex loss: 0.473908  [   50/   89]
per-ex loss: 0.368065  [   51/   89]
per-ex loss: 0.293607  [   52/   89]
per-ex loss: 0.310652  [   53/   89]
per-ex loss: 0.295537  [   54/   89]
per-ex loss: 0.370076  [   55/   89]
per-ex loss: 0.999903  [   56/   89]
per-ex loss: 0.426479  [   57/   89]
per-ex loss: 0.307872  [   58/   89]
per-ex loss: 0.386676  [   59/   89]
per-ex loss: 0.549259  [   60/   89]
per-ex loss: 0.326570  [   61/   89]
per-ex loss: 0.436938  [   62/   89]
per-ex loss: 0.461951  [   63/   89]
per-ex loss: 0.418751  [   64/   89]
per-ex loss: 0.316713  [   65/   89]
per-ex loss: 0.350953  [   66/   89]
per-ex loss: 0.590683  [   67/   89]
per-ex loss: 0.567901  [   68/   89]
per-ex loss: 0.630596  [   69/   89]
per-ex loss: 0.546992  [   70/   89]
per-ex loss: 0.329730  [   71/   89]
per-ex loss: 0.337563  [   72/   89]
per-ex loss: 0.405377  [   73/   89]
per-ex loss: 0.463601  [   74/   89]
per-ex loss: 0.512350  [   75/   89]
per-ex loss: 0.689636  [   76/   89]
per-ex loss: 0.612184  [   77/   89]
per-ex loss: 0.620603  [   78/   89]
per-ex loss: 0.327538  [   79/   89]
per-ex loss: 0.368059  [   80/   89]
per-ex loss: 0.359384  [   81/   89]
per-ex loss: 0.544390  [   82/   89]
per-ex loss: 0.466702  [   83/   89]
per-ex loss: 0.371264  [   84/   89]
per-ex loss: 0.309127  [   85/   89]
per-ex loss: 0.522590  [   86/   89]
per-ex loss: 0.445383  [   87/   89]
per-ex loss: 0.321531  [   88/   89]
per-ex loss: 0.390569  [   89/   89]
Train Error: Avg loss: 0.42996240
validation Error: 
 Avg loss: 0.52733630 
 F1: 0.500848 
 Precision: 0.559201 
 Recall: 0.453523
 IoU: 0.334088

test Error: 
 Avg loss: 0.48211775 
 F1: 0.568098 
 Precision: 0.602263 
 Recall: 0.537602
 IoU: 0.396744

We have finished training iteration 97
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_95_.pth
per-ex loss: 0.347707  [    1/   89]
per-ex loss: 0.492986  [    2/   89]
per-ex loss: 0.490659  [    3/   89]
per-ex loss: 0.324962  [    4/   89]
per-ex loss: 0.417703  [    5/   89]
per-ex loss: 0.511032  [    6/   89]
per-ex loss: 0.482640  [    7/   89]
per-ex loss: 0.297121  [    8/   89]
per-ex loss: 0.625701  [    9/   89]
per-ex loss: 0.333429  [   10/   89]
per-ex loss: 0.561494  [   11/   89]
per-ex loss: 0.361748  [   12/   89]
per-ex loss: 0.311196  [   13/   89]
per-ex loss: 0.504851  [   14/   89]
per-ex loss: 0.291144  [   15/   89]
per-ex loss: 0.431803  [   16/   89]
per-ex loss: 0.369990  [   17/   89]
per-ex loss: 0.355642  [   18/   89]
per-ex loss: 0.386599  [   19/   89]
per-ex loss: 0.282921  [   20/   89]
per-ex loss: 0.367050  [   21/   89]
per-ex loss: 0.328184  [   22/   89]
per-ex loss: 0.357806  [   23/   89]
per-ex loss: 0.395562  [   24/   89]
per-ex loss: 0.280433  [   25/   89]
per-ex loss: 0.317984  [   26/   89]
per-ex loss: 0.465835  [   27/   89]
per-ex loss: 0.317152  [   28/   89]
per-ex loss: 0.473392  [   29/   89]
per-ex loss: 0.513384  [   30/   89]
per-ex loss: 0.494414  [   31/   89]
per-ex loss: 0.354750  [   32/   89]
per-ex loss: 0.397246  [   33/   89]
per-ex loss: 0.521011  [   34/   89]
per-ex loss: 0.497079  [   35/   89]
per-ex loss: 0.397152  [   36/   89]
per-ex loss: 0.345559  [   37/   89]
per-ex loss: 0.618011  [   38/   89]
per-ex loss: 0.351874  [   39/   89]
per-ex loss: 0.433757  [   40/   89]
per-ex loss: 0.549269  [   41/   89]
per-ex loss: 0.500950  [   42/   89]
per-ex loss: 0.510207  [   43/   89]
per-ex loss: 0.309750  [   44/   89]
per-ex loss: 0.400943  [   45/   89]
per-ex loss: 0.326204  [   46/   89]
per-ex loss: 0.355052  [   47/   89]
per-ex loss: 0.357269  [   48/   89]
per-ex loss: 0.346118  [   49/   89]
per-ex loss: 0.347489  [   50/   89]
per-ex loss: 0.544305  [   51/   89]
per-ex loss: 0.529470  [   52/   89]
per-ex loss: 0.593597  [   53/   89]
per-ex loss: 0.999879  [   54/   89]
per-ex loss: 0.362432  [   55/   89]
per-ex loss: 0.561904  [   56/   89]
per-ex loss: 0.282184  [   57/   89]
per-ex loss: 0.514713  [   58/   89]
per-ex loss: 0.529383  [   59/   89]
per-ex loss: 0.310156  [   60/   89]
per-ex loss: 0.361126  [   61/   89]
per-ex loss: 0.630699  [   62/   89]
per-ex loss: 0.356074  [   63/   89]
per-ex loss: 0.366377  [   64/   89]
per-ex loss: 0.367402  [   65/   89]
per-ex loss: 0.537344  [   66/   89]
per-ex loss: 0.284427  [   67/   89]
per-ex loss: 0.422759  [   68/   89]
per-ex loss: 0.269193  [   69/   89]
per-ex loss: 0.346390  [   70/   89]
per-ex loss: 0.279244  [   71/   89]
per-ex loss: 0.408160  [   72/   89]
per-ex loss: 0.325614  [   73/   89]
per-ex loss: 0.394689  [   74/   89]
per-ex loss: 0.398182  [   75/   89]
per-ex loss: 0.436581  [   76/   89]
per-ex loss: 0.609648  [   77/   89]
per-ex loss: 0.320638  [   78/   89]
per-ex loss: 0.581517  [   79/   89]
per-ex loss: 0.626932  [   80/   89]
per-ex loss: 0.523097  [   81/   89]
per-ex loss: 0.371862  [   82/   89]
per-ex loss: 0.561371  [   83/   89]
per-ex loss: 0.351346  [   84/   89]
per-ex loss: 0.542648  [   85/   89]
per-ex loss: 0.577201  [   86/   89]
per-ex loss: 0.531302  [   87/   89]
per-ex loss: 0.532443  [   88/   89]
per-ex loss: 0.358412  [   89/   89]
Train Error: Avg loss: 0.43048219
validation Error: 
 Avg loss: 0.54520838 
 F1: 0.477232 
 Precision: 0.463829 
 Recall: 0.491431
 IoU: 0.313397

test Error: 
 Avg loss: 0.48016180 
 F1: 0.567179 
 Precision: 0.550483 
 Recall: 0.584919
 IoU: 0.395847

We have finished training iteration 98
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_96_.pth
per-ex loss: 0.324526  [    1/   89]
per-ex loss: 0.385447  [    2/   89]
per-ex loss: 0.287234  [    3/   89]
per-ex loss: 0.557929  [    4/   89]
per-ex loss: 0.454984  [    5/   89]
per-ex loss: 0.319291  [    6/   89]
per-ex loss: 0.315251  [    7/   89]
per-ex loss: 0.512085  [    8/   89]
per-ex loss: 0.417277  [    9/   89]
per-ex loss: 0.267116  [   10/   89]
per-ex loss: 0.276613  [   11/   89]
per-ex loss: 0.310698  [   12/   89]
per-ex loss: 0.510222  [   13/   89]
per-ex loss: 0.347411  [   14/   89]
per-ex loss: 0.358066  [   15/   89]
per-ex loss: 0.585662  [   16/   89]
per-ex loss: 0.506655  [   17/   89]
per-ex loss: 0.387760  [   18/   89]
per-ex loss: 0.233681  [   19/   89]
per-ex loss: 0.337599  [   20/   89]
per-ex loss: 0.363092  [   21/   89]
per-ex loss: 0.313390  [   22/   89]
per-ex loss: 0.333802  [   23/   89]
per-ex loss: 0.656914  [   24/   89]
per-ex loss: 0.423646  [   25/   89]
per-ex loss: 0.358263  [   26/   89]
per-ex loss: 0.391837  [   27/   89]
per-ex loss: 0.543777  [   28/   89]
per-ex loss: 0.305608  [   29/   89]
per-ex loss: 0.594698  [   30/   89]
per-ex loss: 0.553179  [   31/   89]
per-ex loss: 0.526567  [   32/   89]
per-ex loss: 0.381047  [   33/   89]
per-ex loss: 0.226045  [   34/   89]
per-ex loss: 0.321841  [   35/   89]
per-ex loss: 0.999937  [   36/   89]
per-ex loss: 0.331561  [   37/   89]
per-ex loss: 0.320042  [   38/   89]
per-ex loss: 0.372615  [   39/   89]
per-ex loss: 0.436555  [   40/   89]
per-ex loss: 0.276985  [   41/   89]
per-ex loss: 0.625047  [   42/   89]
per-ex loss: 0.414899  [   43/   89]
per-ex loss: 0.373136  [   44/   89]
per-ex loss: 0.329028  [   45/   89]
per-ex loss: 0.537566  [   46/   89]
per-ex loss: 0.469743  [   47/   89]
per-ex loss: 0.440941  [   48/   89]
per-ex loss: 0.491102  [   49/   89]
per-ex loss: 0.494551  [   50/   89]
per-ex loss: 0.480804  [   51/   89]
per-ex loss: 0.436089  [   52/   89]
per-ex loss: 0.335349  [   53/   89]
per-ex loss: 0.496781  [   54/   89]
per-ex loss: 0.403943  [   55/   89]
per-ex loss: 0.373289  [   56/   89]
per-ex loss: 0.473023  [   57/   89]
per-ex loss: 0.405739  [   58/   89]
per-ex loss: 0.298365  [   59/   89]
per-ex loss: 0.350665  [   60/   89]
per-ex loss: 0.347425  [   61/   89]
per-ex loss: 0.418282  [   62/   89]
per-ex loss: 0.620715  [   63/   89]
per-ex loss: 0.319994  [   64/   89]
per-ex loss: 0.360940  [   65/   89]
per-ex loss: 0.592054  [   66/   89]
per-ex loss: 0.423184  [   67/   89]
per-ex loss: 0.281897  [   68/   89]
per-ex loss: 0.327198  [   69/   89]
per-ex loss: 0.320907  [   70/   89]
per-ex loss: 0.379598  [   71/   89]
per-ex loss: 0.531523  [   72/   89]
per-ex loss: 0.330720  [   73/   89]
per-ex loss: 0.227913  [   74/   89]
per-ex loss: 0.318024  [   75/   89]
per-ex loss: 0.558694  [   76/   89]
per-ex loss: 0.342849  [   77/   89]
per-ex loss: 0.573187  [   78/   89]
per-ex loss: 0.483544  [   79/   89]
per-ex loss: 0.541991  [   80/   89]
per-ex loss: 0.503948  [   81/   89]
per-ex loss: 0.347962  [   82/   89]
per-ex loss: 0.421790  [   83/   89]
per-ex loss: 0.552655  [   84/   89]
per-ex loss: 0.419432  [   85/   89]
per-ex loss: 0.423947  [   86/   89]
per-ex loss: 0.626053  [   87/   89]
per-ex loss: 0.502849  [   88/   89]
per-ex loss: 0.512175  [   89/   89]
Train Error: Avg loss: 0.42211710
validation Error: 
 Avg loss: 0.53439189 
 F1: 0.493201 
 Precision: 0.508077 
 Recall: 0.479172
 IoU: 0.327317

test Error: 
 Avg loss: 0.48256687 
 F1: 0.562945 
 Precision: 0.559905 
 Recall: 0.566018
 IoU: 0.391735

We have finished training iteration 99
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_75_.pth
per-ex loss: 0.575468  [    1/   89]
per-ex loss: 0.535460  [    2/   89]
per-ex loss: 0.358694  [    3/   89]
per-ex loss: 0.550811  [    4/   89]
per-ex loss: 0.344666  [    5/   89]
per-ex loss: 0.500862  [    6/   89]
per-ex loss: 0.245872  [    7/   89]
per-ex loss: 0.491475  [    8/   89]
per-ex loss: 0.314105  [    9/   89]
per-ex loss: 0.307961  [   10/   89]
per-ex loss: 0.285104  [   11/   89]
per-ex loss: 0.999879  [   12/   89]
per-ex loss: 0.340967  [   13/   89]
per-ex loss: 0.336663  [   14/   89]
per-ex loss: 0.550989  [   15/   89]
per-ex loss: 0.356382  [   16/   89]
per-ex loss: 0.376825  [   17/   89]
per-ex loss: 0.354623  [   18/   89]
per-ex loss: 0.307286  [   19/   89]
per-ex loss: 0.560501  [   20/   89]
per-ex loss: 0.332095  [   21/   89]
per-ex loss: 0.295830  [   22/   89]
per-ex loss: 0.444090  [   23/   89]
per-ex loss: 0.416694  [   24/   89]
per-ex loss: 0.369226  [   25/   89]
per-ex loss: 0.389960  [   26/   89]
per-ex loss: 0.647238  [   27/   89]
per-ex loss: 0.364533  [   28/   89]
per-ex loss: 0.538264  [   29/   89]
per-ex loss: 0.356350  [   30/   89]
per-ex loss: 0.289351  [   31/   89]
per-ex loss: 0.319349  [   32/   89]
per-ex loss: 0.594145  [   33/   89]
per-ex loss: 0.346217  [   34/   89]
per-ex loss: 0.338879  [   35/   89]
per-ex loss: 0.321297  [   36/   89]
per-ex loss: 0.407279  [   37/   89]
per-ex loss: 0.387657  [   38/   89]
per-ex loss: 0.312994  [   39/   89]
per-ex loss: 0.365783  [   40/   89]
per-ex loss: 0.602318  [   41/   89]
per-ex loss: 0.310973  [   42/   89]
per-ex loss: 0.569800  [   43/   89]
per-ex loss: 0.512845  [   44/   89]
per-ex loss: 0.282984  [   45/   89]
per-ex loss: 0.377972  [   46/   89]
per-ex loss: 0.613246  [   47/   89]
per-ex loss: 0.384873  [   48/   89]
per-ex loss: 0.478520  [   49/   89]
per-ex loss: 0.536194  [   50/   89]
per-ex loss: 0.301423  [   51/   89]
per-ex loss: 0.451183  [   52/   89]
per-ex loss: 0.387483  [   53/   89]
per-ex loss: 0.445774  [   54/   89]
per-ex loss: 0.321612  [   55/   89]
per-ex loss: 0.638376  [   56/   89]
per-ex loss: 0.411776  [   57/   89]
per-ex loss: 0.284139  [   58/   89]
per-ex loss: 0.409651  [   59/   89]
per-ex loss: 0.484283  [   60/   89]
per-ex loss: 0.527049  [   61/   89]
per-ex loss: 0.383170  [   62/   89]
per-ex loss: 0.526495  [   63/   89]
per-ex loss: 0.510710  [   64/   89]
per-ex loss: 0.360581  [   65/   89]
per-ex loss: 0.280765  [   66/   89]
per-ex loss: 0.388631  [   67/   89]
per-ex loss: 0.354031  [   68/   89]
per-ex loss: 0.537224  [   69/   89]
per-ex loss: 0.351720  [   70/   89]
per-ex loss: 0.464706  [   71/   89]
per-ex loss: 0.582034  [   72/   89]
per-ex loss: 0.521076  [   73/   89]
per-ex loss: 0.412056  [   74/   89]
per-ex loss: 0.361169  [   75/   89]
per-ex loss: 0.324856  [   76/   89]
per-ex loss: 0.339516  [   77/   89]
per-ex loss: 0.457615  [   78/   89]
per-ex loss: 0.486499  [   79/   89]
per-ex loss: 0.593339  [   80/   89]
per-ex loss: 0.393512  [   81/   89]
per-ex loss: 0.547031  [   82/   89]
per-ex loss: 0.435696  [   83/   89]
per-ex loss: 0.444299  [   84/   89]
per-ex loss: 0.273579  [   85/   89]
per-ex loss: 0.372626  [   86/   89]
per-ex loss: 0.318310  [   87/   89]
per-ex loss: 0.665671  [   88/   89]
per-ex loss: 0.347950  [   89/   89]
Train Error: Avg loss: 0.42583332
validation Error: 
 Avg loss: 0.54901218 
 F1: 0.480672 
 Precision: 0.518937 
 Recall: 0.447662
 IoU: 0.316371

test Error: 
 Avg loss: 0.49066545 
 F1: 0.556023 
 Precision: 0.591853 
 Recall: 0.524284
 IoU: 0.385064

We have finished training iteration 100
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_98_.pth
per-ex loss: 0.378975  [    1/   89]
per-ex loss: 0.394352  [    2/   89]
per-ex loss: 0.516637  [    3/   89]
per-ex loss: 0.324721  [    4/   89]
per-ex loss: 0.525341  [    5/   89]
per-ex loss: 0.525457  [    6/   89]
per-ex loss: 0.283710  [    7/   89]
per-ex loss: 0.487670  [    8/   89]
per-ex loss: 0.332761  [    9/   89]
per-ex loss: 0.410184  [   10/   89]
per-ex loss: 0.394093  [   11/   89]
per-ex loss: 0.368024  [   12/   89]
per-ex loss: 0.317388  [   13/   89]
per-ex loss: 0.418283  [   14/   89]
per-ex loss: 0.197255  [   15/   89]
per-ex loss: 0.642377  [   16/   89]
per-ex loss: 0.483220  [   17/   89]
per-ex loss: 0.285015  [   18/   89]
per-ex loss: 0.709200  [   19/   89]
per-ex loss: 0.425623  [   20/   89]
per-ex loss: 0.450238  [   21/   89]
per-ex loss: 0.509902  [   22/   89]
per-ex loss: 0.485401  [   23/   89]
per-ex loss: 0.541421  [   24/   89]
per-ex loss: 0.303765  [   25/   89]
per-ex loss: 0.335824  [   26/   89]
per-ex loss: 0.347472  [   27/   89]
per-ex loss: 0.551463  [   28/   89]
per-ex loss: 0.418989  [   29/   89]
per-ex loss: 0.302305  [   30/   89]
per-ex loss: 0.407262  [   31/   89]
per-ex loss: 0.332961  [   32/   89]
per-ex loss: 0.415868  [   33/   89]
per-ex loss: 0.362226  [   34/   89]
per-ex loss: 0.403808  [   35/   89]
per-ex loss: 0.516898  [   36/   89]
per-ex loss: 0.434189  [   37/   89]
per-ex loss: 0.313100  [   38/   89]
per-ex loss: 0.511695  [   39/   89]
per-ex loss: 0.542648  [   40/   89]
per-ex loss: 0.614395  [   41/   89]
per-ex loss: 0.320793  [   42/   89]
per-ex loss: 0.600102  [   43/   89]
per-ex loss: 0.355666  [   44/   89]
per-ex loss: 0.588448  [   45/   89]
per-ex loss: 0.341625  [   46/   89]
per-ex loss: 0.430797  [   47/   89]
per-ex loss: 0.619605  [   48/   89]
per-ex loss: 0.336414  [   49/   89]
per-ex loss: 0.560654  [   50/   89]
per-ex loss: 0.641602  [   51/   89]
per-ex loss: 0.376695  [   52/   89]
per-ex loss: 0.367761  [   53/   89]
per-ex loss: 0.520162  [   54/   89]
per-ex loss: 0.562270  [   55/   89]
per-ex loss: 0.490374  [   56/   89]
per-ex loss: 0.347353  [   57/   89]
per-ex loss: 0.338975  [   58/   89]
per-ex loss: 0.362302  [   59/   89]
per-ex loss: 0.401781  [   60/   89]
per-ex loss: 0.382309  [   61/   89]
per-ex loss: 0.618450  [   62/   89]
per-ex loss: 0.488807  [   63/   89]
per-ex loss: 0.291632  [   64/   89]
per-ex loss: 0.316651  [   65/   89]
per-ex loss: 0.550207  [   66/   89]
per-ex loss: 0.360315  [   67/   89]
per-ex loss: 0.589333  [   68/   89]
per-ex loss: 0.406740  [   69/   89]
per-ex loss: 0.323329  [   70/   89]
per-ex loss: 0.363048  [   71/   89]
per-ex loss: 0.600155  [   72/   89]
per-ex loss: 0.328231  [   73/   89]
per-ex loss: 0.245808  [   74/   89]
per-ex loss: 0.446251  [   75/   89]
per-ex loss: 0.332965  [   76/   89]
per-ex loss: 0.315255  [   77/   89]
per-ex loss: 0.286845  [   78/   89]
per-ex loss: 0.422649  [   79/   89]
per-ex loss: 0.347807  [   80/   89]
per-ex loss: 0.296687  [   81/   89]
per-ex loss: 0.999851  [   82/   89]
per-ex loss: 0.597200  [   83/   89]
per-ex loss: 0.459881  [   84/   89]
per-ex loss: 0.307135  [   85/   89]
per-ex loss: 0.516392  [   86/   89]
per-ex loss: 0.355313  [   87/   89]
per-ex loss: 0.373451  [   88/   89]
per-ex loss: 0.343289  [   89/   89]
Train Error: Avg loss: 0.43091553
validation Error: 
 Avg loss: 0.57082891 
 F1: 0.461087 
 Precision: 0.498351 
 Recall: 0.429008
 IoU: 0.299619

test Error: 
 Avg loss: 0.53441933 
 F1: 0.518325 
 Precision: 0.492444 
 Recall: 0.547077
 IoU: 0.349824

We have finished training iteration 101
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_99_.pth
per-ex loss: 0.404826  [    1/   89]
per-ex loss: 0.518619  [    2/   89]
per-ex loss: 0.336824  [    3/   89]
per-ex loss: 0.316225  [    4/   89]
per-ex loss: 0.358333  [    5/   89]
per-ex loss: 0.339975  [    6/   89]
per-ex loss: 0.592825  [    7/   89]
per-ex loss: 0.373361  [    8/   89]
per-ex loss: 0.475730  [    9/   89]
per-ex loss: 0.325688  [   10/   89]
per-ex loss: 0.395409  [   11/   89]
per-ex loss: 0.349497  [   12/   89]
per-ex loss: 0.395144  [   13/   89]
per-ex loss: 0.493757  [   14/   89]
per-ex loss: 0.295768  [   15/   89]
per-ex loss: 0.337402  [   16/   89]
per-ex loss: 0.504007  [   17/   89]
per-ex loss: 0.408512  [   18/   89]
per-ex loss: 0.513088  [   19/   89]
per-ex loss: 0.514874  [   20/   89]
per-ex loss: 0.543163  [   21/   89]
per-ex loss: 0.387274  [   22/   89]
per-ex loss: 0.354690  [   23/   89]
per-ex loss: 0.408221  [   24/   89]
per-ex loss: 0.585669  [   25/   89]
per-ex loss: 0.427771  [   26/   89]
per-ex loss: 0.529596  [   27/   89]
per-ex loss: 0.376257  [   28/   89]
per-ex loss: 0.402150  [   29/   89]
per-ex loss: 0.606557  [   30/   89]
per-ex loss: 0.299370  [   31/   89]
per-ex loss: 0.498256  [   32/   89]
per-ex loss: 0.429073  [   33/   89]
per-ex loss: 0.352913  [   34/   89]
per-ex loss: 0.344579  [   35/   89]
per-ex loss: 0.451345  [   36/   89]
per-ex loss: 0.428610  [   37/   89]
per-ex loss: 0.999930  [   38/   89]
per-ex loss: 0.339134  [   39/   89]
per-ex loss: 0.331235  [   40/   89]
per-ex loss: 0.203831  [   41/   89]
per-ex loss: 0.377585  [   42/   89]
per-ex loss: 0.252182  [   43/   89]
per-ex loss: 0.307931  [   44/   89]
per-ex loss: 0.476616  [   45/   89]
per-ex loss: 0.454143  [   46/   89]
per-ex loss: 0.318139  [   47/   89]
per-ex loss: 0.312131  [   48/   89]
per-ex loss: 0.514931  [   49/   89]
per-ex loss: 0.503961  [   50/   89]
per-ex loss: 0.576107  [   51/   89]
per-ex loss: 0.722762  [   52/   89]
per-ex loss: 0.327150  [   53/   89]
per-ex loss: 0.333109  [   54/   89]
per-ex loss: 0.497053  [   55/   89]
per-ex loss: 0.391166  [   56/   89]
per-ex loss: 0.291794  [   57/   89]
per-ex loss: 0.507214  [   58/   89]
per-ex loss: 0.533162  [   59/   89]
per-ex loss: 0.347660  [   60/   89]
per-ex loss: 0.629089  [   61/   89]
per-ex loss: 0.272364  [   62/   89]
per-ex loss: 0.292536  [   63/   89]
per-ex loss: 0.460990  [   64/   89]
per-ex loss: 0.366959  [   65/   89]
per-ex loss: 0.661910  [   66/   89]
per-ex loss: 0.344892  [   67/   89]
per-ex loss: 0.297388  [   68/   89]
per-ex loss: 0.534428  [   69/   89]
per-ex loss: 0.345486  [   70/   89]
per-ex loss: 0.381056  [   71/   89]
per-ex loss: 0.645964  [   72/   89]
per-ex loss: 0.557802  [   73/   89]
per-ex loss: 0.298147  [   74/   89]
per-ex loss: 0.532343  [   75/   89]
per-ex loss: 0.323188  [   76/   89]
per-ex loss: 0.412381  [   77/   89]
per-ex loss: 0.514101  [   78/   89]
per-ex loss: 0.533907  [   79/   89]
per-ex loss: 0.422250  [   80/   89]
per-ex loss: 0.490742  [   81/   89]
per-ex loss: 0.374841  [   82/   89]
per-ex loss: 0.382342  [   83/   89]
per-ex loss: 0.393403  [   84/   89]
per-ex loss: 0.308028  [   85/   89]
per-ex loss: 0.267227  [   86/   89]
per-ex loss: 0.397563  [   87/   89]
per-ex loss: 0.527287  [   88/   89]
per-ex loss: 0.498891  [   89/   89]
Train Error: Avg loss: 0.42766053
validation Error: 
 Avg loss: 0.53006399 
 F1: 0.499650 
 Precision: 0.572258 
 Recall: 0.443392
 IoU: 0.333022

test Error: 
 Avg loss: 0.47998317 
 F1: 0.569860 
 Precision: 0.622282 
 Recall: 0.525585
 IoU: 0.398465

We have finished training iteration 102
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_100_.pth
per-ex loss: 0.583110  [    1/   89]
per-ex loss: 0.349976  [    2/   89]
per-ex loss: 0.384202  [    3/   89]
per-ex loss: 0.616294  [    4/   89]
per-ex loss: 0.310919  [    5/   89]
per-ex loss: 0.339244  [    6/   89]
per-ex loss: 0.374731  [    7/   89]
per-ex loss: 0.393592  [    8/   89]
per-ex loss: 0.463115  [    9/   89]
per-ex loss: 0.303485  [   10/   89]
per-ex loss: 0.472987  [   11/   89]
per-ex loss: 0.341098  [   12/   89]
per-ex loss: 0.356599  [   13/   89]
per-ex loss: 0.261416  [   14/   89]
per-ex loss: 0.386749  [   15/   89]
per-ex loss: 0.334880  [   16/   89]
per-ex loss: 0.536331  [   17/   89]
per-ex loss: 0.324660  [   18/   89]
per-ex loss: 0.398557  [   19/   89]
per-ex loss: 0.489595  [   20/   89]
per-ex loss: 0.359744  [   21/   89]
per-ex loss: 0.378889  [   22/   89]
per-ex loss: 0.468020  [   23/   89]
per-ex loss: 0.310678  [   24/   89]
per-ex loss: 0.413421  [   25/   89]
per-ex loss: 0.461582  [   26/   89]
per-ex loss: 0.567214  [   27/   89]
per-ex loss: 0.302518  [   28/   89]
per-ex loss: 0.336855  [   29/   89]
per-ex loss: 0.268981  [   30/   89]
per-ex loss: 0.517187  [   31/   89]
per-ex loss: 0.374739  [   32/   89]
per-ex loss: 0.439178  [   33/   89]
per-ex loss: 0.528521  [   34/   89]
per-ex loss: 0.537175  [   35/   89]
per-ex loss: 0.203341  [   36/   89]
per-ex loss: 0.380518  [   37/   89]
per-ex loss: 0.298869  [   38/   89]
per-ex loss: 0.440735  [   39/   89]
per-ex loss: 0.575041  [   40/   89]
per-ex loss: 0.615010  [   41/   89]
per-ex loss: 0.339177  [   42/   89]
per-ex loss: 0.448555  [   43/   89]
per-ex loss: 0.305023  [   44/   89]
per-ex loss: 0.358090  [   45/   89]
per-ex loss: 0.562119  [   46/   89]
per-ex loss: 0.320338  [   47/   89]
per-ex loss: 0.588312  [   48/   89]
per-ex loss: 0.509712  [   49/   89]
per-ex loss: 0.278829  [   50/   89]
per-ex loss: 0.451639  [   51/   89]
per-ex loss: 0.522399  [   52/   89]
per-ex loss: 0.379791  [   53/   89]
per-ex loss: 0.627628  [   54/   89]
per-ex loss: 0.488351  [   55/   89]
per-ex loss: 0.412189  [   56/   89]
per-ex loss: 0.562306  [   57/   89]
per-ex loss: 0.360589  [   58/   89]
per-ex loss: 0.279366  [   59/   89]
per-ex loss: 0.387339  [   60/   89]
per-ex loss: 0.498319  [   61/   89]
per-ex loss: 0.543930  [   62/   89]
per-ex loss: 0.370198  [   63/   89]
per-ex loss: 0.502361  [   64/   89]
per-ex loss: 0.340239  [   65/   89]
per-ex loss: 0.531276  [   66/   89]
per-ex loss: 0.281789  [   67/   89]
per-ex loss: 0.408804  [   68/   89]
per-ex loss: 0.505230  [   69/   89]
per-ex loss: 0.389146  [   70/   89]
per-ex loss: 0.562178  [   71/   89]
per-ex loss: 0.516351  [   72/   89]
per-ex loss: 0.438878  [   73/   89]
per-ex loss: 0.343556  [   74/   89]
per-ex loss: 0.607474  [   75/   89]
per-ex loss: 0.397868  [   76/   89]
per-ex loss: 0.338567  [   77/   89]
per-ex loss: 0.999944  [   78/   89]
per-ex loss: 0.449958  [   79/   89]
per-ex loss: 0.288385  [   80/   89]
per-ex loss: 0.361801  [   81/   89]
per-ex loss: 0.279124  [   82/   89]
per-ex loss: 0.544919  [   83/   89]
per-ex loss: 0.648696  [   84/   89]
per-ex loss: 0.310763  [   85/   89]
per-ex loss: 0.374006  [   86/   89]
per-ex loss: 0.340798  [   87/   89]
per-ex loss: 0.344630  [   88/   89]
per-ex loss: 0.330716  [   89/   89]
Train Error: Avg loss: 0.42507202
validation Error: 
 Avg loss: 0.55204302 
 F1: 0.478801 
 Precision: 0.539634 
 Recall: 0.430293
 IoU: 0.314752

test Error: 
 Avg loss: 0.52008085 
 F1: 0.531226 
 Precision: 0.545442 
 Recall: 0.517732
 IoU: 0.361680

We have finished training iteration 103
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_101_.pth
per-ex loss: 0.364124  [    1/   89]
per-ex loss: 0.380520  [    2/   89]
per-ex loss: 0.521754  [    3/   89]
per-ex loss: 0.430477  [    4/   89]
per-ex loss: 0.518246  [    5/   89]
per-ex loss: 0.416206  [    6/   89]
per-ex loss: 0.287978  [    7/   89]
per-ex loss: 0.615119  [    8/   89]
per-ex loss: 0.521698  [    9/   89]
per-ex loss: 0.361202  [   10/   89]
per-ex loss: 0.358134  [   11/   89]
per-ex loss: 0.389033  [   12/   89]
per-ex loss: 0.386250  [   13/   89]
per-ex loss: 0.445112  [   14/   89]
per-ex loss: 0.646482  [   15/   89]
per-ex loss: 0.313405  [   16/   89]
per-ex loss: 0.380206  [   17/   89]
per-ex loss: 0.322923  [   18/   89]
per-ex loss: 0.406964  [   19/   89]
per-ex loss: 0.596868  [   20/   89]
per-ex loss: 0.412027  [   21/   89]
per-ex loss: 0.354120  [   22/   89]
per-ex loss: 0.322638  [   23/   89]
per-ex loss: 0.701629  [   24/   89]
per-ex loss: 0.490931  [   25/   89]
per-ex loss: 0.320241  [   26/   89]
per-ex loss: 0.258715  [   27/   89]
per-ex loss: 0.543274  [   28/   89]
per-ex loss: 0.660324  [   29/   89]
per-ex loss: 0.999946  [   30/   89]
per-ex loss: 0.518777  [   31/   89]
per-ex loss: 0.317041  [   32/   89]
per-ex loss: 0.527772  [   33/   89]
per-ex loss: 0.274829  [   34/   89]
per-ex loss: 0.388719  [   35/   89]
per-ex loss: 0.506387  [   36/   89]
per-ex loss: 0.454898  [   37/   89]
per-ex loss: 0.360079  [   38/   89]
per-ex loss: 0.490745  [   39/   89]
per-ex loss: 0.197216  [   40/   89]
per-ex loss: 0.300883  [   41/   89]
per-ex loss: 0.347958  [   42/   89]
per-ex loss: 0.315075  [   43/   89]
per-ex loss: 0.398304  [   44/   89]
per-ex loss: 0.435338  [   45/   89]
per-ex loss: 0.234241  [   46/   89]
per-ex loss: 0.357514  [   47/   89]
per-ex loss: 0.403390  [   48/   89]
per-ex loss: 0.597257  [   49/   89]
per-ex loss: 0.320412  [   50/   89]
per-ex loss: 0.369662  [   51/   89]
per-ex loss: 0.532053  [   52/   89]
per-ex loss: 0.351433  [   53/   89]
per-ex loss: 0.282863  [   54/   89]
per-ex loss: 0.353748  [   55/   89]
per-ex loss: 0.496093  [   56/   89]
per-ex loss: 0.363015  [   57/   89]
per-ex loss: 0.528462  [   58/   89]
per-ex loss: 0.325959  [   59/   89]
per-ex loss: 0.339151  [   60/   89]
per-ex loss: 0.322063  [   61/   89]
per-ex loss: 0.470654  [   62/   89]
per-ex loss: 0.517281  [   63/   89]
per-ex loss: 0.308570  [   64/   89]
per-ex loss: 0.466077  [   65/   89]
per-ex loss: 0.390566  [   66/   89]
per-ex loss: 0.386997  [   67/   89]
per-ex loss: 0.583963  [   68/   89]
per-ex loss: 0.376142  [   69/   89]
per-ex loss: 0.450215  [   70/   89]
per-ex loss: 0.483899  [   71/   89]
per-ex loss: 0.509365  [   72/   89]
per-ex loss: 0.344965  [   73/   89]
per-ex loss: 0.278188  [   74/   89]
per-ex loss: 0.480077  [   75/   89]
per-ex loss: 0.419373  [   76/   89]
per-ex loss: 0.385279  [   77/   89]
per-ex loss: 0.289764  [   78/   89]
per-ex loss: 0.304644  [   79/   89]
per-ex loss: 0.484127  [   80/   89]
per-ex loss: 0.350617  [   81/   89]
per-ex loss: 0.510038  [   82/   89]
per-ex loss: 0.520733  [   83/   89]
per-ex loss: 0.351741  [   84/   89]
per-ex loss: 0.512022  [   85/   89]
per-ex loss: 0.331130  [   86/   89]
per-ex loss: 0.362130  [   87/   89]
per-ex loss: 0.286100  [   88/   89]
per-ex loss: 0.432951  [   89/   89]
Train Error: Avg loss: 0.41938754
validation Error: 
 Avg loss: 0.52765044 
 F1: 0.496011 
 Precision: 0.570560 
 Recall: 0.438692
 IoU: 0.329797

test Error: 
 Avg loss: 0.49038110 
 F1: 0.557683 
 Precision: 0.630299 
 Recall: 0.500070
 IoU: 0.386657

We have finished training iteration 104
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_77_.pth
per-ex loss: 0.623354  [    1/   89]
per-ex loss: 0.589357  [    2/   89]
per-ex loss: 0.326074  [    3/   89]
per-ex loss: 0.403374  [    4/   89]
per-ex loss: 0.397175  [    5/   89]
per-ex loss: 0.209987  [    6/   89]
per-ex loss: 0.526651  [    7/   89]
per-ex loss: 0.625169  [    8/   89]
per-ex loss: 0.350434  [    9/   89]
per-ex loss: 0.477040  [   10/   89]
per-ex loss: 0.278665  [   11/   89]
per-ex loss: 0.543033  [   12/   89]
per-ex loss: 0.317073  [   13/   89]
per-ex loss: 0.463761  [   14/   89]
per-ex loss: 0.428170  [   15/   89]
per-ex loss: 0.589815  [   16/   89]
per-ex loss: 0.364877  [   17/   89]
per-ex loss: 0.495246  [   18/   89]
per-ex loss: 0.343271  [   19/   89]
per-ex loss: 0.325371  [   20/   89]
per-ex loss: 0.394265  [   21/   89]
per-ex loss: 0.355324  [   22/   89]
per-ex loss: 0.382153  [   23/   89]
per-ex loss: 0.542655  [   24/   89]
per-ex loss: 0.556768  [   25/   89]
per-ex loss: 0.399025  [   26/   89]
per-ex loss: 0.307232  [   27/   89]
per-ex loss: 0.313299  [   28/   89]
per-ex loss: 0.370822  [   29/   89]
per-ex loss: 0.517612  [   30/   89]
per-ex loss: 0.695171  [   31/   89]
per-ex loss: 0.387559  [   32/   89]
per-ex loss: 0.317477  [   33/   89]
per-ex loss: 0.526334  [   34/   89]
per-ex loss: 0.280239  [   35/   89]
per-ex loss: 0.511466  [   36/   89]
per-ex loss: 0.384720  [   37/   89]
per-ex loss: 0.315388  [   38/   89]
per-ex loss: 0.442670  [   39/   89]
per-ex loss: 0.366120  [   40/   89]
per-ex loss: 0.458390  [   41/   89]
per-ex loss: 0.425617  [   42/   89]
per-ex loss: 0.545161  [   43/   89]
per-ex loss: 0.341283  [   44/   89]
per-ex loss: 0.514519  [   45/   89]
per-ex loss: 0.273262  [   46/   89]
per-ex loss: 0.359951  [   47/   89]
per-ex loss: 0.506929  [   48/   89]
per-ex loss: 0.538474  [   49/   89]
per-ex loss: 0.320746  [   50/   89]
per-ex loss: 0.455380  [   51/   89]
per-ex loss: 0.312490  [   52/   89]
per-ex loss: 0.428352  [   53/   89]
per-ex loss: 0.470361  [   54/   89]
per-ex loss: 0.480708  [   55/   89]
per-ex loss: 0.547034  [   56/   89]
per-ex loss: 0.470676  [   57/   89]
per-ex loss: 0.340625  [   58/   89]
per-ex loss: 0.400884  [   59/   89]
per-ex loss: 0.336936  [   60/   89]
per-ex loss: 0.415982  [   61/   89]
per-ex loss: 0.310311  [   62/   89]
per-ex loss: 0.579952  [   63/   89]
per-ex loss: 0.264712  [   64/   89]
per-ex loss: 0.350855  [   65/   89]
per-ex loss: 0.403187  [   66/   89]
per-ex loss: 0.382112  [   67/   89]
per-ex loss: 0.329552  [   68/   89]
per-ex loss: 0.376059  [   69/   89]
per-ex loss: 0.400081  [   70/   89]
per-ex loss: 0.270208  [   71/   89]
per-ex loss: 0.399286  [   72/   89]
per-ex loss: 0.256511  [   73/   89]
per-ex loss: 0.521952  [   74/   89]
per-ex loss: 0.278979  [   75/   89]
per-ex loss: 0.362380  [   76/   89]
per-ex loss: 0.306498  [   77/   89]
per-ex loss: 0.402323  [   78/   89]
per-ex loss: 0.335240  [   79/   89]
per-ex loss: 0.440752  [   80/   89]
per-ex loss: 0.296723  [   81/   89]
per-ex loss: 0.467210  [   82/   89]
per-ex loss: 0.345405  [   83/   89]
per-ex loss: 0.512789  [   84/   89]
per-ex loss: 0.999918  [   85/   89]
per-ex loss: 0.541812  [   86/   89]
per-ex loss: 0.575929  [   87/   89]
per-ex loss: 0.538982  [   88/   89]
per-ex loss: 0.422341  [   89/   89]
Train Error: Avg loss: 0.42314629
validation Error: 
 Avg loss: 0.58792759 
 F1: 0.450603 
 Precision: 0.535889 
 Recall: 0.388737
 IoU: 0.290825

test Error: 
 Avg loss: 0.54794486 
 F1: 0.505249 
 Precision: 0.542763 
 Recall: 0.472586
 IoU: 0.338016

We have finished training iteration 105
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_103_.pth
per-ex loss: 0.336300  [    1/   89]
per-ex loss: 0.347955  [    2/   89]
per-ex loss: 0.437148  [    3/   89]
per-ex loss: 0.476818  [    4/   89]
per-ex loss: 0.481350  [    5/   89]
per-ex loss: 0.612482  [    6/   89]
per-ex loss: 0.307442  [    7/   89]
per-ex loss: 0.305079  [    8/   89]
per-ex loss: 0.377851  [    9/   89]
per-ex loss: 0.562896  [   10/   89]
per-ex loss: 0.511458  [   11/   89]
per-ex loss: 0.519706  [   12/   89]
per-ex loss: 0.550869  [   13/   89]
per-ex loss: 0.418999  [   14/   89]
per-ex loss: 0.327420  [   15/   89]
per-ex loss: 0.600636  [   16/   89]
per-ex loss: 0.474233  [   17/   89]
per-ex loss: 0.294308  [   18/   89]
per-ex loss: 0.344839  [   19/   89]
per-ex loss: 0.415697  [   20/   89]
per-ex loss: 0.434755  [   21/   89]
per-ex loss: 0.536313  [   22/   89]
per-ex loss: 0.511888  [   23/   89]
per-ex loss: 0.220883  [   24/   89]
per-ex loss: 0.404801  [   25/   89]
per-ex loss: 0.594183  [   26/   89]
per-ex loss: 0.201293  [   27/   89]
per-ex loss: 0.492274  [   28/   89]
per-ex loss: 0.517093  [   29/   89]
per-ex loss: 0.298726  [   30/   89]
per-ex loss: 0.387903  [   31/   89]
per-ex loss: 0.524373  [   32/   89]
per-ex loss: 0.499330  [   33/   89]
per-ex loss: 0.308382  [   34/   89]
per-ex loss: 0.577857  [   35/   89]
per-ex loss: 0.250787  [   36/   89]
per-ex loss: 0.414304  [   37/   89]
per-ex loss: 0.425943  [   38/   89]
per-ex loss: 0.312218  [   39/   89]
per-ex loss: 0.427722  [   40/   89]
per-ex loss: 0.482840  [   41/   89]
per-ex loss: 0.354257  [   42/   89]
per-ex loss: 0.544319  [   43/   89]
per-ex loss: 0.330749  [   44/   89]
per-ex loss: 0.340644  [   45/   89]
per-ex loss: 0.315936  [   46/   89]
per-ex loss: 0.480616  [   47/   89]
per-ex loss: 0.415300  [   48/   89]
per-ex loss: 0.612511  [   49/   89]
per-ex loss: 0.329819  [   50/   89]
per-ex loss: 0.336918  [   51/   89]
per-ex loss: 0.279731  [   52/   89]
per-ex loss: 0.298502  [   53/   89]
per-ex loss: 0.467463  [   54/   89]
per-ex loss: 0.385302  [   55/   89]
per-ex loss: 0.541760  [   56/   89]
per-ex loss: 0.269159  [   57/   89]
per-ex loss: 0.528777  [   58/   89]
per-ex loss: 0.353804  [   59/   89]
per-ex loss: 0.310085  [   60/   89]
per-ex loss: 0.367401  [   61/   89]
per-ex loss: 0.592481  [   62/   89]
per-ex loss: 0.319493  [   63/   89]
per-ex loss: 0.423687  [   64/   89]
per-ex loss: 0.343496  [   65/   89]
per-ex loss: 0.366206  [   66/   89]
per-ex loss: 0.468059  [   67/   89]
per-ex loss: 0.539813  [   68/   89]
per-ex loss: 0.482391  [   69/   89]
per-ex loss: 0.293473  [   70/   89]
per-ex loss: 0.344358  [   71/   89]
per-ex loss: 0.387201  [   72/   89]
per-ex loss: 0.279908  [   73/   89]
per-ex loss: 0.524122  [   74/   89]
per-ex loss: 0.616055  [   75/   89]
per-ex loss: 0.325230  [   76/   89]
per-ex loss: 0.384017  [   77/   89]
per-ex loss: 0.326485  [   78/   89]
per-ex loss: 0.516390  [   79/   89]
per-ex loss: 0.328214  [   80/   89]
per-ex loss: 0.303612  [   81/   89]
per-ex loss: 0.375648  [   82/   89]
per-ex loss: 0.484759  [   83/   89]
per-ex loss: 0.400009  [   84/   89]
per-ex loss: 0.386008  [   85/   89]
per-ex loss: 0.363882  [   86/   89]
per-ex loss: 0.999911  [   87/   89]
per-ex loss: 0.358259  [   88/   89]
per-ex loss: 0.254072  [   89/   89]
Train Error: Avg loss: 0.41772634
validation Error: 
 Avg loss: 0.60268892 
 F1: 0.438624 
 Precision: 0.465904 
 Recall: 0.414362
 IoU: 0.280921

test Error: 
 Avg loss: 0.56731594 
 F1: 0.486240 
 Precision: 0.464008 
 Recall: 0.510710
 IoU: 0.321214

We have finished training iteration 106
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_102_.pth
per-ex loss: 0.538269  [    1/   89]
per-ex loss: 0.374191  [    2/   89]
per-ex loss: 0.348729  [    3/   89]
per-ex loss: 0.421244  [    4/   89]
per-ex loss: 0.451721  [    5/   89]
per-ex loss: 0.462324  [    6/   89]
per-ex loss: 0.309072  [    7/   89]
per-ex loss: 0.536092  [    8/   89]
per-ex loss: 0.634230  [    9/   89]
per-ex loss: 0.516788  [   10/   89]
per-ex loss: 0.349231  [   11/   89]
per-ex loss: 0.285260  [   12/   89]
per-ex loss: 0.382592  [   13/   89]
per-ex loss: 0.406092  [   14/   89]
per-ex loss: 0.999934  [   15/   89]
per-ex loss: 0.497292  [   16/   89]
per-ex loss: 0.221141  [   17/   89]
per-ex loss: 0.605851  [   18/   89]
per-ex loss: 0.307639  [   19/   89]
per-ex loss: 0.385267  [   20/   89]
per-ex loss: 0.349590  [   21/   89]
per-ex loss: 0.182799  [   22/   89]
per-ex loss: 0.534608  [   23/   89]
per-ex loss: 0.471856  [   24/   89]
per-ex loss: 0.390010  [   25/   89]
per-ex loss: 0.276698  [   26/   89]
per-ex loss: 0.369886  [   27/   89]
per-ex loss: 0.479779  [   28/   89]
per-ex loss: 0.433057  [   29/   89]
per-ex loss: 0.306314  [   30/   89]
per-ex loss: 0.404804  [   31/   89]
per-ex loss: 0.298380  [   32/   89]
per-ex loss: 0.402022  [   33/   89]
per-ex loss: 0.362212  [   34/   89]
per-ex loss: 0.522462  [   35/   89]
per-ex loss: 0.410794  [   36/   89]
per-ex loss: 0.583058  [   37/   89]
per-ex loss: 0.529041  [   38/   89]
per-ex loss: 0.337786  [   39/   89]
per-ex loss: 0.341484  [   40/   89]
per-ex loss: 0.302273  [   41/   89]
per-ex loss: 0.371764  [   42/   89]
per-ex loss: 0.432275  [   43/   89]
per-ex loss: 0.338471  [   44/   89]
per-ex loss: 0.302289  [   45/   89]
per-ex loss: 0.308225  [   46/   89]
per-ex loss: 0.514785  [   47/   89]
per-ex loss: 0.353675  [   48/   89]
per-ex loss: 0.673839  [   49/   89]
per-ex loss: 0.315596  [   50/   89]
per-ex loss: 0.581760  [   51/   89]
per-ex loss: 0.489890  [   52/   89]
per-ex loss: 0.486835  [   53/   89]
per-ex loss: 0.486160  [   54/   89]
per-ex loss: 0.345859  [   55/   89]
per-ex loss: 0.348270  [   56/   89]
per-ex loss: 0.543004  [   57/   89]
per-ex loss: 0.434066  [   58/   89]
per-ex loss: 0.612172  [   59/   89]
per-ex loss: 0.329213  [   60/   89]
per-ex loss: 0.288747  [   61/   89]
per-ex loss: 0.480449  [   62/   89]
per-ex loss: 0.313556  [   63/   89]
per-ex loss: 0.508303  [   64/   89]
per-ex loss: 0.315443  [   65/   89]
per-ex loss: 0.479927  [   66/   89]
per-ex loss: 0.339580  [   67/   89]
per-ex loss: 0.471381  [   68/   89]
per-ex loss: 0.533124  [   69/   89]
per-ex loss: 0.509467  [   70/   89]
per-ex loss: 0.390909  [   71/   89]
per-ex loss: 0.255761  [   72/   89]
per-ex loss: 0.341758  [   73/   89]
per-ex loss: 0.510738  [   74/   89]
per-ex loss: 0.369028  [   75/   89]
per-ex loss: 0.369715  [   76/   89]
per-ex loss: 0.561341  [   77/   89]
per-ex loss: 0.352157  [   78/   89]
per-ex loss: 0.285839  [   79/   89]
per-ex loss: 0.400699  [   80/   89]
per-ex loss: 0.515635  [   81/   89]
per-ex loss: 0.560005  [   82/   89]
per-ex loss: 0.521518  [   83/   89]
per-ex loss: 0.484895  [   84/   89]
per-ex loss: 0.395433  [   85/   89]
per-ex loss: 0.386405  [   86/   89]
per-ex loss: 0.433669  [   87/   89]
per-ex loss: 0.333645  [   88/   89]
per-ex loss: 0.335482  [   89/   89]
Train Error: Avg loss: 0.42286106
validation Error: 
 Avg loss: 0.55921265 
 F1: 0.460751 
 Precision: 0.426402 
 Recall: 0.501118
 IoU: 0.299335

test Error: 
 Avg loss: 0.49881098 
 F1: 0.544496 
 Precision: 0.503988 
 Recall: 0.592084
 IoU: 0.374094

We have finished training iteration 107
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_105_.pth
per-ex loss: 0.577324  [    1/   89]
per-ex loss: 0.410582  [    2/   89]
per-ex loss: 0.431964  [    3/   89]
per-ex loss: 0.342029  [    4/   89]
per-ex loss: 0.508975  [    5/   89]
per-ex loss: 0.405194  [    6/   89]
per-ex loss: 0.493340  [    7/   89]
per-ex loss: 0.251624  [    8/   89]
per-ex loss: 0.290789  [    9/   89]
per-ex loss: 0.461720  [   10/   89]
per-ex loss: 0.533865  [   11/   89]
per-ex loss: 0.321397  [   12/   89]
per-ex loss: 0.363556  [   13/   89]
per-ex loss: 0.500060  [   14/   89]
per-ex loss: 0.364455  [   15/   89]
per-ex loss: 0.520917  [   16/   89]
per-ex loss: 0.372937  [   17/   89]
per-ex loss: 0.531327  [   18/   89]
per-ex loss: 0.357765  [   19/   89]
per-ex loss: 0.352548  [   20/   89]
per-ex loss: 0.376087  [   21/   89]
per-ex loss: 0.558122  [   22/   89]
per-ex loss: 0.390975  [   23/   89]
per-ex loss: 0.414495  [   24/   89]
per-ex loss: 0.322830  [   25/   89]
per-ex loss: 0.276005  [   26/   89]
per-ex loss: 0.487665  [   27/   89]
per-ex loss: 0.491701  [   28/   89]
per-ex loss: 0.322982  [   29/   89]
per-ex loss: 0.592452  [   30/   89]
per-ex loss: 0.500713  [   31/   89]
per-ex loss: 0.398417  [   32/   89]
per-ex loss: 0.634466  [   33/   89]
per-ex loss: 0.539644  [   34/   89]
per-ex loss: 0.274429  [   35/   89]
per-ex loss: 0.344070  [   36/   89]
per-ex loss: 0.326638  [   37/   89]
per-ex loss: 0.414670  [   38/   89]
per-ex loss: 0.224504  [   39/   89]
per-ex loss: 0.276211  [   40/   89]
per-ex loss: 0.513321  [   41/   89]
per-ex loss: 0.311024  [   42/   89]
per-ex loss: 0.366437  [   43/   89]
per-ex loss: 0.335957  [   44/   89]
per-ex loss: 0.396830  [   45/   89]
per-ex loss: 0.324996  [   46/   89]
per-ex loss: 0.272126  [   47/   89]
per-ex loss: 0.486000  [   48/   89]
per-ex loss: 0.999879  [   49/   89]
per-ex loss: 0.358108  [   50/   89]
per-ex loss: 0.599076  [   51/   89]
per-ex loss: 0.325888  [   52/   89]
per-ex loss: 0.503955  [   53/   89]
per-ex loss: 0.380565  [   54/   89]
per-ex loss: 0.344142  [   55/   89]
per-ex loss: 0.395725  [   56/   89]
per-ex loss: 0.552177  [   57/   89]
per-ex loss: 0.438046  [   58/   89]
per-ex loss: 0.320677  [   59/   89]
per-ex loss: 0.327623  [   60/   89]
per-ex loss: 0.749610  [   61/   89]
per-ex loss: 0.546338  [   62/   89]
per-ex loss: 0.387148  [   63/   89]
per-ex loss: 0.530894  [   64/   89]
per-ex loss: 0.345765  [   65/   89]
per-ex loss: 0.368964  [   66/   89]
per-ex loss: 0.340441  [   67/   89]
per-ex loss: 0.338073  [   68/   89]
per-ex loss: 0.607425  [   69/   89]
per-ex loss: 0.363000  [   70/   89]
per-ex loss: 0.384856  [   71/   89]
per-ex loss: 0.685122  [   72/   89]
per-ex loss: 0.399289  [   73/   89]
per-ex loss: 0.470250  [   74/   89]
per-ex loss: 0.527594  [   75/   89]
per-ex loss: 0.476111  [   76/   89]
per-ex loss: 0.396729  [   77/   89]
per-ex loss: 0.402727  [   78/   89]
per-ex loss: 0.326482  [   79/   89]
per-ex loss: 0.295153  [   80/   89]
per-ex loss: 0.328436  [   81/   89]
per-ex loss: 0.390665  [   82/   89]
per-ex loss: 0.349538  [   83/   89]
per-ex loss: 0.446457  [   84/   89]
per-ex loss: 0.487136  [   85/   89]
per-ex loss: 0.495809  [   86/   89]
per-ex loss: 0.318021  [   87/   89]
per-ex loss: 0.479563  [   88/   89]
per-ex loss: 0.422752  [   89/   89]
Train Error: Avg loss: 0.42440838
validation Error: 
 Avg loss: 0.57304830 
 F1: 0.452683 
 Precision: 0.397936 
 Recall: 0.524898
 IoU: 0.292560

test Error: 
 Avg loss: 0.49703281 
 F1: 0.542742 
 Precision: 0.489987 
 Recall: 0.608226
 IoU: 0.372440

We have finished training iteration 108
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_106_.pth
per-ex loss: 0.351232  [    1/   89]
per-ex loss: 0.335160  [    2/   89]
per-ex loss: 0.321438  [    3/   89]
per-ex loss: 0.367745  [    4/   89]
per-ex loss: 0.635912  [    5/   89]
per-ex loss: 0.565715  [    6/   89]
per-ex loss: 0.442274  [    7/   89]
per-ex loss: 0.392852  [    8/   89]
per-ex loss: 0.356288  [    9/   89]
per-ex loss: 0.420305  [   10/   89]
per-ex loss: 0.340325  [   11/   89]
per-ex loss: 0.497962  [   12/   89]
per-ex loss: 0.274924  [   13/   89]
per-ex loss: 0.406816  [   14/   89]
per-ex loss: 0.372990  [   15/   89]
per-ex loss: 0.374623  [   16/   89]
per-ex loss: 0.403933  [   17/   89]
per-ex loss: 0.497254  [   18/   89]
per-ex loss: 0.439543  [   19/   89]
per-ex loss: 0.520661  [   20/   89]
per-ex loss: 0.446759  [   21/   89]
per-ex loss: 0.484237  [   22/   89]
per-ex loss: 0.382191  [   23/   89]
per-ex loss: 0.317771  [   24/   89]
per-ex loss: 0.381887  [   25/   89]
per-ex loss: 0.327959  [   26/   89]
per-ex loss: 0.272794  [   27/   89]
per-ex loss: 0.388052  [   28/   89]
per-ex loss: 0.340399  [   29/   89]
per-ex loss: 0.345082  [   30/   89]
per-ex loss: 0.380340  [   31/   89]
per-ex loss: 0.371061  [   32/   89]
per-ex loss: 0.282681  [   33/   89]
per-ex loss: 0.640248  [   34/   89]
per-ex loss: 0.543739  [   35/   89]
per-ex loss: 0.396455  [   36/   89]
per-ex loss: 0.382049  [   37/   89]
per-ex loss: 0.393280  [   38/   89]
per-ex loss: 0.287173  [   39/   89]
per-ex loss: 0.532367  [   40/   89]
per-ex loss: 0.287925  [   41/   89]
per-ex loss: 0.598377  [   42/   89]
per-ex loss: 0.598378  [   43/   89]
per-ex loss: 0.473179  [   44/   89]
per-ex loss: 0.549487  [   45/   89]
per-ex loss: 0.367021  [   46/   89]
per-ex loss: 0.348770  [   47/   89]
per-ex loss: 0.393045  [   48/   89]
per-ex loss: 0.374403  [   49/   89]
per-ex loss: 0.577753  [   50/   89]
per-ex loss: 0.489752  [   51/   89]
per-ex loss: 0.619181  [   52/   89]
per-ex loss: 0.568556  [   53/   89]
per-ex loss: 0.358490  [   54/   89]
per-ex loss: 0.466320  [   55/   89]
per-ex loss: 0.411100  [   56/   89]
per-ex loss: 0.392862  [   57/   89]
per-ex loss: 0.347939  [   58/   89]
per-ex loss: 0.385645  [   59/   89]
per-ex loss: 0.307874  [   60/   89]
per-ex loss: 0.443329  [   61/   89]
per-ex loss: 0.384763  [   62/   89]
per-ex loss: 0.339862  [   63/   89]
per-ex loss: 0.530367  [   64/   89]
per-ex loss: 0.504862  [   65/   89]
per-ex loss: 0.339702  [   66/   89]
per-ex loss: 0.546560  [   67/   89]
per-ex loss: 0.481402  [   68/   89]
per-ex loss: 0.416860  [   69/   89]
per-ex loss: 0.497175  [   70/   89]
per-ex loss: 0.332772  [   71/   89]
per-ex loss: 0.257660  [   72/   89]
per-ex loss: 0.489305  [   73/   89]
per-ex loss: 0.591488  [   74/   89]
per-ex loss: 0.608118  [   75/   89]
per-ex loss: 0.328783  [   76/   89]
per-ex loss: 0.302683  [   77/   89]
per-ex loss: 0.352978  [   78/   89]
per-ex loss: 0.326030  [   79/   89]
per-ex loss: 0.357303  [   80/   89]
per-ex loss: 0.616773  [   81/   89]
per-ex loss: 0.999950  [   82/   89]
per-ex loss: 0.362701  [   83/   89]
per-ex loss: 0.326324  [   84/   89]
per-ex loss: 0.549325  [   85/   89]
per-ex loss: 0.511280  [   86/   89]
per-ex loss: 0.196917  [   87/   89]
per-ex loss: 0.494792  [   88/   89]
per-ex loss: 0.296667  [   89/   89]
Train Error: Avg loss: 0.42603670
validation Error: 
 Avg loss: 0.57560136 
 F1: 0.453072 
 Precision: 0.590255 
 Recall: 0.367630
 IoU: 0.292885

test Error: 
 Avg loss: 0.53451232 
 F1: 0.514605 
 Precision: 0.578526 
 Recall: 0.463403
 IoU: 0.346443

We have finished training iteration 109
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_107_.pth
per-ex loss: 0.544145  [    1/   89]
per-ex loss: 0.364487  [    2/   89]
per-ex loss: 0.301542  [    3/   89]
per-ex loss: 0.294545  [    4/   89]
per-ex loss: 0.390309  [    5/   89]
per-ex loss: 0.474458  [    6/   89]
per-ex loss: 0.331839  [    7/   89]
per-ex loss: 0.515609  [    8/   89]
per-ex loss: 0.325370  [    9/   89]
per-ex loss: 0.316028  [   10/   89]
per-ex loss: 0.450851  [   11/   89]
per-ex loss: 0.416871  [   12/   89]
per-ex loss: 0.298536  [   13/   89]
per-ex loss: 0.289524  [   14/   89]
per-ex loss: 0.530176  [   15/   89]
per-ex loss: 0.259077  [   16/   89]
per-ex loss: 0.356626  [   17/   89]
per-ex loss: 0.429067  [   18/   89]
per-ex loss: 0.478898  [   19/   89]
per-ex loss: 0.547381  [   20/   89]
per-ex loss: 0.528637  [   21/   89]
per-ex loss: 0.328583  [   22/   89]
per-ex loss: 0.614554  [   23/   89]
per-ex loss: 0.346247  [   24/   89]
per-ex loss: 0.447447  [   25/   89]
per-ex loss: 0.522009  [   26/   89]
per-ex loss: 0.500718  [   27/   89]
per-ex loss: 0.381440  [   28/   89]
per-ex loss: 0.302431  [   29/   89]
per-ex loss: 0.386720  [   30/   89]
per-ex loss: 0.565066  [   31/   89]
per-ex loss: 0.513937  [   32/   89]
per-ex loss: 0.465151  [   33/   89]
per-ex loss: 0.337009  [   34/   89]
per-ex loss: 0.226572  [   35/   89]
per-ex loss: 0.530538  [   36/   89]
per-ex loss: 0.399996  [   37/   89]
per-ex loss: 0.408001  [   38/   89]
per-ex loss: 0.582085  [   39/   89]
per-ex loss: 0.362517  [   40/   89]
per-ex loss: 0.265885  [   41/   89]
per-ex loss: 0.291709  [   42/   89]
per-ex loss: 0.530266  [   43/   89]
per-ex loss: 0.428504  [   44/   89]
per-ex loss: 0.568724  [   45/   89]
per-ex loss: 0.547677  [   46/   89]
per-ex loss: 0.348747  [   47/   89]
per-ex loss: 0.321131  [   48/   89]
per-ex loss: 0.378253  [   49/   89]
per-ex loss: 0.346461  [   50/   89]
per-ex loss: 0.628173  [   51/   89]
per-ex loss: 0.287769  [   52/   89]
per-ex loss: 0.505499  [   53/   89]
per-ex loss: 0.634086  [   54/   89]
per-ex loss: 0.626752  [   55/   89]
per-ex loss: 0.258694  [   56/   89]
per-ex loss: 0.339212  [   57/   89]
per-ex loss: 0.367376  [   58/   89]
per-ex loss: 0.579682  [   59/   89]
per-ex loss: 0.350167  [   60/   89]
per-ex loss: 0.342980  [   61/   89]
per-ex loss: 0.383307  [   62/   89]
per-ex loss: 0.396286  [   63/   89]
per-ex loss: 0.999879  [   64/   89]
per-ex loss: 0.336796  [   65/   89]
per-ex loss: 0.281513  [   66/   89]
per-ex loss: 0.234717  [   67/   89]
per-ex loss: 0.455090  [   68/   89]
per-ex loss: 0.431083  [   69/   89]
per-ex loss: 0.412959  [   70/   89]
per-ex loss: 0.310484  [   71/   89]
per-ex loss: 0.253246  [   72/   89]
per-ex loss: 0.326321  [   73/   89]
per-ex loss: 0.373527  [   74/   89]
per-ex loss: 0.367973  [   75/   89]
per-ex loss: 0.376181  [   76/   89]
per-ex loss: 0.510954  [   77/   89]
per-ex loss: 0.361506  [   78/   89]
per-ex loss: 0.536490  [   79/   89]
per-ex loss: 0.487810  [   80/   89]
per-ex loss: 0.341204  [   81/   89]
per-ex loss: 0.489699  [   82/   89]
per-ex loss: 0.352750  [   83/   89]
per-ex loss: 0.359643  [   84/   89]
per-ex loss: 0.372158  [   85/   89]
per-ex loss: 0.367067  [   86/   89]
per-ex loss: 0.372697  [   87/   89]
per-ex loss: 0.589957  [   88/   89]
per-ex loss: 0.340328  [   89/   89]
Train Error: Avg loss: 0.41611652
validation Error: 
 Avg loss: 0.54052591 
 F1: 0.484604 
 Precision: 0.528044 
 Recall: 0.447768
 IoU: 0.319787

test Error: 
 Avg loss: 0.50019051 
 F1: 0.545699 
 Precision: 0.577820 
 Recall: 0.516961
 IoU: 0.375231

We have finished training iteration 110
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_108_.pth
per-ex loss: 0.486197  [    1/   89]
per-ex loss: 0.509802  [    2/   89]
per-ex loss: 0.311288  [    3/   89]
per-ex loss: 0.573894  [    4/   89]
per-ex loss: 0.476025  [    5/   89]
per-ex loss: 0.369675  [    6/   89]
per-ex loss: 0.740941  [    7/   89]
per-ex loss: 0.366015  [    8/   89]
per-ex loss: 0.333712  [    9/   89]
per-ex loss: 0.262498  [   10/   89]
per-ex loss: 0.211346  [   11/   89]
per-ex loss: 0.347395  [   12/   89]
per-ex loss: 0.535948  [   13/   89]
per-ex loss: 0.529221  [   14/   89]
per-ex loss: 0.418036  [   15/   89]
per-ex loss: 0.385373  [   16/   89]
per-ex loss: 0.309139  [   17/   89]
per-ex loss: 0.309721  [   18/   89]
per-ex loss: 0.279394  [   19/   89]
per-ex loss: 0.337182  [   20/   89]
per-ex loss: 0.343111  [   21/   89]
per-ex loss: 0.999904  [   22/   89]
per-ex loss: 0.658360  [   23/   89]
per-ex loss: 0.387879  [   24/   89]
per-ex loss: 0.423024  [   25/   89]
per-ex loss: 0.304732  [   26/   89]
per-ex loss: 0.390092  [   27/   89]
per-ex loss: 0.288260  [   28/   89]
per-ex loss: 0.345007  [   29/   89]
per-ex loss: 0.327876  [   30/   89]
per-ex loss: 0.551576  [   31/   89]
per-ex loss: 0.372169  [   32/   89]
per-ex loss: 0.535348  [   33/   89]
per-ex loss: 0.374681  [   34/   89]
per-ex loss: 0.364031  [   35/   89]
per-ex loss: 0.345164  [   36/   89]
per-ex loss: 0.767186  [   37/   89]
per-ex loss: 0.311383  [   38/   89]
per-ex loss: 0.285630  [   39/   89]
per-ex loss: 0.395199  [   40/   89]
per-ex loss: 0.363139  [   41/   89]
per-ex loss: 0.339693  [   42/   89]
per-ex loss: 0.322820  [   43/   89]
per-ex loss: 0.517755  [   44/   89]
per-ex loss: 0.342253  [   45/   89]
per-ex loss: 0.510853  [   46/   89]
per-ex loss: 0.490498  [   47/   89]
per-ex loss: 0.318900  [   48/   89]
per-ex loss: 0.245580  [   49/   89]
per-ex loss: 0.317867  [   50/   89]
per-ex loss: 0.360015  [   51/   89]
per-ex loss: 0.252296  [   52/   89]
per-ex loss: 0.399421  [   53/   89]
per-ex loss: 0.494648  [   54/   89]
per-ex loss: 0.395059  [   55/   89]
per-ex loss: 0.279288  [   56/   89]
per-ex loss: 0.290405  [   57/   89]
per-ex loss: 0.384527  [   58/   89]
per-ex loss: 0.577441  [   59/   89]
per-ex loss: 0.306097  [   60/   89]
per-ex loss: 0.389971  [   61/   89]
per-ex loss: 0.610496  [   62/   89]
per-ex loss: 0.578230  [   63/   89]
per-ex loss: 0.527154  [   64/   89]
per-ex loss: 0.616535  [   65/   89]
per-ex loss: 0.429048  [   66/   89]
per-ex loss: 0.327703  [   67/   89]
per-ex loss: 0.538628  [   68/   89]
per-ex loss: 0.652875  [   69/   89]
per-ex loss: 0.465645  [   70/   89]
per-ex loss: 0.479864  [   71/   89]
per-ex loss: 0.456359  [   72/   89]
per-ex loss: 0.544290  [   73/   89]
per-ex loss: 0.508397  [   74/   89]
per-ex loss: 0.379760  [   75/   89]
per-ex loss: 0.443093  [   76/   89]
per-ex loss: 0.373660  [   77/   89]
per-ex loss: 0.472903  [   78/   89]
per-ex loss: 0.520496  [   79/   89]
per-ex loss: 0.320026  [   80/   89]
per-ex loss: 0.421313  [   81/   89]
per-ex loss: 0.294638  [   82/   89]
per-ex loss: 0.313272  [   83/   89]
per-ex loss: 0.398534  [   84/   89]
per-ex loss: 0.526749  [   85/   89]
per-ex loss: 0.587404  [   86/   89]
per-ex loss: 0.394789  [   87/   89]
per-ex loss: 0.301231  [   88/   89]
per-ex loss: 0.354332  [   89/   89]
Train Error: Avg loss: 0.42246476
validation Error: 
 Avg loss: 0.54739392 
 F1: 0.475525 
 Precision: 0.423458 
 Recall: 0.542192
 IoU: 0.311927

test Error: 
 Avg loss: 0.50146391 
 F1: 0.533403 
 Precision: 0.464298 
 Recall: 0.626676
 IoU: 0.363701

We have finished training iteration 111
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_109_.pth
per-ex loss: 0.551611  [    1/   89]
per-ex loss: 0.591006  [    2/   89]
per-ex loss: 0.511100  [    3/   89]
per-ex loss: 0.444084  [    4/   89]
per-ex loss: 0.385185  [    5/   89]
per-ex loss: 0.499687  [    6/   89]
per-ex loss: 0.660733  [    7/   89]
per-ex loss: 0.382513  [    8/   89]
per-ex loss: 0.515169  [    9/   89]
per-ex loss: 0.375952  [   10/   89]
per-ex loss: 0.306470  [   11/   89]
per-ex loss: 0.338017  [   12/   89]
per-ex loss: 0.492081  [   13/   89]
per-ex loss: 0.603610  [   14/   89]
per-ex loss: 0.346997  [   15/   89]
per-ex loss: 0.544787  [   16/   89]
per-ex loss: 0.541875  [   17/   89]
per-ex loss: 0.551197  [   18/   89]
per-ex loss: 0.434390  [   19/   89]
per-ex loss: 0.639391  [   20/   89]
per-ex loss: 0.331824  [   21/   89]
per-ex loss: 0.270731  [   22/   89]
per-ex loss: 0.304797  [   23/   89]
per-ex loss: 0.492073  [   24/   89]
per-ex loss: 0.294698  [   25/   89]
per-ex loss: 0.308311  [   26/   89]
per-ex loss: 0.423442  [   27/   89]
per-ex loss: 0.375209  [   28/   89]
per-ex loss: 0.391430  [   29/   89]
per-ex loss: 0.369441  [   30/   89]
per-ex loss: 0.350708  [   31/   89]
per-ex loss: 0.723089  [   32/   89]
per-ex loss: 0.360937  [   33/   89]
per-ex loss: 0.362954  [   34/   89]
per-ex loss: 0.308920  [   35/   89]
per-ex loss: 0.498773  [   36/   89]
per-ex loss: 0.999930  [   37/   89]
per-ex loss: 0.310716  [   38/   89]
per-ex loss: 0.512690  [   39/   89]
per-ex loss: 0.371509  [   40/   89]
per-ex loss: 0.396537  [   41/   89]
per-ex loss: 0.530113  [   42/   89]
per-ex loss: 0.379078  [   43/   89]
per-ex loss: 0.241161  [   44/   89]
per-ex loss: 0.355548  [   45/   89]
per-ex loss: 0.420350  [   46/   89]
per-ex loss: 0.331515  [   47/   89]
per-ex loss: 0.388880  [   48/   89]
per-ex loss: 0.381463  [   49/   89]
per-ex loss: 0.339802  [   50/   89]
per-ex loss: 0.377131  [   51/   89]
per-ex loss: 0.416218  [   52/   89]
per-ex loss: 0.398763  [   53/   89]
per-ex loss: 0.525060  [   54/   89]
per-ex loss: 0.295463  [   55/   89]
per-ex loss: 0.331095  [   56/   89]
per-ex loss: 0.466530  [   57/   89]
per-ex loss: 0.328578  [   58/   89]
per-ex loss: 0.441779  [   59/   89]
per-ex loss: 0.399089  [   60/   89]
per-ex loss: 0.346407  [   61/   89]
per-ex loss: 0.531525  [   62/   89]
per-ex loss: 0.357570  [   63/   89]
per-ex loss: 0.292002  [   64/   89]
per-ex loss: 0.338794  [   65/   89]
per-ex loss: 0.616438  [   66/   89]
per-ex loss: 0.485010  [   67/   89]
per-ex loss: 0.374490  [   68/   89]
per-ex loss: 0.232620  [   69/   89]
per-ex loss: 0.529442  [   70/   89]
per-ex loss: 0.350090  [   71/   89]
per-ex loss: 0.371344  [   72/   89]
per-ex loss: 0.474029  [   73/   89]
per-ex loss: 0.430173  [   74/   89]
per-ex loss: 0.431454  [   75/   89]
per-ex loss: 0.330289  [   76/   89]
per-ex loss: 0.458951  [   77/   89]
per-ex loss: 0.304172  [   78/   89]
per-ex loss: 0.387530  [   79/   89]
per-ex loss: 0.324400  [   80/   89]
per-ex loss: 0.332618  [   81/   89]
per-ex loss: 0.532604  [   82/   89]
per-ex loss: 0.314233  [   83/   89]
per-ex loss: 0.468800  [   84/   89]
per-ex loss: 0.485182  [   85/   89]
per-ex loss: 0.324712  [   86/   89]
per-ex loss: 0.350459  [   87/   89]
per-ex loss: 0.350826  [   88/   89]
per-ex loss: 0.630275  [   89/   89]
Train Error: Avg loss: 0.42223181
validation Error: 
 Avg loss: 0.55544447 
 F1: 0.466796 
 Precision: 0.507666 
 Recall: 0.432016
 IoU: 0.304458

test Error: 
 Avg loss: 0.49391835 
 F1: 0.556960 
 Precision: 0.610774 
 Recall: 0.511860
 IoU: 0.385963

We have finished training iteration 112
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_110_.pth
per-ex loss: 0.334738  [    1/   89]
per-ex loss: 0.449821  [    2/   89]
per-ex loss: 0.319342  [    3/   89]
per-ex loss: 0.313477  [    4/   89]
per-ex loss: 0.392774  [    5/   89]
per-ex loss: 0.498618  [    6/   89]
per-ex loss: 0.354865  [    7/   89]
per-ex loss: 0.573293  [    8/   89]
per-ex loss: 0.314893  [    9/   89]
per-ex loss: 0.462756  [   10/   89]
per-ex loss: 0.189339  [   11/   89]
per-ex loss: 0.276246  [   12/   89]
per-ex loss: 0.311299  [   13/   89]
per-ex loss: 0.472718  [   14/   89]
per-ex loss: 0.310690  [   15/   89]
per-ex loss: 0.372787  [   16/   89]
per-ex loss: 0.340740  [   17/   89]
per-ex loss: 0.641884  [   18/   89]
per-ex loss: 0.394341  [   19/   89]
per-ex loss: 0.999931  [   20/   89]
per-ex loss: 0.622318  [   21/   89]
per-ex loss: 0.396247  [   22/   89]
per-ex loss: 0.323046  [   23/   89]
per-ex loss: 0.639415  [   24/   89]
per-ex loss: 0.259312  [   25/   89]
per-ex loss: 0.360736  [   26/   89]
per-ex loss: 0.342983  [   27/   89]
per-ex loss: 0.505532  [   28/   89]
per-ex loss: 0.400323  [   29/   89]
per-ex loss: 0.529719  [   30/   89]
per-ex loss: 0.299703  [   31/   89]
per-ex loss: 0.456254  [   32/   89]
per-ex loss: 0.306596  [   33/   89]
per-ex loss: 0.479352  [   34/   89]
per-ex loss: 0.336921  [   35/   89]
per-ex loss: 0.272319  [   36/   89]
per-ex loss: 0.568478  [   37/   89]
per-ex loss: 0.258687  [   38/   89]
per-ex loss: 0.375076  [   39/   89]
per-ex loss: 0.422117  [   40/   89]
per-ex loss: 0.583953  [   41/   89]
per-ex loss: 0.251136  [   42/   89]
per-ex loss: 0.329690  [   43/   89]
per-ex loss: 0.469758  [   44/   89]
per-ex loss: 0.506442  [   45/   89]
per-ex loss: 0.284696  [   46/   89]
per-ex loss: 0.305145  [   47/   89]
per-ex loss: 0.360219  [   48/   89]
per-ex loss: 0.540246  [   49/   89]
per-ex loss: 0.573206  [   50/   89]
per-ex loss: 0.429802  [   51/   89]
per-ex loss: 0.462070  [   52/   89]
per-ex loss: 0.335393  [   53/   89]
per-ex loss: 0.550792  [   54/   89]
per-ex loss: 0.356312  [   55/   89]
per-ex loss: 0.568795  [   56/   89]
per-ex loss: 0.432946  [   57/   89]
per-ex loss: 0.482814  [   58/   89]
per-ex loss: 0.421626  [   59/   89]
per-ex loss: 0.321691  [   60/   89]
per-ex loss: 0.220149  [   61/   89]
per-ex loss: 0.341468  [   62/   89]
per-ex loss: 0.430768  [   63/   89]
per-ex loss: 0.320207  [   64/   89]
per-ex loss: 0.282327  [   65/   89]
per-ex loss: 0.356602  [   66/   89]
per-ex loss: 0.533106  [   67/   89]
per-ex loss: 0.365664  [   68/   89]
per-ex loss: 0.469629  [   69/   89]
per-ex loss: 0.281731  [   70/   89]
per-ex loss: 0.483985  [   71/   89]
per-ex loss: 0.437152  [   72/   89]
per-ex loss: 0.378484  [   73/   89]
per-ex loss: 0.708321  [   74/   89]
per-ex loss: 0.471001  [   75/   89]
per-ex loss: 0.388313  [   76/   89]
per-ex loss: 0.403825  [   77/   89]
per-ex loss: 0.312794  [   78/   89]
per-ex loss: 0.485888  [   79/   89]
per-ex loss: 0.375889  [   80/   89]
per-ex loss: 0.331947  [   81/   89]
per-ex loss: 0.532201  [   82/   89]
per-ex loss: 0.540423  [   83/   89]
per-ex loss: 0.384433  [   84/   89]
per-ex loss: 0.505173  [   85/   89]
per-ex loss: 0.343849  [   86/   89]
per-ex loss: 0.351951  [   87/   89]
per-ex loss: 0.293615  [   88/   89]
per-ex loss: 0.414505  [   89/   89]
Train Error: Avg loss: 0.41339121
validation Error: 
 Avg loss: 0.53199701 
 F1: 0.498674 
 Precision: 0.564367 
 Recall: 0.446680
 IoU: 0.332156

test Error: 
 Avg loss: 0.47625127 
 F1: 0.573569 
 Precision: 0.617796 
 Recall: 0.535251
 IoU: 0.402101

We have finished training iteration 113
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_111_.pth
per-ex loss: 0.199518  [    1/   89]
per-ex loss: 0.533549  [    2/   89]
per-ex loss: 0.290664  [    3/   89]
per-ex loss: 0.265701  [    4/   89]
per-ex loss: 0.472262  [    5/   89]
per-ex loss: 0.490003  [    6/   89]
per-ex loss: 0.366099  [    7/   89]
per-ex loss: 0.194490  [    8/   89]
per-ex loss: 0.361171  [    9/   89]
per-ex loss: 0.281696  [   10/   89]
per-ex loss: 0.402162  [   11/   89]
per-ex loss: 0.305174  [   12/   89]
per-ex loss: 0.504148  [   13/   89]
per-ex loss: 0.344145  [   14/   89]
per-ex loss: 0.318926  [   15/   89]
per-ex loss: 0.589170  [   16/   89]
per-ex loss: 0.332863  [   17/   89]
per-ex loss: 0.513062  [   18/   89]
per-ex loss: 0.374828  [   19/   89]
per-ex loss: 0.340838  [   20/   89]
per-ex loss: 0.619388  [   21/   89]
per-ex loss: 0.378965  [   22/   89]
per-ex loss: 0.316137  [   23/   89]
per-ex loss: 0.473796  [   24/   89]
per-ex loss: 0.503485  [   25/   89]
per-ex loss: 0.267068  [   26/   89]
per-ex loss: 0.338363  [   27/   89]
per-ex loss: 0.438281  [   28/   89]
per-ex loss: 0.263793  [   29/   89]
per-ex loss: 0.594057  [   30/   89]
per-ex loss: 0.372563  [   31/   89]
per-ex loss: 0.365941  [   32/   89]
per-ex loss: 0.375154  [   33/   89]
per-ex loss: 0.297259  [   34/   89]
per-ex loss: 0.368386  [   35/   89]
per-ex loss: 0.584822  [   36/   89]
per-ex loss: 0.298007  [   37/   89]
per-ex loss: 0.542284  [   38/   89]
per-ex loss: 0.492473  [   39/   89]
per-ex loss: 0.491968  [   40/   89]
per-ex loss: 0.391027  [   41/   89]
per-ex loss: 0.427968  [   42/   89]
per-ex loss: 0.486766  [   43/   89]
per-ex loss: 0.531646  [   44/   89]
per-ex loss: 0.328731  [   45/   89]
per-ex loss: 0.367711  [   46/   89]
per-ex loss: 0.580953  [   47/   89]
per-ex loss: 0.343716  [   48/   89]
per-ex loss: 0.450676  [   49/   89]
per-ex loss: 0.336063  [   50/   89]
per-ex loss: 0.595457  [   51/   89]
per-ex loss: 0.999927  [   52/   89]
per-ex loss: 0.341609  [   53/   89]
per-ex loss: 0.610905  [   54/   89]
per-ex loss: 0.309575  [   55/   89]
per-ex loss: 0.521114  [   56/   89]
per-ex loss: 0.271002  [   57/   89]
per-ex loss: 0.534685  [   58/   89]
per-ex loss: 0.480620  [   59/   89]
per-ex loss: 0.512387  [   60/   89]
per-ex loss: 0.354561  [   61/   89]
per-ex loss: 0.281041  [   62/   89]
per-ex loss: 0.509840  [   63/   89]
per-ex loss: 0.336282  [   64/   89]
per-ex loss: 0.355186  [   65/   89]
per-ex loss: 0.383229  [   66/   89]
per-ex loss: 0.352843  [   67/   89]
per-ex loss: 0.314647  [   68/   89]
per-ex loss: 0.420438  [   69/   89]
per-ex loss: 0.500556  [   70/   89]
per-ex loss: 0.524867  [   71/   89]
per-ex loss: 0.379012  [   72/   89]
per-ex loss: 0.467258  [   73/   89]
per-ex loss: 0.458497  [   74/   89]
per-ex loss: 0.323129  [   75/   89]
per-ex loss: 0.418848  [   76/   89]
per-ex loss: 0.344486  [   77/   89]
per-ex loss: 0.342097  [   78/   89]
per-ex loss: 0.436823  [   79/   89]
per-ex loss: 0.345207  [   80/   89]
per-ex loss: 0.470168  [   81/   89]
per-ex loss: 0.310955  [   82/   89]
per-ex loss: 0.294887  [   83/   89]
per-ex loss: 0.384508  [   84/   89]
per-ex loss: 0.330849  [   85/   89]
per-ex loss: 0.382832  [   86/   89]
per-ex loss: 0.568313  [   87/   89]
per-ex loss: 0.371910  [   88/   89]
per-ex loss: 0.409351  [   89/   89]
Train Error: Avg loss: 0.41188560
validation Error: 
 Avg loss: 0.55017532 
 F1: 0.474869 
 Precision: 0.533386 
 Recall: 0.427923
 IoU: 0.311363

test Error: 
 Avg loss: 0.50445957 
 F1: 0.543731 
 Precision: 0.599651 
 Recall: 0.497350
 IoU: 0.373372

We have finished training iteration 114
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_112_.pth
per-ex loss: 0.378166  [    1/   89]
per-ex loss: 0.535367  [    2/   89]
per-ex loss: 0.601171  [    3/   89]
per-ex loss: 0.318196  [    4/   89]
per-ex loss: 0.339660  [    5/   89]
per-ex loss: 0.411972  [    6/   89]
per-ex loss: 0.227106  [    7/   89]
per-ex loss: 0.311941  [    8/   89]
per-ex loss: 0.360945  [    9/   89]
per-ex loss: 0.359236  [   10/   89]
per-ex loss: 0.411769  [   11/   89]
per-ex loss: 0.441719  [   12/   89]
per-ex loss: 0.454280  [   13/   89]
per-ex loss: 0.499664  [   14/   89]
per-ex loss: 0.482076  [   15/   89]
per-ex loss: 0.337829  [   16/   89]
per-ex loss: 0.179574  [   17/   89]
per-ex loss: 0.405917  [   18/   89]
per-ex loss: 0.508601  [   19/   89]
per-ex loss: 0.318559  [   20/   89]
per-ex loss: 0.485420  [   21/   89]
per-ex loss: 0.312351  [   22/   89]
per-ex loss: 0.494603  [   23/   89]
per-ex loss: 0.433396  [   24/   89]
per-ex loss: 0.316197  [   25/   89]
per-ex loss: 0.366349  [   26/   89]
per-ex loss: 0.287242  [   27/   89]
per-ex loss: 0.356183  [   28/   89]
per-ex loss: 0.519922  [   29/   89]
per-ex loss: 0.650419  [   30/   89]
per-ex loss: 0.535133  [   31/   89]
per-ex loss: 0.288866  [   32/   89]
per-ex loss: 0.385680  [   33/   89]
per-ex loss: 0.453374  [   34/   89]
per-ex loss: 0.341141  [   35/   89]
per-ex loss: 0.493969  [   36/   89]
per-ex loss: 0.281224  [   37/   89]
per-ex loss: 0.466768  [   38/   89]
per-ex loss: 0.609562  [   39/   89]
per-ex loss: 0.439736  [   40/   89]
per-ex loss: 0.419410  [   41/   89]
per-ex loss: 0.320757  [   42/   89]
per-ex loss: 0.547803  [   43/   89]
per-ex loss: 0.309189  [   44/   89]
per-ex loss: 0.370970  [   45/   89]
per-ex loss: 0.495974  [   46/   89]
per-ex loss: 0.252475  [   47/   89]
per-ex loss: 0.385765  [   48/   89]
per-ex loss: 0.284322  [   49/   89]
per-ex loss: 0.228307  [   50/   89]
per-ex loss: 0.297907  [   51/   89]
per-ex loss: 0.375909  [   52/   89]
per-ex loss: 0.347921  [   53/   89]
per-ex loss: 0.336072  [   54/   89]
per-ex loss: 0.301855  [   55/   89]
per-ex loss: 0.500015  [   56/   89]
per-ex loss: 0.486450  [   57/   89]
per-ex loss: 0.413495  [   58/   89]
per-ex loss: 0.317008  [   59/   89]
per-ex loss: 0.395176  [   60/   89]
per-ex loss: 0.323917  [   61/   89]
per-ex loss: 0.472144  [   62/   89]
per-ex loss: 0.604344  [   63/   89]
per-ex loss: 0.527258  [   64/   89]
per-ex loss: 0.374682  [   65/   89]
per-ex loss: 0.377212  [   66/   89]
per-ex loss: 0.356800  [   67/   89]
per-ex loss: 0.266265  [   68/   89]
per-ex loss: 0.366870  [   69/   89]
per-ex loss: 0.496330  [   70/   89]
per-ex loss: 0.517745  [   71/   89]
per-ex loss: 0.319826  [   72/   89]
per-ex loss: 0.593766  [   73/   89]
per-ex loss: 0.455351  [   74/   89]
per-ex loss: 0.502650  [   75/   89]
per-ex loss: 0.276782  [   76/   89]
per-ex loss: 0.405139  [   77/   89]
per-ex loss: 0.267946  [   78/   89]
per-ex loss: 0.309313  [   79/   89]
per-ex loss: 0.582141  [   80/   89]
per-ex loss: 0.347601  [   81/   89]
per-ex loss: 0.387613  [   82/   89]
per-ex loss: 0.298108  [   83/   89]
per-ex loss: 0.999879  [   84/   89]
per-ex loss: 0.324360  [   85/   89]
per-ex loss: 0.371538  [   86/   89]
per-ex loss: 0.277839  [   87/   89]
per-ex loss: 0.450717  [   88/   89]
per-ex loss: 0.331033  [   89/   89]
Train Error: Avg loss: 0.40419363
validation Error: 
 Avg loss: 0.58609041 
 F1: 0.443153 
 Precision: 0.483448 
 Recall: 0.409059
 IoU: 0.284648

test Error: 
 Avg loss: 0.55296189 
 F1: 0.494449 
 Precision: 0.478619 
 Recall: 0.511362
 IoU: 0.328418

We have finished training iteration 115
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_113_.pth
per-ex loss: 0.351814  [    1/   89]
per-ex loss: 0.503812  [    2/   89]
per-ex loss: 0.625457  [    3/   89]
per-ex loss: 0.310889  [    4/   89]
per-ex loss: 0.272966  [    5/   89]
per-ex loss: 0.392884  [    6/   89]
per-ex loss: 0.466500  [    7/   89]
per-ex loss: 0.377809  [    8/   89]
per-ex loss: 0.260345  [    9/   89]
per-ex loss: 0.310286  [   10/   89]
per-ex loss: 0.427913  [   11/   89]
per-ex loss: 0.495789  [   12/   89]
per-ex loss: 0.325029  [   13/   89]
per-ex loss: 0.458294  [   14/   89]
per-ex loss: 0.381569  [   15/   89]
per-ex loss: 0.357503  [   16/   89]
per-ex loss: 0.424464  [   17/   89]
per-ex loss: 0.641953  [   18/   89]
per-ex loss: 0.463261  [   19/   89]
per-ex loss: 0.319530  [   20/   89]
per-ex loss: 0.247361  [   21/   89]
per-ex loss: 0.644146  [   22/   89]
per-ex loss: 0.355617  [   23/   89]
per-ex loss: 0.445666  [   24/   89]
per-ex loss: 0.362742  [   25/   89]
per-ex loss: 0.487025  [   26/   89]
per-ex loss: 0.350227  [   27/   89]
per-ex loss: 0.392588  [   28/   89]
per-ex loss: 0.351562  [   29/   89]
per-ex loss: 0.518028  [   30/   89]
per-ex loss: 0.364678  [   31/   89]
per-ex loss: 0.526097  [   32/   89]
per-ex loss: 0.419740  [   33/   89]
per-ex loss: 0.356727  [   34/   89]
per-ex loss: 0.486170  [   35/   89]
per-ex loss: 0.306867  [   36/   89]
per-ex loss: 0.349888  [   37/   89]
per-ex loss: 0.171007  [   38/   89]
per-ex loss: 0.425725  [   39/   89]
per-ex loss: 0.497545  [   40/   89]
per-ex loss: 0.378886  [   41/   89]
per-ex loss: 0.275023  [   42/   89]
per-ex loss: 0.576830  [   43/   89]
per-ex loss: 0.326751  [   44/   89]
per-ex loss: 0.376233  [   45/   89]
per-ex loss: 0.327533  [   46/   89]
per-ex loss: 0.999889  [   47/   89]
per-ex loss: 0.608193  [   48/   89]
per-ex loss: 0.492842  [   49/   89]
per-ex loss: 0.350803  [   50/   89]
per-ex loss: 0.506961  [   51/   89]
per-ex loss: 0.325726  [   52/   89]
per-ex loss: 0.417473  [   53/   89]
per-ex loss: 0.574771  [   54/   89]
per-ex loss: 0.289007  [   55/   89]
per-ex loss: 0.459363  [   56/   89]
per-ex loss: 0.337273  [   57/   89]
per-ex loss: 0.357689  [   58/   89]
per-ex loss: 0.257363  [   59/   89]
per-ex loss: 0.300096  [   60/   89]
per-ex loss: 0.472702  [   61/   89]
per-ex loss: 0.358567  [   62/   89]
per-ex loss: 0.252728  [   63/   89]
per-ex loss: 0.336445  [   64/   89]
per-ex loss: 0.279935  [   65/   89]
per-ex loss: 0.350146  [   66/   89]
per-ex loss: 0.329215  [   67/   89]
per-ex loss: 0.455773  [   68/   89]
per-ex loss: 0.208005  [   69/   89]
per-ex loss: 0.422032  [   70/   89]
per-ex loss: 0.460468  [   71/   89]
per-ex loss: 0.535774  [   72/   89]
per-ex loss: 0.505113  [   73/   89]
per-ex loss: 0.538055  [   74/   89]
per-ex loss: 0.327264  [   75/   89]
per-ex loss: 0.350210  [   76/   89]
per-ex loss: 0.362682  [   77/   89]
per-ex loss: 0.482108  [   78/   89]
per-ex loss: 0.588527  [   79/   89]
per-ex loss: 0.263739  [   80/   89]
per-ex loss: 0.339630  [   81/   89]
per-ex loss: 0.310618  [   82/   89]
per-ex loss: 0.453431  [   83/   89]
per-ex loss: 0.469147  [   84/   89]
per-ex loss: 0.369440  [   85/   89]
per-ex loss: 0.522624  [   86/   89]
per-ex loss: 0.419368  [   87/   89]
per-ex loss: 0.482112  [   88/   89]
per-ex loss: 0.314403  [   89/   89]
Train Error: Avg loss: 0.40816224
validation Error: 
 Avg loss: 0.53721749 
 F1: 0.492523 
 Precision: 0.551843 
 Recall: 0.444719
 IoU: 0.326720

test Error: 
 Avg loss: 0.48858102 
 F1: 0.563377 
 Precision: 0.607146 
 Recall: 0.525494
 IoU: 0.392153

We have finished training iteration 116
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_114_.pth
per-ex loss: 0.300030  [    1/   89]
per-ex loss: 0.439378  [    2/   89]
per-ex loss: 0.355443  [    3/   89]
per-ex loss: 0.453003  [    4/   89]
per-ex loss: 0.516755  [    5/   89]
per-ex loss: 0.433865  [    6/   89]
per-ex loss: 0.281719  [    7/   89]
per-ex loss: 0.431696  [    8/   89]
per-ex loss: 0.361851  [    9/   89]
per-ex loss: 0.325662  [   10/   89]
per-ex loss: 0.373043  [   11/   89]
per-ex loss: 0.440927  [   12/   89]
per-ex loss: 0.348157  [   13/   89]
per-ex loss: 0.348747  [   14/   89]
per-ex loss: 0.520930  [   15/   89]
per-ex loss: 0.263951  [   16/   89]
per-ex loss: 0.367409  [   17/   89]
per-ex loss: 0.545660  [   18/   89]
per-ex loss: 0.308239  [   19/   89]
per-ex loss: 0.475876  [   20/   89]
per-ex loss: 0.404534  [   21/   89]
per-ex loss: 0.287217  [   22/   89]
per-ex loss: 0.420459  [   23/   89]
per-ex loss: 0.489612  [   24/   89]
per-ex loss: 0.366685  [   25/   89]
per-ex loss: 0.431083  [   26/   89]
per-ex loss: 0.571118  [   27/   89]
per-ex loss: 0.180072  [   28/   89]
per-ex loss: 0.277029  [   29/   89]
per-ex loss: 0.496388  [   30/   89]
per-ex loss: 0.576011  [   31/   89]
per-ex loss: 0.394372  [   32/   89]
per-ex loss: 0.381812  [   33/   89]
per-ex loss: 0.419014  [   34/   89]
per-ex loss: 0.293210  [   35/   89]
per-ex loss: 0.309895  [   36/   89]
per-ex loss: 0.506648  [   37/   89]
per-ex loss: 0.369435  [   38/   89]
per-ex loss: 0.540992  [   39/   89]
per-ex loss: 0.394246  [   40/   89]
per-ex loss: 0.276658  [   41/   89]
per-ex loss: 0.385035  [   42/   89]
per-ex loss: 0.536051  [   43/   89]
per-ex loss: 0.383963  [   44/   89]
per-ex loss: 0.315096  [   45/   89]
per-ex loss: 0.320923  [   46/   89]
per-ex loss: 0.484373  [   47/   89]
per-ex loss: 0.306681  [   48/   89]
per-ex loss: 0.469355  [   49/   89]
per-ex loss: 0.307964  [   50/   89]
per-ex loss: 0.497186  [   51/   89]
per-ex loss: 0.311675  [   52/   89]
per-ex loss: 0.472627  [   53/   89]
per-ex loss: 0.372239  [   54/   89]
per-ex loss: 0.491181  [   55/   89]
per-ex loss: 0.580526  [   56/   89]
per-ex loss: 0.443061  [   57/   89]
per-ex loss: 0.605093  [   58/   89]
per-ex loss: 0.469801  [   59/   89]
per-ex loss: 0.331864  [   60/   89]
per-ex loss: 0.323695  [   61/   89]
per-ex loss: 0.267009  [   62/   89]
per-ex loss: 0.369925  [   63/   89]
per-ex loss: 0.487620  [   64/   89]
per-ex loss: 0.450992  [   65/   89]
per-ex loss: 0.402869  [   66/   89]
per-ex loss: 0.570033  [   67/   89]
per-ex loss: 0.329041  [   68/   89]
per-ex loss: 0.301116  [   69/   89]
per-ex loss: 0.267406  [   70/   89]
per-ex loss: 0.529696  [   71/   89]
per-ex loss: 0.302055  [   72/   89]
per-ex loss: 0.420013  [   73/   89]
per-ex loss: 0.575324  [   74/   89]
per-ex loss: 0.287951  [   75/   89]
per-ex loss: 0.333688  [   76/   89]
per-ex loss: 0.999929  [   77/   89]
per-ex loss: 0.218393  [   78/   89]
per-ex loss: 0.535685  [   79/   89]
per-ex loss: 0.297300  [   80/   89]
per-ex loss: 0.381859  [   81/   89]
per-ex loss: 0.365279  [   82/   89]
per-ex loss: 0.318391  [   83/   89]
per-ex loss: 0.457982  [   84/   89]
per-ex loss: 0.622978  [   85/   89]
per-ex loss: 0.363148  [   86/   89]
per-ex loss: 0.274935  [   87/   89]
per-ex loss: 0.377422  [   88/   89]
per-ex loss: 0.348303  [   89/   89]
Train Error: Avg loss: 0.40610744
validation Error: 
 Avg loss: 0.55287144 
 F1: 0.472888 
 Precision: 0.561112 
 Recall: 0.408637
 IoU: 0.309661

test Error: 
 Avg loss: 0.51716908 
 F1: 0.526789 
 Precision: 0.583603 
 Recall: 0.480056
 IoU: 0.357579

We have finished training iteration 117
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_115_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.388006  [    1/   89]
per-ex loss: 0.295044  [    2/   89]
per-ex loss: 0.577659  [    3/   89]
per-ex loss: 0.421346  [    4/   89]
per-ex loss: 0.379987  [    5/   89]
per-ex loss: 0.334614  [    6/   89]
per-ex loss: 0.313042  [    7/   89]
per-ex loss: 0.628733  [    8/   89]
per-ex loss: 0.359810  [    9/   89]
per-ex loss: 0.572874  [   10/   89]
per-ex loss: 0.337307  [   11/   89]
per-ex loss: 0.999959  [   12/   89]
per-ex loss: 0.303284  [   13/   89]
per-ex loss: 0.291730  [   14/   89]
per-ex loss: 0.561568  [   15/   89]
per-ex loss: 0.469553  [   16/   89]
per-ex loss: 0.648455  [   17/   89]
per-ex loss: 0.492045  [   18/   89]
per-ex loss: 0.345561  [   19/   89]
per-ex loss: 0.437740  [   20/   89]
per-ex loss: 0.615478  [   21/   89]
per-ex loss: 0.458091  [   22/   89]
per-ex loss: 0.336822  [   23/   89]
per-ex loss: 0.437775  [   24/   89]
per-ex loss: 0.387505  [   25/   89]
per-ex loss: 0.256410  [   26/   89]
per-ex loss: 0.445188  [   27/   89]
per-ex loss: 0.556276  [   28/   89]
per-ex loss: 0.415915  [   29/   89]
per-ex loss: 0.527137  [   30/   89]
per-ex loss: 0.304289  [   31/   89]
per-ex loss: 0.377460  [   32/   89]
per-ex loss: 0.446769  [   33/   89]
per-ex loss: 0.343043  [   34/   89]
per-ex loss: 0.526507  [   35/   89]
per-ex loss: 0.297187  [   36/   89]
per-ex loss: 0.355470  [   37/   89]
per-ex loss: 0.335745  [   38/   89]
per-ex loss: 0.467825  [   39/   89]
per-ex loss: 0.535146  [   40/   89]
per-ex loss: 0.416171  [   41/   89]
per-ex loss: 0.311453  [   42/   89]
per-ex loss: 0.395216  [   43/   89]
per-ex loss: 0.344871  [   44/   89]
per-ex loss: 0.332097  [   45/   89]
per-ex loss: 0.532970  [   46/   89]
per-ex loss: 0.471132  [   47/   89]
per-ex loss: 0.400819  [   48/   89]
per-ex loss: 0.367802  [   49/   89]
per-ex loss: 0.352487  [   50/   89]
per-ex loss: 0.351300  [   51/   89]
per-ex loss: 0.637233  [   52/   89]
per-ex loss: 0.290717  [   53/   89]
per-ex loss: 0.585523  [   54/   89]
per-ex loss: 0.261643  [   55/   89]
per-ex loss: 0.524423  [   56/   89]
per-ex loss: 0.505090  [   57/   89]
per-ex loss: 0.443119  [   58/   89]
per-ex loss: 0.393038  [   59/   89]
per-ex loss: 0.670471  [   60/   89]
per-ex loss: 0.509169  [   61/   89]
per-ex loss: 0.177714  [   62/   89]
per-ex loss: 0.510354  [   63/   89]
per-ex loss: 0.243739  [   64/   89]
per-ex loss: 0.359969  [   65/   89]
per-ex loss: 0.320371  [   66/   89]
per-ex loss: 0.345012  [   67/   89]
per-ex loss: 0.389850  [   68/   89]
per-ex loss: 0.385248  [   69/   89]
per-ex loss: 0.331134  [   70/   89]
per-ex loss: 0.448931  [   71/   89]
per-ex loss: 0.333828  [   72/   89]
per-ex loss: 0.385394  [   73/   89]
per-ex loss: 0.518838  [   74/   89]
per-ex loss: 0.518894  [   75/   89]
per-ex loss: 0.322071  [   76/   89]
per-ex loss: 0.254040  [   77/   89]
per-ex loss: 0.352456  [   78/   89]
per-ex loss: 0.293733  [   79/   89]
per-ex loss: 0.441189  [   80/   89]
per-ex loss: 0.586589  [   81/   89]
per-ex loss: 0.423581  [   82/   89]
per-ex loss: 0.454158  [   83/   89]
per-ex loss: 0.311707  [   84/   89]
per-ex loss: 0.640198  [   85/   89]
per-ex loss: 0.313619  [   86/   89]
per-ex loss: 0.294665  [   87/   89]
per-ex loss: 0.494757  [   88/   89]
per-ex loss: 0.394764  [   89/   89]
Train Error: Avg loss: 0.42166182
validation Error: 
 Avg loss: 0.56342874 
 F1: 0.466699 
 Precision: 0.438327 
 Recall: 0.498999
 IoU: 0.304376

test Error: 
 Avg loss: 0.48553560 
 F1: 0.556734 
 Precision: 0.534186 
 Recall: 0.581269
 IoU: 0.385746

We have finished training iteration 118
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_116_.pth
per-ex loss: 0.542424  [    1/   89]
per-ex loss: 0.375431  [    2/   89]
per-ex loss: 0.531283  [    3/   89]
per-ex loss: 0.381366  [    4/   89]
per-ex loss: 0.411854  [    5/   89]
per-ex loss: 0.635271  [    6/   89]
per-ex loss: 0.320240  [    7/   89]
per-ex loss: 0.342022  [    8/   89]
per-ex loss: 0.384047  [    9/   89]
per-ex loss: 0.295183  [   10/   89]
per-ex loss: 0.331012  [   11/   89]
per-ex loss: 0.534509  [   12/   89]
per-ex loss: 0.408936  [   13/   89]
per-ex loss: 0.374313  [   14/   89]
per-ex loss: 0.366502  [   15/   89]
per-ex loss: 0.502784  [   16/   89]
per-ex loss: 0.309024  [   17/   89]
per-ex loss: 0.573183  [   18/   89]
per-ex loss: 0.308242  [   19/   89]
per-ex loss: 0.373353  [   20/   89]
per-ex loss: 0.593302  [   21/   89]
per-ex loss: 0.376221  [   22/   89]
per-ex loss: 0.458080  [   23/   89]
per-ex loss: 0.580419  [   24/   89]
per-ex loss: 0.323064  [   25/   89]
per-ex loss: 0.380185  [   26/   89]
per-ex loss: 0.278159  [   27/   89]
per-ex loss: 0.494102  [   28/   89]
per-ex loss: 0.512155  [   29/   89]
per-ex loss: 0.438343  [   30/   89]
per-ex loss: 0.999879  [   31/   89]
per-ex loss: 0.338191  [   32/   89]
per-ex loss: 0.488440  [   33/   89]
per-ex loss: 0.346835  [   34/   89]
per-ex loss: 0.430752  [   35/   89]
per-ex loss: 0.598633  [   36/   89]
per-ex loss: 0.303376  [   37/   89]
per-ex loss: 0.587258  [   38/   89]
per-ex loss: 0.261327  [   39/   89]
per-ex loss: 0.439938  [   40/   89]
per-ex loss: 0.376383  [   41/   89]
per-ex loss: 0.260040  [   42/   89]
per-ex loss: 0.411344  [   43/   89]
per-ex loss: 0.392730  [   44/   89]
per-ex loss: 0.265255  [   45/   89]
per-ex loss: 0.350349  [   46/   89]
per-ex loss: 0.537046  [   47/   89]
per-ex loss: 0.320189  [   48/   89]
per-ex loss: 0.461794  [   49/   89]
per-ex loss: 0.584843  [   50/   89]
per-ex loss: 0.382728  [   51/   89]
per-ex loss: 0.270612  [   52/   89]
per-ex loss: 0.457996  [   53/   89]
per-ex loss: 0.469128  [   54/   89]
per-ex loss: 0.248287  [   55/   89]
per-ex loss: 0.321750  [   56/   89]
per-ex loss: 0.316072  [   57/   89]
per-ex loss: 0.329088  [   58/   89]
per-ex loss: 0.507048  [   59/   89]
per-ex loss: 0.457984  [   60/   89]
per-ex loss: 0.249243  [   61/   89]
per-ex loss: 0.482624  [   62/   89]
per-ex loss: 0.336091  [   63/   89]
per-ex loss: 0.378272  [   64/   89]
per-ex loss: 0.339446  [   65/   89]
per-ex loss: 0.363830  [   66/   89]
per-ex loss: 0.315832  [   67/   89]
per-ex loss: 0.330236  [   68/   89]
per-ex loss: 0.326593  [   69/   89]
per-ex loss: 0.614326  [   70/   89]
per-ex loss: 0.321380  [   71/   89]
per-ex loss: 0.403334  [   72/   89]
per-ex loss: 0.269346  [   73/   89]
per-ex loss: 0.639896  [   74/   89]
per-ex loss: 0.422095  [   75/   89]
per-ex loss: 0.334928  [   76/   89]
per-ex loss: 0.373360  [   77/   89]
per-ex loss: 0.254206  [   78/   89]
per-ex loss: 0.430803  [   79/   89]
per-ex loss: 0.465470  [   80/   89]
per-ex loss: 0.546139  [   81/   89]
per-ex loss: 0.549356  [   82/   89]
per-ex loss: 0.374628  [   83/   89]
per-ex loss: 0.469713  [   84/   89]
per-ex loss: 0.647692  [   85/   89]
per-ex loss: 0.483784  [   86/   89]
per-ex loss: 0.526427  [   87/   89]
per-ex loss: 0.423158  [   88/   89]
per-ex loss: 0.235690  [   89/   89]
Train Error: Avg loss: 0.41750791
validation Error: 
 Avg loss: 0.54468813 
 F1: 0.484751 
 Precision: 0.479956 
 Recall: 0.489642
 IoU: 0.319915

test Error: 
 Avg loss: 0.48647576 
 F1: 0.556332 
 Precision: 0.541347 
 Recall: 0.572169
 IoU: 0.385360

We have finished training iteration 119
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_117_.pth
per-ex loss: 0.511386  [    1/   89]
per-ex loss: 0.288051  [    2/   89]
per-ex loss: 0.485618  [    3/   89]
per-ex loss: 0.315606  [    4/   89]
per-ex loss: 0.521942  [    5/   89]
per-ex loss: 0.302289  [    6/   89]
per-ex loss: 0.401092  [    7/   89]
per-ex loss: 0.351015  [    8/   89]
per-ex loss: 0.386956  [    9/   89]
per-ex loss: 0.610912  [   10/   89]
per-ex loss: 0.354040  [   11/   89]
per-ex loss: 0.359749  [   12/   89]
per-ex loss: 0.514514  [   13/   89]
per-ex loss: 0.320375  [   14/   89]
per-ex loss: 0.274442  [   15/   89]
per-ex loss: 0.322426  [   16/   89]
per-ex loss: 0.302988  [   17/   89]
per-ex loss: 0.301045  [   18/   89]
per-ex loss: 0.341171  [   19/   89]
per-ex loss: 0.460365  [   20/   89]
per-ex loss: 0.350284  [   21/   89]
per-ex loss: 0.417340  [   22/   89]
per-ex loss: 0.328824  [   23/   89]
per-ex loss: 0.999933  [   24/   89]
per-ex loss: 0.449255  [   25/   89]
per-ex loss: 0.340294  [   26/   89]
per-ex loss: 0.358479  [   27/   89]
per-ex loss: 0.318742  [   28/   89]
per-ex loss: 0.357096  [   29/   89]
per-ex loss: 0.320978  [   30/   89]
per-ex loss: 0.275636  [   31/   89]
per-ex loss: 0.509749  [   32/   89]
per-ex loss: 0.372242  [   33/   89]
per-ex loss: 0.266139  [   34/   89]
per-ex loss: 0.519617  [   35/   89]
per-ex loss: 0.323814  [   36/   89]
per-ex loss: 0.300715  [   37/   89]
per-ex loss: 0.278151  [   38/   89]
per-ex loss: 0.527046  [   39/   89]
per-ex loss: 0.522967  [   40/   89]
per-ex loss: 0.331029  [   41/   89]
per-ex loss: 0.427003  [   42/   89]
per-ex loss: 0.205967  [   43/   89]
per-ex loss: 0.432766  [   44/   89]
per-ex loss: 0.482714  [   45/   89]
per-ex loss: 0.256178  [   46/   89]
per-ex loss: 0.395627  [   47/   89]
per-ex loss: 0.543087  [   48/   89]
per-ex loss: 0.495424  [   49/   89]
per-ex loss: 0.412881  [   50/   89]
per-ex loss: 0.609102  [   51/   89]
per-ex loss: 0.565583  [   52/   89]
per-ex loss: 0.598690  [   53/   89]
per-ex loss: 0.457944  [   54/   89]
per-ex loss: 0.509415  [   55/   89]
per-ex loss: 0.326425  [   56/   89]
per-ex loss: 0.415079  [   57/   89]
per-ex loss: 0.354941  [   58/   89]
per-ex loss: 0.589069  [   59/   89]
per-ex loss: 0.469533  [   60/   89]
per-ex loss: 0.379065  [   61/   89]
per-ex loss: 0.293479  [   62/   89]
per-ex loss: 0.341337  [   63/   89]
per-ex loss: 0.510739  [   64/   89]
per-ex loss: 0.334531  [   65/   89]
per-ex loss: 0.387939  [   66/   89]
per-ex loss: 0.548180  [   67/   89]
per-ex loss: 0.315161  [   68/   89]
per-ex loss: 0.592687  [   69/   89]
per-ex loss: 0.368387  [   70/   89]
per-ex loss: 0.298677  [   71/   89]
per-ex loss: 0.380685  [   72/   89]
per-ex loss: 0.318194  [   73/   89]
per-ex loss: 0.321364  [   74/   89]
per-ex loss: 0.428557  [   75/   89]
per-ex loss: 0.304460  [   76/   89]
per-ex loss: 0.416940  [   77/   89]
per-ex loss: 0.274762  [   78/   89]
per-ex loss: 0.405883  [   79/   89]
per-ex loss: 0.305434  [   80/   89]
per-ex loss: 0.255009  [   81/   89]
per-ex loss: 0.405779  [   82/   89]
per-ex loss: 0.445446  [   83/   89]
per-ex loss: 0.339020  [   84/   89]
per-ex loss: 0.515483  [   85/   89]
per-ex loss: 0.511476  [   86/   89]
per-ex loss: 0.358298  [   87/   89]
per-ex loss: 0.383123  [   88/   89]
per-ex loss: 0.614859  [   89/   89]
Train Error: Avg loss: 0.40558085
validation Error: 
 Avg loss: 0.53184005 
 F1: 0.497935 
 Precision: 0.546657 
 Recall: 0.457187
 IoU: 0.331500

test Error: 
 Avg loss: 0.48503006 
 F1: 0.560614 
 Precision: 0.601279 
 Recall: 0.525100
 IoU: 0.389481

We have finished training iteration 120
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_118_.pth
per-ex loss: 0.451915  [    1/   89]
per-ex loss: 0.345091  [    2/   89]
per-ex loss: 0.372695  [    3/   89]
per-ex loss: 0.523745  [    4/   89]
per-ex loss: 0.302825  [    5/   89]
per-ex loss: 0.504681  [    6/   89]
per-ex loss: 0.569182  [    7/   89]
per-ex loss: 0.572551  [    8/   89]
per-ex loss: 0.402088  [    9/   89]
per-ex loss: 0.434008  [   10/   89]
per-ex loss: 0.557591  [   11/   89]
per-ex loss: 0.279351  [   12/   89]
per-ex loss: 0.446535  [   13/   89]
per-ex loss: 0.265357  [   14/   89]
per-ex loss: 0.380790  [   15/   89]
per-ex loss: 0.488842  [   16/   89]
per-ex loss: 0.267751  [   17/   89]
per-ex loss: 0.340938  [   18/   89]
per-ex loss: 0.311459  [   19/   89]
per-ex loss: 0.373565  [   20/   89]
per-ex loss: 0.604449  [   21/   89]
per-ex loss: 0.308941  [   22/   89]
per-ex loss: 0.451891  [   23/   89]
per-ex loss: 0.392518  [   24/   89]
per-ex loss: 0.393730  [   25/   89]
per-ex loss: 0.238203  [   26/   89]
per-ex loss: 0.344075  [   27/   89]
per-ex loss: 0.391681  [   28/   89]
per-ex loss: 0.264517  [   29/   89]
per-ex loss: 0.524796  [   30/   89]
per-ex loss: 0.270875  [   31/   89]
per-ex loss: 0.434647  [   32/   89]
per-ex loss: 0.284438  [   33/   89]
per-ex loss: 0.363875  [   34/   89]
per-ex loss: 0.279763  [   35/   89]
per-ex loss: 0.515254  [   36/   89]
per-ex loss: 0.456934  [   37/   89]
per-ex loss: 0.449578  [   38/   89]
per-ex loss: 0.332932  [   39/   89]
per-ex loss: 0.387956  [   40/   89]
per-ex loss: 0.558584  [   41/   89]
per-ex loss: 0.318348  [   42/   89]
per-ex loss: 0.338806  [   43/   89]
per-ex loss: 0.473899  [   44/   89]
per-ex loss: 0.599025  [   45/   89]
per-ex loss: 0.337903  [   46/   89]
per-ex loss: 0.355017  [   47/   89]
per-ex loss: 0.484775  [   48/   89]
per-ex loss: 0.521163  [   49/   89]
per-ex loss: 0.367167  [   50/   89]
per-ex loss: 0.999879  [   51/   89]
per-ex loss: 0.328212  [   52/   89]
per-ex loss: 0.361559  [   53/   89]
per-ex loss: 0.323583  [   54/   89]
per-ex loss: 0.302286  [   55/   89]
per-ex loss: 0.594495  [   56/   89]
per-ex loss: 0.364717  [   57/   89]
per-ex loss: 0.366220  [   58/   89]
per-ex loss: 0.315580  [   59/   89]
per-ex loss: 0.307626  [   60/   89]
per-ex loss: 0.365380  [   61/   89]
per-ex loss: 0.190950  [   62/   89]
per-ex loss: 0.377084  [   63/   89]
per-ex loss: 0.375703  [   64/   89]
per-ex loss: 0.380818  [   65/   89]
per-ex loss: 0.482631  [   66/   89]
per-ex loss: 0.338489  [   67/   89]
per-ex loss: 0.581639  [   68/   89]
per-ex loss: 0.257686  [   69/   89]
per-ex loss: 0.535437  [   70/   89]
per-ex loss: 0.420847  [   71/   89]
per-ex loss: 0.410579  [   72/   89]
per-ex loss: 0.275930  [   73/   89]
per-ex loss: 0.507136  [   74/   89]
per-ex loss: 0.317836  [   75/   89]
per-ex loss: 0.297352  [   76/   89]
per-ex loss: 0.472151  [   77/   89]
per-ex loss: 0.306701  [   78/   89]
per-ex loss: 0.475256  [   79/   89]
per-ex loss: 0.470932  [   80/   89]
per-ex loss: 0.385676  [   81/   89]
per-ex loss: 0.323535  [   82/   89]
per-ex loss: 0.339944  [   83/   89]
per-ex loss: 0.345154  [   84/   89]
per-ex loss: 0.330387  [   85/   89]
per-ex loss: 0.350640  [   86/   89]
per-ex loss: 0.334623  [   87/   89]
per-ex loss: 0.600540  [   88/   89]
per-ex loss: 0.329858  [   89/   89]
Train Error: Avg loss: 0.40091851
validation Error: 
 Avg loss: 0.53831364 
 F1: 0.485895 
 Precision: 0.518693 
 Recall: 0.456998
 IoU: 0.320912

test Error: 
 Avg loss: 0.48678260 
 F1: 0.559356 
 Precision: 0.591923 
 Recall: 0.530186
 IoU: 0.388268

We have finished training iteration 121
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_119_.pth
per-ex loss: 0.310591  [    1/   89]
per-ex loss: 0.393502  [    2/   89]
per-ex loss: 0.581972  [    3/   89]
per-ex loss: 0.534501  [    4/   89]
per-ex loss: 0.505259  [    5/   89]
per-ex loss: 0.384319  [    6/   89]
per-ex loss: 0.462590  [    7/   89]
per-ex loss: 0.366084  [    8/   89]
per-ex loss: 0.538744  [    9/   89]
per-ex loss: 0.286797  [   10/   89]
per-ex loss: 0.301466  [   11/   89]
per-ex loss: 0.317538  [   12/   89]
per-ex loss: 0.345437  [   13/   89]
per-ex loss: 0.327448  [   14/   89]
per-ex loss: 0.224683  [   15/   89]
per-ex loss: 0.454948  [   16/   89]
per-ex loss: 0.486128  [   17/   89]
per-ex loss: 0.270683  [   18/   89]
per-ex loss: 0.523166  [   19/   89]
per-ex loss: 0.292618  [   20/   89]
per-ex loss: 0.472677  [   21/   89]
per-ex loss: 0.403734  [   22/   89]
per-ex loss: 0.318835  [   23/   89]
per-ex loss: 0.552416  [   24/   89]
per-ex loss: 0.553099  [   25/   89]
per-ex loss: 0.505030  [   26/   89]
per-ex loss: 0.445305  [   27/   89]
per-ex loss: 0.498896  [   28/   89]
per-ex loss: 0.247551  [   29/   89]
per-ex loss: 0.334655  [   30/   89]
per-ex loss: 0.354179  [   31/   89]
per-ex loss: 0.327640  [   32/   89]
per-ex loss: 0.571598  [   33/   89]
per-ex loss: 0.446888  [   34/   89]
per-ex loss: 0.520243  [   35/   89]
per-ex loss: 0.347697  [   36/   89]
per-ex loss: 0.444516  [   37/   89]
per-ex loss: 0.379838  [   38/   89]
per-ex loss: 0.373809  [   39/   89]
per-ex loss: 0.588295  [   40/   89]
per-ex loss: 0.410227  [   41/   89]
per-ex loss: 0.238074  [   42/   89]
per-ex loss: 0.305954  [   43/   89]
per-ex loss: 0.524020  [   44/   89]
per-ex loss: 0.422870  [   45/   89]
per-ex loss: 0.485199  [   46/   89]
per-ex loss: 0.377579  [   47/   89]
per-ex loss: 0.460039  [   48/   89]
per-ex loss: 0.373161  [   49/   89]
per-ex loss: 0.471744  [   50/   89]
per-ex loss: 0.358902  [   51/   89]
per-ex loss: 0.669390  [   52/   89]
per-ex loss: 0.414639  [   53/   89]
per-ex loss: 0.354087  [   54/   89]
per-ex loss: 0.283521  [   55/   89]
per-ex loss: 0.603096  [   56/   89]
per-ex loss: 0.414662  [   57/   89]
per-ex loss: 0.335226  [   58/   89]
per-ex loss: 0.613749  [   59/   89]
per-ex loss: 0.364467  [   60/   89]
per-ex loss: 0.446963  [   61/   89]
per-ex loss: 0.563409  [   62/   89]
per-ex loss: 0.278163  [   63/   89]
per-ex loss: 0.289165  [   64/   89]
per-ex loss: 0.305231  [   65/   89]
per-ex loss: 0.481109  [   66/   89]
per-ex loss: 0.570270  [   67/   89]
per-ex loss: 0.346769  [   68/   89]
per-ex loss: 0.268632  [   69/   89]
per-ex loss: 0.406417  [   70/   89]
per-ex loss: 0.282900  [   71/   89]
per-ex loss: 0.278376  [   72/   89]
per-ex loss: 0.346402  [   73/   89]
per-ex loss: 0.598780  [   74/   89]
per-ex loss: 0.515419  [   75/   89]
per-ex loss: 0.345274  [   76/   89]
per-ex loss: 0.329126  [   77/   89]
per-ex loss: 0.319845  [   78/   89]
per-ex loss: 0.399700  [   79/   89]
per-ex loss: 0.363128  [   80/   89]
per-ex loss: 0.999885  [   81/   89]
per-ex loss: 0.452353  [   82/   89]
per-ex loss: 0.366203  [   83/   89]
per-ex loss: 0.615753  [   84/   89]
per-ex loss: 0.335341  [   85/   89]
per-ex loss: 0.382659  [   86/   89]
per-ex loss: 0.323195  [   87/   89]
per-ex loss: 0.494397  [   88/   89]
per-ex loss: 0.370963  [   89/   89]
Train Error: Avg loss: 0.41743604
validation Error: 
 Avg loss: 0.55717430 
 F1: 0.472632 
 Precision: 0.509952 
 Recall: 0.440403
 IoU: 0.309443

test Error: 
 Avg loss: 0.52636890 
 F1: 0.528334 
 Precision: 0.503978 
 Recall: 0.555163
 IoU: 0.359004

We have finished training iteration 122
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_120_.pth
per-ex loss: 0.362996  [    1/   89]
per-ex loss: 0.360413  [    2/   89]
per-ex loss: 0.518143  [    3/   89]
per-ex loss: 0.315602  [    4/   89]
per-ex loss: 0.999882  [    5/   89]
per-ex loss: 0.339130  [    6/   89]
per-ex loss: 0.297853  [    7/   89]
per-ex loss: 0.485959  [    8/   89]
per-ex loss: 0.500969  [    9/   89]
per-ex loss: 0.504201  [   10/   89]
per-ex loss: 0.374359  [   11/   89]
per-ex loss: 0.426218  [   12/   89]
per-ex loss: 0.350112  [   13/   89]
per-ex loss: 0.383444  [   14/   89]
per-ex loss: 0.409675  [   15/   89]
per-ex loss: 0.439874  [   16/   89]
per-ex loss: 0.283638  [   17/   89]
per-ex loss: 0.634726  [   18/   89]
per-ex loss: 0.449179  [   19/   89]
per-ex loss: 0.323013  [   20/   89]
per-ex loss: 0.532400  [   21/   89]
per-ex loss: 0.308603  [   22/   89]
per-ex loss: 0.472287  [   23/   89]
per-ex loss: 0.476289  [   24/   89]
per-ex loss: 0.516630  [   25/   89]
per-ex loss: 0.286627  [   26/   89]
per-ex loss: 0.521602  [   27/   89]
per-ex loss: 0.365210  [   28/   89]
per-ex loss: 0.393175  [   29/   89]
per-ex loss: 0.246344  [   30/   89]
per-ex loss: 0.341184  [   31/   89]
per-ex loss: 0.375205  [   32/   89]
per-ex loss: 0.268978  [   33/   89]
per-ex loss: 0.379116  [   34/   89]
per-ex loss: 0.525909  [   35/   89]
per-ex loss: 0.330907  [   36/   89]
per-ex loss: 0.489952  [   37/   89]
per-ex loss: 0.618046  [   38/   89]
per-ex loss: 0.329748  [   39/   89]
per-ex loss: 0.340983  [   40/   89]
per-ex loss: 0.350558  [   41/   89]
per-ex loss: 0.189375  [   42/   89]
per-ex loss: 0.351941  [   43/   89]
per-ex loss: 0.621978  [   44/   89]
per-ex loss: 0.319295  [   45/   89]
per-ex loss: 0.326925  [   46/   89]
per-ex loss: 0.591205  [   47/   89]
per-ex loss: 0.291831  [   48/   89]
per-ex loss: 0.338196  [   49/   89]
per-ex loss: 0.403698  [   50/   89]
per-ex loss: 0.385936  [   51/   89]
per-ex loss: 0.590071  [   52/   89]
per-ex loss: 0.292286  [   53/   89]
per-ex loss: 0.578426  [   54/   89]
per-ex loss: 0.340261  [   55/   89]
per-ex loss: 0.309715  [   56/   89]
per-ex loss: 0.361623  [   57/   89]
per-ex loss: 0.273099  [   58/   89]
per-ex loss: 0.469038  [   59/   89]
per-ex loss: 0.315654  [   60/   89]
per-ex loss: 0.493769  [   61/   89]
per-ex loss: 0.515989  [   62/   89]
per-ex loss: 0.310786  [   63/   89]
per-ex loss: 0.343723  [   64/   89]
per-ex loss: 0.438200  [   65/   89]
per-ex loss: 0.521921  [   66/   89]
per-ex loss: 0.329211  [   67/   89]
per-ex loss: 0.583588  [   68/   89]
per-ex loss: 0.409217  [   69/   89]
per-ex loss: 0.482057  [   70/   89]
per-ex loss: 0.243687  [   71/   89]
per-ex loss: 0.240237  [   72/   89]
per-ex loss: 0.369334  [   73/   89]
per-ex loss: 0.374256  [   74/   89]
per-ex loss: 0.464256  [   75/   89]
per-ex loss: 0.515011  [   76/   89]
per-ex loss: 0.515511  [   77/   89]
per-ex loss: 0.518331  [   78/   89]
per-ex loss: 0.304449  [   79/   89]
per-ex loss: 0.328203  [   80/   89]
per-ex loss: 0.357126  [   81/   89]
per-ex loss: 0.370108  [   82/   89]
per-ex loss: 0.510160  [   83/   89]
per-ex loss: 0.410415  [   84/   89]
per-ex loss: 0.430873  [   85/   89]
per-ex loss: 0.463993  [   86/   89]
per-ex loss: 0.264270  [   87/   89]
per-ex loss: 0.317161  [   88/   89]
per-ex loss: 0.405633  [   89/   89]
Train Error: Avg loss: 0.40911427
validation Error: 
 Avg loss: 0.52957135 
 F1: 0.498782 
 Precision: 0.575038 
 Recall: 0.440383
 IoU: 0.332252

test Error: 
 Avg loss: 0.48508995 
 F1: 0.563664 
 Precision: 0.629552 
 Recall: 0.510261
 IoU: 0.392432

We have finished training iteration 123
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_121_.pth
per-ex loss: 0.596403  [    1/   89]
per-ex loss: 0.356169  [    2/   89]
per-ex loss: 0.318044  [    3/   89]
per-ex loss: 0.367091  [    4/   89]
per-ex loss: 0.463452  [    5/   89]
per-ex loss: 0.358223  [    6/   89]
per-ex loss: 0.395981  [    7/   89]
per-ex loss: 0.446988  [    8/   89]
per-ex loss: 0.329103  [    9/   89]
per-ex loss: 0.305787  [   10/   89]
per-ex loss: 0.292851  [   11/   89]
per-ex loss: 0.496530  [   12/   89]
per-ex loss: 0.215374  [   13/   89]
per-ex loss: 0.475089  [   14/   89]
per-ex loss: 0.322642  [   15/   89]
per-ex loss: 0.357588  [   16/   89]
per-ex loss: 0.407450  [   17/   89]
per-ex loss: 0.438438  [   18/   89]
per-ex loss: 0.303655  [   19/   89]
per-ex loss: 0.490310  [   20/   89]
per-ex loss: 0.397182  [   21/   89]
per-ex loss: 0.330939  [   22/   89]
per-ex loss: 0.471211  [   23/   89]
per-ex loss: 0.415904  [   24/   89]
per-ex loss: 0.639872  [   25/   89]
per-ex loss: 0.394178  [   26/   89]
per-ex loss: 0.363354  [   27/   89]
per-ex loss: 0.376347  [   28/   89]
per-ex loss: 0.331374  [   29/   89]
per-ex loss: 0.569776  [   30/   89]
per-ex loss: 0.352290  [   31/   89]
per-ex loss: 0.680291  [   32/   89]
per-ex loss: 0.335408  [   33/   89]
per-ex loss: 0.314683  [   34/   89]
per-ex loss: 0.346579  [   35/   89]
per-ex loss: 0.337102  [   36/   89]
per-ex loss: 0.282330  [   37/   89]
per-ex loss: 0.591650  [   38/   89]
per-ex loss: 0.315581  [   39/   89]
per-ex loss: 0.353975  [   40/   89]
per-ex loss: 0.234641  [   41/   89]
per-ex loss: 0.576476  [   42/   89]
per-ex loss: 0.525621  [   43/   89]
per-ex loss: 0.470568  [   44/   89]
per-ex loss: 0.231415  [   45/   89]
per-ex loss: 0.337433  [   46/   89]
per-ex loss: 0.521220  [   47/   89]
per-ex loss: 0.673103  [   48/   89]
per-ex loss: 0.309563  [   49/   89]
per-ex loss: 0.380398  [   50/   89]
per-ex loss: 0.498478  [   51/   89]
per-ex loss: 0.492595  [   52/   89]
per-ex loss: 0.413435  [   53/   89]
per-ex loss: 0.422736  [   54/   89]
per-ex loss: 0.298212  [   55/   89]
per-ex loss: 0.500925  [   56/   89]
per-ex loss: 0.392837  [   57/   89]
per-ex loss: 0.418438  [   58/   89]
per-ex loss: 0.512324  [   59/   89]
per-ex loss: 0.500946  [   60/   89]
per-ex loss: 0.509062  [   61/   89]
per-ex loss: 0.338693  [   62/   89]
per-ex loss: 0.308984  [   63/   89]
per-ex loss: 0.265185  [   64/   89]
per-ex loss: 0.455907  [   65/   89]
per-ex loss: 0.458052  [   66/   89]
per-ex loss: 0.423046  [   67/   89]
per-ex loss: 0.216261  [   68/   89]
per-ex loss: 0.419488  [   69/   89]
per-ex loss: 0.446980  [   70/   89]
per-ex loss: 0.468976  [   71/   89]
per-ex loss: 0.303457  [   72/   89]
per-ex loss: 0.378687  [   73/   89]
per-ex loss: 0.389717  [   74/   89]
per-ex loss: 0.377793  [   75/   89]
per-ex loss: 0.293719  [   76/   89]
per-ex loss: 0.358860  [   77/   89]
per-ex loss: 0.313236  [   78/   89]
per-ex loss: 0.322100  [   79/   89]
per-ex loss: 0.999940  [   80/   89]
per-ex loss: 0.464559  [   81/   89]
per-ex loss: 0.369114  [   82/   89]
per-ex loss: 0.384398  [   83/   89]
per-ex loss: 0.579804  [   84/   89]
per-ex loss: 0.274697  [   85/   89]
per-ex loss: 0.317718  [   86/   89]
per-ex loss: 0.279810  [   87/   89]
per-ex loss: 0.303713  [   88/   89]
per-ex loss: 0.343502  [   89/   89]
Train Error: Avg loss: 0.40465188
validation Error: 
 Avg loss: 0.55510523 
 F1: 0.477042 
 Precision: 0.537778 
 Recall: 0.428632
 IoU: 0.313234

test Error: 
 Avg loss: 0.52094693 
 F1: 0.528532 
 Precision: 0.541845 
 Recall: 0.515858
 IoU: 0.359187

We have finished training iteration 124
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_122_.pth
per-ex loss: 0.394275  [    1/   89]
per-ex loss: 0.527235  [    2/   89]
per-ex loss: 0.270788  [    3/   89]
per-ex loss: 0.329114  [    4/   89]
per-ex loss: 0.296738  [    5/   89]
per-ex loss: 0.387005  [    6/   89]
per-ex loss: 0.369029  [    7/   89]
per-ex loss: 0.258020  [    8/   89]
per-ex loss: 0.238154  [    9/   89]
per-ex loss: 0.557198  [   10/   89]
per-ex loss: 0.581383  [   11/   89]
per-ex loss: 0.377419  [   12/   89]
per-ex loss: 0.171975  [   13/   89]
per-ex loss: 0.314178  [   14/   89]
per-ex loss: 0.434562  [   15/   89]
per-ex loss: 0.352718  [   16/   89]
per-ex loss: 0.581964  [   17/   89]
per-ex loss: 0.369083  [   18/   89]
per-ex loss: 0.415772  [   19/   89]
per-ex loss: 0.346726  [   20/   89]
per-ex loss: 0.467143  [   21/   89]
per-ex loss: 0.288950  [   22/   89]
per-ex loss: 0.265775  [   23/   89]
per-ex loss: 0.439770  [   24/   89]
per-ex loss: 0.298849  [   25/   89]
per-ex loss: 0.428058  [   26/   89]
per-ex loss: 0.276459  [   27/   89]
per-ex loss: 0.476148  [   28/   89]
per-ex loss: 0.323846  [   29/   89]
per-ex loss: 0.545571  [   30/   89]
per-ex loss: 0.367923  [   31/   89]
per-ex loss: 0.343728  [   32/   89]
per-ex loss: 0.233129  [   33/   89]
per-ex loss: 0.456644  [   34/   89]
per-ex loss: 0.362308  [   35/   89]
per-ex loss: 0.521515  [   36/   89]
per-ex loss: 0.303103  [   37/   89]
per-ex loss: 0.342634  [   38/   89]
per-ex loss: 0.326137  [   39/   89]
per-ex loss: 0.464448  [   40/   89]
per-ex loss: 0.496300  [   41/   89]
per-ex loss: 0.428691  [   42/   89]
per-ex loss: 0.301715  [   43/   89]
per-ex loss: 0.233632  [   44/   89]
per-ex loss: 0.562089  [   45/   89]
per-ex loss: 0.492823  [   46/   89]
per-ex loss: 0.283141  [   47/   89]
per-ex loss: 0.327039  [   48/   89]
per-ex loss: 0.320683  [   49/   89]
per-ex loss: 0.500208  [   50/   89]
per-ex loss: 0.648925  [   51/   89]
per-ex loss: 0.269029  [   52/   89]
per-ex loss: 0.331827  [   53/   89]
per-ex loss: 0.301580  [   54/   89]
per-ex loss: 0.594963  [   55/   89]
per-ex loss: 0.486729  [   56/   89]
per-ex loss: 0.309441  [   57/   89]
per-ex loss: 0.370661  [   58/   89]
per-ex loss: 0.378316  [   59/   89]
per-ex loss: 0.349088  [   60/   89]
per-ex loss: 0.441442  [   61/   89]
per-ex loss: 0.447936  [   62/   89]
per-ex loss: 0.471421  [   63/   89]
per-ex loss: 0.351885  [   64/   89]
per-ex loss: 0.419658  [   65/   89]
per-ex loss: 0.273447  [   66/   89]
per-ex loss: 0.302746  [   67/   89]
per-ex loss: 0.302716  [   68/   89]
per-ex loss: 0.218036  [   69/   89]
per-ex loss: 0.563529  [   70/   89]
per-ex loss: 0.360037  [   71/   89]
per-ex loss: 0.357372  [   72/   89]
per-ex loss: 0.580285  [   73/   89]
per-ex loss: 0.476812  [   74/   89]
per-ex loss: 0.552702  [   75/   89]
per-ex loss: 0.999879  [   76/   89]
per-ex loss: 0.488120  [   77/   89]
per-ex loss: 0.481702  [   78/   89]
per-ex loss: 0.438446  [   79/   89]
per-ex loss: 0.345058  [   80/   89]
per-ex loss: 0.352039  [   81/   89]
per-ex loss: 0.242846  [   82/   89]
per-ex loss: 0.309070  [   83/   89]
per-ex loss: 0.354505  [   84/   89]
per-ex loss: 0.513837  [   85/   89]
per-ex loss: 0.389443  [   86/   89]
per-ex loss: 0.314617  [   87/   89]
per-ex loss: 0.579048  [   88/   89]
per-ex loss: 0.338954  [   89/   89]
Train Error: Avg loss: 0.39730271
validation Error: 
 Avg loss: 0.56056405 
 F1: 0.468365 
 Precision: 0.596056 
 Recall: 0.385731
 IoU: 0.305794

test Error: 
 Avg loss: 0.52273686 
 F1: 0.526934 
 Precision: 0.616172 
 Recall: 0.460274
 IoU: 0.357712

We have finished training iteration 125
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_123_.pth
per-ex loss: 0.435183  [    1/   89]
per-ex loss: 0.457410  [    2/   89]
per-ex loss: 0.584842  [    3/   89]
per-ex loss: 0.501746  [    4/   89]
per-ex loss: 0.353438  [    5/   89]
per-ex loss: 0.312686  [    6/   89]
per-ex loss: 0.433643  [    7/   89]
per-ex loss: 0.682850  [    8/   89]
per-ex loss: 0.306656  [    9/   89]
per-ex loss: 0.470352  [   10/   89]
per-ex loss: 0.312719  [   11/   89]
per-ex loss: 0.602371  [   12/   89]
per-ex loss: 0.999879  [   13/   89]
per-ex loss: 0.343482  [   14/   89]
per-ex loss: 0.353340  [   15/   89]
per-ex loss: 0.321988  [   16/   89]
per-ex loss: 0.348145  [   17/   89]
per-ex loss: 0.362124  [   18/   89]
per-ex loss: 0.526018  [   19/   89]
per-ex loss: 0.349344  [   20/   89]
per-ex loss: 0.539666  [   21/   89]
per-ex loss: 0.399844  [   22/   89]
per-ex loss: 0.424070  [   23/   89]
per-ex loss: 0.359474  [   24/   89]
per-ex loss: 0.309287  [   25/   89]
per-ex loss: 0.382829  [   26/   89]
per-ex loss: 0.330499  [   27/   89]
per-ex loss: 0.408412  [   28/   89]
per-ex loss: 0.219271  [   29/   89]
per-ex loss: 0.316502  [   30/   89]
per-ex loss: 0.296576  [   31/   89]
per-ex loss: 0.452447  [   32/   89]
per-ex loss: 0.201303  [   33/   89]
per-ex loss: 0.341577  [   34/   89]
per-ex loss: 0.339908  [   35/   89]
per-ex loss: 0.363320  [   36/   89]
per-ex loss: 0.522363  [   37/   89]
per-ex loss: 0.514189  [   38/   89]
per-ex loss: 0.330655  [   39/   89]
per-ex loss: 0.541757  [   40/   89]
per-ex loss: 0.574507  [   41/   89]
per-ex loss: 0.345759  [   42/   89]
per-ex loss: 0.389239  [   43/   89]
per-ex loss: 0.301374  [   44/   89]
per-ex loss: 0.599434  [   45/   89]
per-ex loss: 0.376751  [   46/   89]
per-ex loss: 0.398238  [   47/   89]
per-ex loss: 0.578449  [   48/   89]
per-ex loss: 0.334904  [   49/   89]
per-ex loss: 0.525918  [   50/   89]
per-ex loss: 0.300321  [   51/   89]
per-ex loss: 0.358695  [   52/   89]
per-ex loss: 0.322616  [   53/   89]
per-ex loss: 0.367164  [   54/   89]
per-ex loss: 0.248066  [   55/   89]
per-ex loss: 0.334532  [   56/   89]
per-ex loss: 0.490369  [   57/   89]
per-ex loss: 0.343312  [   58/   89]
per-ex loss: 0.467660  [   59/   89]
per-ex loss: 0.406880  [   60/   89]
per-ex loss: 0.324961  [   61/   89]
per-ex loss: 0.492080  [   62/   89]
per-ex loss: 0.345668  [   63/   89]
per-ex loss: 0.438688  [   64/   89]
per-ex loss: 0.293693  [   65/   89]
per-ex loss: 0.581691  [   66/   89]
per-ex loss: 0.409940  [   67/   89]
per-ex loss: 0.328577  [   68/   89]
per-ex loss: 0.304089  [   69/   89]
per-ex loss: 0.348015  [   70/   89]
per-ex loss: 0.497573  [   71/   89]
per-ex loss: 0.268289  [   72/   89]
per-ex loss: 0.509417  [   73/   89]
per-ex loss: 0.474996  [   74/   89]
per-ex loss: 0.460994  [   75/   89]
per-ex loss: 0.367464  [   76/   89]
per-ex loss: 0.329191  [   77/   89]
per-ex loss: 0.570368  [   78/   89]
per-ex loss: 0.417882  [   79/   89]
per-ex loss: 0.323801  [   80/   89]
per-ex loss: 0.324106  [   81/   89]
per-ex loss: 0.491608  [   82/   89]
per-ex loss: 0.483601  [   83/   89]
per-ex loss: 0.303516  [   84/   89]
per-ex loss: 0.370271  [   85/   89]
per-ex loss: 0.487088  [   86/   89]
per-ex loss: 0.442327  [   87/   89]
per-ex loss: 0.412654  [   88/   89]
per-ex loss: 0.411408  [   89/   89]
Train Error: Avg loss: 0.41045288
validation Error: 
 Avg loss: 0.54832901 
 F1: 0.483886 
 Precision: 0.480663 
 Recall: 0.487153
 IoU: 0.319162

test Error: 
 Avg loss: 0.48552008 
 F1: 0.559745 
 Precision: 0.558405 
 Recall: 0.561092
 IoU: 0.388643

We have finished training iteration 126
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_124_.pth
per-ex loss: 0.367083  [    1/   89]
per-ex loss: 0.479768  [    2/   89]
per-ex loss: 0.334434  [    3/   89]
per-ex loss: 0.430669  [    4/   89]
per-ex loss: 0.307599  [    5/   89]
per-ex loss: 0.313447  [    6/   89]
per-ex loss: 0.468088  [    7/   89]
per-ex loss: 0.373358  [    8/   89]
per-ex loss: 0.606216  [    9/   89]
per-ex loss: 0.369271  [   10/   89]
per-ex loss: 0.410947  [   11/   89]
per-ex loss: 0.398718  [   12/   89]
per-ex loss: 0.400964  [   13/   89]
per-ex loss: 0.527448  [   14/   89]
per-ex loss: 0.356708  [   15/   89]
per-ex loss: 0.570652  [   16/   89]
per-ex loss: 0.358595  [   17/   89]
per-ex loss: 0.303980  [   18/   89]
per-ex loss: 0.261944  [   19/   89]
per-ex loss: 0.328534  [   20/   89]
per-ex loss: 0.220848  [   21/   89]
per-ex loss: 0.358761  [   22/   89]
per-ex loss: 0.478618  [   23/   89]
per-ex loss: 0.520223  [   24/   89]
per-ex loss: 0.332102  [   25/   89]
per-ex loss: 0.321358  [   26/   89]
per-ex loss: 0.296845  [   27/   89]
per-ex loss: 0.348907  [   28/   89]
per-ex loss: 0.516884  [   29/   89]
per-ex loss: 0.364381  [   30/   89]
per-ex loss: 0.544662  [   31/   89]
per-ex loss: 0.383050  [   32/   89]
per-ex loss: 0.428139  [   33/   89]
per-ex loss: 0.387551  [   34/   89]
per-ex loss: 0.330865  [   35/   89]
per-ex loss: 0.273262  [   36/   89]
per-ex loss: 0.441020  [   37/   89]
per-ex loss: 0.336899  [   38/   89]
per-ex loss: 0.171014  [   39/   89]
per-ex loss: 0.621570  [   40/   89]
per-ex loss: 0.601081  [   41/   89]
per-ex loss: 0.381619  [   42/   89]
per-ex loss: 0.574895  [   43/   89]
per-ex loss: 0.409373  [   44/   89]
per-ex loss: 0.999879  [   45/   89]
per-ex loss: 0.521103  [   46/   89]
per-ex loss: 0.342413  [   47/   89]
per-ex loss: 0.331419  [   48/   89]
per-ex loss: 0.393979  [   49/   89]
per-ex loss: 0.436778  [   50/   89]
per-ex loss: 0.312411  [   51/   89]
per-ex loss: 0.304271  [   52/   89]
per-ex loss: 0.315847  [   53/   89]
per-ex loss: 0.352090  [   54/   89]
per-ex loss: 0.495090  [   55/   89]
per-ex loss: 0.369736  [   56/   89]
per-ex loss: 0.497793  [   57/   89]
per-ex loss: 0.363473  [   58/   89]
per-ex loss: 0.336259  [   59/   89]
per-ex loss: 0.310437  [   60/   89]
per-ex loss: 0.276974  [   61/   89]
per-ex loss: 0.392274  [   62/   89]
per-ex loss: 0.408619  [   63/   89]
per-ex loss: 0.297231  [   64/   89]
per-ex loss: 0.451987  [   65/   89]
per-ex loss: 0.344697  [   66/   89]
per-ex loss: 0.396120  [   67/   89]
per-ex loss: 0.342165  [   68/   89]
per-ex loss: 0.617397  [   69/   89]
per-ex loss: 0.313895  [   70/   89]
per-ex loss: 0.308784  [   71/   89]
per-ex loss: 0.470538  [   72/   89]
per-ex loss: 0.461294  [   73/   89]
per-ex loss: 0.423922  [   74/   89]
per-ex loss: 0.487250  [   75/   89]
per-ex loss: 0.450125  [   76/   89]
per-ex loss: 0.330442  [   77/   89]
per-ex loss: 0.394262  [   78/   89]
per-ex loss: 0.546298  [   79/   89]
per-ex loss: 0.514066  [   80/   89]
per-ex loss: 0.507902  [   81/   89]
per-ex loss: 0.269338  [   82/   89]
per-ex loss: 0.548905  [   83/   89]
per-ex loss: 0.373927  [   84/   89]
per-ex loss: 0.536657  [   85/   89]
per-ex loss: 0.304872  [   86/   89]
per-ex loss: 0.512415  [   87/   89]
per-ex loss: 0.307052  [   88/   89]
per-ex loss: 0.286579  [   89/   89]
Train Error: Avg loss: 0.40641931
validation Error: 
 Avg loss: 0.52719166 
 F1: 0.498087 
 Precision: 0.550364 
 Recall: 0.454880
 IoU: 0.331635

test Error: 
 Avg loss: 0.47939379 
 F1: 0.568308 
 Precision: 0.602357 
 Recall: 0.537902
 IoU: 0.396948

We have finished training iteration 127
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_125_.pth
per-ex loss: 0.292059  [    1/   89]
per-ex loss: 0.423385  [    2/   89]
per-ex loss: 0.384685  [    3/   89]
per-ex loss: 0.503059  [    4/   89]
per-ex loss: 0.494402  [    5/   89]
per-ex loss: 0.358329  [    6/   89]
per-ex loss: 0.346214  [    7/   89]
per-ex loss: 0.351875  [    8/   89]
per-ex loss: 0.356199  [    9/   89]
per-ex loss: 0.319225  [   10/   89]
per-ex loss: 0.323224  [   11/   89]
per-ex loss: 0.469766  [   12/   89]
per-ex loss: 0.193690  [   13/   89]
per-ex loss: 0.284243  [   14/   89]
per-ex loss: 0.379170  [   15/   89]
per-ex loss: 0.349382  [   16/   89]
per-ex loss: 0.412800  [   17/   89]
per-ex loss: 0.534672  [   18/   89]
per-ex loss: 0.256609  [   19/   89]
per-ex loss: 0.357821  [   20/   89]
per-ex loss: 0.531571  [   21/   89]
per-ex loss: 0.412324  [   22/   89]
per-ex loss: 0.509757  [   23/   89]
per-ex loss: 0.999879  [   24/   89]
per-ex loss: 0.498586  [   25/   89]
per-ex loss: 0.373192  [   26/   89]
per-ex loss: 0.281433  [   27/   89]
per-ex loss: 0.572885  [   28/   89]
per-ex loss: 0.428199  [   29/   89]
per-ex loss: 0.394427  [   30/   89]
per-ex loss: 0.403183  [   31/   89]
per-ex loss: 0.322272  [   32/   89]
per-ex loss: 0.467872  [   33/   89]
per-ex loss: 0.373750  [   34/   89]
per-ex loss: 0.280460  [   35/   89]
per-ex loss: 0.366663  [   36/   89]
per-ex loss: 0.329161  [   37/   89]
per-ex loss: 0.570848  [   38/   89]
per-ex loss: 0.325193  [   39/   89]
per-ex loss: 0.493938  [   40/   89]
per-ex loss: 0.223752  [   41/   89]
per-ex loss: 0.615670  [   42/   89]
per-ex loss: 0.315624  [   43/   89]
per-ex loss: 0.610173  [   44/   89]
per-ex loss: 0.372022  [   45/   89]
per-ex loss: 0.396621  [   46/   89]
per-ex loss: 0.525597  [   47/   89]
per-ex loss: 0.351169  [   48/   89]
per-ex loss: 0.456252  [   49/   89]
per-ex loss: 0.402987  [   50/   89]
per-ex loss: 0.295105  [   51/   89]
per-ex loss: 0.442580  [   52/   89]
per-ex loss: 0.297821  [   53/   89]
per-ex loss: 0.542128  [   54/   89]
per-ex loss: 0.323595  [   55/   89]
per-ex loss: 0.525367  [   56/   89]
per-ex loss: 0.381895  [   57/   89]
per-ex loss: 0.445617  [   58/   89]
per-ex loss: 0.632649  [   59/   89]
per-ex loss: 0.293754  [   60/   89]
per-ex loss: 0.389927  [   61/   89]
per-ex loss: 0.296915  [   62/   89]
per-ex loss: 0.328564  [   63/   89]
per-ex loss: 0.298111  [   64/   89]
per-ex loss: 0.347605  [   65/   89]
per-ex loss: 0.488746  [   66/   89]
per-ex loss: 0.562048  [   67/   89]
per-ex loss: 0.384738  [   68/   89]
per-ex loss: 0.560945  [   69/   89]
per-ex loss: 0.493596  [   70/   89]
per-ex loss: 0.239078  [   71/   89]
per-ex loss: 0.285951  [   72/   89]
per-ex loss: 0.572920  [   73/   89]
per-ex loss: 0.443172  [   74/   89]
per-ex loss: 0.366651  [   75/   89]
per-ex loss: 0.356433  [   76/   89]
per-ex loss: 0.355475  [   77/   89]
per-ex loss: 0.360207  [   78/   89]
per-ex loss: 0.303116  [   79/   89]
per-ex loss: 0.307240  [   80/   89]
per-ex loss: 0.353046  [   81/   89]
per-ex loss: 0.468391  [   82/   89]
per-ex loss: 0.498804  [   83/   89]
per-ex loss: 0.232996  [   84/   89]
per-ex loss: 0.317302  [   85/   89]
per-ex loss: 0.382372  [   86/   89]
per-ex loss: 0.292184  [   87/   89]
per-ex loss: 0.518746  [   88/   89]
per-ex loss: 0.310832  [   89/   89]
Train Error: Avg loss: 0.40329088
validation Error: 
 Avg loss: 0.52786111 
 F1: 0.495093 
 Precision: 0.510824 
 Recall: 0.480302
 IoU: 0.328986

test Error: 
 Avg loss: 0.47797803 
 F1: 0.566615 
 Precision: 0.564070 
 Recall: 0.569184
 IoU: 0.395299

We have finished training iteration 128
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_126_.pth
per-ex loss: 0.323401  [    1/   89]
per-ex loss: 0.273463  [    2/   89]
per-ex loss: 0.534715  [    3/   89]
per-ex loss: 0.350116  [    4/   89]
per-ex loss: 0.326133  [    5/   89]
per-ex loss: 0.392246  [    6/   89]
per-ex loss: 0.326610  [    7/   89]
per-ex loss: 0.414731  [    8/   89]
per-ex loss: 0.465879  [    9/   89]
per-ex loss: 0.445557  [   10/   89]
per-ex loss: 0.235558  [   11/   89]
per-ex loss: 0.341469  [   12/   89]
per-ex loss: 0.290122  [   13/   89]
per-ex loss: 0.337546  [   14/   89]
per-ex loss: 0.566440  [   15/   89]
per-ex loss: 0.234912  [   16/   89]
per-ex loss: 0.520233  [   17/   89]
per-ex loss: 0.491919  [   18/   89]
per-ex loss: 0.261897  [   19/   89]
per-ex loss: 0.312271  [   20/   89]
per-ex loss: 0.322746  [   21/   89]
per-ex loss: 0.391970  [   22/   89]
per-ex loss: 0.334589  [   23/   89]
per-ex loss: 0.323927  [   24/   89]
per-ex loss: 0.371473  [   25/   89]
per-ex loss: 0.295438  [   26/   89]
per-ex loss: 0.295658  [   27/   89]
per-ex loss: 0.480359  [   28/   89]
per-ex loss: 0.419523  [   29/   89]
per-ex loss: 0.489336  [   30/   89]
per-ex loss: 0.374122  [   31/   89]
per-ex loss: 0.350012  [   32/   89]
per-ex loss: 0.381565  [   33/   89]
per-ex loss: 0.453283  [   34/   89]
per-ex loss: 0.458762  [   35/   89]
per-ex loss: 0.272003  [   36/   89]
per-ex loss: 0.262984  [   37/   89]
per-ex loss: 0.517755  [   38/   89]
per-ex loss: 0.418889  [   39/   89]
per-ex loss: 0.340444  [   40/   89]
per-ex loss: 0.999945  [   41/   89]
per-ex loss: 0.295196  [   42/   89]
per-ex loss: 0.264098  [   43/   89]
per-ex loss: 0.473718  [   44/   89]
per-ex loss: 0.318603  [   45/   89]
per-ex loss: 0.595884  [   46/   89]
per-ex loss: 0.577940  [   47/   89]
per-ex loss: 0.287748  [   48/   89]
per-ex loss: 0.318847  [   49/   89]
per-ex loss: 0.285146  [   50/   89]
per-ex loss: 0.506207  [   51/   89]
per-ex loss: 0.297556  [   52/   89]
per-ex loss: 0.492101  [   53/   89]
per-ex loss: 0.353926  [   54/   89]
per-ex loss: 0.305469  [   55/   89]
per-ex loss: 0.342270  [   56/   89]
per-ex loss: 0.369905  [   57/   89]
per-ex loss: 0.220698  [   58/   89]
per-ex loss: 0.304201  [   59/   89]
per-ex loss: 0.380979  [   60/   89]
per-ex loss: 0.362423  [   61/   89]
per-ex loss: 0.288365  [   62/   89]
per-ex loss: 0.702203  [   63/   89]
per-ex loss: 0.439193  [   64/   89]
per-ex loss: 0.270772  [   65/   89]
per-ex loss: 0.375101  [   66/   89]
per-ex loss: 0.357341  [   67/   89]
per-ex loss: 0.587134  [   68/   89]
per-ex loss: 0.507644  [   69/   89]
per-ex loss: 0.361087  [   70/   89]
per-ex loss: 0.607478  [   71/   89]
per-ex loss: 0.477862  [   72/   89]
per-ex loss: 0.459600  [   73/   89]
per-ex loss: 0.600283  [   74/   89]
per-ex loss: 0.365732  [   75/   89]
per-ex loss: 0.529425  [   76/   89]
per-ex loss: 0.469259  [   77/   89]
per-ex loss: 0.316284  [   78/   89]
per-ex loss: 0.391434  [   79/   89]
per-ex loss: 0.537885  [   80/   89]
per-ex loss: 0.368446  [   81/   89]
per-ex loss: 0.320027  [   82/   89]
per-ex loss: 0.306999  [   83/   89]
per-ex loss: 0.334637  [   84/   89]
per-ex loss: 0.495089  [   85/   89]
per-ex loss: 0.490350  [   86/   89]
per-ex loss: 0.359597  [   87/   89]
per-ex loss: 0.373228  [   88/   89]
per-ex loss: 0.527747  [   89/   89]
Train Error: Avg loss: 0.39969794
validation Error: 
 Avg loss: 0.54008870 
 F1: 0.485402 
 Precision: 0.558590 
 Recall: 0.429171
 IoU: 0.320483

test Error: 
 Avg loss: 0.49323882 
 F1: 0.554165 
 Precision: 0.636258 
 Recall: 0.490836
 IoU: 0.383284

We have finished training iteration 129
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_104_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.309060  [    1/   89]
per-ex loss: 0.525953  [    2/   89]
per-ex loss: 0.322027  [    3/   89]
per-ex loss: 0.432829  [    4/   89]
per-ex loss: 0.220026  [    5/   89]
per-ex loss: 0.311462  [    6/   89]
per-ex loss: 0.535170  [    7/   89]
per-ex loss: 0.310537  [    8/   89]
per-ex loss: 0.361370  [    9/   89]
per-ex loss: 0.429666  [   10/   89]
per-ex loss: 0.455282  [   11/   89]
per-ex loss: 0.526239  [   12/   89]
per-ex loss: 0.312354  [   13/   89]
per-ex loss: 0.999967  [   14/   89]
per-ex loss: 0.307955  [   15/   89]
per-ex loss: 0.321991  [   16/   89]
per-ex loss: 0.492357  [   17/   89]
per-ex loss: 0.235193  [   18/   89]
per-ex loss: 0.292712  [   19/   89]
per-ex loss: 0.188580  [   20/   89]
per-ex loss: 0.319462  [   21/   89]
per-ex loss: 0.323181  [   22/   89]
per-ex loss: 0.354183  [   23/   89]
per-ex loss: 0.431332  [   24/   89]
per-ex loss: 0.366625  [   25/   89]
per-ex loss: 0.259839  [   26/   89]
per-ex loss: 0.358419  [   27/   89]
per-ex loss: 0.471597  [   28/   89]
per-ex loss: 0.398558  [   29/   89]
per-ex loss: 0.313412  [   30/   89]
per-ex loss: 0.392395  [   31/   89]
per-ex loss: 0.347264  [   32/   89]
per-ex loss: 0.524588  [   33/   89]
per-ex loss: 0.543377  [   34/   89]
per-ex loss: 0.391138  [   35/   89]
per-ex loss: 0.249434  [   36/   89]
per-ex loss: 0.426927  [   37/   89]
per-ex loss: 0.384635  [   38/   89]
per-ex loss: 0.423136  [   39/   89]
per-ex loss: 0.504703  [   40/   89]
per-ex loss: 0.511229  [   41/   89]
per-ex loss: 0.362563  [   42/   89]
per-ex loss: 0.231097  [   43/   89]
per-ex loss: 0.313181  [   44/   89]
per-ex loss: 0.302133  [   45/   89]
per-ex loss: 0.331425  [   46/   89]
per-ex loss: 0.334354  [   47/   89]
per-ex loss: 0.538762  [   48/   89]
per-ex loss: 0.343840  [   49/   89]
per-ex loss: 0.382783  [   50/   89]
per-ex loss: 0.374926  [   51/   89]
per-ex loss: 0.380703  [   52/   89]
per-ex loss: 0.340468  [   53/   89]
per-ex loss: 0.365313  [   54/   89]
per-ex loss: 0.304080  [   55/   89]
per-ex loss: 0.465855  [   56/   89]
per-ex loss: 0.474938  [   57/   89]
per-ex loss: 0.549144  [   58/   89]
per-ex loss: 0.531778  [   59/   89]
per-ex loss: 0.607074  [   60/   89]
per-ex loss: 0.282237  [   61/   89]
per-ex loss: 0.384595  [   62/   89]
per-ex loss: 0.269059  [   63/   89]
per-ex loss: 0.454425  [   64/   89]
per-ex loss: 0.334579  [   65/   89]
per-ex loss: 0.493690  [   66/   89]
per-ex loss: 0.279546  [   67/   89]
per-ex loss: 0.568563  [   68/   89]
per-ex loss: 0.517027  [   69/   89]
per-ex loss: 0.607835  [   70/   89]
per-ex loss: 0.377315  [   71/   89]
per-ex loss: 0.283558  [   72/   89]
per-ex loss: 0.349325  [   73/   89]
per-ex loss: 0.587922  [   74/   89]
per-ex loss: 0.258483  [   75/   89]
per-ex loss: 0.440950  [   76/   89]
per-ex loss: 0.357361  [   77/   89]
per-ex loss: 0.347153  [   78/   89]
per-ex loss: 0.517155  [   79/   89]
per-ex loss: 0.293626  [   80/   89]
per-ex loss: 0.353265  [   81/   89]
per-ex loss: 0.564156  [   82/   89]
per-ex loss: 0.319923  [   83/   89]
per-ex loss: 0.314420  [   84/   89]
per-ex loss: 0.390047  [   85/   89]
per-ex loss: 0.473751  [   86/   89]
per-ex loss: 0.639287  [   87/   89]
per-ex loss: 0.502472  [   88/   89]
per-ex loss: 0.452912  [   89/   89]
Train Error: Avg loss: 0.40151928
validation Error: 
 Avg loss: 0.54613820 
 F1: 0.478816 
 Precision: 0.457632 
 Recall: 0.502056
 IoU: 0.314765

test Error: 
 Avg loss: 0.49025883 
 F1: 0.552006 
 Precision: 0.524193 
 Recall: 0.582937
 IoU: 0.381222

We have finished training iteration 130
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_128_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.259342  [    1/   89]
per-ex loss: 0.547390  [    2/   89]
per-ex loss: 0.246661  [    3/   89]
per-ex loss: 0.498198  [    4/   89]
per-ex loss: 0.220159  [    5/   89]
per-ex loss: 0.463509  [    6/   89]
per-ex loss: 0.358702  [    7/   89]
per-ex loss: 0.364318  [    8/   89]
per-ex loss: 0.574377  [    9/   89]
per-ex loss: 0.268605  [   10/   89]
per-ex loss: 0.448228  [   11/   89]
per-ex loss: 0.358299  [   12/   89]
per-ex loss: 0.442211  [   13/   89]
per-ex loss: 0.640535  [   14/   89]
per-ex loss: 0.352140  [   15/   89]
per-ex loss: 0.513481  [   16/   89]
per-ex loss: 0.423047  [   17/   89]
per-ex loss: 0.568395  [   18/   89]
per-ex loss: 0.362822  [   19/   89]
per-ex loss: 0.374513  [   20/   89]
per-ex loss: 0.459560  [   21/   89]
per-ex loss: 0.339484  [   22/   89]
per-ex loss: 0.314297  [   23/   89]
per-ex loss: 0.313017  [   24/   89]
per-ex loss: 0.426071  [   25/   89]
per-ex loss: 0.322078  [   26/   89]
per-ex loss: 0.427102  [   27/   89]
per-ex loss: 0.999883  [   28/   89]
per-ex loss: 0.470966  [   29/   89]
per-ex loss: 0.313314  [   30/   89]
per-ex loss: 0.362425  [   31/   89]
per-ex loss: 0.325514  [   32/   89]
per-ex loss: 0.311174  [   33/   89]
per-ex loss: 0.432159  [   34/   89]
per-ex loss: 0.494637  [   35/   89]
per-ex loss: 0.378683  [   36/   89]
per-ex loss: 0.531878  [   37/   89]
per-ex loss: 0.581275  [   38/   89]
per-ex loss: 0.645638  [   39/   89]
per-ex loss: 0.450176  [   40/   89]
per-ex loss: 0.404000  [   41/   89]
per-ex loss: 0.497349  [   42/   89]
per-ex loss: 0.365466  [   43/   89]
per-ex loss: 0.379919  [   44/   89]
per-ex loss: 0.307838  [   45/   89]
per-ex loss: 0.394657  [   46/   89]
per-ex loss: 0.274478  [   47/   89]
per-ex loss: 0.258646  [   48/   89]
per-ex loss: 0.444077  [   49/   89]
per-ex loss: 0.296068  [   50/   89]
per-ex loss: 0.576963  [   51/   89]
per-ex loss: 0.341962  [   52/   89]
per-ex loss: 0.302805  [   53/   89]
per-ex loss: 0.559324  [   54/   89]
per-ex loss: 0.331577  [   55/   89]
per-ex loss: 0.331324  [   56/   89]
per-ex loss: 0.313345  [   57/   89]
per-ex loss: 0.453900  [   58/   89]
per-ex loss: 0.469932  [   59/   89]
per-ex loss: 0.300246  [   60/   89]
per-ex loss: 0.264167  [   61/   89]
per-ex loss: 0.365452  [   62/   89]
per-ex loss: 0.397212  [   63/   89]
per-ex loss: 0.498321  [   64/   89]
per-ex loss: 0.237585  [   65/   89]
per-ex loss: 0.306939  [   66/   89]
per-ex loss: 0.338917  [   67/   89]
per-ex loss: 0.266520  [   68/   89]
per-ex loss: 0.456934  [   69/   89]
per-ex loss: 0.596849  [   70/   89]
per-ex loss: 0.267213  [   71/   89]
per-ex loss: 0.345976  [   72/   89]
per-ex loss: 0.300254  [   73/   89]
per-ex loss: 0.372786  [   74/   89]
per-ex loss: 0.319374  [   75/   89]
per-ex loss: 0.297655  [   76/   89]
per-ex loss: 0.502517  [   77/   89]
per-ex loss: 0.348545  [   78/   89]
per-ex loss: 0.361563  [   79/   89]
per-ex loss: 0.370971  [   80/   89]
per-ex loss: 0.293975  [   81/   89]
per-ex loss: 0.402731  [   82/   89]
per-ex loss: 0.481498  [   83/   89]
per-ex loss: 0.452106  [   84/   89]
per-ex loss: 0.362824  [   85/   89]
per-ex loss: 0.486974  [   86/   89]
per-ex loss: 0.281993  [   87/   89]
per-ex loss: 0.569030  [   88/   89]
per-ex loss: 0.356577  [   89/   89]
Train Error: Avg loss: 0.40102918
validation Error: 
 Avg loss: 0.53410401 
 F1: 0.491010 
 Precision: 0.481676 
 Recall: 0.500714
 IoU: 0.325390

test Error: 
 Avg loss: 0.48373483 
 F1: 0.560178 
 Precision: 0.538165 
 Recall: 0.584067
 IoU: 0.389060

We have finished training iteration 131
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_129_.pth
per-ex loss: 0.356625  [    1/   89]
per-ex loss: 0.304539  [    2/   89]
per-ex loss: 0.332611  [    3/   89]
per-ex loss: 0.353883  [    4/   89]
per-ex loss: 0.574058  [    5/   89]
per-ex loss: 0.222499  [    6/   89]
per-ex loss: 0.329568  [    7/   89]
per-ex loss: 0.358584  [    8/   89]
per-ex loss: 0.287022  [    9/   89]
per-ex loss: 0.522897  [   10/   89]
per-ex loss: 0.157927  [   11/   89]
per-ex loss: 0.521996  [   12/   89]
per-ex loss: 0.339795  [   13/   89]
per-ex loss: 0.342696  [   14/   89]
per-ex loss: 0.240635  [   15/   89]
per-ex loss: 0.262355  [   16/   89]
per-ex loss: 0.317592  [   17/   89]
per-ex loss: 0.435079  [   18/   89]
per-ex loss: 0.276692  [   19/   89]
per-ex loss: 0.264082  [   20/   89]
per-ex loss: 0.440931  [   21/   89]
per-ex loss: 0.325158  [   22/   89]
per-ex loss: 0.385490  [   23/   89]
per-ex loss: 0.322901  [   24/   89]
per-ex loss: 0.371165  [   25/   89]
per-ex loss: 0.377697  [   26/   89]
per-ex loss: 0.259554  [   27/   89]
per-ex loss: 0.323079  [   28/   89]
per-ex loss: 0.423426  [   29/   89]
per-ex loss: 0.301069  [   30/   89]
per-ex loss: 0.602572  [   31/   89]
per-ex loss: 0.463436  [   32/   89]
per-ex loss: 0.370342  [   33/   89]
per-ex loss: 0.999929  [   34/   89]
per-ex loss: 0.512750  [   35/   89]
per-ex loss: 0.269773  [   36/   89]
per-ex loss: 0.572551  [   37/   89]
per-ex loss: 0.240232  [   38/   89]
per-ex loss: 0.352938  [   39/   89]
per-ex loss: 0.494200  [   40/   89]
per-ex loss: 0.357196  [   41/   89]
per-ex loss: 0.411777  [   42/   89]
per-ex loss: 0.435578  [   43/   89]
per-ex loss: 0.415960  [   44/   89]
per-ex loss: 0.520316  [   45/   89]
per-ex loss: 0.267704  [   46/   89]
per-ex loss: 0.319602  [   47/   89]
per-ex loss: 0.377906  [   48/   89]
per-ex loss: 0.448928  [   49/   89]
per-ex loss: 0.347485  [   50/   89]
per-ex loss: 0.569965  [   51/   89]
per-ex loss: 0.314589  [   52/   89]
per-ex loss: 0.300410  [   53/   89]
per-ex loss: 0.191421  [   54/   89]
per-ex loss: 0.357077  [   55/   89]
per-ex loss: 0.503003  [   56/   89]
per-ex loss: 0.266723  [   57/   89]
per-ex loss: 0.390463  [   58/   89]
per-ex loss: 0.299315  [   59/   89]
per-ex loss: 0.323573  [   60/   89]
per-ex loss: 0.438399  [   61/   89]
per-ex loss: 0.379970  [   62/   89]
per-ex loss: 0.543940  [   63/   89]
per-ex loss: 0.359192  [   64/   89]
per-ex loss: 0.531094  [   65/   89]
per-ex loss: 0.253213  [   66/   89]
per-ex loss: 0.394295  [   67/   89]
per-ex loss: 0.377787  [   68/   89]
per-ex loss: 0.520754  [   69/   89]
per-ex loss: 0.467900  [   70/   89]
per-ex loss: 0.602981  [   71/   89]
per-ex loss: 0.338181  [   72/   89]
per-ex loss: 0.539316  [   73/   89]
per-ex loss: 0.463620  [   74/   89]
per-ex loss: 0.327411  [   75/   89]
per-ex loss: 0.505621  [   76/   89]
per-ex loss: 0.403712  [   77/   89]
per-ex loss: 0.602051  [   78/   89]
per-ex loss: 0.325188  [   79/   89]
per-ex loss: 0.311813  [   80/   89]
per-ex loss: 0.321034  [   81/   89]
per-ex loss: 0.277336  [   82/   89]
per-ex loss: 0.453085  [   83/   89]
per-ex loss: 0.484886  [   84/   89]
per-ex loss: 0.350878  [   85/   89]
per-ex loss: 0.660534  [   86/   89]
per-ex loss: 0.386488  [   87/   89]
per-ex loss: 0.438590  [   88/   89]
per-ex loss: 0.422542  [   89/   89]
Train Error: Avg loss: 0.39450706
validation Error: 
 Avg loss: 0.53368740 
 F1: 0.495536 
 Precision: 0.506661 
 Recall: 0.484889
 IoU: 0.329377

test Error: 
 Avg loss: 0.48135818 
 F1: 0.565525 
 Precision: 0.554587 
 Recall: 0.576903
 IoU: 0.394238

We have finished training iteration 132
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_130_.pth
per-ex loss: 0.467533  [    1/   89]
per-ex loss: 0.553494  [    2/   89]
per-ex loss: 0.342049  [    3/   89]
per-ex loss: 0.502923  [    4/   89]
per-ex loss: 0.352768  [    5/   89]
per-ex loss: 0.372376  [    6/   89]
per-ex loss: 0.323189  [    7/   89]
per-ex loss: 0.486246  [    8/   89]
per-ex loss: 0.298029  [    9/   89]
per-ex loss: 0.453982  [   10/   89]
per-ex loss: 0.379451  [   11/   89]
per-ex loss: 0.342307  [   12/   89]
per-ex loss: 0.335368  [   13/   89]
per-ex loss: 0.503174  [   14/   89]
per-ex loss: 0.353586  [   15/   89]
per-ex loss: 0.302821  [   16/   89]
per-ex loss: 0.497859  [   17/   89]
per-ex loss: 0.301590  [   18/   89]
per-ex loss: 0.310333  [   19/   89]
per-ex loss: 0.352256  [   20/   89]
per-ex loss: 0.221826  [   21/   89]
per-ex loss: 0.501687  [   22/   89]
per-ex loss: 0.299098  [   23/   89]
per-ex loss: 0.365055  [   24/   89]
per-ex loss: 0.339621  [   25/   89]
per-ex loss: 0.370359  [   26/   89]
per-ex loss: 0.433328  [   27/   89]
per-ex loss: 0.331445  [   28/   89]
per-ex loss: 0.587596  [   29/   89]
per-ex loss: 0.261386  [   30/   89]
per-ex loss: 0.289911  [   31/   89]
per-ex loss: 0.366939  [   32/   89]
per-ex loss: 0.588733  [   33/   89]
per-ex loss: 0.444193  [   34/   89]
per-ex loss: 0.516392  [   35/   89]
per-ex loss: 0.166236  [   36/   89]
per-ex loss: 0.237638  [   37/   89]
per-ex loss: 0.350005  [   38/   89]
per-ex loss: 0.585763  [   39/   89]
per-ex loss: 0.532531  [   40/   89]
per-ex loss: 0.346539  [   41/   89]
per-ex loss: 0.296621  [   42/   89]
per-ex loss: 0.612255  [   43/   89]
per-ex loss: 0.530292  [   44/   89]
per-ex loss: 0.423736  [   45/   89]
per-ex loss: 0.239543  [   46/   89]
per-ex loss: 0.358662  [   47/   89]
per-ex loss: 0.347973  [   48/   89]
per-ex loss: 0.463623  [   49/   89]
per-ex loss: 0.369829  [   50/   89]
per-ex loss: 0.337909  [   51/   89]
per-ex loss: 0.418612  [   52/   89]
per-ex loss: 0.589313  [   53/   89]
per-ex loss: 0.508420  [   54/   89]
per-ex loss: 0.262302  [   55/   89]
per-ex loss: 0.463236  [   56/   89]
per-ex loss: 0.550694  [   57/   89]
per-ex loss: 0.364510  [   58/   89]
per-ex loss: 0.419651  [   59/   89]
per-ex loss: 0.309826  [   60/   89]
per-ex loss: 0.307698  [   61/   89]
per-ex loss: 0.999900  [   62/   89]
per-ex loss: 0.349317  [   63/   89]
per-ex loss: 0.445839  [   64/   89]
per-ex loss: 0.244551  [   65/   89]
per-ex loss: 0.333299  [   66/   89]
per-ex loss: 0.515023  [   67/   89]
per-ex loss: 0.327546  [   68/   89]
per-ex loss: 0.289784  [   69/   89]
per-ex loss: 0.324860  [   70/   89]
per-ex loss: 0.275015  [   71/   89]
per-ex loss: 0.373213  [   72/   89]
per-ex loss: 0.573763  [   73/   89]
per-ex loss: 0.454563  [   74/   89]
per-ex loss: 0.456383  [   75/   89]
per-ex loss: 0.595640  [   76/   89]
per-ex loss: 0.332755  [   77/   89]
per-ex loss: 0.246923  [   78/   89]
per-ex loss: 0.431162  [   79/   89]
per-ex loss: 0.423491  [   80/   89]
per-ex loss: 0.276340  [   81/   89]
per-ex loss: 0.426870  [   82/   89]
per-ex loss: 0.502088  [   83/   89]
per-ex loss: 0.548318  [   84/   89]
per-ex loss: 0.358288  [   85/   89]
per-ex loss: 0.358191  [   86/   89]
per-ex loss: 0.402501  [   87/   89]
per-ex loss: 0.429832  [   88/   89]
per-ex loss: 0.232018  [   89/   89]
Train Error: Avg loss: 0.40078421
validation Error: 
 Avg loss: 0.54081633 
 F1: 0.489071 
 Precision: 0.646516 
 Recall: 0.393294
 IoU: 0.323689

test Error: 
 Avg loss: 0.50390876 
 F1: 0.546931 
 Precision: 0.682000 
 Recall: 0.456518
 IoU: 0.376397

We have finished training iteration 133
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_131_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.297703  [    1/   89]
per-ex loss: 0.328602  [    2/   89]
per-ex loss: 0.324100  [    3/   89]
per-ex loss: 0.362929  [    4/   89]
per-ex loss: 0.368612  [    5/   89]
per-ex loss: 0.350599  [    6/   89]
per-ex loss: 0.364150  [    7/   89]
per-ex loss: 0.559868  [    8/   89]
per-ex loss: 0.319505  [    9/   89]
per-ex loss: 0.456580  [   10/   89]
per-ex loss: 0.296429  [   11/   89]
per-ex loss: 0.340386  [   12/   89]
per-ex loss: 0.316309  [   13/   89]
per-ex loss: 0.353639  [   14/   89]
per-ex loss: 0.359950  [   15/   89]
per-ex loss: 0.360883  [   16/   89]
per-ex loss: 0.450296  [   17/   89]
per-ex loss: 0.476181  [   18/   89]
per-ex loss: 0.290075  [   19/   89]
per-ex loss: 0.359062  [   20/   89]
per-ex loss: 0.389110  [   21/   89]
per-ex loss: 0.588703  [   22/   89]
per-ex loss: 0.411645  [   23/   89]
per-ex loss: 0.459712  [   24/   89]
per-ex loss: 0.327714  [   25/   89]
per-ex loss: 0.458964  [   26/   89]
per-ex loss: 0.534438  [   27/   89]
per-ex loss: 0.564860  [   28/   89]
per-ex loss: 0.304663  [   29/   89]
per-ex loss: 0.470596  [   30/   89]
per-ex loss: 0.314317  [   31/   89]
per-ex loss: 0.321196  [   32/   89]
per-ex loss: 0.511917  [   33/   89]
per-ex loss: 0.511452  [   34/   89]
per-ex loss: 0.441854  [   35/   89]
per-ex loss: 0.418841  [   36/   89]
per-ex loss: 0.366444  [   37/   89]
per-ex loss: 0.243221  [   38/   89]
per-ex loss: 0.620978  [   39/   89]
per-ex loss: 0.594187  [   40/   89]
per-ex loss: 0.999948  [   41/   89]
per-ex loss: 0.304591  [   42/   89]
per-ex loss: 0.348623  [   43/   89]
per-ex loss: 0.450918  [   44/   89]
per-ex loss: 0.340898  [   45/   89]
per-ex loss: 0.459377  [   46/   89]
per-ex loss: 0.574255  [   47/   89]
per-ex loss: 0.502830  [   48/   89]
per-ex loss: 0.407812  [   49/   89]
per-ex loss: 0.346191  [   50/   89]
per-ex loss: 0.253621  [   51/   89]
per-ex loss: 0.450280  [   52/   89]
per-ex loss: 0.371777  [   53/   89]
per-ex loss: 0.655825  [   54/   89]
per-ex loss: 0.575395  [   55/   89]
per-ex loss: 0.297866  [   56/   89]
per-ex loss: 0.335238  [   57/   89]
per-ex loss: 0.334939  [   58/   89]
per-ex loss: 0.500966  [   59/   89]
per-ex loss: 0.515336  [   60/   89]
per-ex loss: 0.274216  [   61/   89]
per-ex loss: 0.264362  [   62/   89]
per-ex loss: 0.313662  [   63/   89]
per-ex loss: 0.323774  [   64/   89]
per-ex loss: 0.421768  [   65/   89]
per-ex loss: 0.375028  [   66/   89]
per-ex loss: 0.546006  [   67/   89]
per-ex loss: 0.418666  [   68/   89]
per-ex loss: 0.371967  [   69/   89]
per-ex loss: 0.603090  [   70/   89]
per-ex loss: 0.233250  [   71/   89]
per-ex loss: 0.303956  [   72/   89]
per-ex loss: 0.361588  [   73/   89]
per-ex loss: 0.355455  [   74/   89]
per-ex loss: 0.206549  [   75/   89]
per-ex loss: 0.513282  [   76/   89]
per-ex loss: 0.288102  [   77/   89]
per-ex loss: 0.267785  [   78/   89]
per-ex loss: 0.510210  [   79/   89]
per-ex loss: 0.363816  [   80/   89]
per-ex loss: 0.346353  [   81/   89]
per-ex loss: 0.373456  [   82/   89]
per-ex loss: 0.321279  [   83/   89]
per-ex loss: 0.351369  [   84/   89]
per-ex loss: 0.575881  [   85/   89]
per-ex loss: 0.196021  [   86/   89]
per-ex loss: 0.278235  [   87/   89]
per-ex loss: 0.352016  [   88/   89]
per-ex loss: 0.440401  [   89/   89]
Train Error: Avg loss: 0.40189777
validation Error: 
 Avg loss: 0.53057740 
 F1: 0.496011 
 Precision: 0.515501 
 Recall: 0.477940
 IoU: 0.329797

test Error: 
 Avg loss: 0.47388567 
 F1: 0.571312 
 Precision: 0.575189 
 Recall: 0.567487
 IoU: 0.399886

We have finished training iteration 134
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_132_.pth
per-ex loss: 0.549914  [    1/   89]
per-ex loss: 0.335843  [    2/   89]
per-ex loss: 0.371004  [    3/   89]
per-ex loss: 0.492120  [    4/   89]
per-ex loss: 0.413016  [    5/   89]
per-ex loss: 0.274746  [    6/   89]
per-ex loss: 0.460188  [    7/   89]
per-ex loss: 0.423546  [    8/   89]
per-ex loss: 0.192378  [    9/   89]
per-ex loss: 0.999899  [   10/   89]
per-ex loss: 0.541004  [   11/   89]
per-ex loss: 0.362328  [   12/   89]
per-ex loss: 0.293133  [   13/   89]
per-ex loss: 0.451761  [   14/   89]
per-ex loss: 0.345758  [   15/   89]
per-ex loss: 0.257559  [   16/   89]
per-ex loss: 0.348207  [   17/   89]
per-ex loss: 0.356755  [   18/   89]
per-ex loss: 0.577841  [   19/   89]
per-ex loss: 0.436151  [   20/   89]
per-ex loss: 0.521569  [   21/   89]
per-ex loss: 0.398211  [   22/   89]
per-ex loss: 0.556356  [   23/   89]
per-ex loss: 0.309762  [   24/   89]
per-ex loss: 0.421926  [   25/   89]
per-ex loss: 0.428953  [   26/   89]
per-ex loss: 0.590933  [   27/   89]
per-ex loss: 0.325170  [   28/   89]
per-ex loss: 0.554457  [   29/   89]
per-ex loss: 0.307487  [   30/   89]
per-ex loss: 0.304412  [   31/   89]
per-ex loss: 0.353920  [   32/   89]
per-ex loss: 0.546362  [   33/   89]
per-ex loss: 0.379069  [   34/   89]
per-ex loss: 0.350298  [   35/   89]
per-ex loss: 0.530958  [   36/   89]
per-ex loss: 0.440183  [   37/   89]
per-ex loss: 0.292933  [   38/   89]
per-ex loss: 0.567636  [   39/   89]
per-ex loss: 0.391590  [   40/   89]
per-ex loss: 0.281507  [   41/   89]
per-ex loss: 0.332042  [   42/   89]
per-ex loss: 0.417040  [   43/   89]
per-ex loss: 0.341189  [   44/   89]
per-ex loss: 0.343319  [   45/   89]
per-ex loss: 0.385280  [   46/   89]
per-ex loss: 0.517585  [   47/   89]
per-ex loss: 0.374256  [   48/   89]
per-ex loss: 0.322579  [   49/   89]
per-ex loss: 0.597688  [   50/   89]
per-ex loss: 0.454351  [   51/   89]
per-ex loss: 0.432708  [   52/   89]
per-ex loss: 0.331035  [   53/   89]
per-ex loss: 0.274821  [   54/   89]
per-ex loss: 0.329980  [   55/   89]
per-ex loss: 0.637382  [   56/   89]
per-ex loss: 0.288345  [   57/   89]
per-ex loss: 0.348506  [   58/   89]
per-ex loss: 0.478360  [   59/   89]
per-ex loss: 0.337467  [   60/   89]
per-ex loss: 0.498081  [   61/   89]
per-ex loss: 0.319098  [   62/   89]
per-ex loss: 0.276713  [   63/   89]
per-ex loss: 0.510348  [   64/   89]
per-ex loss: 0.364551  [   65/   89]
per-ex loss: 0.229763  [   66/   89]
per-ex loss: 0.338881  [   67/   89]
per-ex loss: 0.346682  [   68/   89]
per-ex loss: 0.492737  [   69/   89]
per-ex loss: 0.257223  [   70/   89]
per-ex loss: 0.597931  [   71/   89]
per-ex loss: 0.364465  [   72/   89]
per-ex loss: 0.424364  [   73/   89]
per-ex loss: 0.420935  [   74/   89]
per-ex loss: 0.386521  [   75/   89]
per-ex loss: 0.324402  [   76/   89]
per-ex loss: 0.294278  [   77/   89]
per-ex loss: 0.430857  [   78/   89]
per-ex loss: 0.255273  [   79/   89]
per-ex loss: 0.336184  [   80/   89]
per-ex loss: 0.332491  [   81/   89]
per-ex loss: 0.199569  [   82/   89]
per-ex loss: 0.445966  [   83/   89]
per-ex loss: 0.337473  [   84/   89]
per-ex loss: 0.267867  [   85/   89]
per-ex loss: 0.241732  [   86/   89]
per-ex loss: 0.286030  [   87/   89]
per-ex loss: 0.434921  [   88/   89]
per-ex loss: 0.395543  [   89/   89]
Train Error: Avg loss: 0.39653545
validation Error: 
 Avg loss: 0.53893018 
 F1: 0.486336 
 Precision: 0.556059 
 Recall: 0.432150
 IoU: 0.321298

test Error: 
 Avg loss: 0.49119986 
 F1: 0.557845 
 Precision: 0.629759 
 Recall: 0.500672
 IoU: 0.386814

We have finished training iteration 135
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_133_.pth
per-ex loss: 0.319745  [    1/   89]
per-ex loss: 0.213521  [    2/   89]
per-ex loss: 0.526040  [    3/   89]
per-ex loss: 0.345179  [    4/   89]
per-ex loss: 0.499272  [    5/   89]
per-ex loss: 0.562760  [    6/   89]
per-ex loss: 0.302706  [    7/   89]
per-ex loss: 0.344718  [    8/   89]
per-ex loss: 0.558863  [    9/   89]
per-ex loss: 0.398458  [   10/   89]
per-ex loss: 0.369813  [   11/   89]
per-ex loss: 0.480437  [   12/   89]
per-ex loss: 0.258072  [   13/   89]
per-ex loss: 0.325519  [   14/   89]
per-ex loss: 0.483405  [   15/   89]
per-ex loss: 0.470880  [   16/   89]
per-ex loss: 0.358072  [   17/   89]
per-ex loss: 0.521579  [   18/   89]
per-ex loss: 0.429194  [   19/   89]
per-ex loss: 0.234002  [   20/   89]
per-ex loss: 0.268362  [   21/   89]
per-ex loss: 0.460662  [   22/   89]
per-ex loss: 0.338848  [   23/   89]
per-ex loss: 0.305461  [   24/   89]
per-ex loss: 0.603672  [   25/   89]
per-ex loss: 0.330988  [   26/   89]
per-ex loss: 0.336971  [   27/   89]
per-ex loss: 0.475304  [   28/   89]
per-ex loss: 0.324092  [   29/   89]
per-ex loss: 0.360098  [   30/   89]
per-ex loss: 0.587444  [   31/   89]
per-ex loss: 0.419706  [   32/   89]
per-ex loss: 0.380224  [   33/   89]
per-ex loss: 0.353151  [   34/   89]
per-ex loss: 0.431970  [   35/   89]
per-ex loss: 0.504166  [   36/   89]
per-ex loss: 0.368803  [   37/   89]
per-ex loss: 0.540876  [   38/   89]
per-ex loss: 0.320849  [   39/   89]
per-ex loss: 0.580257  [   40/   89]
per-ex loss: 0.468790  [   41/   89]
per-ex loss: 0.325939  [   42/   89]
per-ex loss: 0.320928  [   43/   89]
per-ex loss: 0.450314  [   44/   89]
per-ex loss: 0.349365  [   45/   89]
per-ex loss: 0.445106  [   46/   89]
per-ex loss: 0.316209  [   47/   89]
per-ex loss: 0.324343  [   48/   89]
per-ex loss: 0.550905  [   49/   89]
per-ex loss: 0.234550  [   50/   89]
per-ex loss: 0.237717  [   51/   89]
per-ex loss: 0.405436  [   52/   89]
per-ex loss: 0.346399  [   53/   89]
per-ex loss: 0.294939  [   54/   89]
per-ex loss: 0.353418  [   55/   89]
per-ex loss: 0.335339  [   56/   89]
per-ex loss: 0.444236  [   57/   89]
per-ex loss: 0.415289  [   58/   89]
per-ex loss: 0.400458  [   59/   89]
per-ex loss: 0.358333  [   60/   89]
per-ex loss: 0.426018  [   61/   89]
per-ex loss: 0.364634  [   62/   89]
per-ex loss: 0.294198  [   63/   89]
per-ex loss: 0.314510  [   64/   89]
per-ex loss: 0.999879  [   65/   89]
per-ex loss: 0.309369  [   66/   89]
per-ex loss: 0.236235  [   67/   89]
per-ex loss: 0.473802  [   68/   89]
per-ex loss: 0.395690  [   69/   89]
per-ex loss: 0.304077  [   70/   89]
per-ex loss: 0.486381  [   71/   89]
per-ex loss: 0.401215  [   72/   89]
per-ex loss: 0.267262  [   73/   89]
per-ex loss: 0.562057  [   74/   89]
per-ex loss: 0.348265  [   75/   89]
per-ex loss: 0.310741  [   76/   89]
per-ex loss: 0.428729  [   77/   89]
per-ex loss: 0.332621  [   78/   89]
per-ex loss: 0.617626  [   79/   89]
per-ex loss: 0.383588  [   80/   89]
per-ex loss: 0.505101  [   81/   89]
per-ex loss: 0.386133  [   82/   89]
per-ex loss: 0.552114  [   83/   89]
per-ex loss: 0.329835  [   84/   89]
per-ex loss: 0.454324  [   85/   89]
per-ex loss: 0.597478  [   86/   89]
per-ex loss: 0.329054  [   87/   89]
per-ex loss: 0.276778  [   88/   89]
per-ex loss: 0.481220  [   89/   89]
Train Error: Avg loss: 0.40270875
validation Error: 
 Avg loss: 0.53444573 
 F1: 0.486809 
 Precision: 0.522590 
 Recall: 0.455614
 IoU: 0.321710

test Error: 
 Avg loss: 0.49580027 
 F1: 0.549475 
 Precision: 0.572900 
 Recall: 0.527891
 IoU: 0.378811

We have finished training iteration 136
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_134_.pth
per-ex loss: 0.240770  [    1/   89]
per-ex loss: 0.349792  [    2/   89]
per-ex loss: 0.284600  [    3/   89]
per-ex loss: 0.289341  [    4/   89]
per-ex loss: 0.631174  [    5/   89]
per-ex loss: 0.477544  [    6/   89]
per-ex loss: 0.353747  [    7/   89]
per-ex loss: 0.357629  [    8/   89]
per-ex loss: 0.513523  [    9/   89]
per-ex loss: 0.232114  [   10/   89]
per-ex loss: 0.275577  [   11/   89]
per-ex loss: 0.297912  [   12/   89]
per-ex loss: 0.358506  [   13/   89]
per-ex loss: 0.449107  [   14/   89]
per-ex loss: 0.519476  [   15/   89]
per-ex loss: 0.470823  [   16/   89]
per-ex loss: 0.357552  [   17/   89]
per-ex loss: 0.574005  [   18/   89]
per-ex loss: 0.325470  [   19/   89]
per-ex loss: 0.520359  [   20/   89]
per-ex loss: 0.316419  [   21/   89]
per-ex loss: 0.517416  [   22/   89]
per-ex loss: 0.324434  [   23/   89]
per-ex loss: 0.452357  [   24/   89]
per-ex loss: 0.496396  [   25/   89]
per-ex loss: 0.501654  [   26/   89]
per-ex loss: 0.371548  [   27/   89]
per-ex loss: 0.184371  [   28/   89]
per-ex loss: 0.221249  [   29/   89]
per-ex loss: 0.455238  [   30/   89]
per-ex loss: 0.557976  [   31/   89]
per-ex loss: 0.300411  [   32/   89]
per-ex loss: 0.333345  [   33/   89]
per-ex loss: 0.404679  [   34/   89]
per-ex loss: 0.576919  [   35/   89]
per-ex loss: 0.307560  [   36/   89]
per-ex loss: 0.301640  [   37/   89]
per-ex loss: 0.457178  [   38/   89]
per-ex loss: 0.598846  [   39/   89]
per-ex loss: 0.534731  [   40/   89]
per-ex loss: 0.313368  [   41/   89]
per-ex loss: 0.307110  [   42/   89]
per-ex loss: 0.378043  [   43/   89]
per-ex loss: 0.427087  [   44/   89]
per-ex loss: 0.422587  [   45/   89]
per-ex loss: 0.414234  [   46/   89]
per-ex loss: 0.427066  [   47/   89]
per-ex loss: 0.593653  [   48/   89]
per-ex loss: 0.344805  [   49/   89]
per-ex loss: 0.252313  [   50/   89]
per-ex loss: 0.350220  [   51/   89]
per-ex loss: 0.384920  [   52/   89]
per-ex loss: 0.442734  [   53/   89]
per-ex loss: 0.315087  [   54/   89]
per-ex loss: 0.338996  [   55/   89]
per-ex loss: 0.999951  [   56/   89]
per-ex loss: 0.510888  [   57/   89]
per-ex loss: 0.257198  [   58/   89]
per-ex loss: 0.279642  [   59/   89]
per-ex loss: 0.335568  [   60/   89]
per-ex loss: 0.314485  [   61/   89]
per-ex loss: 0.521269  [   62/   89]
per-ex loss: 0.353733  [   63/   89]
per-ex loss: 0.414727  [   64/   89]
per-ex loss: 0.304020  [   65/   89]
per-ex loss: 0.481558  [   66/   89]
per-ex loss: 0.300783  [   67/   89]
per-ex loss: 0.265472  [   68/   89]
per-ex loss: 0.508601  [   69/   89]
per-ex loss: 0.368395  [   70/   89]
per-ex loss: 0.428683  [   71/   89]
per-ex loss: 0.379626  [   72/   89]
per-ex loss: 0.258697  [   73/   89]
per-ex loss: 0.319954  [   74/   89]
per-ex loss: 0.329409  [   75/   89]
per-ex loss: 0.313830  [   76/   89]
per-ex loss: 0.349625  [   77/   89]
per-ex loss: 0.240114  [   78/   89]
per-ex loss: 0.423889  [   79/   89]
per-ex loss: 0.413736  [   80/   89]
per-ex loss: 0.330328  [   81/   89]
per-ex loss: 0.443887  [   82/   89]
per-ex loss: 0.335097  [   83/   89]
per-ex loss: 0.521179  [   84/   89]
per-ex loss: 0.514959  [   85/   89]
per-ex loss: 0.334238  [   86/   89]
per-ex loss: 0.543783  [   87/   89]
per-ex loss: 0.260029  [   88/   89]
per-ex loss: 0.368184  [   89/   89]
Train Error: Avg loss: 0.39473199
validation Error: 
 Avg loss: 0.53397636 
 F1: 0.490034 
 Precision: 0.500796 
 Recall: 0.479725
 IoU: 0.324533

test Error: 
 Avg loss: 0.48712366 
 F1: 0.558005 
 Precision: 0.564361 
 Recall: 0.551789
 IoU: 0.386967

We have finished training iteration 137
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_135_.pth
per-ex loss: 0.345470  [    1/   89]
per-ex loss: 0.316573  [    2/   89]
per-ex loss: 0.292358  [    3/   89]
per-ex loss: 0.211304  [    4/   89]
per-ex loss: 0.342637  [    5/   89]
per-ex loss: 0.322998  [    6/   89]
per-ex loss: 0.265244  [    7/   89]
per-ex loss: 0.261465  [    8/   89]
per-ex loss: 0.441016  [    9/   89]
per-ex loss: 0.485776  [   10/   89]
per-ex loss: 0.176062  [   11/   89]
per-ex loss: 0.343549  [   12/   89]
per-ex loss: 0.543109  [   13/   89]
per-ex loss: 0.286135  [   14/   89]
per-ex loss: 0.321573  [   15/   89]
per-ex loss: 0.223512  [   16/   89]
per-ex loss: 0.287236  [   17/   89]
per-ex loss: 0.500294  [   18/   89]
per-ex loss: 0.233399  [   19/   89]
per-ex loss: 0.353082  [   20/   89]
per-ex loss: 0.315242  [   21/   89]
per-ex loss: 0.530692  [   22/   89]
per-ex loss: 0.341481  [   23/   89]
per-ex loss: 0.558449  [   24/   89]
per-ex loss: 0.451979  [   25/   89]
per-ex loss: 0.581142  [   26/   89]
per-ex loss: 0.473346  [   27/   89]
per-ex loss: 0.303076  [   28/   89]
per-ex loss: 0.275130  [   29/   89]
per-ex loss: 0.648358  [   30/   89]
per-ex loss: 0.378110  [   31/   89]
per-ex loss: 0.203871  [   32/   89]
per-ex loss: 0.373127  [   33/   89]
per-ex loss: 0.359015  [   34/   89]
per-ex loss: 0.421424  [   35/   89]
per-ex loss: 0.277641  [   36/   89]
per-ex loss: 0.445991  [   37/   89]
per-ex loss: 0.592746  [   38/   89]
per-ex loss: 0.407518  [   39/   89]
per-ex loss: 0.563935  [   40/   89]
per-ex loss: 0.314453  [   41/   89]
per-ex loss: 0.465207  [   42/   89]
per-ex loss: 0.540489  [   43/   89]
per-ex loss: 0.336394  [   44/   89]
per-ex loss: 0.271292  [   45/   89]
per-ex loss: 0.452765  [   46/   89]
per-ex loss: 0.583013  [   47/   89]
per-ex loss: 0.294001  [   48/   89]
per-ex loss: 0.374030  [   49/   89]
per-ex loss: 0.519096  [   50/   89]
per-ex loss: 0.270519  [   51/   89]
per-ex loss: 0.357907  [   52/   89]
per-ex loss: 0.491226  [   53/   89]
per-ex loss: 0.296652  [   54/   89]
per-ex loss: 0.376367  [   55/   89]
per-ex loss: 0.341357  [   56/   89]
per-ex loss: 0.369139  [   57/   89]
per-ex loss: 0.283115  [   58/   89]
per-ex loss: 0.364502  [   59/   89]
per-ex loss: 0.483349  [   60/   89]
per-ex loss: 0.398695  [   61/   89]
per-ex loss: 0.290795  [   62/   89]
per-ex loss: 0.371838  [   63/   89]
per-ex loss: 0.517855  [   64/   89]
per-ex loss: 0.560186  [   65/   89]
per-ex loss: 0.329095  [   66/   89]
per-ex loss: 0.484936  [   67/   89]
per-ex loss: 0.344220  [   68/   89]
per-ex loss: 0.479179  [   69/   89]
per-ex loss: 0.553223  [   70/   89]
per-ex loss: 0.535891  [   71/   89]
per-ex loss: 0.459361  [   72/   89]
per-ex loss: 0.356408  [   73/   89]
per-ex loss: 0.374260  [   74/   89]
per-ex loss: 0.308683  [   75/   89]
per-ex loss: 0.999879  [   76/   89]
per-ex loss: 0.346705  [   77/   89]
per-ex loss: 0.340859  [   78/   89]
per-ex loss: 0.479105  [   79/   89]
per-ex loss: 0.478076  [   80/   89]
per-ex loss: 0.462681  [   81/   89]
per-ex loss: 0.417433  [   82/   89]
per-ex loss: 0.318436  [   83/   89]
per-ex loss: 0.304102  [   84/   89]
per-ex loss: 0.285570  [   85/   89]
per-ex loss: 0.397131  [   86/   89]
per-ex loss: 0.336289  [   87/   89]
per-ex loss: 0.296426  [   88/   89]
per-ex loss: 0.306306  [   89/   89]
Train Error: Avg loss: 0.39296138
validation Error: 
 Avg loss: 0.53988200 
 F1: 0.487793 
 Precision: 0.535383 
 Recall: 0.447972
 IoU: 0.322570

test Error: 
 Avg loss: 0.50082891 
 F1: 0.546974 
 Precision: 0.552926 
 Recall: 0.541148
 IoU: 0.376437

We have finished training iteration 138
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_136_.pth
per-ex loss: 0.503509  [    1/   89]
per-ex loss: 0.265770  [    2/   89]
per-ex loss: 0.516826  [    3/   89]
per-ex loss: 0.262918  [    4/   89]
per-ex loss: 0.333947  [    5/   89]
per-ex loss: 0.267681  [    6/   89]
per-ex loss: 0.274800  [    7/   89]
per-ex loss: 0.298164  [    8/   89]
per-ex loss: 0.270073  [    9/   89]
per-ex loss: 0.450453  [   10/   89]
per-ex loss: 0.390898  [   11/   89]
per-ex loss: 0.479724  [   12/   89]
per-ex loss: 0.351414  [   13/   89]
per-ex loss: 0.324124  [   14/   89]
per-ex loss: 0.368443  [   15/   89]
per-ex loss: 0.347188  [   16/   89]
per-ex loss: 0.326757  [   17/   89]
per-ex loss: 0.361308  [   18/   89]
per-ex loss: 0.296317  [   19/   89]
per-ex loss: 0.314981  [   20/   89]
per-ex loss: 0.325941  [   21/   89]
per-ex loss: 0.376750  [   22/   89]
per-ex loss: 0.415255  [   23/   89]
per-ex loss: 0.333089  [   24/   89]
per-ex loss: 0.546449  [   25/   89]
per-ex loss: 0.179649  [   26/   89]
per-ex loss: 0.490738  [   27/   89]
per-ex loss: 0.323841  [   28/   89]
per-ex loss: 0.328791  [   29/   89]
per-ex loss: 0.312633  [   30/   89]
per-ex loss: 0.469422  [   31/   89]
per-ex loss: 0.586478  [   32/   89]
per-ex loss: 0.579280  [   33/   89]
per-ex loss: 0.358468  [   34/   89]
per-ex loss: 0.408293  [   35/   89]
per-ex loss: 0.323892  [   36/   89]
per-ex loss: 0.428771  [   37/   89]
per-ex loss: 0.449458  [   38/   89]
per-ex loss: 0.319731  [   39/   89]
per-ex loss: 0.310889  [   40/   89]
per-ex loss: 0.532274  [   41/   89]
per-ex loss: 0.269066  [   42/   89]
per-ex loss: 0.379357  [   43/   89]
per-ex loss: 0.541099  [   44/   89]
per-ex loss: 0.999924  [   45/   89]
per-ex loss: 0.320702  [   46/   89]
per-ex loss: 0.310423  [   47/   89]
per-ex loss: 0.429995  [   48/   89]
per-ex loss: 0.291415  [   49/   89]
per-ex loss: 0.347798  [   50/   89]
per-ex loss: 0.595810  [   51/   89]
per-ex loss: 0.399178  [   52/   89]
per-ex loss: 0.278089  [   53/   89]
per-ex loss: 0.547995  [   54/   89]
per-ex loss: 0.319364  [   55/   89]
per-ex loss: 0.230864  [   56/   89]
per-ex loss: 0.461939  [   57/   89]
per-ex loss: 0.427469  [   58/   89]
per-ex loss: 0.595605  [   59/   89]
per-ex loss: 0.479014  [   60/   89]
per-ex loss: 0.366462  [   61/   89]
per-ex loss: 0.323226  [   62/   89]
per-ex loss: 0.573722  [   63/   89]
per-ex loss: 0.306927  [   64/   89]
per-ex loss: 0.495729  [   65/   89]
per-ex loss: 0.317267  [   66/   89]
per-ex loss: 0.322001  [   67/   89]
per-ex loss: 0.286111  [   68/   89]
per-ex loss: 0.329246  [   69/   89]
per-ex loss: 0.253867  [   70/   89]
per-ex loss: 0.485138  [   71/   89]
per-ex loss: 0.478110  [   72/   89]
per-ex loss: 0.535386  [   73/   89]
per-ex loss: 0.303449  [   74/   89]
per-ex loss: 0.372046  [   75/   89]
per-ex loss: 0.526848  [   76/   89]
per-ex loss: 0.376623  [   77/   89]
per-ex loss: 0.394085  [   78/   89]
per-ex loss: 0.313551  [   79/   89]
per-ex loss: 0.279104  [   80/   89]
per-ex loss: 0.593523  [   81/   89]
per-ex loss: 0.361886  [   82/   89]
per-ex loss: 0.367070  [   83/   89]
per-ex loss: 0.446566  [   84/   89]
per-ex loss: 0.410244  [   85/   89]
per-ex loss: 0.512533  [   86/   89]
per-ex loss: 0.464808  [   87/   89]
per-ex loss: 0.377492  [   88/   89]
per-ex loss: 0.390498  [   89/   89]
Train Error: Avg loss: 0.39543831
validation Error: 
 Avg loss: 0.53484550 
 F1: 0.492724 
 Precision: 0.508178 
 Recall: 0.478181
 IoU: 0.326897

test Error: 
 Avg loss: 0.48533229 
 F1: 0.557417 
 Precision: 0.561454 
 Recall: 0.553438
 IoU: 0.386402

We have finished training iteration 139
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_137_.pth
per-ex loss: 0.411762  [    1/   89]
per-ex loss: 0.394338  [    2/   89]
per-ex loss: 0.320429  [    3/   89]
per-ex loss: 0.426854  [    4/   89]
per-ex loss: 0.425916  [    5/   89]
per-ex loss: 0.370245  [    6/   89]
per-ex loss: 0.348389  [    7/   89]
per-ex loss: 0.241613  [    8/   89]
per-ex loss: 0.640740  [    9/   89]
per-ex loss: 0.440233  [   10/   89]
per-ex loss: 0.356719  [   11/   89]
per-ex loss: 0.347231  [   12/   89]
per-ex loss: 0.280913  [   13/   89]
per-ex loss: 0.433958  [   14/   89]
per-ex loss: 0.999932  [   15/   89]
per-ex loss: 0.518465  [   16/   89]
per-ex loss: 0.450505  [   17/   89]
per-ex loss: 0.264418  [   18/   89]
per-ex loss: 0.365209  [   19/   89]
per-ex loss: 0.519674  [   20/   89]
per-ex loss: 0.504079  [   21/   89]
per-ex loss: 0.397112  [   22/   89]
per-ex loss: 0.633480  [   23/   89]
per-ex loss: 0.622539  [   24/   89]
per-ex loss: 0.331497  [   25/   89]
per-ex loss: 0.380209  [   26/   89]
per-ex loss: 0.355738  [   27/   89]
per-ex loss: 0.519861  [   28/   89]
per-ex loss: 0.439897  [   29/   89]
per-ex loss: 0.280093  [   30/   89]
per-ex loss: 0.529808  [   31/   89]
per-ex loss: 0.316819  [   32/   89]
per-ex loss: 0.272927  [   33/   89]
per-ex loss: 0.567451  [   34/   89]
per-ex loss: 0.388065  [   35/   89]
per-ex loss: 0.497331  [   36/   89]
per-ex loss: 0.334448  [   37/   89]
per-ex loss: 0.386156  [   38/   89]
per-ex loss: 0.495486  [   39/   89]
per-ex loss: 0.303078  [   40/   89]
per-ex loss: 0.565435  [   41/   89]
per-ex loss: 0.264636  [   42/   89]
per-ex loss: 0.455091  [   43/   89]
per-ex loss: 0.486919  [   44/   89]
per-ex loss: 0.322556  [   45/   89]
per-ex loss: 0.365523  [   46/   89]
per-ex loss: 0.345264  [   47/   89]
per-ex loss: 0.453644  [   48/   89]
per-ex loss: 0.353482  [   49/   89]
per-ex loss: 0.407684  [   50/   89]
per-ex loss: 0.652475  [   51/   89]
per-ex loss: 0.532959  [   52/   89]
per-ex loss: 0.359031  [   53/   89]
per-ex loss: 0.425999  [   54/   89]
per-ex loss: 0.358122  [   55/   89]
per-ex loss: 0.182157  [   56/   89]
per-ex loss: 0.290327  [   57/   89]
per-ex loss: 0.425377  [   58/   89]
per-ex loss: 0.465734  [   59/   89]
per-ex loss: 0.279034  [   60/   89]
per-ex loss: 0.359305  [   61/   89]
per-ex loss: 0.468419  [   62/   89]
per-ex loss: 0.386074  [   63/   89]
per-ex loss: 0.362271  [   64/   89]
per-ex loss: 0.328465  [   65/   89]
per-ex loss: 0.559377  [   66/   89]
per-ex loss: 0.382743  [   67/   89]
per-ex loss: 0.509724  [   68/   89]
per-ex loss: 0.562344  [   69/   89]
per-ex loss: 0.572223  [   70/   89]
per-ex loss: 0.453923  [   71/   89]
per-ex loss: 0.368035  [   72/   89]
per-ex loss: 0.468748  [   73/   89]
per-ex loss: 0.470716  [   74/   89]
per-ex loss: 0.575529  [   75/   89]
per-ex loss: 0.281887  [   76/   89]
per-ex loss: 0.235765  [   77/   89]
per-ex loss: 0.535888  [   78/   89]
per-ex loss: 0.368298  [   79/   89]
per-ex loss: 0.422377  [   80/   89]
per-ex loss: 0.331454  [   81/   89]
per-ex loss: 0.294405  [   82/   89]
per-ex loss: 0.367843  [   83/   89]
per-ex loss: 0.274704  [   84/   89]
per-ex loss: 0.338341  [   85/   89]
per-ex loss: 0.364118  [   86/   89]
per-ex loss: 0.300107  [   87/   89]
per-ex loss: 0.391644  [   88/   89]
per-ex loss: 0.253553  [   89/   89]
Train Error: Avg loss: 0.41228476
validation Error: 
 Avg loss: 0.54122145 
 F1: 0.479768 
 Precision: 0.485094 
 Recall: 0.474557
 IoU: 0.315588

test Error: 
 Avg loss: 0.49664441 
 F1: 0.546281 
 Precision: 0.542892 
 Recall: 0.549713
 IoU: 0.375782

We have finished training iteration 140
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_138_.pth
per-ex loss: 0.318323  [    1/   89]
per-ex loss: 0.365412  [    2/   89]
per-ex loss: 0.410396  [    3/   89]
per-ex loss: 0.527792  [    4/   89]
per-ex loss: 0.455240  [    5/   89]
per-ex loss: 0.505456  [    6/   89]
per-ex loss: 0.428699  [    7/   89]
per-ex loss: 0.433185  [    8/   89]
per-ex loss: 0.339686  [    9/   89]
per-ex loss: 0.395918  [   10/   89]
per-ex loss: 0.406744  [   11/   89]
per-ex loss: 0.512156  [   12/   89]
per-ex loss: 0.319917  [   13/   89]
per-ex loss: 0.394942  [   14/   89]
per-ex loss: 0.390933  [   15/   89]
per-ex loss: 0.321718  [   16/   89]
per-ex loss: 0.345902  [   17/   89]
per-ex loss: 0.361781  [   18/   89]
per-ex loss: 0.317654  [   19/   89]
per-ex loss: 0.350930  [   20/   89]
per-ex loss: 0.316960  [   21/   89]
per-ex loss: 0.363556  [   22/   89]
per-ex loss: 0.350395  [   23/   89]
per-ex loss: 0.264824  [   24/   89]
per-ex loss: 0.511468  [   25/   89]
per-ex loss: 0.592868  [   26/   89]
per-ex loss: 0.265805  [   27/   89]
per-ex loss: 0.213526  [   28/   89]
per-ex loss: 0.411544  [   29/   89]
per-ex loss: 0.331855  [   30/   89]
per-ex loss: 0.330136  [   31/   89]
per-ex loss: 0.505322  [   32/   89]
per-ex loss: 0.346340  [   33/   89]
per-ex loss: 0.369699  [   34/   89]
per-ex loss: 0.519636  [   35/   89]
per-ex loss: 0.275797  [   36/   89]
per-ex loss: 0.408256  [   37/   89]
per-ex loss: 0.295870  [   38/   89]
per-ex loss: 0.246042  [   39/   89]
per-ex loss: 0.453762  [   40/   89]
per-ex loss: 0.391657  [   41/   89]
per-ex loss: 0.391705  [   42/   89]
per-ex loss: 0.307976  [   43/   89]
per-ex loss: 0.253112  [   44/   89]
per-ex loss: 0.490562  [   45/   89]
per-ex loss: 0.322311  [   46/   89]
per-ex loss: 0.537984  [   47/   89]
per-ex loss: 0.293274  [   48/   89]
per-ex loss: 0.626594  [   49/   89]
per-ex loss: 0.271860  [   50/   89]
per-ex loss: 0.345336  [   51/   89]
per-ex loss: 0.999961  [   52/   89]
per-ex loss: 0.513000  [   53/   89]
per-ex loss: 0.327229  [   54/   89]
per-ex loss: 0.365346  [   55/   89]
per-ex loss: 0.315493  [   56/   89]
per-ex loss: 0.254499  [   57/   89]
per-ex loss: 0.615288  [   58/   89]
per-ex loss: 0.338860  [   59/   89]
per-ex loss: 0.315982  [   60/   89]
per-ex loss: 0.625480  [   61/   89]
per-ex loss: 0.320274  [   62/   89]
per-ex loss: 0.203800  [   63/   89]
per-ex loss: 0.590877  [   64/   89]
per-ex loss: 0.459399  [   65/   89]
per-ex loss: 0.274815  [   66/   89]
per-ex loss: 0.305043  [   67/   89]
per-ex loss: 0.334166  [   68/   89]
per-ex loss: 0.571243  [   69/   89]
per-ex loss: 0.398572  [   70/   89]
per-ex loss: 0.354431  [   71/   89]
per-ex loss: 0.332311  [   72/   89]
per-ex loss: 0.312410  [   73/   89]
per-ex loss: 0.445673  [   74/   89]
per-ex loss: 0.377353  [   75/   89]
per-ex loss: 0.488087  [   76/   89]
per-ex loss: 0.217118  [   77/   89]
per-ex loss: 0.361554  [   78/   89]
per-ex loss: 0.297281  [   79/   89]
per-ex loss: 0.251814  [   80/   89]
per-ex loss: 0.459571  [   81/   89]
per-ex loss: 0.529278  [   82/   89]
per-ex loss: 0.299150  [   83/   89]
per-ex loss: 0.393288  [   84/   89]
per-ex loss: 0.614330  [   85/   89]
per-ex loss: 0.261497  [   86/   89]
per-ex loss: 0.478643  [   87/   89]
per-ex loss: 0.459215  [   88/   89]
per-ex loss: 0.264283  [   89/   89]
Train Error: Avg loss: 0.39140934
validation Error: 
 Avg loss: 0.52830814 
 F1: 0.495758 
 Precision: 0.526792 
 Recall: 0.468178
 IoU: 0.329574

test Error: 
 Avg loss: 0.48130707 
 F1: 0.564825 
 Precision: 0.582858 
 Recall: 0.547874
 IoU: 0.393558

We have finished training iteration 141
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_139_.pth
per-ex loss: 0.306917  [    1/   89]
per-ex loss: 0.375478  [    2/   89]
per-ex loss: 0.253542  [    3/   89]
per-ex loss: 0.279731  [    4/   89]
per-ex loss: 0.280351  [    5/   89]
per-ex loss: 0.313260  [    6/   89]
per-ex loss: 0.292231  [    7/   89]
per-ex loss: 0.390907  [    8/   89]
per-ex loss: 0.543494  [    9/   89]
per-ex loss: 0.287736  [   10/   89]
per-ex loss: 0.422174  [   11/   89]
per-ex loss: 0.294897  [   12/   89]
per-ex loss: 0.402263  [   13/   89]
per-ex loss: 0.344010  [   14/   89]
per-ex loss: 0.376285  [   15/   89]
per-ex loss: 0.430827  [   16/   89]
per-ex loss: 0.518421  [   17/   89]
per-ex loss: 0.432105  [   18/   89]
per-ex loss: 0.365182  [   19/   89]
per-ex loss: 0.303383  [   20/   89]
per-ex loss: 0.580591  [   21/   89]
per-ex loss: 0.504982  [   22/   89]
per-ex loss: 0.484956  [   23/   89]
per-ex loss: 0.280879  [   24/   89]
per-ex loss: 0.316814  [   25/   89]
per-ex loss: 0.297789  [   26/   89]
per-ex loss: 0.197841  [   27/   89]
per-ex loss: 0.292882  [   28/   89]
per-ex loss: 0.576546  [   29/   89]
per-ex loss: 0.354720  [   30/   89]
per-ex loss: 0.358622  [   31/   89]
per-ex loss: 0.455377  [   32/   89]
per-ex loss: 0.379913  [   33/   89]
per-ex loss: 0.266045  [   34/   89]
per-ex loss: 0.415870  [   35/   89]
per-ex loss: 0.310128  [   36/   89]
per-ex loss: 0.486824  [   37/   89]
per-ex loss: 0.591003  [   38/   89]
per-ex loss: 0.370458  [   39/   89]
per-ex loss: 0.254562  [   40/   89]
per-ex loss: 0.391753  [   41/   89]
per-ex loss: 0.316254  [   42/   89]
per-ex loss: 0.382168  [   43/   89]
per-ex loss: 0.551436  [   44/   89]
per-ex loss: 0.999907  [   45/   89]
per-ex loss: 0.668642  [   46/   89]
per-ex loss: 0.290240  [   47/   89]
per-ex loss: 0.585071  [   48/   89]
per-ex loss: 0.252577  [   49/   89]
per-ex loss: 0.479126  [   50/   89]
per-ex loss: 0.455980  [   51/   89]
per-ex loss: 0.442350  [   52/   89]
per-ex loss: 0.429588  [   53/   89]
per-ex loss: 0.341123  [   54/   89]
per-ex loss: 0.471036  [   55/   89]
per-ex loss: 0.258328  [   56/   89]
per-ex loss: 0.416627  [   57/   89]
per-ex loss: 0.346937  [   58/   89]
per-ex loss: 0.557382  [   59/   89]
per-ex loss: 0.241840  [   60/   89]
per-ex loss: 0.475795  [   61/   89]
per-ex loss: 0.277859  [   62/   89]
per-ex loss: 0.350498  [   63/   89]
per-ex loss: 0.298238  [   64/   89]
per-ex loss: 0.596795  [   65/   89]
per-ex loss: 0.320505  [   66/   89]
per-ex loss: 0.258137  [   67/   89]
per-ex loss: 0.313786  [   68/   89]
per-ex loss: 0.357001  [   69/   89]
per-ex loss: 0.316142  [   70/   89]
per-ex loss: 0.457400  [   71/   89]
per-ex loss: 0.347710  [   72/   89]
per-ex loss: 0.350401  [   73/   89]
per-ex loss: 0.493515  [   74/   89]
per-ex loss: 0.363216  [   75/   89]
per-ex loss: 0.595952  [   76/   89]
per-ex loss: 0.360744  [   77/   89]
per-ex loss: 0.329349  [   78/   89]
per-ex loss: 0.308309  [   79/   89]
per-ex loss: 0.348696  [   80/   89]
per-ex loss: 0.512524  [   81/   89]
per-ex loss: 0.368463  [   82/   89]
per-ex loss: 0.278385  [   83/   89]
per-ex loss: 0.506696  [   84/   89]
per-ex loss: 0.293671  [   85/   89]
per-ex loss: 0.324759  [   86/   89]
per-ex loss: 0.420146  [   87/   89]
per-ex loss: 0.289518  [   88/   89]
per-ex loss: 0.353285  [   89/   89]
Train Error: Avg loss: 0.39029053
validation Error: 
 Avg loss: 0.52305616 
 F1: 0.501737 
 Precision: 0.540789 
 Recall: 0.467946
 IoU: 0.334879

test Error: 
 Avg loss: 0.47842743 
 F1: 0.564589 
 Precision: 0.581126 
 Recall: 0.548967
 IoU: 0.393329

We have finished training iteration 142
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_140_.pth
per-ex loss: 0.295975  [    1/   89]
per-ex loss: 0.335363  [    2/   89]
per-ex loss: 0.289630  [    3/   89]
per-ex loss: 0.357405  [    4/   89]
per-ex loss: 0.295128  [    5/   89]
per-ex loss: 0.226518  [    6/   89]
per-ex loss: 0.417417  [    7/   89]
per-ex loss: 0.398009  [    8/   89]
per-ex loss: 0.468805  [    9/   89]
per-ex loss: 0.483988  [   10/   89]
per-ex loss: 0.356617  [   11/   89]
per-ex loss: 0.260571  [   12/   89]
per-ex loss: 0.388672  [   13/   89]
per-ex loss: 0.326920  [   14/   89]
per-ex loss: 0.199479  [   15/   89]
per-ex loss: 0.326455  [   16/   89]
per-ex loss: 0.468855  [   17/   89]
per-ex loss: 0.164594  [   18/   89]
per-ex loss: 0.323312  [   19/   89]
per-ex loss: 0.522150  [   20/   89]
per-ex loss: 0.405935  [   21/   89]
per-ex loss: 0.300554  [   22/   89]
per-ex loss: 0.281321  [   23/   89]
per-ex loss: 0.311660  [   24/   89]
per-ex loss: 0.372405  [   25/   89]
per-ex loss: 0.311539  [   26/   89]
per-ex loss: 0.407152  [   27/   89]
per-ex loss: 0.411227  [   28/   89]
per-ex loss: 0.502104  [   29/   89]
per-ex loss: 0.308801  [   30/   89]
per-ex loss: 0.458555  [   31/   89]
per-ex loss: 0.322682  [   32/   89]
per-ex loss: 0.476794  [   33/   89]
per-ex loss: 0.408748  [   34/   89]
per-ex loss: 0.490562  [   35/   89]
per-ex loss: 0.264237  [   36/   89]
per-ex loss: 0.442942  [   37/   89]
per-ex loss: 0.412386  [   38/   89]
per-ex loss: 0.338370  [   39/   89]
per-ex loss: 0.411338  [   40/   89]
per-ex loss: 0.468601  [   41/   89]
per-ex loss: 0.305206  [   42/   89]
per-ex loss: 0.592537  [   43/   89]
per-ex loss: 0.237562  [   44/   89]
per-ex loss: 0.468218  [   45/   89]
per-ex loss: 0.322576  [   46/   89]
per-ex loss: 0.335362  [   47/   89]
per-ex loss: 0.314192  [   48/   89]
per-ex loss: 0.522179  [   49/   89]
per-ex loss: 0.539628  [   50/   89]
per-ex loss: 0.351905  [   51/   89]
per-ex loss: 0.313911  [   52/   89]
per-ex loss: 0.577799  [   53/   89]
per-ex loss: 0.331801  [   54/   89]
per-ex loss: 0.294911  [   55/   89]
per-ex loss: 0.475221  [   56/   89]
per-ex loss: 0.567832  [   57/   89]
per-ex loss: 0.266606  [   58/   89]
per-ex loss: 0.288131  [   59/   89]
per-ex loss: 0.999939  [   60/   89]
per-ex loss: 0.316583  [   61/   89]
per-ex loss: 0.485929  [   62/   89]
per-ex loss: 0.370273  [   63/   89]
per-ex loss: 0.189600  [   64/   89]
per-ex loss: 0.312217  [   65/   89]
per-ex loss: 0.358474  [   66/   89]
per-ex loss: 0.469227  [   67/   89]
per-ex loss: 0.469374  [   68/   89]
per-ex loss: 0.411169  [   69/   89]
per-ex loss: 0.303444  [   70/   89]
per-ex loss: 0.397377  [   71/   89]
per-ex loss: 0.556686  [   72/   89]
per-ex loss: 0.456205  [   73/   89]
per-ex loss: 0.338250  [   74/   89]
per-ex loss: 0.372620  [   75/   89]
per-ex loss: 0.346130  [   76/   89]
per-ex loss: 0.297314  [   77/   89]
per-ex loss: 0.624243  [   78/   89]
per-ex loss: 0.267007  [   79/   89]
per-ex loss: 0.508462  [   80/   89]
per-ex loss: 0.243091  [   81/   89]
per-ex loss: 0.371871  [   82/   89]
per-ex loss: 0.369371  [   83/   89]
per-ex loss: 0.288547  [   84/   89]
per-ex loss: 0.437781  [   85/   89]
per-ex loss: 0.560563  [   86/   89]
per-ex loss: 0.366142  [   87/   89]
per-ex loss: 0.513636  [   88/   89]
per-ex loss: 0.298482  [   89/   89]
Train Error: Avg loss: 0.38673437
validation Error: 
 Avg loss: 0.54257623 
 F1: 0.478247 
 Precision: 0.477201 
 Recall: 0.479298
 IoU: 0.314274

test Error: 
 Avg loss: 0.49828625 
 F1: 0.544241 
 Precision: 0.529405 
 Recall: 0.559933
 IoU: 0.373854

We have finished training iteration 143
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_141_.pth
per-ex loss: 0.464401  [    1/   89]
per-ex loss: 0.326271  [    2/   89]
per-ex loss: 0.553802  [    3/   89]
per-ex loss: 0.488949  [    4/   89]
per-ex loss: 0.557677  [    5/   89]
per-ex loss: 0.282988  [    6/   89]
per-ex loss: 0.232194  [    7/   89]
per-ex loss: 0.527141  [    8/   89]
per-ex loss: 0.336934  [    9/   89]
per-ex loss: 0.491181  [   10/   89]
per-ex loss: 0.433427  [   11/   89]
per-ex loss: 0.587029  [   12/   89]
per-ex loss: 0.432371  [   13/   89]
per-ex loss: 0.341842  [   14/   89]
per-ex loss: 0.488780  [   15/   89]
per-ex loss: 0.327099  [   16/   89]
per-ex loss: 0.334553  [   17/   89]
per-ex loss: 0.401291  [   18/   89]
per-ex loss: 0.425259  [   19/   89]
per-ex loss: 0.376808  [   20/   89]
per-ex loss: 0.489006  [   21/   89]
per-ex loss: 0.447258  [   22/   89]
per-ex loss: 0.237456  [   23/   89]
per-ex loss: 0.321267  [   24/   89]
per-ex loss: 0.277849  [   25/   89]
per-ex loss: 0.311127  [   26/   89]
per-ex loss: 0.531236  [   27/   89]
per-ex loss: 0.334340  [   28/   89]
per-ex loss: 0.494339  [   29/   89]
per-ex loss: 0.568047  [   30/   89]
per-ex loss: 0.622048  [   31/   89]
per-ex loss: 0.504982  [   32/   89]
per-ex loss: 0.315469  [   33/   89]
per-ex loss: 0.384983  [   34/   89]
per-ex loss: 0.304877  [   35/   89]
per-ex loss: 0.467849  [   36/   89]
per-ex loss: 0.401140  [   37/   89]
per-ex loss: 0.508911  [   38/   89]
per-ex loss: 0.315965  [   39/   89]
per-ex loss: 0.523426  [   40/   89]
per-ex loss: 0.540098  [   41/   89]
per-ex loss: 0.192855  [   42/   89]
per-ex loss: 0.313774  [   43/   89]
per-ex loss: 0.557400  [   44/   89]
per-ex loss: 0.297505  [   45/   89]
per-ex loss: 0.246910  [   46/   89]
per-ex loss: 0.314708  [   47/   89]
per-ex loss: 0.373952  [   48/   89]
per-ex loss: 0.329681  [   49/   89]
per-ex loss: 0.269045  [   50/   89]
per-ex loss: 0.332810  [   51/   89]
per-ex loss: 0.267802  [   52/   89]
per-ex loss: 0.457969  [   53/   89]
per-ex loss: 0.372263  [   54/   89]
per-ex loss: 0.999918  [   55/   89]
per-ex loss: 0.349670  [   56/   89]
per-ex loss: 0.349923  [   57/   89]
per-ex loss: 0.298124  [   58/   89]
per-ex loss: 0.485454  [   59/   89]
per-ex loss: 0.234520  [   60/   89]
per-ex loss: 0.278776  [   61/   89]
per-ex loss: 0.379342  [   62/   89]
per-ex loss: 0.304045  [   63/   89]
per-ex loss: 0.287379  [   64/   89]
per-ex loss: 0.340489  [   65/   89]
per-ex loss: 0.343241  [   66/   89]
per-ex loss: 0.313206  [   67/   89]
per-ex loss: 0.347480  [   68/   89]
per-ex loss: 0.303339  [   69/   89]
per-ex loss: 0.329500  [   70/   89]
per-ex loss: 0.297864  [   71/   89]
per-ex loss: 0.230738  [   72/   89]
per-ex loss: 0.469657  [   73/   89]
per-ex loss: 0.514146  [   74/   89]
per-ex loss: 0.261343  [   75/   89]
per-ex loss: 0.216776  [   76/   89]
per-ex loss: 0.386412  [   77/   89]
per-ex loss: 0.453547  [   78/   89]
per-ex loss: 0.485434  [   79/   89]
per-ex loss: 0.353766  [   80/   89]
per-ex loss: 0.504448  [   81/   89]
per-ex loss: 0.335490  [   82/   89]
per-ex loss: 0.212911  [   83/   89]
per-ex loss: 0.369390  [   84/   89]
per-ex loss: 0.333269  [   85/   89]
per-ex loss: 0.270083  [   86/   89]
per-ex loss: 0.522621  [   87/   89]
per-ex loss: 0.587791  [   88/   89]
per-ex loss: 0.360762  [   89/   89]
Train Error: Avg loss: 0.39044045
validation Error: 
 Avg loss: 0.53374936 
 F1: 0.491517 
 Precision: 0.521111 
 Recall: 0.465103
 IoU: 0.325835

test Error: 
 Avg loss: 0.48848138 
 F1: 0.556737 
 Precision: 0.584676 
 Recall: 0.531346
 IoU: 0.385749

We have finished training iteration 144
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_97_.pth
per-ex loss: 0.273106  [    1/   89]
per-ex loss: 0.429497  [    2/   89]
per-ex loss: 0.363629  [    3/   89]
per-ex loss: 0.439605  [    4/   89]
per-ex loss: 0.513614  [    5/   89]
per-ex loss: 0.369256  [    6/   89]
per-ex loss: 0.329446  [    7/   89]
per-ex loss: 0.239218  [    8/   89]
per-ex loss: 0.577090  [    9/   89]
per-ex loss: 0.325637  [   10/   89]
per-ex loss: 0.345232  [   11/   89]
per-ex loss: 0.482942  [   12/   89]
per-ex loss: 0.444689  [   13/   89]
per-ex loss: 0.350741  [   14/   89]
per-ex loss: 0.241467  [   15/   89]
per-ex loss: 0.431640  [   16/   89]
per-ex loss: 0.292943  [   17/   89]
per-ex loss: 0.340201  [   18/   89]
per-ex loss: 0.225622  [   19/   89]
per-ex loss: 0.425284  [   20/   89]
per-ex loss: 0.385638  [   21/   89]
per-ex loss: 0.294927  [   22/   89]
per-ex loss: 0.307677  [   23/   89]
per-ex loss: 0.205783  [   24/   89]
per-ex loss: 0.316590  [   25/   89]
per-ex loss: 0.389529  [   26/   89]
per-ex loss: 0.391653  [   27/   89]
per-ex loss: 0.226265  [   28/   89]
per-ex loss: 0.261631  [   29/   89]
per-ex loss: 0.361285  [   30/   89]
per-ex loss: 0.537226  [   31/   89]
per-ex loss: 0.579618  [   32/   89]
per-ex loss: 0.263874  [   33/   89]
per-ex loss: 0.273427  [   34/   89]
per-ex loss: 0.316032  [   35/   89]
per-ex loss: 0.365114  [   36/   89]
per-ex loss: 0.465024  [   37/   89]
per-ex loss: 0.277457  [   38/   89]
per-ex loss: 0.358614  [   39/   89]
per-ex loss: 0.458701  [   40/   89]
per-ex loss: 0.343896  [   41/   89]
per-ex loss: 0.307432  [   42/   89]
per-ex loss: 0.505015  [   43/   89]
per-ex loss: 0.415406  [   44/   89]
per-ex loss: 0.345801  [   45/   89]
per-ex loss: 0.417107  [   46/   89]
per-ex loss: 0.569456  [   47/   89]
per-ex loss: 0.348007  [   48/   89]
per-ex loss: 0.529589  [   49/   89]
per-ex loss: 0.999898  [   50/   89]
per-ex loss: 0.395316  [   51/   89]
per-ex loss: 0.350264  [   52/   89]
per-ex loss: 0.199017  [   53/   89]
per-ex loss: 0.255038  [   54/   89]
per-ex loss: 0.657154  [   55/   89]
per-ex loss: 0.463437  [   56/   89]
per-ex loss: 0.358836  [   57/   89]
per-ex loss: 0.488742  [   58/   89]
per-ex loss: 0.389789  [   59/   89]
per-ex loss: 0.331880  [   60/   89]
per-ex loss: 0.335235  [   61/   89]
per-ex loss: 0.655631  [   62/   89]
per-ex loss: 0.620124  [   63/   89]
per-ex loss: 0.349593  [   64/   89]
per-ex loss: 0.424775  [   65/   89]
per-ex loss: 0.440858  [   66/   89]
per-ex loss: 0.476571  [   67/   89]
per-ex loss: 0.500482  [   68/   89]
per-ex loss: 0.522662  [   69/   89]
per-ex loss: 0.375701  [   70/   89]
per-ex loss: 0.336023  [   71/   89]
per-ex loss: 0.592533  [   72/   89]
per-ex loss: 0.310550  [   73/   89]
per-ex loss: 0.334999  [   74/   89]
per-ex loss: 0.302474  [   75/   89]
per-ex loss: 0.371676  [   76/   89]
per-ex loss: 0.379077  [   77/   89]
per-ex loss: 0.330590  [   78/   89]
per-ex loss: 0.297688  [   79/   89]
per-ex loss: 0.484813  [   80/   89]
per-ex loss: 0.188450  [   81/   89]
per-ex loss: 0.449297  [   82/   89]
per-ex loss: 0.287305  [   83/   89]
per-ex loss: 0.279385  [   84/   89]
per-ex loss: 0.484157  [   85/   89]
per-ex loss: 0.296943  [   86/   89]
per-ex loss: 0.572421  [   87/   89]
per-ex loss: 0.390157  [   88/   89]
per-ex loss: 0.369426  [   89/   89]
Train Error: Avg loss: 0.39222023
validation Error: 
 Avg loss: 0.54852613 
 F1: 0.473919 
 Precision: 0.546762 
 Recall: 0.418203
 IoU: 0.310546

test Error: 
 Avg loss: 0.49763187 
 F1: 0.551775 
 Precision: 0.640821 
 Recall: 0.484457
 IoU: 0.381001

We have finished training iteration 145
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_143_.pth
per-ex loss: 0.300108  [    1/   89]
per-ex loss: 0.331448  [    2/   89]
per-ex loss: 0.312796  [    3/   89]
per-ex loss: 0.296804  [    4/   89]
per-ex loss: 0.999937  [    5/   89]
per-ex loss: 0.382777  [    6/   89]
per-ex loss: 0.398845  [    7/   89]
per-ex loss: 0.455031  [    8/   89]
per-ex loss: 0.428350  [    9/   89]
per-ex loss: 0.289405  [   10/   89]
per-ex loss: 0.408660  [   11/   89]
per-ex loss: 0.341224  [   12/   89]
per-ex loss: 0.385073  [   13/   89]
per-ex loss: 0.562140  [   14/   89]
per-ex loss: 0.277958  [   15/   89]
per-ex loss: 0.272412  [   16/   89]
per-ex loss: 0.289675  [   17/   89]
per-ex loss: 0.437754  [   18/   89]
per-ex loss: 0.300353  [   19/   89]
per-ex loss: 0.282258  [   20/   89]
per-ex loss: 0.299882  [   21/   89]
per-ex loss: 0.403961  [   22/   89]
per-ex loss: 0.269463  [   23/   89]
per-ex loss: 0.389512  [   24/   89]
per-ex loss: 0.326361  [   25/   89]
per-ex loss: 0.401091  [   26/   89]
per-ex loss: 0.573879  [   27/   89]
per-ex loss: 0.699048  [   28/   89]
per-ex loss: 0.342504  [   29/   89]
per-ex loss: 0.449669  [   30/   89]
per-ex loss: 0.201778  [   31/   89]
per-ex loss: 0.518595  [   32/   89]
per-ex loss: 0.314702  [   33/   89]
per-ex loss: 0.286606  [   34/   89]
per-ex loss: 0.349313  [   35/   89]
per-ex loss: 0.306791  [   36/   89]
per-ex loss: 0.505551  [   37/   89]
per-ex loss: 0.399596  [   38/   89]
per-ex loss: 0.321750  [   39/   89]
per-ex loss: 0.235435  [   40/   89]
per-ex loss: 0.499312  [   41/   89]
per-ex loss: 0.282621  [   42/   89]
per-ex loss: 0.371768  [   43/   89]
per-ex loss: 0.341024  [   44/   89]
per-ex loss: 0.253370  [   45/   89]
per-ex loss: 0.332706  [   46/   89]
per-ex loss: 0.565011  [   47/   89]
per-ex loss: 0.361831  [   48/   89]
per-ex loss: 0.335755  [   49/   89]
per-ex loss: 0.303013  [   50/   89]
per-ex loss: 0.348516  [   51/   89]
per-ex loss: 0.308597  [   52/   89]
per-ex loss: 0.268294  [   53/   89]
per-ex loss: 0.418596  [   54/   89]
per-ex loss: 0.295900  [   55/   89]
per-ex loss: 0.264017  [   56/   89]
per-ex loss: 0.515089  [   57/   89]
per-ex loss: 0.420359  [   58/   89]
per-ex loss: 0.411664  [   59/   89]
per-ex loss: 0.453627  [   60/   89]
per-ex loss: 0.530034  [   61/   89]
per-ex loss: 0.531688  [   62/   89]
per-ex loss: 0.378863  [   63/   89]
per-ex loss: 0.466279  [   64/   89]
per-ex loss: 0.445330  [   65/   89]
per-ex loss: 0.337781  [   66/   89]
per-ex loss: 0.339856  [   67/   89]
per-ex loss: 0.417541  [   68/   89]
per-ex loss: 0.308290  [   69/   89]
per-ex loss: 0.455000  [   70/   89]
per-ex loss: 0.633997  [   71/   89]
per-ex loss: 0.274226  [   72/   89]
per-ex loss: 0.317188  [   73/   89]
per-ex loss: 0.315005  [   74/   89]
per-ex loss: 0.460140  [   75/   89]
per-ex loss: 0.405858  [   76/   89]
per-ex loss: 0.565571  [   77/   89]
per-ex loss: 0.345370  [   78/   89]
per-ex loss: 0.371176  [   79/   89]
per-ex loss: 0.325959  [   80/   89]
per-ex loss: 0.522106  [   81/   89]
per-ex loss: 0.396291  [   82/   89]
per-ex loss: 0.166304  [   83/   89]
per-ex loss: 0.351137  [   84/   89]
per-ex loss: 0.380879  [   85/   89]
per-ex loss: 0.453761  [   86/   89]
per-ex loss: 0.507489  [   87/   89]
per-ex loss: 0.531361  [   88/   89]
per-ex loss: 0.470554  [   89/   89]
Train Error: Avg loss: 0.38993934
validation Error: 
 Avg loss: 0.54689761 
 F1: 0.475875 
 Precision: 0.441535 
 Recall: 0.516006
 IoU: 0.312228

test Error: 
 Avg loss: 0.48680244 
 F1: 0.554748 
 Precision: 0.512788 
 Recall: 0.604187
 IoU: 0.383842

We have finished training iteration 146
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_144_.pth
per-ex loss: 0.323449  [    1/   89]
per-ex loss: 0.153543  [    2/   89]
per-ex loss: 0.311634  [    3/   89]
per-ex loss: 0.330760  [    4/   89]
per-ex loss: 0.511330  [    5/   89]
per-ex loss: 0.620212  [    6/   89]
per-ex loss: 0.378503  [    7/   89]
per-ex loss: 0.345777  [    8/   89]
per-ex loss: 0.393232  [    9/   89]
per-ex loss: 0.436518  [   10/   89]
per-ex loss: 0.440026  [   11/   89]
per-ex loss: 0.290866  [   12/   89]
per-ex loss: 0.542208  [   13/   89]
per-ex loss: 0.766908  [   14/   89]
per-ex loss: 0.377945  [   15/   89]
per-ex loss: 0.251265  [   16/   89]
per-ex loss: 0.376434  [   17/   89]
per-ex loss: 0.325407  [   18/   89]
per-ex loss: 0.327799  [   19/   89]
per-ex loss: 0.365029  [   20/   89]
per-ex loss: 0.347935  [   21/   89]
per-ex loss: 0.508818  [   22/   89]
per-ex loss: 0.280909  [   23/   89]
per-ex loss: 0.289441  [   24/   89]
per-ex loss: 0.509050  [   25/   89]
per-ex loss: 0.283546  [   26/   89]
per-ex loss: 0.226196  [   27/   89]
per-ex loss: 0.537881  [   28/   89]
per-ex loss: 0.295682  [   29/   89]
per-ex loss: 0.416682  [   30/   89]
per-ex loss: 0.327879  [   31/   89]
per-ex loss: 0.353012  [   32/   89]
per-ex loss: 0.370020  [   33/   89]
per-ex loss: 0.479756  [   34/   89]
per-ex loss: 0.619112  [   35/   89]
per-ex loss: 0.305764  [   36/   89]
per-ex loss: 0.425389  [   37/   89]
per-ex loss: 0.330139  [   38/   89]
per-ex loss: 0.279169  [   39/   89]
per-ex loss: 0.334149  [   40/   89]
per-ex loss: 0.263001  [   41/   89]
per-ex loss: 0.262162  [   42/   89]
per-ex loss: 0.431431  [   43/   89]
per-ex loss: 0.571414  [   44/   89]
per-ex loss: 0.307128  [   45/   89]
per-ex loss: 0.277472  [   46/   89]
per-ex loss: 0.394379  [   47/   89]
per-ex loss: 0.444164  [   48/   89]
per-ex loss: 0.364236  [   49/   89]
per-ex loss: 0.583696  [   50/   89]
per-ex loss: 0.265084  [   51/   89]
per-ex loss: 0.508290  [   52/   89]
per-ex loss: 0.340528  [   53/   89]
per-ex loss: 0.431528  [   54/   89]
per-ex loss: 0.195004  [   55/   89]
per-ex loss: 0.289382  [   56/   89]
per-ex loss: 0.283754  [   57/   89]
per-ex loss: 0.417976  [   58/   89]
per-ex loss: 0.339115  [   59/   89]
per-ex loss: 0.349968  [   60/   89]
per-ex loss: 0.378964  [   61/   89]
per-ex loss: 0.525302  [   62/   89]
per-ex loss: 0.531073  [   63/   89]
per-ex loss: 0.445017  [   64/   89]
per-ex loss: 0.488015  [   65/   89]
per-ex loss: 0.259810  [   66/   89]
per-ex loss: 0.361920  [   67/   89]
per-ex loss: 0.435599  [   68/   89]
per-ex loss: 0.319799  [   69/   89]
per-ex loss: 0.333247  [   70/   89]
per-ex loss: 0.999879  [   71/   89]
per-ex loss: 0.319208  [   72/   89]
per-ex loss: 0.328156  [   73/   89]
per-ex loss: 0.456524  [   74/   89]
per-ex loss: 0.296805  [   75/   89]
per-ex loss: 0.563141  [   76/   89]
per-ex loss: 0.383254  [   77/   89]
per-ex loss: 0.312928  [   78/   89]
per-ex loss: 0.509090  [   79/   89]
per-ex loss: 0.235307  [   80/   89]
per-ex loss: 0.257528  [   81/   89]
per-ex loss: 0.209166  [   82/   89]
per-ex loss: 0.343390  [   83/   89]
per-ex loss: 0.479765  [   84/   89]
per-ex loss: 0.344578  [   85/   89]
per-ex loss: 0.491916  [   86/   89]
per-ex loss: 0.273211  [   87/   89]
per-ex loss: 0.542897  [   88/   89]
per-ex loss: 0.328106  [   89/   89]
Train Error: Avg loss: 0.38722113
validation Error: 
 Avg loss: 0.53774159 
 F1: 0.490235 
 Precision: 0.563004 
 Recall: 0.434124
 IoU: 0.324709

test Error: 
 Avg loss: 0.50148881 
 F1: 0.545692 
 Precision: 0.603801 
 Recall: 0.497786
 IoU: 0.375225

We have finished training iteration 147
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_145_.pth
per-ex loss: 0.302819  [    1/   89]
per-ex loss: 0.516691  [    2/   89]
per-ex loss: 0.288800  [    3/   89]
per-ex loss: 0.392301  [    4/   89]
per-ex loss: 0.398327  [    5/   89]
per-ex loss: 0.465527  [    6/   89]
per-ex loss: 0.266337  [    7/   89]
per-ex loss: 0.334572  [    8/   89]
per-ex loss: 0.537000  [    9/   89]
per-ex loss: 0.380867  [   10/   89]
per-ex loss: 0.262510  [   11/   89]
per-ex loss: 0.318244  [   12/   89]
per-ex loss: 0.303758  [   13/   89]
per-ex loss: 0.408934  [   14/   89]
per-ex loss: 0.269776  [   15/   89]
per-ex loss: 0.343477  [   16/   89]
per-ex loss: 0.352522  [   17/   89]
per-ex loss: 0.405533  [   18/   89]
per-ex loss: 0.382371  [   19/   89]
per-ex loss: 0.256262  [   20/   89]
per-ex loss: 0.521775  [   21/   89]
per-ex loss: 0.268257  [   22/   89]
per-ex loss: 0.562905  [   23/   89]
per-ex loss: 0.393949  [   24/   89]
per-ex loss: 0.329030  [   25/   89]
per-ex loss: 0.217016  [   26/   89]
per-ex loss: 0.352905  [   27/   89]
per-ex loss: 0.205991  [   28/   89]
per-ex loss: 0.504859  [   29/   89]
per-ex loss: 0.353437  [   30/   89]
per-ex loss: 0.398806  [   31/   89]
per-ex loss: 0.515933  [   32/   89]
per-ex loss: 0.364477  [   33/   89]
per-ex loss: 0.534778  [   34/   89]
per-ex loss: 0.225952  [   35/   89]
per-ex loss: 0.353152  [   36/   89]
per-ex loss: 0.308711  [   37/   89]
per-ex loss: 0.309054  [   38/   89]
per-ex loss: 0.297670  [   39/   89]
per-ex loss: 0.342390  [   40/   89]
per-ex loss: 0.574329  [   41/   89]
per-ex loss: 0.449363  [   42/   89]
per-ex loss: 0.328661  [   43/   89]
per-ex loss: 0.293445  [   44/   89]
per-ex loss: 0.478388  [   45/   89]
per-ex loss: 0.438422  [   46/   89]
per-ex loss: 0.281221  [   47/   89]
per-ex loss: 0.278824  [   48/   89]
per-ex loss: 0.310745  [   49/   89]
per-ex loss: 0.432358  [   50/   89]
per-ex loss: 0.304586  [   51/   89]
per-ex loss: 0.432129  [   52/   89]
per-ex loss: 0.370522  [   53/   89]
per-ex loss: 0.586536  [   54/   89]
per-ex loss: 0.229513  [   55/   89]
per-ex loss: 0.999915  [   56/   89]
per-ex loss: 0.315177  [   57/   89]
per-ex loss: 0.282951  [   58/   89]
per-ex loss: 0.312488  [   59/   89]
per-ex loss: 0.315402  [   60/   89]
per-ex loss: 0.337175  [   61/   89]
per-ex loss: 0.325141  [   62/   89]
per-ex loss: 0.323109  [   63/   89]
per-ex loss: 0.489596  [   64/   89]
per-ex loss: 0.512184  [   65/   89]
per-ex loss: 0.289981  [   66/   89]
per-ex loss: 0.263259  [   67/   89]
per-ex loss: 0.408748  [   68/   89]
per-ex loss: 0.309033  [   69/   89]
per-ex loss: 0.276525  [   70/   89]
per-ex loss: 0.556025  [   71/   89]
per-ex loss: 0.405666  [   72/   89]
per-ex loss: 0.365526  [   73/   89]
per-ex loss: 0.372996  [   74/   89]
per-ex loss: 0.593688  [   75/   89]
per-ex loss: 0.472071  [   76/   89]
per-ex loss: 0.371901  [   77/   89]
per-ex loss: 0.661608  [   78/   89]
per-ex loss: 0.503358  [   79/   89]
per-ex loss: 0.316934  [   80/   89]
per-ex loss: 0.446189  [   81/   89]
per-ex loss: 0.288227  [   82/   89]
per-ex loss: 0.471023  [   83/   89]
per-ex loss: 0.371143  [   84/   89]
per-ex loss: 0.260487  [   85/   89]
per-ex loss: 0.315528  [   86/   89]
per-ex loss: 0.533409  [   87/   89]
per-ex loss: 0.316684  [   88/   89]
per-ex loss: 0.341144  [   89/   89]
Train Error: Avg loss: 0.38270793
validation Error: 
 Avg loss: 0.53323622 
 F1: 0.492175 
 Precision: 0.529341 
 Recall: 0.459886
 IoU: 0.326414

test Error: 
 Avg loss: 0.49305072 
 F1: 0.553974 
 Precision: 0.577085 
 Recall: 0.532642
 IoU: 0.383101

We have finished training iteration 148
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_146_.pth
per-ex loss: 0.253336  [    1/   89]
per-ex loss: 0.364730  [    2/   89]
per-ex loss: 0.358336  [    3/   89]
per-ex loss: 0.563971  [    4/   89]
per-ex loss: 0.464467  [    5/   89]
per-ex loss: 0.471211  [    6/   89]
per-ex loss: 0.474651  [    7/   89]
per-ex loss: 0.352448  [    8/   89]
per-ex loss: 0.315609  [    9/   89]
per-ex loss: 0.350470  [   10/   89]
per-ex loss: 0.503798  [   11/   89]
per-ex loss: 0.312428  [   12/   89]
per-ex loss: 0.372624  [   13/   89]
per-ex loss: 0.455153  [   14/   89]
per-ex loss: 0.380128  [   15/   89]
per-ex loss: 0.519040  [   16/   89]
per-ex loss: 0.488513  [   17/   89]
per-ex loss: 0.308149  [   18/   89]
per-ex loss: 0.442638  [   19/   89]
per-ex loss: 0.471031  [   20/   89]
per-ex loss: 0.381746  [   21/   89]
per-ex loss: 0.346971  [   22/   89]
per-ex loss: 0.321614  [   23/   89]
per-ex loss: 0.345700  [   24/   89]
per-ex loss: 0.288633  [   25/   89]
per-ex loss: 0.252869  [   26/   89]
per-ex loss: 0.606011  [   27/   89]
per-ex loss: 0.526406  [   28/   89]
per-ex loss: 0.550067  [   29/   89]
per-ex loss: 0.317442  [   30/   89]
per-ex loss: 0.208668  [   31/   89]
per-ex loss: 0.470631  [   32/   89]
per-ex loss: 0.201892  [   33/   89]
per-ex loss: 0.321590  [   34/   89]
per-ex loss: 0.184468  [   35/   89]
per-ex loss: 0.362696  [   36/   89]
per-ex loss: 0.341444  [   37/   89]
per-ex loss: 0.427080  [   38/   89]
per-ex loss: 0.295222  [   39/   89]
per-ex loss: 0.517834  [   40/   89]
per-ex loss: 0.475497  [   41/   89]
per-ex loss: 0.314841  [   42/   89]
per-ex loss: 0.476590  [   43/   89]
per-ex loss: 0.374269  [   44/   89]
per-ex loss: 0.381122  [   45/   89]
per-ex loss: 0.335041  [   46/   89]
per-ex loss: 0.288602  [   47/   89]
per-ex loss: 0.271014  [   48/   89]
per-ex loss: 0.493116  [   49/   89]
per-ex loss: 0.348892  [   50/   89]
per-ex loss: 0.275627  [   51/   89]
per-ex loss: 0.296394  [   52/   89]
per-ex loss: 0.409438  [   53/   89]
per-ex loss: 0.312943  [   54/   89]
per-ex loss: 0.358631  [   55/   89]
per-ex loss: 0.411019  [   56/   89]
per-ex loss: 0.299227  [   57/   89]
per-ex loss: 0.336738  [   58/   89]
per-ex loss: 0.449493  [   59/   89]
per-ex loss: 0.352592  [   60/   89]
per-ex loss: 0.999946  [   61/   89]
per-ex loss: 0.585674  [   62/   89]
per-ex loss: 0.356780  [   63/   89]
per-ex loss: 0.585350  [   64/   89]
per-ex loss: 0.268554  [   65/   89]
per-ex loss: 0.555629  [   66/   89]
per-ex loss: 0.529949  [   67/   89]
per-ex loss: 0.411836  [   68/   89]
per-ex loss: 0.349052  [   69/   89]
per-ex loss: 0.413947  [   70/   89]
per-ex loss: 0.383392  [   71/   89]
per-ex loss: 0.408871  [   72/   89]
per-ex loss: 0.306358  [   73/   89]
per-ex loss: 0.343287  [   74/   89]
per-ex loss: 0.299577  [   75/   89]
per-ex loss: 0.556319  [   76/   89]
per-ex loss: 0.338826  [   77/   89]
per-ex loss: 0.445219  [   78/   89]
per-ex loss: 0.453303  [   79/   89]
per-ex loss: 0.280419  [   80/   89]
per-ex loss: 0.347263  [   81/   89]
per-ex loss: 0.437974  [   82/   89]
per-ex loss: 0.273741  [   83/   89]
per-ex loss: 0.575626  [   84/   89]
per-ex loss: 0.263676  [   85/   89]
per-ex loss: 0.272168  [   86/   89]
per-ex loss: 0.302529  [   87/   89]
per-ex loss: 0.586866  [   88/   89]
per-ex loss: 0.352016  [   89/   89]
Train Error: Avg loss: 0.39367357
validation Error: 
 Avg loss: 0.53139775 
 F1: 0.490834 
 Precision: 0.507846 
 Recall: 0.474926
 IoU: 0.325236

test Error: 
 Avg loss: 0.48235028 
 F1: 0.558822 
 Precision: 0.565363 
 Recall: 0.552430
 IoU: 0.387753

We have finished training iteration 149
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_147_.pth
per-ex loss: 0.301118  [    1/   89]
per-ex loss: 0.316788  [    2/   89]
per-ex loss: 0.297731  [    3/   89]
per-ex loss: 0.344994  [    4/   89]
per-ex loss: 0.999944  [    5/   89]
per-ex loss: 0.307779  [    6/   89]
per-ex loss: 0.478132  [    7/   89]
per-ex loss: 0.411885  [    8/   89]
per-ex loss: 0.355940  [    9/   89]
per-ex loss: 0.491925  [   10/   89]
per-ex loss: 0.432721  [   11/   89]
per-ex loss: 0.494713  [   12/   89]
per-ex loss: 0.459425  [   13/   89]
per-ex loss: 0.374406  [   14/   89]
per-ex loss: 0.320850  [   15/   89]
per-ex loss: 0.351920  [   16/   89]
per-ex loss: 0.401963  [   17/   89]
per-ex loss: 0.332264  [   18/   89]
per-ex loss: 0.302157  [   19/   89]
per-ex loss: 0.319528  [   20/   89]
per-ex loss: 0.227673  [   21/   89]
per-ex loss: 0.354245  [   22/   89]
per-ex loss: 0.353580  [   23/   89]
per-ex loss: 0.284489  [   24/   89]
per-ex loss: 0.378964  [   25/   89]
per-ex loss: 0.282156  [   26/   89]
per-ex loss: 0.343415  [   27/   89]
per-ex loss: 0.567228  [   28/   89]
per-ex loss: 0.330240  [   29/   89]
per-ex loss: 0.591652  [   30/   89]
per-ex loss: 0.287317  [   31/   89]
per-ex loss: 0.554698  [   32/   89]
per-ex loss: 0.328140  [   33/   89]
per-ex loss: 0.211173  [   34/   89]
per-ex loss: 0.317948  [   35/   89]
per-ex loss: 0.527044  [   36/   89]
per-ex loss: 0.447266  [   37/   89]
per-ex loss: 0.197921  [   38/   89]
per-ex loss: 0.528544  [   39/   89]
per-ex loss: 0.325259  [   40/   89]
per-ex loss: 0.385675  [   41/   89]
per-ex loss: 0.533747  [   42/   89]
per-ex loss: 0.189964  [   43/   89]
per-ex loss: 0.462010  [   44/   89]
per-ex loss: 0.308594  [   45/   89]
per-ex loss: 0.349386  [   46/   89]
per-ex loss: 0.555030  [   47/   89]
per-ex loss: 0.327227  [   48/   89]
per-ex loss: 0.360576  [   49/   89]
per-ex loss: 0.474975  [   50/   89]
per-ex loss: 0.440295  [   51/   89]
per-ex loss: 0.446109  [   52/   89]
per-ex loss: 0.547500  [   53/   89]
per-ex loss: 0.234562  [   54/   89]
per-ex loss: 0.378033  [   55/   89]
per-ex loss: 0.353910  [   56/   89]
per-ex loss: 0.487475  [   57/   89]
per-ex loss: 0.282421  [   58/   89]
per-ex loss: 0.364529  [   59/   89]
per-ex loss: 0.302846  [   60/   89]
per-ex loss: 0.304568  [   61/   89]
per-ex loss: 0.497844  [   62/   89]
per-ex loss: 0.317720  [   63/   89]
per-ex loss: 0.337090  [   64/   89]
per-ex loss: 0.561722  [   65/   89]
per-ex loss: 0.299154  [   66/   89]
per-ex loss: 0.288810  [   67/   89]
per-ex loss: 0.301605  [   68/   89]
per-ex loss: 0.614625  [   69/   89]
per-ex loss: 0.348441  [   70/   89]
per-ex loss: 0.593889  [   71/   89]
per-ex loss: 0.289313  [   72/   89]
per-ex loss: 0.364883  [   73/   89]
per-ex loss: 0.423545  [   74/   89]
per-ex loss: 0.320388  [   75/   89]
per-ex loss: 0.340708  [   76/   89]
per-ex loss: 0.437021  [   77/   89]
per-ex loss: 0.430161  [   78/   89]
per-ex loss: 0.276910  [   79/   89]
per-ex loss: 0.285530  [   80/   89]
per-ex loss: 0.345463  [   81/   89]
per-ex loss: 0.342020  [   82/   89]
per-ex loss: 0.390746  [   83/   89]
per-ex loss: 0.313863  [   84/   89]
per-ex loss: 0.394311  [   85/   89]
per-ex loss: 0.378150  [   86/   89]
per-ex loss: 0.435068  [   87/   89]
per-ex loss: 0.321199  [   88/   89]
per-ex loss: 0.368633  [   89/   89]
Train Error: Avg loss: 0.38475701
validation Error: 
 Avg loss: 0.55225162 
 F1: 0.466494 
 Precision: 0.433526 
 Recall: 0.504889
 IoU: 0.304201

test Error: 
 Avg loss: 0.49482914 
 F1: 0.544187 
 Precision: 0.491691 
 Recall: 0.609232
 IoU: 0.373803

We have finished training iteration 150
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_148_.pth
per-ex loss: 0.273008  [    1/   89]
per-ex loss: 0.327581  [    2/   89]
per-ex loss: 0.404578  [    3/   89]
per-ex loss: 0.393321  [    4/   89]
per-ex loss: 0.571126  [    5/   89]
per-ex loss: 0.519605  [    6/   89]
per-ex loss: 0.311463  [    7/   89]
per-ex loss: 0.527588  [    8/   89]
per-ex loss: 0.174528  [    9/   89]
per-ex loss: 0.309012  [   10/   89]
per-ex loss: 0.348196  [   11/   89]
per-ex loss: 0.249771  [   12/   89]
per-ex loss: 0.243836  [   13/   89]
per-ex loss: 0.463161  [   14/   89]
per-ex loss: 0.361646  [   15/   89]
per-ex loss: 0.368388  [   16/   89]
per-ex loss: 0.336498  [   17/   89]
per-ex loss: 0.303772  [   18/   89]
per-ex loss: 0.309467  [   19/   89]
per-ex loss: 0.454542  [   20/   89]
per-ex loss: 0.228371  [   21/   89]
per-ex loss: 0.341583  [   22/   89]
per-ex loss: 0.312981  [   23/   89]
per-ex loss: 0.250214  [   24/   89]
per-ex loss: 0.402124  [   25/   89]
per-ex loss: 0.241858  [   26/   89]
per-ex loss: 0.429106  [   27/   89]
per-ex loss: 0.287978  [   28/   89]
per-ex loss: 0.289723  [   29/   89]
per-ex loss: 0.577754  [   30/   89]
per-ex loss: 0.350962  [   31/   89]
per-ex loss: 0.256474  [   32/   89]
per-ex loss: 0.328920  [   33/   89]
per-ex loss: 0.436488  [   34/   89]
per-ex loss: 0.305796  [   35/   89]
per-ex loss: 0.343529  [   36/   89]
per-ex loss: 0.300675  [   37/   89]
per-ex loss: 0.575219  [   38/   89]
per-ex loss: 0.458015  [   39/   89]
per-ex loss: 0.480456  [   40/   89]
per-ex loss: 0.422240  [   41/   89]
per-ex loss: 0.581571  [   42/   89]
per-ex loss: 0.420559  [   43/   89]
per-ex loss: 0.431197  [   44/   89]
per-ex loss: 0.296244  [   45/   89]
per-ex loss: 0.466851  [   46/   89]
per-ex loss: 0.414201  [   47/   89]
per-ex loss: 0.347839  [   48/   89]
per-ex loss: 0.321522  [   49/   89]
per-ex loss: 0.315530  [   50/   89]
per-ex loss: 0.999883  [   51/   89]
per-ex loss: 0.409051  [   52/   89]
per-ex loss: 0.331515  [   53/   89]
per-ex loss: 0.443862  [   54/   89]
per-ex loss: 0.332002  [   55/   89]
per-ex loss: 0.335992  [   56/   89]
per-ex loss: 0.505800  [   57/   89]
per-ex loss: 0.303900  [   58/   89]
per-ex loss: 0.496770  [   59/   89]
per-ex loss: 0.346886  [   60/   89]
per-ex loss: 0.509976  [   61/   89]
per-ex loss: 0.324305  [   62/   89]
per-ex loss: 0.291067  [   63/   89]
per-ex loss: 0.340573  [   64/   89]
per-ex loss: 0.385833  [   65/   89]
per-ex loss: 0.298541  [   66/   89]
per-ex loss: 0.533316  [   67/   89]
per-ex loss: 0.323438  [   68/   89]
per-ex loss: 0.330085  [   69/   89]
per-ex loss: 0.290057  [   70/   89]
per-ex loss: 0.276984  [   71/   89]
per-ex loss: 0.602455  [   72/   89]
per-ex loss: 0.296914  [   73/   89]
per-ex loss: 0.331197  [   74/   89]
per-ex loss: 0.193755  [   75/   89]
per-ex loss: 0.442573  [   76/   89]
per-ex loss: 0.491938  [   77/   89]
per-ex loss: 0.362049  [   78/   89]
per-ex loss: 0.331739  [   79/   89]
per-ex loss: 0.429384  [   80/   89]
per-ex loss: 0.417680  [   81/   89]
per-ex loss: 0.255599  [   82/   89]
per-ex loss: 0.562530  [   83/   89]
per-ex loss: 0.556180  [   84/   89]
per-ex loss: 0.588449  [   85/   89]
per-ex loss: 0.472944  [   86/   89]
per-ex loss: 0.362235  [   87/   89]
per-ex loss: 0.350801  [   88/   89]
per-ex loss: 0.327899  [   89/   89]
Train Error: Avg loss: 0.38486768
validation Error: 
 Avg loss: 0.55523000 
 F1: 0.460561 
 Precision: 0.420230 
 Recall: 0.509453
 IoU: 0.299174

test Error: 
 Avg loss: 0.49372504 
 F1: 0.543753 
 Precision: 0.498299 
 Recall: 0.598333
 IoU: 0.373394

We have finished training iteration 151
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_149_.pth
per-ex loss: 0.327738  [    1/   89]
per-ex loss: 0.393012  [    2/   89]
per-ex loss: 0.284732  [    3/   89]
per-ex loss: 0.438387  [    4/   89]
per-ex loss: 0.318350  [    5/   89]
per-ex loss: 0.284412  [    6/   89]
per-ex loss: 0.251613  [    7/   89]
per-ex loss: 0.427542  [    8/   89]
per-ex loss: 0.454918  [    9/   89]
per-ex loss: 0.338044  [   10/   89]
per-ex loss: 0.537094  [   11/   89]
per-ex loss: 0.340634  [   12/   89]
per-ex loss: 0.666870  [   13/   89]
per-ex loss: 0.441744  [   14/   89]
per-ex loss: 0.351198  [   15/   89]
per-ex loss: 0.316096  [   16/   89]
per-ex loss: 0.329242  [   17/   89]
per-ex loss: 0.332601  [   18/   89]
per-ex loss: 0.335998  [   19/   89]
per-ex loss: 0.445615  [   20/   89]
per-ex loss: 0.293456  [   21/   89]
per-ex loss: 0.341890  [   22/   89]
per-ex loss: 0.313932  [   23/   89]
per-ex loss: 0.196684  [   24/   89]
per-ex loss: 0.239240  [   25/   89]
per-ex loss: 0.655966  [   26/   89]
per-ex loss: 0.306589  [   27/   89]
per-ex loss: 0.173144  [   28/   89]
per-ex loss: 0.259250  [   29/   89]
per-ex loss: 0.465251  [   30/   89]
per-ex loss: 0.520814  [   31/   89]
per-ex loss: 0.416656  [   32/   89]
per-ex loss: 0.493159  [   33/   89]
per-ex loss: 0.363366  [   34/   89]
per-ex loss: 0.435288  [   35/   89]
per-ex loss: 0.317041  [   36/   89]
per-ex loss: 0.395961  [   37/   89]
per-ex loss: 0.414985  [   38/   89]
per-ex loss: 0.299170  [   39/   89]
per-ex loss: 0.413161  [   40/   89]
per-ex loss: 0.290017  [   41/   89]
per-ex loss: 0.426226  [   42/   89]
per-ex loss: 0.468882  [   43/   89]
per-ex loss: 0.275192  [   44/   89]
per-ex loss: 0.548747  [   45/   89]
per-ex loss: 0.463903  [   46/   89]
per-ex loss: 0.512212  [   47/   89]
per-ex loss: 0.321027  [   48/   89]
per-ex loss: 0.343708  [   49/   89]
per-ex loss: 0.609505  [   50/   89]
per-ex loss: 0.369213  [   51/   89]
per-ex loss: 0.306060  [   52/   89]
per-ex loss: 0.261095  [   53/   89]
per-ex loss: 0.350809  [   54/   89]
per-ex loss: 0.570717  [   55/   89]
per-ex loss: 0.448425  [   56/   89]
per-ex loss: 0.305139  [   57/   89]
per-ex loss: 0.272039  [   58/   89]
per-ex loss: 0.463176  [   59/   89]
per-ex loss: 0.239246  [   60/   89]
per-ex loss: 0.316730  [   61/   89]
per-ex loss: 0.286582  [   62/   89]
per-ex loss: 0.267274  [   63/   89]
per-ex loss: 0.296129  [   64/   89]
per-ex loss: 0.999951  [   65/   89]
per-ex loss: 0.328915  [   66/   89]
per-ex loss: 0.331813  [   67/   89]
per-ex loss: 0.470481  [   68/   89]
per-ex loss: 0.350628  [   69/   89]
per-ex loss: 0.359137  [   70/   89]
per-ex loss: 0.496619  [   71/   89]
per-ex loss: 0.545033  [   72/   89]
per-ex loss: 0.494583  [   73/   89]
per-ex loss: 0.512913  [   74/   89]
per-ex loss: 0.369799  [   75/   89]
per-ex loss: 0.231174  [   76/   89]
per-ex loss: 0.357485  [   77/   89]
per-ex loss: 0.261754  [   78/   89]
per-ex loss: 0.321012  [   79/   89]
per-ex loss: 0.259460  [   80/   89]
per-ex loss: 0.564520  [   81/   89]
per-ex loss: 0.347154  [   82/   89]
per-ex loss: 0.314711  [   83/   89]
per-ex loss: 0.537507  [   84/   89]
per-ex loss: 0.283751  [   85/   89]
per-ex loss: 0.349373  [   86/   89]
per-ex loss: 0.276124  [   87/   89]
per-ex loss: 0.334566  [   88/   89]
per-ex loss: 0.356270  [   89/   89]
Train Error: Avg loss: 0.38199590
validation Error: 
 Avg loss: 0.53001835 
 F1: 0.493348 
 Precision: 0.545600 
 Recall: 0.450229
 IoU: 0.327446

test Error: 
 Avg loss: 0.49100905 
 F1: 0.554077 
 Precision: 0.588231 
 Recall: 0.523671
 IoU: 0.383199

We have finished training iteration 152
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_150_.pth
per-ex loss: 0.351578  [    1/   89]
per-ex loss: 0.261894  [    2/   89]
per-ex loss: 0.555649  [    3/   89]
per-ex loss: 0.356391  [    4/   89]
per-ex loss: 0.328709  [    5/   89]
per-ex loss: 0.582357  [    6/   89]
per-ex loss: 0.351789  [    7/   89]
per-ex loss: 0.278368  [    8/   89]
per-ex loss: 0.378902  [    9/   89]
per-ex loss: 0.548199  [   10/   89]
per-ex loss: 0.327456  [   11/   89]
per-ex loss: 0.344288  [   12/   89]
per-ex loss: 0.334241  [   13/   89]
per-ex loss: 0.304943  [   14/   89]
per-ex loss: 0.177253  [   15/   89]
per-ex loss: 0.501696  [   16/   89]
per-ex loss: 0.326733  [   17/   89]
per-ex loss: 0.329068  [   18/   89]
per-ex loss: 0.315721  [   19/   89]
per-ex loss: 0.510316  [   20/   89]
per-ex loss: 0.387119  [   21/   89]
per-ex loss: 0.372014  [   22/   89]
per-ex loss: 0.262166  [   23/   89]
per-ex loss: 0.415378  [   24/   89]
per-ex loss: 0.306809  [   25/   89]
per-ex loss: 0.274215  [   26/   89]
per-ex loss: 0.295360  [   27/   89]
per-ex loss: 0.430308  [   28/   89]
per-ex loss: 0.264686  [   29/   89]
per-ex loss: 0.302249  [   30/   89]
per-ex loss: 0.436345  [   31/   89]
per-ex loss: 0.344756  [   32/   89]
per-ex loss: 0.526433  [   33/   89]
per-ex loss: 0.286496  [   34/   89]
per-ex loss: 0.324019  [   35/   89]
per-ex loss: 0.300545  [   36/   89]
per-ex loss: 0.416565  [   37/   89]
per-ex loss: 0.309457  [   38/   89]
per-ex loss: 0.309204  [   39/   89]
per-ex loss: 0.377160  [   40/   89]
per-ex loss: 0.306545  [   41/   89]
per-ex loss: 0.419646  [   42/   89]
per-ex loss: 0.448420  [   43/   89]
per-ex loss: 0.438302  [   44/   89]
per-ex loss: 0.291400  [   45/   89]
per-ex loss: 0.530225  [   46/   89]
per-ex loss: 0.521687  [   47/   89]
per-ex loss: 0.496209  [   48/   89]
per-ex loss: 0.626626  [   49/   89]
per-ex loss: 0.264317  [   50/   89]
per-ex loss: 0.300529  [   51/   89]
per-ex loss: 0.545064  [   52/   89]
per-ex loss: 0.369735  [   53/   89]
per-ex loss: 0.336283  [   54/   89]
per-ex loss: 0.418996  [   55/   89]
per-ex loss: 0.434931  [   56/   89]
per-ex loss: 0.498164  [   57/   89]
per-ex loss: 0.435948  [   58/   89]
per-ex loss: 0.472661  [   59/   89]
per-ex loss: 0.498535  [   60/   89]
per-ex loss: 0.333744  [   61/   89]
per-ex loss: 0.439350  [   62/   89]
per-ex loss: 0.357027  [   63/   89]
per-ex loss: 0.374336  [   64/   89]
per-ex loss: 0.334506  [   65/   89]
per-ex loss: 0.287728  [   66/   89]
per-ex loss: 0.515053  [   67/   89]
per-ex loss: 0.220582  [   68/   89]
per-ex loss: 0.322051  [   69/   89]
per-ex loss: 0.585739  [   70/   89]
per-ex loss: 0.414369  [   71/   89]
per-ex loss: 0.334533  [   72/   89]
per-ex loss: 0.552965  [   73/   89]
per-ex loss: 0.999931  [   74/   89]
per-ex loss: 0.317608  [   75/   89]
per-ex loss: 0.447815  [   76/   89]
per-ex loss: 0.249469  [   77/   89]
per-ex loss: 0.219920  [   78/   89]
per-ex loss: 0.583480  [   79/   89]
per-ex loss: 0.448771  [   80/   89]
per-ex loss: 0.404369  [   81/   89]
per-ex loss: 0.331481  [   82/   89]
per-ex loss: 0.277112  [   83/   89]
per-ex loss: 0.258499  [   84/   89]
per-ex loss: 0.591571  [   85/   89]
per-ex loss: 0.352466  [   86/   89]
per-ex loss: 0.302982  [   87/   89]
per-ex loss: 0.391756  [   88/   89]
per-ex loss: 0.290501  [   89/   89]
Train Error: Avg loss: 0.38877267
validation Error: 
 Avg loss: 0.54777817 
 F1: 0.481138 
 Precision: 0.570019 
 Recall: 0.416236
 IoU: 0.316775

test Error: 
 Avg loss: 0.50948601 
 F1: 0.535447 
 Precision: 0.602697 
 Recall: 0.481698
 IoU: 0.365604

We have finished training iteration 153
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_151_.pth
per-ex loss: 0.428071  [    1/   89]
per-ex loss: 0.297094  [    2/   89]
per-ex loss: 0.240410  [    3/   89]
per-ex loss: 0.207456  [    4/   89]
per-ex loss: 0.291378  [    5/   89]
per-ex loss: 0.533352  [    6/   89]
per-ex loss: 0.443127  [    7/   89]
per-ex loss: 0.561097  [    8/   89]
per-ex loss: 0.342569  [    9/   89]
per-ex loss: 0.502112  [   10/   89]
per-ex loss: 0.197754  [   11/   89]
per-ex loss: 0.463247  [   12/   89]
per-ex loss: 0.301041  [   13/   89]
per-ex loss: 0.562941  [   14/   89]
per-ex loss: 0.374514  [   15/   89]
per-ex loss: 0.400439  [   16/   89]
per-ex loss: 0.326026  [   17/   89]
per-ex loss: 0.290329  [   18/   89]
per-ex loss: 0.321930  [   19/   89]
per-ex loss: 0.349646  [   20/   89]
per-ex loss: 0.592280  [   21/   89]
per-ex loss: 0.220738  [   22/   89]
per-ex loss: 0.254011  [   23/   89]
per-ex loss: 0.202080  [   24/   89]
per-ex loss: 0.283815  [   25/   89]
per-ex loss: 0.403292  [   26/   89]
per-ex loss: 0.366773  [   27/   89]
per-ex loss: 0.434708  [   28/   89]
per-ex loss: 0.294080  [   29/   89]
per-ex loss: 0.337137  [   30/   89]
per-ex loss: 0.479863  [   31/   89]
per-ex loss: 0.248404  [   32/   89]
per-ex loss: 0.404574  [   33/   89]
per-ex loss: 0.348113  [   34/   89]
per-ex loss: 0.260891  [   35/   89]
per-ex loss: 0.301355  [   36/   89]
per-ex loss: 0.360678  [   37/   89]
per-ex loss: 0.258930  [   38/   89]
per-ex loss: 0.556976  [   39/   89]
per-ex loss: 0.395843  [   40/   89]
per-ex loss: 0.376379  [   41/   89]
per-ex loss: 0.519046  [   42/   89]
per-ex loss: 0.544812  [   43/   89]
per-ex loss: 0.421381  [   44/   89]
per-ex loss: 0.227065  [   45/   89]
per-ex loss: 0.492290  [   46/   89]
per-ex loss: 0.473473  [   47/   89]
per-ex loss: 0.312759  [   48/   89]
per-ex loss: 0.243578  [   49/   89]
per-ex loss: 0.286219  [   50/   89]
per-ex loss: 0.315766  [   51/   89]
per-ex loss: 0.522078  [   52/   89]
per-ex loss: 0.344430  [   53/   89]
per-ex loss: 0.406592  [   54/   89]
per-ex loss: 0.303045  [   55/   89]
per-ex loss: 0.345537  [   56/   89]
per-ex loss: 0.521199  [   57/   89]
per-ex loss: 0.373913  [   58/   89]
per-ex loss: 0.298928  [   59/   89]
per-ex loss: 0.455357  [   60/   89]
per-ex loss: 0.363445  [   61/   89]
per-ex loss: 0.314613  [   62/   89]
per-ex loss: 0.644003  [   63/   89]
per-ex loss: 0.507590  [   64/   89]
per-ex loss: 0.460647  [   65/   89]
per-ex loss: 0.492027  [   66/   89]
per-ex loss: 0.434927  [   67/   89]
per-ex loss: 0.405345  [   68/   89]
per-ex loss: 0.303489  [   69/   89]
per-ex loss: 0.467283  [   70/   89]
per-ex loss: 0.347034  [   71/   89]
per-ex loss: 0.350531  [   72/   89]
per-ex loss: 0.606505  [   73/   89]
per-ex loss: 0.348910  [   74/   89]
per-ex loss: 0.367506  [   75/   89]
per-ex loss: 0.248079  [   76/   89]
per-ex loss: 0.312749  [   77/   89]
per-ex loss: 0.999560  [   78/   89]
per-ex loss: 0.293389  [   79/   89]
per-ex loss: 0.283918  [   80/   89]
per-ex loss: 0.277808  [   81/   89]
per-ex loss: 0.550953  [   82/   89]
per-ex loss: 0.472439  [   83/   89]
per-ex loss: 0.412808  [   84/   89]
per-ex loss: 0.334994  [   85/   89]
per-ex loss: 0.468808  [   86/   89]
per-ex loss: 0.272834  [   87/   89]
per-ex loss: 0.305644  [   88/   89]
per-ex loss: 0.327913  [   89/   89]
Train Error: Avg loss: 0.38425480
validation Error: 
 Avg loss: 0.55031939 
 F1: 0.477992 
 Precision: 0.544084 
 Recall: 0.426218
 IoU: 0.314054

test Error: 
 Avg loss: 0.51606561 
 F1: 0.529882 
 Precision: 0.549777 
 Recall: 0.511376
 IoU: 0.360435

We have finished training iteration 154
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_152_.pth
per-ex loss: 0.372272  [    1/   89]
per-ex loss: 0.311187  [    2/   89]
per-ex loss: 0.343969  [    3/   89]
per-ex loss: 0.251095  [    4/   89]
per-ex loss: 0.303462  [    5/   89]
per-ex loss: 0.298590  [    6/   89]
per-ex loss: 0.513708  [    7/   89]
per-ex loss: 0.334786  [    8/   89]
per-ex loss: 0.591843  [    9/   89]
per-ex loss: 0.424683  [   10/   89]
per-ex loss: 0.276345  [   11/   89]
per-ex loss: 0.361078  [   12/   89]
per-ex loss: 0.422899  [   13/   89]
per-ex loss: 0.309969  [   14/   89]
per-ex loss: 0.316902  [   15/   89]
per-ex loss: 0.506482  [   16/   89]
per-ex loss: 0.552604  [   17/   89]
per-ex loss: 0.350054  [   18/   89]
per-ex loss: 0.518457  [   19/   89]
per-ex loss: 0.275944  [   20/   89]
per-ex loss: 0.508511  [   21/   89]
per-ex loss: 0.374773  [   22/   89]
per-ex loss: 0.310009  [   23/   89]
per-ex loss: 0.379262  [   24/   89]
per-ex loss: 0.522682  [   25/   89]
per-ex loss: 0.248432  [   26/   89]
per-ex loss: 0.268162  [   27/   89]
per-ex loss: 0.471320  [   28/   89]
per-ex loss: 0.493537  [   29/   89]
per-ex loss: 0.194700  [   30/   89]
per-ex loss: 0.399963  [   31/   89]
per-ex loss: 0.288060  [   32/   89]
per-ex loss: 0.460975  [   33/   89]
per-ex loss: 0.453633  [   34/   89]
per-ex loss: 0.276380  [   35/   89]
per-ex loss: 0.342009  [   36/   89]
per-ex loss: 0.338006  [   37/   89]
per-ex loss: 0.340275  [   38/   89]
per-ex loss: 0.496369  [   39/   89]
per-ex loss: 0.181860  [   40/   89]
per-ex loss: 0.199899  [   41/   89]
per-ex loss: 0.441966  [   42/   89]
per-ex loss: 0.462803  [   43/   89]
per-ex loss: 0.146338  [   44/   89]
per-ex loss: 0.329814  [   45/   89]
per-ex loss: 0.491782  [   46/   89]
per-ex loss: 0.418935  [   47/   89]
per-ex loss: 0.336366  [   48/   89]
per-ex loss: 0.579097  [   49/   89]
per-ex loss: 0.386856  [   50/   89]
per-ex loss: 0.400078  [   51/   89]
per-ex loss: 0.397179  [   52/   89]
per-ex loss: 0.447718  [   53/   89]
per-ex loss: 0.338838  [   54/   89]
per-ex loss: 0.399414  [   55/   89]
per-ex loss: 0.471907  [   56/   89]
per-ex loss: 0.598214  [   57/   89]
per-ex loss: 0.339089  [   58/   89]
per-ex loss: 0.321503  [   59/   89]
per-ex loss: 0.298186  [   60/   89]
per-ex loss: 0.335507  [   61/   89]
per-ex loss: 0.310592  [   62/   89]
per-ex loss: 0.358887  [   63/   89]
per-ex loss: 0.338255  [   64/   89]
per-ex loss: 0.322403  [   65/   89]
per-ex loss: 0.410770  [   66/   89]
per-ex loss: 0.439412  [   67/   89]
per-ex loss: 0.323938  [   68/   89]
per-ex loss: 0.403607  [   69/   89]
per-ex loss: 0.307397  [   70/   89]
per-ex loss: 0.257009  [   71/   89]
per-ex loss: 0.326453  [   72/   89]
per-ex loss: 0.256175  [   73/   89]
per-ex loss: 0.365774  [   74/   89]
per-ex loss: 0.285102  [   75/   89]
per-ex loss: 0.999946  [   76/   89]
per-ex loss: 0.428977  [   77/   89]
per-ex loss: 0.455148  [   78/   89]
per-ex loss: 0.560207  [   79/   89]
per-ex loss: 0.440569  [   80/   89]
per-ex loss: 0.472952  [   81/   89]
per-ex loss: 0.337912  [   82/   89]
per-ex loss: 0.329263  [   83/   89]
per-ex loss: 0.288289  [   84/   89]
per-ex loss: 0.588349  [   85/   89]
per-ex loss: 0.607838  [   86/   89]
per-ex loss: 0.494444  [   87/   89]
per-ex loss: 0.351622  [   88/   89]
per-ex loss: 0.332744  [   89/   89]
Train Error: Avg loss: 0.38787381
validation Error: 
 Avg loss: 0.54276887 
 F1: 0.483712 
 Precision: 0.513045 
 Recall: 0.457552
 IoU: 0.319011

test Error: 
 Avg loss: 0.51113371 
 F1: 0.537558 
 Precision: 0.521526 
 Recall: 0.554607
 IoU: 0.367576

We have finished training iteration 155
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_153_.pth
per-ex loss: 0.462479  [    1/   89]
per-ex loss: 0.442522  [    2/   89]
per-ex loss: 0.256491  [    3/   89]
per-ex loss: 0.282937  [    4/   89]
per-ex loss: 0.362131  [    5/   89]
per-ex loss: 0.351460  [    6/   89]
per-ex loss: 0.334147  [    7/   89]
per-ex loss: 0.372468  [    8/   89]
per-ex loss: 0.579306  [    9/   89]
per-ex loss: 0.284201  [   10/   89]
per-ex loss: 0.345278  [   11/   89]
per-ex loss: 0.407653  [   12/   89]
per-ex loss: 0.322695  [   13/   89]
per-ex loss: 0.555745  [   14/   89]
per-ex loss: 0.249051  [   15/   89]
per-ex loss: 0.484736  [   16/   89]
per-ex loss: 0.296959  [   17/   89]
per-ex loss: 0.573883  [   18/   89]
per-ex loss: 0.333066  [   19/   89]
per-ex loss: 0.390908  [   20/   89]
per-ex loss: 0.575998  [   21/   89]
per-ex loss: 0.208162  [   22/   89]
per-ex loss: 0.283245  [   23/   89]
per-ex loss: 0.158059  [   24/   89]
per-ex loss: 0.390918  [   25/   89]
per-ex loss: 0.413054  [   26/   89]
per-ex loss: 0.482150  [   27/   89]
per-ex loss: 0.302591  [   28/   89]
per-ex loss: 0.491414  [   29/   89]
per-ex loss: 0.298446  [   30/   89]
per-ex loss: 0.278789  [   31/   89]
per-ex loss: 0.348304  [   32/   89]
per-ex loss: 0.388732  [   33/   89]
per-ex loss: 0.209109  [   34/   89]
per-ex loss: 0.300371  [   35/   89]
per-ex loss: 0.483638  [   36/   89]
per-ex loss: 0.307208  [   37/   89]
per-ex loss: 0.388231  [   38/   89]
per-ex loss: 0.453053  [   39/   89]
per-ex loss: 0.317279  [   40/   89]
per-ex loss: 0.382952  [   41/   89]
per-ex loss: 0.395545  [   42/   89]
per-ex loss: 0.567050  [   43/   89]
per-ex loss: 0.254467  [   44/   89]
per-ex loss: 0.536553  [   45/   89]
per-ex loss: 0.341537  [   46/   89]
per-ex loss: 0.282927  [   47/   89]
per-ex loss: 0.495555  [   48/   89]
per-ex loss: 0.302368  [   49/   89]
per-ex loss: 0.331056  [   50/   89]
per-ex loss: 0.407999  [   51/   89]
per-ex loss: 0.322990  [   52/   89]
per-ex loss: 0.361428  [   53/   89]
per-ex loss: 0.175560  [   54/   89]
per-ex loss: 0.411020  [   55/   89]
per-ex loss: 0.323048  [   56/   89]
per-ex loss: 0.571295  [   57/   89]
per-ex loss: 0.275676  [   58/   89]
per-ex loss: 0.288509  [   59/   89]
per-ex loss: 0.999922  [   60/   89]
per-ex loss: 0.506066  [   61/   89]
per-ex loss: 0.455068  [   62/   89]
per-ex loss: 0.241703  [   63/   89]
per-ex loss: 0.327015  [   64/   89]
per-ex loss: 0.285456  [   65/   89]
per-ex loss: 0.304493  [   66/   89]
per-ex loss: 0.290087  [   67/   89]
per-ex loss: 0.256074  [   68/   89]
per-ex loss: 0.288760  [   69/   89]
per-ex loss: 0.287606  [   70/   89]
per-ex loss: 0.446063  [   71/   89]
per-ex loss: 0.472724  [   72/   89]
per-ex loss: 0.378800  [   73/   89]
per-ex loss: 0.304829  [   74/   89]
per-ex loss: 0.335965  [   75/   89]
per-ex loss: 0.502072  [   76/   89]
per-ex loss: 0.488640  [   77/   89]
per-ex loss: 0.282503  [   78/   89]
per-ex loss: 0.480236  [   79/   89]
per-ex loss: 0.307062  [   80/   89]
per-ex loss: 0.294465  [   81/   89]
per-ex loss: 0.364320  [   82/   89]
per-ex loss: 0.505716  [   83/   89]
per-ex loss: 0.319660  [   84/   89]
per-ex loss: 0.287993  [   85/   89]
per-ex loss: 0.465579  [   86/   89]
per-ex loss: 0.268623  [   87/   89]
per-ex loss: 0.327679  [   88/   89]
per-ex loss: 0.299725  [   89/   89]
Train Error: Avg loss: 0.37268878
validation Error: 
 Avg loss: 0.52650533 
 F1: 0.500868 
 Precision: 0.570576 
 Recall: 0.446339
 IoU: 0.334106

test Error: 
 Avg loss: 0.48285180 
 F1: 0.565171 
 Precision: 0.616292 
 Recall: 0.521881
 IoU: 0.393894

We have finished training iteration 156
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_154_.pth
per-ex loss: 0.308572  [    1/   89]
per-ex loss: 0.340569  [    2/   89]
per-ex loss: 0.310950  [    3/   89]
per-ex loss: 0.385984  [    4/   89]
per-ex loss: 0.616070  [    5/   89]
per-ex loss: 0.339805  [    6/   89]
per-ex loss: 0.306306  [    7/   89]
per-ex loss: 0.319352  [    8/   89]
per-ex loss: 0.379502  [    9/   89]
per-ex loss: 0.249337  [   10/   89]
per-ex loss: 0.565944  [   11/   89]
per-ex loss: 0.416257  [   12/   89]
per-ex loss: 0.409058  [   13/   89]
per-ex loss: 0.492661  [   14/   89]
per-ex loss: 0.254930  [   15/   89]
per-ex loss: 0.408044  [   16/   89]
per-ex loss: 0.436130  [   17/   89]
per-ex loss: 0.498764  [   18/   89]
per-ex loss: 0.453000  [   19/   89]
per-ex loss: 0.414447  [   20/   89]
per-ex loss: 0.297529  [   21/   89]
per-ex loss: 0.337486  [   22/   89]
per-ex loss: 0.326239  [   23/   89]
per-ex loss: 0.540317  [   24/   89]
per-ex loss: 0.571732  [   25/   89]
per-ex loss: 0.394026  [   26/   89]
per-ex loss: 0.309537  [   27/   89]
per-ex loss: 0.999949  [   28/   89]
per-ex loss: 0.448234  [   29/   89]
per-ex loss: 0.352506  [   30/   89]
per-ex loss: 0.599925  [   31/   89]
per-ex loss: 0.359127  [   32/   89]
per-ex loss: 0.518007  [   33/   89]
per-ex loss: 0.260512  [   34/   89]
per-ex loss: 0.496701  [   35/   89]
per-ex loss: 0.294497  [   36/   89]
per-ex loss: 0.350464  [   37/   89]
per-ex loss: 0.640738  [   38/   89]
per-ex loss: 0.339019  [   39/   89]
per-ex loss: 0.373917  [   40/   89]
per-ex loss: 0.365691  [   41/   89]
per-ex loss: 0.327769  [   42/   89]
per-ex loss: 0.316800  [   43/   89]
per-ex loss: 0.417592  [   44/   89]
per-ex loss: 0.461394  [   45/   89]
per-ex loss: 0.279756  [   46/   89]
per-ex loss: 0.255301  [   47/   89]
per-ex loss: 0.220353  [   48/   89]
per-ex loss: 0.151921  [   49/   89]
per-ex loss: 0.250402  [   50/   89]
per-ex loss: 0.460249  [   51/   89]
per-ex loss: 0.250566  [   52/   89]
per-ex loss: 0.285004  [   53/   89]
per-ex loss: 0.409926  [   54/   89]
per-ex loss: 0.282655  [   55/   89]
per-ex loss: 0.420002  [   56/   89]
per-ex loss: 0.294995  [   57/   89]
per-ex loss: 0.548469  [   58/   89]
per-ex loss: 0.443632  [   59/   89]
per-ex loss: 0.345034  [   60/   89]
per-ex loss: 0.480132  [   61/   89]
per-ex loss: 0.295770  [   62/   89]
per-ex loss: 0.312599  [   63/   89]
per-ex loss: 0.246314  [   64/   89]
per-ex loss: 0.435939  [   65/   89]
per-ex loss: 0.394163  [   66/   89]
per-ex loss: 0.320649  [   67/   89]
per-ex loss: 0.288785  [   68/   89]
per-ex loss: 0.311657  [   69/   89]
per-ex loss: 0.308572  [   70/   89]
per-ex loss: 0.371480  [   71/   89]
per-ex loss: 0.372829  [   72/   89]
per-ex loss: 0.497634  [   73/   89]
per-ex loss: 0.336427  [   74/   89]
per-ex loss: 0.291790  [   75/   89]
per-ex loss: 0.524618  [   76/   89]
per-ex loss: 0.492437  [   77/   89]
per-ex loss: 0.332775  [   78/   89]
per-ex loss: 0.406930  [   79/   89]
per-ex loss: 0.359967  [   80/   89]
per-ex loss: 0.438712  [   81/   89]
per-ex loss: 0.292665  [   82/   89]
per-ex loss: 0.259447  [   83/   89]
per-ex loss: 0.575997  [   84/   89]
per-ex loss: 0.293722  [   85/   89]
per-ex loss: 0.458745  [   86/   89]
per-ex loss: 0.343499  [   87/   89]
per-ex loss: 0.352039  [   88/   89]
per-ex loss: 0.336547  [   89/   89]
Train Error: Avg loss: 0.38467969
validation Error: 
 Avg loss: 0.52961925 
 F1: 0.496342 
 Precision: 0.567563 
 Recall: 0.441002
 IoU: 0.330089

test Error: 
 Avg loss: 0.49104358 
 F1: 0.554975 
 Precision: 0.613313 
 Recall: 0.506771
 IoU: 0.384059

We have finished training iteration 157
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_155_.pth
per-ex loss: 0.376514  [    1/   89]
per-ex loss: 0.194909  [    2/   89]
per-ex loss: 0.238110  [    3/   89]
per-ex loss: 0.283946  [    4/   89]
per-ex loss: 0.556319  [    5/   89]
per-ex loss: 0.392696  [    6/   89]
per-ex loss: 0.438011  [    7/   89]
per-ex loss: 0.520590  [    8/   89]
per-ex loss: 0.294456  [    9/   89]
per-ex loss: 0.311317  [   10/   89]
per-ex loss: 0.292374  [   11/   89]
per-ex loss: 0.408012  [   12/   89]
per-ex loss: 0.503519  [   13/   89]
per-ex loss: 0.559746  [   14/   89]
per-ex loss: 0.307997  [   15/   89]
per-ex loss: 0.399468  [   16/   89]
per-ex loss: 0.437805  [   17/   89]
per-ex loss: 0.413042  [   18/   89]
per-ex loss: 0.504349  [   19/   89]
per-ex loss: 0.444747  [   20/   89]
per-ex loss: 0.363404  [   21/   89]
per-ex loss: 0.252301  [   22/   89]
per-ex loss: 0.564939  [   23/   89]
per-ex loss: 0.174035  [   24/   89]
per-ex loss: 0.358836  [   25/   89]
per-ex loss: 0.335040  [   26/   89]
per-ex loss: 0.600056  [   27/   89]
per-ex loss: 0.252330  [   28/   89]
per-ex loss: 0.270642  [   29/   89]
per-ex loss: 0.326294  [   30/   89]
per-ex loss: 0.999920  [   31/   89]
per-ex loss: 0.251189  [   32/   89]
per-ex loss: 0.252260  [   33/   89]
per-ex loss: 0.590320  [   34/   89]
per-ex loss: 0.282866  [   35/   89]
per-ex loss: 0.306759  [   36/   89]
per-ex loss: 0.485097  [   37/   89]
per-ex loss: 0.585465  [   38/   89]
per-ex loss: 0.291478  [   39/   89]
per-ex loss: 0.320302  [   40/   89]
per-ex loss: 0.249295  [   41/   89]
per-ex loss: 0.520750  [   42/   89]
per-ex loss: 0.453013  [   43/   89]
per-ex loss: 0.332055  [   44/   89]
per-ex loss: 0.340187  [   45/   89]
per-ex loss: 0.289261  [   46/   89]
per-ex loss: 0.533522  [   47/   89]
per-ex loss: 0.407782  [   48/   89]
per-ex loss: 0.297736  [   49/   89]
per-ex loss: 0.422003  [   50/   89]
per-ex loss: 0.407687  [   51/   89]
per-ex loss: 0.355174  [   52/   89]
per-ex loss: 0.399829  [   53/   89]
per-ex loss: 0.517509  [   54/   89]
per-ex loss: 0.459743  [   55/   89]
per-ex loss: 0.468956  [   56/   89]
per-ex loss: 0.410754  [   57/   89]
per-ex loss: 0.433603  [   58/   89]
per-ex loss: 0.276888  [   59/   89]
per-ex loss: 0.338340  [   60/   89]
per-ex loss: 0.250105  [   61/   89]
per-ex loss: 0.285463  [   62/   89]
per-ex loss: 0.258397  [   63/   89]
per-ex loss: 0.263192  [   64/   89]
per-ex loss: 0.392502  [   65/   89]
per-ex loss: 0.389170  [   66/   89]
per-ex loss: 0.331911  [   67/   89]
per-ex loss: 0.491296  [   68/   89]
per-ex loss: 0.301953  [   69/   89]
per-ex loss: 0.280652  [   70/   89]
per-ex loss: 0.432535  [   71/   89]
per-ex loss: 0.307202  [   72/   89]
per-ex loss: 0.480357  [   73/   89]
per-ex loss: 0.408329  [   74/   89]
per-ex loss: 0.300637  [   75/   89]
per-ex loss: 0.299665  [   76/   89]
per-ex loss: 0.473024  [   77/   89]
per-ex loss: 0.362096  [   78/   89]
per-ex loss: 0.350565  [   79/   89]
per-ex loss: 0.205544  [   80/   89]
per-ex loss: 0.328533  [   81/   89]
per-ex loss: 0.416873  [   82/   89]
per-ex loss: 0.308170  [   83/   89]
per-ex loss: 0.472772  [   84/   89]
per-ex loss: 0.347478  [   85/   89]
per-ex loss: 0.412555  [   86/   89]
per-ex loss: 0.309621  [   87/   89]
per-ex loss: 0.358548  [   88/   89]
per-ex loss: 0.514627  [   89/   89]
Train Error: Avg loss: 0.38190249
validation Error: 
 Avg loss: 0.54854850 
 F1: 0.476689 
 Precision: 0.469816 
 Recall: 0.483767
 IoU: 0.312930

test Error: 
 Avg loss: 0.49135966 
 F1: 0.552232 
 Precision: 0.545826 
 Recall: 0.558789
 IoU: 0.381436

We have finished training iteration 158
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_127_.pth
per-ex loss: 0.389126  [    1/   89]
per-ex loss: 0.582893  [    2/   89]
per-ex loss: 0.316914  [    3/   89]
per-ex loss: 0.411264  [    4/   89]
per-ex loss: 0.284702  [    5/   89]
per-ex loss: 0.164046  [    6/   89]
per-ex loss: 0.224922  [    7/   89]
per-ex loss: 0.540228  [    8/   89]
per-ex loss: 0.301011  [    9/   89]
per-ex loss: 0.384002  [   10/   89]
per-ex loss: 0.282616  [   11/   89]
per-ex loss: 0.295321  [   12/   89]
per-ex loss: 0.412818  [   13/   89]
per-ex loss: 0.337428  [   14/   89]
per-ex loss: 0.464991  [   15/   89]
per-ex loss: 0.319148  [   16/   89]
per-ex loss: 0.420021  [   17/   89]
per-ex loss: 0.557999  [   18/   89]
per-ex loss: 0.339114  [   19/   89]
per-ex loss: 0.258082  [   20/   89]
per-ex loss: 0.237004  [   21/   89]
per-ex loss: 0.428004  [   22/   89]
per-ex loss: 0.401877  [   23/   89]
per-ex loss: 0.536626  [   24/   89]
per-ex loss: 0.215137  [   25/   89]
per-ex loss: 0.433756  [   26/   89]
per-ex loss: 0.276522  [   27/   89]
per-ex loss: 0.302642  [   28/   89]
per-ex loss: 0.346430  [   29/   89]
per-ex loss: 0.392862  [   30/   89]
per-ex loss: 0.340935  [   31/   89]
per-ex loss: 0.637726  [   32/   89]
per-ex loss: 0.324172  [   33/   89]
per-ex loss: 0.443063  [   34/   89]
per-ex loss: 0.389445  [   35/   89]
per-ex loss: 0.486681  [   36/   89]
per-ex loss: 0.392534  [   37/   89]
per-ex loss: 0.366587  [   38/   89]
per-ex loss: 0.540243  [   39/   89]
per-ex loss: 0.295914  [   40/   89]
per-ex loss: 0.349116  [   41/   89]
per-ex loss: 0.366988  [   42/   89]
per-ex loss: 0.286160  [   43/   89]
per-ex loss: 0.487875  [   44/   89]
per-ex loss: 0.389476  [   45/   89]
per-ex loss: 0.305820  [   46/   89]
per-ex loss: 0.326131  [   47/   89]
per-ex loss: 0.543242  [   48/   89]
per-ex loss: 0.289921  [   49/   89]
per-ex loss: 0.511955  [   50/   89]
per-ex loss: 0.468612  [   51/   89]
per-ex loss: 0.283362  [   52/   89]
per-ex loss: 0.434203  [   53/   89]
per-ex loss: 0.553136  [   54/   89]
per-ex loss: 0.999949  [   55/   89]
per-ex loss: 0.273884  [   56/   89]
per-ex loss: 0.300164  [   57/   89]
per-ex loss: 0.499806  [   58/   89]
per-ex loss: 0.306983  [   59/   89]
per-ex loss: 0.424457  [   60/   89]
per-ex loss: 0.321472  [   61/   89]
per-ex loss: 0.587939  [   62/   89]
per-ex loss: 0.344427  [   63/   89]
per-ex loss: 0.350433  [   64/   89]
per-ex loss: 0.522328  [   65/   89]
per-ex loss: 0.409394  [   66/   89]
per-ex loss: 0.221062  [   67/   89]
per-ex loss: 0.317248  [   68/   89]
per-ex loss: 0.497575  [   69/   89]
per-ex loss: 0.335646  [   70/   89]
per-ex loss: 0.306426  [   71/   89]
per-ex loss: 0.185922  [   72/   89]
per-ex loss: 0.348358  [   73/   89]
per-ex loss: 0.459548  [   74/   89]
per-ex loss: 0.318952  [   75/   89]
per-ex loss: 0.548874  [   76/   89]
per-ex loss: 0.237729  [   77/   89]
per-ex loss: 0.470036  [   78/   89]
per-ex loss: 0.470477  [   79/   89]
per-ex loss: 0.471601  [   80/   89]
per-ex loss: 0.263234  [   81/   89]
per-ex loss: 0.288333  [   82/   89]
per-ex loss: 0.482866  [   83/   89]
per-ex loss: 0.298957  [   84/   89]
per-ex loss: 0.342764  [   85/   89]
per-ex loss: 0.428602  [   86/   89]
per-ex loss: 0.298623  [   87/   89]
per-ex loss: 0.371412  [   88/   89]
per-ex loss: 0.319058  [   89/   89]
Train Error: Avg loss: 0.38534091
validation Error: 
 Avg loss: 0.53474768 
 F1: 0.493131 
 Precision: 0.536401 
 Recall: 0.456321
 IoU: 0.327255

test Error: 
 Avg loss: 0.50153273 
 F1: 0.542838 
 Precision: 0.570537 
 Recall: 0.517705
 IoU: 0.372531

We have finished training iteration 159
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_157_.pth
per-ex loss: 0.341041  [    1/   89]
per-ex loss: 0.281457  [    2/   89]
per-ex loss: 0.387470  [    3/   89]
per-ex loss: 0.369220  [    4/   89]
per-ex loss: 0.193766  [    5/   89]
per-ex loss: 0.411653  [    6/   89]
per-ex loss: 0.277334  [    7/   89]
per-ex loss: 0.421948  [    8/   89]
per-ex loss: 0.229770  [    9/   89]
per-ex loss: 0.553893  [   10/   89]
per-ex loss: 0.575117  [   11/   89]
per-ex loss: 0.649488  [   12/   89]
per-ex loss: 0.339287  [   13/   89]
per-ex loss: 0.333389  [   14/   89]
per-ex loss: 0.287744  [   15/   89]
per-ex loss: 0.284632  [   16/   89]
per-ex loss: 0.384048  [   17/   89]
per-ex loss: 0.361941  [   18/   89]
per-ex loss: 0.273237  [   19/   89]
per-ex loss: 0.293432  [   20/   89]
per-ex loss: 0.243787  [   21/   89]
per-ex loss: 0.323497  [   22/   89]
per-ex loss: 0.367547  [   23/   89]
per-ex loss: 0.293598  [   24/   89]
per-ex loss: 0.302243  [   25/   89]
per-ex loss: 0.244695  [   26/   89]
per-ex loss: 0.344993  [   27/   89]
per-ex loss: 0.409634  [   28/   89]
per-ex loss: 0.324703  [   29/   89]
per-ex loss: 0.371879  [   30/   89]
per-ex loss: 0.161473  [   31/   89]
per-ex loss: 0.575182  [   32/   89]
per-ex loss: 0.999898  [   33/   89]
per-ex loss: 0.288254  [   34/   89]
per-ex loss: 0.399753  [   35/   89]
per-ex loss: 0.702306  [   36/   89]
per-ex loss: 0.544556  [   37/   89]
per-ex loss: 0.568527  [   38/   89]
per-ex loss: 0.321576  [   39/   89]
per-ex loss: 0.306947  [   40/   89]
per-ex loss: 0.337772  [   41/   89]
per-ex loss: 0.387908  [   42/   89]
per-ex loss: 0.538649  [   43/   89]
per-ex loss: 0.214826  [   44/   89]
per-ex loss: 0.388568  [   45/   89]
per-ex loss: 0.324038  [   46/   89]
per-ex loss: 0.395352  [   47/   89]
per-ex loss: 0.456061  [   48/   89]
per-ex loss: 0.449890  [   49/   89]
per-ex loss: 0.473320  [   50/   89]
per-ex loss: 0.407455  [   51/   89]
per-ex loss: 0.306012  [   52/   89]
per-ex loss: 0.342530  [   53/   89]
per-ex loss: 0.299974  [   54/   89]
per-ex loss: 0.581202  [   55/   89]
per-ex loss: 0.453802  [   56/   89]
per-ex loss: 0.310827  [   57/   89]
per-ex loss: 0.262698  [   58/   89]
per-ex loss: 0.346946  [   59/   89]
per-ex loss: 0.545753  [   60/   89]
per-ex loss: 0.258909  [   61/   89]
per-ex loss: 0.539691  [   62/   89]
per-ex loss: 0.499807  [   63/   89]
per-ex loss: 0.520200  [   64/   89]
per-ex loss: 0.379208  [   65/   89]
per-ex loss: 0.347676  [   66/   89]
per-ex loss: 0.405912  [   67/   89]
per-ex loss: 0.292741  [   68/   89]
per-ex loss: 0.356312  [   69/   89]
per-ex loss: 0.526100  [   70/   89]
per-ex loss: 0.288647  [   71/   89]
per-ex loss: 0.431365  [   72/   89]
per-ex loss: 0.237459  [   73/   89]
per-ex loss: 0.288575  [   74/   89]
per-ex loss: 0.344235  [   75/   89]
per-ex loss: 0.315707  [   76/   89]
per-ex loss: 0.427033  [   77/   89]
per-ex loss: 0.301532  [   78/   89]
per-ex loss: 0.298292  [   79/   89]
per-ex loss: 0.348733  [   80/   89]
per-ex loss: 0.470887  [   81/   89]
per-ex loss: 0.256705  [   82/   89]
per-ex loss: 0.293505  [   83/   89]
per-ex loss: 0.314968  [   84/   89]
per-ex loss: 0.332339  [   85/   89]
per-ex loss: 0.489243  [   86/   89]
per-ex loss: 0.370228  [   87/   89]
per-ex loss: 0.312042  [   88/   89]
per-ex loss: 0.399394  [   89/   89]
Train Error: Avg loss: 0.37995446
validation Error: 
 Avg loss: 0.53852757 
 F1: 0.487324 
 Precision: 0.499833 
 Recall: 0.475426
 IoU: 0.322160

test Error: 
 Avg loss: 0.48894238 
 F1: 0.555444 
 Precision: 0.562104 
 Recall: 0.548940
 IoU: 0.384509

We have finished training iteration 160
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_158_.pth
per-ex loss: 0.513361  [    1/   89]
per-ex loss: 0.539518  [    2/   89]
per-ex loss: 0.252210  [    3/   89]
per-ex loss: 0.407258  [    4/   89]
per-ex loss: 0.202825  [    5/   89]
per-ex loss: 0.353586  [    6/   89]
per-ex loss: 0.636106  [    7/   89]
per-ex loss: 0.406290  [    8/   89]
per-ex loss: 0.445047  [    9/   89]
per-ex loss: 0.288325  [   10/   89]
per-ex loss: 0.999938  [   11/   89]
per-ex loss: 0.312723  [   12/   89]
per-ex loss: 0.458277  [   13/   89]
per-ex loss: 0.290994  [   14/   89]
per-ex loss: 0.399085  [   15/   89]
per-ex loss: 0.321010  [   16/   89]
per-ex loss: 0.323955  [   17/   89]
per-ex loss: 0.321998  [   18/   89]
per-ex loss: 0.179408  [   19/   89]
per-ex loss: 0.328952  [   20/   89]
per-ex loss: 0.327772  [   21/   89]
per-ex loss: 0.293727  [   22/   89]
per-ex loss: 0.380607  [   23/   89]
per-ex loss: 0.356165  [   24/   89]
per-ex loss: 0.287965  [   25/   89]
per-ex loss: 0.334496  [   26/   89]
per-ex loss: 0.271383  [   27/   89]
per-ex loss: 0.444780  [   28/   89]
per-ex loss: 0.301284  [   29/   89]
per-ex loss: 0.263472  [   30/   89]
per-ex loss: 0.266449  [   31/   89]
per-ex loss: 0.654687  [   32/   89]
per-ex loss: 0.307455  [   33/   89]
per-ex loss: 0.239861  [   34/   89]
per-ex loss: 0.452530  [   35/   89]
per-ex loss: 0.401593  [   36/   89]
per-ex loss: 0.419450  [   37/   89]
per-ex loss: 0.574499  [   38/   89]
per-ex loss: 0.330328  [   39/   89]
per-ex loss: 0.325552  [   40/   89]
per-ex loss: 0.314606  [   41/   89]
per-ex loss: 0.253448  [   42/   89]
per-ex loss: 0.310216  [   43/   89]
per-ex loss: 0.398440  [   44/   89]
per-ex loss: 0.190305  [   45/   89]
per-ex loss: 0.343883  [   46/   89]
per-ex loss: 0.226805  [   47/   89]
per-ex loss: 0.247293  [   48/   89]
per-ex loss: 0.242188  [   49/   89]
per-ex loss: 0.428561  [   50/   89]
per-ex loss: 0.244602  [   51/   89]
per-ex loss: 0.364219  [   52/   89]
per-ex loss: 0.283387  [   53/   89]
per-ex loss: 0.540352  [   54/   89]
per-ex loss: 0.244340  [   55/   89]
per-ex loss: 0.368366  [   56/   89]
per-ex loss: 0.340231  [   57/   89]
per-ex loss: 0.351815  [   58/   89]
per-ex loss: 0.459907  [   59/   89]
per-ex loss: 0.527732  [   60/   89]
per-ex loss: 0.521778  [   61/   89]
per-ex loss: 0.347965  [   62/   89]
per-ex loss: 0.422828  [   63/   89]
per-ex loss: 0.301468  [   64/   89]
per-ex loss: 0.570787  [   65/   89]
per-ex loss: 0.271985  [   66/   89]
per-ex loss: 0.286326  [   67/   89]
per-ex loss: 0.324104  [   68/   89]
per-ex loss: 0.347690  [   69/   89]
per-ex loss: 0.326575  [   70/   89]
per-ex loss: 0.551036  [   71/   89]
per-ex loss: 0.315552  [   72/   89]
per-ex loss: 0.590035  [   73/   89]
per-ex loss: 0.282166  [   74/   89]
per-ex loss: 0.360168  [   75/   89]
per-ex loss: 0.552285  [   76/   89]
per-ex loss: 0.399171  [   77/   89]
per-ex loss: 0.431612  [   78/   89]
per-ex loss: 0.377544  [   79/   89]
per-ex loss: 0.309462  [   80/   89]
per-ex loss: 0.531395  [   81/   89]
per-ex loss: 0.497275  [   82/   89]
per-ex loss: 0.478050  [   83/   89]
per-ex loss: 0.419060  [   84/   89]
per-ex loss: 0.263924  [   85/   89]
per-ex loss: 0.316071  [   86/   89]
per-ex loss: 0.321423  [   87/   89]
per-ex loss: 0.170948  [   88/   89]
per-ex loss: 0.492957  [   89/   89]
Train Error: Avg loss: 0.37390178
validation Error: 
 Avg loss: 0.54037063 
 F1: 0.484074 
 Precision: 0.462417 
 Recall: 0.507859
 IoU: 0.319325

test Error: 
 Avg loss: 0.49419032 
 F1: 0.546271 
 Precision: 0.512394 
 Recall: 0.584944
 IoU: 0.375772

We have finished training iteration 161
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_159_.pth
per-ex loss: 0.304246  [    1/   89]
per-ex loss: 0.245845  [    2/   89]
per-ex loss: 0.481997  [    3/   89]
per-ex loss: 0.235877  [    4/   89]
per-ex loss: 0.292074  [    5/   89]
per-ex loss: 0.422023  [    6/   89]
per-ex loss: 0.271716  [    7/   89]
per-ex loss: 0.240010  [    8/   89]
per-ex loss: 0.377786  [    9/   89]
per-ex loss: 0.253367  [   10/   89]
per-ex loss: 0.450839  [   11/   89]
per-ex loss: 0.315749  [   12/   89]
per-ex loss: 0.201074  [   13/   89]
per-ex loss: 0.359758  [   14/   89]
per-ex loss: 0.332086  [   15/   89]
per-ex loss: 0.217776  [   16/   89]
per-ex loss: 0.268698  [   17/   89]
per-ex loss: 0.306296  [   18/   89]
per-ex loss: 0.179723  [   19/   89]
per-ex loss: 0.421062  [   20/   89]
per-ex loss: 0.403563  [   21/   89]
per-ex loss: 0.398954  [   22/   89]
per-ex loss: 0.373600  [   23/   89]
per-ex loss: 0.511104  [   24/   89]
per-ex loss: 0.351579  [   25/   89]
per-ex loss: 0.361019  [   26/   89]
per-ex loss: 0.426829  [   27/   89]
per-ex loss: 0.369382  [   28/   89]
per-ex loss: 0.350853  [   29/   89]
per-ex loss: 0.287766  [   30/   89]
per-ex loss: 0.336171  [   31/   89]
per-ex loss: 0.289810  [   32/   89]
per-ex loss: 0.423748  [   33/   89]
per-ex loss: 0.286870  [   34/   89]
per-ex loss: 0.524005  [   35/   89]
per-ex loss: 0.365817  [   36/   89]
per-ex loss: 0.327744  [   37/   89]
per-ex loss: 0.326303  [   38/   89]
per-ex loss: 0.301476  [   39/   89]
per-ex loss: 0.321515  [   40/   89]
per-ex loss: 0.427483  [   41/   89]
per-ex loss: 0.381385  [   42/   89]
per-ex loss: 0.281322  [   43/   89]
per-ex loss: 0.315888  [   44/   89]
per-ex loss: 0.468740  [   45/   89]
per-ex loss: 0.324445  [   46/   89]
per-ex loss: 0.408605  [   47/   89]
per-ex loss: 0.427295  [   48/   89]
per-ex loss: 0.408035  [   49/   89]
per-ex loss: 0.498414  [   50/   89]
per-ex loss: 0.309200  [   51/   89]
per-ex loss: 0.185062  [   52/   89]
per-ex loss: 0.420730  [   53/   89]
per-ex loss: 0.335216  [   54/   89]
per-ex loss: 0.322788  [   55/   89]
per-ex loss: 0.356273  [   56/   89]
per-ex loss: 0.299103  [   57/   89]
per-ex loss: 0.598462  [   58/   89]
per-ex loss: 0.543374  [   59/   89]
per-ex loss: 0.557795  [   60/   89]
per-ex loss: 0.570912  [   61/   89]
per-ex loss: 0.456740  [   62/   89]
per-ex loss: 0.332162  [   63/   89]
per-ex loss: 0.352762  [   64/   89]
per-ex loss: 0.999914  [   65/   89]
per-ex loss: 0.355640  [   66/   89]
per-ex loss: 0.217205  [   67/   89]
per-ex loss: 0.322616  [   68/   89]
per-ex loss: 0.551670  [   69/   89]
per-ex loss: 0.376311  [   70/   89]
per-ex loss: 0.621543  [   71/   89]
per-ex loss: 0.298874  [   72/   89]
per-ex loss: 0.513375  [   73/   89]
per-ex loss: 0.302615  [   74/   89]
per-ex loss: 0.522284  [   75/   89]
per-ex loss: 0.356286  [   76/   89]
per-ex loss: 0.573401  [   77/   89]
per-ex loss: 0.283403  [   78/   89]
per-ex loss: 0.318945  [   79/   89]
per-ex loss: 0.430217  [   80/   89]
per-ex loss: 0.449821  [   81/   89]
per-ex loss: 0.353783  [   82/   89]
per-ex loss: 0.416036  [   83/   89]
per-ex loss: 0.517522  [   84/   89]
per-ex loss: 0.367338  [   85/   89]
per-ex loss: 0.549881  [   86/   89]
per-ex loss: 0.591199  [   87/   89]
per-ex loss: 0.434945  [   88/   89]
per-ex loss: 0.263293  [   89/   89]
Train Error: Avg loss: 0.38299343
validation Error: 
 Avg loss: 0.53698057 
 F1: 0.487178 
 Precision: 0.501584 
 Recall: 0.473577
 IoU: 0.322033

test Error: 
 Avg loss: 0.49009039 
 F1: 0.557260 
 Precision: 0.569345 
 Recall: 0.545678
 IoU: 0.386251

We have finished training iteration 162
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_160_.pth
per-ex loss: 0.264079  [    1/   89]
per-ex loss: 0.521529  [    2/   89]
per-ex loss: 0.475072  [    3/   89]
per-ex loss: 0.455355  [    4/   89]
per-ex loss: 0.393345  [    5/   89]
per-ex loss: 0.286181  [    6/   89]
per-ex loss: 0.497583  [    7/   89]
per-ex loss: 0.289080  [    8/   89]
per-ex loss: 0.266625  [    9/   89]
per-ex loss: 0.191751  [   10/   89]
per-ex loss: 0.482574  [   11/   89]
per-ex loss: 0.292241  [   12/   89]
per-ex loss: 0.999944  [   13/   89]
per-ex loss: 0.343366  [   14/   89]
per-ex loss: 0.526797  [   15/   89]
per-ex loss: 0.585002  [   16/   89]
per-ex loss: 0.299779  [   17/   89]
per-ex loss: 0.559089  [   18/   89]
per-ex loss: 0.288093  [   19/   89]
per-ex loss: 0.253669  [   20/   89]
per-ex loss: 0.430748  [   21/   89]
per-ex loss: 0.298389  [   22/   89]
per-ex loss: 0.436913  [   23/   89]
per-ex loss: 0.457389  [   24/   89]
per-ex loss: 0.347014  [   25/   89]
per-ex loss: 0.313638  [   26/   89]
per-ex loss: 0.259458  [   27/   89]
per-ex loss: 0.527873  [   28/   89]
per-ex loss: 0.143654  [   29/   89]
per-ex loss: 0.482865  [   30/   89]
per-ex loss: 0.321804  [   31/   89]
per-ex loss: 0.373091  [   32/   89]
per-ex loss: 0.337848  [   33/   89]
per-ex loss: 0.493271  [   34/   89]
per-ex loss: 0.345634  [   35/   89]
per-ex loss: 0.308256  [   36/   89]
per-ex loss: 0.286122  [   37/   89]
per-ex loss: 0.278542  [   38/   89]
per-ex loss: 0.463975  [   39/   89]
per-ex loss: 0.255201  [   40/   89]
per-ex loss: 0.427189  [   41/   89]
per-ex loss: 0.445481  [   42/   89]
per-ex loss: 0.369999  [   43/   89]
per-ex loss: 0.451899  [   44/   89]
per-ex loss: 0.281933  [   45/   89]
per-ex loss: 0.336179  [   46/   89]
per-ex loss: 0.275109  [   47/   89]
per-ex loss: 0.432229  [   48/   89]
per-ex loss: 0.443940  [   49/   89]
per-ex loss: 0.361322  [   50/   89]
per-ex loss: 0.326351  [   51/   89]
per-ex loss: 0.427072  [   52/   89]
per-ex loss: 0.231017  [   53/   89]
per-ex loss: 0.290342  [   54/   89]
per-ex loss: 0.419257  [   55/   89]
per-ex loss: 0.283208  [   56/   89]
per-ex loss: 0.252032  [   57/   89]
per-ex loss: 0.343063  [   58/   89]
per-ex loss: 0.577416  [   59/   89]
per-ex loss: 0.288641  [   60/   89]
per-ex loss: 0.434994  [   61/   89]
per-ex loss: 0.487714  [   62/   89]
per-ex loss: 0.305668  [   63/   89]
per-ex loss: 0.560247  [   64/   89]
per-ex loss: 0.269375  [   65/   89]
per-ex loss: 0.338594  [   66/   89]
per-ex loss: 0.201613  [   67/   89]
per-ex loss: 0.588866  [   68/   89]
per-ex loss: 0.254294  [   69/   89]
per-ex loss: 0.300829  [   70/   89]
per-ex loss: 0.381900  [   71/   89]
per-ex loss: 0.374245  [   72/   89]
per-ex loss: 0.297523  [   73/   89]
per-ex loss: 0.277049  [   74/   89]
per-ex loss: 0.371411  [   75/   89]
per-ex loss: 0.347983  [   76/   89]
per-ex loss: 0.271237  [   77/   89]
per-ex loss: 0.443866  [   78/   89]
per-ex loss: 0.314798  [   79/   89]
per-ex loss: 0.319175  [   80/   89]
per-ex loss: 0.285105  [   81/   89]
per-ex loss: 0.283996  [   82/   89]
per-ex loss: 0.221528  [   83/   89]
per-ex loss: 0.337799  [   84/   89]
per-ex loss: 0.393120  [   85/   89]
per-ex loss: 0.266465  [   86/   89]
per-ex loss: 0.344157  [   87/   89]
per-ex loss: 0.399514  [   88/   89]
per-ex loss: 0.298301  [   89/   89]
Train Error: Avg loss: 0.36705489
validation Error: 
 Avg loss: 0.53707373 
 F1: 0.485729 
 Precision: 0.502349 
 Recall: 0.470173
 IoU: 0.320767

test Error: 
 Avg loss: 0.49283222 
 F1: 0.551934 
 Precision: 0.560645 
 Recall: 0.543489
 IoU: 0.381152

We have finished training iteration 163
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_161_.pth
per-ex loss: 0.510736  [    1/   89]
per-ex loss: 0.239236  [    2/   89]
per-ex loss: 0.283950  [    3/   89]
per-ex loss: 0.463843  [    4/   89]
per-ex loss: 0.500508  [    5/   89]
per-ex loss: 0.341223  [    6/   89]
per-ex loss: 0.349830  [    7/   89]
per-ex loss: 0.300710  [    8/   89]
per-ex loss: 0.458804  [    9/   89]
per-ex loss: 0.258609  [   10/   89]
per-ex loss: 0.281200  [   11/   89]
per-ex loss: 0.379081  [   12/   89]
per-ex loss: 0.576774  [   13/   89]
per-ex loss: 0.297258  [   14/   89]
per-ex loss: 0.276695  [   15/   89]
per-ex loss: 0.204082  [   16/   89]
per-ex loss: 0.348217  [   17/   89]
per-ex loss: 0.293080  [   18/   89]
per-ex loss: 0.380937  [   19/   89]
per-ex loss: 0.564108  [   20/   89]
per-ex loss: 0.339504  [   21/   89]
per-ex loss: 0.531576  [   22/   89]
per-ex loss: 0.184225  [   23/   89]
per-ex loss: 0.342931  [   24/   89]
per-ex loss: 0.470732  [   25/   89]
per-ex loss: 0.324392  [   26/   89]
per-ex loss: 0.384402  [   27/   89]
per-ex loss: 0.324511  [   28/   89]
per-ex loss: 0.300079  [   29/   89]
per-ex loss: 0.323191  [   30/   89]
per-ex loss: 0.324998  [   31/   89]
per-ex loss: 0.488338  [   32/   89]
per-ex loss: 0.485500  [   33/   89]
per-ex loss: 0.999955  [   34/   89]
per-ex loss: 0.340392  [   35/   89]
per-ex loss: 0.382715  [   36/   89]
per-ex loss: 0.262294  [   37/   89]
per-ex loss: 0.438411  [   38/   89]
per-ex loss: 0.448079  [   39/   89]
per-ex loss: 0.308324  [   40/   89]
per-ex loss: 0.577700  [   41/   89]
per-ex loss: 0.640655  [   42/   89]
per-ex loss: 0.543322  [   43/   89]
per-ex loss: 0.467575  [   44/   89]
per-ex loss: 0.266436  [   45/   89]
per-ex loss: 0.547708  [   46/   89]
per-ex loss: 0.529101  [   47/   89]
per-ex loss: 0.268132  [   48/   89]
per-ex loss: 0.248421  [   49/   89]
per-ex loss: 0.225007  [   50/   89]
per-ex loss: 0.157863  [   51/   89]
per-ex loss: 0.509906  [   52/   89]
per-ex loss: 0.296005  [   53/   89]
per-ex loss: 0.250031  [   54/   89]
per-ex loss: 0.621233  [   55/   89]
per-ex loss: 0.302615  [   56/   89]
per-ex loss: 0.260374  [   57/   89]
per-ex loss: 0.177911  [   58/   89]
per-ex loss: 0.321353  [   59/   89]
per-ex loss: 0.277709  [   60/   89]
per-ex loss: 0.214547  [   61/   89]
per-ex loss: 0.411445  [   62/   89]
per-ex loss: 0.427448  [   63/   89]
per-ex loss: 0.578796  [   64/   89]
per-ex loss: 0.708216  [   65/   89]
per-ex loss: 0.368979  [   66/   89]
per-ex loss: 0.390230  [   67/   89]
per-ex loss: 0.477717  [   68/   89]
per-ex loss: 0.442321  [   69/   89]
per-ex loss: 0.276158  [   70/   89]
per-ex loss: 0.439833  [   71/   89]
per-ex loss: 0.293309  [   72/   89]
per-ex loss: 0.382026  [   73/   89]
per-ex loss: 0.461339  [   74/   89]
per-ex loss: 0.396745  [   75/   89]
per-ex loss: 0.281527  [   76/   89]
per-ex loss: 0.347457  [   77/   89]
per-ex loss: 0.361606  [   78/   89]
per-ex loss: 0.308992  [   79/   89]
per-ex loss: 0.409212  [   80/   89]
per-ex loss: 0.415386  [   81/   89]
per-ex loss: 0.312853  [   82/   89]
per-ex loss: 0.369749  [   83/   89]
per-ex loss: 0.437402  [   84/   89]
per-ex loss: 0.636557  [   85/   89]
per-ex loss: 0.305046  [   86/   89]
per-ex loss: 0.400509  [   87/   89]
per-ex loss: 0.359111  [   88/   89]
per-ex loss: 0.276737  [   89/   89]
Train Error: Avg loss: 0.38532293
validation Error: 
 Avg loss: 0.56367179 
 F1: 0.457569 
 Precision: 0.431679 
 Recall: 0.486762
 IoU: 0.296654

test Error: 
 Avg loss: 0.50521375 
 F1: 0.536558 
 Precision: 0.513714 
 Recall: 0.561529
 IoU: 0.366642

We have finished training iteration 164
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_162_.pth
per-ex loss: 0.489759  [    1/   89]
per-ex loss: 0.599897  [    2/   89]
per-ex loss: 0.501644  [    3/   89]
per-ex loss: 0.282348  [    4/   89]
per-ex loss: 0.401787  [    5/   89]
per-ex loss: 0.354652  [    6/   89]
per-ex loss: 0.309105  [    7/   89]
per-ex loss: 0.408689  [    8/   89]
per-ex loss: 0.273589  [    9/   89]
per-ex loss: 0.312270  [   10/   89]
per-ex loss: 0.518614  [   11/   89]
per-ex loss: 0.320211  [   12/   89]
per-ex loss: 0.312396  [   13/   89]
per-ex loss: 0.178824  [   14/   89]
per-ex loss: 0.388296  [   15/   89]
per-ex loss: 0.500920  [   16/   89]
per-ex loss: 0.513967  [   17/   89]
per-ex loss: 0.421642  [   18/   89]
per-ex loss: 0.504355  [   19/   89]
per-ex loss: 0.292984  [   20/   89]
per-ex loss: 0.266312  [   21/   89]
per-ex loss: 0.469626  [   22/   89]
per-ex loss: 0.470727  [   23/   89]
per-ex loss: 0.389792  [   24/   89]
per-ex loss: 0.412607  [   25/   89]
per-ex loss: 0.269011  [   26/   89]
per-ex loss: 0.433223  [   27/   89]
per-ex loss: 0.298285  [   28/   89]
per-ex loss: 0.270329  [   29/   89]
per-ex loss: 0.286409  [   30/   89]
per-ex loss: 0.288047  [   31/   89]
per-ex loss: 0.250987  [   32/   89]
per-ex loss: 0.515905  [   33/   89]
per-ex loss: 0.270608  [   34/   89]
per-ex loss: 0.300673  [   35/   89]
per-ex loss: 0.335741  [   36/   89]
per-ex loss: 0.381210  [   37/   89]
per-ex loss: 0.361815  [   38/   89]
per-ex loss: 0.345240  [   39/   89]
per-ex loss: 0.557253  [   40/   89]
per-ex loss: 0.396210  [   41/   89]
per-ex loss: 0.301065  [   42/   89]
per-ex loss: 0.342520  [   43/   89]
per-ex loss: 0.248140  [   44/   89]
per-ex loss: 0.311324  [   45/   89]
per-ex loss: 0.328600  [   46/   89]
per-ex loss: 0.520442  [   47/   89]
per-ex loss: 0.321246  [   48/   89]
per-ex loss: 0.410580  [   49/   89]
per-ex loss: 0.454586  [   50/   89]
per-ex loss: 0.392371  [   51/   89]
per-ex loss: 0.353283  [   52/   89]
per-ex loss: 0.560658  [   53/   89]
per-ex loss: 0.294927  [   54/   89]
per-ex loss: 0.416434  [   55/   89]
per-ex loss: 0.280480  [   56/   89]
per-ex loss: 0.599417  [   57/   89]
per-ex loss: 0.505738  [   58/   89]
per-ex loss: 0.353348  [   59/   89]
per-ex loss: 0.508232  [   60/   89]
per-ex loss: 0.349146  [   61/   89]
per-ex loss: 0.319905  [   62/   89]
per-ex loss: 0.456801  [   63/   89]
per-ex loss: 0.269615  [   64/   89]
per-ex loss: 0.315572  [   65/   89]
per-ex loss: 0.445383  [   66/   89]
per-ex loss: 0.999904  [   67/   89]
per-ex loss: 0.347208  [   68/   89]
per-ex loss: 0.331975  [   69/   89]
per-ex loss: 0.489297  [   70/   89]
per-ex loss: 0.444026  [   71/   89]
per-ex loss: 0.524041  [   72/   89]
per-ex loss: 0.451364  [   73/   89]
per-ex loss: 0.319799  [   74/   89]
per-ex loss: 0.352596  [   75/   89]
per-ex loss: 0.266355  [   76/   89]
per-ex loss: 0.570363  [   77/   89]
per-ex loss: 0.253888  [   78/   89]
per-ex loss: 0.305735  [   79/   89]
per-ex loss: 0.255398  [   80/   89]
per-ex loss: 0.320331  [   81/   89]
per-ex loss: 0.423126  [   82/   89]
per-ex loss: 0.218836  [   83/   89]
per-ex loss: 0.346605  [   84/   89]
per-ex loss: 0.346301  [   85/   89]
per-ex loss: 0.329783  [   86/   89]
per-ex loss: 0.323703  [   87/   89]
per-ex loss: 0.294306  [   88/   89]
per-ex loss: 0.566592  [   89/   89]
Train Error: Avg loss: 0.38423934
validation Error: 
 Avg loss: 0.52990809 
 F1: 0.496319 
 Precision: 0.555331 
 Recall: 0.448644
 IoU: 0.330069

test Error: 
 Avg loss: 0.48988782 
 F1: 0.554704 
 Precision: 0.597100 
 Recall: 0.517930
 IoU: 0.383800

We have finished training iteration 165
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_163_.pth
per-ex loss: 0.342997  [    1/   89]
per-ex loss: 0.479460  [    2/   89]
per-ex loss: 0.243966  [    3/   89]
per-ex loss: 0.244835  [    4/   89]
per-ex loss: 0.301136  [    5/   89]
per-ex loss: 0.999915  [    6/   89]
per-ex loss: 0.291258  [    7/   89]
per-ex loss: 0.342298  [    8/   89]
per-ex loss: 0.294331  [    9/   89]
per-ex loss: 0.540927  [   10/   89]
per-ex loss: 0.288227  [   11/   89]
per-ex loss: 0.380132  [   12/   89]
per-ex loss: 0.301936  [   13/   89]
per-ex loss: 0.444238  [   14/   89]
per-ex loss: 0.420950  [   15/   89]
per-ex loss: 0.288110  [   16/   89]
per-ex loss: 0.535301  [   17/   89]
per-ex loss: 0.301651  [   18/   89]
per-ex loss: 0.273283  [   19/   89]
per-ex loss: 0.482689  [   20/   89]
per-ex loss: 0.205501  [   21/   89]
per-ex loss: 0.496622  [   22/   89]
per-ex loss: 0.303096  [   23/   89]
per-ex loss: 0.406080  [   24/   89]
per-ex loss: 0.350987  [   25/   89]
per-ex loss: 0.318475  [   26/   89]
per-ex loss: 0.337734  [   27/   89]
per-ex loss: 0.299442  [   28/   89]
per-ex loss: 0.164610  [   29/   89]
per-ex loss: 0.304586  [   30/   89]
per-ex loss: 0.378665  [   31/   89]
per-ex loss: 0.495067  [   32/   89]
per-ex loss: 0.337494  [   33/   89]
per-ex loss: 0.247535  [   34/   89]
per-ex loss: 0.502458  [   35/   89]
per-ex loss: 0.394236  [   36/   89]
per-ex loss: 0.473751  [   37/   89]
per-ex loss: 0.458887  [   38/   89]
per-ex loss: 0.431285  [   39/   89]
per-ex loss: 0.528087  [   40/   89]
per-ex loss: 0.508653  [   41/   89]
per-ex loss: 0.260551  [   42/   89]
per-ex loss: 0.268150  [   43/   89]
per-ex loss: 0.288905  [   44/   89]
per-ex loss: 0.435964  [   45/   89]
per-ex loss: 0.327619  [   46/   89]
per-ex loss: 0.271770  [   47/   89]
per-ex loss: 0.299049  [   48/   89]
per-ex loss: 0.444813  [   49/   89]
per-ex loss: 0.268667  [   50/   89]
per-ex loss: 0.331723  [   51/   89]
per-ex loss: 0.375689  [   52/   89]
per-ex loss: 0.375493  [   53/   89]
per-ex loss: 0.314450  [   54/   89]
per-ex loss: 0.505517  [   55/   89]
per-ex loss: 0.265996  [   56/   89]
per-ex loss: 0.242526  [   57/   89]
per-ex loss: 0.187129  [   58/   89]
per-ex loss: 0.264739  [   59/   89]
per-ex loss: 0.263600  [   60/   89]
per-ex loss: 0.344066  [   61/   89]
per-ex loss: 0.268206  [   62/   89]
per-ex loss: 0.363866  [   63/   89]
per-ex loss: 0.345259  [   64/   89]
per-ex loss: 0.321692  [   65/   89]
per-ex loss: 0.297876  [   66/   89]
per-ex loss: 0.420542  [   67/   89]
per-ex loss: 0.381189  [   68/   89]
per-ex loss: 0.261972  [   69/   89]
per-ex loss: 0.300846  [   70/   89]
per-ex loss: 0.290332  [   71/   89]
per-ex loss: 0.269661  [   72/   89]
per-ex loss: 0.347708  [   73/   89]
per-ex loss: 0.575158  [   74/   89]
per-ex loss: 0.552930  [   75/   89]
per-ex loss: 0.462771  [   76/   89]
per-ex loss: 0.276116  [   77/   89]
per-ex loss: 0.514536  [   78/   89]
per-ex loss: 0.572057  [   79/   89]
per-ex loss: 0.363867  [   80/   89]
per-ex loss: 0.569156  [   81/   89]
per-ex loss: 0.164812  [   82/   89]
per-ex loss: 0.430493  [   83/   89]
per-ex loss: 0.329280  [   84/   89]
per-ex loss: 0.320659  [   85/   89]
per-ex loss: 0.576226  [   86/   89]
per-ex loss: 0.495459  [   87/   89]
per-ex loss: 0.502125  [   88/   89]
per-ex loss: 0.402454  [   89/   89]
Train Error: Avg loss: 0.37139947
validation Error: 
 Avg loss: 0.54276314 
 F1: 0.486336 
 Precision: 0.491569 
 Recall: 0.481213
 IoU: 0.321297

test Error: 
 Avg loss: 0.48917552 
 F1: 0.555200 
 Precision: 0.559095 
 Recall: 0.551359
 IoU: 0.384275

We have finished training iteration 166
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_164_.pth
per-ex loss: 0.277829  [    1/   89]
per-ex loss: 0.999923  [    2/   89]
per-ex loss: 0.468965  [    3/   89]
per-ex loss: 0.418939  [    4/   89]
per-ex loss: 0.520219  [    5/   89]
per-ex loss: 0.391803  [    6/   89]
per-ex loss: 0.264884  [    7/   89]
per-ex loss: 0.574683  [    8/   89]
per-ex loss: 0.366700  [    9/   89]
per-ex loss: 0.152000  [   10/   89]
per-ex loss: 0.282297  [   11/   89]
per-ex loss: 0.656701  [   12/   89]
per-ex loss: 0.427367  [   13/   89]
per-ex loss: 0.292802  [   14/   89]
per-ex loss: 0.437496  [   15/   89]
per-ex loss: 0.368134  [   16/   89]
per-ex loss: 0.347057  [   17/   89]
per-ex loss: 0.481597  [   18/   89]
per-ex loss: 0.346113  [   19/   89]
per-ex loss: 0.358065  [   20/   89]
per-ex loss: 0.382926  [   21/   89]
per-ex loss: 0.330739  [   22/   89]
per-ex loss: 0.432367  [   23/   89]
per-ex loss: 0.506248  [   24/   89]
per-ex loss: 0.267877  [   25/   89]
per-ex loss: 0.356012  [   26/   89]
per-ex loss: 0.339053  [   27/   89]
per-ex loss: 0.443678  [   28/   89]
per-ex loss: 0.286588  [   29/   89]
per-ex loss: 0.473528  [   30/   89]
per-ex loss: 0.283715  [   31/   89]
per-ex loss: 0.377851  [   32/   89]
per-ex loss: 0.438488  [   33/   89]
per-ex loss: 0.424858  [   34/   89]
per-ex loss: 0.284948  [   35/   89]
per-ex loss: 0.332593  [   36/   89]
per-ex loss: 0.290375  [   37/   89]
per-ex loss: 0.381809  [   38/   89]
per-ex loss: 0.299089  [   39/   89]
per-ex loss: 0.462163  [   40/   89]
per-ex loss: 0.196652  [   41/   89]
per-ex loss: 0.251039  [   42/   89]
per-ex loss: 0.322868  [   43/   89]
per-ex loss: 0.362642  [   44/   89]
per-ex loss: 0.466283  [   45/   89]
per-ex loss: 0.354419  [   46/   89]
per-ex loss: 0.487169  [   47/   89]
per-ex loss: 0.245741  [   48/   89]
per-ex loss: 0.330970  [   49/   89]
per-ex loss: 0.258309  [   50/   89]
per-ex loss: 0.434860  [   51/   89]
per-ex loss: 0.351590  [   52/   89]
per-ex loss: 0.320140  [   53/   89]
per-ex loss: 0.471622  [   54/   89]
per-ex loss: 0.348340  [   55/   89]
per-ex loss: 0.318657  [   56/   89]
per-ex loss: 0.593387  [   57/   89]
per-ex loss: 0.216792  [   58/   89]
per-ex loss: 0.279946  [   59/   89]
per-ex loss: 0.270428  [   60/   89]
per-ex loss: 0.384064  [   61/   89]
per-ex loss: 0.313590  [   62/   89]
per-ex loss: 0.331996  [   63/   89]
per-ex loss: 0.271473  [   64/   89]
per-ex loss: 0.411287  [   65/   89]
per-ex loss: 0.522285  [   66/   89]
per-ex loss: 0.507843  [   67/   89]
per-ex loss: 0.285721  [   68/   89]
per-ex loss: 0.329943  [   69/   89]
per-ex loss: 0.315031  [   70/   89]
per-ex loss: 0.288252  [   71/   89]
per-ex loss: 0.249081  [   72/   89]
per-ex loss: 0.196217  [   73/   89]
per-ex loss: 0.240231  [   74/   89]
per-ex loss: 0.587136  [   75/   89]
per-ex loss: 0.569189  [   76/   89]
per-ex loss: 0.529140  [   77/   89]
per-ex loss: 0.270386  [   78/   89]
per-ex loss: 0.350228  [   79/   89]
per-ex loss: 0.281345  [   80/   89]
per-ex loss: 0.425919  [   81/   89]
per-ex loss: 0.273650  [   82/   89]
per-ex loss: 0.305593  [   83/   89]
per-ex loss: 0.577806  [   84/   89]
per-ex loss: 0.325291  [   85/   89]
per-ex loss: 0.533119  [   86/   89]
per-ex loss: 0.358850  [   87/   89]
per-ex loss: 0.389658  [   88/   89]
per-ex loss: 0.364423  [   89/   89]
Train Error: Avg loss: 0.37639417
validation Error: 
 Avg loss: 0.53906354 
 F1: 0.493929 
 Precision: 0.500978 
 Recall: 0.487075
 IoU: 0.327958

test Error: 
 Avg loss: 0.48530742 
 F1: 0.559477 
 Precision: 0.550522 
 Recall: 0.568729
 IoU: 0.388385

We have finished training iteration 167
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_165_.pth
per-ex loss: 0.251016  [    1/   89]
per-ex loss: 0.381818  [    2/   89]
per-ex loss: 0.999879  [    3/   89]
per-ex loss: 0.595889  [    4/   89]
per-ex loss: 0.332165  [    5/   89]
per-ex loss: 0.324175  [    6/   89]
per-ex loss: 0.515327  [    7/   89]
per-ex loss: 0.374351  [    8/   89]
per-ex loss: 0.417376  [    9/   89]
per-ex loss: 0.455608  [   10/   89]
per-ex loss: 0.431706  [   11/   89]
per-ex loss: 0.431864  [   12/   89]
per-ex loss: 0.309994  [   13/   89]
per-ex loss: 0.322971  [   14/   89]
per-ex loss: 0.487411  [   15/   89]
per-ex loss: 0.428706  [   16/   89]
per-ex loss: 0.326578  [   17/   89]
per-ex loss: 0.182875  [   18/   89]
per-ex loss: 0.346945  [   19/   89]
per-ex loss: 0.340699  [   20/   89]
per-ex loss: 0.242630  [   21/   89]
per-ex loss: 0.393635  [   22/   89]
per-ex loss: 0.529774  [   23/   89]
per-ex loss: 0.358052  [   24/   89]
per-ex loss: 0.284778  [   25/   89]
per-ex loss: 0.190369  [   26/   89]
per-ex loss: 0.267898  [   27/   89]
per-ex loss: 0.416192  [   28/   89]
per-ex loss: 0.312015  [   29/   89]
per-ex loss: 0.395447  [   30/   89]
per-ex loss: 0.298152  [   31/   89]
per-ex loss: 0.253662  [   32/   89]
per-ex loss: 0.342252  [   33/   89]
per-ex loss: 0.379599  [   34/   89]
per-ex loss: 0.272670  [   35/   89]
per-ex loss: 0.269883  [   36/   89]
per-ex loss: 0.312513  [   37/   89]
per-ex loss: 0.557529  [   38/   89]
per-ex loss: 0.348528  [   39/   89]
per-ex loss: 0.465061  [   40/   89]
per-ex loss: 0.230966  [   41/   89]
per-ex loss: 0.230827  [   42/   89]
per-ex loss: 0.338204  [   43/   89]
per-ex loss: 0.295087  [   44/   89]
per-ex loss: 0.351896  [   45/   89]
per-ex loss: 0.391825  [   46/   89]
per-ex loss: 0.506954  [   47/   89]
per-ex loss: 0.273801  [   48/   89]
per-ex loss: 0.506604  [   49/   89]
per-ex loss: 0.286444  [   50/   89]
per-ex loss: 0.431612  [   51/   89]
per-ex loss: 0.322863  [   52/   89]
per-ex loss: 0.558621  [   53/   89]
per-ex loss: 0.476930  [   54/   89]
per-ex loss: 0.547705  [   55/   89]
per-ex loss: 0.482140  [   56/   89]
per-ex loss: 0.241200  [   57/   89]
per-ex loss: 0.295533  [   58/   89]
per-ex loss: 0.325306  [   59/   89]
per-ex loss: 0.281470  [   60/   89]
per-ex loss: 0.299140  [   61/   89]
per-ex loss: 0.236294  [   62/   89]
per-ex loss: 0.244574  [   63/   89]
per-ex loss: 0.671875  [   64/   89]
per-ex loss: 0.294730  [   65/   89]
per-ex loss: 0.513034  [   66/   89]
per-ex loss: 0.340201  [   67/   89]
per-ex loss: 0.324370  [   68/   89]
per-ex loss: 0.362874  [   69/   89]
per-ex loss: 0.491612  [   70/   89]
per-ex loss: 0.495232  [   71/   89]
per-ex loss: 0.393582  [   72/   89]
per-ex loss: 0.290829  [   73/   89]
per-ex loss: 0.313641  [   74/   89]
per-ex loss: 0.355059  [   75/   89]
per-ex loss: 0.336082  [   76/   89]
per-ex loss: 0.470899  [   77/   89]
per-ex loss: 0.266151  [   78/   89]
per-ex loss: 0.307653  [   79/   89]
per-ex loss: 0.579772  [   80/   89]
per-ex loss: 0.590737  [   81/   89]
per-ex loss: 0.354951  [   82/   89]
per-ex loss: 0.338505  [   83/   89]
per-ex loss: 0.282310  [   84/   89]
per-ex loss: 0.347312  [   85/   89]
per-ex loss: 0.300695  [   86/   89]
per-ex loss: 0.472975  [   87/   89]
per-ex loss: 0.401525  [   88/   89]
per-ex loss: 0.528529  [   89/   89]
Train Error: Avg loss: 0.37897888
validation Error: 
 Avg loss: 0.54006083 
 F1: 0.484148 
 Precision: 0.481229 
 Recall: 0.487102
 IoU: 0.319390

test Error: 
 Avg loss: 0.48766982 
 F1: 0.555148 
 Precision: 0.541255 
 Recall: 0.569773
 IoU: 0.384225

We have finished training iteration 168
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_166_.pth
per-ex loss: 0.593423  [    1/   89]
per-ex loss: 0.345752  [    2/   89]
per-ex loss: 0.342566  [    3/   89]
per-ex loss: 0.339951  [    4/   89]
per-ex loss: 0.458084  [    5/   89]
per-ex loss: 0.327432  [    6/   89]
per-ex loss: 0.481644  [    7/   89]
per-ex loss: 0.294075  [    8/   89]
per-ex loss: 0.310338  [    9/   89]
per-ex loss: 0.280440  [   10/   89]
per-ex loss: 0.242976  [   11/   89]
per-ex loss: 0.318810  [   12/   89]
per-ex loss: 0.381330  [   13/   89]
per-ex loss: 0.276439  [   14/   89]
per-ex loss: 0.240351  [   15/   89]
per-ex loss: 0.495268  [   16/   89]
per-ex loss: 0.395195  [   17/   89]
per-ex loss: 0.338263  [   18/   89]
per-ex loss: 0.487276  [   19/   89]
per-ex loss: 0.492934  [   20/   89]
per-ex loss: 0.269966  [   21/   89]
per-ex loss: 0.302292  [   22/   89]
per-ex loss: 0.406017  [   23/   89]
per-ex loss: 0.141713  [   24/   89]
per-ex loss: 0.394339  [   25/   89]
per-ex loss: 0.252687  [   26/   89]
per-ex loss: 0.407028  [   27/   89]
per-ex loss: 0.534181  [   28/   89]
per-ex loss: 0.364593  [   29/   89]
per-ex loss: 0.288569  [   30/   89]
per-ex loss: 0.442745  [   31/   89]
per-ex loss: 0.340263  [   32/   89]
per-ex loss: 0.315591  [   33/   89]
per-ex loss: 0.444321  [   34/   89]
per-ex loss: 0.365580  [   35/   89]
per-ex loss: 0.353100  [   36/   89]
per-ex loss: 0.555617  [   37/   89]
per-ex loss: 0.387986  [   38/   89]
per-ex loss: 0.295962  [   39/   89]
per-ex loss: 0.314275  [   40/   89]
per-ex loss: 0.346600  [   41/   89]
per-ex loss: 0.352305  [   42/   89]
per-ex loss: 0.279660  [   43/   89]
per-ex loss: 0.440799  [   44/   89]
per-ex loss: 0.194487  [   45/   89]
per-ex loss: 0.247812  [   46/   89]
per-ex loss: 0.318946  [   47/   89]
per-ex loss: 0.320197  [   48/   89]
per-ex loss: 0.467291  [   49/   89]
per-ex loss: 0.341087  [   50/   89]
per-ex loss: 0.486764  [   51/   89]
per-ex loss: 0.431216  [   52/   89]
per-ex loss: 0.426277  [   53/   89]
per-ex loss: 0.292090  [   54/   89]
per-ex loss: 0.434056  [   55/   89]
per-ex loss: 0.464351  [   56/   89]
per-ex loss: 0.254701  [   57/   89]
per-ex loss: 0.289178  [   58/   89]
per-ex loss: 0.327259  [   59/   89]
per-ex loss: 0.620575  [   60/   89]
per-ex loss: 0.394325  [   61/   89]
per-ex loss: 0.285381  [   62/   89]
per-ex loss: 0.307806  [   63/   89]
per-ex loss: 0.300525  [   64/   89]
per-ex loss: 0.474641  [   65/   89]
per-ex loss: 0.356569  [   66/   89]
per-ex loss: 0.349415  [   67/   89]
per-ex loss: 0.550811  [   68/   89]
per-ex loss: 0.586941  [   69/   89]
per-ex loss: 0.328297  [   70/   89]
per-ex loss: 0.318069  [   71/   89]
per-ex loss: 0.337034  [   72/   89]
per-ex loss: 0.328809  [   73/   89]
per-ex loss: 0.402598  [   74/   89]
per-ex loss: 0.650566  [   75/   89]
per-ex loss: 0.291806  [   76/   89]
per-ex loss: 0.337300  [   77/   89]
per-ex loss: 0.417649  [   78/   89]
per-ex loss: 0.351570  [   79/   89]
per-ex loss: 0.999879  [   80/   89]
per-ex loss: 0.557339  [   81/   89]
per-ex loss: 0.405996  [   82/   89]
per-ex loss: 0.348490  [   83/   89]
per-ex loss: 0.336521  [   84/   89]
per-ex loss: 0.461420  [   85/   89]
per-ex loss: 0.211992  [   86/   89]
per-ex loss: 0.221108  [   87/   89]
per-ex loss: 0.264139  [   88/   89]
per-ex loss: 0.187632  [   89/   89]
Train Error: Avg loss: 0.37437807
validation Error: 
 Avg loss: 0.55443856 
 F1: 0.476646 
 Precision: 0.529105 
 Recall: 0.433651
 IoU: 0.312893

test Error: 
 Avg loss: 0.52567044 
 F1: 0.524172 
 Precision: 0.515577 
 Recall: 0.533058
 IoU: 0.355172

We have finished training iteration 169
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_167_.pth
per-ex loss: 0.177036  [    1/   89]
per-ex loss: 0.999902  [    2/   89]
per-ex loss: 0.399084  [    3/   89]
per-ex loss: 0.307820  [    4/   89]
per-ex loss: 0.248215  [    5/   89]
per-ex loss: 0.419674  [    6/   89]
per-ex loss: 0.380079  [    7/   89]
per-ex loss: 0.284366  [    8/   89]
per-ex loss: 0.338698  [    9/   89]
per-ex loss: 0.351602  [   10/   89]
per-ex loss: 0.346971  [   11/   89]
per-ex loss: 0.350852  [   12/   89]
per-ex loss: 0.433240  [   13/   89]
per-ex loss: 0.233783  [   14/   89]
per-ex loss: 0.325590  [   15/   89]
per-ex loss: 0.561877  [   16/   89]
per-ex loss: 0.288169  [   17/   89]
per-ex loss: 0.387497  [   18/   89]
per-ex loss: 0.360631  [   19/   89]
per-ex loss: 0.354293  [   20/   89]
per-ex loss: 0.271274  [   21/   89]
per-ex loss: 0.521459  [   22/   89]
per-ex loss: 0.585296  [   23/   89]
per-ex loss: 0.341437  [   24/   89]
per-ex loss: 0.499536  [   25/   89]
per-ex loss: 0.321339  [   26/   89]
per-ex loss: 0.569579  [   27/   89]
per-ex loss: 0.338379  [   28/   89]
per-ex loss: 0.339678  [   29/   89]
per-ex loss: 0.267062  [   30/   89]
per-ex loss: 0.317236  [   31/   89]
per-ex loss: 0.483054  [   32/   89]
per-ex loss: 0.199428  [   33/   89]
per-ex loss: 0.445519  [   34/   89]
per-ex loss: 0.199893  [   35/   89]
per-ex loss: 0.144515  [   36/   89]
per-ex loss: 0.305015  [   37/   89]
per-ex loss: 0.588350  [   38/   89]
per-ex loss: 0.242167  [   39/   89]
per-ex loss: 0.267595  [   40/   89]
per-ex loss: 0.425603  [   41/   89]
per-ex loss: 0.248948  [   42/   89]
per-ex loss: 0.430131  [   43/   89]
per-ex loss: 0.195607  [   44/   89]
per-ex loss: 0.542326  [   45/   89]
per-ex loss: 0.284859  [   46/   89]
per-ex loss: 0.275256  [   47/   89]
per-ex loss: 0.570872  [   48/   89]
per-ex loss: 0.318515  [   49/   89]
per-ex loss: 0.286802  [   50/   89]
per-ex loss: 0.435592  [   51/   89]
per-ex loss: 0.401179  [   52/   89]
per-ex loss: 0.341604  [   53/   89]
per-ex loss: 0.291028  [   54/   89]
per-ex loss: 0.293119  [   55/   89]
per-ex loss: 0.236643  [   56/   89]
per-ex loss: 0.341748  [   57/   89]
per-ex loss: 0.279174  [   58/   89]
per-ex loss: 0.436642  [   59/   89]
per-ex loss: 0.472175  [   60/   89]
per-ex loss: 0.496212  [   61/   89]
per-ex loss: 0.498759  [   62/   89]
per-ex loss: 0.356285  [   63/   89]
per-ex loss: 0.294752  [   64/   89]
per-ex loss: 0.364278  [   65/   89]
per-ex loss: 0.498807  [   66/   89]
per-ex loss: 0.304053  [   67/   89]
per-ex loss: 0.355963  [   68/   89]
per-ex loss: 0.437431  [   69/   89]
per-ex loss: 0.211543  [   70/   89]
per-ex loss: 0.287982  [   71/   89]
per-ex loss: 0.500271  [   72/   89]
per-ex loss: 0.266769  [   73/   89]
per-ex loss: 0.297391  [   74/   89]
per-ex loss: 0.400935  [   75/   89]
per-ex loss: 0.421010  [   76/   89]
per-ex loss: 0.298978  [   77/   89]
per-ex loss: 0.531874  [   78/   89]
per-ex loss: 0.399144  [   79/   89]
per-ex loss: 0.515459  [   80/   89]
per-ex loss: 0.294027  [   81/   89]
per-ex loss: 0.321424  [   82/   89]
per-ex loss: 0.267522  [   83/   89]
per-ex loss: 0.467503  [   84/   89]
per-ex loss: 0.333974  [   85/   89]
per-ex loss: 0.368413  [   86/   89]
per-ex loss: 0.336247  [   87/   89]
per-ex loss: 0.503688  [   88/   89]
per-ex loss: 0.345388  [   89/   89]
Train Error: Avg loss: 0.36945057
validation Error: 
 Avg loss: 0.54102448 
 F1: 0.482775 
 Precision: 0.479845 
 Recall: 0.485740
 IoU: 0.318196

test Error: 
 Avg loss: 0.48846449 
 F1: 0.553630 
 Precision: 0.528683 
 Recall: 0.581048
 IoU: 0.382772

We have finished training iteration 170
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_168_.pth
per-ex loss: 0.251710  [    1/   89]
per-ex loss: 0.269633  [    2/   89]
per-ex loss: 0.415751  [    3/   89]
per-ex loss: 0.310813  [    4/   89]
per-ex loss: 0.355125  [    5/   89]
per-ex loss: 0.404281  [    6/   89]
per-ex loss: 0.347726  [    7/   89]
per-ex loss: 0.443667  [    8/   89]
per-ex loss: 0.445724  [    9/   89]
per-ex loss: 0.282306  [   10/   89]
per-ex loss: 0.501331  [   11/   89]
per-ex loss: 0.295106  [   12/   89]
per-ex loss: 0.469589  [   13/   89]
per-ex loss: 0.293560  [   14/   89]
per-ex loss: 0.289501  [   15/   89]
per-ex loss: 0.461219  [   16/   89]
per-ex loss: 0.302103  [   17/   89]
per-ex loss: 0.233706  [   18/   89]
per-ex loss: 0.999932  [   19/   89]
per-ex loss: 0.285568  [   20/   89]
per-ex loss: 0.364576  [   21/   89]
per-ex loss: 0.357205  [   22/   89]
per-ex loss: 0.574225  [   23/   89]
per-ex loss: 0.345185  [   24/   89]
per-ex loss: 0.371511  [   25/   89]
per-ex loss: 0.476440  [   26/   89]
per-ex loss: 0.326735  [   27/   89]
per-ex loss: 0.416180  [   28/   89]
per-ex loss: 0.290407  [   29/   89]
per-ex loss: 0.280725  [   30/   89]
per-ex loss: 0.352960  [   31/   89]
per-ex loss: 0.536954  [   32/   89]
per-ex loss: 0.160506  [   33/   89]
per-ex loss: 0.310295  [   34/   89]
per-ex loss: 0.456913  [   35/   89]
per-ex loss: 0.306692  [   36/   89]
per-ex loss: 0.331723  [   37/   89]
per-ex loss: 0.345728  [   38/   89]
per-ex loss: 0.329754  [   39/   89]
per-ex loss: 0.270479  [   40/   89]
per-ex loss: 0.419988  [   41/   89]
per-ex loss: 0.408721  [   42/   89]
per-ex loss: 0.444570  [   43/   89]
per-ex loss: 0.252360  [   44/   89]
per-ex loss: 0.280027  [   45/   89]
per-ex loss: 0.434005  [   46/   89]
per-ex loss: 0.298292  [   47/   89]
per-ex loss: 0.335873  [   48/   89]
per-ex loss: 0.332659  [   49/   89]
per-ex loss: 0.245314  [   50/   89]
per-ex loss: 0.396470  [   51/   89]
per-ex loss: 0.530587  [   52/   89]
per-ex loss: 0.452234  [   53/   89]
per-ex loss: 0.283762  [   54/   89]
per-ex loss: 0.413297  [   55/   89]
per-ex loss: 0.485477  [   56/   89]
per-ex loss: 0.398696  [   57/   89]
per-ex loss: 0.270657  [   58/   89]
per-ex loss: 0.517748  [   59/   89]
per-ex loss: 0.287538  [   60/   89]
per-ex loss: 0.408534  [   61/   89]
per-ex loss: 0.282187  [   62/   89]
per-ex loss: 0.161829  [   63/   89]
per-ex loss: 0.252056  [   64/   89]
per-ex loss: 0.401906  [   65/   89]
per-ex loss: 0.319503  [   66/   89]
per-ex loss: 0.573711  [   67/   89]
per-ex loss: 0.272939  [   68/   89]
per-ex loss: 0.433014  [   69/   89]
per-ex loss: 0.231434  [   70/   89]
per-ex loss: 0.475332  [   71/   89]
per-ex loss: 0.338046  [   72/   89]
per-ex loss: 0.416974  [   73/   89]
per-ex loss: 0.254003  [   74/   89]
per-ex loss: 0.287291  [   75/   89]
per-ex loss: 0.564734  [   76/   89]
per-ex loss: 0.270472  [   77/   89]
per-ex loss: 0.551483  [   78/   89]
per-ex loss: 0.334553  [   79/   89]
per-ex loss: 0.533088  [   80/   89]
per-ex loss: 0.362142  [   81/   89]
per-ex loss: 0.318579  [   82/   89]
per-ex loss: 0.286912  [   83/   89]
per-ex loss: 0.300162  [   84/   89]
per-ex loss: 0.327991  [   85/   89]
per-ex loss: 0.228174  [   86/   89]
per-ex loss: 0.246088  [   87/   89]
per-ex loss: 0.241963  [   88/   89]
per-ex loss: 0.544421  [   89/   89]
Train Error: Avg loss: 0.36597014
validation Error: 
 Avg loss: 0.53411913 
 F1: 0.493486 
 Precision: 0.547776 
 Recall: 0.448988
 IoU: 0.327568

test Error: 
 Avg loss: 0.48543229 
 F1: 0.561403 
 Precision: 0.605279 
 Recall: 0.523458
 IoU: 0.390243

We have finished training iteration 171
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_169_.pth
per-ex loss: 0.557465  [    1/   89]
per-ex loss: 0.330638  [    2/   89]
per-ex loss: 0.546715  [    3/   89]
per-ex loss: 0.256541  [    4/   89]
per-ex loss: 0.485113  [    5/   89]
per-ex loss: 0.333212  [    6/   89]
per-ex loss: 0.303907  [    7/   89]
per-ex loss: 0.190170  [    8/   89]
per-ex loss: 0.512189  [    9/   89]
per-ex loss: 0.320810  [   10/   89]
per-ex loss: 0.241703  [   11/   89]
per-ex loss: 0.187255  [   12/   89]
per-ex loss: 0.334010  [   13/   89]
per-ex loss: 0.256329  [   14/   89]
per-ex loss: 0.366783  [   15/   89]
per-ex loss: 0.403320  [   16/   89]
per-ex loss: 0.349975  [   17/   89]
per-ex loss: 0.430042  [   18/   89]
per-ex loss: 0.345448  [   19/   89]
per-ex loss: 0.506850  [   20/   89]
per-ex loss: 0.296806  [   21/   89]
per-ex loss: 0.501585  [   22/   89]
per-ex loss: 0.278562  [   23/   89]
per-ex loss: 0.575744  [   24/   89]
per-ex loss: 0.417825  [   25/   89]
per-ex loss: 0.304596  [   26/   89]
per-ex loss: 0.572062  [   27/   89]
per-ex loss: 0.481326  [   28/   89]
per-ex loss: 0.288132  [   29/   89]
per-ex loss: 0.449305  [   30/   89]
per-ex loss: 0.420493  [   31/   89]
per-ex loss: 0.576488  [   32/   89]
per-ex loss: 0.515490  [   33/   89]
per-ex loss: 0.186022  [   34/   89]
per-ex loss: 0.224634  [   35/   89]
per-ex loss: 0.212616  [   36/   89]
per-ex loss: 0.311994  [   37/   89]
per-ex loss: 0.272702  [   38/   89]
per-ex loss: 0.294127  [   39/   89]
per-ex loss: 0.391948  [   40/   89]
per-ex loss: 0.568156  [   41/   89]
per-ex loss: 0.222886  [   42/   89]
per-ex loss: 0.293219  [   43/   89]
per-ex loss: 0.371542  [   44/   89]
per-ex loss: 0.315762  [   45/   89]
per-ex loss: 0.232363  [   46/   89]
per-ex loss: 0.324614  [   47/   89]
per-ex loss: 0.272104  [   48/   89]
per-ex loss: 0.354310  [   49/   89]
per-ex loss: 0.398184  [   50/   89]
per-ex loss: 0.253388  [   51/   89]
per-ex loss: 0.306967  [   52/   89]
per-ex loss: 0.463167  [   53/   89]
per-ex loss: 0.257351  [   54/   89]
per-ex loss: 0.450608  [   55/   89]
per-ex loss: 0.326217  [   56/   89]
per-ex loss: 0.537839  [   57/   89]
per-ex loss: 0.237585  [   58/   89]
per-ex loss: 0.223999  [   59/   89]
per-ex loss: 0.338119  [   60/   89]
per-ex loss: 0.268247  [   61/   89]
per-ex loss: 0.262765  [   62/   89]
per-ex loss: 0.338315  [   63/   89]
per-ex loss: 0.311649  [   64/   89]
per-ex loss: 0.445407  [   65/   89]
per-ex loss: 0.365826  [   66/   89]
per-ex loss: 0.392884  [   67/   89]
per-ex loss: 0.306754  [   68/   89]
per-ex loss: 0.351002  [   69/   89]
per-ex loss: 0.198701  [   70/   89]
per-ex loss: 0.478798  [   71/   89]
per-ex loss: 0.444714  [   72/   89]
per-ex loss: 0.367235  [   73/   89]
per-ex loss: 0.378397  [   74/   89]
per-ex loss: 0.262371  [   75/   89]
per-ex loss: 0.292615  [   76/   89]
per-ex loss: 0.407998  [   77/   89]
per-ex loss: 0.226989  [   78/   89]
per-ex loss: 0.217328  [   79/   89]
per-ex loss: 0.318645  [   80/   89]
per-ex loss: 0.512674  [   81/   89]
per-ex loss: 0.288942  [   82/   89]
per-ex loss: 0.462558  [   83/   89]
per-ex loss: 0.406532  [   84/   89]
per-ex loss: 0.270979  [   85/   89]
per-ex loss: 0.366562  [   86/   89]
per-ex loss: 0.305679  [   87/   89]
per-ex loss: 0.999922  [   88/   89]
per-ex loss: 0.347591  [   89/   89]
Train Error: Avg loss: 0.36154371
validation Error: 
 Avg loss: 0.63087393 
 F1: 0.407412 
 Precision: 0.381268 
 Recall: 0.437405
 IoU: 0.255818

test Error: 
 Avg loss: 0.61046191 
 F1: 0.433954 
 Precision: 0.363452 
 Recall: 0.538391
 IoU: 0.277102

We have finished training iteration 172
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_170_.pth
per-ex loss: 0.370968  [    1/   89]
per-ex loss: 0.396860  [    2/   89]
per-ex loss: 0.388419  [    3/   89]
per-ex loss: 0.256141  [    4/   89]
per-ex loss: 0.515686  [    5/   89]
per-ex loss: 0.333044  [    6/   89]
per-ex loss: 0.278875  [    7/   89]
per-ex loss: 0.339925  [    8/   89]
per-ex loss: 0.403231  [    9/   89]
per-ex loss: 0.261883  [   10/   89]
per-ex loss: 0.314207  [   11/   89]
per-ex loss: 0.308191  [   12/   89]
per-ex loss: 0.278023  [   13/   89]
per-ex loss: 0.290201  [   14/   89]
per-ex loss: 0.324064  [   15/   89]
per-ex loss: 0.303341  [   16/   89]
per-ex loss: 0.403669  [   17/   89]
per-ex loss: 0.362457  [   18/   89]
per-ex loss: 0.331348  [   19/   89]
per-ex loss: 0.349100  [   20/   89]
per-ex loss: 0.342961  [   21/   89]
per-ex loss: 0.536120  [   22/   89]
per-ex loss: 0.263231  [   23/   89]
per-ex loss: 0.373452  [   24/   89]
per-ex loss: 0.507529  [   25/   89]
per-ex loss: 0.359743  [   26/   89]
per-ex loss: 0.299762  [   27/   89]
per-ex loss: 0.336336  [   28/   89]
per-ex loss: 0.233284  [   29/   89]
per-ex loss: 0.304251  [   30/   89]
per-ex loss: 0.361307  [   31/   89]
per-ex loss: 0.237951  [   32/   89]
per-ex loss: 0.334732  [   33/   89]
per-ex loss: 0.243219  [   34/   89]
per-ex loss: 0.595206  [   35/   89]
per-ex loss: 0.458232  [   36/   89]
per-ex loss: 0.320352  [   37/   89]
per-ex loss: 0.281665  [   38/   89]
per-ex loss: 0.358727  [   39/   89]
per-ex loss: 0.261079  [   40/   89]
per-ex loss: 0.277177  [   41/   89]
per-ex loss: 0.434907  [   42/   89]
per-ex loss: 0.622874  [   43/   89]
per-ex loss: 0.369008  [   44/   89]
per-ex loss: 0.628003  [   45/   89]
per-ex loss: 0.442336  [   46/   89]
per-ex loss: 0.231900  [   47/   89]
per-ex loss: 0.547632  [   48/   89]
per-ex loss: 0.380208  [   49/   89]
per-ex loss: 0.334771  [   50/   89]
per-ex loss: 0.351141  [   51/   89]
per-ex loss: 0.468778  [   52/   89]
per-ex loss: 0.333400  [   53/   89]
per-ex loss: 0.440132  [   54/   89]
per-ex loss: 0.349874  [   55/   89]
per-ex loss: 0.403129  [   56/   89]
per-ex loss: 0.331343  [   57/   89]
per-ex loss: 0.323187  [   58/   89]
per-ex loss: 0.201868  [   59/   89]
per-ex loss: 0.382056  [   60/   89]
per-ex loss: 0.472084  [   61/   89]
per-ex loss: 0.351037  [   62/   89]
per-ex loss: 0.203407  [   63/   89]
per-ex loss: 0.181212  [   64/   89]
per-ex loss: 0.511233  [   65/   89]
per-ex loss: 0.300228  [   66/   89]
per-ex loss: 0.999922  [   67/   89]
per-ex loss: 0.142418  [   68/   89]
per-ex loss: 0.420516  [   69/   89]
per-ex loss: 0.240782  [   70/   89]
per-ex loss: 0.569920  [   71/   89]
per-ex loss: 0.519303  [   72/   89]
per-ex loss: 0.287983  [   73/   89]
per-ex loss: 0.340982  [   74/   89]
per-ex loss: 0.531001  [   75/   89]
per-ex loss: 0.427757  [   76/   89]
per-ex loss: 0.329790  [   77/   89]
per-ex loss: 0.282459  [   78/   89]
per-ex loss: 0.302836  [   79/   89]
per-ex loss: 0.383601  [   80/   89]
per-ex loss: 0.648242  [   81/   89]
per-ex loss: 0.485051  [   82/   89]
per-ex loss: 0.423541  [   83/   89]
per-ex loss: 0.255733  [   84/   89]
per-ex loss: 0.554889  [   85/   89]
per-ex loss: 0.254783  [   86/   89]
per-ex loss: 0.360168  [   87/   89]
per-ex loss: 0.267991  [   88/   89]
per-ex loss: 0.329222  [   89/   89]
Train Error: Avg loss: 0.37135494
validation Error: 
 Avg loss: 0.53023254 
 F1: 0.495875 
 Precision: 0.548005 
 Recall: 0.452802
 IoU: 0.329677

test Error: 
 Avg loss: 0.49765996 
 F1: 0.545715 
 Precision: 0.572285 
 Recall: 0.521503
 IoU: 0.375247

We have finished training iteration 173
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_171_.pth
per-ex loss: 0.288502  [    1/   89]
per-ex loss: 0.278772  [    2/   89]
per-ex loss: 0.330938  [    3/   89]
per-ex loss: 0.293957  [    4/   89]
per-ex loss: 0.371416  [    5/   89]
per-ex loss: 0.250471  [    6/   89]
per-ex loss: 0.381456  [    7/   89]
per-ex loss: 0.579207  [    8/   89]
per-ex loss: 0.345149  [    9/   89]
per-ex loss: 0.246619  [   10/   89]
per-ex loss: 0.329152  [   11/   89]
per-ex loss: 0.300913  [   12/   89]
per-ex loss: 0.283998  [   13/   89]
per-ex loss: 0.246973  [   14/   89]
per-ex loss: 0.295182  [   15/   89]
per-ex loss: 0.243924  [   16/   89]
per-ex loss: 0.308320  [   17/   89]
per-ex loss: 0.361683  [   18/   89]
per-ex loss: 0.417575  [   19/   89]
per-ex loss: 0.515234  [   20/   89]
per-ex loss: 0.308801  [   21/   89]
per-ex loss: 0.316993  [   22/   89]
per-ex loss: 0.311547  [   23/   89]
per-ex loss: 0.456550  [   24/   89]
per-ex loss: 0.474388  [   25/   89]
per-ex loss: 0.542327  [   26/   89]
per-ex loss: 0.332276  [   27/   89]
per-ex loss: 0.301510  [   28/   89]
per-ex loss: 0.504102  [   29/   89]
per-ex loss: 0.279430  [   30/   89]
per-ex loss: 0.434943  [   31/   89]
per-ex loss: 0.352706  [   32/   89]
per-ex loss: 0.374776  [   33/   89]
per-ex loss: 0.555469  [   34/   89]
per-ex loss: 0.488228  [   35/   89]
per-ex loss: 0.249649  [   36/   89]
per-ex loss: 0.427494  [   37/   89]
per-ex loss: 0.335012  [   38/   89]
per-ex loss: 0.261891  [   39/   89]
per-ex loss: 0.403162  [   40/   89]
per-ex loss: 0.345882  [   41/   89]
per-ex loss: 0.323700  [   42/   89]
per-ex loss: 0.386708  [   43/   89]
per-ex loss: 0.580028  [   44/   89]
per-ex loss: 0.551194  [   45/   89]
per-ex loss: 0.431179  [   46/   89]
per-ex loss: 0.224510  [   47/   89]
per-ex loss: 0.585097  [   48/   89]
per-ex loss: 0.347709  [   49/   89]
per-ex loss: 0.311943  [   50/   89]
per-ex loss: 0.504402  [   51/   89]
per-ex loss: 0.229166  [   52/   89]
per-ex loss: 0.321032  [   53/   89]
per-ex loss: 0.243213  [   54/   89]
per-ex loss: 0.324143  [   55/   89]
per-ex loss: 0.326169  [   56/   89]
per-ex loss: 0.565690  [   57/   89]
per-ex loss: 0.261378  [   58/   89]
per-ex loss: 0.282036  [   59/   89]
per-ex loss: 0.337621  [   60/   89]
per-ex loss: 0.367693  [   61/   89]
per-ex loss: 0.361033  [   62/   89]
per-ex loss: 0.449791  [   63/   89]
per-ex loss: 0.409796  [   64/   89]
per-ex loss: 0.422817  [   65/   89]
per-ex loss: 0.139074  [   66/   89]
per-ex loss: 0.219462  [   67/   89]
per-ex loss: 0.286778  [   68/   89]
per-ex loss: 0.214755  [   69/   89]
per-ex loss: 0.189254  [   70/   89]
per-ex loss: 0.230702  [   71/   89]
per-ex loss: 0.496028  [   72/   89]
per-ex loss: 0.372929  [   73/   89]
per-ex loss: 0.396639  [   74/   89]
per-ex loss: 0.319595  [   75/   89]
per-ex loss: 0.325064  [   76/   89]
per-ex loss: 0.289353  [   77/   89]
per-ex loss: 0.556253  [   78/   89]
per-ex loss: 0.363308  [   79/   89]
per-ex loss: 0.465491  [   80/   89]
per-ex loss: 0.278660  [   81/   89]
per-ex loss: 0.474677  [   82/   89]
per-ex loss: 0.383160  [   83/   89]
per-ex loss: 0.999951  [   84/   89]
per-ex loss: 0.442238  [   85/   89]
per-ex loss: 0.480904  [   86/   89]
per-ex loss: 0.362389  [   87/   89]
per-ex loss: 0.337060  [   88/   89]
per-ex loss: 0.353746  [   89/   89]
Train Error: Avg loss: 0.36912467
validation Error: 
 Avg loss: 0.60956196 
 F1: 0.421692 
 Precision: 0.374418 
 Recall: 0.482628
 IoU: 0.267180

test Error: 
 Avg loss: 0.58859045 
 F1: 0.451243 
 Precision: 0.359908 
 Recall: 0.604702
 IoU: 0.291358

We have finished training iteration 174
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_172_.pth
per-ex loss: 0.183677  [    1/   89]
per-ex loss: 0.381395  [    2/   89]
per-ex loss: 0.702047  [    3/   89]
per-ex loss: 0.352818  [    4/   89]
per-ex loss: 0.536282  [    5/   89]
per-ex loss: 0.439243  [    6/   89]
per-ex loss: 0.373896  [    7/   89]
per-ex loss: 0.500831  [    8/   89]
per-ex loss: 0.252690  [    9/   89]
per-ex loss: 0.240734  [   10/   89]
per-ex loss: 0.279328  [   11/   89]
per-ex loss: 0.330603  [   12/   89]
per-ex loss: 0.312098  [   13/   89]
per-ex loss: 0.351255  [   14/   89]
per-ex loss: 0.326379  [   15/   89]
per-ex loss: 0.337066  [   16/   89]
per-ex loss: 0.332285  [   17/   89]
per-ex loss: 0.306745  [   18/   89]
per-ex loss: 0.275245  [   19/   89]
per-ex loss: 0.506190  [   20/   89]
per-ex loss: 0.323688  [   21/   89]
per-ex loss: 0.359027  [   22/   89]
per-ex loss: 0.337643  [   23/   89]
per-ex loss: 0.284299  [   24/   89]
per-ex loss: 0.484719  [   25/   89]
per-ex loss: 0.405973  [   26/   89]
per-ex loss: 0.514987  [   27/   89]
per-ex loss: 0.328949  [   28/   89]
per-ex loss: 0.281056  [   29/   89]
per-ex loss: 0.212863  [   30/   89]
per-ex loss: 0.354295  [   31/   89]
per-ex loss: 0.563567  [   32/   89]
per-ex loss: 0.486867  [   33/   89]
per-ex loss: 0.405725  [   34/   89]
per-ex loss: 0.356516  [   35/   89]
per-ex loss: 0.428946  [   36/   89]
per-ex loss: 0.269380  [   37/   89]
per-ex loss: 0.337164  [   38/   89]
per-ex loss: 0.328659  [   39/   89]
per-ex loss: 0.428264  [   40/   89]
per-ex loss: 0.337759  [   41/   89]
per-ex loss: 0.280041  [   42/   89]
per-ex loss: 0.244917  [   43/   89]
per-ex loss: 0.424666  [   44/   89]
per-ex loss: 0.290128  [   45/   89]
per-ex loss: 0.367869  [   46/   89]
per-ex loss: 0.591622  [   47/   89]
per-ex loss: 0.498926  [   48/   89]
per-ex loss: 0.246515  [   49/   89]
per-ex loss: 0.311581  [   50/   89]
per-ex loss: 0.298330  [   51/   89]
per-ex loss: 0.332875  [   52/   89]
per-ex loss: 0.297085  [   53/   89]
per-ex loss: 0.486270  [   54/   89]
per-ex loss: 0.281854  [   55/   89]
per-ex loss: 0.272242  [   56/   89]
per-ex loss: 0.272735  [   57/   89]
per-ex loss: 0.239940  [   58/   89]
per-ex loss: 0.284895  [   59/   89]
per-ex loss: 0.276309  [   60/   89]
per-ex loss: 0.450503  [   61/   89]
per-ex loss: 0.286272  [   62/   89]
per-ex loss: 0.303366  [   63/   89]
per-ex loss: 0.324606  [   64/   89]
per-ex loss: 0.631538  [   65/   89]
per-ex loss: 0.456950  [   66/   89]
per-ex loss: 0.327352  [   67/   89]
per-ex loss: 0.430228  [   68/   89]
per-ex loss: 0.999937  [   69/   89]
per-ex loss: 0.452799  [   70/   89]
per-ex loss: 0.238647  [   71/   89]
per-ex loss: 0.366341  [   72/   89]
per-ex loss: 0.404860  [   73/   89]
per-ex loss: 0.404769  [   74/   89]
per-ex loss: 0.498051  [   75/   89]
per-ex loss: 0.229992  [   76/   89]
per-ex loss: 0.363367  [   77/   89]
per-ex loss: 0.207596  [   78/   89]
per-ex loss: 0.209219  [   79/   89]
per-ex loss: 0.400551  [   80/   89]
per-ex loss: 0.461829  [   81/   89]
per-ex loss: 0.430363  [   82/   89]
per-ex loss: 0.376339  [   83/   89]
per-ex loss: 0.371916  [   84/   89]
per-ex loss: 0.390255  [   85/   89]
per-ex loss: 0.551726  [   86/   89]
per-ex loss: 0.342622  [   87/   89]
per-ex loss: 0.400007  [   88/   89]
per-ex loss: 0.301090  [   89/   89]
Train Error: Avg loss: 0.37149487
validation Error: 
 Avg loss: 0.54142672 
 F1: 0.479743 
 Precision: 0.470468 
 Recall: 0.489390
 IoU: 0.315567

test Error: 
 Avg loss: 0.50570285 
 F1: 0.534562 
 Precision: 0.500634 
 Recall: 0.573422
 IoU: 0.364779

We have finished training iteration 175
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_173_.pth
per-ex loss: 0.297155  [    1/   89]
per-ex loss: 0.398899  [    2/   89]
per-ex loss: 0.565257  [    3/   89]
per-ex loss: 0.365204  [    4/   89]
per-ex loss: 0.441278  [    5/   89]
per-ex loss: 0.279183  [    6/   89]
per-ex loss: 0.177957  [    7/   89]
per-ex loss: 0.416274  [    8/   89]
per-ex loss: 0.264896  [    9/   89]
per-ex loss: 0.205217  [   10/   89]
per-ex loss: 0.497047  [   11/   89]
per-ex loss: 0.380777  [   12/   89]
per-ex loss: 0.514112  [   13/   89]
per-ex loss: 0.265812  [   14/   89]
per-ex loss: 0.283844  [   15/   89]
per-ex loss: 0.186695  [   16/   89]
per-ex loss: 0.331985  [   17/   89]
per-ex loss: 0.357303  [   18/   89]
per-ex loss: 0.323073  [   19/   89]
per-ex loss: 0.508079  [   20/   89]
per-ex loss: 0.363105  [   21/   89]
per-ex loss: 0.346202  [   22/   89]
per-ex loss: 0.255726  [   23/   89]
per-ex loss: 0.240740  [   24/   89]
per-ex loss: 0.228001  [   25/   89]
per-ex loss: 0.349545  [   26/   89]
per-ex loss: 0.314270  [   27/   89]
per-ex loss: 0.308566  [   28/   89]
per-ex loss: 0.699381  [   29/   89]
per-ex loss: 0.453557  [   30/   89]
per-ex loss: 0.365742  [   31/   89]
per-ex loss: 0.479448  [   32/   89]
per-ex loss: 0.372646  [   33/   89]
per-ex loss: 0.441345  [   34/   89]
per-ex loss: 0.326838  [   35/   89]
per-ex loss: 0.292792  [   36/   89]
per-ex loss: 0.241031  [   37/   89]
per-ex loss: 0.382724  [   38/   89]
per-ex loss: 0.497360  [   39/   89]
per-ex loss: 0.420953  [   40/   89]
per-ex loss: 0.366385  [   41/   89]
per-ex loss: 0.270465  [   42/   89]
per-ex loss: 0.386819  [   43/   89]
per-ex loss: 0.515076  [   44/   89]
per-ex loss: 0.292457  [   45/   89]
per-ex loss: 0.330306  [   46/   89]
per-ex loss: 0.307582  [   47/   89]
per-ex loss: 0.293519  [   48/   89]
per-ex loss: 0.319164  [   49/   89]
per-ex loss: 0.338017  [   50/   89]
per-ex loss: 0.346794  [   51/   89]
per-ex loss: 0.277271  [   52/   89]
per-ex loss: 0.234582  [   53/   89]
per-ex loss: 0.419967  [   54/   89]
per-ex loss: 0.357512  [   55/   89]
per-ex loss: 0.361906  [   56/   89]
per-ex loss: 0.553080  [   57/   89]
per-ex loss: 0.530520  [   58/   89]
per-ex loss: 0.431172  [   59/   89]
per-ex loss: 0.423582  [   60/   89]
per-ex loss: 0.334883  [   61/   89]
per-ex loss: 0.311651  [   62/   89]
per-ex loss: 0.244531  [   63/   89]
per-ex loss: 0.513279  [   64/   89]
per-ex loss: 0.286252  [   65/   89]
per-ex loss: 0.577140  [   66/   89]
per-ex loss: 0.357408  [   67/   89]
per-ex loss: 0.202683  [   68/   89]
per-ex loss: 0.294882  [   69/   89]
per-ex loss: 0.429572  [   70/   89]
per-ex loss: 0.587047  [   71/   89]
per-ex loss: 0.326421  [   72/   89]
per-ex loss: 0.261476  [   73/   89]
per-ex loss: 0.369003  [   74/   89]
per-ex loss: 0.348832  [   75/   89]
per-ex loss: 0.412722  [   76/   89]
per-ex loss: 0.419931  [   77/   89]
per-ex loss: 0.307127  [   78/   89]
per-ex loss: 0.269688  [   79/   89]
per-ex loss: 0.303879  [   80/   89]
per-ex loss: 0.275550  [   81/   89]
per-ex loss: 0.597035  [   82/   89]
per-ex loss: 0.213534  [   83/   89]
per-ex loss: 0.416052  [   84/   89]
per-ex loss: 0.300966  [   85/   89]
per-ex loss: 0.999879  [   86/   89]
per-ex loss: 0.422151  [   87/   89]
per-ex loss: 0.404526  [   88/   89]
per-ex loss: 0.430147  [   89/   89]
Train Error: Avg loss: 0.37094875
validation Error: 
 Avg loss: 0.63334620 
 F1: 0.395395 
 Precision: 0.325858 
 Recall: 0.502663
 IoU: 0.246413

test Error: 
 Avg loss: 0.60881919 
 F1: 0.428179 
 Precision: 0.321392 
 Recall: 0.641239
 IoU: 0.272409

We have finished training iteration 176
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_174_.pth
per-ex loss: 0.412086  [    1/   89]
per-ex loss: 0.297291  [    2/   89]
per-ex loss: 0.503474  [    3/   89]
per-ex loss: 0.305160  [    4/   89]
per-ex loss: 0.209612  [    5/   89]
per-ex loss: 0.999950  [    6/   89]
per-ex loss: 0.562332  [    7/   89]
per-ex loss: 0.474223  [    8/   89]
per-ex loss: 0.387319  [    9/   89]
per-ex loss: 0.312748  [   10/   89]
per-ex loss: 0.278120  [   11/   89]
per-ex loss: 0.483249  [   12/   89]
per-ex loss: 0.357419  [   13/   89]
per-ex loss: 0.542519  [   14/   89]
per-ex loss: 0.359631  [   15/   89]
per-ex loss: 0.313389  [   16/   89]
per-ex loss: 0.382175  [   17/   89]
per-ex loss: 0.333863  [   18/   89]
per-ex loss: 0.192293  [   19/   89]
per-ex loss: 0.332730  [   20/   89]
per-ex loss: 0.276618  [   21/   89]
per-ex loss: 0.317332  [   22/   89]
per-ex loss: 0.429688  [   23/   89]
per-ex loss: 0.309559  [   24/   89]
per-ex loss: 0.169453  [   25/   89]
per-ex loss: 0.259098  [   26/   89]
per-ex loss: 0.350895  [   27/   89]
per-ex loss: 0.279727  [   28/   89]
per-ex loss: 0.280166  [   29/   89]
per-ex loss: 0.367850  [   30/   89]
per-ex loss: 0.283066  [   31/   89]
per-ex loss: 0.382162  [   32/   89]
per-ex loss: 0.354581  [   33/   89]
per-ex loss: 0.272621  [   34/   89]
per-ex loss: 0.332934  [   35/   89]
per-ex loss: 0.223371  [   36/   89]
per-ex loss: 0.252917  [   37/   89]
per-ex loss: 0.251991  [   38/   89]
per-ex loss: 0.461010  [   39/   89]
per-ex loss: 0.198028  [   40/   89]
per-ex loss: 0.348252  [   41/   89]
per-ex loss: 0.510368  [   42/   89]
per-ex loss: 0.340715  [   43/   89]
per-ex loss: 0.481719  [   44/   89]
per-ex loss: 0.350604  [   45/   89]
per-ex loss: 0.358983  [   46/   89]
per-ex loss: 0.218234  [   47/   89]
per-ex loss: 0.284973  [   48/   89]
per-ex loss: 0.395112  [   49/   89]
per-ex loss: 0.356228  [   50/   89]
per-ex loss: 0.233363  [   51/   89]
per-ex loss: 0.303230  [   52/   89]
per-ex loss: 0.275154  [   53/   89]
per-ex loss: 0.522479  [   54/   89]
per-ex loss: 0.416371  [   55/   89]
per-ex loss: 0.393327  [   56/   89]
per-ex loss: 0.308799  [   57/   89]
per-ex loss: 0.395080  [   58/   89]
per-ex loss: 0.303846  [   59/   89]
per-ex loss: 0.275689  [   60/   89]
per-ex loss: 0.512540  [   61/   89]
per-ex loss: 0.308614  [   62/   89]
per-ex loss: 0.475036  [   63/   89]
per-ex loss: 0.481937  [   64/   89]
per-ex loss: 0.139599  [   65/   89]
per-ex loss: 0.555643  [   66/   89]
per-ex loss: 0.302402  [   67/   89]
per-ex loss: 0.264619  [   68/   89]
per-ex loss: 0.370977  [   69/   89]
per-ex loss: 0.566798  [   70/   89]
per-ex loss: 0.256288  [   71/   89]
per-ex loss: 0.345036  [   72/   89]
per-ex loss: 0.272533  [   73/   89]
per-ex loss: 0.439162  [   74/   89]
per-ex loss: 0.523547  [   75/   89]
per-ex loss: 0.330311  [   76/   89]
per-ex loss: 0.353133  [   77/   89]
per-ex loss: 0.240519  [   78/   89]
per-ex loss: 0.552688  [   79/   89]
per-ex loss: 0.253180  [   80/   89]
per-ex loss: 0.256942  [   81/   89]
per-ex loss: 0.448094  [   82/   89]
per-ex loss: 0.277064  [   83/   89]
per-ex loss: 0.499724  [   84/   89]
per-ex loss: 0.329643  [   85/   89]
per-ex loss: 0.445892  [   86/   89]
per-ex loss: 0.245985  [   87/   89]
per-ex loss: 0.337981  [   88/   89]
per-ex loss: 0.401510  [   89/   89]
Train Error: Avg loss: 0.35901764
validation Error: 
 Avg loss: 0.55344328 
 F1: 0.468589 
 Precision: 0.454362 
 Recall: 0.483736
 IoU: 0.305985

test Error: 
 Avg loss: 0.50808834 
 F1: 0.534095 
 Precision: 0.509631 
 Recall: 0.561026
 IoU: 0.364345

We have finished training iteration 177
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_175_.pth
per-ex loss: 0.315645  [    1/   89]
per-ex loss: 0.484605  [    2/   89]
per-ex loss: 0.221432  [    3/   89]
per-ex loss: 0.305171  [    4/   89]
per-ex loss: 0.539145  [    5/   89]
per-ex loss: 0.276521  [    6/   89]
per-ex loss: 0.227609  [    7/   89]
per-ex loss: 0.513669  [    8/   89]
per-ex loss: 0.471384  [    9/   89]
per-ex loss: 0.396728  [   10/   89]
per-ex loss: 0.367636  [   11/   89]
per-ex loss: 0.276953  [   12/   89]
per-ex loss: 0.281446  [   13/   89]
per-ex loss: 0.293735  [   14/   89]
per-ex loss: 0.354099  [   15/   89]
per-ex loss: 0.329116  [   16/   89]
per-ex loss: 0.569899  [   17/   89]
per-ex loss: 0.309263  [   18/   89]
per-ex loss: 0.200836  [   19/   89]
per-ex loss: 0.332558  [   20/   89]
per-ex loss: 0.402678  [   21/   89]
per-ex loss: 0.416758  [   22/   89]
per-ex loss: 0.273417  [   23/   89]
per-ex loss: 0.556481  [   24/   89]
per-ex loss: 0.291227  [   25/   89]
per-ex loss: 0.704053  [   26/   89]
per-ex loss: 0.278092  [   27/   89]
per-ex loss: 0.343289  [   28/   89]
per-ex loss: 0.223588  [   29/   89]
per-ex loss: 0.554773  [   30/   89]
per-ex loss: 0.334045  [   31/   89]
per-ex loss: 0.272233  [   32/   89]
per-ex loss: 0.239076  [   33/   89]
per-ex loss: 0.351149  [   34/   89]
per-ex loss: 0.340457  [   35/   89]
per-ex loss: 0.415323  [   36/   89]
per-ex loss: 0.999879  [   37/   89]
per-ex loss: 0.240909  [   38/   89]
per-ex loss: 0.304904  [   39/   89]
per-ex loss: 0.289987  [   40/   89]
per-ex loss: 0.321313  [   41/   89]
per-ex loss: 0.488926  [   42/   89]
per-ex loss: 0.507227  [   43/   89]
per-ex loss: 0.289091  [   44/   89]
per-ex loss: 0.456883  [   45/   89]
per-ex loss: 0.303468  [   46/   89]
per-ex loss: 0.312234  [   47/   89]
per-ex loss: 0.248070  [   48/   89]
per-ex loss: 0.309638  [   49/   89]
per-ex loss: 0.213446  [   50/   89]
per-ex loss: 0.240029  [   51/   89]
per-ex loss: 0.431957  [   52/   89]
per-ex loss: 0.417970  [   53/   89]
per-ex loss: 0.183105  [   54/   89]
per-ex loss: 0.383615  [   55/   89]
per-ex loss: 0.542174  [   56/   89]
per-ex loss: 0.322851  [   57/   89]
per-ex loss: 0.386223  [   58/   89]
per-ex loss: 0.493957  [   59/   89]
per-ex loss: 0.250672  [   60/   89]
per-ex loss: 0.348561  [   61/   89]
per-ex loss: 0.297651  [   62/   89]
per-ex loss: 0.319710  [   63/   89]
per-ex loss: 0.515092  [   64/   89]
per-ex loss: 0.458675  [   65/   89]
per-ex loss: 0.300191  [   66/   89]
per-ex loss: 0.375309  [   67/   89]
per-ex loss: 0.495623  [   68/   89]
per-ex loss: 0.247078  [   69/   89]
per-ex loss: 0.478224  [   70/   89]
per-ex loss: 0.250497  [   71/   89]
per-ex loss: 0.363647  [   72/   89]
per-ex loss: 0.184269  [   73/   89]
per-ex loss: 0.358928  [   74/   89]
per-ex loss: 0.338595  [   75/   89]
per-ex loss: 0.504492  [   76/   89]
per-ex loss: 0.305026  [   77/   89]
per-ex loss: 0.292471  [   78/   89]
per-ex loss: 0.330565  [   79/   89]
per-ex loss: 0.441025  [   80/   89]
per-ex loss: 0.318164  [   81/   89]
per-ex loss: 0.390512  [   82/   89]
per-ex loss: 0.321154  [   83/   89]
per-ex loss: 0.274408  [   84/   89]
per-ex loss: 0.273904  [   85/   89]
per-ex loss: 0.305237  [   86/   89]
per-ex loss: 0.299952  [   87/   89]
per-ex loss: 0.282518  [   88/   89]
per-ex loss: 0.450565  [   89/   89]
Train Error: Avg loss: 0.36095118
validation Error: 
 Avg loss: 0.53880161 
 F1: 0.489728 
 Precision: 0.520539 
 Recall: 0.462360
 IoU: 0.324265

test Error: 
 Avg loss: 0.49158812 
 F1: 0.554804 
 Precision: 0.572714 
 Recall: 0.537981
 IoU: 0.383895

We have finished training iteration 178
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_176_.pth
per-ex loss: 0.469456  [    1/   89]
per-ex loss: 0.283163  [    2/   89]
per-ex loss: 0.228331  [    3/   89]
per-ex loss: 0.293878  [    4/   89]
per-ex loss: 0.420934  [    5/   89]
per-ex loss: 0.312539  [    6/   89]
per-ex loss: 0.501555  [    7/   89]
per-ex loss: 0.290868  [    8/   89]
per-ex loss: 0.269064  [    9/   89]
per-ex loss: 0.441068  [   10/   89]
per-ex loss: 0.466751  [   11/   89]
per-ex loss: 0.307768  [   12/   89]
per-ex loss: 0.343883  [   13/   89]
per-ex loss: 0.359716  [   14/   89]
per-ex loss: 0.529976  [   15/   89]
per-ex loss: 0.584753  [   16/   89]
per-ex loss: 0.304983  [   17/   89]
per-ex loss: 0.339312  [   18/   89]
per-ex loss: 0.340396  [   19/   89]
per-ex loss: 0.298089  [   20/   89]
per-ex loss: 0.363790  [   21/   89]
per-ex loss: 0.338388  [   22/   89]
per-ex loss: 0.439455  [   23/   89]
per-ex loss: 0.308575  [   24/   89]
per-ex loss: 0.268755  [   25/   89]
per-ex loss: 0.337670  [   26/   89]
per-ex loss: 0.168292  [   27/   89]
per-ex loss: 0.406703  [   28/   89]
per-ex loss: 0.531039  [   29/   89]
per-ex loss: 0.382105  [   30/   89]
per-ex loss: 0.402338  [   31/   89]
per-ex loss: 0.155930  [   32/   89]
per-ex loss: 0.519806  [   33/   89]
per-ex loss: 0.300817  [   34/   89]
per-ex loss: 0.365978  [   35/   89]
per-ex loss: 0.519214  [   36/   89]
per-ex loss: 0.407156  [   37/   89]
per-ex loss: 0.341139  [   38/   89]
per-ex loss: 0.366760  [   39/   89]
per-ex loss: 0.293009  [   40/   89]
per-ex loss: 0.293815  [   41/   89]
per-ex loss: 0.261393  [   42/   89]
per-ex loss: 0.367627  [   43/   89]
per-ex loss: 0.592490  [   44/   89]
per-ex loss: 0.310823  [   45/   89]
per-ex loss: 0.244068  [   46/   89]
per-ex loss: 0.192388  [   47/   89]
per-ex loss: 0.287460  [   48/   89]
per-ex loss: 0.547326  [   49/   89]
per-ex loss: 0.285452  [   50/   89]
per-ex loss: 0.531634  [   51/   89]
per-ex loss: 0.561711  [   52/   89]
per-ex loss: 0.360720  [   53/   89]
per-ex loss: 0.349294  [   54/   89]
per-ex loss: 0.535096  [   55/   89]
per-ex loss: 0.448984  [   56/   89]
per-ex loss: 0.351141  [   57/   89]
per-ex loss: 0.300968  [   58/   89]
per-ex loss: 0.339157  [   59/   89]
per-ex loss: 0.324155  [   60/   89]
per-ex loss: 0.338595  [   61/   89]
per-ex loss: 0.489871  [   62/   89]
per-ex loss: 0.308543  [   63/   89]
per-ex loss: 0.283572  [   64/   89]
per-ex loss: 0.298081  [   65/   89]
per-ex loss: 0.405088  [   66/   89]
per-ex loss: 0.526096  [   67/   89]
per-ex loss: 0.305071  [   68/   89]
per-ex loss: 0.182165  [   69/   89]
per-ex loss: 0.567966  [   70/   89]
per-ex loss: 0.328234  [   71/   89]
per-ex loss: 0.201688  [   72/   89]
per-ex loss: 0.512831  [   73/   89]
per-ex loss: 0.389102  [   74/   89]
per-ex loss: 0.345575  [   75/   89]
per-ex loss: 0.419882  [   76/   89]
per-ex loss: 0.270099  [   77/   89]
per-ex loss: 0.392994  [   78/   89]
per-ex loss: 0.255031  [   79/   89]
per-ex loss: 0.282552  [   80/   89]
per-ex loss: 0.231160  [   81/   89]
per-ex loss: 0.999926  [   82/   89]
per-ex loss: 0.252773  [   83/   89]
per-ex loss: 0.320499  [   84/   89]
per-ex loss: 0.231564  [   85/   89]
per-ex loss: 0.510364  [   86/   89]
per-ex loss: 0.309234  [   87/   89]
per-ex loss: 0.305910  [   88/   89]
per-ex loss: 0.380014  [   89/   89]
Train Error: Avg loss: 0.36779308
validation Error: 
 Avg loss: 0.58285193 
 F1: 0.449870 
 Precision: 0.466935 
 Recall: 0.434009
 IoU: 0.290214

test Error: 
 Avg loss: 0.55675659 
 F1: 0.490988 
 Precision: 0.447364 
 Recall: 0.544039
 IoU: 0.325371

We have finished training iteration 179
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_177_.pth
per-ex loss: 0.283820  [    1/   89]
per-ex loss: 0.425241  [    2/   89]
per-ex loss: 0.247085  [    3/   89]
per-ex loss: 0.138611  [    4/   89]
per-ex loss: 0.441370  [    5/   89]
per-ex loss: 0.331820  [    6/   89]
per-ex loss: 0.480487  [    7/   89]
per-ex loss: 0.286330  [    8/   89]
per-ex loss: 0.314724  [    9/   89]
per-ex loss: 0.342314  [   10/   89]
per-ex loss: 0.253743  [   11/   89]
per-ex loss: 0.484408  [   12/   89]
per-ex loss: 0.439972  [   13/   89]
per-ex loss: 0.437213  [   14/   89]
per-ex loss: 0.330860  [   15/   89]
per-ex loss: 0.342432  [   16/   89]
per-ex loss: 0.431140  [   17/   89]
per-ex loss: 0.287225  [   18/   89]
per-ex loss: 0.439834  [   19/   89]
per-ex loss: 0.486727  [   20/   89]
per-ex loss: 0.317202  [   21/   89]
per-ex loss: 0.334829  [   22/   89]
per-ex loss: 0.463834  [   23/   89]
per-ex loss: 0.503317  [   24/   89]
per-ex loss: 0.415316  [   25/   89]
per-ex loss: 0.290236  [   26/   89]
per-ex loss: 0.375170  [   27/   89]
per-ex loss: 0.259357  [   28/   89]
per-ex loss: 0.999937  [   29/   89]
per-ex loss: 0.580783  [   30/   89]
per-ex loss: 0.508852  [   31/   89]
per-ex loss: 0.396730  [   32/   89]
per-ex loss: 0.282883  [   33/   89]
per-ex loss: 0.308672  [   34/   89]
per-ex loss: 0.309754  [   35/   89]
per-ex loss: 0.309838  [   36/   89]
per-ex loss: 0.505199  [   37/   89]
per-ex loss: 0.387274  [   38/   89]
per-ex loss: 0.637498  [   39/   89]
per-ex loss: 0.230417  [   40/   89]
per-ex loss: 0.312125  [   41/   89]
per-ex loss: 0.535733  [   42/   89]
per-ex loss: 0.339427  [   43/   89]
per-ex loss: 0.342129  [   44/   89]
per-ex loss: 0.400348  [   45/   89]
per-ex loss: 0.278976  [   46/   89]
per-ex loss: 0.326992  [   47/   89]
per-ex loss: 0.419087  [   48/   89]
per-ex loss: 0.283638  [   49/   89]
per-ex loss: 0.345138  [   50/   89]
per-ex loss: 0.330583  [   51/   89]
per-ex loss: 0.455786  [   52/   89]
per-ex loss: 0.226492  [   53/   89]
per-ex loss: 0.343415  [   54/   89]
per-ex loss: 0.459851  [   55/   89]
per-ex loss: 0.560165  [   56/   89]
per-ex loss: 0.243998  [   57/   89]
per-ex loss: 0.508835  [   58/   89]
per-ex loss: 0.343932  [   59/   89]
per-ex loss: 0.304502  [   60/   89]
per-ex loss: 0.287656  [   61/   89]
per-ex loss: 0.311534  [   62/   89]
per-ex loss: 0.240281  [   63/   89]
per-ex loss: 0.556653  [   64/   89]
per-ex loss: 0.272167  [   65/   89]
per-ex loss: 0.307188  [   66/   89]
per-ex loss: 0.297736  [   67/   89]
per-ex loss: 0.299597  [   68/   89]
per-ex loss: 0.245533  [   69/   89]
per-ex loss: 0.208040  [   70/   89]
per-ex loss: 0.436328  [   71/   89]
per-ex loss: 0.344552  [   72/   89]
per-ex loss: 0.567914  [   73/   89]
per-ex loss: 0.176007  [   74/   89]
per-ex loss: 0.393387  [   75/   89]
per-ex loss: 0.474549  [   76/   89]
per-ex loss: 0.230641  [   77/   89]
per-ex loss: 0.423908  [   78/   89]
per-ex loss: 0.288947  [   79/   89]
per-ex loss: 0.289550  [   80/   89]
per-ex loss: 0.300351  [   81/   89]
per-ex loss: 0.318505  [   82/   89]
per-ex loss: 0.350667  [   83/   89]
per-ex loss: 0.318417  [   84/   89]
per-ex loss: 0.515986  [   85/   89]
per-ex loss: 0.407655  [   86/   89]
per-ex loss: 0.336944  [   87/   89]
per-ex loss: 0.341411  [   88/   89]
per-ex loss: 0.214342  [   89/   89]
Train Error: Avg loss: 0.36809049
validation Error: 
 Avg loss: 0.53217931 
 F1: 0.495282 
 Precision: 0.559666 
 Recall: 0.444184
 IoU: 0.329153

test Error: 
 Avg loss: 0.49384370 
 F1: 0.553171 
 Precision: 0.609136 
 Recall: 0.506624
 IoU: 0.382333

We have finished training iteration 180
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_178_.pth
per-ex loss: 0.253453  [    1/   89]
per-ex loss: 0.397363  [    2/   89]
per-ex loss: 0.326263  [    3/   89]
per-ex loss: 0.427199  [    4/   89]
per-ex loss: 0.345327  [    5/   89]
per-ex loss: 0.524554  [    6/   89]
per-ex loss: 0.230149  [    7/   89]
per-ex loss: 0.371413  [    8/   89]
per-ex loss: 0.999956  [    9/   89]
per-ex loss: 0.506780  [   10/   89]
per-ex loss: 0.439657  [   11/   89]
per-ex loss: 0.460447  [   12/   89]
per-ex loss: 0.266442  [   13/   89]
per-ex loss: 0.318710  [   14/   89]
per-ex loss: 0.536012  [   15/   89]
per-ex loss: 0.272145  [   16/   89]
per-ex loss: 0.282947  [   17/   89]
per-ex loss: 0.384664  [   18/   89]
per-ex loss: 0.221446  [   19/   89]
per-ex loss: 0.330987  [   20/   89]
per-ex loss: 0.266778  [   21/   89]
per-ex loss: 0.409196  [   22/   89]
per-ex loss: 0.575757  [   23/   89]
per-ex loss: 0.350393  [   24/   89]
per-ex loss: 0.312773  [   25/   89]
per-ex loss: 0.280428  [   26/   89]
per-ex loss: 0.444878  [   27/   89]
per-ex loss: 0.332660  [   28/   89]
per-ex loss: 0.408481  [   29/   89]
per-ex loss: 0.293502  [   30/   89]
per-ex loss: 0.212186  [   31/   89]
per-ex loss: 0.446728  [   32/   89]
per-ex loss: 0.244673  [   33/   89]
per-ex loss: 0.573571  [   34/   89]
per-ex loss: 0.279909  [   35/   89]
per-ex loss: 0.534285  [   36/   89]
per-ex loss: 0.337823  [   37/   89]
per-ex loss: 0.239032  [   38/   89]
per-ex loss: 0.371584  [   39/   89]
per-ex loss: 0.336241  [   40/   89]
per-ex loss: 0.552687  [   41/   89]
per-ex loss: 0.486773  [   42/   89]
per-ex loss: 0.269025  [   43/   89]
per-ex loss: 0.249398  [   44/   89]
per-ex loss: 0.330066  [   45/   89]
per-ex loss: 0.315736  [   46/   89]
per-ex loss: 0.181921  [   47/   89]
per-ex loss: 0.521462  [   48/   89]
per-ex loss: 0.281447  [   49/   89]
per-ex loss: 0.458948  [   50/   89]
per-ex loss: 0.322277  [   51/   89]
per-ex loss: 0.296641  [   52/   89]
per-ex loss: 0.348915  [   53/   89]
per-ex loss: 0.277168  [   54/   89]
per-ex loss: 0.264397  [   55/   89]
per-ex loss: 0.354012  [   56/   89]
per-ex loss: 0.362963  [   57/   89]
per-ex loss: 0.333475  [   58/   89]
per-ex loss: 0.387167  [   59/   89]
per-ex loss: 0.233386  [   60/   89]
per-ex loss: 0.293773  [   61/   89]
per-ex loss: 0.322818  [   62/   89]
per-ex loss: 0.336771  [   63/   89]
per-ex loss: 0.174677  [   64/   89]
per-ex loss: 0.270620  [   65/   89]
per-ex loss: 0.399487  [   66/   89]
per-ex loss: 0.335151  [   67/   89]
per-ex loss: 0.337307  [   68/   89]
per-ex loss: 0.313809  [   69/   89]
per-ex loss: 0.431695  [   70/   89]
per-ex loss: 0.285094  [   71/   89]
per-ex loss: 0.506150  [   72/   89]
per-ex loss: 0.368505  [   73/   89]
per-ex loss: 0.310503  [   74/   89]
per-ex loss: 0.334635  [   75/   89]
per-ex loss: 0.436180  [   76/   89]
per-ex loss: 0.297109  [   77/   89]
per-ex loss: 0.317933  [   78/   89]
per-ex loss: 0.504992  [   79/   89]
per-ex loss: 0.400355  [   80/   89]
per-ex loss: 0.373417  [   81/   89]
per-ex loss: 0.175544  [   82/   89]
per-ex loss: 0.315027  [   83/   89]
per-ex loss: 0.567499  [   84/   89]
per-ex loss: 0.571769  [   85/   89]
per-ex loss: 0.323183  [   86/   89]
per-ex loss: 0.478193  [   87/   89]
per-ex loss: 0.290833  [   88/   89]
per-ex loss: 0.329312  [   89/   89]
Train Error: Avg loss: 0.36378646
validation Error: 
 Avg loss: 0.53586156 
 F1: 0.495447 
 Precision: 0.524138 
 Recall: 0.469735
 IoU: 0.329299

test Error: 
 Avg loss: 0.48112468 
 F1: 0.565614 
 Precision: 0.590980 
 Recall: 0.542336
 IoU: 0.394325

We have finished training iteration 181
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_179_.pth
per-ex loss: 0.223795  [    1/   89]
per-ex loss: 0.528978  [    2/   89]
per-ex loss: 0.394297  [    3/   89]
per-ex loss: 0.190127  [    4/   89]
per-ex loss: 0.288961  [    5/   89]
per-ex loss: 0.256098  [    6/   89]
per-ex loss: 0.438821  [    7/   89]
per-ex loss: 0.239317  [    8/   89]
per-ex loss: 0.158043  [    9/   89]
per-ex loss: 0.313640  [   10/   89]
per-ex loss: 0.315660  [   11/   89]
per-ex loss: 0.216240  [   12/   89]
per-ex loss: 0.322302  [   13/   89]
per-ex loss: 0.293855  [   14/   89]
per-ex loss: 0.205817  [   15/   89]
per-ex loss: 0.280736  [   16/   89]
per-ex loss: 0.578326  [   17/   89]
per-ex loss: 0.320743  [   18/   89]
per-ex loss: 0.244472  [   19/   89]
per-ex loss: 0.337586  [   20/   89]
per-ex loss: 0.999879  [   21/   89]
per-ex loss: 0.262451  [   22/   89]
per-ex loss: 0.311125  [   23/   89]
per-ex loss: 0.500573  [   24/   89]
per-ex loss: 0.302452  [   25/   89]
per-ex loss: 0.302324  [   26/   89]
per-ex loss: 0.365258  [   27/   89]
per-ex loss: 0.316963  [   28/   89]
per-ex loss: 0.352566  [   29/   89]
per-ex loss: 0.310273  [   30/   89]
per-ex loss: 0.480723  [   31/   89]
per-ex loss: 0.223374  [   32/   89]
per-ex loss: 0.480651  [   33/   89]
per-ex loss: 0.378060  [   34/   89]
per-ex loss: 0.607483  [   35/   89]
per-ex loss: 0.286257  [   36/   89]
per-ex loss: 0.238005  [   37/   89]
per-ex loss: 0.230456  [   38/   89]
per-ex loss: 0.328844  [   39/   89]
per-ex loss: 0.321582  [   40/   89]
per-ex loss: 0.186658  [   41/   89]
per-ex loss: 0.266507  [   42/   89]
per-ex loss: 0.426945  [   43/   89]
per-ex loss: 0.276741  [   44/   89]
per-ex loss: 0.354077  [   45/   89]
per-ex loss: 0.360717  [   46/   89]
per-ex loss: 0.363444  [   47/   89]
per-ex loss: 0.253527  [   48/   89]
per-ex loss: 0.271261  [   49/   89]
per-ex loss: 0.352783  [   50/   89]
per-ex loss: 0.415653  [   51/   89]
per-ex loss: 0.357137  [   52/   89]
per-ex loss: 0.446869  [   53/   89]
per-ex loss: 0.335063  [   54/   89]
per-ex loss: 0.632596  [   55/   89]
per-ex loss: 0.439970  [   56/   89]
per-ex loss: 0.324514  [   57/   89]
per-ex loss: 0.540273  [   58/   89]
per-ex loss: 0.282661  [   59/   89]
per-ex loss: 0.533960  [   60/   89]
per-ex loss: 0.275728  [   61/   89]
per-ex loss: 0.175763  [   62/   89]
per-ex loss: 0.395264  [   63/   89]
per-ex loss: 0.252415  [   64/   89]
per-ex loss: 0.353177  [   65/   89]
per-ex loss: 0.296290  [   66/   89]
per-ex loss: 0.577701  [   67/   89]
per-ex loss: 0.296322  [   68/   89]
per-ex loss: 0.360617  [   69/   89]
per-ex loss: 0.372832  [   70/   89]
per-ex loss: 0.531006  [   71/   89]
per-ex loss: 0.274273  [   72/   89]
per-ex loss: 0.237071  [   73/   89]
per-ex loss: 0.491667  [   74/   89]
per-ex loss: 0.235151  [   75/   89]
per-ex loss: 0.377728  [   76/   89]
per-ex loss: 0.557396  [   77/   89]
per-ex loss: 0.330943  [   78/   89]
per-ex loss: 0.389831  [   79/   89]
per-ex loss: 0.354461  [   80/   89]
per-ex loss: 0.494488  [   81/   89]
per-ex loss: 0.298460  [   82/   89]
per-ex loss: 0.583151  [   83/   89]
per-ex loss: 0.275026  [   84/   89]
per-ex loss: 0.469057  [   85/   89]
per-ex loss: 0.470174  [   86/   89]
per-ex loss: 0.304265  [   87/   89]
per-ex loss: 0.290211  [   88/   89]
per-ex loss: 0.494429  [   89/   89]
Train Error: Avg loss: 0.35936368
validation Error: 
 Avg loss: 0.53378498 
 F1: 0.496597 
 Precision: 0.533288 
 Recall: 0.464630
 IoU: 0.330316

test Error: 
 Avg loss: 0.48265972 
 F1: 0.562306 
 Precision: 0.585737 
 Recall: 0.540677
 IoU: 0.391117

We have finished training iteration 182
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_180_.pth
per-ex loss: 0.303655  [    1/   89]
per-ex loss: 0.312106  [    2/   89]
per-ex loss: 0.255361  [    3/   89]
per-ex loss: 0.386898  [    4/   89]
per-ex loss: 0.179876  [    5/   89]
per-ex loss: 0.357601  [    6/   89]
per-ex loss: 0.352621  [    7/   89]
per-ex loss: 0.283855  [    8/   89]
per-ex loss: 0.427665  [    9/   89]
per-ex loss: 0.506493  [   10/   89]
per-ex loss: 0.406636  [   11/   89]
per-ex loss: 0.322723  [   12/   89]
per-ex loss: 0.396412  [   13/   89]
per-ex loss: 0.372235  [   14/   89]
per-ex loss: 0.356582  [   15/   89]
per-ex loss: 0.170398  [   16/   89]
per-ex loss: 0.416510  [   17/   89]
per-ex loss: 0.265636  [   18/   89]
per-ex loss: 0.492741  [   19/   89]
per-ex loss: 0.284562  [   20/   89]
per-ex loss: 0.403214  [   21/   89]
per-ex loss: 0.320435  [   22/   89]
per-ex loss: 0.534469  [   23/   89]
per-ex loss: 0.339286  [   24/   89]
per-ex loss: 0.288320  [   25/   89]
per-ex loss: 0.263146  [   26/   89]
per-ex loss: 0.215884  [   27/   89]
per-ex loss: 0.343313  [   28/   89]
per-ex loss: 0.274760  [   29/   89]
per-ex loss: 0.329739  [   30/   89]
per-ex loss: 0.428749  [   31/   89]
per-ex loss: 0.269107  [   32/   89]
per-ex loss: 0.469810  [   33/   89]
per-ex loss: 0.283916  [   34/   89]
per-ex loss: 0.303913  [   35/   89]
per-ex loss: 0.232488  [   36/   89]
per-ex loss: 0.286645  [   37/   89]
per-ex loss: 0.257763  [   38/   89]
per-ex loss: 0.333599  [   39/   89]
per-ex loss: 0.543995  [   40/   89]
per-ex loss: 0.256438  [   41/   89]
per-ex loss: 0.313968  [   42/   89]
per-ex loss: 0.141935  [   43/   89]
per-ex loss: 0.332542  [   44/   89]
per-ex loss: 0.489468  [   45/   89]
per-ex loss: 0.476799  [   46/   89]
per-ex loss: 0.279806  [   47/   89]
per-ex loss: 0.387848  [   48/   89]
per-ex loss: 0.332958  [   49/   89]
per-ex loss: 0.309564  [   50/   89]
per-ex loss: 0.415154  [   51/   89]
per-ex loss: 0.457617  [   52/   89]
per-ex loss: 0.223358  [   53/   89]
per-ex loss: 0.405427  [   54/   89]
per-ex loss: 0.419464  [   55/   89]
per-ex loss: 0.324400  [   56/   89]
per-ex loss: 0.491030  [   57/   89]
per-ex loss: 0.348927  [   58/   89]
per-ex loss: 0.333304  [   59/   89]
per-ex loss: 0.266444  [   60/   89]
per-ex loss: 0.308779  [   61/   89]
per-ex loss: 0.569642  [   62/   89]
per-ex loss: 0.306952  [   63/   89]
per-ex loss: 0.414566  [   64/   89]
per-ex loss: 0.389148  [   65/   89]
per-ex loss: 0.538207  [   66/   89]
per-ex loss: 0.325102  [   67/   89]
per-ex loss: 0.556101  [   68/   89]
per-ex loss: 0.376722  [   69/   89]
per-ex loss: 0.591760  [   70/   89]
per-ex loss: 0.231695  [   71/   89]
per-ex loss: 0.257408  [   72/   89]
per-ex loss: 0.219269  [   73/   89]
per-ex loss: 0.306336  [   74/   89]
per-ex loss: 0.424489  [   75/   89]
per-ex loss: 0.258772  [   76/   89]
per-ex loss: 0.378446  [   77/   89]
per-ex loss: 0.269229  [   78/   89]
per-ex loss: 0.483098  [   79/   89]
per-ex loss: 0.319976  [   80/   89]
per-ex loss: 0.999920  [   81/   89]
per-ex loss: 0.331090  [   82/   89]
per-ex loss: 0.225358  [   83/   89]
per-ex loss: 0.569875  [   84/   89]
per-ex loss: 0.365300  [   85/   89]
per-ex loss: 0.319458  [   86/   89]
per-ex loss: 0.376189  [   87/   89]
per-ex loss: 0.323580  [   88/   89]
per-ex loss: 0.626608  [   89/   89]
Train Error: Avg loss: 0.36227689
validation Error: 
 Avg loss: 0.56184644 
 F1: 0.467343 
 Precision: 0.514517 
 Recall: 0.428093
 IoU: 0.304924

test Error: 
 Avg loss: 0.53557243 
 F1: 0.512875 
 Precision: 0.493810 
 Recall: 0.533472
 IoU: 0.344877

We have finished training iteration 183
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_181_.pth
per-ex loss: 0.316987  [    1/   89]
per-ex loss: 0.461197  [    2/   89]
per-ex loss: 0.545239  [    3/   89]
per-ex loss: 0.279452  [    4/   89]
per-ex loss: 0.416709  [    5/   89]
per-ex loss: 0.272428  [    6/   89]
per-ex loss: 0.262072  [    7/   89]
per-ex loss: 0.275764  [    8/   89]
per-ex loss: 0.476184  [    9/   89]
per-ex loss: 0.610376  [   10/   89]
per-ex loss: 0.521562  [   11/   89]
per-ex loss: 0.202094  [   12/   89]
per-ex loss: 0.417057  [   13/   89]
per-ex loss: 0.541581  [   14/   89]
per-ex loss: 0.463849  [   15/   89]
per-ex loss: 0.512326  [   16/   89]
per-ex loss: 0.254213  [   17/   89]
per-ex loss: 0.344061  [   18/   89]
per-ex loss: 0.454645  [   19/   89]
per-ex loss: 0.588721  [   20/   89]
per-ex loss: 0.328431  [   21/   89]
per-ex loss: 0.313677  [   22/   89]
per-ex loss: 0.337257  [   23/   89]
per-ex loss: 0.579472  [   24/   89]
per-ex loss: 0.278034  [   25/   89]
per-ex loss: 0.172078  [   26/   89]
per-ex loss: 0.404008  [   27/   89]
per-ex loss: 0.276859  [   28/   89]
per-ex loss: 0.217215  [   29/   89]
per-ex loss: 0.313054  [   30/   89]
per-ex loss: 0.388252  [   31/   89]
per-ex loss: 0.482408  [   32/   89]
per-ex loss: 0.212778  [   33/   89]
per-ex loss: 0.463847  [   34/   89]
per-ex loss: 0.524477  [   35/   89]
per-ex loss: 0.244814  [   36/   89]
per-ex loss: 0.393477  [   37/   89]
per-ex loss: 0.553994  [   38/   89]
per-ex loss: 0.391870  [   39/   89]
per-ex loss: 0.295448  [   40/   89]
per-ex loss: 0.356469  [   41/   89]
per-ex loss: 0.280234  [   42/   89]
per-ex loss: 0.194944  [   43/   89]
per-ex loss: 0.460279  [   44/   89]
per-ex loss: 0.999879  [   45/   89]
per-ex loss: 0.345137  [   46/   89]
per-ex loss: 0.397886  [   47/   89]
per-ex loss: 0.427091  [   48/   89]
per-ex loss: 0.412225  [   49/   89]
per-ex loss: 0.207198  [   50/   89]
per-ex loss: 0.321539  [   51/   89]
per-ex loss: 0.284563  [   52/   89]
per-ex loss: 0.349903  [   53/   89]
per-ex loss: 0.253774  [   54/   89]
per-ex loss: 0.285696  [   55/   89]
per-ex loss: 0.310410  [   56/   89]
per-ex loss: 0.229249  [   57/   89]
per-ex loss: 0.210699  [   58/   89]
per-ex loss: 0.328827  [   59/   89]
per-ex loss: 0.313957  [   60/   89]
per-ex loss: 0.337444  [   61/   89]
per-ex loss: 0.233048  [   62/   89]
per-ex loss: 0.288625  [   63/   89]
per-ex loss: 0.436581  [   64/   89]
per-ex loss: 0.381554  [   65/   89]
per-ex loss: 0.494030  [   66/   89]
per-ex loss: 0.208115  [   67/   89]
per-ex loss: 0.356593  [   68/   89]
per-ex loss: 0.334166  [   69/   89]
per-ex loss: 0.267978  [   70/   89]
per-ex loss: 0.366473  [   71/   89]
per-ex loss: 0.527070  [   72/   89]
per-ex loss: 0.300699  [   73/   89]
per-ex loss: 0.326419  [   74/   89]
per-ex loss: 0.253640  [   75/   89]
per-ex loss: 0.439526  [   76/   89]
per-ex loss: 0.186273  [   77/   89]
per-ex loss: 0.335065  [   78/   89]
per-ex loss: 0.476649  [   79/   89]
per-ex loss: 0.389546  [   80/   89]
per-ex loss: 0.293746  [   81/   89]
per-ex loss: 0.297286  [   82/   89]
per-ex loss: 0.234049  [   83/   89]
per-ex loss: 0.560873  [   84/   89]
per-ex loss: 0.326503  [   85/   89]
per-ex loss: 0.512032  [   86/   89]
per-ex loss: 0.546683  [   87/   89]
per-ex loss: 0.361672  [   88/   89]
per-ex loss: 0.288028  [   89/   89]
Train Error: Avg loss: 0.36762121
validation Error: 
 Avg loss: 0.53207142 
 F1: 0.495933 
 Precision: 0.558472 
 Recall: 0.445991
 IoU: 0.329728

test Error: 
 Avg loss: 0.48707739 
 F1: 0.559431 
 Precision: 0.613782 
 Recall: 0.513923
 IoU: 0.388341

We have finished training iteration 184
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_182_.pth
per-ex loss: 0.467597  [    1/   89]
per-ex loss: 0.273812  [    2/   89]
per-ex loss: 0.372972  [    3/   89]
per-ex loss: 0.336800  [    4/   89]
per-ex loss: 0.288474  [    5/   89]
per-ex loss: 0.444478  [    6/   89]
per-ex loss: 0.323480  [    7/   89]
per-ex loss: 0.288633  [    8/   89]
per-ex loss: 0.332180  [    9/   89]
per-ex loss: 0.311437  [   10/   89]
per-ex loss: 0.395274  [   11/   89]
per-ex loss: 0.518998  [   12/   89]
per-ex loss: 0.236638  [   13/   89]
per-ex loss: 0.338793  [   14/   89]
per-ex loss: 0.322894  [   15/   89]
per-ex loss: 0.285662  [   16/   89]
per-ex loss: 0.431550  [   17/   89]
per-ex loss: 0.310931  [   18/   89]
per-ex loss: 0.436671  [   19/   89]
per-ex loss: 0.231081  [   20/   89]
per-ex loss: 0.237867  [   21/   89]
per-ex loss: 0.332938  [   22/   89]
per-ex loss: 0.548643  [   23/   89]
per-ex loss: 0.283742  [   24/   89]
per-ex loss: 0.425638  [   25/   89]
per-ex loss: 0.369239  [   26/   89]
per-ex loss: 0.326818  [   27/   89]
per-ex loss: 0.307180  [   28/   89]
per-ex loss: 0.371983  [   29/   89]
per-ex loss: 0.364088  [   30/   89]
per-ex loss: 0.298280  [   31/   89]
per-ex loss: 0.570800  [   32/   89]
per-ex loss: 0.276053  [   33/   89]
per-ex loss: 0.267197  [   34/   89]
per-ex loss: 0.145985  [   35/   89]
per-ex loss: 0.482169  [   36/   89]
per-ex loss: 0.391765  [   37/   89]
per-ex loss: 0.532237  [   38/   89]
per-ex loss: 0.219855  [   39/   89]
per-ex loss: 0.388863  [   40/   89]
per-ex loss: 0.516435  [   41/   89]
per-ex loss: 0.299425  [   42/   89]
per-ex loss: 0.628401  [   43/   89]
per-ex loss: 0.280110  [   44/   89]
per-ex loss: 0.583700  [   45/   89]
per-ex loss: 0.451239  [   46/   89]
per-ex loss: 0.365908  [   47/   89]
per-ex loss: 0.378536  [   48/   89]
per-ex loss: 0.562654  [   49/   89]
per-ex loss: 0.270585  [   50/   89]
per-ex loss: 0.299704  [   51/   89]
per-ex loss: 0.296194  [   52/   89]
per-ex loss: 0.252328  [   53/   89]
per-ex loss: 0.260855  [   54/   89]
per-ex loss: 0.143033  [   55/   89]
per-ex loss: 0.507902  [   56/   89]
per-ex loss: 0.265907  [   57/   89]
per-ex loss: 0.333256  [   58/   89]
per-ex loss: 0.341122  [   59/   89]
per-ex loss: 0.279016  [   60/   89]
per-ex loss: 0.277581  [   61/   89]
per-ex loss: 0.491252  [   62/   89]
per-ex loss: 0.538460  [   63/   89]
per-ex loss: 0.543234  [   64/   89]
per-ex loss: 0.387140  [   65/   89]
per-ex loss: 0.327265  [   66/   89]
per-ex loss: 0.292197  [   67/   89]
per-ex loss: 0.315526  [   68/   89]
per-ex loss: 0.275970  [   69/   89]
per-ex loss: 0.259780  [   70/   89]
per-ex loss: 0.285727  [   71/   89]
per-ex loss: 0.295425  [   72/   89]
per-ex loss: 0.578968  [   73/   89]
per-ex loss: 0.363076  [   74/   89]
per-ex loss: 0.285854  [   75/   89]
per-ex loss: 0.999906  [   76/   89]
per-ex loss: 0.272405  [   77/   89]
per-ex loss: 0.452469  [   78/   89]
per-ex loss: 0.294198  [   79/   89]
per-ex loss: 0.409032  [   80/   89]
per-ex loss: 0.290947  [   81/   89]
per-ex loss: 0.317175  [   82/   89]
per-ex loss: 0.396589  [   83/   89]
per-ex loss: 0.310457  [   84/   89]
per-ex loss: 0.214272  [   85/   89]
per-ex loss: 0.313969  [   86/   89]
per-ex loss: 0.434878  [   87/   89]
per-ex loss: 0.346295  [   88/   89]
per-ex loss: 0.284482  [   89/   89]
Train Error: Avg loss: 0.36247793
validation Error: 
 Avg loss: 0.54828224 
 F1: 0.479082 
 Precision: 0.592020 
 Recall: 0.402331
 IoU: 0.314995

test Error: 
 Avg loss: 0.51597465 
 F1: 0.529408 
 Precision: 0.601528 
 Recall: 0.472730
 IoU: 0.359996

We have finished training iteration 185
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_183_.pth
per-ex loss: 0.307625  [    1/   89]
per-ex loss: 0.999936  [    2/   89]
per-ex loss: 0.518732  [    3/   89]
per-ex loss: 0.191921  [    4/   89]
per-ex loss: 0.443117  [    5/   89]
per-ex loss: 0.467488  [    6/   89]
per-ex loss: 0.234315  [    7/   89]
per-ex loss: 0.555641  [    8/   89]
per-ex loss: 0.319153  [    9/   89]
per-ex loss: 0.350072  [   10/   89]
per-ex loss: 0.441016  [   11/   89]
per-ex loss: 0.254485  [   12/   89]
per-ex loss: 0.332821  [   13/   89]
per-ex loss: 0.166062  [   14/   89]
per-ex loss: 0.288988  [   15/   89]
per-ex loss: 0.190345  [   16/   89]
per-ex loss: 0.229417  [   17/   89]
per-ex loss: 0.596280  [   18/   89]
per-ex loss: 0.392900  [   19/   89]
per-ex loss: 0.314888  [   20/   89]
per-ex loss: 0.282590  [   21/   89]
per-ex loss: 0.541314  [   22/   89]
per-ex loss: 0.284020  [   23/   89]
per-ex loss: 0.296293  [   24/   89]
per-ex loss: 0.243817  [   25/   89]
per-ex loss: 0.312613  [   26/   89]
per-ex loss: 0.244676  [   27/   89]
per-ex loss: 0.386381  [   28/   89]
per-ex loss: 0.328565  [   29/   89]
per-ex loss: 0.498366  [   30/   89]
per-ex loss: 0.249211  [   31/   89]
per-ex loss: 0.146524  [   32/   89]
per-ex loss: 0.231508  [   33/   89]
per-ex loss: 0.434751  [   34/   89]
per-ex loss: 0.354279  [   35/   89]
per-ex loss: 0.467981  [   36/   89]
per-ex loss: 0.410156  [   37/   89]
per-ex loss: 0.508951  [   38/   89]
per-ex loss: 0.248108  [   39/   89]
per-ex loss: 0.205307  [   40/   89]
per-ex loss: 0.329238  [   41/   89]
per-ex loss: 0.327682  [   42/   89]
per-ex loss: 0.357234  [   43/   89]
per-ex loss: 0.417840  [   44/   89]
per-ex loss: 0.248963  [   45/   89]
per-ex loss: 0.477219  [   46/   89]
per-ex loss: 0.442042  [   47/   89]
per-ex loss: 0.326872  [   48/   89]
per-ex loss: 0.424399  [   49/   89]
per-ex loss: 0.279409  [   50/   89]
per-ex loss: 0.323277  [   51/   89]
per-ex loss: 0.328671  [   52/   89]
per-ex loss: 0.376480  [   53/   89]
per-ex loss: 0.456497  [   54/   89]
per-ex loss: 0.337316  [   55/   89]
per-ex loss: 0.307464  [   56/   89]
per-ex loss: 0.285391  [   57/   89]
per-ex loss: 0.498040  [   58/   89]
per-ex loss: 0.324974  [   59/   89]
per-ex loss: 0.287734  [   60/   89]
per-ex loss: 0.415127  [   61/   89]
per-ex loss: 0.301220  [   62/   89]
per-ex loss: 0.221422  [   63/   89]
per-ex loss: 0.277625  [   64/   89]
per-ex loss: 0.226316  [   65/   89]
per-ex loss: 0.255487  [   66/   89]
per-ex loss: 0.500834  [   67/   89]
per-ex loss: 0.536606  [   68/   89]
per-ex loss: 0.264864  [   69/   89]
per-ex loss: 0.323570  [   70/   89]
per-ex loss: 0.238114  [   71/   89]
per-ex loss: 0.307631  [   72/   89]
per-ex loss: 0.266191  [   73/   89]
per-ex loss: 0.334991  [   74/   89]
per-ex loss: 0.293880  [   75/   89]
per-ex loss: 0.534920  [   76/   89]
per-ex loss: 0.353455  [   77/   89]
per-ex loss: 0.413152  [   78/   89]
per-ex loss: 0.356323  [   79/   89]
per-ex loss: 0.296372  [   80/   89]
per-ex loss: 0.461305  [   81/   89]
per-ex loss: 0.238534  [   82/   89]
per-ex loss: 0.719582  [   83/   89]
per-ex loss: 0.330370  [   84/   89]
per-ex loss: 0.632646  [   85/   89]
per-ex loss: 0.283991  [   86/   89]
per-ex loss: 0.474940  [   87/   89]
per-ex loss: 0.274201  [   88/   89]
per-ex loss: 0.391527  [   89/   89]
Train Error: Avg loss: 0.35901741
validation Error: 
 Avg loss: 0.54109282 
 F1: 0.486770 
 Precision: 0.497955 
 Recall: 0.476076
 IoU: 0.321676

test Error: 
 Avg loss: 0.49223956 
 F1: 0.554962 
 Precision: 0.540537 
 Recall: 0.570179
 IoU: 0.384047

We have finished training iteration 186
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_184_.pth
per-ex loss: 0.397701  [    1/   89]
per-ex loss: 0.337071  [    2/   89]
per-ex loss: 0.308285  [    3/   89]
per-ex loss: 0.347409  [    4/   89]
per-ex loss: 0.447551  [    5/   89]
per-ex loss: 0.321923  [    6/   89]
per-ex loss: 0.507188  [    7/   89]
per-ex loss: 0.407713  [    8/   89]
per-ex loss: 0.294640  [    9/   89]
per-ex loss: 0.356074  [   10/   89]
per-ex loss: 0.368752  [   11/   89]
per-ex loss: 0.205049  [   12/   89]
per-ex loss: 0.347364  [   13/   89]
per-ex loss: 0.298283  [   14/   89]
per-ex loss: 0.317323  [   15/   89]
per-ex loss: 0.242088  [   16/   89]
per-ex loss: 0.469566  [   17/   89]
per-ex loss: 0.299612  [   18/   89]
per-ex loss: 0.446368  [   19/   89]
per-ex loss: 0.322775  [   20/   89]
per-ex loss: 0.321552  [   21/   89]
per-ex loss: 0.485228  [   22/   89]
per-ex loss: 0.290562  [   23/   89]
per-ex loss: 0.296347  [   24/   89]
per-ex loss: 0.328340  [   25/   89]
per-ex loss: 0.294779  [   26/   89]
per-ex loss: 0.358928  [   27/   89]
per-ex loss: 0.999912  [   28/   89]
per-ex loss: 0.208450  [   29/   89]
per-ex loss: 0.300006  [   30/   89]
per-ex loss: 0.374569  [   31/   89]
per-ex loss: 0.571278  [   32/   89]
per-ex loss: 0.314716  [   33/   89]
per-ex loss: 0.594291  [   34/   89]
per-ex loss: 0.479288  [   35/   89]
per-ex loss: 0.444346  [   36/   89]
per-ex loss: 0.263940  [   37/   89]
per-ex loss: 0.221398  [   38/   89]
per-ex loss: 0.508577  [   39/   89]
per-ex loss: 0.332681  [   40/   89]
per-ex loss: 0.443966  [   41/   89]
per-ex loss: 0.249973  [   42/   89]
per-ex loss: 0.238292  [   43/   89]
per-ex loss: 0.297831  [   44/   89]
per-ex loss: 0.269627  [   45/   89]
per-ex loss: 0.280799  [   46/   89]
per-ex loss: 0.347738  [   47/   89]
per-ex loss: 0.504497  [   48/   89]
per-ex loss: 0.230873  [   49/   89]
per-ex loss: 0.363591  [   50/   89]
per-ex loss: 0.253035  [   51/   89]
per-ex loss: 0.269114  [   52/   89]
per-ex loss: 0.254252  [   53/   89]
per-ex loss: 0.503747  [   54/   89]
per-ex loss: 0.463539  [   55/   89]
per-ex loss: 0.178155  [   56/   89]
per-ex loss: 0.259954  [   57/   89]
per-ex loss: 0.504608  [   58/   89]
per-ex loss: 0.303161  [   59/   89]
per-ex loss: 0.369787  [   60/   89]
per-ex loss: 0.378015  [   61/   89]
per-ex loss: 0.409143  [   62/   89]
per-ex loss: 0.414002  [   63/   89]
per-ex loss: 0.417308  [   64/   89]
per-ex loss: 0.176375  [   65/   89]
per-ex loss: 0.266489  [   66/   89]
per-ex loss: 0.350816  [   67/   89]
per-ex loss: 0.311650  [   68/   89]
per-ex loss: 0.400357  [   69/   89]
per-ex loss: 0.322843  [   70/   89]
per-ex loss: 0.324937  [   71/   89]
per-ex loss: 0.180512  [   72/   89]
per-ex loss: 0.271510  [   73/   89]
per-ex loss: 0.242164  [   74/   89]
per-ex loss: 0.492938  [   75/   89]
per-ex loss: 0.318133  [   76/   89]
per-ex loss: 0.213473  [   77/   89]
per-ex loss: 0.239480  [   78/   89]
per-ex loss: 0.292016  [   79/   89]
per-ex loss: 0.422332  [   80/   89]
per-ex loss: 0.566533  [   81/   89]
per-ex loss: 0.605684  [   82/   89]
per-ex loss: 0.205208  [   83/   89]
per-ex loss: 0.238384  [   84/   89]
per-ex loss: 0.298249  [   85/   89]
per-ex loss: 0.581270  [   86/   89]
per-ex loss: 0.353892  [   87/   89]
per-ex loss: 0.406111  [   88/   89]
per-ex loss: 0.344458  [   89/   89]
Train Error: Avg loss: 0.35576122
validation Error: 
 Avg loss: 0.55104861 
 F1: 0.475002 
 Precision: 0.460929 
 Recall: 0.489963
 IoU: 0.311478

test Error: 
 Avg loss: 0.48773275 
 F1: 0.554208 
 Precision: 0.536772 
 Recall: 0.572814
 IoU: 0.383324

We have finished training iteration 187
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_185_.pth
per-ex loss: 0.260062  [    1/   89]
per-ex loss: 0.264035  [    2/   89]
per-ex loss: 0.401334  [    3/   89]
per-ex loss: 0.331461  [    4/   89]
per-ex loss: 0.284734  [    5/   89]
per-ex loss: 0.495577  [    6/   89]
per-ex loss: 0.291065  [    7/   89]
per-ex loss: 0.364225  [    8/   89]
per-ex loss: 0.343256  [    9/   89]
per-ex loss: 0.440320  [   10/   89]
per-ex loss: 0.274939  [   11/   89]
per-ex loss: 0.454412  [   12/   89]
per-ex loss: 0.420283  [   13/   89]
per-ex loss: 0.468395  [   14/   89]
per-ex loss: 0.323920  [   15/   89]
per-ex loss: 0.512487  [   16/   89]
per-ex loss: 0.249237  [   17/   89]
per-ex loss: 0.306646  [   18/   89]
per-ex loss: 0.175189  [   19/   89]
per-ex loss: 0.364619  [   20/   89]
per-ex loss: 0.551427  [   21/   89]
per-ex loss: 0.324638  [   22/   89]
per-ex loss: 0.532750  [   23/   89]
per-ex loss: 0.267324  [   24/   89]
per-ex loss: 0.275543  [   25/   89]
per-ex loss: 0.288359  [   26/   89]
per-ex loss: 0.572894  [   27/   89]
per-ex loss: 0.273995  [   28/   89]
per-ex loss: 0.255745  [   29/   89]
per-ex loss: 0.412284  [   30/   89]
per-ex loss: 0.331800  [   31/   89]
per-ex loss: 0.501705  [   32/   89]
per-ex loss: 0.474801  [   33/   89]
per-ex loss: 0.356181  [   34/   89]
per-ex loss: 0.377174  [   35/   89]
per-ex loss: 0.196835  [   36/   89]
per-ex loss: 0.381340  [   37/   89]
per-ex loss: 0.279526  [   38/   89]
per-ex loss: 0.226843  [   39/   89]
per-ex loss: 0.278690  [   40/   89]
per-ex loss: 0.275870  [   41/   89]
per-ex loss: 0.579296  [   42/   89]
per-ex loss: 0.245541  [   43/   89]
per-ex loss: 0.160827  [   44/   89]
per-ex loss: 0.244852  [   45/   89]
per-ex loss: 0.167194  [   46/   89]
per-ex loss: 0.630568  [   47/   89]
per-ex loss: 0.436681  [   48/   89]
per-ex loss: 0.531462  [   49/   89]
per-ex loss: 0.234265  [   50/   89]
per-ex loss: 0.438516  [   51/   89]
per-ex loss: 0.327095  [   52/   89]
per-ex loss: 0.279812  [   53/   89]
per-ex loss: 0.296919  [   54/   89]
per-ex loss: 0.530252  [   55/   89]
per-ex loss: 0.311133  [   56/   89]
per-ex loss: 0.477573  [   57/   89]
per-ex loss: 0.427026  [   58/   89]
per-ex loss: 0.312911  [   59/   89]
per-ex loss: 0.396507  [   60/   89]
per-ex loss: 0.294083  [   61/   89]
per-ex loss: 0.275231  [   62/   89]
per-ex loss: 0.429195  [   63/   89]
per-ex loss: 0.279498  [   64/   89]
per-ex loss: 0.374270  [   65/   89]
per-ex loss: 0.237118  [   66/   89]
per-ex loss: 0.321462  [   67/   89]
per-ex loss: 0.425057  [   68/   89]
per-ex loss: 0.325319  [   69/   89]
per-ex loss: 0.349912  [   70/   89]
per-ex loss: 0.245730  [   71/   89]
per-ex loss: 0.383101  [   72/   89]
per-ex loss: 0.419570  [   73/   89]
per-ex loss: 0.427202  [   74/   89]
per-ex loss: 0.253654  [   75/   89]
per-ex loss: 0.204522  [   76/   89]
per-ex loss: 0.999918  [   77/   89]
per-ex loss: 0.314810  [   78/   89]
per-ex loss: 0.280995  [   79/   89]
per-ex loss: 0.368233  [   80/   89]
per-ex loss: 0.445635  [   81/   89]
per-ex loss: 0.331768  [   82/   89]
per-ex loss: 0.289616  [   83/   89]
per-ex loss: 0.305399  [   84/   89]
per-ex loss: 0.205411  [   85/   89]
per-ex loss: 0.246021  [   86/   89]
per-ex loss: 0.317491  [   87/   89]
per-ex loss: 0.518392  [   88/   89]
per-ex loss: 0.233550  [   89/   89]
Train Error: Avg loss: 0.35497210
validation Error: 
 Avg loss: 0.54902016 
 F1: 0.477731 
 Precision: 0.535139 
 Recall: 0.431447
 IoU: 0.313828

test Error: 
 Avg loss: 0.51403272 
 F1: 0.531499 
 Precision: 0.540620 
 Recall: 0.522680
 IoU: 0.361933

We have finished training iteration 188
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_186_.pth
per-ex loss: 0.370373  [    1/   89]
per-ex loss: 0.497205  [    2/   89]
per-ex loss: 0.445974  [    3/   89]
per-ex loss: 0.288856  [    4/   89]
per-ex loss: 0.278177  [    5/   89]
per-ex loss: 0.329681  [    6/   89]
per-ex loss: 0.375130  [    7/   89]
per-ex loss: 0.342799  [    8/   89]
per-ex loss: 0.266669  [    9/   89]
per-ex loss: 0.341790  [   10/   89]
per-ex loss: 0.190283  [   11/   89]
per-ex loss: 0.490597  [   12/   89]
per-ex loss: 0.331652  [   13/   89]
per-ex loss: 0.330905  [   14/   89]
per-ex loss: 0.524349  [   15/   89]
per-ex loss: 0.320081  [   16/   89]
per-ex loss: 0.479900  [   17/   89]
per-ex loss: 0.314388  [   18/   89]
per-ex loss: 0.402024  [   19/   89]
per-ex loss: 0.488096  [   20/   89]
per-ex loss: 0.321721  [   21/   89]
per-ex loss: 0.201718  [   22/   89]
per-ex loss: 0.314710  [   23/   89]
per-ex loss: 0.367772  [   24/   89]
per-ex loss: 0.338091  [   25/   89]
per-ex loss: 0.311413  [   26/   89]
per-ex loss: 0.425409  [   27/   89]
per-ex loss: 0.246310  [   28/   89]
per-ex loss: 0.317418  [   29/   89]
per-ex loss: 0.286430  [   30/   89]
per-ex loss: 0.999937  [   31/   89]
per-ex loss: 0.312104  [   32/   89]
per-ex loss: 0.446841  [   33/   89]
per-ex loss: 0.311197  [   34/   89]
per-ex loss: 0.566120  [   35/   89]
per-ex loss: 0.460353  [   36/   89]
per-ex loss: 0.292081  [   37/   89]
per-ex loss: 0.350496  [   38/   89]
per-ex loss: 0.555763  [   39/   89]
per-ex loss: 0.349506  [   40/   89]
per-ex loss: 0.275984  [   41/   89]
per-ex loss: 0.239749  [   42/   89]
per-ex loss: 0.261388  [   43/   89]
per-ex loss: 0.293496  [   44/   89]
per-ex loss: 0.343612  [   45/   89]
per-ex loss: 0.364882  [   46/   89]
per-ex loss: 0.542776  [   47/   89]
per-ex loss: 0.273864  [   48/   89]
per-ex loss: 0.181559  [   49/   89]
per-ex loss: 0.267746  [   50/   89]
per-ex loss: 0.556254  [   51/   89]
per-ex loss: 0.260315  [   52/   89]
per-ex loss: 0.233287  [   53/   89]
per-ex loss: 0.279530  [   54/   89]
per-ex loss: 0.360316  [   55/   89]
per-ex loss: 0.492548  [   56/   89]
per-ex loss: 0.421189  [   57/   89]
per-ex loss: 0.313451  [   58/   89]
per-ex loss: 0.279288  [   59/   89]
per-ex loss: 0.327132  [   60/   89]
per-ex loss: 0.286369  [   61/   89]
per-ex loss: 0.532921  [   62/   89]
per-ex loss: 0.284719  [   63/   89]
per-ex loss: 0.334319  [   64/   89]
per-ex loss: 0.337129  [   65/   89]
per-ex loss: 0.520172  [   66/   89]
per-ex loss: 0.269122  [   67/   89]
per-ex loss: 0.393057  [   68/   89]
per-ex loss: 0.436130  [   69/   89]
per-ex loss: 0.410719  [   70/   89]
per-ex loss: 0.329057  [   71/   89]
per-ex loss: 0.351827  [   72/   89]
per-ex loss: 0.226626  [   73/   89]
per-ex loss: 0.274366  [   74/   89]
per-ex loss: 0.388282  [   75/   89]
per-ex loss: 0.218716  [   76/   89]
per-ex loss: 0.271384  [   77/   89]
per-ex loss: 0.302157  [   78/   89]
per-ex loss: 0.484318  [   79/   89]
per-ex loss: 0.388054  [   80/   89]
per-ex loss: 0.316248  [   81/   89]
per-ex loss: 0.255978  [   82/   89]
per-ex loss: 0.260809  [   83/   89]
per-ex loss: 0.431242  [   84/   89]
per-ex loss: 0.334040  [   85/   89]
per-ex loss: 0.403708  [   86/   89]
per-ex loss: 0.316477  [   87/   89]
per-ex loss: 0.324914  [   88/   89]
per-ex loss: 0.504912  [   89/   89]
Train Error: Avg loss: 0.35888160
validation Error: 
 Avg loss: 0.54576502 
 F1: 0.482232 
 Precision: 0.501140 
 Recall: 0.464699
 IoU: 0.317724

test Error: 
 Avg loss: 0.49763401 
 F1: 0.549197 
 Precision: 0.551589 
 Recall: 0.546826
 IoU: 0.378547

We have finished training iteration 189
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_187_.pth
per-ex loss: 0.176823  [    1/   89]
per-ex loss: 0.552884  [    2/   89]
per-ex loss: 0.377807  [    3/   89]
per-ex loss: 0.463210  [    4/   89]
per-ex loss: 0.326977  [    5/   89]
per-ex loss: 0.256699  [    6/   89]
per-ex loss: 0.212317  [    7/   89]
per-ex loss: 0.240120  [    8/   89]
per-ex loss: 0.236061  [    9/   89]
per-ex loss: 0.292018  [   10/   89]
per-ex loss: 0.254359  [   11/   89]
per-ex loss: 0.448179  [   12/   89]
per-ex loss: 0.436690  [   13/   89]
per-ex loss: 0.374687  [   14/   89]
per-ex loss: 0.999894  [   15/   89]
per-ex loss: 0.317508  [   16/   89]
per-ex loss: 0.491639  [   17/   89]
per-ex loss: 0.205841  [   18/   89]
per-ex loss: 0.279873  [   19/   89]
per-ex loss: 0.536399  [   20/   89]
per-ex loss: 0.283840  [   21/   89]
per-ex loss: 0.244452  [   22/   89]
per-ex loss: 0.330798  [   23/   89]
per-ex loss: 0.284993  [   24/   89]
per-ex loss: 0.152318  [   25/   89]
per-ex loss: 0.482714  [   26/   89]
per-ex loss: 0.249383  [   27/   89]
per-ex loss: 0.231726  [   28/   89]
per-ex loss: 0.414434  [   29/   89]
per-ex loss: 0.294727  [   30/   89]
per-ex loss: 0.220926  [   31/   89]
per-ex loss: 0.497692  [   32/   89]
per-ex loss: 0.190486  [   33/   89]
per-ex loss: 0.385816  [   34/   89]
per-ex loss: 0.484198  [   35/   89]
per-ex loss: 0.423567  [   36/   89]
per-ex loss: 0.561406  [   37/   89]
per-ex loss: 0.297009  [   38/   89]
per-ex loss: 0.292913  [   39/   89]
per-ex loss: 0.347613  [   40/   89]
per-ex loss: 0.397660  [   41/   89]
per-ex loss: 0.336071  [   42/   89]
per-ex loss: 0.248892  [   43/   89]
per-ex loss: 0.359794  [   44/   89]
per-ex loss: 0.386814  [   45/   89]
per-ex loss: 0.529143  [   46/   89]
per-ex loss: 0.353770  [   47/   89]
per-ex loss: 0.291556  [   48/   89]
per-ex loss: 0.314327  [   49/   89]
per-ex loss: 0.270776  [   50/   89]
per-ex loss: 0.454176  [   51/   89]
per-ex loss: 0.523618  [   52/   89]
per-ex loss: 0.313450  [   53/   89]
per-ex loss: 0.463932  [   54/   89]
per-ex loss: 0.327080  [   55/   89]
per-ex loss: 0.144425  [   56/   89]
per-ex loss: 0.221663  [   57/   89]
per-ex loss: 0.297363  [   58/   89]
per-ex loss: 0.177158  [   59/   89]
per-ex loss: 0.418742  [   60/   89]
per-ex loss: 0.254890  [   61/   89]
per-ex loss: 0.320581  [   62/   89]
per-ex loss: 0.278388  [   63/   89]
per-ex loss: 0.539396  [   64/   89]
per-ex loss: 0.285006  [   65/   89]
per-ex loss: 0.308830  [   66/   89]
per-ex loss: 0.319248  [   67/   89]
per-ex loss: 0.287778  [   68/   89]
per-ex loss: 0.411960  [   69/   89]
per-ex loss: 0.278153  [   70/   89]
per-ex loss: 0.518178  [   71/   89]
per-ex loss: 0.283123  [   72/   89]
per-ex loss: 0.496872  [   73/   89]
per-ex loss: 0.289770  [   74/   89]
per-ex loss: 0.397949  [   75/   89]
per-ex loss: 0.354746  [   76/   89]
per-ex loss: 0.308628  [   77/   89]
per-ex loss: 0.331561  [   78/   89]
per-ex loss: 0.422377  [   79/   89]
per-ex loss: 0.263343  [   80/   89]
per-ex loss: 0.283940  [   81/   89]
per-ex loss: 0.317461  [   82/   89]
per-ex loss: 0.279234  [   83/   89]
per-ex loss: 0.221805  [   84/   89]
per-ex loss: 0.349863  [   85/   89]
per-ex loss: 0.227678  [   86/   89]
per-ex loss: 0.275149  [   87/   89]
per-ex loss: 0.454101  [   88/   89]
per-ex loss: 0.554067  [   89/   89]
Train Error: Avg loss: 0.34714021
validation Error: 
 Avg loss: 0.55273966 
 F1: 0.471608 
 Precision: 0.456314 
 Recall: 0.487963
 IoU: 0.308565

test Error: 
 Avg loss: 0.49302664 
 F1: 0.550456 
 Precision: 0.531335 
 Recall: 0.571004
 IoU: 0.379744

We have finished training iteration 190
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_188_.pth
per-ex loss: 0.347915  [    1/   89]
per-ex loss: 0.372421  [    2/   89]
per-ex loss: 0.478363  [    3/   89]
per-ex loss: 0.335498  [    4/   89]
per-ex loss: 0.248846  [    5/   89]
per-ex loss: 0.330947  [    6/   89]
per-ex loss: 0.241192  [    7/   89]
per-ex loss: 0.475373  [    8/   89]
per-ex loss: 0.268219  [    9/   89]
per-ex loss: 0.237417  [   10/   89]
per-ex loss: 0.468866  [   11/   89]
per-ex loss: 0.286856  [   12/   89]
per-ex loss: 0.507569  [   13/   89]
per-ex loss: 0.302935  [   14/   89]
per-ex loss: 0.487830  [   15/   89]
per-ex loss: 0.322752  [   16/   89]
per-ex loss: 0.291822  [   17/   89]
per-ex loss: 0.350324  [   18/   89]
per-ex loss: 0.505126  [   19/   89]
per-ex loss: 0.345262  [   20/   89]
per-ex loss: 0.241589  [   21/   89]
per-ex loss: 0.326836  [   22/   89]
per-ex loss: 0.669884  [   23/   89]
per-ex loss: 0.268373  [   24/   89]
per-ex loss: 0.489982  [   25/   89]
per-ex loss: 0.302532  [   26/   89]
per-ex loss: 0.331491  [   27/   89]
per-ex loss: 0.252500  [   28/   89]
per-ex loss: 0.275690  [   29/   89]
per-ex loss: 0.393253  [   30/   89]
per-ex loss: 0.307496  [   31/   89]
per-ex loss: 0.463153  [   32/   89]
per-ex loss: 0.542459  [   33/   89]
per-ex loss: 0.318316  [   34/   89]
per-ex loss: 0.386920  [   35/   89]
per-ex loss: 0.205336  [   36/   89]
per-ex loss: 0.494833  [   37/   89]
per-ex loss: 0.263246  [   38/   89]
per-ex loss: 0.325007  [   39/   89]
per-ex loss: 0.329962  [   40/   89]
per-ex loss: 0.506861  [   41/   89]
per-ex loss: 0.429994  [   42/   89]
per-ex loss: 0.424506  [   43/   89]
per-ex loss: 0.568859  [   44/   89]
per-ex loss: 0.267747  [   45/   89]
per-ex loss: 0.346726  [   46/   89]
per-ex loss: 0.457601  [   47/   89]
per-ex loss: 0.229333  [   48/   89]
per-ex loss: 0.189845  [   49/   89]
per-ex loss: 0.284666  [   50/   89]
per-ex loss: 0.262600  [   51/   89]
per-ex loss: 0.333080  [   52/   89]
per-ex loss: 0.584043  [   53/   89]
per-ex loss: 0.364673  [   54/   89]
per-ex loss: 0.300368  [   55/   89]
per-ex loss: 0.343813  [   56/   89]
per-ex loss: 0.261494  [   57/   89]
per-ex loss: 0.306615  [   58/   89]
per-ex loss: 0.338005  [   59/   89]
per-ex loss: 0.431542  [   60/   89]
per-ex loss: 0.292861  [   61/   89]
per-ex loss: 0.273872  [   62/   89]
per-ex loss: 0.304692  [   63/   89]
per-ex loss: 0.163063  [   64/   89]
per-ex loss: 0.271070  [   65/   89]
per-ex loss: 0.409958  [   66/   89]
per-ex loss: 0.297588  [   67/   89]
per-ex loss: 0.405548  [   68/   89]
per-ex loss: 0.239404  [   69/   89]
per-ex loss: 0.213401  [   70/   89]
per-ex loss: 0.542195  [   71/   89]
per-ex loss: 0.459059  [   72/   89]
per-ex loss: 0.384069  [   73/   89]
per-ex loss: 0.383837  [   74/   89]
per-ex loss: 0.317562  [   75/   89]
per-ex loss: 0.219665  [   76/   89]
per-ex loss: 0.385589  [   77/   89]
per-ex loss: 0.284739  [   78/   89]
per-ex loss: 0.178040  [   79/   89]
per-ex loss: 0.259685  [   80/   89]
per-ex loss: 0.366094  [   81/   89]
per-ex loss: 0.554332  [   82/   89]
per-ex loss: 0.365753  [   83/   89]
per-ex loss: 0.216214  [   84/   89]
per-ex loss: 0.180720  [   85/   89]
per-ex loss: 0.999901  [   86/   89]
per-ex loss: 0.278762  [   87/   89]
per-ex loss: 0.399418  [   88/   89]
per-ex loss: 0.494892  [   89/   89]
Train Error: Avg loss: 0.35695216
validation Error: 
 Avg loss: 0.61557325 
 F1: 0.412185 
 Precision: 0.372495 
 Recall: 0.461342
 IoU: 0.259593

test Error: 
 Avg loss: 0.59268718 
 F1: 0.441758 
 Precision: 0.354724 
 Recall: 0.585386
 IoU: 0.283498

We have finished training iteration 191
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_189_.pth
per-ex loss: 0.401939  [    1/   89]
per-ex loss: 0.509345  [    2/   89]
per-ex loss: 0.280834  [    3/   89]
per-ex loss: 0.328027  [    4/   89]
per-ex loss: 0.432287  [    5/   89]
per-ex loss: 0.407161  [    6/   89]
per-ex loss: 0.330721  [    7/   89]
per-ex loss: 0.582634  [    8/   89]
per-ex loss: 0.513151  [    9/   89]
per-ex loss: 0.391660  [   10/   89]
per-ex loss: 0.543830  [   11/   89]
per-ex loss: 0.464037  [   12/   89]
per-ex loss: 0.331564  [   13/   89]
per-ex loss: 0.292266  [   14/   89]
per-ex loss: 0.216404  [   15/   89]
per-ex loss: 0.534054  [   16/   89]
per-ex loss: 0.325964  [   17/   89]
per-ex loss: 0.158636  [   18/   89]
per-ex loss: 0.350330  [   19/   89]
per-ex loss: 0.239399  [   20/   89]
per-ex loss: 0.999913  [   21/   89]
per-ex loss: 0.384510  [   22/   89]
per-ex loss: 0.334710  [   23/   89]
per-ex loss: 0.306064  [   24/   89]
per-ex loss: 0.245831  [   25/   89]
per-ex loss: 0.287603  [   26/   89]
per-ex loss: 0.335177  [   27/   89]
per-ex loss: 0.361194  [   28/   89]
per-ex loss: 0.324718  [   29/   89]
per-ex loss: 0.348444  [   30/   89]
per-ex loss: 0.291618  [   31/   89]
per-ex loss: 0.210464  [   32/   89]
per-ex loss: 0.342910  [   33/   89]
per-ex loss: 0.422214  [   34/   89]
per-ex loss: 0.318623  [   35/   89]
per-ex loss: 0.249904  [   36/   89]
per-ex loss: 0.179594  [   37/   89]
per-ex loss: 0.445917  [   38/   89]
per-ex loss: 0.307382  [   39/   89]
per-ex loss: 0.295241  [   40/   89]
per-ex loss: 0.244710  [   41/   89]
per-ex loss: 0.490961  [   42/   89]
per-ex loss: 0.330996  [   43/   89]
per-ex loss: 0.455539  [   44/   89]
per-ex loss: 0.559165  [   45/   89]
per-ex loss: 0.486063  [   46/   89]
per-ex loss: 0.449152  [   47/   89]
per-ex loss: 0.266536  [   48/   89]
per-ex loss: 0.454360  [   49/   89]
per-ex loss: 0.270214  [   50/   89]
per-ex loss: 0.280999  [   51/   89]
per-ex loss: 0.329190  [   52/   89]
per-ex loss: 0.279396  [   53/   89]
per-ex loss: 0.401522  [   54/   89]
per-ex loss: 0.460424  [   55/   89]
per-ex loss: 0.296553  [   56/   89]
per-ex loss: 0.388171  [   57/   89]
per-ex loss: 0.439710  [   58/   89]
per-ex loss: 0.151208  [   59/   89]
per-ex loss: 0.562701  [   60/   89]
per-ex loss: 0.340113  [   61/   89]
per-ex loss: 0.287888  [   62/   89]
per-ex loss: 0.489609  [   63/   89]
per-ex loss: 0.237274  [   64/   89]
per-ex loss: 0.530452  [   65/   89]
per-ex loss: 0.216948  [   66/   89]
per-ex loss: 0.400519  [   67/   89]
per-ex loss: 0.383920  [   68/   89]
per-ex loss: 0.286626  [   69/   89]
per-ex loss: 0.264548  [   70/   89]
per-ex loss: 0.392439  [   71/   89]
per-ex loss: 0.543406  [   72/   89]
per-ex loss: 0.474779  [   73/   89]
per-ex loss: 0.172118  [   74/   89]
per-ex loss: 0.239854  [   75/   89]
per-ex loss: 0.257881  [   76/   89]
per-ex loss: 0.442808  [   77/   89]
per-ex loss: 0.342035  [   78/   89]
per-ex loss: 0.272200  [   79/   89]
per-ex loss: 0.299343  [   80/   89]
per-ex loss: 0.273586  [   81/   89]
per-ex loss: 0.232172  [   82/   89]
per-ex loss: 0.227502  [   83/   89]
per-ex loss: 0.426134  [   84/   89]
per-ex loss: 0.260071  [   85/   89]
per-ex loss: 0.352277  [   86/   89]
per-ex loss: 0.270524  [   87/   89]
per-ex loss: 0.299961  [   88/   89]
per-ex loss: 0.287292  [   89/   89]
Train Error: Avg loss: 0.35649576
validation Error: 
 Avg loss: 0.54205329 
 F1: 0.483260 
 Precision: 0.509265 
 Recall: 0.459782
 IoU: 0.318618

test Error: 
 Avg loss: 0.49179741 
 F1: 0.553989 
 Precision: 0.578752 
 Recall: 0.531258
 IoU: 0.383115

We have finished training iteration 192
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_190_.pth
per-ex loss: 0.317944  [    1/   89]
per-ex loss: 0.249178  [    2/   89]
per-ex loss: 0.307326  [    3/   89]
per-ex loss: 0.309709  [    4/   89]
per-ex loss: 0.477602  [    5/   89]
per-ex loss: 0.345617  [    6/   89]
per-ex loss: 0.999933  [    7/   89]
per-ex loss: 0.422556  [    8/   89]
per-ex loss: 0.290860  [    9/   89]
per-ex loss: 0.349856  [   10/   89]
per-ex loss: 0.270501  [   11/   89]
per-ex loss: 0.499965  [   12/   89]
per-ex loss: 0.280801  [   13/   89]
per-ex loss: 0.269866  [   14/   89]
per-ex loss: 0.448806  [   15/   89]
per-ex loss: 0.496356  [   16/   89]
per-ex loss: 0.464144  [   17/   89]
per-ex loss: 0.399853  [   18/   89]
per-ex loss: 0.396692  [   19/   89]
per-ex loss: 0.470626  [   20/   89]
per-ex loss: 0.136687  [   21/   89]
per-ex loss: 0.359036  [   22/   89]
per-ex loss: 0.418955  [   23/   89]
per-ex loss: 0.257645  [   24/   89]
per-ex loss: 0.321779  [   25/   89]
per-ex loss: 0.450340  [   26/   89]
per-ex loss: 0.265235  [   27/   89]
per-ex loss: 0.333240  [   28/   89]
per-ex loss: 0.376916  [   29/   89]
per-ex loss: 0.254349  [   30/   89]
per-ex loss: 0.532621  [   31/   89]
per-ex loss: 0.244697  [   32/   89]
per-ex loss: 0.229364  [   33/   89]
per-ex loss: 0.423278  [   34/   89]
per-ex loss: 0.277289  [   35/   89]
per-ex loss: 0.425197  [   36/   89]
per-ex loss: 0.208029  [   37/   89]
per-ex loss: 0.401643  [   38/   89]
per-ex loss: 0.218949  [   39/   89]
per-ex loss: 0.302040  [   40/   89]
per-ex loss: 0.228660  [   41/   89]
per-ex loss: 0.350338  [   42/   89]
per-ex loss: 0.279940  [   43/   89]
per-ex loss: 0.320942  [   44/   89]
per-ex loss: 0.574088  [   45/   89]
per-ex loss: 0.287950  [   46/   89]
per-ex loss: 0.304624  [   47/   89]
per-ex loss: 0.423723  [   48/   89]
per-ex loss: 0.598871  [   49/   89]
per-ex loss: 0.589897  [   50/   89]
per-ex loss: 0.398858  [   51/   89]
per-ex loss: 0.544663  [   52/   89]
per-ex loss: 0.336955  [   53/   89]
per-ex loss: 0.483795  [   54/   89]
per-ex loss: 0.269306  [   55/   89]
per-ex loss: 0.348151  [   56/   89]
per-ex loss: 0.422687  [   57/   89]
per-ex loss: 0.228369  [   58/   89]
per-ex loss: 0.327085  [   59/   89]
per-ex loss: 0.409491  [   60/   89]
per-ex loss: 0.484222  [   61/   89]
per-ex loss: 0.316890  [   62/   89]
per-ex loss: 0.443176  [   63/   89]
per-ex loss: 0.407679  [   64/   89]
per-ex loss: 0.309118  [   65/   89]
per-ex loss: 0.346676  [   66/   89]
per-ex loss: 0.288405  [   67/   89]
per-ex loss: 0.436922  [   68/   89]
per-ex loss: 0.233102  [   69/   89]
per-ex loss: 0.508064  [   70/   89]
per-ex loss: 0.268302  [   71/   89]
per-ex loss: 0.308161  [   72/   89]
per-ex loss: 0.384240  [   73/   89]
per-ex loss: 0.295988  [   74/   89]
per-ex loss: 0.328118  [   75/   89]
per-ex loss: 0.231182  [   76/   89]
per-ex loss: 0.276769  [   77/   89]
per-ex loss: 0.354422  [   78/   89]
per-ex loss: 0.347910  [   79/   89]
per-ex loss: 0.353100  [   80/   89]
per-ex loss: 0.232541  [   81/   89]
per-ex loss: 0.396584  [   82/   89]
per-ex loss: 0.540784  [   83/   89]
per-ex loss: 0.204596  [   84/   89]
per-ex loss: 0.286700  [   85/   89]
per-ex loss: 0.323755  [   86/   89]
per-ex loss: 0.355086  [   87/   89]
per-ex loss: 0.427914  [   88/   89]
per-ex loss: 0.263905  [   89/   89]
Train Error: Avg loss: 0.36166496
validation Error: 
 Avg loss: 0.53799351 
 F1: 0.493463 
 Precision: 0.548322 
 Recall: 0.448583
 IoU: 0.327548

test Error: 
 Avg loss: 0.48884768 
 F1: 0.557558 
 Precision: 0.589382 
 Recall: 0.528994
 IoU: 0.386537

We have finished training iteration 193
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_191_.pth
per-ex loss: 0.396033  [    1/   89]
per-ex loss: 0.235269  [    2/   89]
per-ex loss: 0.492177  [    3/   89]
per-ex loss: 0.278963  [    4/   89]
per-ex loss: 0.450078  [    5/   89]
per-ex loss: 0.298719  [    6/   89]
per-ex loss: 0.279312  [    7/   89]
per-ex loss: 0.503927  [    8/   89]
per-ex loss: 0.315333  [    9/   89]
per-ex loss: 0.290044  [   10/   89]
per-ex loss: 0.190419  [   11/   89]
per-ex loss: 0.574574  [   12/   89]
per-ex loss: 0.152387  [   13/   89]
per-ex loss: 0.476021  [   14/   89]
per-ex loss: 0.241026  [   15/   89]
per-ex loss: 0.472651  [   16/   89]
per-ex loss: 0.417453  [   17/   89]
per-ex loss: 0.286526  [   18/   89]
per-ex loss: 0.310296  [   19/   89]
per-ex loss: 0.481082  [   20/   89]
per-ex loss: 0.266210  [   21/   89]
per-ex loss: 0.313048  [   22/   89]
per-ex loss: 0.283922  [   23/   89]
per-ex loss: 0.549853  [   24/   89]
per-ex loss: 0.303888  [   25/   89]
per-ex loss: 0.556027  [   26/   89]
per-ex loss: 0.257338  [   27/   89]
per-ex loss: 0.368202  [   28/   89]
per-ex loss: 0.510097  [   29/   89]
per-ex loss: 0.331620  [   30/   89]
per-ex loss: 0.331665  [   31/   89]
per-ex loss: 0.330810  [   32/   89]
per-ex loss: 0.275287  [   33/   89]
per-ex loss: 0.362453  [   34/   89]
per-ex loss: 0.299184  [   35/   89]
per-ex loss: 0.463122  [   36/   89]
per-ex loss: 0.373707  [   37/   89]
per-ex loss: 0.392768  [   38/   89]
per-ex loss: 0.315481  [   39/   89]
per-ex loss: 0.326277  [   40/   89]
per-ex loss: 0.999912  [   41/   89]
per-ex loss: 0.310144  [   42/   89]
per-ex loss: 0.396709  [   43/   89]
per-ex loss: 0.289090  [   44/   89]
per-ex loss: 0.549823  [   45/   89]
per-ex loss: 0.289534  [   46/   89]
per-ex loss: 0.270038  [   47/   89]
per-ex loss: 0.338565  [   48/   89]
per-ex loss: 0.478300  [   49/   89]
per-ex loss: 0.450188  [   50/   89]
per-ex loss: 0.434748  [   51/   89]
per-ex loss: 0.324521  [   52/   89]
per-ex loss: 0.531046  [   53/   89]
per-ex loss: 0.380693  [   54/   89]
per-ex loss: 0.420733  [   55/   89]
per-ex loss: 0.255624  [   56/   89]
per-ex loss: 0.139777  [   57/   89]
per-ex loss: 0.258144  [   58/   89]
per-ex loss: 0.315457  [   59/   89]
per-ex loss: 0.270690  [   60/   89]
per-ex loss: 0.443209  [   61/   89]
per-ex loss: 0.280016  [   62/   89]
per-ex loss: 0.398749  [   63/   89]
per-ex loss: 0.343551  [   64/   89]
per-ex loss: 0.361939  [   65/   89]
per-ex loss: 0.275018  [   66/   89]
per-ex loss: 0.495113  [   67/   89]
per-ex loss: 0.351104  [   68/   89]
per-ex loss: 0.401560  [   69/   89]
per-ex loss: 0.232379  [   70/   89]
per-ex loss: 0.339951  [   71/   89]
per-ex loss: 0.492059  [   72/   89]
per-ex loss: 0.378410  [   73/   89]
per-ex loss: 0.408371  [   74/   89]
per-ex loss: 0.278691  [   75/   89]
per-ex loss: 0.286150  [   76/   89]
per-ex loss: 0.338747  [   77/   89]
per-ex loss: 0.509341  [   78/   89]
per-ex loss: 0.346011  [   79/   89]
per-ex loss: 0.541500  [   80/   89]
per-ex loss: 0.228910  [   81/   89]
per-ex loss: 0.303134  [   82/   89]
per-ex loss: 0.416671  [   83/   89]
per-ex loss: 0.218970  [   84/   89]
per-ex loss: 0.160384  [   85/   89]
per-ex loss: 0.330073  [   86/   89]
per-ex loss: 0.268108  [   87/   89]
per-ex loss: 0.246507  [   88/   89]
per-ex loss: 0.285421  [   89/   89]
Train Error: Avg loss: 0.35974193
validation Error: 
 Avg loss: 0.53783867 
 F1: 0.486800 
 Precision: 0.545901 
 Recall: 0.439246
 IoU: 0.321702

test Error: 
 Avg loss: 0.49338832 
 F1: 0.552433 
 Precision: 0.605799 
 Recall: 0.507707
 IoU: 0.381628

We have finished training iteration 194
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_192_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.332843  [    1/   89]
per-ex loss: 0.321973  [    2/   89]
per-ex loss: 0.388944  [    3/   89]
per-ex loss: 0.341951  [    4/   89]
per-ex loss: 0.367233  [    5/   89]
per-ex loss: 0.307382  [    6/   89]
per-ex loss: 0.309718  [    7/   89]
per-ex loss: 0.232330  [    8/   89]
per-ex loss: 0.477505  [    9/   89]
per-ex loss: 0.297942  [   10/   89]
per-ex loss: 0.537153  [   11/   89]
per-ex loss: 0.999879  [   12/   89]
per-ex loss: 0.272676  [   13/   89]
per-ex loss: 0.454422  [   14/   89]
per-ex loss: 0.331878  [   15/   89]
per-ex loss: 0.503071  [   16/   89]
per-ex loss: 0.264038  [   17/   89]
per-ex loss: 0.282035  [   18/   89]
per-ex loss: 0.280888  [   19/   89]
per-ex loss: 0.221579  [   20/   89]
per-ex loss: 0.278386  [   21/   89]
per-ex loss: 0.479334  [   22/   89]
per-ex loss: 0.418227  [   23/   89]
per-ex loss: 0.290297  [   24/   89]
per-ex loss: 0.402714  [   25/   89]
per-ex loss: 0.171164  [   26/   89]
per-ex loss: 0.414029  [   27/   89]
per-ex loss: 0.219367  [   28/   89]
per-ex loss: 0.282705  [   29/   89]
per-ex loss: 0.354859  [   30/   89]
per-ex loss: 0.265546  [   31/   89]
per-ex loss: 0.326701  [   32/   89]
per-ex loss: 0.327240  [   33/   89]
per-ex loss: 0.338829  [   34/   89]
per-ex loss: 0.437421  [   35/   89]
per-ex loss: 0.579159  [   36/   89]
per-ex loss: 0.262675  [   37/   89]
per-ex loss: 0.369449  [   38/   89]
per-ex loss: 0.307517  [   39/   89]
per-ex loss: 0.275860  [   40/   89]
per-ex loss: 0.395598  [   41/   89]
per-ex loss: 0.531663  [   42/   89]
per-ex loss: 0.251397  [   43/   89]
per-ex loss: 0.150600  [   44/   89]
per-ex loss: 0.540970  [   45/   89]
per-ex loss: 0.437761  [   46/   89]
per-ex loss: 0.503641  [   47/   89]
per-ex loss: 0.200170  [   48/   89]
per-ex loss: 0.286656  [   49/   89]
per-ex loss: 0.328848  [   50/   89]
per-ex loss: 0.256816  [   51/   89]
per-ex loss: 0.313721  [   52/   89]
per-ex loss: 0.340066  [   53/   89]
per-ex loss: 0.289348  [   54/   89]
per-ex loss: 0.352638  [   55/   89]
per-ex loss: 0.386237  [   56/   89]
per-ex loss: 0.274785  [   57/   89]
per-ex loss: 0.606206  [   58/   89]
per-ex loss: 0.414244  [   59/   89]
per-ex loss: 0.243740  [   60/   89]
per-ex loss: 0.296633  [   61/   89]
per-ex loss: 0.341953  [   62/   89]
per-ex loss: 0.281020  [   63/   89]
per-ex loss: 0.384524  [   64/   89]
per-ex loss: 0.295812  [   65/   89]
per-ex loss: 0.391396  [   66/   89]
per-ex loss: 0.147148  [   67/   89]
per-ex loss: 0.304677  [   68/   89]
per-ex loss: 0.457077  [   69/   89]
per-ex loss: 0.556305  [   70/   89]
per-ex loss: 0.198787  [   71/   89]
per-ex loss: 0.379438  [   72/   89]
per-ex loss: 0.241473  [   73/   89]
per-ex loss: 0.484818  [   74/   89]
per-ex loss: 0.336713  [   75/   89]
per-ex loss: 0.541736  [   76/   89]
per-ex loss: 0.330942  [   77/   89]
per-ex loss: 0.318506  [   78/   89]
per-ex loss: 0.333383  [   79/   89]
per-ex loss: 0.445288  [   80/   89]
per-ex loss: 0.432098  [   81/   89]
per-ex loss: 0.449885  [   82/   89]
per-ex loss: 0.451109  [   83/   89]
per-ex loss: 0.327055  [   84/   89]
per-ex loss: 0.242633  [   85/   89]
per-ex loss: 0.328507  [   86/   89]
per-ex loss: 0.197170  [   87/   89]
per-ex loss: 0.428519  [   88/   89]
per-ex loss: 0.370408  [   89/   89]
Train Error: Avg loss: 0.35648354
validation Error: 
 Avg loss: 0.54524866 
 F1: 0.476642 
 Precision: 0.448378 
 Recall: 0.508709
 IoU: 0.312889

test Error: 
 Avg loss: 0.50136244 
 F1: 0.537573 
 Precision: 0.485510 
 Recall: 0.602143
 IoU: 0.367590

We have finished training iteration 195
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_193_.pth
per-ex loss: 0.330300  [    1/   89]
per-ex loss: 0.377650  [    2/   89]
per-ex loss: 0.237566  [    3/   89]
per-ex loss: 0.253825  [    4/   89]
per-ex loss: 0.322852  [    5/   89]
per-ex loss: 0.492746  [    6/   89]
per-ex loss: 0.335942  [    7/   89]
per-ex loss: 0.238096  [    8/   89]
per-ex loss: 0.498302  [    9/   89]
per-ex loss: 0.329399  [   10/   89]
per-ex loss: 0.311488  [   11/   89]
per-ex loss: 0.293148  [   12/   89]
per-ex loss: 0.439416  [   13/   89]
per-ex loss: 0.536554  [   14/   89]
per-ex loss: 0.335036  [   15/   89]
per-ex loss: 0.325826  [   16/   89]
per-ex loss: 0.337109  [   17/   89]
per-ex loss: 0.338316  [   18/   89]
per-ex loss: 0.280503  [   19/   89]
per-ex loss: 0.374424  [   20/   89]
per-ex loss: 0.334260  [   21/   89]
per-ex loss: 0.255373  [   22/   89]
per-ex loss: 0.340769  [   23/   89]
per-ex loss: 0.347117  [   24/   89]
per-ex loss: 0.430522  [   25/   89]
per-ex loss: 0.246668  [   26/   89]
per-ex loss: 0.496384  [   27/   89]
per-ex loss: 0.448813  [   28/   89]
per-ex loss: 0.439234  [   29/   89]
per-ex loss: 0.333276  [   30/   89]
per-ex loss: 0.323886  [   31/   89]
per-ex loss: 0.348665  [   32/   89]
per-ex loss: 0.350259  [   33/   89]
per-ex loss: 0.359252  [   34/   89]
per-ex loss: 0.451059  [   35/   89]
per-ex loss: 0.278328  [   36/   89]
per-ex loss: 0.345252  [   37/   89]
per-ex loss: 0.268678  [   38/   89]
per-ex loss: 0.210885  [   39/   89]
per-ex loss: 0.290492  [   40/   89]
per-ex loss: 0.245542  [   41/   89]
per-ex loss: 0.299344  [   42/   89]
per-ex loss: 0.206278  [   43/   89]
per-ex loss: 0.280936  [   44/   89]
per-ex loss: 0.593500  [   45/   89]
per-ex loss: 0.331325  [   46/   89]
per-ex loss: 0.273982  [   47/   89]
per-ex loss: 0.304775  [   48/   89]
per-ex loss: 0.339805  [   49/   89]
per-ex loss: 0.479157  [   50/   89]
per-ex loss: 0.302733  [   51/   89]
per-ex loss: 0.406804  [   52/   89]
per-ex loss: 0.379772  [   53/   89]
per-ex loss: 0.492122  [   54/   89]
per-ex loss: 0.554591  [   55/   89]
per-ex loss: 0.537410  [   56/   89]
per-ex loss: 0.224805  [   57/   89]
per-ex loss: 0.541283  [   58/   89]
per-ex loss: 0.270583  [   59/   89]
per-ex loss: 0.276217  [   60/   89]
per-ex loss: 0.999892  [   61/   89]
per-ex loss: 0.315157  [   62/   89]
per-ex loss: 0.340551  [   63/   89]
per-ex loss: 0.335327  [   64/   89]
per-ex loss: 0.334236  [   65/   89]
per-ex loss: 0.284298  [   66/   89]
per-ex loss: 0.234043  [   67/   89]
per-ex loss: 0.539275  [   68/   89]
per-ex loss: 0.390361  [   69/   89]
per-ex loss: 0.423901  [   70/   89]
per-ex loss: 0.252959  [   71/   89]
per-ex loss: 0.266555  [   72/   89]
per-ex loss: 0.424360  [   73/   89]
per-ex loss: 0.265969  [   74/   89]
per-ex loss: 0.178366  [   75/   89]
per-ex loss: 0.381448  [   76/   89]
per-ex loss: 0.340191  [   77/   89]
per-ex loss: 0.351650  [   78/   89]
per-ex loss: 0.247154  [   79/   89]
per-ex loss: 0.441185  [   80/   89]
per-ex loss: 0.337639  [   81/   89]
per-ex loss: 0.406873  [   82/   89]
per-ex loss: 0.231055  [   83/   89]
per-ex loss: 0.545636  [   84/   89]
per-ex loss: 0.501045  [   85/   89]
per-ex loss: 0.410683  [   86/   89]
per-ex loss: 0.264013  [   87/   89]
per-ex loss: 0.177947  [   88/   89]
per-ex loss: 0.385278  [   89/   89]
Train Error: Avg loss: 0.35770428
validation Error: 
 Avg loss: 0.52661423 
 F1: 0.499993 
 Precision: 0.559015 
 Recall: 0.452244
 IoU: 0.333327

test Error: 
 Avg loss: 0.48408031 
 F1: 0.560279 
 Precision: 0.597228 
 Recall: 0.527634
 IoU: 0.389158

We have finished training iteration 196
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_194_.pth
per-ex loss: 0.312858  [    1/   89]
per-ex loss: 0.264329  [    2/   89]
per-ex loss: 0.329732  [    3/   89]
per-ex loss: 0.312454  [    4/   89]
per-ex loss: 0.522095  [    5/   89]
per-ex loss: 0.401583  [    6/   89]
per-ex loss: 0.387647  [    7/   89]
per-ex loss: 0.233925  [    8/   89]
per-ex loss: 0.432254  [    9/   89]
per-ex loss: 0.260607  [   10/   89]
per-ex loss: 0.298620  [   11/   89]
per-ex loss: 0.289943  [   12/   89]
per-ex loss: 0.359260  [   13/   89]
per-ex loss: 0.387014  [   14/   89]
per-ex loss: 0.609939  [   15/   89]
per-ex loss: 0.398679  [   16/   89]
per-ex loss: 0.164222  [   17/   89]
per-ex loss: 0.411797  [   18/   89]
per-ex loss: 0.224954  [   19/   89]
per-ex loss: 0.447354  [   20/   89]
per-ex loss: 0.377368  [   21/   89]
per-ex loss: 0.299237  [   22/   89]
per-ex loss: 0.291887  [   23/   89]
per-ex loss: 0.249688  [   24/   89]
per-ex loss: 0.226199  [   25/   89]
per-ex loss: 0.309770  [   26/   89]
per-ex loss: 0.465698  [   27/   89]
per-ex loss: 0.274782  [   28/   89]
per-ex loss: 0.281685  [   29/   89]
per-ex loss: 0.311839  [   30/   89]
per-ex loss: 0.289851  [   31/   89]
per-ex loss: 0.272514  [   32/   89]
per-ex loss: 0.239042  [   33/   89]
per-ex loss: 0.433074  [   34/   89]
per-ex loss: 0.313761  [   35/   89]
per-ex loss: 0.426423  [   36/   89]
per-ex loss: 0.225361  [   37/   89]
per-ex loss: 0.362422  [   38/   89]
per-ex loss: 0.289739  [   39/   89]
per-ex loss: 0.281665  [   40/   89]
per-ex loss: 0.260103  [   41/   89]
per-ex loss: 0.342749  [   42/   89]
per-ex loss: 0.381746  [   43/   89]
per-ex loss: 0.357064  [   44/   89]
per-ex loss: 0.311575  [   45/   89]
per-ex loss: 0.345693  [   46/   89]
per-ex loss: 0.336219  [   47/   89]
per-ex loss: 0.219222  [   48/   89]
per-ex loss: 0.196275  [   49/   89]
per-ex loss: 0.338189  [   50/   89]
per-ex loss: 0.598515  [   51/   89]
per-ex loss: 0.323689  [   52/   89]
per-ex loss: 0.437968  [   53/   89]
per-ex loss: 0.310511  [   54/   89]
per-ex loss: 0.486100  [   55/   89]
per-ex loss: 0.999880  [   56/   89]
per-ex loss: 0.561320  [   57/   89]
per-ex loss: 0.448893  [   58/   89]
per-ex loss: 0.569035  [   59/   89]
per-ex loss: 0.273804  [   60/   89]
per-ex loss: 0.226401  [   61/   89]
per-ex loss: 0.281089  [   62/   89]
per-ex loss: 0.271055  [   63/   89]
per-ex loss: 0.201453  [   64/   89]
per-ex loss: 0.255754  [   65/   89]
per-ex loss: 0.279680  [   66/   89]
per-ex loss: 0.479756  [   67/   89]
per-ex loss: 0.582686  [   68/   89]
per-ex loss: 0.323474  [   69/   89]
per-ex loss: 0.198372  [   70/   89]
per-ex loss: 0.419322  [   71/   89]
per-ex loss: 0.560165  [   72/   89]
per-ex loss: 0.500442  [   73/   89]
per-ex loss: 0.279910  [   74/   89]
per-ex loss: 0.387870  [   75/   89]
per-ex loss: 0.262161  [   76/   89]
per-ex loss: 0.322377  [   77/   89]
per-ex loss: 0.533193  [   78/   89]
per-ex loss: 0.321777  [   79/   89]
per-ex loss: 0.376247  [   80/   89]
per-ex loss: 0.302390  [   81/   89]
per-ex loss: 0.382136  [   82/   89]
per-ex loss: 0.287208  [   83/   89]
per-ex loss: 0.232967  [   84/   89]
per-ex loss: 0.208677  [   85/   89]
per-ex loss: 0.332721  [   86/   89]
per-ex loss: 0.354075  [   87/   89]
per-ex loss: 0.337929  [   88/   89]
per-ex loss: 0.271969  [   89/   89]
Train Error: Avg loss: 0.34992218
validation Error: 
 Avg loss: 0.53165338 
 F1: 0.499825 
 Precision: 0.596473 
 Recall: 0.430130
 IoU: 0.333177

test Error: 
 Avg loss: 0.48458169 
 F1: 0.561303 
 Precision: 0.630430 
 Recall: 0.505837
 IoU: 0.390147

We have finished training iteration 197
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_195_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.281184  [    1/   89]
per-ex loss: 0.505690  [    2/   89]
per-ex loss: 0.321458  [    3/   89]
per-ex loss: 0.356783  [    4/   89]
per-ex loss: 0.298973  [    5/   89]
per-ex loss: 0.435923  [    6/   89]
per-ex loss: 0.264596  [    7/   89]
per-ex loss: 0.189880  [    8/   89]
per-ex loss: 0.318947  [    9/   89]
per-ex loss: 0.336456  [   10/   89]
per-ex loss: 0.335174  [   11/   89]
per-ex loss: 0.250764  [   12/   89]
per-ex loss: 0.282120  [   13/   89]
per-ex loss: 0.274197  [   14/   89]
per-ex loss: 0.414877  [   15/   89]
per-ex loss: 0.441917  [   16/   89]
per-ex loss: 0.374185  [   17/   89]
per-ex loss: 0.487423  [   18/   89]
per-ex loss: 0.405892  [   19/   89]
per-ex loss: 0.318625  [   20/   89]
per-ex loss: 0.342487  [   21/   89]
per-ex loss: 0.487574  [   22/   89]
per-ex loss: 0.363383  [   23/   89]
per-ex loss: 0.500783  [   24/   89]
per-ex loss: 0.388241  [   25/   89]
per-ex loss: 0.224235  [   26/   89]
per-ex loss: 0.282092  [   27/   89]
per-ex loss: 0.480449  [   28/   89]
per-ex loss: 0.292434  [   29/   89]
per-ex loss: 0.488414  [   30/   89]
per-ex loss: 0.220010  [   31/   89]
per-ex loss: 0.216360  [   32/   89]
per-ex loss: 0.342346  [   33/   89]
per-ex loss: 0.261342  [   34/   89]
per-ex loss: 0.236237  [   35/   89]
per-ex loss: 0.350883  [   36/   89]
per-ex loss: 0.414211  [   37/   89]
per-ex loss: 0.173912  [   38/   89]
per-ex loss: 0.286734  [   39/   89]
per-ex loss: 0.156886  [   40/   89]
per-ex loss: 0.537122  [   41/   89]
per-ex loss: 0.268749  [   42/   89]
per-ex loss: 0.277181  [   43/   89]
per-ex loss: 0.226379  [   44/   89]
per-ex loss: 0.288422  [   45/   89]
per-ex loss: 0.211559  [   46/   89]
per-ex loss: 0.402853  [   47/   89]
per-ex loss: 0.567839  [   48/   89]
per-ex loss: 0.481757  [   49/   89]
per-ex loss: 0.247128  [   50/   89]
per-ex loss: 0.147381  [   51/   89]
per-ex loss: 0.285423  [   52/   89]
per-ex loss: 0.322547  [   53/   89]
per-ex loss: 0.270572  [   54/   89]
per-ex loss: 0.287759  [   55/   89]
per-ex loss: 0.243588  [   56/   89]
per-ex loss: 0.408446  [   57/   89]
per-ex loss: 0.424012  [   58/   89]
per-ex loss: 0.290167  [   59/   89]
per-ex loss: 0.283137  [   60/   89]
per-ex loss: 0.284317  [   61/   89]
per-ex loss: 0.138833  [   62/   89]
per-ex loss: 0.311415  [   63/   89]
per-ex loss: 0.999879  [   64/   89]
per-ex loss: 0.530549  [   65/   89]
per-ex loss: 0.460685  [   66/   89]
per-ex loss: 0.397769  [   67/   89]
per-ex loss: 0.537085  [   68/   89]
per-ex loss: 0.315971  [   69/   89]
per-ex loss: 0.406838  [   70/   89]
per-ex loss: 0.307595  [   71/   89]
per-ex loss: 0.428237  [   72/   89]
per-ex loss: 0.460926  [   73/   89]
per-ex loss: 0.233122  [   74/   89]
per-ex loss: 0.322774  [   75/   89]
per-ex loss: 0.559236  [   76/   89]
per-ex loss: 0.252630  [   77/   89]
per-ex loss: 0.385130  [   78/   89]
per-ex loss: 0.275419  [   79/   89]
per-ex loss: 0.290159  [   80/   89]
per-ex loss: 0.329229  [   81/   89]
per-ex loss: 0.411726  [   82/   89]
per-ex loss: 0.352604  [   83/   89]
per-ex loss: 0.447832  [   84/   89]
per-ex loss: 0.349626  [   85/   89]
per-ex loss: 0.240824  [   86/   89]
per-ex loss: 0.231478  [   87/   89]
per-ex loss: 0.450822  [   88/   89]
per-ex loss: 0.320055  [   89/   89]
Train Error: Avg loss: 0.34729062
validation Error: 
 Avg loss: 0.54040411 
 F1: 0.482775 
 Precision: 0.466145 
 Recall: 0.500636
 IoU: 0.318196

test Error: 
 Avg loss: 0.49114281 
 F1: 0.548952 
 Precision: 0.513439 
 Recall: 0.589743
 IoU: 0.378314

We have finished training iteration 198
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_196_.pth
per-ex loss: 0.436949  [    1/   89]
per-ex loss: 0.305588  [    2/   89]
per-ex loss: 0.474004  [    3/   89]
per-ex loss: 0.412447  [    4/   89]
per-ex loss: 0.544963  [    5/   89]
per-ex loss: 0.418521  [    6/   89]
per-ex loss: 0.322037  [    7/   89]
per-ex loss: 0.416760  [    8/   89]
per-ex loss: 0.281725  [    9/   89]
per-ex loss: 0.507252  [   10/   89]
per-ex loss: 0.319519  [   11/   89]
per-ex loss: 0.471541  [   12/   89]
per-ex loss: 0.504157  [   13/   89]
per-ex loss: 0.405917  [   14/   89]
per-ex loss: 0.241028  [   15/   89]
per-ex loss: 0.267547  [   16/   89]
per-ex loss: 0.540450  [   17/   89]
per-ex loss: 0.285968  [   18/   89]
per-ex loss: 0.226129  [   19/   89]
per-ex loss: 0.319338  [   20/   89]
per-ex loss: 0.227446  [   21/   89]
per-ex loss: 0.312426  [   22/   89]
per-ex loss: 0.411870  [   23/   89]
per-ex loss: 0.565873  [   24/   89]
per-ex loss: 0.227580  [   25/   89]
per-ex loss: 0.490032  [   26/   89]
per-ex loss: 0.248451  [   27/   89]
per-ex loss: 0.306263  [   28/   89]
per-ex loss: 0.294564  [   29/   89]
per-ex loss: 0.251563  [   30/   89]
per-ex loss: 0.302639  [   31/   89]
per-ex loss: 0.257190  [   32/   89]
per-ex loss: 0.149279  [   33/   89]
per-ex loss: 0.363620  [   34/   89]
per-ex loss: 0.280946  [   35/   89]
per-ex loss: 0.294179  [   36/   89]
per-ex loss: 0.281274  [   37/   89]
per-ex loss: 0.454300  [   38/   89]
per-ex loss: 0.168883  [   39/   89]
per-ex loss: 0.319021  [   40/   89]
per-ex loss: 0.437932  [   41/   89]
per-ex loss: 0.523353  [   42/   89]
per-ex loss: 0.337987  [   43/   89]
per-ex loss: 0.272257  [   44/   89]
per-ex loss: 0.286456  [   45/   89]
per-ex loss: 0.394377  [   46/   89]
per-ex loss: 0.296409  [   47/   89]
per-ex loss: 0.286431  [   48/   89]
per-ex loss: 0.420131  [   49/   89]
per-ex loss: 0.999919  [   50/   89]
per-ex loss: 0.258941  [   51/   89]
per-ex loss: 0.203317  [   52/   89]
per-ex loss: 0.399670  [   53/   89]
per-ex loss: 0.336680  [   54/   89]
per-ex loss: 0.349435  [   55/   89]
per-ex loss: 0.368283  [   56/   89]
per-ex loss: 0.333168  [   57/   89]
per-ex loss: 0.271779  [   58/   89]
per-ex loss: 0.365044  [   59/   89]
per-ex loss: 0.392794  [   60/   89]
per-ex loss: 0.200518  [   61/   89]
per-ex loss: 0.581281  [   62/   89]
per-ex loss: 0.491404  [   63/   89]
per-ex loss: 0.435632  [   64/   89]
per-ex loss: 0.536269  [   65/   89]
per-ex loss: 0.381456  [   66/   89]
per-ex loss: 0.338954  [   67/   89]
per-ex loss: 0.238602  [   68/   89]
per-ex loss: 0.178183  [   69/   89]
per-ex loss: 0.223260  [   70/   89]
per-ex loss: 0.310218  [   71/   89]
per-ex loss: 0.372108  [   72/   89]
per-ex loss: 0.430916  [   73/   89]
per-ex loss: 0.287313  [   74/   89]
per-ex loss: 0.293693  [   75/   89]
per-ex loss: 0.317514  [   76/   89]
per-ex loss: 0.211227  [   77/   89]
per-ex loss: 0.444662  [   78/   89]
per-ex loss: 0.276176  [   79/   89]
per-ex loss: 0.380417  [   80/   89]
per-ex loss: 0.516892  [   81/   89]
per-ex loss: 0.363298  [   82/   89]
per-ex loss: 0.362191  [   83/   89]
per-ex loss: 0.268110  [   84/   89]
per-ex loss: 0.312079  [   85/   89]
per-ex loss: 0.292204  [   86/   89]
per-ex loss: 0.269633  [   87/   89]
per-ex loss: 0.303385  [   88/   89]
per-ex loss: 0.330630  [   89/   89]
Train Error: Avg loss: 0.35271714
validation Error: 
 Avg loss: 0.54256336 
 F1: 0.484305 
 Precision: 0.499126 
 Recall: 0.470338
 IoU: 0.319526

test Error: 
 Avg loss: 0.49501280 
 F1: 0.549209 
 Precision: 0.551132 
 Recall: 0.547300
 IoU: 0.378559

We have finished training iteration 199
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_197_.pth
per-ex loss: 0.368708  [    1/   89]
per-ex loss: 0.483131  [    2/   89]
per-ex loss: 0.257797  [    3/   89]
per-ex loss: 0.278218  [    4/   89]
per-ex loss: 0.434640  [    5/   89]
per-ex loss: 0.309379  [    6/   89]
per-ex loss: 0.366520  [    7/   89]
per-ex loss: 0.325492  [    8/   89]
per-ex loss: 0.335050  [    9/   89]
per-ex loss: 0.326186  [   10/   89]
per-ex loss: 0.251035  [   11/   89]
per-ex loss: 0.280126  [   12/   89]
per-ex loss: 0.260149  [   13/   89]
per-ex loss: 0.252639  [   14/   89]
per-ex loss: 0.498651  [   15/   89]
per-ex loss: 0.551509  [   16/   89]
per-ex loss: 0.296185  [   17/   89]
per-ex loss: 0.200413  [   18/   89]
per-ex loss: 0.362569  [   19/   89]
per-ex loss: 0.283907  [   20/   89]
per-ex loss: 0.496123  [   21/   89]
per-ex loss: 0.307034  [   22/   89]
per-ex loss: 0.318695  [   23/   89]
per-ex loss: 0.236591  [   24/   89]
per-ex loss: 0.414156  [   25/   89]
per-ex loss: 0.344834  [   26/   89]
per-ex loss: 0.270230  [   27/   89]
per-ex loss: 0.517387  [   28/   89]
per-ex loss: 0.526759  [   29/   89]
per-ex loss: 0.492270  [   30/   89]
per-ex loss: 0.999879  [   31/   89]
per-ex loss: 0.320217  [   32/   89]
per-ex loss: 0.322250  [   33/   89]
per-ex loss: 0.463596  [   34/   89]
per-ex loss: 0.395015  [   35/   89]
per-ex loss: 0.506541  [   36/   89]
per-ex loss: 0.242695  [   37/   89]
per-ex loss: 0.274627  [   38/   89]
per-ex loss: 0.310437  [   39/   89]
per-ex loss: 0.451120  [   40/   89]
per-ex loss: 0.388878  [   41/   89]
per-ex loss: 0.290906  [   42/   89]
per-ex loss: 0.379156  [   43/   89]
per-ex loss: 0.217510  [   44/   89]
per-ex loss: 0.479874  [   45/   89]
per-ex loss: 0.176765  [   46/   89]
per-ex loss: 0.269712  [   47/   89]
per-ex loss: 0.440209  [   48/   89]
per-ex loss: 0.261061  [   49/   89]
per-ex loss: 0.245860  [   50/   89]
per-ex loss: 0.553497  [   51/   89]
per-ex loss: 0.426566  [   52/   89]
per-ex loss: 0.482362  [   53/   89]
per-ex loss: 0.402936  [   54/   89]
per-ex loss: 0.388408  [   55/   89]
per-ex loss: 0.532637  [   56/   89]
per-ex loss: 0.371341  [   57/   89]
per-ex loss: 0.449519  [   58/   89]
per-ex loss: 0.380856  [   59/   89]
per-ex loss: 0.297248  [   60/   89]
per-ex loss: 0.551061  [   61/   89]
per-ex loss: 0.246358  [   62/   89]
per-ex loss: 0.297348  [   63/   89]
per-ex loss: 0.348235  [   64/   89]
per-ex loss: 0.390403  [   65/   89]
per-ex loss: 0.210582  [   66/   89]
per-ex loss: 0.385919  [   67/   89]
per-ex loss: 0.614706  [   68/   89]
per-ex loss: 0.347310  [   69/   89]
per-ex loss: 0.572529  [   70/   89]
per-ex loss: 0.243593  [   71/   89]
per-ex loss: 0.475516  [   72/   89]
per-ex loss: 0.323394  [   73/   89]
per-ex loss: 0.257152  [   74/   89]
per-ex loss: 0.154174  [   75/   89]
per-ex loss: 0.406089  [   76/   89]
per-ex loss: 0.346958  [   77/   89]
per-ex loss: 0.266679  [   78/   89]
per-ex loss: 0.268697  [   79/   89]
per-ex loss: 0.219138  [   80/   89]
per-ex loss: 0.388743  [   81/   89]
per-ex loss: 0.284639  [   82/   89]
per-ex loss: 0.269606  [   83/   89]
per-ex loss: 0.152093  [   84/   89]
per-ex loss: 0.315399  [   85/   89]
per-ex loss: 0.388914  [   86/   89]
per-ex loss: 0.504348  [   87/   89]
per-ex loss: 0.257725  [   88/   89]
per-ex loss: 0.296137  [   89/   89]
Train Error: Avg loss: 0.36125286
validation Error: 
 Avg loss: 0.53216991 
 F1: 0.493395 
 Precision: 0.548334 
 Recall: 0.448462
 IoU: 0.327488

test Error: 
 Avg loss: 0.48111407 
 F1: 0.565528 
 Precision: 0.613839 
 Recall: 0.524266
 IoU: 0.394241

We have finished training iteration 200
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_198_.pth
per-ex loss: 0.213808  [    1/   89]
per-ex loss: 0.276603  [    2/   89]
per-ex loss: 0.280795  [    3/   89]
per-ex loss: 0.312380  [    4/   89]
per-ex loss: 0.378512  [    5/   89]
per-ex loss: 0.163330  [    6/   89]
per-ex loss: 0.534551  [    7/   89]
per-ex loss: 0.289131  [    8/   89]
per-ex loss: 0.552875  [    9/   89]
per-ex loss: 0.380137  [   10/   89]
per-ex loss: 0.434301  [   11/   89]
per-ex loss: 0.474946  [   12/   89]
per-ex loss: 0.239266  [   13/   89]
per-ex loss: 0.194243  [   14/   89]
per-ex loss: 0.354364  [   15/   89]
per-ex loss: 0.307277  [   16/   89]
per-ex loss: 0.231225  [   17/   89]
per-ex loss: 0.350023  [   18/   89]
per-ex loss: 0.225963  [   19/   89]
per-ex loss: 0.288309  [   20/   89]
per-ex loss: 0.156352  [   21/   89]
per-ex loss: 0.375017  [   22/   89]
per-ex loss: 0.331545  [   23/   89]
per-ex loss: 0.446231  [   24/   89]
per-ex loss: 0.356400  [   25/   89]
per-ex loss: 0.499101  [   26/   89]
per-ex loss: 0.204508  [   27/   89]
per-ex loss: 0.213557  [   28/   89]
per-ex loss: 0.370680  [   29/   89]
per-ex loss: 0.468424  [   30/   89]
per-ex loss: 0.280252  [   31/   89]
per-ex loss: 0.333191  [   32/   89]
per-ex loss: 0.285874  [   33/   89]
per-ex loss: 0.258126  [   34/   89]
per-ex loss: 0.279900  [   35/   89]
per-ex loss: 0.338609  [   36/   89]
per-ex loss: 0.496172  [   37/   89]
per-ex loss: 0.381186  [   38/   89]
per-ex loss: 0.305287  [   39/   89]
per-ex loss: 0.334753  [   40/   89]
per-ex loss: 0.484798  [   41/   89]
per-ex loss: 0.999894  [   42/   89]
per-ex loss: 0.558421  [   43/   89]
per-ex loss: 0.504762  [   44/   89]
per-ex loss: 0.277817  [   45/   89]
per-ex loss: 0.450325  [   46/   89]
per-ex loss: 0.356681  [   47/   89]
per-ex loss: 0.360738  [   48/   89]
per-ex loss: 0.332370  [   49/   89]
per-ex loss: 0.299344  [   50/   89]
per-ex loss: 0.192173  [   51/   89]
per-ex loss: 0.150091  [   52/   89]
per-ex loss: 0.437136  [   53/   89]
per-ex loss: 0.420989  [   54/   89]
per-ex loss: 0.352801  [   55/   89]
per-ex loss: 0.265786  [   56/   89]
per-ex loss: 0.407103  [   57/   89]
per-ex loss: 0.336994  [   58/   89]
per-ex loss: 0.255763  [   59/   89]
per-ex loss: 0.275124  [   60/   89]
per-ex loss: 0.345977  [   61/   89]
per-ex loss: 0.276292  [   62/   89]
per-ex loss: 0.575186  [   63/   89]
per-ex loss: 0.294488  [   64/   89]
per-ex loss: 0.256626  [   65/   89]
per-ex loss: 0.208007  [   66/   89]
per-ex loss: 0.277363  [   67/   89]
per-ex loss: 0.472902  [   68/   89]
per-ex loss: 0.217414  [   69/   89]
per-ex loss: 0.359411  [   70/   89]
per-ex loss: 0.296005  [   71/   89]
per-ex loss: 0.393324  [   72/   89]
per-ex loss: 0.377877  [   73/   89]
per-ex loss: 0.306532  [   74/   89]
per-ex loss: 0.333357  [   75/   89]
per-ex loss: 0.684063  [   76/   89]
per-ex loss: 0.216192  [   77/   89]
per-ex loss: 0.487002  [   78/   89]
per-ex loss: 0.250658  [   79/   89]
per-ex loss: 0.409981  [   80/   89]
per-ex loss: 0.442305  [   81/   89]
per-ex loss: 0.326582  [   82/   89]
per-ex loss: 0.502936  [   83/   89]
per-ex loss: 0.552567  [   84/   89]
per-ex loss: 0.430256  [   85/   89]
per-ex loss: 0.287458  [   86/   89]
per-ex loss: 0.413650  [   87/   89]
per-ex loss: 0.331890  [   88/   89]
per-ex loss: 0.499189  [   89/   89]
Train Error: Avg loss: 0.35664948
validation Error: 
 Avg loss: 0.55632473 
 F1: 0.463539 
 Precision: 0.413132 
 Recall: 0.527956
 IoU: 0.301693

test Error: 
 Avg loss: 0.50343978 
 F1: 0.533550 
 Precision: 0.469401 
 Recall: 0.618007
 IoU: 0.363838

We have finished training iteration 201
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_199_.pth
per-ex loss: 0.506788  [    1/   89]
per-ex loss: 0.470112  [    2/   89]
per-ex loss: 0.432222  [    3/   89]
per-ex loss: 0.451172  [    4/   89]
per-ex loss: 0.419206  [    5/   89]
per-ex loss: 0.477991  [    6/   89]
per-ex loss: 0.306079  [    7/   89]
per-ex loss: 0.159768  [    8/   89]
per-ex loss: 0.273885  [    9/   89]
per-ex loss: 0.518630  [   10/   89]
per-ex loss: 0.273527  [   11/   89]
per-ex loss: 0.496532  [   12/   89]
per-ex loss: 0.283456  [   13/   89]
per-ex loss: 0.146065  [   14/   89]
per-ex loss: 0.258532  [   15/   89]
per-ex loss: 0.444537  [   16/   89]
per-ex loss: 0.301350  [   17/   89]
per-ex loss: 0.308579  [   18/   89]
per-ex loss: 0.539597  [   19/   89]
per-ex loss: 0.506348  [   20/   89]
per-ex loss: 0.569176  [   21/   89]
per-ex loss: 0.399292  [   22/   89]
per-ex loss: 0.544786  [   23/   89]
per-ex loss: 0.275052  [   24/   89]
per-ex loss: 0.212557  [   25/   89]
per-ex loss: 0.319613  [   26/   89]
per-ex loss: 0.329204  [   27/   89]
per-ex loss: 0.402993  [   28/   89]
per-ex loss: 0.371342  [   29/   89]
per-ex loss: 0.148905  [   30/   89]
per-ex loss: 0.363321  [   31/   89]
per-ex loss: 0.289908  [   32/   89]
per-ex loss: 0.388148  [   33/   89]
per-ex loss: 0.235560  [   34/   89]
per-ex loss: 0.246134  [   35/   89]
per-ex loss: 0.254448  [   36/   89]
per-ex loss: 0.324082  [   37/   89]
per-ex loss: 0.405033  [   38/   89]
per-ex loss: 0.340053  [   39/   89]
per-ex loss: 0.208085  [   40/   89]
per-ex loss: 0.551913  [   41/   89]
per-ex loss: 0.313933  [   42/   89]
per-ex loss: 0.252988  [   43/   89]
per-ex loss: 0.488318  [   44/   89]
per-ex loss: 0.326856  [   45/   89]
per-ex loss: 0.267887  [   46/   89]
per-ex loss: 0.393056  [   47/   89]
per-ex loss: 0.357578  [   48/   89]
per-ex loss: 0.262122  [   49/   89]
per-ex loss: 0.291415  [   50/   89]
per-ex loss: 0.324081  [   51/   89]
per-ex loss: 0.283006  [   52/   89]
per-ex loss: 0.436408  [   53/   89]
per-ex loss: 0.294098  [   54/   89]
per-ex loss: 0.999890  [   55/   89]
per-ex loss: 0.346573  [   56/   89]
per-ex loss: 0.311920  [   57/   89]
per-ex loss: 0.331451  [   58/   89]
per-ex loss: 0.462510  [   59/   89]
per-ex loss: 0.391711  [   60/   89]
per-ex loss: 0.277385  [   61/   89]
per-ex loss: 0.486928  [   62/   89]
per-ex loss: 0.251418  [   63/   89]
per-ex loss: 0.392041  [   64/   89]
per-ex loss: 0.193628  [   65/   89]
per-ex loss: 0.315842  [   66/   89]
per-ex loss: 0.372055  [   67/   89]
per-ex loss: 0.285369  [   68/   89]
per-ex loss: 0.306715  [   69/   89]
per-ex loss: 0.398950  [   70/   89]
per-ex loss: 0.239741  [   71/   89]
per-ex loss: 0.288179  [   72/   89]
per-ex loss: 0.343944  [   73/   89]
per-ex loss: 0.394462  [   74/   89]
per-ex loss: 0.407880  [   75/   89]
per-ex loss: 0.427505  [   76/   89]
per-ex loss: 0.232582  [   77/   89]
per-ex loss: 0.306380  [   78/   89]
per-ex loss: 0.337632  [   79/   89]
per-ex loss: 0.522743  [   80/   89]
per-ex loss: 0.309797  [   81/   89]
per-ex loss: 0.238477  [   82/   89]
per-ex loss: 0.257477  [   83/   89]
per-ex loss: 0.261768  [   84/   89]
per-ex loss: 0.260635  [   85/   89]
per-ex loss: 0.299745  [   86/   89]
per-ex loss: 0.329571  [   87/   89]
per-ex loss: 0.296672  [   88/   89]
per-ex loss: 0.447525  [   89/   89]
Train Error: Avg loss: 0.35250361
validation Error: 
 Avg loss: 0.54620445 
 F1: 0.475149 
 Precision: 0.517797 
 Recall: 0.438992
 IoU: 0.311604

test Error: 
 Avg loss: 0.49493007 
 F1: 0.552587 
 Precision: 0.589321 
 Recall: 0.520163
 IoU: 0.381775

We have finished training iteration 202
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_200_.pth
per-ex loss: 0.252262  [    1/   89]
per-ex loss: 0.557653  [    2/   89]
per-ex loss: 0.269635  [    3/   89]
per-ex loss: 0.526480  [    4/   89]
per-ex loss: 0.301152  [    5/   89]
per-ex loss: 0.270950  [    6/   89]
per-ex loss: 0.435370  [    7/   89]
per-ex loss: 0.257539  [    8/   89]
per-ex loss: 0.245712  [    9/   89]
per-ex loss: 0.328303  [   10/   89]
per-ex loss: 0.498566  [   11/   89]
per-ex loss: 0.424638  [   12/   89]
per-ex loss: 0.460972  [   13/   89]
per-ex loss: 0.204596  [   14/   89]
per-ex loss: 0.415831  [   15/   89]
per-ex loss: 0.439623  [   16/   89]
per-ex loss: 0.309407  [   17/   89]
per-ex loss: 0.433876  [   18/   89]
per-ex loss: 0.440628  [   19/   89]
per-ex loss: 0.290399  [   20/   89]
per-ex loss: 0.211988  [   21/   89]
per-ex loss: 0.275515  [   22/   89]
per-ex loss: 0.327288  [   23/   89]
per-ex loss: 0.469191  [   24/   89]
per-ex loss: 0.302079  [   25/   89]
per-ex loss: 0.381380  [   26/   89]
per-ex loss: 0.307855  [   27/   89]
per-ex loss: 0.403655  [   28/   89]
per-ex loss: 0.159355  [   29/   89]
per-ex loss: 0.410077  [   30/   89]
per-ex loss: 0.365022  [   31/   89]
per-ex loss: 0.359013  [   32/   89]
per-ex loss: 0.300038  [   33/   89]
per-ex loss: 0.364420  [   34/   89]
per-ex loss: 0.272797  [   35/   89]
per-ex loss: 0.312153  [   36/   89]
per-ex loss: 0.489799  [   37/   89]
per-ex loss: 0.304718  [   38/   89]
per-ex loss: 0.310331  [   39/   89]
per-ex loss: 0.262890  [   40/   89]
per-ex loss: 0.281388  [   41/   89]
per-ex loss: 0.294280  [   42/   89]
per-ex loss: 0.249091  [   43/   89]
per-ex loss: 0.223013  [   44/   89]
per-ex loss: 0.512494  [   45/   89]
per-ex loss: 0.198912  [   46/   89]
per-ex loss: 0.321576  [   47/   89]
per-ex loss: 0.257703  [   48/   89]
per-ex loss: 0.414984  [   49/   89]
per-ex loss: 0.275523  [   50/   89]
per-ex loss: 0.593606  [   51/   89]
per-ex loss: 0.272568  [   52/   89]
per-ex loss: 0.470117  [   53/   89]
per-ex loss: 0.331544  [   54/   89]
per-ex loss: 0.268963  [   55/   89]
per-ex loss: 0.431850  [   56/   89]
per-ex loss: 0.292346  [   57/   89]
per-ex loss: 0.500271  [   58/   89]
per-ex loss: 0.149448  [   59/   89]
per-ex loss: 0.195425  [   60/   89]
per-ex loss: 0.279539  [   61/   89]
per-ex loss: 0.590044  [   62/   89]
per-ex loss: 0.500796  [   63/   89]
per-ex loss: 0.332736  [   64/   89]
per-ex loss: 0.476879  [   65/   89]
per-ex loss: 0.371933  [   66/   89]
per-ex loss: 0.287993  [   67/   89]
per-ex loss: 0.532306  [   68/   89]
per-ex loss: 0.308325  [   69/   89]
per-ex loss: 0.363659  [   70/   89]
per-ex loss: 0.548530  [   71/   89]
per-ex loss: 0.385298  [   72/   89]
per-ex loss: 0.338288  [   73/   89]
per-ex loss: 0.318102  [   74/   89]
per-ex loss: 0.323836  [   75/   89]
per-ex loss: 0.286387  [   76/   89]
per-ex loss: 0.235546  [   77/   89]
per-ex loss: 0.142043  [   78/   89]
per-ex loss: 0.261852  [   79/   89]
per-ex loss: 0.256398  [   80/   89]
per-ex loss: 0.999916  [   81/   89]
per-ex loss: 0.241623  [   82/   89]
per-ex loss: 0.361067  [   83/   89]
per-ex loss: 0.385846  [   84/   89]
per-ex loss: 0.378169  [   85/   89]
per-ex loss: 0.295558  [   86/   89]
per-ex loss: 0.228086  [   87/   89]
per-ex loss: 0.386659  [   88/   89]
per-ex loss: 0.298339  [   89/   89]
Train Error: Avg loss: 0.35060685
validation Error: 
 Avg loss: 0.56899540 
 F1: 0.462960 
 Precision: 0.498678 
 Recall: 0.432016
 IoU: 0.301202

test Error: 
 Avg loss: 0.53468622 
 F1: 0.512973 
 Precision: 0.482888 
 Recall: 0.547055
 IoU: 0.344965

We have finished training iteration 203
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_201_.pth
per-ex loss: 0.493779  [    1/   89]
per-ex loss: 0.367539  [    2/   89]
per-ex loss: 0.285149  [    3/   89]
per-ex loss: 0.293243  [    4/   89]
per-ex loss: 0.305450  [    5/   89]
per-ex loss: 0.434473  [    6/   89]
per-ex loss: 0.463689  [    7/   89]
per-ex loss: 0.309253  [    8/   89]
per-ex loss: 0.554386  [    9/   89]
per-ex loss: 0.168307  [   10/   89]
per-ex loss: 0.299282  [   11/   89]
per-ex loss: 0.358731  [   12/   89]
per-ex loss: 0.423061  [   13/   89]
per-ex loss: 0.247748  [   14/   89]
per-ex loss: 0.331177  [   15/   89]
per-ex loss: 0.260318  [   16/   89]
per-ex loss: 0.552140  [   17/   89]
per-ex loss: 0.403998  [   18/   89]
per-ex loss: 0.249807  [   19/   89]
per-ex loss: 0.690877  [   20/   89]
per-ex loss: 0.317322  [   21/   89]
per-ex loss: 0.209178  [   22/   89]
per-ex loss: 0.691717  [   23/   89]
per-ex loss: 0.464124  [   24/   89]
per-ex loss: 0.291463  [   25/   89]
per-ex loss: 0.406485  [   26/   89]
per-ex loss: 0.199423  [   27/   89]
per-ex loss: 0.258316  [   28/   89]
per-ex loss: 0.438112  [   29/   89]
per-ex loss: 0.283112  [   30/   89]
per-ex loss: 0.287015  [   31/   89]
per-ex loss: 0.208856  [   32/   89]
per-ex loss: 0.393813  [   33/   89]
per-ex loss: 0.294222  [   34/   89]
per-ex loss: 0.248842  [   35/   89]
per-ex loss: 0.402762  [   36/   89]
per-ex loss: 0.242715  [   37/   89]
per-ex loss: 0.290314  [   38/   89]
per-ex loss: 0.305884  [   39/   89]
per-ex loss: 0.438801  [   40/   89]
per-ex loss: 0.267311  [   41/   89]
per-ex loss: 0.355260  [   42/   89]
per-ex loss: 0.309339  [   43/   89]
per-ex loss: 0.413586  [   44/   89]
per-ex loss: 0.139618  [   45/   89]
per-ex loss: 0.239390  [   46/   89]
per-ex loss: 0.399967  [   47/   89]
per-ex loss: 0.369222  [   48/   89]
per-ex loss: 0.239315  [   49/   89]
per-ex loss: 0.349151  [   50/   89]
per-ex loss: 0.205445  [   51/   89]
per-ex loss: 0.447618  [   52/   89]
per-ex loss: 0.532831  [   53/   89]
per-ex loss: 0.337946  [   54/   89]
per-ex loss: 0.424067  [   55/   89]
per-ex loss: 0.322256  [   56/   89]
per-ex loss: 0.520070  [   57/   89]
per-ex loss: 0.337356  [   58/   89]
per-ex loss: 0.230279  [   59/   89]
per-ex loss: 0.502734  [   60/   89]
per-ex loss: 0.281171  [   61/   89]
per-ex loss: 0.999906  [   62/   89]
per-ex loss: 0.490092  [   63/   89]
per-ex loss: 0.324597  [   64/   89]
per-ex loss: 0.273393  [   65/   89]
per-ex loss: 0.444333  [   66/   89]
per-ex loss: 0.326108  [   67/   89]
per-ex loss: 0.415785  [   68/   89]
per-ex loss: 0.314244  [   69/   89]
per-ex loss: 0.256266  [   70/   89]
per-ex loss: 0.356394  [   71/   89]
per-ex loss: 0.334119  [   72/   89]
per-ex loss: 0.301219  [   73/   89]
per-ex loss: 0.270981  [   74/   89]
per-ex loss: 0.442599  [   75/   89]
per-ex loss: 0.368252  [   76/   89]
per-ex loss: 0.361805  [   77/   89]
per-ex loss: 0.543804  [   78/   89]
per-ex loss: 0.282905  [   79/   89]
per-ex loss: 0.408137  [   80/   89]
per-ex loss: 0.273433  [   81/   89]
per-ex loss: 0.460179  [   82/   89]
per-ex loss: 0.263162  [   83/   89]
per-ex loss: 0.336772  [   84/   89]
per-ex loss: 0.169951  [   85/   89]
per-ex loss: 0.345037  [   86/   89]
per-ex loss: 0.525446  [   87/   89]
per-ex loss: 0.194718  [   88/   89]
per-ex loss: 0.561316  [   89/   89]
Train Error: Avg loss: 0.35992992
validation Error: 
 Avg loss: 0.53980154 
 F1: 0.486761 
 Precision: 0.508972 
 Recall: 0.466408
 IoU: 0.321669

test Error: 
 Avg loss: 0.50096980 
 F1: 0.544713 
 Precision: 0.537621 
 Recall: 0.551995
 IoU: 0.374299

We have finished training iteration 204
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_202_.pth
per-ex loss: 0.381877  [    1/   89]
per-ex loss: 0.392359  [    2/   89]
per-ex loss: 0.371208  [    3/   89]
per-ex loss: 0.999960  [    4/   89]
per-ex loss: 0.400712  [    5/   89]
per-ex loss: 0.173107  [    6/   89]
per-ex loss: 0.442170  [    7/   89]
per-ex loss: 0.604881  [    8/   89]
per-ex loss: 0.375992  [    9/   89]
per-ex loss: 0.266188  [   10/   89]
per-ex loss: 0.138730  [   11/   89]
per-ex loss: 0.402430  [   12/   89]
per-ex loss: 0.299746  [   13/   89]
per-ex loss: 0.420404  [   14/   89]
per-ex loss: 0.502493  [   15/   89]
per-ex loss: 0.286753  [   16/   89]
per-ex loss: 0.339004  [   17/   89]
per-ex loss: 0.506373  [   18/   89]
per-ex loss: 0.297536  [   19/   89]
per-ex loss: 0.233602  [   20/   89]
per-ex loss: 0.339234  [   21/   89]
per-ex loss: 0.355311  [   22/   89]
per-ex loss: 0.481868  [   23/   89]
per-ex loss: 0.242463  [   24/   89]
per-ex loss: 0.338180  [   25/   89]
per-ex loss: 0.323673  [   26/   89]
per-ex loss: 0.416526  [   27/   89]
per-ex loss: 0.213776  [   28/   89]
per-ex loss: 0.329870  [   29/   89]
per-ex loss: 0.205357  [   30/   89]
per-ex loss: 0.445109  [   31/   89]
per-ex loss: 0.259446  [   32/   89]
per-ex loss: 0.351416  [   33/   89]
per-ex loss: 0.280131  [   34/   89]
per-ex loss: 0.163356  [   35/   89]
per-ex loss: 0.130944  [   36/   89]
per-ex loss: 0.241069  [   37/   89]
per-ex loss: 0.312568  [   38/   89]
per-ex loss: 0.309448  [   39/   89]
per-ex loss: 0.291620  [   40/   89]
per-ex loss: 0.228293  [   41/   89]
per-ex loss: 0.487974  [   42/   89]
per-ex loss: 0.400651  [   43/   89]
per-ex loss: 0.207140  [   44/   89]
per-ex loss: 0.169477  [   45/   89]
per-ex loss: 0.537894  [   46/   89]
per-ex loss: 0.529842  [   47/   89]
per-ex loss: 0.372491  [   48/   89]
per-ex loss: 0.382632  [   49/   89]
per-ex loss: 0.408441  [   50/   89]
per-ex loss: 0.343877  [   51/   89]
per-ex loss: 0.276327  [   52/   89]
per-ex loss: 0.343305  [   53/   89]
per-ex loss: 0.272094  [   54/   89]
per-ex loss: 0.484470  [   55/   89]
per-ex loss: 0.224371  [   56/   89]
per-ex loss: 0.329143  [   57/   89]
per-ex loss: 0.536530  [   58/   89]
per-ex loss: 0.474495  [   59/   89]
per-ex loss: 0.238659  [   60/   89]
per-ex loss: 0.267622  [   61/   89]
per-ex loss: 0.245942  [   62/   89]
per-ex loss: 0.278181  [   63/   89]
per-ex loss: 0.331417  [   64/   89]
per-ex loss: 0.204494  [   65/   89]
per-ex loss: 0.565750  [   66/   89]
per-ex loss: 0.277385  [   67/   89]
per-ex loss: 0.345403  [   68/   89]
per-ex loss: 0.478394  [   69/   89]
per-ex loss: 0.265874  [   70/   89]
per-ex loss: 0.305100  [   71/   89]
per-ex loss: 0.266132  [   72/   89]
per-ex loss: 0.470417  [   73/   89]
per-ex loss: 0.233722  [   74/   89]
per-ex loss: 0.275576  [   75/   89]
per-ex loss: 0.424833  [   76/   89]
per-ex loss: 0.434134  [   77/   89]
per-ex loss: 0.319765  [   78/   89]
per-ex loss: 0.505086  [   79/   89]
per-ex loss: 0.235444  [   80/   89]
per-ex loss: 0.311635  [   81/   89]
per-ex loss: 0.483600  [   82/   89]
per-ex loss: 0.318393  [   83/   89]
per-ex loss: 0.377060  [   84/   89]
per-ex loss: 0.392674  [   85/   89]
per-ex loss: 0.215060  [   86/   89]
per-ex loss: 0.383629  [   87/   89]
per-ex loss: 0.292732  [   88/   89]
per-ex loss: 0.330674  [   89/   89]
Train Error: Avg loss: 0.34776546
validation Error: 
 Avg loss: 0.54949619 
 F1: 0.478504 
 Precision: 0.468490 
 Recall: 0.488956
 IoU: 0.314496

test Error: 
 Avg loss: 0.49407448 
 F1: 0.549806 
 Precision: 0.526611 
 Recall: 0.575137
 IoU: 0.379126

We have finished training iteration 205
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_203_.pth
per-ex loss: 0.255425  [    1/   89]
per-ex loss: 0.317436  [    2/   89]
per-ex loss: 0.271741  [    3/   89]
per-ex loss: 0.294656  [    4/   89]
per-ex loss: 0.567886  [    5/   89]
per-ex loss: 0.304311  [    6/   89]
per-ex loss: 0.261732  [    7/   89]
per-ex loss: 0.315084  [    8/   89]
per-ex loss: 0.279570  [    9/   89]
per-ex loss: 0.468997  [   10/   89]
per-ex loss: 0.362126  [   11/   89]
per-ex loss: 0.261678  [   12/   89]
per-ex loss: 0.348243  [   13/   89]
per-ex loss: 0.436915  [   14/   89]
per-ex loss: 0.413454  [   15/   89]
per-ex loss: 0.200540  [   16/   89]
per-ex loss: 0.337578  [   17/   89]
per-ex loss: 0.431776  [   18/   89]
per-ex loss: 0.268084  [   19/   89]
per-ex loss: 0.999898  [   20/   89]
per-ex loss: 0.195323  [   21/   89]
per-ex loss: 0.570166  [   22/   89]
per-ex loss: 0.383900  [   23/   89]
per-ex loss: 0.334673  [   24/   89]
per-ex loss: 0.390795  [   25/   89]
per-ex loss: 0.410084  [   26/   89]
per-ex loss: 0.437339  [   27/   89]
per-ex loss: 0.266693  [   28/   89]
per-ex loss: 0.512473  [   29/   89]
per-ex loss: 0.263122  [   30/   89]
per-ex loss: 0.514754  [   31/   89]
per-ex loss: 0.279604  [   32/   89]
per-ex loss: 0.569885  [   33/   89]
per-ex loss: 0.466482  [   34/   89]
per-ex loss: 0.527411  [   35/   89]
per-ex loss: 0.339751  [   36/   89]
per-ex loss: 0.267337  [   37/   89]
per-ex loss: 0.396273  [   38/   89]
per-ex loss: 0.421270  [   39/   89]
per-ex loss: 0.175681  [   40/   89]
per-ex loss: 0.350226  [   41/   89]
per-ex loss: 0.274375  [   42/   89]
per-ex loss: 0.316408  [   43/   89]
per-ex loss: 0.289401  [   44/   89]
per-ex loss: 0.364928  [   45/   89]
per-ex loss: 0.253554  [   46/   89]
per-ex loss: 0.214756  [   47/   89]
per-ex loss: 0.201718  [   48/   89]
per-ex loss: 0.261351  [   49/   89]
per-ex loss: 0.266933  [   50/   89]
per-ex loss: 0.415964  [   51/   89]
per-ex loss: 0.220584  [   52/   89]
per-ex loss: 0.335699  [   53/   89]
per-ex loss: 0.319321  [   54/   89]
per-ex loss: 0.197835  [   55/   89]
per-ex loss: 0.322755  [   56/   89]
per-ex loss: 0.322952  [   57/   89]
per-ex loss: 0.313459  [   58/   89]
per-ex loss: 0.274677  [   59/   89]
per-ex loss: 0.510813  [   60/   89]
per-ex loss: 0.294456  [   61/   89]
per-ex loss: 0.274381  [   62/   89]
per-ex loss: 0.471629  [   63/   89]
per-ex loss: 0.345085  [   64/   89]
per-ex loss: 0.284217  [   65/   89]
per-ex loss: 0.279636  [   66/   89]
per-ex loss: 0.260556  [   67/   89]
per-ex loss: 0.230529  [   68/   89]
per-ex loss: 0.319797  [   69/   89]
per-ex loss: 0.245252  [   70/   89]
per-ex loss: 0.276528  [   71/   89]
per-ex loss: 0.201054  [   72/   89]
per-ex loss: 0.452616  [   73/   89]
per-ex loss: 0.375038  [   74/   89]
per-ex loss: 0.523555  [   75/   89]
per-ex loss: 0.338161  [   76/   89]
per-ex loss: 0.277162  [   77/   89]
per-ex loss: 0.524026  [   78/   89]
per-ex loss: 0.453761  [   79/   89]
per-ex loss: 0.409519  [   80/   89]
per-ex loss: 0.344026  [   81/   89]
per-ex loss: 0.348400  [   82/   89]
per-ex loss: 0.304057  [   83/   89]
per-ex loss: 0.270775  [   84/   89]
per-ex loss: 0.176589  [   85/   89]
per-ex loss: 0.456988  [   86/   89]
per-ex loss: 0.222372  [   87/   89]
per-ex loss: 0.295016  [   88/   89]
per-ex loss: 0.297795  [   89/   89]
Train Error: Avg loss: 0.34495316
validation Error: 
 Avg loss: 0.53694951 
 F1: 0.490972 
 Precision: 0.493578 
 Recall: 0.488393
 IoU: 0.325356

test Error: 
 Avg loss: 0.49437169 
 F1: 0.549690 
 Precision: 0.536066 
 Recall: 0.564024
 IoU: 0.379015

We have finished training iteration 206
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_204_.pth
per-ex loss: 0.166832  [    1/   89]
per-ex loss: 0.482546  [    2/   89]
per-ex loss: 0.487569  [    3/   89]
per-ex loss: 0.381408  [    4/   89]
per-ex loss: 0.550001  [    5/   89]
per-ex loss: 0.264248  [    6/   89]
per-ex loss: 0.226666  [    7/   89]
per-ex loss: 0.415359  [    8/   89]
per-ex loss: 0.265542  [    9/   89]
per-ex loss: 0.259594  [   10/   89]
per-ex loss: 0.383813  [   11/   89]
per-ex loss: 0.299175  [   12/   89]
per-ex loss: 0.282783  [   13/   89]
per-ex loss: 0.356618  [   14/   89]
per-ex loss: 0.286157  [   15/   89]
per-ex loss: 0.322396  [   16/   89]
per-ex loss: 0.243980  [   17/   89]
per-ex loss: 0.418046  [   18/   89]
per-ex loss: 0.516118  [   19/   89]
per-ex loss: 0.624329  [   20/   89]
per-ex loss: 0.185400  [   21/   89]
per-ex loss: 0.615132  [   22/   89]
per-ex loss: 0.270754  [   23/   89]
per-ex loss: 0.276427  [   24/   89]
per-ex loss: 0.282810  [   25/   89]
per-ex loss: 0.357588  [   26/   89]
per-ex loss: 0.273784  [   27/   89]
per-ex loss: 0.454893  [   28/   89]
per-ex loss: 0.384402  [   29/   89]
per-ex loss: 0.312432  [   30/   89]
per-ex loss: 0.264316  [   31/   89]
per-ex loss: 0.231898  [   32/   89]
per-ex loss: 0.326794  [   33/   89]
per-ex loss: 0.292442  [   34/   89]
per-ex loss: 0.477032  [   35/   89]
per-ex loss: 0.508503  [   36/   89]
per-ex loss: 0.239967  [   37/   89]
per-ex loss: 0.397376  [   38/   89]
per-ex loss: 0.215922  [   39/   89]
per-ex loss: 0.326802  [   40/   89]
per-ex loss: 0.127367  [   41/   89]
per-ex loss: 0.383756  [   42/   89]
per-ex loss: 0.171158  [   43/   89]
per-ex loss: 0.255019  [   44/   89]
per-ex loss: 0.310119  [   45/   89]
per-ex loss: 0.999879  [   46/   89]
per-ex loss: 0.334641  [   47/   89]
per-ex loss: 0.231603  [   48/   89]
per-ex loss: 0.173884  [   49/   89]
per-ex loss: 0.273908  [   50/   89]
per-ex loss: 0.278641  [   51/   89]
per-ex loss: 0.465563  [   52/   89]
per-ex loss: 0.221304  [   53/   89]
per-ex loss: 0.193564  [   54/   89]
per-ex loss: 0.249092  [   55/   89]
per-ex loss: 0.284627  [   56/   89]
per-ex loss: 0.263336  [   57/   89]
per-ex loss: 0.293432  [   58/   89]
per-ex loss: 0.301550  [   59/   89]
per-ex loss: 0.431026  [   60/   89]
per-ex loss: 0.373285  [   61/   89]
per-ex loss: 0.268056  [   62/   89]
per-ex loss: 0.252962  [   63/   89]
per-ex loss: 0.312069  [   64/   89]
per-ex loss: 0.549300  [   65/   89]
per-ex loss: 0.395323  [   66/   89]
per-ex loss: 0.242850  [   67/   89]
per-ex loss: 0.280521  [   68/   89]
per-ex loss: 0.339077  [   69/   89]
per-ex loss: 0.325286  [   70/   89]
per-ex loss: 0.345480  [   71/   89]
per-ex loss: 0.444187  [   72/   89]
per-ex loss: 0.347692  [   73/   89]
per-ex loss: 0.548521  [   74/   89]
per-ex loss: 0.376718  [   75/   89]
per-ex loss: 0.502641  [   76/   89]
per-ex loss: 0.324954  [   77/   89]
per-ex loss: 0.383412  [   78/   89]
per-ex loss: 0.540415  [   79/   89]
per-ex loss: 0.193049  [   80/   89]
per-ex loss: 0.241388  [   81/   89]
per-ex loss: 0.302055  [   82/   89]
per-ex loss: 0.312506  [   83/   89]
per-ex loss: 0.434346  [   84/   89]
per-ex loss: 0.239388  [   85/   89]
per-ex loss: 0.243057  [   86/   89]
per-ex loss: 0.222046  [   87/   89]
per-ex loss: 0.333649  [   88/   89]
per-ex loss: 0.264128  [   89/   89]
Train Error: Avg loss: 0.33831106
validation Error: 
 Avg loss: 0.54813640 
 F1: 0.481074 
 Precision: 0.455796 
 Recall: 0.509321
 IoU: 0.316720

test Error: 
 Avg loss: 0.49979898 
 F1: 0.540348 
 Precision: 0.502684 
 Recall: 0.584113
 IoU: 0.370190

We have finished training iteration 207
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_205_.pth
per-ex loss: 0.586130  [    1/   89]
per-ex loss: 0.467203  [    2/   89]
per-ex loss: 0.330579  [    3/   89]
per-ex loss: 0.277057  [    4/   89]
per-ex loss: 0.297349  [    5/   89]
per-ex loss: 0.443659  [    6/   89]
per-ex loss: 0.252569  [    7/   89]
per-ex loss: 0.276556  [    8/   89]
per-ex loss: 0.475475  [    9/   89]
per-ex loss: 0.332913  [   10/   89]
per-ex loss: 0.302966  [   11/   89]
per-ex loss: 0.388514  [   12/   89]
per-ex loss: 0.333298  [   13/   89]
per-ex loss: 0.294861  [   14/   89]
per-ex loss: 0.229314  [   15/   89]
per-ex loss: 0.475173  [   16/   89]
per-ex loss: 0.366843  [   17/   89]
per-ex loss: 0.448265  [   18/   89]
per-ex loss: 0.256301  [   19/   89]
per-ex loss: 0.249772  [   20/   89]
per-ex loss: 0.272721  [   21/   89]
per-ex loss: 0.422423  [   22/   89]
per-ex loss: 0.999900  [   23/   89]
per-ex loss: 0.370461  [   24/   89]
per-ex loss: 0.244392  [   25/   89]
per-ex loss: 0.234129  [   26/   89]
per-ex loss: 0.373284  [   27/   89]
per-ex loss: 0.470034  [   28/   89]
per-ex loss: 0.323843  [   29/   89]
per-ex loss: 0.320529  [   30/   89]
per-ex loss: 0.276759  [   31/   89]
per-ex loss: 0.498498  [   32/   89]
per-ex loss: 0.265695  [   33/   89]
per-ex loss: 0.473226  [   34/   89]
per-ex loss: 0.298930  [   35/   89]
per-ex loss: 0.307678  [   36/   89]
per-ex loss: 0.271623  [   37/   89]
per-ex loss: 0.310591  [   38/   89]
per-ex loss: 0.268801  [   39/   89]
per-ex loss: 0.305228  [   40/   89]
per-ex loss: 0.258503  [   41/   89]
per-ex loss: 0.368551  [   42/   89]
per-ex loss: 0.541601  [   43/   89]
per-ex loss: 0.242430  [   44/   89]
per-ex loss: 0.392408  [   45/   89]
per-ex loss: 0.250397  [   46/   89]
per-ex loss: 0.225352  [   47/   89]
per-ex loss: 0.160748  [   48/   89]
per-ex loss: 0.265650  [   49/   89]
per-ex loss: 0.210503  [   50/   89]
per-ex loss: 0.158605  [   51/   89]
per-ex loss: 0.703648  [   52/   89]
per-ex loss: 0.172982  [   53/   89]
per-ex loss: 0.370504  [   54/   89]
per-ex loss: 0.539465  [   55/   89]
per-ex loss: 0.206549  [   56/   89]
per-ex loss: 0.245087  [   57/   89]
per-ex loss: 0.508727  [   58/   89]
per-ex loss: 0.312809  [   59/   89]
per-ex loss: 0.416671  [   60/   89]
per-ex loss: 0.303487  [   61/   89]
per-ex loss: 0.570563  [   62/   89]
per-ex loss: 0.211946  [   63/   89]
per-ex loss: 0.355132  [   64/   89]
per-ex loss: 0.492839  [   65/   89]
per-ex loss: 0.331839  [   66/   89]
per-ex loss: 0.333276  [   67/   89]
per-ex loss: 0.611723  [   68/   89]
per-ex loss: 0.339522  [   69/   89]
per-ex loss: 0.375656  [   70/   89]
per-ex loss: 0.332481  [   71/   89]
per-ex loss: 0.395420  [   72/   89]
per-ex loss: 0.152689  [   73/   89]
per-ex loss: 0.348040  [   74/   89]
per-ex loss: 0.246627  [   75/   89]
per-ex loss: 0.271830  [   76/   89]
per-ex loss: 0.486102  [   77/   89]
per-ex loss: 0.123623  [   78/   89]
per-ex loss: 0.387929  [   79/   89]
per-ex loss: 0.277144  [   80/   89]
per-ex loss: 0.231073  [   81/   89]
per-ex loss: 0.255133  [   82/   89]
per-ex loss: 0.481408  [   83/   89]
per-ex loss: 0.412645  [   84/   89]
per-ex loss: 0.231454  [   85/   89]
per-ex loss: 0.290614  [   86/   89]
per-ex loss: 0.304057  [   87/   89]
per-ex loss: 0.288929  [   88/   89]
per-ex loss: 0.396153  [   89/   89]
Train Error: Avg loss: 0.34588839
validation Error: 
 Avg loss: 0.55108130 
 F1: 0.477893 
 Precision: 0.461957 
 Recall: 0.494969
 IoU: 0.313968

test Error: 
 Avg loss: 0.48539454 
 F1: 0.556386 
 Precision: 0.530233 
 Recall: 0.585251
 IoU: 0.385411

We have finished training iteration 208
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_206_.pth
per-ex loss: 0.536474  [    1/   89]
per-ex loss: 0.517366  [    2/   89]
per-ex loss: 0.252511  [    3/   89]
per-ex loss: 0.478147  [    4/   89]
per-ex loss: 0.296296  [    5/   89]
per-ex loss: 0.524326  [    6/   89]
per-ex loss: 0.268118  [    7/   89]
per-ex loss: 0.220754  [    8/   89]
per-ex loss: 0.367381  [    9/   89]
per-ex loss: 0.300041  [   10/   89]
per-ex loss: 0.210227  [   11/   89]
per-ex loss: 0.267116  [   12/   89]
per-ex loss: 0.275835  [   13/   89]
per-ex loss: 0.276974  [   14/   89]
per-ex loss: 0.288198  [   15/   89]
per-ex loss: 0.503706  [   16/   89]
per-ex loss: 0.287569  [   17/   89]
per-ex loss: 0.287870  [   18/   89]
per-ex loss: 0.331247  [   19/   89]
per-ex loss: 0.434822  [   20/   89]
per-ex loss: 0.500763  [   21/   89]
per-ex loss: 0.281082  [   22/   89]
per-ex loss: 0.280611  [   23/   89]
per-ex loss: 0.148304  [   24/   89]
per-ex loss: 0.313529  [   25/   89]
per-ex loss: 0.207784  [   26/   89]
per-ex loss: 0.270144  [   27/   89]
per-ex loss: 0.246497  [   28/   89]
per-ex loss: 0.271430  [   29/   89]
per-ex loss: 0.297550  [   30/   89]
per-ex loss: 0.366103  [   31/   89]
per-ex loss: 0.555626  [   32/   89]
per-ex loss: 0.277054  [   33/   89]
per-ex loss: 0.122069  [   34/   89]
per-ex loss: 0.246196  [   35/   89]
per-ex loss: 0.325487  [   36/   89]
per-ex loss: 0.449313  [   37/   89]
per-ex loss: 0.382244  [   38/   89]
per-ex loss: 0.171656  [   39/   89]
per-ex loss: 0.446928  [   40/   89]
per-ex loss: 0.375767  [   41/   89]
per-ex loss: 0.268138  [   42/   89]
per-ex loss: 0.331117  [   43/   89]
per-ex loss: 0.471301  [   44/   89]
per-ex loss: 0.241413  [   45/   89]
per-ex loss: 0.254087  [   46/   89]
per-ex loss: 0.219138  [   47/   89]
per-ex loss: 0.311763  [   48/   89]
per-ex loss: 0.475447  [   49/   89]
per-ex loss: 0.556068  [   50/   89]
per-ex loss: 0.556962  [   51/   89]
per-ex loss: 0.266075  [   52/   89]
per-ex loss: 0.467987  [   53/   89]
per-ex loss: 0.319246  [   54/   89]
per-ex loss: 0.312493  [   55/   89]
per-ex loss: 0.359716  [   56/   89]
per-ex loss: 0.282570  [   57/   89]
per-ex loss: 0.213653  [   58/   89]
per-ex loss: 0.252866  [   59/   89]
per-ex loss: 0.372169  [   60/   89]
per-ex loss: 0.426021  [   61/   89]
per-ex loss: 0.399794  [   62/   89]
per-ex loss: 0.271411  [   63/   89]
per-ex loss: 0.296916  [   64/   89]
per-ex loss: 0.292110  [   65/   89]
per-ex loss: 0.339693  [   66/   89]
per-ex loss: 0.474549  [   67/   89]
per-ex loss: 0.237844  [   68/   89]
per-ex loss: 0.275881  [   69/   89]
per-ex loss: 0.326175  [   70/   89]
per-ex loss: 0.149209  [   71/   89]
per-ex loss: 0.254711  [   72/   89]
per-ex loss: 0.229170  [   73/   89]
per-ex loss: 0.999884  [   74/   89]
per-ex loss: 0.182624  [   75/   89]
per-ex loss: 0.339011  [   76/   89]
per-ex loss: 0.308460  [   77/   89]
per-ex loss: 0.364087  [   78/   89]
per-ex loss: 0.328712  [   79/   89]
per-ex loss: 0.345294  [   80/   89]
per-ex loss: 0.395396  [   81/   89]
per-ex loss: 0.197496  [   82/   89]
per-ex loss: 0.540973  [   83/   89]
per-ex loss: 0.276821  [   84/   89]
per-ex loss: 0.420364  [   85/   89]
per-ex loss: 0.490039  [   86/   89]
per-ex loss: 0.312686  [   87/   89]
per-ex loss: 0.266705  [   88/   89]
per-ex loss: 0.206967  [   89/   89]
Train Error: Avg loss: 0.33643066
validation Error: 
 Avg loss: 0.54237308 
 F1: 0.483905 
 Precision: 0.528888 
 Recall: 0.445974
 IoU: 0.319179

test Error: 
 Avg loss: 0.50710250 
 F1: 0.535095 
 Precision: 0.541165 
 Recall: 0.529159
 IoU: 0.365276

We have finished training iteration 209
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_207_.pth
per-ex loss: 0.571366  [    1/   89]
per-ex loss: 0.221718  [    2/   89]
per-ex loss: 0.303284  [    3/   89]
per-ex loss: 0.267591  [    4/   89]
per-ex loss: 0.336857  [    5/   89]
per-ex loss: 0.322506  [    6/   89]
per-ex loss: 0.625264  [    7/   89]
per-ex loss: 0.351642  [    8/   89]
per-ex loss: 0.269756  [    9/   89]
per-ex loss: 0.304158  [   10/   89]
per-ex loss: 0.528362  [   11/   89]
per-ex loss: 0.275539  [   12/   89]
per-ex loss: 0.374157  [   13/   89]
per-ex loss: 0.317507  [   14/   89]
per-ex loss: 0.201712  [   15/   89]
per-ex loss: 0.337991  [   16/   89]
per-ex loss: 0.213061  [   17/   89]
per-ex loss: 0.312661  [   18/   89]
per-ex loss: 0.338187  [   19/   89]
per-ex loss: 0.402112  [   20/   89]
per-ex loss: 0.468437  [   21/   89]
per-ex loss: 0.258670  [   22/   89]
per-ex loss: 0.300077  [   23/   89]
per-ex loss: 0.307073  [   24/   89]
per-ex loss: 0.308734  [   25/   89]
per-ex loss: 0.535689  [   26/   89]
per-ex loss: 0.409773  [   27/   89]
per-ex loss: 0.237706  [   28/   89]
per-ex loss: 0.411660  [   29/   89]
per-ex loss: 0.314350  [   30/   89]
per-ex loss: 0.338301  [   31/   89]
per-ex loss: 0.298173  [   32/   89]
per-ex loss: 0.374032  [   33/   89]
per-ex loss: 0.374942  [   34/   89]
per-ex loss: 0.267912  [   35/   89]
per-ex loss: 0.221406  [   36/   89]
per-ex loss: 0.396750  [   37/   89]
per-ex loss: 0.247827  [   38/   89]
per-ex loss: 0.280835  [   39/   89]
per-ex loss: 0.210333  [   40/   89]
per-ex loss: 0.229944  [   41/   89]
per-ex loss: 0.258319  [   42/   89]
per-ex loss: 0.392114  [   43/   89]
per-ex loss: 0.485442  [   44/   89]
per-ex loss: 0.274911  [   45/   89]
per-ex loss: 0.491711  [   46/   89]
per-ex loss: 0.265158  [   47/   89]
per-ex loss: 0.123591  [   48/   89]
per-ex loss: 0.346257  [   49/   89]
per-ex loss: 0.999929  [   50/   89]
per-ex loss: 0.238662  [   51/   89]
per-ex loss: 0.282620  [   52/   89]
per-ex loss: 0.203056  [   53/   89]
per-ex loss: 0.379321  [   54/   89]
per-ex loss: 0.398011  [   55/   89]
per-ex loss: 0.382419  [   56/   89]
per-ex loss: 0.445556  [   57/   89]
per-ex loss: 0.276026  [   58/   89]
per-ex loss: 0.407952  [   59/   89]
per-ex loss: 0.293876  [   60/   89]
per-ex loss: 0.312287  [   61/   89]
per-ex loss: 0.422798  [   62/   89]
per-ex loss: 0.574576  [   63/   89]
per-ex loss: 0.534107  [   64/   89]
per-ex loss: 0.420774  [   65/   89]
per-ex loss: 0.308306  [   66/   89]
per-ex loss: 0.481650  [   67/   89]
per-ex loss: 0.493182  [   68/   89]
per-ex loss: 0.550027  [   69/   89]
per-ex loss: 0.417116  [   70/   89]
per-ex loss: 0.439945  [   71/   89]
per-ex loss: 0.577996  [   72/   89]
per-ex loss: 0.333316  [   73/   89]
per-ex loss: 0.339541  [   74/   89]
per-ex loss: 0.361135  [   75/   89]
per-ex loss: 0.280134  [   76/   89]
per-ex loss: 0.177236  [   77/   89]
per-ex loss: 0.252542  [   78/   89]
per-ex loss: 0.300598  [   79/   89]
per-ex loss: 0.523329  [   80/   89]
per-ex loss: 0.412537  [   81/   89]
per-ex loss: 0.318336  [   82/   89]
per-ex loss: 0.223171  [   83/   89]
per-ex loss: 0.261227  [   84/   89]
per-ex loss: 0.339981  [   85/   89]
per-ex loss: 0.214275  [   86/   89]
per-ex loss: 0.328930  [   87/   89]
per-ex loss: 0.225128  [   88/   89]
per-ex loss: 0.265500  [   89/   89]
Train Error: Avg loss: 0.35171536
validation Error: 
 Avg loss: 0.52906990 
 F1: 0.498159 
 Precision: 0.545988 
 Recall: 0.458034
 IoU: 0.331699

test Error: 
 Avg loss: 0.48340936 
 F1: 0.564194 
 Precision: 0.585517 
 Recall: 0.544370
 IoU: 0.392946

We have finished training iteration 210
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_208_.pth
per-ex loss: 0.349071  [    1/   89]
per-ex loss: 0.547522  [    2/   89]
per-ex loss: 0.235379  [    3/   89]
per-ex loss: 0.266664  [    4/   89]
per-ex loss: 0.318030  [    5/   89]
per-ex loss: 0.414547  [    6/   89]
per-ex loss: 0.321767  [    7/   89]
per-ex loss: 0.174755  [    8/   89]
per-ex loss: 0.213747  [    9/   89]
per-ex loss: 0.320205  [   10/   89]
per-ex loss: 0.262408  [   11/   89]
per-ex loss: 0.532154  [   12/   89]
per-ex loss: 0.214245  [   13/   89]
per-ex loss: 0.363917  [   14/   89]
per-ex loss: 0.242854  [   15/   89]
per-ex loss: 0.284646  [   16/   89]
per-ex loss: 0.308880  [   17/   89]
per-ex loss: 0.457688  [   18/   89]
per-ex loss: 0.256887  [   19/   89]
per-ex loss: 0.296412  [   20/   89]
per-ex loss: 0.236353  [   21/   89]
per-ex loss: 0.370214  [   22/   89]
per-ex loss: 0.221336  [   23/   89]
per-ex loss: 0.240108  [   24/   89]
per-ex loss: 0.334066  [   25/   89]
per-ex loss: 0.257051  [   26/   89]
per-ex loss: 0.281515  [   27/   89]
per-ex loss: 0.489920  [   28/   89]
per-ex loss: 0.254674  [   29/   89]
per-ex loss: 0.317924  [   30/   89]
per-ex loss: 0.545481  [   31/   89]
per-ex loss: 0.333307  [   32/   89]
per-ex loss: 0.469253  [   33/   89]
per-ex loss: 0.266377  [   34/   89]
per-ex loss: 0.591784  [   35/   89]
per-ex loss: 0.209946  [   36/   89]
per-ex loss: 0.315889  [   37/   89]
per-ex loss: 0.493034  [   38/   89]
per-ex loss: 0.277346  [   39/   89]
per-ex loss: 0.390554  [   40/   89]
per-ex loss: 0.148232  [   41/   89]
per-ex loss: 0.334049  [   42/   89]
per-ex loss: 0.313783  [   43/   89]
per-ex loss: 0.460676  [   44/   89]
per-ex loss: 0.324317  [   45/   89]
per-ex loss: 0.389015  [   46/   89]
per-ex loss: 0.267748  [   47/   89]
per-ex loss: 0.212281  [   48/   89]
per-ex loss: 0.420117  [   49/   89]
per-ex loss: 0.318458  [   50/   89]
per-ex loss: 0.281268  [   51/   89]
per-ex loss: 0.324305  [   52/   89]
per-ex loss: 0.257764  [   53/   89]
per-ex loss: 0.278834  [   54/   89]
per-ex loss: 0.999879  [   55/   89]
per-ex loss: 0.379109  [   56/   89]
per-ex loss: 0.371922  [   57/   89]
per-ex loss: 0.329036  [   58/   89]
per-ex loss: 0.523312  [   59/   89]
per-ex loss: 0.322547  [   60/   89]
per-ex loss: 0.277626  [   61/   89]
per-ex loss: 0.531408  [   62/   89]
per-ex loss: 0.251608  [   63/   89]
per-ex loss: 0.307213  [   64/   89]
per-ex loss: 0.478494  [   65/   89]
per-ex loss: 0.426049  [   66/   89]
per-ex loss: 0.220206  [   67/   89]
per-ex loss: 0.305486  [   68/   89]
per-ex loss: 0.264069  [   69/   89]
per-ex loss: 0.271718  [   70/   89]
per-ex loss: 0.383191  [   71/   89]
per-ex loss: 0.250266  [   72/   89]
per-ex loss: 0.234070  [   73/   89]
per-ex loss: 0.250647  [   74/   89]
per-ex loss: 0.408964  [   75/   89]
per-ex loss: 0.340479  [   76/   89]
per-ex loss: 0.288913  [   77/   89]
per-ex loss: 0.404266  [   78/   89]
per-ex loss: 0.330785  [   79/   89]
per-ex loss: 0.134405  [   80/   89]
per-ex loss: 0.487376  [   81/   89]
per-ex loss: 0.422937  [   82/   89]
per-ex loss: 0.369924  [   83/   89]
per-ex loss: 0.484468  [   84/   89]
per-ex loss: 0.213405  [   85/   89]
per-ex loss: 0.373044  [   86/   89]
per-ex loss: 0.319041  [   87/   89]
per-ex loss: 0.255504  [   88/   89]
per-ex loss: 0.332397  [   89/   89]
Train Error: Avg loss: 0.33879240
validation Error: 
 Avg loss: 0.54054915 
 F1: 0.484336 
 Precision: 0.487909 
 Recall: 0.480815
 IoU: 0.319554

test Error: 
 Avg loss: 0.48981319 
 F1: 0.553622 
 Precision: 0.554203 
 Recall: 0.553042
 IoU: 0.382764

We have finished training iteration 211
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_209_.pth
per-ex loss: 0.417175  [    1/   89]
per-ex loss: 0.247861  [    2/   89]
per-ex loss: 0.348651  [    3/   89]
per-ex loss: 0.201075  [    4/   89]
per-ex loss: 0.277500  [    5/   89]
per-ex loss: 0.370136  [    6/   89]
per-ex loss: 0.229542  [    7/   89]
per-ex loss: 0.379650  [    8/   89]
per-ex loss: 0.390543  [    9/   89]
per-ex loss: 0.319033  [   10/   89]
per-ex loss: 0.306024  [   11/   89]
per-ex loss: 0.311004  [   12/   89]
per-ex loss: 0.413786  [   13/   89]
per-ex loss: 0.154030  [   14/   89]
per-ex loss: 0.457879  [   15/   89]
per-ex loss: 0.477846  [   16/   89]
per-ex loss: 0.462456  [   17/   89]
per-ex loss: 0.484452  [   18/   89]
per-ex loss: 0.335291  [   19/   89]
per-ex loss: 0.283427  [   20/   89]
per-ex loss: 0.327018  [   21/   89]
per-ex loss: 0.302355  [   22/   89]
per-ex loss: 0.316509  [   23/   89]
per-ex loss: 0.242407  [   24/   89]
per-ex loss: 0.303624  [   25/   89]
per-ex loss: 0.336654  [   26/   89]
per-ex loss: 0.403111  [   27/   89]
per-ex loss: 0.284970  [   28/   89]
per-ex loss: 0.484537  [   29/   89]
per-ex loss: 0.482150  [   30/   89]
per-ex loss: 0.369287  [   31/   89]
per-ex loss: 0.150185  [   32/   89]
per-ex loss: 0.268411  [   33/   89]
per-ex loss: 0.561735  [   34/   89]
per-ex loss: 0.275689  [   35/   89]
per-ex loss: 0.192938  [   36/   89]
per-ex loss: 0.265239  [   37/   89]
per-ex loss: 0.527892  [   38/   89]
per-ex loss: 0.237211  [   39/   89]
per-ex loss: 0.347284  [   40/   89]
per-ex loss: 0.316289  [   41/   89]
per-ex loss: 0.263171  [   42/   89]
per-ex loss: 0.482515  [   43/   89]
per-ex loss: 0.422566  [   44/   89]
per-ex loss: 0.301934  [   45/   89]
per-ex loss: 0.192036  [   46/   89]
per-ex loss: 0.266401  [   47/   89]
per-ex loss: 0.283467  [   48/   89]
per-ex loss: 0.254849  [   49/   89]
per-ex loss: 0.273447  [   50/   89]
per-ex loss: 0.376372  [   51/   89]
per-ex loss: 0.695653  [   52/   89]
per-ex loss: 0.244823  [   53/   89]
per-ex loss: 0.291474  [   54/   89]
per-ex loss: 0.182727  [   55/   89]
per-ex loss: 0.294550  [   56/   89]
per-ex loss: 0.220443  [   57/   89]
per-ex loss: 0.374318  [   58/   89]
per-ex loss: 0.338968  [   59/   89]
per-ex loss: 0.236564  [   60/   89]
per-ex loss: 0.396431  [   61/   89]
per-ex loss: 0.287083  [   62/   89]
per-ex loss: 0.282310  [   63/   89]
per-ex loss: 0.547176  [   64/   89]
per-ex loss: 0.319430  [   65/   89]
per-ex loss: 0.451544  [   66/   89]
per-ex loss: 0.339783  [   67/   89]
per-ex loss: 0.249512  [   68/   89]
per-ex loss: 0.440160  [   69/   89]
per-ex loss: 0.317907  [   70/   89]
per-ex loss: 0.289311  [   71/   89]
per-ex loss: 0.563934  [   72/   89]
per-ex loss: 0.301702  [   73/   89]
per-ex loss: 0.533724  [   74/   89]
per-ex loss: 0.163187  [   75/   89]
per-ex loss: 0.272808  [   76/   89]
per-ex loss: 0.475665  [   77/   89]
per-ex loss: 0.326452  [   78/   89]
per-ex loss: 0.367317  [   79/   89]
per-ex loss: 0.450318  [   80/   89]
per-ex loss: 0.556149  [   81/   89]
per-ex loss: 0.230478  [   82/   89]
per-ex loss: 0.426989  [   83/   89]
per-ex loss: 0.372724  [   84/   89]
per-ex loss: 0.264627  [   85/   89]
per-ex loss: 0.999900  [   86/   89]
per-ex loss: 0.382402  [   87/   89]
per-ex loss: 0.445366  [   88/   89]
per-ex loss: 0.208307  [   89/   89]
Train Error: Avg loss: 0.34968348
validation Error: 
 Avg loss: 0.63091058 
 F1: 0.406407 
 Precision: 0.355296 
 Recall: 0.474693
 IoU: 0.255025

test Error: 
 Avg loss: 0.60519559 
 F1: 0.437277 
 Precision: 0.344484 
 Recall: 0.598490
 IoU: 0.279817

We have finished training iteration 212
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_210_.pth
per-ex loss: 0.327861  [    1/   89]
per-ex loss: 0.212242  [    2/   89]
per-ex loss: 0.406524  [    3/   89]
per-ex loss: 0.320924  [    4/   89]
per-ex loss: 0.302250  [    5/   89]
per-ex loss: 0.340661  [    6/   89]
per-ex loss: 0.523400  [    7/   89]
per-ex loss: 0.273873  [    8/   89]
per-ex loss: 0.379306  [    9/   89]
per-ex loss: 0.278114  [   10/   89]
per-ex loss: 0.358833  [   11/   89]
per-ex loss: 0.302514  [   12/   89]
per-ex loss: 0.625122  [   13/   89]
per-ex loss: 0.165523  [   14/   89]
per-ex loss: 0.212851  [   15/   89]
per-ex loss: 0.271938  [   16/   89]
per-ex loss: 0.335607  [   17/   89]
per-ex loss: 0.243521  [   18/   89]
per-ex loss: 0.294517  [   19/   89]
per-ex loss: 0.230658  [   20/   89]
per-ex loss: 0.275725  [   21/   89]
per-ex loss: 0.269595  [   22/   89]
per-ex loss: 0.309494  [   23/   89]
per-ex loss: 0.269339  [   24/   89]
per-ex loss: 0.263612  [   25/   89]
per-ex loss: 0.302880  [   26/   89]
per-ex loss: 0.297366  [   27/   89]
per-ex loss: 0.362577  [   28/   89]
per-ex loss: 0.301408  [   29/   89]
per-ex loss: 0.268223  [   30/   89]
per-ex loss: 0.254836  [   31/   89]
per-ex loss: 0.558884  [   32/   89]
per-ex loss: 0.399167  [   33/   89]
per-ex loss: 0.462310  [   34/   89]
per-ex loss: 0.305697  [   35/   89]
per-ex loss: 0.412093  [   36/   89]
per-ex loss: 0.262670  [   37/   89]
per-ex loss: 0.365445  [   38/   89]
per-ex loss: 0.421089  [   39/   89]
per-ex loss: 0.269738  [   40/   89]
per-ex loss: 0.316891  [   41/   89]
per-ex loss: 0.320397  [   42/   89]
per-ex loss: 0.328573  [   43/   89]
per-ex loss: 0.432716  [   44/   89]
per-ex loss: 0.414393  [   45/   89]
per-ex loss: 0.227514  [   46/   89]
per-ex loss: 0.258621  [   47/   89]
per-ex loss: 0.464968  [   48/   89]
per-ex loss: 0.521131  [   49/   89]
per-ex loss: 0.343772  [   50/   89]
per-ex loss: 0.139890  [   51/   89]
per-ex loss: 0.490478  [   52/   89]
per-ex loss: 0.535047  [   53/   89]
per-ex loss: 0.312129  [   54/   89]
per-ex loss: 0.415806  [   55/   89]
per-ex loss: 0.342198  [   56/   89]
per-ex loss: 0.404191  [   57/   89]
per-ex loss: 0.471368  [   58/   89]
per-ex loss: 0.313681  [   59/   89]
per-ex loss: 0.146063  [   60/   89]
per-ex loss: 0.260495  [   61/   89]
per-ex loss: 0.528353  [   62/   89]
per-ex loss: 0.217768  [   63/   89]
per-ex loss: 0.209109  [   64/   89]
per-ex loss: 0.364833  [   65/   89]
per-ex loss: 0.359934  [   66/   89]
per-ex loss: 0.261077  [   67/   89]
per-ex loss: 0.432257  [   68/   89]
per-ex loss: 0.314244  [   69/   89]
per-ex loss: 0.358825  [   70/   89]
per-ex loss: 0.232630  [   71/   89]
per-ex loss: 0.308703  [   72/   89]
per-ex loss: 0.254302  [   73/   89]
per-ex loss: 0.345602  [   74/   89]
per-ex loss: 0.469526  [   75/   89]
per-ex loss: 0.219829  [   76/   89]
per-ex loss: 0.137313  [   77/   89]
per-ex loss: 0.266515  [   78/   89]
per-ex loss: 0.459115  [   79/   89]
per-ex loss: 0.366357  [   80/   89]
per-ex loss: 0.318770  [   81/   89]
per-ex loss: 0.370891  [   82/   89]
per-ex loss: 0.340677  [   83/   89]
per-ex loss: 0.269424  [   84/   89]
per-ex loss: 0.247305  [   85/   89]
per-ex loss: 0.273752  [   86/   89]
per-ex loss: 0.274905  [   87/   89]
per-ex loss: 0.999954  [   88/   89]
per-ex loss: 0.288713  [   89/   89]
Train Error: Avg loss: 0.33695947
validation Error: 
 Avg loss: 0.66099712 
 F1: 0.377435 
 Precision: 0.296338 
 Recall: 0.519642
 IoU: 0.232616

test Error: 
 Avg loss: 0.63719132 
 F1: 0.400376 
 Precision: 0.290123 
 Recall: 0.645789
 IoU: 0.250294

We have finished training iteration 213
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_211_.pth
per-ex loss: 0.518176  [    1/   89]
per-ex loss: 0.457482  [    2/   89]
per-ex loss: 0.225286  [    3/   89]
per-ex loss: 0.294353  [    4/   89]
per-ex loss: 0.261166  [    5/   89]
per-ex loss: 0.400550  [    6/   89]
per-ex loss: 0.467218  [    7/   89]
per-ex loss: 0.372152  [    8/   89]
per-ex loss: 0.269944  [    9/   89]
per-ex loss: 0.249189  [   10/   89]
per-ex loss: 0.300930  [   11/   89]
per-ex loss: 0.405125  [   12/   89]
per-ex loss: 0.277710  [   13/   89]
per-ex loss: 0.492582  [   14/   89]
per-ex loss: 0.248424  [   15/   89]
per-ex loss: 0.438977  [   16/   89]
per-ex loss: 0.247834  [   17/   89]
per-ex loss: 0.272837  [   18/   89]
per-ex loss: 0.260933  [   19/   89]
per-ex loss: 0.512320  [   20/   89]
per-ex loss: 0.315471  [   21/   89]
per-ex loss: 0.231117  [   22/   89]
per-ex loss: 0.360589  [   23/   89]
per-ex loss: 0.320095  [   24/   89]
per-ex loss: 0.338339  [   25/   89]
per-ex loss: 0.348599  [   26/   89]
per-ex loss: 0.227984  [   27/   89]
per-ex loss: 0.540272  [   28/   89]
per-ex loss: 0.323843  [   29/   89]
per-ex loss: 0.217699  [   30/   89]
per-ex loss: 0.318719  [   31/   89]
per-ex loss: 0.266598  [   32/   89]
per-ex loss: 0.259884  [   33/   89]
per-ex loss: 0.412632  [   34/   89]
per-ex loss: 0.283896  [   35/   89]
per-ex loss: 0.350014  [   36/   89]
per-ex loss: 0.367204  [   37/   89]
per-ex loss: 0.309583  [   38/   89]
per-ex loss: 0.355953  [   39/   89]
per-ex loss: 0.499865  [   40/   89]
per-ex loss: 0.426591  [   41/   89]
per-ex loss: 0.399471  [   42/   89]
per-ex loss: 0.340152  [   43/   89]
per-ex loss: 0.168833  [   44/   89]
per-ex loss: 0.182376  [   45/   89]
per-ex loss: 0.487441  [   46/   89]
per-ex loss: 0.535223  [   47/   89]
per-ex loss: 0.312761  [   48/   89]
per-ex loss: 0.270730  [   49/   89]
per-ex loss: 0.224915  [   50/   89]
per-ex loss: 0.341249  [   51/   89]
per-ex loss: 0.575730  [   52/   89]
per-ex loss: 0.307126  [   53/   89]
per-ex loss: 0.519833  [   54/   89]
per-ex loss: 0.467762  [   55/   89]
per-ex loss: 0.318411  [   56/   89]
per-ex loss: 0.311671  [   57/   89]
per-ex loss: 0.342834  [   58/   89]
per-ex loss: 0.379416  [   59/   89]
per-ex loss: 0.253447  [   60/   89]
per-ex loss: 0.324129  [   61/   89]
per-ex loss: 0.220085  [   62/   89]
per-ex loss: 0.263452  [   63/   89]
per-ex loss: 0.261320  [   64/   89]
per-ex loss: 0.268883  [   65/   89]
per-ex loss: 0.428598  [   66/   89]
per-ex loss: 0.313878  [   67/   89]
per-ex loss: 0.317452  [   68/   89]
per-ex loss: 0.142749  [   69/   89]
per-ex loss: 0.239770  [   70/   89]
per-ex loss: 0.191621  [   71/   89]
per-ex loss: 0.209748  [   72/   89]
per-ex loss: 0.214013  [   73/   89]
per-ex loss: 0.999958  [   74/   89]
per-ex loss: 0.373722  [   75/   89]
per-ex loss: 0.233475  [   76/   89]
per-ex loss: 0.517540  [   77/   89]
per-ex loss: 0.443625  [   78/   89]
per-ex loss: 0.232520  [   79/   89]
per-ex loss: 0.275130  [   80/   89]
per-ex loss: 0.378808  [   81/   89]
per-ex loss: 0.609390  [   82/   89]
per-ex loss: 0.344851  [   83/   89]
per-ex loss: 0.333414  [   84/   89]
per-ex loss: 0.214274  [   85/   89]
per-ex loss: 0.261904  [   86/   89]
per-ex loss: 0.466229  [   87/   89]
per-ex loss: 0.375979  [   88/   89]
per-ex loss: 0.414353  [   89/   89]
Train Error: Avg loss: 0.34447626
validation Error: 
 Avg loss: 0.55424992 
 F1: 0.473762 
 Precision: 0.554972 
 Recall: 0.413285
 IoU: 0.310412

test Error: 
 Avg loss: 0.51433826 
 F1: 0.531525 
 Precision: 0.573304 
 Recall: 0.495421
 IoU: 0.361957

We have finished training iteration 214
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_212_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.349740  [    1/   89]
per-ex loss: 0.460334  [    2/   89]
per-ex loss: 0.262230  [    3/   89]
per-ex loss: 0.234328  [    4/   89]
per-ex loss: 0.321577  [    5/   89]
per-ex loss: 0.277113  [    6/   89]
per-ex loss: 0.300119  [    7/   89]
per-ex loss: 0.488668  [    8/   89]
per-ex loss: 0.183682  [    9/   89]
per-ex loss: 0.416252  [   10/   89]
per-ex loss: 0.425710  [   11/   89]
per-ex loss: 0.292023  [   12/   89]
per-ex loss: 0.244358  [   13/   89]
per-ex loss: 0.275419  [   14/   89]
per-ex loss: 0.287998  [   15/   89]
per-ex loss: 0.538496  [   16/   89]
per-ex loss: 0.244017  [   17/   89]
per-ex loss: 0.532307  [   18/   89]
per-ex loss: 0.292564  [   19/   89]
per-ex loss: 0.317330  [   20/   89]
per-ex loss: 0.999927  [   21/   89]
per-ex loss: 0.273603  [   22/   89]
per-ex loss: 0.372607  [   23/   89]
per-ex loss: 0.373405  [   24/   89]
per-ex loss: 0.315034  [   25/   89]
per-ex loss: 0.260444  [   26/   89]
per-ex loss: 0.395194  [   27/   89]
per-ex loss: 0.370264  [   28/   89]
per-ex loss: 0.407700  [   29/   89]
per-ex loss: 0.212371  [   30/   89]
per-ex loss: 0.446599  [   31/   89]
per-ex loss: 0.355328  [   32/   89]
per-ex loss: 0.255554  [   33/   89]
per-ex loss: 0.291127  [   34/   89]
per-ex loss: 0.203302  [   35/   89]
per-ex loss: 0.168972  [   36/   89]
per-ex loss: 0.556561  [   37/   89]
per-ex loss: 0.292112  [   38/   89]
per-ex loss: 0.472130  [   39/   89]
per-ex loss: 0.177162  [   40/   89]
per-ex loss: 0.303795  [   41/   89]
per-ex loss: 0.239847  [   42/   89]
per-ex loss: 0.184838  [   43/   89]
per-ex loss: 0.242293  [   44/   89]
per-ex loss: 0.537513  [   45/   89]
per-ex loss: 0.134328  [   46/   89]
per-ex loss: 0.310077  [   47/   89]
per-ex loss: 0.417003  [   48/   89]
per-ex loss: 0.431152  [   49/   89]
per-ex loss: 0.447573  [   50/   89]
per-ex loss: 0.334557  [   51/   89]
per-ex loss: 0.235163  [   52/   89]
per-ex loss: 0.255498  [   53/   89]
per-ex loss: 0.494173  [   54/   89]
per-ex loss: 0.570289  [   55/   89]
per-ex loss: 0.259517  [   56/   89]
per-ex loss: 0.419441  [   57/   89]
per-ex loss: 0.286426  [   58/   89]
per-ex loss: 0.287752  [   59/   89]
per-ex loss: 0.445962  [   60/   89]
per-ex loss: 0.214042  [   61/   89]
per-ex loss: 0.372788  [   62/   89]
per-ex loss: 0.454688  [   63/   89]
per-ex loss: 0.575958  [   64/   89]
per-ex loss: 0.342168  [   65/   89]
per-ex loss: 0.321898  [   66/   89]
per-ex loss: 0.556663  [   67/   89]
per-ex loss: 0.485520  [   68/   89]
per-ex loss: 0.276088  [   69/   89]
per-ex loss: 0.339378  [   70/   89]
per-ex loss: 0.280149  [   71/   89]
per-ex loss: 0.326553  [   72/   89]
per-ex loss: 0.337630  [   73/   89]
per-ex loss: 0.306915  [   74/   89]
per-ex loss: 0.317656  [   75/   89]
per-ex loss: 0.219348  [   76/   89]
per-ex loss: 0.274584  [   77/   89]
per-ex loss: 0.319590  [   78/   89]
per-ex loss: 0.509964  [   79/   89]
per-ex loss: 0.234574  [   80/   89]
per-ex loss: 0.466968  [   81/   89]
per-ex loss: 0.235442  [   82/   89]
per-ex loss: 0.238912  [   83/   89]
per-ex loss: 0.348479  [   84/   89]
per-ex loss: 0.501391  [   85/   89]
per-ex loss: 0.368902  [   86/   89]
per-ex loss: 0.261707  [   87/   89]
per-ex loss: 0.541569  [   88/   89]
per-ex loss: 0.289030  [   89/   89]
Train Error: Avg loss: 0.34943158
validation Error: 
 Avg loss: 0.54431581 
 F1: 0.483342 
 Precision: 0.473657 
 Recall: 0.493431
 IoU: 0.318689

test Error: 
 Avg loss: 0.48341938 
 F1: 0.558124 
 Precision: 0.534807 
 Recall: 0.583566
 IoU: 0.387081

We have finished training iteration 215
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_213_.pth
per-ex loss: 0.158835  [    1/   89]
per-ex loss: 0.449971  [    2/   89]
per-ex loss: 0.309913  [    3/   89]
per-ex loss: 0.209147  [    4/   89]
per-ex loss: 0.179469  [    5/   89]
per-ex loss: 0.269032  [    6/   89]
per-ex loss: 0.999879  [    7/   89]
per-ex loss: 0.269553  [    8/   89]
per-ex loss: 0.518334  [    9/   89]
per-ex loss: 0.274624  [   10/   89]
per-ex loss: 0.570584  [   11/   89]
per-ex loss: 0.421656  [   12/   89]
per-ex loss: 0.247597  [   13/   89]
per-ex loss: 0.377350  [   14/   89]
per-ex loss: 0.195659  [   15/   89]
per-ex loss: 0.314779  [   16/   89]
per-ex loss: 0.525988  [   17/   89]
per-ex loss: 0.160185  [   18/   89]
per-ex loss: 0.246732  [   19/   89]
per-ex loss: 0.241759  [   20/   89]
per-ex loss: 0.287639  [   21/   89]
per-ex loss: 0.258124  [   22/   89]
per-ex loss: 0.276440  [   23/   89]
per-ex loss: 0.343892  [   24/   89]
per-ex loss: 0.308798  [   25/   89]
per-ex loss: 0.439348  [   26/   89]
per-ex loss: 0.484329  [   27/   89]
per-ex loss: 0.206551  [   28/   89]
per-ex loss: 0.401028  [   29/   89]
per-ex loss: 0.229692  [   30/   89]
per-ex loss: 0.328834  [   31/   89]
per-ex loss: 0.210270  [   32/   89]
per-ex loss: 0.224066  [   33/   89]
per-ex loss: 0.264676  [   34/   89]
per-ex loss: 0.529472  [   35/   89]
per-ex loss: 0.374249  [   36/   89]
per-ex loss: 0.450071  [   37/   89]
per-ex loss: 0.269635  [   38/   89]
per-ex loss: 0.235076  [   39/   89]
per-ex loss: 0.423403  [   40/   89]
per-ex loss: 0.312818  [   41/   89]
per-ex loss: 0.209823  [   42/   89]
per-ex loss: 0.249826  [   43/   89]
per-ex loss: 0.289534  [   44/   89]
per-ex loss: 0.344558  [   45/   89]
per-ex loss: 0.313700  [   46/   89]
per-ex loss: 0.400903  [   47/   89]
per-ex loss: 0.373066  [   48/   89]
per-ex loss: 0.228862  [   49/   89]
per-ex loss: 0.370770  [   50/   89]
per-ex loss: 0.265466  [   51/   89]
per-ex loss: 0.445726  [   52/   89]
per-ex loss: 0.286424  [   53/   89]
per-ex loss: 0.269441  [   54/   89]
per-ex loss: 0.490343  [   55/   89]
per-ex loss: 0.318887  [   56/   89]
per-ex loss: 0.571573  [   57/   89]
per-ex loss: 0.336471  [   58/   89]
per-ex loss: 0.208292  [   59/   89]
per-ex loss: 0.408473  [   60/   89]
per-ex loss: 0.490536  [   61/   89]
per-ex loss: 0.440631  [   62/   89]
per-ex loss: 0.264945  [   63/   89]
per-ex loss: 0.465537  [   64/   89]
per-ex loss: 0.335326  [   65/   89]
per-ex loss: 0.284302  [   66/   89]
per-ex loss: 0.269453  [   67/   89]
per-ex loss: 0.315904  [   68/   89]
per-ex loss: 0.373435  [   69/   89]
per-ex loss: 0.214132  [   70/   89]
per-ex loss: 0.482180  [   71/   89]
per-ex loss: 0.184752  [   72/   89]
per-ex loss: 0.553885  [   73/   89]
per-ex loss: 0.328968  [   74/   89]
per-ex loss: 0.234349  [   75/   89]
per-ex loss: 0.438078  [   76/   89]
per-ex loss: 0.159973  [   77/   89]
per-ex loss: 0.258515  [   78/   89]
per-ex loss: 0.234333  [   79/   89]
per-ex loss: 0.363633  [   80/   89]
per-ex loss: 0.502028  [   81/   89]
per-ex loss: 0.329831  [   82/   89]
per-ex loss: 0.347726  [   83/   89]
per-ex loss: 0.424964  [   84/   89]
per-ex loss: 0.308874  [   85/   89]
per-ex loss: 0.274400  [   86/   89]
per-ex loss: 0.287532  [   87/   89]
per-ex loss: 0.359169  [   88/   89]
per-ex loss: 0.461906  [   89/   89]
Train Error: Avg loss: 0.33899880
validation Error: 
 Avg loss: 0.53442337 
 F1: 0.496276 
 Precision: 0.543613 
 Recall: 0.456523
 IoU: 0.330031

test Error: 
 Avg loss: 0.48911419 
 F1: 0.556359 
 Precision: 0.593402 
 Recall: 0.523669
 IoU: 0.385386

We have finished training iteration 216
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_214_.pth
per-ex loss: 0.195339  [    1/   89]
per-ex loss: 0.252672  [    2/   89]
per-ex loss: 0.251236  [    3/   89]
per-ex loss: 0.662108  [    4/   89]
per-ex loss: 0.562865  [    5/   89]
per-ex loss: 0.326640  [    6/   89]
per-ex loss: 0.583955  [    7/   89]
per-ex loss: 0.446413  [    8/   89]
per-ex loss: 0.325657  [    9/   89]
per-ex loss: 0.488155  [   10/   89]
per-ex loss: 0.322306  [   11/   89]
per-ex loss: 0.212119  [   12/   89]
per-ex loss: 0.454380  [   13/   89]
per-ex loss: 0.256622  [   14/   89]
per-ex loss: 0.288537  [   15/   89]
per-ex loss: 0.279337  [   16/   89]
per-ex loss: 0.270861  [   17/   89]
per-ex loss: 0.406062  [   18/   89]
per-ex loss: 0.367450  [   19/   89]
per-ex loss: 0.678704  [   20/   89]
per-ex loss: 0.192410  [   21/   89]
per-ex loss: 0.329126  [   22/   89]
per-ex loss: 0.328471  [   23/   89]
per-ex loss: 0.438424  [   24/   89]
per-ex loss: 0.296199  [   25/   89]
per-ex loss: 0.440596  [   26/   89]
per-ex loss: 0.588794  [   27/   89]
per-ex loss: 0.327199  [   28/   89]
per-ex loss: 0.241200  [   29/   89]
per-ex loss: 0.520407  [   30/   89]
per-ex loss: 0.428005  [   31/   89]
per-ex loss: 0.235579  [   32/   89]
per-ex loss: 0.332140  [   33/   89]
per-ex loss: 0.282377  [   34/   89]
per-ex loss: 0.451143  [   35/   89]
per-ex loss: 0.286167  [   36/   89]
per-ex loss: 0.999879  [   37/   89]
per-ex loss: 0.370600  [   38/   89]
per-ex loss: 0.199808  [   39/   89]
per-ex loss: 0.261294  [   40/   89]
per-ex loss: 0.490136  [   41/   89]
per-ex loss: 0.366098  [   42/   89]
per-ex loss: 0.273759  [   43/   89]
per-ex loss: 0.282055  [   44/   89]
per-ex loss: 0.435605  [   45/   89]
per-ex loss: 0.370814  [   46/   89]
per-ex loss: 0.446272  [   47/   89]
per-ex loss: 0.228420  [   48/   89]
per-ex loss: 0.311872  [   49/   89]
per-ex loss: 0.378475  [   50/   89]
per-ex loss: 0.297890  [   51/   89]
per-ex loss: 0.528316  [   52/   89]
per-ex loss: 0.508214  [   53/   89]
per-ex loss: 0.352790  [   54/   89]
per-ex loss: 0.290557  [   55/   89]
per-ex loss: 0.172084  [   56/   89]
per-ex loss: 0.286825  [   57/   89]
per-ex loss: 0.480847  [   58/   89]
per-ex loss: 0.442534  [   59/   89]
per-ex loss: 0.340793  [   60/   89]
per-ex loss: 0.289658  [   61/   89]
per-ex loss: 0.404405  [   62/   89]
per-ex loss: 0.340458  [   63/   89]
per-ex loss: 0.381188  [   64/   89]
per-ex loss: 0.567494  [   65/   89]
per-ex loss: 0.355122  [   66/   89]
per-ex loss: 0.306935  [   67/   89]
per-ex loss: 0.321369  [   68/   89]
per-ex loss: 0.470580  [   69/   89]
per-ex loss: 0.287024  [   70/   89]
per-ex loss: 0.273354  [   71/   89]
per-ex loss: 0.310661  [   72/   89]
per-ex loss: 0.474083  [   73/   89]
per-ex loss: 0.315329  [   74/   89]
per-ex loss: 0.323180  [   75/   89]
per-ex loss: 0.306544  [   76/   89]
per-ex loss: 0.536717  [   77/   89]
per-ex loss: 0.302337  [   78/   89]
per-ex loss: 0.224010  [   79/   89]
per-ex loss: 0.418117  [   80/   89]
per-ex loss: 0.267537  [   81/   89]
per-ex loss: 0.232923  [   82/   89]
per-ex loss: 0.426923  [   83/   89]
per-ex loss: 0.387614  [   84/   89]
per-ex loss: 0.217121  [   85/   89]
per-ex loss: 0.271596  [   86/   89]
per-ex loss: 0.223277  [   87/   89]
per-ex loss: 0.266914  [   88/   89]
per-ex loss: 0.245800  [   89/   89]
Train Error: Avg loss: 0.36195356
validation Error: 
 Avg loss: 0.53620360 
 F1: 0.492387 
 Precision: 0.528771 
 Recall: 0.460687
 IoU: 0.326600

test Error: 
 Avg loss: 0.48473874 
 F1: 0.560041 
 Precision: 0.580481 
 Recall: 0.540991
 IoU: 0.388928

We have finished training iteration 217
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_215_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.566135  [    1/   89]
per-ex loss: 0.411106  [    2/   89]
per-ex loss: 0.252962  [    3/   89]
per-ex loss: 0.355942  [    4/   89]
per-ex loss: 0.218582  [    5/   89]
per-ex loss: 0.354574  [    6/   89]
per-ex loss: 0.341576  [    7/   89]
per-ex loss: 0.304826  [    8/   89]
per-ex loss: 0.549726  [    9/   89]
per-ex loss: 0.284731  [   10/   89]
per-ex loss: 0.314345  [   11/   89]
per-ex loss: 0.237610  [   12/   89]
per-ex loss: 0.380489  [   13/   89]
per-ex loss: 0.297914  [   14/   89]
per-ex loss: 0.192161  [   15/   89]
per-ex loss: 0.226869  [   16/   89]
per-ex loss: 0.282808  [   17/   89]
per-ex loss: 0.329950  [   18/   89]
per-ex loss: 0.283232  [   19/   89]
per-ex loss: 0.138178  [   20/   89]
per-ex loss: 0.238168  [   21/   89]
per-ex loss: 0.305834  [   22/   89]
per-ex loss: 0.373371  [   23/   89]
per-ex loss: 0.208482  [   24/   89]
per-ex loss: 0.365253  [   25/   89]
per-ex loss: 0.300203  [   26/   89]
per-ex loss: 0.388251  [   27/   89]
per-ex loss: 0.397131  [   28/   89]
per-ex loss: 0.284102  [   29/   89]
per-ex loss: 0.286959  [   30/   89]
per-ex loss: 0.300321  [   31/   89]
per-ex loss: 0.255015  [   32/   89]
per-ex loss: 0.343210  [   33/   89]
per-ex loss: 0.314586  [   34/   89]
per-ex loss: 0.412692  [   35/   89]
per-ex loss: 0.455731  [   36/   89]
per-ex loss: 0.999941  [   37/   89]
per-ex loss: 0.241965  [   38/   89]
per-ex loss: 0.502441  [   39/   89]
per-ex loss: 0.260079  [   40/   89]
per-ex loss: 0.218694  [   41/   89]
per-ex loss: 0.369909  [   42/   89]
per-ex loss: 0.206357  [   43/   89]
per-ex loss: 0.319204  [   44/   89]
per-ex loss: 0.230068  [   45/   89]
per-ex loss: 0.353765  [   46/   89]
per-ex loss: 0.498764  [   47/   89]
per-ex loss: 0.370673  [   48/   89]
per-ex loss: 0.244466  [   49/   89]
per-ex loss: 0.304504  [   50/   89]
per-ex loss: 0.254128  [   51/   89]
per-ex loss: 0.370688  [   52/   89]
per-ex loss: 0.279075  [   53/   89]
per-ex loss: 0.457848  [   54/   89]
per-ex loss: 0.265418  [   55/   89]
per-ex loss: 0.374175  [   56/   89]
per-ex loss: 0.468112  [   57/   89]
per-ex loss: 0.261593  [   58/   89]
per-ex loss: 0.317904  [   59/   89]
per-ex loss: 0.540442  [   60/   89]
per-ex loss: 0.477367  [   61/   89]
per-ex loss: 0.246070  [   62/   89]
per-ex loss: 0.269765  [   63/   89]
per-ex loss: 0.378815  [   64/   89]
per-ex loss: 0.286960  [   65/   89]
per-ex loss: 0.154379  [   66/   89]
per-ex loss: 0.311436  [   67/   89]
per-ex loss: 0.490805  [   68/   89]
per-ex loss: 0.377595  [   69/   89]
per-ex loss: 0.478675  [   70/   89]
per-ex loss: 0.516208  [   71/   89]
per-ex loss: 0.402525  [   72/   89]
per-ex loss: 0.416058  [   73/   89]
per-ex loss: 0.351642  [   74/   89]
per-ex loss: 0.386464  [   75/   89]
per-ex loss: 0.297087  [   76/   89]
per-ex loss: 0.347156  [   77/   89]
per-ex loss: 0.290151  [   78/   89]
per-ex loss: 0.416760  [   79/   89]
per-ex loss: 0.308663  [   80/   89]
per-ex loss: 0.516363  [   81/   89]
per-ex loss: 0.205884  [   82/   89]
per-ex loss: 0.377744  [   83/   89]
per-ex loss: 0.210988  [   84/   89]
per-ex loss: 0.276780  [   85/   89]
per-ex loss: 0.623608  [   86/   89]
per-ex loss: 0.591347  [   87/   89]
per-ex loss: 0.399823  [   88/   89]
per-ex loss: 0.261191  [   89/   89]
Train Error: Avg loss: 0.34754548
validation Error: 
 Avg loss: 0.55273291 
 F1: 0.477668 
 Precision: 0.460762 
 Recall: 0.495862
 IoU: 0.313774

test Error: 
 Avg loss: 0.48762410 
 F1: 0.554484 
 Precision: 0.529915 
 Recall: 0.581441
 IoU: 0.383589

We have finished training iteration 218
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_216_.pth
per-ex loss: 0.306203  [    1/   89]
per-ex loss: 0.285365  [    2/   89]
per-ex loss: 0.484639  [    3/   89]
per-ex loss: 0.372917  [    4/   89]
per-ex loss: 0.275972  [    5/   89]
per-ex loss: 0.387400  [    6/   89]
per-ex loss: 0.330483  [    7/   89]
per-ex loss: 0.342457  [    8/   89]
per-ex loss: 0.463946  [    9/   89]
per-ex loss: 0.508306  [   10/   89]
per-ex loss: 0.255809  [   11/   89]
per-ex loss: 0.405943  [   12/   89]
per-ex loss: 0.547850  [   13/   89]
per-ex loss: 0.424049  [   14/   89]
per-ex loss: 0.276625  [   15/   89]
per-ex loss: 0.211738  [   16/   89]
per-ex loss: 0.256326  [   17/   89]
per-ex loss: 0.329736  [   18/   89]
per-ex loss: 0.236473  [   19/   89]
per-ex loss: 0.495841  [   20/   89]
per-ex loss: 0.408740  [   21/   89]
per-ex loss: 0.337309  [   22/   89]
per-ex loss: 0.450699  [   23/   89]
per-ex loss: 0.262296  [   24/   89]
per-ex loss: 0.328916  [   25/   89]
per-ex loss: 0.432292  [   26/   89]
per-ex loss: 0.326560  [   27/   89]
per-ex loss: 0.567430  [   28/   89]
per-ex loss: 0.477294  [   29/   89]
per-ex loss: 0.999897  [   30/   89]
per-ex loss: 0.285283  [   31/   89]
per-ex loss: 0.475082  [   32/   89]
per-ex loss: 0.146390  [   33/   89]
per-ex loss: 0.421289  [   34/   89]
per-ex loss: 0.418013  [   35/   89]
per-ex loss: 0.550513  [   36/   89]
per-ex loss: 0.199618  [   37/   89]
per-ex loss: 0.214553  [   38/   89]
per-ex loss: 0.391414  [   39/   89]
per-ex loss: 0.314183  [   40/   89]
per-ex loss: 0.449918  [   41/   89]
per-ex loss: 0.262273  [   42/   89]
per-ex loss: 0.280286  [   43/   89]
per-ex loss: 0.184778  [   44/   89]
per-ex loss: 0.233160  [   45/   89]
per-ex loss: 0.303428  [   46/   89]
per-ex loss: 0.543437  [   47/   89]
per-ex loss: 0.306032  [   48/   89]
per-ex loss: 0.197972  [   49/   89]
per-ex loss: 0.322425  [   50/   89]
per-ex loss: 0.449271  [   51/   89]
per-ex loss: 0.424742  [   52/   89]
per-ex loss: 0.276921  [   53/   89]
per-ex loss: 0.140889  [   54/   89]
per-ex loss: 0.331574  [   55/   89]
per-ex loss: 0.675359  [   56/   89]
per-ex loss: 0.404484  [   57/   89]
per-ex loss: 0.409224  [   58/   89]
per-ex loss: 0.246716  [   59/   89]
per-ex loss: 0.230499  [   60/   89]
per-ex loss: 0.172383  [   61/   89]
per-ex loss: 0.292021  [   62/   89]
per-ex loss: 0.233062  [   63/   89]
per-ex loss: 0.609903  [   64/   89]
per-ex loss: 0.304487  [   65/   89]
per-ex loss: 0.268958  [   66/   89]
per-ex loss: 0.308596  [   67/   89]
per-ex loss: 0.351677  [   68/   89]
per-ex loss: 0.377168  [   69/   89]
per-ex loss: 0.222756  [   70/   89]
per-ex loss: 0.488710  [   71/   89]
per-ex loss: 0.322923  [   72/   89]
per-ex loss: 0.248379  [   73/   89]
per-ex loss: 0.482881  [   74/   89]
per-ex loss: 0.193402  [   75/   89]
per-ex loss: 0.315652  [   76/   89]
per-ex loss: 0.252553  [   77/   89]
per-ex loss: 0.144819  [   78/   89]
per-ex loss: 0.368567  [   79/   89]
per-ex loss: 0.278367  [   80/   89]
per-ex loss: 0.252883  [   81/   89]
per-ex loss: 0.287166  [   82/   89]
per-ex loss: 0.164686  [   83/   89]
per-ex loss: 0.207874  [   84/   89]
per-ex loss: 0.277636  [   85/   89]
per-ex loss: 0.493410  [   86/   89]
per-ex loss: 0.286118  [   87/   89]
per-ex loss: 0.552487  [   88/   89]
per-ex loss: 0.419227  [   89/   89]
Train Error: Avg loss: 0.34896614
validation Error: 
 Avg loss: 0.53710248 
 F1: 0.492027 
 Precision: 0.535573 
 Recall: 0.455030
 IoU: 0.326284

test Error: 
 Avg loss: 0.49007657 
 F1: 0.556348 
 Precision: 0.590015 
 Recall: 0.526316
 IoU: 0.385375

We have finished training iteration 219
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_217_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
per-ex loss: 0.209086  [    1/   89]
per-ex loss: 0.364032  [    2/   89]
per-ex loss: 0.375842  [    3/   89]
per-ex loss: 0.223271  [    4/   89]
per-ex loss: 0.256473  [    5/   89]
per-ex loss: 0.243640  [    6/   89]
per-ex loss: 0.388966  [    7/   89]
per-ex loss: 0.425599  [    8/   89]
per-ex loss: 0.208411  [    9/   89]
per-ex loss: 0.433450  [   10/   89]
per-ex loss: 0.278846  [   11/   89]
per-ex loss: 0.154387  [   12/   89]
per-ex loss: 0.369338  [   13/   89]
per-ex loss: 0.375616  [   14/   89]
per-ex loss: 0.206597  [   15/   89]
per-ex loss: 0.258049  [   16/   89]
per-ex loss: 0.510675  [   17/   89]
per-ex loss: 0.245677  [   18/   89]
per-ex loss: 0.388556  [   19/   89]
per-ex loss: 0.259479  [   20/   89]
per-ex loss: 0.203627  [   21/   89]
per-ex loss: 0.248469  [   22/   89]
per-ex loss: 0.199017  [   23/   89]
per-ex loss: 0.257450  [   24/   89]
per-ex loss: 0.161949  [   25/   89]
per-ex loss: 0.497254  [   26/   89]
per-ex loss: 0.458055  [   27/   89]
per-ex loss: 0.416548  [   28/   89]
per-ex loss: 0.304275  [   29/   89]
per-ex loss: 0.487123  [   30/   89]
per-ex loss: 0.225330  [   31/   89]
per-ex loss: 0.269819  [   32/   89]
per-ex loss: 0.408260  [   33/   89]
per-ex loss: 0.272578  [   34/   89]
per-ex loss: 0.325128  [   35/   89]
per-ex loss: 0.465242  [   36/   89]
per-ex loss: 0.346675  [   37/   89]
per-ex loss: 0.123582  [   38/   89]
per-ex loss: 0.331707  [   39/   89]
per-ex loss: 0.460428  [   40/   89]
per-ex loss: 0.129523  [   41/   89]
per-ex loss: 0.245611  [   42/   89]
per-ex loss: 0.335406  [   43/   89]
per-ex loss: 0.583968  [   44/   89]
per-ex loss: 0.287820  [   45/   89]
per-ex loss: 0.510386  [   46/   89]
per-ex loss: 0.414611  [   47/   89]
per-ex loss: 0.281754  [   48/   89]
per-ex loss: 0.308768  [   49/   89]
per-ex loss: 0.305611  [   50/   89]
per-ex loss: 0.373673  [   51/   89]
per-ex loss: 0.210043  [   52/   89]
per-ex loss: 0.457362  [   53/   89]
per-ex loss: 0.491298  [   54/   89]
per-ex loss: 0.360729  [   55/   89]
per-ex loss: 0.431163  [   56/   89]
per-ex loss: 0.519314  [   57/   89]
per-ex loss: 0.999894  [   58/   89]
per-ex loss: 0.317039  [   59/   89]
per-ex loss: 0.318171  [   60/   89]
per-ex loss: 0.350266  [   61/   89]
per-ex loss: 0.253172  [   62/   89]
per-ex loss: 0.354372  [   63/   89]
per-ex loss: 0.250984  [   64/   89]
per-ex loss: 0.435200  [   65/   89]
per-ex loss: 0.292439  [   66/   89]
per-ex loss: 0.295357  [   67/   89]
per-ex loss: 0.262354  [   68/   89]
per-ex loss: 0.538971  [   69/   89]
per-ex loss: 0.353332  [   70/   89]
per-ex loss: 0.530404  [   71/   89]
per-ex loss: 0.227063  [   72/   89]
per-ex loss: 0.204982  [   73/   89]
per-ex loss: 0.332597  [   74/   89]
per-ex loss: 0.295642  [   75/   89]
per-ex loss: 0.206547  [   76/   89]
per-ex loss: 0.324304  [   77/   89]
per-ex loss: 0.391486  [   78/   89]
per-ex loss: 0.370808  [   79/   89]
per-ex loss: 0.293427  [   80/   89]
per-ex loss: 0.327397  [   81/   89]
per-ex loss: 0.300713  [   82/   89]
per-ex loss: 0.308774  [   83/   89]
per-ex loss: 0.285355  [   84/   89]
per-ex loss: 0.189334  [   85/   89]
per-ex loss: 0.296277  [   86/   89]
per-ex loss: 0.480821  [   87/   89]
per-ex loss: 0.324532  [   88/   89]
per-ex loss: 0.529379  [   89/   89]
Train Error: Avg loss: 0.33850493
validation Error: 
 Avg loss: 0.53375342 
 F1: 0.491543 
 Precision: 0.536283 
 Recall: 0.453693
 IoU: 0.325858

test Error: 
 Avg loss: 0.48691898 
 F1: 0.559542 
 Precision: 0.598908 
 Recall: 0.525032
 IoU: 0.388447

We have finished training iteration 220
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_218_.pth
per-ex loss: 0.598603  [    1/   89]
per-ex loss: 0.258369  [    2/   89]
per-ex loss: 0.371480  [    3/   89]
per-ex loss: 0.293796  [    4/   89]
per-ex loss: 0.402049  [    5/   89]
per-ex loss: 0.274589  [    6/   89]
per-ex loss: 0.344196  [    7/   89]
per-ex loss: 0.369712  [    8/   89]
per-ex loss: 0.246356  [    9/   89]
per-ex loss: 0.551854  [   10/   89]
per-ex loss: 0.269858  [   11/   89]
per-ex loss: 0.258534  [   12/   89]
per-ex loss: 0.232196  [   13/   89]
per-ex loss: 0.448457  [   14/   89]
per-ex loss: 0.555516  [   15/   89]
per-ex loss: 0.293421  [   16/   89]
per-ex loss: 0.266357  [   17/   89]
per-ex loss: 0.263274  [   18/   89]
per-ex loss: 0.242788  [   19/   89]
per-ex loss: 0.448326  [   20/   89]
per-ex loss: 0.259246  [   21/   89]
per-ex loss: 0.214808  [   22/   89]
per-ex loss: 0.512366  [   23/   89]
per-ex loss: 0.282439  [   24/   89]
per-ex loss: 0.217681  [   25/   89]
per-ex loss: 0.288022  [   26/   89]
per-ex loss: 0.275667  [   27/   89]
per-ex loss: 0.480860  [   28/   89]
per-ex loss: 0.219820  [   29/   89]
per-ex loss: 0.368838  [   30/   89]
per-ex loss: 0.249824  [   31/   89]
per-ex loss: 0.317237  [   32/   89]
per-ex loss: 0.222987  [   33/   89]
per-ex loss: 0.145607  [   34/   89]
per-ex loss: 0.341134  [   35/   89]
per-ex loss: 0.537950  [   36/   89]
per-ex loss: 0.356317  [   37/   89]
per-ex loss: 0.160181  [   38/   89]
per-ex loss: 0.403282  [   39/   89]
per-ex loss: 0.238138  [   40/   89]
per-ex loss: 0.381581  [   41/   89]
per-ex loss: 0.480373  [   42/   89]
per-ex loss: 0.485903  [   43/   89]
per-ex loss: 0.346409  [   44/   89]
per-ex loss: 0.459168  [   45/   89]
per-ex loss: 0.225600  [   46/   89]
per-ex loss: 0.262193  [   47/   89]
per-ex loss: 0.362163  [   48/   89]
per-ex loss: 0.274879  [   49/   89]
per-ex loss: 0.325150  [   50/   89]
per-ex loss: 0.999879  [   51/   89]
per-ex loss: 0.496135  [   52/   89]
per-ex loss: 0.286459  [   53/   89]
per-ex loss: 0.400239  [   54/   89]
per-ex loss: 0.424695  [   55/   89]
per-ex loss: 0.294775  [   56/   89]
per-ex loss: 0.486330  [   57/   89]
per-ex loss: 0.171215  [   58/   89]
per-ex loss: 0.158563  [   59/   89]
per-ex loss: 0.313541  [   60/   89]
per-ex loss: 0.350416  [   61/   89]
per-ex loss: 0.333776  [   62/   89]
per-ex loss: 0.430227  [   63/   89]
per-ex loss: 0.374540  [   64/   89]
per-ex loss: 0.328752  [   65/   89]
per-ex loss: 0.249844  [   66/   89]
per-ex loss: 0.156251  [   67/   89]
per-ex loss: 0.317991  [   68/   89]
per-ex loss: 0.354336  [   69/   89]
per-ex loss: 0.529896  [   70/   89]
per-ex loss: 0.339985  [   71/   89]
per-ex loss: 0.449061  [   72/   89]
per-ex loss: 0.305155  [   73/   89]
per-ex loss: 0.339650  [   74/   89]
per-ex loss: 0.297177  [   75/   89]
per-ex loss: 0.352395  [   76/   89]
per-ex loss: 0.268142  [   77/   89]
per-ex loss: 0.205409  [   78/   89]
per-ex loss: 0.212077  [   79/   89]
per-ex loss: 0.245916  [   80/   89]
per-ex loss: 0.457419  [   81/   89]
per-ex loss: 0.332180  [   82/   89]
per-ex loss: 0.295571  [   83/   89]
per-ex loss: 0.478040  [   84/   89]
per-ex loss: 0.274750  [   85/   89]
per-ex loss: 0.545050  [   86/   89]
per-ex loss: 0.264647  [   87/   89]
per-ex loss: 0.239878  [   88/   89]
per-ex loss: 0.219534  [   89/   89]
Train Error: Avg loss: 0.34006124
validation Error: 
 Avg loss: 0.53472160 
 F1: 0.490670 
 Precision: 0.524455 
 Recall: 0.460973
 IoU: 0.325091

test Error: 
 Avg loss: 0.48893461 
 F1: 0.557364 
 Precision: 0.584497 
 Recall: 0.532639
 IoU: 0.386351

We have finished training iteration 221
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_219_.pth
per-ex loss: 0.276093  [    1/   89]
per-ex loss: 0.490670  [    2/   89]
per-ex loss: 0.274756  [    3/   89]
per-ex loss: 0.263406  [    4/   89]
per-ex loss: 0.398491  [    5/   89]
per-ex loss: 0.346823  [    6/   89]
per-ex loss: 0.435098  [    7/   89]
per-ex loss: 0.296298  [    8/   89]
per-ex loss: 0.333091  [    9/   89]
per-ex loss: 0.298676  [   10/   89]
per-ex loss: 0.134075  [   11/   89]
per-ex loss: 0.151982  [   12/   89]
per-ex loss: 0.603123  [   13/   89]
per-ex loss: 0.439432  [   14/   89]
per-ex loss: 0.286501  [   15/   89]
per-ex loss: 0.300041  [   16/   89]
per-ex loss: 0.189193  [   17/   89]
per-ex loss: 0.232190  [   18/   89]
per-ex loss: 0.241123  [   19/   89]
per-ex loss: 0.652047  [   20/   89]
per-ex loss: 0.284032  [   21/   89]
per-ex loss: 0.410702  [   22/   89]
per-ex loss: 0.321160  [   23/   89]
per-ex loss: 0.429950  [   24/   89]
per-ex loss: 0.488716  [   25/   89]
per-ex loss: 0.466710  [   26/   89]
per-ex loss: 0.298846  [   27/   89]
per-ex loss: 0.475436  [   28/   89]
per-ex loss: 0.283688  [   29/   89]
per-ex loss: 0.298160  [   30/   89]
per-ex loss: 0.212388  [   31/   89]
per-ex loss: 0.558195  [   32/   89]
per-ex loss: 0.548770  [   33/   89]
per-ex loss: 0.407672  [   34/   89]
per-ex loss: 0.309784  [   35/   89]
per-ex loss: 0.199676  [   36/   89]
per-ex loss: 0.355073  [   37/   89]
per-ex loss: 0.388927  [   38/   89]
per-ex loss: 0.382241  [   39/   89]
per-ex loss: 0.362586  [   40/   89]
per-ex loss: 0.290898  [   41/   89]
per-ex loss: 0.315202  [   42/   89]
per-ex loss: 0.208970  [   43/   89]
per-ex loss: 0.384168  [   44/   89]
per-ex loss: 0.242991  [   45/   89]
per-ex loss: 0.436118  [   46/   89]
per-ex loss: 0.999958  [   47/   89]
per-ex loss: 0.523650  [   48/   89]
per-ex loss: 0.238493  [   49/   89]
per-ex loss: 0.185714  [   50/   89]
per-ex loss: 0.517508  [   51/   89]
per-ex loss: 0.265140  [   52/   89]
per-ex loss: 0.443023  [   53/   89]
per-ex loss: 0.276459  [   54/   89]
per-ex loss: 0.315074  [   55/   89]
per-ex loss: 0.239958  [   56/   89]
per-ex loss: 0.280994  [   57/   89]
per-ex loss: 0.266897  [   58/   89]
per-ex loss: 0.359248  [   59/   89]
per-ex loss: 0.275175  [   60/   89]
per-ex loss: 0.293417  [   61/   89]
per-ex loss: 0.330027  [   62/   89]
per-ex loss: 0.315245  [   63/   89]
per-ex loss: 0.264200  [   64/   89]
per-ex loss: 0.240977  [   65/   89]
per-ex loss: 0.456945  [   66/   89]
per-ex loss: 0.355063  [   67/   89]
per-ex loss: 0.519031  [   68/   89]
per-ex loss: 0.228298  [   69/   89]
per-ex loss: 0.301888  [   70/   89]
per-ex loss: 0.201729  [   71/   89]
per-ex loss: 0.288151  [   72/   89]
per-ex loss: 0.463946  [   73/   89]
per-ex loss: 0.232426  [   74/   89]
per-ex loss: 0.383373  [   75/   89]
per-ex loss: 0.282380  [   76/   89]
per-ex loss: 0.276943  [   77/   89]
per-ex loss: 0.415504  [   78/   89]
per-ex loss: 0.396793  [   79/   89]
per-ex loss: 0.265201  [   80/   89]
per-ex loss: 0.234773  [   81/   89]
per-ex loss: 0.307027  [   82/   89]
per-ex loss: 0.416131  [   83/   89]
per-ex loss: 0.221597  [   84/   89]
per-ex loss: 0.330378  [   85/   89]
per-ex loss: 0.263817  [   86/   89]
per-ex loss: 0.270638  [   87/   89]
per-ex loss: 0.263036  [   88/   89]
per-ex loss: 0.281370  [   89/   89]
Train Error: Avg loss: 0.34035689
validation Error: 
 Avg loss: 0.53576089 
 F1: 0.491520 
 Precision: 0.525682 
 Recall: 0.461528
 IoU: 0.325838

test Error: 
 Avg loss: 0.49146873 
 F1: 0.554074 
 Precision: 0.575163 
 Recall: 0.534477
 IoU: 0.383197

We have finished training iteration 222
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_220_.pth
per-ex loss: 0.302043  [    1/   89]
per-ex loss: 0.192339  [    2/   89]
per-ex loss: 0.362445  [    3/   89]
per-ex loss: 0.354209  [    4/   89]
per-ex loss: 0.284507  [    5/   89]
per-ex loss: 0.269160  [    6/   89]
per-ex loss: 0.395217  [    7/   89]
per-ex loss: 0.361364  [    8/   89]
per-ex loss: 0.999879  [    9/   89]
per-ex loss: 0.370102  [   10/   89]
per-ex loss: 0.247594  [   11/   89]
per-ex loss: 0.268237  [   12/   89]
per-ex loss: 0.250839  [   13/   89]
per-ex loss: 0.295575  [   14/   89]
per-ex loss: 0.287782  [   15/   89]
per-ex loss: 0.142112  [   16/   89]
per-ex loss: 0.251462  [   17/   89]
per-ex loss: 0.313128  [   18/   89]
per-ex loss: 0.451331  [   19/   89]
per-ex loss: 0.321726  [   20/   89]
per-ex loss: 0.309575  [   21/   89]
per-ex loss: 0.203792  [   22/   89]
per-ex loss: 0.249077  [   23/   89]
per-ex loss: 0.252132  [   24/   89]
per-ex loss: 0.156044  [   25/   89]
per-ex loss: 0.304270  [   26/   89]
per-ex loss: 0.342502  [   27/   89]
per-ex loss: 0.187099  [   28/   89]
per-ex loss: 0.275451  [   29/   89]
per-ex loss: 0.512020  [   30/   89]
per-ex loss: 0.248012  [   31/   89]
per-ex loss: 0.441679  [   32/   89]
per-ex loss: 0.275355  [   33/   89]
per-ex loss: 0.294730  [   34/   89]
per-ex loss: 0.263390  [   35/   89]
per-ex loss: 0.251095  [   36/   89]
per-ex loss: 0.374939  [   37/   89]
per-ex loss: 0.248466  [   38/   89]
per-ex loss: 0.286747  [   39/   89]
per-ex loss: 0.476138  [   40/   89]
per-ex loss: 0.194993  [   41/   89]
per-ex loss: 0.254172  [   42/   89]
per-ex loss: 0.134009  [   43/   89]
per-ex loss: 0.157397  [   44/   89]
per-ex loss: 0.198647  [   45/   89]
per-ex loss: 0.434589  [   46/   89]
per-ex loss: 0.573256  [   47/   89]
per-ex loss: 0.265911  [   48/   89]
per-ex loss: 0.279011  [   49/   89]
per-ex loss: 0.330677  [   50/   89]
per-ex loss: 0.459596  [   51/   89]
per-ex loss: 0.354236  [   52/   89]
per-ex loss: 0.199358  [   53/   89]
per-ex loss: 0.410236  [   54/   89]
per-ex loss: 0.317617  [   55/   89]
per-ex loss: 0.465583  [   56/   89]
per-ex loss: 0.444220  [   57/   89]
per-ex loss: 0.596951  [   58/   89]
per-ex loss: 0.506347  [   59/   89]
per-ex loss: 0.245385  [   60/   89]
per-ex loss: 0.284546  [   61/   89]
per-ex loss: 0.434077  [   62/   89]
per-ex loss: 0.226086  [   63/   89]
per-ex loss: 0.481930  [   64/   89]
per-ex loss: 0.242181  [   65/   89]
per-ex loss: 0.254961  [   66/   89]
per-ex loss: 0.248603  [   67/   89]
per-ex loss: 0.438262  [   68/   89]
per-ex loss: 0.285707  [   69/   89]
per-ex loss: 0.266669  [   70/   89]
per-ex loss: 0.283615  [   71/   89]
per-ex loss: 0.301394  [   72/   89]
per-ex loss: 0.331660  [   73/   89]
per-ex loss: 0.471340  [   74/   89]
per-ex loss: 0.371634  [   75/   89]
per-ex loss: 0.196952  [   76/   89]
per-ex loss: 0.536470  [   77/   89]
per-ex loss: 0.533653  [   78/   89]
per-ex loss: 0.218085  [   79/   89]
per-ex loss: 0.378527  [   80/   89]
per-ex loss: 0.300435  [   81/   89]
per-ex loss: 0.399203  [   82/   89]
per-ex loss: 0.333451  [   83/   89]
per-ex loss: 0.122021  [   84/   89]
per-ex loss: 0.360927  [   85/   89]
per-ex loss: 0.220608  [   86/   89]
per-ex loss: 0.431891  [   87/   89]
per-ex loss: 0.281086  [   88/   89]
per-ex loss: 0.349924  [   89/   89]
Train Error: Avg loss: 0.32676013
validation Error: 
 Avg loss: 0.53835466 
 F1: 0.490324 
 Precision: 0.491346 
 Recall: 0.489306
 IoU: 0.324787

test Error: 
 Avg loss: 0.48850594 
 F1: 0.552390 
 Precision: 0.535896 
 Recall: 0.569931
 IoU: 0.381588

We have finished training iteration 223
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_221_.pth
per-ex loss: 0.298923  [    1/   89]
per-ex loss: 0.412801  [    2/   89]
per-ex loss: 0.407926  [    3/   89]
per-ex loss: 0.261023  [    4/   89]
per-ex loss: 0.217286  [    5/   89]
per-ex loss: 0.215985  [    6/   89]
per-ex loss: 0.235075  [    7/   89]
per-ex loss: 0.369072  [    8/   89]
per-ex loss: 0.137647  [    9/   89]
per-ex loss: 0.395890  [   10/   89]
per-ex loss: 0.278695  [   11/   89]
per-ex loss: 0.357162  [   12/   89]
per-ex loss: 0.328976  [   13/   89]
per-ex loss: 0.303392  [   14/   89]
per-ex loss: 0.213259  [   15/   89]
per-ex loss: 0.485619  [   16/   89]
per-ex loss: 0.290347  [   17/   89]
per-ex loss: 0.410307  [   18/   89]
per-ex loss: 0.185900  [   19/   89]
per-ex loss: 0.377762  [   20/   89]
per-ex loss: 0.519345  [   21/   89]
per-ex loss: 0.316919  [   22/   89]
per-ex loss: 0.334048  [   23/   89]
per-ex loss: 0.393619  [   24/   89]
per-ex loss: 0.301510  [   25/   89]
per-ex loss: 0.243634  [   26/   89]
per-ex loss: 0.442069  [   27/   89]
per-ex loss: 0.240778  [   28/   89]
per-ex loss: 0.387314  [   29/   89]
per-ex loss: 0.324582  [   30/   89]
per-ex loss: 0.428543  [   31/   89]
per-ex loss: 0.559249  [   32/   89]
per-ex loss: 0.456306  [   33/   89]
per-ex loss: 0.275743  [   34/   89]
per-ex loss: 0.269406  [   35/   89]
per-ex loss: 0.411061  [   36/   89]
per-ex loss: 0.224015  [   37/   89]
per-ex loss: 0.218834  [   38/   89]
per-ex loss: 0.999936  [   39/   89]
per-ex loss: 0.440281  [   40/   89]
per-ex loss: 0.269831  [   41/   89]
per-ex loss: 0.494624  [   42/   89]
per-ex loss: 0.529963  [   43/   89]
per-ex loss: 0.302102  [   44/   89]
per-ex loss: 0.252120  [   45/   89]
per-ex loss: 0.230399  [   46/   89]
per-ex loss: 0.409850  [   47/   89]
per-ex loss: 0.214502  [   48/   89]
per-ex loss: 0.456651  [   49/   89]
per-ex loss: 0.225474  [   50/   89]
per-ex loss: 0.381248  [   51/   89]
per-ex loss: 0.323943  [   52/   89]
per-ex loss: 0.429571  [   53/   89]
per-ex loss: 0.425064  [   54/   89]
per-ex loss: 0.494758  [   55/   89]
per-ex loss: 0.553541  [   56/   89]
per-ex loss: 0.229826  [   57/   89]
per-ex loss: 0.260477  [   58/   89]
per-ex loss: 0.304427  [   59/   89]
per-ex loss: 0.297046  [   60/   89]
per-ex loss: 0.223368  [   61/   89]
per-ex loss: 0.322154  [   62/   89]
per-ex loss: 0.558808  [   63/   89]
per-ex loss: 0.241350  [   64/   89]
per-ex loss: 0.529563  [   65/   89]
per-ex loss: 0.398535  [   66/   89]
per-ex loss: 0.269298  [   67/   89]
per-ex loss: 0.485987  [   68/   89]
per-ex loss: 0.284399  [   69/   89]
per-ex loss: 0.329090  [   70/   89]
per-ex loss: 0.406391  [   71/   89]
per-ex loss: 0.141701  [   72/   89]
per-ex loss: 0.333507  [   73/   89]
per-ex loss: 0.298505  [   74/   89]
per-ex loss: 0.153931  [   75/   89]
per-ex loss: 0.289954  [   76/   89]
per-ex loss: 0.270280  [   77/   89]
per-ex loss: 0.215006  [   78/   89]
per-ex loss: 0.285536  [   79/   89]
per-ex loss: 0.259481  [   80/   89]
per-ex loss: 0.223766  [   81/   89]
per-ex loss: 0.291036  [   82/   89]
per-ex loss: 0.289595  [   83/   89]
per-ex loss: 0.237845  [   84/   89]
per-ex loss: 0.466464  [   85/   89]
per-ex loss: 0.320139  [   86/   89]
per-ex loss: 0.434896  [   87/   89]
per-ex loss: 0.362751  [   88/   89]
per-ex loss: 0.584946  [   89/   89]
Train Error: Avg loss: 0.34341498
validation Error: 
 Avg loss: 0.54600858 
 F1: 0.482033 
 Precision: 0.519347 
 Recall: 0.449721
 IoU: 0.317551

test Error: 
 Avg loss: 0.49787290 
 F1: 0.549141 
 Precision: 0.573189 
 Recall: 0.527029
 IoU: 0.378494

We have finished training iteration 224
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_222_.pth
per-ex loss: 0.310105  [    1/   89]
per-ex loss: 0.276540  [    2/   89]
per-ex loss: 0.544057  [    3/   89]
per-ex loss: 0.292040  [    4/   89]
per-ex loss: 0.256931  [    5/   89]
per-ex loss: 0.289837  [    6/   89]
per-ex loss: 0.425356  [    7/   89]
per-ex loss: 0.199891  [    8/   89]
per-ex loss: 0.229694  [    9/   89]
per-ex loss: 0.318763  [   10/   89]
per-ex loss: 0.355971  [   11/   89]
per-ex loss: 0.243554  [   12/   89]
per-ex loss: 0.264907  [   13/   89]
per-ex loss: 0.249758  [   14/   89]
per-ex loss: 0.202277  [   15/   89]
per-ex loss: 0.481059  [   16/   89]
per-ex loss: 0.312776  [   17/   89]
per-ex loss: 0.232115  [   18/   89]
per-ex loss: 0.351707  [   19/   89]
per-ex loss: 0.312214  [   20/   89]
per-ex loss: 0.219545  [   21/   89]
per-ex loss: 0.523249  [   22/   89]
per-ex loss: 0.205165  [   23/   89]
per-ex loss: 0.283645  [   24/   89]
per-ex loss: 0.303914  [   25/   89]
per-ex loss: 0.241938  [   26/   89]
per-ex loss: 0.362986  [   27/   89]
per-ex loss: 0.227890  [   28/   89]
per-ex loss: 0.514539  [   29/   89]
per-ex loss: 0.269901  [   30/   89]
per-ex loss: 0.477482  [   31/   89]
per-ex loss: 0.448130  [   32/   89]
per-ex loss: 0.304136  [   33/   89]
per-ex loss: 0.374959  [   34/   89]
per-ex loss: 0.500116  [   35/   89]
per-ex loss: 0.319774  [   36/   89]
per-ex loss: 0.317151  [   37/   89]
per-ex loss: 0.346997  [   38/   89]
per-ex loss: 0.261854  [   39/   89]
per-ex loss: 0.294517  [   40/   89]
per-ex loss: 0.303693  [   41/   89]
per-ex loss: 0.516178  [   42/   89]
per-ex loss: 0.284105  [   43/   89]
per-ex loss: 0.406499  [   44/   89]
per-ex loss: 0.427964  [   45/   89]
per-ex loss: 0.362197  [   46/   89]
per-ex loss: 0.415489  [   47/   89]
per-ex loss: 0.352680  [   48/   89]
per-ex loss: 0.216575  [   49/   89]
per-ex loss: 0.439515  [   50/   89]
per-ex loss: 0.337229  [   51/   89]
per-ex loss: 0.384907  [   52/   89]
per-ex loss: 0.469717  [   53/   89]
per-ex loss: 0.401559  [   54/   89]
per-ex loss: 0.536742  [   55/   89]
per-ex loss: 0.176845  [   56/   89]
per-ex loss: 0.285101  [   57/   89]
per-ex loss: 0.373202  [   58/   89]
per-ex loss: 0.298935  [   59/   89]
per-ex loss: 0.360603  [   60/   89]
per-ex loss: 0.260214  [   61/   89]
per-ex loss: 0.209222  [   62/   89]
per-ex loss: 0.307832  [   63/   89]
per-ex loss: 0.351222  [   64/   89]
per-ex loss: 0.221664  [   65/   89]
per-ex loss: 0.269518  [   66/   89]
per-ex loss: 0.145639  [   67/   89]
per-ex loss: 0.414347  [   68/   89]
per-ex loss: 0.244347  [   69/   89]
per-ex loss: 0.447539  [   70/   89]
per-ex loss: 0.153611  [   71/   89]
per-ex loss: 0.238320  [   72/   89]
per-ex loss: 0.322995  [   73/   89]
per-ex loss: 0.287007  [   74/   89]
per-ex loss: 0.517306  [   75/   89]
per-ex loss: 0.338220  [   76/   89]
per-ex loss: 0.999879  [   77/   89]
per-ex loss: 0.431779  [   78/   89]
per-ex loss: 0.271006  [   79/   89]
per-ex loss: 0.484615  [   80/   89]
per-ex loss: 0.213602  [   81/   89]
per-ex loss: 0.505294  [   82/   89]
per-ex loss: 0.260432  [   83/   89]
per-ex loss: 0.259383  [   84/   89]
per-ex loss: 0.370454  [   85/   89]
per-ex loss: 0.315925  [   86/   89]
per-ex loss: 0.479951  [   87/   89]
per-ex loss: 0.214924  [   88/   89]
per-ex loss: 0.466044  [   89/   89]
Train Error: Avg loss: 0.34044343
validation Error: 
 Avg loss: 0.55561281 
 F1: 0.473481 
 Precision: 0.597381 
 Recall: 0.392148
 IoU: 0.310171

test Error: 
 Avg loss: 0.51770974 
 F1: 0.531516 
 Precision: 0.597071 
 Recall: 0.478932
 IoU: 0.361949

We have finished training iteration 225
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_223_.pth
per-ex loss: 0.394723  [    1/   89]
per-ex loss: 0.357039  [    2/   89]
per-ex loss: 0.185430  [    3/   89]
per-ex loss: 0.239579  [    4/   89]
per-ex loss: 0.416947  [    5/   89]
per-ex loss: 0.149318  [    6/   89]
per-ex loss: 0.550758  [    7/   89]
per-ex loss: 0.392998  [    8/   89]
per-ex loss: 0.548339  [    9/   89]
per-ex loss: 0.473336  [   10/   89]
per-ex loss: 0.406255  [   11/   89]
per-ex loss: 0.442464  [   12/   89]
per-ex loss: 0.247586  [   13/   89]
per-ex loss: 0.438571  [   14/   89]
per-ex loss: 0.431690  [   15/   89]
per-ex loss: 0.491081  [   16/   89]
per-ex loss: 0.426870  [   17/   89]
per-ex loss: 0.329214  [   18/   89]
per-ex loss: 0.232693  [   19/   89]
per-ex loss: 0.361483  [   20/   89]
per-ex loss: 0.272668  [   21/   89]
per-ex loss: 0.546668  [   22/   89]
per-ex loss: 0.300505  [   23/   89]
per-ex loss: 0.280052  [   24/   89]
per-ex loss: 0.262651  [   25/   89]
per-ex loss: 0.514849  [   26/   89]
per-ex loss: 0.255604  [   27/   89]
per-ex loss: 0.369746  [   28/   89]
per-ex loss: 0.281156  [   29/   89]
per-ex loss: 0.152950  [   30/   89]
per-ex loss: 0.264454  [   31/   89]
per-ex loss: 0.197211  [   32/   89]
per-ex loss: 0.321060  [   33/   89]
per-ex loss: 0.430697  [   34/   89]
per-ex loss: 0.401134  [   35/   89]
per-ex loss: 0.329583  [   36/   89]
per-ex loss: 0.370364  [   37/   89]
per-ex loss: 0.232963  [   38/   89]
per-ex loss: 0.276425  [   39/   89]
per-ex loss: 0.354786  [   40/   89]
per-ex loss: 0.122226  [   41/   89]
per-ex loss: 0.257743  [   42/   89]
per-ex loss: 0.351047  [   43/   89]
per-ex loss: 0.276520  [   44/   89]
per-ex loss: 0.308740  [   45/   89]
per-ex loss: 0.376035  [   46/   89]
per-ex loss: 0.282757  [   47/   89]
per-ex loss: 0.348338  [   48/   89]
per-ex loss: 0.360735  [   49/   89]
per-ex loss: 0.562798  [   50/   89]
per-ex loss: 0.443305  [   51/   89]
per-ex loss: 0.293857  [   52/   89]
per-ex loss: 0.301962  [   53/   89]
per-ex loss: 0.262992  [   54/   89]
per-ex loss: 0.316024  [   55/   89]
per-ex loss: 0.291678  [   56/   89]
per-ex loss: 0.999934  [   57/   89]
per-ex loss: 0.419549  [   58/   89]
per-ex loss: 0.204793  [   59/   89]
per-ex loss: 0.241549  [   60/   89]
per-ex loss: 0.149775  [   61/   89]
per-ex loss: 0.169692  [   62/   89]
per-ex loss: 0.244876  [   63/   89]
per-ex loss: 0.445527  [   64/   89]
per-ex loss: 0.219263  [   65/   89]
per-ex loss: 0.313936  [   66/   89]
per-ex loss: 0.265693  [   67/   89]
per-ex loss: 0.211390  [   68/   89]
per-ex loss: 0.305114  [   69/   89]
per-ex loss: 0.276959  [   70/   89]
per-ex loss: 0.291899  [   71/   89]
per-ex loss: 0.413464  [   72/   89]
per-ex loss: 0.332749  [   73/   89]
per-ex loss: 0.308152  [   74/   89]
per-ex loss: 0.235548  [   75/   89]
per-ex loss: 0.521765  [   76/   89]
per-ex loss: 0.314075  [   77/   89]
per-ex loss: 0.315230  [   78/   89]
per-ex loss: 0.299092  [   79/   89]
per-ex loss: 0.217250  [   80/   89]
per-ex loss: 0.407587  [   81/   89]
per-ex loss: 0.192999  [   82/   89]
per-ex loss: 0.218712  [   83/   89]
per-ex loss: 0.300346  [   84/   89]
per-ex loss: 0.348008  [   85/   89]
per-ex loss: 0.428739  [   86/   89]
per-ex loss: 0.374229  [   87/   89]
per-ex loss: 0.436796  [   88/   89]
per-ex loss: 0.209234  [   89/   89]
Train Error: Avg loss: 0.33396160
validation Error: 
 Avg loss: 0.53945103 
 F1: 0.486610 
 Precision: 0.504021 
 Recall: 0.470362
 IoU: 0.321537

test Error: 
 Avg loss: 0.48815452 
 F1: 0.553646 
 Precision: 0.557199 
 Recall: 0.550138
 IoU: 0.382787

We have finished training iteration 226
Deleting model ./unet_bwzo315_train/saved_model_wrapper/models/UNet_224_.pth
per-ex loss: 0.255152  [    1/   89]
per-ex loss: 0.308726  [    2/   89]
per-ex loss: 0.343361  [    3/   89]
per-ex loss: 0.231718  [    4/   89]
per-ex loss: 0.264261  [    5/   89]
per-ex loss: 0.300289  [    6/   89]
per-ex loss: 0.438208  [    7/   89]
per-ex loss: 0.398549  [    8/   89]
per-ex loss: 0.225569  [    9/   89]
per-ex loss: 0.336827  [   10/   89]
per-ex loss: 0.636335  [   11/   89]
per-ex loss: 0.287792  [   12/   89]
per-ex loss: 0.391312  [   13/   89]
per-ex loss: 0.478305  [   14/   89]
per-ex loss: 0.321479  [   15/   89]
per-ex loss: 0.162342  [   16/   89]
per-ex loss: 0.607302  [   17/   89]
per-ex loss: 0.337854  [   18/   89]
per-ex loss: 0.407161  [   19/   89]
per-ex loss: 0.413545  [   20/   89]
per-ex loss: 0.533449  [   21/   89]
per-ex loss: 0.201289  [   22/   89]
per-ex loss: 0.200669  [   23/   89]
per-ex loss: 0.242077  [   24/   89]
per-ex loss: 0.999906  [   25/   89]
per-ex loss: 0.487823  [   26/   89]
per-ex loss: 0.269183  [   27/   89]
per-ex loss: 0.272374  [   28/   89]
per-ex loss: 0.475279  [   29/   89]
per-ex loss: 0.335124  [   30/   89]
per-ex loss: 0.549732  [   31/   89]
per-ex loss: 0.274079  [   32/   89]
per-ex loss: 0.237349  [   33/   89]
per-ex loss: 0.234727  [   34/   89]
per-ex loss: 0.293433  [   35/   89]
per-ex loss: 0.403518  [   36/   89]
per-ex loss: 0.268605  [   37/   89]
per-ex loss: 0.392825  [   38/   89]
per-ex loss: 0.243760  [   39/   89]
per-ex loss: 0.259768  [   40/   89]
per-ex loss: 0.176580  [   41/   89]
per-ex loss: 0.310475  [   42/   89]
per-ex loss: 0.234493  [   43/   89]
per-ex loss: 0.253185  [   44/   89]
per-ex loss: 0.361193  [   45/   89]
per-ex loss: 0.534436  [   46/   89]
per-ex loss: 0.277365  [   47/   89]
per-ex loss: 0.308849  [   48/   89]
per-ex loss: 0.267052  [   49/   89]
per-ex loss: 0.327459  [   50/   89]
per-ex loss: 0.409435  [   51/   89]
per-ex loss: 0.217618  [   52/   89]
per-ex loss: 0.370988  [   53/   89]
per-ex loss: 0.311593  [   54/   89]
per-ex loss: 0.458728  [   55/   89]
per-ex loss: 0.467338  [   56/   89]
per-ex loss: 0.302425  [   57/   89]
per-ex loss: 0.260908  [   58/   89]
per-ex loss: 0.374700  [   59/   89]
per-ex loss: 0.251159  [   60/   89]
per-ex loss: 0.272234  [   61/   89]
per-ex loss: 0.261343  [   62/   89]
per-ex loss: 0.226730  [   63/   89]
per-ex loss: 0.322564  [   64/   89]
per-ex loss: 0.557283  [   65/   89]
per-ex loss: 0.339398  [   66/   89]
per-ex loss: 0.271993  [   67/   89]
per-ex loss: 0.134856  [   68/   89]
per-ex loss: 0.220784  [   69/   89]
per-ex loss: 0.151877  [   70/   89]
per-ex loss: 0.368355  [   71/   89]
per-ex loss: 0.299878  [   72/   89]
per-ex loss: 0.310120  [   73/   89]
per-ex loss: 0.396223  [   74/   89]
per-ex loss: 0.548534  [   75/   89]
per-ex loss: 0.275451  [   76/   89]
per-ex loss: 0.409370  [   77/   89]
per-ex loss: 0.280680  [   78/   89]
per-ex loss: 0.246876  [   79/   89]
per-ex loss: 0.251834  [   80/   89]
per-ex loss: 0.546341  [   81/   89]
per-ex loss: 0.287484  [   82/   89]
per-ex loss: 0.474552  [   83/   89]
per-ex loss: 0.264560  [   84/   89]
per-ex loss: 0.552203  [   85/   89]
per-ex loss: 0.223885  [   86/   89]
per-ex loss: 0.585370  [   87/   89]
per-ex loss: 0.401793  [   88/   89]
per-ex loss: 0.332583  [   89/   89]
Train Error: Avg loss: 0.34395713
validation Error: 
 Avg loss: 0.55350848 
 F1: 0.477470 
 Precision: 0.451122 
 Recall: 0.507087
 IoU: 0.313603

slurmstepd: error: *** STEP 16600.0 ON aga1 CANCELLED AT 2025-01-08T17:40:12 ***
