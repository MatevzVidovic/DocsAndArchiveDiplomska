/shared/home/matevz.vidovic/Diplomska/Prototip/Delo/model_wrapper.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model = torch.load(self.prev_model_path, map_location=torch.device(device))
unet_original_main.py do_log: True
Log file name: log_07_14-49-45_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(ptd='./Data/vein_and_sclera_data', sd='unet_bas_train', mti=250, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_original_bas.yaml', ntibp=None, ptp=None, map=None)
YAML: {'batch_size': 2, 'learning_rate': 1e-05, 'num_of_dataloader_workers': 7, 'train_epoch_size_limit': 400, 'num_epochs_per_training_iteration': 1, 'cleanup_k': 3, 'dataset_option': 'augment', 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': False, 'loss_fn_name': 'MCDL', 'alphas': [], 'model': '64_2_6', 'input_width': 2048, 'input_height': 1024, 'input_channels': 3, 'output_channels': 2, 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq'}
Validation phase: False
Namespace(ptd='./Data/vein_and_sclera_data', sd='unet_bas_train', mti=250, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_original_bas.yaml', ntibp=None, ptp=None, map=None)
Device: cuda
dataset_aug.py do_log: True
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 89
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 89
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 45
val dataloader num of batches: 14
test dataloader num of batches: 6
Loaded model path:  ./unet_bas_train/saved_model_wrapper/models/UNet_200_.pth
Per-example loss in batch: 0.256336  [    2/   89]
Per-example loss in batch: 0.248237  [    4/   89]
Per-example loss in batch: 0.270134  [    6/   89]
Per-example loss in batch: 0.236541  [    8/   89]
Per-example loss in batch: 0.262297  [   10/   89]
Per-example loss in batch: 0.207261  [   12/   89]
Per-example loss in batch: 0.222202  [   14/   89]
Per-example loss in batch: 0.301486  [   16/   89]
Per-example loss in batch: 0.267902  [   18/   89]
Per-example loss in batch: 0.241706  [   20/   89]
Per-example loss in batch: 0.272940  [   22/   89]
Per-example loss in batch: 0.309290  [   24/   89]
Per-example loss in batch: 0.298671  [   26/   89]
Per-example loss in batch: 0.188151  [   28/   89]
Per-example loss in batch: 0.239082  [   30/   89]
Per-example loss in batch: 0.253873  [   32/   89]
Per-example loss in batch: 0.260910  [   34/   89]
Per-example loss in batch: 0.242322  [   36/   89]
Per-example loss in batch: 0.269951  [   38/   89]
Per-example loss in batch: 0.253553  [   40/   89]
Per-example loss in batch: 0.296654  [   42/   89]
Per-example loss in batch: 0.245665  [   44/   89]
Per-example loss in batch: 0.316178  [   46/   89]
Per-example loss in batch: 0.207417  [   48/   89]
Per-example loss in batch: 0.210251  [   50/   89]
Per-example loss in batch: 0.195801  [   52/   89]
Per-example loss in batch: 0.217897  [   54/   89]
Per-example loss in batch: 0.275749  [   56/   89]
Per-example loss in batch: 0.213816  [   58/   89]
Per-example loss in batch: 0.320253  [   60/   89]
Per-example loss in batch: 0.343430  [   62/   89]
Per-example loss in batch: 0.246779  [   64/   89]
Per-example loss in batch: 0.338911  [   66/   89]
Per-example loss in batch: 0.208349  [   68/   89]
Per-example loss in batch: 0.248427  [   70/   89]
Per-example loss in batch: 0.215977  [   72/   89]
Per-example loss in batch: 0.245615  [   74/   89]
Per-example loss in batch: 0.287494  [   76/   89]
Per-example loss in batch: 0.266141  [   78/   89]
Per-example loss in batch: 0.215709  [   80/   89]
Per-example loss in batch: 0.258385  [   82/   89]
Per-example loss in batch: 0.274096  [   84/   89]
Per-example loss in batch: 0.220691  [   86/   89]
Per-example loss in batch: 0.189912  [   88/   89]
Per-example loss in batch: 0.396634  [   89/   89]
Train Error: Avg loss: 0.25529794
validation Error: 
 Avg loss: 0.27632874 
 F1: 0.486545 
 Precision: 0.562980 
 Recall: 0.428384
 IoU: 0.321480

test Error: 
 Avg loss: 0.25591548 
 F1: 0.518387 
 Precision: 0.606100 
 Recall: 0.452851
 IoU: 0.349880

We have finished training iteration 201
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_199_.pth
Per-example loss in batch: 0.236916  [    2/   89]
Per-example loss in batch: 0.326696  [    4/   89]
Per-example loss in batch: 0.237930  [    6/   89]
Per-example loss in batch: 0.297429  [    8/   89]
Per-example loss in batch: 0.245298  [   10/   89]
Per-example loss in batch: 0.274613  [   12/   89]
Per-example loss in batch: 0.238651  [   14/   89]
Per-example loss in batch: 0.311778  [   16/   89]
Per-example loss in batch: 0.312587  [   18/   89]
Per-example loss in batch: 0.202886  [   20/   89]
Per-example loss in batch: 0.190031  [   22/   89]
Per-example loss in batch: 0.304873  [   24/   89]
Per-example loss in batch: 0.195668  [   26/   89]
Per-example loss in batch: 0.304314  [   28/   89]
Per-example loss in batch: 0.247716  [   30/   89]
Per-example loss in batch: 0.259176  [   32/   89]
Per-example loss in batch: 0.330558  [   34/   89]
Per-example loss in batch: 0.287966  [   36/   89]
Per-example loss in batch: 0.250321  [   38/   89]
Per-example loss in batch: 0.247255  [   40/   89]
Per-example loss in batch: 0.211525  [   42/   89]
Per-example loss in batch: 0.208030  [   44/   89]
Per-example loss in batch: 0.246065  [   46/   89]
Per-example loss in batch: 0.294236  [   48/   89]
Per-example loss in batch: 0.215198  [   50/   89]
Per-example loss in batch: 0.241673  [   52/   89]
Per-example loss in batch: 0.231208  [   54/   89]
Per-example loss in batch: 0.237020  [   56/   89]
Per-example loss in batch: 0.226582  [   58/   89]
Per-example loss in batch: 0.210706  [   60/   89]
Per-example loss in batch: 0.270321  [   62/   89]
Per-example loss in batch: 0.300157  [   64/   89]
Per-example loss in batch: 0.212923  [   66/   89]
Per-example loss in batch: 0.214550  [   68/   89]
Per-example loss in batch: 0.335513  [   70/   89]
Per-example loss in batch: 0.224988  [   72/   89]
Per-example loss in batch: 0.275081  [   74/   89]
Per-example loss in batch: 0.239955  [   76/   89]
Per-example loss in batch: 0.231989  [   78/   89]
Per-example loss in batch: 0.211548  [   80/   89]
Per-example loss in batch: 0.333448  [   82/   89]
Per-example loss in batch: 0.265240  [   84/   89]
Per-example loss in batch: 0.300749  [   86/   89]
Per-example loss in batch: 0.235253  [   88/   89]
Per-example loss in batch: 0.514863  [   89/   89]
Train Error: Avg loss: 0.25919213
validation Error: 
 Avg loss: 0.27351501 
 F1: 0.488982 
 Precision: 0.561930 
 Recall: 0.432799
 IoU: 0.323611

test Error: 
 Avg loss: 0.25327370 
 F1: 0.523719 
 Precision: 0.609029 
 Recall: 0.459372
 IoU: 0.354755

We have finished training iteration 202
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_200_.pth
Per-example loss in batch: 0.223138  [    2/   89]
Per-example loss in batch: 0.319241  [    4/   89]
Per-example loss in batch: 0.277861  [    6/   89]
Per-example loss in batch: 0.277391  [    8/   89]
Per-example loss in batch: 0.222925  [   10/   89]
Per-example loss in batch: 0.207413  [   12/   89]
Per-example loss in batch: 0.240240  [   14/   89]
Per-example loss in batch: 0.227475  [   16/   89]
Per-example loss in batch: 0.296521  [   18/   89]
Per-example loss in batch: 0.308468  [   20/   89]
Per-example loss in batch: 0.211592  [   22/   89]
Per-example loss in batch: 0.258251  [   24/   89]
Per-example loss in batch: 0.270048  [   26/   89]
Per-example loss in batch: 0.211017  [   28/   89]
Per-example loss in batch: 0.286812  [   30/   89]
Per-example loss in batch: 0.292763  [   32/   89]
Per-example loss in batch: 0.239149  [   34/   89]
Per-example loss in batch: 0.237042  [   36/   89]
Per-example loss in batch: 0.232379  [   38/   89]
Per-example loss in batch: 0.244692  [   40/   89]
Per-example loss in batch: 0.211279  [   42/   89]
Per-example loss in batch: 0.256250  [   44/   89]
Per-example loss in batch: 0.221587  [   46/   89]
Per-example loss in batch: 0.290109  [   48/   89]
Per-example loss in batch: 0.249527  [   50/   89]
Per-example loss in batch: 0.218773  [   52/   89]
Per-example loss in batch: 0.292936  [   54/   89]
Per-example loss in batch: 0.215502  [   56/   89]
Per-example loss in batch: 0.217868  [   58/   89]
Per-example loss in batch: 0.313405  [   60/   89]
Per-example loss in batch: 0.231064  [   62/   89]
Per-example loss in batch: 0.252610  [   64/   89]
Per-example loss in batch: 0.291427  [   66/   89]
Per-example loss in batch: 0.188301  [   68/   89]
Per-example loss in batch: 0.264546  [   70/   89]
Per-example loss in batch: 0.280373  [   72/   89]
Per-example loss in batch: 0.304125  [   74/   89]
Per-example loss in batch: 0.223633  [   76/   89]
Per-example loss in batch: 0.328443  [   78/   89]
Per-example loss in batch: 0.247303  [   80/   89]
Per-example loss in batch: 0.346821  [   82/   89]
Per-example loss in batch: 0.235848  [   84/   89]
Per-example loss in batch: 0.292472  [   86/   89]
Per-example loss in batch: 0.336832  [   88/   89]
Per-example loss in batch: 0.433607  [   89/   89]
Train Error: Avg loss: 0.26094951
validation Error: 
 Avg loss: 0.27600986 
 F1: 0.486611 
 Precision: 0.554029 
 Recall: 0.433820
 IoU: 0.321537

test Error: 
 Avg loss: 0.25424101 
 F1: 0.521432 
 Precision: 0.601995 
 Recall: 0.459886
 IoU: 0.352660

We have finished training iteration 203
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_201_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.255129  [    2/   89]
Per-example loss in batch: 0.285724  [    4/   89]
Per-example loss in batch: 0.212951  [    6/   89]
Per-example loss in batch: 0.250405  [    8/   89]
Per-example loss in batch: 0.235082  [   10/   89]
Per-example loss in batch: 0.205876  [   12/   89]
Per-example loss in batch: 0.222901  [   14/   89]
Per-example loss in batch: 0.263245  [   16/   89]
Per-example loss in batch: 0.211265  [   18/   89]
Per-example loss in batch: 0.232153  [   20/   89]
Per-example loss in batch: 0.204796  [   22/   89]
Per-example loss in batch: 0.213019  [   24/   89]
Per-example loss in batch: 0.183171  [   26/   89]
Per-example loss in batch: 0.344207  [   28/   89]
Per-example loss in batch: 0.254048  [   30/   89]
Per-example loss in batch: 0.303759  [   32/   89]
Per-example loss in batch: 0.222027  [   34/   89]
Per-example loss in batch: 0.303990  [   36/   89]
Per-example loss in batch: 0.242836  [   38/   89]
Per-example loss in batch: 0.259043  [   40/   89]
Per-example loss in batch: 0.251333  [   42/   89]
Per-example loss in batch: 0.224420  [   44/   89]
Per-example loss in batch: 0.215204  [   46/   89]
Per-example loss in batch: 0.194298  [   48/   89]
Per-example loss in batch: 0.232396  [   50/   89]
Per-example loss in batch: 0.234245  [   52/   89]
Per-example loss in batch: 0.227093  [   54/   89]
Per-example loss in batch: 0.289069  [   56/   89]
Per-example loss in batch: 0.246027  [   58/   89]
Per-example loss in batch: 0.350034  [   60/   89]
Per-example loss in batch: 0.275873  [   62/   89]
Per-example loss in batch: 0.275192  [   64/   89]
Per-example loss in batch: 0.329031  [   66/   89]
Per-example loss in batch: 0.363199  [   68/   89]
Per-example loss in batch: 0.268653  [   70/   89]
Per-example loss in batch: 0.294786  [   72/   89]
Per-example loss in batch: 0.329706  [   74/   89]
Per-example loss in batch: 0.218890  [   76/   89]
Per-example loss in batch: 0.230628  [   78/   89]
Per-example loss in batch: 0.198182  [   80/   89]
Per-example loss in batch: 0.318960  [   82/   89]
Per-example loss in batch: 0.235612  [   84/   89]
Per-example loss in batch: 0.275226  [   86/   89]
Per-example loss in batch: 0.221196  [   88/   89]
Per-example loss in batch: 0.567445  [   89/   89]
Train Error: Avg loss: 0.25817083
validation Error: 
 Avg loss: 0.27139377 
 F1: 0.487537 
 Precision: 0.555210 
 Recall: 0.434569
 IoU: 0.322346

test Error: 
 Avg loss: 0.25411422 
 F1: 0.522136 
 Precision: 0.600434 
 Recall: 0.461903
 IoU: 0.353305

We have finished training iteration 204
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_176_.pth
Per-example loss in batch: 0.302182  [    2/   89]
Per-example loss in batch: 0.309361  [    4/   89]
Per-example loss in batch: 0.224666  [    6/   89]
Per-example loss in batch: 0.371162  [    8/   89]
Per-example loss in batch: 0.216286  [   10/   89]
Per-example loss in batch: 0.209167  [   12/   89]
Per-example loss in batch: 0.212454  [   14/   89]
Per-example loss in batch: 0.302353  [   16/   89]
Per-example loss in batch: 0.293330  [   18/   89]
Per-example loss in batch: 0.268189  [   20/   89]
Per-example loss in batch: 0.289530  [   22/   89]
Per-example loss in batch: 0.238730  [   24/   89]
Per-example loss in batch: 0.220001  [   26/   89]
Per-example loss in batch: 0.238174  [   28/   89]
Per-example loss in batch: 0.179442  [   30/   89]
Per-example loss in batch: 0.287620  [   32/   89]
Per-example loss in batch: 0.291799  [   34/   89]
Per-example loss in batch: 0.262894  [   36/   89]
Per-example loss in batch: 0.262596  [   38/   89]
Per-example loss in batch: 0.199468  [   40/   89]
Per-example loss in batch: 0.244355  [   42/   89]
Per-example loss in batch: 0.223236  [   44/   89]
Per-example loss in batch: 0.236352  [   46/   89]
Per-example loss in batch: 0.189147  [   48/   89]
Per-example loss in batch: 0.231842  [   50/   89]
Per-example loss in batch: 0.219137  [   52/   89]
Per-example loss in batch: 0.216253  [   54/   89]
Per-example loss in batch: 0.316513  [   56/   89]
Per-example loss in batch: 0.260649  [   58/   89]
Per-example loss in batch: 0.245682  [   60/   89]
Per-example loss in batch: 0.228233  [   62/   89]
Per-example loss in batch: 0.219075  [   64/   89]
Per-example loss in batch: 0.311977  [   66/   89]
Per-example loss in batch: 0.230388  [   68/   89]
Per-example loss in batch: 0.213610  [   70/   89]
Per-example loss in batch: 0.259043  [   72/   89]
Per-example loss in batch: 0.317151  [   74/   89]
Per-example loss in batch: 0.255484  [   76/   89]
Per-example loss in batch: 0.247755  [   78/   89]
Per-example loss in batch: 0.294569  [   80/   89]
Per-example loss in batch: 0.235266  [   82/   89]
Per-example loss in batch: 0.258070  [   84/   89]
Per-example loss in batch: 0.266574  [   86/   89]
Per-example loss in batch: 0.306701  [   88/   89]
Per-example loss in batch: 0.486069  [   89/   89]
Train Error: Avg loss: 0.25729216
validation Error: 
 Avg loss: 0.27753044 
 F1: 0.484386 
 Precision: 0.582896 
 Recall: 0.414359
 IoU: 0.319597

test Error: 
 Avg loss: 0.25781156 
 F1: 0.513543 
 Precision: 0.620751 
 Recall: 0.437913
 IoU: 0.345481

We have finished training iteration 205
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_203_.pth
Per-example loss in batch: 0.208280  [    2/   89]
Per-example loss in batch: 0.356447  [    4/   89]
Per-example loss in batch: 0.203374  [    6/   89]
Per-example loss in batch: 0.290771  [    8/   89]
Per-example loss in batch: 0.247220  [   10/   89]
Per-example loss in batch: 0.192686  [   12/   89]
Per-example loss in batch: 0.311787  [   14/   89]
Per-example loss in batch: 0.242638  [   16/   89]
Per-example loss in batch: 0.216068  [   18/   89]
Per-example loss in batch: 0.233673  [   20/   89]
Per-example loss in batch: 0.191770  [   22/   89]
Per-example loss in batch: 0.299161  [   24/   89]
Per-example loss in batch: 0.246009  [   26/   89]
Per-example loss in batch: 0.223597  [   28/   89]
Per-example loss in batch: 0.249997  [   30/   89]
Per-example loss in batch: 0.257415  [   32/   89]
Per-example loss in batch: 0.266313  [   34/   89]
Per-example loss in batch: 0.244916  [   36/   89]
Per-example loss in batch: 0.319354  [   38/   89]
Per-example loss in batch: 0.291235  [   40/   89]
Per-example loss in batch: 0.210509  [   42/   89]
Per-example loss in batch: 0.264035  [   44/   89]
Per-example loss in batch: 0.319085  [   46/   89]
Per-example loss in batch: 0.276663  [   48/   89]
Per-example loss in batch: 0.229555  [   50/   89]
Per-example loss in batch: 0.302763  [   52/   89]
Per-example loss in batch: 0.278651  [   54/   89]
Per-example loss in batch: 0.334610  [   56/   89]
Per-example loss in batch: 0.269381  [   58/   89]
Per-example loss in batch: 0.263501  [   60/   89]
Per-example loss in batch: 0.226237  [   62/   89]
Per-example loss in batch: 0.241895  [   64/   89]
Per-example loss in batch: 0.243604  [   66/   89]
Per-example loss in batch: 0.216322  [   68/   89]
Per-example loss in batch: 0.260230  [   70/   89]
Per-example loss in batch: 0.224369  [   72/   89]
Per-example loss in batch: 0.245856  [   74/   89]
Per-example loss in batch: 0.331007  [   76/   89]
Per-example loss in batch: 0.236780  [   78/   89]
Per-example loss in batch: 0.221227  [   80/   89]
Per-example loss in batch: 0.262936  [   82/   89]
Per-example loss in batch: 0.227456  [   84/   89]
Per-example loss in batch: 0.289776  [   86/   89]
Per-example loss in batch: 0.224813  [   88/   89]
Per-example loss in batch: 0.483768  [   89/   89]
Train Error: Avg loss: 0.25923275
validation Error: 
 Avg loss: 0.27316426 
 F1: 0.490299 
 Precision: 0.573160 
 Recall: 0.428370
 IoU: 0.324765

test Error: 
 Avg loss: 0.25336503 
 F1: 0.523724 
 Precision: 0.619291 
 Recall: 0.453710
 IoU: 0.354761

We have finished training iteration 206
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_173_.pth
Per-example loss in batch: 0.246649  [    2/   89]
Per-example loss in batch: 0.205842  [    4/   89]
Per-example loss in batch: 0.216902  [    6/   89]
Per-example loss in batch: 0.193686  [    8/   89]
Per-example loss in batch: 0.254220  [   10/   89]
Per-example loss in batch: 0.299722  [   12/   89]
Per-example loss in batch: 0.307017  [   14/   89]
Per-example loss in batch: 0.217727  [   16/   89]
Per-example loss in batch: 0.313183  [   18/   89]
Per-example loss in batch: 0.245400  [   20/   89]
Per-example loss in batch: 0.210483  [   22/   89]
Per-example loss in batch: 0.216281  [   24/   89]
Per-example loss in batch: 0.287716  [   26/   89]
Per-example loss in batch: 0.210192  [   28/   89]
Per-example loss in batch: 0.298508  [   30/   89]
Per-example loss in batch: 0.309066  [   32/   89]
Per-example loss in batch: 0.301002  [   34/   89]
Per-example loss in batch: 0.220805  [   36/   89]
Per-example loss in batch: 0.251474  [   38/   89]
Per-example loss in batch: 0.347833  [   40/   89]
Per-example loss in batch: 0.215244  [   42/   89]
Per-example loss in batch: 0.240077  [   44/   89]
Per-example loss in batch: 0.239724  [   46/   89]
Per-example loss in batch: 0.259061  [   48/   89]
Per-example loss in batch: 0.239939  [   50/   89]
Per-example loss in batch: 0.306841  [   52/   89]
Per-example loss in batch: 0.276793  [   54/   89]
Per-example loss in batch: 0.246753  [   56/   89]
Per-example loss in batch: 0.242101  [   58/   89]
Per-example loss in batch: 0.231321  [   60/   89]
Per-example loss in batch: 0.290275  [   62/   89]
Per-example loss in batch: 0.301240  [   64/   89]
Per-example loss in batch: 0.231950  [   66/   89]
Per-example loss in batch: 0.247394  [   68/   89]
Per-example loss in batch: 0.250883  [   70/   89]
Per-example loss in batch: 0.325021  [   72/   89]
Per-example loss in batch: 0.206671  [   74/   89]
Per-example loss in batch: 0.218294  [   76/   89]
Per-example loss in batch: 0.254688  [   78/   89]
Per-example loss in batch: 0.226614  [   80/   89]
Per-example loss in batch: 0.213284  [   82/   89]
Per-example loss in batch: 0.253534  [   84/   89]
Per-example loss in batch: 0.231702  [   86/   89]
Per-example loss in batch: 0.240776  [   88/   89]
Per-example loss in batch: 0.666859  [   89/   89]
Train Error: Avg loss: 0.25791735
validation Error: 
 Avg loss: 0.27957457 
 F1: 0.488234 
 Precision: 0.551351 
 Recall: 0.438084
 IoU: 0.322956

test Error: 
 Avg loss: 0.25304026 
 F1: 0.524567 
 Precision: 0.600578 
 Recall: 0.465635
 IoU: 0.355534

We have finished training iteration 207
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_205_.pth
Per-example loss in batch: 0.239276  [    2/   89]
Per-example loss in batch: 0.215004  [    4/   89]
Per-example loss in batch: 0.258288  [    6/   89]
Per-example loss in batch: 0.272845  [    8/   89]
Per-example loss in batch: 0.272568  [   10/   89]
Per-example loss in batch: 0.219949  [   12/   89]
Per-example loss in batch: 0.188248  [   14/   89]
Per-example loss in batch: 0.351421  [   16/   89]
Per-example loss in batch: 0.246146  [   18/   89]
Per-example loss in batch: 0.207350  [   20/   89]
Per-example loss in batch: 0.286510  [   22/   89]
Per-example loss in batch: 0.231755  [   24/   89]
Per-example loss in batch: 0.321135  [   26/   89]
Per-example loss in batch: 0.228025  [   28/   89]
Per-example loss in batch: 0.248843  [   30/   89]
Per-example loss in batch: 0.223969  [   32/   89]
Per-example loss in batch: 0.196361  [   34/   89]
Per-example loss in batch: 0.233822  [   36/   89]
Per-example loss in batch: 0.226202  [   38/   89]
Per-example loss in batch: 0.316191  [   40/   89]
Per-example loss in batch: 0.232334  [   42/   89]
Per-example loss in batch: 0.280118  [   44/   89]
Per-example loss in batch: 0.328031  [   46/   89]
Per-example loss in batch: 0.180971  [   48/   89]
Per-example loss in batch: 0.276225  [   50/   89]
Per-example loss in batch: 0.209045  [   52/   89]
Per-example loss in batch: 0.257749  [   54/   89]
Per-example loss in batch: 0.198655  [   56/   89]
Per-example loss in batch: 0.207185  [   58/   89]
Per-example loss in batch: 0.200332  [   60/   89]
Per-example loss in batch: 0.271823  [   62/   89]
Per-example loss in batch: 0.361474  [   64/   89]
Per-example loss in batch: 0.296091  [   66/   89]
Per-example loss in batch: 0.284464  [   68/   89]
Per-example loss in batch: 0.228342  [   70/   89]
Per-example loss in batch: 0.217691  [   72/   89]
Per-example loss in batch: 0.229571  [   74/   89]
Per-example loss in batch: 0.205910  [   76/   89]
Per-example loss in batch: 0.303918  [   78/   89]
Per-example loss in batch: 0.272336  [   80/   89]
Per-example loss in batch: 0.211036  [   82/   89]
Per-example loss in batch: 0.244005  [   84/   89]
Per-example loss in batch: 0.233528  [   86/   89]
Per-example loss in batch: 0.287799  [   88/   89]
Per-example loss in batch: 0.600025  [   89/   89]
Train Error: Avg loss: 0.25398990
validation Error: 
 Avg loss: 0.27412848 
 F1: 0.488918 
 Precision: 0.558729 
 Recall: 0.434615
 IoU: 0.323555

test Error: 
 Avg loss: 0.25327460 
 F1: 0.523726 
 Precision: 0.605610 
 Recall: 0.461348
 IoU: 0.354762

We have finished training iteration 208
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_202_.pth
Per-example loss in batch: 0.288128  [    2/   89]
Per-example loss in batch: 0.248216  [    4/   89]
Per-example loss in batch: 0.283273  [    6/   89]
Per-example loss in batch: 0.209278  [    8/   89]
Per-example loss in batch: 0.312179  [   10/   89]
Per-example loss in batch: 0.198855  [   12/   89]
Per-example loss in batch: 0.284589  [   14/   89]
Per-example loss in batch: 0.199140  [   16/   89]
Per-example loss in batch: 0.250592  [   18/   89]
Per-example loss in batch: 0.335707  [   20/   89]
Per-example loss in batch: 0.262381  [   22/   89]
Per-example loss in batch: 0.256925  [   24/   89]
Per-example loss in batch: 0.253159  [   26/   89]
Per-example loss in batch: 0.321448  [   28/   89]
Per-example loss in batch: 0.259346  [   30/   89]
Per-example loss in batch: 0.229811  [   32/   89]
Per-example loss in batch: 0.253770  [   34/   89]
Per-example loss in batch: 0.323876  [   36/   89]
Per-example loss in batch: 0.221264  [   38/   89]
Per-example loss in batch: 0.225615  [   40/   89]
Per-example loss in batch: 0.198522  [   42/   89]
Per-example loss in batch: 0.229893  [   44/   89]
Per-example loss in batch: 0.215892  [   46/   89]
Per-example loss in batch: 0.204329  [   48/   89]
Per-example loss in batch: 0.193244  [   50/   89]
Per-example loss in batch: 0.247599  [   52/   89]
Per-example loss in batch: 0.267151  [   54/   89]
Per-example loss in batch: 0.226982  [   56/   89]
Per-example loss in batch: 0.212508  [   58/   89]
Per-example loss in batch: 0.345411  [   60/   89]
Per-example loss in batch: 0.241024  [   62/   89]
Per-example loss in batch: 0.287845  [   64/   89]
Per-example loss in batch: 0.272621  [   66/   89]
Per-example loss in batch: 0.194602  [   68/   89]
Per-example loss in batch: 0.264095  [   70/   89]
Per-example loss in batch: 0.333792  [   72/   89]
Per-example loss in batch: 0.248434  [   74/   89]
Per-example loss in batch: 0.290201  [   76/   89]
Per-example loss in batch: 0.298604  [   78/   89]
Per-example loss in batch: 0.256057  [   80/   89]
Per-example loss in batch: 0.258977  [   82/   89]
Per-example loss in batch: 0.292070  [   84/   89]
Per-example loss in batch: 0.230064  [   86/   89]
Per-example loss in batch: 0.345210  [   88/   89]
Per-example loss in batch: 0.593216  [   89/   89]
Train Error: Avg loss: 0.26223115
validation Error: 
 Avg loss: 0.27933510 
 F1: 0.486015 
 Precision: 0.552986 
 Recall: 0.433513
 IoU: 0.321017

test Error: 
 Avg loss: 0.25564967 
 F1: 0.518702 
 Precision: 0.596320 
 Recall: 0.458963
 IoU: 0.350168

We have finished training iteration 209
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_207_.pth
Per-example loss in batch: 0.273320  [    2/   89]
Per-example loss in batch: 0.281248  [    4/   89]
Per-example loss in batch: 0.212647  [    6/   89]
Per-example loss in batch: 0.278260  [    8/   89]
Per-example loss in batch: 0.174526  [   10/   89]
Per-example loss in batch: 0.254691  [   12/   89]
Per-example loss in batch: 0.282646  [   14/   89]
Per-example loss in batch: 0.256477  [   16/   89]
Per-example loss in batch: 0.320458  [   18/   89]
Per-example loss in batch: 0.246116  [   20/   89]
Per-example loss in batch: 0.249451  [   22/   89]
Per-example loss in batch: 0.268501  [   24/   89]
Per-example loss in batch: 0.251352  [   26/   89]
Per-example loss in batch: 0.267963  [   28/   89]
Per-example loss in batch: 0.244244  [   30/   89]
Per-example loss in batch: 0.248352  [   32/   89]
Per-example loss in batch: 0.338572  [   34/   89]
Per-example loss in batch: 0.312391  [   36/   89]
Per-example loss in batch: 0.339379  [   38/   89]
Per-example loss in batch: 0.263109  [   40/   89]
Per-example loss in batch: 0.229197  [   42/   89]
Per-example loss in batch: 0.209359  [   44/   89]
Per-example loss in batch: 0.193528  [   46/   89]
Per-example loss in batch: 0.235276  [   48/   89]
Per-example loss in batch: 0.301895  [   50/   89]
Per-example loss in batch: 0.202123  [   52/   89]
Per-example loss in batch: 0.205166  [   54/   89]
Per-example loss in batch: 0.224349  [   56/   89]
Per-example loss in batch: 0.230611  [   58/   89]
Per-example loss in batch: 0.238720  [   60/   89]
Per-example loss in batch: 0.290528  [   62/   89]
Per-example loss in batch: 0.252604  [   64/   89]
Per-example loss in batch: 0.314569  [   66/   89]
Per-example loss in batch: 0.251437  [   68/   89]
Per-example loss in batch: 0.256197  [   70/   89]
Per-example loss in batch: 0.194369  [   72/   89]
Per-example loss in batch: 0.195546  [   74/   89]
Per-example loss in batch: 0.218815  [   76/   89]
Per-example loss in batch: 0.197953  [   78/   89]
Per-example loss in batch: 0.227360  [   80/   89]
Per-example loss in batch: 0.213147  [   82/   89]
Per-example loss in batch: 0.265412  [   84/   89]
Per-example loss in batch: 0.321373  [   86/   89]
Per-example loss in batch: 0.248911  [   88/   89]
Per-example loss in batch: 0.745619  [   89/   89]
Train Error: Avg loss: 0.25741475
validation Error: 
 Avg loss: 0.27386411 
 F1: 0.489512 
 Precision: 0.577075 
 Recall: 0.425021
 IoU: 0.324076

test Error: 
 Avg loss: 0.25437924 
 F1: 0.521507 
 Precision: 0.618014 
 Recall: 0.451070
 IoU: 0.352729

We have finished training iteration 210
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_208_.pth
Per-example loss in batch: 0.272513  [    2/   89]
Per-example loss in batch: 0.194365  [    4/   89]
Per-example loss in batch: 0.296005  [    6/   89]
Per-example loss in batch: 0.239855  [    8/   89]
Per-example loss in batch: 0.226623  [   10/   89]
Per-example loss in batch: 0.202463  [   12/   89]
Per-example loss in batch: 0.234201  [   14/   89]
Per-example loss in batch: 0.296468  [   16/   89]
Per-example loss in batch: 0.313091  [   18/   89]
Per-example loss in batch: 0.216993  [   20/   89]
Per-example loss in batch: 0.213761  [   22/   89]
Per-example loss in batch: 0.319394  [   24/   89]
Per-example loss in batch: 0.226529  [   26/   89]
Per-example loss in batch: 0.228738  [   28/   89]
Per-example loss in batch: 0.234186  [   30/   89]
Per-example loss in batch: 0.235536  [   32/   89]
Per-example loss in batch: 0.257947  [   34/   89]
Per-example loss in batch: 0.302145  [   36/   89]
Per-example loss in batch: 0.338485  [   38/   89]
Per-example loss in batch: 0.202099  [   40/   89]
Per-example loss in batch: 0.219291  [   42/   89]
Per-example loss in batch: 0.248195  [   44/   89]
Per-example loss in batch: 0.193297  [   46/   89]
Per-example loss in batch: 0.241694  [   48/   89]
Per-example loss in batch: 0.230335  [   50/   89]
Per-example loss in batch: 0.240069  [   52/   89]
Per-example loss in batch: 0.224307  [   54/   89]
Per-example loss in batch: 0.243379  [   56/   89]
Per-example loss in batch: 0.295656  [   58/   89]
Per-example loss in batch: 0.265656  [   60/   89]
Per-example loss in batch: 0.182567  [   62/   89]
Per-example loss in batch: 0.249116  [   64/   89]
Per-example loss in batch: 0.334376  [   66/   89]
Per-example loss in batch: 0.215098  [   68/   89]
Per-example loss in batch: 0.247103  [   70/   89]
Per-example loss in batch: 0.283134  [   72/   89]
Per-example loss in batch: 0.233951  [   74/   89]
Per-example loss in batch: 0.263344  [   76/   89]
Per-example loss in batch: 0.241596  [   78/   89]
Per-example loss in batch: 0.288411  [   80/   89]
Per-example loss in batch: 0.302332  [   82/   89]
Per-example loss in batch: 0.282307  [   84/   89]
Per-example loss in batch: 0.260481  [   86/   89]
Per-example loss in batch: 0.286885  [   88/   89]
Per-example loss in batch: 0.555816  [   89/   89]
Train Error: Avg loss: 0.25622212
validation Error: 
 Avg loss: 0.27516792 
 F1: 0.490051 
 Precision: 0.567666 
 Recall: 0.431107
 IoU: 0.324548

test Error: 
 Avg loss: 0.25279360 
 F1: 0.525380 
 Precision: 0.613839 
 Recall: 0.459206
 IoU: 0.356282

We have finished training iteration 211
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_209_.pth
Per-example loss in batch: 0.278008  [    2/   89]
Per-example loss in batch: 0.261051  [    4/   89]
Per-example loss in batch: 0.287311  [    6/   89]
Per-example loss in batch: 0.214470  [    8/   89]
Per-example loss in batch: 0.182162  [   10/   89]
Per-example loss in batch: 0.253052  [   12/   89]
Per-example loss in batch: 0.235361  [   14/   89]
Per-example loss in batch: 0.283704  [   16/   89]
Per-example loss in batch: 0.260984  [   18/   89]
Per-example loss in batch: 0.307762  [   20/   89]
Per-example loss in batch: 0.272728  [   22/   89]
Per-example loss in batch: 0.242206  [   24/   89]
Per-example loss in batch: 0.307881  [   26/   89]
Per-example loss in batch: 0.221852  [   28/   89]
Per-example loss in batch: 0.206526  [   30/   89]
Per-example loss in batch: 0.288211  [   32/   89]
Per-example loss in batch: 0.252029  [   34/   89]
Per-example loss in batch: 0.281047  [   36/   89]
Per-example loss in batch: 0.190441  [   38/   89]
Per-example loss in batch: 0.247584  [   40/   89]
Per-example loss in batch: 0.251068  [   42/   89]
Per-example loss in batch: 0.225968  [   44/   89]
Per-example loss in batch: 0.225264  [   46/   89]
Per-example loss in batch: 0.252168  [   48/   89]
Per-example loss in batch: 0.264405  [   50/   89]
Per-example loss in batch: 0.246049  [   52/   89]
Per-example loss in batch: 0.231101  [   54/   89]
Per-example loss in batch: 0.314899  [   56/   89]
Per-example loss in batch: 0.281575  [   58/   89]
Per-example loss in batch: 0.247443  [   60/   89]
Per-example loss in batch: 0.268161  [   62/   89]
Per-example loss in batch: 0.300287  [   64/   89]
Per-example loss in batch: 0.220219  [   66/   89]
Per-example loss in batch: 0.263610  [   68/   89]
Per-example loss in batch: 0.261501  [   70/   89]
Per-example loss in batch: 0.203878  [   72/   89]
Per-example loss in batch: 0.235084  [   74/   89]
Per-example loss in batch: 0.284916  [   76/   89]
Per-example loss in batch: 0.244985  [   78/   89]
Per-example loss in batch: 0.245389  [   80/   89]
Per-example loss in batch: 0.201243  [   82/   89]
Per-example loss in batch: 0.246629  [   84/   89]
Per-example loss in batch: 0.209404  [   86/   89]
Per-example loss in batch: 0.251586  [   88/   89]
Per-example loss in batch: 0.472755  [   89/   89]
Train Error: Avg loss: 0.25365349
validation Error: 
 Avg loss: 0.26895673 
 F1: 0.490918 
 Precision: 0.548718 
 Recall: 0.444134
 IoU: 0.325309

test Error: 
 Avg loss: 0.25241606 
 F1: 0.525105 
 Precision: 0.592341 
 Recall: 0.471576
 IoU: 0.356028

We have finished training iteration 212
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_210_.pth
Per-example loss in batch: 0.218637  [    2/   89]
Per-example loss in batch: 0.255208  [    4/   89]
Per-example loss in batch: 0.272550  [    6/   89]
Per-example loss in batch: 0.204085  [    8/   89]
Per-example loss in batch: 0.368743  [   10/   89]
Per-example loss in batch: 0.255526  [   12/   89]
Per-example loss in batch: 0.244962  [   14/   89]
Per-example loss in batch: 0.223567  [   16/   89]
Per-example loss in batch: 0.273581  [   18/   89]
Per-example loss in batch: 0.208678  [   20/   89]
Per-example loss in batch: 0.269138  [   22/   89]
Per-example loss in batch: 0.218279  [   24/   89]
Per-example loss in batch: 0.211924  [   26/   89]
Per-example loss in batch: 0.218686  [   28/   89]
Per-example loss in batch: 0.235604  [   30/   89]
Per-example loss in batch: 0.282450  [   32/   89]
Per-example loss in batch: 0.239990  [   34/   89]
Per-example loss in batch: 0.221735  [   36/   89]
Per-example loss in batch: 0.216265  [   38/   89]
Per-example loss in batch: 0.262763  [   40/   89]
Per-example loss in batch: 0.235152  [   42/   89]
Per-example loss in batch: 0.236627  [   44/   89]
Per-example loss in batch: 0.227750  [   46/   89]
Per-example loss in batch: 0.257064  [   48/   89]
Per-example loss in batch: 0.261038  [   50/   89]
Per-example loss in batch: 0.251664  [   52/   89]
Per-example loss in batch: 0.293862  [   54/   89]
Per-example loss in batch: 0.222148  [   56/   89]
Per-example loss in batch: 0.252512  [   58/   89]
Per-example loss in batch: 0.325591  [   60/   89]
Per-example loss in batch: 0.242435  [   62/   89]
Per-example loss in batch: 0.264812  [   64/   89]
Per-example loss in batch: 0.293404  [   66/   89]
Per-example loss in batch: 0.249538  [   68/   89]
Per-example loss in batch: 0.214151  [   70/   89]
Per-example loss in batch: 0.188622  [   72/   89]
Per-example loss in batch: 0.193495  [   74/   89]
Per-example loss in batch: 0.205166  [   76/   89]
Per-example loss in batch: 0.289868  [   78/   89]
Per-example loss in batch: 0.207447  [   80/   89]
Per-example loss in batch: 0.301777  [   82/   89]
Per-example loss in batch: 0.282480  [   84/   89]
Per-example loss in batch: 0.296170  [   86/   89]
Per-example loss in batch: 0.236572  [   88/   89]
Per-example loss in batch: 0.706888  [   89/   89]
Train Error: Avg loss: 0.25359918
validation Error: 
 Avg loss: 0.27373241 
 F1: 0.490309 
 Precision: 0.568932 
 Recall: 0.430779
 IoU: 0.324775

test Error: 
 Avg loss: 0.25311787 
 F1: 0.524373 
 Precision: 0.615150 
 Recall: 0.456943
 IoU: 0.355356

We have finished training iteration 213
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_211_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.274678  [    2/   89]
Per-example loss in batch: 0.351764  [    4/   89]
Per-example loss in batch: 0.208950  [    6/   89]
Per-example loss in batch: 0.232868  [    8/   89]
Per-example loss in batch: 0.287868  [   10/   89]
Per-example loss in batch: 0.201770  [   12/   89]
Per-example loss in batch: 0.222991  [   14/   89]
Per-example loss in batch: 0.264123  [   16/   89]
Per-example loss in batch: 0.195609  [   18/   89]
Per-example loss in batch: 0.230033  [   20/   89]
Per-example loss in batch: 0.278354  [   22/   89]
Per-example loss in batch: 0.218756  [   24/   89]
Per-example loss in batch: 0.313025  [   26/   89]
Per-example loss in batch: 0.218641  [   28/   89]
Per-example loss in batch: 0.271291  [   30/   89]
Per-example loss in batch: 0.276431  [   32/   89]
Per-example loss in batch: 0.203205  [   34/   89]
Per-example loss in batch: 0.226581  [   36/   89]
Per-example loss in batch: 0.299761  [   38/   89]
Per-example loss in batch: 0.194566  [   40/   89]
Per-example loss in batch: 0.312044  [   42/   89]
Per-example loss in batch: 0.217267  [   44/   89]
Per-example loss in batch: 0.385316  [   46/   89]
Per-example loss in batch: 0.248252  [   48/   89]
Per-example loss in batch: 0.273153  [   50/   89]
Per-example loss in batch: 0.319007  [   52/   89]
Per-example loss in batch: 0.226899  [   54/   89]
Per-example loss in batch: 0.204886  [   56/   89]
Per-example loss in batch: 0.295865  [   58/   89]
Per-example loss in batch: 0.218200  [   60/   89]
Per-example loss in batch: 0.322693  [   62/   89]
Per-example loss in batch: 0.195880  [   64/   89]
Per-example loss in batch: 0.289854  [   66/   89]
Per-example loss in batch: 0.205878  [   68/   89]
Per-example loss in batch: 0.212937  [   70/   89]
Per-example loss in batch: 0.304459  [   72/   89]
Per-example loss in batch: 0.216699  [   74/   89]
Per-example loss in batch: 0.262246  [   76/   89]
Per-example loss in batch: 0.275311  [   78/   89]
Per-example loss in batch: 0.302340  [   80/   89]
Per-example loss in batch: 0.327427  [   82/   89]
Per-example loss in batch: 0.247724  [   84/   89]
Per-example loss in batch: 0.278168  [   86/   89]
Per-example loss in batch: 0.227646  [   88/   89]
Per-example loss in batch: 0.561907  [   89/   89]
Train Error: Avg loss: 0.26117687
validation Error: 
 Avg loss: 0.27575250 
 F1: 0.487339 
 Precision: 0.542499 
 Recall: 0.442361
 IoU: 0.322173

test Error: 
 Avg loss: 0.25386314 
 F1: 0.523067 
 Precision: 0.591764 
 Recall: 0.468660
 IoU: 0.354157

We have finished training iteration 214
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_206_.pth
Per-example loss in batch: 0.292617  [    2/   89]
Per-example loss in batch: 0.194639  [    4/   89]
Per-example loss in batch: 0.281783  [    6/   89]
Per-example loss in batch: 0.230380  [    8/   89]
Per-example loss in batch: 0.239466  [   10/   89]
Per-example loss in batch: 0.338171  [   12/   89]
Per-example loss in batch: 0.198366  [   14/   89]
Per-example loss in batch: 0.204046  [   16/   89]
Per-example loss in batch: 0.246510  [   18/   89]
Per-example loss in batch: 0.200453  [   20/   89]
Per-example loss in batch: 0.368070  [   22/   89]
Per-example loss in batch: 0.285834  [   24/   89]
Per-example loss in batch: 0.244610  [   26/   89]
Per-example loss in batch: 0.219614  [   28/   89]
Per-example loss in batch: 0.305028  [   30/   89]
Per-example loss in batch: 0.249403  [   32/   89]
Per-example loss in batch: 0.233389  [   34/   89]
Per-example loss in batch: 0.245921  [   36/   89]
Per-example loss in batch: 0.202531  [   38/   89]
Per-example loss in batch: 0.303624  [   40/   89]
Per-example loss in batch: 0.245533  [   42/   89]
Per-example loss in batch: 0.240195  [   44/   89]
Per-example loss in batch: 0.192130  [   46/   89]
Per-example loss in batch: 0.212221  [   48/   89]
Per-example loss in batch: 0.256920  [   50/   89]
Per-example loss in batch: 0.303748  [   52/   89]
Per-example loss in batch: 0.300752  [   54/   89]
Per-example loss in batch: 0.274820  [   56/   89]
Per-example loss in batch: 0.258874  [   58/   89]
Per-example loss in batch: 0.275452  [   60/   89]
Per-example loss in batch: 0.207152  [   62/   89]
Per-example loss in batch: 0.257662  [   64/   89]
Per-example loss in batch: 0.252454  [   66/   89]
Per-example loss in batch: 0.246945  [   68/   89]
Per-example loss in batch: 0.280607  [   70/   89]
Per-example loss in batch: 0.239123  [   72/   89]
Per-example loss in batch: 0.217040  [   74/   89]
Per-example loss in batch: 0.298016  [   76/   89]
Per-example loss in batch: 0.287005  [   78/   89]
Per-example loss in batch: 0.250543  [   80/   89]
Per-example loss in batch: 0.283286  [   82/   89]
Per-example loss in batch: 0.215513  [   84/   89]
Per-example loss in batch: 0.302260  [   86/   89]
Per-example loss in batch: 0.182096  [   88/   89]
Per-example loss in batch: 0.404573  [   89/   89]
Train Error: Avg loss: 0.25544015
validation Error: 
 Avg loss: 0.27333733 
 F1: 0.490471 
 Precision: 0.572540 
 Recall: 0.428981
 IoU: 0.324917

test Error: 
 Avg loss: 0.25406348 
 F1: 0.522236 
 Precision: 0.611876 
 Recall: 0.455504
 IoU: 0.353396

We have finished training iteration 215
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_213_.pth
Per-example loss in batch: 0.242614  [    2/   89]
Per-example loss in batch: 0.307569  [    4/   89]
Per-example loss in batch: 0.272086  [    6/   89]
Per-example loss in batch: 0.217647  [    8/   89]
Per-example loss in batch: 0.238050  [   10/   89]
Per-example loss in batch: 0.260104  [   12/   89]
Per-example loss in batch: 0.196417  [   14/   89]
Per-example loss in batch: 0.196263  [   16/   89]
Per-example loss in batch: 0.268416  [   18/   89]
Per-example loss in batch: 0.244697  [   20/   89]
Per-example loss in batch: 0.284411  [   22/   89]
Per-example loss in batch: 0.264075  [   24/   89]
Per-example loss in batch: 0.198113  [   26/   89]
Per-example loss in batch: 0.202782  [   28/   89]
Per-example loss in batch: 0.320362  [   30/   89]
Per-example loss in batch: 0.213108  [   32/   89]
Per-example loss in batch: 0.257530  [   34/   89]
Per-example loss in batch: 0.249523  [   36/   89]
Per-example loss in batch: 0.314462  [   38/   89]
Per-example loss in batch: 0.274593  [   40/   89]
Per-example loss in batch: 0.236740  [   42/   89]
Per-example loss in batch: 0.208886  [   44/   89]
Per-example loss in batch: 0.231187  [   46/   89]
Per-example loss in batch: 0.257553  [   48/   89]
Per-example loss in batch: 0.236822  [   50/   89]
Per-example loss in batch: 0.316243  [   52/   89]
Per-example loss in batch: 0.293998  [   54/   89]
Per-example loss in batch: 0.238105  [   56/   89]
Per-example loss in batch: 0.334586  [   58/   89]
Per-example loss in batch: 0.227534  [   60/   89]
Per-example loss in batch: 0.264092  [   62/   89]
Per-example loss in batch: 0.235553  [   64/   89]
Per-example loss in batch: 0.222171  [   66/   89]
Per-example loss in batch: 0.220504  [   68/   89]
Per-example loss in batch: 0.276227  [   70/   89]
Per-example loss in batch: 0.309132  [   72/   89]
Per-example loss in batch: 0.222845  [   74/   89]
Per-example loss in batch: 0.287971  [   76/   89]
Per-example loss in batch: 0.172716  [   78/   89]
Per-example loss in batch: 0.285023  [   80/   89]
Per-example loss in batch: 0.210173  [   82/   89]
Per-example loss in batch: 0.259713  [   84/   89]
Per-example loss in batch: 0.198523  [   86/   89]
Per-example loss in batch: 0.310667  [   88/   89]
Per-example loss in batch: 0.578443  [   89/   89]
Train Error: Avg loss: 0.25548327
validation Error: 
 Avg loss: 0.27940235 
 F1: 0.483180 
 Precision: 0.544729 
 Recall: 0.434127
 IoU: 0.318548

test Error: 
 Avg loss: 0.25676544 
 F1: 0.515693 
 Precision: 0.584733 
 Recall: 0.461235
 IoU: 0.347431

We have finished training iteration 216
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_214_.pth
Per-example loss in batch: 0.319987  [    2/   89]
Per-example loss in batch: 0.199715  [    4/   89]
Per-example loss in batch: 0.259179  [    6/   89]
Per-example loss in batch: 0.329166  [    8/   89]
Per-example loss in batch: 0.280307  [   10/   89]
Per-example loss in batch: 0.347635  [   12/   89]
Per-example loss in batch: 0.224195  [   14/   89]
Per-example loss in batch: 0.225744  [   16/   89]
Per-example loss in batch: 0.239093  [   18/   89]
Per-example loss in batch: 0.255430  [   20/   89]
Per-example loss in batch: 0.259652  [   22/   89]
Per-example loss in batch: 0.217284  [   24/   89]
Per-example loss in batch: 0.212669  [   26/   89]
Per-example loss in batch: 0.240451  [   28/   89]
Per-example loss in batch: 0.296373  [   30/   89]
Per-example loss in batch: 0.230846  [   32/   89]
Per-example loss in batch: 0.274752  [   34/   89]
Per-example loss in batch: 0.200102  [   36/   89]
Per-example loss in batch: 0.233794  [   38/   89]
Per-example loss in batch: 0.386572  [   40/   89]
Per-example loss in batch: 0.282855  [   42/   89]
Per-example loss in batch: 0.215421  [   44/   89]
Per-example loss in batch: 0.318560  [   46/   89]
Per-example loss in batch: 0.286923  [   48/   89]
Per-example loss in batch: 0.280152  [   50/   89]
Per-example loss in batch: 0.235539  [   52/   89]
Per-example loss in batch: 0.213804  [   54/   89]
Per-example loss in batch: 0.262786  [   56/   89]
Per-example loss in batch: 0.233955  [   58/   89]
Per-example loss in batch: 0.319520  [   60/   89]
Per-example loss in batch: 0.243459  [   62/   89]
Per-example loss in batch: 0.323898  [   64/   89]
Per-example loss in batch: 0.190231  [   66/   89]
Per-example loss in batch: 0.233799  [   68/   89]
Per-example loss in batch: 0.209733  [   70/   89]
Per-example loss in batch: 0.228554  [   72/   89]
Per-example loss in batch: 0.301358  [   74/   89]
Per-example loss in batch: 0.275845  [   76/   89]
Per-example loss in batch: 0.292845  [   78/   89]
Per-example loss in batch: 0.258931  [   80/   89]
Per-example loss in batch: 0.280649  [   82/   89]
Per-example loss in batch: 0.261088  [   84/   89]
Per-example loss in batch: 0.216896  [   86/   89]
Per-example loss in batch: 0.195363  [   88/   89]
Per-example loss in batch: 0.422940  [   89/   89]
Train Error: Avg loss: 0.26082199
validation Error: 
 Avg loss: 0.27715310 
 F1: 0.488539 
 Precision: 0.547298 
 Recall: 0.441174
 IoU: 0.323223

test Error: 
 Avg loss: 0.25330818 
 F1: 0.523701 
 Precision: 0.590955 
 Recall: 0.470191
 IoU: 0.354740

We have finished training iteration 217
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_215_.pth
Per-example loss in batch: 0.307393  [    2/   89]
Per-example loss in batch: 0.208876  [    4/   89]
Per-example loss in batch: 0.186720  [    6/   89]
Per-example loss in batch: 0.282454  [    8/   89]
Per-example loss in batch: 0.318346  [   10/   89]
Per-example loss in batch: 0.327283  [   12/   89]
Per-example loss in batch: 0.257602  [   14/   89]
Per-example loss in batch: 0.269145  [   16/   89]
Per-example loss in batch: 0.207315  [   18/   89]
Per-example loss in batch: 0.265286  [   20/   89]
Per-example loss in batch: 0.209574  [   22/   89]
Per-example loss in batch: 0.277851  [   24/   89]
Per-example loss in batch: 0.230572  [   26/   89]
Per-example loss in batch: 0.304014  [   28/   89]
Per-example loss in batch: 0.279486  [   30/   89]
Per-example loss in batch: 0.191101  [   32/   89]
Per-example loss in batch: 0.269883  [   34/   89]
Per-example loss in batch: 0.275479  [   36/   89]
Per-example loss in batch: 0.200842  [   38/   89]
Per-example loss in batch: 0.251516  [   40/   89]
Per-example loss in batch: 0.328368  [   42/   89]
Per-example loss in batch: 0.280678  [   44/   89]
Per-example loss in batch: 0.219442  [   46/   89]
Per-example loss in batch: 0.196967  [   48/   89]
Per-example loss in batch: 0.229591  [   50/   89]
Per-example loss in batch: 0.217448  [   52/   89]
Per-example loss in batch: 0.235960  [   54/   89]
Per-example loss in batch: 0.300001  [   56/   89]
Per-example loss in batch: 0.206442  [   58/   89]
Per-example loss in batch: 0.249068  [   60/   89]
Per-example loss in batch: 0.237505  [   62/   89]
Per-example loss in batch: 0.283441  [   64/   89]
Per-example loss in batch: 0.220980  [   66/   89]
Per-example loss in batch: 0.236658  [   68/   89]
Per-example loss in batch: 0.322129  [   70/   89]
Per-example loss in batch: 0.295726  [   72/   89]
Per-example loss in batch: 0.278893  [   74/   89]
Per-example loss in batch: 0.317138  [   76/   89]
Per-example loss in batch: 0.260005  [   78/   89]
Per-example loss in batch: 0.241083  [   80/   89]
Per-example loss in batch: 0.185918  [   82/   89]
Per-example loss in batch: 0.189785  [   84/   89]
Per-example loss in batch: 0.204624  [   86/   89]
Per-example loss in batch: 0.274937  [   88/   89]
Per-example loss in batch: 0.477246  [   89/   89]
Train Error: Avg loss: 0.25555390
validation Error: 
 Avg loss: 0.27610008 
 F1: 0.490360 
 Precision: 0.572307 
 Recall: 0.428941
 IoU: 0.324819

test Error: 
 Avg loss: 0.25469932 
 F1: 0.520855 
 Precision: 0.610726 
 Recall: 0.454041
 IoU: 0.352132

We have finished training iteration 218
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_216_.pth
Per-example loss in batch: 0.203330  [    2/   89]
Per-example loss in batch: 0.242051  [    4/   89]
Per-example loss in batch: 0.187531  [    6/   89]
Per-example loss in batch: 0.226476  [    8/   89]
Per-example loss in batch: 0.257721  [   10/   89]
Per-example loss in batch: 0.243498  [   12/   89]
Per-example loss in batch: 0.226984  [   14/   89]
Per-example loss in batch: 0.304016  [   16/   89]
Per-example loss in batch: 0.231411  [   18/   89]
Per-example loss in batch: 0.276989  [   20/   89]
Per-example loss in batch: 0.275580  [   22/   89]
Per-example loss in batch: 0.330052  [   24/   89]
Per-example loss in batch: 0.250451  [   26/   89]
Per-example loss in batch: 0.253532  [   28/   89]
Per-example loss in batch: 0.273303  [   30/   89]
Per-example loss in batch: 0.217127  [   32/   89]
Per-example loss in batch: 0.250921  [   34/   89]
Per-example loss in batch: 0.197966  [   36/   89]
Per-example loss in batch: 0.283192  [   38/   89]
Per-example loss in batch: 0.331906  [   40/   89]
Per-example loss in batch: 0.320676  [   42/   89]
Per-example loss in batch: 0.267115  [   44/   89]
Per-example loss in batch: 0.312032  [   46/   89]
Per-example loss in batch: 0.235958  [   48/   89]
Per-example loss in batch: 0.208356  [   50/   89]
Per-example loss in batch: 0.262057  [   52/   89]
Per-example loss in batch: 0.248016  [   54/   89]
Per-example loss in batch: 0.236609  [   56/   89]
Per-example loss in batch: 0.303271  [   58/   89]
Per-example loss in batch: 0.194676  [   60/   89]
Per-example loss in batch: 0.242816  [   62/   89]
Per-example loss in batch: 0.263941  [   64/   89]
Per-example loss in batch: 0.205035  [   66/   89]
Per-example loss in batch: 0.233713  [   68/   89]
Per-example loss in batch: 0.275954  [   70/   89]
Per-example loss in batch: 0.288655  [   72/   89]
Per-example loss in batch: 0.196083  [   74/   89]
Per-example loss in batch: 0.241293  [   76/   89]
Per-example loss in batch: 0.264114  [   78/   89]
Per-example loss in batch: 0.234577  [   80/   89]
Per-example loss in batch: 0.230414  [   82/   89]
Per-example loss in batch: 0.325558  [   84/   89]
Per-example loss in batch: 0.241500  [   86/   89]
Per-example loss in batch: 0.288227  [   88/   89]
Per-example loss in batch: 0.527303  [   89/   89]
Train Error: Avg loss: 0.25726591
validation Error: 
 Avg loss: 0.27950628 
 F1: 0.490814 
 Precision: 0.563157 
 Recall: 0.434942
 IoU: 0.325218

test Error: 
 Avg loss: 0.25335890 
 F1: 0.523475 
 Precision: 0.607219 
 Recall: 0.460030
 IoU: 0.354532

We have finished training iteration 219
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_217_.pth
Per-example loss in batch: 0.334623  [    2/   89]
Per-example loss in batch: 0.313003  [    4/   89]
Per-example loss in batch: 0.349612  [    6/   89]
Per-example loss in batch: 0.222911  [    8/   89]
Per-example loss in batch: 0.234231  [   10/   89]
Per-example loss in batch: 0.193038  [   12/   89]
Per-example loss in batch: 0.221155  [   14/   89]
Per-example loss in batch: 0.211148  [   16/   89]
Per-example loss in batch: 0.237101  [   18/   89]
Per-example loss in batch: 0.270341  [   20/   89]
Per-example loss in batch: 0.235279  [   22/   89]
Per-example loss in batch: 0.240573  [   24/   89]
Per-example loss in batch: 0.201489  [   26/   89]
Per-example loss in batch: 0.233290  [   28/   89]
Per-example loss in batch: 0.272304  [   30/   89]
Per-example loss in batch: 0.219723  [   32/   89]
Per-example loss in batch: 0.230260  [   34/   89]
Per-example loss in batch: 0.339935  [   36/   89]
Per-example loss in batch: 0.267471  [   38/   89]
Per-example loss in batch: 0.228414  [   40/   89]
Per-example loss in batch: 0.252401  [   42/   89]
Per-example loss in batch: 0.257134  [   44/   89]
Per-example loss in batch: 0.314016  [   46/   89]
Per-example loss in batch: 0.276366  [   48/   89]
Per-example loss in batch: 0.261381  [   50/   89]
Per-example loss in batch: 0.303407  [   52/   89]
Per-example loss in batch: 0.263040  [   54/   89]
Per-example loss in batch: 0.207460  [   56/   89]
Per-example loss in batch: 0.261714  [   58/   89]
Per-example loss in batch: 0.232548  [   60/   89]
Per-example loss in batch: 0.303404  [   62/   89]
Per-example loss in batch: 0.184610  [   64/   89]
Per-example loss in batch: 0.307753  [   66/   89]
Per-example loss in batch: 0.218897  [   68/   89]
Per-example loss in batch: 0.181665  [   70/   89]
Per-example loss in batch: 0.358985  [   72/   89]
Per-example loss in batch: 0.234769  [   74/   89]
Per-example loss in batch: 0.248485  [   76/   89]
Per-example loss in batch: 0.291864  [   78/   89]
Per-example loss in batch: 0.265535  [   80/   89]
Per-example loss in batch: 0.299044  [   82/   89]
Per-example loss in batch: 0.315866  [   84/   89]
Per-example loss in batch: 0.256616  [   86/   89]
Per-example loss in batch: 0.236084  [   88/   89]
Per-example loss in batch: 0.540149  [   89/   89]
Train Error: Avg loss: 0.26200045
validation Error: 
 Avg loss: 0.27191161 
 F1: 0.489370 
 Precision: 0.567383 
 Recall: 0.430216
 IoU: 0.323951

test Error: 
 Avg loss: 0.25437486 
 F1: 0.521301 
 Precision: 0.608531 
 Recall: 0.455944
 IoU: 0.352540

We have finished training iteration 220
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_218_.pth
Per-example loss in batch: 0.254884  [    2/   89]
Per-example loss in batch: 0.241982  [    4/   89]
Per-example loss in batch: 0.312980  [    6/   89]
Per-example loss in batch: 0.239239  [    8/   89]
Per-example loss in batch: 0.281324  [   10/   89]
Per-example loss in batch: 0.323569  [   12/   89]
Per-example loss in batch: 0.311636  [   14/   89]
Per-example loss in batch: 0.228664  [   16/   89]
Per-example loss in batch: 0.239311  [   18/   89]
Per-example loss in batch: 0.220391  [   20/   89]
Per-example loss in batch: 0.299338  [   22/   89]
Per-example loss in batch: 0.228509  [   24/   89]
Per-example loss in batch: 0.262979  [   26/   89]
Per-example loss in batch: 0.223749  [   28/   89]
Per-example loss in batch: 0.208142  [   30/   89]
Per-example loss in batch: 0.255727  [   32/   89]
Per-example loss in batch: 0.266126  [   34/   89]
Per-example loss in batch: 0.240901  [   36/   89]
Per-example loss in batch: 0.216718  [   38/   89]
Per-example loss in batch: 0.200034  [   40/   89]
Per-example loss in batch: 0.233185  [   42/   89]
Per-example loss in batch: 0.354822  [   44/   89]
Per-example loss in batch: 0.235107  [   46/   89]
Per-example loss in batch: 0.240131  [   48/   89]
Per-example loss in batch: 0.301629  [   50/   89]
Per-example loss in batch: 0.273887  [   52/   89]
Per-example loss in batch: 0.291954  [   54/   89]
Per-example loss in batch: 0.222104  [   56/   89]
Per-example loss in batch: 0.320804  [   58/   89]
Per-example loss in batch: 0.188905  [   60/   89]
Per-example loss in batch: 0.230404  [   62/   89]
Per-example loss in batch: 0.264465  [   64/   89]
Per-example loss in batch: 0.219190  [   66/   89]
Per-example loss in batch: 0.234326  [   68/   89]
Per-example loss in batch: 0.250997  [   70/   89]
Per-example loss in batch: 0.311149  [   72/   89]
Per-example loss in batch: 0.260911  [   74/   89]
Per-example loss in batch: 0.275270  [   76/   89]
Per-example loss in batch: 0.348629  [   78/   89]
Per-example loss in batch: 0.243474  [   80/   89]
Per-example loss in batch: 0.218893  [   82/   89]
Per-example loss in batch: 0.202218  [   84/   89]
Per-example loss in batch: 0.241389  [   86/   89]
Per-example loss in batch: 0.284432  [   88/   89]
Per-example loss in batch: 0.413674  [   89/   89]
Train Error: Avg loss: 0.25868119
validation Error: 
 Avg loss: 0.27746958 
 F1: 0.487390 
 Precision: 0.535908 
 Recall: 0.446929
 IoU: 0.322218

test Error: 
 Avg loss: 0.25298592 
 F1: 0.524408 
 Precision: 0.582333 
 Recall: 0.476963
 IoU: 0.355388

We have finished training iteration 221
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_219_.pth
Per-example loss in batch: 0.202446  [    2/   89]
Per-example loss in batch: 0.241688  [    4/   89]
Per-example loss in batch: 0.274236  [    6/   89]
Per-example loss in batch: 0.288494  [    8/   89]
Per-example loss in batch: 0.248208  [   10/   89]
Per-example loss in batch: 0.293904  [   12/   89]
Per-example loss in batch: 0.253338  [   14/   89]
Per-example loss in batch: 0.206819  [   16/   89]
Per-example loss in batch: 0.186777  [   18/   89]
Per-example loss in batch: 0.250113  [   20/   89]
Per-example loss in batch: 0.297263  [   22/   89]
Per-example loss in batch: 0.282480  [   24/   89]
Per-example loss in batch: 0.288023  [   26/   89]
Per-example loss in batch: 0.225475  [   28/   89]
Per-example loss in batch: 0.311920  [   30/   89]
Per-example loss in batch: 0.247423  [   32/   89]
Per-example loss in batch: 0.302264  [   34/   89]
Per-example loss in batch: 0.252601  [   36/   89]
Per-example loss in batch: 0.323406  [   38/   89]
Per-example loss in batch: 0.280449  [   40/   89]
Per-example loss in batch: 0.258070  [   42/   89]
Per-example loss in batch: 0.202969  [   44/   89]
Per-example loss in batch: 0.266526  [   46/   89]
Per-example loss in batch: 0.281036  [   48/   89]
Per-example loss in batch: 0.313578  [   50/   89]
Per-example loss in batch: 0.258978  [   52/   89]
Per-example loss in batch: 0.220973  [   54/   89]
Per-example loss in batch: 0.200222  [   56/   89]
Per-example loss in batch: 0.276508  [   58/   89]
Per-example loss in batch: 0.200528  [   60/   89]
Per-example loss in batch: 0.214194  [   62/   89]
Per-example loss in batch: 0.270028  [   64/   89]
Per-example loss in batch: 0.262900  [   66/   89]
Per-example loss in batch: 0.248373  [   68/   89]
Per-example loss in batch: 0.258714  [   70/   89]
Per-example loss in batch: 0.231741  [   72/   89]
Per-example loss in batch: 0.239249  [   74/   89]
Per-example loss in batch: 0.337608  [   76/   89]
Per-example loss in batch: 0.245812  [   78/   89]
Per-example loss in batch: 0.271106  [   80/   89]
Per-example loss in batch: 0.238424  [   82/   89]
Per-example loss in batch: 0.226864  [   84/   89]
Per-example loss in batch: 0.222283  [   86/   89]
Per-example loss in batch: 0.336737  [   88/   89]
Per-example loss in batch: 0.435122  [   89/   89]
Train Error: Avg loss: 0.25973730
validation Error: 
 Avg loss: 0.27647894 
 F1: 0.490605 
 Precision: 0.561908 
 Recall: 0.435359
 IoU: 0.325034

test Error: 
 Avg loss: 0.25282067 
 F1: 0.524888 
 Precision: 0.606804 
 Recall: 0.462458
 IoU: 0.355829

We have finished training iteration 222
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_220_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.271082  [    2/   89]
Per-example loss in batch: 0.296852  [    4/   89]
Per-example loss in batch: 0.317370  [    6/   89]
Per-example loss in batch: 0.294128  [    8/   89]
Per-example loss in batch: 0.211441  [   10/   89]
Per-example loss in batch: 0.295152  [   12/   89]
Per-example loss in batch: 0.282907  [   14/   89]
Per-example loss in batch: 0.211790  [   16/   89]
Per-example loss in batch: 0.335410  [   18/   89]
Per-example loss in batch: 0.243694  [   20/   89]
Per-example loss in batch: 0.282807  [   22/   89]
Per-example loss in batch: 0.217077  [   24/   89]
Per-example loss in batch: 0.316437  [   26/   89]
Per-example loss in batch: 0.255272  [   28/   89]
Per-example loss in batch: 0.219809  [   30/   89]
Per-example loss in batch: 0.203193  [   32/   89]
Per-example loss in batch: 0.206013  [   34/   89]
Per-example loss in batch: 0.277536  [   36/   89]
Per-example loss in batch: 0.202665  [   38/   89]
Per-example loss in batch: 0.216385  [   40/   89]
Per-example loss in batch: 0.272296  [   42/   89]
Per-example loss in batch: 0.274973  [   44/   89]
Per-example loss in batch: 0.207494  [   46/   89]
Per-example loss in batch: 0.238377  [   48/   89]
Per-example loss in batch: 0.292584  [   50/   89]
Per-example loss in batch: 0.383902  [   52/   89]
Per-example loss in batch: 0.227017  [   54/   89]
Per-example loss in batch: 0.281412  [   56/   89]
Per-example loss in batch: 0.273509  [   58/   89]
Per-example loss in batch: 0.327361  [   60/   89]
Per-example loss in batch: 0.246678  [   62/   89]
Per-example loss in batch: 0.236865  [   64/   89]
Per-example loss in batch: 0.300862  [   66/   89]
Per-example loss in batch: 0.248829  [   68/   89]
Per-example loss in batch: 0.255586  [   70/   89]
Per-example loss in batch: 0.233176  [   72/   89]
Per-example loss in batch: 0.262508  [   74/   89]
Per-example loss in batch: 0.240925  [   76/   89]
Per-example loss in batch: 0.181298  [   78/   89]
Per-example loss in batch: 0.264056  [   80/   89]
Per-example loss in batch: 0.218052  [   82/   89]
Per-example loss in batch: 0.237683  [   84/   89]
Per-example loss in batch: 0.251177  [   86/   89]
Per-example loss in batch: 0.262601  [   88/   89]
Per-example loss in batch: 0.574330  [   89/   89]
Train Error: Avg loss: 0.26209901
validation Error: 
 Avg loss: 0.28190255 
 F1: 0.486782 
 Precision: 0.536881 
 Recall: 0.445236
 IoU: 0.321687

test Error: 
 Avg loss: 0.25370851 
 F1: 0.523120 
 Precision: 0.584987 
 Recall: 0.473087
 IoU: 0.354206

We have finished training iteration 223
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_221_.pth
Per-example loss in batch: 0.211636  [    2/   89]
Per-example loss in batch: 0.336228  [    4/   89]
Per-example loss in batch: 0.204743  [    6/   89]
Per-example loss in batch: 0.216404  [    8/   89]
Per-example loss in batch: 0.267024  [   10/   89]
Per-example loss in batch: 0.209562  [   12/   89]
Per-example loss in batch: 0.236026  [   14/   89]
Per-example loss in batch: 0.214432  [   16/   89]
Per-example loss in batch: 0.311157  [   18/   89]
Per-example loss in batch: 0.220401  [   20/   89]
Per-example loss in batch: 0.229245  [   22/   89]
Per-example loss in batch: 0.265173  [   24/   89]
Per-example loss in batch: 0.233714  [   26/   89]
Per-example loss in batch: 0.322075  [   28/   89]
Per-example loss in batch: 0.381283  [   30/   89]
Per-example loss in batch: 0.287009  [   32/   89]
Per-example loss in batch: 0.276106  [   34/   89]
Per-example loss in batch: 0.319053  [   36/   89]
Per-example loss in batch: 0.267799  [   38/   89]
Per-example loss in batch: 0.264788  [   40/   89]
Per-example loss in batch: 0.258983  [   42/   89]
Per-example loss in batch: 0.198498  [   44/   89]
Per-example loss in batch: 0.280207  [   46/   89]
Per-example loss in batch: 0.216301  [   48/   89]
Per-example loss in batch: 0.232652  [   50/   89]
Per-example loss in batch: 0.288364  [   52/   89]
Per-example loss in batch: 0.213569  [   54/   89]
Per-example loss in batch: 0.251556  [   56/   89]
Per-example loss in batch: 0.282206  [   58/   89]
Per-example loss in batch: 0.273060  [   60/   89]
Per-example loss in batch: 0.247031  [   62/   89]
Per-example loss in batch: 0.197201  [   64/   89]
Per-example loss in batch: 0.213284  [   66/   89]
Per-example loss in batch: 0.216195  [   68/   89]
Per-example loss in batch: 0.263249  [   70/   89]
Per-example loss in batch: 0.202987  [   72/   89]
Per-example loss in batch: 0.327982  [   74/   89]
Per-example loss in batch: 0.263105  [   76/   89]
Per-example loss in batch: 0.226170  [   78/   89]
Per-example loss in batch: 0.229778  [   80/   89]
Per-example loss in batch: 0.257702  [   82/   89]
Per-example loss in batch: 0.311420  [   84/   89]
Per-example loss in batch: 0.271407  [   86/   89]
Per-example loss in batch: 0.262930  [   88/   89]
Per-example loss in batch: 0.563574  [   89/   89]
Train Error: Avg loss: 0.25935911
validation Error: 
 Avg loss: 0.27534143 
 F1: 0.489412 
 Precision: 0.554453 
 Recall: 0.438029
 IoU: 0.323988

test Error: 
 Avg loss: 0.25313326 
 F1: 0.524292 
 Precision: 0.599586 
 Recall: 0.465799
 IoU: 0.355281

We have finished training iteration 224
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_222_.pth
Per-example loss in batch: 0.304087  [    2/   89]
Per-example loss in batch: 0.263006  [    4/   89]
Per-example loss in batch: 0.272234  [    6/   89]
Per-example loss in batch: 0.240036  [    8/   89]
Per-example loss in batch: 0.198075  [   10/   89]
Per-example loss in batch: 0.251509  [   12/   89]
Per-example loss in batch: 0.258732  [   14/   89]
Per-example loss in batch: 0.205849  [   16/   89]
Per-example loss in batch: 0.235947  [   18/   89]
Per-example loss in batch: 0.352764  [   20/   89]
Per-example loss in batch: 0.219162  [   22/   89]
Per-example loss in batch: 0.312257  [   24/   89]
Per-example loss in batch: 0.238220  [   26/   89]
Per-example loss in batch: 0.313643  [   28/   89]
Per-example loss in batch: 0.185634  [   30/   89]
Per-example loss in batch: 0.291212  [   32/   89]
Per-example loss in batch: 0.262887  [   34/   89]
Per-example loss in batch: 0.222244  [   36/   89]
Per-example loss in batch: 0.237594  [   38/   89]
Per-example loss in batch: 0.253382  [   40/   89]
Per-example loss in batch: 0.248402  [   42/   89]
Per-example loss in batch: 0.189333  [   44/   89]
Per-example loss in batch: 0.296219  [   46/   89]
Per-example loss in batch: 0.283680  [   48/   89]
Per-example loss in batch: 0.258050  [   50/   89]
Per-example loss in batch: 0.244132  [   52/   89]
Per-example loss in batch: 0.314879  [   54/   89]
Per-example loss in batch: 0.313966  [   56/   89]
Per-example loss in batch: 0.235046  [   58/   89]
Per-example loss in batch: 0.323610  [   60/   89]
Per-example loss in batch: 0.234117  [   62/   89]
Per-example loss in batch: 0.228101  [   64/   89]
Per-example loss in batch: 0.287165  [   66/   89]
Per-example loss in batch: 0.294069  [   68/   89]
Per-example loss in batch: 0.203576  [   70/   89]
Per-example loss in batch: 0.264679  [   72/   89]
Per-example loss in batch: 0.264472  [   74/   89]
Per-example loss in batch: 0.312894  [   76/   89]
Per-example loss in batch: 0.263376  [   78/   89]
Per-example loss in batch: 0.230921  [   80/   89]
Per-example loss in batch: 0.274393  [   82/   89]
Per-example loss in batch: 0.315449  [   84/   89]
Per-example loss in batch: 0.193271  [   86/   89]
Per-example loss in batch: 0.221320  [   88/   89]
Per-example loss in batch: 0.617331  [   89/   89]
Train Error: Avg loss: 0.26342152
validation Error: 
 Avg loss: 0.27243010 
 F1: 0.488260 
 Precision: 0.558860 
 Recall: 0.433498
 IoU: 0.322979

test Error: 
 Avg loss: 0.25400327 
 F1: 0.523102 
 Precision: 0.606528 
 Recall: 0.459851
 IoU: 0.354190

We have finished training iteration 225
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_223_.pth
Per-example loss in batch: 0.197722  [    2/   89]
Per-example loss in batch: 0.296829  [    4/   89]
Per-example loss in batch: 0.296790  [    6/   89]
Per-example loss in batch: 0.207105  [    8/   89]
Per-example loss in batch: 0.240324  [   10/   89]
Per-example loss in batch: 0.225430  [   12/   89]
Per-example loss in batch: 0.202047  [   14/   89]
Per-example loss in batch: 0.286002  [   16/   89]
Per-example loss in batch: 0.245160  [   18/   89]
Per-example loss in batch: 0.189901  [   20/   89]
Per-example loss in batch: 0.276984  [   22/   89]
Per-example loss in batch: 0.209974  [   24/   89]
Per-example loss in batch: 0.197642  [   26/   89]
Per-example loss in batch: 0.318191  [   28/   89]
Per-example loss in batch: 0.318745  [   30/   89]
Per-example loss in batch: 0.227084  [   32/   89]
Per-example loss in batch: 0.274261  [   34/   89]
Per-example loss in batch: 0.270213  [   36/   89]
Per-example loss in batch: 0.281240  [   38/   89]
Per-example loss in batch: 0.225320  [   40/   89]
Per-example loss in batch: 0.286308  [   42/   89]
Per-example loss in batch: 0.180225  [   44/   89]
Per-example loss in batch: 0.229413  [   46/   89]
Per-example loss in batch: 0.238125  [   48/   89]
Per-example loss in batch: 0.229579  [   50/   89]
Per-example loss in batch: 0.326208  [   52/   89]
Per-example loss in batch: 0.206059  [   54/   89]
Per-example loss in batch: 0.234748  [   56/   89]
Per-example loss in batch: 0.228940  [   58/   89]
Per-example loss in batch: 0.210815  [   60/   89]
Per-example loss in batch: 0.221229  [   62/   89]
Per-example loss in batch: 0.234575  [   64/   89]
Per-example loss in batch: 0.290376  [   66/   89]
Per-example loss in batch: 0.312098  [   68/   89]
Per-example loss in batch: 0.321455  [   70/   89]
Per-example loss in batch: 0.253258  [   72/   89]
Per-example loss in batch: 0.226483  [   74/   89]
Per-example loss in batch: 0.241884  [   76/   89]
Per-example loss in batch: 0.253469  [   78/   89]
Per-example loss in batch: 0.268222  [   80/   89]
Per-example loss in batch: 0.284241  [   82/   89]
Per-example loss in batch: 0.212178  [   84/   89]
Per-example loss in batch: 0.222578  [   86/   89]
Per-example loss in batch: 0.308351  [   88/   89]
Per-example loss in batch: 0.562258  [   89/   89]
Train Error: Avg loss: 0.25368335
validation Error: 
 Avg loss: 0.27639602 
 F1: 0.489924 
 Precision: 0.569352 
 Recall: 0.429944
 IoU: 0.324437

test Error: 
 Avg loss: 0.25415115 
 F1: 0.522223 
 Precision: 0.613428 
 Recall: 0.454629
 IoU: 0.353385

We have finished training iteration 226
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_224_.pth
Per-example loss in batch: 0.324397  [    2/   89]
Per-example loss in batch: 0.206421  [    4/   89]
Per-example loss in batch: 0.303386  [    6/   89]
Per-example loss in batch: 0.248548  [    8/   89]
Per-example loss in batch: 0.226188  [   10/   89]
Per-example loss in batch: 0.207348  [   12/   89]
Per-example loss in batch: 0.208813  [   14/   89]
Per-example loss in batch: 0.223044  [   16/   89]
Per-example loss in batch: 0.221523  [   18/   89]
Per-example loss in batch: 0.203162  [   20/   89]
Per-example loss in batch: 0.228945  [   22/   89]
Per-example loss in batch: 0.227919  [   24/   89]
Per-example loss in batch: 0.208054  [   26/   89]
Per-example loss in batch: 0.276688  [   28/   89]
Per-example loss in batch: 0.276944  [   30/   89]
Per-example loss in batch: 0.219899  [   32/   89]
Per-example loss in batch: 0.303819  [   34/   89]
Per-example loss in batch: 0.365913  [   36/   89]
Per-example loss in batch: 0.254980  [   38/   89]
Per-example loss in batch: 0.215202  [   40/   89]
Per-example loss in batch: 0.297043  [   42/   89]
Per-example loss in batch: 0.245582  [   44/   89]
Per-example loss in batch: 0.246228  [   46/   89]
Per-example loss in batch: 0.350586  [   48/   89]
Per-example loss in batch: 0.269349  [   50/   89]
Per-example loss in batch: 0.177585  [   52/   89]
Per-example loss in batch: 0.211142  [   54/   89]
Per-example loss in batch: 0.221288  [   56/   89]
Per-example loss in batch: 0.255081  [   58/   89]
Per-example loss in batch: 0.229426  [   60/   89]
Per-example loss in batch: 0.277050  [   62/   89]
Per-example loss in batch: 0.210477  [   64/   89]
Per-example loss in batch: 0.267376  [   66/   89]
Per-example loss in batch: 0.223798  [   68/   89]
Per-example loss in batch: 0.273632  [   70/   89]
Per-example loss in batch: 0.250215  [   72/   89]
Per-example loss in batch: 0.302898  [   74/   89]
Per-example loss in batch: 0.276831  [   76/   89]
Per-example loss in batch: 0.255931  [   78/   89]
Per-example loss in batch: 0.268210  [   80/   89]
Per-example loss in batch: 0.304589  [   82/   89]
Per-example loss in batch: 0.303783  [   84/   89]
Per-example loss in batch: 0.248673  [   86/   89]
Per-example loss in batch: 0.236739  [   88/   89]
Per-example loss in batch: 0.720712  [   89/   89]
Train Error: Avg loss: 0.25876542
validation Error: 
 Avg loss: 0.27293003 
 F1: 0.487196 
 Precision: 0.561818 
 Recall: 0.430074
 IoU: 0.322049

test Error: 
 Avg loss: 0.25555360 
 F1: 0.518748 
 Precision: 0.603123 
 Recall: 0.455083
 IoU: 0.350209

We have finished training iteration 227
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_225_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.204737  [    2/   89]
Per-example loss in batch: 0.209197  [    4/   89]
Per-example loss in batch: 0.290166  [    6/   89]
Per-example loss in batch: 0.285154  [    8/   89]
Per-example loss in batch: 0.232758  [   10/   89]
Per-example loss in batch: 0.314748  [   12/   89]
Per-example loss in batch: 0.293730  [   14/   89]
Per-example loss in batch: 0.262490  [   16/   89]
Per-example loss in batch: 0.304628  [   18/   89]
Per-example loss in batch: 0.204477  [   20/   89]
Per-example loss in batch: 0.212829  [   22/   89]
Per-example loss in batch: 0.241232  [   24/   89]
Per-example loss in batch: 0.336921  [   26/   89]
Per-example loss in batch: 0.254933  [   28/   89]
Per-example loss in batch: 0.196666  [   30/   89]
Per-example loss in batch: 0.307378  [   32/   89]
Per-example loss in batch: 0.220243  [   34/   89]
Per-example loss in batch: 0.265563  [   36/   89]
Per-example loss in batch: 0.216712  [   38/   89]
Per-example loss in batch: 0.300443  [   40/   89]
Per-example loss in batch: 0.182153  [   42/   89]
Per-example loss in batch: 0.246085  [   44/   89]
Per-example loss in batch: 0.241298  [   46/   89]
Per-example loss in batch: 0.260331  [   48/   89]
Per-example loss in batch: 0.207507  [   50/   89]
Per-example loss in batch: 0.260575  [   52/   89]
Per-example loss in batch: 0.225037  [   54/   89]
Per-example loss in batch: 0.279319  [   56/   89]
Per-example loss in batch: 0.263051  [   58/   89]
Per-example loss in batch: 0.233070  [   60/   89]
Per-example loss in batch: 0.246661  [   62/   89]
Per-example loss in batch: 0.286842  [   64/   89]
Per-example loss in batch: 0.207530  [   66/   89]
Per-example loss in batch: 0.291445  [   68/   89]
Per-example loss in batch: 0.323143  [   70/   89]
Per-example loss in batch: 0.194739  [   72/   89]
Per-example loss in batch: 0.325716  [   74/   89]
Per-example loss in batch: 0.231825  [   76/   89]
Per-example loss in batch: 0.210412  [   78/   89]
Per-example loss in batch: 0.268438  [   80/   89]
Per-example loss in batch: 0.285127  [   82/   89]
Per-example loss in batch: 0.238799  [   84/   89]
Per-example loss in batch: 0.313455  [   86/   89]
Per-example loss in batch: 0.198499  [   88/   89]
Per-example loss in batch: 0.414803  [   89/   89]
Train Error: Avg loss: 0.25580815
validation Error: 
 Avg loss: 0.27560449 
 F1: 0.489899 
 Precision: 0.576818 
 Recall: 0.425745
 IoU: 0.324415

test Error: 
 Avg loss: 0.25433332 
 F1: 0.521896 
 Precision: 0.619046 
 Recall: 0.451102
 IoU: 0.353084

We have finished training iteration 228
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_226_.pth
Per-example loss in batch: 0.247051  [    2/   89]
Per-example loss in batch: 0.274202  [    4/   89]
Per-example loss in batch: 0.284623  [    6/   89]
Per-example loss in batch: 0.262846  [    8/   89]
Per-example loss in batch: 0.270081  [   10/   89]
Per-example loss in batch: 0.221797  [   12/   89]
Per-example loss in batch: 0.267717  [   14/   89]
Per-example loss in batch: 0.256897  [   16/   89]
Per-example loss in batch: 0.214005  [   18/   89]
Per-example loss in batch: 0.203334  [   20/   89]
Per-example loss in batch: 0.247570  [   22/   89]
Per-example loss in batch: 0.231494  [   24/   89]
Per-example loss in batch: 0.277566  [   26/   89]
Per-example loss in batch: 0.200386  [   28/   89]
Per-example loss in batch: 0.195313  [   30/   89]
Per-example loss in batch: 0.212588  [   32/   89]
Per-example loss in batch: 0.262842  [   34/   89]
Per-example loss in batch: 0.246833  [   36/   89]
Per-example loss in batch: 0.247081  [   38/   89]
Per-example loss in batch: 0.273830  [   40/   89]
Per-example loss in batch: 0.336131  [   42/   89]
Per-example loss in batch: 0.282271  [   44/   89]
Per-example loss in batch: 0.235061  [   46/   89]
Per-example loss in batch: 0.252495  [   48/   89]
Per-example loss in batch: 0.311877  [   50/   89]
Per-example loss in batch: 0.237415  [   52/   89]
Per-example loss in batch: 0.297528  [   54/   89]
Per-example loss in batch: 0.288943  [   56/   89]
Per-example loss in batch: 0.269193  [   58/   89]
Per-example loss in batch: 0.290386  [   60/   89]
Per-example loss in batch: 0.241762  [   62/   89]
Per-example loss in batch: 0.227602  [   64/   89]
Per-example loss in batch: 0.231005  [   66/   89]
Per-example loss in batch: 0.279334  [   68/   89]
Per-example loss in batch: 0.246179  [   70/   89]
Per-example loss in batch: 0.229025  [   72/   89]
Per-example loss in batch: 0.262716  [   74/   89]
Per-example loss in batch: 0.267025  [   76/   89]
Per-example loss in batch: 0.280004  [   78/   89]
Per-example loss in batch: 0.274666  [   80/   89]
Per-example loss in batch: 0.209528  [   82/   89]
Per-example loss in batch: 0.264248  [   84/   89]
Per-example loss in batch: 0.248188  [   86/   89]
Per-example loss in batch: 0.199227  [   88/   89]
Per-example loss in batch: 0.408720  [   89/   89]
Train Error: Avg loss: 0.25537587
validation Error: 
 Avg loss: 0.27685624 
 F1: 0.490118 
 Precision: 0.559037 
 Recall: 0.436327
 IoU: 0.324607

test Error: 
 Avg loss: 0.25337380 
 F1: 0.523746 
 Precision: 0.603315 
 Recall: 0.462720
 IoU: 0.354781

We have finished training iteration 229
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_227_.pth
Per-example loss in batch: 0.194906  [    2/   89]
Per-example loss in batch: 0.250856  [    4/   89]
Per-example loss in batch: 0.259524  [    6/   89]
Per-example loss in batch: 0.241250  [    8/   89]
Per-example loss in batch: 0.302824  [   10/   89]
Per-example loss in batch: 0.306173  [   12/   89]
Per-example loss in batch: 0.308923  [   14/   89]
Per-example loss in batch: 0.242899  [   16/   89]
Per-example loss in batch: 0.239188  [   18/   89]
Per-example loss in batch: 0.254629  [   20/   89]
Per-example loss in batch: 0.252861  [   22/   89]
Per-example loss in batch: 0.228178  [   24/   89]
Per-example loss in batch: 0.308751  [   26/   89]
Per-example loss in batch: 0.304527  [   28/   89]
Per-example loss in batch: 0.368391  [   30/   89]
Per-example loss in batch: 0.312779  [   32/   89]
Per-example loss in batch: 0.252201  [   34/   89]
Per-example loss in batch: 0.239605  [   36/   89]
Per-example loss in batch: 0.192074  [   38/   89]
Per-example loss in batch: 0.254380  [   40/   89]
Per-example loss in batch: 0.246636  [   42/   89]
Per-example loss in batch: 0.315479  [   44/   89]
Per-example loss in batch: 0.345626  [   46/   89]
Per-example loss in batch: 0.197627  [   48/   89]
Per-example loss in batch: 0.272594  [   50/   89]
Per-example loss in batch: 0.210683  [   52/   89]
Per-example loss in batch: 0.304413  [   54/   89]
Per-example loss in batch: 0.242808  [   56/   89]
Per-example loss in batch: 0.209399  [   58/   89]
Per-example loss in batch: 0.236285  [   60/   89]
Per-example loss in batch: 0.228413  [   62/   89]
Per-example loss in batch: 0.196892  [   64/   89]
Per-example loss in batch: 0.225480  [   66/   89]
Per-example loss in batch: 0.316567  [   68/   89]
Per-example loss in batch: 0.237393  [   70/   89]
Per-example loss in batch: 0.225397  [   72/   89]
Per-example loss in batch: 0.203792  [   74/   89]
Per-example loss in batch: 0.210612  [   76/   89]
Per-example loss in batch: 0.246402  [   78/   89]
Per-example loss in batch: 0.332651  [   80/   89]
Per-example loss in batch: 0.227530  [   82/   89]
Per-example loss in batch: 0.222298  [   84/   89]
Per-example loss in batch: 0.257805  [   86/   89]
Per-example loss in batch: 0.179589  [   88/   89]
Per-example loss in batch: 0.494651  [   89/   89]
Train Error: Avg loss: 0.25740705
validation Error: 
 Avg loss: 0.27751631 
 F1: 0.489779 
 Precision: 0.567858 
 Recall: 0.430575
 IoU: 0.324309

test Error: 
 Avg loss: 0.25404700 
 F1: 0.522643 
 Precision: 0.611504 
 Recall: 0.456331
 IoU: 0.353769

We have finished training iteration 230
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_228_.pth
Per-example loss in batch: 0.233631  [    2/   89]
Per-example loss in batch: 0.227895  [    4/   89]
Per-example loss in batch: 0.278590  [    6/   89]
Per-example loss in batch: 0.322418  [    8/   89]
Per-example loss in batch: 0.309788  [   10/   89]
Per-example loss in batch: 0.266190  [   12/   89]
Per-example loss in batch: 0.206931  [   14/   89]
Per-example loss in batch: 0.243823  [   16/   89]
Per-example loss in batch: 0.281836  [   18/   89]
Per-example loss in batch: 0.217340  [   20/   89]
Per-example loss in batch: 0.232677  [   22/   89]
Per-example loss in batch: 0.197833  [   24/   89]
Per-example loss in batch: 0.212976  [   26/   89]
Per-example loss in batch: 0.232974  [   28/   89]
Per-example loss in batch: 0.278669  [   30/   89]
Per-example loss in batch: 0.211170  [   32/   89]
Per-example loss in batch: 0.223381  [   34/   89]
Per-example loss in batch: 0.263810  [   36/   89]
Per-example loss in batch: 0.234553  [   38/   89]
Per-example loss in batch: 0.245083  [   40/   89]
Per-example loss in batch: 0.192179  [   42/   89]
Per-example loss in batch: 0.205041  [   44/   89]
Per-example loss in batch: 0.238538  [   46/   89]
Per-example loss in batch: 0.251687  [   48/   89]
Per-example loss in batch: 0.284912  [   50/   89]
Per-example loss in batch: 0.286748  [   52/   89]
Per-example loss in batch: 0.239279  [   54/   89]
Per-example loss in batch: 0.326757  [   56/   89]
Per-example loss in batch: 0.292777  [   58/   89]
Per-example loss in batch: 0.296110  [   60/   89]
Per-example loss in batch: 0.211800  [   62/   89]
Per-example loss in batch: 0.260702  [   64/   89]
Per-example loss in batch: 0.287582  [   66/   89]
Per-example loss in batch: 0.324546  [   68/   89]
Per-example loss in batch: 0.260111  [   70/   89]
Per-example loss in batch: 0.282069  [   72/   89]
Per-example loss in batch: 0.229330  [   74/   89]
Per-example loss in batch: 0.211174  [   76/   89]
Per-example loss in batch: 0.234923  [   78/   89]
Per-example loss in batch: 0.285981  [   80/   89]
Per-example loss in batch: 0.315740  [   82/   89]
Per-example loss in batch: 0.189869  [   84/   89]
Per-example loss in batch: 0.228307  [   86/   89]
Per-example loss in batch: 0.332163  [   88/   89]
Per-example loss in batch: 0.528504  [   89/   89]
Train Error: Avg loss: 0.25739648
validation Error: 
 Avg loss: 0.27542038 
 F1: 0.490082 
 Precision: 0.576398 
 Recall: 0.426251
 IoU: 0.324575

test Error: 
 Avg loss: 0.25533606 
 F1: 0.519163 
 Precision: 0.616903 
 Recall: 0.448158
 IoU: 0.350588

We have finished training iteration 231
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_229_.pth
Per-example loss in batch: 0.211332  [    2/   89]
Per-example loss in batch: 0.205860  [    4/   89]
Per-example loss in batch: 0.241257  [    6/   89]
Per-example loss in batch: 0.258035  [    8/   89]
Per-example loss in batch: 0.312593  [   10/   89]
Per-example loss in batch: 0.340665  [   12/   89]
Per-example loss in batch: 0.224230  [   14/   89]
Per-example loss in batch: 0.224224  [   16/   89]
Per-example loss in batch: 0.298788  [   18/   89]
Per-example loss in batch: 0.304273  [   20/   89]
Per-example loss in batch: 0.241292  [   22/   89]
Per-example loss in batch: 0.246611  [   24/   89]
Per-example loss in batch: 0.245902  [   26/   89]
Per-example loss in batch: 0.218193  [   28/   89]
Per-example loss in batch: 0.240128  [   30/   89]
Per-example loss in batch: 0.196794  [   32/   89]
Per-example loss in batch: 0.226052  [   34/   89]
Per-example loss in batch: 0.288851  [   36/   89]
Per-example loss in batch: 0.237676  [   38/   89]
Per-example loss in batch: 0.228985  [   40/   89]
Per-example loss in batch: 0.305438  [   42/   89]
Per-example loss in batch: 0.242423  [   44/   89]
Per-example loss in batch: 0.225298  [   46/   89]
Per-example loss in batch: 0.192238  [   48/   89]
Per-example loss in batch: 0.266188  [   50/   89]
Per-example loss in batch: 0.303808  [   52/   89]
Per-example loss in batch: 0.229546  [   54/   89]
Per-example loss in batch: 0.313445  [   56/   89]
Per-example loss in batch: 0.266629  [   58/   89]
Per-example loss in batch: 0.253724  [   60/   89]
Per-example loss in batch: 0.262895  [   62/   89]
Per-example loss in batch: 0.213853  [   64/   89]
Per-example loss in batch: 0.231138  [   66/   89]
Per-example loss in batch: 0.286261  [   68/   89]
Per-example loss in batch: 0.238800  [   70/   89]
Per-example loss in batch: 0.300627  [   72/   89]
Per-example loss in batch: 0.235887  [   74/   89]
Per-example loss in batch: 0.179260  [   76/   89]
Per-example loss in batch: 0.237041  [   78/   89]
Per-example loss in batch: 0.201145  [   80/   89]
Per-example loss in batch: 0.231612  [   82/   89]
Per-example loss in batch: 0.219487  [   84/   89]
Per-example loss in batch: 0.273988  [   86/   89]
Per-example loss in batch: 0.231285  [   88/   89]
Per-example loss in batch: 0.714816  [   89/   89]
Train Error: Avg loss: 0.25373398
validation Error: 
 Avg loss: 0.26775528 
 F1: 0.491144 
 Precision: 0.552453 
 Recall: 0.442084
 IoU: 0.325508

test Error: 
 Avg loss: 0.25288214 
 F1: 0.524797 
 Precision: 0.595441 
 Recall: 0.469137
 IoU: 0.355745

We have finished training iteration 232
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_230_.pth
Per-example loss in batch: 0.272301  [    2/   89]
Per-example loss in batch: 0.287589  [    4/   89]
Per-example loss in batch: 0.219289  [    6/   89]
Per-example loss in batch: 0.250726  [    8/   89]
Per-example loss in batch: 0.264847  [   10/   89]
Per-example loss in batch: 0.240293  [   12/   89]
Per-example loss in batch: 0.271562  [   14/   89]
Per-example loss in batch: 0.222242  [   16/   89]
Per-example loss in batch: 0.231191  [   18/   89]
Per-example loss in batch: 0.242473  [   20/   89]
Per-example loss in batch: 0.257330  [   22/   89]
Per-example loss in batch: 0.277398  [   24/   89]
Per-example loss in batch: 0.279772  [   26/   89]
Per-example loss in batch: 0.312977  [   28/   89]
Per-example loss in batch: 0.196772  [   30/   89]
Per-example loss in batch: 0.215035  [   32/   89]
Per-example loss in batch: 0.210351  [   34/   89]
Per-example loss in batch: 0.222946  [   36/   89]
Per-example loss in batch: 0.311840  [   38/   89]
Per-example loss in batch: 0.260373  [   40/   89]
Per-example loss in batch: 0.338284  [   42/   89]
Per-example loss in batch: 0.316362  [   44/   89]
Per-example loss in batch: 0.260159  [   46/   89]
Per-example loss in batch: 0.214331  [   48/   89]
Per-example loss in batch: 0.263102  [   50/   89]
Per-example loss in batch: 0.245831  [   52/   89]
Per-example loss in batch: 0.249623  [   54/   89]
Per-example loss in batch: 0.236076  [   56/   89]
Per-example loss in batch: 0.278530  [   58/   89]
Per-example loss in batch: 0.279304  [   60/   89]
Per-example loss in batch: 0.238999  [   62/   89]
Per-example loss in batch: 0.230133  [   64/   89]
Per-example loss in batch: 0.218293  [   66/   89]
Per-example loss in batch: 0.276532  [   68/   89]
Per-example loss in batch: 0.283863  [   70/   89]
Per-example loss in batch: 0.240765  [   72/   89]
Per-example loss in batch: 0.219349  [   74/   89]
Per-example loss in batch: 0.262599  [   76/   89]
Per-example loss in batch: 0.270722  [   78/   89]
Per-example loss in batch: 0.261315  [   80/   89]
Per-example loss in batch: 0.283206  [   82/   89]
Per-example loss in batch: 0.241352  [   84/   89]
Per-example loss in batch: 0.198171  [   86/   89]
Per-example loss in batch: 0.201339  [   88/   89]
Per-example loss in batch: 0.386875  [   89/   89]
Train Error: Avg loss: 0.25503342
validation Error: 
 Avg loss: 0.27367204 
 F1: 0.491140 
 Precision: 0.554638 
 Recall: 0.440687
 IoU: 0.325504

test Error: 
 Avg loss: 0.25285259 
 F1: 0.524831 
 Precision: 0.596206 
 Recall: 0.468718
 IoU: 0.355777

We have finished training iteration 233
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_231_.pth
Per-example loss in batch: 0.306542  [    2/   89]
Per-example loss in batch: 0.224657  [    4/   89]
Per-example loss in batch: 0.231253  [    6/   89]
Per-example loss in batch: 0.225850  [    8/   89]
Per-example loss in batch: 0.241999  [   10/   89]
Per-example loss in batch: 0.205507  [   12/   89]
Per-example loss in batch: 0.233175  [   14/   89]
Per-example loss in batch: 0.235471  [   16/   89]
Per-example loss in batch: 0.229701  [   18/   89]
Per-example loss in batch: 0.216620  [   20/   89]
Per-example loss in batch: 0.195639  [   22/   89]
Per-example loss in batch: 0.288218  [   24/   89]
Per-example loss in batch: 0.261595  [   26/   89]
Per-example loss in batch: 0.228303  [   28/   89]
Per-example loss in batch: 0.243363  [   30/   89]
Per-example loss in batch: 0.299290  [   32/   89]
Per-example loss in batch: 0.188584  [   34/   89]
Per-example loss in batch: 0.299627  [   36/   89]
Per-example loss in batch: 0.255103  [   38/   89]
Per-example loss in batch: 0.276485  [   40/   89]
Per-example loss in batch: 0.218870  [   42/   89]
Per-example loss in batch: 0.179503  [   44/   89]
Per-example loss in batch: 0.296811  [   46/   89]
Per-example loss in batch: 0.231063  [   48/   89]
Per-example loss in batch: 0.229233  [   50/   89]
Per-example loss in batch: 0.357596  [   52/   89]
Per-example loss in batch: 0.292822  [   54/   89]
Per-example loss in batch: 0.273993  [   56/   89]
Per-example loss in batch: 0.271475  [   58/   89]
Per-example loss in batch: 0.237228  [   60/   89]
Per-example loss in batch: 0.213371  [   62/   89]
Per-example loss in batch: 0.268722  [   64/   89]
Per-example loss in batch: 0.233400  [   66/   89]
Per-example loss in batch: 0.265266  [   68/   89]
Per-example loss in batch: 0.241976  [   70/   89]
Per-example loss in batch: 0.344218  [   72/   89]
Per-example loss in batch: 0.250879  [   74/   89]
Per-example loss in batch: 0.256747  [   76/   89]
Per-example loss in batch: 0.233408  [   78/   89]
Per-example loss in batch: 0.223726  [   80/   89]
Per-example loss in batch: 0.189911  [   82/   89]
Per-example loss in batch: 0.283108  [   84/   89]
Per-example loss in batch: 0.253064  [   86/   89]
Per-example loss in batch: 0.320027  [   88/   89]
Per-example loss in batch: 0.741207  [   89/   89]
Train Error: Avg loss: 0.25671918
validation Error: 
 Avg loss: 0.27373187 
 F1: 0.490742 
 Precision: 0.560974 
 Recall: 0.436139
 IoU: 0.325154

test Error: 
 Avg loss: 0.25307557 
 F1: 0.524178 
 Precision: 0.605330 
 Recall: 0.462213
 IoU: 0.355177

We have finished training iteration 234
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_204_.pth
Per-example loss in batch: 0.245258  [    2/   89]
Per-example loss in batch: 0.264224  [    4/   89]
Per-example loss in batch: 0.204613  [    6/   89]
Per-example loss in batch: 0.237614  [    8/   89]
Per-example loss in batch: 0.261951  [   10/   89]
Per-example loss in batch: 0.287169  [   12/   89]
Per-example loss in batch: 0.272030  [   14/   89]
Per-example loss in batch: 0.234689  [   16/   89]
Per-example loss in batch: 0.230260  [   18/   89]
Per-example loss in batch: 0.183937  [   20/   89]
Per-example loss in batch: 0.205105  [   22/   89]
Per-example loss in batch: 0.253548  [   24/   89]
Per-example loss in batch: 0.290468  [   26/   89]
Per-example loss in batch: 0.210524  [   28/   89]
Per-example loss in batch: 0.246085  [   30/   89]
Per-example loss in batch: 0.280228  [   32/   89]
Per-example loss in batch: 0.216019  [   34/   89]
Per-example loss in batch: 0.281681  [   36/   89]
Per-example loss in batch: 0.220483  [   38/   89]
Per-example loss in batch: 0.367738  [   40/   89]
Per-example loss in batch: 0.205402  [   42/   89]
Per-example loss in batch: 0.206219  [   44/   89]
Per-example loss in batch: 0.251706  [   46/   89]
Per-example loss in batch: 0.362833  [   48/   89]
Per-example loss in batch: 0.291435  [   50/   89]
Per-example loss in batch: 0.277379  [   52/   89]
Per-example loss in batch: 0.209193  [   54/   89]
Per-example loss in batch: 0.286252  [   56/   89]
Per-example loss in batch: 0.221151  [   58/   89]
Per-example loss in batch: 0.287640  [   60/   89]
Per-example loss in batch: 0.258950  [   62/   89]
Per-example loss in batch: 0.257537  [   64/   89]
Per-example loss in batch: 0.304262  [   66/   89]
Per-example loss in batch: 0.319171  [   68/   89]
Per-example loss in batch: 0.227897  [   70/   89]
Per-example loss in batch: 0.264541  [   72/   89]
Per-example loss in batch: 0.294844  [   74/   89]
Per-example loss in batch: 0.195728  [   76/   89]
Per-example loss in batch: 0.228370  [   78/   89]
Per-example loss in batch: 0.224108  [   80/   89]
Per-example loss in batch: 0.218797  [   82/   89]
Per-example loss in batch: 0.266789  [   84/   89]
Per-example loss in batch: 0.277618  [   86/   89]
Per-example loss in batch: 0.305695  [   88/   89]
Per-example loss in batch: 0.555843  [   89/   89]
Train Error: Avg loss: 0.25876552
validation Error: 
 Avg loss: 0.27429197 
 F1: 0.489648 
 Precision: 0.574658 
 Recall: 0.426549
 IoU: 0.324195

test Error: 
 Avg loss: 0.25479604 
 F1: 0.520662 
 Precision: 0.615815 
 Recall: 0.450979
 IoU: 0.351956

We have finished training iteration 235
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_233_.pth
Per-example loss in batch: 0.205111  [    2/   89]
Per-example loss in batch: 0.319715  [    4/   89]
Per-example loss in batch: 0.239237  [    6/   89]
Per-example loss in batch: 0.284438  [    8/   89]
Per-example loss in batch: 0.226420  [   10/   89]
Per-example loss in batch: 0.300729  [   12/   89]
Per-example loss in batch: 0.310532  [   14/   89]
Per-example loss in batch: 0.255992  [   16/   89]
Per-example loss in batch: 0.333435  [   18/   89]
Per-example loss in batch: 0.262979  [   20/   89]
Per-example loss in batch: 0.253400  [   22/   89]
Per-example loss in batch: 0.279731  [   24/   89]
Per-example loss in batch: 0.246838  [   26/   89]
Per-example loss in batch: 0.236391  [   28/   89]
Per-example loss in batch: 0.222359  [   30/   89]
Per-example loss in batch: 0.216187  [   32/   89]
Per-example loss in batch: 0.379262  [   34/   89]
Per-example loss in batch: 0.312567  [   36/   89]
Per-example loss in batch: 0.307493  [   38/   89]
Per-example loss in batch: 0.227214  [   40/   89]
Per-example loss in batch: 0.221198  [   42/   89]
Per-example loss in batch: 0.293214  [   44/   89]
Per-example loss in batch: 0.273768  [   46/   89]
Per-example loss in batch: 0.317086  [   48/   89]
Per-example loss in batch: 0.211032  [   50/   89]
Per-example loss in batch: 0.253523  [   52/   89]
Per-example loss in batch: 0.200909  [   54/   89]
Per-example loss in batch: 0.214988  [   56/   89]
Per-example loss in batch: 0.246457  [   58/   89]
Per-example loss in batch: 0.317793  [   60/   89]
Per-example loss in batch: 0.267799  [   62/   89]
Per-example loss in batch: 0.302354  [   64/   89]
Per-example loss in batch: 0.245683  [   66/   89]
Per-example loss in batch: 0.228980  [   68/   89]
Per-example loss in batch: 0.216936  [   70/   89]
Per-example loss in batch: 0.197812  [   72/   89]
Per-example loss in batch: 0.204242  [   74/   89]
Per-example loss in batch: 0.256573  [   76/   89]
Per-example loss in batch: 0.293993  [   78/   89]
Per-example loss in batch: 0.284145  [   80/   89]
Per-example loss in batch: 0.240253  [   82/   89]
Per-example loss in batch: 0.210620  [   84/   89]
Per-example loss in batch: 0.292008  [   86/   89]
Per-example loss in batch: 0.310027  [   88/   89]
Per-example loss in batch: 0.402095  [   89/   89]
Train Error: Avg loss: 0.26342629
validation Error: 
 Avg loss: 0.28052286 
 F1: 0.484213 
 Precision: 0.528317 
 Recall: 0.446905
 IoU: 0.319447

test Error: 
 Avg loss: 0.25507945 
 F1: 0.518976 
 Precision: 0.570905 
 Recall: 0.475706
 IoU: 0.350417

We have finished training iteration 236
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_234_.pth
Per-example loss in batch: 0.271394  [    2/   89]
Per-example loss in batch: 0.300815  [    4/   89]
Per-example loss in batch: 0.221643  [    6/   89]
Per-example loss in batch: 0.219212  [    8/   89]
Per-example loss in batch: 0.245401  [   10/   89]
Per-example loss in batch: 0.251260  [   12/   89]
Per-example loss in batch: 0.215332  [   14/   89]
Per-example loss in batch: 0.178438  [   16/   89]
Per-example loss in batch: 0.216450  [   18/   89]
Per-example loss in batch: 0.316434  [   20/   89]
Per-example loss in batch: 0.291692  [   22/   89]
Per-example loss in batch: 0.271711  [   24/   89]
Per-example loss in batch: 0.227752  [   26/   89]
Per-example loss in batch: 0.207363  [   28/   89]
Per-example loss in batch: 0.252788  [   30/   89]
Per-example loss in batch: 0.311026  [   32/   89]
Per-example loss in batch: 0.323050  [   34/   89]
Per-example loss in batch: 0.226260  [   36/   89]
Per-example loss in batch: 0.278621  [   38/   89]
Per-example loss in batch: 0.229076  [   40/   89]
Per-example loss in batch: 0.203490  [   42/   89]
Per-example loss in batch: 0.344907  [   44/   89]
Per-example loss in batch: 0.207625  [   46/   89]
Per-example loss in batch: 0.245161  [   48/   89]
Per-example loss in batch: 0.291279  [   50/   89]
Per-example loss in batch: 0.283170  [   52/   89]
Per-example loss in batch: 0.243071  [   54/   89]
Per-example loss in batch: 0.235496  [   56/   89]
Per-example loss in batch: 0.262132  [   58/   89]
Per-example loss in batch: 0.179274  [   60/   89]
Per-example loss in batch: 0.224704  [   62/   89]
Per-example loss in batch: 0.216591  [   64/   89]
Per-example loss in batch: 0.248435  [   66/   89]
Per-example loss in batch: 0.202207  [   68/   89]
Per-example loss in batch: 0.331461  [   70/   89]
Per-example loss in batch: 0.307997  [   72/   89]
Per-example loss in batch: 0.255228  [   74/   89]
Per-example loss in batch: 0.191646  [   76/   89]
Per-example loss in batch: 0.327861  [   78/   89]
Per-example loss in batch: 0.208594  [   80/   89]
Per-example loss in batch: 0.222299  [   82/   89]
Per-example loss in batch: 0.308075  [   84/   89]
Per-example loss in batch: 0.226737  [   86/   89]
Per-example loss in batch: 0.253300  [   88/   89]
Per-example loss in batch: 0.662536  [   89/   89]
Train Error: Avg loss: 0.25635341
validation Error: 
 Avg loss: 0.27248991 
 F1: 0.490884 
 Precision: 0.579472 
 Recall: 0.425791
 IoU: 0.325279

test Error: 
 Avg loss: 0.25364176 
 F1: 0.522849 
 Precision: 0.620607 
 Recall: 0.451698
 IoU: 0.353958

We have finished training iteration 237
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_235_.pth
Per-example loss in batch: 0.253295  [    2/   89]
Per-example loss in batch: 0.232723  [    4/   89]
Per-example loss in batch: 0.222203  [    6/   89]
Per-example loss in batch: 0.217755  [    8/   89]
Per-example loss in batch: 0.231984  [   10/   89]
Per-example loss in batch: 0.206174  [   12/   89]
Per-example loss in batch: 0.235641  [   14/   89]
Per-example loss in batch: 0.238669  [   16/   89]
Per-example loss in batch: 0.275342  [   18/   89]
Per-example loss in batch: 0.270225  [   20/   89]
Per-example loss in batch: 0.312289  [   22/   89]
Per-example loss in batch: 0.303327  [   24/   89]
Per-example loss in batch: 0.278455  [   26/   89]
Per-example loss in batch: 0.220181  [   28/   89]
Per-example loss in batch: 0.296899  [   30/   89]
Per-example loss in batch: 0.224213  [   32/   89]
Per-example loss in batch: 0.299981  [   34/   89]
Per-example loss in batch: 0.229985  [   36/   89]
Per-example loss in batch: 0.238982  [   38/   89]
Per-example loss in batch: 0.344066  [   40/   89]
Per-example loss in batch: 0.254980  [   42/   89]
Per-example loss in batch: 0.298049  [   44/   89]
Per-example loss in batch: 0.280269  [   46/   89]
Per-example loss in batch: 0.297764  [   48/   89]
Per-example loss in batch: 0.305976  [   50/   89]
Per-example loss in batch: 0.197965  [   52/   89]
Per-example loss in batch: 0.281012  [   54/   89]
Per-example loss in batch: 0.185978  [   56/   89]
Per-example loss in batch: 0.223847  [   58/   89]
Per-example loss in batch: 0.203121  [   60/   89]
Per-example loss in batch: 0.269487  [   62/   89]
Per-example loss in batch: 0.292802  [   64/   89]
Per-example loss in batch: 0.198277  [   66/   89]
Per-example loss in batch: 0.191190  [   68/   89]
Per-example loss in batch: 0.283253  [   70/   89]
Per-example loss in batch: 0.279934  [   72/   89]
Per-example loss in batch: 0.319558  [   74/   89]
Per-example loss in batch: 0.294004  [   76/   89]
Per-example loss in batch: 0.307300  [   78/   89]
Per-example loss in batch: 0.266101  [   80/   89]
Per-example loss in batch: 0.303393  [   82/   89]
Per-example loss in batch: 0.207825  [   84/   89]
Per-example loss in batch: 0.279990  [   86/   89]
Per-example loss in batch: 0.228083  [   88/   89]
Per-example loss in batch: 0.597196  [   89/   89]
Train Error: Avg loss: 0.26249770
validation Error: 
 Avg loss: 0.26997821 
 F1: 0.491278 
 Precision: 0.563457 
 Recall: 0.435491
 IoU: 0.325625

test Error: 
 Avg loss: 0.25296961 
 F1: 0.524523 
 Precision: 0.605549 
 Recall: 0.462622
 IoU: 0.355494

We have finished training iteration 238
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_236_.pth
Per-example loss in batch: 0.256337  [    2/   89]
Per-example loss in batch: 0.189489  [    4/   89]
Per-example loss in batch: 0.242604  [    6/   89]
Per-example loss in batch: 0.205698  [    8/   89]
Per-example loss in batch: 0.315388  [   10/   89]
Per-example loss in batch: 0.213029  [   12/   89]
Per-example loss in batch: 0.274990  [   14/   89]
Per-example loss in batch: 0.232312  [   16/   89]
Per-example loss in batch: 0.262347  [   18/   89]
Per-example loss in batch: 0.299150  [   20/   89]
Per-example loss in batch: 0.223058  [   22/   89]
Per-example loss in batch: 0.221112  [   24/   89]
Per-example loss in batch: 0.206120  [   26/   89]
Per-example loss in batch: 0.306617  [   28/   89]
Per-example loss in batch: 0.334159  [   30/   89]
Per-example loss in batch: 0.285394  [   32/   89]
Per-example loss in batch: 0.317488  [   34/   89]
Per-example loss in batch: 0.295508  [   36/   89]
Per-example loss in batch: 0.204336  [   38/   89]
Per-example loss in batch: 0.221683  [   40/   89]
Per-example loss in batch: 0.228665  [   42/   89]
Per-example loss in batch: 0.310777  [   44/   89]
Per-example loss in batch: 0.241341  [   46/   89]
Per-example loss in batch: 0.224411  [   48/   89]
Per-example loss in batch: 0.222171  [   50/   89]
Per-example loss in batch: 0.185055  [   52/   89]
Per-example loss in batch: 0.198759  [   54/   89]
Per-example loss in batch: 0.242380  [   56/   89]
Per-example loss in batch: 0.292767  [   58/   89]
Per-example loss in batch: 0.241283  [   60/   89]
Per-example loss in batch: 0.305196  [   62/   89]
Per-example loss in batch: 0.257240  [   64/   89]
Per-example loss in batch: 0.319354  [   66/   89]
Per-example loss in batch: 0.280555  [   68/   89]
Per-example loss in batch: 0.217409  [   70/   89]
Per-example loss in batch: 0.261280  [   72/   89]
Per-example loss in batch: 0.267460  [   74/   89]
Per-example loss in batch: 0.322406  [   76/   89]
Per-example loss in batch: 0.200151  [   78/   89]
Per-example loss in batch: 0.273388  [   80/   89]
Per-example loss in batch: 0.247855  [   82/   89]
Per-example loss in batch: 0.238493  [   84/   89]
Per-example loss in batch: 0.299702  [   86/   89]
Per-example loss in batch: 0.233974  [   88/   89]
Per-example loss in batch: 0.443900  [   89/   89]
Train Error: Avg loss: 0.25709761
validation Error: 
 Avg loss: 0.27545829 
 F1: 0.490934 
 Precision: 0.555818 
 Recall: 0.439615
 IoU: 0.325323

test Error: 
 Avg loss: 0.25348063 
 F1: 0.522637 
 Precision: 0.597198 
 Recall: 0.464627
 IoU: 0.353763

We have finished training iteration 239
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_237_.pth
Per-example loss in batch: 0.214543  [    2/   89]
Per-example loss in batch: 0.300407  [    4/   89]
Per-example loss in batch: 0.266287  [    6/   89]
Per-example loss in batch: 0.295001  [    8/   89]
Per-example loss in batch: 0.226845  [   10/   89]
Per-example loss in batch: 0.202195  [   12/   89]
Per-example loss in batch: 0.234384  [   14/   89]
Per-example loss in batch: 0.255828  [   16/   89]
Per-example loss in batch: 0.243614  [   18/   89]
Per-example loss in batch: 0.258535  [   20/   89]
Per-example loss in batch: 0.236147  [   22/   89]
Per-example loss in batch: 0.261788  [   24/   89]
Per-example loss in batch: 0.278004  [   26/   89]
Per-example loss in batch: 0.247066  [   28/   89]
Per-example loss in batch: 0.218794  [   30/   89]
Per-example loss in batch: 0.259245  [   32/   89]
Per-example loss in batch: 0.269400  [   34/   89]
Per-example loss in batch: 0.350832  [   36/   89]
Per-example loss in batch: 0.221158  [   38/   89]
Per-example loss in batch: 0.233936  [   40/   89]
Per-example loss in batch: 0.263721  [   42/   89]
Per-example loss in batch: 0.240557  [   44/   89]
Per-example loss in batch: 0.245456  [   46/   89]
Per-example loss in batch: 0.196857  [   48/   89]
Per-example loss in batch: 0.257372  [   50/   89]
Per-example loss in batch: 0.241359  [   52/   89]
Per-example loss in batch: 0.262248  [   54/   89]
Per-example loss in batch: 0.238801  [   56/   89]
Per-example loss in batch: 0.236638  [   58/   89]
Per-example loss in batch: 0.200218  [   60/   89]
Per-example loss in batch: 0.298471  [   62/   89]
Per-example loss in batch: 0.191721  [   64/   89]
Per-example loss in batch: 0.293268  [   66/   89]
Per-example loss in batch: 0.299419  [   68/   89]
Per-example loss in batch: 0.245671  [   70/   89]
Per-example loss in batch: 0.275559  [   72/   89]
Per-example loss in batch: 0.246210  [   74/   89]
Per-example loss in batch: 0.262487  [   76/   89]
Per-example loss in batch: 0.258583  [   78/   89]
Per-example loss in batch: 0.321950  [   80/   89]
Per-example loss in batch: 0.307878  [   82/   89]
Per-example loss in batch: 0.246945  [   84/   89]
Per-example loss in batch: 0.217692  [   86/   89]
Per-example loss in batch: 0.303798  [   88/   89]
Per-example loss in batch: 0.654576  [   89/   89]
Train Error: Avg loss: 0.25964431
validation Error: 
 Avg loss: 0.27439962 
 F1: 0.487474 
 Precision: 0.573459 
 Recall: 0.423912
 IoU: 0.322291

test Error: 
 Avg loss: 0.25576050 
 F1: 0.518678 
 Precision: 0.614980 
 Recall: 0.448453
 IoU: 0.350145

We have finished training iteration 240
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_238_.pth
Per-example loss in batch: 0.262246  [    2/   89]
Per-example loss in batch: 0.310283  [    4/   89]
Per-example loss in batch: 0.192539  [    6/   89]
Per-example loss in batch: 0.281604  [    8/   89]
Per-example loss in batch: 0.181997  [   10/   89]
Per-example loss in batch: 0.310394  [   12/   89]
Per-example loss in batch: 0.327928  [   14/   89]
Per-example loss in batch: 0.216872  [   16/   89]
Per-example loss in batch: 0.227735  [   18/   89]
Per-example loss in batch: 0.294262  [   20/   89]
Per-example loss in batch: 0.230567  [   22/   89]
Per-example loss in batch: 0.233629  [   24/   89]
Per-example loss in batch: 0.260533  [   26/   89]
Per-example loss in batch: 0.261443  [   28/   89]
Per-example loss in batch: 0.258398  [   30/   89]
Per-example loss in batch: 0.214116  [   32/   89]
Per-example loss in batch: 0.349462  [   34/   89]
Per-example loss in batch: 0.202357  [   36/   89]
Per-example loss in batch: 0.210992  [   38/   89]
Per-example loss in batch: 0.238371  [   40/   89]
Per-example loss in batch: 0.261293  [   42/   89]
Per-example loss in batch: 0.308815  [   44/   89]
Per-example loss in batch: 0.214711  [   46/   89]
Per-example loss in batch: 0.256539  [   48/   89]
Per-example loss in batch: 0.309518  [   50/   89]
Per-example loss in batch: 0.301133  [   52/   89]
Per-example loss in batch: 0.255883  [   54/   89]
Per-example loss in batch: 0.301883  [   56/   89]
Per-example loss in batch: 0.193856  [   58/   89]
Per-example loss in batch: 0.214249  [   60/   89]
Per-example loss in batch: 0.248976  [   62/   89]
Per-example loss in batch: 0.241630  [   64/   89]
Per-example loss in batch: 0.226660  [   66/   89]
Per-example loss in batch: 0.316649  [   68/   89]
Per-example loss in batch: 0.264129  [   70/   89]
Per-example loss in batch: 0.251445  [   72/   89]
Per-example loss in batch: 0.270064  [   74/   89]
Per-example loss in batch: 0.208142  [   76/   89]
Per-example loss in batch: 0.240967  [   78/   89]
Per-example loss in batch: 0.235283  [   80/   89]
Per-example loss in batch: 0.259820  [   82/   89]
Per-example loss in batch: 0.227756  [   84/   89]
Per-example loss in batch: 0.300380  [   86/   89]
Per-example loss in batch: 0.232157  [   88/   89]
Per-example loss in batch: 0.493560  [   89/   89]
Train Error: Avg loss: 0.25740333
validation Error: 
 Avg loss: 0.27646389 
 F1: 0.488751 
 Precision: 0.572234 
 Recall: 0.426525
 IoU: 0.323408

test Error: 
 Avg loss: 0.25536822 
 F1: 0.519267 
 Precision: 0.610516 
 Recall: 0.451748
 IoU: 0.350683

We have finished training iteration 241
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_239_.pth
Per-example loss in batch: 0.300737  [    2/   89]
Per-example loss in batch: 0.323643  [    4/   89]
Per-example loss in batch: 0.228677  [    6/   89]
Per-example loss in batch: 0.315689  [    8/   89]
Per-example loss in batch: 0.209050  [   10/   89]
Per-example loss in batch: 0.239541  [   12/   89]
Per-example loss in batch: 0.288676  [   14/   89]
Per-example loss in batch: 0.236543  [   16/   89]
Per-example loss in batch: 0.226890  [   18/   89]
Per-example loss in batch: 0.320170  [   20/   89]
Per-example loss in batch: 0.334699  [   22/   89]
Per-example loss in batch: 0.191946  [   24/   89]
Per-example loss in batch: 0.223477  [   26/   89]
Per-example loss in batch: 0.260080  [   28/   89]
Per-example loss in batch: 0.297002  [   30/   89]
Per-example loss in batch: 0.221449  [   32/   89]
Per-example loss in batch: 0.277436  [   34/   89]
Per-example loss in batch: 0.308166  [   36/   89]
Per-example loss in batch: 0.206328  [   38/   89]
Per-example loss in batch: 0.223011  [   40/   89]
Per-example loss in batch: 0.300572  [   42/   89]
Per-example loss in batch: 0.204485  [   44/   89]
Per-example loss in batch: 0.281101  [   46/   89]
Per-example loss in batch: 0.299189  [   48/   89]
Per-example loss in batch: 0.288649  [   50/   89]
Per-example loss in batch: 0.306926  [   52/   89]
Per-example loss in batch: 0.250643  [   54/   89]
Per-example loss in batch: 0.184548  [   56/   89]
Per-example loss in batch: 0.245412  [   58/   89]
Per-example loss in batch: 0.236629  [   60/   89]
Per-example loss in batch: 0.293818  [   62/   89]
Per-example loss in batch: 0.256876  [   64/   89]
Per-example loss in batch: 0.221561  [   66/   89]
Per-example loss in batch: 0.333225  [   68/   89]
Per-example loss in batch: 0.286497  [   70/   89]
Per-example loss in batch: 0.200372  [   72/   89]
Per-example loss in batch: 0.246746  [   74/   89]
Per-example loss in batch: 0.271625  [   76/   89]
Per-example loss in batch: 0.241456  [   78/   89]
Per-example loss in batch: 0.280602  [   80/   89]
Per-example loss in batch: 0.255780  [   82/   89]
Per-example loss in batch: 0.277347  [   84/   89]
Per-example loss in batch: 0.230179  [   86/   89]
Per-example loss in batch: 0.252834  [   88/   89]
Per-example loss in batch: 0.479255  [   89/   89]
Train Error: Avg loss: 0.26336881
validation Error: 
 Avg loss: 0.28018839 
 F1: 0.490892 
 Precision: 0.586542 
 Recall: 0.422065
 IoU: 0.325287

test Error: 
 Avg loss: 0.25429644 
 F1: 0.521877 
 Precision: 0.626793 
 Recall: 0.447048
 IoU: 0.353068

We have finished training iteration 242
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_240_.pth
Per-example loss in batch: 0.295536  [    2/   89]
Per-example loss in batch: 0.334766  [    4/   89]
Per-example loss in batch: 0.302182  [    6/   89]
Per-example loss in batch: 0.286703  [    8/   89]
Per-example loss in batch: 0.236936  [   10/   89]
Per-example loss in batch: 0.243561  [   12/   89]
Per-example loss in batch: 0.256303  [   14/   89]
Per-example loss in batch: 0.179675  [   16/   89]
Per-example loss in batch: 0.250261  [   18/   89]
Per-example loss in batch: 0.265265  [   20/   89]
Per-example loss in batch: 0.271305  [   22/   89]
Per-example loss in batch: 0.305269  [   24/   89]
Per-example loss in batch: 0.225844  [   26/   89]
Per-example loss in batch: 0.323931  [   28/   89]
Per-example loss in batch: 0.284021  [   30/   89]
Per-example loss in batch: 0.196542  [   32/   89]
Per-example loss in batch: 0.215892  [   34/   89]
Per-example loss in batch: 0.288475  [   36/   89]
Per-example loss in batch: 0.277409  [   38/   89]
Per-example loss in batch: 0.179278  [   40/   89]
Per-example loss in batch: 0.235715  [   42/   89]
Per-example loss in batch: 0.342649  [   44/   89]
Per-example loss in batch: 0.285104  [   46/   89]
Per-example loss in batch: 0.301939  [   48/   89]
Per-example loss in batch: 0.225575  [   50/   89]
Per-example loss in batch: 0.303957  [   52/   89]
Per-example loss in batch: 0.234097  [   54/   89]
Per-example loss in batch: 0.251036  [   56/   89]
Per-example loss in batch: 0.338667  [   58/   89]
Per-example loss in batch: 0.218541  [   60/   89]
Per-example loss in batch: 0.229212  [   62/   89]
Per-example loss in batch: 0.348269  [   64/   89]
Per-example loss in batch: 0.235324  [   66/   89]
Per-example loss in batch: 0.227090  [   68/   89]
Per-example loss in batch: 0.299846  [   70/   89]
Per-example loss in batch: 0.267232  [   72/   89]
Per-example loss in batch: 0.229438  [   74/   89]
Per-example loss in batch: 0.208056  [   76/   89]
Per-example loss in batch: 0.215157  [   78/   89]
Per-example loss in batch: 0.323600  [   80/   89]
Per-example loss in batch: 0.252758  [   82/   89]
Per-example loss in batch: 0.302156  [   84/   89]
Per-example loss in batch: 0.195965  [   86/   89]
Per-example loss in batch: 0.247173  [   88/   89]
Per-example loss in batch: 0.571604  [   89/   89]
Train Error: Avg loss: 0.26569694
validation Error: 
 Avg loss: 0.27254483 
 F1: 0.492607 
 Precision: 0.569063 
 Recall: 0.434262
 IoU: 0.326794

test Error: 
 Avg loss: 0.25291123 
 F1: 0.524185 
 Precision: 0.607656 
 Recall: 0.460876
 IoU: 0.355183

We have finished training iteration 243
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_241_.pth
Per-example loss in batch: 0.235585  [    2/   89]
Per-example loss in batch: 0.275181  [    4/   89]
Per-example loss in batch: 0.255588  [    6/   89]
Per-example loss in batch: 0.323444  [    8/   89]
Per-example loss in batch: 0.227890  [   10/   89]
Per-example loss in batch: 0.249285  [   12/   89]
Per-example loss in batch: 0.246890  [   14/   89]
Per-example loss in batch: 0.275522  [   16/   89]
Per-example loss in batch: 0.284095  [   18/   89]
Per-example loss in batch: 0.316017  [   20/   89]
Per-example loss in batch: 0.225036  [   22/   89]
Per-example loss in batch: 0.228936  [   24/   89]
Per-example loss in batch: 0.266874  [   26/   89]
Per-example loss in batch: 0.227544  [   28/   89]
Per-example loss in batch: 0.229218  [   30/   89]
Per-example loss in batch: 0.257381  [   32/   89]
Per-example loss in batch: 0.260199  [   34/   89]
Per-example loss in batch: 0.223151  [   36/   89]
Per-example loss in batch: 0.320672  [   38/   89]
Per-example loss in batch: 0.247034  [   40/   89]
Per-example loss in batch: 0.259248  [   42/   89]
Per-example loss in batch: 0.250433  [   44/   89]
Per-example loss in batch: 0.202140  [   46/   89]
Per-example loss in batch: 0.237307  [   48/   89]
Per-example loss in batch: 0.288444  [   50/   89]
Per-example loss in batch: 0.276770  [   52/   89]
Per-example loss in batch: 0.248285  [   54/   89]
Per-example loss in batch: 0.299833  [   56/   89]
Per-example loss in batch: 0.269691  [   58/   89]
Per-example loss in batch: 0.210262  [   60/   89]
Per-example loss in batch: 0.305410  [   62/   89]
Per-example loss in batch: 0.219294  [   64/   89]
Per-example loss in batch: 0.205808  [   66/   89]
Per-example loss in batch: 0.227436  [   68/   89]
Per-example loss in batch: 0.231388  [   70/   89]
Per-example loss in batch: 0.184899  [   72/   89]
Per-example loss in batch: 0.255215  [   74/   89]
Per-example loss in batch: 0.273904  [   76/   89]
Per-example loss in batch: 0.345584  [   78/   89]
Per-example loss in batch: 0.239336  [   80/   89]
Per-example loss in batch: 0.323927  [   82/   89]
Per-example loss in batch: 0.272809  [   84/   89]
Per-example loss in batch: 0.220536  [   86/   89]
Per-example loss in batch: 0.213406  [   88/   89]
Per-example loss in batch: 0.387792  [   89/   89]
Train Error: Avg loss: 0.25687194
validation Error: 
 Avg loss: 0.27836937 
 F1: 0.491063 
 Precision: 0.551959 
 Recall: 0.442269
 IoU: 0.325437

test Error: 
 Avg loss: 0.25346915 
 F1: 0.522979 
 Precision: 0.589683 
 Recall: 0.469833
 IoU: 0.354077

We have finished training iteration 244
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_242_.pth
Per-example loss in batch: 0.330583  [    2/   89]
Per-example loss in batch: 0.251795  [    4/   89]
Per-example loss in batch: 0.236278  [    6/   89]
Per-example loss in batch: 0.240303  [    8/   89]
Per-example loss in batch: 0.317965  [   10/   89]
Per-example loss in batch: 0.274554  [   12/   89]
Per-example loss in batch: 0.306484  [   14/   89]
Per-example loss in batch: 0.255522  [   16/   89]
Per-example loss in batch: 0.218212  [   18/   89]
Per-example loss in batch: 0.308923  [   20/   89]
Per-example loss in batch: 0.211098  [   22/   89]
Per-example loss in batch: 0.206319  [   24/   89]
Per-example loss in batch: 0.302769  [   26/   89]
Per-example loss in batch: 0.328861  [   28/   89]
Per-example loss in batch: 0.204454  [   30/   89]
Per-example loss in batch: 0.206491  [   32/   89]
Per-example loss in batch: 0.248355  [   34/   89]
Per-example loss in batch: 0.266905  [   36/   89]
Per-example loss in batch: 0.301613  [   38/   89]
Per-example loss in batch: 0.259227  [   40/   89]
Per-example loss in batch: 0.263120  [   42/   89]
Per-example loss in batch: 0.229029  [   44/   89]
Per-example loss in batch: 0.273118  [   46/   89]
Per-example loss in batch: 0.258259  [   48/   89]
Per-example loss in batch: 0.239016  [   50/   89]
Per-example loss in batch: 0.262553  [   52/   89]
Per-example loss in batch: 0.292478  [   54/   89]
Per-example loss in batch: 0.271725  [   56/   89]
Per-example loss in batch: 0.253662  [   58/   89]
Per-example loss in batch: 0.216100  [   60/   89]
Per-example loss in batch: 0.201011  [   62/   89]
Per-example loss in batch: 0.229861  [   64/   89]
Per-example loss in batch: 0.248735  [   66/   89]
Per-example loss in batch: 0.235163  [   68/   89]
Per-example loss in batch: 0.236201  [   70/   89]
Per-example loss in batch: 0.264973  [   72/   89]
Per-example loss in batch: 0.242411  [   74/   89]
Per-example loss in batch: 0.292099  [   76/   89]
Per-example loss in batch: 0.240354  [   78/   89]
Per-example loss in batch: 0.235618  [   80/   89]
Per-example loss in batch: 0.273078  [   82/   89]
Per-example loss in batch: 0.335788  [   84/   89]
Per-example loss in batch: 0.258543  [   86/   89]
Per-example loss in batch: 0.223705  [   88/   89]
Per-example loss in batch: 0.431104  [   89/   89]
Train Error: Avg loss: 0.25997445
validation Error: 
 Avg loss: 0.27539749 
 F1: 0.490632 
 Precision: 0.575198 
 Recall: 0.427745
 IoU: 0.325058

test Error: 
 Avg loss: 0.25321589 
 F1: 0.523819 
 Precision: 0.615644 
 Recall: 0.455831
 IoU: 0.354847

We have finished training iteration 245
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_243_.pth
Per-example loss in batch: 0.302285  [    2/   89]
Per-example loss in batch: 0.226932  [    4/   89]
Per-example loss in batch: 0.195228  [    6/   89]
Per-example loss in batch: 0.180553  [    8/   89]
Per-example loss in batch: 0.275257  [   10/   89]
Per-example loss in batch: 0.243431  [   12/   89]
Per-example loss in batch: 0.314231  [   14/   89]
Per-example loss in batch: 0.202439  [   16/   89]
Per-example loss in batch: 0.268365  [   18/   89]
Per-example loss in batch: 0.238538  [   20/   89]
Per-example loss in batch: 0.269742  [   22/   89]
Per-example loss in batch: 0.270126  [   24/   89]
Per-example loss in batch: 0.251585  [   26/   89]
Per-example loss in batch: 0.246487  [   28/   89]
Per-example loss in batch: 0.237823  [   30/   89]
Per-example loss in batch: 0.226428  [   32/   89]
Per-example loss in batch: 0.197719  [   34/   89]
Per-example loss in batch: 0.261161  [   36/   89]
Per-example loss in batch: 0.264696  [   38/   89]
Per-example loss in batch: 0.200390  [   40/   89]
Per-example loss in batch: 0.311023  [   42/   89]
Per-example loss in batch: 0.280483  [   44/   89]
Per-example loss in batch: 0.276448  [   46/   89]
Per-example loss in batch: 0.267600  [   48/   89]
Per-example loss in batch: 0.232613  [   50/   89]
Per-example loss in batch: 0.259708  [   52/   89]
Per-example loss in batch: 0.332014  [   54/   89]
Per-example loss in batch: 0.313378  [   56/   89]
Per-example loss in batch: 0.298149  [   58/   89]
Per-example loss in batch: 0.299366  [   60/   89]
Per-example loss in batch: 0.292828  [   62/   89]
Per-example loss in batch: 0.246816  [   64/   89]
Per-example loss in batch: 0.250554  [   66/   89]
Per-example loss in batch: 0.215533  [   68/   89]
Per-example loss in batch: 0.219758  [   70/   89]
Per-example loss in batch: 0.224842  [   72/   89]
Per-example loss in batch: 0.225807  [   74/   89]
Per-example loss in batch: 0.270357  [   76/   89]
Per-example loss in batch: 0.233850  [   78/   89]
Per-example loss in batch: 0.288590  [   80/   89]
Per-example loss in batch: 0.255085  [   82/   89]
Per-example loss in batch: 0.241181  [   84/   89]
Per-example loss in batch: 0.255036  [   86/   89]
Per-example loss in batch: 0.207235  [   88/   89]
Per-example loss in batch: 0.401938  [   89/   89]
Train Error: Avg loss: 0.25556491
validation Error: 
 Avg loss: 0.27372832 
 F1: 0.492385 
 Precision: 0.555936 
 Recall: 0.441873
 IoU: 0.326599

test Error: 
 Avg loss: 0.25246360 
 F1: 0.525400 
 Precision: 0.598764 
 Recall: 0.468052
 IoU: 0.356300

We have finished training iteration 246
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_244_.pth
Per-example loss in batch: 0.274782  [    2/   89]
Per-example loss in batch: 0.276776  [    4/   89]
Per-example loss in batch: 0.234689  [    6/   89]
Per-example loss in batch: 0.230900  [    8/   89]
Per-example loss in batch: 0.283275  [   10/   89]
Per-example loss in batch: 0.243303  [   12/   89]
Per-example loss in batch: 0.238686  [   14/   89]
Per-example loss in batch: 0.204959  [   16/   89]
Per-example loss in batch: 0.247856  [   18/   89]
Per-example loss in batch: 0.232660  [   20/   89]
Per-example loss in batch: 0.285102  [   22/   89]
Per-example loss in batch: 0.244876  [   24/   89]
Per-example loss in batch: 0.232811  [   26/   89]
Per-example loss in batch: 0.226243  [   28/   89]
Per-example loss in batch: 0.228393  [   30/   89]
Per-example loss in batch: 0.250208  [   32/   89]
Per-example loss in batch: 0.311271  [   34/   89]
Per-example loss in batch: 0.244587  [   36/   89]
Per-example loss in batch: 0.306574  [   38/   89]
Per-example loss in batch: 0.310793  [   40/   89]
Per-example loss in batch: 0.239653  [   42/   89]
Per-example loss in batch: 0.208682  [   44/   89]
Per-example loss in batch: 0.240128  [   46/   89]
Per-example loss in batch: 0.237392  [   48/   89]
Per-example loss in batch: 0.324784  [   50/   89]
Per-example loss in batch: 0.271294  [   52/   89]
Per-example loss in batch: 0.225702  [   54/   89]
Per-example loss in batch: 0.220696  [   56/   89]
Per-example loss in batch: 0.291794  [   58/   89]
Per-example loss in batch: 0.319461  [   60/   89]
Per-example loss in batch: 0.220639  [   62/   89]
Per-example loss in batch: 0.305604  [   64/   89]
Per-example loss in batch: 0.241704  [   66/   89]
Per-example loss in batch: 0.244711  [   68/   89]
Per-example loss in batch: 0.272039  [   70/   89]
Per-example loss in batch: 0.238791  [   72/   89]
Per-example loss in batch: 0.215484  [   74/   89]
Per-example loss in batch: 0.261941  [   76/   89]
Per-example loss in batch: 0.185641  [   78/   89]
Per-example loss in batch: 0.307656  [   80/   89]
Per-example loss in batch: 0.291059  [   82/   89]
Per-example loss in batch: 0.304810  [   84/   89]
Per-example loss in batch: 0.250617  [   86/   89]
Per-example loss in batch: 0.257182  [   88/   89]
Per-example loss in batch: 0.504865  [   89/   89]
Train Error: Avg loss: 0.25929527
validation Error: 
 Avg loss: 0.27439128 
 F1: 0.487588 
 Precision: 0.569409 
 Recall: 0.426328
 IoU: 0.322391

test Error: 
 Avg loss: 0.25626580 
 F1: 0.517172 
 Precision: 0.606607 
 Recall: 0.450720
 IoU: 0.348774

We have finished training iteration 247
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_245_.pth
Per-example loss in batch: 0.320191  [    2/   89]
Per-example loss in batch: 0.214863  [    4/   89]
Per-example loss in batch: 0.209184  [    6/   89]
Per-example loss in batch: 0.210102  [    8/   89]
Per-example loss in batch: 0.245832  [   10/   89]
Per-example loss in batch: 0.236263  [   12/   89]
Per-example loss in batch: 0.363894  [   14/   89]
Per-example loss in batch: 0.277781  [   16/   89]
Per-example loss in batch: 0.202881  [   18/   89]
Per-example loss in batch: 0.250509  [   20/   89]
Per-example loss in batch: 0.336799  [   22/   89]
Per-example loss in batch: 0.235076  [   24/   89]
Per-example loss in batch: 0.262360  [   26/   89]
Per-example loss in batch: 0.237208  [   28/   89]
Per-example loss in batch: 0.348913  [   30/   89]
Per-example loss in batch: 0.314762  [   32/   89]
Per-example loss in batch: 0.188465  [   34/   89]
Per-example loss in batch: 0.261403  [   36/   89]
Per-example loss in batch: 0.278739  [   38/   89]
Per-example loss in batch: 0.206758  [   40/   89]
Per-example loss in batch: 0.224381  [   42/   89]
Per-example loss in batch: 0.261095  [   44/   89]
Per-example loss in batch: 0.312054  [   46/   89]
Per-example loss in batch: 0.312629  [   48/   89]
Per-example loss in batch: 0.241889  [   50/   89]
Per-example loss in batch: 0.214040  [   52/   89]
Per-example loss in batch: 0.260452  [   54/   89]
Per-example loss in batch: 0.284436  [   56/   89]
Per-example loss in batch: 0.229233  [   58/   89]
Per-example loss in batch: 0.229458  [   60/   89]
Per-example loss in batch: 0.264203  [   62/   89]
Per-example loss in batch: 0.252879  [   64/   89]
Per-example loss in batch: 0.314141  [   66/   89]
Per-example loss in batch: 0.229440  [   68/   89]
Per-example loss in batch: 0.287408  [   70/   89]
Per-example loss in batch: 0.267457  [   72/   89]
Per-example loss in batch: 0.196591  [   74/   89]
Per-example loss in batch: 0.278025  [   76/   89]
Per-example loss in batch: 0.209370  [   78/   89]
Per-example loss in batch: 0.226801  [   80/   89]
Per-example loss in batch: 0.283932  [   82/   89]
Per-example loss in batch: 0.228881  [   84/   89]
Per-example loss in batch: 0.303715  [   86/   89]
Per-example loss in batch: 0.225286  [   88/   89]
Per-example loss in batch: 0.665143  [   89/   89]
Train Error: Avg loss: 0.26230009
validation Error: 
 Avg loss: 0.27534269 
 F1: 0.490495 
 Precision: 0.561847 
 Recall: 0.435223
 IoU: 0.324937

test Error: 
 Avg loss: 0.25388681 
 F1: 0.522495 
 Precision: 0.602108 
 Recall: 0.461476
 IoU: 0.353633

We have finished training iteration 248
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_246_.pth
Per-example loss in batch: 0.273332  [    2/   89]
Per-example loss in batch: 0.216237  [    4/   89]
Per-example loss in batch: 0.211749  [    6/   89]
Per-example loss in batch: 0.245696  [    8/   89]
Per-example loss in batch: 0.252029  [   10/   89]
Per-example loss in batch: 0.245391  [   12/   89]
Per-example loss in batch: 0.236010  [   14/   89]
Per-example loss in batch: 0.255928  [   16/   89]
Per-example loss in batch: 0.210526  [   18/   89]
Per-example loss in batch: 0.250447  [   20/   89]
Per-example loss in batch: 0.343169  [   22/   89]
Per-example loss in batch: 0.338292  [   24/   89]
Per-example loss in batch: 0.247759  [   26/   89]
Per-example loss in batch: 0.264139  [   28/   89]
Per-example loss in batch: 0.229390  [   30/   89]
Per-example loss in batch: 0.221163  [   32/   89]
Per-example loss in batch: 0.247196  [   34/   89]
Per-example loss in batch: 0.290112  [   36/   89]
Per-example loss in batch: 0.339754  [   38/   89]
Per-example loss in batch: 0.194077  [   40/   89]
Per-example loss in batch: 0.227504  [   42/   89]
Per-example loss in batch: 0.212208  [   44/   89]
Per-example loss in batch: 0.236779  [   46/   89]
Per-example loss in batch: 0.240501  [   48/   89]
Per-example loss in batch: 0.294119  [   50/   89]
Per-example loss in batch: 0.234384  [   52/   89]
Per-example loss in batch: 0.305902  [   54/   89]
Per-example loss in batch: 0.297721  [   56/   89]
Per-example loss in batch: 0.269691  [   58/   89]
Per-example loss in batch: 0.221353  [   60/   89]
Per-example loss in batch: 0.310821  [   62/   89]
Per-example loss in batch: 0.308653  [   64/   89]
Per-example loss in batch: 0.209399  [   66/   89]
Per-example loss in batch: 0.200973  [   68/   89]
Per-example loss in batch: 0.273563  [   70/   89]
Per-example loss in batch: 0.308635  [   72/   89]
Per-example loss in batch: 0.240289  [   74/   89]
Per-example loss in batch: 0.294144  [   76/   89]
Per-example loss in batch: 0.232313  [   78/   89]
Per-example loss in batch: 0.291855  [   80/   89]
Per-example loss in batch: 0.274243  [   82/   89]
Per-example loss in batch: 0.218945  [   84/   89]
Per-example loss in batch: 0.296408  [   86/   89]
Per-example loss in batch: 0.226697  [   88/   89]
Per-example loss in batch: 0.468365  [   89/   89]
Train Error: Avg loss: 0.26008260
validation Error: 
 Avg loss: 0.27921789 
 F1: 0.490786 
 Precision: 0.571360 
 Recall: 0.430129
 IoU: 0.325193

test Error: 
 Avg loss: 0.25366098 
 F1: 0.522836 
 Precision: 0.612759 
 Recall: 0.455929
 IoU: 0.353946

We have finished training iteration 249
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_247_.pth
Per-example loss in batch: 0.206385  [    2/   89]
Per-example loss in batch: 0.333200  [    4/   89]
Per-example loss in batch: 0.304161  [    6/   89]
Per-example loss in batch: 0.206085  [    8/   89]
Per-example loss in batch: 0.201735  [   10/   89]
Per-example loss in batch: 0.198454  [   12/   89]
Per-example loss in batch: 0.267768  [   14/   89]
Per-example loss in batch: 0.195366  [   16/   89]
Per-example loss in batch: 0.226153  [   18/   89]
Per-example loss in batch: 0.257876  [   20/   89]
Per-example loss in batch: 0.291149  [   22/   89]
Per-example loss in batch: 0.251249  [   24/   89]
Per-example loss in batch: 0.265602  [   26/   89]
Per-example loss in batch: 0.263595  [   28/   89]
Per-example loss in batch: 0.264781  [   30/   89]
Per-example loss in batch: 0.258199  [   32/   89]
Per-example loss in batch: 0.344790  [   34/   89]
Per-example loss in batch: 0.213411  [   36/   89]
Per-example loss in batch: 0.226816  [   38/   89]
Per-example loss in batch: 0.247177  [   40/   89]
Per-example loss in batch: 0.224375  [   42/   89]
Per-example loss in batch: 0.206502  [   44/   89]
Per-example loss in batch: 0.254438  [   46/   89]
Per-example loss in batch: 0.303909  [   48/   89]
Per-example loss in batch: 0.198762  [   50/   89]
Per-example loss in batch: 0.219496  [   52/   89]
Per-example loss in batch: 0.211258  [   54/   89]
Per-example loss in batch: 0.246140  [   56/   89]
Per-example loss in batch: 0.239014  [   58/   89]
Per-example loss in batch: 0.211440  [   60/   89]
Per-example loss in batch: 0.323678  [   62/   89]
Per-example loss in batch: 0.269089  [   64/   89]
Per-example loss in batch: 0.214767  [   66/   89]
Per-example loss in batch: 0.321462  [   68/   89]
Per-example loss in batch: 0.269406  [   70/   89]
Per-example loss in batch: 0.329297  [   72/   89]
Per-example loss in batch: 0.240731  [   74/   89]
Per-example loss in batch: 0.206219  [   76/   89]
Per-example loss in batch: 0.205822  [   78/   89]
Per-example loss in batch: 0.288457  [   80/   89]
Per-example loss in batch: 0.277883  [   82/   89]
Per-example loss in batch: 0.231395  [   84/   89]
Per-example loss in batch: 0.253345  [   86/   89]
Per-example loss in batch: 0.356285  [   88/   89]
Per-example loss in batch: 0.656519  [   89/   89]
Train Error: Avg loss: 0.25742428
validation Error: 
 Avg loss: 0.27595093 
 F1: 0.484435 
 Precision: 0.573905 
 Recall: 0.419098
 IoU: 0.319639

test Error: 
 Avg loss: 0.25794147 
 F1: 0.513426 
 Precision: 0.611486 
 Recall: 0.442469
 IoU: 0.345375

We have finished training iteration 250
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_248_.pth
Per-example loss in batch: 0.287942  [    2/   89]
Per-example loss in batch: 0.274749  [    4/   89]
Per-example loss in batch: 0.296608  [    6/   89]
Per-example loss in batch: 0.218204  [    8/   89]
Per-example loss in batch: 0.205195  [   10/   89]
Per-example loss in batch: 0.268773  [   12/   89]
Per-example loss in batch: 0.199287  [   14/   89]
Per-example loss in batch: 0.300377  [   16/   89]
Per-example loss in batch: 0.274468  [   18/   89]
Per-example loss in batch: 0.244863  [   20/   89]
Per-example loss in batch: 0.268119  [   22/   89]
Per-example loss in batch: 0.207506  [   24/   89]
Per-example loss in batch: 0.229215  [   26/   89]
Per-example loss in batch: 0.222549  [   28/   89]
Per-example loss in batch: 0.254005  [   30/   89]
Per-example loss in batch: 0.246653  [   32/   89]
Per-example loss in batch: 0.194947  [   34/   89]
Per-example loss in batch: 0.228739  [   36/   89]
Per-example loss in batch: 0.315420  [   38/   89]
Per-example loss in batch: 0.247054  [   40/   89]
Per-example loss in batch: 0.256907  [   42/   89]
Per-example loss in batch: 0.332151  [   44/   89]
Per-example loss in batch: 0.260464  [   46/   89]
Per-example loss in batch: 0.238089  [   48/   89]
Per-example loss in batch: 0.196630  [   50/   89]
Per-example loss in batch: 0.223250  [   52/   89]
Per-example loss in batch: 0.239241  [   54/   89]
Per-example loss in batch: 0.219229  [   56/   89]
Per-example loss in batch: 0.322869  [   58/   89]
Per-example loss in batch: 0.240838  [   60/   89]
Per-example loss in batch: 0.215472  [   62/   89]
Per-example loss in batch: 0.317202  [   64/   89]
Per-example loss in batch: 0.226209  [   66/   89]
Per-example loss in batch: 0.234766  [   68/   89]
Per-example loss in batch: 0.234636  [   70/   89]
Per-example loss in batch: 0.242441  [   72/   89]
Per-example loss in batch: 0.254774  [   74/   89]
Per-example loss in batch: 0.261431  [   76/   89]
Per-example loss in batch: 0.285124  [   78/   89]
Per-example loss in batch: 0.292831  [   80/   89]
Per-example loss in batch: 0.252884  [   82/   89]
Per-example loss in batch: 0.276185  [   84/   89]
Per-example loss in batch: 0.202574  [   86/   89]
Per-example loss in batch: 0.240870  [   88/   89]
Per-example loss in batch: 0.376675  [   89/   89]
Train Error: Avg loss: 0.25258599
validation Error: 
 Avg loss: 0.27751693 
 F1: 0.490897 
 Precision: 0.546864 
 Recall: 0.445322
 IoU: 0.325290

test Error: 
 Avg loss: 0.25202060 
 F1: 0.526514 
 Precision: 0.594789 
 Recall: 0.472299
 IoU: 0.357325

We have finished training iteration 251
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_249_.pth
Per-example loss in batch: 0.235042  [    2/   89]
Per-example loss in batch: 0.230857  [    4/   89]
Per-example loss in batch: 0.288426  [    6/   89]
Per-example loss in batch: 0.243339  [    8/   89]
Per-example loss in batch: 0.273192  [   10/   89]
Per-example loss in batch: 0.332521  [   12/   89]
Per-example loss in batch: 0.232387  [   14/   89]
Per-example loss in batch: 0.215280  [   16/   89]
Per-example loss in batch: 0.225535  [   18/   89]
Per-example loss in batch: 0.254017  [   20/   89]
Per-example loss in batch: 0.238075  [   22/   89]
Per-example loss in batch: 0.315643  [   24/   89]
Per-example loss in batch: 0.261926  [   26/   89]
Per-example loss in batch: 0.216643  [   28/   89]
Per-example loss in batch: 0.288223  [   30/   89]
Per-example loss in batch: 0.209617  [   32/   89]
Per-example loss in batch: 0.228820  [   34/   89]
Per-example loss in batch: 0.222343  [   36/   89]
Per-example loss in batch: 0.221864  [   38/   89]
Per-example loss in batch: 0.214230  [   40/   89]
Per-example loss in batch: 0.258331  [   42/   89]
Per-example loss in batch: 0.249444  [   44/   89]
Per-example loss in batch: 0.247196  [   46/   89]
Per-example loss in batch: 0.256533  [   48/   89]
Per-example loss in batch: 0.240773  [   50/   89]
Per-example loss in batch: 0.295255  [   52/   89]
Per-example loss in batch: 0.225455  [   54/   89]
Per-example loss in batch: 0.221140  [   56/   89]
Per-example loss in batch: 0.218315  [   58/   89]
Per-example loss in batch: 0.324259  [   60/   89]
Per-example loss in batch: 0.296206  [   62/   89]
Per-example loss in batch: 0.186961  [   64/   89]
Per-example loss in batch: 0.221180  [   66/   89]
Per-example loss in batch: 0.317201  [   68/   89]
Per-example loss in batch: 0.286618  [   70/   89]
Per-example loss in batch: 0.255690  [   72/   89]
Per-example loss in batch: 0.267809  [   74/   89]
Per-example loss in batch: 0.244863  [   76/   89]
Per-example loss in batch: 0.279114  [   78/   89]
Per-example loss in batch: 0.237704  [   80/   89]
Per-example loss in batch: 0.281971  [   82/   89]
Per-example loss in batch: 0.306569  [   84/   89]
Per-example loss in batch: 0.217587  [   86/   89]
Per-example loss in batch: 0.191967  [   88/   89]
Per-example loss in batch: 0.685622  [   89/   89]
Train Error: Avg loss: 0.25660515
validation Error: 
 Avg loss: 0.27555487 
 F1: 0.489447 
 Precision: 0.549419 
 Recall: 0.441279
 IoU: 0.324018

test Error: 
 Avg loss: 0.25393481 
 F1: 0.522189 
 Precision: 0.590325 
 Recall: 0.468153
 IoU: 0.353353

We have finished training iteration 252
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_250_.pth
Per-example loss in batch: 0.229022  [    2/   89]
Per-example loss in batch: 0.247726  [    4/   89]
Per-example loss in batch: 0.274443  [    6/   89]
Per-example loss in batch: 0.173160  [    8/   89]
Per-example loss in batch: 0.212237  [   10/   89]
Per-example loss in batch: 0.212544  [   12/   89]
Per-example loss in batch: 0.319143  [   14/   89]
Per-example loss in batch: 0.294995  [   16/   89]
Per-example loss in batch: 0.288550  [   18/   89]
Per-example loss in batch: 0.212013  [   20/   89]
Per-example loss in batch: 0.221678  [   22/   89]
Per-example loss in batch: 0.200966  [   24/   89]
Per-example loss in batch: 0.213681  [   26/   89]
Per-example loss in batch: 0.294098  [   28/   89]
Per-example loss in batch: 0.290270  [   30/   89]
Per-example loss in batch: 0.280825  [   32/   89]
Per-example loss in batch: 0.264717  [   34/   89]
Per-example loss in batch: 0.231546  [   36/   89]
Per-example loss in batch: 0.287213  [   38/   89]
Per-example loss in batch: 0.222868  [   40/   89]
Per-example loss in batch: 0.218797  [   42/   89]
Per-example loss in batch: 0.199949  [   44/   89]
Per-example loss in batch: 0.215372  [   46/   89]
Per-example loss in batch: 0.308349  [   48/   89]
Per-example loss in batch: 0.186116  [   50/   89]
Per-example loss in batch: 0.211203  [   52/   89]
Per-example loss in batch: 0.251518  [   54/   89]
Per-example loss in batch: 0.242172  [   56/   89]
Per-example loss in batch: 0.370019  [   58/   89]
Per-example loss in batch: 0.264120  [   60/   89]
Per-example loss in batch: 0.249023  [   62/   89]
Per-example loss in batch: 0.223393  [   64/   89]
Per-example loss in batch: 0.282475  [   66/   89]
Per-example loss in batch: 0.263736  [   68/   89]
Per-example loss in batch: 0.312079  [   70/   89]
Per-example loss in batch: 0.264575  [   72/   89]
Per-example loss in batch: 0.250397  [   74/   89]
Per-example loss in batch: 0.351757  [   76/   89]
Per-example loss in batch: 0.249626  [   78/   89]
Per-example loss in batch: 0.228566  [   80/   89]
Per-example loss in batch: 0.268934  [   82/   89]
Per-example loss in batch: 0.274410  [   84/   89]
Per-example loss in batch: 0.279082  [   86/   89]
Per-example loss in batch: 0.214229  [   88/   89]
Per-example loss in batch: 0.795326  [   89/   89]
Train Error: Avg loss: 0.25953384
validation Error: 
 Avg loss: 0.27889058 
 F1: 0.486260 
 Precision: 0.544254 
 Recall: 0.439435
 IoU: 0.321231

test Error: 
 Avg loss: 0.25416381 
 F1: 0.521821 
 Precision: 0.586998 
 Recall: 0.469670
 IoU: 0.353016

We have finished training iteration 253
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_251_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.200903  [    2/   89]
Per-example loss in batch: 0.253100  [    4/   89]
Per-example loss in batch: 0.263418  [    6/   89]
Per-example loss in batch: 0.194994  [    8/   89]
Per-example loss in batch: 0.249137  [   10/   89]
Per-example loss in batch: 0.274045  [   12/   89]
Per-example loss in batch: 0.214209  [   14/   89]
Per-example loss in batch: 0.295856  [   16/   89]
Per-example loss in batch: 0.192451  [   18/   89]
Per-example loss in batch: 0.272542  [   20/   89]
Per-example loss in batch: 0.311737  [   22/   89]
Per-example loss in batch: 0.205113  [   24/   89]
Per-example loss in batch: 0.257281  [   26/   89]
Per-example loss in batch: 0.276349  [   28/   89]
Per-example loss in batch: 0.290952  [   30/   89]
Per-example loss in batch: 0.321105  [   32/   89]
Per-example loss in batch: 0.296672  [   34/   89]
Per-example loss in batch: 0.248239  [   36/   89]
Per-example loss in batch: 0.315506  [   38/   89]
Per-example loss in batch: 0.241254  [   40/   89]
Per-example loss in batch: 0.206009  [   42/   89]
Per-example loss in batch: 0.219357  [   44/   89]
Per-example loss in batch: 0.315238  [   46/   89]
Per-example loss in batch: 0.242803  [   48/   89]
Per-example loss in batch: 0.199761  [   50/   89]
Per-example loss in batch: 0.211833  [   52/   89]
Per-example loss in batch: 0.225420  [   54/   89]
Per-example loss in batch: 0.248130  [   56/   89]
Per-example loss in batch: 0.280596  [   58/   89]
Per-example loss in batch: 0.228192  [   60/   89]
Per-example loss in batch: 0.229764  [   62/   89]
Per-example loss in batch: 0.272872  [   64/   89]
Per-example loss in batch: 0.295339  [   66/   89]
Per-example loss in batch: 0.250773  [   68/   89]
Per-example loss in batch: 0.215326  [   70/   89]
Per-example loss in batch: 0.353058  [   72/   89]
Per-example loss in batch: 0.193934  [   74/   89]
Per-example loss in batch: 0.307143  [   76/   89]
Per-example loss in batch: 0.235865  [   78/   89]
Per-example loss in batch: 0.232607  [   80/   89]
Per-example loss in batch: 0.224911  [   82/   89]
Per-example loss in batch: 0.256593  [   84/   89]
Per-example loss in batch: 0.214454  [   86/   89]
Per-example loss in batch: 0.295883  [   88/   89]
Per-example loss in batch: 0.459027  [   89/   89]
Train Error: Avg loss: 0.25528623
validation Error: 
 Avg loss: 0.28146817 
 F1: 0.488047 
 Precision: 0.537058 
 Recall: 0.447233
 IoU: 0.322792

test Error: 
 Avg loss: 0.25373095 
 F1: 0.523057 
 Precision: 0.580987 
 Recall: 0.475632
 IoU: 0.354148

We have finished training iteration 254
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_252_.pth
Per-example loss in batch: 0.319997  [    2/   89]
Per-example loss in batch: 0.220754  [    4/   89]
Per-example loss in batch: 0.270513  [    6/   89]
Per-example loss in batch: 0.297548  [    8/   89]
Per-example loss in batch: 0.249027  [   10/   89]
Per-example loss in batch: 0.193086  [   12/   89]
Per-example loss in batch: 0.185381  [   14/   89]
Per-example loss in batch: 0.208831  [   16/   89]
Per-example loss in batch: 0.260474  [   18/   89]
Per-example loss in batch: 0.197824  [   20/   89]
Per-example loss in batch: 0.284721  [   22/   89]
Per-example loss in batch: 0.209863  [   24/   89]
Per-example loss in batch: 0.277208  [   26/   89]
Per-example loss in batch: 0.234906  [   28/   89]
Per-example loss in batch: 0.257032  [   30/   89]
Per-example loss in batch: 0.322855  [   32/   89]
Per-example loss in batch: 0.252979  [   34/   89]
Per-example loss in batch: 0.304255  [   36/   89]
Per-example loss in batch: 0.203353  [   38/   89]
Per-example loss in batch: 0.308146  [   40/   89]
Per-example loss in batch: 0.246864  [   42/   89]
Per-example loss in batch: 0.276773  [   44/   89]
Per-example loss in batch: 0.329451  [   46/   89]
Per-example loss in batch: 0.317487  [   48/   89]
Per-example loss in batch: 0.272558  [   50/   89]
Per-example loss in batch: 0.241957  [   52/   89]
Per-example loss in batch: 0.228074  [   54/   89]
Per-example loss in batch: 0.245255  [   56/   89]
Per-example loss in batch: 0.242897  [   58/   89]
Per-example loss in batch: 0.210214  [   60/   89]
Per-example loss in batch: 0.209992  [   62/   89]
Per-example loss in batch: 0.230055  [   64/   89]
Per-example loss in batch: 0.259756  [   66/   89]
Per-example loss in batch: 0.220057  [   68/   89]
Per-example loss in batch: 0.221468  [   70/   89]
Per-example loss in batch: 0.199273  [   72/   89]
Per-example loss in batch: 0.267192  [   74/   89]
Per-example loss in batch: 0.233779  [   76/   89]
Per-example loss in batch: 0.221111  [   78/   89]
Per-example loss in batch: 0.221803  [   80/   89]
Per-example loss in batch: 0.263116  [   82/   89]
Per-example loss in batch: 0.254048  [   84/   89]
Per-example loss in batch: 0.254700  [   86/   89]
Per-example loss in batch: 0.340365  [   88/   89]
Per-example loss in batch: 0.425884  [   89/   89]
Train Error: Avg loss: 0.25348175
validation Error: 
 Avg loss: 0.27686838 
 F1: 0.489605 
 Precision: 0.547514 
 Recall: 0.442775
 IoU: 0.324157

test Error: 
 Avg loss: 0.25346010 
 F1: 0.523073 
 Precision: 0.590129 
 Recall: 0.469701
 IoU: 0.354163

We have finished training iteration 255
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_253_.pth
Per-example loss in batch: 0.244887  [    2/   89]
Per-example loss in batch: 0.310402  [    4/   89]
Per-example loss in batch: 0.269703  [    6/   89]
Per-example loss in batch: 0.254705  [    8/   89]
Per-example loss in batch: 0.228234  [   10/   89]
Per-example loss in batch: 0.279921  [   12/   89]
Per-example loss in batch: 0.229927  [   14/   89]
Per-example loss in batch: 0.210505  [   16/   89]
Per-example loss in batch: 0.203928  [   18/   89]
Per-example loss in batch: 0.256741  [   20/   89]
Per-example loss in batch: 0.215581  [   22/   89]
Per-example loss in batch: 0.205153  [   24/   89]
Per-example loss in batch: 0.221140  [   26/   89]
Per-example loss in batch: 0.299061  [   28/   89]
Per-example loss in batch: 0.231405  [   30/   89]
Per-example loss in batch: 0.236588  [   32/   89]
Per-example loss in batch: 0.226927  [   34/   89]
Per-example loss in batch: 0.203154  [   36/   89]
Per-example loss in batch: 0.297856  [   38/   89]
Per-example loss in batch: 0.304845  [   40/   89]
Per-example loss in batch: 0.314465  [   42/   89]
Per-example loss in batch: 0.247844  [   44/   89]
Per-example loss in batch: 0.256994  [   46/   89]
Per-example loss in batch: 0.380906  [   48/   89]
Per-example loss in batch: 0.314706  [   50/   89]
Per-example loss in batch: 0.192164  [   52/   89]
Per-example loss in batch: 0.240921  [   54/   89]
Per-example loss in batch: 0.250566  [   56/   89]
Per-example loss in batch: 0.285997  [   58/   89]
Per-example loss in batch: 0.259362  [   60/   89]
Per-example loss in batch: 0.249007  [   62/   89]
Per-example loss in batch: 0.200039  [   64/   89]
Per-example loss in batch: 0.233697  [   66/   89]
Per-example loss in batch: 0.307406  [   68/   89]
Per-example loss in batch: 0.226886  [   70/   89]
Per-example loss in batch: 0.265441  [   72/   89]
Per-example loss in batch: 0.340367  [   74/   89]
Per-example loss in batch: 0.209053  [   76/   89]
Per-example loss in batch: 0.297248  [   78/   89]
Per-example loss in batch: 0.272589  [   80/   89]
Per-example loss in batch: 0.257010  [   82/   89]
Per-example loss in batch: 0.251983  [   84/   89]
Per-example loss in batch: 0.235018  [   86/   89]
Per-example loss in batch: 0.245341  [   88/   89]
Per-example loss in batch: 0.666502  [   89/   89]
Train Error: Avg loss: 0.26064997
validation Error: 
 Avg loss: 0.27445769 
 F1: 0.489499 
 Precision: 0.550858 
 Recall: 0.440439
 IoU: 0.324064

test Error: 
 Avg loss: 0.25407878 
 F1: 0.522280 
 Precision: 0.593797 
 Recall: 0.466138
 IoU: 0.353436

We have finished training iteration 256
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_254_.pth
Per-example loss in batch: 0.259012  [    2/   89]
Per-example loss in batch: 0.225554  [    4/   89]
Per-example loss in batch: 0.284128  [    6/   89]
Per-example loss in batch: 0.269888  [    8/   89]
Per-example loss in batch: 0.273211  [   10/   89]
Per-example loss in batch: 0.359153  [   12/   89]
Per-example loss in batch: 0.279283  [   14/   89]
Per-example loss in batch: 0.296726  [   16/   89]
Per-example loss in batch: 0.225280  [   18/   89]
Per-example loss in batch: 0.202805  [   20/   89]
Per-example loss in batch: 0.197226  [   22/   89]
Per-example loss in batch: 0.274798  [   24/   89]
Per-example loss in batch: 0.369473  [   26/   89]
Per-example loss in batch: 0.304784  [   28/   89]
Per-example loss in batch: 0.254930  [   30/   89]
Per-example loss in batch: 0.238118  [   32/   89]
Per-example loss in batch: 0.295407  [   34/   89]
Per-example loss in batch: 0.232417  [   36/   89]
Per-example loss in batch: 0.230391  [   38/   89]
Per-example loss in batch: 0.234976  [   40/   89]
Per-example loss in batch: 0.186022  [   42/   89]
Per-example loss in batch: 0.311763  [   44/   89]
Per-example loss in batch: 0.268267  [   46/   89]
Per-example loss in batch: 0.353160  [   48/   89]
Per-example loss in batch: 0.289985  [   50/   89]
Per-example loss in batch: 0.304424  [   52/   89]
Per-example loss in batch: 0.203201  [   54/   89]
Per-example loss in batch: 0.242340  [   56/   89]
Per-example loss in batch: 0.297155  [   58/   89]
Per-example loss in batch: 0.214905  [   60/   89]
Per-example loss in batch: 0.248253  [   62/   89]
Per-example loss in batch: 0.295700  [   64/   89]
Per-example loss in batch: 0.215613  [   66/   89]
Per-example loss in batch: 0.235922  [   68/   89]
Per-example loss in batch: 0.320419  [   70/   89]
Per-example loss in batch: 0.226672  [   72/   89]
Per-example loss in batch: 0.307515  [   74/   89]
Per-example loss in batch: 0.215974  [   76/   89]
Per-example loss in batch: 0.276802  [   78/   89]
Per-example loss in batch: 0.212779  [   80/   89]
Per-example loss in batch: 0.199927  [   82/   89]
Per-example loss in batch: 0.314946  [   84/   89]
Per-example loss in batch: 0.268206  [   86/   89]
Per-example loss in batch: 0.219856  [   88/   89]
Per-example loss in batch: 0.421054  [   89/   89]
Train Error: Avg loss: 0.26399752
validation Error: 
 Avg loss: 0.27875740 
 F1: 0.489337 
 Precision: 0.557360 
 Recall: 0.436111
 IoU: 0.323922

test Error: 
 Avg loss: 0.25536598 
 F1: 0.519453 
 Precision: 0.596293 
 Recall: 0.460156
 IoU: 0.350852

We have finished training iteration 257
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_255_.pth
Per-example loss in batch: 0.311454  [    2/   89]
Per-example loss in batch: 0.254390  [    4/   89]
Per-example loss in batch: 0.216482  [    6/   89]
Per-example loss in batch: 0.214755  [    8/   89]
Per-example loss in batch: 0.219779  [   10/   89]
Per-example loss in batch: 0.288659  [   12/   89]
Per-example loss in batch: 0.209425  [   14/   89]
Per-example loss in batch: 0.346980  [   16/   89]
Per-example loss in batch: 0.257886  [   18/   89]
Per-example loss in batch: 0.232987  [   20/   89]
Per-example loss in batch: 0.189343  [   22/   89]
Per-example loss in batch: 0.226446  [   24/   89]
Per-example loss in batch: 0.210889  [   26/   89]
Per-example loss in batch: 0.211974  [   28/   89]
Per-example loss in batch: 0.215866  [   30/   89]
Per-example loss in batch: 0.278786  [   32/   89]
Per-example loss in batch: 0.213058  [   34/   89]
Per-example loss in batch: 0.278183  [   36/   89]
Per-example loss in batch: 0.264368  [   38/   89]
Per-example loss in batch: 0.277293  [   40/   89]
Per-example loss in batch: 0.263880  [   42/   89]
Per-example loss in batch: 0.268807  [   44/   89]
Per-example loss in batch: 0.221810  [   46/   89]
Per-example loss in batch: 0.210534  [   48/   89]
Per-example loss in batch: 0.282843  [   50/   89]
Per-example loss in batch: 0.257989  [   52/   89]
Per-example loss in batch: 0.286658  [   54/   89]
Per-example loss in batch: 0.286882  [   56/   89]
Per-example loss in batch: 0.288831  [   58/   89]
Per-example loss in batch: 0.267308  [   60/   89]
Per-example loss in batch: 0.251250  [   62/   89]
Per-example loss in batch: 0.248446  [   64/   89]
Per-example loss in batch: 0.212209  [   66/   89]
Per-example loss in batch: 0.206517  [   68/   89]
Per-example loss in batch: 0.199834  [   70/   89]
Per-example loss in batch: 0.315650  [   72/   89]
Per-example loss in batch: 0.361085  [   74/   89]
Per-example loss in batch: 0.273581  [   76/   89]
Per-example loss in batch: 0.276158  [   78/   89]
Per-example loss in batch: 0.268202  [   80/   89]
Per-example loss in batch: 0.198119  [   82/   89]
Per-example loss in batch: 0.215021  [   84/   89]
Per-example loss in batch: 0.232017  [   86/   89]
Per-example loss in batch: 0.225183  [   88/   89]
Per-example loss in batch: 0.460691  [   89/   89]
Train Error: Avg loss: 0.25321712
validation Error: 
 Avg loss: 0.27452711 
 F1: 0.484846 
 Precision: 0.590787 
 Recall: 0.411122
 IoU: 0.319998

test Error: 
 Avg loss: 0.25803692 
 F1: 0.513314 
 Precision: 0.624576 
 Recall: 0.435698
 IoU: 0.345274

We have finished training iteration 258
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_256_.pth
Per-example loss in batch: 0.281678  [    2/   89]
Per-example loss in batch: 0.250123  [    4/   89]
Per-example loss in batch: 0.302105  [    6/   89]
Per-example loss in batch: 0.268582  [    8/   89]
Per-example loss in batch: 0.249759  [   10/   89]
Per-example loss in batch: 0.192425  [   12/   89]
Per-example loss in batch: 0.336316  [   14/   89]
Per-example loss in batch: 0.241860  [   16/   89]
Per-example loss in batch: 0.213521  [   18/   89]
Per-example loss in batch: 0.255782  [   20/   89]
Per-example loss in batch: 0.233147  [   22/   89]
Per-example loss in batch: 0.275438  [   24/   89]
Per-example loss in batch: 0.194390  [   26/   89]
Per-example loss in batch: 0.247290  [   28/   89]
Per-example loss in batch: 0.252113  [   30/   89]
Per-example loss in batch: 0.355474  [   32/   89]
Per-example loss in batch: 0.243769  [   34/   89]
Per-example loss in batch: 0.281972  [   36/   89]
Per-example loss in batch: 0.262685  [   38/   89]
Per-example loss in batch: 0.199896  [   40/   89]
Per-example loss in batch: 0.241805  [   42/   89]
Per-example loss in batch: 0.234445  [   44/   89]
Per-example loss in batch: 0.190559  [   46/   89]
Per-example loss in batch: 0.294292  [   48/   89]
Per-example loss in batch: 0.299847  [   50/   89]
Per-example loss in batch: 0.242478  [   52/   89]
Per-example loss in batch: 0.270230  [   54/   89]
Per-example loss in batch: 0.269769  [   56/   89]
Per-example loss in batch: 0.321609  [   58/   89]
Per-example loss in batch: 0.222328  [   60/   89]
Per-example loss in batch: 0.310570  [   62/   89]
Per-example loss in batch: 0.229086  [   64/   89]
Per-example loss in batch: 0.301827  [   66/   89]
Per-example loss in batch: 0.207818  [   68/   89]
Per-example loss in batch: 0.316614  [   70/   89]
Per-example loss in batch: 0.286190  [   72/   89]
Per-example loss in batch: 0.242288  [   74/   89]
Per-example loss in batch: 0.217650  [   76/   89]
Per-example loss in batch: 0.275721  [   78/   89]
Per-example loss in batch: 0.218322  [   80/   89]
Per-example loss in batch: 0.310297  [   82/   89]
Per-example loss in batch: 0.357831  [   84/   89]
Per-example loss in batch: 0.228706  [   86/   89]
Per-example loss in batch: 0.239951  [   88/   89]
Per-example loss in batch: 0.565387  [   89/   89]
Train Error: Avg loss: 0.26407305
validation Error: 
 Avg loss: 0.27743756 
 F1: 0.489937 
 Precision: 0.557077 
 Recall: 0.437240
 IoU: 0.324448

test Error: 
 Avg loss: 0.25370773 
 F1: 0.523048 
 Precision: 0.598217 
 Recall: 0.464661
 IoU: 0.354140

We have finished training iteration 259
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_257_.pth
Per-example loss in batch: 0.328993  [    2/   89]
Per-example loss in batch: 0.257941  [    4/   89]
Per-example loss in batch: 0.243324  [    6/   89]
Per-example loss in batch: 0.251668  [    8/   89]
Per-example loss in batch: 0.232755  [   10/   89]
Per-example loss in batch: 0.204944  [   12/   89]
Per-example loss in batch: 0.303755  [   14/   89]
Per-example loss in batch: 0.356620  [   16/   89]
Per-example loss in batch: 0.217419  [   18/   89]
Per-example loss in batch: 0.347950  [   20/   89]
Per-example loss in batch: 0.289512  [   22/   89]
Per-example loss in batch: 0.205627  [   24/   89]
Per-example loss in batch: 0.206976  [   26/   89]
Per-example loss in batch: 0.299519  [   28/   89]
Per-example loss in batch: 0.297956  [   30/   89]
Per-example loss in batch: 0.224194  [   32/   89]
Per-example loss in batch: 0.250438  [   34/   89]
Per-example loss in batch: 0.298357  [   36/   89]
Per-example loss in batch: 0.223354  [   38/   89]
Per-example loss in batch: 0.243137  [   40/   89]
Per-example loss in batch: 0.255002  [   42/   89]
Per-example loss in batch: 0.257160  [   44/   89]
Per-example loss in batch: 0.247062  [   46/   89]
Per-example loss in batch: 0.218071  [   48/   89]
Per-example loss in batch: 0.284459  [   50/   89]
Per-example loss in batch: 0.242859  [   52/   89]
Per-example loss in batch: 0.349601  [   54/   89]
Per-example loss in batch: 0.211260  [   56/   89]
Per-example loss in batch: 0.212697  [   58/   89]
Per-example loss in batch: 0.224944  [   60/   89]
Per-example loss in batch: 0.244054  [   62/   89]
Per-example loss in batch: 0.262371  [   64/   89]
Per-example loss in batch: 0.239825  [   66/   89]
Per-example loss in batch: 0.329822  [   68/   89]
Per-example loss in batch: 0.264024  [   70/   89]
Per-example loss in batch: 0.219034  [   72/   89]
Per-example loss in batch: 0.224353  [   74/   89]
Per-example loss in batch: 0.250436  [   76/   89]
Per-example loss in batch: 0.208411  [   78/   89]
Per-example loss in batch: 0.271873  [   80/   89]
Per-example loss in batch: 0.220338  [   82/   89]
Per-example loss in batch: 0.257055  [   84/   89]
Per-example loss in batch: 0.215269  [   86/   89]
Per-example loss in batch: 0.226390  [   88/   89]
Per-example loss in batch: 0.422975  [   89/   89]
Train Error: Avg loss: 0.25690551
validation Error: 
 Avg loss: 0.27257460 
 F1: 0.489316 
 Precision: 0.569356 
 Recall: 0.429006
 IoU: 0.323903

test Error: 
 Avg loss: 0.25377696 
 F1: 0.523097 
 Precision: 0.612330 
 Recall: 0.456563
 IoU: 0.354185

We have finished training iteration 260
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_258_.pth
Per-example loss in batch: 0.329336  [    2/   89]
Per-example loss in batch: 0.204777  [    4/   89]
Per-example loss in batch: 0.224200  [    6/   89]
Per-example loss in batch: 0.202990  [    8/   89]
Per-example loss in batch: 0.303543  [   10/   89]
Per-example loss in batch: 0.238692  [   12/   89]
Per-example loss in batch: 0.202201  [   14/   89]
Per-example loss in batch: 0.305845  [   16/   89]
Per-example loss in batch: 0.285210  [   18/   89]
Per-example loss in batch: 0.217587  [   20/   89]
Per-example loss in batch: 0.239940  [   22/   89]
Per-example loss in batch: 0.222453  [   24/   89]
Per-example loss in batch: 0.219175  [   26/   89]
Per-example loss in batch: 0.272080  [   28/   89]
Per-example loss in batch: 0.208398  [   30/   89]
Per-example loss in batch: 0.254774  [   32/   89]
Per-example loss in batch: 0.283514  [   34/   89]
Per-example loss in batch: 0.327010  [   36/   89]
Per-example loss in batch: 0.299436  [   38/   89]
Per-example loss in batch: 0.266302  [   40/   89]
Per-example loss in batch: 0.225940  [   42/   89]
Per-example loss in batch: 0.217432  [   44/   89]
Per-example loss in batch: 0.266942  [   46/   89]
Per-example loss in batch: 0.200159  [   48/   89]
Per-example loss in batch: 0.222028  [   50/   89]
Per-example loss in batch: 0.217583  [   52/   89]
Per-example loss in batch: 0.288595  [   54/   89]
Per-example loss in batch: 0.283418  [   56/   89]
Per-example loss in batch: 0.311552  [   58/   89]
Per-example loss in batch: 0.299652  [   60/   89]
Per-example loss in batch: 0.229555  [   62/   89]
Per-example loss in batch: 0.304269  [   64/   89]
Per-example loss in batch: 0.216864  [   66/   89]
Per-example loss in batch: 0.276348  [   68/   89]
Per-example loss in batch: 0.240847  [   70/   89]
Per-example loss in batch: 0.233741  [   72/   89]
Per-example loss in batch: 0.197521  [   74/   89]
Per-example loss in batch: 0.223209  [   76/   89]
Per-example loss in batch: 0.222516  [   78/   89]
Per-example loss in batch: 0.318031  [   80/   89]
Per-example loss in batch: 0.231694  [   82/   89]
Per-example loss in batch: 0.312539  [   84/   89]
Per-example loss in batch: 0.199359  [   86/   89]
Per-example loss in batch: 0.213528  [   88/   89]
Per-example loss in batch: 0.595508  [   89/   89]
Train Error: Avg loss: 0.25524806
validation Error: 
 Avg loss: 0.27444733 
 F1: 0.490226 
 Precision: 0.545218 
 Recall: 0.445311
 IoU: 0.324702

test Error: 
 Avg loss: 0.25204002 
 F1: 0.526681 
 Precision: 0.591624 
 Recall: 0.474585
 IoU: 0.357479

We have finished training iteration 261
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_259_.pth
Per-example loss in batch: 0.190967  [    2/   89]
Per-example loss in batch: 0.298284  [    4/   89]
Per-example loss in batch: 0.349963  [    6/   89]
Per-example loss in batch: 0.284037  [    8/   89]
Per-example loss in batch: 0.180196  [   10/   89]
Per-example loss in batch: 0.201018  [   12/   89]
Per-example loss in batch: 0.266430  [   14/   89]
Per-example loss in batch: 0.220914  [   16/   89]
Per-example loss in batch: 0.268604  [   18/   89]
Per-example loss in batch: 0.274290  [   20/   89]
Per-example loss in batch: 0.246721  [   22/   89]
Per-example loss in batch: 0.273928  [   24/   89]
Per-example loss in batch: 0.274067  [   26/   89]
Per-example loss in batch: 0.252356  [   28/   89]
Per-example loss in batch: 0.264175  [   30/   89]
Per-example loss in batch: 0.217912  [   32/   89]
Per-example loss in batch: 0.296809  [   34/   89]
Per-example loss in batch: 0.284412  [   36/   89]
Per-example loss in batch: 0.234832  [   38/   89]
Per-example loss in batch: 0.277435  [   40/   89]
Per-example loss in batch: 0.223128  [   42/   89]
Per-example loss in batch: 0.312790  [   44/   89]
Per-example loss in batch: 0.218195  [   46/   89]
Per-example loss in batch: 0.264790  [   48/   89]
Per-example loss in batch: 0.255571  [   50/   89]
Per-example loss in batch: 0.178911  [   52/   89]
Per-example loss in batch: 0.349262  [   54/   89]
Per-example loss in batch: 0.324333  [   56/   89]
Per-example loss in batch: 0.291157  [   58/   89]
Per-example loss in batch: 0.316408  [   60/   89]
Per-example loss in batch: 0.213872  [   62/   89]
Per-example loss in batch: 0.188405  [   64/   89]
Per-example loss in batch: 0.258746  [   66/   89]
Per-example loss in batch: 0.258308  [   68/   89]
Per-example loss in batch: 0.201640  [   70/   89]
Per-example loss in batch: 0.228231  [   72/   89]
Per-example loss in batch: 0.259947  [   74/   89]
Per-example loss in batch: 0.210732  [   76/   89]
Per-example loss in batch: 0.263592  [   78/   89]
Per-example loss in batch: 0.223184  [   80/   89]
Per-example loss in batch: 0.256333  [   82/   89]
Per-example loss in batch: 0.229122  [   84/   89]
Per-example loss in batch: 0.256164  [   86/   89]
Per-example loss in batch: 0.255837  [   88/   89]
Per-example loss in batch: 0.475276  [   89/   89]
Train Error: Avg loss: 0.25693580
validation Error: 
 Avg loss: 0.28102263 
 F1: 0.488832 
 Precision: 0.569244 
 Recall: 0.428326
 IoU: 0.323480

test Error: 
 Avg loss: 0.25409905 
 F1: 0.522522 
 Precision: 0.611622 
 Recall: 0.456081
 IoU: 0.353658

We have finished training iteration 262
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_260_.pth
Per-example loss in batch: 0.222210  [    2/   89]
Per-example loss in batch: 0.344418  [    4/   89]
Per-example loss in batch: 0.286578  [    6/   89]
Per-example loss in batch: 0.242821  [    8/   89]
Per-example loss in batch: 0.211533  [   10/   89]
Per-example loss in batch: 0.323722  [   12/   89]
Per-example loss in batch: 0.201326  [   14/   89]
Per-example loss in batch: 0.242617  [   16/   89]
Per-example loss in batch: 0.272494  [   18/   89]
Per-example loss in batch: 0.227595  [   20/   89]
Per-example loss in batch: 0.250035  [   22/   89]
Per-example loss in batch: 0.228236  [   24/   89]
Per-example loss in batch: 0.260533  [   26/   89]
Per-example loss in batch: 0.279689  [   28/   89]
Per-example loss in batch: 0.264619  [   30/   89]
Per-example loss in batch: 0.261891  [   32/   89]
Per-example loss in batch: 0.289404  [   34/   89]
Per-example loss in batch: 0.186244  [   36/   89]
Per-example loss in batch: 0.256909  [   38/   89]
Per-example loss in batch: 0.220951  [   40/   89]
Per-example loss in batch: 0.257141  [   42/   89]
Per-example loss in batch: 0.235554  [   44/   89]
Per-example loss in batch: 0.283929  [   46/   89]
Per-example loss in batch: 0.230222  [   48/   89]
Per-example loss in batch: 0.278226  [   50/   89]
Per-example loss in batch: 0.222314  [   52/   89]
Per-example loss in batch: 0.234234  [   54/   89]
Per-example loss in batch: 0.209562  [   56/   89]
Per-example loss in batch: 0.279164  [   58/   89]
Per-example loss in batch: 0.253813  [   60/   89]
Per-example loss in batch: 0.294630  [   62/   89]
Per-example loss in batch: 0.215062  [   64/   89]
Per-example loss in batch: 0.215973  [   66/   89]
Per-example loss in batch: 0.257023  [   68/   89]
Per-example loss in batch: 0.238373  [   70/   89]
Per-example loss in batch: 0.318201  [   72/   89]
Per-example loss in batch: 0.177828  [   74/   89]
Per-example loss in batch: 0.228760  [   76/   89]
Per-example loss in batch: 0.250108  [   78/   89]
Per-example loss in batch: 0.324498  [   80/   89]
Per-example loss in batch: 0.250849  [   82/   89]
Per-example loss in batch: 0.251592  [   84/   89]
Per-example loss in batch: 0.252951  [   86/   89]
Per-example loss in batch: 0.279999  [   88/   89]
Per-example loss in batch: 0.744680  [   89/   89]
Train Error: Avg loss: 0.25811620
validation Error: 
 Avg loss: 0.27057246 
 F1: 0.489678 
 Precision: 0.574299 
 Recall: 0.426792
 IoU: 0.324221

test Error: 
 Avg loss: 0.25455760 
 F1: 0.521364 
 Precision: 0.614783 
 Recall: 0.452591
 IoU: 0.352598

We have finished training iteration 263
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_261_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.264852  [    2/   89]
Per-example loss in batch: 0.317217  [    4/   89]
Per-example loss in batch: 0.288432  [    6/   89]
Per-example loss in batch: 0.203212  [    8/   89]
Per-example loss in batch: 0.325118  [   10/   89]
Per-example loss in batch: 0.301479  [   12/   89]
Per-example loss in batch: 0.276598  [   14/   89]
Per-example loss in batch: 0.212045  [   16/   89]
Per-example loss in batch: 0.350362  [   18/   89]
Per-example loss in batch: 0.257023  [   20/   89]
Per-example loss in batch: 0.212874  [   22/   89]
Per-example loss in batch: 0.196188  [   24/   89]
Per-example loss in batch: 0.211759  [   26/   89]
Per-example loss in batch: 0.268323  [   28/   89]
Per-example loss in batch: 0.221769  [   30/   89]
Per-example loss in batch: 0.233862  [   32/   89]
Per-example loss in batch: 0.228559  [   34/   89]
Per-example loss in batch: 0.251885  [   36/   89]
Per-example loss in batch: 0.244325  [   38/   89]
Per-example loss in batch: 0.212667  [   40/   89]
Per-example loss in batch: 0.212415  [   42/   89]
Per-example loss in batch: 0.250451  [   44/   89]
Per-example loss in batch: 0.253307  [   46/   89]
Per-example loss in batch: 0.231260  [   48/   89]
Per-example loss in batch: 0.308348  [   50/   89]
Per-example loss in batch: 0.318511  [   52/   89]
Per-example loss in batch: 0.226619  [   54/   89]
Per-example loss in batch: 0.280712  [   56/   89]
Per-example loss in batch: 0.208371  [   58/   89]
Per-example loss in batch: 0.218120  [   60/   89]
Per-example loss in batch: 0.191505  [   62/   89]
Per-example loss in batch: 0.282906  [   64/   89]
Per-example loss in batch: 0.202363  [   66/   89]
Per-example loss in batch: 0.296357  [   68/   89]
Per-example loss in batch: 0.291276  [   70/   89]
Per-example loss in batch: 0.296219  [   72/   89]
Per-example loss in batch: 0.218456  [   74/   89]
Per-example loss in batch: 0.242087  [   76/   89]
Per-example loss in batch: 0.279165  [   78/   89]
Per-example loss in batch: 0.251649  [   80/   89]
Per-example loss in batch: 0.334710  [   82/   89]
Per-example loss in batch: 0.205560  [   84/   89]
Per-example loss in batch: 0.201867  [   86/   89]
Per-example loss in batch: 0.319627  [   88/   89]
Per-example loss in batch: 0.575279  [   89/   89]
Train Error: Avg loss: 0.25815841
validation Error: 
 Avg loss: 0.26900614 
 F1: 0.490751 
 Precision: 0.567345 
 Recall: 0.432378
 IoU: 0.325163

test Error: 
 Avg loss: 0.25359280 
 F1: 0.523694 
 Precision: 0.612820 
 Recall: 0.457201
 IoU: 0.354733

We have finished training iteration 264
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_262_.pth
Per-example loss in batch: 0.229994  [    2/   89]
Per-example loss in batch: 0.312665  [    4/   89]
Per-example loss in batch: 0.195763  [    6/   89]
Per-example loss in batch: 0.234135  [    8/   89]
Per-example loss in batch: 0.303954  [   10/   89]
Per-example loss in batch: 0.255657  [   12/   89]
Per-example loss in batch: 0.295828  [   14/   89]
Per-example loss in batch: 0.264570  [   16/   89]
Per-example loss in batch: 0.199106  [   18/   89]
Per-example loss in batch: 0.262995  [   20/   89]
Per-example loss in batch: 0.285735  [   22/   89]
Per-example loss in batch: 0.231384  [   24/   89]
Per-example loss in batch: 0.244046  [   26/   89]
Per-example loss in batch: 0.268536  [   28/   89]
Per-example loss in batch: 0.209454  [   30/   89]
Per-example loss in batch: 0.319129  [   32/   89]
Per-example loss in batch: 0.277121  [   34/   89]
Per-example loss in batch: 0.291466  [   36/   89]
Per-example loss in batch: 0.248631  [   38/   89]
Per-example loss in batch: 0.221876  [   40/   89]
Per-example loss in batch: 0.354624  [   42/   89]
Per-example loss in batch: 0.336303  [   44/   89]
Per-example loss in batch: 0.226517  [   46/   89]
Per-example loss in batch: 0.206016  [   48/   89]
Per-example loss in batch: 0.200420  [   50/   89]
Per-example loss in batch: 0.235713  [   52/   89]
Per-example loss in batch: 0.305942  [   54/   89]
Per-example loss in batch: 0.243831  [   56/   89]
Per-example loss in batch: 0.249599  [   58/   89]
Per-example loss in batch: 0.177986  [   60/   89]
Per-example loss in batch: 0.203150  [   62/   89]
Per-example loss in batch: 0.411891  [   64/   89]
Per-example loss in batch: 0.311995  [   66/   89]
Per-example loss in batch: 0.269986  [   68/   89]
Per-example loss in batch: 0.231453  [   70/   89]
Per-example loss in batch: 0.264215  [   72/   89]
Per-example loss in batch: 0.209318  [   74/   89]
Per-example loss in batch: 0.205770  [   76/   89]
Per-example loss in batch: 0.220478  [   78/   89]
Per-example loss in batch: 0.237888  [   80/   89]
Per-example loss in batch: 0.216343  [   82/   89]
Per-example loss in batch: 0.330924  [   84/   89]
Per-example loss in batch: 0.298860  [   86/   89]
Per-example loss in batch: 0.206571  [   88/   89]
Per-example loss in batch: 0.701755  [   89/   89]
Train Error: Avg loss: 0.26199356
validation Error: 
 Avg loss: 0.27431858 
 F1: 0.489095 
 Precision: 0.567103 
 Recall: 0.429952
 IoU: 0.323710

test Error: 
 Avg loss: 0.25411333 
 F1: 0.522215 
 Precision: 0.611670 
 Recall: 0.455587
 IoU: 0.353377

We have finished training iteration 265
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_263_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.227671  [    2/   89]
Per-example loss in batch: 0.299319  [    4/   89]
Per-example loss in batch: 0.196132  [    6/   89]
Per-example loss in batch: 0.255165  [    8/   89]
Per-example loss in batch: 0.217159  [   10/   89]
Per-example loss in batch: 0.258281  [   12/   89]
Per-example loss in batch: 0.298611  [   14/   89]
Per-example loss in batch: 0.246281  [   16/   89]
Per-example loss in batch: 0.209678  [   18/   89]
Per-example loss in batch: 0.279902  [   20/   89]
Per-example loss in batch: 0.237747  [   22/   89]
Per-example loss in batch: 0.299373  [   24/   89]
Per-example loss in batch: 0.222607  [   26/   89]
Per-example loss in batch: 0.262250  [   28/   89]
Per-example loss in batch: 0.338892  [   30/   89]
Per-example loss in batch: 0.225988  [   32/   89]
Per-example loss in batch: 0.280927  [   34/   89]
Per-example loss in batch: 0.303508  [   36/   89]
Per-example loss in batch: 0.254119  [   38/   89]
Per-example loss in batch: 0.305097  [   40/   89]
Per-example loss in batch: 0.226881  [   42/   89]
Per-example loss in batch: 0.267385  [   44/   89]
Per-example loss in batch: 0.264847  [   46/   89]
Per-example loss in batch: 0.311955  [   48/   89]
Per-example loss in batch: 0.238661  [   50/   89]
Per-example loss in batch: 0.226358  [   52/   89]
Per-example loss in batch: 0.253095  [   54/   89]
Per-example loss in batch: 0.259790  [   56/   89]
Per-example loss in batch: 0.218815  [   58/   89]
Per-example loss in batch: 0.235037  [   60/   89]
Per-example loss in batch: 0.252421  [   62/   89]
Per-example loss in batch: 0.239761  [   64/   89]
Per-example loss in batch: 0.214531  [   66/   89]
Per-example loss in batch: 0.228710  [   68/   89]
Per-example loss in batch: 0.342582  [   70/   89]
Per-example loss in batch: 0.264494  [   72/   89]
Per-example loss in batch: 0.261905  [   74/   89]
Per-example loss in batch: 0.228128  [   76/   89]
Per-example loss in batch: 0.181802  [   78/   89]
Per-example loss in batch: 0.202947  [   80/   89]
Per-example loss in batch: 0.293621  [   82/   89]
Per-example loss in batch: 0.317600  [   84/   89]
Per-example loss in batch: 0.246707  [   86/   89]
Per-example loss in batch: 0.210256  [   88/   89]
Per-example loss in batch: 0.537400  [   89/   89]
Train Error: Avg loss: 0.25788078
validation Error: 
 Avg loss: 0.27775415 
 F1: 0.489353 
 Precision: 0.562204 
 Recall: 0.433216
 IoU: 0.323936

test Error: 
 Avg loss: 0.25367418 
 F1: 0.523061 
 Precision: 0.605138 
 Recall: 0.460590
 IoU: 0.354152

We have finished training iteration 266
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_264_.pth
Per-example loss in batch: 0.336915  [    2/   89]
Per-example loss in batch: 0.230963  [    4/   89]
Per-example loss in batch: 0.269062  [    6/   89]
Per-example loss in batch: 0.215080  [    8/   89]
Per-example loss in batch: 0.244452  [   10/   89]
Per-example loss in batch: 0.265345  [   12/   89]
Per-example loss in batch: 0.234479  [   14/   89]
Per-example loss in batch: 0.283782  [   16/   89]
Per-example loss in batch: 0.255432  [   18/   89]
Per-example loss in batch: 0.234344  [   20/   89]
Per-example loss in batch: 0.265835  [   22/   89]
Per-example loss in batch: 0.272581  [   24/   89]
Per-example loss in batch: 0.233246  [   26/   89]
Per-example loss in batch: 0.334664  [   28/   89]
Per-example loss in batch: 0.258571  [   30/   89]
Per-example loss in batch: 0.259318  [   32/   89]
Per-example loss in batch: 0.313702  [   34/   89]
Per-example loss in batch: 0.218100  [   36/   89]
Per-example loss in batch: 0.208837  [   38/   89]
Per-example loss in batch: 0.301194  [   40/   89]
Per-example loss in batch: 0.267987  [   42/   89]
Per-example loss in batch: 0.393107  [   44/   89]
Per-example loss in batch: 0.243173  [   46/   89]
Per-example loss in batch: 0.210191  [   48/   89]
Per-example loss in batch: 0.238344  [   50/   89]
Per-example loss in batch: 0.223926  [   52/   89]
Per-example loss in batch: 0.230644  [   54/   89]
Per-example loss in batch: 0.264210  [   56/   89]
Per-example loss in batch: 0.270762  [   58/   89]
Per-example loss in batch: 0.221616  [   60/   89]
Per-example loss in batch: 0.261542  [   62/   89]
Per-example loss in batch: 0.207856  [   64/   89]
Per-example loss in batch: 0.287467  [   66/   89]
Per-example loss in batch: 0.257455  [   68/   89]
Per-example loss in batch: 0.244642  [   70/   89]
Per-example loss in batch: 0.303336  [   72/   89]
Per-example loss in batch: 0.286785  [   74/   89]
Per-example loss in batch: 0.310030  [   76/   89]
Per-example loss in batch: 0.215921  [   78/   89]
Per-example loss in batch: 0.235803  [   80/   89]
Per-example loss in batch: 0.210195  [   82/   89]
Per-example loss in batch: 0.285789  [   84/   89]
Per-example loss in batch: 0.211152  [   86/   89]
Per-example loss in batch: 0.219798  [   88/   89]
Per-example loss in batch: 0.415665  [   89/   89]
Train Error: Avg loss: 0.25944863
validation Error: 
 Avg loss: 0.27497726 
 F1: 0.486910 
 Precision: 0.576761 
 Recall: 0.421281
 IoU: 0.321799

test Error: 
 Avg loss: 0.25618353 
 F1: 0.517532 
 Precision: 0.618552 
 Recall: 0.444877
 IoU: 0.349102

We have finished training iteration 267
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_265_.pth
Per-example loss in batch: 0.260431  [    2/   89]
Per-example loss in batch: 0.345913  [    4/   89]
Per-example loss in batch: 0.310171  [    6/   89]
Per-example loss in batch: 0.255010  [    8/   89]
Per-example loss in batch: 0.294106  [   10/   89]
Per-example loss in batch: 0.291167  [   12/   89]
Per-example loss in batch: 0.213708  [   14/   89]
Per-example loss in batch: 0.219290  [   16/   89]
Per-example loss in batch: 0.299775  [   18/   89]
Per-example loss in batch: 0.246705  [   20/   89]
Per-example loss in batch: 0.202061  [   22/   89]
Per-example loss in batch: 0.331737  [   24/   89]
Per-example loss in batch: 0.223372  [   26/   89]
Per-example loss in batch: 0.316611  [   28/   89]
Per-example loss in batch: 0.356630  [   30/   89]
Per-example loss in batch: 0.212549  [   32/   89]
Per-example loss in batch: 0.253771  [   34/   89]
Per-example loss in batch: 0.262963  [   36/   89]
Per-example loss in batch: 0.290492  [   38/   89]
Per-example loss in batch: 0.232673  [   40/   89]
Per-example loss in batch: 0.211471  [   42/   89]
Per-example loss in batch: 0.288740  [   44/   89]
Per-example loss in batch: 0.219495  [   46/   89]
Per-example loss in batch: 0.223777  [   48/   89]
Per-example loss in batch: 0.229718  [   50/   89]
Per-example loss in batch: 0.241111  [   52/   89]
Per-example loss in batch: 0.236766  [   54/   89]
Per-example loss in batch: 0.240492  [   56/   89]
Per-example loss in batch: 0.228962  [   58/   89]
Per-example loss in batch: 0.264138  [   60/   89]
Per-example loss in batch: 0.256753  [   62/   89]
Per-example loss in batch: 0.202589  [   64/   89]
Per-example loss in batch: 0.225219  [   66/   89]
Per-example loss in batch: 0.260801  [   68/   89]
Per-example loss in batch: 0.257007  [   70/   89]
Per-example loss in batch: 0.250750  [   72/   89]
Per-example loss in batch: 0.293877  [   74/   89]
Per-example loss in batch: 0.314270  [   76/   89]
Per-example loss in batch: 0.290959  [   78/   89]
Per-example loss in batch: 0.208342  [   80/   89]
Per-example loss in batch: 0.221331  [   82/   89]
Per-example loss in batch: 0.257311  [   84/   89]
Per-example loss in batch: 0.229113  [   86/   89]
Per-example loss in batch: 0.348525  [   88/   89]
Per-example loss in batch: 0.391763  [   89/   89]
Train Error: Avg loss: 0.26104564
validation Error: 
 Avg loss: 0.27125393 
 F1: 0.488471 
 Precision: 0.572217 
 Recall: 0.426109
 IoU: 0.323164

test Error: 
 Avg loss: 0.25503169 
 F1: 0.520427 
 Precision: 0.615386 
 Recall: 0.450855
 IoU: 0.351741

We have finished training iteration 268
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_266_.pth
Per-example loss in batch: 0.337754  [    2/   89]
Per-example loss in batch: 0.184347  [    4/   89]
Per-example loss in batch: 0.221417  [    6/   89]
Per-example loss in batch: 0.299785  [    8/   89]
Per-example loss in batch: 0.242502  [   10/   89]
Per-example loss in batch: 0.205507  [   12/   89]
Per-example loss in batch: 0.237284  [   14/   89]
Per-example loss in batch: 0.276105  [   16/   89]
Per-example loss in batch: 0.323003  [   18/   89]
Per-example loss in batch: 0.231260  [   20/   89]
Per-example loss in batch: 0.230509  [   22/   89]
Per-example loss in batch: 0.295826  [   24/   89]
Per-example loss in batch: 0.231939  [   26/   89]
Per-example loss in batch: 0.255935  [   28/   89]
Per-example loss in batch: 0.221806  [   30/   89]
Per-example loss in batch: 0.249580  [   32/   89]
Per-example loss in batch: 0.316180  [   34/   89]
Per-example loss in batch: 0.286093  [   36/   89]
Per-example loss in batch: 0.206080  [   38/   89]
Per-example loss in batch: 0.298011  [   40/   89]
Per-example loss in batch: 0.288273  [   42/   89]
Per-example loss in batch: 0.248597  [   44/   89]
Per-example loss in batch: 0.229732  [   46/   89]
Per-example loss in batch: 0.249723  [   48/   89]
Per-example loss in batch: 0.210479  [   50/   89]
Per-example loss in batch: 0.226130  [   52/   89]
Per-example loss in batch: 0.227932  [   54/   89]
Per-example loss in batch: 0.214372  [   56/   89]
Per-example loss in batch: 0.287534  [   58/   89]
Per-example loss in batch: 0.315983  [   60/   89]
Per-example loss in batch: 0.211818  [   62/   89]
Per-example loss in batch: 0.215125  [   64/   89]
Per-example loss in batch: 0.223008  [   66/   89]
Per-example loss in batch: 0.240254  [   68/   89]
Per-example loss in batch: 0.247578  [   70/   89]
Per-example loss in batch: 0.265020  [   72/   89]
Per-example loss in batch: 0.308127  [   74/   89]
Per-example loss in batch: 0.321957  [   76/   89]
Per-example loss in batch: 0.291257  [   78/   89]
Per-example loss in batch: 0.219666  [   80/   89]
Per-example loss in batch: 0.286441  [   82/   89]
Per-example loss in batch: 0.231693  [   84/   89]
Per-example loss in batch: 0.318845  [   86/   89]
Per-example loss in batch: 0.249527  [   88/   89]
Per-example loss in batch: 0.400281  [   89/   89]
Train Error: Avg loss: 0.25798051
validation Error: 
 Avg loss: 0.27180507 
 F1: 0.491027 
 Precision: 0.577143 
 Recall: 0.427273
 IoU: 0.325405

test Error: 
 Avg loss: 0.25495790 
 F1: 0.520392 
 Precision: 0.616593 
 Recall: 0.450158
 IoU: 0.351709

We have finished training iteration 269
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_267_.pth
Per-example loss in batch: 0.242862  [    2/   89]
Per-example loss in batch: 0.247032  [    4/   89]
Per-example loss in batch: 0.260020  [    6/   89]
Per-example loss in batch: 0.230867  [    8/   89]
Per-example loss in batch: 0.280390  [   10/   89]
Per-example loss in batch: 0.327709  [   12/   89]
Per-example loss in batch: 0.280388  [   14/   89]
Per-example loss in batch: 0.310997  [   16/   89]
Per-example loss in batch: 0.276044  [   18/   89]
Per-example loss in batch: 0.245484  [   20/   89]
Per-example loss in batch: 0.352653  [   22/   89]
Per-example loss in batch: 0.280086  [   24/   89]
Per-example loss in batch: 0.270996  [   26/   89]
Per-example loss in batch: 0.237429  [   28/   89]
Per-example loss in batch: 0.237660  [   30/   89]
Per-example loss in batch: 0.231915  [   32/   89]
Per-example loss in batch: 0.283877  [   34/   89]
Per-example loss in batch: 0.226493  [   36/   89]
Per-example loss in batch: 0.245028  [   38/   89]
Per-example loss in batch: 0.241352  [   40/   89]
Per-example loss in batch: 0.326153  [   42/   89]
Per-example loss in batch: 0.316738  [   44/   89]
Per-example loss in batch: 0.234131  [   46/   89]
Per-example loss in batch: 0.284690  [   48/   89]
Per-example loss in batch: 0.213015  [   50/   89]
Per-example loss in batch: 0.285896  [   52/   89]
Per-example loss in batch: 0.275729  [   54/   89]
Per-example loss in batch: 0.195589  [   56/   89]
Per-example loss in batch: 0.242776  [   58/   89]
Per-example loss in batch: 0.312860  [   60/   89]
Per-example loss in batch: 0.275850  [   62/   89]
Per-example loss in batch: 0.246739  [   64/   89]
Per-example loss in batch: 0.240905  [   66/   89]
Per-example loss in batch: 0.179284  [   68/   89]
Per-example loss in batch: 0.236975  [   70/   89]
Per-example loss in batch: 0.232674  [   72/   89]
Per-example loss in batch: 0.232742  [   74/   89]
Per-example loss in batch: 0.216090  [   76/   89]
Per-example loss in batch: 0.240154  [   78/   89]
Per-example loss in batch: 0.184429  [   80/   89]
Per-example loss in batch: 0.296789  [   82/   89]
Per-example loss in batch: 0.182034  [   84/   89]
Per-example loss in batch: 0.265815  [   86/   89]
Per-example loss in batch: 0.289287  [   88/   89]
Per-example loss in batch: 0.666997  [   89/   89]
Train Error: Avg loss: 0.26180052
validation Error: 
 Avg loss: 0.27288281 
 F1: 0.489409 
 Precision: 0.533226 
 Recall: 0.452247
 IoU: 0.323985

test Error: 
 Avg loss: 0.25222725 
 F1: 0.526067 
 Precision: 0.582661 
 Recall: 0.479493
 IoU: 0.356914

We have finished training iteration 270
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_268_.pth
Per-example loss in batch: 0.315409  [    2/   89]
Per-example loss in batch: 0.303682  [    4/   89]
Per-example loss in batch: 0.207299  [    6/   89]
Per-example loss in batch: 0.252068  [    8/   89]
Per-example loss in batch: 0.254154  [   10/   89]
Per-example loss in batch: 0.237681  [   12/   89]
Per-example loss in batch: 0.257346  [   14/   89]
Per-example loss in batch: 0.239306  [   16/   89]
Per-example loss in batch: 0.291175  [   18/   89]
Per-example loss in batch: 0.315719  [   20/   89]
Per-example loss in batch: 0.293562  [   22/   89]
Per-example loss in batch: 0.222766  [   24/   89]
Per-example loss in batch: 0.267588  [   26/   89]
Per-example loss in batch: 0.287111  [   28/   89]
Per-example loss in batch: 0.256631  [   30/   89]
Per-example loss in batch: 0.346798  [   32/   89]
Per-example loss in batch: 0.227707  [   34/   89]
Per-example loss in batch: 0.311339  [   36/   89]
Per-example loss in batch: 0.190456  [   38/   89]
Per-example loss in batch: 0.273718  [   40/   89]
Per-example loss in batch: 0.224084  [   42/   89]
Per-example loss in batch: 0.226471  [   44/   89]
Per-example loss in batch: 0.198652  [   46/   89]
Per-example loss in batch: 0.293166  [   48/   89]
Per-example loss in batch: 0.208963  [   50/   89]
Per-example loss in batch: 0.203472  [   52/   89]
Per-example loss in batch: 0.258649  [   54/   89]
Per-example loss in batch: 0.271911  [   56/   89]
Per-example loss in batch: 0.219573  [   58/   89]
Per-example loss in batch: 0.218593  [   60/   89]
Per-example loss in batch: 0.225019  [   62/   89]
Per-example loss in batch: 0.313158  [   64/   89]
Per-example loss in batch: 0.193106  [   66/   89]
Per-example loss in batch: 0.280697  [   68/   89]
Per-example loss in batch: 0.238364  [   70/   89]
Per-example loss in batch: 0.297748  [   72/   89]
Per-example loss in batch: 0.205784  [   74/   89]
Per-example loss in batch: 0.327121  [   76/   89]
Per-example loss in batch: 0.229265  [   78/   89]
Per-example loss in batch: 0.173416  [   80/   89]
Per-example loss in batch: 0.255083  [   82/   89]
Per-example loss in batch: 0.209608  [   84/   89]
Per-example loss in batch: 0.244969  [   86/   89]
Per-example loss in batch: 0.298876  [   88/   89]
Per-example loss in batch: 0.411324  [   89/   89]
Train Error: Avg loss: 0.25557133
validation Error: 
 Avg loss: 0.27638122 
 F1: 0.490682 
 Precision: 0.580268 
 Recall: 0.425058
 IoU: 0.325101

test Error: 
 Avg loss: 0.25549919 
 F1: 0.519388 
 Precision: 0.621245 
 Recall: 0.446226
 IoU: 0.350792

We have finished training iteration 271
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_269_.pth
Per-example loss in batch: 0.231101  [    2/   89]
Per-example loss in batch: 0.241485  [    4/   89]
Per-example loss in batch: 0.250114  [    6/   89]
Per-example loss in batch: 0.218347  [    8/   89]
Per-example loss in batch: 0.319198  [   10/   89]
Per-example loss in batch: 0.243108  [   12/   89]
Per-example loss in batch: 0.207678  [   14/   89]
Per-example loss in batch: 0.257699  [   16/   89]
Per-example loss in batch: 0.219829  [   18/   89]
Per-example loss in batch: 0.206234  [   20/   89]
Per-example loss in batch: 0.279809  [   22/   89]
Per-example loss in batch: 0.307345  [   24/   89]
Per-example loss in batch: 0.250802  [   26/   89]
Per-example loss in batch: 0.213813  [   28/   89]
Per-example loss in batch: 0.338442  [   30/   89]
Per-example loss in batch: 0.230413  [   32/   89]
Per-example loss in batch: 0.327205  [   34/   89]
Per-example loss in batch: 0.229641  [   36/   89]
Per-example loss in batch: 0.215796  [   38/   89]
Per-example loss in batch: 0.220514  [   40/   89]
Per-example loss in batch: 0.276159  [   42/   89]
Per-example loss in batch: 0.215979  [   44/   89]
Per-example loss in batch: 0.296606  [   46/   89]
Per-example loss in batch: 0.317315  [   48/   89]
Per-example loss in batch: 0.277676  [   50/   89]
Per-example loss in batch: 0.232584  [   52/   89]
Per-example loss in batch: 0.223966  [   54/   89]
Per-example loss in batch: 0.287513  [   56/   89]
Per-example loss in batch: 0.188882  [   58/   89]
Per-example loss in batch: 0.244026  [   60/   89]
Per-example loss in batch: 0.298152  [   62/   89]
Per-example loss in batch: 0.206242  [   64/   89]
Per-example loss in batch: 0.202932  [   66/   89]
Per-example loss in batch: 0.282934  [   68/   89]
Per-example loss in batch: 0.286944  [   70/   89]
Per-example loss in batch: 0.234161  [   72/   89]
Per-example loss in batch: 0.238178  [   74/   89]
Per-example loss in batch: 0.214401  [   76/   89]
Per-example loss in batch: 0.303646  [   78/   89]
Per-example loss in batch: 0.213465  [   80/   89]
Per-example loss in batch: 0.219308  [   82/   89]
Per-example loss in batch: 0.210646  [   84/   89]
Per-example loss in batch: 0.239277  [   86/   89]
Per-example loss in batch: 0.248060  [   88/   89]
Per-example loss in batch: 0.689722  [   89/   89]
Train Error: Avg loss: 0.25421314
validation Error: 
 Avg loss: 0.27817186 
 F1: 0.492800 
 Precision: 0.554195 
 Recall: 0.443652
 IoU: 0.326964

test Error: 
 Avg loss: 0.25153204 
 F1: 0.528003 
 Precision: 0.603197 
 Recall: 0.469478
 IoU: 0.358698

We have finished training iteration 272
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_270_.pth
Per-example loss in batch: 0.250976  [    2/   89]
Per-example loss in batch: 0.276197  [    4/   89]
Per-example loss in batch: 0.236605  [    6/   89]
Per-example loss in batch: 0.232836  [    8/   89]
Per-example loss in batch: 0.287062  [   10/   89]
Per-example loss in batch: 0.243871  [   12/   89]
Per-example loss in batch: 0.240664  [   14/   89]
Per-example loss in batch: 0.229536  [   16/   89]
Per-example loss in batch: 0.204399  [   18/   89]
Per-example loss in batch: 0.281809  [   20/   89]
Per-example loss in batch: 0.309159  [   22/   89]
Per-example loss in batch: 0.250001  [   24/   89]
Per-example loss in batch: 0.241448  [   26/   89]
Per-example loss in batch: 0.256325  [   28/   89]
Per-example loss in batch: 0.230068  [   30/   89]
Per-example loss in batch: 0.332177  [   32/   89]
Per-example loss in batch: 0.207303  [   34/   89]
Per-example loss in batch: 0.236311  [   36/   89]
Per-example loss in batch: 0.296132  [   38/   89]
Per-example loss in batch: 0.245491  [   40/   89]
Per-example loss in batch: 0.212944  [   42/   89]
Per-example loss in batch: 0.239822  [   44/   89]
Per-example loss in batch: 0.264251  [   46/   89]
Per-example loss in batch: 0.237858  [   48/   89]
Per-example loss in batch: 0.265519  [   50/   89]
Per-example loss in batch: 0.191065  [   52/   89]
Per-example loss in batch: 0.276323  [   54/   89]
Per-example loss in batch: 0.202441  [   56/   89]
Per-example loss in batch: 0.228352  [   58/   89]
Per-example loss in batch: 0.301948  [   60/   89]
Per-example loss in batch: 0.223484  [   62/   89]
Per-example loss in batch: 0.210254  [   64/   89]
Per-example loss in batch: 0.255377  [   66/   89]
Per-example loss in batch: 0.250583  [   68/   89]
Per-example loss in batch: 0.278476  [   70/   89]
Per-example loss in batch: 0.281313  [   72/   89]
Per-example loss in batch: 0.194682  [   74/   89]
Per-example loss in batch: 0.319039  [   76/   89]
Per-example loss in batch: 0.280341  [   78/   89]
Per-example loss in batch: 0.267502  [   80/   89]
Per-example loss in batch: 0.334112  [   82/   89]
Per-example loss in batch: 0.280783  [   84/   89]
Per-example loss in batch: 0.228145  [   86/   89]
Per-example loss in batch: 0.293170  [   88/   89]
Per-example loss in batch: 0.610916  [   89/   89]
Train Error: Avg loss: 0.25868792
validation Error: 
 Avg loss: 0.27167985 
 F1: 0.489764 
 Precision: 0.583287 
 Recall: 0.422087
 IoU: 0.324296

test Error: 
 Avg loss: 0.25412740 
 F1: 0.522672 
 Precision: 0.626553 
 Recall: 0.448339
 IoU: 0.353796

We have finished training iteration 273
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_271_.pth
Per-example loss in batch: 0.333670  [    2/   89]
Per-example loss in batch: 0.255460  [    4/   89]
Per-example loss in batch: 0.204917  [    6/   89]
Per-example loss in batch: 0.245906  [    8/   89]
Per-example loss in batch: 0.191142  [   10/   89]
Per-example loss in batch: 0.214419  [   12/   89]
Per-example loss in batch: 0.236174  [   14/   89]
Per-example loss in batch: 0.200005  [   16/   89]
Per-example loss in batch: 0.204947  [   18/   89]
Per-example loss in batch: 0.359872  [   20/   89]
Per-example loss in batch: 0.217228  [   22/   89]
Per-example loss in batch: 0.247091  [   24/   89]
Per-example loss in batch: 0.258689  [   26/   89]
Per-example loss in batch: 0.236258  [   28/   89]
Per-example loss in batch: 0.273270  [   30/   89]
Per-example loss in batch: 0.301741  [   32/   89]
Per-example loss in batch: 0.312574  [   34/   89]
Per-example loss in batch: 0.264031  [   36/   89]
Per-example loss in batch: 0.252031  [   38/   89]
Per-example loss in batch: 0.215107  [   40/   89]
Per-example loss in batch: 0.323273  [   42/   89]
Per-example loss in batch: 0.286258  [   44/   89]
Per-example loss in batch: 0.325713  [   46/   89]
Per-example loss in batch: 0.235705  [   48/   89]
Per-example loss in batch: 0.303662  [   50/   89]
Per-example loss in batch: 0.259466  [   52/   89]
Per-example loss in batch: 0.224218  [   54/   89]
Per-example loss in batch: 0.214204  [   56/   89]
Per-example loss in batch: 0.262718  [   58/   89]
Per-example loss in batch: 0.259271  [   60/   89]
Per-example loss in batch: 0.214181  [   62/   89]
Per-example loss in batch: 0.247707  [   64/   89]
Per-example loss in batch: 0.247120  [   66/   89]
Per-example loss in batch: 0.224243  [   68/   89]
Per-example loss in batch: 0.301985  [   70/   89]
Per-example loss in batch: 0.244654  [   72/   89]
Per-example loss in batch: 0.248839  [   74/   89]
Per-example loss in batch: 0.350242  [   76/   89]
Per-example loss in batch: 0.285765  [   78/   89]
Per-example loss in batch: 0.289592  [   80/   89]
Per-example loss in batch: 0.266116  [   82/   89]
Per-example loss in batch: 0.247944  [   84/   89]
Per-example loss in batch: 0.189994  [   86/   89]
Per-example loss in batch: 0.243297  [   88/   89]
Per-example loss in batch: 0.688931  [   89/   89]
Train Error: Avg loss: 0.26213853
validation Error: 
 Avg loss: 0.27950497 
 F1: 0.489854 
 Precision: 0.570575 
 Recall: 0.429142
 IoU: 0.324375

test Error: 
 Avg loss: 0.25442594 
 F1: 0.521311 
 Precision: 0.610616 
 Recall: 0.454795
 IoU: 0.352549

We have finished training iteration 274
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_272_.pth
Per-example loss in batch: 0.204418  [    2/   89]
Per-example loss in batch: 0.223742  [    4/   89]
Per-example loss in batch: 0.276003  [    6/   89]
Per-example loss in batch: 0.224813  [    8/   89]
Per-example loss in batch: 0.216323  [   10/   89]
Per-example loss in batch: 0.243706  [   12/   89]
Per-example loss in batch: 0.292602  [   14/   89]
Per-example loss in batch: 0.225223  [   16/   89]
Per-example loss in batch: 0.194909  [   18/   89]
Per-example loss in batch: 0.275469  [   20/   89]
Per-example loss in batch: 0.261002  [   22/   89]
Per-example loss in batch: 0.204273  [   24/   89]
Per-example loss in batch: 0.216998  [   26/   89]
Per-example loss in batch: 0.222074  [   28/   89]
Per-example loss in batch: 0.282230  [   30/   89]
Per-example loss in batch: 0.269879  [   32/   89]
Per-example loss in batch: 0.290948  [   34/   89]
Per-example loss in batch: 0.227726  [   36/   89]
Per-example loss in batch: 0.285438  [   38/   89]
Per-example loss in batch: 0.284327  [   40/   89]
Per-example loss in batch: 0.293811  [   42/   89]
Per-example loss in batch: 0.234335  [   44/   89]
Per-example loss in batch: 0.248441  [   46/   89]
Per-example loss in batch: 0.311946  [   48/   89]
Per-example loss in batch: 0.293061  [   50/   89]
Per-example loss in batch: 0.317253  [   52/   89]
Per-example loss in batch: 0.248913  [   54/   89]
Per-example loss in batch: 0.278114  [   56/   89]
Per-example loss in batch: 0.229959  [   58/   89]
Per-example loss in batch: 0.223143  [   60/   89]
Per-example loss in batch: 0.232581  [   62/   89]
Per-example loss in batch: 0.248890  [   64/   89]
Per-example loss in batch: 0.290185  [   66/   89]
Per-example loss in batch: 0.256092  [   68/   89]
Per-example loss in batch: 0.197159  [   70/   89]
Per-example loss in batch: 0.257403  [   72/   89]
Per-example loss in batch: 0.342038  [   74/   89]
Per-example loss in batch: 0.213199  [   76/   89]
Per-example loss in batch: 0.251691  [   78/   89]
Per-example loss in batch: 0.285203  [   80/   89]
Per-example loss in batch: 0.297502  [   82/   89]
Per-example loss in batch: 0.286135  [   84/   89]
Per-example loss in batch: 0.223730  [   86/   89]
Per-example loss in batch: 0.222446  [   88/   89]
Per-example loss in batch: 0.570544  [   89/   89]
Train Error: Avg loss: 0.25821581
validation Error: 
 Avg loss: 0.28042653 
 F1: 0.487271 
 Precision: 0.558415 
 Recall: 0.432207
 IoU: 0.322114

test Error: 
 Avg loss: 0.25512200 
 F1: 0.519958 
 Precision: 0.599590 
 Recall: 0.458998
 IoU: 0.351313

We have finished training iteration 275
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_273_.pth
Per-example loss in batch: 0.211518  [    2/   89]
Per-example loss in batch: 0.198954  [    4/   89]
Per-example loss in batch: 0.260552  [    6/   89]
Per-example loss in batch: 0.313029  [    8/   89]
Per-example loss in batch: 0.290087  [   10/   89]
Per-example loss in batch: 0.254779  [   12/   89]
Per-example loss in batch: 0.227787  [   14/   89]
Per-example loss in batch: 0.287836  [   16/   89]
Per-example loss in batch: 0.307200  [   18/   89]
Per-example loss in batch: 0.245391  [   20/   89]
Per-example loss in batch: 0.320363  [   22/   89]
Per-example loss in batch: 0.247414  [   24/   89]
Per-example loss in batch: 0.249856  [   26/   89]
Per-example loss in batch: 0.224047  [   28/   89]
Per-example loss in batch: 0.267417  [   30/   89]
Per-example loss in batch: 0.212210  [   32/   89]
Per-example loss in batch: 0.248111  [   34/   89]
Per-example loss in batch: 0.212888  [   36/   89]
Per-example loss in batch: 0.260155  [   38/   89]
Per-example loss in batch: 0.242634  [   40/   89]
Per-example loss in batch: 0.232013  [   42/   89]
Per-example loss in batch: 0.196362  [   44/   89]
Per-example loss in batch: 0.240904  [   46/   89]
Per-example loss in batch: 0.228108  [   48/   89]
Per-example loss in batch: 0.236697  [   50/   89]
Per-example loss in batch: 0.226498  [   52/   89]
Per-example loss in batch: 0.216536  [   54/   89]
Per-example loss in batch: 0.217407  [   56/   89]
Per-example loss in batch: 0.210306  [   58/   89]
Per-example loss in batch: 0.223012  [   60/   89]
Per-example loss in batch: 0.253412  [   62/   89]
Per-example loss in batch: 0.322600  [   64/   89]
Per-example loss in batch: 0.200562  [   66/   89]
Per-example loss in batch: 0.262113  [   68/   89]
Per-example loss in batch: 0.212234  [   70/   89]
Per-example loss in batch: 0.307261  [   72/   89]
Per-example loss in batch: 0.331399  [   74/   89]
Per-example loss in batch: 0.321171  [   76/   89]
Per-example loss in batch: 0.309734  [   78/   89]
Per-example loss in batch: 0.214351  [   80/   89]
Per-example loss in batch: 0.212831  [   82/   89]
Per-example loss in batch: 0.256577  [   84/   89]
Per-example loss in batch: 0.247326  [   86/   89]
Per-example loss in batch: 0.218485  [   88/   89]
Per-example loss in batch: 0.598766  [   89/   89]
Train Error: Avg loss: 0.25347215
validation Error: 
 Avg loss: 0.27178763 
 F1: 0.488527 
 Precision: 0.546663 
 Recall: 0.441567
 IoU: 0.323213

test Error: 
 Avg loss: 0.25318786 
 F1: 0.524377 
 Precision: 0.590868 
 Recall: 0.471337
 IoU: 0.355360

We have finished training iteration 276
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_274_.pth
Per-example loss in batch: 0.211353  [    2/   89]
Per-example loss in batch: 0.256320  [    4/   89]
Per-example loss in batch: 0.216351  [    6/   89]
Per-example loss in batch: 0.231459  [    8/   89]
Per-example loss in batch: 0.305179  [   10/   89]
Per-example loss in batch: 0.186158  [   12/   89]
Per-example loss in batch: 0.356266  [   14/   89]
Per-example loss in batch: 0.329280  [   16/   89]
Per-example loss in batch: 0.202083  [   18/   89]
Per-example loss in batch: 0.268273  [   20/   89]
Per-example loss in batch: 0.346163  [   22/   89]
Per-example loss in batch: 0.351264  [   24/   89]
Per-example loss in batch: 0.246175  [   26/   89]
Per-example loss in batch: 0.319304  [   28/   89]
Per-example loss in batch: 0.207402  [   30/   89]
Per-example loss in batch: 0.283894  [   32/   89]
Per-example loss in batch: 0.196076  [   34/   89]
Per-example loss in batch: 0.212225  [   36/   89]
Per-example loss in batch: 0.267517  [   38/   89]
Per-example loss in batch: 0.213788  [   40/   89]
Per-example loss in batch: 0.230754  [   42/   89]
Per-example loss in batch: 0.282118  [   44/   89]
Per-example loss in batch: 0.224709  [   46/   89]
Per-example loss in batch: 0.283894  [   48/   89]
Per-example loss in batch: 0.318024  [   50/   89]
Per-example loss in batch: 0.234223  [   52/   89]
Per-example loss in batch: 0.263791  [   54/   89]
Per-example loss in batch: 0.265701  [   56/   89]
Per-example loss in batch: 0.265656  [   58/   89]
Per-example loss in batch: 0.184936  [   60/   89]
Per-example loss in batch: 0.208520  [   62/   89]
Per-example loss in batch: 0.274080  [   64/   89]
Per-example loss in batch: 0.223222  [   66/   89]
Per-example loss in batch: 0.218367  [   68/   89]
Per-example loss in batch: 0.229915  [   70/   89]
Per-example loss in batch: 0.268109  [   72/   89]
Per-example loss in batch: 0.237681  [   74/   89]
Per-example loss in batch: 0.250350  [   76/   89]
Per-example loss in batch: 0.288205  [   78/   89]
Per-example loss in batch: 0.229646  [   80/   89]
Per-example loss in batch: 0.336384  [   82/   89]
Per-example loss in batch: 0.287655  [   84/   89]
Per-example loss in batch: 0.285675  [   86/   89]
Per-example loss in batch: 0.237678  [   88/   89]
Per-example loss in batch: 0.656771  [   89/   89]
Train Error: Avg loss: 0.26211692
validation Error: 
 Avg loss: 0.27426657 
 F1: 0.491390 
 Precision: 0.544684 
 Recall: 0.447595
 IoU: 0.325724

test Error: 
 Avg loss: 0.25210811 
 F1: 0.526216 
 Precision: 0.592074 
 Recall: 0.473543
 IoU: 0.357051

We have finished training iteration 277
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_275_.pth
Per-example loss in batch: 0.234196  [    2/   89]
Per-example loss in batch: 0.242303  [    4/   89]
Per-example loss in batch: 0.197851  [    6/   89]
Per-example loss in batch: 0.279460  [    8/   89]
Per-example loss in batch: 0.267565  [   10/   89]
Per-example loss in batch: 0.223424  [   12/   89]
Per-example loss in batch: 0.213721  [   14/   89]
Per-example loss in batch: 0.256672  [   16/   89]
Per-example loss in batch: 0.232338  [   18/   89]
Per-example loss in batch: 0.252299  [   20/   89]
Per-example loss in batch: 0.251871  [   22/   89]
Per-example loss in batch: 0.229438  [   24/   89]
Per-example loss in batch: 0.341127  [   26/   89]
Per-example loss in batch: 0.301957  [   28/   89]
Per-example loss in batch: 0.241513  [   30/   89]
Per-example loss in batch: 0.319298  [   32/   89]
Per-example loss in batch: 0.267067  [   34/   89]
Per-example loss in batch: 0.282172  [   36/   89]
Per-example loss in batch: 0.244913  [   38/   89]
Per-example loss in batch: 0.248925  [   40/   89]
Per-example loss in batch: 0.247635  [   42/   89]
Per-example loss in batch: 0.319840  [   44/   89]
Per-example loss in batch: 0.247448  [   46/   89]
Per-example loss in batch: 0.323584  [   48/   89]
Per-example loss in batch: 0.257586  [   50/   89]
Per-example loss in batch: 0.201658  [   52/   89]
Per-example loss in batch: 0.304119  [   54/   89]
Per-example loss in batch: 0.333839  [   56/   89]
Per-example loss in batch: 0.221108  [   58/   89]
Per-example loss in batch: 0.268096  [   60/   89]
Per-example loss in batch: 0.256879  [   62/   89]
Per-example loss in batch: 0.304159  [   64/   89]
Per-example loss in batch: 0.205703  [   66/   89]
Per-example loss in batch: 0.250898  [   68/   89]
Per-example loss in batch: 0.295785  [   70/   89]
Per-example loss in batch: 0.231019  [   72/   89]
Per-example loss in batch: 0.231970  [   74/   89]
Per-example loss in batch: 0.205139  [   76/   89]
Per-example loss in batch: 0.214144  [   78/   89]
Per-example loss in batch: 0.227278  [   80/   89]
Per-example loss in batch: 0.208273  [   82/   89]
Per-example loss in batch: 0.231757  [   84/   89]
Per-example loss in batch: 0.334527  [   86/   89]
Per-example loss in batch: 0.219290  [   88/   89]
Per-example loss in batch: 0.421740  [   89/   89]
Train Error: Avg loss: 0.25799350
validation Error: 
 Avg loss: 0.27620460 
 F1: 0.490879 
 Precision: 0.556666 
 Recall: 0.438998
 IoU: 0.325275

test Error: 
 Avg loss: 0.25238141 
 F1: 0.525926 
 Precision: 0.600783 
 Recall: 0.467657
 IoU: 0.356784

We have finished training iteration 278
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_276_.pth
Per-example loss in batch: 0.220148  [    2/   89]
Per-example loss in batch: 0.213800  [    4/   89]
Per-example loss in batch: 0.204388  [    6/   89]
Per-example loss in batch: 0.276360  [    8/   89]
Per-example loss in batch: 0.238380  [   10/   89]
Per-example loss in batch: 0.248179  [   12/   89]
Per-example loss in batch: 0.250187  [   14/   89]
Per-example loss in batch: 0.282212  [   16/   89]
Per-example loss in batch: 0.262121  [   18/   89]
Per-example loss in batch: 0.203665  [   20/   89]
Per-example loss in batch: 0.181085  [   22/   89]
Per-example loss in batch: 0.280870  [   24/   89]
Per-example loss in batch: 0.236817  [   26/   89]
Per-example loss in batch: 0.221946  [   28/   89]
Per-example loss in batch: 0.251626  [   30/   89]
Per-example loss in batch: 0.225492  [   32/   89]
Per-example loss in batch: 0.258794  [   34/   89]
Per-example loss in batch: 0.236949  [   36/   89]
Per-example loss in batch: 0.235419  [   38/   89]
Per-example loss in batch: 0.213097  [   40/   89]
Per-example loss in batch: 0.314624  [   42/   89]
Per-example loss in batch: 0.203275  [   44/   89]
Per-example loss in batch: 0.207007  [   46/   89]
Per-example loss in batch: 0.284105  [   48/   89]
Per-example loss in batch: 0.316613  [   50/   89]
Per-example loss in batch: 0.252090  [   52/   89]
Per-example loss in batch: 0.270607  [   54/   89]
Per-example loss in batch: 0.262856  [   56/   89]
Per-example loss in batch: 0.237783  [   58/   89]
Per-example loss in batch: 0.231546  [   60/   89]
Per-example loss in batch: 0.215554  [   62/   89]
Per-example loss in batch: 0.337565  [   64/   89]
Per-example loss in batch: 0.297854  [   66/   89]
Per-example loss in batch: 0.217460  [   68/   89]
Per-example loss in batch: 0.233105  [   70/   89]
Per-example loss in batch: 0.237614  [   72/   89]
Per-example loss in batch: 0.213903  [   74/   89]
Per-example loss in batch: 0.350014  [   76/   89]
Per-example loss in batch: 0.223988  [   78/   89]
Per-example loss in batch: 0.246934  [   80/   89]
Per-example loss in batch: 0.305230  [   82/   89]
Per-example loss in batch: 0.309095  [   84/   89]
Per-example loss in batch: 0.308419  [   86/   89]
Per-example loss in batch: 0.203569  [   88/   89]
Per-example loss in batch: 0.485483  [   89/   89]
Train Error: Avg loss: 0.25314800
validation Error: 
 Avg loss: 0.28052335 
 F1: 0.487125 
 Precision: 0.543311 
 Recall: 0.441471
 IoU: 0.321986

test Error: 
 Avg loss: 0.25464455 
 F1: 0.520064 
 Precision: 0.584388 
 Recall: 0.468497
 IoU: 0.351410

We have finished training iteration 279
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_277_.pth
Per-example loss in batch: 0.206651  [    2/   89]
Per-example loss in batch: 0.220447  [    4/   89]
Per-example loss in batch: 0.233926  [    6/   89]
Per-example loss in batch: 0.223014  [    8/   89]
Per-example loss in batch: 0.245326  [   10/   89]
Per-example loss in batch: 0.222233  [   12/   89]
Per-example loss in batch: 0.280786  [   14/   89]
Per-example loss in batch: 0.204476  [   16/   89]
Per-example loss in batch: 0.349608  [   18/   89]
Per-example loss in batch: 0.205014  [   20/   89]
Per-example loss in batch: 0.217790  [   22/   89]
Per-example loss in batch: 0.219154  [   24/   89]
Per-example loss in batch: 0.238849  [   26/   89]
Per-example loss in batch: 0.315095  [   28/   89]
Per-example loss in batch: 0.262944  [   30/   89]
Per-example loss in batch: 0.303194  [   32/   89]
Per-example loss in batch: 0.226710  [   34/   89]
Per-example loss in batch: 0.215563  [   36/   89]
Per-example loss in batch: 0.240048  [   38/   89]
Per-example loss in batch: 0.211269  [   40/   89]
Per-example loss in batch: 0.329423  [   42/   89]
Per-example loss in batch: 0.273711  [   44/   89]
Per-example loss in batch: 0.284229  [   46/   89]
Per-example loss in batch: 0.231770  [   48/   89]
Per-example loss in batch: 0.259448  [   50/   89]
Per-example loss in batch: 0.254325  [   52/   89]
Per-example loss in batch: 0.239507  [   54/   89]
Per-example loss in batch: 0.319922  [   56/   89]
Per-example loss in batch: 0.244364  [   58/   89]
Per-example loss in batch: 0.200390  [   60/   89]
Per-example loss in batch: 0.202385  [   62/   89]
Per-example loss in batch: 0.283279  [   64/   89]
Per-example loss in batch: 0.325002  [   66/   89]
Per-example loss in batch: 0.372224  [   68/   89]
Per-example loss in batch: 0.297837  [   70/   89]
Per-example loss in batch: 0.245703  [   72/   89]
Per-example loss in batch: 0.261538  [   74/   89]
Per-example loss in batch: 0.252051  [   76/   89]
Per-example loss in batch: 0.224465  [   78/   89]
Per-example loss in batch: 0.219545  [   80/   89]
Per-example loss in batch: 0.257819  [   82/   89]
Per-example loss in batch: 0.299468  [   84/   89]
Per-example loss in batch: 0.284887  [   86/   89]
Per-example loss in batch: 0.238406  [   88/   89]
Per-example loss in batch: 0.620218  [   89/   89]
Train Error: Avg loss: 0.25963835
validation Error: 
 Avg loss: 0.26991199 
 F1: 0.491945 
 Precision: 0.556427 
 Recall: 0.440856
 IoU: 0.326212

test Error: 
 Avg loss: 0.25262458 
 F1: 0.524625 
 Precision: 0.600329 
 Recall: 0.465876
 IoU: 0.355587

We have finished training iteration 280
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_278_.pth
Per-example loss in batch: 0.231927  [    2/   89]
Per-example loss in batch: 0.215743  [    4/   89]
Per-example loss in batch: 0.194785  [    6/   89]
Per-example loss in batch: 0.303215  [    8/   89]
Per-example loss in batch: 0.235852  [   10/   89]
Per-example loss in batch: 0.312362  [   12/   89]
Per-example loss in batch: 0.264449  [   14/   89]
Per-example loss in batch: 0.213519  [   16/   89]
Per-example loss in batch: 0.216107  [   18/   89]
Per-example loss in batch: 0.214301  [   20/   89]
Per-example loss in batch: 0.295082  [   22/   89]
Per-example loss in batch: 0.261797  [   24/   89]
Per-example loss in batch: 0.269262  [   26/   89]
Per-example loss in batch: 0.349569  [   28/   89]
Per-example loss in batch: 0.211794  [   30/   89]
Per-example loss in batch: 0.296637  [   32/   89]
Per-example loss in batch: 0.237978  [   34/   89]
Per-example loss in batch: 0.268945  [   36/   89]
Per-example loss in batch: 0.219835  [   38/   89]
Per-example loss in batch: 0.260792  [   40/   89]
Per-example loss in batch: 0.276246  [   42/   89]
Per-example loss in batch: 0.339676  [   44/   89]
Per-example loss in batch: 0.188036  [   46/   89]
Per-example loss in batch: 0.249831  [   48/   89]
Per-example loss in batch: 0.302137  [   50/   89]
Per-example loss in batch: 0.262927  [   52/   89]
Per-example loss in batch: 0.264747  [   54/   89]
Per-example loss in batch: 0.272640  [   56/   89]
Per-example loss in batch: 0.256589  [   58/   89]
Per-example loss in batch: 0.242067  [   60/   89]
Per-example loss in batch: 0.285687  [   62/   89]
Per-example loss in batch: 0.208100  [   64/   89]
Per-example loss in batch: 0.220469  [   66/   89]
Per-example loss in batch: 0.294585  [   68/   89]
Per-example loss in batch: 0.240704  [   70/   89]
Per-example loss in batch: 0.217431  [   72/   89]
Per-example loss in batch: 0.282176  [   74/   89]
Per-example loss in batch: 0.330495  [   76/   89]
Per-example loss in batch: 0.303790  [   78/   89]
Per-example loss in batch: 0.215205  [   80/   89]
Per-example loss in batch: 0.234728  [   82/   89]
Per-example loss in batch: 0.205488  [   84/   89]
Per-example loss in batch: 0.214604  [   86/   89]
Per-example loss in batch: 0.277660  [   88/   89]
Per-example loss in batch: 0.563198  [   89/   89]
Train Error: Avg loss: 0.25936115
validation Error: 
 Avg loss: 0.27942294 
 F1: 0.491708 
 Precision: 0.565611 
 Recall: 0.434885
 IoU: 0.326003

test Error: 
 Avg loss: 0.25384577 
 F1: 0.522971 
 Precision: 0.606017 
 Recall: 0.459942
 IoU: 0.354069

We have finished training iteration 281
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_279_.pth
Per-example loss in batch: 0.259238  [    2/   89]
Per-example loss in batch: 0.249461  [    4/   89]
Per-example loss in batch: 0.254003  [    6/   89]
Per-example loss in batch: 0.188479  [    8/   89]
Per-example loss in batch: 0.281486  [   10/   89]
Per-example loss in batch: 0.317255  [   12/   89]
Per-example loss in batch: 0.291038  [   14/   89]
Per-example loss in batch: 0.223330  [   16/   89]
Per-example loss in batch: 0.266316  [   18/   89]
Per-example loss in batch: 0.284341  [   20/   89]
Per-example loss in batch: 0.287745  [   22/   89]
Per-example loss in batch: 0.244301  [   24/   89]
Per-example loss in batch: 0.326269  [   26/   89]
Per-example loss in batch: 0.272653  [   28/   89]
Per-example loss in batch: 0.226930  [   30/   89]
Per-example loss in batch: 0.304752  [   32/   89]
Per-example loss in batch: 0.256015  [   34/   89]
Per-example loss in batch: 0.222542  [   36/   89]
Per-example loss in batch: 0.223884  [   38/   89]
Per-example loss in batch: 0.241220  [   40/   89]
Per-example loss in batch: 0.320084  [   42/   89]
Per-example loss in batch: 0.201042  [   44/   89]
Per-example loss in batch: 0.218233  [   46/   89]
Per-example loss in batch: 0.261801  [   48/   89]
Per-example loss in batch: 0.254131  [   50/   89]
Per-example loss in batch: 0.223818  [   52/   89]
Per-example loss in batch: 0.361123  [   54/   89]
Per-example loss in batch: 0.211015  [   56/   89]
Per-example loss in batch: 0.250019  [   58/   89]
Per-example loss in batch: 0.294682  [   60/   89]
Per-example loss in batch: 0.217972  [   62/   89]
Per-example loss in batch: 0.195168  [   64/   89]
Per-example loss in batch: 0.267031  [   66/   89]
Per-example loss in batch: 0.292908  [   68/   89]
Per-example loss in batch: 0.224816  [   70/   89]
Per-example loss in batch: 0.246783  [   72/   89]
Per-example loss in batch: 0.236410  [   74/   89]
Per-example loss in batch: 0.287385  [   76/   89]
Per-example loss in batch: 0.297886  [   78/   89]
Per-example loss in batch: 0.219387  [   80/   89]
Per-example loss in batch: 0.214397  [   82/   89]
Per-example loss in batch: 0.276047  [   84/   89]
Per-example loss in batch: 0.310301  [   86/   89]
Per-example loss in batch: 0.256197  [   88/   89]
Per-example loss in batch: 0.707826  [   89/   89]
Train Error: Avg loss: 0.26323160
validation Error: 
 Avg loss: 0.27790988 
 F1: 0.493914 
 Precision: 0.547410 
 Recall: 0.449943
 IoU: 0.327945

test Error: 
 Avg loss: 0.25051428 
 F1: 0.529881 
 Precision: 0.593688 
 Recall: 0.478459
 IoU: 0.360434

We have finished training iteration 282
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_280_.pth
Per-example loss in batch: 0.213008  [    2/   89]
Per-example loss in batch: 0.306901  [    4/   89]
Per-example loss in batch: 0.292054  [    6/   89]
Per-example loss in batch: 0.205213  [    8/   89]
Per-example loss in batch: 0.289115  [   10/   89]
Per-example loss in batch: 0.260281  [   12/   89]
Per-example loss in batch: 0.275361  [   14/   89]
Per-example loss in batch: 0.250519  [   16/   89]
Per-example loss in batch: 0.333906  [   18/   89]
Per-example loss in batch: 0.276536  [   20/   89]
Per-example loss in batch: 0.262497  [   22/   89]
Per-example loss in batch: 0.246327  [   24/   89]
Per-example loss in batch: 0.275095  [   26/   89]
Per-example loss in batch: 0.316084  [   28/   89]
Per-example loss in batch: 0.191523  [   30/   89]
Per-example loss in batch: 0.192192  [   32/   89]
Per-example loss in batch: 0.289164  [   34/   89]
Per-example loss in batch: 0.206147  [   36/   89]
Per-example loss in batch: 0.233758  [   38/   89]
Per-example loss in batch: 0.215925  [   40/   89]
Per-example loss in batch: 0.327716  [   42/   89]
Per-example loss in batch: 0.211486  [   44/   89]
Per-example loss in batch: 0.234397  [   46/   89]
Per-example loss in batch: 0.273482  [   48/   89]
Per-example loss in batch: 0.278244  [   50/   89]
Per-example loss in batch: 0.278029  [   52/   89]
Per-example loss in batch: 0.273008  [   54/   89]
Per-example loss in batch: 0.284139  [   56/   89]
Per-example loss in batch: 0.273369  [   58/   89]
Per-example loss in batch: 0.199313  [   60/   89]
Per-example loss in batch: 0.212808  [   62/   89]
Per-example loss in batch: 0.237660  [   64/   89]
Per-example loss in batch: 0.229689  [   66/   89]
Per-example loss in batch: 0.242676  [   68/   89]
Per-example loss in batch: 0.297280  [   70/   89]
Per-example loss in batch: 0.252392  [   72/   89]
Per-example loss in batch: 0.316311  [   74/   89]
Per-example loss in batch: 0.203177  [   76/   89]
Per-example loss in batch: 0.242803  [   78/   89]
Per-example loss in batch: 0.191106  [   80/   89]
Per-example loss in batch: 0.207769  [   82/   89]
Per-example loss in batch: 0.313432  [   84/   89]
Per-example loss in batch: 0.233504  [   86/   89]
Per-example loss in batch: 0.233569  [   88/   89]
Per-example loss in batch: 0.478369  [   89/   89]
Train Error: Avg loss: 0.25658767
validation Error: 
 Avg loss: 0.27146658 
 F1: 0.489515 
 Precision: 0.583538 
 Recall: 0.421587
 IoU: 0.324078

test Error: 
 Avg loss: 0.25529164 
 F1: 0.519562 
 Precision: 0.621993 
 Recall: 0.446097
 IoU: 0.350951

We have finished training iteration 283
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_281_.pth
Per-example loss in batch: 0.286164  [    2/   89]
Per-example loss in batch: 0.306527  [    4/   89]
Per-example loss in batch: 0.274110  [    6/   89]
Per-example loss in batch: 0.299275  [    8/   89]
Per-example loss in batch: 0.336042  [   10/   89]
Per-example loss in batch: 0.222532  [   12/   89]
Per-example loss in batch: 0.255700  [   14/   89]
Per-example loss in batch: 0.256055  [   16/   89]
Per-example loss in batch: 0.308917  [   18/   89]
Per-example loss in batch: 0.230463  [   20/   89]
Per-example loss in batch: 0.288384  [   22/   89]
Per-example loss in batch: 0.305361  [   24/   89]
Per-example loss in batch: 0.264125  [   26/   89]
Per-example loss in batch: 0.214384  [   28/   89]
Per-example loss in batch: 0.247483  [   30/   89]
Per-example loss in batch: 0.255376  [   32/   89]
Per-example loss in batch: 0.312909  [   34/   89]
Per-example loss in batch: 0.258453  [   36/   89]
Per-example loss in batch: 0.313254  [   38/   89]
Per-example loss in batch: 0.210366  [   40/   89]
Per-example loss in batch: 0.231704  [   42/   89]
Per-example loss in batch: 0.318165  [   44/   89]
Per-example loss in batch: 0.241497  [   46/   89]
Per-example loss in batch: 0.211138  [   48/   89]
Per-example loss in batch: 0.207197  [   50/   89]
Per-example loss in batch: 0.251038  [   52/   89]
Per-example loss in batch: 0.214987  [   54/   89]
Per-example loss in batch: 0.224077  [   56/   89]
Per-example loss in batch: 0.308110  [   58/   89]
Per-example loss in batch: 0.263964  [   60/   89]
Per-example loss in batch: 0.222184  [   62/   89]
Per-example loss in batch: 0.193814  [   64/   89]
Per-example loss in batch: 0.249364  [   66/   89]
Per-example loss in batch: 0.254769  [   68/   89]
Per-example loss in batch: 0.230884  [   70/   89]
Per-example loss in batch: 0.216305  [   72/   89]
Per-example loss in batch: 0.211908  [   74/   89]
Per-example loss in batch: 0.284644  [   76/   89]
Per-example loss in batch: 0.230789  [   78/   89]
Per-example loss in batch: 0.376404  [   80/   89]
Per-example loss in batch: 0.207984  [   82/   89]
Per-example loss in batch: 0.209171  [   84/   89]
Per-example loss in batch: 0.211424  [   86/   89]
Per-example loss in batch: 0.255735  [   88/   89]
Per-example loss in batch: 0.606506  [   89/   89]
Train Error: Avg loss: 0.26014355
validation Error: 
 Avg loss: 0.27541293 
 F1: 0.492838 
 Precision: 0.568729 
 Recall: 0.434817
 IoU: 0.326998

test Error: 
 Avg loss: 0.25334167 
 F1: 0.523269 
 Precision: 0.607499 
 Recall: 0.459551
 IoU: 0.354342

We have finished training iteration 284
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_282_.pth
Per-example loss in batch: 0.199423  [    2/   89]
Per-example loss in batch: 0.255910  [    4/   89]
Per-example loss in batch: 0.218048  [    6/   89]
Per-example loss in batch: 0.213342  [    8/   89]
Per-example loss in batch: 0.291874  [   10/   89]
Per-example loss in batch: 0.228723  [   12/   89]
Per-example loss in batch: 0.297056  [   14/   89]
Per-example loss in batch: 0.282922  [   16/   89]
Per-example loss in batch: 0.324089  [   18/   89]
Per-example loss in batch: 0.296410  [   20/   89]
Per-example loss in batch: 0.306041  [   22/   89]
Per-example loss in batch: 0.236225  [   24/   89]
Per-example loss in batch: 0.276411  [   26/   89]
Per-example loss in batch: 0.218576  [   28/   89]
Per-example loss in batch: 0.385520  [   30/   89]
Per-example loss in batch: 0.230375  [   32/   89]
Per-example loss in batch: 0.260813  [   34/   89]
Per-example loss in batch: 0.238798  [   36/   89]
Per-example loss in batch: 0.193467  [   38/   89]
Per-example loss in batch: 0.196541  [   40/   89]
Per-example loss in batch: 0.294403  [   42/   89]
Per-example loss in batch: 0.241465  [   44/   89]
Per-example loss in batch: 0.237011  [   46/   89]
Per-example loss in batch: 0.212550  [   48/   89]
Per-example loss in batch: 0.292033  [   50/   89]
Per-example loss in batch: 0.212540  [   52/   89]
Per-example loss in batch: 0.226784  [   54/   89]
Per-example loss in batch: 0.250875  [   56/   89]
Per-example loss in batch: 0.236697  [   58/   89]
Per-example loss in batch: 0.229595  [   60/   89]
Per-example loss in batch: 0.246608  [   62/   89]
Per-example loss in batch: 0.227435  [   64/   89]
Per-example loss in batch: 0.261850  [   66/   89]
Per-example loss in batch: 0.235768  [   68/   89]
Per-example loss in batch: 0.228052  [   70/   89]
Per-example loss in batch: 0.287612  [   72/   89]
Per-example loss in batch: 0.205636  [   74/   89]
Per-example loss in batch: 0.259290  [   76/   89]
Per-example loss in batch: 0.234113  [   78/   89]
Per-example loss in batch: 0.274007  [   80/   89]
Per-example loss in batch: 0.330686  [   82/   89]
Per-example loss in batch: 0.254652  [   84/   89]
Per-example loss in batch: 0.320026  [   86/   89]
Per-example loss in batch: 0.212713  [   88/   89]
Per-example loss in batch: 0.599848  [   89/   89]
Train Error: Avg loss: 0.25759304
validation Error: 
 Avg loss: 0.27240730 
 F1: 0.491092 
 Precision: 0.573191 
 Recall: 0.429564
 IoU: 0.325462

test Error: 
 Avg loss: 0.25459412 
 F1: 0.521261 
 Precision: 0.613388 
 Recall: 0.453193
 IoU: 0.352503

We have finished training iteration 285
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_283_.pth
Per-example loss in batch: 0.192328  [    2/   89]
Per-example loss in batch: 0.287787  [    4/   89]
Per-example loss in batch: 0.199339  [    6/   89]
Per-example loss in batch: 0.208786  [    8/   89]
Per-example loss in batch: 0.223472  [   10/   89]
Per-example loss in batch: 0.273354  [   12/   89]
Per-example loss in batch: 0.318271  [   14/   89]
Per-example loss in batch: 0.229823  [   16/   89]
Per-example loss in batch: 0.302166  [   18/   89]
Per-example loss in batch: 0.304971  [   20/   89]
Per-example loss in batch: 0.228601  [   22/   89]
Per-example loss in batch: 0.256976  [   24/   89]
Per-example loss in batch: 0.264453  [   26/   89]
Per-example loss in batch: 0.205321  [   28/   89]
Per-example loss in batch: 0.259223  [   30/   89]
Per-example loss in batch: 0.233761  [   32/   89]
Per-example loss in batch: 0.202680  [   34/   89]
Per-example loss in batch: 0.230109  [   36/   89]
Per-example loss in batch: 0.274586  [   38/   89]
Per-example loss in batch: 0.242906  [   40/   89]
Per-example loss in batch: 0.289704  [   42/   89]
Per-example loss in batch: 0.241604  [   44/   89]
Per-example loss in batch: 0.293097  [   46/   89]
Per-example loss in batch: 0.250842  [   48/   89]
Per-example loss in batch: 0.244810  [   50/   89]
Per-example loss in batch: 0.283220  [   52/   89]
Per-example loss in batch: 0.203721  [   54/   89]
Per-example loss in batch: 0.247434  [   56/   89]
Per-example loss in batch: 0.273981  [   58/   89]
Per-example loss in batch: 0.289112  [   60/   89]
Per-example loss in batch: 0.223691  [   62/   89]
Per-example loss in batch: 0.190926  [   64/   89]
Per-example loss in batch: 0.308303  [   66/   89]
Per-example loss in batch: 0.225518  [   68/   89]
Per-example loss in batch: 0.224862  [   70/   89]
Per-example loss in batch: 0.266515  [   72/   89]
Per-example loss in batch: 0.204916  [   74/   89]
Per-example loss in batch: 0.232736  [   76/   89]
Per-example loss in batch: 0.334185  [   78/   89]
Per-example loss in batch: 0.230044  [   80/   89]
Per-example loss in batch: 0.300873  [   82/   89]
Per-example loss in batch: 0.302721  [   84/   89]
Per-example loss in batch: 0.205005  [   86/   89]
Per-example loss in batch: 0.228307  [   88/   89]
Per-example loss in batch: 0.572111  [   89/   89]
Train Error: Avg loss: 0.25440668
validation Error: 
 Avg loss: 0.27429310 
 F1: 0.490614 
 Precision: 0.567765 
 Recall: 0.431923
 IoU: 0.325042

test Error: 
 Avg loss: 0.25290070 
 F1: 0.525000 
 Precision: 0.612199 
 Recall: 0.459544
 IoU: 0.355932

We have finished training iteration 286
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_284_.pth
Per-example loss in batch: 0.225533  [    2/   89]
Per-example loss in batch: 0.345501  [    4/   89]
Per-example loss in batch: 0.203791  [    6/   89]
Per-example loss in batch: 0.208249  [    8/   89]
Per-example loss in batch: 0.218710  [   10/   89]
Per-example loss in batch: 0.298351  [   12/   89]
Per-example loss in batch: 0.286970  [   14/   89]
Per-example loss in batch: 0.223252  [   16/   89]
Per-example loss in batch: 0.263597  [   18/   89]
Per-example loss in batch: 0.215297  [   20/   89]
Per-example loss in batch: 0.305241  [   22/   89]
Per-example loss in batch: 0.303241  [   24/   89]
Per-example loss in batch: 0.236616  [   26/   89]
Per-example loss in batch: 0.253078  [   28/   89]
Per-example loss in batch: 0.217531  [   30/   89]
Per-example loss in batch: 0.251009  [   32/   89]
Per-example loss in batch: 0.314375  [   34/   89]
Per-example loss in batch: 0.324751  [   36/   89]
Per-example loss in batch: 0.228714  [   38/   89]
Per-example loss in batch: 0.188831  [   40/   89]
Per-example loss in batch: 0.239518  [   42/   89]
Per-example loss in batch: 0.256937  [   44/   89]
Per-example loss in batch: 0.261654  [   46/   89]
Per-example loss in batch: 0.278699  [   48/   89]
Per-example loss in batch: 0.246478  [   50/   89]
Per-example loss in batch: 0.220759  [   52/   89]
Per-example loss in batch: 0.287607  [   54/   89]
Per-example loss in batch: 0.229510  [   56/   89]
Per-example loss in batch: 0.223463  [   58/   89]
Per-example loss in batch: 0.257735  [   60/   89]
Per-example loss in batch: 0.256298  [   62/   89]
Per-example loss in batch: 0.242649  [   64/   89]
Per-example loss in batch: 0.262773  [   66/   89]
Per-example loss in batch: 0.247888  [   68/   89]
Per-example loss in batch: 0.267746  [   70/   89]
Per-example loss in batch: 0.296834  [   72/   89]
Per-example loss in batch: 0.344870  [   74/   89]
Per-example loss in batch: 0.234553  [   76/   89]
Per-example loss in batch: 0.276381  [   78/   89]
Per-example loss in batch: 0.289992  [   80/   89]
Per-example loss in batch: 0.246855  [   82/   89]
Per-example loss in batch: 0.233680  [   84/   89]
Per-example loss in batch: 0.216719  [   86/   89]
Per-example loss in batch: 0.181642  [   88/   89]
Per-example loss in batch: 0.624405  [   89/   89]
Train Error: Avg loss: 0.25901307
validation Error: 
 Avg loss: 0.27060772 
 F1: 0.491198 
 Precision: 0.572447 
 Recall: 0.430147
 IoU: 0.325555

test Error: 
 Avg loss: 0.25236229 
 F1: 0.525807 
 Precision: 0.616361 
 Recall: 0.458453
 IoU: 0.356675

We have finished training iteration 287
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_285_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.240155  [    2/   89]
Per-example loss in batch: 0.227408  [    4/   89]
Per-example loss in batch: 0.200705  [    6/   89]
Per-example loss in batch: 0.246946  [    8/   89]
Per-example loss in batch: 0.246923  [   10/   89]
Per-example loss in batch: 0.275323  [   12/   89]
Per-example loss in batch: 0.237057  [   14/   89]
Per-example loss in batch: 0.238946  [   16/   89]
Per-example loss in batch: 0.293218  [   18/   89]
Per-example loss in batch: 0.191251  [   20/   89]
Per-example loss in batch: 0.352775  [   22/   89]
Per-example loss in batch: 0.220642  [   24/   89]
Per-example loss in batch: 0.258143  [   26/   89]
Per-example loss in batch: 0.263690  [   28/   89]
Per-example loss in batch: 0.209628  [   30/   89]
Per-example loss in batch: 0.237597  [   32/   89]
Per-example loss in batch: 0.187264  [   34/   89]
Per-example loss in batch: 0.213226  [   36/   89]
Per-example loss in batch: 0.264929  [   38/   89]
Per-example loss in batch: 0.279291  [   40/   89]
Per-example loss in batch: 0.237789  [   42/   89]
Per-example loss in batch: 0.235619  [   44/   89]
Per-example loss in batch: 0.209037  [   46/   89]
Per-example loss in batch: 0.300079  [   48/   89]
Per-example loss in batch: 0.300002  [   50/   89]
Per-example loss in batch: 0.301137  [   52/   89]
Per-example loss in batch: 0.234926  [   54/   89]
Per-example loss in batch: 0.252361  [   56/   89]
Per-example loss in batch: 0.267405  [   58/   89]
Per-example loss in batch: 0.246839  [   60/   89]
Per-example loss in batch: 0.252922  [   62/   89]
Per-example loss in batch: 0.287861  [   64/   89]
Per-example loss in batch: 0.303509  [   66/   89]
Per-example loss in batch: 0.249564  [   68/   89]
Per-example loss in batch: 0.216172  [   70/   89]
Per-example loss in batch: 0.279330  [   72/   89]
Per-example loss in batch: 0.334119  [   74/   89]
Per-example loss in batch: 0.203387  [   76/   89]
Per-example loss in batch: 0.365246  [   78/   89]
Per-example loss in batch: 0.201141  [   80/   89]
Per-example loss in batch: 0.206646  [   82/   89]
Per-example loss in batch: 0.286102  [   84/   89]
Per-example loss in batch: 0.310944  [   86/   89]
Per-example loss in batch: 0.272200  [   88/   89]
Per-example loss in batch: 0.348830  [   89/   89]
Train Error: Avg loss: 0.25649150
validation Error: 
 Avg loss: 0.27569521 
 F1: 0.488672 
 Precision: 0.556908 
 Recall: 0.435332
 IoU: 0.323340

test Error: 
 Avg loss: 0.25437393 
 F1: 0.521541 
 Precision: 0.601732 
 Recall: 0.460210
 IoU: 0.352760

We have finished training iteration 288
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_286_.pth
Per-example loss in batch: 0.250648  [    2/   89]
Per-example loss in batch: 0.268648  [    4/   89]
Per-example loss in batch: 0.224253  [    6/   89]
Per-example loss in batch: 0.271815  [    8/   89]
Per-example loss in batch: 0.275238  [   10/   89]
Per-example loss in batch: 0.202171  [   12/   89]
Per-example loss in batch: 0.305151  [   14/   89]
Per-example loss in batch: 0.271228  [   16/   89]
Per-example loss in batch: 0.212461  [   18/   89]
Per-example loss in batch: 0.336946  [   20/   89]
Per-example loss in batch: 0.267178  [   22/   89]
Per-example loss in batch: 0.317865  [   24/   89]
Per-example loss in batch: 0.251742  [   26/   89]
Per-example loss in batch: 0.218804  [   28/   89]
Per-example loss in batch: 0.212372  [   30/   89]
Per-example loss in batch: 0.330284  [   32/   89]
Per-example loss in batch: 0.305238  [   34/   89]
Per-example loss in batch: 0.352573  [   36/   89]
Per-example loss in batch: 0.276536  [   38/   89]
Per-example loss in batch: 0.268803  [   40/   89]
Per-example loss in batch: 0.253312  [   42/   89]
Per-example loss in batch: 0.296027  [   44/   89]
Per-example loss in batch: 0.241722  [   46/   89]
Per-example loss in batch: 0.212250  [   48/   89]
Per-example loss in batch: 0.193414  [   50/   89]
Per-example loss in batch: 0.245284  [   52/   89]
Per-example loss in batch: 0.242744  [   54/   89]
Per-example loss in batch: 0.292203  [   56/   89]
Per-example loss in batch: 0.241127  [   58/   89]
Per-example loss in batch: 0.317769  [   60/   89]
Per-example loss in batch: 0.198607  [   62/   89]
Per-example loss in batch: 0.308004  [   64/   89]
Per-example loss in batch: 0.245152  [   66/   89]
Per-example loss in batch: 0.221010  [   68/   89]
Per-example loss in batch: 0.212526  [   70/   89]
Per-example loss in batch: 0.289362  [   72/   89]
Per-example loss in batch: 0.299553  [   74/   89]
Per-example loss in batch: 0.253820  [   76/   89]
Per-example loss in batch: 0.328741  [   78/   89]
Per-example loss in batch: 0.186626  [   80/   89]
Per-example loss in batch: 0.211925  [   82/   89]
Per-example loss in batch: 0.217803  [   84/   89]
Per-example loss in batch: 0.250407  [   86/   89]
Per-example loss in batch: 0.205163  [   88/   89]
Per-example loss in batch: 0.548758  [   89/   89]
Train Error: Avg loss: 0.26199738
validation Error: 
 Avg loss: 0.27429393 
 F1: 0.490063 
 Precision: 0.550982 
 Recall: 0.441273
 IoU: 0.324558

test Error: 
 Avg loss: 0.25270016 
 F1: 0.524726 
 Precision: 0.596691 
 Recall: 0.468251
 IoU: 0.355680

We have finished training iteration 289
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_287_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.316459  [    2/   89]
Per-example loss in batch: 0.223263  [    4/   89]
Per-example loss in batch: 0.202214  [    6/   89]
Per-example loss in batch: 0.276350  [    8/   89]
Per-example loss in batch: 0.267128  [   10/   89]
Per-example loss in batch: 0.289098  [   12/   89]
Per-example loss in batch: 0.272133  [   14/   89]
Per-example loss in batch: 0.228518  [   16/   89]
Per-example loss in batch: 0.195231  [   18/   89]
Per-example loss in batch: 0.250796  [   20/   89]
Per-example loss in batch: 0.255920  [   22/   89]
Per-example loss in batch: 0.219270  [   24/   89]
Per-example loss in batch: 0.215348  [   26/   89]
Per-example loss in batch: 0.181188  [   28/   89]
Per-example loss in batch: 0.321051  [   30/   89]
Per-example loss in batch: 0.312830  [   32/   89]
Per-example loss in batch: 0.274265  [   34/   89]
Per-example loss in batch: 0.303374  [   36/   89]
Per-example loss in batch: 0.264644  [   38/   89]
Per-example loss in batch: 0.180024  [   40/   89]
Per-example loss in batch: 0.263812  [   42/   89]
Per-example loss in batch: 0.229645  [   44/   89]
Per-example loss in batch: 0.250217  [   46/   89]
Per-example loss in batch: 0.222943  [   48/   89]
Per-example loss in batch: 0.235366  [   50/   89]
Per-example loss in batch: 0.230682  [   52/   89]
Per-example loss in batch: 0.299244  [   54/   89]
Per-example loss in batch: 0.320842  [   56/   89]
Per-example loss in batch: 0.344960  [   58/   89]
Per-example loss in batch: 0.307351  [   60/   89]
Per-example loss in batch: 0.260010  [   62/   89]
Per-example loss in batch: 0.221556  [   64/   89]
Per-example loss in batch: 0.282834  [   66/   89]
Per-example loss in batch: 0.233225  [   68/   89]
Per-example loss in batch: 0.278734  [   70/   89]
Per-example loss in batch: 0.245093  [   72/   89]
Per-example loss in batch: 0.236993  [   74/   89]
Per-example loss in batch: 0.198180  [   76/   89]
Per-example loss in batch: 0.222816  [   78/   89]
Per-example loss in batch: 0.273930  [   80/   89]
Per-example loss in batch: 0.247736  [   82/   89]
Per-example loss in batch: 0.216981  [   84/   89]
Per-example loss in batch: 0.276316  [   86/   89]
Per-example loss in batch: 0.355270  [   88/   89]
Per-example loss in batch: 0.677576  [   89/   89]
Train Error: Avg loss: 0.26163217
validation Error: 
 Avg loss: 0.27199488 
 F1: 0.489108 
 Precision: 0.548777 
 Recall: 0.441142
 IoU: 0.323721

test Error: 
 Avg loss: 0.25313047 
 F1: 0.524056 
 Precision: 0.594990 
 Recall: 0.468234
 IoU: 0.355065

We have finished training iteration 290
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_288_.pth
Per-example loss in batch: 0.286028  [    2/   89]
Per-example loss in batch: 0.205829  [    4/   89]
Per-example loss in batch: 0.240493  [    6/   89]
Per-example loss in batch: 0.223822  [    8/   89]
Per-example loss in batch: 0.253767  [   10/   89]
Per-example loss in batch: 0.286329  [   12/   89]
Per-example loss in batch: 0.318466  [   14/   89]
Per-example loss in batch: 0.334692  [   16/   89]
Per-example loss in batch: 0.206082  [   18/   89]
Per-example loss in batch: 0.236386  [   20/   89]
Per-example loss in batch: 0.221591  [   22/   89]
Per-example loss in batch: 0.249343  [   24/   89]
Per-example loss in batch: 0.233048  [   26/   89]
Per-example loss in batch: 0.212408  [   28/   89]
Per-example loss in batch: 0.302038  [   30/   89]
Per-example loss in batch: 0.240552  [   32/   89]
Per-example loss in batch: 0.253733  [   34/   89]
Per-example loss in batch: 0.220226  [   36/   89]
Per-example loss in batch: 0.323292  [   38/   89]
Per-example loss in batch: 0.218752  [   40/   89]
Per-example loss in batch: 0.293514  [   42/   89]
Per-example loss in batch: 0.175028  [   44/   89]
Per-example loss in batch: 0.242054  [   46/   89]
Per-example loss in batch: 0.227254  [   48/   89]
Per-example loss in batch: 0.248396  [   50/   89]
Per-example loss in batch: 0.239721  [   52/   89]
Per-example loss in batch: 0.240238  [   54/   89]
Per-example loss in batch: 0.195097  [   56/   89]
Per-example loss in batch: 0.204686  [   58/   89]
Per-example loss in batch: 0.251544  [   60/   89]
Per-example loss in batch: 0.339717  [   62/   89]
Per-example loss in batch: 0.311181  [   64/   89]
Per-example loss in batch: 0.326998  [   66/   89]
Per-example loss in batch: 0.199617  [   68/   89]
Per-example loss in batch: 0.316757  [   70/   89]
Per-example loss in batch: 0.267219  [   72/   89]
Per-example loss in batch: 0.266180  [   74/   89]
Per-example loss in batch: 0.246261  [   76/   89]
Per-example loss in batch: 0.309104  [   78/   89]
Per-example loss in batch: 0.233368  [   80/   89]
Per-example loss in batch: 0.262631  [   82/   89]
Per-example loss in batch: 0.298670  [   84/   89]
Per-example loss in batch: 0.225665  [   86/   89]
Per-example loss in batch: 0.293112  [   88/   89]
Per-example loss in batch: 0.644525  [   89/   89]
Train Error: Avg loss: 0.26074499
validation Error: 
 Avg loss: 0.27190849 
 F1: 0.490962 
 Precision: 0.556432 
 Recall: 0.439277
 IoU: 0.325348

test Error: 
 Avg loss: 0.25245882 
 F1: 0.525185 
 Precision: 0.602847 
 Recall: 0.465249
 IoU: 0.356102

We have finished training iteration 291
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_289_.pth
Per-example loss in batch: 0.222820  [    2/   89]
Per-example loss in batch: 0.250805  [    4/   89]
Per-example loss in batch: 0.338834  [    6/   89]
Per-example loss in batch: 0.247524  [    8/   89]
Per-example loss in batch: 0.200154  [   10/   89]
Per-example loss in batch: 0.257347  [   12/   89]
Per-example loss in batch: 0.222338  [   14/   89]
Per-example loss in batch: 0.203367  [   16/   89]
Per-example loss in batch: 0.222119  [   18/   89]
Per-example loss in batch: 0.184225  [   20/   89]
Per-example loss in batch: 0.239770  [   22/   89]
Per-example loss in batch: 0.284196  [   24/   89]
Per-example loss in batch: 0.323334  [   26/   89]
Per-example loss in batch: 0.284212  [   28/   89]
Per-example loss in batch: 0.211994  [   30/   89]
Per-example loss in batch: 0.275929  [   32/   89]
Per-example loss in batch: 0.217690  [   34/   89]
Per-example loss in batch: 0.199837  [   36/   89]
Per-example loss in batch: 0.291957  [   38/   89]
Per-example loss in batch: 0.234996  [   40/   89]
Per-example loss in batch: 0.210690  [   42/   89]
Per-example loss in batch: 0.297832  [   44/   89]
Per-example loss in batch: 0.221596  [   46/   89]
Per-example loss in batch: 0.229792  [   48/   89]
Per-example loss in batch: 0.262505  [   50/   89]
Per-example loss in batch: 0.284788  [   52/   89]
Per-example loss in batch: 0.204524  [   54/   89]
Per-example loss in batch: 0.228641  [   56/   89]
Per-example loss in batch: 0.296981  [   58/   89]
Per-example loss in batch: 0.231371  [   60/   89]
Per-example loss in batch: 0.212834  [   62/   89]
Per-example loss in batch: 0.319507  [   64/   89]
Per-example loss in batch: 0.322161  [   66/   89]
Per-example loss in batch: 0.267059  [   68/   89]
Per-example loss in batch: 0.242697  [   70/   89]
Per-example loss in batch: 0.332891  [   72/   89]
Per-example loss in batch: 0.239061  [   74/   89]
Per-example loss in batch: 0.207271  [   76/   89]
Per-example loss in batch: 0.266070  [   78/   89]
Per-example loss in batch: 0.311069  [   80/   89]
Per-example loss in batch: 0.254962  [   82/   89]
Per-example loss in batch: 0.308335  [   84/   89]
Per-example loss in batch: 0.285590  [   86/   89]
Per-example loss in batch: 0.224849  [   88/   89]
Per-example loss in batch: 0.592549  [   89/   89]
Train Error: Avg loss: 0.25781574
validation Error: 
 Avg loss: 0.27030193 
 F1: 0.492030 
 Precision: 0.576204 
 Recall: 0.429314
 IoU: 0.326286

test Error: 
 Avg loss: 0.25368678 
 F1: 0.522326 
 Precision: 0.617406 
 Recall: 0.452622
 IoU: 0.353478

We have finished training iteration 292
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_290_.pth
Per-example loss in batch: 0.248645  [    2/   89]
Per-example loss in batch: 0.323575  [    4/   89]
Per-example loss in batch: 0.273558  [    6/   89]
Per-example loss in batch: 0.231439  [    8/   89]
Per-example loss in batch: 0.194040  [   10/   89]
Per-example loss in batch: 0.262765  [   12/   89]
Per-example loss in batch: 0.301356  [   14/   89]
Per-example loss in batch: 0.289253  [   16/   89]
Per-example loss in batch: 0.256616  [   18/   89]
Per-example loss in batch: 0.236151  [   20/   89]
Per-example loss in batch: 0.248920  [   22/   89]
Per-example loss in batch: 0.194287  [   24/   89]
Per-example loss in batch: 0.205351  [   26/   89]
Per-example loss in batch: 0.226444  [   28/   89]
Per-example loss in batch: 0.347892  [   30/   89]
Per-example loss in batch: 0.237136  [   32/   89]
Per-example loss in batch: 0.238642  [   34/   89]
Per-example loss in batch: 0.240237  [   36/   89]
Per-example loss in batch: 0.275841  [   38/   89]
Per-example loss in batch: 0.236480  [   40/   89]
Per-example loss in batch: 0.261847  [   42/   89]
Per-example loss in batch: 0.279763  [   44/   89]
Per-example loss in batch: 0.319036  [   46/   89]
Per-example loss in batch: 0.314204  [   48/   89]
Per-example loss in batch: 0.294205  [   50/   89]
Per-example loss in batch: 0.207020  [   52/   89]
Per-example loss in batch: 0.283333  [   54/   89]
Per-example loss in batch: 0.307429  [   56/   89]
Per-example loss in batch: 0.330164  [   58/   89]
Per-example loss in batch: 0.238923  [   60/   89]
Per-example loss in batch: 0.204111  [   62/   89]
Per-example loss in batch: 0.242367  [   64/   89]
Per-example loss in batch: 0.176268  [   66/   89]
Per-example loss in batch: 0.260627  [   68/   89]
Per-example loss in batch: 0.251944  [   70/   89]
Per-example loss in batch: 0.240507  [   72/   89]
Per-example loss in batch: 0.226975  [   74/   89]
Per-example loss in batch: 0.291885  [   76/   89]
Per-example loss in batch: 0.333297  [   78/   89]
Per-example loss in batch: 0.212775  [   80/   89]
Per-example loss in batch: 0.185809  [   82/   89]
Per-example loss in batch: 0.328975  [   84/   89]
Per-example loss in batch: 0.267948  [   86/   89]
Per-example loss in batch: 0.232508  [   88/   89]
Per-example loss in batch: 0.377607  [   89/   89]
Train Error: Avg loss: 0.25953605
validation Error: 
 Avg loss: 0.27186679 
 F1: 0.492886 
 Precision: 0.561356 
 Recall: 0.439304
 IoU: 0.327040

test Error: 
 Avg loss: 0.25194382 
 F1: 0.526721 
 Precision: 0.605794 
 Recall: 0.465907
 IoU: 0.357516

We have finished training iteration 293
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_291_.pth
Per-example loss in batch: 0.251017  [    2/   89]
Per-example loss in batch: 0.236768  [    4/   89]
Per-example loss in batch: 0.295820  [    6/   89]
Per-example loss in batch: 0.319726  [    8/   89]
Per-example loss in batch: 0.303169  [   10/   89]
Per-example loss in batch: 0.301384  [   12/   89]
Per-example loss in batch: 0.210757  [   14/   89]
Per-example loss in batch: 0.226270  [   16/   89]
Per-example loss in batch: 0.308001  [   18/   89]
Per-example loss in batch: 0.263050  [   20/   89]
Per-example loss in batch: 0.261914  [   22/   89]
Per-example loss in batch: 0.302320  [   24/   89]
Per-example loss in batch: 0.233899  [   26/   89]
Per-example loss in batch: 0.319809  [   28/   89]
Per-example loss in batch: 0.240980  [   30/   89]
Per-example loss in batch: 0.300013  [   32/   89]
Per-example loss in batch: 0.220093  [   34/   89]
Per-example loss in batch: 0.189242  [   36/   89]
Per-example loss in batch: 0.283000  [   38/   89]
Per-example loss in batch: 0.255309  [   40/   89]
Per-example loss in batch: 0.215008  [   42/   89]
Per-example loss in batch: 0.258713  [   44/   89]
Per-example loss in batch: 0.196961  [   46/   89]
Per-example loss in batch: 0.233829  [   48/   89]
Per-example loss in batch: 0.204990  [   50/   89]
Per-example loss in batch: 0.318980  [   52/   89]
Per-example loss in batch: 0.260540  [   54/   89]
Per-example loss in batch: 0.278078  [   56/   89]
Per-example loss in batch: 0.279204  [   58/   89]
Per-example loss in batch: 0.281292  [   60/   89]
Per-example loss in batch: 0.275469  [   62/   89]
Per-example loss in batch: 0.297799  [   64/   89]
Per-example loss in batch: 0.317126  [   66/   89]
Per-example loss in batch: 0.214931  [   68/   89]
Per-example loss in batch: 0.197226  [   70/   89]
Per-example loss in batch: 0.221200  [   72/   89]
Per-example loss in batch: 0.255783  [   74/   89]
Per-example loss in batch: 0.252946  [   76/   89]
Per-example loss in batch: 0.219939  [   78/   89]
Per-example loss in batch: 0.257557  [   80/   89]
Per-example loss in batch: 0.252827  [   82/   89]
Per-example loss in batch: 0.200647  [   84/   89]
Per-example loss in batch: 0.257967  [   86/   89]
Per-example loss in batch: 0.252699  [   88/   89]
Per-example loss in batch: 0.629408  [   89/   89]
Train Error: Avg loss: 0.26154956
validation Error: 
 Avg loss: 0.27805609 
 F1: 0.490600 
 Precision: 0.558147 
 Recall: 0.437637
 IoU: 0.325030

test Error: 
 Avg loss: 0.25257178 
 F1: 0.525190 
 Precision: 0.602795 
 Recall: 0.465289
 IoU: 0.356107

We have finished training iteration 294
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_292_.pth
Per-example loss in batch: 0.261109  [    2/   89]
Per-example loss in batch: 0.231411  [    4/   89]
Per-example loss in batch: 0.285356  [    6/   89]
Per-example loss in batch: 0.213061  [    8/   89]
Per-example loss in batch: 0.222798  [   10/   89]
Per-example loss in batch: 0.266752  [   12/   89]
Per-example loss in batch: 0.326628  [   14/   89]
Per-example loss in batch: 0.286859  [   16/   89]
Per-example loss in batch: 0.217401  [   18/   89]
Per-example loss in batch: 0.205847  [   20/   89]
Per-example loss in batch: 0.252511  [   22/   89]
Per-example loss in batch: 0.312150  [   24/   89]
Per-example loss in batch: 0.261684  [   26/   89]
Per-example loss in batch: 0.220560  [   28/   89]
Per-example loss in batch: 0.242270  [   30/   89]
Per-example loss in batch: 0.217178  [   32/   89]
Per-example loss in batch: 0.266861  [   34/   89]
Per-example loss in batch: 0.246848  [   36/   89]
Per-example loss in batch: 0.296982  [   38/   89]
Per-example loss in batch: 0.316158  [   40/   89]
Per-example loss in batch: 0.287382  [   42/   89]
Per-example loss in batch: 0.220019  [   44/   89]
Per-example loss in batch: 0.363902  [   46/   89]
Per-example loss in batch: 0.261263  [   48/   89]
Per-example loss in batch: 0.214991  [   50/   89]
Per-example loss in batch: 0.194696  [   52/   89]
Per-example loss in batch: 0.238404  [   54/   89]
Per-example loss in batch: 0.220378  [   56/   89]
Per-example loss in batch: 0.231826  [   58/   89]
Per-example loss in batch: 0.254265  [   60/   89]
Per-example loss in batch: 0.214930  [   62/   89]
Per-example loss in batch: 0.282632  [   64/   89]
Per-example loss in batch: 0.224077  [   66/   89]
Per-example loss in batch: 0.246081  [   68/   89]
Per-example loss in batch: 0.222882  [   70/   89]
Per-example loss in batch: 0.262969  [   72/   89]
Per-example loss in batch: 0.236829  [   74/   89]
Per-example loss in batch: 0.294999  [   76/   89]
Per-example loss in batch: 0.262009  [   78/   89]
Per-example loss in batch: 0.247869  [   80/   89]
Per-example loss in batch: 0.293516  [   82/   89]
Per-example loss in batch: 0.289102  [   84/   89]
Per-example loss in batch: 0.211357  [   86/   89]
Per-example loss in batch: 0.200883  [   88/   89]
Per-example loss in batch: 0.591389  [   89/   89]
Train Error: Avg loss: 0.25670517
validation Error: 
 Avg loss: 0.27491825 
 F1: 0.488809 
 Precision: 0.559071 
 Recall: 0.434236
 IoU: 0.323459

test Error: 
 Avg loss: 0.25412545 
 F1: 0.521817 
 Precision: 0.604662 
 Recall: 0.458938
 IoU: 0.353012

We have finished training iteration 295
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_293_.pth
Per-example loss in batch: 0.267915  [    2/   89]
Per-example loss in batch: 0.245580  [    4/   89]
Per-example loss in batch: 0.276358  [    6/   89]
Per-example loss in batch: 0.274287  [    8/   89]
Per-example loss in batch: 0.288996  [   10/   89]
Per-example loss in batch: 0.203122  [   12/   89]
Per-example loss in batch: 0.212564  [   14/   89]
Per-example loss in batch: 0.224855  [   16/   89]
Per-example loss in batch: 0.334880  [   18/   89]
Per-example loss in batch: 0.256090  [   20/   89]
Per-example loss in batch: 0.234247  [   22/   89]
Per-example loss in batch: 0.224708  [   24/   89]
Per-example loss in batch: 0.250665  [   26/   89]
Per-example loss in batch: 0.193590  [   28/   89]
Per-example loss in batch: 0.313835  [   30/   89]
Per-example loss in batch: 0.227027  [   32/   89]
Per-example loss in batch: 0.306358  [   34/   89]
Per-example loss in batch: 0.276933  [   36/   89]
Per-example loss in batch: 0.229511  [   38/   89]
Per-example loss in batch: 0.219425  [   40/   89]
Per-example loss in batch: 0.207132  [   42/   89]
Per-example loss in batch: 0.214106  [   44/   89]
Per-example loss in batch: 0.248485  [   46/   89]
Per-example loss in batch: 0.247933  [   48/   89]
Per-example loss in batch: 0.288307  [   50/   89]
Per-example loss in batch: 0.300775  [   52/   89]
Per-example loss in batch: 0.212163  [   54/   89]
Per-example loss in batch: 0.281378  [   56/   89]
Per-example loss in batch: 0.270310  [   58/   89]
Per-example loss in batch: 0.231954  [   60/   89]
Per-example loss in batch: 0.199758  [   62/   89]
Per-example loss in batch: 0.338807  [   64/   89]
Per-example loss in batch: 0.262372  [   66/   89]
Per-example loss in batch: 0.259394  [   68/   89]
Per-example loss in batch: 0.222473  [   70/   89]
Per-example loss in batch: 0.285351  [   72/   89]
Per-example loss in batch: 0.290502  [   74/   89]
Per-example loss in batch: 0.228550  [   76/   89]
Per-example loss in batch: 0.320232  [   78/   89]
Per-example loss in batch: 0.234653  [   80/   89]
Per-example loss in batch: 0.263388  [   82/   89]
Per-example loss in batch: 0.232028  [   84/   89]
Per-example loss in batch: 0.253595  [   86/   89]
Per-example loss in batch: 0.246718  [   88/   89]
Per-example loss in batch: 0.417131  [   89/   89]
Train Error: Avg loss: 0.25640166
validation Error: 
 Avg loss: 0.27734237 
 F1: 0.489981 
 Precision: 0.567934 
 Recall: 0.430845
 IoU: 0.324487

test Error: 
 Avg loss: 0.25323955 
 F1: 0.523291 
 Precision: 0.612186 
 Recall: 0.456940
 IoU: 0.354363

We have finished training iteration 296
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_294_.pth
Per-example loss in batch: 0.211303  [    2/   89]
Per-example loss in batch: 0.224709  [    4/   89]
Per-example loss in batch: 0.260029  [    6/   89]
Per-example loss in batch: 0.242642  [    8/   89]
Per-example loss in batch: 0.228178  [   10/   89]
Per-example loss in batch: 0.295494  [   12/   89]
Per-example loss in batch: 0.265511  [   14/   89]
Per-example loss in batch: 0.207244  [   16/   89]
Per-example loss in batch: 0.270184  [   18/   89]
Per-example loss in batch: 0.221671  [   20/   89]
Per-example loss in batch: 0.223338  [   22/   89]
Per-example loss in batch: 0.296961  [   24/   89]
Per-example loss in batch: 0.250687  [   26/   89]
Per-example loss in batch: 0.221674  [   28/   89]
Per-example loss in batch: 0.270326  [   30/   89]
Per-example loss in batch: 0.294322  [   32/   89]
Per-example loss in batch: 0.309066  [   34/   89]
Per-example loss in batch: 0.285352  [   36/   89]
Per-example loss in batch: 0.214504  [   38/   89]
Per-example loss in batch: 0.248841  [   40/   89]
Per-example loss in batch: 0.259641  [   42/   89]
Per-example loss in batch: 0.251303  [   44/   89]
Per-example loss in batch: 0.324945  [   46/   89]
Per-example loss in batch: 0.206947  [   48/   89]
Per-example loss in batch: 0.339071  [   50/   89]
Per-example loss in batch: 0.195476  [   52/   89]
Per-example loss in batch: 0.243598  [   54/   89]
Per-example loss in batch: 0.304415  [   56/   89]
Per-example loss in batch: 0.217554  [   58/   89]
Per-example loss in batch: 0.257292  [   60/   89]
Per-example loss in batch: 0.186904  [   62/   89]
Per-example loss in batch: 0.234731  [   64/   89]
Per-example loss in batch: 0.225805  [   66/   89]
Per-example loss in batch: 0.205804  [   68/   89]
Per-example loss in batch: 0.226997  [   70/   89]
Per-example loss in batch: 0.301418  [   72/   89]
Per-example loss in batch: 0.192318  [   74/   89]
Per-example loss in batch: 0.297816  [   76/   89]
Per-example loss in batch: 0.258623  [   78/   89]
Per-example loss in batch: 0.217567  [   80/   89]
Per-example loss in batch: 0.259007  [   82/   89]
Per-example loss in batch: 0.326828  [   84/   89]
Per-example loss in batch: 0.360118  [   86/   89]
Per-example loss in batch: 0.288623  [   88/   89]
Per-example loss in batch: 0.491947  [   89/   89]
Train Error: Avg loss: 0.25777100
validation Error: 
 Avg loss: 0.27101592 
 F1: 0.489570 
 Precision: 0.563820 
 Recall: 0.432601
 IoU: 0.324127

test Error: 
 Avg loss: 0.25405299 
 F1: 0.522146 
 Precision: 0.609510 
 Recall: 0.456686
 IoU: 0.353313

We have finished training iteration 297
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_295_.pth
Per-example loss in batch: 0.319374  [    2/   89]
Per-example loss in batch: 0.251749  [    4/   89]
Per-example loss in batch: 0.256417  [    6/   89]
Per-example loss in batch: 0.282848  [    8/   89]
Per-example loss in batch: 0.240391  [   10/   89]
Per-example loss in batch: 0.291029  [   12/   89]
Per-example loss in batch: 0.220847  [   14/   89]
Per-example loss in batch: 0.263352  [   16/   89]
Per-example loss in batch: 0.323693  [   18/   89]
Per-example loss in batch: 0.209098  [   20/   89]
Per-example loss in batch: 0.285678  [   22/   89]
Per-example loss in batch: 0.281907  [   24/   89]
Per-example loss in batch: 0.327724  [   26/   89]
Per-example loss in batch: 0.273746  [   28/   89]
Per-example loss in batch: 0.244214  [   30/   89]
Per-example loss in batch: 0.300638  [   32/   89]
Per-example loss in batch: 0.284893  [   34/   89]
Per-example loss in batch: 0.216162  [   36/   89]
Per-example loss in batch: 0.220033  [   38/   89]
Per-example loss in batch: 0.233947  [   40/   89]
Per-example loss in batch: 0.231607  [   42/   89]
Per-example loss in batch: 0.228945  [   44/   89]
Per-example loss in batch: 0.242246  [   46/   89]
Per-example loss in batch: 0.230855  [   48/   89]
Per-example loss in batch: 0.319170  [   50/   89]
Per-example loss in batch: 0.269008  [   52/   89]
Per-example loss in batch: 0.235216  [   54/   89]
Per-example loss in batch: 0.212398  [   56/   89]
Per-example loss in batch: 0.264691  [   58/   89]
Per-example loss in batch: 0.311744  [   60/   89]
Per-example loss in batch: 0.323048  [   62/   89]
Per-example loss in batch: 0.190232  [   64/   89]
Per-example loss in batch: 0.240317  [   66/   89]
Per-example loss in batch: 0.340634  [   68/   89]
Per-example loss in batch: 0.254268  [   70/   89]
Per-example loss in batch: 0.200119  [   72/   89]
Per-example loss in batch: 0.296387  [   74/   89]
Per-example loss in batch: 0.221868  [   76/   89]
Per-example loss in batch: 0.292041  [   78/   89]
Per-example loss in batch: 0.325707  [   80/   89]
Per-example loss in batch: 0.268463  [   82/   89]
Per-example loss in batch: 0.269998  [   84/   89]
Per-example loss in batch: 0.209254  [   86/   89]
Per-example loss in batch: 0.297564  [   88/   89]
Per-example loss in batch: 0.676095  [   89/   89]
Train Error: Avg loss: 0.26834979
validation Error: 
 Avg loss: 0.27200910 
 F1: 0.485460 
 Precision: 0.578575 
 Recall: 0.418161
 IoU: 0.320533

test Error: 
 Avg loss: 0.25626485 
 F1: 0.517123 
 Precision: 0.616943 
 Recall: 0.445107
 IoU: 0.348730

We have finished training iteration 298
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_296_.pth
Per-example loss in batch: 0.234932  [    2/   89]
Per-example loss in batch: 0.291602  [    4/   89]
Per-example loss in batch: 0.315121  [    6/   89]
Per-example loss in batch: 0.247891  [    8/   89]
Per-example loss in batch: 0.297420  [   10/   89]
Per-example loss in batch: 0.255637  [   12/   89]
Per-example loss in batch: 0.229715  [   14/   89]
Per-example loss in batch: 0.212602  [   16/   89]
Per-example loss in batch: 0.298843  [   18/   89]
Per-example loss in batch: 0.250597  [   20/   89]
Per-example loss in batch: 0.228456  [   22/   89]
Per-example loss in batch: 0.216366  [   24/   89]
Per-example loss in batch: 0.197718  [   26/   89]
Per-example loss in batch: 0.253731  [   28/   89]
Per-example loss in batch: 0.214408  [   30/   89]
Per-example loss in batch: 0.226190  [   32/   89]
Per-example loss in batch: 0.303578  [   34/   89]
Per-example loss in batch: 0.260609  [   36/   89]
Per-example loss in batch: 0.202140  [   38/   89]
Per-example loss in batch: 0.217356  [   40/   89]
Per-example loss in batch: 0.194318  [   42/   89]
Per-example loss in batch: 0.308896  [   44/   89]
Per-example loss in batch: 0.229540  [   46/   89]
Per-example loss in batch: 0.256840  [   48/   89]
Per-example loss in batch: 0.279228  [   50/   89]
Per-example loss in batch: 0.218439  [   52/   89]
Per-example loss in batch: 0.304729  [   54/   89]
Per-example loss in batch: 0.199466  [   56/   89]
Per-example loss in batch: 0.299907  [   58/   89]
Per-example loss in batch: 0.326932  [   60/   89]
Per-example loss in batch: 0.239543  [   62/   89]
Per-example loss in batch: 0.235968  [   64/   89]
Per-example loss in batch: 0.203012  [   66/   89]
Per-example loss in batch: 0.265675  [   68/   89]
Per-example loss in batch: 0.298742  [   70/   89]
Per-example loss in batch: 0.224865  [   72/   89]
Per-example loss in batch: 0.248923  [   74/   89]
Per-example loss in batch: 0.261271  [   76/   89]
Per-example loss in batch: 0.231723  [   78/   89]
Per-example loss in batch: 0.202636  [   80/   89]
Per-example loss in batch: 0.229013  [   82/   89]
Per-example loss in batch: 0.312029  [   84/   89]
Per-example loss in batch: 0.336165  [   86/   89]
Per-example loss in batch: 0.329888  [   88/   89]
Per-example loss in batch: 0.652624  [   89/   89]
Train Error: Avg loss: 0.25885327
validation Error: 
 Avg loss: 0.27849921 
 F1: 0.490214 
 Precision: 0.577284 
 Recall: 0.425967
 IoU: 0.324691

test Error: 
 Avg loss: 0.25293559 
 F1: 0.524243 
 Precision: 0.621485 
 Recall: 0.453314
 IoU: 0.355237

We have finished training iteration 299
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_297_.pth
Per-example loss in batch: 0.329391  [    2/   89]
Per-example loss in batch: 0.239835  [    4/   89]
Per-example loss in batch: 0.204185  [    6/   89]
Per-example loss in batch: 0.351205  [    8/   89]
Per-example loss in batch: 0.240374  [   10/   89]
Per-example loss in batch: 0.323748  [   12/   89]
Per-example loss in batch: 0.212014  [   14/   89]
Per-example loss in batch: 0.247143  [   16/   89]
Per-example loss in batch: 0.206935  [   18/   89]
Per-example loss in batch: 0.248729  [   20/   89]
Per-example loss in batch: 0.274863  [   22/   89]
Per-example loss in batch: 0.216691  [   24/   89]
Per-example loss in batch: 0.261539  [   26/   89]
Per-example loss in batch: 0.264118  [   28/   89]
Per-example loss in batch: 0.214215  [   30/   89]
Per-example loss in batch: 0.238044  [   32/   89]
Per-example loss in batch: 0.336133  [   34/   89]
Per-example loss in batch: 0.228146  [   36/   89]
Per-example loss in batch: 0.253101  [   38/   89]
Per-example loss in batch: 0.269594  [   40/   89]
Per-example loss in batch: 0.295837  [   42/   89]
Per-example loss in batch: 0.229266  [   44/   89]
Per-example loss in batch: 0.208457  [   46/   89]
Per-example loss in batch: 0.243931  [   48/   89]
Per-example loss in batch: 0.204656  [   50/   89]
Per-example loss in batch: 0.229427  [   52/   89]
Per-example loss in batch: 0.230463  [   54/   89]
Per-example loss in batch: 0.298706  [   56/   89]
Per-example loss in batch: 0.292759  [   58/   89]
Per-example loss in batch: 0.282634  [   60/   89]
Per-example loss in batch: 0.345642  [   62/   89]
Per-example loss in batch: 0.284455  [   64/   89]
Per-example loss in batch: 0.215153  [   66/   89]
Per-example loss in batch: 0.251193  [   68/   89]
Per-example loss in batch: 0.247755  [   70/   89]
Per-example loss in batch: 0.186746  [   72/   89]
Per-example loss in batch: 0.211161  [   74/   89]
Per-example loss in batch: 0.192360  [   76/   89]
Per-example loss in batch: 0.223985  [   78/   89]
Per-example loss in batch: 0.236243  [   80/   89]
Per-example loss in batch: 0.282022  [   82/   89]
Per-example loss in batch: 0.273215  [   84/   89]
Per-example loss in batch: 0.252160  [   86/   89]
Per-example loss in batch: 0.294070  [   88/   89]
Per-example loss in batch: 0.398783  [   89/   89]
Train Error: Avg loss: 0.25554359
validation Error: 
 Avg loss: 0.27632091 
 F1: 0.488604 
 Precision: 0.528758 
 Recall: 0.454117
 IoU: 0.323280

test Error: 
 Avg loss: 0.25328597 
 F1: 0.523386 
 Precision: 0.574271 
 Recall: 0.480785
 IoU: 0.354450

We have finished training iteration 300
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_298_.pth
Per-example loss in batch: 0.296871  [    2/   89]
Per-example loss in batch: 0.253418  [    4/   89]
Per-example loss in batch: 0.266164  [    6/   89]
Per-example loss in batch: 0.276664  [    8/   89]
Per-example loss in batch: 0.238391  [   10/   89]
Per-example loss in batch: 0.222697  [   12/   89]
Per-example loss in batch: 0.320414  [   14/   89]
Per-example loss in batch: 0.286038  [   16/   89]
Per-example loss in batch: 0.208427  [   18/   89]
Per-example loss in batch: 0.291473  [   20/   89]
Per-example loss in batch: 0.315969  [   22/   89]
Per-example loss in batch: 0.245751  [   24/   89]
Per-example loss in batch: 0.308277  [   26/   89]
Per-example loss in batch: 0.227522  [   28/   89]
Per-example loss in batch: 0.258936  [   30/   89]
Per-example loss in batch: 0.217908  [   32/   89]
Per-example loss in batch: 0.335453  [   34/   89]
Per-example loss in batch: 0.269472  [   36/   89]
Per-example loss in batch: 0.196514  [   38/   89]
Per-example loss in batch: 0.296187  [   40/   89]
Per-example loss in batch: 0.261422  [   42/   89]
Per-example loss in batch: 0.228396  [   44/   89]
Per-example loss in batch: 0.222767  [   46/   89]
Per-example loss in batch: 0.263616  [   48/   89]
Per-example loss in batch: 0.298333  [   50/   89]
Per-example loss in batch: 0.245582  [   52/   89]
Per-example loss in batch: 0.225096  [   54/   89]
Per-example loss in batch: 0.273137  [   56/   89]
Per-example loss in batch: 0.307817  [   58/   89]
Per-example loss in batch: 0.228527  [   60/   89]
Per-example loss in batch: 0.215268  [   62/   89]
Per-example loss in batch: 0.289382  [   64/   89]
Per-example loss in batch: 0.240240  [   66/   89]
Per-example loss in batch: 0.231894  [   68/   89]
Per-example loss in batch: 0.286546  [   70/   89]
Per-example loss in batch: 0.173360  [   72/   89]
Per-example loss in batch: 0.252205  [   74/   89]
Per-example loss in batch: 0.295723  [   76/   89]
Per-example loss in batch: 0.330242  [   78/   89]
Per-example loss in batch: 0.246614  [   80/   89]
Per-example loss in batch: 0.265053  [   82/   89]
Per-example loss in batch: 0.217036  [   84/   89]
Per-example loss in batch: 0.263088  [   86/   89]
Per-example loss in batch: 0.189002  [   88/   89]
Per-example loss in batch: 0.370782  [   89/   89]
Train Error: Avg loss: 0.25996140
validation Error: 
 Avg loss: 0.27367809 
 F1: 0.491344 
 Precision: 0.565522 
 Recall: 0.434369
 IoU: 0.325683

test Error: 
 Avg loss: 0.25408682 
 F1: 0.521760 
 Precision: 0.604954 
 Recall: 0.458681
 IoU: 0.352960

We have finished training iteration 301
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_299_.pth
Per-example loss in batch: 0.308044  [    2/   89]
Per-example loss in batch: 0.197725  [    4/   89]
Per-example loss in batch: 0.221300  [    6/   89]
Per-example loss in batch: 0.278318  [    8/   89]
Per-example loss in batch: 0.240092  [   10/   89]
Per-example loss in batch: 0.276132  [   12/   89]
Per-example loss in batch: 0.278892  [   14/   89]
Per-example loss in batch: 0.265964  [   16/   89]
Per-example loss in batch: 0.233999  [   18/   89]
Per-example loss in batch: 0.312303  [   20/   89]
Per-example loss in batch: 0.215201  [   22/   89]
Per-example loss in batch: 0.242856  [   24/   89]
Per-example loss in batch: 0.242205  [   26/   89]
Per-example loss in batch: 0.234486  [   28/   89]
Per-example loss in batch: 0.212314  [   30/   89]
Per-example loss in batch: 0.327143  [   32/   89]
Per-example loss in batch: 0.247349  [   34/   89]
Per-example loss in batch: 0.214015  [   36/   89]
Per-example loss in batch: 0.228107  [   38/   89]
Per-example loss in batch: 0.232608  [   40/   89]
Per-example loss in batch: 0.273079  [   42/   89]
Per-example loss in batch: 0.245926  [   44/   89]
Per-example loss in batch: 0.243515  [   46/   89]
Per-example loss in batch: 0.294726  [   48/   89]
Per-example loss in batch: 0.285633  [   50/   89]
Per-example loss in batch: 0.227581  [   52/   89]
Per-example loss in batch: 0.260329  [   54/   89]
Per-example loss in batch: 0.252083  [   56/   89]
Per-example loss in batch: 0.367124  [   58/   89]
Per-example loss in batch: 0.235299  [   60/   89]
Per-example loss in batch: 0.199568  [   62/   89]
Per-example loss in batch: 0.276404  [   64/   89]
Per-example loss in batch: 0.311161  [   66/   89]
Per-example loss in batch: 0.250697  [   68/   89]
Per-example loss in batch: 0.322836  [   70/   89]
Per-example loss in batch: 0.328402  [   72/   89]
Per-example loss in batch: 0.267503  [   74/   89]
Per-example loss in batch: 0.225329  [   76/   89]
Per-example loss in batch: 0.260365  [   78/   89]
Per-example loss in batch: 0.205352  [   80/   89]
Per-example loss in batch: 0.212736  [   82/   89]
Per-example loss in batch: 0.228285  [   84/   89]
Per-example loss in batch: 0.222031  [   86/   89]
Per-example loss in batch: 0.264837  [   88/   89]
Per-example loss in batch: 0.725470  [   89/   89]
Train Error: Avg loss: 0.26140657
validation Error: 
 Avg loss: 0.27336030 
 F1: 0.491860 
 Precision: 0.556486 
 Recall: 0.440682
 IoU: 0.326137

test Error: 
 Avg loss: 0.25347167 
 F1: 0.523129 
 Precision: 0.596236 
 Recall: 0.465992
 IoU: 0.354215

We have finished training iteration 302
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_300_.pth
Per-example loss in batch: 0.354088  [    2/   89]
Per-example loss in batch: 0.267810  [    4/   89]
Per-example loss in batch: 0.281477  [    6/   89]
Per-example loss in batch: 0.179643  [    8/   89]
Per-example loss in batch: 0.206419  [   10/   89]
Per-example loss in batch: 0.299228  [   12/   89]
Per-example loss in batch: 0.271859  [   14/   89]
Per-example loss in batch: 0.200832  [   16/   89]
Per-example loss in batch: 0.253694  [   18/   89]
Per-example loss in batch: 0.236754  [   20/   89]
Per-example loss in batch: 0.258903  [   22/   89]
Per-example loss in batch: 0.238142  [   24/   89]
Per-example loss in batch: 0.233178  [   26/   89]
Per-example loss in batch: 0.195632  [   28/   89]
Per-example loss in batch: 0.208642  [   30/   89]
Per-example loss in batch: 0.241469  [   32/   89]
Per-example loss in batch: 0.268011  [   34/   89]
Per-example loss in batch: 0.261530  [   36/   89]
Per-example loss in batch: 0.276281  [   38/   89]
Per-example loss in batch: 0.229515  [   40/   89]
Per-example loss in batch: 0.234624  [   42/   89]
Per-example loss in batch: 0.196518  [   44/   89]
Per-example loss in batch: 0.175217  [   46/   89]
Per-example loss in batch: 0.318627  [   48/   89]
Per-example loss in batch: 0.279745  [   50/   89]
Per-example loss in batch: 0.305244  [   52/   89]
Per-example loss in batch: 0.252272  [   54/   89]
Per-example loss in batch: 0.241406  [   56/   89]
Per-example loss in batch: 0.231633  [   58/   89]
Per-example loss in batch: 0.235544  [   60/   89]
Per-example loss in batch: 0.319300  [   62/   89]
Per-example loss in batch: 0.339152  [   64/   89]
Per-example loss in batch: 0.237648  [   66/   89]
Per-example loss in batch: 0.238299  [   68/   89]
Per-example loss in batch: 0.243424  [   70/   89]
Per-example loss in batch: 0.261561  [   72/   89]
Per-example loss in batch: 0.261990  [   74/   89]
Per-example loss in batch: 0.309154  [   76/   89]
Per-example loss in batch: 0.229734  [   78/   89]
Per-example loss in batch: 0.267038  [   80/   89]
Per-example loss in batch: 0.269339  [   82/   89]
Per-example loss in batch: 0.297040  [   84/   89]
Per-example loss in batch: 0.262936  [   86/   89]
Per-example loss in batch: 0.235573  [   88/   89]
Per-example loss in batch: 0.410000  [   89/   89]
Train Error: Avg loss: 0.25642976
validation Error: 
 Avg loss: 0.27218315 
 F1: 0.492287 
 Precision: 0.555478 
 Recall: 0.442005
 IoU: 0.326513

test Error: 
 Avg loss: 0.25278478 
 F1: 0.524613 
 Precision: 0.596805 
 Recall: 0.468002
 IoU: 0.355577

We have finished training iteration 303
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_301_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.282900  [    2/   89]
Per-example loss in batch: 0.239857  [    4/   89]
Per-example loss in batch: 0.238845  [    6/   89]
Per-example loss in batch: 0.241154  [    8/   89]
Per-example loss in batch: 0.280578  [   10/   89]
Per-example loss in batch: 0.244027  [   12/   89]
Per-example loss in batch: 0.306757  [   14/   89]
Per-example loss in batch: 0.224128  [   16/   89]
Per-example loss in batch: 0.192721  [   18/   89]
Per-example loss in batch: 0.299476  [   20/   89]
Per-example loss in batch: 0.222634  [   22/   89]
Per-example loss in batch: 0.226768  [   24/   89]
Per-example loss in batch: 0.279665  [   26/   89]
Per-example loss in batch: 0.262099  [   28/   89]
Per-example loss in batch: 0.226494  [   30/   89]
Per-example loss in batch: 0.275489  [   32/   89]
Per-example loss in batch: 0.319934  [   34/   89]
Per-example loss in batch: 0.190948  [   36/   89]
Per-example loss in batch: 0.213163  [   38/   89]
Per-example loss in batch: 0.257794  [   40/   89]
Per-example loss in batch: 0.246081  [   42/   89]
Per-example loss in batch: 0.199263  [   44/   89]
Per-example loss in batch: 0.312313  [   46/   89]
Per-example loss in batch: 0.252725  [   48/   89]
Per-example loss in batch: 0.216793  [   50/   89]
Per-example loss in batch: 0.261484  [   52/   89]
Per-example loss in batch: 0.324430  [   54/   89]
Per-example loss in batch: 0.312489  [   56/   89]
Per-example loss in batch: 0.239866  [   58/   89]
Per-example loss in batch: 0.312835  [   60/   89]
Per-example loss in batch: 0.291124  [   62/   89]
Per-example loss in batch: 0.205917  [   64/   89]
Per-example loss in batch: 0.239519  [   66/   89]
Per-example loss in batch: 0.214248  [   68/   89]
Per-example loss in batch: 0.263216  [   70/   89]
Per-example loss in batch: 0.273046  [   72/   89]
Per-example loss in batch: 0.205619  [   74/   89]
Per-example loss in batch: 0.185981  [   76/   89]
Per-example loss in batch: 0.246307  [   78/   89]
Per-example loss in batch: 0.329074  [   80/   89]
Per-example loss in batch: 0.245213  [   82/   89]
Per-example loss in batch: 0.206344  [   84/   89]
Per-example loss in batch: 0.245339  [   86/   89]
Per-example loss in batch: 0.207650  [   88/   89]
Per-example loss in batch: 0.618562  [   89/   89]
Train Error: Avg loss: 0.25554128
validation Error: 
 Avg loss: 0.27365942 
 F1: 0.491756 
 Precision: 0.556491 
 Recall: 0.440513
 IoU: 0.326045

test Error: 
 Avg loss: 0.25215818 
 F1: 0.526449 
 Precision: 0.602034 
 Recall: 0.467726
 IoU: 0.357265

We have finished training iteration 304
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_302_.pth
Per-example loss in batch: 0.230666  [    2/   89]
Per-example loss in batch: 0.325136  [    4/   89]
Per-example loss in batch: 0.245224  [    6/   89]
Per-example loss in batch: 0.204112  [    8/   89]
Per-example loss in batch: 0.301017  [   10/   89]
Per-example loss in batch: 0.234979  [   12/   89]
Per-example loss in batch: 0.265889  [   14/   89]
Per-example loss in batch: 0.232489  [   16/   89]
Per-example loss in batch: 0.206519  [   18/   89]
Per-example loss in batch: 0.254870  [   20/   89]
Per-example loss in batch: 0.337145  [   22/   89]
Per-example loss in batch: 0.267461  [   24/   89]
Per-example loss in batch: 0.274628  [   26/   89]
Per-example loss in batch: 0.267415  [   28/   89]
Per-example loss in batch: 0.283755  [   30/   89]
Per-example loss in batch: 0.249274  [   32/   89]
Per-example loss in batch: 0.286729  [   34/   89]
Per-example loss in batch: 0.270237  [   36/   89]
Per-example loss in batch: 0.277549  [   38/   89]
Per-example loss in batch: 0.361239  [   40/   89]
Per-example loss in batch: 0.209506  [   42/   89]
Per-example loss in batch: 0.246158  [   44/   89]
Per-example loss in batch: 0.229286  [   46/   89]
Per-example loss in batch: 0.217421  [   48/   89]
Per-example loss in batch: 0.271730  [   50/   89]
Per-example loss in batch: 0.272790  [   52/   89]
Per-example loss in batch: 0.229410  [   54/   89]
Per-example loss in batch: 0.221153  [   56/   89]
Per-example loss in batch: 0.239366  [   58/   89]
Per-example loss in batch: 0.289921  [   60/   89]
Per-example loss in batch: 0.285501  [   62/   89]
Per-example loss in batch: 0.229308  [   64/   89]
Per-example loss in batch: 0.195476  [   66/   89]
Per-example loss in batch: 0.232900  [   68/   89]
Per-example loss in batch: 0.220412  [   70/   89]
Per-example loss in batch: 0.213345  [   72/   89]
Per-example loss in batch: 0.243943  [   74/   89]
Per-example loss in batch: 0.227441  [   76/   89]
Per-example loss in batch: 0.216700  [   78/   89]
Per-example loss in batch: 0.223791  [   80/   89]
Per-example loss in batch: 0.340651  [   82/   89]
Per-example loss in batch: 0.235889  [   84/   89]
Per-example loss in batch: 0.317313  [   86/   89]
Per-example loss in batch: 0.309604  [   88/   89]
Per-example loss in batch: 0.412377  [   89/   89]
Train Error: Avg loss: 0.25846151
validation Error: 
 Avg loss: 0.27755494 
 F1: 0.490794 
 Precision: 0.535326 
 Recall: 0.453102
 IoU: 0.325200

test Error: 
 Avg loss: 0.25146176 
 F1: 0.527458 
 Precision: 0.581437 
 Recall: 0.482650
 IoU: 0.358196

We have finished training iteration 305
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_303_.pth
Per-example loss in batch: 0.216032  [    2/   89]
Per-example loss in batch: 0.269526  [    4/   89]
Per-example loss in batch: 0.298246  [    6/   89]
Per-example loss in batch: 0.262874  [    8/   89]
Per-example loss in batch: 0.215513  [   10/   89]
Per-example loss in batch: 0.235455  [   12/   89]
Per-example loss in batch: 0.331405  [   14/   89]
Per-example loss in batch: 0.256174  [   16/   89]
Per-example loss in batch: 0.299035  [   18/   89]
Per-example loss in batch: 0.236281  [   20/   89]
Per-example loss in batch: 0.347222  [   22/   89]
Per-example loss in batch: 0.308275  [   24/   89]
Per-example loss in batch: 0.365395  [   26/   89]
Per-example loss in batch: 0.238139  [   28/   89]
Per-example loss in batch: 0.226747  [   30/   89]
Per-example loss in batch: 0.268073  [   32/   89]
Per-example loss in batch: 0.222661  [   34/   89]
Per-example loss in batch: 0.326102  [   36/   89]
Per-example loss in batch: 0.210228  [   38/   89]
Per-example loss in batch: 0.203363  [   40/   89]
Per-example loss in batch: 0.247878  [   42/   89]
Per-example loss in batch: 0.197367  [   44/   89]
Per-example loss in batch: 0.252120  [   46/   89]
Per-example loss in batch: 0.192708  [   48/   89]
Per-example loss in batch: 0.278393  [   50/   89]
Per-example loss in batch: 0.312529  [   52/   89]
Per-example loss in batch: 0.237014  [   54/   89]
Per-example loss in batch: 0.220037  [   56/   89]
Per-example loss in batch: 0.263532  [   58/   89]
Per-example loss in batch: 0.195058  [   60/   89]
Per-example loss in batch: 0.239202  [   62/   89]
Per-example loss in batch: 0.240762  [   64/   89]
Per-example loss in batch: 0.309107  [   66/   89]
Per-example loss in batch: 0.268066  [   68/   89]
Per-example loss in batch: 0.254174  [   70/   89]
Per-example loss in batch: 0.285032  [   72/   89]
Per-example loss in batch: 0.265626  [   74/   89]
Per-example loss in batch: 0.263405  [   76/   89]
Per-example loss in batch: 0.206389  [   78/   89]
Per-example loss in batch: 0.339844  [   80/   89]
Per-example loss in batch: 0.214537  [   82/   89]
Per-example loss in batch: 0.232107  [   84/   89]
Per-example loss in batch: 0.285076  [   86/   89]
Per-example loss in batch: 0.190558  [   88/   89]
Per-example loss in batch: 0.432927  [   89/   89]
Train Error: Avg loss: 0.25940964
validation Error: 
 Avg loss: 0.26782398 
 F1: 0.492224 
 Precision: 0.588417 
 Recall: 0.423063
 IoU: 0.326457

test Error: 
 Avg loss: 0.25354339 
 F1: 0.523099 
 Precision: 0.630702 
 Recall: 0.446861
 IoU: 0.354187

We have finished training iteration 306
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_304_.pth
Per-example loss in batch: 0.352716  [    2/   89]
Per-example loss in batch: 0.263826  [    4/   89]
Per-example loss in batch: 0.253318  [    6/   89]
Per-example loss in batch: 0.230913  [    8/   89]
Per-example loss in batch: 0.270542  [   10/   89]
Per-example loss in batch: 0.206123  [   12/   89]
Per-example loss in batch: 0.233966  [   14/   89]
Per-example loss in batch: 0.312213  [   16/   89]
Per-example loss in batch: 0.305642  [   18/   89]
Per-example loss in batch: 0.337964  [   20/   89]
Per-example loss in batch: 0.290832  [   22/   89]
Per-example loss in batch: 0.269910  [   24/   89]
Per-example loss in batch: 0.280208  [   26/   89]
Per-example loss in batch: 0.238524  [   28/   89]
Per-example loss in batch: 0.190345  [   30/   89]
Per-example loss in batch: 0.325349  [   32/   89]
Per-example loss in batch: 0.245936  [   34/   89]
Per-example loss in batch: 0.242388  [   36/   89]
Per-example loss in batch: 0.296403  [   38/   89]
Per-example loss in batch: 0.309411  [   40/   89]
Per-example loss in batch: 0.227214  [   42/   89]
Per-example loss in batch: 0.325393  [   44/   89]
Per-example loss in batch: 0.202176  [   46/   89]
Per-example loss in batch: 0.243989  [   48/   89]
Per-example loss in batch: 0.289779  [   50/   89]
Per-example loss in batch: 0.212463  [   52/   89]
Per-example loss in batch: 0.214225  [   54/   89]
Per-example loss in batch: 0.283063  [   56/   89]
Per-example loss in batch: 0.263347  [   58/   89]
Per-example loss in batch: 0.336445  [   60/   89]
Per-example loss in batch: 0.285971  [   62/   89]
Per-example loss in batch: 0.223988  [   64/   89]
Per-example loss in batch: 0.257481  [   66/   89]
Per-example loss in batch: 0.213426  [   68/   89]
Per-example loss in batch: 0.191804  [   70/   89]
Per-example loss in batch: 0.213458  [   72/   89]
Per-example loss in batch: 0.220213  [   74/   89]
Per-example loss in batch: 0.246441  [   76/   89]
Per-example loss in batch: 0.190907  [   78/   89]
Per-example loss in batch: 0.257111  [   80/   89]
Per-example loss in batch: 0.208353  [   82/   89]
Per-example loss in batch: 0.290947  [   84/   89]
Per-example loss in batch: 0.206063  [   86/   89]
Per-example loss in batch: 0.327886  [   88/   89]
Per-example loss in batch: 0.642282  [   89/   89]
Train Error: Avg loss: 0.26314183
validation Error: 
 Avg loss: 0.28019920 
 F1: 0.488301 
 Precision: 0.581253 
 Recall: 0.420980
 IoU: 0.323015

test Error: 
 Avg loss: 0.25678315 
 F1: 0.516466 
 Precision: 0.618259 
 Recall: 0.443454
 IoU: 0.348132

We have finished training iteration 307
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_305_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.300960  [    2/   89]
Per-example loss in batch: 0.207368  [    4/   89]
Per-example loss in batch: 0.214742  [    6/   89]
Per-example loss in batch: 0.299433  [    8/   89]
Per-example loss in batch: 0.293513  [   10/   89]
Per-example loss in batch: 0.264877  [   12/   89]
Per-example loss in batch: 0.191745  [   14/   89]
Per-example loss in batch: 0.320608  [   16/   89]
Per-example loss in batch: 0.258180  [   18/   89]
Per-example loss in batch: 0.264765  [   20/   89]
Per-example loss in batch: 0.227028  [   22/   89]
Per-example loss in batch: 0.234264  [   24/   89]
Per-example loss in batch: 0.231055  [   26/   89]
Per-example loss in batch: 0.209566  [   28/   89]
Per-example loss in batch: 0.278723  [   30/   89]
Per-example loss in batch: 0.277885  [   32/   89]
Per-example loss in batch: 0.237516  [   34/   89]
Per-example loss in batch: 0.205685  [   36/   89]
Per-example loss in batch: 0.219716  [   38/   89]
Per-example loss in batch: 0.287017  [   40/   89]
Per-example loss in batch: 0.212010  [   42/   89]
Per-example loss in batch: 0.330333  [   44/   89]
Per-example loss in batch: 0.200968  [   46/   89]
Per-example loss in batch: 0.313215  [   48/   89]
Per-example loss in batch: 0.291818  [   50/   89]
Per-example loss in batch: 0.242663  [   52/   89]
Per-example loss in batch: 0.319131  [   54/   89]
Per-example loss in batch: 0.204952  [   56/   89]
Per-example loss in batch: 0.201212  [   58/   89]
Per-example loss in batch: 0.202925  [   60/   89]
Per-example loss in batch: 0.328237  [   62/   89]
Per-example loss in batch: 0.271470  [   64/   89]
Per-example loss in batch: 0.298389  [   66/   89]
Per-example loss in batch: 0.256220  [   68/   89]
Per-example loss in batch: 0.211352  [   70/   89]
Per-example loss in batch: 0.252537  [   72/   89]
Per-example loss in batch: 0.231907  [   74/   89]
Per-example loss in batch: 0.193610  [   76/   89]
Per-example loss in batch: 0.324363  [   78/   89]
Per-example loss in batch: 0.270619  [   80/   89]
Per-example loss in batch: 0.225962  [   82/   89]
Per-example loss in batch: 0.232764  [   84/   89]
Per-example loss in batch: 0.276362  [   86/   89]
Per-example loss in batch: 0.226616  [   88/   89]
Per-example loss in batch: 0.547680  [   89/   89]
Train Error: Avg loss: 0.25658696
validation Error: 
 Avg loss: 0.27044049 
 F1: 0.490781 
 Precision: 0.573021 
 Recall: 0.429185
 IoU: 0.325189

test Error: 
 Avg loss: 0.25324877 
 F1: 0.523713 
 Precision: 0.615111 
 Recall: 0.455963
 IoU: 0.354750

We have finished training iteration 308
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_212_.pth
Per-example loss in batch: 0.259568  [    2/   89]
Per-example loss in batch: 0.237619  [    4/   89]
Per-example loss in batch: 0.250957  [    6/   89]
Per-example loss in batch: 0.326747  [    8/   89]
Per-example loss in batch: 0.332334  [   10/   89]
Per-example loss in batch: 0.203905  [   12/   89]
Per-example loss in batch: 0.326173  [   14/   89]
Per-example loss in batch: 0.262886  [   16/   89]
Per-example loss in batch: 0.358710  [   18/   89]
Per-example loss in batch: 0.283625  [   20/   89]
Per-example loss in batch: 0.230864  [   22/   89]
Per-example loss in batch: 0.228199  [   24/   89]
Per-example loss in batch: 0.245582  [   26/   89]
Per-example loss in batch: 0.301026  [   28/   89]
Per-example loss in batch: 0.245260  [   30/   89]
Per-example loss in batch: 0.235813  [   32/   89]
Per-example loss in batch: 0.314658  [   34/   89]
Per-example loss in batch: 0.246580  [   36/   89]
Per-example loss in batch: 0.263951  [   38/   89]
Per-example loss in batch: 0.236637  [   40/   89]
Per-example loss in batch: 0.213506  [   42/   89]
Per-example loss in batch: 0.266993  [   44/   89]
Per-example loss in batch: 0.323333  [   46/   89]
Per-example loss in batch: 0.204523  [   48/   89]
Per-example loss in batch: 0.253909  [   50/   89]
Per-example loss in batch: 0.299596  [   52/   89]
Per-example loss in batch: 0.243806  [   54/   89]
Per-example loss in batch: 0.284072  [   56/   89]
Per-example loss in batch: 0.211086  [   58/   89]
Per-example loss in batch: 0.257731  [   60/   89]
Per-example loss in batch: 0.203735  [   62/   89]
Per-example loss in batch: 0.214509  [   64/   89]
Per-example loss in batch: 0.277937  [   66/   89]
Per-example loss in batch: 0.209686  [   68/   89]
Per-example loss in batch: 0.223666  [   70/   89]
Per-example loss in batch: 0.233852  [   72/   89]
Per-example loss in batch: 0.264919  [   74/   89]
Per-example loss in batch: 0.285893  [   76/   89]
Per-example loss in batch: 0.291718  [   78/   89]
Per-example loss in batch: 0.218675  [   80/   89]
Per-example loss in batch: 0.286656  [   82/   89]
Per-example loss in batch: 0.209177  [   84/   89]
Per-example loss in batch: 0.313374  [   86/   89]
Per-example loss in batch: 0.199736  [   88/   89]
Per-example loss in batch: 0.482594  [   89/   89]
Train Error: Avg loss: 0.26122422
validation Error: 
 Avg loss: 0.27750814 
 F1: 0.488838 
 Precision: 0.565404 
 Recall: 0.430536
 IoU: 0.323485

test Error: 
 Avg loss: 0.25575300 
 F1: 0.517773 
 Precision: 0.604500 
 Recall: 0.452809
 IoU: 0.349321

We have finished training iteration 309
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_307_.pth
Per-example loss in batch: 0.337364  [    2/   89]
Per-example loss in batch: 0.323379  [    4/   89]
Per-example loss in batch: 0.254415  [    6/   89]
Per-example loss in batch: 0.282949  [    8/   89]
Per-example loss in batch: 0.245309  [   10/   89]
Per-example loss in batch: 0.293922  [   12/   89]
Per-example loss in batch: 0.281436  [   14/   89]
Per-example loss in batch: 0.209789  [   16/   89]
Per-example loss in batch: 0.193649  [   18/   89]
Per-example loss in batch: 0.233702  [   20/   89]
Per-example loss in batch: 0.183127  [   22/   89]
Per-example loss in batch: 0.220057  [   24/   89]
Per-example loss in batch: 0.227954  [   26/   89]
Per-example loss in batch: 0.213241  [   28/   89]
Per-example loss in batch: 0.240602  [   30/   89]
Per-example loss in batch: 0.236452  [   32/   89]
Per-example loss in batch: 0.260591  [   34/   89]
Per-example loss in batch: 0.229855  [   36/   89]
Per-example loss in batch: 0.290372  [   38/   89]
Per-example loss in batch: 0.299944  [   40/   89]
Per-example loss in batch: 0.222798  [   42/   89]
Per-example loss in batch: 0.182763  [   44/   89]
Per-example loss in batch: 0.262371  [   46/   89]
Per-example loss in batch: 0.223531  [   48/   89]
Per-example loss in batch: 0.271864  [   50/   89]
Per-example loss in batch: 0.233865  [   52/   89]
Per-example loss in batch: 0.212224  [   54/   89]
Per-example loss in batch: 0.226478  [   56/   89]
Per-example loss in batch: 0.278976  [   58/   89]
Per-example loss in batch: 0.323924  [   60/   89]
Per-example loss in batch: 0.252566  [   62/   89]
Per-example loss in batch: 0.249445  [   64/   89]
Per-example loss in batch: 0.240215  [   66/   89]
Per-example loss in batch: 0.280575  [   68/   89]
Per-example loss in batch: 0.239415  [   70/   89]
Per-example loss in batch: 0.264455  [   72/   89]
Per-example loss in batch: 0.289580  [   74/   89]
Per-example loss in batch: 0.297497  [   76/   89]
Per-example loss in batch: 0.265968  [   78/   89]
Per-example loss in batch: 0.267474  [   80/   89]
Per-example loss in batch: 0.290675  [   82/   89]
Per-example loss in batch: 0.212018  [   84/   89]
Per-example loss in batch: 0.264177  [   86/   89]
Per-example loss in batch: 0.259153  [   88/   89]
Per-example loss in batch: 0.723803  [   89/   89]
Train Error: Avg loss: 0.25914643
validation Error: 
 Avg loss: 0.27371606 
 F1: 0.492036 
 Precision: 0.545476 
 Recall: 0.448132
 IoU: 0.326291

test Error: 
 Avg loss: 0.25142473 
 F1: 0.527269 
 Precision: 0.590360 
 Recall: 0.476360
 IoU: 0.358021

We have finished training iteration 310
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_308_.pth
Per-example loss in batch: 0.223667  [    2/   89]
Per-example loss in batch: 0.197276  [    4/   89]
Per-example loss in batch: 0.239321  [    6/   89]
Per-example loss in batch: 0.280605  [    8/   89]
Per-example loss in batch: 0.294492  [   10/   89]
Per-example loss in batch: 0.269314  [   12/   89]
Per-example loss in batch: 0.222370  [   14/   89]
Per-example loss in batch: 0.222239  [   16/   89]
Per-example loss in batch: 0.221211  [   18/   89]
Per-example loss in batch: 0.252044  [   20/   89]
Per-example loss in batch: 0.286688  [   22/   89]
Per-example loss in batch: 0.307748  [   24/   89]
Per-example loss in batch: 0.270271  [   26/   89]
Per-example loss in batch: 0.224246  [   28/   89]
Per-example loss in batch: 0.298419  [   30/   89]
Per-example loss in batch: 0.245804  [   32/   89]
Per-example loss in batch: 0.205109  [   34/   89]
Per-example loss in batch: 0.196009  [   36/   89]
Per-example loss in batch: 0.261928  [   38/   89]
Per-example loss in batch: 0.236709  [   40/   89]
Per-example loss in batch: 0.221172  [   42/   89]
Per-example loss in batch: 0.296936  [   44/   89]
Per-example loss in batch: 0.313270  [   46/   89]
Per-example loss in batch: 0.253912  [   48/   89]
Per-example loss in batch: 0.268353  [   50/   89]
Per-example loss in batch: 0.296186  [   52/   89]
Per-example loss in batch: 0.243230  [   54/   89]
Per-example loss in batch: 0.211586  [   56/   89]
Per-example loss in batch: 0.240755  [   58/   89]
Per-example loss in batch: 0.201926  [   60/   89]
Per-example loss in batch: 0.232640  [   62/   89]
Per-example loss in batch: 0.246548  [   64/   89]
Per-example loss in batch: 0.270387  [   66/   89]
Per-example loss in batch: 0.216847  [   68/   89]
Per-example loss in batch: 0.222563  [   70/   89]
Per-example loss in batch: 0.287977  [   72/   89]
Per-example loss in batch: 0.281664  [   74/   89]
Per-example loss in batch: 0.276586  [   76/   89]
Per-example loss in batch: 0.353761  [   78/   89]
Per-example loss in batch: 0.242714  [   80/   89]
Per-example loss in batch: 0.258176  [   82/   89]
Per-example loss in batch: 0.186574  [   84/   89]
Per-example loss in batch: 0.342436  [   86/   89]
Per-example loss in batch: 0.261178  [   88/   89]
Per-example loss in batch: 0.662874  [   89/   89]
Train Error: Avg loss: 0.25874797
validation Error: 
 Avg loss: 0.27797141 
 F1: 0.489402 
 Precision: 0.574257 
 Recall: 0.426395
 IoU: 0.323979

test Error: 
 Avg loss: 0.25520641 
 F1: 0.519597 
 Precision: 0.609654 
 Recall: 0.452722
 IoU: 0.350984

We have finished training iteration 311
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_309_.pth
Per-example loss in batch: 0.226389  [    2/   89]
Per-example loss in batch: 0.254324  [    4/   89]
Per-example loss in batch: 0.208719  [    6/   89]
Per-example loss in batch: 0.284549  [    8/   89]
Per-example loss in batch: 0.246685  [   10/   89]
Per-example loss in batch: 0.249176  [   12/   89]
Per-example loss in batch: 0.212866  [   14/   89]
Per-example loss in batch: 0.285774  [   16/   89]
Per-example loss in batch: 0.291204  [   18/   89]
Per-example loss in batch: 0.200284  [   20/   89]
Per-example loss in batch: 0.249791  [   22/   89]
Per-example loss in batch: 0.277253  [   24/   89]
Per-example loss in batch: 0.224132  [   26/   89]
Per-example loss in batch: 0.217506  [   28/   89]
Per-example loss in batch: 0.310345  [   30/   89]
Per-example loss in batch: 0.241579  [   32/   89]
Per-example loss in batch: 0.365877  [   34/   89]
Per-example loss in batch: 0.194582  [   36/   89]
Per-example loss in batch: 0.323235  [   38/   89]
Per-example loss in batch: 0.308197  [   40/   89]
Per-example loss in batch: 0.215175  [   42/   89]
Per-example loss in batch: 0.202372  [   44/   89]
Per-example loss in batch: 0.204132  [   46/   89]
Per-example loss in batch: 0.274588  [   48/   89]
Per-example loss in batch: 0.225723  [   50/   89]
Per-example loss in batch: 0.333648  [   52/   89]
Per-example loss in batch: 0.222335  [   54/   89]
Per-example loss in batch: 0.205667  [   56/   89]
Per-example loss in batch: 0.243096  [   58/   89]
Per-example loss in batch: 0.209800  [   60/   89]
Per-example loss in batch: 0.308899  [   62/   89]
Per-example loss in batch: 0.259721  [   64/   89]
Per-example loss in batch: 0.239288  [   66/   89]
Per-example loss in batch: 0.338324  [   68/   89]
Per-example loss in batch: 0.290702  [   70/   89]
Per-example loss in batch: 0.231447  [   72/   89]
Per-example loss in batch: 0.326045  [   74/   89]
Per-example loss in batch: 0.231999  [   76/   89]
Per-example loss in batch: 0.219745  [   78/   89]
Per-example loss in batch: 0.198055  [   80/   89]
Per-example loss in batch: 0.248068  [   82/   89]
Per-example loss in batch: 0.218434  [   84/   89]
Per-example loss in batch: 0.215303  [   86/   89]
Per-example loss in batch: 0.261853  [   88/   89]
Per-example loss in batch: 0.624013  [   89/   89]
Train Error: Avg loss: 0.25637954
validation Error: 
 Avg loss: 0.27045852 
 F1: 0.492220 
 Precision: 0.572088 
 Recall: 0.431921
 IoU: 0.326454

test Error: 
 Avg loss: 0.25363003 
 F1: 0.522779 
 Precision: 0.611207 
 Recall: 0.456704
 IoU: 0.353893

We have finished training iteration 312
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_310_.pth
Per-example loss in batch: 0.306024  [    2/   89]
Per-example loss in batch: 0.216386  [    4/   89]
Per-example loss in batch: 0.275846  [    6/   89]
Per-example loss in batch: 0.224444  [    8/   89]
Per-example loss in batch: 0.285978  [   10/   89]
Per-example loss in batch: 0.237388  [   12/   89]
Per-example loss in batch: 0.243819  [   14/   89]
Per-example loss in batch: 0.253792  [   16/   89]
Per-example loss in batch: 0.229224  [   18/   89]
Per-example loss in batch: 0.311875  [   20/   89]
Per-example loss in batch: 0.256735  [   22/   89]
Per-example loss in batch: 0.294703  [   24/   89]
Per-example loss in batch: 0.222819  [   26/   89]
Per-example loss in batch: 0.231352  [   28/   89]
Per-example loss in batch: 0.284263  [   30/   89]
Per-example loss in batch: 0.265008  [   32/   89]
Per-example loss in batch: 0.333392  [   34/   89]
Per-example loss in batch: 0.248487  [   36/   89]
Per-example loss in batch: 0.303792  [   38/   89]
Per-example loss in batch: 0.187318  [   40/   89]
Per-example loss in batch: 0.228443  [   42/   89]
Per-example loss in batch: 0.257233  [   44/   89]
Per-example loss in batch: 0.201376  [   46/   89]
Per-example loss in batch: 0.227008  [   48/   89]
Per-example loss in batch: 0.216145  [   50/   89]
Per-example loss in batch: 0.272926  [   52/   89]
Per-example loss in batch: 0.215413  [   54/   89]
Per-example loss in batch: 0.281159  [   56/   89]
Per-example loss in batch: 0.238386  [   58/   89]
Per-example loss in batch: 0.250072  [   60/   89]
Per-example loss in batch: 0.280978  [   62/   89]
Per-example loss in batch: 0.245758  [   64/   89]
Per-example loss in batch: 0.204219  [   66/   89]
Per-example loss in batch: 0.265330  [   68/   89]
Per-example loss in batch: 0.260718  [   70/   89]
Per-example loss in batch: 0.243740  [   72/   89]
Per-example loss in batch: 0.254888  [   74/   89]
Per-example loss in batch: 0.235446  [   76/   89]
Per-example loss in batch: 0.235297  [   78/   89]
Per-example loss in batch: 0.191635  [   80/   89]
Per-example loss in batch: 0.242558  [   82/   89]
Per-example loss in batch: 0.226745  [   84/   89]
Per-example loss in batch: 0.378417  [   86/   89]
Per-example loss in batch: 0.282544  [   88/   89]
Per-example loss in batch: 0.413569  [   89/   89]
Train Error: Avg loss: 0.25518796
validation Error: 
 Avg loss: 0.27162053 
 F1: 0.490107 
 Precision: 0.579213 
 Recall: 0.424762
 IoU: 0.324597

test Error: 
 Avg loss: 0.25494131 
 F1: 0.519583 
 Precision: 0.614940 
 Recall: 0.449828
 IoU: 0.350970

We have finished training iteration 313
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_311_.pth
Per-example loss in batch: 0.205478  [    2/   89]
Per-example loss in batch: 0.297070  [    4/   89]
Per-example loss in batch: 0.257874  [    6/   89]
Per-example loss in batch: 0.261384  [    8/   89]
Per-example loss in batch: 0.265468  [   10/   89]
Per-example loss in batch: 0.298518  [   12/   89]
Per-example loss in batch: 0.240772  [   14/   89]
Per-example loss in batch: 0.238034  [   16/   89]
Per-example loss in batch: 0.243630  [   18/   89]
Per-example loss in batch: 0.218029  [   20/   89]
Per-example loss in batch: 0.249068  [   22/   89]
Per-example loss in batch: 0.282731  [   24/   89]
Per-example loss in batch: 0.211552  [   26/   89]
Per-example loss in batch: 0.256554  [   28/   89]
Per-example loss in batch: 0.228670  [   30/   89]
Per-example loss in batch: 0.301766  [   32/   89]
Per-example loss in batch: 0.176384  [   34/   89]
Per-example loss in batch: 0.290138  [   36/   89]
Per-example loss in batch: 0.199765  [   38/   89]
Per-example loss in batch: 0.264363  [   40/   89]
Per-example loss in batch: 0.215177  [   42/   89]
Per-example loss in batch: 0.295608  [   44/   89]
Per-example loss in batch: 0.320376  [   46/   89]
Per-example loss in batch: 0.246675  [   48/   89]
Per-example loss in batch: 0.295136  [   50/   89]
Per-example loss in batch: 0.342265  [   52/   89]
Per-example loss in batch: 0.258057  [   54/   89]
Per-example loss in batch: 0.186563  [   56/   89]
Per-example loss in batch: 0.310091  [   58/   89]
Per-example loss in batch: 0.254885  [   60/   89]
Per-example loss in batch: 0.227713  [   62/   89]
Per-example loss in batch: 0.226653  [   64/   89]
Per-example loss in batch: 0.315201  [   66/   89]
Per-example loss in batch: 0.366059  [   68/   89]
Per-example loss in batch: 0.209257  [   70/   89]
Per-example loss in batch: 0.255612  [   72/   89]
Per-example loss in batch: 0.245335  [   74/   89]
Per-example loss in batch: 0.247054  [   76/   89]
Per-example loss in batch: 0.208429  [   78/   89]
Per-example loss in batch: 0.216523  [   80/   89]
Per-example loss in batch: 0.302071  [   82/   89]
Per-example loss in batch: 0.198207  [   84/   89]
Per-example loss in batch: 0.290030  [   86/   89]
Per-example loss in batch: 0.197015  [   88/   89]
Per-example loss in batch: 0.670392  [   89/   89]
Train Error: Avg loss: 0.25960531
validation Error: 
 Avg loss: 0.27304275 
 F1: 0.491902 
 Precision: 0.556497 
 Recall: 0.440743
 IoU: 0.326174

test Error: 
 Avg loss: 0.25111101 
 F1: 0.528625 
 Precision: 0.602406 
 Recall: 0.470945
 IoU: 0.359273

We have finished training iteration 314
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_312_.pth
Per-example loss in batch: 0.237841  [    2/   89]
Per-example loss in batch: 0.227590  [    4/   89]
Per-example loss in batch: 0.226313  [    6/   89]
Per-example loss in batch: 0.314837  [    8/   89]
Per-example loss in batch: 0.236347  [   10/   89]
Per-example loss in batch: 0.232206  [   12/   89]
Per-example loss in batch: 0.223052  [   14/   89]
Per-example loss in batch: 0.304867  [   16/   89]
Per-example loss in batch: 0.191762  [   18/   89]
Per-example loss in batch: 0.235171  [   20/   89]
Per-example loss in batch: 0.255154  [   22/   89]
Per-example loss in batch: 0.209846  [   24/   89]
Per-example loss in batch: 0.251474  [   26/   89]
Per-example loss in batch: 0.200743  [   28/   89]
Per-example loss in batch: 0.240950  [   30/   89]
Per-example loss in batch: 0.253130  [   32/   89]
Per-example loss in batch: 0.221442  [   34/   89]
Per-example loss in batch: 0.293246  [   36/   89]
Per-example loss in batch: 0.203139  [   38/   89]
Per-example loss in batch: 0.219673  [   40/   89]
Per-example loss in batch: 0.279799  [   42/   89]
Per-example loss in batch: 0.205445  [   44/   89]
Per-example loss in batch: 0.300906  [   46/   89]
Per-example loss in batch: 0.268725  [   48/   89]
Per-example loss in batch: 0.210693  [   50/   89]
Per-example loss in batch: 0.274835  [   52/   89]
Per-example loss in batch: 0.318325  [   54/   89]
Per-example loss in batch: 0.215596  [   56/   89]
Per-example loss in batch: 0.300532  [   58/   89]
Per-example loss in batch: 0.226042  [   60/   89]
Per-example loss in batch: 0.203938  [   62/   89]
Per-example loss in batch: 0.257958  [   64/   89]
Per-example loss in batch: 0.282194  [   66/   89]
Per-example loss in batch: 0.226373  [   68/   89]
Per-example loss in batch: 0.198948  [   70/   89]
Per-example loss in batch: 0.243708  [   72/   89]
Per-example loss in batch: 0.305077  [   74/   89]
Per-example loss in batch: 0.295573  [   76/   89]
Per-example loss in batch: 0.310733  [   78/   89]
Per-example loss in batch: 0.261384  [   80/   89]
Per-example loss in batch: 0.216097  [   82/   89]
Per-example loss in batch: 0.240051  [   84/   89]
Per-example loss in batch: 0.359218  [   86/   89]
Per-example loss in batch: 0.250521  [   88/   89]
Per-example loss in batch: 0.565307  [   89/   89]
Train Error: Avg loss: 0.25424963
validation Error: 
 Avg loss: 0.27791937 
 F1: 0.493103 
 Precision: 0.568839 
 Recall: 0.435165
 IoU: 0.327231

test Error: 
 Avg loss: 0.25202601 
 F1: 0.526269 
 Precision: 0.614180 
 Recall: 0.460374
 IoU: 0.357100

We have finished training iteration 315
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_313_.pth
Per-example loss in batch: 0.208158  [    2/   89]
Per-example loss in batch: 0.234926  [    4/   89]
Per-example loss in batch: 0.257838  [    6/   89]
Per-example loss in batch: 0.278245  [    8/   89]
Per-example loss in batch: 0.255940  [   10/   89]
Per-example loss in batch: 0.269286  [   12/   89]
Per-example loss in batch: 0.270653  [   14/   89]
Per-example loss in batch: 0.214805  [   16/   89]
Per-example loss in batch: 0.284337  [   18/   89]
Per-example loss in batch: 0.311156  [   20/   89]
Per-example loss in batch: 0.337504  [   22/   89]
Per-example loss in batch: 0.272605  [   24/   89]
Per-example loss in batch: 0.313602  [   26/   89]
Per-example loss in batch: 0.189316  [   28/   89]
Per-example loss in batch: 0.237003  [   30/   89]
Per-example loss in batch: 0.241015  [   32/   89]
Per-example loss in batch: 0.209222  [   34/   89]
Per-example loss in batch: 0.237186  [   36/   89]
Per-example loss in batch: 0.220446  [   38/   89]
Per-example loss in batch: 0.281312  [   40/   89]
Per-example loss in batch: 0.247356  [   42/   89]
Per-example loss in batch: 0.243866  [   44/   89]
Per-example loss in batch: 0.198452  [   46/   89]
Per-example loss in batch: 0.220131  [   48/   89]
Per-example loss in batch: 0.212280  [   50/   89]
Per-example loss in batch: 0.272533  [   52/   89]
Per-example loss in batch: 0.210005  [   54/   89]
Per-example loss in batch: 0.281233  [   56/   89]
Per-example loss in batch: 0.328794  [   58/   89]
Per-example loss in batch: 0.323090  [   60/   89]
Per-example loss in batch: 0.233021  [   62/   89]
Per-example loss in batch: 0.200913  [   64/   89]
Per-example loss in batch: 0.306290  [   66/   89]
Per-example loss in batch: 0.245725  [   68/   89]
Per-example loss in batch: 0.304934  [   70/   89]
Per-example loss in batch: 0.343995  [   72/   89]
Per-example loss in batch: 0.231499  [   74/   89]
Per-example loss in batch: 0.222129  [   76/   89]
Per-example loss in batch: 0.210781  [   78/   89]
Per-example loss in batch: 0.295688  [   80/   89]
Per-example loss in batch: 0.280139  [   82/   89]
Per-example loss in batch: 0.328439  [   84/   89]
Per-example loss in batch: 0.323281  [   86/   89]
Per-example loss in batch: 0.199916  [   88/   89]
Per-example loss in batch: 0.408371  [   89/   89]
Train Error: Avg loss: 0.26052204
validation Error: 
 Avg loss: 0.27911428 
 F1: 0.487479 
 Precision: 0.574417 
 Recall: 0.423398
 IoU: 0.322296

test Error: 
 Avg loss: 0.25531930 
 F1: 0.518729 
 Precision: 0.611045 
 Recall: 0.450645
 IoU: 0.350191

We have finished training iteration 316
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_314_.pth
Per-example loss in batch: 0.196044  [    2/   89]
Per-example loss in batch: 0.213116  [    4/   89]
Per-example loss in batch: 0.307475  [    6/   89]
Per-example loss in batch: 0.248629  [    8/   89]
Per-example loss in batch: 0.230967  [   10/   89]
Per-example loss in batch: 0.274345  [   12/   89]
Per-example loss in batch: 0.229202  [   14/   89]
Per-example loss in batch: 0.352654  [   16/   89]
Per-example loss in batch: 0.271353  [   18/   89]
Per-example loss in batch: 0.235693  [   20/   89]
Per-example loss in batch: 0.215758  [   22/   89]
Per-example loss in batch: 0.209403  [   24/   89]
Per-example loss in batch: 0.247991  [   26/   89]
Per-example loss in batch: 0.182974  [   28/   89]
Per-example loss in batch: 0.312991  [   30/   89]
Per-example loss in batch: 0.257643  [   32/   89]
Per-example loss in batch: 0.208884  [   34/   89]
Per-example loss in batch: 0.214226  [   36/   89]
Per-example loss in batch: 0.233208  [   38/   89]
Per-example loss in batch: 0.228022  [   40/   89]
Per-example loss in batch: 0.228086  [   42/   89]
Per-example loss in batch: 0.319706  [   44/   89]
Per-example loss in batch: 0.212609  [   46/   89]
Per-example loss in batch: 0.219608  [   48/   89]
Per-example loss in batch: 0.197560  [   50/   89]
Per-example loss in batch: 0.324157  [   52/   89]
Per-example loss in batch: 0.240525  [   54/   89]
Per-example loss in batch: 0.251022  [   56/   89]
Per-example loss in batch: 0.198080  [   58/   89]
Per-example loss in batch: 0.296517  [   60/   89]
Per-example loss in batch: 0.301362  [   62/   89]
Per-example loss in batch: 0.263523  [   64/   89]
Per-example loss in batch: 0.201851  [   66/   89]
Per-example loss in batch: 0.279644  [   68/   89]
Per-example loss in batch: 0.193870  [   70/   89]
Per-example loss in batch: 0.229783  [   72/   89]
Per-example loss in batch: 0.252490  [   74/   89]
Per-example loss in batch: 0.268339  [   76/   89]
Per-example loss in batch: 0.265336  [   78/   89]
Per-example loss in batch: 0.331473  [   80/   89]
Per-example loss in batch: 0.281091  [   82/   89]
Per-example loss in batch: 0.349112  [   84/   89]
Per-example loss in batch: 0.268328  [   86/   89]
Per-example loss in batch: 0.288828  [   88/   89]
Per-example loss in batch: 0.403900  [   89/   89]
Train Error: Avg loss: 0.25472870
validation Error: 
 Avg loss: 0.27191586 
 F1: 0.492943 
 Precision: 0.561427 
 Recall: 0.439350
 IoU: 0.327090

test Error: 
 Avg loss: 0.25137484 
 F1: 0.527775 
 Precision: 0.605046 
 Recall: 0.468006
 IoU: 0.358488

We have finished training iteration 317
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_315_.pth
Per-example loss in batch: 0.324042  [    2/   89]
Per-example loss in batch: 0.242428  [    4/   89]
Per-example loss in batch: 0.284329  [    6/   89]
Per-example loss in batch: 0.222014  [    8/   89]
Per-example loss in batch: 0.257867  [   10/   89]
Per-example loss in batch: 0.295550  [   12/   89]
Per-example loss in batch: 0.315354  [   14/   89]
Per-example loss in batch: 0.328593  [   16/   89]
Per-example loss in batch: 0.226989  [   18/   89]
Per-example loss in batch: 0.218483  [   20/   89]
Per-example loss in batch: 0.187152  [   22/   89]
Per-example loss in batch: 0.232090  [   24/   89]
Per-example loss in batch: 0.300930  [   26/   89]
Per-example loss in batch: 0.245577  [   28/   89]
Per-example loss in batch: 0.179827  [   30/   89]
Per-example loss in batch: 0.295686  [   32/   89]
Per-example loss in batch: 0.270803  [   34/   89]
Per-example loss in batch: 0.302305  [   36/   89]
Per-example loss in batch: 0.248783  [   38/   89]
Per-example loss in batch: 0.260674  [   40/   89]
Per-example loss in batch: 0.326440  [   42/   89]
Per-example loss in batch: 0.208526  [   44/   89]
Per-example loss in batch: 0.239916  [   46/   89]
Per-example loss in batch: 0.237216  [   48/   89]
Per-example loss in batch: 0.273703  [   50/   89]
Per-example loss in batch: 0.268948  [   52/   89]
Per-example loss in batch: 0.276560  [   54/   89]
Per-example loss in batch: 0.234791  [   56/   89]
Per-example loss in batch: 0.357675  [   58/   89]
Per-example loss in batch: 0.199664  [   60/   89]
Per-example loss in batch: 0.198347  [   62/   89]
Per-example loss in batch: 0.179811  [   64/   89]
Per-example loss in batch: 0.256487  [   66/   89]
Per-example loss in batch: 0.217374  [   68/   89]
Per-example loss in batch: 0.342640  [   70/   89]
Per-example loss in batch: 0.328092  [   72/   89]
Per-example loss in batch: 0.304145  [   74/   89]
Per-example loss in batch: 0.248948  [   76/   89]
Per-example loss in batch: 0.221390  [   78/   89]
Per-example loss in batch: 0.260763  [   80/   89]
Per-example loss in batch: 0.200793  [   82/   89]
Per-example loss in batch: 0.246056  [   84/   89]
Per-example loss in batch: 0.202496  [   86/   89]
Per-example loss in batch: 0.260693  [   88/   89]
Per-example loss in batch: 0.689878  [   89/   89]
Train Error: Avg loss: 0.26237949
validation Error: 
 Avg loss: 0.27224305 
 F1: 0.490104 
 Precision: 0.583291 
 Recall: 0.422591
 IoU: 0.324595

test Error: 
 Avg loss: 0.25513035 
 F1: 0.519226 
 Precision: 0.615037 
 Recall: 0.449242
 IoU: 0.350645

We have finished training iteration 318
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_316_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.247203  [    2/   89]
Per-example loss in batch: 0.260060  [    4/   89]
Per-example loss in batch: 0.345003  [    6/   89]
Per-example loss in batch: 0.267361  [    8/   89]
Per-example loss in batch: 0.234353  [   10/   89]
Per-example loss in batch: 0.263358  [   12/   89]
Per-example loss in batch: 0.270390  [   14/   89]
Per-example loss in batch: 0.243530  [   16/   89]
Per-example loss in batch: 0.209421  [   18/   89]
Per-example loss in batch: 0.243234  [   20/   89]
Per-example loss in batch: 0.192676  [   22/   89]
Per-example loss in batch: 0.307714  [   24/   89]
Per-example loss in batch: 0.221512  [   26/   89]
Per-example loss in batch: 0.266017  [   28/   89]
Per-example loss in batch: 0.270443  [   30/   89]
Per-example loss in batch: 0.292192  [   32/   89]
Per-example loss in batch: 0.279412  [   34/   89]
Per-example loss in batch: 0.230603  [   36/   89]
Per-example loss in batch: 0.262800  [   38/   89]
Per-example loss in batch: 0.207202  [   40/   89]
Per-example loss in batch: 0.336928  [   42/   89]
Per-example loss in batch: 0.242081  [   44/   89]
Per-example loss in batch: 0.187916  [   46/   89]
Per-example loss in batch: 0.223084  [   48/   89]
Per-example loss in batch: 0.254098  [   50/   89]
Per-example loss in batch: 0.283171  [   52/   89]
Per-example loss in batch: 0.265494  [   54/   89]
Per-example loss in batch: 0.217013  [   56/   89]
Per-example loss in batch: 0.244668  [   58/   89]
Per-example loss in batch: 0.219736  [   60/   89]
Per-example loss in batch: 0.286640  [   62/   89]
Per-example loss in batch: 0.250979  [   64/   89]
Per-example loss in batch: 0.180902  [   66/   89]
Per-example loss in batch: 0.309038  [   68/   89]
Per-example loss in batch: 0.301642  [   70/   89]
Per-example loss in batch: 0.302882  [   72/   89]
Per-example loss in batch: 0.297030  [   74/   89]
Per-example loss in batch: 0.310542  [   76/   89]
Per-example loss in batch: 0.301168  [   78/   89]
Per-example loss in batch: 0.226220  [   80/   89]
Per-example loss in batch: 0.230034  [   82/   89]
Per-example loss in batch: 0.185675  [   84/   89]
Per-example loss in batch: 0.350082  [   86/   89]
Per-example loss in batch: 0.268446  [   88/   89]
Per-example loss in batch: 0.418760  [   89/   89]
Train Error: Avg loss: 0.26065913
validation Error: 
 Avg loss: 0.27109807 
 F1: 0.492210 
 Precision: 0.555406 
 Recall: 0.441927
 IoU: 0.326445

test Error: 
 Avg loss: 0.25086114 
 F1: 0.528686 
 Precision: 0.600569 
 Recall: 0.472171
 IoU: 0.359329

We have finished training iteration 319
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_317_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.236827  [    2/   89]
Per-example loss in batch: 0.301325  [    4/   89]
Per-example loss in batch: 0.230960  [    6/   89]
Per-example loss in batch: 0.245580  [    8/   89]
Per-example loss in batch: 0.351530  [   10/   89]
Per-example loss in batch: 0.212814  [   12/   89]
Per-example loss in batch: 0.297470  [   14/   89]
Per-example loss in batch: 0.323941  [   16/   89]
Per-example loss in batch: 0.241481  [   18/   89]
Per-example loss in batch: 0.342203  [   20/   89]
Per-example loss in batch: 0.241635  [   22/   89]
Per-example loss in batch: 0.209468  [   24/   89]
Per-example loss in batch: 0.231244  [   26/   89]
Per-example loss in batch: 0.261865  [   28/   89]
Per-example loss in batch: 0.361530  [   30/   89]
Per-example loss in batch: 0.247416  [   32/   89]
Per-example loss in batch: 0.215795  [   34/   89]
Per-example loss in batch: 0.223588  [   36/   89]
Per-example loss in batch: 0.222704  [   38/   89]
Per-example loss in batch: 0.286895  [   40/   89]
Per-example loss in batch: 0.216601  [   42/   89]
Per-example loss in batch: 0.205988  [   44/   89]
Per-example loss in batch: 0.267211  [   46/   89]
Per-example loss in batch: 0.266222  [   48/   89]
Per-example loss in batch: 0.184971  [   50/   89]
Per-example loss in batch: 0.304510  [   52/   89]
Per-example loss in batch: 0.208349  [   54/   89]
Per-example loss in batch: 0.202643  [   56/   89]
Per-example loss in batch: 0.223994  [   58/   89]
Per-example loss in batch: 0.207779  [   60/   89]
Per-example loss in batch: 0.232579  [   62/   89]
Per-example loss in batch: 0.194906  [   64/   89]
Per-example loss in batch: 0.236035  [   66/   89]
Per-example loss in batch: 0.194349  [   68/   89]
Per-example loss in batch: 0.289803  [   70/   89]
Per-example loss in batch: 0.203380  [   72/   89]
Per-example loss in batch: 0.310314  [   74/   89]
Per-example loss in batch: 0.191345  [   76/   89]
Per-example loss in batch: 0.282332  [   78/   89]
Per-example loss in batch: 0.290141  [   80/   89]
Per-example loss in batch: 0.195674  [   82/   89]
Per-example loss in batch: 0.367831  [   84/   89]
Per-example loss in batch: 0.318504  [   86/   89]
Per-example loss in batch: 0.233719  [   88/   89]
Per-example loss in batch: 0.630171  [   89/   89]
Train Error: Avg loss: 0.25686597
validation Error: 
 Avg loss: 0.27624539 
 F1: 0.490604 
 Precision: 0.564402 
 Recall: 0.433873
 IoU: 0.325033

test Error: 
 Avg loss: 0.25262869 
 F1: 0.525030 
 Precision: 0.607718 
 Recall: 0.462148
 IoU: 0.355960

We have finished training iteration 320
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_318_.pth
Per-example loss in batch: 0.198459  [    2/   89]
Per-example loss in batch: 0.214349  [    4/   89]
Per-example loss in batch: 0.287177  [    6/   89]
Per-example loss in batch: 0.255370  [    8/   89]
Per-example loss in batch: 0.243335  [   10/   89]
Per-example loss in batch: 0.296741  [   12/   89]
Per-example loss in batch: 0.266699  [   14/   89]
Per-example loss in batch: 0.265760  [   16/   89]
Per-example loss in batch: 0.285556  [   18/   89]
Per-example loss in batch: 0.230801  [   20/   89]
Per-example loss in batch: 0.224357  [   22/   89]
Per-example loss in batch: 0.295286  [   24/   89]
Per-example loss in batch: 0.248006  [   26/   89]
Per-example loss in batch: 0.212161  [   28/   89]
Per-example loss in batch: 0.262768  [   30/   89]
Per-example loss in batch: 0.208277  [   32/   89]
Per-example loss in batch: 0.248602  [   34/   89]
Per-example loss in batch: 0.230827  [   36/   89]
Per-example loss in batch: 0.192900  [   38/   89]
Per-example loss in batch: 0.279070  [   40/   89]
Per-example loss in batch: 0.262550  [   42/   89]
Per-example loss in batch: 0.232509  [   44/   89]
Per-example loss in batch: 0.282576  [   46/   89]
Per-example loss in batch: 0.197376  [   48/   89]
Per-example loss in batch: 0.215409  [   50/   89]
Per-example loss in batch: 0.291647  [   52/   89]
Per-example loss in batch: 0.291569  [   54/   89]
Per-example loss in batch: 0.231770  [   56/   89]
Per-example loss in batch: 0.240935  [   58/   89]
Per-example loss in batch: 0.330232  [   60/   89]
Per-example loss in batch: 0.219095  [   62/   89]
Per-example loss in batch: 0.229075  [   64/   89]
Per-example loss in batch: 0.314667  [   66/   89]
Per-example loss in batch: 0.258455  [   68/   89]
Per-example loss in batch: 0.328939  [   70/   89]
Per-example loss in batch: 0.263841  [   72/   89]
Per-example loss in batch: 0.231359  [   74/   89]
Per-example loss in batch: 0.201216  [   76/   89]
Per-example loss in batch: 0.253758  [   78/   89]
Per-example loss in batch: 0.240944  [   80/   89]
Per-example loss in batch: 0.247612  [   82/   89]
Per-example loss in batch: 0.190170  [   84/   89]
Per-example loss in batch: 0.284037  [   86/   89]
Per-example loss in batch: 0.221309  [   88/   89]
Per-example loss in batch: 0.433128  [   89/   89]
Train Error: Avg loss: 0.25222735
validation Error: 
 Avg loss: 0.26973426 
 F1: 0.490463 
 Precision: 0.554138 
 Recall: 0.439913
 IoU: 0.324909

test Error: 
 Avg loss: 0.25125747 
 F1: 0.528455 
 Precision: 0.603171 
 Recall: 0.470210
 IoU: 0.359116

We have finished training iteration 321
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_319_.pth
Per-example loss in batch: 0.225039  [    2/   89]
Per-example loss in batch: 0.275576  [    4/   89]
Per-example loss in batch: 0.247669  [    6/   89]
Per-example loss in batch: 0.222347  [    8/   89]
Per-example loss in batch: 0.246139  [   10/   89]
Per-example loss in batch: 0.210083  [   12/   89]
Per-example loss in batch: 0.190112  [   14/   89]
Per-example loss in batch: 0.210822  [   16/   89]
Per-example loss in batch: 0.244611  [   18/   89]
Per-example loss in batch: 0.269262  [   20/   89]
Per-example loss in batch: 0.224265  [   22/   89]
Per-example loss in batch: 0.223525  [   24/   89]
Per-example loss in batch: 0.212567  [   26/   89]
Per-example loss in batch: 0.288548  [   28/   89]
Per-example loss in batch: 0.244081  [   30/   89]
Per-example loss in batch: 0.295699  [   32/   89]
Per-example loss in batch: 0.290843  [   34/   89]
Per-example loss in batch: 0.214700  [   36/   89]
Per-example loss in batch: 0.227519  [   38/   89]
Per-example loss in batch: 0.255552  [   40/   89]
Per-example loss in batch: 0.261559  [   42/   89]
Per-example loss in batch: 0.234640  [   44/   89]
Per-example loss in batch: 0.254880  [   46/   89]
Per-example loss in batch: 0.247627  [   48/   89]
Per-example loss in batch: 0.224058  [   50/   89]
Per-example loss in batch: 0.311872  [   52/   89]
Per-example loss in batch: 0.273398  [   54/   89]
Per-example loss in batch: 0.282980  [   56/   89]
Per-example loss in batch: 0.180724  [   58/   89]
Per-example loss in batch: 0.227115  [   60/   89]
Per-example loss in batch: 0.318576  [   62/   89]
Per-example loss in batch: 0.311474  [   64/   89]
Per-example loss in batch: 0.316030  [   66/   89]
Per-example loss in batch: 0.192168  [   68/   89]
Per-example loss in batch: 0.222079  [   70/   89]
Per-example loss in batch: 0.213102  [   72/   89]
Per-example loss in batch: 0.238882  [   74/   89]
Per-example loss in batch: 0.336071  [   76/   89]
Per-example loss in batch: 0.302948  [   78/   89]
Per-example loss in batch: 0.296707  [   80/   89]
Per-example loss in batch: 0.313787  [   82/   89]
Per-example loss in batch: 0.295340  [   84/   89]
Per-example loss in batch: 0.254309  [   86/   89]
Per-example loss in batch: 0.220140  [   88/   89]
Per-example loss in batch: 0.440544  [   89/   89]
Train Error: Avg loss: 0.25549881
validation Error: 
 Avg loss: 0.27378068 
 F1: 0.491337 
 Precision: 0.560172 
 Recall: 0.437568
 IoU: 0.325677

test Error: 
 Avg loss: 0.25219291 
 F1: 0.526030 
 Precision: 0.607643 
 Recall: 0.463743
 IoU: 0.356879

We have finished training iteration 322
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_320_.pth
Per-example loss in batch: 0.257967  [    2/   89]
Per-example loss in batch: 0.211261  [    4/   89]
Per-example loss in batch: 0.294046  [    6/   89]
Per-example loss in batch: 0.205402  [    8/   89]
Per-example loss in batch: 0.235240  [   10/   89]
Per-example loss in batch: 0.271351  [   12/   89]
Per-example loss in batch: 0.300433  [   14/   89]
Per-example loss in batch: 0.234593  [   16/   89]
Per-example loss in batch: 0.266582  [   18/   89]
Per-example loss in batch: 0.222557  [   20/   89]
Per-example loss in batch: 0.175855  [   22/   89]
Per-example loss in batch: 0.249129  [   24/   89]
Per-example loss in batch: 0.245253  [   26/   89]
Per-example loss in batch: 0.228961  [   28/   89]
Per-example loss in batch: 0.240391  [   30/   89]
Per-example loss in batch: 0.249492  [   32/   89]
Per-example loss in batch: 0.229522  [   34/   89]
Per-example loss in batch: 0.243391  [   36/   89]
Per-example loss in batch: 0.240717  [   38/   89]
Per-example loss in batch: 0.362329  [   40/   89]
Per-example loss in batch: 0.241840  [   42/   89]
Per-example loss in batch: 0.237124  [   44/   89]
Per-example loss in batch: 0.316934  [   46/   89]
Per-example loss in batch: 0.232718  [   48/   89]
Per-example loss in batch: 0.217299  [   50/   89]
Per-example loss in batch: 0.201073  [   52/   89]
Per-example loss in batch: 0.217178  [   54/   89]
Per-example loss in batch: 0.275745  [   56/   89]
Per-example loss in batch: 0.244838  [   58/   89]
Per-example loss in batch: 0.283677  [   60/   89]
Per-example loss in batch: 0.224648  [   62/   89]
Per-example loss in batch: 0.291008  [   64/   89]
Per-example loss in batch: 0.295485  [   66/   89]
Per-example loss in batch: 0.315710  [   68/   89]
Per-example loss in batch: 0.271459  [   70/   89]
Per-example loss in batch: 0.308968  [   72/   89]
Per-example loss in batch: 0.325267  [   74/   89]
Per-example loss in batch: 0.243763  [   76/   89]
Per-example loss in batch: 0.348969  [   78/   89]
Per-example loss in batch: 0.210452  [   80/   89]
Per-example loss in batch: 0.232951  [   82/   89]
Per-example loss in batch: 0.226701  [   84/   89]
Per-example loss in batch: 0.254658  [   86/   89]
Per-example loss in batch: 0.206490  [   88/   89]
Per-example loss in batch: 0.377863  [   89/   89]
Train Error: Avg loss: 0.25569347
validation Error: 
 Avg loss: 0.27674169 
 F1: 0.491411 
 Precision: 0.562955 
 Recall: 0.436001
 IoU: 0.325742

test Error: 
 Avg loss: 0.25286003 
 F1: 0.524720 
 Precision: 0.607984 
 Recall: 0.461515
 IoU: 0.355675

We have finished training iteration 323
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_321_.pth
Per-example loss in batch: 0.206949  [    2/   89]
Per-example loss in batch: 0.192010  [    4/   89]
Per-example loss in batch: 0.200744  [    6/   89]
Per-example loss in batch: 0.233786  [    8/   89]
Per-example loss in batch: 0.239443  [   10/   89]
Per-example loss in batch: 0.279839  [   12/   89]
Per-example loss in batch: 0.332572  [   14/   89]
Per-example loss in batch: 0.274258  [   16/   89]
Per-example loss in batch: 0.207871  [   18/   89]
Per-example loss in batch: 0.194100  [   20/   89]
Per-example loss in batch: 0.278683  [   22/   89]
Per-example loss in batch: 0.276665  [   24/   89]
Per-example loss in batch: 0.230267  [   26/   89]
Per-example loss in batch: 0.260997  [   28/   89]
Per-example loss in batch: 0.204501  [   30/   89]
Per-example loss in batch: 0.222834  [   32/   89]
Per-example loss in batch: 0.196854  [   34/   89]
Per-example loss in batch: 0.258980  [   36/   89]
Per-example loss in batch: 0.238929  [   38/   89]
Per-example loss in batch: 0.249913  [   40/   89]
Per-example loss in batch: 0.223355  [   42/   89]
Per-example loss in batch: 0.254641  [   44/   89]
Per-example loss in batch: 0.236840  [   46/   89]
Per-example loss in batch: 0.219935  [   48/   89]
Per-example loss in batch: 0.277583  [   50/   89]
Per-example loss in batch: 0.296872  [   52/   89]
Per-example loss in batch: 0.197731  [   54/   89]
Per-example loss in batch: 0.232584  [   56/   89]
Per-example loss in batch: 0.305948  [   58/   89]
Per-example loss in batch: 0.223319  [   60/   89]
Per-example loss in batch: 0.320718  [   62/   89]
Per-example loss in batch: 0.288611  [   64/   89]
Per-example loss in batch: 0.225857  [   66/   89]
Per-example loss in batch: 0.250840  [   68/   89]
Per-example loss in batch: 0.306141  [   70/   89]
Per-example loss in batch: 0.318614  [   72/   89]
Per-example loss in batch: 0.240581  [   74/   89]
Per-example loss in batch: 0.241340  [   76/   89]
Per-example loss in batch: 0.336139  [   78/   89]
Per-example loss in batch: 0.256153  [   80/   89]
Per-example loss in batch: 0.316390  [   82/   89]
Per-example loss in batch: 0.325874  [   84/   89]
Per-example loss in batch: 0.298520  [   86/   89]
Per-example loss in batch: 0.202666  [   88/   89]
Per-example loss in batch: 0.408725  [   89/   89]
Train Error: Avg loss: 0.25577103
validation Error: 
 Avg loss: 0.27441247 
 F1: 0.492498 
 Precision: 0.558373 
 Recall: 0.440526
 IoU: 0.326698

test Error: 
 Avg loss: 0.25184711 
 F1: 0.526782 
 Precision: 0.605046 
 Recall: 0.466446
 IoU: 0.357572

We have finished training iteration 324
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_322_.pth
Per-example loss in batch: 0.209436  [    2/   89]
Per-example loss in batch: 0.199355  [    4/   89]
Per-example loss in batch: 0.276431  [    6/   89]
Per-example loss in batch: 0.205799  [    8/   89]
Per-example loss in batch: 0.245257  [   10/   89]
Per-example loss in batch: 0.250464  [   12/   89]
Per-example loss in batch: 0.322807  [   14/   89]
Per-example loss in batch: 0.355628  [   16/   89]
Per-example loss in batch: 0.216720  [   18/   89]
Per-example loss in batch: 0.295715  [   20/   89]
Per-example loss in batch: 0.244529  [   22/   89]
Per-example loss in batch: 0.279719  [   24/   89]
Per-example loss in batch: 0.265284  [   26/   89]
Per-example loss in batch: 0.191503  [   28/   89]
Per-example loss in batch: 0.244554  [   30/   89]
Per-example loss in batch: 0.337042  [   32/   89]
Per-example loss in batch: 0.239261  [   34/   89]
Per-example loss in batch: 0.219215  [   36/   89]
Per-example loss in batch: 0.304465  [   38/   89]
Per-example loss in batch: 0.209302  [   40/   89]
Per-example loss in batch: 0.206158  [   42/   89]
Per-example loss in batch: 0.335198  [   44/   89]
Per-example loss in batch: 0.320215  [   46/   89]
Per-example loss in batch: 0.213347  [   48/   89]
Per-example loss in batch: 0.201048  [   50/   89]
Per-example loss in batch: 0.220931  [   52/   89]
Per-example loss in batch: 0.233099  [   54/   89]
Per-example loss in batch: 0.180520  [   56/   89]
Per-example loss in batch: 0.284116  [   58/   89]
Per-example loss in batch: 0.249351  [   60/   89]
Per-example loss in batch: 0.261250  [   62/   89]
Per-example loss in batch: 0.212577  [   64/   89]
Per-example loss in batch: 0.258779  [   66/   89]
Per-example loss in batch: 0.358139  [   68/   89]
Per-example loss in batch: 0.269668  [   70/   89]
Per-example loss in batch: 0.227685  [   72/   89]
Per-example loss in batch: 0.318227  [   74/   89]
Per-example loss in batch: 0.272471  [   76/   89]
Per-example loss in batch: 0.233883  [   78/   89]
Per-example loss in batch: 0.236376  [   80/   89]
Per-example loss in batch: 0.241504  [   82/   89]
Per-example loss in batch: 0.246107  [   84/   89]
Per-example loss in batch: 0.223059  [   86/   89]
Per-example loss in batch: 0.225240  [   88/   89]
Per-example loss in batch: 0.571251  [   89/   89]
Train Error: Avg loss: 0.25678791
validation Error: 
 Avg loss: 0.27721561 
 F1: 0.486730 
 Precision: 0.556229 
 Recall: 0.432670
 IoU: 0.321642

test Error: 
 Avg loss: 0.25429038 
 F1: 0.521204 
 Precision: 0.599564 
 Recall: 0.460959
 IoU: 0.352452

We have finished training iteration 325
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_323_.pth
Per-example loss in batch: 0.248910  [    2/   89]
Per-example loss in batch: 0.293759  [    4/   89]
Per-example loss in batch: 0.223869  [    6/   89]
Per-example loss in batch: 0.336045  [    8/   89]
Per-example loss in batch: 0.243136  [   10/   89]
Per-example loss in batch: 0.281851  [   12/   89]
Per-example loss in batch: 0.308846  [   14/   89]
Per-example loss in batch: 0.217416  [   16/   89]
Per-example loss in batch: 0.208954  [   18/   89]
Per-example loss in batch: 0.306609  [   20/   89]
Per-example loss in batch: 0.340378  [   22/   89]
Per-example loss in batch: 0.269617  [   24/   89]
Per-example loss in batch: 0.213858  [   26/   89]
Per-example loss in batch: 0.219467  [   28/   89]
Per-example loss in batch: 0.288980  [   30/   89]
Per-example loss in batch: 0.218086  [   32/   89]
Per-example loss in batch: 0.320908  [   34/   89]
Per-example loss in batch: 0.254135  [   36/   89]
Per-example loss in batch: 0.237581  [   38/   89]
Per-example loss in batch: 0.304015  [   40/   89]
Per-example loss in batch: 0.225705  [   42/   89]
Per-example loss in batch: 0.280339  [   44/   89]
Per-example loss in batch: 0.281325  [   46/   89]
Per-example loss in batch: 0.212687  [   48/   89]
Per-example loss in batch: 0.230924  [   50/   89]
Per-example loss in batch: 0.244646  [   52/   89]
Per-example loss in batch: 0.329381  [   54/   89]
Per-example loss in batch: 0.278332  [   56/   89]
Per-example loss in batch: 0.332568  [   58/   89]
Per-example loss in batch: 0.207093  [   60/   89]
Per-example loss in batch: 0.212478  [   62/   89]
Per-example loss in batch: 0.235133  [   64/   89]
Per-example loss in batch: 0.341671  [   66/   89]
Per-example loss in batch: 0.289996  [   68/   89]
Per-example loss in batch: 0.237199  [   70/   89]
Per-example loss in batch: 0.233687  [   72/   89]
Per-example loss in batch: 0.205222  [   74/   89]
Per-example loss in batch: 0.213508  [   76/   89]
Per-example loss in batch: 0.277299  [   78/   89]
Per-example loss in batch: 0.228425  [   80/   89]
Per-example loss in batch: 0.212781  [   82/   89]
Per-example loss in batch: 0.230390  [   84/   89]
Per-example loss in batch: 0.226270  [   86/   89]
Per-example loss in batch: 0.240454  [   88/   89]
Per-example loss in batch: 0.406343  [   89/   89]
Train Error: Avg loss: 0.25948551
validation Error: 
 Avg loss: 0.27074102 
 F1: 0.491726 
 Precision: 0.557803 
 Recall: 0.439645
 IoU: 0.326019

test Error: 
 Avg loss: 0.25208324 
 F1: 0.526627 
 Precision: 0.604753 
 Recall: 0.466377
 IoU: 0.357430

We have finished training iteration 326
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_324_.pth
Per-example loss in batch: 0.239053  [    2/   89]
Per-example loss in batch: 0.236100  [    4/   89]
Per-example loss in batch: 0.213007  [    6/   89]
Per-example loss in batch: 0.255424  [    8/   89]
Per-example loss in batch: 0.182351  [   10/   89]
Per-example loss in batch: 0.320450  [   12/   89]
Per-example loss in batch: 0.275177  [   14/   89]
Per-example loss in batch: 0.288134  [   16/   89]
Per-example loss in batch: 0.259859  [   18/   89]
Per-example loss in batch: 0.250842  [   20/   89]
Per-example loss in batch: 0.251127  [   22/   89]
Per-example loss in batch: 0.287534  [   24/   89]
Per-example loss in batch: 0.262159  [   26/   89]
Per-example loss in batch: 0.248899  [   28/   89]
Per-example loss in batch: 0.221203  [   30/   89]
Per-example loss in batch: 0.272179  [   32/   89]
Per-example loss in batch: 0.313902  [   34/   89]
Per-example loss in batch: 0.204482  [   36/   89]
Per-example loss in batch: 0.277306  [   38/   89]
Per-example loss in batch: 0.222137  [   40/   89]
Per-example loss in batch: 0.224614  [   42/   89]
Per-example loss in batch: 0.210952  [   44/   89]
Per-example loss in batch: 0.304768  [   46/   89]
Per-example loss in batch: 0.314531  [   48/   89]
Per-example loss in batch: 0.304396  [   50/   89]
Per-example loss in batch: 0.285802  [   52/   89]
Per-example loss in batch: 0.218164  [   54/   89]
Per-example loss in batch: 0.266785  [   56/   89]
Per-example loss in batch: 0.305518  [   58/   89]
Per-example loss in batch: 0.315369  [   60/   89]
Per-example loss in batch: 0.220449  [   62/   89]
Per-example loss in batch: 0.265965  [   64/   89]
Per-example loss in batch: 0.248192  [   66/   89]
Per-example loss in batch: 0.213930  [   68/   89]
Per-example loss in batch: 0.223212  [   70/   89]
Per-example loss in batch: 0.266836  [   72/   89]
Per-example loss in batch: 0.212043  [   74/   89]
Per-example loss in batch: 0.275154  [   76/   89]
Per-example loss in batch: 0.230893  [   78/   89]
Per-example loss in batch: 0.329693  [   80/   89]
Per-example loss in batch: 0.219618  [   82/   89]
Per-example loss in batch: 0.240873  [   84/   89]
Per-example loss in batch: 0.206822  [   86/   89]
Per-example loss in batch: 0.299843  [   88/   89]
Per-example loss in batch: 0.425430  [   89/   89]
Train Error: Avg loss: 0.25839241
validation Error: 
 Avg loss: 0.27673986 
 F1: 0.489267 
 Precision: 0.561508 
 Recall: 0.433495
 IoU: 0.323860

test Error: 
 Avg loss: 0.25335452 
 F1: 0.523797 
 Precision: 0.605457 
 Recall: 0.461547
 IoU: 0.354827

We have finished training iteration 327
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_325_.pth
Per-example loss in batch: 0.278732  [    2/   89]
Per-example loss in batch: 0.209711  [    4/   89]
Per-example loss in batch: 0.335841  [    6/   89]
Per-example loss in batch: 0.291260  [    8/   89]
Per-example loss in batch: 0.212020  [   10/   89]
Per-example loss in batch: 0.233063  [   12/   89]
Per-example loss in batch: 0.237157  [   14/   89]
Per-example loss in batch: 0.223059  [   16/   89]
Per-example loss in batch: 0.305185  [   18/   89]
Per-example loss in batch: 0.315978  [   20/   89]
Per-example loss in batch: 0.237030  [   22/   89]
Per-example loss in batch: 0.229876  [   24/   89]
Per-example loss in batch: 0.224575  [   26/   89]
Per-example loss in batch: 0.259283  [   28/   89]
Per-example loss in batch: 0.228981  [   30/   89]
Per-example loss in batch: 0.241018  [   32/   89]
Per-example loss in batch: 0.224949  [   34/   89]
Per-example loss in batch: 0.211973  [   36/   89]
Per-example loss in batch: 0.281647  [   38/   89]
Per-example loss in batch: 0.308212  [   40/   89]
Per-example loss in batch: 0.237906  [   42/   89]
Per-example loss in batch: 0.216002  [   44/   89]
Per-example loss in batch: 0.236022  [   46/   89]
Per-example loss in batch: 0.241910  [   48/   89]
Per-example loss in batch: 0.280282  [   50/   89]
Per-example loss in batch: 0.244147  [   52/   89]
Per-example loss in batch: 0.323308  [   54/   89]
Per-example loss in batch: 0.261131  [   56/   89]
Per-example loss in batch: 0.290762  [   58/   89]
Per-example loss in batch: 0.213973  [   60/   89]
Per-example loss in batch: 0.207943  [   62/   89]
Per-example loss in batch: 0.227671  [   64/   89]
Per-example loss in batch: 0.314689  [   66/   89]
Per-example loss in batch: 0.213611  [   68/   89]
Per-example loss in batch: 0.244394  [   70/   89]
Per-example loss in batch: 0.251989  [   72/   89]
Per-example loss in batch: 0.247923  [   74/   89]
Per-example loss in batch: 0.247400  [   76/   89]
Per-example loss in batch: 0.313008  [   78/   89]
Per-example loss in batch: 0.228144  [   80/   89]
Per-example loss in batch: 0.212726  [   82/   89]
Per-example loss in batch: 0.199665  [   84/   89]
Per-example loss in batch: 0.290800  [   86/   89]
Per-example loss in batch: 0.326301  [   88/   89]
Per-example loss in batch: 0.453350  [   89/   89]
Train Error: Avg loss: 0.25590858
validation Error: 
 Avg loss: 0.27510389 
 F1: 0.488704 
 Precision: 0.561683 
 Recall: 0.432508
 IoU: 0.323367

test Error: 
 Avg loss: 0.25420880 
 F1: 0.521804 
 Precision: 0.601619 
 Recall: 0.460687
 IoU: 0.353001

We have finished training iteration 328
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_326_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.236031  [    2/   89]
Per-example loss in batch: 0.202661  [    4/   89]
Per-example loss in batch: 0.258714  [    6/   89]
Per-example loss in batch: 0.256264  [    8/   89]
Per-example loss in batch: 0.280647  [   10/   89]
Per-example loss in batch: 0.289531  [   12/   89]
Per-example loss in batch: 0.328671  [   14/   89]
Per-example loss in batch: 0.298998  [   16/   89]
Per-example loss in batch: 0.305488  [   18/   89]
Per-example loss in batch: 0.234690  [   20/   89]
Per-example loss in batch: 0.187587  [   22/   89]
Per-example loss in batch: 0.232814  [   24/   89]
Per-example loss in batch: 0.228537  [   26/   89]
Per-example loss in batch: 0.235391  [   28/   89]
Per-example loss in batch: 0.299134  [   30/   89]
Per-example loss in batch: 0.332765  [   32/   89]
Per-example loss in batch: 0.229672  [   34/   89]
Per-example loss in batch: 0.284717  [   36/   89]
Per-example loss in batch: 0.216846  [   38/   89]
Per-example loss in batch: 0.211173  [   40/   89]
Per-example loss in batch: 0.209315  [   42/   89]
Per-example loss in batch: 0.312902  [   44/   89]
Per-example loss in batch: 0.284194  [   46/   89]
Per-example loss in batch: 0.224569  [   48/   89]
Per-example loss in batch: 0.234081  [   50/   89]
Per-example loss in batch: 0.339946  [   52/   89]
Per-example loss in batch: 0.260272  [   54/   89]
Per-example loss in batch: 0.208047  [   56/   89]
Per-example loss in batch: 0.223398  [   58/   89]
Per-example loss in batch: 0.218815  [   60/   89]
Per-example loss in batch: 0.212345  [   62/   89]
Per-example loss in batch: 0.229722  [   64/   89]
Per-example loss in batch: 0.189017  [   66/   89]
Per-example loss in batch: 0.282089  [   68/   89]
Per-example loss in batch: 0.328866  [   70/   89]
Per-example loss in batch: 0.234976  [   72/   89]
Per-example loss in batch: 0.250267  [   74/   89]
Per-example loss in batch: 0.239397  [   76/   89]
Per-example loss in batch: 0.297593  [   78/   89]
Per-example loss in batch: 0.308865  [   80/   89]
Per-example loss in batch: 0.287240  [   82/   89]
Per-example loss in batch: 0.295762  [   84/   89]
Per-example loss in batch: 0.246215  [   86/   89]
Per-example loss in batch: 0.228118  [   88/   89]
Per-example loss in batch: 0.461026  [   89/   89]
Train Error: Avg loss: 0.25903036
validation Error: 
 Avg loss: 0.27365757 
 F1: 0.489787 
 Precision: 0.558486 
 Recall: 0.436138
 IoU: 0.324317

test Error: 
 Avg loss: 0.25342171 
 F1: 0.524358 
 Precision: 0.600683 
 Recall: 0.465243
 IoU: 0.355343

We have finished training iteration 329
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_327_.pth
Per-example loss in batch: 0.226251  [    2/   89]
Per-example loss in batch: 0.223024  [    4/   89]
Per-example loss in batch: 0.221308  [    6/   89]
Per-example loss in batch: 0.321201  [    8/   89]
Per-example loss in batch: 0.247412  [   10/   89]
Per-example loss in batch: 0.246359  [   12/   89]
Per-example loss in batch: 0.238953  [   14/   89]
Per-example loss in batch: 0.226694  [   16/   89]
Per-example loss in batch: 0.262288  [   18/   89]
Per-example loss in batch: 0.203863  [   20/   89]
Per-example loss in batch: 0.269198  [   22/   89]
Per-example loss in batch: 0.261114  [   24/   89]
Per-example loss in batch: 0.244992  [   26/   89]
Per-example loss in batch: 0.212202  [   28/   89]
Per-example loss in batch: 0.205675  [   30/   89]
Per-example loss in batch: 0.196015  [   32/   89]
Per-example loss in batch: 0.273870  [   34/   89]
Per-example loss in batch: 0.300061  [   36/   89]
Per-example loss in batch: 0.213988  [   38/   89]
Per-example loss in batch: 0.298707  [   40/   89]
Per-example loss in batch: 0.216727  [   42/   89]
Per-example loss in batch: 0.242583  [   44/   89]
Per-example loss in batch: 0.219475  [   46/   89]
Per-example loss in batch: 0.338202  [   48/   89]
Per-example loss in batch: 0.217817  [   50/   89]
Per-example loss in batch: 0.280483  [   52/   89]
Per-example loss in batch: 0.244052  [   54/   89]
Per-example loss in batch: 0.281466  [   56/   89]
Per-example loss in batch: 0.229233  [   58/   89]
Per-example loss in batch: 0.276026  [   60/   89]
Per-example loss in batch: 0.212635  [   62/   89]
Per-example loss in batch: 0.255130  [   64/   89]
Per-example loss in batch: 0.270572  [   66/   89]
Per-example loss in batch: 0.316691  [   68/   89]
Per-example loss in batch: 0.305772  [   70/   89]
Per-example loss in batch: 0.223918  [   72/   89]
Per-example loss in batch: 0.245598  [   74/   89]
Per-example loss in batch: 0.223510  [   76/   89]
Per-example loss in batch: 0.178167  [   78/   89]
Per-example loss in batch: 0.289425  [   80/   89]
Per-example loss in batch: 0.320622  [   82/   89]
Per-example loss in batch: 0.257451  [   84/   89]
Per-example loss in batch: 0.318327  [   86/   89]
Per-example loss in batch: 0.238133  [   88/   89]
Per-example loss in batch: 0.660164  [   89/   89]
Train Error: Avg loss: 0.25674769
validation Error: 
 Avg loss: 0.27746558 
 F1: 0.490895 
 Precision: 0.562856 
 Recall: 0.435249
 IoU: 0.325289

test Error: 
 Avg loss: 0.25331847 
 F1: 0.523375 
 Precision: 0.604006 
 Recall: 0.461736
 IoU: 0.354440

We have finished training iteration 330
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_328_.pth
Per-example loss in batch: 0.272206  [    2/   89]
Per-example loss in batch: 0.291862  [    4/   89]
Per-example loss in batch: 0.300974  [    6/   89]
Per-example loss in batch: 0.229314  [    8/   89]
Per-example loss in batch: 0.300235  [   10/   89]
Per-example loss in batch: 0.187333  [   12/   89]
Per-example loss in batch: 0.248576  [   14/   89]
Per-example loss in batch: 0.360405  [   16/   89]
Per-example loss in batch: 0.215810  [   18/   89]
Per-example loss in batch: 0.285217  [   20/   89]
Per-example loss in batch: 0.255191  [   22/   89]
Per-example loss in batch: 0.224402  [   24/   89]
Per-example loss in batch: 0.244530  [   26/   89]
Per-example loss in batch: 0.244729  [   28/   89]
Per-example loss in batch: 0.202092  [   30/   89]
Per-example loss in batch: 0.274639  [   32/   89]
Per-example loss in batch: 0.262851  [   34/   89]
Per-example loss in batch: 0.266535  [   36/   89]
Per-example loss in batch: 0.225164  [   38/   89]
Per-example loss in batch: 0.243197  [   40/   89]
Per-example loss in batch: 0.189068  [   42/   89]
Per-example loss in batch: 0.205247  [   44/   89]
Per-example loss in batch: 0.187127  [   46/   89]
Per-example loss in batch: 0.243758  [   48/   89]
Per-example loss in batch: 0.225372  [   50/   89]
Per-example loss in batch: 0.264338  [   52/   89]
Per-example loss in batch: 0.195169  [   54/   89]
Per-example loss in batch: 0.216851  [   56/   89]
Per-example loss in batch: 0.307364  [   58/   89]
Per-example loss in batch: 0.237851  [   60/   89]
Per-example loss in batch: 0.218465  [   62/   89]
Per-example loss in batch: 0.271358  [   64/   89]
Per-example loss in batch: 0.256336  [   66/   89]
Per-example loss in batch: 0.268977  [   68/   89]
Per-example loss in batch: 0.234983  [   70/   89]
Per-example loss in batch: 0.314396  [   72/   89]
Per-example loss in batch: 0.220964  [   74/   89]
Per-example loss in batch: 0.291803  [   76/   89]
Per-example loss in batch: 0.277801  [   78/   89]
Per-example loss in batch: 0.257364  [   80/   89]
Per-example loss in batch: 0.254737  [   82/   89]
Per-example loss in batch: 0.308745  [   84/   89]
Per-example loss in batch: 0.289405  [   86/   89]
Per-example loss in batch: 0.313221  [   88/   89]
Per-example loss in batch: 0.590819  [   89/   89]
Train Error: Avg loss: 0.25800835
validation Error: 
 Avg loss: 0.28111925 
 F1: 0.491005 
 Precision: 0.567892 
 Recall: 0.432455
 IoU: 0.325386

test Error: 
 Avg loss: 0.25309105 
 F1: 0.524125 
 Precision: 0.611906 
 Recall: 0.458369
 IoU: 0.355128

We have finished training iteration 331
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_329_.pth
Per-example loss in batch: 0.334706  [    2/   89]
Per-example loss in batch: 0.265958  [    4/   89]
Per-example loss in batch: 0.306728  [    6/   89]
Per-example loss in batch: 0.214484  [    8/   89]
Per-example loss in batch: 0.261637  [   10/   89]
Per-example loss in batch: 0.209768  [   12/   89]
Per-example loss in batch: 0.250662  [   14/   89]
Per-example loss in batch: 0.267052  [   16/   89]
Per-example loss in batch: 0.273505  [   18/   89]
Per-example loss in batch: 0.354191  [   20/   89]
Per-example loss in batch: 0.207214  [   22/   89]
Per-example loss in batch: 0.373844  [   24/   89]
Per-example loss in batch: 0.253659  [   26/   89]
Per-example loss in batch: 0.258270  [   28/   89]
Per-example loss in batch: 0.287003  [   30/   89]
Per-example loss in batch: 0.222520  [   32/   89]
Per-example loss in batch: 0.212045  [   34/   89]
Per-example loss in batch: 0.238546  [   36/   89]
Per-example loss in batch: 0.316165  [   38/   89]
Per-example loss in batch: 0.204681  [   40/   89]
Per-example loss in batch: 0.244660  [   42/   89]
Per-example loss in batch: 0.225466  [   44/   89]
Per-example loss in batch: 0.190497  [   46/   89]
Per-example loss in batch: 0.208016  [   48/   89]
Per-example loss in batch: 0.213405  [   50/   89]
Per-example loss in batch: 0.245256  [   52/   89]
Per-example loss in batch: 0.284050  [   54/   89]
Per-example loss in batch: 0.199170  [   56/   89]
Per-example loss in batch: 0.222139  [   58/   89]
Per-example loss in batch: 0.272464  [   60/   89]
Per-example loss in batch: 0.297001  [   62/   89]
Per-example loss in batch: 0.196428  [   64/   89]
Per-example loss in batch: 0.265694  [   66/   89]
Per-example loss in batch: 0.337557  [   68/   89]
Per-example loss in batch: 0.254982  [   70/   89]
Per-example loss in batch: 0.240042  [   72/   89]
Per-example loss in batch: 0.255445  [   74/   89]
Per-example loss in batch: 0.225976  [   76/   89]
Per-example loss in batch: 0.334930  [   78/   89]
Per-example loss in batch: 0.274905  [   80/   89]
Per-example loss in batch: 0.221516  [   82/   89]
Per-example loss in batch: 0.207107  [   84/   89]
Per-example loss in batch: 0.231683  [   86/   89]
Per-example loss in batch: 0.240905  [   88/   89]
Per-example loss in batch: 0.558500  [   89/   89]
Train Error: Avg loss: 0.25800411
validation Error: 
 Avg loss: 0.27355673 
 F1: 0.483977 
 Precision: 0.527756 
 Recall: 0.446904
 IoU: 0.319241

test Error: 
 Avg loss: 0.25436351 
 F1: 0.520275 
 Precision: 0.569129 
 Recall: 0.479144
 IoU: 0.351602

We have finished training iteration 332
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_330_.pth
Per-example loss in batch: 0.248738  [    2/   89]
Per-example loss in batch: 0.227217  [    4/   89]
Per-example loss in batch: 0.219640  [    6/   89]
Per-example loss in batch: 0.285269  [    8/   89]
Per-example loss in batch: 0.210277  [   10/   89]
Per-example loss in batch: 0.364354  [   12/   89]
Per-example loss in batch: 0.289853  [   14/   89]
Per-example loss in batch: 0.235253  [   16/   89]
Per-example loss in batch: 0.190723  [   18/   89]
Per-example loss in batch: 0.202966  [   20/   89]
Per-example loss in batch: 0.249316  [   22/   89]
Per-example loss in batch: 0.239546  [   24/   89]
Per-example loss in batch: 0.320509  [   26/   89]
Per-example loss in batch: 0.265707  [   28/   89]
Per-example loss in batch: 0.346680  [   30/   89]
Per-example loss in batch: 0.305781  [   32/   89]
Per-example loss in batch: 0.231195  [   34/   89]
Per-example loss in batch: 0.238409  [   36/   89]
Per-example loss in batch: 0.256724  [   38/   89]
Per-example loss in batch: 0.222403  [   40/   89]
Per-example loss in batch: 0.258457  [   42/   89]
Per-example loss in batch: 0.220763  [   44/   89]
Per-example loss in batch: 0.244811  [   46/   89]
Per-example loss in batch: 0.307900  [   48/   89]
Per-example loss in batch: 0.206254  [   50/   89]
Per-example loss in batch: 0.253324  [   52/   89]
Per-example loss in batch: 0.217514  [   54/   89]
Per-example loss in batch: 0.229461  [   56/   89]
Per-example loss in batch: 0.233797  [   58/   89]
Per-example loss in batch: 0.268549  [   60/   89]
Per-example loss in batch: 0.316727  [   62/   89]
Per-example loss in batch: 0.256524  [   64/   89]
Per-example loss in batch: 0.228496  [   66/   89]
Per-example loss in batch: 0.266599  [   68/   89]
Per-example loss in batch: 0.236102  [   70/   89]
Per-example loss in batch: 0.286212  [   72/   89]
Per-example loss in batch: 0.211287  [   74/   89]
Per-example loss in batch: 0.268768  [   76/   89]
Per-example loss in batch: 0.291008  [   78/   89]
Per-example loss in batch: 0.279365  [   80/   89]
Per-example loss in batch: 0.275838  [   82/   89]
Per-example loss in batch: 0.255223  [   84/   89]
Per-example loss in batch: 0.210358  [   86/   89]
Per-example loss in batch: 0.219578  [   88/   89]
Per-example loss in batch: 0.342825  [   89/   89]
Train Error: Avg loss: 0.25539078
validation Error: 
 Avg loss: 0.27183263 
 F1: 0.490925 
 Precision: 0.545858 
 Recall: 0.446037
 IoU: 0.325315

test Error: 
 Avg loss: 0.25135164 
 F1: 0.527775 
 Precision: 0.591217 
 Recall: 0.476629
 IoU: 0.358488

We have finished training iteration 333
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_331_.pth
Per-example loss in batch: 0.232082  [    2/   89]
Per-example loss in batch: 0.214186  [    4/   89]
Per-example loss in batch: 0.185358  [    6/   89]
Per-example loss in batch: 0.226740  [    8/   89]
Per-example loss in batch: 0.192041  [   10/   89]
Per-example loss in batch: 0.315632  [   12/   89]
Per-example loss in batch: 0.321684  [   14/   89]
Per-example loss in batch: 0.246834  [   16/   89]
Per-example loss in batch: 0.235674  [   18/   89]
Per-example loss in batch: 0.277021  [   20/   89]
Per-example loss in batch: 0.182984  [   22/   89]
Per-example loss in batch: 0.318558  [   24/   89]
Per-example loss in batch: 0.296011  [   26/   89]
Per-example loss in batch: 0.221285  [   28/   89]
Per-example loss in batch: 0.237489  [   30/   89]
Per-example loss in batch: 0.238828  [   32/   89]
Per-example loss in batch: 0.213803  [   34/   89]
Per-example loss in batch: 0.255940  [   36/   89]
Per-example loss in batch: 0.343175  [   38/   89]
Per-example loss in batch: 0.218718  [   40/   89]
Per-example loss in batch: 0.300203  [   42/   89]
Per-example loss in batch: 0.224926  [   44/   89]
Per-example loss in batch: 0.247159  [   46/   89]
Per-example loss in batch: 0.230182  [   48/   89]
Per-example loss in batch: 0.249483  [   50/   89]
Per-example loss in batch: 0.203860  [   52/   89]
Per-example loss in batch: 0.281230  [   54/   89]
Per-example loss in batch: 0.253020  [   56/   89]
Per-example loss in batch: 0.208191  [   58/   89]
Per-example loss in batch: 0.257623  [   60/   89]
Per-example loss in batch: 0.232130  [   62/   89]
Per-example loss in batch: 0.317674  [   64/   89]
Per-example loss in batch: 0.257886  [   66/   89]
Per-example loss in batch: 0.301742  [   68/   89]
Per-example loss in batch: 0.202776  [   70/   89]
Per-example loss in batch: 0.227612  [   72/   89]
Per-example loss in batch: 0.243387  [   74/   89]
Per-example loss in batch: 0.259775  [   76/   89]
Per-example loss in batch: 0.289992  [   78/   89]
Per-example loss in batch: 0.275028  [   80/   89]
Per-example loss in batch: 0.310608  [   82/   89]
Per-example loss in batch: 0.218684  [   84/   89]
Per-example loss in batch: 0.256639  [   86/   89]
Per-example loss in batch: 0.264995  [   88/   89]
Per-example loss in batch: 0.554351  [   89/   89]
Train Error: Avg loss: 0.25541630
validation Error: 
 Avg loss: 0.27280036 
 F1: 0.491735 
 Precision: 0.565784 
 Recall: 0.434826
 IoU: 0.326027

test Error: 
 Avg loss: 0.25207595 
 F1: 0.525961 
 Precision: 0.607171 
 Recall: 0.463912
 IoU: 0.356816

We have finished training iteration 334
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_332_.pth
Per-example loss in batch: 0.238055  [    2/   89]
Per-example loss in batch: 0.243242  [    4/   89]
Per-example loss in batch: 0.264943  [    6/   89]
Per-example loss in batch: 0.235099  [    8/   89]
Per-example loss in batch: 0.277708  [   10/   89]
Per-example loss in batch: 0.234068  [   12/   89]
Per-example loss in batch: 0.289039  [   14/   89]
Per-example loss in batch: 0.298458  [   16/   89]
Per-example loss in batch: 0.259881  [   18/   89]
Per-example loss in batch: 0.201279  [   20/   89]
Per-example loss in batch: 0.303146  [   22/   89]
Per-example loss in batch: 0.217734  [   24/   89]
Per-example loss in batch: 0.244023  [   26/   89]
Per-example loss in batch: 0.241190  [   28/   89]
Per-example loss in batch: 0.316672  [   30/   89]
Per-example loss in batch: 0.183891  [   32/   89]
Per-example loss in batch: 0.197916  [   34/   89]
Per-example loss in batch: 0.200341  [   36/   89]
Per-example loss in batch: 0.323829  [   38/   89]
Per-example loss in batch: 0.219755  [   40/   89]
Per-example loss in batch: 0.217527  [   42/   89]
Per-example loss in batch: 0.259414  [   44/   89]
Per-example loss in batch: 0.261206  [   46/   89]
Per-example loss in batch: 0.252384  [   48/   89]
Per-example loss in batch: 0.243426  [   50/   89]
Per-example loss in batch: 0.204946  [   52/   89]
Per-example loss in batch: 0.260728  [   54/   89]
Per-example loss in batch: 0.345962  [   56/   89]
Per-example loss in batch: 0.189481  [   58/   89]
Per-example loss in batch: 0.242531  [   60/   89]
Per-example loss in batch: 0.218505  [   62/   89]
Per-example loss in batch: 0.315506  [   64/   89]
Per-example loss in batch: 0.249023  [   66/   89]
Per-example loss in batch: 0.343833  [   68/   89]
Per-example loss in batch: 0.247313  [   70/   89]
Per-example loss in batch: 0.297212  [   72/   89]
Per-example loss in batch: 0.238109  [   74/   89]
Per-example loss in batch: 0.291135  [   76/   89]
Per-example loss in batch: 0.249524  [   78/   89]
Per-example loss in batch: 0.215025  [   80/   89]
Per-example loss in batch: 0.319275  [   82/   89]
Per-example loss in batch: 0.311903  [   84/   89]
Per-example loss in batch: 0.268456  [   86/   89]
Per-example loss in batch: 0.248629  [   88/   89]
Per-example loss in batch: 0.507353  [   89/   89]
Train Error: Avg loss: 0.25921348
validation Error: 
 Avg loss: 0.27845746 
 F1: 0.489420 
 Precision: 0.542725 
 Recall: 0.445649
 IoU: 0.323994

test Error: 
 Avg loss: 0.25365446 
 F1: 0.522082 
 Precision: 0.579338 
 Recall: 0.475125
 IoU: 0.353255

We have finished training iteration 335
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_333_.pth
Per-example loss in batch: 0.267496  [    2/   89]
Per-example loss in batch: 0.231726  [    4/   89]
Per-example loss in batch: 0.195749  [    6/   89]
Per-example loss in batch: 0.265639  [    8/   89]
Per-example loss in batch: 0.217754  [   10/   89]
Per-example loss in batch: 0.182473  [   12/   89]
Per-example loss in batch: 0.256881  [   14/   89]
Per-example loss in batch: 0.323329  [   16/   89]
Per-example loss in batch: 0.254751  [   18/   89]
Per-example loss in batch: 0.203665  [   20/   89]
Per-example loss in batch: 0.349431  [   22/   89]
Per-example loss in batch: 0.226884  [   24/   89]
Per-example loss in batch: 0.231554  [   26/   89]
Per-example loss in batch: 0.303030  [   28/   89]
Per-example loss in batch: 0.338696  [   30/   89]
Per-example loss in batch: 0.321297  [   32/   89]
Per-example loss in batch: 0.197457  [   34/   89]
Per-example loss in batch: 0.243643  [   36/   89]
Per-example loss in batch: 0.210809  [   38/   89]
Per-example loss in batch: 0.243568  [   40/   89]
Per-example loss in batch: 0.310170  [   42/   89]
Per-example loss in batch: 0.281509  [   44/   89]
Per-example loss in batch: 0.243844  [   46/   89]
Per-example loss in batch: 0.248556  [   48/   89]
Per-example loss in batch: 0.227047  [   50/   89]
Per-example loss in batch: 0.247075  [   52/   89]
Per-example loss in batch: 0.202867  [   54/   89]
Per-example loss in batch: 0.215739  [   56/   89]
Per-example loss in batch: 0.284004  [   58/   89]
Per-example loss in batch: 0.233781  [   60/   89]
Per-example loss in batch: 0.248201  [   62/   89]
Per-example loss in batch: 0.242240  [   64/   89]
Per-example loss in batch: 0.197810  [   66/   89]
Per-example loss in batch: 0.204763  [   68/   89]
Per-example loss in batch: 0.359777  [   70/   89]
Per-example loss in batch: 0.281026  [   72/   89]
Per-example loss in batch: 0.254031  [   74/   89]
Per-example loss in batch: 0.302069  [   76/   89]
Per-example loss in batch: 0.277703  [   78/   89]
Per-example loss in batch: 0.208761  [   80/   89]
Per-example loss in batch: 0.179653  [   82/   89]
Per-example loss in batch: 0.200111  [   84/   89]
Per-example loss in batch: 0.223810  [   86/   89]
Per-example loss in batch: 0.348998  [   88/   89]
Per-example loss in batch: 0.495735  [   89/   89]
Train Error: Avg loss: 0.25476960
validation Error: 
 Avg loss: 0.27723288 
 F1: 0.490096 
 Precision: 0.543087 
 Recall: 0.446526
 IoU: 0.324587

test Error: 
 Avg loss: 0.25182641 
 F1: 0.526370 
 Precision: 0.583772 
 Recall: 0.479247
 IoU: 0.357193

We have finished training iteration 336
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_334_.pth
Per-example loss in batch: 0.211265  [    2/   89]
Per-example loss in batch: 0.347465  [    4/   89]
Per-example loss in batch: 0.240466  [    6/   89]
Per-example loss in batch: 0.242378  [    8/   89]
Per-example loss in batch: 0.305016  [   10/   89]
Per-example loss in batch: 0.248098  [   12/   89]
Per-example loss in batch: 0.231153  [   14/   89]
Per-example loss in batch: 0.254187  [   16/   89]
Per-example loss in batch: 0.245520  [   18/   89]
Per-example loss in batch: 0.315334  [   20/   89]
Per-example loss in batch: 0.181668  [   22/   89]
Per-example loss in batch: 0.272810  [   24/   89]
Per-example loss in batch: 0.290099  [   26/   89]
Per-example loss in batch: 0.231087  [   28/   89]
Per-example loss in batch: 0.330454  [   30/   89]
Per-example loss in batch: 0.272864  [   32/   89]
Per-example loss in batch: 0.233543  [   34/   89]
Per-example loss in batch: 0.305734  [   36/   89]
Per-example loss in batch: 0.234613  [   38/   89]
Per-example loss in batch: 0.231616  [   40/   89]
Per-example loss in batch: 0.204157  [   42/   89]
Per-example loss in batch: 0.215124  [   44/   89]
Per-example loss in batch: 0.267640  [   46/   89]
Per-example loss in batch: 0.216955  [   48/   89]
Per-example loss in batch: 0.236062  [   50/   89]
Per-example loss in batch: 0.287171  [   52/   89]
Per-example loss in batch: 0.302253  [   54/   89]
Per-example loss in batch: 0.277153  [   56/   89]
Per-example loss in batch: 0.204218  [   58/   89]
Per-example loss in batch: 0.239285  [   60/   89]
Per-example loss in batch: 0.260045  [   62/   89]
Per-example loss in batch: 0.190307  [   64/   89]
Per-example loss in batch: 0.268077  [   66/   89]
Per-example loss in batch: 0.331687  [   68/   89]
Per-example loss in batch: 0.210785  [   70/   89]
Per-example loss in batch: 0.247065  [   72/   89]
Per-example loss in batch: 0.296811  [   74/   89]
Per-example loss in batch: 0.208103  [   76/   89]
Per-example loss in batch: 0.229912  [   78/   89]
Per-example loss in batch: 0.271263  [   80/   89]
Per-example loss in batch: 0.203349  [   82/   89]
Per-example loss in batch: 0.228903  [   84/   89]
Per-example loss in batch: 0.224667  [   86/   89]
Per-example loss in batch: 0.290145  [   88/   89]
Per-example loss in batch: 0.460361  [   89/   89]
Train Error: Avg loss: 0.25543119
validation Error: 
 Avg loss: 0.27550754 
 F1: 0.491519 
 Precision: 0.550588 
 Recall: 0.443897
 IoU: 0.325837

test Error: 
 Avg loss: 0.25214366 
 F1: 0.526268 
 Precision: 0.594413 
 Recall: 0.472140
 IoU: 0.357099

We have finished training iteration 337
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_335_.pth
Per-example loss in batch: 0.228337  [    2/   89]
Per-example loss in batch: 0.233965  [    4/   89]
Per-example loss in batch: 0.223552  [    6/   89]
Per-example loss in batch: 0.325994  [    8/   89]
Per-example loss in batch: 0.225096  [   10/   89]
Per-example loss in batch: 0.192158  [   12/   89]
Per-example loss in batch: 0.297239  [   14/   89]
Per-example loss in batch: 0.231259  [   16/   89]
Per-example loss in batch: 0.282207  [   18/   89]
Per-example loss in batch: 0.230128  [   20/   89]
Per-example loss in batch: 0.248991  [   22/   89]
Per-example loss in batch: 0.238220  [   24/   89]
Per-example loss in batch: 0.242794  [   26/   89]
Per-example loss in batch: 0.222495  [   28/   89]
Per-example loss in batch: 0.342864  [   30/   89]
Per-example loss in batch: 0.230397  [   32/   89]
Per-example loss in batch: 0.283686  [   34/   89]
Per-example loss in batch: 0.328398  [   36/   89]
Per-example loss in batch: 0.321433  [   38/   89]
Per-example loss in batch: 0.266159  [   40/   89]
Per-example loss in batch: 0.246857  [   42/   89]
Per-example loss in batch: 0.260838  [   44/   89]
Per-example loss in batch: 0.209233  [   46/   89]
Per-example loss in batch: 0.198086  [   48/   89]
Per-example loss in batch: 0.232776  [   50/   89]
Per-example loss in batch: 0.328078  [   52/   89]
Per-example loss in batch: 0.245015  [   54/   89]
Per-example loss in batch: 0.256984  [   56/   89]
Per-example loss in batch: 0.264565  [   58/   89]
Per-example loss in batch: 0.190858  [   60/   89]
Per-example loss in batch: 0.202546  [   62/   89]
Per-example loss in batch: 0.252054  [   64/   89]
Per-example loss in batch: 0.203130  [   66/   89]
Per-example loss in batch: 0.242739  [   68/   89]
Per-example loss in batch: 0.323467  [   70/   89]
Per-example loss in batch: 0.307342  [   72/   89]
Per-example loss in batch: 0.308518  [   74/   89]
Per-example loss in batch: 0.289197  [   76/   89]
Per-example loss in batch: 0.302383  [   78/   89]
Per-example loss in batch: 0.284559  [   80/   89]
Per-example loss in batch: 0.216645  [   82/   89]
Per-example loss in batch: 0.230393  [   84/   89]
Per-example loss in batch: 0.277686  [   86/   89]
Per-example loss in batch: 0.196710  [   88/   89]
Per-example loss in batch: 0.396437  [   89/   89]
Train Error: Avg loss: 0.25762360
validation Error: 
 Avg loss: 0.27594620 
 F1: 0.491360 
 Precision: 0.552837 
 Recall: 0.442188
 IoU: 0.325697

test Error: 
 Avg loss: 0.25234518 
 F1: 0.525475 
 Precision: 0.595484 
 Recall: 0.470195
 IoU: 0.356369

We have finished training iteration 338
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_336_.pth
Per-example loss in batch: 0.301796  [    2/   89]
Per-example loss in batch: 0.278496  [    4/   89]
Per-example loss in batch: 0.222840  [    6/   89]
Per-example loss in batch: 0.197725  [    8/   89]
Per-example loss in batch: 0.309136  [   10/   89]
Per-example loss in batch: 0.213394  [   12/   89]
Per-example loss in batch: 0.234538  [   14/   89]
Per-example loss in batch: 0.314922  [   16/   89]
Per-example loss in batch: 0.285476  [   18/   89]
Per-example loss in batch: 0.215928  [   20/   89]
Per-example loss in batch: 0.311571  [   22/   89]
Per-example loss in batch: 0.252569  [   24/   89]
Per-example loss in batch: 0.187895  [   26/   89]
Per-example loss in batch: 0.241716  [   28/   89]
Per-example loss in batch: 0.275781  [   30/   89]
Per-example loss in batch: 0.242968  [   32/   89]
Per-example loss in batch: 0.247540  [   34/   89]
Per-example loss in batch: 0.200184  [   36/   89]
Per-example loss in batch: 0.363462  [   38/   89]
Per-example loss in batch: 0.225563  [   40/   89]
Per-example loss in batch: 0.276938  [   42/   89]
Per-example loss in batch: 0.265979  [   44/   89]
Per-example loss in batch: 0.276547  [   46/   89]
Per-example loss in batch: 0.283198  [   48/   89]
Per-example loss in batch: 0.227675  [   50/   89]
Per-example loss in batch: 0.317011  [   52/   89]
Per-example loss in batch: 0.232146  [   54/   89]
Per-example loss in batch: 0.220949  [   56/   89]
Per-example loss in batch: 0.231274  [   58/   89]
Per-example loss in batch: 0.341956  [   60/   89]
Per-example loss in batch: 0.229847  [   62/   89]
Per-example loss in batch: 0.285987  [   64/   89]
Per-example loss in batch: 0.238643  [   66/   89]
Per-example loss in batch: 0.265840  [   68/   89]
Per-example loss in batch: 0.210125  [   70/   89]
Per-example loss in batch: 0.240449  [   72/   89]
Per-example loss in batch: 0.210552  [   74/   89]
Per-example loss in batch: 0.191326  [   76/   89]
Per-example loss in batch: 0.224048  [   78/   89]
Per-example loss in batch: 0.261031  [   80/   89]
Per-example loss in batch: 0.198965  [   82/   89]
Per-example loss in batch: 0.333523  [   84/   89]
Per-example loss in batch: 0.227934  [   86/   89]
Per-example loss in batch: 0.220103  [   88/   89]
Per-example loss in batch: 0.525243  [   89/   89]
Train Error: Avg loss: 0.25613859
validation Error: 
 Avg loss: 0.27010283 
 F1: 0.492326 
 Precision: 0.573392 
 Recall: 0.431343
 IoU: 0.326547

test Error: 
 Avg loss: 0.25313842 
 F1: 0.523668 
 Precision: 0.610730 
 Recall: 0.458332
 IoU: 0.354709

We have finished training iteration 339
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_337_.pth
Per-example loss in batch: 0.202967  [    2/   89]
Per-example loss in batch: 0.244315  [    4/   89]
Per-example loss in batch: 0.233722  [    6/   89]
Per-example loss in batch: 0.210170  [    8/   89]
Per-example loss in batch: 0.235401  [   10/   89]
Per-example loss in batch: 0.264226  [   12/   89]
Per-example loss in batch: 0.246827  [   14/   89]
Per-example loss in batch: 0.283201  [   16/   89]
Per-example loss in batch: 0.249029  [   18/   89]
Per-example loss in batch: 0.290666  [   20/   89]
Per-example loss in batch: 0.231118  [   22/   89]
Per-example loss in batch: 0.188684  [   24/   89]
Per-example loss in batch: 0.223201  [   26/   89]
Per-example loss in batch: 0.248151  [   28/   89]
Per-example loss in batch: 0.260886  [   30/   89]
Per-example loss in batch: 0.270022  [   32/   89]
Per-example loss in batch: 0.222918  [   34/   89]
Per-example loss in batch: 0.218084  [   36/   89]
Per-example loss in batch: 0.320816  [   38/   89]
Per-example loss in batch: 0.346805  [   40/   89]
Per-example loss in batch: 0.226349  [   42/   89]
Per-example loss in batch: 0.336083  [   44/   89]
Per-example loss in batch: 0.311175  [   46/   89]
Per-example loss in batch: 0.283822  [   48/   89]
Per-example loss in batch: 0.255885  [   50/   89]
Per-example loss in batch: 0.218365  [   52/   89]
Per-example loss in batch: 0.293040  [   54/   89]
Per-example loss in batch: 0.240183  [   56/   89]
Per-example loss in batch: 0.318252  [   58/   89]
Per-example loss in batch: 0.281202  [   60/   89]
Per-example loss in batch: 0.209981  [   62/   89]
Per-example loss in batch: 0.205301  [   64/   89]
Per-example loss in batch: 0.222681  [   66/   89]
Per-example loss in batch: 0.220614  [   68/   89]
Per-example loss in batch: 0.207699  [   70/   89]
Per-example loss in batch: 0.236756  [   72/   89]
Per-example loss in batch: 0.336636  [   74/   89]
Per-example loss in batch: 0.303360  [   76/   89]
Per-example loss in batch: 0.206550  [   78/   89]
Per-example loss in batch: 0.352379  [   80/   89]
Per-example loss in batch: 0.241871  [   82/   89]
Per-example loss in batch: 0.238372  [   84/   89]
Per-example loss in batch: 0.240589  [   86/   89]
Per-example loss in batch: 0.262491  [   88/   89]
Per-example loss in batch: 0.338513  [   89/   89]
Train Error: Avg loss: 0.25640670
validation Error: 
 Avg loss: 0.27363799 
 F1: 0.492008 
 Precision: 0.566698 
 Recall: 0.434713
 IoU: 0.326267

test Error: 
 Avg loss: 0.25262942 
 F1: 0.525369 
 Precision: 0.609475 
 Recall: 0.461662
 IoU: 0.356272

We have finished training iteration 340
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_338_.pth
Per-example loss in batch: 0.223142  [    2/   89]
Per-example loss in batch: 0.219309  [    4/   89]
Per-example loss in batch: 0.223711  [    6/   89]
Per-example loss in batch: 0.187788  [    8/   89]
Per-example loss in batch: 0.288338  [   10/   89]
Per-example loss in batch: 0.240312  [   12/   89]
Per-example loss in batch: 0.255501  [   14/   89]
Per-example loss in batch: 0.222527  [   16/   89]
Per-example loss in batch: 0.281662  [   18/   89]
Per-example loss in batch: 0.248943  [   20/   89]
Per-example loss in batch: 0.277526  [   22/   89]
Per-example loss in batch: 0.264072  [   24/   89]
Per-example loss in batch: 0.212915  [   26/   89]
Per-example loss in batch: 0.231601  [   28/   89]
Per-example loss in batch: 0.255099  [   30/   89]
Per-example loss in batch: 0.291148  [   32/   89]
Per-example loss in batch: 0.226347  [   34/   89]
Per-example loss in batch: 0.285844  [   36/   89]
Per-example loss in batch: 0.206561  [   38/   89]
Per-example loss in batch: 0.264830  [   40/   89]
Per-example loss in batch: 0.236908  [   42/   89]
Per-example loss in batch: 0.208256  [   44/   89]
Per-example loss in batch: 0.245388  [   46/   89]
Per-example loss in batch: 0.231326  [   48/   89]
Per-example loss in batch: 0.299989  [   50/   89]
Per-example loss in batch: 0.277495  [   52/   89]
Per-example loss in batch: 0.211984  [   54/   89]
Per-example loss in batch: 0.323093  [   56/   89]
Per-example loss in batch: 0.285815  [   58/   89]
Per-example loss in batch: 0.190673  [   60/   89]
Per-example loss in batch: 0.285970  [   62/   89]
Per-example loss in batch: 0.226912  [   64/   89]
Per-example loss in batch: 0.350801  [   66/   89]
Per-example loss in batch: 0.244202  [   68/   89]
Per-example loss in batch: 0.272297  [   70/   89]
Per-example loss in batch: 0.205714  [   72/   89]
Per-example loss in batch: 0.245917  [   74/   89]
Per-example loss in batch: 0.305938  [   76/   89]
Per-example loss in batch: 0.241214  [   78/   89]
Per-example loss in batch: 0.213058  [   80/   89]
Per-example loss in batch: 0.243173  [   82/   89]
Per-example loss in batch: 0.306331  [   84/   89]
Per-example loss in batch: 0.257453  [   86/   89]
Per-example loss in batch: 0.290152  [   88/   89]
Per-example loss in batch: 0.467380  [   89/   89]
Train Error: Avg loss: 0.25485226
validation Error: 
 Avg loss: 0.27351828 
 F1: 0.490098 
 Precision: 0.574521 
 Recall: 0.427308
 IoU: 0.324590

test Error: 
 Avg loss: 0.25298790 
 F1: 0.524627 
 Precision: 0.617931 
 Recall: 0.455803
 IoU: 0.355589

We have finished training iteration 341
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_339_.pth
Per-example loss in batch: 0.300914  [    2/   89]
Per-example loss in batch: 0.228053  [    4/   89]
Per-example loss in batch: 0.282559  [    6/   89]
Per-example loss in batch: 0.329437  [    8/   89]
Per-example loss in batch: 0.266290  [   10/   89]
Per-example loss in batch: 0.219557  [   12/   89]
Per-example loss in batch: 0.219309  [   14/   89]
Per-example loss in batch: 0.268814  [   16/   89]
Per-example loss in batch: 0.254823  [   18/   89]
Per-example loss in batch: 0.241007  [   20/   89]
Per-example loss in batch: 0.266617  [   22/   89]
Per-example loss in batch: 0.213196  [   24/   89]
Per-example loss in batch: 0.229304  [   26/   89]
Per-example loss in batch: 0.273573  [   28/   89]
Per-example loss in batch: 0.257593  [   30/   89]
Per-example loss in batch: 0.290988  [   32/   89]
Per-example loss in batch: 0.196315  [   34/   89]
Per-example loss in batch: 0.211899  [   36/   89]
Per-example loss in batch: 0.263369  [   38/   89]
Per-example loss in batch: 0.233765  [   40/   89]
Per-example loss in batch: 0.236898  [   42/   89]
Per-example loss in batch: 0.220084  [   44/   89]
Per-example loss in batch: 0.316354  [   46/   89]
Per-example loss in batch: 0.241659  [   48/   89]
Per-example loss in batch: 0.180297  [   50/   89]
Per-example loss in batch: 0.196718  [   52/   89]
Per-example loss in batch: 0.229034  [   54/   89]
Per-example loss in batch: 0.265265  [   56/   89]
Per-example loss in batch: 0.221219  [   58/   89]
Per-example loss in batch: 0.214951  [   60/   89]
Per-example loss in batch: 0.268086  [   62/   89]
Per-example loss in batch: 0.298769  [   64/   89]
Per-example loss in batch: 0.327899  [   66/   89]
Per-example loss in batch: 0.245895  [   68/   89]
Per-example loss in batch: 0.205717  [   70/   89]
Per-example loss in batch: 0.348997  [   72/   89]
Per-example loss in batch: 0.240620  [   74/   89]
Per-example loss in batch: 0.248389  [   76/   89]
Per-example loss in batch: 0.214105  [   78/   89]
Per-example loss in batch: 0.315924  [   80/   89]
Per-example loss in batch: 0.239695  [   82/   89]
Per-example loss in batch: 0.258421  [   84/   89]
Per-example loss in batch: 0.292829  [   86/   89]
Per-example loss in batch: 0.300708  [   88/   89]
Per-example loss in batch: 0.501834  [   89/   89]
Train Error: Avg loss: 0.25678281
validation Error: 
 Avg loss: 0.28082305 
 F1: 0.486451 
 Precision: 0.582938 
 Recall: 0.417369
 IoU: 0.321398

test Error: 
 Avg loss: 0.25673823 
 F1: 0.515985 
 Precision: 0.618744 
 Recall: 0.442496
 IoU: 0.347695

We have finished training iteration 342
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_340_.pth
Per-example loss in batch: 0.174398  [    2/   89]
Per-example loss in batch: 0.354369  [    4/   89]
Per-example loss in batch: 0.190446  [    6/   89]
Per-example loss in batch: 0.214308  [    8/   89]
Per-example loss in batch: 0.314874  [   10/   89]
Per-example loss in batch: 0.187996  [   12/   89]
Per-example loss in batch: 0.300945  [   14/   89]
Per-example loss in batch: 0.257015  [   16/   89]
Per-example loss in batch: 0.216826  [   18/   89]
Per-example loss in batch: 0.311651  [   20/   89]
Per-example loss in batch: 0.251825  [   22/   89]
Per-example loss in batch: 0.250613  [   24/   89]
Per-example loss in batch: 0.316708  [   26/   89]
Per-example loss in batch: 0.322831  [   28/   89]
Per-example loss in batch: 0.244292  [   30/   89]
Per-example loss in batch: 0.237550  [   32/   89]
Per-example loss in batch: 0.199622  [   34/   89]
Per-example loss in batch: 0.319388  [   36/   89]
Per-example loss in batch: 0.246303  [   38/   89]
Per-example loss in batch: 0.245871  [   40/   89]
Per-example loss in batch: 0.226928  [   42/   89]
Per-example loss in batch: 0.275973  [   44/   89]
Per-example loss in batch: 0.193688  [   46/   89]
Per-example loss in batch: 0.235316  [   48/   89]
Per-example loss in batch: 0.217066  [   50/   89]
Per-example loss in batch: 0.227319  [   52/   89]
Per-example loss in batch: 0.294614  [   54/   89]
Per-example loss in batch: 0.298997  [   56/   89]
Per-example loss in batch: 0.292209  [   58/   89]
Per-example loss in batch: 0.293172  [   60/   89]
Per-example loss in batch: 0.251100  [   62/   89]
Per-example loss in batch: 0.328661  [   64/   89]
Per-example loss in batch: 0.240964  [   66/   89]
Per-example loss in batch: 0.239079  [   68/   89]
Per-example loss in batch: 0.328038  [   70/   89]
Per-example loss in batch: 0.286777  [   72/   89]
Per-example loss in batch: 0.200983  [   74/   89]
Per-example loss in batch: 0.281278  [   76/   89]
Per-example loss in batch: 0.316748  [   78/   89]
Per-example loss in batch: 0.236852  [   80/   89]
Per-example loss in batch: 0.228173  [   82/   89]
Per-example loss in batch: 0.241111  [   84/   89]
Per-example loss in batch: 0.243102  [   86/   89]
Per-example loss in batch: 0.219650  [   88/   89]
Per-example loss in batch: 0.427313  [   89/   89]
Train Error: Avg loss: 0.25998397
validation Error: 
 Avg loss: 0.27222941 
 F1: 0.491376 
 Precision: 0.564206 
 Recall: 0.435199
 IoU: 0.325711

test Error: 
 Avg loss: 0.25200385 
 F1: 0.525876 
 Precision: 0.604535 
 Recall: 0.465330
 IoU: 0.356738

We have finished training iteration 343
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_341_.pth
Per-example loss in batch: 0.231377  [    2/   89]
Per-example loss in batch: 0.228282  [    4/   89]
Per-example loss in batch: 0.275345  [    6/   89]
Per-example loss in batch: 0.265405  [    8/   89]
Per-example loss in batch: 0.209599  [   10/   89]
Per-example loss in batch: 0.251433  [   12/   89]
Per-example loss in batch: 0.246175  [   14/   89]
Per-example loss in batch: 0.273051  [   16/   89]
Per-example loss in batch: 0.228536  [   18/   89]
Per-example loss in batch: 0.306281  [   20/   89]
Per-example loss in batch: 0.211409  [   22/   89]
Per-example loss in batch: 0.284091  [   24/   89]
Per-example loss in batch: 0.274248  [   26/   89]
Per-example loss in batch: 0.204704  [   28/   89]
Per-example loss in batch: 0.244656  [   30/   89]
Per-example loss in batch: 0.266927  [   32/   89]
Per-example loss in batch: 0.236528  [   34/   89]
Per-example loss in batch: 0.238591  [   36/   89]
Per-example loss in batch: 0.226736  [   38/   89]
Per-example loss in batch: 0.247024  [   40/   89]
Per-example loss in batch: 0.237924  [   42/   89]
Per-example loss in batch: 0.225409  [   44/   89]
Per-example loss in batch: 0.304801  [   46/   89]
Per-example loss in batch: 0.229969  [   48/   89]
Per-example loss in batch: 0.299879  [   50/   89]
Per-example loss in batch: 0.233798  [   52/   89]
Per-example loss in batch: 0.200167  [   54/   89]
Per-example loss in batch: 0.197878  [   56/   89]
Per-example loss in batch: 0.256739  [   58/   89]
Per-example loss in batch: 0.307693  [   60/   89]
Per-example loss in batch: 0.210532  [   62/   89]
Per-example loss in batch: 0.239508  [   64/   89]
Per-example loss in batch: 0.252327  [   66/   89]
Per-example loss in batch: 0.212391  [   68/   89]
Per-example loss in batch: 0.303933  [   70/   89]
Per-example loss in batch: 0.215171  [   72/   89]
Per-example loss in batch: 0.322236  [   74/   89]
Per-example loss in batch: 0.296812  [   76/   89]
Per-example loss in batch: 0.295154  [   78/   89]
Per-example loss in batch: 0.224891  [   80/   89]
Per-example loss in batch: 0.324835  [   82/   89]
Per-example loss in batch: 0.250186  [   84/   89]
Per-example loss in batch: 0.296509  [   86/   89]
Per-example loss in batch: 0.233209  [   88/   89]
Per-example loss in batch: 0.349545  [   89/   89]
Train Error: Avg loss: 0.25386788
validation Error: 
 Avg loss: 0.27444867 
 F1: 0.489418 
 Precision: 0.534950 
 Recall: 0.451029
 IoU: 0.323993

test Error: 
 Avg loss: 0.25173898 
 F1: 0.527205 
 Precision: 0.579789 
 Recall: 0.483366
 IoU: 0.357962

We have finished training iteration 344
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_342_.pth
Per-example loss in batch: 0.260005  [    2/   89]
Per-example loss in batch: 0.242711  [    4/   89]
Per-example loss in batch: 0.249385  [    6/   89]
Per-example loss in batch: 0.235777  [    8/   89]
Per-example loss in batch: 0.200308  [   10/   89]
Per-example loss in batch: 0.215580  [   12/   89]
Per-example loss in batch: 0.275601  [   14/   89]
Per-example loss in batch: 0.220793  [   16/   89]
Per-example loss in batch: 0.297464  [   18/   89]
Per-example loss in batch: 0.254139  [   20/   89]
Per-example loss in batch: 0.291745  [   22/   89]
Per-example loss in batch: 0.262095  [   24/   89]
Per-example loss in batch: 0.256260  [   26/   89]
Per-example loss in batch: 0.213761  [   28/   89]
Per-example loss in batch: 0.285082  [   30/   89]
Per-example loss in batch: 0.291258  [   32/   89]
Per-example loss in batch: 0.333498  [   34/   89]
Per-example loss in batch: 0.244366  [   36/   89]
Per-example loss in batch: 0.277967  [   38/   89]
Per-example loss in batch: 0.255650  [   40/   89]
Per-example loss in batch: 0.222395  [   42/   89]
Per-example loss in batch: 0.256793  [   44/   89]
Per-example loss in batch: 0.257450  [   46/   89]
Per-example loss in batch: 0.241713  [   48/   89]
Per-example loss in batch: 0.218612  [   50/   89]
Per-example loss in batch: 0.313088  [   52/   89]
Per-example loss in batch: 0.251127  [   54/   89]
Per-example loss in batch: 0.243625  [   56/   89]
Per-example loss in batch: 0.251333  [   58/   89]
Per-example loss in batch: 0.263758  [   60/   89]
Per-example loss in batch: 0.228467  [   62/   89]
Per-example loss in batch: 0.276814  [   64/   89]
Per-example loss in batch: 0.315160  [   66/   89]
Per-example loss in batch: 0.225249  [   68/   89]
Per-example loss in batch: 0.244087  [   70/   89]
Per-example loss in batch: 0.227929  [   72/   89]
Per-example loss in batch: 0.214773  [   74/   89]
Per-example loss in batch: 0.221977  [   76/   89]
Per-example loss in batch: 0.278805  [   78/   89]
Per-example loss in batch: 0.349362  [   80/   89]
Per-example loss in batch: 0.196626  [   82/   89]
Per-example loss in batch: 0.199073  [   84/   89]
Per-example loss in batch: 0.315838  [   86/   89]
Per-example loss in batch: 0.174577  [   88/   89]
Per-example loss in batch: 0.456823  [   89/   89]
Train Error: Avg loss: 0.25574125
validation Error: 
 Avg loss: 0.27480327 
 F1: 0.492058 
 Precision: 0.559839 
 Recall: 0.438917
 IoU: 0.326311

test Error: 
 Avg loss: 0.25280599 
 F1: 0.524189 
 Precision: 0.599677 
 Recall: 0.465580
 IoU: 0.355187

We have finished training iteration 345
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_343_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.265769  [    2/   89]
Per-example loss in batch: 0.208707  [    4/   89]
Per-example loss in batch: 0.311129  [    6/   89]
Per-example loss in batch: 0.231656  [    8/   89]
Per-example loss in batch: 0.304443  [   10/   89]
Per-example loss in batch: 0.241864  [   12/   89]
Per-example loss in batch: 0.187328  [   14/   89]
Per-example loss in batch: 0.253777  [   16/   89]
Per-example loss in batch: 0.215274  [   18/   89]
Per-example loss in batch: 0.249029  [   20/   89]
Per-example loss in batch: 0.217401  [   22/   89]
Per-example loss in batch: 0.208335  [   24/   89]
Per-example loss in batch: 0.214548  [   26/   89]
Per-example loss in batch: 0.261742  [   28/   89]
Per-example loss in batch: 0.257804  [   30/   89]
Per-example loss in batch: 0.230439  [   32/   89]
Per-example loss in batch: 0.276362  [   34/   89]
Per-example loss in batch: 0.255143  [   36/   89]
Per-example loss in batch: 0.308959  [   38/   89]
Per-example loss in batch: 0.253832  [   40/   89]
Per-example loss in batch: 0.285199  [   42/   89]
Per-example loss in batch: 0.231005  [   44/   89]
Per-example loss in batch: 0.307300  [   46/   89]
Per-example loss in batch: 0.204807  [   48/   89]
Per-example loss in batch: 0.228046  [   50/   89]
Per-example loss in batch: 0.236490  [   52/   89]
Per-example loss in batch: 0.255555  [   54/   89]
Per-example loss in batch: 0.251843  [   56/   89]
Per-example loss in batch: 0.283976  [   58/   89]
Per-example loss in batch: 0.304401  [   60/   89]
Per-example loss in batch: 0.235602  [   62/   89]
Per-example loss in batch: 0.214594  [   64/   89]
Per-example loss in batch: 0.237346  [   66/   89]
Per-example loss in batch: 0.307886  [   68/   89]
Per-example loss in batch: 0.260278  [   70/   89]
Per-example loss in batch: 0.287620  [   72/   89]
Per-example loss in batch: 0.233935  [   74/   89]
Per-example loss in batch: 0.263903  [   76/   89]
Per-example loss in batch: 0.245131  [   78/   89]
Per-example loss in batch: 0.216838  [   80/   89]
Per-example loss in batch: 0.225285  [   82/   89]
Per-example loss in batch: 0.229046  [   84/   89]
Per-example loss in batch: 0.296879  [   86/   89]
Per-example loss in batch: 0.297520  [   88/   89]
Per-example loss in batch: 0.745057  [   89/   89]
Train Error: Avg loss: 0.25767540
validation Error: 
 Avg loss: 0.27326134 
 F1: 0.489867 
 Precision: 0.582742 
 Recall: 0.422526
 IoU: 0.324387

test Error: 
 Avg loss: 0.25437973 
 F1: 0.521843 
 Precision: 0.621389 
 Recall: 0.449787
 IoU: 0.353036

We have finished training iteration 346
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_344_.pth
Per-example loss in batch: 0.274069  [    2/   89]
Per-example loss in batch: 0.288519  [    4/   89]
Per-example loss in batch: 0.234274  [    6/   89]
Per-example loss in batch: 0.289241  [    8/   89]
Per-example loss in batch: 0.309917  [   10/   89]
Per-example loss in batch: 0.235294  [   12/   89]
Per-example loss in batch: 0.281933  [   14/   89]
Per-example loss in batch: 0.261667  [   16/   89]
Per-example loss in batch: 0.204813  [   18/   89]
Per-example loss in batch: 0.226221  [   20/   89]
Per-example loss in batch: 0.308051  [   22/   89]
Per-example loss in batch: 0.235921  [   24/   89]
Per-example loss in batch: 0.201407  [   26/   89]
Per-example loss in batch: 0.237471  [   28/   89]
Per-example loss in batch: 0.206125  [   30/   89]
Per-example loss in batch: 0.238556  [   32/   89]
Per-example loss in batch: 0.224071  [   34/   89]
Per-example loss in batch: 0.275608  [   36/   89]
Per-example loss in batch: 0.226347  [   38/   89]
Per-example loss in batch: 0.254083  [   40/   89]
Per-example loss in batch: 0.241919  [   42/   89]
Per-example loss in batch: 0.279581  [   44/   89]
Per-example loss in batch: 0.367394  [   46/   89]
Per-example loss in batch: 0.247427  [   48/   89]
Per-example loss in batch: 0.225767  [   50/   89]
Per-example loss in batch: 0.192723  [   52/   89]
Per-example loss in batch: 0.338710  [   54/   89]
Per-example loss in batch: 0.213371  [   56/   89]
Per-example loss in batch: 0.243705  [   58/   89]
Per-example loss in batch: 0.222504  [   60/   89]
Per-example loss in batch: 0.214436  [   62/   89]
Per-example loss in batch: 0.221079  [   64/   89]
Per-example loss in batch: 0.322914  [   66/   89]
Per-example loss in batch: 0.279702  [   68/   89]
Per-example loss in batch: 0.192087  [   70/   89]
Per-example loss in batch: 0.231956  [   72/   89]
Per-example loss in batch: 0.309777  [   74/   89]
Per-example loss in batch: 0.337626  [   76/   89]
Per-example loss in batch: 0.292541  [   78/   89]
Per-example loss in batch: 0.209894  [   80/   89]
Per-example loss in batch: 0.326952  [   82/   89]
Per-example loss in batch: 0.233615  [   84/   89]
Per-example loss in batch: 0.314012  [   86/   89]
Per-example loss in batch: 0.316628  [   88/   89]
Per-example loss in batch: 0.596450  [   89/   89]
Train Error: Avg loss: 0.26265474
validation Error: 
 Avg loss: 0.27496117 
 F1: 0.489901 
 Precision: 0.567706 
 Recall: 0.430852
 IoU: 0.324417

test Error: 
 Avg loss: 0.25372624 
 F1: 0.523029 
 Precision: 0.607429 
 Recall: 0.459222
 IoU: 0.354123

We have finished training iteration 347
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_345_.pth
Per-example loss in batch: 0.199357  [    2/   89]
Per-example loss in batch: 0.265307  [    4/   89]
Per-example loss in batch: 0.215122  [    6/   89]
Per-example loss in batch: 0.211845  [    8/   89]
Per-example loss in batch: 0.303063  [   10/   89]
Per-example loss in batch: 0.280372  [   12/   89]
Per-example loss in batch: 0.216070  [   14/   89]
Per-example loss in batch: 0.192344  [   16/   89]
Per-example loss in batch: 0.328830  [   18/   89]
Per-example loss in batch: 0.288865  [   20/   89]
Per-example loss in batch: 0.306677  [   22/   89]
Per-example loss in batch: 0.292307  [   24/   89]
Per-example loss in batch: 0.317185  [   26/   89]
Per-example loss in batch: 0.207565  [   28/   89]
Per-example loss in batch: 0.264573  [   30/   89]
Per-example loss in batch: 0.243588  [   32/   89]
Per-example loss in batch: 0.225719  [   34/   89]
Per-example loss in batch: 0.310782  [   36/   89]
Per-example loss in batch: 0.260536  [   38/   89]
Per-example loss in batch: 0.316683  [   40/   89]
Per-example loss in batch: 0.244078  [   42/   89]
Per-example loss in batch: 0.232448  [   44/   89]
Per-example loss in batch: 0.193719  [   46/   89]
Per-example loss in batch: 0.265297  [   48/   89]
Per-example loss in batch: 0.205900  [   50/   89]
Per-example loss in batch: 0.252995  [   52/   89]
Per-example loss in batch: 0.351111  [   54/   89]
Per-example loss in batch: 0.237094  [   56/   89]
Per-example loss in batch: 0.268082  [   58/   89]
Per-example loss in batch: 0.212081  [   60/   89]
Per-example loss in batch: 0.222165  [   62/   89]
Per-example loss in batch: 0.240636  [   64/   89]
Per-example loss in batch: 0.280780  [   66/   89]
Per-example loss in batch: 0.250971  [   68/   89]
Per-example loss in batch: 0.204771  [   70/   89]
Per-example loss in batch: 0.226066  [   72/   89]
Per-example loss in batch: 0.240059  [   74/   89]
Per-example loss in batch: 0.234770  [   76/   89]
Per-example loss in batch: 0.362187  [   78/   89]
Per-example loss in batch: 0.270361  [   80/   89]
Per-example loss in batch: 0.247845  [   82/   89]
Per-example loss in batch: 0.281185  [   84/   89]
Per-example loss in batch: 0.230994  [   86/   89]
Per-example loss in batch: 0.254755  [   88/   89]
Per-example loss in batch: 0.520875  [   89/   89]
Train Error: Avg loss: 0.25882195
validation Error: 
 Avg loss: 0.27109616 
 F1: 0.491401 
 Precision: 0.564105 
 Recall: 0.435298
 IoU: 0.325733

test Error: 
 Avg loss: 0.25305868 
 F1: 0.524398 
 Precision: 0.602672 
 Recall: 0.464118
 IoU: 0.355379

We have finished training iteration 348
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_346_.pth
Per-example loss in batch: 0.348955  [    2/   89]
Per-example loss in batch: 0.297942  [    4/   89]
Per-example loss in batch: 0.253736  [    6/   89]
Per-example loss in batch: 0.283951  [    8/   89]
Per-example loss in batch: 0.250847  [   10/   89]
Per-example loss in batch: 0.191381  [   12/   89]
Per-example loss in batch: 0.318266  [   14/   89]
Per-example loss in batch: 0.296459  [   16/   89]
Per-example loss in batch: 0.226136  [   18/   89]
Per-example loss in batch: 0.282396  [   20/   89]
Per-example loss in batch: 0.216562  [   22/   89]
Per-example loss in batch: 0.249683  [   24/   89]
Per-example loss in batch: 0.201148  [   26/   89]
Per-example loss in batch: 0.298973  [   28/   89]
Per-example loss in batch: 0.215131  [   30/   89]
Per-example loss in batch: 0.203120  [   32/   89]
Per-example loss in batch: 0.297566  [   34/   89]
Per-example loss in batch: 0.197416  [   36/   89]
Per-example loss in batch: 0.371803  [   38/   89]
Per-example loss in batch: 0.225967  [   40/   89]
Per-example loss in batch: 0.197894  [   42/   89]
Per-example loss in batch: 0.341371  [   44/   89]
Per-example loss in batch: 0.207280  [   46/   89]
Per-example loss in batch: 0.225595  [   48/   89]
Per-example loss in batch: 0.280469  [   50/   89]
Per-example loss in batch: 0.267022  [   52/   89]
Per-example loss in batch: 0.200637  [   54/   89]
Per-example loss in batch: 0.244855  [   56/   89]
Per-example loss in batch: 0.263428  [   58/   89]
Per-example loss in batch: 0.219857  [   60/   89]
Per-example loss in batch: 0.228397  [   62/   89]
Per-example loss in batch: 0.223221  [   64/   89]
Per-example loss in batch: 0.326955  [   66/   89]
Per-example loss in batch: 0.283661  [   68/   89]
Per-example loss in batch: 0.227610  [   70/   89]
Per-example loss in batch: 0.212886  [   72/   89]
Per-example loss in batch: 0.239202  [   74/   89]
Per-example loss in batch: 0.266359  [   76/   89]
Per-example loss in batch: 0.278464  [   78/   89]
Per-example loss in batch: 0.238561  [   80/   89]
Per-example loss in batch: 0.335811  [   82/   89]
Per-example loss in batch: 0.212817  [   84/   89]
Per-example loss in batch: 0.236029  [   86/   89]
Per-example loss in batch: 0.206976  [   88/   89]
Per-example loss in batch: 0.415144  [   89/   89]
Train Error: Avg loss: 0.25618803
validation Error: 
 Avg loss: 0.27942323 
 F1: 0.489982 
 Precision: 0.532695 
 Recall: 0.453610
 IoU: 0.324487

test Error: 
 Avg loss: 0.25197765 
 F1: 0.526812 
 Precision: 0.577388 
 Recall: 0.484382
 IoU: 0.357600

We have finished training iteration 349
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_347_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.179374  [    2/   89]
Per-example loss in batch: 0.319563  [    4/   89]
Per-example loss in batch: 0.216082  [    6/   89]
Per-example loss in batch: 0.220283  [    8/   89]
Per-example loss in batch: 0.250752  [   10/   89]
Per-example loss in batch: 0.271822  [   12/   89]
Per-example loss in batch: 0.187340  [   14/   89]
Per-example loss in batch: 0.227735  [   16/   89]
Per-example loss in batch: 0.338641  [   18/   89]
Per-example loss in batch: 0.313930  [   20/   89]
Per-example loss in batch: 0.187015  [   22/   89]
Per-example loss in batch: 0.236916  [   24/   89]
Per-example loss in batch: 0.310295  [   26/   89]
Per-example loss in batch: 0.282237  [   28/   89]
Per-example loss in batch: 0.223633  [   30/   89]
Per-example loss in batch: 0.234238  [   32/   89]
Per-example loss in batch: 0.281101  [   34/   89]
Per-example loss in batch: 0.292456  [   36/   89]
Per-example loss in batch: 0.242724  [   38/   89]
Per-example loss in batch: 0.229479  [   40/   89]
Per-example loss in batch: 0.322971  [   42/   89]
Per-example loss in batch: 0.285149  [   44/   89]
Per-example loss in batch: 0.236565  [   46/   89]
Per-example loss in batch: 0.289328  [   48/   89]
Per-example loss in batch: 0.261678  [   50/   89]
Per-example loss in batch: 0.235073  [   52/   89]
Per-example loss in batch: 0.203617  [   54/   89]
Per-example loss in batch: 0.309244  [   56/   89]
Per-example loss in batch: 0.257915  [   58/   89]
Per-example loss in batch: 0.341142  [   60/   89]
Per-example loss in batch: 0.203224  [   62/   89]
Per-example loss in batch: 0.260339  [   64/   89]
Per-example loss in batch: 0.205876  [   66/   89]
Per-example loss in batch: 0.301668  [   68/   89]
Per-example loss in batch: 0.286767  [   70/   89]
Per-example loss in batch: 0.273611  [   72/   89]
Per-example loss in batch: 0.303977  [   74/   89]
Per-example loss in batch: 0.197742  [   76/   89]
Per-example loss in batch: 0.267644  [   78/   89]
Per-example loss in batch: 0.262723  [   80/   89]
Per-example loss in batch: 0.226228  [   82/   89]
Per-example loss in batch: 0.271831  [   84/   89]
Per-example loss in batch: 0.237887  [   86/   89]
Per-example loss in batch: 0.206494  [   88/   89]
Per-example loss in batch: 0.625583  [   89/   89]
Train Error: Avg loss: 0.26083371
validation Error: 
 Avg loss: 0.27704993 
 F1: 0.486203 
 Precision: 0.558799 
 Recall: 0.430300
 IoU: 0.321181

test Error: 
 Avg loss: 0.25572691 
 F1: 0.518480 
 Precision: 0.594374 
 Recall: 0.459773
 IoU: 0.349965

We have finished training iteration 350
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_348_.pth
Per-example loss in batch: 0.314677  [    2/   89]
Per-example loss in batch: 0.317583  [    4/   89]
Per-example loss in batch: 0.259030  [    6/   89]
Per-example loss in batch: 0.278764  [    8/   89]
Per-example loss in batch: 0.252382  [   10/   89]
Per-example loss in batch: 0.222660  [   12/   89]
Per-example loss in batch: 0.216772  [   14/   89]
Per-example loss in batch: 0.250861  [   16/   89]
Per-example loss in batch: 0.255279  [   18/   89]
Per-example loss in batch: 0.199137  [   20/   89]
Per-example loss in batch: 0.239461  [   22/   89]
Per-example loss in batch: 0.242066  [   24/   89]
Per-example loss in batch: 0.241267  [   26/   89]
Per-example loss in batch: 0.216326  [   28/   89]
Per-example loss in batch: 0.200607  [   30/   89]
Per-example loss in batch: 0.244510  [   32/   89]
Per-example loss in batch: 0.399480  [   34/   89]
Per-example loss in batch: 0.314409  [   36/   89]
Per-example loss in batch: 0.303678  [   38/   89]
Per-example loss in batch: 0.279014  [   40/   89]
Per-example loss in batch: 0.255864  [   42/   89]
Per-example loss in batch: 0.206925  [   44/   89]
Per-example loss in batch: 0.286429  [   46/   89]
Per-example loss in batch: 0.252205  [   48/   89]
Per-example loss in batch: 0.306973  [   50/   89]
Per-example loss in batch: 0.302343  [   52/   89]
Per-example loss in batch: 0.308695  [   54/   89]
Per-example loss in batch: 0.251562  [   56/   89]
Per-example loss in batch: 0.258381  [   58/   89]
Per-example loss in batch: 0.295796  [   60/   89]
Per-example loss in batch: 0.219138  [   62/   89]
Per-example loss in batch: 0.263959  [   64/   89]
Per-example loss in batch: 0.223401  [   66/   89]
Per-example loss in batch: 0.182361  [   68/   89]
Per-example loss in batch: 0.262366  [   70/   89]
Per-example loss in batch: 0.269980  [   72/   89]
Per-example loss in batch: 0.230799  [   74/   89]
Per-example loss in batch: 0.199108  [   76/   89]
Per-example loss in batch: 0.249791  [   78/   89]
Per-example loss in batch: 0.215258  [   80/   89]
Per-example loss in batch: 0.318990  [   82/   89]
Per-example loss in batch: 0.317121  [   84/   89]
Per-example loss in batch: 0.221677  [   86/   89]
Per-example loss in batch: 0.241907  [   88/   89]
Per-example loss in batch: 0.649585  [   89/   89]
Train Error: Avg loss: 0.26323113
validation Error: 
 Avg loss: 0.27385723 
 F1: 0.490521 
 Precision: 0.575210 
 Recall: 0.427569
 IoU: 0.324960

test Error: 
 Avg loss: 0.25415069 
 F1: 0.522117 
 Precision: 0.615290 
 Recall: 0.453452
 IoU: 0.353287

We have finished training iteration 351
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_349_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.283334  [    2/   89]
Per-example loss in batch: 0.263048  [    4/   89]
Per-example loss in batch: 0.202740  [    6/   89]
Per-example loss in batch: 0.264541  [    8/   89]
Per-example loss in batch: 0.236504  [   10/   89]
Per-example loss in batch: 0.278554  [   12/   89]
Per-example loss in batch: 0.279115  [   14/   89]
Per-example loss in batch: 0.190836  [   16/   89]
Per-example loss in batch: 0.332764  [   18/   89]
Per-example loss in batch: 0.321786  [   20/   89]
Per-example loss in batch: 0.202729  [   22/   89]
Per-example loss in batch: 0.282829  [   24/   89]
Per-example loss in batch: 0.227332  [   26/   89]
Per-example loss in batch: 0.271028  [   28/   89]
Per-example loss in batch: 0.232961  [   30/   89]
Per-example loss in batch: 0.240532  [   32/   89]
Per-example loss in batch: 0.229158  [   34/   89]
Per-example loss in batch: 0.222230  [   36/   89]
Per-example loss in batch: 0.218261  [   38/   89]
Per-example loss in batch: 0.229588  [   40/   89]
Per-example loss in batch: 0.339432  [   42/   89]
Per-example loss in batch: 0.247965  [   44/   89]
Per-example loss in batch: 0.215101  [   46/   89]
Per-example loss in batch: 0.254598  [   48/   89]
Per-example loss in batch: 0.266460  [   50/   89]
Per-example loss in batch: 0.229117  [   52/   89]
Per-example loss in batch: 0.214886  [   54/   89]
Per-example loss in batch: 0.267411  [   56/   89]
Per-example loss in batch: 0.261997  [   58/   89]
Per-example loss in batch: 0.248746  [   60/   89]
Per-example loss in batch: 0.304531  [   62/   89]
Per-example loss in batch: 0.245112  [   64/   89]
Per-example loss in batch: 0.247499  [   66/   89]
Per-example loss in batch: 0.234345  [   68/   89]
Per-example loss in batch: 0.252759  [   70/   89]
Per-example loss in batch: 0.251050  [   72/   89]
Per-example loss in batch: 0.252902  [   74/   89]
Per-example loss in batch: 0.227858  [   76/   89]
Per-example loss in batch: 0.213758  [   78/   89]
Per-example loss in batch: 0.312160  [   80/   89]
Per-example loss in batch: 0.192691  [   82/   89]
Per-example loss in batch: 0.231566  [   84/   89]
Per-example loss in batch: 0.309771  [   86/   89]
Per-example loss in batch: 0.214450  [   88/   89]
Per-example loss in batch: 0.580797  [   89/   89]
Train Error: Avg loss: 0.25475131
validation Error: 
 Avg loss: 0.27566963 
 F1: 0.490353 
 Precision: 0.552862 
 Recall: 0.440543
 IoU: 0.324813

test Error: 
 Avg loss: 0.25254160 
 F1: 0.524932 
 Precision: 0.594837 
 Recall: 0.469730
 IoU: 0.355870

We have finished training iteration 352
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_350_.pth
Per-example loss in batch: 0.223238  [    2/   89]
Per-example loss in batch: 0.271685  [    4/   89]
Per-example loss in batch: 0.228323  [    6/   89]
Per-example loss in batch: 0.229751  [    8/   89]
Per-example loss in batch: 0.241365  [   10/   89]
Per-example loss in batch: 0.236616  [   12/   89]
Per-example loss in batch: 0.250490  [   14/   89]
Per-example loss in batch: 0.183967  [   16/   89]
Per-example loss in batch: 0.252323  [   18/   89]
Per-example loss in batch: 0.278107  [   20/   89]
Per-example loss in batch: 0.233118  [   22/   89]
Per-example loss in batch: 0.207227  [   24/   89]
Per-example loss in batch: 0.227412  [   26/   89]
Per-example loss in batch: 0.315971  [   28/   89]
Per-example loss in batch: 0.237844  [   30/   89]
Per-example loss in batch: 0.265550  [   32/   89]
Per-example loss in batch: 0.212843  [   34/   89]
Per-example loss in batch: 0.236146  [   36/   89]
Per-example loss in batch: 0.299094  [   38/   89]
Per-example loss in batch: 0.210878  [   40/   89]
Per-example loss in batch: 0.319149  [   42/   89]
Per-example loss in batch: 0.240953  [   44/   89]
Per-example loss in batch: 0.235196  [   46/   89]
Per-example loss in batch: 0.237688  [   48/   89]
Per-example loss in batch: 0.255003  [   50/   89]
Per-example loss in batch: 0.304236  [   52/   89]
Per-example loss in batch: 0.225158  [   54/   89]
Per-example loss in batch: 0.231996  [   56/   89]
Per-example loss in batch: 0.199450  [   58/   89]
Per-example loss in batch: 0.212120  [   60/   89]
Per-example loss in batch: 0.256622  [   62/   89]
Per-example loss in batch: 0.272185  [   64/   89]
Per-example loss in batch: 0.329905  [   66/   89]
Per-example loss in batch: 0.252234  [   68/   89]
Per-example loss in batch: 0.337321  [   70/   89]
Per-example loss in batch: 0.208397  [   72/   89]
Per-example loss in batch: 0.329141  [   74/   89]
Per-example loss in batch: 0.249729  [   76/   89]
Per-example loss in batch: 0.285682  [   78/   89]
Per-example loss in batch: 0.237936  [   80/   89]
Per-example loss in batch: 0.276801  [   82/   89]
Per-example loss in batch: 0.288347  [   84/   89]
Per-example loss in batch: 0.196460  [   86/   89]
Per-example loss in batch: 0.257412  [   88/   89]
Per-example loss in batch: 0.388199  [   89/   89]
Train Error: Avg loss: 0.25337459
validation Error: 
 Avg loss: 0.27621006 
 F1: 0.492557 
 Precision: 0.551889 
 Recall: 0.444744
 IoU: 0.326750

test Error: 
 Avg loss: 0.25170248 
 F1: 0.526958 
 Precision: 0.593941 
 Recall: 0.473552
 IoU: 0.357735

We have finished training iteration 353
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_351_.pth
Per-example loss in batch: 0.203932  [    2/   89]
Per-example loss in batch: 0.224720  [    4/   89]
Per-example loss in batch: 0.288439  [    6/   89]
Per-example loss in batch: 0.300917  [    8/   89]
Per-example loss in batch: 0.294757  [   10/   89]
Per-example loss in batch: 0.291822  [   12/   89]
Per-example loss in batch: 0.266778  [   14/   89]
Per-example loss in batch: 0.251333  [   16/   89]
Per-example loss in batch: 0.188540  [   18/   89]
Per-example loss in batch: 0.267555  [   20/   89]
Per-example loss in batch: 0.186095  [   22/   89]
Per-example loss in batch: 0.289723  [   24/   89]
Per-example loss in batch: 0.218395  [   26/   89]
Per-example loss in batch: 0.287951  [   28/   89]
Per-example loss in batch: 0.254287  [   30/   89]
Per-example loss in batch: 0.205783  [   32/   89]
Per-example loss in batch: 0.237361  [   34/   89]
Per-example loss in batch: 0.201203  [   36/   89]
Per-example loss in batch: 0.231309  [   38/   89]
Per-example loss in batch: 0.209476  [   40/   89]
Per-example loss in batch: 0.308026  [   42/   89]
Per-example loss in batch: 0.250534  [   44/   89]
Per-example loss in batch: 0.254601  [   46/   89]
Per-example loss in batch: 0.289196  [   48/   89]
Per-example loss in batch: 0.269367  [   50/   89]
Per-example loss in batch: 0.230275  [   52/   89]
Per-example loss in batch: 0.212724  [   54/   89]
Per-example loss in batch: 0.231405  [   56/   89]
Per-example loss in batch: 0.275468  [   58/   89]
Per-example loss in batch: 0.275901  [   60/   89]
Per-example loss in batch: 0.225410  [   62/   89]
Per-example loss in batch: 0.226549  [   64/   89]
Per-example loss in batch: 0.299378  [   66/   89]
Per-example loss in batch: 0.288596  [   68/   89]
Per-example loss in batch: 0.242099  [   70/   89]
Per-example loss in batch: 0.219744  [   72/   89]
Per-example loss in batch: 0.243945  [   74/   89]
Per-example loss in batch: 0.272825  [   76/   89]
Per-example loss in batch: 0.223945  [   78/   89]
Per-example loss in batch: 0.281344  [   80/   89]
Per-example loss in batch: 0.198981  [   82/   89]
Per-example loss in batch: 0.252879  [   84/   89]
Per-example loss in batch: 0.227888  [   86/   89]
Per-example loss in batch: 0.226164  [   88/   89]
Per-example loss in batch: 0.571370  [   89/   89]
Train Error: Avg loss: 0.25198436
validation Error: 
 Avg loss: 0.26976288 
 F1: 0.492759 
 Precision: 0.552594 
 Recall: 0.444616
 IoU: 0.326928

test Error: 
 Avg loss: 0.25275414 
 F1: 0.524139 
 Precision: 0.592516 
 Recall: 0.469911
 IoU: 0.355142

We have finished training iteration 354
Deleting model ./unet_bas_train/saved_model_wrapper/models/UNet_352_.pth
Per-example loss in batch: 0.251452  [    2/   89]
Per-example loss in batch: 0.282699  [    4/   89]
Per-example loss in batch: 0.246768  [    6/   89]
Per-example loss in batch: 0.242265  [    8/   89]
Per-example loss in batch: 0.341553  [   10/   89]
Per-example loss in batch: 0.224149  [   12/   89]
Per-example loss in batch: 0.242193  [   14/   89]
Per-example loss in batch: 0.194351  [   16/   89]
Per-example loss in batch: 0.225105  [   18/   89]
Per-example loss in batch: 0.188586  [   20/   89]
Per-example loss in batch: 0.299122  [   22/   89]
Per-example loss in batch: 0.201715  [   24/   89]
Per-example loss in batch: 0.266388  [   26/   89]
Per-example loss in batch: 0.243049  [   28/   89]
Per-example loss in batch: 0.259631  [   30/   89]
Per-example loss in batch: 0.181638  [   32/   89]
Per-example loss in batch: 0.312036  [   34/   89]
Per-example loss in batch: 0.258844  [   36/   89]
Per-example loss in batch: 0.196216  [   38/   89]
Per-example loss in batch: 0.283662  [   40/   89]
Per-example loss in batch: 0.317038  [   42/   89]
Per-example loss in batch: 0.263989  [   44/   89]
Per-example loss in batch: 0.308469  [   46/   89]
Per-example loss in batch: 0.238387  [   48/   89]
Per-example loss in batch: 0.219078  [   50/   89]
Per-example loss in batch: 0.253194  [   52/   89]
Per-example loss in batch: 0.226616  [   54/   89]
Per-example loss in batch: 0.291658  [   56/   89]
Per-example loss in batch: 0.331402  [   58/   89]
Per-example loss in batch: 0.290755  [   60/   89]
Per-example loss in batch: 0.198510  [   62/   89]
Per-example loss in batch: 0.190730  [   64/   89]
Per-example loss in batch: 0.262180  [   66/   89]
Per-example loss in batch: 0.273249  [   68/   89]
Per-example loss in batch: 0.270791  [   70/   89]
Per-example loss in batch: 0.263061  [   72/   89]
Per-example loss in batch: 0.234301  [   74/   89]
Per-example loss in batch: 0.336896  [   76/   89]
Per-example loss in batch: 0.237511  [   78/   89]
Per-example loss in batch: 0.292425  [   80/   89]
Per-example loss in batch: 0.263661  [   82/   89]
Per-example loss in batch: 0.296499  [   84/   89]
Per-example loss in batch: 0.237774  [   86/   89]
Per-example loss in batch: 0.204252  [   88/   89]
Per-example loss in batch: 0.593129  [   89/   89]
Train Error: Avg loss: 0.25933511
validation Error: 
 Avg loss: 0.27626378 
 F1: 0.490077 
 Precision: 0.545294 
 Recall: 0.445014
 IoU: 0.324571

slurmstepd: error: *** STEP 16444.0 ON aga2 CANCELLED AT 2025-01-07T16:45:17 ***
