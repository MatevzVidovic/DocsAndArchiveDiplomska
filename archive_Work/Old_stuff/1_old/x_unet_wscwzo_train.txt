/shared/home/matevz.vidovic/Diplomska/Prototip/Delo/model_wrapper.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model = torch.load(self.prev_model_path, map_location=torch.device(device))
unet_original_main.py do_log: True
Log file name: log_07_14-49-29_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(ptd='./Data/vein_and_sclera_data', sd='unet_wscwzo_train', mti=250, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_original_wscwzo.yaml', ntibp=None, ptp=None, map=None)
YAML: {'batch_size': 2, 'learning_rate': 1e-05, 'num_of_dataloader_workers': 7, 'train_epoch_size_limit': 400, 'num_epochs_per_training_iteration': 1, 'cleanup_k': 3, 'dataset_option': 'aug_with_sclera', 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': True, 'loss_fn_name': 'MCDL', 'alphas': [], 'model': '64_2_6', 'input_width': 2048, 'input_height': 1024, 'input_channels': 4, 'output_channels': 2, 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq'}
Validation phase: False
Namespace(ptd='./Data/vein_and_sclera_data', sd='unet_wscwzo_train', mti=250, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_unet/unet_original_wscwzo.yaml', ntibp=None, ptp=None, map=None)
Device: cuda
dataset_aug_with_sclera.py do_log: True
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 89
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 89
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 45
val dataloader num of batches: 14
test dataloader num of batches: 6
Loaded model path:  ./unet_wscwzo_train/saved_model_wrapper/models/UNet_74_.pth
Per-example loss in batch: 0.403332  [    2/   89]
Per-example loss in batch: 0.422366  [    4/   89]
Per-example loss in batch: 0.445715  [    6/   89]
Per-example loss in batch: 0.346302  [    8/   89]
Per-example loss in batch: 0.384656  [   10/   89]
Per-example loss in batch: 0.389081  [   12/   89]
Per-example loss in batch: 0.395358  [   14/   89]
Per-example loss in batch: 0.390536  [   16/   89]
Per-example loss in batch: 0.454060  [   18/   89]
Per-example loss in batch: 0.410740  [   20/   89]
Per-example loss in batch: 0.391079  [   22/   89]
Per-example loss in batch: 0.371894  [   24/   89]
Per-example loss in batch: 0.421794  [   26/   89]
Per-example loss in batch: 0.417585  [   28/   89]
Per-example loss in batch: 0.303540  [   30/   89]
Per-example loss in batch: 0.345654  [   32/   89]
Per-example loss in batch: 0.332982  [   34/   89]
Per-example loss in batch: 0.318001  [   36/   89]
Per-example loss in batch: 0.279442  [   38/   89]
Per-example loss in batch: 0.353404  [   40/   89]
Per-example loss in batch: 0.416995  [   42/   89]
Per-example loss in batch: 0.320793  [   44/   89]
Per-example loss in batch: 0.344561  [   46/   89]
Per-example loss in batch: 0.438169  [   48/   89]
Per-example loss in batch: 0.378514  [   50/   89]
Per-example loss in batch: 0.442697  [   52/   89]
Per-example loss in batch: 0.389620  [   54/   89]
Per-example loss in batch: 0.381274  [   56/   89]
Per-example loss in batch: 0.417758  [   58/   89]
Per-example loss in batch: 0.433717  [   60/   89]
Per-example loss in batch: 0.405165  [   62/   89]
Per-example loss in batch: 0.339400  [   64/   89]
Per-example loss in batch: 0.419855  [   66/   89]
Per-example loss in batch: 0.453434  [   68/   89]
Per-example loss in batch: 0.395696  [   70/   89]
Per-example loss in batch: 0.421737  [   72/   89]
Per-example loss in batch: 0.433252  [   74/   89]
Per-example loss in batch: 0.404904  [   76/   89]
Per-example loss in batch: 0.390874  [   78/   89]
Per-example loss in batch: 0.398140  [   80/   89]
Per-example loss in batch: 0.377710  [   82/   89]
Per-example loss in batch: 0.371976  [   84/   89]
Per-example loss in batch: 0.321040  [   86/   89]
Per-example loss in batch: 0.395342  [   88/   89]
Per-example loss in batch: 0.744724  [   89/   89]
Train Error: Avg loss: 0.39196645
validation Error: 
 Avg loss: 0.50195955 
 F1: 0.241956 
 Precision: 0.877641 
 Recall: 0.140321
 IoU: 0.137628

test Error: 
 Avg loss: 0.48242359 
 F1: 0.247288 
 Precision: 0.873336 
 Recall: 0.144036
 IoU: 0.141089

We have finished training iteration 75
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_73_.pth
Per-example loss in batch: 0.403987  [    2/   89]
Per-example loss in batch: 0.415710  [    4/   89]
Per-example loss in batch: 0.397464  [    6/   89]
Per-example loss in batch: 0.325443  [    8/   89]
Per-example loss in batch: 0.414732  [   10/   89]
Per-example loss in batch: 0.382703  [   12/   89]
Per-example loss in batch: 0.419311  [   14/   89]
Per-example loss in batch: 0.398623  [   16/   89]
Per-example loss in batch: 0.373293  [   18/   89]
Per-example loss in batch: 0.434005  [   20/   89]
Per-example loss in batch: 0.459262  [   22/   89]
Per-example loss in batch: 0.447591  [   24/   89]
Per-example loss in batch: 0.388850  [   26/   89]
Per-example loss in batch: 0.443620  [   28/   89]
Per-example loss in batch: 0.423035  [   30/   89]
Per-example loss in batch: 0.344463  [   32/   89]
Per-example loss in batch: 0.404218  [   34/   89]
Per-example loss in batch: 0.346331  [   36/   89]
Per-example loss in batch: 0.444970  [   38/   89]
Per-example loss in batch: 0.314308  [   40/   89]
Per-example loss in batch: 0.339849  [   42/   89]
Per-example loss in batch: 0.387541  [   44/   89]
Per-example loss in batch: 0.342719  [   46/   89]
Per-example loss in batch: 0.430328  [   48/   89]
Per-example loss in batch: 0.352205  [   50/   89]
Per-example loss in batch: 0.387358  [   52/   89]
Per-example loss in batch: 0.436125  [   54/   89]
Per-example loss in batch: 0.441732  [   56/   89]
Per-example loss in batch: 0.381965  [   58/   89]
Per-example loss in batch: 0.369415  [   60/   89]
Per-example loss in batch: 0.354123  [   62/   89]
Per-example loss in batch: 0.432277  [   64/   89]
Per-example loss in batch: 0.400662  [   66/   89]
Per-example loss in batch: 0.395117  [   68/   89]
Per-example loss in batch: 0.308039  [   70/   89]
Per-example loss in batch: 0.389920  [   72/   89]
Per-example loss in batch: 0.354691  [   74/   89]
Per-example loss in batch: 0.383902  [   76/   89]
Per-example loss in batch: 0.422843  [   78/   89]
Per-example loss in batch: 0.384586  [   80/   89]
Per-example loss in batch: 0.372212  [   82/   89]
Per-example loss in batch: 0.406446  [   84/   89]
Per-example loss in batch: 0.338181  [   86/   89]
Per-example loss in batch: 0.380845  [   88/   89]
Per-example loss in batch: 0.844126  [   89/   89]
Train Error: Avg loss: 0.39543966
validation Error: 
 Avg loss: 0.50129418 
 F1: 0.239768 
 Precision: 0.880727 
 Recall: 0.138774
 IoU: 0.136214

test Error: 
 Avg loss: 0.48131492 
 F1: 0.245527 
 Precision: 0.884327 
 Recall: 0.142553
 IoU: 0.139944

We have finished training iteration 76
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_74_.pth
Per-example loss in batch: 0.384659  [    2/   89]
Per-example loss in batch: 0.412971  [    4/   89]
Per-example loss in batch: 0.361632  [    6/   89]
Per-example loss in batch: 0.427408  [    8/   89]
Per-example loss in batch: 0.405513  [   10/   89]
Per-example loss in batch: 0.382861  [   12/   89]
Per-example loss in batch: 0.419902  [   14/   89]
Per-example loss in batch: 0.345212  [   16/   89]
Per-example loss in batch: 0.328161  [   18/   89]
Per-example loss in batch: 0.364185  [   20/   89]
Per-example loss in batch: 0.365994  [   22/   89]
Per-example loss in batch: 0.358617  [   24/   89]
Per-example loss in batch: 0.377135  [   26/   89]
Per-example loss in batch: 0.313607  [   28/   89]
Per-example loss in batch: 0.366689  [   30/   89]
Per-example loss in batch: 0.433669  [   32/   89]
Per-example loss in batch: 0.453004  [   34/   89]
Per-example loss in batch: 0.420461  [   36/   89]
Per-example loss in batch: 0.364464  [   38/   89]
Per-example loss in batch: 0.362845  [   40/   89]
Per-example loss in batch: 0.297408  [   42/   89]
Per-example loss in batch: 0.331567  [   44/   89]
Per-example loss in batch: 0.403248  [   46/   89]
Per-example loss in batch: 0.379059  [   48/   89]
Per-example loss in batch: 0.328846  [   50/   89]
Per-example loss in batch: 0.350035  [   52/   89]
Per-example loss in batch: 0.327233  [   54/   89]
Per-example loss in batch: 0.397702  [   56/   89]
Per-example loss in batch: 0.413814  [   58/   89]
Per-example loss in batch: 0.396862  [   60/   89]
Per-example loss in batch: 0.384424  [   62/   89]
Per-example loss in batch: 0.448330  [   64/   89]
Per-example loss in batch: 0.419504  [   66/   89]
Per-example loss in batch: 0.444040  [   68/   89]
Per-example loss in batch: 0.386160  [   70/   89]
Per-example loss in batch: 0.419798  [   72/   89]
Per-example loss in batch: 0.387890  [   74/   89]
Per-example loss in batch: 0.357613  [   76/   89]
Per-example loss in batch: 0.374551  [   78/   89]
Per-example loss in batch: 0.429819  [   80/   89]
Per-example loss in batch: 0.327071  [   82/   89]
Per-example loss in batch: 0.293262  [   84/   89]
Per-example loss in batch: 0.362016  [   86/   89]
Per-example loss in batch: 0.391672  [   88/   89]
Per-example loss in batch: 0.862814  [   89/   89]
Train Error: Avg loss: 0.38499605
validation Error: 
 Avg loss: 0.50122469 
 F1: 0.257916 
 Precision: 0.851332 
 Recall: 0.151980
 IoU: 0.148051

test Error: 
 Avg loss: 0.48063368 
 F1: 0.265145 
 Precision: 0.853705 
 Recall: 0.156945
 IoU: 0.152834

We have finished training iteration 77
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_75_.pth
Per-example loss in batch: 0.312309  [    2/   89]
Per-example loss in batch: 0.433108  [    4/   89]
Per-example loss in batch: 0.404464  [    6/   89]
Per-example loss in batch: 0.411426  [    8/   89]
Per-example loss in batch: 0.330419  [   10/   89]
Per-example loss in batch: 0.420732  [   12/   89]
Per-example loss in batch: 0.384963  [   14/   89]
Per-example loss in batch: 0.315077  [   16/   89]
Per-example loss in batch: 0.411222  [   18/   89]
Per-example loss in batch: 0.373430  [   20/   89]
Per-example loss in batch: 0.383334  [   22/   89]
Per-example loss in batch: 0.295813  [   24/   89]
Per-example loss in batch: 0.415485  [   26/   89]
Per-example loss in batch: 0.354846  [   28/   89]
Per-example loss in batch: 0.351171  [   30/   89]
Per-example loss in batch: 0.362198  [   32/   89]
Per-example loss in batch: 0.352207  [   34/   89]
Per-example loss in batch: 0.366756  [   36/   89]
Per-example loss in batch: 0.386065  [   38/   89]
Per-example loss in batch: 0.448371  [   40/   89]
Per-example loss in batch: 0.353899  [   42/   89]
Per-example loss in batch: 0.440635  [   44/   89]
Per-example loss in batch: 0.358931  [   46/   89]
Per-example loss in batch: 0.322464  [   48/   89]
Per-example loss in batch: 0.332687  [   50/   89]
Per-example loss in batch: 0.414567  [   52/   89]
Per-example loss in batch: 0.387298  [   54/   89]
Per-example loss in batch: 0.325321  [   56/   89]
Per-example loss in batch: 0.268677  [   58/   89]
Per-example loss in batch: 0.320480  [   60/   89]
Per-example loss in batch: 0.376171  [   62/   89]
Per-example loss in batch: 0.355598  [   64/   89]
Per-example loss in batch: 0.421484  [   66/   89]
Per-example loss in batch: 0.412880  [   68/   89]
Per-example loss in batch: 0.415543  [   70/   89]
Per-example loss in batch: 0.303429  [   72/   89]
Per-example loss in batch: 0.305276  [   74/   89]
Per-example loss in batch: 0.373184  [   76/   89]
Per-example loss in batch: 0.403428  [   78/   89]
Per-example loss in batch: 0.342158  [   80/   89]
Per-example loss in batch: 0.396117  [   82/   89]
Per-example loss in batch: 0.363269  [   84/   89]
Per-example loss in batch: 0.450806  [   86/   89]
Per-example loss in batch: 0.431483  [   88/   89]
Per-example loss in batch: 0.524111  [   89/   89]
Train Error: Avg loss: 0.37418506
validation Error: 
 Avg loss: 0.50208001 
 F1: 0.263324 
 Precision: 0.815074 
 Recall: 0.157027
 IoU: 0.151625

test Error: 
 Avg loss: 0.48187430 
 F1: 0.271967 
 Precision: 0.827372 
 Recall: 0.162729
 IoU: 0.157385

We have finished training iteration 78
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_76_.pth
Per-example loss in batch: 0.327235  [    2/   89]
Per-example loss in batch: 0.404249  [    4/   89]
Per-example loss in batch: 0.352685  [    6/   89]
Per-example loss in batch: 0.404076  [    8/   89]
Per-example loss in batch: 0.382536  [   10/   89]
Per-example loss in batch: 0.356370  [   12/   89]
Per-example loss in batch: 0.300843  [   14/   89]
Per-example loss in batch: 0.429320  [   16/   89]
Per-example loss in batch: 0.321384  [   18/   89]
Per-example loss in batch: 0.409103  [   20/   89]
Per-example loss in batch: 0.385452  [   22/   89]
Per-example loss in batch: 0.356214  [   24/   89]
Per-example loss in batch: 0.286874  [   26/   89]
Per-example loss in batch: 0.284335  [   28/   89]
Per-example loss in batch: 0.395208  [   30/   89]
Per-example loss in batch: 0.294784  [   32/   89]
Per-example loss in batch: 0.426455  [   34/   89]
Per-example loss in batch: 0.381444  [   36/   89]
Per-example loss in batch: 0.394895  [   38/   89]
Per-example loss in batch: 0.438716  [   40/   89]
Per-example loss in batch: 0.356044  [   42/   89]
Per-example loss in batch: 0.403951  [   44/   89]
Per-example loss in batch: 0.423924  [   46/   89]
Per-example loss in batch: 0.355192  [   48/   89]
Per-example loss in batch: 0.394581  [   50/   89]
Per-example loss in batch: 0.383646  [   52/   89]
Per-example loss in batch: 0.413778  [   54/   89]
Per-example loss in batch: 0.408662  [   56/   89]
Per-example loss in batch: 0.363499  [   58/   89]
Per-example loss in batch: 0.374658  [   60/   89]
Per-example loss in batch: 0.455451  [   62/   89]
Per-example loss in batch: 0.448991  [   64/   89]
Per-example loss in batch: 0.322015  [   66/   89]
Per-example loss in batch: 0.261280  [   68/   89]
Per-example loss in batch: 0.259469  [   70/   89]
Per-example loss in batch: 0.342952  [   72/   89]
Per-example loss in batch: 0.311051  [   74/   89]
Per-example loss in batch: 0.246442  [   76/   89]
Per-example loss in batch: 0.378692  [   78/   89]
Per-example loss in batch: 0.396227  [   80/   89]
Per-example loss in batch: 0.394916  [   82/   89]
Per-example loss in batch: 0.308396  [   84/   89]
Per-example loss in batch: 0.408082  [   86/   89]
Per-example loss in batch: 0.415677  [   88/   89]
Per-example loss in batch: 0.631017  [   89/   89]
Train Error: Avg loss: 0.37023060
validation Error: 
 Avg loss: 0.50419897 
 F1: 0.277603 
 Precision: 0.792527 
 Recall: 0.168272
 IoU: 0.161172

test Error: 
 Avg loss: 0.48416054 
 F1: 0.286491 
 Precision: 0.796697 
 Recall: 0.174647
 IoU: 0.167195

We have finished training iteration 79
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_77_.pth
Per-example loss in batch: 0.341696  [    2/   89]
Per-example loss in batch: 0.416235  [    4/   89]
Per-example loss in batch: 0.395428  [    6/   89]
Per-example loss in batch: 0.394639  [    8/   89]
Per-example loss in batch: 0.311658  [   10/   89]
Per-example loss in batch: 0.386194  [   12/   89]
Per-example loss in batch: 0.385489  [   14/   89]
Per-example loss in batch: 0.329522  [   16/   89]
Per-example loss in batch: 0.337388  [   18/   89]
Per-example loss in batch: 0.355327  [   20/   89]
Per-example loss in batch: 0.317172  [   22/   89]
Per-example loss in batch: 0.376004  [   24/   89]
Per-example loss in batch: 0.407910  [   26/   89]
Per-example loss in batch: 0.430074  [   28/   89]
Per-example loss in batch: 0.372117  [   30/   89]
Per-example loss in batch: 0.384585  [   32/   89]
Per-example loss in batch: 0.315093  [   34/   89]
Per-example loss in batch: 0.300672  [   36/   89]
Per-example loss in batch: 0.431816  [   38/   89]
Per-example loss in batch: 0.337157  [   40/   89]
Per-example loss in batch: 0.307281  [   42/   89]
Per-example loss in batch: 0.304739  [   44/   89]
Per-example loss in batch: 0.303234  [   46/   89]
Per-example loss in batch: 0.385889  [   48/   89]
Per-example loss in batch: 0.360816  [   50/   89]
Per-example loss in batch: 0.325638  [   52/   89]
Per-example loss in batch: 0.328708  [   54/   89]
Per-example loss in batch: 0.278480  [   56/   89]
Per-example loss in batch: 0.328944  [   58/   89]
Per-example loss in batch: 0.395129  [   60/   89]
Per-example loss in batch: 0.349555  [   62/   89]
Per-example loss in batch: 0.314461  [   64/   89]
Per-example loss in batch: 0.316091  [   66/   89]
Per-example loss in batch: 0.346055  [   68/   89]
Per-example loss in batch: 0.418071  [   70/   89]
Per-example loss in batch: 0.370079  [   72/   89]
Per-example loss in batch: 0.314285  [   74/   89]
Per-example loss in batch: 0.326948  [   76/   89]
Per-example loss in batch: 0.362656  [   78/   89]
Per-example loss in batch: 0.366822  [   80/   89]
Per-example loss in batch: 0.410270  [   82/   89]
Per-example loss in batch: 0.316360  [   84/   89]
Per-example loss in batch: 0.332570  [   86/   89]
Per-example loss in batch: 0.408684  [   88/   89]
Per-example loss in batch: 0.832711  [   89/   89]
Train Error: Avg loss: 0.35987181
validation Error: 
 Avg loss: 0.50293697 
 F1: 0.288728 
 Precision: 0.767641 
 Recall: 0.177801
 IoU: 0.168721

test Error: 
 Avg loss: 0.48250320 
 F1: 0.302872 
 Precision: 0.786156 
 Recall: 0.187567
 IoU: 0.178461

We have finished training iteration 80
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_78_.pth
Per-example loss in batch: 0.391594  [    2/   89]
Per-example loss in batch: 0.318723  [    4/   89]
Per-example loss in batch: 0.294927  [    6/   89]
Per-example loss in batch: 0.332563  [    8/   89]
Per-example loss in batch: 0.339170  [   10/   89]
Per-example loss in batch: 0.344628  [   12/   89]
Per-example loss in batch: 0.407763  [   14/   89]
Per-example loss in batch: 0.411057  [   16/   89]
Per-example loss in batch: 0.409257  [   18/   89]
Per-example loss in batch: 0.333283  [   20/   89]
Per-example loss in batch: 0.407213  [   22/   89]
Per-example loss in batch: 0.327485  [   24/   89]
Per-example loss in batch: 0.225192  [   26/   89]
Per-example loss in batch: 0.300098  [   28/   89]
Per-example loss in batch: 0.303257  [   30/   89]
Per-example loss in batch: 0.313997  [   32/   89]
Per-example loss in batch: 0.330344  [   34/   89]
Per-example loss in batch: 0.318027  [   36/   89]
Per-example loss in batch: 0.359700  [   38/   89]
Per-example loss in batch: 0.288225  [   40/   89]
Per-example loss in batch: 0.419495  [   42/   89]
Per-example loss in batch: 0.408195  [   44/   89]
Per-example loss in batch: 0.252991  [   46/   89]
Per-example loss in batch: 0.391718  [   48/   89]
Per-example loss in batch: 0.297788  [   50/   89]
Per-example loss in batch: 0.329189  [   52/   89]
Per-example loss in batch: 0.342911  [   54/   89]
Per-example loss in batch: 0.387782  [   56/   89]
Per-example loss in batch: 0.376469  [   58/   89]
Per-example loss in batch: 0.380919  [   60/   89]
Per-example loss in batch: 0.293160  [   62/   89]
Per-example loss in batch: 0.356533  [   64/   89]
Per-example loss in batch: 0.317823  [   66/   89]
Per-example loss in batch: 0.342991  [   68/   89]
Per-example loss in batch: 0.398814  [   70/   89]
Per-example loss in batch: 0.318674  [   72/   89]
Per-example loss in batch: 0.379025  [   74/   89]
Per-example loss in batch: 0.364564  [   76/   89]
Per-example loss in batch: 0.367179  [   78/   89]
Per-example loss in batch: 0.371039  [   80/   89]
Per-example loss in batch: 0.349441  [   82/   89]
Per-example loss in batch: 0.382253  [   84/   89]
Per-example loss in batch: 0.329296  [   86/   89]
Per-example loss in batch: 0.277694  [   88/   89]
Per-example loss in batch: 0.774738  [   89/   89]
Train Error: Avg loss: 0.35010814
validation Error: 
 Avg loss: 0.50536470 
 F1: 0.312461 
 Precision: 0.712171 
 Recall: 0.200134
 IoU: 0.185158

test Error: 
 Avg loss: 0.48540146 
 F1: 0.327938 
 Precision: 0.723972 
 Recall: 0.211979
 IoU: 0.196128

We have finished training iteration 81
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_79_.pth
Per-example loss in batch: 0.312820  [    2/   89]
Per-example loss in batch: 0.324281  [    4/   89]
Per-example loss in batch: 0.266717  [    6/   89]
Per-example loss in batch: 0.350976  [    8/   89]
Per-example loss in batch: 0.386390  [   10/   89]
Per-example loss in batch: 0.355393  [   12/   89]
Per-example loss in batch: 0.269949  [   14/   89]
Per-example loss in batch: 0.304217  [   16/   89]
Per-example loss in batch: 0.331806  [   18/   89]
Per-example loss in batch: 0.249181  [   20/   89]
Per-example loss in batch: 0.359311  [   22/   89]
Per-example loss in batch: 0.371503  [   24/   89]
Per-example loss in batch: 0.331447  [   26/   89]
Per-example loss in batch: 0.312519  [   28/   89]
Per-example loss in batch: 0.271492  [   30/   89]
Per-example loss in batch: 0.400212  [   32/   89]
Per-example loss in batch: 0.359374  [   34/   89]
Per-example loss in batch: 0.368006  [   36/   89]
Per-example loss in batch: 0.286031  [   38/   89]
Per-example loss in batch: 0.262154  [   40/   89]
Per-example loss in batch: 0.305465  [   42/   89]
Per-example loss in batch: 0.273074  [   44/   89]
Per-example loss in batch: 0.290058  [   46/   89]
Per-example loss in batch: 0.368579  [   48/   89]
Per-example loss in batch: 0.388152  [   50/   89]
Per-example loss in batch: 0.378410  [   52/   89]
Per-example loss in batch: 0.304346  [   54/   89]
Per-example loss in batch: 0.377454  [   56/   89]
Per-example loss in batch: 0.384813  [   58/   89]
Per-example loss in batch: 0.276085  [   60/   89]
Per-example loss in batch: 0.358194  [   62/   89]
Per-example loss in batch: 0.394653  [   64/   89]
Per-example loss in batch: 0.314676  [   66/   89]
Per-example loss in batch: 0.378288  [   68/   89]
Per-example loss in batch: 0.337861  [   70/   89]
Per-example loss in batch: 0.377800  [   72/   89]
Per-example loss in batch: 0.286227  [   74/   89]
Per-example loss in batch: 0.355891  [   76/   89]
Per-example loss in batch: 0.367394  [   78/   89]
Per-example loss in batch: 0.298961  [   80/   89]
Per-example loss in batch: 0.355416  [   82/   89]
Per-example loss in batch: 0.387718  [   84/   89]
Per-example loss in batch: 0.379548  [   86/   89]
Per-example loss in batch: 0.346491  [   88/   89]
Per-example loss in batch: 0.869250  [   89/   89]
Train Error: Avg loss: 0.34143723
validation Error: 
 Avg loss: 0.50619983 
 F1: 0.346097 
 Precision: 0.625496 
 Recall: 0.239234
 IoU: 0.209261

test Error: 
 Avg loss: 0.48726401 
 F1: 0.365190 
 Precision: 0.642240 
 Recall: 0.255131
 IoU: 0.223384

We have finished training iteration 82
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_80_.pth
Per-example loss in batch: 0.338238  [    2/   89]
Per-example loss in batch: 0.233583  [    4/   89]
Per-example loss in batch: 0.342853  [    6/   89]
Per-example loss in batch: 0.244291  [    8/   89]
Per-example loss in batch: 0.314346  [   10/   89]
Per-example loss in batch: 0.388286  [   12/   89]
Per-example loss in batch: 0.304284  [   14/   89]
Per-example loss in batch: 0.262824  [   16/   89]
Per-example loss in batch: 0.347374  [   18/   89]
Per-example loss in batch: 0.259685  [   20/   89]
Per-example loss in batch: 0.334849  [   22/   89]
Per-example loss in batch: 0.390326  [   24/   89]
Per-example loss in batch: 0.317659  [   26/   89]
Per-example loss in batch: 0.389084  [   28/   89]
Per-example loss in batch: 0.336672  [   30/   89]
Per-example loss in batch: 0.375947  [   32/   89]
Per-example loss in batch: 0.346891  [   34/   89]
Per-example loss in batch: 0.374497  [   36/   89]
Per-example loss in batch: 0.295170  [   38/   89]
Per-example loss in batch: 0.395423  [   40/   89]
Per-example loss in batch: 0.248235  [   42/   89]
Per-example loss in batch: 0.357884  [   44/   89]
Per-example loss in batch: 0.370647  [   46/   89]
Per-example loss in batch: 0.332336  [   48/   89]
Per-example loss in batch: 0.299134  [   50/   89]
Per-example loss in batch: 0.408983  [   52/   89]
Per-example loss in batch: 0.313717  [   54/   89]
Per-example loss in batch: 0.287356  [   56/   89]
Per-example loss in batch: 0.246212  [   58/   89]
Per-example loss in batch: 0.285015  [   60/   89]
Per-example loss in batch: 0.378455  [   62/   89]
Per-example loss in batch: 0.402649  [   64/   89]
Per-example loss in batch: 0.257857  [   66/   89]
Per-example loss in batch: 0.316777  [   68/   89]
Per-example loss in batch: 0.415325  [   70/   89]
Per-example loss in batch: 0.408432  [   72/   89]
Per-example loss in batch: 0.344935  [   74/   89]
Per-example loss in batch: 0.327519  [   76/   89]
Per-example loss in batch: 0.309424  [   78/   89]
Per-example loss in batch: 0.407243  [   80/   89]
Per-example loss in batch: 0.266291  [   82/   89]
Per-example loss in batch: 0.286292  [   84/   89]
Per-example loss in batch: 0.325495  [   86/   89]
Per-example loss in batch: 0.256451  [   88/   89]
Per-example loss in batch: 0.826480  [   89/   89]
Train Error: Avg loss: 0.33389190
validation Error: 
 Avg loss: 0.50667239 
 F1: 0.360890 
 Precision: 0.625785 
 Recall: 0.253558
 IoU: 0.220174

test Error: 
 Avg loss: 0.48671837 
 F1: 0.385866 
 Precision: 0.656720 
 Recall: 0.273192
 IoU: 0.239054

We have finished training iteration 83
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_81_.pth
Per-example loss in batch: 0.358679  [    2/   89]
Per-example loss in batch: 0.354943  [    4/   89]
Per-example loss in batch: 0.279812  [    6/   89]
Per-example loss in batch: 0.304434  [    8/   89]
Per-example loss in batch: 0.336634  [   10/   89]
Per-example loss in batch: 0.371000  [   12/   89]
Per-example loss in batch: 0.370769  [   14/   89]
Per-example loss in batch: 0.274960  [   16/   89]
Per-example loss in batch: 0.330362  [   18/   89]
Per-example loss in batch: 0.314428  [   20/   89]
Per-example loss in batch: 0.299763  [   22/   89]
Per-example loss in batch: 0.286406  [   24/   89]
Per-example loss in batch: 0.387093  [   26/   89]
Per-example loss in batch: 0.369642  [   28/   89]
Per-example loss in batch: 0.256188  [   30/   89]
Per-example loss in batch: 0.339916  [   32/   89]
Per-example loss in batch: 0.340284  [   34/   89]
Per-example loss in batch: 0.362971  [   36/   89]
Per-example loss in batch: 0.275060  [   38/   89]
Per-example loss in batch: 0.300360  [   40/   89]
Per-example loss in batch: 0.351988  [   42/   89]
Per-example loss in batch: 0.377059  [   44/   89]
Per-example loss in batch: 0.262516  [   46/   89]
Per-example loss in batch: 0.288228  [   48/   89]
Per-example loss in batch: 0.323016  [   50/   89]
Per-example loss in batch: 0.383612  [   52/   89]
Per-example loss in batch: 0.376630  [   54/   89]
Per-example loss in batch: 0.387989  [   56/   89]
Per-example loss in batch: 0.317852  [   58/   89]
Per-example loss in batch: 0.297213  [   60/   89]
Per-example loss in batch: 0.286721  [   62/   89]
Per-example loss in batch: 0.347857  [   64/   89]
Per-example loss in batch: 0.252733  [   66/   89]
Per-example loss in batch: 0.369659  [   68/   89]
Per-example loss in batch: 0.266366  [   70/   89]
Per-example loss in batch: 0.304330  [   72/   89]
Per-example loss in batch: 0.333748  [   74/   89]
Per-example loss in batch: 0.319353  [   76/   89]
Per-example loss in batch: 0.328061  [   78/   89]
Per-example loss in batch: 0.377511  [   80/   89]
Per-example loss in batch: 0.252523  [   82/   89]
Per-example loss in batch: 0.283433  [   84/   89]
Per-example loss in batch: 0.246395  [   86/   89]
Per-example loss in batch: 0.228073  [   88/   89]
Per-example loss in batch: 0.583702  [   89/   89]
Train Error: Avg loss: 0.32288590
validation Error: 
 Avg loss: 0.50781945 
 F1: 0.375447 
 Precision: 0.569029 
 Recall: 0.280143
 IoU: 0.231108

test Error: 
 Avg loss: 0.48811847 
 F1: 0.404320 
 Precision: 0.599057 
 Recall: 0.305131
 IoU: 0.253384

We have finished training iteration 84
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_82_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.311761  [    2/   89]
Per-example loss in batch: 0.263054  [    4/   89]
Per-example loss in batch: 0.282017  [    6/   89]
Per-example loss in batch: 0.255020  [    8/   89]
Per-example loss in batch: 0.357206  [   10/   89]
Per-example loss in batch: 0.264570  [   12/   89]
Per-example loss in batch: 0.330081  [   14/   89]
Per-example loss in batch: 0.403186  [   16/   89]
Per-example loss in batch: 0.347855  [   18/   89]
Per-example loss in batch: 0.247042  [   20/   89]
Per-example loss in batch: 0.290269  [   22/   89]
Per-example loss in batch: 0.318504  [   24/   89]
Per-example loss in batch: 0.254162  [   26/   89]
Per-example loss in batch: 0.302883  [   28/   89]
Per-example loss in batch: 0.240140  [   30/   89]
Per-example loss in batch: 0.315952  [   32/   89]
Per-example loss in batch: 0.269706  [   34/   89]
Per-example loss in batch: 0.336378  [   36/   89]
Per-example loss in batch: 0.335368  [   38/   89]
Per-example loss in batch: 0.350648  [   40/   89]
Per-example loss in batch: 0.386496  [   42/   89]
Per-example loss in batch: 0.327044  [   44/   89]
Per-example loss in batch: 0.239790  [   46/   89]
Per-example loss in batch: 0.284310  [   48/   89]
Per-example loss in batch: 0.309936  [   50/   89]
Per-example loss in batch: 0.265331  [   52/   89]
Per-example loss in batch: 0.395057  [   54/   89]
Per-example loss in batch: 0.350228  [   56/   89]
Per-example loss in batch: 0.382847  [   58/   89]
Per-example loss in batch: 0.281065  [   60/   89]
Per-example loss in batch: 0.287920  [   62/   89]
Per-example loss in batch: 0.331166  [   64/   89]
Per-example loss in batch: 0.348611  [   66/   89]
Per-example loss in batch: 0.322029  [   68/   89]
Per-example loss in batch: 0.376488  [   70/   89]
Per-example loss in batch: 0.293308  [   72/   89]
Per-example loss in batch: 0.318729  [   74/   89]
Per-example loss in batch: 0.385657  [   76/   89]
Per-example loss in batch: 0.373588  [   78/   89]
Per-example loss in batch: 0.338005  [   80/   89]
Per-example loss in batch: 0.344146  [   82/   89]
Per-example loss in batch: 0.310103  [   84/   89]
Per-example loss in batch: 0.354227  [   86/   89]
Per-example loss in batch: 0.349301  [   88/   89]
Per-example loss in batch: 0.762091  [   89/   89]
Train Error: Avg loss: 0.32387032
validation Error: 
 Avg loss: 0.50765607 
 F1: 0.369356 
 Precision: 0.604785 
 Recall: 0.265862
 IoU: 0.226509

test Error: 
 Avg loss: 0.48759729 
 F1: 0.395927 
 Precision: 0.635555 
 Recall: 0.287520
 IoU: 0.246826

We have finished training iteration 85
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_83_.pth
Per-example loss in batch: 0.356499  [    2/   89]
Per-example loss in batch: 0.267180  [    4/   89]
Per-example loss in batch: 0.332092  [    6/   89]
Per-example loss in batch: 0.259519  [    8/   89]
Per-example loss in batch: 0.257894  [   10/   89]
Per-example loss in batch: 0.259199  [   12/   89]
Per-example loss in batch: 0.355864  [   14/   89]
Per-example loss in batch: 0.359319  [   16/   89]
Per-example loss in batch: 0.368071  [   18/   89]
Per-example loss in batch: 0.345299  [   20/   89]
Per-example loss in batch: 0.283435  [   22/   89]
Per-example loss in batch: 0.324868  [   24/   89]
Per-example loss in batch: 0.242898  [   26/   89]
Per-example loss in batch: 0.245361  [   28/   89]
Per-example loss in batch: 0.351762  [   30/   89]
Per-example loss in batch: 0.335919  [   32/   89]
Per-example loss in batch: 0.313188  [   34/   89]
Per-example loss in batch: 0.348277  [   36/   89]
Per-example loss in batch: 0.380836  [   38/   89]
Per-example loss in batch: 0.255367  [   40/   89]
Per-example loss in batch: 0.260660  [   42/   89]
Per-example loss in batch: 0.295523  [   44/   89]
Per-example loss in batch: 0.348420  [   46/   89]
Per-example loss in batch: 0.360395  [   48/   89]
Per-example loss in batch: 0.349594  [   50/   89]
Per-example loss in batch: 0.222072  [   52/   89]
Per-example loss in batch: 0.319043  [   54/   89]
Per-example loss in batch: 0.293946  [   56/   89]
Per-example loss in batch: 0.299942  [   58/   89]
Per-example loss in batch: 0.257666  [   60/   89]
Per-example loss in batch: 0.386692  [   62/   89]
Per-example loss in batch: 0.331207  [   64/   89]
Per-example loss in batch: 0.305268  [   66/   89]
Per-example loss in batch: 0.255513  [   68/   89]
Per-example loss in batch: 0.352811  [   70/   89]
Per-example loss in batch: 0.308622  [   72/   89]
Per-example loss in batch: 0.319277  [   74/   89]
Per-example loss in batch: 0.261355  [   76/   89]
Per-example loss in batch: 0.349677  [   78/   89]
Per-example loss in batch: 0.268456  [   80/   89]
Per-example loss in batch: 0.293340  [   82/   89]
Per-example loss in batch: 0.322398  [   84/   89]
Per-example loss in batch: 0.312473  [   86/   89]
Per-example loss in batch: 0.260496  [   88/   89]
Per-example loss in batch: 0.763994  [   89/   89]
Train Error: Avg loss: 0.31370094
validation Error: 
 Avg loss: 0.50758100 
 F1: 0.378185 
 Precision: 0.579400 
 Recall: 0.280702
 IoU: 0.233186

test Error: 
 Avg loss: 0.48827093 
 F1: 0.405338 
 Precision: 0.608898 
 Recall: 0.303782
 IoU: 0.254185

We have finished training iteration 86
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_84_.pth
Per-example loss in batch: 0.274523  [    2/   89]
Per-example loss in batch: 0.297991  [    4/   89]
Per-example loss in batch: 0.397169  [    6/   89]
Per-example loss in batch: 0.270971  [    8/   89]
Per-example loss in batch: 0.302581  [   10/   89]
Per-example loss in batch: 0.331048  [   12/   89]
Per-example loss in batch: 0.268436  [   14/   89]
Per-example loss in batch: 0.331805  [   16/   89]
Per-example loss in batch: 0.292798  [   18/   89]
Per-example loss in batch: 0.356103  [   20/   89]
Per-example loss in batch: 0.276893  [   22/   89]
Per-example loss in batch: 0.324443  [   24/   89]
Per-example loss in batch: 0.345926  [   26/   89]
Per-example loss in batch: 0.361576  [   28/   89]
Per-example loss in batch: 0.241134  [   30/   89]
Per-example loss in batch: 0.243294  [   32/   89]
Per-example loss in batch: 0.315708  [   34/   89]
Per-example loss in batch: 0.393201  [   36/   89]
Per-example loss in batch: 0.255721  [   38/   89]
Per-example loss in batch: 0.361382  [   40/   89]
Per-example loss in batch: 0.271845  [   42/   89]
Per-example loss in batch: 0.291510  [   44/   89]
Per-example loss in batch: 0.384954  [   46/   89]
Per-example loss in batch: 0.370841  [   48/   89]
Per-example loss in batch: 0.287946  [   50/   89]
Per-example loss in batch: 0.326557  [   52/   89]
Per-example loss in batch: 0.310832  [   54/   89]
Per-example loss in batch: 0.247762  [   56/   89]
Per-example loss in batch: 0.251814  [   58/   89]
Per-example loss in batch: 0.363346  [   60/   89]
Per-example loss in batch: 0.264332  [   62/   89]
Per-example loss in batch: 0.277284  [   64/   89]
Per-example loss in batch: 0.333693  [   66/   89]
Per-example loss in batch: 0.315237  [   68/   89]
Per-example loss in batch: 0.308934  [   70/   89]
Per-example loss in batch: 0.380297  [   72/   89]
Per-example loss in batch: 0.411057  [   74/   89]
Per-example loss in batch: 0.355370  [   76/   89]
Per-example loss in batch: 0.248624  [   78/   89]
Per-example loss in batch: 0.319794  [   80/   89]
Per-example loss in batch: 0.268840  [   82/   89]
Per-example loss in batch: 0.342939  [   84/   89]
Per-example loss in batch: 0.261871  [   86/   89]
Per-example loss in batch: 0.368845  [   88/   89]
Per-example loss in batch: 0.527539  [   89/   89]
Train Error: Avg loss: 0.31620214
validation Error: 
 Avg loss: 0.50809670 
 F1: 0.381804 
 Precision: 0.583728 
 Recall: 0.283675
 IoU: 0.235944

test Error: 
 Avg loss: 0.48820128 
 F1: 0.409740 
 Precision: 0.620489 
 Recall: 0.305856
 IoU: 0.257656

We have finished training iteration 87
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_85_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.352523  [    2/   89]
Per-example loss in batch: 0.286697  [    4/   89]
Per-example loss in batch: 0.292686  [    6/   89]
Per-example loss in batch: 0.301190  [    8/   89]
Per-example loss in batch: 0.245719  [   10/   89]
Per-example loss in batch: 0.326287  [   12/   89]
Per-example loss in batch: 0.333202  [   14/   89]
Per-example loss in batch: 0.264561  [   16/   89]
Per-example loss in batch: 0.383988  [   18/   89]
Per-example loss in batch: 0.343932  [   20/   89]
Per-example loss in batch: 0.273048  [   22/   89]
Per-example loss in batch: 0.347572  [   24/   89]
Per-example loss in batch: 0.286882  [   26/   89]
Per-example loss in batch: 0.254538  [   28/   89]
Per-example loss in batch: 0.363302  [   30/   89]
Per-example loss in batch: 0.293938  [   32/   89]
Per-example loss in batch: 0.313763  [   34/   89]
Per-example loss in batch: 0.295077  [   36/   89]
Per-example loss in batch: 0.303964  [   38/   89]
Per-example loss in batch: 0.339653  [   40/   89]
Per-example loss in batch: 0.334950  [   42/   89]
Per-example loss in batch: 0.325377  [   44/   89]
Per-example loss in batch: 0.358285  [   46/   89]
Per-example loss in batch: 0.313680  [   48/   89]
Per-example loss in batch: 0.391396  [   50/   89]
Per-example loss in batch: 0.320930  [   52/   89]
Per-example loss in batch: 0.359479  [   54/   89]
Per-example loss in batch: 0.317252  [   56/   89]
Per-example loss in batch: 0.327179  [   58/   89]
Per-example loss in batch: 0.271243  [   60/   89]
Per-example loss in batch: 0.354314  [   62/   89]
Per-example loss in batch: 0.313813  [   64/   89]
Per-example loss in batch: 0.257512  [   66/   89]
Per-example loss in batch: 0.312645  [   68/   89]
Per-example loss in batch: 0.316879  [   70/   89]
Per-example loss in batch: 0.265387  [   72/   89]
Per-example loss in batch: 0.298209  [   74/   89]
Per-example loss in batch: 0.310440  [   76/   89]
Per-example loss in batch: 0.234840  [   78/   89]
Per-example loss in batch: 0.244700  [   80/   89]
Per-example loss in batch: 0.328571  [   82/   89]
Per-example loss in batch: 0.253762  [   84/   89]
Per-example loss in batch: 0.255752  [   86/   89]
Per-example loss in batch: 0.310766  [   88/   89]
Per-example loss in batch: 0.475031  [   89/   89]
Train Error: Avg loss: 0.31050342
validation Error: 
 Avg loss: 0.50875213 
 F1: 0.386437 
 Precision: 0.542014 
 Recall: 0.300254
 IoU: 0.239493

test Error: 
 Avg loss: 0.48894233 
 F1: 0.417643 
 Precision: 0.584554 
 Recall: 0.324879
 IoU: 0.263938

We have finished training iteration 88
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_86_.pth
Per-example loss in batch: 0.402154  [    2/   89]
Per-example loss in batch: 0.313488  [    4/   89]
Per-example loss in batch: 0.375123  [    6/   89]
Per-example loss in batch: 0.287326  [    8/   89]
Per-example loss in batch: 0.259503  [   10/   89]
Per-example loss in batch: 0.311725  [   12/   89]
Per-example loss in batch: 0.229125  [   14/   89]
Per-example loss in batch: 0.303279  [   16/   89]
Per-example loss in batch: 0.366177  [   18/   89]
Per-example loss in batch: 0.243790  [   20/   89]
Per-example loss in batch: 0.400448  [   22/   89]
Per-example loss in batch: 0.330999  [   24/   89]
Per-example loss in batch: 0.315282  [   26/   89]
Per-example loss in batch: 0.333017  [   28/   89]
Per-example loss in batch: 0.285949  [   30/   89]
Per-example loss in batch: 0.321481  [   32/   89]
Per-example loss in batch: 0.327129  [   34/   89]
Per-example loss in batch: 0.226777  [   36/   89]
Per-example loss in batch: 0.360563  [   38/   89]
Per-example loss in batch: 0.377753  [   40/   89]
Per-example loss in batch: 0.310081  [   42/   89]
Per-example loss in batch: 0.338512  [   44/   89]
Per-example loss in batch: 0.350645  [   46/   89]
Per-example loss in batch: 0.276724  [   48/   89]
Per-example loss in batch: 0.320993  [   50/   89]
Per-example loss in batch: 0.272594  [   52/   89]
Per-example loss in batch: 0.225143  [   54/   89]
Per-example loss in batch: 0.288771  [   56/   89]
Per-example loss in batch: 0.266065  [   58/   89]
Per-example loss in batch: 0.237030  [   60/   89]
Per-example loss in batch: 0.277597  [   62/   89]
Per-example loss in batch: 0.389634  [   64/   89]
Per-example loss in batch: 0.328396  [   66/   89]
Per-example loss in batch: 0.300604  [   68/   89]
Per-example loss in batch: 0.319039  [   70/   89]
Per-example loss in batch: 0.279345  [   72/   89]
Per-example loss in batch: 0.301851  [   74/   89]
Per-example loss in batch: 0.339776  [   76/   89]
Per-example loss in batch: 0.250695  [   78/   89]
Per-example loss in batch: 0.296770  [   80/   89]
Per-example loss in batch: 0.383572  [   82/   89]
Per-example loss in batch: 0.353170  [   84/   89]
Per-example loss in batch: 0.313926  [   86/   89]
Per-example loss in batch: 0.332359  [   88/   89]
Per-example loss in batch: 0.521921  [   89/   89]
Train Error: Avg loss: 0.31427730
validation Error: 
 Avg loss: 0.50807012 
 F1: 0.384008 
 Precision: 0.596610 
 Recall: 0.283118
 IoU: 0.237630

test Error: 
 Avg loss: 0.48836879 
 F1: 0.408028 
 Precision: 0.634505 
 Recall: 0.300698
 IoU: 0.256303

We have finished training iteration 89
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_87_.pth
Per-example loss in batch: 0.342858  [    2/   89]
Per-example loss in batch: 0.302337  [    4/   89]
Per-example loss in batch: 0.268160  [    6/   89]
Per-example loss in batch: 0.341372  [    8/   89]
Per-example loss in batch: 0.307618  [   10/   89]
Per-example loss in batch: 0.266169  [   12/   89]
Per-example loss in batch: 0.271405  [   14/   89]
Per-example loss in batch: 0.236632  [   16/   89]
Per-example loss in batch: 0.234917  [   18/   89]
Per-example loss in batch: 0.327058  [   20/   89]
Per-example loss in batch: 0.216799  [   22/   89]
Per-example loss in batch: 0.373359  [   24/   89]
Per-example loss in batch: 0.315748  [   26/   89]
Per-example loss in batch: 0.286259  [   28/   89]
Per-example loss in batch: 0.310965  [   30/   89]
Per-example loss in batch: 0.343979  [   32/   89]
Per-example loss in batch: 0.354274  [   34/   89]
Per-example loss in batch: 0.247591  [   36/   89]
Per-example loss in batch: 0.261658  [   38/   89]
Per-example loss in batch: 0.280990  [   40/   89]
Per-example loss in batch: 0.365846  [   42/   89]
Per-example loss in batch: 0.378959  [   44/   89]
Per-example loss in batch: 0.322973  [   46/   89]
Per-example loss in batch: 0.297676  [   48/   89]
Per-example loss in batch: 0.252837  [   50/   89]
Per-example loss in batch: 0.358635  [   52/   89]
Per-example loss in batch: 0.425909  [   54/   89]
Per-example loss in batch: 0.336799  [   56/   89]
Per-example loss in batch: 0.364053  [   58/   89]
Per-example loss in batch: 0.329013  [   60/   89]
Per-example loss in batch: 0.299797  [   62/   89]
Per-example loss in batch: 0.338627  [   64/   89]
Per-example loss in batch: 0.289628  [   66/   89]
Per-example loss in batch: 0.320203  [   68/   89]
Per-example loss in batch: 0.323870  [   70/   89]
Per-example loss in batch: 0.285509  [   72/   89]
Per-example loss in batch: 0.299713  [   74/   89]
Per-example loss in batch: 0.273856  [   76/   89]
Per-example loss in batch: 0.292693  [   78/   89]
Per-example loss in batch: 0.223753  [   80/   89]
Per-example loss in batch: 0.324158  [   82/   89]
Per-example loss in batch: 0.328066  [   84/   89]
Per-example loss in batch: 0.383054  [   86/   89]
Per-example loss in batch: 0.350908  [   88/   89]
Per-example loss in batch: 0.725456  [   89/   89]
Train Error: Avg loss: 0.31504294
validation Error: 
 Avg loss: 0.50843853 
 F1: 0.384181 
 Precision: 0.574466 
 Recall: 0.288589
 IoU: 0.237762

test Error: 
 Avg loss: 0.48915012 
 F1: 0.410859 
 Precision: 0.605964 
 Recall: 0.310792
 IoU: 0.258542

We have finished training iteration 90
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_88_.pth
Per-example loss in batch: 0.259695  [    2/   89]
Per-example loss in batch: 0.290754  [    4/   89]
Per-example loss in batch: 0.378951  [    6/   89]
Per-example loss in batch: 0.293239  [    8/   89]
Per-example loss in batch: 0.280231  [   10/   89]
Per-example loss in batch: 0.320062  [   12/   89]
Per-example loss in batch: 0.375861  [   14/   89]
Per-example loss in batch: 0.370837  [   16/   89]
Per-example loss in batch: 0.352455  [   18/   89]
Per-example loss in batch: 0.361421  [   20/   89]
Per-example loss in batch: 0.373827  [   22/   89]
Per-example loss in batch: 0.293777  [   24/   89]
Per-example loss in batch: 0.262529  [   26/   89]
Per-example loss in batch: 0.293829  [   28/   89]
Per-example loss in batch: 0.257572  [   30/   89]
Per-example loss in batch: 0.274037  [   32/   89]
Per-example loss in batch: 0.250619  [   34/   89]
Per-example loss in batch: 0.279724  [   36/   89]
Per-example loss in batch: 0.322545  [   38/   89]
Per-example loss in batch: 0.324493  [   40/   89]
Per-example loss in batch: 0.351946  [   42/   89]
Per-example loss in batch: 0.277311  [   44/   89]
Per-example loss in batch: 0.345065  [   46/   89]
Per-example loss in batch: 0.246762  [   48/   89]
Per-example loss in batch: 0.286007  [   50/   89]
Per-example loss in batch: 0.226355  [   52/   89]
Per-example loss in batch: 0.386210  [   54/   89]
Per-example loss in batch: 0.353666  [   56/   89]
Per-example loss in batch: 0.273577  [   58/   89]
Per-example loss in batch: 0.349715  [   60/   89]
Per-example loss in batch: 0.280667  [   62/   89]
Per-example loss in batch: 0.238991  [   64/   89]
Per-example loss in batch: 0.248217  [   66/   89]
Per-example loss in batch: 0.339939  [   68/   89]
Per-example loss in batch: 0.291408  [   70/   89]
Per-example loss in batch: 0.335567  [   72/   89]
Per-example loss in batch: 0.333096  [   74/   89]
Per-example loss in batch: 0.340104  [   76/   89]
Per-example loss in batch: 0.364213  [   78/   89]
Per-example loss in batch: 0.305490  [   80/   89]
Per-example loss in batch: 0.289270  [   82/   89]
Per-example loss in batch: 0.224824  [   84/   89]
Per-example loss in batch: 0.328033  [   86/   89]
Per-example loss in batch: 0.311330  [   88/   89]
Per-example loss in batch: 0.515324  [   89/   89]
Train Error: Avg loss: 0.31015476
validation Error: 
 Avg loss: 0.50828284 
 F1: 0.387619 
 Precision: 0.581900 
 Recall: 0.290597
 IoU: 0.240402

test Error: 
 Avg loss: 0.48902175 
 F1: 0.415184 
 Precision: 0.618289 
 Recall: 0.312522
 IoU: 0.261976

We have finished training iteration 91
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_89_.pth
Per-example loss in batch: 0.252531  [    2/   89]
Per-example loss in batch: 0.350204  [    4/   89]
Per-example loss in batch: 0.409549  [    6/   89]
Per-example loss in batch: 0.341721  [    8/   89]
Per-example loss in batch: 0.259251  [   10/   89]
Per-example loss in batch: 0.316841  [   12/   89]
Per-example loss in batch: 0.257237  [   14/   89]
Per-example loss in batch: 0.299290  [   16/   89]
Per-example loss in batch: 0.238179  [   18/   89]
Per-example loss in batch: 0.277296  [   20/   89]
Per-example loss in batch: 0.368436  [   22/   89]
Per-example loss in batch: 0.359587  [   24/   89]
Per-example loss in batch: 0.311946  [   26/   89]
Per-example loss in batch: 0.277971  [   28/   89]
Per-example loss in batch: 0.355885  [   30/   89]
Per-example loss in batch: 0.365476  [   32/   89]
Per-example loss in batch: 0.235848  [   34/   89]
Per-example loss in batch: 0.255628  [   36/   89]
Per-example loss in batch: 0.347342  [   38/   89]
Per-example loss in batch: 0.283203  [   40/   89]
Per-example loss in batch: 0.332755  [   42/   89]
Per-example loss in batch: 0.327407  [   44/   89]
Per-example loss in batch: 0.256027  [   46/   89]
Per-example loss in batch: 0.241791  [   48/   89]
Per-example loss in batch: 0.298237  [   50/   89]
Per-example loss in batch: 0.229289  [   52/   89]
Per-example loss in batch: 0.298357  [   54/   89]
Per-example loss in batch: 0.325831  [   56/   89]
Per-example loss in batch: 0.357409  [   58/   89]
Per-example loss in batch: 0.295081  [   60/   89]
Per-example loss in batch: 0.282697  [   62/   89]
Per-example loss in batch: 0.282439  [   64/   89]
Per-example loss in batch: 0.362760  [   66/   89]
Per-example loss in batch: 0.312678  [   68/   89]
Per-example loss in batch: 0.266986  [   70/   89]
Per-example loss in batch: 0.293733  [   72/   89]
Per-example loss in batch: 0.257526  [   74/   89]
Per-example loss in batch: 0.239692  [   76/   89]
Per-example loss in batch: 0.254844  [   78/   89]
Per-example loss in batch: 0.356984  [   80/   89]
Per-example loss in batch: 0.341316  [   82/   89]
Per-example loss in batch: 0.348246  [   84/   89]
Per-example loss in batch: 0.259591  [   86/   89]
Per-example loss in batch: 0.346587  [   88/   89]
Per-example loss in batch: 0.807417  [   89/   89]
Train Error: Avg loss: 0.30866057
validation Error: 
 Avg loss: 0.50872688 
 F1: 0.382983 
 Precision: 0.587527 
 Recall: 0.284081
 IoU: 0.236845

test Error: 
 Avg loss: 0.48897429 
 F1: 0.409348 
 Precision: 0.617883 
 Recall: 0.306054
 IoU: 0.257346

We have finished training iteration 92
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_90_.pth
Per-example loss in batch: 0.260640  [    2/   89]
Per-example loss in batch: 0.328461  [    4/   89]
Per-example loss in batch: 0.332821  [    6/   89]
Per-example loss in batch: 0.268543  [    8/   89]
Per-example loss in batch: 0.352347  [   10/   89]
Per-example loss in batch: 0.264208  [   12/   89]
Per-example loss in batch: 0.275596  [   14/   89]
Per-example loss in batch: 0.288178  [   16/   89]
Per-example loss in batch: 0.285807  [   18/   89]
Per-example loss in batch: 0.371352  [   20/   89]
Per-example loss in batch: 0.302924  [   22/   89]
Per-example loss in batch: 0.257572  [   24/   89]
Per-example loss in batch: 0.388763  [   26/   89]
Per-example loss in batch: 0.241912  [   28/   89]
Per-example loss in batch: 0.312064  [   30/   89]
Per-example loss in batch: 0.314684  [   32/   89]
Per-example loss in batch: 0.307509  [   34/   89]
Per-example loss in batch: 0.279020  [   36/   89]
Per-example loss in batch: 0.305080  [   38/   89]
Per-example loss in batch: 0.230063  [   40/   89]
Per-example loss in batch: 0.364417  [   42/   89]
Per-example loss in batch: 0.378713  [   44/   89]
Per-example loss in batch: 0.263755  [   46/   89]
Per-example loss in batch: 0.289662  [   48/   89]
Per-example loss in batch: 0.251912  [   50/   89]
Per-example loss in batch: 0.279551  [   52/   89]
Per-example loss in batch: 0.327792  [   54/   89]
Per-example loss in batch: 0.331823  [   56/   89]
Per-example loss in batch: 0.235507  [   58/   89]
Per-example loss in batch: 0.251636  [   60/   89]
Per-example loss in batch: 0.251119  [   62/   89]
Per-example loss in batch: 0.364027  [   64/   89]
Per-example loss in batch: 0.377706  [   66/   89]
Per-example loss in batch: 0.339932  [   68/   89]
Per-example loss in batch: 0.238677  [   70/   89]
Per-example loss in batch: 0.353765  [   72/   89]
Per-example loss in batch: 0.296416  [   74/   89]
Per-example loss in batch: 0.242579  [   76/   89]
Per-example loss in batch: 0.348437  [   78/   89]
Per-example loss in batch: 0.320546  [   80/   89]
Per-example loss in batch: 0.301973  [   82/   89]
Per-example loss in batch: 0.343201  [   84/   89]
Per-example loss in batch: 0.337209  [   86/   89]
Per-example loss in batch: 0.263697  [   88/   89]
Per-example loss in batch: 0.695426  [   89/   89]
Train Error: Avg loss: 0.30717548
validation Error: 
 Avg loss: 0.50946533 
 F1: 0.390966 
 Precision: 0.557778 
 Recall: 0.300960
 IoU: 0.242982

test Error: 
 Avg loss: 0.48982379 
 F1: 0.418288 
 Precision: 0.592498 
 Recall: 0.323245
 IoU: 0.264453

We have finished training iteration 93
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_91_.pth
Per-example loss in batch: 0.324298  [    2/   89]
Per-example loss in batch: 0.356574  [    4/   89]
Per-example loss in batch: 0.321338  [    6/   89]
Per-example loss in batch: 0.271364  [    8/   89]
Per-example loss in batch: 0.275871  [   10/   89]
Per-example loss in batch: 0.332747  [   12/   89]
Per-example loss in batch: 0.296328  [   14/   89]
Per-example loss in batch: 0.277420  [   16/   89]
Per-example loss in batch: 0.269181  [   18/   89]
Per-example loss in batch: 0.339437  [   20/   89]
Per-example loss in batch: 0.392177  [   22/   89]
Per-example loss in batch: 0.289760  [   24/   89]
Per-example loss in batch: 0.261142  [   26/   89]
Per-example loss in batch: 0.281726  [   28/   89]
Per-example loss in batch: 0.308639  [   30/   89]
Per-example loss in batch: 0.250054  [   32/   89]
Per-example loss in batch: 0.338435  [   34/   89]
Per-example loss in batch: 0.242103  [   36/   89]
Per-example loss in batch: 0.312663  [   38/   89]
Per-example loss in batch: 0.253324  [   40/   89]
Per-example loss in batch: 0.342826  [   42/   89]
Per-example loss in batch: 0.263517  [   44/   89]
Per-example loss in batch: 0.293395  [   46/   89]
Per-example loss in batch: 0.235428  [   48/   89]
Per-example loss in batch: 0.252234  [   50/   89]
Per-example loss in batch: 0.295413  [   52/   89]
Per-example loss in batch: 0.308480  [   54/   89]
Per-example loss in batch: 0.292039  [   56/   89]
Per-example loss in batch: 0.275217  [   58/   89]
Per-example loss in batch: 0.372990  [   60/   89]
Per-example loss in batch: 0.336990  [   62/   89]
Per-example loss in batch: 0.306412  [   64/   89]
Per-example loss in batch: 0.286687  [   66/   89]
Per-example loss in batch: 0.322239  [   68/   89]
Per-example loss in batch: 0.341462  [   70/   89]
Per-example loss in batch: 0.272167  [   72/   89]
Per-example loss in batch: 0.323740  [   74/   89]
Per-example loss in batch: 0.324511  [   76/   89]
Per-example loss in batch: 0.314893  [   78/   89]
Per-example loss in batch: 0.363452  [   80/   89]
Per-example loss in batch: 0.301935  [   82/   89]
Per-example loss in batch: 0.291709  [   84/   89]
Per-example loss in batch: 0.248273  [   86/   89]
Per-example loss in batch: 0.345900  [   88/   89]
Per-example loss in batch: 0.530662  [   89/   89]
Train Error: Avg loss: 0.30498475
validation Error: 
 Avg loss: 0.51033666 
 F1: 0.395820 
 Precision: 0.484141 
 Recall: 0.334752
 IoU: 0.246743

test Error: 
 Avg loss: 0.49071291 
 F1: 0.431863 
 Precision: 0.525273 
 Recall: 0.366659
 IoU: 0.275398

We have finished training iteration 94
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_92_.pth
Per-example loss in batch: 0.252553  [    2/   89]
Per-example loss in batch: 0.341549  [    4/   89]
Per-example loss in batch: 0.336672  [    6/   89]
Per-example loss in batch: 0.315216  [    8/   89]
Per-example loss in batch: 0.267429  [   10/   89]
Per-example loss in batch: 0.269713  [   12/   89]
Per-example loss in batch: 0.324592  [   14/   89]
Per-example loss in batch: 0.316083  [   16/   89]
Per-example loss in batch: 0.262907  [   18/   89]
Per-example loss in batch: 0.301063  [   20/   89]
Per-example loss in batch: 0.360190  [   22/   89]
Per-example loss in batch: 0.334842  [   24/   89]
Per-example loss in batch: 0.326326  [   26/   89]
Per-example loss in batch: 0.303620  [   28/   89]
Per-example loss in batch: 0.370685  [   30/   89]
Per-example loss in batch: 0.391882  [   32/   89]
Per-example loss in batch: 0.263687  [   34/   89]
Per-example loss in batch: 0.324832  [   36/   89]
Per-example loss in batch: 0.314570  [   38/   89]
Per-example loss in batch: 0.255482  [   40/   89]
Per-example loss in batch: 0.279695  [   42/   89]
Per-example loss in batch: 0.245187  [   44/   89]
Per-example loss in batch: 0.335094  [   46/   89]
Per-example loss in batch: 0.354922  [   48/   89]
Per-example loss in batch: 0.349784  [   50/   89]
Per-example loss in batch: 0.388850  [   52/   89]
Per-example loss in batch: 0.298731  [   54/   89]
Per-example loss in batch: 0.336263  [   56/   89]
Per-example loss in batch: 0.279201  [   58/   89]
Per-example loss in batch: 0.300056  [   60/   89]
Per-example loss in batch: 0.271416  [   62/   89]
Per-example loss in batch: 0.265900  [   64/   89]
Per-example loss in batch: 0.386782  [   66/   89]
Per-example loss in batch: 0.323383  [   68/   89]
Per-example loss in batch: 0.349354  [   70/   89]
Per-example loss in batch: 0.354964  [   72/   89]
Per-example loss in batch: 0.339390  [   74/   89]
Per-example loss in batch: 0.239907  [   76/   89]
Per-example loss in batch: 0.308817  [   78/   89]
Per-example loss in batch: 0.247543  [   80/   89]
Per-example loss in batch: 0.354289  [   82/   89]
Per-example loss in batch: 0.257627  [   84/   89]
Per-example loss in batch: 0.289170  [   86/   89]
Per-example loss in batch: 0.235169  [   88/   89]
Per-example loss in batch: 0.441133  [   89/   89]
Train Error: Avg loss: 0.31114501
validation Error: 
 Avg loss: 0.50984606 
 F1: 0.394269 
 Precision: 0.528842 
 Recall: 0.314292
 IoU: 0.245539

test Error: 
 Avg loss: 0.49029146 
 F1: 0.428100 
 Precision: 0.566191 
 Recall: 0.344160
 IoU: 0.272345

We have finished training iteration 95
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_93_.pth
Per-example loss in batch: 0.353956  [    2/   89]
Per-example loss in batch: 0.353990  [    4/   89]
Per-example loss in batch: 0.273192  [    6/   89]
Per-example loss in batch: 0.259960  [    8/   89]
Per-example loss in batch: 0.238290  [   10/   89]
Per-example loss in batch: 0.334554  [   12/   89]
Per-example loss in batch: 0.297837  [   14/   89]
Per-example loss in batch: 0.296283  [   16/   89]
Per-example loss in batch: 0.269000  [   18/   89]
Per-example loss in batch: 0.242762  [   20/   89]
Per-example loss in batch: 0.273390  [   22/   89]
Per-example loss in batch: 0.233364  [   24/   89]
Per-example loss in batch: 0.343679  [   26/   89]
Per-example loss in batch: 0.276606  [   28/   89]
Per-example loss in batch: 0.325910  [   30/   89]
Per-example loss in batch: 0.308255  [   32/   89]
Per-example loss in batch: 0.339759  [   34/   89]
Per-example loss in batch: 0.275028  [   36/   89]
Per-example loss in batch: 0.364740  [   38/   89]
Per-example loss in batch: 0.367913  [   40/   89]
Per-example loss in batch: 0.325527  [   42/   89]
Per-example loss in batch: 0.353122  [   44/   89]
Per-example loss in batch: 0.260315  [   46/   89]
Per-example loss in batch: 0.249420  [   48/   89]
Per-example loss in batch: 0.282054  [   50/   89]
Per-example loss in batch: 0.334133  [   52/   89]
Per-example loss in batch: 0.307391  [   54/   89]
Per-example loss in batch: 0.320287  [   56/   89]
Per-example loss in batch: 0.278538  [   58/   89]
Per-example loss in batch: 0.315861  [   60/   89]
Per-example loss in batch: 0.232472  [   62/   89]
Per-example loss in batch: 0.245083  [   64/   89]
Per-example loss in batch: 0.334644  [   66/   89]
Per-example loss in batch: 0.360080  [   68/   89]
Per-example loss in batch: 0.304416  [   70/   89]
Per-example loss in batch: 0.324081  [   72/   89]
Per-example loss in batch: 0.266527  [   74/   89]
Per-example loss in batch: 0.358646  [   76/   89]
Per-example loss in batch: 0.323729  [   78/   89]
Per-example loss in batch: 0.355935  [   80/   89]
Per-example loss in batch: 0.301499  [   82/   89]
Per-example loss in batch: 0.280128  [   84/   89]
Per-example loss in batch: 0.353339  [   86/   89]
Per-example loss in batch: 0.331271  [   88/   89]
Per-example loss in batch: 0.618603  [   89/   89]
Train Error: Avg loss: 0.30868023
validation Error: 
 Avg loss: 0.51044924 
 F1: 0.399582 
 Precision: 0.502549 
 Recall: 0.331634
 IoU: 0.249673

test Error: 
 Avg loss: 0.49082470 
 F1: 0.433279 
 Precision: 0.547190 
 Recall: 0.358623
 IoU: 0.276551

We have finished training iteration 96
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_94_.pth
Per-example loss in batch: 0.329901  [    2/   89]
Per-example loss in batch: 0.350973  [    4/   89]
Per-example loss in batch: 0.350126  [    6/   89]
Per-example loss in batch: 0.305371  [    8/   89]
Per-example loss in batch: 0.348879  [   10/   89]
Per-example loss in batch: 0.284173  [   12/   89]
Per-example loss in batch: 0.299295  [   14/   89]
Per-example loss in batch: 0.265699  [   16/   89]
Per-example loss in batch: 0.269875  [   18/   89]
Per-example loss in batch: 0.367806  [   20/   89]
Per-example loss in batch: 0.314174  [   22/   89]
Per-example loss in batch: 0.279145  [   24/   89]
Per-example loss in batch: 0.266356  [   26/   89]
Per-example loss in batch: 0.324375  [   28/   89]
Per-example loss in batch: 0.370553  [   30/   89]
Per-example loss in batch: 0.259596  [   32/   89]
Per-example loss in batch: 0.311130  [   34/   89]
Per-example loss in batch: 0.307920  [   36/   89]
Per-example loss in batch: 0.337210  [   38/   89]
Per-example loss in batch: 0.360920  [   40/   89]
Per-example loss in batch: 0.373231  [   42/   89]
Per-example loss in batch: 0.302954  [   44/   89]
Per-example loss in batch: 0.253349  [   46/   89]
Per-example loss in batch: 0.250548  [   48/   89]
Per-example loss in batch: 0.325397  [   50/   89]
Per-example loss in batch: 0.269379  [   52/   89]
Per-example loss in batch: 0.236047  [   54/   89]
Per-example loss in batch: 0.335060  [   56/   89]
Per-example loss in batch: 0.282566  [   58/   89]
Per-example loss in batch: 0.331375  [   60/   89]
Per-example loss in batch: 0.256857  [   62/   89]
Per-example loss in batch: 0.321907  [   64/   89]
Per-example loss in batch: 0.250755  [   66/   89]
Per-example loss in batch: 0.292246  [   68/   89]
Per-example loss in batch: 0.277007  [   70/   89]
Per-example loss in batch: 0.299787  [   72/   89]
Per-example loss in batch: 0.288659  [   74/   89]
Per-example loss in batch: 0.236413  [   76/   89]
Per-example loss in batch: 0.248781  [   78/   89]
Per-example loss in batch: 0.366095  [   80/   89]
Per-example loss in batch: 0.271219  [   82/   89]
Per-example loss in batch: 0.315305  [   84/   89]
Per-example loss in batch: 0.321141  [   86/   89]
Per-example loss in batch: 0.357361  [   88/   89]
Per-example loss in batch: 0.502732  [   89/   89]
Train Error: Avg loss: 0.30602884
validation Error: 
 Avg loss: 0.50986120 
 F1: 0.393656 
 Precision: 0.544079 
 Recall: 0.308394
 IoU: 0.245063

test Error: 
 Avg loss: 0.49029851 
 F1: 0.422889 
 Precision: 0.579072 
 Recall: 0.333058
 IoU: 0.268141

We have finished training iteration 97
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_95_.pth
Per-example loss in batch: 0.286795  [    2/   89]
Per-example loss in batch: 0.295124  [    4/   89]
Per-example loss in batch: 0.317388  [    6/   89]
Per-example loss in batch: 0.257710  [    8/   89]
Per-example loss in batch: 0.335304  [   10/   89]
Per-example loss in batch: 0.314576  [   12/   89]
Per-example loss in batch: 0.231465  [   14/   89]
Per-example loss in batch: 0.288473  [   16/   89]
Per-example loss in batch: 0.320556  [   18/   89]
Per-example loss in batch: 0.362695  [   20/   89]
Per-example loss in batch: 0.281679  [   22/   89]
Per-example loss in batch: 0.347984  [   24/   89]
Per-example loss in batch: 0.339648  [   26/   89]
Per-example loss in batch: 0.355124  [   28/   89]
Per-example loss in batch: 0.304229  [   30/   89]
Per-example loss in batch: 0.322227  [   32/   89]
Per-example loss in batch: 0.282054  [   34/   89]
Per-example loss in batch: 0.247397  [   36/   89]
Per-example loss in batch: 0.332370  [   38/   89]
Per-example loss in batch: 0.373298  [   40/   89]
Per-example loss in batch: 0.272472  [   42/   89]
Per-example loss in batch: 0.252184  [   44/   89]
Per-example loss in batch: 0.313199  [   46/   89]
Per-example loss in batch: 0.363004  [   48/   89]
Per-example loss in batch: 0.275007  [   50/   89]
Per-example loss in batch: 0.310253  [   52/   89]
Per-example loss in batch: 0.333071  [   54/   89]
Per-example loss in batch: 0.231019  [   56/   89]
Per-example loss in batch: 0.247297  [   58/   89]
Per-example loss in batch: 0.232948  [   60/   89]
Per-example loss in batch: 0.326560  [   62/   89]
Per-example loss in batch: 0.263004  [   64/   89]
Per-example loss in batch: 0.303450  [   66/   89]
Per-example loss in batch: 0.271322  [   68/   89]
Per-example loss in batch: 0.320813  [   70/   89]
Per-example loss in batch: 0.245231  [   72/   89]
Per-example loss in batch: 0.311272  [   74/   89]
Per-example loss in batch: 0.350136  [   76/   89]
Per-example loss in batch: 0.269516  [   78/   89]
Per-example loss in batch: 0.358294  [   80/   89]
Per-example loss in batch: 0.392531  [   82/   89]
Per-example loss in batch: 0.282862  [   84/   89]
Per-example loss in batch: 0.296475  [   86/   89]
Per-example loss in batch: 0.339983  [   88/   89]
Per-example loss in batch: 0.641380  [   89/   89]
Train Error: Avg loss: 0.30738628
validation Error: 
 Avg loss: 0.50979480 
 F1: 0.395339 
 Precision: 0.501722 
 Recall: 0.326177
 IoU: 0.246369

test Error: 
 Avg loss: 0.49072192 
 F1: 0.431031 
 Precision: 0.539731 
 Recall: 0.358775
 IoU: 0.274722

We have finished training iteration 98
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_96_.pth
Per-example loss in batch: 0.295033  [    2/   89]
Per-example loss in batch: 0.287837  [    4/   89]
Per-example loss in batch: 0.291292  [    6/   89]
Per-example loss in batch: 0.326992  [    8/   89]
Per-example loss in batch: 0.280745  [   10/   89]
Per-example loss in batch: 0.322244  [   12/   89]
Per-example loss in batch: 0.261940  [   14/   89]
Per-example loss in batch: 0.335924  [   16/   89]
Per-example loss in batch: 0.386095  [   18/   89]
Per-example loss in batch: 0.313178  [   20/   89]
Per-example loss in batch: 0.273336  [   22/   89]
Per-example loss in batch: 0.259149  [   24/   89]
Per-example loss in batch: 0.229956  [   26/   89]
Per-example loss in batch: 0.245249  [   28/   89]
Per-example loss in batch: 0.345442  [   30/   89]
Per-example loss in batch: 0.277821  [   32/   89]
Per-example loss in batch: 0.274136  [   34/   89]
Per-example loss in batch: 0.315934  [   36/   89]
Per-example loss in batch: 0.275362  [   38/   89]
Per-example loss in batch: 0.281627  [   40/   89]
Per-example loss in batch: 0.303443  [   42/   89]
Per-example loss in batch: 0.338574  [   44/   89]
Per-example loss in batch: 0.299538  [   46/   89]
Per-example loss in batch: 0.335914  [   48/   89]
Per-example loss in batch: 0.333011  [   50/   89]
Per-example loss in batch: 0.296843  [   52/   89]
Per-example loss in batch: 0.289176  [   54/   89]
Per-example loss in batch: 0.369393  [   56/   89]
Per-example loss in batch: 0.351040  [   58/   89]
Per-example loss in batch: 0.221810  [   60/   89]
Per-example loss in batch: 0.325044  [   62/   89]
Per-example loss in batch: 0.299928  [   64/   89]
Per-example loss in batch: 0.253825  [   66/   89]
Per-example loss in batch: 0.287270  [   68/   89]
Per-example loss in batch: 0.375717  [   70/   89]
Per-example loss in batch: 0.407775  [   72/   89]
Per-example loss in batch: 0.335413  [   74/   89]
Per-example loss in batch: 0.414086  [   76/   89]
Per-example loss in batch: 0.265699  [   78/   89]
Per-example loss in batch: 0.321045  [   80/   89]
Per-example loss in batch: 0.352008  [   82/   89]
Per-example loss in batch: 0.271789  [   84/   89]
Per-example loss in batch: 0.241521  [   86/   89]
Per-example loss in batch: 0.287449  [   88/   89]
Per-example loss in batch: 0.500713  [   89/   89]
Train Error: Avg loss: 0.30802158
validation Error: 
 Avg loss: 0.50945603 
 F1: 0.393571 
 Precision: 0.569457 
 Recall: 0.300696
 IoU: 0.244997

test Error: 
 Avg loss: 0.48972040 
 F1: 0.421972 
 Precision: 0.605632 
 Recall: 0.323783
 IoU: 0.267405

We have finished training iteration 99
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_97_.pth
Per-example loss in batch: 0.294092  [    2/   89]
Per-example loss in batch: 0.412626  [    4/   89]
Per-example loss in batch: 0.258155  [    6/   89]
Per-example loss in batch: 0.307605  [    8/   89]
Per-example loss in batch: 0.250480  [   10/   89]
Per-example loss in batch: 0.292884  [   12/   89]
Per-example loss in batch: 0.373590  [   14/   89]
Per-example loss in batch: 0.433767  [   16/   89]
Per-example loss in batch: 0.255810  [   18/   89]
Per-example loss in batch: 0.388119  [   20/   89]
Per-example loss in batch: 0.255265  [   22/   89]
Per-example loss in batch: 0.274492  [   24/   89]
Per-example loss in batch: 0.300461  [   26/   89]
Per-example loss in batch: 0.339008  [   28/   89]
Per-example loss in batch: 0.247680  [   30/   89]
Per-example loss in batch: 0.264867  [   32/   89]
Per-example loss in batch: 0.291995  [   34/   89]
Per-example loss in batch: 0.291866  [   36/   89]
Per-example loss in batch: 0.293768  [   38/   89]
Per-example loss in batch: 0.324684  [   40/   89]
Per-example loss in batch: 0.308666  [   42/   89]
Per-example loss in batch: 0.370226  [   44/   89]
Per-example loss in batch: 0.339029  [   46/   89]
Per-example loss in batch: 0.356551  [   48/   89]
Per-example loss in batch: 0.272225  [   50/   89]
Per-example loss in batch: 0.302860  [   52/   89]
Per-example loss in batch: 0.361108  [   54/   89]
Per-example loss in batch: 0.336095  [   56/   89]
Per-example loss in batch: 0.305270  [   58/   89]
Per-example loss in batch: 0.307997  [   60/   89]
Per-example loss in batch: 0.243952  [   62/   89]
Per-example loss in batch: 0.235807  [   64/   89]
Per-example loss in batch: 0.285090  [   66/   89]
Per-example loss in batch: 0.270318  [   68/   89]
Per-example loss in batch: 0.245787  [   70/   89]
Per-example loss in batch: 0.286021  [   72/   89]
Per-example loss in batch: 0.334141  [   74/   89]
Per-example loss in batch: 0.306743  [   76/   89]
Per-example loss in batch: 0.249387  [   78/   89]
Per-example loss in batch: 0.367021  [   80/   89]
Per-example loss in batch: 0.255058  [   82/   89]
Per-example loss in batch: 0.261371  [   84/   89]
Per-example loss in batch: 0.231679  [   86/   89]
Per-example loss in batch: 0.334208  [   88/   89]
Per-example loss in batch: 0.547521  [   89/   89]
Train Error: Avg loss: 0.30542891
validation Error: 
 Avg loss: 0.50990106 
 F1: 0.400245 
 Precision: 0.517804 
 Recall: 0.326189
 IoU: 0.250192

test Error: 
 Avg loss: 0.49029776 
 F1: 0.434509 
 Precision: 0.562069 
 Recall: 0.354139
 IoU: 0.277555

We have finished training iteration 100
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_98_.pth
Per-example loss in batch: 0.307935  [    2/   89]
Per-example loss in batch: 0.262537  [    4/   89]
Per-example loss in batch: 0.294136  [    6/   89]
Per-example loss in batch: 0.371175  [    8/   89]
Per-example loss in batch: 0.339524  [   10/   89]
Per-example loss in batch: 0.330458  [   12/   89]
Per-example loss in batch: 0.307785  [   14/   89]
Per-example loss in batch: 0.256777  [   16/   89]
Per-example loss in batch: 0.319346  [   18/   89]
Per-example loss in batch: 0.245666  [   20/   89]
Per-example loss in batch: 0.379668  [   22/   89]
Per-example loss in batch: 0.290225  [   24/   89]
Per-example loss in batch: 0.263991  [   26/   89]
Per-example loss in batch: 0.312847  [   28/   89]
Per-example loss in batch: 0.288736  [   30/   89]
Per-example loss in batch: 0.234583  [   32/   89]
Per-example loss in batch: 0.234880  [   34/   89]
Per-example loss in batch: 0.331752  [   36/   89]
Per-example loss in batch: 0.326314  [   38/   89]
Per-example loss in batch: 0.322221  [   40/   89]
Per-example loss in batch: 0.278533  [   42/   89]
Per-example loss in batch: 0.326558  [   44/   89]
Per-example loss in batch: 0.269059  [   46/   89]
Per-example loss in batch: 0.317877  [   48/   89]
Per-example loss in batch: 0.364696  [   50/   89]
Per-example loss in batch: 0.233137  [   52/   89]
Per-example loss in batch: 0.362954  [   54/   89]
Per-example loss in batch: 0.367639  [   56/   89]
Per-example loss in batch: 0.330813  [   58/   89]
Per-example loss in batch: 0.291508  [   60/   89]
Per-example loss in batch: 0.346001  [   62/   89]
Per-example loss in batch: 0.263857  [   64/   89]
Per-example loss in batch: 0.273889  [   66/   89]
Per-example loss in batch: 0.324722  [   68/   89]
Per-example loss in batch: 0.372870  [   70/   89]
Per-example loss in batch: 0.287472  [   72/   89]
Per-example loss in batch: 0.273177  [   74/   89]
Per-example loss in batch: 0.291203  [   76/   89]
Per-example loss in batch: 0.277336  [   78/   89]
Per-example loss in batch: 0.341009  [   80/   89]
Per-example loss in batch: 0.307947  [   82/   89]
Per-example loss in batch: 0.311953  [   84/   89]
Per-example loss in batch: 0.285964  [   86/   89]
Per-example loss in batch: 0.284991  [   88/   89]
Per-example loss in batch: 0.668153  [   89/   89]
Train Error: Avg loss: 0.30875949
validation Error: 
 Avg loss: 0.51024906 
 F1: 0.398293 
 Precision: 0.510850 
 Recall: 0.326381
 IoU: 0.248668

test Error: 
 Avg loss: 0.49050712 
 F1: 0.434566 
 Precision: 0.552389 
 Recall: 0.358170
 IoU: 0.277601

We have finished training iteration 101
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_99_.pth
Per-example loss in batch: 0.262767  [    2/   89]
Per-example loss in batch: 0.300267  [    4/   89]
Per-example loss in batch: 0.328817  [    6/   89]
Per-example loss in batch: 0.252657  [    8/   89]
Per-example loss in batch: 0.352585  [   10/   89]
Per-example loss in batch: 0.322701  [   12/   89]
Per-example loss in batch: 0.305330  [   14/   89]
Per-example loss in batch: 0.367740  [   16/   89]
Per-example loss in batch: 0.294800  [   18/   89]
Per-example loss in batch: 0.323321  [   20/   89]
Per-example loss in batch: 0.238663  [   22/   89]
Per-example loss in batch: 0.345057  [   24/   89]
Per-example loss in batch: 0.330074  [   26/   89]
Per-example loss in batch: 0.260014  [   28/   89]
Per-example loss in batch: 0.282922  [   30/   89]
Per-example loss in batch: 0.249985  [   32/   89]
Per-example loss in batch: 0.260832  [   34/   89]
Per-example loss in batch: 0.317499  [   36/   89]
Per-example loss in batch: 0.347240  [   38/   89]
Per-example loss in batch: 0.231090  [   40/   89]
Per-example loss in batch: 0.300738  [   42/   89]
Per-example loss in batch: 0.323193  [   44/   89]
Per-example loss in batch: 0.323379  [   46/   89]
Per-example loss in batch: 0.299507  [   48/   89]
Per-example loss in batch: 0.355487  [   50/   89]
Per-example loss in batch: 0.258297  [   52/   89]
Per-example loss in batch: 0.294812  [   54/   89]
Per-example loss in batch: 0.365252  [   56/   89]
Per-example loss in batch: 0.263778  [   58/   89]
Per-example loss in batch: 0.330861  [   60/   89]
Per-example loss in batch: 0.274552  [   62/   89]
Per-example loss in batch: 0.312803  [   64/   89]
Per-example loss in batch: 0.257705  [   66/   89]
Per-example loss in batch: 0.285547  [   68/   89]
Per-example loss in batch: 0.250460  [   70/   89]
Per-example loss in batch: 0.305915  [   72/   89]
Per-example loss in batch: 0.324535  [   74/   89]
Per-example loss in batch: 0.288465  [   76/   89]
Per-example loss in batch: 0.314946  [   78/   89]
Per-example loss in batch: 0.268880  [   80/   89]
Per-example loss in batch: 0.278875  [   82/   89]
Per-example loss in batch: 0.278275  [   84/   89]
Per-example loss in batch: 0.302340  [   86/   89]
Per-example loss in batch: 0.327924  [   88/   89]
Per-example loss in batch: 0.558305  [   89/   89]
Train Error: Avg loss: 0.30202338
validation Error: 
 Avg loss: 0.51116158 
 F1: 0.399751 
 Precision: 0.447790 
 Recall: 0.361021
 IoU: 0.249806

test Error: 
 Avg loss: 0.49195041 
 F1: 0.439261 
 Precision: 0.491874 
 Recall: 0.396815
 IoU: 0.281444

We have finished training iteration 102
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_100_.pth
Per-example loss in batch: 0.225727  [    2/   89]
Per-example loss in batch: 0.357050  [    4/   89]
Per-example loss in batch: 0.325086  [    6/   89]
Per-example loss in batch: 0.294522  [    8/   89]
Per-example loss in batch: 0.262431  [   10/   89]
Per-example loss in batch: 0.334542  [   12/   89]
Per-example loss in batch: 0.283871  [   14/   89]
Per-example loss in batch: 0.280500  [   16/   89]
Per-example loss in batch: 0.326601  [   18/   89]
Per-example loss in batch: 0.257710  [   20/   89]
Per-example loss in batch: 0.305239  [   22/   89]
Per-example loss in batch: 0.334468  [   24/   89]
Per-example loss in batch: 0.264195  [   26/   89]
Per-example loss in batch: 0.321418  [   28/   89]
Per-example loss in batch: 0.285728  [   30/   89]
Per-example loss in batch: 0.242288  [   32/   89]
Per-example loss in batch: 0.321414  [   34/   89]
Per-example loss in batch: 0.309407  [   36/   89]
Per-example loss in batch: 0.406503  [   38/   89]
Per-example loss in batch: 0.263170  [   40/   89]
Per-example loss in batch: 0.325639  [   42/   89]
Per-example loss in batch: 0.310824  [   44/   89]
Per-example loss in batch: 0.336738  [   46/   89]
Per-example loss in batch: 0.328632  [   48/   89]
Per-example loss in batch: 0.235988  [   50/   89]
Per-example loss in batch: 0.283512  [   52/   89]
Per-example loss in batch: 0.312187  [   54/   89]
Per-example loss in batch: 0.263247  [   56/   89]
Per-example loss in batch: 0.279753  [   58/   89]
Per-example loss in batch: 0.329327  [   60/   89]
Per-example loss in batch: 0.231218  [   62/   89]
Per-example loss in batch: 0.277205  [   64/   89]
Per-example loss in batch: 0.315987  [   66/   89]
Per-example loss in batch: 0.287899  [   68/   89]
Per-example loss in batch: 0.342475  [   70/   89]
Per-example loss in batch: 0.338017  [   72/   89]
Per-example loss in batch: 0.277686  [   74/   89]
Per-example loss in batch: 0.312033  [   76/   89]
Per-example loss in batch: 0.333352  [   78/   89]
Per-example loss in batch: 0.253014  [   80/   89]
Per-example loss in batch: 0.288621  [   82/   89]
Per-example loss in batch: 0.257182  [   84/   89]
Per-example loss in batch: 0.295074  [   86/   89]
Per-example loss in batch: 0.331864  [   88/   89]
Per-example loss in batch: 0.476373  [   89/   89]
Train Error: Avg loss: 0.30084342
validation Error: 
 Avg loss: 0.51099641 
 F1: 0.402607 
 Precision: 0.470951 
 Recall: 0.351585
 IoU: 0.252040

test Error: 
 Avg loss: 0.49124673 
 F1: 0.442632 
 Precision: 0.520809 
 Recall: 0.384862
 IoU: 0.284218

We have finished training iteration 103
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_101_.pth
Per-example loss in batch: 0.277556  [    2/   89]
Per-example loss in batch: 0.312479  [    4/   89]
Per-example loss in batch: 0.348920  [    6/   89]
Per-example loss in batch: 0.303697  [    8/   89]
Per-example loss in batch: 0.243211  [   10/   89]
Per-example loss in batch: 0.246761  [   12/   89]
Per-example loss in batch: 0.299417  [   14/   89]
Per-example loss in batch: 0.354334  [   16/   89]
Per-example loss in batch: 0.316018  [   18/   89]
Per-example loss in batch: 0.304065  [   20/   89]
Per-example loss in batch: 0.260418  [   22/   89]
Per-example loss in batch: 0.299598  [   24/   89]
Per-example loss in batch: 0.257216  [   26/   89]
Per-example loss in batch: 0.317747  [   28/   89]
Per-example loss in batch: 0.250297  [   30/   89]
Per-example loss in batch: 0.337672  [   32/   89]
Per-example loss in batch: 0.246953  [   34/   89]
Per-example loss in batch: 0.261153  [   36/   89]
Per-example loss in batch: 0.322102  [   38/   89]
Per-example loss in batch: 0.360850  [   40/   89]
Per-example loss in batch: 0.316104  [   42/   89]
Per-example loss in batch: 0.316564  [   44/   89]
Per-example loss in batch: 0.323535  [   46/   89]
Per-example loss in batch: 0.265391  [   48/   89]
Per-example loss in batch: 0.343136  [   50/   89]
Per-example loss in batch: 0.238624  [   52/   89]
Per-example loss in batch: 0.283621  [   54/   89]
Per-example loss in batch: 0.320608  [   56/   89]
Per-example loss in batch: 0.289352  [   58/   89]
Per-example loss in batch: 0.300208  [   60/   89]
Per-example loss in batch: 0.340520  [   62/   89]
Per-example loss in batch: 0.350264  [   64/   89]
Per-example loss in batch: 0.299781  [   66/   89]
Per-example loss in batch: 0.313922  [   68/   89]
Per-example loss in batch: 0.223329  [   70/   89]
Per-example loss in batch: 0.260534  [   72/   89]
Per-example loss in batch: 0.298408  [   74/   89]
Per-example loss in batch: 0.297270  [   76/   89]
Per-example loss in batch: 0.244677  [   78/   89]
Per-example loss in batch: 0.284186  [   80/   89]
Per-example loss in batch: 0.359016  [   82/   89]
Per-example loss in batch: 0.340542  [   84/   89]
Per-example loss in batch: 0.301533  [   86/   89]
Per-example loss in batch: 0.343204  [   88/   89]
Per-example loss in batch: 0.533836  [   89/   89]
Train Error: Avg loss: 0.30206098
validation Error: 
 Avg loss: 0.51083280 
 F1: 0.403033 
 Precision: 0.491333 
 Recall: 0.341635
 IoU: 0.252374

test Error: 
 Avg loss: 0.49098283 
 F1: 0.440632 
 Precision: 0.538512 
 Recall: 0.372862
 IoU: 0.282571

We have finished training iteration 104
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_102_.pth
Per-example loss in batch: 0.340745  [    2/   89]
Per-example loss in batch: 0.245590  [    4/   89]
Per-example loss in batch: 0.340166  [    6/   89]
Per-example loss in batch: 0.271614  [    8/   89]
Per-example loss in batch: 0.311819  [   10/   89]
Per-example loss in batch: 0.293917  [   12/   89]
Per-example loss in batch: 0.354907  [   14/   89]
Per-example loss in batch: 0.275392  [   16/   89]
Per-example loss in batch: 0.271895  [   18/   89]
Per-example loss in batch: 0.263375  [   20/   89]
Per-example loss in batch: 0.258216  [   22/   89]
Per-example loss in batch: 0.342370  [   24/   89]
Per-example loss in batch: 0.272801  [   26/   89]
Per-example loss in batch: 0.324881  [   28/   89]
Per-example loss in batch: 0.345804  [   30/   89]
Per-example loss in batch: 0.327196  [   32/   89]
Per-example loss in batch: 0.325674  [   34/   89]
Per-example loss in batch: 0.336968  [   36/   89]
Per-example loss in batch: 0.240866  [   38/   89]
Per-example loss in batch: 0.299882  [   40/   89]
Per-example loss in batch: 0.313573  [   42/   89]
Per-example loss in batch: 0.284204  [   44/   89]
Per-example loss in batch: 0.319189  [   46/   89]
Per-example loss in batch: 0.294149  [   48/   89]
Per-example loss in batch: 0.274308  [   50/   89]
Per-example loss in batch: 0.297142  [   52/   89]
Per-example loss in batch: 0.237367  [   54/   89]
Per-example loss in batch: 0.322378  [   56/   89]
Per-example loss in batch: 0.256759  [   58/   89]
Per-example loss in batch: 0.385805  [   60/   89]
Per-example loss in batch: 0.324894  [   62/   89]
Per-example loss in batch: 0.291974  [   64/   89]
Per-example loss in batch: 0.247657  [   66/   89]
Per-example loss in batch: 0.243321  [   68/   89]
Per-example loss in batch: 0.370220  [   70/   89]
Per-example loss in batch: 0.263772  [   72/   89]
Per-example loss in batch: 0.299000  [   74/   89]
Per-example loss in batch: 0.348501  [   76/   89]
Per-example loss in batch: 0.285523  [   78/   89]
Per-example loss in batch: 0.278215  [   80/   89]
Per-example loss in batch: 0.304955  [   82/   89]
Per-example loss in batch: 0.250636  [   84/   89]
Per-example loss in batch: 0.261633  [   86/   89]
Per-example loss in batch: 0.331811  [   88/   89]
Per-example loss in batch: 0.780072  [   89/   89]
Train Error: Avg loss: 0.30384492
validation Error: 
 Avg loss: 0.51025002 
 F1: 0.400424 
 Precision: 0.512973 
 Recall: 0.328376
 IoU: 0.250331

test Error: 
 Avg loss: 0.49069901 
 F1: 0.434479 
 Precision: 0.553910 
 Recall: 0.357415
 IoU: 0.277530

We have finished training iteration 105
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_103_.pth
Per-example loss in batch: 0.295456  [    2/   89]
Per-example loss in batch: 0.346278  [    4/   89]
Per-example loss in batch: 0.300320  [    6/   89]
Per-example loss in batch: 0.276165  [    8/   89]
Per-example loss in batch: 0.253451  [   10/   89]
Per-example loss in batch: 0.370538  [   12/   89]
Per-example loss in batch: 0.299409  [   14/   89]
Per-example loss in batch: 0.370288  [   16/   89]
Per-example loss in batch: 0.289447  [   18/   89]
Per-example loss in batch: 0.244842  [   20/   89]
Per-example loss in batch: 0.266849  [   22/   89]
Per-example loss in batch: 0.273854  [   24/   89]
Per-example loss in batch: 0.396004  [   26/   89]
Per-example loss in batch: 0.346222  [   28/   89]
Per-example loss in batch: 0.268152  [   30/   89]
Per-example loss in batch: 0.283140  [   32/   89]
Per-example loss in batch: 0.307637  [   34/   89]
Per-example loss in batch: 0.332417  [   36/   89]
Per-example loss in batch: 0.256974  [   38/   89]
Per-example loss in batch: 0.261856  [   40/   89]
Per-example loss in batch: 0.346723  [   42/   89]
Per-example loss in batch: 0.304246  [   44/   89]
Per-example loss in batch: 0.261073  [   46/   89]
Per-example loss in batch: 0.268993  [   48/   89]
Per-example loss in batch: 0.277691  [   50/   89]
Per-example loss in batch: 0.263436  [   52/   89]
Per-example loss in batch: 0.281110  [   54/   89]
Per-example loss in batch: 0.347759  [   56/   89]
Per-example loss in batch: 0.276792  [   58/   89]
Per-example loss in batch: 0.300311  [   60/   89]
Per-example loss in batch: 0.284697  [   62/   89]
Per-example loss in batch: 0.279555  [   64/   89]
Per-example loss in batch: 0.322813  [   66/   89]
Per-example loss in batch: 0.272622  [   68/   89]
Per-example loss in batch: 0.252743  [   70/   89]
Per-example loss in batch: 0.323653  [   72/   89]
Per-example loss in batch: 0.346301  [   74/   89]
Per-example loss in batch: 0.312329  [   76/   89]
Per-example loss in batch: 0.329867  [   78/   89]
Per-example loss in batch: 0.225267  [   80/   89]
Per-example loss in batch: 0.333729  [   82/   89]
Per-example loss in batch: 0.362795  [   84/   89]
Per-example loss in batch: 0.319162  [   86/   89]
Per-example loss in batch: 0.270521  [   88/   89]
Per-example loss in batch: 0.725337  [   89/   89]
Train Error: Avg loss: 0.30485745
validation Error: 
 Avg loss: 0.51083112 
 F1: 0.403153 
 Precision: 0.478810 
 Recall: 0.348143
 IoU: 0.252468

test Error: 
 Avg loss: 0.49127783 
 F1: 0.440447 
 Precision: 0.523708 
 Recall: 0.380029
 IoU: 0.282419

We have finished training iteration 106
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_104_.pth
Per-example loss in batch: 0.352203  [    2/   89]
Per-example loss in batch: 0.277977  [    4/   89]
Per-example loss in batch: 0.324993  [    6/   89]
Per-example loss in batch: 0.245148  [    8/   89]
Per-example loss in batch: 0.240397  [   10/   89]
Per-example loss in batch: 0.348921  [   12/   89]
Per-example loss in batch: 0.358468  [   14/   89]
Per-example loss in batch: 0.371180  [   16/   89]
Per-example loss in batch: 0.337013  [   18/   89]
Per-example loss in batch: 0.318751  [   20/   89]
Per-example loss in batch: 0.297681  [   22/   89]
Per-example loss in batch: 0.264387  [   24/   89]
Per-example loss in batch: 0.309721  [   26/   89]
Per-example loss in batch: 0.305122  [   28/   89]
Per-example loss in batch: 0.374563  [   30/   89]
Per-example loss in batch: 0.260533  [   32/   89]
Per-example loss in batch: 0.263368  [   34/   89]
Per-example loss in batch: 0.311615  [   36/   89]
Per-example loss in batch: 0.267789  [   38/   89]
Per-example loss in batch: 0.266621  [   40/   89]
Per-example loss in batch: 0.295748  [   42/   89]
Per-example loss in batch: 0.296061  [   44/   89]
Per-example loss in batch: 0.281037  [   46/   89]
Per-example loss in batch: 0.256986  [   48/   89]
Per-example loss in batch: 0.270218  [   50/   89]
Per-example loss in batch: 0.247676  [   52/   89]
Per-example loss in batch: 0.331251  [   54/   89]
Per-example loss in batch: 0.348727  [   56/   89]
Per-example loss in batch: 0.283260  [   58/   89]
Per-example loss in batch: 0.285719  [   60/   89]
Per-example loss in batch: 0.297612  [   62/   89]
Per-example loss in batch: 0.282721  [   64/   89]
Per-example loss in batch: 0.301056  [   66/   89]
Per-example loss in batch: 0.233551  [   68/   89]
Per-example loss in batch: 0.239566  [   70/   89]
Per-example loss in batch: 0.318439  [   72/   89]
Per-example loss in batch: 0.339412  [   74/   89]
Per-example loss in batch: 0.287897  [   76/   89]
Per-example loss in batch: 0.361679  [   78/   89]
Per-example loss in batch: 0.275399  [   80/   89]
Per-example loss in batch: 0.363848  [   82/   89]
Per-example loss in batch: 0.294750  [   84/   89]
Per-example loss in batch: 0.254931  [   86/   89]
Per-example loss in batch: 0.278450  [   88/   89]
Per-example loss in batch: 0.672326  [   89/   89]
Train Error: Avg loss: 0.30244063
validation Error: 
 Avg loss: 0.51104523 
 F1: 0.403477 
 Precision: 0.471883 
 Recall: 0.352392
 IoU: 0.252722

test Error: 
 Avg loss: 0.49106953 
 F1: 0.441396 
 Precision: 0.520136 
 Recall: 0.383361
 IoU: 0.283199

We have finished training iteration 107
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_105_.pth
Per-example loss in batch: 0.356107  [    2/   89]
Per-example loss in batch: 0.396001  [    4/   89]
Per-example loss in batch: 0.250037  [    6/   89]
Per-example loss in batch: 0.293885  [    8/   89]
Per-example loss in batch: 0.260604  [   10/   89]
Per-example loss in batch: 0.274862  [   12/   89]
Per-example loss in batch: 0.255895  [   14/   89]
Per-example loss in batch: 0.244989  [   16/   89]
Per-example loss in batch: 0.261390  [   18/   89]
Per-example loss in batch: 0.320913  [   20/   89]
Per-example loss in batch: 0.311358  [   22/   89]
Per-example loss in batch: 0.343170  [   24/   89]
Per-example loss in batch: 0.400853  [   26/   89]
Per-example loss in batch: 0.229128  [   28/   89]
Per-example loss in batch: 0.278842  [   30/   89]
Per-example loss in batch: 0.338178  [   32/   89]
Per-example loss in batch: 0.330710  [   34/   89]
Per-example loss in batch: 0.293126  [   36/   89]
Per-example loss in batch: 0.329387  [   38/   89]
Per-example loss in batch: 0.320533  [   40/   89]
Per-example loss in batch: 0.240653  [   42/   89]
Per-example loss in batch: 0.304514  [   44/   89]
Per-example loss in batch: 0.278421  [   46/   89]
Per-example loss in batch: 0.284813  [   48/   89]
Per-example loss in batch: 0.349309  [   50/   89]
Per-example loss in batch: 0.252191  [   52/   89]
Per-example loss in batch: 0.253586  [   54/   89]
Per-example loss in batch: 0.308330  [   56/   89]
Per-example loss in batch: 0.326203  [   58/   89]
Per-example loss in batch: 0.227973  [   60/   89]
Per-example loss in batch: 0.320211  [   62/   89]
Per-example loss in batch: 0.299719  [   64/   89]
Per-example loss in batch: 0.261768  [   66/   89]
Per-example loss in batch: 0.356892  [   68/   89]
Per-example loss in batch: 0.293291  [   70/   89]
Per-example loss in batch: 0.316752  [   72/   89]
Per-example loss in batch: 0.306987  [   74/   89]
Per-example loss in batch: 0.250228  [   76/   89]
Per-example loss in batch: 0.352032  [   78/   89]
Per-example loss in batch: 0.290005  [   80/   89]
Per-example loss in batch: 0.339358  [   82/   89]
Per-example loss in batch: 0.264768  [   84/   89]
Per-example loss in batch: 0.244175  [   86/   89]
Per-example loss in batch: 0.369426  [   88/   89]
Per-example loss in batch: 0.653531  [   89/   89]
Train Error: Avg loss: 0.30355823
validation Error: 
 Avg loss: 0.51030679 
 F1: 0.403506 
 Precision: 0.488006 
 Recall: 0.343950
 IoU: 0.252745

test Error: 
 Avg loss: 0.49098388 
 F1: 0.441373 
 Precision: 0.534161 
 Recall: 0.376049
 IoU: 0.283180

We have finished training iteration 108
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_106_.pth
Per-example loss in batch: 0.272314  [    2/   89]
Per-example loss in batch: 0.334820  [    4/   89]
Per-example loss in batch: 0.282713  [    6/   89]
Per-example loss in batch: 0.325573  [    8/   89]
Per-example loss in batch: 0.254156  [   10/   89]
Per-example loss in batch: 0.261556  [   12/   89]
Per-example loss in batch: 0.279703  [   14/   89]
Per-example loss in batch: 0.301548  [   16/   89]
Per-example loss in batch: 0.244627  [   18/   89]
Per-example loss in batch: 0.290295  [   20/   89]
Per-example loss in batch: 0.311751  [   22/   89]
Per-example loss in batch: 0.285528  [   24/   89]
Per-example loss in batch: 0.302140  [   26/   89]
Per-example loss in batch: 0.331272  [   28/   89]
Per-example loss in batch: 0.291757  [   30/   89]
Per-example loss in batch: 0.341317  [   32/   89]
Per-example loss in batch: 0.358348  [   34/   89]
Per-example loss in batch: 0.349817  [   36/   89]
Per-example loss in batch: 0.266813  [   38/   89]
Per-example loss in batch: 0.330738  [   40/   89]
Per-example loss in batch: 0.243923  [   42/   89]
Per-example loss in batch: 0.308118  [   44/   89]
Per-example loss in batch: 0.265027  [   46/   89]
Per-example loss in batch: 0.346732  [   48/   89]
Per-example loss in batch: 0.231299  [   50/   89]
Per-example loss in batch: 0.345699  [   52/   89]
Per-example loss in batch: 0.309767  [   54/   89]
Per-example loss in batch: 0.281955  [   56/   89]
Per-example loss in batch: 0.246462  [   58/   89]
Per-example loss in batch: 0.282711  [   60/   89]
Per-example loss in batch: 0.342177  [   62/   89]
Per-example loss in batch: 0.307437  [   64/   89]
Per-example loss in batch: 0.325090  [   66/   89]
Per-example loss in batch: 0.235582  [   68/   89]
Per-example loss in batch: 0.364456  [   70/   89]
Per-example loss in batch: 0.320931  [   72/   89]
Per-example loss in batch: 0.244846  [   74/   89]
Per-example loss in batch: 0.304824  [   76/   89]
Per-example loss in batch: 0.373926  [   78/   89]
Per-example loss in batch: 0.322433  [   80/   89]
Per-example loss in batch: 0.257005  [   82/   89]
Per-example loss in batch: 0.232158  [   84/   89]
Per-example loss in batch: 0.388542  [   86/   89]
Per-example loss in batch: 0.338187  [   88/   89]
Per-example loss in batch: 0.535696  [   89/   89]
Train Error: Avg loss: 0.30345894
validation Error: 
 Avg loss: 0.51105043 
 F1: 0.403911 
 Precision: 0.465073 
 Recall: 0.356966
 IoU: 0.253063

test Error: 
 Avg loss: 0.49135180 
 F1: 0.443185 
 Precision: 0.512152 
 Recall: 0.390588
 IoU: 0.284675

We have finished training iteration 109
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_107_.pth
Per-example loss in batch: 0.375462  [    2/   89]
Per-example loss in batch: 0.317494  [    4/   89]
Per-example loss in batch: 0.349611  [    6/   89]
Per-example loss in batch: 0.248575  [    8/   89]
Per-example loss in batch: 0.326549  [   10/   89]
Per-example loss in batch: 0.298785  [   12/   89]
Per-example loss in batch: 0.266262  [   14/   89]
Per-example loss in batch: 0.349183  [   16/   89]
Per-example loss in batch: 0.299854  [   18/   89]
Per-example loss in batch: 0.248788  [   20/   89]
Per-example loss in batch: 0.243271  [   22/   89]
Per-example loss in batch: 0.325974  [   24/   89]
Per-example loss in batch: 0.275242  [   26/   89]
Per-example loss in batch: 0.372826  [   28/   89]
Per-example loss in batch: 0.304248  [   30/   89]
Per-example loss in batch: 0.263395  [   32/   89]
Per-example loss in batch: 0.272214  [   34/   89]
Per-example loss in batch: 0.243678  [   36/   89]
Per-example loss in batch: 0.283797  [   38/   89]
Per-example loss in batch: 0.248075  [   40/   89]
Per-example loss in batch: 0.296890  [   42/   89]
Per-example loss in batch: 0.354510  [   44/   89]
Per-example loss in batch: 0.358569  [   46/   89]
Per-example loss in batch: 0.300522  [   48/   89]
Per-example loss in batch: 0.249673  [   50/   89]
Per-example loss in batch: 0.261926  [   52/   89]
Per-example loss in batch: 0.310365  [   54/   89]
Per-example loss in batch: 0.306726  [   56/   89]
Per-example loss in batch: 0.275647  [   58/   89]
Per-example loss in batch: 0.344373  [   60/   89]
Per-example loss in batch: 0.324351  [   62/   89]
Per-example loss in batch: 0.255633  [   64/   89]
Per-example loss in batch: 0.243217  [   66/   89]
Per-example loss in batch: 0.310093  [   68/   89]
Per-example loss in batch: 0.385914  [   70/   89]
Per-example loss in batch: 0.231808  [   72/   89]
Per-example loss in batch: 0.257487  [   74/   89]
Per-example loss in batch: 0.298304  [   76/   89]
Per-example loss in batch: 0.328040  [   78/   89]
Per-example loss in batch: 0.269017  [   80/   89]
Per-example loss in batch: 0.223537  [   82/   89]
Per-example loss in batch: 0.256133  [   84/   89]
Per-example loss in batch: 0.315878  [   86/   89]
Per-example loss in batch: 0.362099  [   88/   89]
Per-example loss in batch: 0.717357  [   89/   89]
Train Error: Avg loss: 0.30095893
validation Error: 
 Avg loss: 0.51060201 
 F1: 0.404584 
 Precision: 0.485692 
 Recall: 0.346689
 IoU: 0.253591

test Error: 
 Avg loss: 0.49099750 
 F1: 0.441757 
 Precision: 0.530642 
 Recall: 0.378377
 IoU: 0.283497

We have finished training iteration 110
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_108_.pth
Per-example loss in batch: 0.254025  [    2/   89]
Per-example loss in batch: 0.320899  [    4/   89]
Per-example loss in batch: 0.237166  [    6/   89]
Per-example loss in batch: 0.283866  [    8/   89]
Per-example loss in batch: 0.290330  [   10/   89]
Per-example loss in batch: 0.342261  [   12/   89]
Per-example loss in batch: 0.340526  [   14/   89]
Per-example loss in batch: 0.259380  [   16/   89]
Per-example loss in batch: 0.302471  [   18/   89]
Per-example loss in batch: 0.360348  [   20/   89]
Per-example loss in batch: 0.244422  [   22/   89]
Per-example loss in batch: 0.291760  [   24/   89]
Per-example loss in batch: 0.249903  [   26/   89]
Per-example loss in batch: 0.288661  [   28/   89]
Per-example loss in batch: 0.369975  [   30/   89]
Per-example loss in batch: 0.269984  [   32/   89]
Per-example loss in batch: 0.243323  [   34/   89]
Per-example loss in batch: 0.327126  [   36/   89]
Per-example loss in batch: 0.239535  [   38/   89]
Per-example loss in batch: 0.371756  [   40/   89]
Per-example loss in batch: 0.278983  [   42/   89]
Per-example loss in batch: 0.286748  [   44/   89]
Per-example loss in batch: 0.250804  [   46/   89]
Per-example loss in batch: 0.275830  [   48/   89]
Per-example loss in batch: 0.332494  [   50/   89]
Per-example loss in batch: 0.284041  [   52/   89]
Per-example loss in batch: 0.283767  [   54/   89]
Per-example loss in batch: 0.391734  [   56/   89]
Per-example loss in batch: 0.284789  [   58/   89]
Per-example loss in batch: 0.276836  [   60/   89]
Per-example loss in batch: 0.290728  [   62/   89]
Per-example loss in batch: 0.308857  [   64/   89]
Per-example loss in batch: 0.339757  [   66/   89]
Per-example loss in batch: 0.266172  [   68/   89]
Per-example loss in batch: 0.328383  [   70/   89]
Per-example loss in batch: 0.269409  [   72/   89]
Per-example loss in batch: 0.256279  [   74/   89]
Per-example loss in batch: 0.309478  [   76/   89]
Per-example loss in batch: 0.336324  [   78/   89]
Per-example loss in batch: 0.253752  [   80/   89]
Per-example loss in batch: 0.353747  [   82/   89]
Per-example loss in batch: 0.306581  [   84/   89]
Per-example loss in batch: 0.354114  [   86/   89]
Per-example loss in batch: 0.236069  [   88/   89]
Per-example loss in batch: 0.594660  [   89/   89]
Train Error: Avg loss: 0.29979154
validation Error: 
 Avg loss: 0.51031506 
 F1: 0.405843 
 Precision: 0.482789 
 Recall: 0.350052
 IoU: 0.254581

test Error: 
 Avg loss: 0.49090407 
 F1: 0.442030 
 Precision: 0.530957 
 Recall: 0.378618
 IoU: 0.283722

We have finished training iteration 111
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_109_.pth
Per-example loss in batch: 0.365125  [    2/   89]
Per-example loss in batch: 0.326923  [    4/   89]
Per-example loss in batch: 0.257900  [    6/   89]
Per-example loss in batch: 0.294620  [    8/   89]
Per-example loss in batch: 0.306647  [   10/   89]
Per-example loss in batch: 0.376258  [   12/   89]
Per-example loss in batch: 0.300031  [   14/   89]
Per-example loss in batch: 0.302949  [   16/   89]
Per-example loss in batch: 0.276719  [   18/   89]
Per-example loss in batch: 0.306475  [   20/   89]
Per-example loss in batch: 0.315820  [   22/   89]
Per-example loss in batch: 0.242521  [   24/   89]
Per-example loss in batch: 0.254698  [   26/   89]
Per-example loss in batch: 0.313231  [   28/   89]
Per-example loss in batch: 0.239401  [   30/   89]
Per-example loss in batch: 0.297240  [   32/   89]
Per-example loss in batch: 0.331499  [   34/   89]
Per-example loss in batch: 0.311987  [   36/   89]
Per-example loss in batch: 0.261587  [   38/   89]
Per-example loss in batch: 0.256750  [   40/   89]
Per-example loss in batch: 0.251710  [   42/   89]
Per-example loss in batch: 0.258951  [   44/   89]
Per-example loss in batch: 0.396490  [   46/   89]
Per-example loss in batch: 0.283025  [   48/   89]
Per-example loss in batch: 0.264777  [   50/   89]
Per-example loss in batch: 0.232028  [   52/   89]
Per-example loss in batch: 0.264181  [   54/   89]
Per-example loss in batch: 0.355125  [   56/   89]
Per-example loss in batch: 0.291351  [   58/   89]
Per-example loss in batch: 0.261848  [   60/   89]
Per-example loss in batch: 0.347919  [   62/   89]
Per-example loss in batch: 0.300780  [   64/   89]
Per-example loss in batch: 0.312571  [   66/   89]
Per-example loss in batch: 0.271308  [   68/   89]
Per-example loss in batch: 0.345360  [   70/   89]
Per-example loss in batch: 0.313104  [   72/   89]
Per-example loss in batch: 0.354184  [   74/   89]
Per-example loss in batch: 0.350127  [   76/   89]
Per-example loss in batch: 0.327217  [   78/   89]
Per-example loss in batch: 0.326118  [   80/   89]
Per-example loss in batch: 0.237538  [   82/   89]
Per-example loss in batch: 0.250994  [   84/   89]
Per-example loss in batch: 0.319561  [   86/   89]
Per-example loss in batch: 0.332719  [   88/   89]
Per-example loss in batch: 0.502623  [   89/   89]
Train Error: Avg loss: 0.30199278
validation Error: 
 Avg loss: 0.51069502 
 F1: 0.404895 
 Precision: 0.480837 
 Recall: 0.349669
 IoU: 0.253836

test Error: 
 Avg loss: 0.49094161 
 F1: 0.443338 
 Precision: 0.531341 
 Recall: 0.380343
 IoU: 0.284800

We have finished training iteration 112
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_110_.pth
Per-example loss in batch: 0.330779  [    2/   89]
Per-example loss in batch: 0.261871  [    4/   89]
Per-example loss in batch: 0.345091  [    6/   89]
Per-example loss in batch: 0.275128  [    8/   89]
Per-example loss in batch: 0.253394  [   10/   89]
Per-example loss in batch: 0.343150  [   12/   89]
Per-example loss in batch: 0.348916  [   14/   89]
Per-example loss in batch: 0.318151  [   16/   89]
Per-example loss in batch: 0.284297  [   18/   89]
Per-example loss in batch: 0.292639  [   20/   89]
Per-example loss in batch: 0.294676  [   22/   89]
Per-example loss in batch: 0.329059  [   24/   89]
Per-example loss in batch: 0.244858  [   26/   89]
Per-example loss in batch: 0.342484  [   28/   89]
Per-example loss in batch: 0.371600  [   30/   89]
Per-example loss in batch: 0.345549  [   32/   89]
Per-example loss in batch: 0.267523  [   34/   89]
Per-example loss in batch: 0.246000  [   36/   89]
Per-example loss in batch: 0.356161  [   38/   89]
Per-example loss in batch: 0.307637  [   40/   89]
Per-example loss in batch: 0.320075  [   42/   89]
Per-example loss in batch: 0.278569  [   44/   89]
Per-example loss in batch: 0.253878  [   46/   89]
Per-example loss in batch: 0.253818  [   48/   89]
Per-example loss in batch: 0.260100  [   50/   89]
Per-example loss in batch: 0.304657  [   52/   89]
Per-example loss in batch: 0.271464  [   54/   89]
Per-example loss in batch: 0.393733  [   56/   89]
Per-example loss in batch: 0.241067  [   58/   89]
Per-example loss in batch: 0.267556  [   60/   89]
Per-example loss in batch: 0.240928  [   62/   89]
Per-example loss in batch: 0.359509  [   64/   89]
Per-example loss in batch: 0.240105  [   66/   89]
Per-example loss in batch: 0.266026  [   68/   89]
Per-example loss in batch: 0.320422  [   70/   89]
Per-example loss in batch: 0.273426  [   72/   89]
Per-example loss in batch: 0.286317  [   74/   89]
Per-example loss in batch: 0.280498  [   76/   89]
Per-example loss in batch: 0.344564  [   78/   89]
Per-example loss in batch: 0.321404  [   80/   89]
Per-example loss in batch: 0.308964  [   82/   89]
Per-example loss in batch: 0.305040  [   84/   89]
Per-example loss in batch: 0.350165  [   86/   89]
Per-example loss in batch: 0.319788  [   88/   89]
Per-example loss in batch: 0.720335  [   89/   89]
Train Error: Avg loss: 0.30519562
validation Error: 
 Avg loss: 0.51047683 
 F1: 0.404417 
 Precision: 0.517454 
 Recall: 0.331912
 IoU: 0.253460

test Error: 
 Avg loss: 0.49054737 
 F1: 0.437924 
 Precision: 0.560455 
 Recall: 0.359359
 IoU: 0.280348

We have finished training iteration 113
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_111_.pth
Per-example loss in batch: 0.295526  [    2/   89]
Per-example loss in batch: 0.356809  [    4/   89]
Per-example loss in batch: 0.302867  [    6/   89]
Per-example loss in batch: 0.301562  [    8/   89]
Per-example loss in batch: 0.352790  [   10/   89]
Per-example loss in batch: 0.301257  [   12/   89]
Per-example loss in batch: 0.259077  [   14/   89]
Per-example loss in batch: 0.310355  [   16/   89]
Per-example loss in batch: 0.330602  [   18/   89]
Per-example loss in batch: 0.319370  [   20/   89]
Per-example loss in batch: 0.285360  [   22/   89]
Per-example loss in batch: 0.332568  [   24/   89]
Per-example loss in batch: 0.320813  [   26/   89]
Per-example loss in batch: 0.316711  [   28/   89]
Per-example loss in batch: 0.271632  [   30/   89]
Per-example loss in batch: 0.245636  [   32/   89]
Per-example loss in batch: 0.364623  [   34/   89]
Per-example loss in batch: 0.278179  [   36/   89]
Per-example loss in batch: 0.390889  [   38/   89]
Per-example loss in batch: 0.319968  [   40/   89]
Per-example loss in batch: 0.250440  [   42/   89]
Per-example loss in batch: 0.312371  [   44/   89]
Per-example loss in batch: 0.301835  [   46/   89]
Per-example loss in batch: 0.308978  [   48/   89]
Per-example loss in batch: 0.248370  [   50/   89]
Per-example loss in batch: 0.346071  [   52/   89]
Per-example loss in batch: 0.279997  [   54/   89]
Per-example loss in batch: 0.261790  [   56/   89]
Per-example loss in batch: 0.327221  [   58/   89]
Per-example loss in batch: 0.262761  [   60/   89]
Per-example loss in batch: 0.238760  [   62/   89]
Per-example loss in batch: 0.267237  [   64/   89]
Per-example loss in batch: 0.309747  [   66/   89]
Per-example loss in batch: 0.276315  [   68/   89]
Per-example loss in batch: 0.259631  [   70/   89]
Per-example loss in batch: 0.303828  [   72/   89]
Per-example loss in batch: 0.222491  [   74/   89]
Per-example loss in batch: 0.245340  [   76/   89]
Per-example loss in batch: 0.299305  [   78/   89]
Per-example loss in batch: 0.318439  [   80/   89]
Per-example loss in batch: 0.328121  [   82/   89]
Per-example loss in batch: 0.270579  [   84/   89]
Per-example loss in batch: 0.246335  [   86/   89]
Per-example loss in batch: 0.299537  [   88/   89]
Per-example loss in batch: 0.653755  [   89/   89]
Train Error: Avg loss: 0.30042634
validation Error: 
 Avg loss: 0.51073998 
 F1: 0.404932 
 Precision: 0.494690 
 Recall: 0.342744
 IoU: 0.253865

test Error: 
 Avg loss: 0.49092459 
 F1: 0.440526 
 Precision: 0.537908 
 Recall: 0.372998
 IoU: 0.282483

We have finished training iteration 114
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_112_.pth
Per-example loss in batch: 0.233821  [    2/   89]
Per-example loss in batch: 0.343963  [    4/   89]
Per-example loss in batch: 0.354106  [    6/   89]
Per-example loss in batch: 0.231042  [    8/   89]
Per-example loss in batch: 0.371359  [   10/   89]
Per-example loss in batch: 0.345907  [   12/   89]
Per-example loss in batch: 0.315912  [   14/   89]
Per-example loss in batch: 0.313378  [   16/   89]
Per-example loss in batch: 0.263933  [   18/   89]
Per-example loss in batch: 0.247477  [   20/   89]
Per-example loss in batch: 0.291004  [   22/   89]
Per-example loss in batch: 0.227505  [   24/   89]
Per-example loss in batch: 0.326454  [   26/   89]
Per-example loss in batch: 0.261561  [   28/   89]
Per-example loss in batch: 0.305543  [   30/   89]
Per-example loss in batch: 0.270952  [   32/   89]
Per-example loss in batch: 0.329148  [   34/   89]
Per-example loss in batch: 0.325050  [   36/   89]
Per-example loss in batch: 0.284133  [   38/   89]
Per-example loss in batch: 0.358904  [   40/   89]
Per-example loss in batch: 0.267486  [   42/   89]
Per-example loss in batch: 0.322500  [   44/   89]
Per-example loss in batch: 0.280402  [   46/   89]
Per-example loss in batch: 0.287795  [   48/   89]
Per-example loss in batch: 0.308789  [   50/   89]
Per-example loss in batch: 0.234076  [   52/   89]
Per-example loss in batch: 0.337292  [   54/   89]
Per-example loss in batch: 0.302559  [   56/   89]
Per-example loss in batch: 0.263207  [   58/   89]
Per-example loss in batch: 0.366918  [   60/   89]
Per-example loss in batch: 0.320556  [   62/   89]
Per-example loss in batch: 0.228306  [   64/   89]
Per-example loss in batch: 0.252224  [   66/   89]
Per-example loss in batch: 0.321173  [   68/   89]
Per-example loss in batch: 0.297691  [   70/   89]
Per-example loss in batch: 0.341334  [   72/   89]
Per-example loss in batch: 0.358239  [   74/   89]
Per-example loss in batch: 0.276749  [   76/   89]
Per-example loss in batch: 0.365017  [   78/   89]
Per-example loss in batch: 0.285231  [   80/   89]
Per-example loss in batch: 0.351644  [   82/   89]
Per-example loss in batch: 0.335842  [   84/   89]
Per-example loss in batch: 0.282191  [   86/   89]
Per-example loss in batch: 0.284717  [   88/   89]
Per-example loss in batch: 0.492043  [   89/   89]
Train Error: Avg loss: 0.30380026
validation Error: 
 Avg loss: 0.50958852 
 F1: 0.402281 
 Precision: 0.563909 
 Recall: 0.312665
 IoU: 0.251784

test Error: 
 Avg loss: 0.49001993 
 F1: 0.432518 
 Precision: 0.603306 
 Recall: 0.337092
 IoU: 0.275932

We have finished training iteration 115
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_113_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.235571  [    2/   89]
Per-example loss in batch: 0.345189  [    4/   89]
Per-example loss in batch: 0.225191  [    6/   89]
Per-example loss in batch: 0.399032  [    8/   89]
Per-example loss in batch: 0.256922  [   10/   89]
Per-example loss in batch: 0.232121  [   12/   89]
Per-example loss in batch: 0.365494  [   14/   89]
Per-example loss in batch: 0.305623  [   16/   89]
Per-example loss in batch: 0.247098  [   18/   89]
Per-example loss in batch: 0.305349  [   20/   89]
Per-example loss in batch: 0.245959  [   22/   89]
Per-example loss in batch: 0.393363  [   24/   89]
Per-example loss in batch: 0.308392  [   26/   89]
Per-example loss in batch: 0.244299  [   28/   89]
Per-example loss in batch: 0.315399  [   30/   89]
Per-example loss in batch: 0.283585  [   32/   89]
Per-example loss in batch: 0.243055  [   34/   89]
Per-example loss in batch: 0.336718  [   36/   89]
Per-example loss in batch: 0.275694  [   38/   89]
Per-example loss in batch: 0.282126  [   40/   89]
Per-example loss in batch: 0.322784  [   42/   89]
Per-example loss in batch: 0.328736  [   44/   89]
Per-example loss in batch: 0.326017  [   46/   89]
Per-example loss in batch: 0.347143  [   48/   89]
Per-example loss in batch: 0.360528  [   50/   89]
Per-example loss in batch: 0.322402  [   52/   89]
Per-example loss in batch: 0.352656  [   54/   89]
Per-example loss in batch: 0.300649  [   56/   89]
Per-example loss in batch: 0.226498  [   58/   89]
Per-example loss in batch: 0.295580  [   60/   89]
Per-example loss in batch: 0.320521  [   62/   89]
Per-example loss in batch: 0.300215  [   64/   89]
Per-example loss in batch: 0.314206  [   66/   89]
Per-example loss in batch: 0.340731  [   68/   89]
Per-example loss in batch: 0.318073  [   70/   89]
Per-example loss in batch: 0.305053  [   72/   89]
Per-example loss in batch: 0.303248  [   74/   89]
Per-example loss in batch: 0.267193  [   76/   89]
Per-example loss in batch: 0.293412  [   78/   89]
Per-example loss in batch: 0.309554  [   80/   89]
Per-example loss in batch: 0.285108  [   82/   89]
Per-example loss in batch: 0.310512  [   84/   89]
Per-example loss in batch: 0.342063  [   86/   89]
Per-example loss in batch: 0.279126  [   88/   89]
Per-example loss in batch: 0.448812  [   89/   89]
Train Error: Avg loss: 0.30432791
validation Error: 
 Avg loss: 0.51158797 
 F1: 0.404945 
 Precision: 0.451609 
 Recall: 0.367022
 IoU: 0.253875

test Error: 
 Avg loss: 0.49176149 
 F1: 0.445666 
 Precision: 0.496915 
 Recall: 0.404000
 IoU: 0.286725

We have finished training iteration 116
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_114_.pth
Per-example loss in batch: 0.308249  [    2/   89]
Per-example loss in batch: 0.328300  [    4/   89]
Per-example loss in batch: 0.312517  [    6/   89]
Per-example loss in batch: 0.228784  [    8/   89]
Per-example loss in batch: 0.322340  [   10/   89]
Per-example loss in batch: 0.372473  [   12/   89]
Per-example loss in batch: 0.276949  [   14/   89]
Per-example loss in batch: 0.318011  [   16/   89]
Per-example loss in batch: 0.309152  [   18/   89]
Per-example loss in batch: 0.269196  [   20/   89]
Per-example loss in batch: 0.329676  [   22/   89]
Per-example loss in batch: 0.278436  [   24/   89]
Per-example loss in batch: 0.223427  [   26/   89]
Per-example loss in batch: 0.324684  [   28/   89]
Per-example loss in batch: 0.256634  [   30/   89]
Per-example loss in batch: 0.378963  [   32/   89]
Per-example loss in batch: 0.280915  [   34/   89]
Per-example loss in batch: 0.286374  [   36/   89]
Per-example loss in batch: 0.248927  [   38/   89]
Per-example loss in batch: 0.300706  [   40/   89]
Per-example loss in batch: 0.299024  [   42/   89]
Per-example loss in batch: 0.320163  [   44/   89]
Per-example loss in batch: 0.235706  [   46/   89]
Per-example loss in batch: 0.282395  [   48/   89]
Per-example loss in batch: 0.349490  [   50/   89]
Per-example loss in batch: 0.271082  [   52/   89]
Per-example loss in batch: 0.321941  [   54/   89]
Per-example loss in batch: 0.224094  [   56/   89]
Per-example loss in batch: 0.237691  [   58/   89]
Per-example loss in batch: 0.247574  [   60/   89]
Per-example loss in batch: 0.322821  [   62/   89]
Per-example loss in batch: 0.315123  [   64/   89]
Per-example loss in batch: 0.285947  [   66/   89]
Per-example loss in batch: 0.311457  [   68/   89]
Per-example loss in batch: 0.379210  [   70/   89]
Per-example loss in batch: 0.337529  [   72/   89]
Per-example loss in batch: 0.243157  [   74/   89]
Per-example loss in batch: 0.398110  [   76/   89]
Per-example loss in batch: 0.351049  [   78/   89]
Per-example loss in batch: 0.363610  [   80/   89]
Per-example loss in batch: 0.272128  [   82/   89]
Per-example loss in batch: 0.372332  [   84/   89]
Per-example loss in batch: 0.250530  [   86/   89]
Per-example loss in batch: 0.240706  [   88/   89]
Per-example loss in batch: 0.687378  [   89/   89]
Train Error: Avg loss: 0.30407357
validation Error: 
 Avg loss: 0.51082823 
 F1: 0.405381 
 Precision: 0.488907 
 Recall: 0.346231
 IoU: 0.254218

test Error: 
 Avg loss: 0.49125487 
 F1: 0.442152 
 Precision: 0.528508 
 Recall: 0.380053
 IoU: 0.283823

We have finished training iteration 117
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_115_.pth
Per-example loss in batch: 0.289894  [    2/   89]
Per-example loss in batch: 0.308892  [    4/   89]
Per-example loss in batch: 0.380082  [    6/   89]
Per-example loss in batch: 0.316447  [    8/   89]
Per-example loss in batch: 0.334679  [   10/   89]
Per-example loss in batch: 0.278470  [   12/   89]
Per-example loss in batch: 0.310861  [   14/   89]
Per-example loss in batch: 0.240681  [   16/   89]
Per-example loss in batch: 0.282541  [   18/   89]
Per-example loss in batch: 0.320914  [   20/   89]
Per-example loss in batch: 0.348537  [   22/   89]
Per-example loss in batch: 0.267292  [   24/   89]
Per-example loss in batch: 0.331106  [   26/   89]
Per-example loss in batch: 0.229053  [   28/   89]
Per-example loss in batch: 0.369333  [   30/   89]
Per-example loss in batch: 0.305055  [   32/   89]
Per-example loss in batch: 0.321446  [   34/   89]
Per-example loss in batch: 0.235412  [   36/   89]
Per-example loss in batch: 0.281708  [   38/   89]
Per-example loss in batch: 0.308805  [   40/   89]
Per-example loss in batch: 0.295489  [   42/   89]
Per-example loss in batch: 0.268642  [   44/   89]
Per-example loss in batch: 0.343875  [   46/   89]
Per-example loss in batch: 0.275385  [   48/   89]
Per-example loss in batch: 0.324973  [   50/   89]
Per-example loss in batch: 0.261537  [   52/   89]
Per-example loss in batch: 0.242305  [   54/   89]
Per-example loss in batch: 0.217152  [   56/   89]
Per-example loss in batch: 0.306508  [   58/   89]
Per-example loss in batch: 0.340739  [   60/   89]
Per-example loss in batch: 0.284528  [   62/   89]
Per-example loss in batch: 0.293268  [   64/   89]
Per-example loss in batch: 0.317139  [   66/   89]
Per-example loss in batch: 0.292004  [   68/   89]
Per-example loss in batch: 0.270941  [   70/   89]
Per-example loss in batch: 0.268673  [   72/   89]
Per-example loss in batch: 0.285689  [   74/   89]
Per-example loss in batch: 0.358083  [   76/   89]
Per-example loss in batch: 0.322511  [   78/   89]
Per-example loss in batch: 0.362520  [   80/   89]
Per-example loss in batch: 0.294080  [   82/   89]
Per-example loss in batch: 0.375964  [   84/   89]
Per-example loss in batch: 0.309844  [   86/   89]
Per-example loss in batch: 0.234658  [   88/   89]
Per-example loss in batch: 0.630675  [   89/   89]
Train Error: Avg loss: 0.30388869
validation Error: 
 Avg loss: 0.51017487 
 F1: 0.405183 
 Precision: 0.524538 
 Recall: 0.330076
 IoU: 0.254062

test Error: 
 Avg loss: 0.49041154 
 F1: 0.438736 
 Precision: 0.566511 
 Recall: 0.357993
 IoU: 0.281014

We have finished training iteration 118
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_116_.pth
Per-example loss in batch: 0.272976  [    2/   89]
Per-example loss in batch: 0.285514  [    4/   89]
Per-example loss in batch: 0.308541  [    6/   89]
Per-example loss in batch: 0.325902  [    8/   89]
Per-example loss in batch: 0.314248  [   10/   89]
Per-example loss in batch: 0.306259  [   12/   89]
Per-example loss in batch: 0.322490  [   14/   89]
Per-example loss in batch: 0.305789  [   16/   89]
Per-example loss in batch: 0.303781  [   18/   89]
Per-example loss in batch: 0.278833  [   20/   89]
Per-example loss in batch: 0.350044  [   22/   89]
Per-example loss in batch: 0.244893  [   24/   89]
Per-example loss in batch: 0.246927  [   26/   89]
Per-example loss in batch: 0.340671  [   28/   89]
Per-example loss in batch: 0.248558  [   30/   89]
Per-example loss in batch: 0.367420  [   32/   89]
Per-example loss in batch: 0.301400  [   34/   89]
Per-example loss in batch: 0.249852  [   36/   89]
Per-example loss in batch: 0.301249  [   38/   89]
Per-example loss in batch: 0.296901  [   40/   89]
Per-example loss in batch: 0.352079  [   42/   89]
Per-example loss in batch: 0.244065  [   44/   89]
Per-example loss in batch: 0.268393  [   46/   89]
Per-example loss in batch: 0.323104  [   48/   89]
Per-example loss in batch: 0.256139  [   50/   89]
Per-example loss in batch: 0.246170  [   52/   89]
Per-example loss in batch: 0.374243  [   54/   89]
Per-example loss in batch: 0.286809  [   56/   89]
Per-example loss in batch: 0.265324  [   58/   89]
Per-example loss in batch: 0.334032  [   60/   89]
Per-example loss in batch: 0.322933  [   62/   89]
Per-example loss in batch: 0.269367  [   64/   89]
Per-example loss in batch: 0.357416  [   66/   89]
Per-example loss in batch: 0.346270  [   68/   89]
Per-example loss in batch: 0.230544  [   70/   89]
Per-example loss in batch: 0.274342  [   72/   89]
Per-example loss in batch: 0.244330  [   74/   89]
Per-example loss in batch: 0.316001  [   76/   89]
Per-example loss in batch: 0.340167  [   78/   89]
Per-example loss in batch: 0.228485  [   80/   89]
Per-example loss in batch: 0.339484  [   82/   89]
Per-example loss in batch: 0.249149  [   84/   89]
Per-example loss in batch: 0.359535  [   86/   89]
Per-example loss in batch: 0.292748  [   88/   89]
Per-example loss in batch: 0.692590  [   89/   89]
Train Error: Avg loss: 0.30201520
validation Error: 
 Avg loss: 0.51071091 
 F1: 0.406320 
 Precision: 0.508171 
 Recall: 0.338480
 IoU: 0.254957

test Error: 
 Avg loss: 0.49089313 
 F1: 0.441412 
 Precision: 0.550496 
 Recall: 0.368410
 IoU: 0.283213

We have finished training iteration 119
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_117_.pth
Per-example loss in batch: 0.302436  [    2/   89]
Per-example loss in batch: 0.226572  [    4/   89]
Per-example loss in batch: 0.299417  [    6/   89]
Per-example loss in batch: 0.261753  [    8/   89]
Per-example loss in batch: 0.298360  [   10/   89]
Per-example loss in batch: 0.283056  [   12/   89]
Per-example loss in batch: 0.329924  [   14/   89]
Per-example loss in batch: 0.236096  [   16/   89]
Per-example loss in batch: 0.282869  [   18/   89]
Per-example loss in batch: 0.307815  [   20/   89]
Per-example loss in batch: 0.327122  [   22/   89]
Per-example loss in batch: 0.342857  [   24/   89]
Per-example loss in batch: 0.308508  [   26/   89]
Per-example loss in batch: 0.243541  [   28/   89]
Per-example loss in batch: 0.271098  [   30/   89]
Per-example loss in batch: 0.358140  [   32/   89]
Per-example loss in batch: 0.290145  [   34/   89]
Per-example loss in batch: 0.324624  [   36/   89]
Per-example loss in batch: 0.311605  [   38/   89]
Per-example loss in batch: 0.285529  [   40/   89]
Per-example loss in batch: 0.330784  [   42/   89]
Per-example loss in batch: 0.304315  [   44/   89]
Per-example loss in batch: 0.349068  [   46/   89]
Per-example loss in batch: 0.361375  [   48/   89]
Per-example loss in batch: 0.354392  [   50/   89]
Per-example loss in batch: 0.298031  [   52/   89]
Per-example loss in batch: 0.294322  [   54/   89]
Per-example loss in batch: 0.268411  [   56/   89]
Per-example loss in batch: 0.244208  [   58/   89]
Per-example loss in batch: 0.275296  [   60/   89]
Per-example loss in batch: 0.230574  [   62/   89]
Per-example loss in batch: 0.275647  [   64/   89]
Per-example loss in batch: 0.347782  [   66/   89]
Per-example loss in batch: 0.255009  [   68/   89]
Per-example loss in batch: 0.288580  [   70/   89]
Per-example loss in batch: 0.339446  [   72/   89]
Per-example loss in batch: 0.340061  [   74/   89]
Per-example loss in batch: 0.278707  [   76/   89]
Per-example loss in batch: 0.272140  [   78/   89]
Per-example loss in batch: 0.336100  [   80/   89]
Per-example loss in batch: 0.301855  [   82/   89]
Per-example loss in batch: 0.247692  [   84/   89]
Per-example loss in batch: 0.321662  [   86/   89]
Per-example loss in batch: 0.285695  [   88/   89]
Per-example loss in batch: 0.760607  [   89/   89]
Train Error: Avg loss: 0.30276227
validation Error: 
 Avg loss: 0.51136242 
 F1: 0.405583 
 Precision: 0.458360 
 Recall: 0.363705
 IoU: 0.254377

test Error: 
 Avg loss: 0.49162156 
 F1: 0.444155 
 Precision: 0.506757 
 Recall: 0.395320
 IoU: 0.285475

We have finished training iteration 120
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_118_.pth
Per-example loss in batch: 0.222546  [    2/   89]
Per-example loss in batch: 0.250647  [    4/   89]
Per-example loss in batch: 0.361873  [    6/   89]
Per-example loss in batch: 0.326743  [    8/   89]
Per-example loss in batch: 0.355966  [   10/   89]
Per-example loss in batch: 0.343931  [   12/   89]
Per-example loss in batch: 0.271164  [   14/   89]
Per-example loss in batch: 0.265598  [   16/   89]
Per-example loss in batch: 0.335464  [   18/   89]
Per-example loss in batch: 0.349910  [   20/   89]
Per-example loss in batch: 0.312909  [   22/   89]
Per-example loss in batch: 0.334031  [   24/   89]
Per-example loss in batch: 0.257269  [   26/   89]
Per-example loss in batch: 0.244042  [   28/   89]
Per-example loss in batch: 0.261805  [   30/   89]
Per-example loss in batch: 0.323537  [   32/   89]
Per-example loss in batch: 0.329164  [   34/   89]
Per-example loss in batch: 0.253250  [   36/   89]
Per-example loss in batch: 0.262574  [   38/   89]
Per-example loss in batch: 0.289335  [   40/   89]
Per-example loss in batch: 0.290411  [   42/   89]
Per-example loss in batch: 0.265988  [   44/   89]
Per-example loss in batch: 0.323239  [   46/   89]
Per-example loss in batch: 0.248027  [   48/   89]
Per-example loss in batch: 0.312242  [   50/   89]
Per-example loss in batch: 0.313788  [   52/   89]
Per-example loss in batch: 0.246211  [   54/   89]
Per-example loss in batch: 0.367235  [   56/   89]
Per-example loss in batch: 0.266255  [   58/   89]
Per-example loss in batch: 0.269087  [   60/   89]
Per-example loss in batch: 0.272701  [   62/   89]
Per-example loss in batch: 0.319226  [   64/   89]
Per-example loss in batch: 0.376176  [   66/   89]
Per-example loss in batch: 0.343575  [   68/   89]
Per-example loss in batch: 0.307388  [   70/   89]
Per-example loss in batch: 0.313013  [   72/   89]
Per-example loss in batch: 0.267952  [   74/   89]
Per-example loss in batch: 0.279312  [   76/   89]
Per-example loss in batch: 0.272450  [   78/   89]
Per-example loss in batch: 0.375785  [   80/   89]
Per-example loss in batch: 0.246095  [   82/   89]
Per-example loss in batch: 0.254856  [   84/   89]
Per-example loss in batch: 0.342174  [   86/   89]
Per-example loss in batch: 0.271345  [   88/   89]
Per-example loss in batch: 0.551443  [   89/   89]
Train Error: Avg loss: 0.30049461
validation Error: 
 Avg loss: 0.51083780 
 F1: 0.407573 
 Precision: 0.495365 
 Recall: 0.346215
 IoU: 0.255945

test Error: 
 Avg loss: 0.49119042 
 F1: 0.441508 
 Precision: 0.538241 
 Recall: 0.374249
 IoU: 0.283292

We have finished training iteration 121
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_119_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.387720  [    2/   89]
Per-example loss in batch: 0.237609  [    4/   89]
Per-example loss in batch: 0.332880  [    6/   89]
Per-example loss in batch: 0.331443  [    8/   89]
Per-example loss in batch: 0.240326  [   10/   89]
Per-example loss in batch: 0.253412  [   12/   89]
Per-example loss in batch: 0.247343  [   14/   89]
Per-example loss in batch: 0.393022  [   16/   89]
Per-example loss in batch: 0.292264  [   18/   89]
Per-example loss in batch: 0.308090  [   20/   89]
Per-example loss in batch: 0.281985  [   22/   89]
Per-example loss in batch: 0.311274  [   24/   89]
Per-example loss in batch: 0.322567  [   26/   89]
Per-example loss in batch: 0.215655  [   28/   89]
Per-example loss in batch: 0.387176  [   30/   89]
Per-example loss in batch: 0.276369  [   32/   89]
Per-example loss in batch: 0.274598  [   34/   89]
Per-example loss in batch: 0.279220  [   36/   89]
Per-example loss in batch: 0.297080  [   38/   89]
Per-example loss in batch: 0.295461  [   40/   89]
Per-example loss in batch: 0.327073  [   42/   89]
Per-example loss in batch: 0.271413  [   44/   89]
Per-example loss in batch: 0.336119  [   46/   89]
Per-example loss in batch: 0.263921  [   48/   89]
Per-example loss in batch: 0.394896  [   50/   89]
Per-example loss in batch: 0.325134  [   52/   89]
Per-example loss in batch: 0.316778  [   54/   89]
Per-example loss in batch: 0.215584  [   56/   89]
Per-example loss in batch: 0.323430  [   58/   89]
Per-example loss in batch: 0.265924  [   60/   89]
Per-example loss in batch: 0.247249  [   62/   89]
Per-example loss in batch: 0.301507  [   64/   89]
Per-example loss in batch: 0.297674  [   66/   89]
Per-example loss in batch: 0.237541  [   68/   89]
Per-example loss in batch: 0.252206  [   70/   89]
Per-example loss in batch: 0.339206  [   72/   89]
Per-example loss in batch: 0.239527  [   74/   89]
Per-example loss in batch: 0.286841  [   76/   89]
Per-example loss in batch: 0.352857  [   78/   89]
Per-example loss in batch: 0.246999  [   80/   89]
Per-example loss in batch: 0.327707  [   82/   89]
Per-example loss in batch: 0.259458  [   84/   89]
Per-example loss in batch: 0.395113  [   86/   89]
Per-example loss in batch: 0.294302  [   88/   89]
Per-example loss in batch: 0.601502  [   89/   89]
Train Error: Avg loss: 0.30077983
validation Error: 
 Avg loss: 0.51014803 
 F1: 0.404712 
 Precision: 0.521789 
 Recall: 0.330545
 IoU: 0.253692

test Error: 
 Avg loss: 0.49057904 
 F1: 0.437375 
 Precision: 0.560114 
 Recall: 0.358759
 IoU: 0.279897

We have finished training iteration 122
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_120_.pth
Per-example loss in batch: 0.247337  [    2/   89]
Per-example loss in batch: 0.336077  [    4/   89]
Per-example loss in batch: 0.295707  [    6/   89]
Per-example loss in batch: 0.374290  [    8/   89]
Per-example loss in batch: 0.322345  [   10/   89]
Per-example loss in batch: 0.278889  [   12/   89]
Per-example loss in batch: 0.304892  [   14/   89]
Per-example loss in batch: 0.298349  [   16/   89]
Per-example loss in batch: 0.306275  [   18/   89]
Per-example loss in batch: 0.314618  [   20/   89]
Per-example loss in batch: 0.253989  [   22/   89]
Per-example loss in batch: 0.364910  [   24/   89]
Per-example loss in batch: 0.322098  [   26/   89]
Per-example loss in batch: 0.266538  [   28/   89]
Per-example loss in batch: 0.291731  [   30/   89]
Per-example loss in batch: 0.319073  [   32/   89]
Per-example loss in batch: 0.295414  [   34/   89]
Per-example loss in batch: 0.354565  [   36/   89]
Per-example loss in batch: 0.309845  [   38/   89]
Per-example loss in batch: 0.240264  [   40/   89]
Per-example loss in batch: 0.287710  [   42/   89]
Per-example loss in batch: 0.313112  [   44/   89]
Per-example loss in batch: 0.264219  [   46/   89]
Per-example loss in batch: 0.241982  [   48/   89]
Per-example loss in batch: 0.253692  [   50/   89]
Per-example loss in batch: 0.250689  [   52/   89]
Per-example loss in batch: 0.319262  [   54/   89]
Per-example loss in batch: 0.344929  [   56/   89]
Per-example loss in batch: 0.366519  [   58/   89]
Per-example loss in batch: 0.252784  [   60/   89]
Per-example loss in batch: 0.251709  [   62/   89]
Per-example loss in batch: 0.355534  [   64/   89]
Per-example loss in batch: 0.312610  [   66/   89]
Per-example loss in batch: 0.394556  [   68/   89]
Per-example loss in batch: 0.255250  [   70/   89]
Per-example loss in batch: 0.295252  [   72/   89]
Per-example loss in batch: 0.269222  [   74/   89]
Per-example loss in batch: 0.369582  [   76/   89]
Per-example loss in batch: 0.337247  [   78/   89]
Per-example loss in batch: 0.242734  [   80/   89]
Per-example loss in batch: 0.271082  [   82/   89]
Per-example loss in batch: 0.273420  [   84/   89]
Per-example loss in batch: 0.241209  [   86/   89]
Per-example loss in batch: 0.242296  [   88/   89]
Per-example loss in batch: 0.547923  [   89/   89]
Train Error: Avg loss: 0.30062401
validation Error: 
 Avg loss: 0.51064481 
 F1: 0.407663 
 Precision: 0.505800 
 Recall: 0.341420
 IoU: 0.256016

test Error: 
 Avg loss: 0.49071686 
 F1: 0.441642 
 Precision: 0.550771 
 Recall: 0.368607
 IoU: 0.283402

We have finished training iteration 123
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_121_.pth
Per-example loss in batch: 0.305809  [    2/   89]
Per-example loss in batch: 0.263469  [    4/   89]
Per-example loss in batch: 0.310517  [    6/   89]
Per-example loss in batch: 0.234951  [    8/   89]
Per-example loss in batch: 0.261209  [   10/   89]
Per-example loss in batch: 0.361177  [   12/   89]
Per-example loss in batch: 0.336720  [   14/   89]
Per-example loss in batch: 0.335149  [   16/   89]
Per-example loss in batch: 0.243173  [   18/   89]
Per-example loss in batch: 0.350093  [   20/   89]
Per-example loss in batch: 0.331558  [   22/   89]
Per-example loss in batch: 0.260818  [   24/   89]
Per-example loss in batch: 0.356844  [   26/   89]
Per-example loss in batch: 0.226507  [   28/   89]
Per-example loss in batch: 0.243397  [   30/   89]
Per-example loss in batch: 0.350089  [   32/   89]
Per-example loss in batch: 0.264935  [   34/   89]
Per-example loss in batch: 0.320157  [   36/   89]
Per-example loss in batch: 0.243728  [   38/   89]
Per-example loss in batch: 0.294089  [   40/   89]
Per-example loss in batch: 0.384858  [   42/   89]
Per-example loss in batch: 0.303631  [   44/   89]
Per-example loss in batch: 0.267427  [   46/   89]
Per-example loss in batch: 0.263995  [   48/   89]
Per-example loss in batch: 0.238338  [   50/   89]
Per-example loss in batch: 0.351682  [   52/   89]
Per-example loss in batch: 0.332403  [   54/   89]
Per-example loss in batch: 0.246661  [   56/   89]
Per-example loss in batch: 0.334698  [   58/   89]
Per-example loss in batch: 0.282755  [   60/   89]
Per-example loss in batch: 0.354789  [   62/   89]
Per-example loss in batch: 0.264679  [   64/   89]
Per-example loss in batch: 0.320828  [   66/   89]
Per-example loss in batch: 0.225793  [   68/   89]
Per-example loss in batch: 0.303740  [   70/   89]
Per-example loss in batch: 0.303586  [   72/   89]
Per-example loss in batch: 0.279873  [   74/   89]
Per-example loss in batch: 0.312577  [   76/   89]
Per-example loss in batch: 0.271019  [   78/   89]
Per-example loss in batch: 0.289970  [   80/   89]
Per-example loss in batch: 0.272933  [   82/   89]
Per-example loss in batch: 0.284111  [   84/   89]
Per-example loss in batch: 0.320399  [   86/   89]
Per-example loss in batch: 0.298009  [   88/   89]
Per-example loss in batch: 0.710927  [   89/   89]
Train Error: Avg loss: 0.30019340
validation Error: 
 Avg loss: 0.51055025 
 F1: 0.408013 
 Precision: 0.511319 
 Recall: 0.339435
 IoU: 0.256292

test Error: 
 Avg loss: 0.49089780 
 F1: 0.440881 
 Precision: 0.552349 
 Recall: 0.366848
 IoU: 0.282775

We have finished training iteration 124
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_122_.pth
Per-example loss in batch: 0.271215  [    2/   89]
Per-example loss in batch: 0.329860  [    4/   89]
Per-example loss in batch: 0.301113  [    6/   89]
Per-example loss in batch: 0.267740  [    8/   89]
Per-example loss in batch: 0.345786  [   10/   89]
Per-example loss in batch: 0.315196  [   12/   89]
Per-example loss in batch: 0.293883  [   14/   89]
Per-example loss in batch: 0.275674  [   16/   89]
Per-example loss in batch: 0.300511  [   18/   89]
Per-example loss in batch: 0.236033  [   20/   89]
Per-example loss in batch: 0.327437  [   22/   89]
Per-example loss in batch: 0.314845  [   24/   89]
Per-example loss in batch: 0.261760  [   26/   89]
Per-example loss in batch: 0.234987  [   28/   89]
Per-example loss in batch: 0.279354  [   30/   89]
Per-example loss in batch: 0.372030  [   32/   89]
Per-example loss in batch: 0.313206  [   34/   89]
Per-example loss in batch: 0.292584  [   36/   89]
Per-example loss in batch: 0.331115  [   38/   89]
Per-example loss in batch: 0.238929  [   40/   89]
Per-example loss in batch: 0.281027  [   42/   89]
Per-example loss in batch: 0.350306  [   44/   89]
Per-example loss in batch: 0.281020  [   46/   89]
Per-example loss in batch: 0.231495  [   48/   89]
Per-example loss in batch: 0.326617  [   50/   89]
Per-example loss in batch: 0.314728  [   52/   89]
Per-example loss in batch: 0.295739  [   54/   89]
Per-example loss in batch: 0.284837  [   56/   89]
Per-example loss in batch: 0.297175  [   58/   89]
Per-example loss in batch: 0.359874  [   60/   89]
Per-example loss in batch: 0.288143  [   62/   89]
Per-example loss in batch: 0.282936  [   64/   89]
Per-example loss in batch: 0.308551  [   66/   89]
Per-example loss in batch: 0.242965  [   68/   89]
Per-example loss in batch: 0.360543  [   70/   89]
Per-example loss in batch: 0.364705  [   72/   89]
Per-example loss in batch: 0.299765  [   74/   89]
Per-example loss in batch: 0.351853  [   76/   89]
Per-example loss in batch: 0.321895  [   78/   89]
Per-example loss in batch: 0.353238  [   80/   89]
Per-example loss in batch: 0.275924  [   82/   89]
Per-example loss in batch: 0.263913  [   84/   89]
Per-example loss in batch: 0.268987  [   86/   89]
Per-example loss in batch: 0.299756  [   88/   89]
Per-example loss in batch: 0.511042  [   89/   89]
Train Error: Avg loss: 0.30257903
validation Error: 
 Avg loss: 0.51074811 
 F1: 0.408662 
 Precision: 0.484550 
 Recall: 0.353325
 IoU: 0.256804

test Error: 
 Avg loss: 0.49112434 
 F1: 0.444526 
 Precision: 0.528040 
 Recall: 0.383821
 IoU: 0.285782

We have finished training iteration 125
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_123_.pth
Per-example loss in batch: 0.279131  [    2/   89]
Per-example loss in batch: 0.306967  [    4/   89]
Per-example loss in batch: 0.314041  [    6/   89]
Per-example loss in batch: 0.323744  [    8/   89]
Per-example loss in batch: 0.230412  [   10/   89]
Per-example loss in batch: 0.352916  [   12/   89]
Per-example loss in batch: 0.334417  [   14/   89]
Per-example loss in batch: 0.229247  [   16/   89]
Per-example loss in batch: 0.227192  [   18/   89]
Per-example loss in batch: 0.259940  [   20/   89]
Per-example loss in batch: 0.237203  [   22/   89]
Per-example loss in batch: 0.236085  [   24/   89]
Per-example loss in batch: 0.241891  [   26/   89]
Per-example loss in batch: 0.331124  [   28/   89]
Per-example loss in batch: 0.271349  [   30/   89]
Per-example loss in batch: 0.248598  [   32/   89]
Per-example loss in batch: 0.304163  [   34/   89]
Per-example loss in batch: 0.289885  [   36/   89]
Per-example loss in batch: 0.287118  [   38/   89]
Per-example loss in batch: 0.253030  [   40/   89]
Per-example loss in batch: 0.295090  [   42/   89]
Per-example loss in batch: 0.225557  [   44/   89]
Per-example loss in batch: 0.332851  [   46/   89]
Per-example loss in batch: 0.216581  [   48/   89]
Per-example loss in batch: 0.294418  [   50/   89]
Per-example loss in batch: 0.251355  [   52/   89]
Per-example loss in batch: 0.357656  [   54/   89]
Per-example loss in batch: 0.357787  [   56/   89]
Per-example loss in batch: 0.298204  [   58/   89]
Per-example loss in batch: 0.316401  [   60/   89]
Per-example loss in batch: 0.385499  [   62/   89]
Per-example loss in batch: 0.231236  [   64/   89]
Per-example loss in batch: 0.324118  [   66/   89]
Per-example loss in batch: 0.241690  [   68/   89]
Per-example loss in batch: 0.248865  [   70/   89]
Per-example loss in batch: 0.370907  [   72/   89]
Per-example loss in batch: 0.350785  [   74/   89]
Per-example loss in batch: 0.324279  [   76/   89]
Per-example loss in batch: 0.293014  [   78/   89]
Per-example loss in batch: 0.366055  [   80/   89]
Per-example loss in batch: 0.294424  [   82/   89]
Per-example loss in batch: 0.332037  [   84/   89]
Per-example loss in batch: 0.328067  [   86/   89]
Per-example loss in batch: 0.298434  [   88/   89]
Per-example loss in batch: 0.703094  [   89/   89]
Train Error: Avg loss: 0.29764742
validation Error: 
 Avg loss: 0.51133856 
 F1: 0.406936 
 Precision: 0.452467 
 Recall: 0.369731
 IoU: 0.255443

test Error: 
 Avg loss: 0.49154806 
 F1: 0.445132 
 Precision: 0.498157 
 Recall: 0.402309
 IoU: 0.286283

We have finished training iteration 126
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_124_.pth
Per-example loss in batch: 0.252152  [    2/   89]
Per-example loss in batch: 0.366975  [    4/   89]
Per-example loss in batch: 0.323481  [    6/   89]
Per-example loss in batch: 0.361569  [    8/   89]
Per-example loss in batch: 0.290527  [   10/   89]
Per-example loss in batch: 0.245914  [   12/   89]
Per-example loss in batch: 0.285656  [   14/   89]
Per-example loss in batch: 0.320466  [   16/   89]
Per-example loss in batch: 0.233693  [   18/   89]
Per-example loss in batch: 0.244130  [   20/   89]
Per-example loss in batch: 0.223818  [   22/   89]
Per-example loss in batch: 0.314932  [   24/   89]
Per-example loss in batch: 0.261929  [   26/   89]
Per-example loss in batch: 0.293535  [   28/   89]
Per-example loss in batch: 0.296767  [   30/   89]
Per-example loss in batch: 0.301948  [   32/   89]
Per-example loss in batch: 0.252894  [   34/   89]
Per-example loss in batch: 0.243328  [   36/   89]
Per-example loss in batch: 0.299047  [   38/   89]
Per-example loss in batch: 0.275681  [   40/   89]
Per-example loss in batch: 0.368229  [   42/   89]
Per-example loss in batch: 0.343374  [   44/   89]
Per-example loss in batch: 0.268386  [   46/   89]
Per-example loss in batch: 0.347699  [   48/   89]
Per-example loss in batch: 0.334153  [   50/   89]
Per-example loss in batch: 0.345114  [   52/   89]
Per-example loss in batch: 0.276660  [   54/   89]
Per-example loss in batch: 0.286806  [   56/   89]
Per-example loss in batch: 0.306799  [   58/   89]
Per-example loss in batch: 0.268440  [   60/   89]
Per-example loss in batch: 0.304475  [   62/   89]
Per-example loss in batch: 0.240506  [   64/   89]
Per-example loss in batch: 0.271937  [   66/   89]
Per-example loss in batch: 0.259735  [   68/   89]
Per-example loss in batch: 0.288722  [   70/   89]
Per-example loss in batch: 0.363886  [   72/   89]
Per-example loss in batch: 0.258533  [   74/   89]
Per-example loss in batch: 0.291947  [   76/   89]
Per-example loss in batch: 0.286317  [   78/   89]
Per-example loss in batch: 0.262659  [   80/   89]
Per-example loss in batch: 0.337170  [   82/   89]
Per-example loss in batch: 0.259059  [   84/   89]
Per-example loss in batch: 0.300297  [   86/   89]
Per-example loss in batch: 0.369592  [   88/   89]
Per-example loss in batch: 0.757557  [   89/   89]
Train Error: Avg loss: 0.29904979
validation Error: 
 Avg loss: 0.51061572 
 F1: 0.408520 
 Precision: 0.505599 
 Recall: 0.342716
 IoU: 0.256692

test Error: 
 Avg loss: 0.49091726 
 F1: 0.441589 
 Precision: 0.547571 
 Recall: 0.369980
 IoU: 0.283359

We have finished training iteration 127
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_125_.pth
Per-example loss in batch: 0.348160  [    2/   89]
Per-example loss in batch: 0.313769  [    4/   89]
Per-example loss in batch: 0.361667  [    6/   89]
Per-example loss in batch: 0.232259  [    8/   89]
Per-example loss in batch: 0.251705  [   10/   89]
Per-example loss in batch: 0.258723  [   12/   89]
Per-example loss in batch: 0.320300  [   14/   89]
Per-example loss in batch: 0.256712  [   16/   89]
Per-example loss in batch: 0.315801  [   18/   89]
Per-example loss in batch: 0.304467  [   20/   89]
Per-example loss in batch: 0.337054  [   22/   89]
Per-example loss in batch: 0.302123  [   24/   89]
Per-example loss in batch: 0.332990  [   26/   89]
Per-example loss in batch: 0.266049  [   28/   89]
Per-example loss in batch: 0.245349  [   30/   89]
Per-example loss in batch: 0.363625  [   32/   89]
Per-example loss in batch: 0.354571  [   34/   89]
Per-example loss in batch: 0.282484  [   36/   89]
Per-example loss in batch: 0.333997  [   38/   89]
Per-example loss in batch: 0.238280  [   40/   89]
Per-example loss in batch: 0.335593  [   42/   89]
Per-example loss in batch: 0.271236  [   44/   89]
Per-example loss in batch: 0.257905  [   46/   89]
Per-example loss in batch: 0.282031  [   48/   89]
Per-example loss in batch: 0.303855  [   50/   89]
Per-example loss in batch: 0.350446  [   52/   89]
Per-example loss in batch: 0.258439  [   54/   89]
Per-example loss in batch: 0.252737  [   56/   89]
Per-example loss in batch: 0.339533  [   58/   89]
Per-example loss in batch: 0.263072  [   60/   89]
Per-example loss in batch: 0.227962  [   62/   89]
Per-example loss in batch: 0.259085  [   64/   89]
Per-example loss in batch: 0.321437  [   66/   89]
Per-example loss in batch: 0.257115  [   68/   89]
Per-example loss in batch: 0.273677  [   70/   89]
Per-example loss in batch: 0.271059  [   72/   89]
Per-example loss in batch: 0.322209  [   74/   89]
Per-example loss in batch: 0.303250  [   76/   89]
Per-example loss in batch: 0.357828  [   78/   89]
Per-example loss in batch: 0.286341  [   80/   89]
Per-example loss in batch: 0.274136  [   82/   89]
Per-example loss in batch: 0.273253  [   84/   89]
Per-example loss in batch: 0.285111  [   86/   89]
Per-example loss in batch: 0.313236  [   88/   89]
Per-example loss in batch: 0.795739  [   89/   89]
Train Error: Avg loss: 0.30019101
validation Error: 
 Avg loss: 0.51055202 
 F1: 0.407918 
 Precision: 0.512467 
 Recall: 0.338800
 IoU: 0.256217

test Error: 
 Avg loss: 0.49072391 
 F1: 0.441715 
 Precision: 0.554387 
 Recall: 0.367106
 IoU: 0.283463

We have finished training iteration 128
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_126_.pth
Per-example loss in batch: 0.373105  [    2/   89]
Per-example loss in batch: 0.294504  [    4/   89]
Per-example loss in batch: 0.318858  [    6/   89]
Per-example loss in batch: 0.289830  [    8/   89]
Per-example loss in batch: 0.232500  [   10/   89]
Per-example loss in batch: 0.342963  [   12/   89]
Per-example loss in batch: 0.347254  [   14/   89]
Per-example loss in batch: 0.290879  [   16/   89]
Per-example loss in batch: 0.319732  [   18/   89]
Per-example loss in batch: 0.323655  [   20/   89]
Per-example loss in batch: 0.266833  [   22/   89]
Per-example loss in batch: 0.271438  [   24/   89]
Per-example loss in batch: 0.363093  [   26/   89]
Per-example loss in batch: 0.335189  [   28/   89]
Per-example loss in batch: 0.302046  [   30/   89]
Per-example loss in batch: 0.251547  [   32/   89]
Per-example loss in batch: 0.262833  [   34/   89]
Per-example loss in batch: 0.242766  [   36/   89]
Per-example loss in batch: 0.239429  [   38/   89]
Per-example loss in batch: 0.290180  [   40/   89]
Per-example loss in batch: 0.264821  [   42/   89]
Per-example loss in batch: 0.357041  [   44/   89]
Per-example loss in batch: 0.256830  [   46/   89]
Per-example loss in batch: 0.306431  [   48/   89]
Per-example loss in batch: 0.294041  [   50/   89]
Per-example loss in batch: 0.242054  [   52/   89]
Per-example loss in batch: 0.302925  [   54/   89]
Per-example loss in batch: 0.271068  [   56/   89]
Per-example loss in batch: 0.313344  [   58/   89]
Per-example loss in batch: 0.323808  [   60/   89]
Per-example loss in batch: 0.227169  [   62/   89]
Per-example loss in batch: 0.244737  [   64/   89]
Per-example loss in batch: 0.362662  [   66/   89]
Per-example loss in batch: 0.348853  [   68/   89]
Per-example loss in batch: 0.299022  [   70/   89]
Per-example loss in batch: 0.300485  [   72/   89]
Per-example loss in batch: 0.262049  [   74/   89]
Per-example loss in batch: 0.354693  [   76/   89]
Per-example loss in batch: 0.371873  [   78/   89]
Per-example loss in batch: 0.278940  [   80/   89]
Per-example loss in batch: 0.336460  [   82/   89]
Per-example loss in batch: 0.227691  [   84/   89]
Per-example loss in batch: 0.304918  [   86/   89]
Per-example loss in batch: 0.352084  [   88/   89]
Per-example loss in batch: 0.648172  [   89/   89]
Train Error: Avg loss: 0.30307234
validation Error: 
 Avg loss: 0.51151168 
 F1: 0.406830 
 Precision: 0.447737 
 Recall: 0.372772
 IoU: 0.255359

test Error: 
 Avg loss: 0.49175470 
 F1: 0.443570 
 Precision: 0.491083 
 Recall: 0.404440
 IoU: 0.284992

We have finished training iteration 129
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_127_.pth
Per-example loss in batch: 0.240590  [    2/   89]
Per-example loss in batch: 0.312340  [    4/   89]
Per-example loss in batch: 0.349056  [    6/   89]
Per-example loss in batch: 0.251411  [    8/   89]
Per-example loss in batch: 0.306643  [   10/   89]
Per-example loss in batch: 0.302376  [   12/   89]
Per-example loss in batch: 0.320730  [   14/   89]
Per-example loss in batch: 0.313206  [   16/   89]
Per-example loss in batch: 0.352028  [   18/   89]
Per-example loss in batch: 0.329460  [   20/   89]
Per-example loss in batch: 0.280523  [   22/   89]
Per-example loss in batch: 0.240557  [   24/   89]
Per-example loss in batch: 0.332833  [   26/   89]
Per-example loss in batch: 0.327837  [   28/   89]
Per-example loss in batch: 0.344120  [   30/   89]
Per-example loss in batch: 0.306979  [   32/   89]
Per-example loss in batch: 0.352612  [   34/   89]
Per-example loss in batch: 0.256073  [   36/   89]
Per-example loss in batch: 0.270651  [   38/   89]
Per-example loss in batch: 0.350908  [   40/   89]
Per-example loss in batch: 0.241927  [   42/   89]
Per-example loss in batch: 0.338046  [   44/   89]
Per-example loss in batch: 0.305783  [   46/   89]
Per-example loss in batch: 0.283359  [   48/   89]
Per-example loss in batch: 0.355861  [   50/   89]
Per-example loss in batch: 0.244166  [   52/   89]
Per-example loss in batch: 0.245312  [   54/   89]
Per-example loss in batch: 0.229703  [   56/   89]
Per-example loss in batch: 0.305038  [   58/   89]
Per-example loss in batch: 0.270764  [   60/   89]
Per-example loss in batch: 0.252529  [   62/   89]
Per-example loss in batch: 0.264343  [   64/   89]
Per-example loss in batch: 0.344970  [   66/   89]
Per-example loss in batch: 0.233143  [   68/   89]
Per-example loss in batch: 0.298327  [   70/   89]
Per-example loss in batch: 0.276615  [   72/   89]
Per-example loss in batch: 0.274992  [   74/   89]
Per-example loss in batch: 0.302931  [   76/   89]
Per-example loss in batch: 0.263570  [   78/   89]
Per-example loss in batch: 0.333603  [   80/   89]
Per-example loss in batch: 0.248542  [   82/   89]
Per-example loss in batch: 0.312873  [   84/   89]
Per-example loss in batch: 0.225775  [   86/   89]
Per-example loss in batch: 0.320151  [   88/   89]
Per-example loss in batch: 0.521343  [   89/   89]
Train Error: Avg loss: 0.29604336
validation Error: 
 Avg loss: 0.51091248 
 F1: 0.409381 
 Precision: 0.471464 
 Recall: 0.361747
 IoU: 0.257372

test Error: 
 Avg loss: 0.49128205 
 F1: 0.445466 
 Precision: 0.515195 
 Recall: 0.392361
 IoU: 0.286559

We have finished training iteration 130
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_128_.pth
Per-example loss in batch: 0.263856  [    2/   89]
Per-example loss in batch: 0.247855  [    4/   89]
Per-example loss in batch: 0.307963  [    6/   89]
Per-example loss in batch: 0.233831  [    8/   89]
Per-example loss in batch: 0.241216  [   10/   89]
Per-example loss in batch: 0.371251  [   12/   89]
Per-example loss in batch: 0.244007  [   14/   89]
Per-example loss in batch: 0.345956  [   16/   89]
Per-example loss in batch: 0.267140  [   18/   89]
Per-example loss in batch: 0.328759  [   20/   89]
Per-example loss in batch: 0.380467  [   22/   89]
Per-example loss in batch: 0.292921  [   24/   89]
Per-example loss in batch: 0.237024  [   26/   89]
Per-example loss in batch: 0.221577  [   28/   89]
Per-example loss in batch: 0.313556  [   30/   89]
Per-example loss in batch: 0.355273  [   32/   89]
Per-example loss in batch: 0.282332  [   34/   89]
Per-example loss in batch: 0.246544  [   36/   89]
Per-example loss in batch: 0.223140  [   38/   89]
Per-example loss in batch: 0.364617  [   40/   89]
Per-example loss in batch: 0.331112  [   42/   89]
Per-example loss in batch: 0.348674  [   44/   89]
Per-example loss in batch: 0.298360  [   46/   89]
Per-example loss in batch: 0.306612  [   48/   89]
Per-example loss in batch: 0.319594  [   50/   89]
Per-example loss in batch: 0.250358  [   52/   89]
Per-example loss in batch: 0.295710  [   54/   89]
Per-example loss in batch: 0.304694  [   56/   89]
Per-example loss in batch: 0.245767  [   58/   89]
Per-example loss in batch: 0.327795  [   60/   89]
Per-example loss in batch: 0.250239  [   62/   89]
Per-example loss in batch: 0.260612  [   64/   89]
Per-example loss in batch: 0.222041  [   66/   89]
Per-example loss in batch: 0.308292  [   68/   89]
Per-example loss in batch: 0.300228  [   70/   89]
Per-example loss in batch: 0.302444  [   72/   89]
Per-example loss in batch: 0.358426  [   74/   89]
Per-example loss in batch: 0.327658  [   76/   89]
Per-example loss in batch: 0.252928  [   78/   89]
Per-example loss in batch: 0.269094  [   80/   89]
Per-example loss in batch: 0.378371  [   82/   89]
Per-example loss in batch: 0.338155  [   84/   89]
Per-example loss in batch: 0.317536  [   86/   89]
Per-example loss in batch: 0.257673  [   88/   89]
Per-example loss in batch: 0.745808  [   89/   89]
Train Error: Avg loss: 0.29920362
validation Error: 
 Avg loss: 0.51010208 
 F1: 0.407422 
 Precision: 0.535319 
 Recall: 0.328852
 IoU: 0.255825

test Error: 
 Avg loss: 0.49034563 
 F1: 0.440067 
 Precision: 0.580112 
 Recall: 0.354489
 IoU: 0.282106

We have finished training iteration 131
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_129_.pth
Per-example loss in batch: 0.335553  [    2/   89]
Per-example loss in batch: 0.290265  [    4/   89]
Per-example loss in batch: 0.271553  [    6/   89]
Per-example loss in batch: 0.290508  [    8/   89]
Per-example loss in batch: 0.288265  [   10/   89]
Per-example loss in batch: 0.295643  [   12/   89]
Per-example loss in batch: 0.239159  [   14/   89]
Per-example loss in batch: 0.295590  [   16/   89]
Per-example loss in batch: 0.324303  [   18/   89]
Per-example loss in batch: 0.245259  [   20/   89]
Per-example loss in batch: 0.245446  [   22/   89]
Per-example loss in batch: 0.310761  [   24/   89]
Per-example loss in batch: 0.261786  [   26/   89]
Per-example loss in batch: 0.363017  [   28/   89]
Per-example loss in batch: 0.243703  [   30/   89]
Per-example loss in batch: 0.320998  [   32/   89]
Per-example loss in batch: 0.252032  [   34/   89]
Per-example loss in batch: 0.253105  [   36/   89]
Per-example loss in batch: 0.365869  [   38/   89]
Per-example loss in batch: 0.318716  [   40/   89]
Per-example loss in batch: 0.330424  [   42/   89]
Per-example loss in batch: 0.264796  [   44/   89]
Per-example loss in batch: 0.329302  [   46/   89]
Per-example loss in batch: 0.256105  [   48/   89]
Per-example loss in batch: 0.236707  [   50/   89]
Per-example loss in batch: 0.268487  [   52/   89]
Per-example loss in batch: 0.350973  [   54/   89]
Per-example loss in batch: 0.275998  [   56/   89]
Per-example loss in batch: 0.233673  [   58/   89]
Per-example loss in batch: 0.359049  [   60/   89]
Per-example loss in batch: 0.353340  [   62/   89]
Per-example loss in batch: 0.276763  [   64/   89]
Per-example loss in batch: 0.266724  [   66/   89]
Per-example loss in batch: 0.237527  [   68/   89]
Per-example loss in batch: 0.277058  [   70/   89]
Per-example loss in batch: 0.267688  [   72/   89]
Per-example loss in batch: 0.281764  [   74/   89]
Per-example loss in batch: 0.267617  [   76/   89]
Per-example loss in batch: 0.319966  [   78/   89]
Per-example loss in batch: 0.308891  [   80/   89]
Per-example loss in batch: 0.362903  [   82/   89]
Per-example loss in batch: 0.318064  [   84/   89]
Per-example loss in batch: 0.327428  [   86/   89]
Per-example loss in batch: 0.296124  [   88/   89]
Per-example loss in batch: 0.748870  [   89/   89]
Train Error: Avg loss: 0.29782776
validation Error: 
 Avg loss: 0.51107917 
 F1: 0.407547 
 Precision: 0.463465 
 Recall: 0.363669
 IoU: 0.255924

test Error: 
 Avg loss: 0.49152222 
 F1: 0.443448 
 Precision: 0.505590 
 Recall: 0.394910
 IoU: 0.284891

We have finished training iteration 132
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_130_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.278443  [    2/   89]
Per-example loss in batch: 0.249078  [    4/   89]
Per-example loss in batch: 0.311943  [    6/   89]
Per-example loss in batch: 0.368743  [    8/   89]
Per-example loss in batch: 0.269725  [   10/   89]
Per-example loss in batch: 0.281516  [   12/   89]
Per-example loss in batch: 0.244574  [   14/   89]
Per-example loss in batch: 0.333454  [   16/   89]
Per-example loss in batch: 0.225999  [   18/   89]
Per-example loss in batch: 0.231706  [   20/   89]
Per-example loss in batch: 0.226541  [   22/   89]
Per-example loss in batch: 0.289546  [   24/   89]
Per-example loss in batch: 0.337476  [   26/   89]
Per-example loss in batch: 0.275412  [   28/   89]
Per-example loss in batch: 0.356957  [   30/   89]
Per-example loss in batch: 0.321881  [   32/   89]
Per-example loss in batch: 0.339686  [   34/   89]
Per-example loss in batch: 0.332471  [   36/   89]
Per-example loss in batch: 0.237728  [   38/   89]
Per-example loss in batch: 0.275970  [   40/   89]
Per-example loss in batch: 0.249794  [   42/   89]
Per-example loss in batch: 0.331882  [   44/   89]
Per-example loss in batch: 0.326279  [   46/   89]
Per-example loss in batch: 0.365462  [   48/   89]
Per-example loss in batch: 0.300140  [   50/   89]
Per-example loss in batch: 0.269447  [   52/   89]
Per-example loss in batch: 0.238549  [   54/   89]
Per-example loss in batch: 0.230246  [   56/   89]
Per-example loss in batch: 0.254485  [   58/   89]
Per-example loss in batch: 0.265285  [   60/   89]
Per-example loss in batch: 0.329083  [   62/   89]
Per-example loss in batch: 0.290951  [   64/   89]
Per-example loss in batch: 0.323345  [   66/   89]
Per-example loss in batch: 0.327286  [   68/   89]
Per-example loss in batch: 0.378987  [   70/   89]
Per-example loss in batch: 0.357946  [   72/   89]
Per-example loss in batch: 0.262040  [   74/   89]
Per-example loss in batch: 0.289201  [   76/   89]
Per-example loss in batch: 0.281312  [   78/   89]
Per-example loss in batch: 0.346924  [   80/   89]
Per-example loss in batch: 0.337489  [   82/   89]
Per-example loss in batch: 0.322491  [   84/   89]
Per-example loss in batch: 0.279605  [   86/   89]
Per-example loss in batch: 0.251932  [   88/   89]
Per-example loss in batch: 0.694436  [   89/   89]
Train Error: Avg loss: 0.29991528
validation Error: 
 Avg loss: 0.51031758 
 F1: 0.409177 
 Precision: 0.493922 
 Recall: 0.349254
 IoU: 0.257211

test Error: 
 Avg loss: 0.49113569 
 F1: 0.445216 
 Precision: 0.536662 
 Recall: 0.380397
 IoU: 0.286352

We have finished training iteration 133
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_131_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.307820  [    2/   89]
Per-example loss in batch: 0.292695  [    4/   89]
Per-example loss in batch: 0.248181  [    6/   89]
Per-example loss in batch: 0.327560  [    8/   89]
Per-example loss in batch: 0.270597  [   10/   89]
Per-example loss in batch: 0.321023  [   12/   89]
Per-example loss in batch: 0.292788  [   14/   89]
Per-example loss in batch: 0.321350  [   16/   89]
Per-example loss in batch: 0.287151  [   18/   89]
Per-example loss in batch: 0.357994  [   20/   89]
Per-example loss in batch: 0.257807  [   22/   89]
Per-example loss in batch: 0.260256  [   24/   89]
Per-example loss in batch: 0.291231  [   26/   89]
Per-example loss in batch: 0.334488  [   28/   89]
Per-example loss in batch: 0.271526  [   30/   89]
Per-example loss in batch: 0.335402  [   32/   89]
Per-example loss in batch: 0.287184  [   34/   89]
Per-example loss in batch: 0.394817  [   36/   89]
Per-example loss in batch: 0.294534  [   38/   89]
Per-example loss in batch: 0.287329  [   40/   89]
Per-example loss in batch: 0.281004  [   42/   89]
Per-example loss in batch: 0.353140  [   44/   89]
Per-example loss in batch: 0.356910  [   46/   89]
Per-example loss in batch: 0.243640  [   48/   89]
Per-example loss in batch: 0.304606  [   50/   89]
Per-example loss in batch: 0.309420  [   52/   89]
Per-example loss in batch: 0.299003  [   54/   89]
Per-example loss in batch: 0.373135  [   56/   89]
Per-example loss in batch: 0.339116  [   58/   89]
Per-example loss in batch: 0.334034  [   60/   89]
Per-example loss in batch: 0.298094  [   62/   89]
Per-example loss in batch: 0.288738  [   64/   89]
Per-example loss in batch: 0.279469  [   66/   89]
Per-example loss in batch: 0.275340  [   68/   89]
Per-example loss in batch: 0.236158  [   70/   89]
Per-example loss in batch: 0.261420  [   72/   89]
Per-example loss in batch: 0.376787  [   74/   89]
Per-example loss in batch: 0.378844  [   76/   89]
Per-example loss in batch: 0.314552  [   78/   89]
Per-example loss in batch: 0.338100  [   80/   89]
Per-example loss in batch: 0.322559  [   82/   89]
Per-example loss in batch: 0.228178  [   84/   89]
Per-example loss in batch: 0.275764  [   86/   89]
Per-example loss in batch: 0.286933  [   88/   89]
Per-example loss in batch: 0.744560  [   89/   89]
Train Error: Avg loss: 0.30941466
validation Error: 
 Avg loss: 0.51013107 
 F1: 0.409473 
 Precision: 0.499878 
 Recall: 0.346760
 IoU: 0.257445

test Error: 
 Avg loss: 0.49067652 
 F1: 0.446774 
 Precision: 0.546701 
 Recall: 0.377732
 IoU: 0.287643

We have finished training iteration 134
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_132_.pth
Per-example loss in batch: 0.361504  [    2/   89]
Per-example loss in batch: 0.354587  [    4/   89]
Per-example loss in batch: 0.243194  [    6/   89]
Per-example loss in batch: 0.304421  [    8/   89]
Per-example loss in batch: 0.329723  [   10/   89]
Per-example loss in batch: 0.294337  [   12/   89]
Per-example loss in batch: 0.332306  [   14/   89]
Per-example loss in batch: 0.246465  [   16/   89]
Per-example loss in batch: 0.330341  [   18/   89]
Per-example loss in batch: 0.311057  [   20/   89]
Per-example loss in batch: 0.239435  [   22/   89]
Per-example loss in batch: 0.265170  [   24/   89]
Per-example loss in batch: 0.378490  [   26/   89]
Per-example loss in batch: 0.343942  [   28/   89]
Per-example loss in batch: 0.275506  [   30/   89]
Per-example loss in batch: 0.347795  [   32/   89]
Per-example loss in batch: 0.246510  [   34/   89]
Per-example loss in batch: 0.244833  [   36/   89]
Per-example loss in batch: 0.294969  [   38/   89]
Per-example loss in batch: 0.255546  [   40/   89]
Per-example loss in batch: 0.278725  [   42/   89]
Per-example loss in batch: 0.381630  [   44/   89]
Per-example loss in batch: 0.318716  [   46/   89]
Per-example loss in batch: 0.231849  [   48/   89]
Per-example loss in batch: 0.310501  [   50/   89]
Per-example loss in batch: 0.282085  [   52/   89]
Per-example loss in batch: 0.241340  [   54/   89]
Per-example loss in batch: 0.281314  [   56/   89]
Per-example loss in batch: 0.357273  [   58/   89]
Per-example loss in batch: 0.363768  [   60/   89]
Per-example loss in batch: 0.275255  [   62/   89]
Per-example loss in batch: 0.344348  [   64/   89]
Per-example loss in batch: 0.348451  [   66/   89]
Per-example loss in batch: 0.261598  [   68/   89]
Per-example loss in batch: 0.240883  [   70/   89]
Per-example loss in batch: 0.289358  [   72/   89]
Per-example loss in batch: 0.304071  [   74/   89]
Per-example loss in batch: 0.246339  [   76/   89]
Per-example loss in batch: 0.317127  [   78/   89]
Per-example loss in batch: 0.282579  [   80/   89]
Per-example loss in batch: 0.268592  [   82/   89]
Per-example loss in batch: 0.279346  [   84/   89]
Per-example loss in batch: 0.252688  [   86/   89]
Per-example loss in batch: 0.251220  [   88/   89]
Per-example loss in batch: 0.536333  [   89/   89]
Train Error: Avg loss: 0.29836759
validation Error: 
 Avg loss: 0.51079075 
 F1: 0.409172 
 Precision: 0.492833 
 Recall: 0.349792
 IoU: 0.257207

test Error: 
 Avg loss: 0.49123994 
 F1: 0.442275 
 Precision: 0.532111 
 Recall: 0.378391
 IoU: 0.283923

We have finished training iteration 135
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_133_.pth
Per-example loss in batch: 0.243353  [    2/   89]
Per-example loss in batch: 0.243476  [    4/   89]
Per-example loss in batch: 0.262559  [    6/   89]
Per-example loss in batch: 0.320785  [    8/   89]
Per-example loss in batch: 0.268411  [   10/   89]
Per-example loss in batch: 0.281764  [   12/   89]
Per-example loss in batch: 0.243143  [   14/   89]
Per-example loss in batch: 0.348653  [   16/   89]
Per-example loss in batch: 0.296194  [   18/   89]
Per-example loss in batch: 0.370261  [   20/   89]
Per-example loss in batch: 0.267895  [   22/   89]
Per-example loss in batch: 0.329271  [   24/   89]
Per-example loss in batch: 0.325138  [   26/   89]
Per-example loss in batch: 0.310163  [   28/   89]
Per-example loss in batch: 0.284597  [   30/   89]
Per-example loss in batch: 0.312643  [   32/   89]
Per-example loss in batch: 0.277629  [   34/   89]
Per-example loss in batch: 0.308308  [   36/   89]
Per-example loss in batch: 0.282962  [   38/   89]
Per-example loss in batch: 0.282486  [   40/   89]
Per-example loss in batch: 0.368661  [   42/   89]
Per-example loss in batch: 0.243081  [   44/   89]
Per-example loss in batch: 0.383104  [   46/   89]
Per-example loss in batch: 0.301490  [   48/   89]
Per-example loss in batch: 0.337588  [   50/   89]
Per-example loss in batch: 0.241649  [   52/   89]
Per-example loss in batch: 0.285398  [   54/   89]
Per-example loss in batch: 0.240382  [   56/   89]
Per-example loss in batch: 0.342649  [   58/   89]
Per-example loss in batch: 0.372702  [   60/   89]
Per-example loss in batch: 0.304261  [   62/   89]
Per-example loss in batch: 0.254189  [   64/   89]
Per-example loss in batch: 0.261453  [   66/   89]
Per-example loss in batch: 0.288341  [   68/   89]
Per-example loss in batch: 0.329056  [   70/   89]
Per-example loss in batch: 0.311873  [   72/   89]
Per-example loss in batch: 0.233885  [   74/   89]
Per-example loss in batch: 0.258454  [   76/   89]
Per-example loss in batch: 0.272336  [   78/   89]
Per-example loss in batch: 0.318112  [   80/   89]
Per-example loss in batch: 0.286451  [   82/   89]
Per-example loss in batch: 0.221224  [   84/   89]
Per-example loss in batch: 0.322313  [   86/   89]
Per-example loss in batch: 0.275982  [   88/   89]
Per-example loss in batch: 0.467643  [   89/   89]
Train Error: Avg loss: 0.29546394
validation Error: 
 Avg loss: 0.50988030 
 F1: 0.411378 
 Precision: 0.510751 
 Recall: 0.344376
 IoU: 0.258953

test Error: 
 Avg loss: 0.49070064 
 F1: 0.445058 
 Precision: 0.555987 
 Recall: 0.371032
 IoU: 0.286222

We have finished training iteration 136
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_134_.pth
Per-example loss in batch: 0.270316  [    2/   89]
Per-example loss in batch: 0.356025  [    4/   89]
Per-example loss in batch: 0.250037  [    6/   89]
Per-example loss in batch: 0.324343  [    8/   89]
Per-example loss in batch: 0.275866  [   10/   89]
Per-example loss in batch: 0.273021  [   12/   89]
Per-example loss in batch: 0.297450  [   14/   89]
Per-example loss in batch: 0.365417  [   16/   89]
Per-example loss in batch: 0.348607  [   18/   89]
Per-example loss in batch: 0.265790  [   20/   89]
Per-example loss in batch: 0.343221  [   22/   89]
Per-example loss in batch: 0.345356  [   24/   89]
Per-example loss in batch: 0.260621  [   26/   89]
Per-example loss in batch: 0.229437  [   28/   89]
Per-example loss in batch: 0.270219  [   30/   89]
Per-example loss in batch: 0.281137  [   32/   89]
Per-example loss in batch: 0.248350  [   34/   89]
Per-example loss in batch: 0.287982  [   36/   89]
Per-example loss in batch: 0.294858  [   38/   89]
Per-example loss in batch: 0.224947  [   40/   89]
Per-example loss in batch: 0.330328  [   42/   89]
Per-example loss in batch: 0.292518  [   44/   89]
Per-example loss in batch: 0.313263  [   46/   89]
Per-example loss in batch: 0.346512  [   48/   89]
Per-example loss in batch: 0.265223  [   50/   89]
Per-example loss in batch: 0.307079  [   52/   89]
Per-example loss in batch: 0.254023  [   54/   89]
Per-example loss in batch: 0.371690  [   56/   89]
Per-example loss in batch: 0.290283  [   58/   89]
Per-example loss in batch: 0.223581  [   60/   89]
Per-example loss in batch: 0.324719  [   62/   89]
Per-example loss in batch: 0.359581  [   64/   89]
Per-example loss in batch: 0.338220  [   66/   89]
Per-example loss in batch: 0.271800  [   68/   89]
Per-example loss in batch: 0.302600  [   70/   89]
Per-example loss in batch: 0.297358  [   72/   89]
Per-example loss in batch: 0.279980  [   74/   89]
Per-example loss in batch: 0.302120  [   76/   89]
Per-example loss in batch: 0.378769  [   78/   89]
Per-example loss in batch: 0.254140  [   80/   89]
Per-example loss in batch: 0.302693  [   82/   89]
Per-example loss in batch: 0.246997  [   84/   89]
Per-example loss in batch: 0.335581  [   86/   89]
Per-example loss in batch: 0.328930  [   88/   89]
Per-example loss in batch: 0.476322  [   89/   89]
Train Error: Avg loss: 0.30043038
validation Error: 
 Avg loss: 0.51023434 
 F1: 0.411208 
 Precision: 0.498146 
 Recall: 0.350107
 IoU: 0.258818

test Error: 
 Avg loss: 0.49081721 
 F1: 0.446890 
 Precision: 0.543662 
 Recall: 0.379363
 IoU: 0.287739

We have finished training iteration 137
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_135_.pth
Per-example loss in batch: 0.280837  [    2/   89]
Per-example loss in batch: 0.326945  [    4/   89]
Per-example loss in batch: 0.251349  [    6/   89]
Per-example loss in batch: 0.269061  [    8/   89]
Per-example loss in batch: 0.297934  [   10/   89]
Per-example loss in batch: 0.274770  [   12/   89]
Per-example loss in batch: 0.276760  [   14/   89]
Per-example loss in batch: 0.320363  [   16/   89]
Per-example loss in batch: 0.239721  [   18/   89]
Per-example loss in batch: 0.284569  [   20/   89]
Per-example loss in batch: 0.290029  [   22/   89]
Per-example loss in batch: 0.334217  [   24/   89]
Per-example loss in batch: 0.282722  [   26/   89]
Per-example loss in batch: 0.346006  [   28/   89]
Per-example loss in batch: 0.308913  [   30/   89]
Per-example loss in batch: 0.268517  [   32/   89]
Per-example loss in batch: 0.399318  [   34/   89]
Per-example loss in batch: 0.265333  [   36/   89]
Per-example loss in batch: 0.353562  [   38/   89]
Per-example loss in batch: 0.308134  [   40/   89]
Per-example loss in batch: 0.321168  [   42/   89]
Per-example loss in batch: 0.234468  [   44/   89]
Per-example loss in batch: 0.273446  [   46/   89]
Per-example loss in batch: 0.224745  [   48/   89]
Per-example loss in batch: 0.303049  [   50/   89]
Per-example loss in batch: 0.324304  [   52/   89]
Per-example loss in batch: 0.276870  [   54/   89]
Per-example loss in batch: 0.230786  [   56/   89]
Per-example loss in batch: 0.310378  [   58/   89]
Per-example loss in batch: 0.266838  [   60/   89]
Per-example loss in batch: 0.331192  [   62/   89]
Per-example loss in batch: 0.246084  [   64/   89]
Per-example loss in batch: 0.351566  [   66/   89]
Per-example loss in batch: 0.326426  [   68/   89]
Per-example loss in batch: 0.256589  [   70/   89]
Per-example loss in batch: 0.264769  [   72/   89]
Per-example loss in batch: 0.330793  [   74/   89]
Per-example loss in batch: 0.251302  [   76/   89]
Per-example loss in batch: 0.311946  [   78/   89]
Per-example loss in batch: 0.368750  [   80/   89]
Per-example loss in batch: 0.252578  [   82/   89]
Per-example loss in batch: 0.286530  [   84/   89]
Per-example loss in batch: 0.328069  [   86/   89]
Per-example loss in batch: 0.359022  [   88/   89]
Per-example loss in batch: 0.567160  [   89/   89]
Train Error: Avg loss: 0.29874849
validation Error: 
 Avg loss: 0.51022684 
 F1: 0.412395 
 Precision: 0.509526 
 Recall: 0.346367
 IoU: 0.259759

test Error: 
 Avg loss: 0.49055079 
 F1: 0.445131 
 Precision: 0.554103 
 Recall: 0.371977
 IoU: 0.286282

We have finished training iteration 138
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_136_.pth
Per-example loss in batch: 0.327940  [    2/   89]
Per-example loss in batch: 0.261276  [    4/   89]
Per-example loss in batch: 0.305429  [    6/   89]
Per-example loss in batch: 0.332377  [    8/   89]
Per-example loss in batch: 0.302798  [   10/   89]
Per-example loss in batch: 0.306638  [   12/   89]
Per-example loss in batch: 0.226866  [   14/   89]
Per-example loss in batch: 0.343309  [   16/   89]
Per-example loss in batch: 0.224195  [   18/   89]
Per-example loss in batch: 0.359753  [   20/   89]
Per-example loss in batch: 0.268786  [   22/   89]
Per-example loss in batch: 0.313900  [   24/   89]
Per-example loss in batch: 0.375977  [   26/   89]
Per-example loss in batch: 0.314369  [   28/   89]
Per-example loss in batch: 0.323109  [   30/   89]
Per-example loss in batch: 0.362988  [   32/   89]
Per-example loss in batch: 0.256903  [   34/   89]
Per-example loss in batch: 0.322004  [   36/   89]
Per-example loss in batch: 0.327553  [   38/   89]
Per-example loss in batch: 0.281673  [   40/   89]
Per-example loss in batch: 0.298315  [   42/   89]
Per-example loss in batch: 0.351069  [   44/   89]
Per-example loss in batch: 0.238228  [   46/   89]
Per-example loss in batch: 0.339307  [   48/   89]
Per-example loss in batch: 0.332901  [   50/   89]
Per-example loss in batch: 0.231588  [   52/   89]
Per-example loss in batch: 0.334435  [   54/   89]
Per-example loss in batch: 0.266769  [   56/   89]
Per-example loss in batch: 0.349628  [   58/   89]
Per-example loss in batch: 0.375239  [   60/   89]
Per-example loss in batch: 0.284206  [   62/   89]
Per-example loss in batch: 0.323947  [   64/   89]
Per-example loss in batch: 0.313826  [   66/   89]
Per-example loss in batch: 0.236587  [   68/   89]
Per-example loss in batch: 0.344101  [   70/   89]
Per-example loss in batch: 0.286401  [   72/   89]
Per-example loss in batch: 0.225863  [   74/   89]
Per-example loss in batch: 0.295644  [   76/   89]
Per-example loss in batch: 0.278072  [   78/   89]
Per-example loss in batch: 0.237272  [   80/   89]
Per-example loss in batch: 0.232731  [   82/   89]
Per-example loss in batch: 0.312762  [   84/   89]
Per-example loss in batch: 0.248376  [   86/   89]
Per-example loss in batch: 0.244451  [   88/   89]
Per-example loss in batch: 0.629276  [   89/   89]
Train Error: Avg loss: 0.30189211
validation Error: 
 Avg loss: 0.51031815 
 F1: 0.410193 
 Precision: 0.515234 
 Recall: 0.340728
 IoU: 0.258014

test Error: 
 Avg loss: 0.49053576 
 F1: 0.445204 
 Precision: 0.556031 
 Recall: 0.371215
 IoU: 0.286342

We have finished training iteration 139
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_137_.pth
Per-example loss in batch: 0.326115  [    2/   89]
Per-example loss in batch: 0.276729  [    4/   89]
Per-example loss in batch: 0.244158  [    6/   89]
Per-example loss in batch: 0.295823  [    8/   89]
Per-example loss in batch: 0.365567  [   10/   89]
Per-example loss in batch: 0.264482  [   12/   89]
Per-example loss in batch: 0.314149  [   14/   89]
Per-example loss in batch: 0.348893  [   16/   89]
Per-example loss in batch: 0.286663  [   18/   89]
Per-example loss in batch: 0.375362  [   20/   89]
Per-example loss in batch: 0.280377  [   22/   89]
Per-example loss in batch: 0.251383  [   24/   89]
Per-example loss in batch: 0.322956  [   26/   89]
Per-example loss in batch: 0.248083  [   28/   89]
Per-example loss in batch: 0.253566  [   30/   89]
Per-example loss in batch: 0.317960  [   32/   89]
Per-example loss in batch: 0.329872  [   34/   89]
Per-example loss in batch: 0.296702  [   36/   89]
Per-example loss in batch: 0.225281  [   38/   89]
Per-example loss in batch: 0.359376  [   40/   89]
Per-example loss in batch: 0.276468  [   42/   89]
Per-example loss in batch: 0.240764  [   44/   89]
Per-example loss in batch: 0.261254  [   46/   89]
Per-example loss in batch: 0.330327  [   48/   89]
Per-example loss in batch: 0.242564  [   50/   89]
Per-example loss in batch: 0.356230  [   52/   89]
Per-example loss in batch: 0.288412  [   54/   89]
Per-example loss in batch: 0.289316  [   56/   89]
Per-example loss in batch: 0.239150  [   58/   89]
Per-example loss in batch: 0.265531  [   60/   89]
Per-example loss in batch: 0.310087  [   62/   89]
Per-example loss in batch: 0.291305  [   64/   89]
Per-example loss in batch: 0.259495  [   66/   89]
Per-example loss in batch: 0.347771  [   68/   89]
Per-example loss in batch: 0.301018  [   70/   89]
Per-example loss in batch: 0.335800  [   72/   89]
Per-example loss in batch: 0.322095  [   74/   89]
Per-example loss in batch: 0.310516  [   76/   89]
Per-example loss in batch: 0.316693  [   78/   89]
Per-example loss in batch: 0.258697  [   80/   89]
Per-example loss in batch: 0.304909  [   82/   89]
Per-example loss in batch: 0.298343  [   84/   89]
Per-example loss in batch: 0.271539  [   86/   89]
Per-example loss in batch: 0.274820  [   88/   89]
Per-example loss in batch: 0.546900  [   89/   89]
Train Error: Avg loss: 0.29775396
validation Error: 
 Avg loss: 0.51078024 
 F1: 0.408881 
 Precision: 0.492048 
 Recall: 0.349763
 IoU: 0.256977

test Error: 
 Avg loss: 0.49107964 
 F1: 0.442724 
 Precision: 0.533213 
 Recall: 0.378492
 IoU: 0.284294

We have finished training iteration 140
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_138_.pth
Per-example loss in batch: 0.278409  [    2/   89]
Per-example loss in batch: 0.366926  [    4/   89]
Per-example loss in batch: 0.267271  [    6/   89]
Per-example loss in batch: 0.334835  [    8/   89]
Per-example loss in batch: 0.327112  [   10/   89]
Per-example loss in batch: 0.270179  [   12/   89]
Per-example loss in batch: 0.276458  [   14/   89]
Per-example loss in batch: 0.376235  [   16/   89]
Per-example loss in batch: 0.288634  [   18/   89]
Per-example loss in batch: 0.308736  [   20/   89]
Per-example loss in batch: 0.295400  [   22/   89]
Per-example loss in batch: 0.372332  [   24/   89]
Per-example loss in batch: 0.290684  [   26/   89]
Per-example loss in batch: 0.238976  [   28/   89]
Per-example loss in batch: 0.313667  [   30/   89]
Per-example loss in batch: 0.289870  [   32/   89]
Per-example loss in batch: 0.295292  [   34/   89]
Per-example loss in batch: 0.270502  [   36/   89]
Per-example loss in batch: 0.311527  [   38/   89]
Per-example loss in batch: 0.244729  [   40/   89]
Per-example loss in batch: 0.270141  [   42/   89]
Per-example loss in batch: 0.361732  [   44/   89]
Per-example loss in batch: 0.358127  [   46/   89]
Per-example loss in batch: 0.323327  [   48/   89]
Per-example loss in batch: 0.258474  [   50/   89]
Per-example loss in batch: 0.249798  [   52/   89]
Per-example loss in batch: 0.349492  [   54/   89]
Per-example loss in batch: 0.310614  [   56/   89]
Per-example loss in batch: 0.261428  [   58/   89]
Per-example loss in batch: 0.317854  [   60/   89]
Per-example loss in batch: 0.311331  [   62/   89]
Per-example loss in batch: 0.272905  [   64/   89]
Per-example loss in batch: 0.282155  [   66/   89]
Per-example loss in batch: 0.260183  [   68/   89]
Per-example loss in batch: 0.319067  [   70/   89]
Per-example loss in batch: 0.249647  [   72/   89]
Per-example loss in batch: 0.298677  [   74/   89]
Per-example loss in batch: 0.317226  [   76/   89]
Per-example loss in batch: 0.227765  [   78/   89]
Per-example loss in batch: 0.340348  [   80/   89]
Per-example loss in batch: 0.277818  [   82/   89]
Per-example loss in batch: 0.291262  [   84/   89]
Per-example loss in batch: 0.238476  [   86/   89]
Per-example loss in batch: 0.223459  [   88/   89]
Per-example loss in batch: 0.710543  [   89/   89]
Train Error: Avg loss: 0.29987302
validation Error: 
 Avg loss: 0.51106456 
 F1: 0.412264 
 Precision: 0.468212 
 Recall: 0.368260
 IoU: 0.259655

test Error: 
 Avg loss: 0.49118097 
 F1: 0.450894 
 Precision: 0.515825 
 Recall: 0.400483
 IoU: 0.291067

We have finished training iteration 141
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_139_.pth
Per-example loss in batch: 0.303970  [    2/   89]
Per-example loss in batch: 0.283048  [    4/   89]
Per-example loss in batch: 0.239664  [    6/   89]
Per-example loss in batch: 0.268704  [    8/   89]
Per-example loss in batch: 0.258071  [   10/   89]
Per-example loss in batch: 0.268135  [   12/   89]
Per-example loss in batch: 0.343413  [   14/   89]
Per-example loss in batch: 0.250143  [   16/   89]
Per-example loss in batch: 0.320462  [   18/   89]
Per-example loss in batch: 0.372397  [   20/   89]
Per-example loss in batch: 0.367756  [   22/   89]
Per-example loss in batch: 0.242579  [   24/   89]
Per-example loss in batch: 0.306677  [   26/   89]
Per-example loss in batch: 0.341572  [   28/   89]
Per-example loss in batch: 0.278996  [   30/   89]
Per-example loss in batch: 0.226069  [   32/   89]
Per-example loss in batch: 0.267588  [   34/   89]
Per-example loss in batch: 0.219877  [   36/   89]
Per-example loss in batch: 0.267781  [   38/   89]
Per-example loss in batch: 0.351655  [   40/   89]
Per-example loss in batch: 0.296371  [   42/   89]
Per-example loss in batch: 0.327512  [   44/   89]
Per-example loss in batch: 0.330274  [   46/   89]
Per-example loss in batch: 0.241653  [   48/   89]
Per-example loss in batch: 0.309000  [   50/   89]
Per-example loss in batch: 0.248696  [   52/   89]
Per-example loss in batch: 0.238153  [   54/   89]
Per-example loss in batch: 0.268939  [   56/   89]
Per-example loss in batch: 0.270698  [   58/   89]
Per-example loss in batch: 0.325938  [   60/   89]
Per-example loss in batch: 0.254851  [   62/   89]
Per-example loss in batch: 0.317692  [   64/   89]
Per-example loss in batch: 0.352704  [   66/   89]
Per-example loss in batch: 0.313023  [   68/   89]
Per-example loss in batch: 0.280504  [   70/   89]
Per-example loss in batch: 0.293252  [   72/   89]
Per-example loss in batch: 0.351270  [   74/   89]
Per-example loss in batch: 0.263808  [   76/   89]
Per-example loss in batch: 0.258305  [   78/   89]
Per-example loss in batch: 0.332135  [   80/   89]
Per-example loss in batch: 0.323051  [   82/   89]
Per-example loss in batch: 0.366517  [   84/   89]
Per-example loss in batch: 0.243518  [   86/   89]
Per-example loss in batch: 0.310995  [   88/   89]
Per-example loss in batch: 0.699869  [   89/   89]
Train Error: Avg loss: 0.29769327
validation Error: 
 Avg loss: 0.51048338 
 F1: 0.410994 
 Precision: 0.514846 
 Recall: 0.342006
 IoU: 0.258648

test Error: 
 Avg loss: 0.49073344 
 F1: 0.443869 
 Precision: 0.554758 
 Recall: 0.369926
 IoU: 0.285239

We have finished training iteration 142
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_140_.pth
Per-example loss in batch: 0.321002  [    2/   89]
Per-example loss in batch: 0.257213  [    4/   89]
Per-example loss in batch: 0.327445  [    6/   89]
Per-example loss in batch: 0.311769  [    8/   89]
Per-example loss in batch: 0.266397  [   10/   89]
Per-example loss in batch: 0.261826  [   12/   89]
Per-example loss in batch: 0.281848  [   14/   89]
Per-example loss in batch: 0.249892  [   16/   89]
Per-example loss in batch: 0.310925  [   18/   89]
Per-example loss in batch: 0.243940  [   20/   89]
Per-example loss in batch: 0.238247  [   22/   89]
Per-example loss in batch: 0.328922  [   24/   89]
Per-example loss in batch: 0.310232  [   26/   89]
Per-example loss in batch: 0.245013  [   28/   89]
Per-example loss in batch: 0.279678  [   30/   89]
Per-example loss in batch: 0.228178  [   32/   89]
Per-example loss in batch: 0.366842  [   34/   89]
Per-example loss in batch: 0.348779  [   36/   89]
Per-example loss in batch: 0.298827  [   38/   89]
Per-example loss in batch: 0.297069  [   40/   89]
Per-example loss in batch: 0.240418  [   42/   89]
Per-example loss in batch: 0.301453  [   44/   89]
Per-example loss in batch: 0.275225  [   46/   89]
Per-example loss in batch: 0.306371  [   48/   89]
Per-example loss in batch: 0.242165  [   50/   89]
Per-example loss in batch: 0.315724  [   52/   89]
Per-example loss in batch: 0.305450  [   54/   89]
Per-example loss in batch: 0.231582  [   56/   89]
Per-example loss in batch: 0.334932  [   58/   89]
Per-example loss in batch: 0.373129  [   60/   89]
Per-example loss in batch: 0.299488  [   62/   89]
Per-example loss in batch: 0.302870  [   64/   89]
Per-example loss in batch: 0.344220  [   66/   89]
Per-example loss in batch: 0.347222  [   68/   89]
Per-example loss in batch: 0.306827  [   70/   89]
Per-example loss in batch: 0.300419  [   72/   89]
Per-example loss in batch: 0.350484  [   74/   89]
Per-example loss in batch: 0.237405  [   76/   89]
Per-example loss in batch: 0.246465  [   78/   89]
Per-example loss in batch: 0.234007  [   80/   89]
Per-example loss in batch: 0.334607  [   82/   89]
Per-example loss in batch: 0.327769  [   84/   89]
Per-example loss in batch: 0.354238  [   86/   89]
Per-example loss in batch: 0.277608  [   88/   89]
Per-example loss in batch: 0.582202  [   89/   89]
Train Error: Avg loss: 0.29787021
validation Error: 
 Avg loss: 0.51073274 
 F1: 0.413501 
 Precision: 0.485244 
 Recall: 0.360239
 IoU: 0.260637

test Error: 
 Avg loss: 0.49108831 
 F1: 0.449452 
 Precision: 0.531206 
 Recall: 0.389506
 IoU: 0.289867

We have finished training iteration 143
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_141_.pth
Per-example loss in batch: 0.276132  [    2/   89]
Per-example loss in batch: 0.365977  [    4/   89]
Per-example loss in batch: 0.243259  [    6/   89]
Per-example loss in batch: 0.287286  [    8/   89]
Per-example loss in batch: 0.352544  [   10/   89]
Per-example loss in batch: 0.268276  [   12/   89]
Per-example loss in batch: 0.215337  [   14/   89]
Per-example loss in batch: 0.324652  [   16/   89]
Per-example loss in batch: 0.385913  [   18/   89]
Per-example loss in batch: 0.303537  [   20/   89]
Per-example loss in batch: 0.251047  [   22/   89]
Per-example loss in batch: 0.271017  [   24/   89]
Per-example loss in batch: 0.364150  [   26/   89]
Per-example loss in batch: 0.350911  [   28/   89]
Per-example loss in batch: 0.290031  [   30/   89]
Per-example loss in batch: 0.273047  [   32/   89]
Per-example loss in batch: 0.311252  [   34/   89]
Per-example loss in batch: 0.277052  [   36/   89]
Per-example loss in batch: 0.331602  [   38/   89]
Per-example loss in batch: 0.321350  [   40/   89]
Per-example loss in batch: 0.241785  [   42/   89]
Per-example loss in batch: 0.333218  [   44/   89]
Per-example loss in batch: 0.274689  [   46/   89]
Per-example loss in batch: 0.280622  [   48/   89]
Per-example loss in batch: 0.223597  [   50/   89]
Per-example loss in batch: 0.356532  [   52/   89]
Per-example loss in batch: 0.269016  [   54/   89]
Per-example loss in batch: 0.250199  [   56/   89]
Per-example loss in batch: 0.265907  [   58/   89]
Per-example loss in batch: 0.366408  [   60/   89]
Per-example loss in batch: 0.293008  [   62/   89]
Per-example loss in batch: 0.270802  [   64/   89]
Per-example loss in batch: 0.239369  [   66/   89]
Per-example loss in batch: 0.322847  [   68/   89]
Per-example loss in batch: 0.297094  [   70/   89]
Per-example loss in batch: 0.363591  [   72/   89]
Per-example loss in batch: 0.336608  [   74/   89]
Per-example loss in batch: 0.296948  [   76/   89]
Per-example loss in batch: 0.310562  [   78/   89]
Per-example loss in batch: 0.320638  [   80/   89]
Per-example loss in batch: 0.250212  [   82/   89]
Per-example loss in batch: 0.240646  [   84/   89]
Per-example loss in batch: 0.271195  [   86/   89]
Per-example loss in batch: 0.270256  [   88/   89]
Per-example loss in batch: 0.750952  [   89/   89]
Train Error: Avg loss: 0.30079993
validation Error: 
 Avg loss: 0.51069027 
 F1: 0.410532 
 Precision: 0.467463 
 Recall: 0.365962
 IoU: 0.258283

test Error: 
 Avg loss: 0.49138061 
 F1: 0.448643 
 Precision: 0.509564 
 Recall: 0.400734
 IoU: 0.289194

We have finished training iteration 144
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_142_.pth
Per-example loss in batch: 0.332264  [    2/   89]
Per-example loss in batch: 0.249647  [    4/   89]
Per-example loss in batch: 0.249842  [    6/   89]
Per-example loss in batch: 0.300199  [    8/   89]
Per-example loss in batch: 0.276297  [   10/   89]
Per-example loss in batch: 0.249760  [   12/   89]
Per-example loss in batch: 0.249163  [   14/   89]
Per-example loss in batch: 0.268688  [   16/   89]
Per-example loss in batch: 0.278332  [   18/   89]
Per-example loss in batch: 0.346311  [   20/   89]
Per-example loss in batch: 0.363067  [   22/   89]
Per-example loss in batch: 0.280087  [   24/   89]
Per-example loss in batch: 0.290623  [   26/   89]
Per-example loss in batch: 0.253542  [   28/   89]
Per-example loss in batch: 0.302837  [   30/   89]
Per-example loss in batch: 0.244952  [   32/   89]
Per-example loss in batch: 0.353512  [   34/   89]
Per-example loss in batch: 0.294048  [   36/   89]
Per-example loss in batch: 0.324490  [   38/   89]
Per-example loss in batch: 0.308724  [   40/   89]
Per-example loss in batch: 0.319025  [   42/   89]
Per-example loss in batch: 0.334801  [   44/   89]
Per-example loss in batch: 0.296367  [   46/   89]
Per-example loss in batch: 0.293846  [   48/   89]
Per-example loss in batch: 0.296386  [   50/   89]
Per-example loss in batch: 0.248315  [   52/   89]
Per-example loss in batch: 0.335533  [   54/   89]
Per-example loss in batch: 0.330423  [   56/   89]
Per-example loss in batch: 0.269968  [   58/   89]
Per-example loss in batch: 0.389941  [   60/   89]
Per-example loss in batch: 0.345159  [   62/   89]
Per-example loss in batch: 0.299138  [   64/   89]
Per-example loss in batch: 0.288541  [   66/   89]
Per-example loss in batch: 0.296789  [   68/   89]
Per-example loss in batch: 0.239980  [   70/   89]
Per-example loss in batch: 0.438362  [   72/   89]
Per-example loss in batch: 0.244020  [   74/   89]
Per-example loss in batch: 0.310308  [   76/   89]
Per-example loss in batch: 0.320746  [   78/   89]
Per-example loss in batch: 0.385408  [   80/   89]
Per-example loss in batch: 0.266398  [   82/   89]
Per-example loss in batch: 0.263428  [   84/   89]
Per-example loss in batch: 0.272945  [   86/   89]
Per-example loss in batch: 0.297800  [   88/   89]
Per-example loss in batch: 0.578354  [   89/   89]
Train Error: Avg loss: 0.30312775
validation Error: 
 Avg loss: 0.51001106 
 F1: 0.409118 
 Precision: 0.550976 
 Recall: 0.325351
 IoU: 0.257164

test Error: 
 Avg loss: 0.49029044 
 F1: 0.439347 
 Precision: 0.588288 
 Recall: 0.350587
 IoU: 0.281515

We have finished training iteration 145
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_143_.pth
Per-example loss in batch: 0.306535  [    2/   89]
Per-example loss in batch: 0.234106  [    4/   89]
Per-example loss in batch: 0.363097  [    6/   89]
Per-example loss in batch: 0.316441  [    8/   89]
Per-example loss in batch: 0.342946  [   10/   89]
Per-example loss in batch: 0.304651  [   12/   89]
Per-example loss in batch: 0.332680  [   14/   89]
Per-example loss in batch: 0.353890  [   16/   89]
Per-example loss in batch: 0.249607  [   18/   89]
Per-example loss in batch: 0.318467  [   20/   89]
Per-example loss in batch: 0.312023  [   22/   89]
Per-example loss in batch: 0.291343  [   24/   89]
Per-example loss in batch: 0.353011  [   26/   89]
Per-example loss in batch: 0.313619  [   28/   89]
Per-example loss in batch: 0.269088  [   30/   89]
Per-example loss in batch: 0.301943  [   32/   89]
Per-example loss in batch: 0.233314  [   34/   89]
Per-example loss in batch: 0.269929  [   36/   89]
Per-example loss in batch: 0.348494  [   38/   89]
Per-example loss in batch: 0.355627  [   40/   89]
Per-example loss in batch: 0.295371  [   42/   89]
Per-example loss in batch: 0.246476  [   44/   89]
Per-example loss in batch: 0.232291  [   46/   89]
Per-example loss in batch: 0.262916  [   48/   89]
Per-example loss in batch: 0.246072  [   50/   89]
Per-example loss in batch: 0.236582  [   52/   89]
Per-example loss in batch: 0.249673  [   54/   89]
Per-example loss in batch: 0.308407  [   56/   89]
Per-example loss in batch: 0.311646  [   58/   89]
Per-example loss in batch: 0.299335  [   60/   89]
Per-example loss in batch: 0.258884  [   62/   89]
Per-example loss in batch: 0.325086  [   64/   89]
Per-example loss in batch: 0.353081  [   66/   89]
Per-example loss in batch: 0.257414  [   68/   89]
Per-example loss in batch: 0.331188  [   70/   89]
Per-example loss in batch: 0.304378  [   72/   89]
Per-example loss in batch: 0.241478  [   74/   89]
Per-example loss in batch: 0.361355  [   76/   89]
Per-example loss in batch: 0.258934  [   78/   89]
Per-example loss in batch: 0.280841  [   80/   89]
Per-example loss in batch: 0.268672  [   82/   89]
Per-example loss in batch: 0.294991  [   84/   89]
Per-example loss in batch: 0.295442  [   86/   89]
Per-example loss in batch: 0.306634  [   88/   89]
Per-example loss in batch: 0.608633  [   89/   89]
Train Error: Avg loss: 0.29892746
validation Error: 
 Avg loss: 0.51048347 
 F1: 0.411946 
 Precision: 0.473092 
 Recall: 0.364797
 IoU: 0.259403

test Error: 
 Avg loss: 0.49125839 
 F1: 0.447695 
 Precision: 0.517171 
 Recall: 0.394674
 IoU: 0.288406

We have finished training iteration 146
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_144_.pth
Per-example loss in batch: 0.292015  [    2/   89]
Per-example loss in batch: 0.277202  [    4/   89]
Per-example loss in batch: 0.241106  [    6/   89]
Per-example loss in batch: 0.331853  [    8/   89]
Per-example loss in batch: 0.304850  [   10/   89]
Per-example loss in batch: 0.240951  [   12/   89]
Per-example loss in batch: 0.244771  [   14/   89]
Per-example loss in batch: 0.322300  [   16/   89]
Per-example loss in batch: 0.297289  [   18/   89]
Per-example loss in batch: 0.337114  [   20/   89]
Per-example loss in batch: 0.288959  [   22/   89]
Per-example loss in batch: 0.277432  [   24/   89]
Per-example loss in batch: 0.350430  [   26/   89]
Per-example loss in batch: 0.245378  [   28/   89]
Per-example loss in batch: 0.312868  [   30/   89]
Per-example loss in batch: 0.285197  [   32/   89]
Per-example loss in batch: 0.286981  [   34/   89]
Per-example loss in batch: 0.299424  [   36/   89]
Per-example loss in batch: 0.360038  [   38/   89]
Per-example loss in batch: 0.278018  [   40/   89]
Per-example loss in batch: 0.246294  [   42/   89]
Per-example loss in batch: 0.331702  [   44/   89]
Per-example loss in batch: 0.315277  [   46/   89]
Per-example loss in batch: 0.331895  [   48/   89]
Per-example loss in batch: 0.294301  [   50/   89]
Per-example loss in batch: 0.273919  [   52/   89]
Per-example loss in batch: 0.305990  [   54/   89]
Per-example loss in batch: 0.290938  [   56/   89]
Per-example loss in batch: 0.274297  [   58/   89]
Per-example loss in batch: 0.356406  [   60/   89]
Per-example loss in batch: 0.321586  [   62/   89]
Per-example loss in batch: 0.307709  [   64/   89]
Per-example loss in batch: 0.375367  [   66/   89]
Per-example loss in batch: 0.268965  [   68/   89]
Per-example loss in batch: 0.316835  [   70/   89]
Per-example loss in batch: 0.252130  [   72/   89]
Per-example loss in batch: 0.297529  [   74/   89]
Per-example loss in batch: 0.275209  [   76/   89]
Per-example loss in batch: 0.299343  [   78/   89]
Per-example loss in batch: 0.362555  [   80/   89]
Per-example loss in batch: 0.306112  [   82/   89]
Per-example loss in batch: 0.265071  [   84/   89]
Per-example loss in batch: 0.327253  [   86/   89]
Per-example loss in batch: 0.221583  [   88/   89]
Per-example loss in batch: 0.611343  [   89/   89]
Train Error: Avg loss: 0.30108127
validation Error: 
 Avg loss: 0.51081481 
 F1: 0.411408 
 Precision: 0.497477 
 Recall: 0.350728
 IoU: 0.258976

test Error: 
 Avg loss: 0.49106918 
 F1: 0.446724 
 Precision: 0.536640 
 Recall: 0.382615
 IoU: 0.287601

We have finished training iteration 147
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_145_.pth
Per-example loss in batch: 0.264615  [    2/   89]
Per-example loss in batch: 0.236090  [    4/   89]
Per-example loss in batch: 0.360782  [    6/   89]
Per-example loss in batch: 0.290921  [    8/   89]
Per-example loss in batch: 0.269416  [   10/   89]
Per-example loss in batch: 0.307215  [   12/   89]
Per-example loss in batch: 0.346492  [   14/   89]
Per-example loss in batch: 0.284853  [   16/   89]
Per-example loss in batch: 0.299522  [   18/   89]
Per-example loss in batch: 0.382462  [   20/   89]
Per-example loss in batch: 0.275695  [   22/   89]
Per-example loss in batch: 0.245533  [   24/   89]
Per-example loss in batch: 0.256282  [   26/   89]
Per-example loss in batch: 0.297068  [   28/   89]
Per-example loss in batch: 0.257320  [   30/   89]
Per-example loss in batch: 0.238265  [   32/   89]
Per-example loss in batch: 0.349482  [   34/   89]
Per-example loss in batch: 0.302795  [   36/   89]
Per-example loss in batch: 0.281854  [   38/   89]
Per-example loss in batch: 0.340858  [   40/   89]
Per-example loss in batch: 0.301345  [   42/   89]
Per-example loss in batch: 0.277839  [   44/   89]
Per-example loss in batch: 0.286767  [   46/   89]
Per-example loss in batch: 0.297164  [   48/   89]
Per-example loss in batch: 0.317200  [   50/   89]
Per-example loss in batch: 0.254039  [   52/   89]
Per-example loss in batch: 0.312731  [   54/   89]
Per-example loss in batch: 0.299821  [   56/   89]
Per-example loss in batch: 0.314517  [   58/   89]
Per-example loss in batch: 0.277282  [   60/   89]
Per-example loss in batch: 0.296575  [   62/   89]
Per-example loss in batch: 0.232164  [   64/   89]
Per-example loss in batch: 0.360503  [   66/   89]
Per-example loss in batch: 0.305793  [   68/   89]
Per-example loss in batch: 0.418826  [   70/   89]
Per-example loss in batch: 0.340743  [   72/   89]
Per-example loss in batch: 0.345968  [   74/   89]
Per-example loss in batch: 0.281204  [   76/   89]
Per-example loss in batch: 0.271466  [   78/   89]
Per-example loss in batch: 0.368247  [   80/   89]
Per-example loss in batch: 0.306728  [   82/   89]
Per-example loss in batch: 0.251648  [   84/   89]
Per-example loss in batch: 0.321333  [   86/   89]
Per-example loss in batch: 0.282442  [   88/   89]
Per-example loss in batch: 0.514414  [   89/   89]
Train Error: Avg loss: 0.30263082
validation Error: 
 Avg loss: 0.51106881 
 F1: 0.410242 
 Precision: 0.470970 
 Recall: 0.363386
 IoU: 0.258053

test Error: 
 Avg loss: 0.49130356 
 F1: 0.447435 
 Precision: 0.512388 
 Recall: 0.397096
 IoU: 0.288191

We have finished training iteration 148
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_146_.pth
Per-example loss in batch: 0.298834  [    2/   89]
Per-example loss in batch: 0.294168  [    4/   89]
Per-example loss in batch: 0.267909  [    6/   89]
Per-example loss in batch: 0.289680  [    8/   89]
Per-example loss in batch: 0.282674  [   10/   89]
Per-example loss in batch: 0.247278  [   12/   89]
Per-example loss in batch: 0.223113  [   14/   89]
Per-example loss in batch: 0.325730  [   16/   89]
Per-example loss in batch: 0.360281  [   18/   89]
Per-example loss in batch: 0.287400  [   20/   89]
Per-example loss in batch: 0.312296  [   22/   89]
Per-example loss in batch: 0.299871  [   24/   89]
Per-example loss in batch: 0.328490  [   26/   89]
Per-example loss in batch: 0.288194  [   28/   89]
Per-example loss in batch: 0.328944  [   30/   89]
Per-example loss in batch: 0.301881  [   32/   89]
Per-example loss in batch: 0.355249  [   34/   89]
Per-example loss in batch: 0.247069  [   36/   89]
Per-example loss in batch: 0.260674  [   38/   89]
Per-example loss in batch: 0.291435  [   40/   89]
Per-example loss in batch: 0.275921  [   42/   89]
Per-example loss in batch: 0.261725  [   44/   89]
Per-example loss in batch: 0.279508  [   46/   89]
Per-example loss in batch: 0.349347  [   48/   89]
Per-example loss in batch: 0.367239  [   50/   89]
Per-example loss in batch: 0.311977  [   52/   89]
Per-example loss in batch: 0.297551  [   54/   89]
Per-example loss in batch: 0.229530  [   56/   89]
Per-example loss in batch: 0.249471  [   58/   89]
Per-example loss in batch: 0.280146  [   60/   89]
Per-example loss in batch: 0.247020  [   62/   89]
Per-example loss in batch: 0.292903  [   64/   89]
Per-example loss in batch: 0.289252  [   66/   89]
Per-example loss in batch: 0.228584  [   68/   89]
Per-example loss in batch: 0.246012  [   70/   89]
Per-example loss in batch: 0.309298  [   72/   89]
Per-example loss in batch: 0.315881  [   74/   89]
Per-example loss in batch: 0.256011  [   76/   89]
Per-example loss in batch: 0.331069  [   78/   89]
Per-example loss in batch: 0.265138  [   80/   89]
Per-example loss in batch: 0.232777  [   82/   89]
Per-example loss in batch: 0.359032  [   84/   89]
Per-example loss in batch: 0.308059  [   86/   89]
Per-example loss in batch: 0.299887  [   88/   89]
Per-example loss in batch: 0.667010  [   89/   89]
Train Error: Avg loss: 0.29456213
validation Error: 
 Avg loss: 0.51100729 
 F1: 0.412950 
 Precision: 0.460930 
 Recall: 0.374018
 IoU: 0.260200

test Error: 
 Avg loss: 0.49135778 
 F1: 0.450757 
 Precision: 0.507098 
 Recall: 0.405684
 IoU: 0.290953

We have finished training iteration 149
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_147_.pth
Per-example loss in batch: 0.272332  [    2/   89]
Per-example loss in batch: 0.363480  [    4/   89]
Per-example loss in batch: 0.269724  [    6/   89]
Per-example loss in batch: 0.313552  [    8/   89]
Per-example loss in batch: 0.322160  [   10/   89]
Per-example loss in batch: 0.270842  [   12/   89]
Per-example loss in batch: 0.293592  [   14/   89]
Per-example loss in batch: 0.232200  [   16/   89]
Per-example loss in batch: 0.280838  [   18/   89]
Per-example loss in batch: 0.379023  [   20/   89]
Per-example loss in batch: 0.248519  [   22/   89]
Per-example loss in batch: 0.295964  [   24/   89]
Per-example loss in batch: 0.240742  [   26/   89]
Per-example loss in batch: 0.290094  [   28/   89]
Per-example loss in batch: 0.343769  [   30/   89]
Per-example loss in batch: 0.231909  [   32/   89]
Per-example loss in batch: 0.302955  [   34/   89]
Per-example loss in batch: 0.375714  [   36/   89]
Per-example loss in batch: 0.287515  [   38/   89]
Per-example loss in batch: 0.336219  [   40/   89]
Per-example loss in batch: 0.353981  [   42/   89]
Per-example loss in batch: 0.322942  [   44/   89]
Per-example loss in batch: 0.219675  [   46/   89]
Per-example loss in batch: 0.278415  [   48/   89]
Per-example loss in batch: 0.348074  [   50/   89]
Per-example loss in batch: 0.273929  [   52/   89]
Per-example loss in batch: 0.249476  [   54/   89]
Per-example loss in batch: 0.254732  [   56/   89]
Per-example loss in batch: 0.244781  [   58/   89]
Per-example loss in batch: 0.238908  [   60/   89]
Per-example loss in batch: 0.281138  [   62/   89]
Per-example loss in batch: 0.334151  [   64/   89]
Per-example loss in batch: 0.268203  [   66/   89]
Per-example loss in batch: 0.294934  [   68/   89]
Per-example loss in batch: 0.268834  [   70/   89]
Per-example loss in batch: 0.286572  [   72/   89]
Per-example loss in batch: 0.346018  [   74/   89]
Per-example loss in batch: 0.309009  [   76/   89]
Per-example loss in batch: 0.338841  [   78/   89]
Per-example loss in batch: 0.342175  [   80/   89]
Per-example loss in batch: 0.308396  [   82/   89]
Per-example loss in batch: 0.257619  [   84/   89]
Per-example loss in batch: 0.237002  [   86/   89]
Per-example loss in batch: 0.339693  [   88/   89]
Per-example loss in batch: 0.739593  [   89/   89]
Train Error: Avg loss: 0.29929071
validation Error: 
 Avg loss: 0.51164117 
 F1: 0.409362 
 Precision: 0.434146 
 Recall: 0.387254
 IoU: 0.257357

test Error: 
 Avg loss: 0.49189795 
 F1: 0.447588 
 Precision: 0.475361 
 Recall: 0.422881
 IoU: 0.288318

We have finished training iteration 150
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_148_.pth
Per-example loss in batch: 0.278780  [    2/   89]
Per-example loss in batch: 0.232007  [    4/   89]
Per-example loss in batch: 0.343252  [    6/   89]
Per-example loss in batch: 0.275231  [    8/   89]
Per-example loss in batch: 0.262688  [   10/   89]
Per-example loss in batch: 0.322081  [   12/   89]
Per-example loss in batch: 0.252811  [   14/   89]
Per-example loss in batch: 0.256025  [   16/   89]
Per-example loss in batch: 0.321315  [   18/   89]
Per-example loss in batch: 0.284936  [   20/   89]
Per-example loss in batch: 0.244854  [   22/   89]
Per-example loss in batch: 0.345701  [   24/   89]
Per-example loss in batch: 0.369875  [   26/   89]
Per-example loss in batch: 0.328098  [   28/   89]
Per-example loss in batch: 0.301755  [   30/   89]
Per-example loss in batch: 0.287884  [   32/   89]
Per-example loss in batch: 0.276444  [   34/   89]
Per-example loss in batch: 0.300873  [   36/   89]
Per-example loss in batch: 0.331015  [   38/   89]
Per-example loss in batch: 0.273450  [   40/   89]
Per-example loss in batch: 0.307005  [   42/   89]
Per-example loss in batch: 0.306925  [   44/   89]
Per-example loss in batch: 0.291987  [   46/   89]
Per-example loss in batch: 0.299545  [   48/   89]
Per-example loss in batch: 0.228099  [   50/   89]
Per-example loss in batch: 0.301296  [   52/   89]
Per-example loss in batch: 0.220704  [   54/   89]
Per-example loss in batch: 0.236591  [   56/   89]
Per-example loss in batch: 0.325444  [   58/   89]
Per-example loss in batch: 0.359647  [   60/   89]
Per-example loss in batch: 0.312324  [   62/   89]
Per-example loss in batch: 0.365143  [   64/   89]
Per-example loss in batch: 0.334782  [   66/   89]
Per-example loss in batch: 0.293136  [   68/   89]
Per-example loss in batch: 0.220559  [   70/   89]
Per-example loss in batch: 0.364166  [   72/   89]
Per-example loss in batch: 0.288943  [   74/   89]
Per-example loss in batch: 0.297307  [   76/   89]
Per-example loss in batch: 0.277993  [   78/   89]
Per-example loss in batch: 0.286639  [   80/   89]
Per-example loss in batch: 0.337350  [   82/   89]
Per-example loss in batch: 0.337668  [   84/   89]
Per-example loss in batch: 0.280061  [   86/   89]
Per-example loss in batch: 0.276282  [   88/   89]
Per-example loss in batch: 0.534053  [   89/   89]
Train Error: Avg loss: 0.29900449
validation Error: 
 Avg loss: 0.51048131 
 F1: 0.413332 
 Precision: 0.511737 
 Recall: 0.346669
 IoU: 0.260503

test Error: 
 Avg loss: 0.49050330 
 F1: 0.448389 
 Precision: 0.556770 
 Recall: 0.375328
 IoU: 0.288983

We have finished training iteration 151
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_149_.pth
Per-example loss in batch: 0.294521  [    2/   89]
Per-example loss in batch: 0.336894  [    4/   89]
Per-example loss in batch: 0.241977  [    6/   89]
Per-example loss in batch: 0.289870  [    8/   89]
Per-example loss in batch: 0.319168  [   10/   89]
Per-example loss in batch: 0.255656  [   12/   89]
Per-example loss in batch: 0.305007  [   14/   89]
Per-example loss in batch: 0.319175  [   16/   89]
Per-example loss in batch: 0.270473  [   18/   89]
Per-example loss in batch: 0.247140  [   20/   89]
Per-example loss in batch: 0.259343  [   22/   89]
Per-example loss in batch: 0.277426  [   24/   89]
Per-example loss in batch: 0.287263  [   26/   89]
Per-example loss in batch: 0.281356  [   28/   89]
Per-example loss in batch: 0.264880  [   30/   89]
Per-example loss in batch: 0.316093  [   32/   89]
Per-example loss in batch: 0.368842  [   34/   89]
Per-example loss in batch: 0.360427  [   36/   89]
Per-example loss in batch: 0.282763  [   38/   89]
Per-example loss in batch: 0.252565  [   40/   89]
Per-example loss in batch: 0.339456  [   42/   89]
Per-example loss in batch: 0.356963  [   44/   89]
Per-example loss in batch: 0.327766  [   46/   89]
Per-example loss in batch: 0.243950  [   48/   89]
Per-example loss in batch: 0.230922  [   50/   89]
Per-example loss in batch: 0.347980  [   52/   89]
Per-example loss in batch: 0.325730  [   54/   89]
Per-example loss in batch: 0.307750  [   56/   89]
Per-example loss in batch: 0.296996  [   58/   89]
Per-example loss in batch: 0.355033  [   60/   89]
Per-example loss in batch: 0.352407  [   62/   89]
Per-example loss in batch: 0.318609  [   64/   89]
Per-example loss in batch: 0.264971  [   66/   89]
Per-example loss in batch: 0.249980  [   68/   89]
Per-example loss in batch: 0.245630  [   70/   89]
Per-example loss in batch: 0.297730  [   72/   89]
Per-example loss in batch: 0.238366  [   74/   89]
Per-example loss in batch: 0.321324  [   76/   89]
Per-example loss in batch: 0.313521  [   78/   89]
Per-example loss in batch: 0.240895  [   80/   89]
Per-example loss in batch: 0.324107  [   82/   89]
Per-example loss in batch: 0.262679  [   84/   89]
Per-example loss in batch: 0.261106  [   86/   89]
Per-example loss in batch: 0.275618  [   88/   89]
Per-example loss in batch: 0.474504  [   89/   89]
Train Error: Avg loss: 0.29590069
validation Error: 
 Avg loss: 0.51048428 
 F1: 0.415359 
 Precision: 0.493626 
 Recall: 0.358514
 IoU: 0.262115

test Error: 
 Avg loss: 0.49079725 
 F1: 0.449862 
 Precision: 0.539552 
 Recall: 0.385740
 IoU: 0.290208

We have finished training iteration 152
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_150_.pth
Per-example loss in batch: 0.339614  [    2/   89]
Per-example loss in batch: 0.287353  [    4/   89]
Per-example loss in batch: 0.327226  [    6/   89]
Per-example loss in batch: 0.246551  [    8/   89]
Per-example loss in batch: 0.339993  [   10/   89]
Per-example loss in batch: 0.265299  [   12/   89]
Per-example loss in batch: 0.293940  [   14/   89]
Per-example loss in batch: 0.257381  [   16/   89]
Per-example loss in batch: 0.276745  [   18/   89]
Per-example loss in batch: 0.341618  [   20/   89]
Per-example loss in batch: 0.232860  [   22/   89]
Per-example loss in batch: 0.343128  [   24/   89]
Per-example loss in batch: 0.324608  [   26/   89]
Per-example loss in batch: 0.251680  [   28/   89]
Per-example loss in batch: 0.256810  [   30/   89]
Per-example loss in batch: 0.313398  [   32/   89]
Per-example loss in batch: 0.234771  [   34/   89]
Per-example loss in batch: 0.326414  [   36/   89]
Per-example loss in batch: 0.287573  [   38/   89]
Per-example loss in batch: 0.365564  [   40/   89]
Per-example loss in batch: 0.286214  [   42/   89]
Per-example loss in batch: 0.375262  [   44/   89]
Per-example loss in batch: 0.262506  [   46/   89]
Per-example loss in batch: 0.225505  [   48/   89]
Per-example loss in batch: 0.265863  [   50/   89]
Per-example loss in batch: 0.222045  [   52/   89]
Per-example loss in batch: 0.304537  [   54/   89]
Per-example loss in batch: 0.262773  [   56/   89]
Per-example loss in batch: 0.279184  [   58/   89]
Per-example loss in batch: 0.310295  [   60/   89]
Per-example loss in batch: 0.327788  [   62/   89]
Per-example loss in batch: 0.285829  [   64/   89]
Per-example loss in batch: 0.338844  [   66/   89]
Per-example loss in batch: 0.225280  [   68/   89]
Per-example loss in batch: 0.263915  [   70/   89]
Per-example loss in batch: 0.284039  [   72/   89]
Per-example loss in batch: 0.339196  [   74/   89]
Per-example loss in batch: 0.363334  [   76/   89]
Per-example loss in batch: 0.254711  [   78/   89]
Per-example loss in batch: 0.335137  [   80/   89]
Per-example loss in batch: 0.284650  [   82/   89]
Per-example loss in batch: 0.336138  [   84/   89]
Per-example loss in batch: 0.293081  [   86/   89]
Per-example loss in batch: 0.251044  [   88/   89]
Per-example loss in batch: 0.462676  [   89/   89]
Train Error: Avg loss: 0.29485467
validation Error: 
 Avg loss: 0.51129578 
 F1: 0.413188 
 Precision: 0.450301 
 Recall: 0.381726
 IoU: 0.260388

test Error: 
 Avg loss: 0.49166722 
 F1: 0.451926 
 Precision: 0.495787 
 Recall: 0.415195
 IoU: 0.291928

We have finished training iteration 153
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_151_.pth
Per-example loss in batch: 0.272261  [    2/   89]
Per-example loss in batch: 0.289168  [    4/   89]
Per-example loss in batch: 0.248130  [    6/   89]
Per-example loss in batch: 0.247810  [    8/   89]
Per-example loss in batch: 0.235442  [   10/   89]
Per-example loss in batch: 0.348760  [   12/   89]
Per-example loss in batch: 0.299154  [   14/   89]
Per-example loss in batch: 0.352271  [   16/   89]
Per-example loss in batch: 0.358546  [   18/   89]
Per-example loss in batch: 0.312407  [   20/   89]
Per-example loss in batch: 0.397343  [   22/   89]
Per-example loss in batch: 0.231000  [   24/   89]
Per-example loss in batch: 0.304528  [   26/   89]
Per-example loss in batch: 0.293176  [   28/   89]
Per-example loss in batch: 0.355835  [   30/   89]
Per-example loss in batch: 0.302677  [   32/   89]
Per-example loss in batch: 0.368559  [   34/   89]
Per-example loss in batch: 0.263172  [   36/   89]
Per-example loss in batch: 0.263214  [   38/   89]
Per-example loss in batch: 0.276727  [   40/   89]
Per-example loss in batch: 0.241384  [   42/   89]
Per-example loss in batch: 0.287703  [   44/   89]
Per-example loss in batch: 0.231090  [   46/   89]
Per-example loss in batch: 0.357778  [   48/   89]
Per-example loss in batch: 0.356479  [   50/   89]
Per-example loss in batch: 0.283940  [   52/   89]
Per-example loss in batch: 0.277320  [   54/   89]
Per-example loss in batch: 0.262346  [   56/   89]
Per-example loss in batch: 0.223001  [   58/   89]
Per-example loss in batch: 0.221022  [   60/   89]
Per-example loss in batch: 0.260516  [   62/   89]
Per-example loss in batch: 0.283223  [   64/   89]
Per-example loss in batch: 0.323195  [   66/   89]
Per-example loss in batch: 0.295039  [   68/   89]
Per-example loss in batch: 0.271869  [   70/   89]
Per-example loss in batch: 0.313647  [   72/   89]
Per-example loss in batch: 0.318044  [   74/   89]
Per-example loss in batch: 0.316904  [   76/   89]
Per-example loss in batch: 0.291815  [   78/   89]
Per-example loss in batch: 0.272823  [   80/   89]
Per-example loss in batch: 0.214988  [   82/   89]
Per-example loss in batch: 0.311011  [   84/   89]
Per-example loss in batch: 0.366099  [   86/   89]
Per-example loss in batch: 0.257914  [   88/   89]
Per-example loss in batch: 0.638192  [   89/   89]
Train Error: Avg loss: 0.29614444
validation Error: 
 Avg loss: 0.51057547 
 F1: 0.414834 
 Precision: 0.483907 
 Recall: 0.363017
 IoU: 0.261697

test Error: 
 Avg loss: 0.49086889 
 F1: 0.452068 
 Precision: 0.530707 
 Recall: 0.393726
 IoU: 0.292046

We have finished training iteration 154
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_152_.pth
Per-example loss in batch: 0.259961  [    2/   89]
Per-example loss in batch: 0.336719  [    4/   89]
Per-example loss in batch: 0.342000  [    6/   89]
Per-example loss in batch: 0.232255  [    8/   89]
Per-example loss in batch: 0.235525  [   10/   89]
Per-example loss in batch: 0.366011  [   12/   89]
Per-example loss in batch: 0.321924  [   14/   89]
Per-example loss in batch: 0.291616  [   16/   89]
Per-example loss in batch: 0.299215  [   18/   89]
Per-example loss in batch: 0.267807  [   20/   89]
Per-example loss in batch: 0.296808  [   22/   89]
Per-example loss in batch: 0.269530  [   24/   89]
Per-example loss in batch: 0.350756  [   26/   89]
Per-example loss in batch: 0.262758  [   28/   89]
Per-example loss in batch: 0.374720  [   30/   89]
Per-example loss in batch: 0.243450  [   32/   89]
Per-example loss in batch: 0.305626  [   34/   89]
Per-example loss in batch: 0.279116  [   36/   89]
Per-example loss in batch: 0.280142  [   38/   89]
Per-example loss in batch: 0.273586  [   40/   89]
Per-example loss in batch: 0.277383  [   42/   89]
Per-example loss in batch: 0.372571  [   44/   89]
Per-example loss in batch: 0.371159  [   46/   89]
Per-example loss in batch: 0.339532  [   48/   89]
Per-example loss in batch: 0.309143  [   50/   89]
Per-example loss in batch: 0.250878  [   52/   89]
Per-example loss in batch: 0.217367  [   54/   89]
Per-example loss in batch: 0.304769  [   56/   89]
Per-example loss in batch: 0.336390  [   58/   89]
Per-example loss in batch: 0.339934  [   60/   89]
Per-example loss in batch: 0.263411  [   62/   89]
Per-example loss in batch: 0.303694  [   64/   89]
Per-example loss in batch: 0.288340  [   66/   89]
Per-example loss in batch: 0.292471  [   68/   89]
Per-example loss in batch: 0.364784  [   70/   89]
Per-example loss in batch: 0.252196  [   72/   89]
Per-example loss in batch: 0.281007  [   74/   89]
Per-example loss in batch: 0.330580  [   76/   89]
Per-example loss in batch: 0.301856  [   78/   89]
Per-example loss in batch: 0.329461  [   80/   89]
Per-example loss in batch: 0.263765  [   82/   89]
Per-example loss in batch: 0.283296  [   84/   89]
Per-example loss in batch: 0.240484  [   86/   89]
Per-example loss in batch: 0.273303  [   88/   89]
Per-example loss in batch: 0.761066  [   89/   89]
Train Error: Avg loss: 0.30242319
validation Error: 
 Avg loss: 0.51027944 
 F1: 0.413864 
 Precision: 0.486086 
 Recall: 0.360328
 IoU: 0.260926

test Error: 
 Avg loss: 0.49121326 
 F1: 0.448818 
 Precision: 0.524967 
 Recall: 0.391962
 IoU: 0.289339

We have finished training iteration 155
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_153_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.289771  [    2/   89]
Per-example loss in batch: 0.224890  [    4/   89]
Per-example loss in batch: 0.328481  [    6/   89]
Per-example loss in batch: 0.275772  [    8/   89]
Per-example loss in batch: 0.277190  [   10/   89]
Per-example loss in batch: 0.239081  [   12/   89]
Per-example loss in batch: 0.358493  [   14/   89]
Per-example loss in batch: 0.315911  [   16/   89]
Per-example loss in batch: 0.369274  [   18/   89]
Per-example loss in batch: 0.259722  [   20/   89]
Per-example loss in batch: 0.230294  [   22/   89]
Per-example loss in batch: 0.304493  [   24/   89]
Per-example loss in batch: 0.256726  [   26/   89]
Per-example loss in batch: 0.373563  [   28/   89]
Per-example loss in batch: 0.255343  [   30/   89]
Per-example loss in batch: 0.266750  [   32/   89]
Per-example loss in batch: 0.240997  [   34/   89]
Per-example loss in batch: 0.318761  [   36/   89]
Per-example loss in batch: 0.329563  [   38/   89]
Per-example loss in batch: 0.268760  [   40/   89]
Per-example loss in batch: 0.281331  [   42/   89]
Per-example loss in batch: 0.246198  [   44/   89]
Per-example loss in batch: 0.307906  [   46/   89]
Per-example loss in batch: 0.368853  [   48/   89]
Per-example loss in batch: 0.341686  [   50/   89]
Per-example loss in batch: 0.291103  [   52/   89]
Per-example loss in batch: 0.320052  [   54/   89]
Per-example loss in batch: 0.332550  [   56/   89]
Per-example loss in batch: 0.269701  [   58/   89]
Per-example loss in batch: 0.290656  [   60/   89]
Per-example loss in batch: 0.254979  [   62/   89]
Per-example loss in batch: 0.256451  [   64/   89]
Per-example loss in batch: 0.246657  [   66/   89]
Per-example loss in batch: 0.290599  [   68/   89]
Per-example loss in batch: 0.286169  [   70/   89]
Per-example loss in batch: 0.241775  [   72/   89]
Per-example loss in batch: 0.283089  [   74/   89]
Per-example loss in batch: 0.246731  [   76/   89]
Per-example loss in batch: 0.361112  [   78/   89]
Per-example loss in batch: 0.248767  [   80/   89]
Per-example loss in batch: 0.248547  [   82/   89]
Per-example loss in batch: 0.289612  [   84/   89]
Per-example loss in batch: 0.276251  [   86/   89]
Per-example loss in batch: 0.309033  [   88/   89]
Per-example loss in batch: 0.756925  [   89/   89]
Train Error: Avg loss: 0.29330577
validation Error: 
 Avg loss: 0.51093424 
 F1: 0.414852 
 Precision: 0.467217 
 Recall: 0.373042
 IoU: 0.261712

test Error: 
 Avg loss: 0.49124832 
 F1: 0.450578 
 Precision: 0.509468 
 Recall: 0.403892
 IoU: 0.290804

We have finished training iteration 156
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_154_.pth
Per-example loss in batch: 0.308496  [    2/   89]
Per-example loss in batch: 0.310158  [    4/   89]
Per-example loss in batch: 0.276846  [    6/   89]
Per-example loss in batch: 0.279179  [    8/   89]
Per-example loss in batch: 0.259735  [   10/   89]
Per-example loss in batch: 0.226697  [   12/   89]
Per-example loss in batch: 0.319976  [   14/   89]
Per-example loss in batch: 0.384670  [   16/   89]
Per-example loss in batch: 0.346576  [   18/   89]
Per-example loss in batch: 0.240229  [   20/   89]
Per-example loss in batch: 0.250764  [   22/   89]
Per-example loss in batch: 0.307352  [   24/   89]
Per-example loss in batch: 0.298594  [   26/   89]
Per-example loss in batch: 0.349618  [   28/   89]
Per-example loss in batch: 0.310014  [   30/   89]
Per-example loss in batch: 0.334971  [   32/   89]
Per-example loss in batch: 0.337283  [   34/   89]
Per-example loss in batch: 0.296385  [   36/   89]
Per-example loss in batch: 0.250177  [   38/   89]
Per-example loss in batch: 0.306256  [   40/   89]
Per-example loss in batch: 0.261585  [   42/   89]
Per-example loss in batch: 0.278117  [   44/   89]
Per-example loss in batch: 0.266692  [   46/   89]
Per-example loss in batch: 0.321622  [   48/   89]
Per-example loss in batch: 0.255080  [   50/   89]
Per-example loss in batch: 0.286984  [   52/   89]
Per-example loss in batch: 0.234911  [   54/   89]
Per-example loss in batch: 0.236912  [   56/   89]
Per-example loss in batch: 0.234276  [   58/   89]
Per-example loss in batch: 0.321209  [   60/   89]
Per-example loss in batch: 0.246579  [   62/   89]
Per-example loss in batch: 0.309240  [   64/   89]
Per-example loss in batch: 0.264829  [   66/   89]
Per-example loss in batch: 0.347383  [   68/   89]
Per-example loss in batch: 0.273355  [   70/   89]
Per-example loss in batch: 0.294110  [   72/   89]
Per-example loss in batch: 0.249505  [   74/   89]
Per-example loss in batch: 0.280209  [   76/   89]
Per-example loss in batch: 0.302568  [   78/   89]
Per-example loss in batch: 0.258666  [   80/   89]
Per-example loss in batch: 0.288181  [   82/   89]
Per-example loss in batch: 0.283567  [   84/   89]
Per-example loss in batch: 0.362252  [   86/   89]
Per-example loss in batch: 0.331318  [   88/   89]
Per-example loss in batch: 0.482973  [   89/   89]
Train Error: Avg loss: 0.29268800
validation Error: 
 Avg loss: 0.51068355 
 F1: 0.415595 
 Precision: 0.474841 
 Recall: 0.369494
 IoU: 0.262304

test Error: 
 Avg loss: 0.49098314 
 F1: 0.453004 
 Precision: 0.518942 
 Recall: 0.401933
 IoU: 0.292828

We have finished training iteration 157
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_155_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.232958  [    2/   89]
Per-example loss in batch: 0.351655  [    4/   89]
Per-example loss in batch: 0.318535  [    6/   89]
Per-example loss in batch: 0.368746  [    8/   89]
Per-example loss in batch: 0.323932  [   10/   89]
Per-example loss in batch: 0.277182  [   12/   89]
Per-example loss in batch: 0.333577  [   14/   89]
Per-example loss in batch: 0.243721  [   16/   89]
Per-example loss in batch: 0.266889  [   18/   89]
Per-example loss in batch: 0.253238  [   20/   89]
Per-example loss in batch: 0.350896  [   22/   89]
Per-example loss in batch: 0.362889  [   24/   89]
Per-example loss in batch: 0.311462  [   26/   89]
Per-example loss in batch: 0.340959  [   28/   89]
Per-example loss in batch: 0.253475  [   30/   89]
Per-example loss in batch: 0.222056  [   32/   89]
Per-example loss in batch: 0.301702  [   34/   89]
Per-example loss in batch: 0.274128  [   36/   89]
Per-example loss in batch: 0.267422  [   38/   89]
Per-example loss in batch: 0.266088  [   40/   89]
Per-example loss in batch: 0.311297  [   42/   89]
Per-example loss in batch: 0.247165  [   44/   89]
Per-example loss in batch: 0.291249  [   46/   89]
Per-example loss in batch: 0.292863  [   48/   89]
Per-example loss in batch: 0.293652  [   50/   89]
Per-example loss in batch: 0.253252  [   52/   89]
Per-example loss in batch: 0.322128  [   54/   89]
Per-example loss in batch: 0.225485  [   56/   89]
Per-example loss in batch: 0.310562  [   58/   89]
Per-example loss in batch: 0.299742  [   60/   89]
Per-example loss in batch: 0.288529  [   62/   89]
Per-example loss in batch: 0.313229  [   64/   89]
Per-example loss in batch: 0.370746  [   66/   89]
Per-example loss in batch: 0.291668  [   68/   89]
Per-example loss in batch: 0.261862  [   70/   89]
Per-example loss in batch: 0.264827  [   72/   89]
Per-example loss in batch: 0.324367  [   74/   89]
Per-example loss in batch: 0.328230  [   76/   89]
Per-example loss in batch: 0.316462  [   78/   89]
Per-example loss in batch: 0.284846  [   80/   89]
Per-example loss in batch: 0.235647  [   82/   89]
Per-example loss in batch: 0.226638  [   84/   89]
Per-example loss in batch: 0.256680  [   86/   89]
Per-example loss in batch: 0.293671  [   88/   89]
Per-example loss in batch: 0.509425  [   89/   89]
Train Error: Avg loss: 0.29395559
validation Error: 
 Avg loss: 0.51019395 
 F1: 0.416585 
 Precision: 0.481474 
 Recall: 0.367110
 IoU: 0.263093

test Error: 
 Avg loss: 0.49104062 
 F1: 0.453002 
 Precision: 0.525784 
 Recall: 0.397920
 IoU: 0.292827

We have finished training iteration 158
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_156_.pth
Per-example loss in batch: 0.252392  [    2/   89]
Per-example loss in batch: 0.289387  [    4/   89]
Per-example loss in batch: 0.302944  [    6/   89]
Per-example loss in batch: 0.242765  [    8/   89]
Per-example loss in batch: 0.313604  [   10/   89]
Per-example loss in batch: 0.305706  [   12/   89]
Per-example loss in batch: 0.358015  [   14/   89]
Per-example loss in batch: 0.255784  [   16/   89]
Per-example loss in batch: 0.292314  [   18/   89]
Per-example loss in batch: 0.323191  [   20/   89]
Per-example loss in batch: 0.231617  [   22/   89]
Per-example loss in batch: 0.271843  [   24/   89]
Per-example loss in batch: 0.308018  [   26/   89]
Per-example loss in batch: 0.279842  [   28/   89]
Per-example loss in batch: 0.266602  [   30/   89]
Per-example loss in batch: 0.297677  [   32/   89]
Per-example loss in batch: 0.287709  [   34/   89]
Per-example loss in batch: 0.294545  [   36/   89]
Per-example loss in batch: 0.299270  [   38/   89]
Per-example loss in batch: 0.232083  [   40/   89]
Per-example loss in batch: 0.237004  [   42/   89]
Per-example loss in batch: 0.371587  [   44/   89]
Per-example loss in batch: 0.323875  [   46/   89]
Per-example loss in batch: 0.353109  [   48/   89]
Per-example loss in batch: 0.309790  [   50/   89]
Per-example loss in batch: 0.314423  [   52/   89]
Per-example loss in batch: 0.262934  [   54/   89]
Per-example loss in batch: 0.263453  [   56/   89]
Per-example loss in batch: 0.383111  [   58/   89]
Per-example loss in batch: 0.269522  [   60/   89]
Per-example loss in batch: 0.344983  [   62/   89]
Per-example loss in batch: 0.297978  [   64/   89]
Per-example loss in batch: 0.330865  [   66/   89]
Per-example loss in batch: 0.332557  [   68/   89]
Per-example loss in batch: 0.285792  [   70/   89]
Per-example loss in batch: 0.283890  [   72/   89]
Per-example loss in batch: 0.235354  [   74/   89]
Per-example loss in batch: 0.317245  [   76/   89]
Per-example loss in batch: 0.276632  [   78/   89]
Per-example loss in batch: 0.258883  [   80/   89]
Per-example loss in batch: 0.248067  [   82/   89]
Per-example loss in batch: 0.261279  [   84/   89]
Per-example loss in batch: 0.364125  [   86/   89]
Per-example loss in batch: 0.261823  [   88/   89]
Per-example loss in batch: 0.494367  [   89/   89]
Train Error: Avg loss: 0.29529823
validation Error: 
 Avg loss: 0.51133156 
 F1: 0.413323 
 Precision: 0.445235 
 Recall: 0.385680
 IoU: 0.260496

test Error: 
 Avg loss: 0.49149597 
 F1: 0.451738 
 Precision: 0.489081 
 Recall: 0.419693
 IoU: 0.291771

We have finished training iteration 159
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_157_.pth
Per-example loss in batch: 0.336715  [    2/   89]
Per-example loss in batch: 0.308987  [    4/   89]
Per-example loss in batch: 0.294172  [    6/   89]
Per-example loss in batch: 0.266554  [    8/   89]
Per-example loss in batch: 0.293052  [   10/   89]
Per-example loss in batch: 0.231229  [   12/   89]
Per-example loss in batch: 0.294827  [   14/   89]
Per-example loss in batch: 0.281336  [   16/   89]
Per-example loss in batch: 0.348663  [   18/   89]
Per-example loss in batch: 0.270936  [   20/   89]
Per-example loss in batch: 0.268683  [   22/   89]
Per-example loss in batch: 0.222058  [   24/   89]
Per-example loss in batch: 0.277299  [   26/   89]
Per-example loss in batch: 0.294985  [   28/   89]
Per-example loss in batch: 0.355794  [   30/   89]
Per-example loss in batch: 0.320650  [   32/   89]
Per-example loss in batch: 0.249072  [   34/   89]
Per-example loss in batch: 0.373845  [   36/   89]
Per-example loss in batch: 0.256347  [   38/   89]
Per-example loss in batch: 0.260595  [   40/   89]
Per-example loss in batch: 0.292994  [   42/   89]
Per-example loss in batch: 0.260158  [   44/   89]
Per-example loss in batch: 0.238662  [   46/   89]
Per-example loss in batch: 0.288049  [   48/   89]
Per-example loss in batch: 0.267992  [   50/   89]
Per-example loss in batch: 0.262710  [   52/   89]
Per-example loss in batch: 0.253672  [   54/   89]
Per-example loss in batch: 0.328672  [   56/   89]
Per-example loss in batch: 0.361634  [   58/   89]
Per-example loss in batch: 0.260663  [   60/   89]
Per-example loss in batch: 0.266193  [   62/   89]
Per-example loss in batch: 0.352853  [   64/   89]
Per-example loss in batch: 0.292581  [   66/   89]
Per-example loss in batch: 0.351812  [   68/   89]
Per-example loss in batch: 0.321974  [   70/   89]
Per-example loss in batch: 0.304352  [   72/   89]
Per-example loss in batch: 0.297455  [   74/   89]
Per-example loss in batch: 0.318964  [   76/   89]
Per-example loss in batch: 0.315836  [   78/   89]
Per-example loss in batch: 0.350367  [   80/   89]
Per-example loss in batch: 0.242752  [   82/   89]
Per-example loss in batch: 0.287077  [   84/   89]
Per-example loss in batch: 0.317523  [   86/   89]
Per-example loss in batch: 0.261497  [   88/   89]
Per-example loss in batch: 0.628344  [   89/   89]
Train Error: Avg loss: 0.29699808
validation Error: 
 Avg loss: 0.50924838 
 F1: 0.413808 
 Precision: 0.547708 
 Recall: 0.332517
 IoU: 0.260882

test Error: 
 Avg loss: 0.49009635 
 F1: 0.442984 
 Precision: 0.585127 
 Recall: 0.356403
 IoU: 0.284508

We have finished training iteration 160
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_158_.pth
Per-example loss in batch: 0.263666  [    2/   89]
Per-example loss in batch: 0.367523  [    4/   89]
Per-example loss in batch: 0.335325  [    6/   89]
Per-example loss in batch: 0.332617  [    8/   89]
Per-example loss in batch: 0.368146  [   10/   89]
Per-example loss in batch: 0.276461  [   12/   89]
Per-example loss in batch: 0.279032  [   14/   89]
Per-example loss in batch: 0.335578  [   16/   89]
Per-example loss in batch: 0.328923  [   18/   89]
Per-example loss in batch: 0.276177  [   20/   89]
Per-example loss in batch: 0.246081  [   22/   89]
Per-example loss in batch: 0.253863  [   24/   89]
Per-example loss in batch: 0.225704  [   26/   89]
Per-example loss in batch: 0.235560  [   28/   89]
Per-example loss in batch: 0.242170  [   30/   89]
Per-example loss in batch: 0.293309  [   32/   89]
Per-example loss in batch: 0.261087  [   34/   89]
Per-example loss in batch: 0.370180  [   36/   89]
Per-example loss in batch: 0.267574  [   38/   89]
Per-example loss in batch: 0.246317  [   40/   89]
Per-example loss in batch: 0.245971  [   42/   89]
Per-example loss in batch: 0.265013  [   44/   89]
Per-example loss in batch: 0.289528  [   46/   89]
Per-example loss in batch: 0.266292  [   48/   89]
Per-example loss in batch: 0.287644  [   50/   89]
Per-example loss in batch: 0.307673  [   52/   89]
Per-example loss in batch: 0.270230  [   54/   89]
Per-example loss in batch: 0.332859  [   56/   89]
Per-example loss in batch: 0.317478  [   58/   89]
Per-example loss in batch: 0.361648  [   60/   89]
Per-example loss in batch: 0.334864  [   62/   89]
Per-example loss in batch: 0.333333  [   64/   89]
Per-example loss in batch: 0.305107  [   66/   89]
Per-example loss in batch: 0.326196  [   68/   89]
Per-example loss in batch: 0.381577  [   70/   89]
Per-example loss in batch: 0.363827  [   72/   89]
Per-example loss in batch: 0.286363  [   74/   89]
Per-example loss in batch: 0.349029  [   76/   89]
Per-example loss in batch: 0.227334  [   78/   89]
Per-example loss in batch: 0.344936  [   80/   89]
Per-example loss in batch: 0.313413  [   82/   89]
Per-example loss in batch: 0.271304  [   84/   89]
Per-example loss in batch: 0.234367  [   86/   89]
Per-example loss in batch: 0.376381  [   88/   89]
Per-example loss in batch: 0.615578  [   89/   89]
Train Error: Avg loss: 0.30349333
validation Error: 
 Avg loss: 0.51039111 
 F1: 0.415069 
 Precision: 0.473168 
 Recall: 0.369677
 IoU: 0.261884

test Error: 
 Avg loss: 0.49127237 
 F1: 0.451020 
 Precision: 0.513805 
 Recall: 0.401909
 IoU: 0.291173

We have finished training iteration 161
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_159_.pth
Per-example loss in batch: 0.259133  [    2/   89]
Per-example loss in batch: 0.317379  [    4/   89]
Per-example loss in batch: 0.229250  [    6/   89]
Per-example loss in batch: 0.269649  [    8/   89]
Per-example loss in batch: 0.232241  [   10/   89]
Per-example loss in batch: 0.326736  [   12/   89]
Per-example loss in batch: 0.289905  [   14/   89]
Per-example loss in batch: 0.297759  [   16/   89]
Per-example loss in batch: 0.250448  [   18/   89]
Per-example loss in batch: 0.351630  [   20/   89]
Per-example loss in batch: 0.271752  [   22/   89]
Per-example loss in batch: 0.281719  [   24/   89]
Per-example loss in batch: 0.366681  [   26/   89]
Per-example loss in batch: 0.319282  [   28/   89]
Per-example loss in batch: 0.310113  [   30/   89]
Per-example loss in batch: 0.350253  [   32/   89]
Per-example loss in batch: 0.318758  [   34/   89]
Per-example loss in batch: 0.259315  [   36/   89]
Per-example loss in batch: 0.285748  [   38/   89]
Per-example loss in batch: 0.272850  [   40/   89]
Per-example loss in batch: 0.246337  [   42/   89]
Per-example loss in batch: 0.318646  [   44/   89]
Per-example loss in batch: 0.280414  [   46/   89]
Per-example loss in batch: 0.273810  [   48/   89]
Per-example loss in batch: 0.243678  [   50/   89]
Per-example loss in batch: 0.338240  [   52/   89]
Per-example loss in batch: 0.244011  [   54/   89]
Per-example loss in batch: 0.292849  [   56/   89]
Per-example loss in batch: 0.269892  [   58/   89]
Per-example loss in batch: 0.247476  [   60/   89]
Per-example loss in batch: 0.368604  [   62/   89]
Per-example loss in batch: 0.240868  [   64/   89]
Per-example loss in batch: 0.258474  [   66/   89]
Per-example loss in batch: 0.337595  [   68/   89]
Per-example loss in batch: 0.273276  [   70/   89]
Per-example loss in batch: 0.357107  [   72/   89]
Per-example loss in batch: 0.300889  [   74/   89]
Per-example loss in batch: 0.299687  [   76/   89]
Per-example loss in batch: 0.306366  [   78/   89]
Per-example loss in batch: 0.370739  [   80/   89]
Per-example loss in batch: 0.271872  [   82/   89]
Per-example loss in batch: 0.233527  [   84/   89]
Per-example loss in batch: 0.305233  [   86/   89]
Per-example loss in batch: 0.260816  [   88/   89]
Per-example loss in batch: 0.484030  [   89/   89]
Train Error: Avg loss: 0.29310162
validation Error: 
 Avg loss: 0.51134318 
 F1: 0.414090 
 Precision: 0.448706 
 Recall: 0.384433
 IoU: 0.261106

test Error: 
 Avg loss: 0.49158929 
 F1: 0.452521 
 Precision: 0.493505 
 Recall: 0.417822
 IoU: 0.292425

We have finished training iteration 162
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_160_.pth
Per-example loss in batch: 0.262771  [    2/   89]
Per-example loss in batch: 0.287458  [    4/   89]
Per-example loss in batch: 0.256688  [    6/   89]
Per-example loss in batch: 0.296014  [    8/   89]
Per-example loss in batch: 0.282022  [   10/   89]
Per-example loss in batch: 0.328221  [   12/   89]
Per-example loss in batch: 0.269896  [   14/   89]
Per-example loss in batch: 0.233205  [   16/   89]
Per-example loss in batch: 0.321881  [   18/   89]
Per-example loss in batch: 0.316795  [   20/   89]
Per-example loss in batch: 0.287488  [   22/   89]
Per-example loss in batch: 0.317986  [   24/   89]
Per-example loss in batch: 0.219213  [   26/   89]
Per-example loss in batch: 0.356313  [   28/   89]
Per-example loss in batch: 0.283616  [   30/   89]
Per-example loss in batch: 0.250925  [   32/   89]
Per-example loss in batch: 0.241811  [   34/   89]
Per-example loss in batch: 0.275721  [   36/   89]
Per-example loss in batch: 0.314460  [   38/   89]
Per-example loss in batch: 0.330332  [   40/   89]
Per-example loss in batch: 0.373177  [   42/   89]
Per-example loss in batch: 0.330120  [   44/   89]
Per-example loss in batch: 0.267737  [   46/   89]
Per-example loss in batch: 0.266587  [   48/   89]
Per-example loss in batch: 0.236368  [   50/   89]
Per-example loss in batch: 0.273251  [   52/   89]
Per-example loss in batch: 0.350783  [   54/   89]
Per-example loss in batch: 0.241493  [   56/   89]
Per-example loss in batch: 0.335038  [   58/   89]
Per-example loss in batch: 0.234600  [   60/   89]
Per-example loss in batch: 0.341345  [   62/   89]
Per-example loss in batch: 0.267368  [   64/   89]
Per-example loss in batch: 0.324084  [   66/   89]
Per-example loss in batch: 0.348023  [   68/   89]
Per-example loss in batch: 0.350207  [   70/   89]
Per-example loss in batch: 0.299831  [   72/   89]
Per-example loss in batch: 0.297129  [   74/   89]
Per-example loss in batch: 0.229341  [   76/   89]
Per-example loss in batch: 0.267236  [   78/   89]
Per-example loss in batch: 0.217879  [   80/   89]
Per-example loss in batch: 0.340170  [   82/   89]
Per-example loss in batch: 0.321601  [   84/   89]
Per-example loss in batch: 0.358550  [   86/   89]
Per-example loss in batch: 0.329749  [   88/   89]
Per-example loss in batch: 0.576666  [   89/   89]
Train Error: Avg loss: 0.29714195
validation Error: 
 Avg loss: 0.51036416 
 F1: 0.417182 
 Precision: 0.500451 
 Recall: 0.357670
 IoU: 0.263569

test Error: 
 Avg loss: 0.49066090 
 F1: 0.451501 
 Precision: 0.542748 
 Recall: 0.386519
 IoU: 0.291573

We have finished training iteration 163
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_161_.pth
Per-example loss in batch: 0.289333  [    2/   89]
Per-example loss in batch: 0.331548  [    4/   89]
Per-example loss in batch: 0.261035  [    6/   89]
Per-example loss in batch: 0.286045  [    8/   89]
Per-example loss in batch: 0.334529  [   10/   89]
Per-example loss in batch: 0.357271  [   12/   89]
Per-example loss in batch: 0.291576  [   14/   89]
Per-example loss in batch: 0.364645  [   16/   89]
Per-example loss in batch: 0.301638  [   18/   89]
Per-example loss in batch: 0.243519  [   20/   89]
Per-example loss in batch: 0.252618  [   22/   89]
Per-example loss in batch: 0.246543  [   24/   89]
Per-example loss in batch: 0.277234  [   26/   89]
Per-example loss in batch: 0.324640  [   28/   89]
Per-example loss in batch: 0.337135  [   30/   89]
Per-example loss in batch: 0.297603  [   32/   89]
Per-example loss in batch: 0.348161  [   34/   89]
Per-example loss in batch: 0.314219  [   36/   89]
Per-example loss in batch: 0.279681  [   38/   89]
Per-example loss in batch: 0.378635  [   40/   89]
Per-example loss in batch: 0.335579  [   42/   89]
Per-example loss in batch: 0.283072  [   44/   89]
Per-example loss in batch: 0.241106  [   46/   89]
Per-example loss in batch: 0.229428  [   48/   89]
Per-example loss in batch: 0.294985  [   50/   89]
Per-example loss in batch: 0.310125  [   52/   89]
Per-example loss in batch: 0.311330  [   54/   89]
Per-example loss in batch: 0.299869  [   56/   89]
Per-example loss in batch: 0.259963  [   58/   89]
Per-example loss in batch: 0.278717  [   60/   89]
Per-example loss in batch: 0.272720  [   62/   89]
Per-example loss in batch: 0.364792  [   64/   89]
Per-example loss in batch: 0.299413  [   66/   89]
Per-example loss in batch: 0.233095  [   68/   89]
Per-example loss in batch: 0.319991  [   70/   89]
Per-example loss in batch: 0.248933  [   72/   89]
Per-example loss in batch: 0.286745  [   74/   89]
Per-example loss in batch: 0.365379  [   76/   89]
Per-example loss in batch: 0.312947  [   78/   89]
Per-example loss in batch: 0.295918  [   80/   89]
Per-example loss in batch: 0.350785  [   82/   89]
Per-example loss in batch: 0.276491  [   84/   89]
Per-example loss in batch: 0.261563  [   86/   89]
Per-example loss in batch: 0.265808  [   88/   89]
Per-example loss in batch: 0.584562  [   89/   89]
Train Error: Avg loss: 0.30131786
validation Error: 
 Avg loss: 0.51105815 
 F1: 0.415304 
 Precision: 0.470615 
 Recall: 0.371626
 IoU: 0.262071

test Error: 
 Avg loss: 0.49141229 
 F1: 0.451503 
 Precision: 0.509997 
 Recall: 0.405046
 IoU: 0.291575

We have finished training iteration 164
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_162_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.233364  [    2/   89]
Per-example loss in batch: 0.345581  [    4/   89]
Per-example loss in batch: 0.300969  [    6/   89]
Per-example loss in batch: 0.283442  [    8/   89]
Per-example loss in batch: 0.321045  [   10/   89]
Per-example loss in batch: 0.321958  [   12/   89]
Per-example loss in batch: 0.270724  [   14/   89]
Per-example loss in batch: 0.303797  [   16/   89]
Per-example loss in batch: 0.246644  [   18/   89]
Per-example loss in batch: 0.288888  [   20/   89]
Per-example loss in batch: 0.349741  [   22/   89]
Per-example loss in batch: 0.352163  [   24/   89]
Per-example loss in batch: 0.353231  [   26/   89]
Per-example loss in batch: 0.309236  [   28/   89]
Per-example loss in batch: 0.269530  [   30/   89]
Per-example loss in batch: 0.292039  [   32/   89]
Per-example loss in batch: 0.353864  [   34/   89]
Per-example loss in batch: 0.283074  [   36/   89]
Per-example loss in batch: 0.368608  [   38/   89]
Per-example loss in batch: 0.281638  [   40/   89]
Per-example loss in batch: 0.325685  [   42/   89]
Per-example loss in batch: 0.330526  [   44/   89]
Per-example loss in batch: 0.288493  [   46/   89]
Per-example loss in batch: 0.274620  [   48/   89]
Per-example loss in batch: 0.235959  [   50/   89]
Per-example loss in batch: 0.314915  [   52/   89]
Per-example loss in batch: 0.289451  [   54/   89]
Per-example loss in batch: 0.260366  [   56/   89]
Per-example loss in batch: 0.326733  [   58/   89]
Per-example loss in batch: 0.249312  [   60/   89]
Per-example loss in batch: 0.253327  [   62/   89]
Per-example loss in batch: 0.285003  [   64/   89]
Per-example loss in batch: 0.334226  [   66/   89]
Per-example loss in batch: 0.219861  [   68/   89]
Per-example loss in batch: 0.357305  [   70/   89]
Per-example loss in batch: 0.296038  [   72/   89]
Per-example loss in batch: 0.289191  [   74/   89]
Per-example loss in batch: 0.259160  [   76/   89]
Per-example loss in batch: 0.315303  [   78/   89]
Per-example loss in batch: 0.305806  [   80/   89]
Per-example loss in batch: 0.253892  [   82/   89]
Per-example loss in batch: 0.245489  [   84/   89]
Per-example loss in batch: 0.226153  [   86/   89]
Per-example loss in batch: 0.230013  [   88/   89]
Per-example loss in batch: 0.532230  [   89/   89]
Train Error: Avg loss: 0.29578602
validation Error: 
 Avg loss: 0.51096511 
 F1: 0.415698 
 Precision: 0.477786 
 Recall: 0.367890
 IoU: 0.262385

test Error: 
 Avg loss: 0.49128331 
 F1: 0.451323 
 Precision: 0.517508 
 Recall: 0.400147
 IoU: 0.291425

We have finished training iteration 165
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_163_.pth
Per-example loss in batch: 0.248015  [    2/   89]
Per-example loss in batch: 0.315515  [    4/   89]
Per-example loss in batch: 0.273699  [    6/   89]
Per-example loss in batch: 0.290005  [    8/   89]
Per-example loss in batch: 0.308921  [   10/   89]
Per-example loss in batch: 0.233812  [   12/   89]
Per-example loss in batch: 0.300493  [   14/   89]
Per-example loss in batch: 0.277669  [   16/   89]
Per-example loss in batch: 0.254535  [   18/   89]
Per-example loss in batch: 0.225548  [   20/   89]
Per-example loss in batch: 0.277837  [   22/   89]
Per-example loss in batch: 0.229249  [   24/   89]
Per-example loss in batch: 0.237402  [   26/   89]
Per-example loss in batch: 0.307789  [   28/   89]
Per-example loss in batch: 0.335317  [   30/   89]
Per-example loss in batch: 0.276164  [   32/   89]
Per-example loss in batch: 0.297424  [   34/   89]
Per-example loss in batch: 0.318493  [   36/   89]
Per-example loss in batch: 0.314261  [   38/   89]
Per-example loss in batch: 0.305962  [   40/   89]
Per-example loss in batch: 0.322891  [   42/   89]
Per-example loss in batch: 0.295997  [   44/   89]
Per-example loss in batch: 0.366060  [   46/   89]
Per-example loss in batch: 0.375225  [   48/   89]
Per-example loss in batch: 0.243931  [   50/   89]
Per-example loss in batch: 0.279030  [   52/   89]
Per-example loss in batch: 0.358605  [   54/   89]
Per-example loss in batch: 0.335918  [   56/   89]
Per-example loss in batch: 0.254268  [   58/   89]
Per-example loss in batch: 0.230752  [   60/   89]
Per-example loss in batch: 0.212549  [   62/   89]
Per-example loss in batch: 0.309879  [   64/   89]
Per-example loss in batch: 0.301338  [   66/   89]
Per-example loss in batch: 0.291987  [   68/   89]
Per-example loss in batch: 0.328241  [   70/   89]
Per-example loss in batch: 0.319571  [   72/   89]
Per-example loss in batch: 0.367404  [   74/   89]
Per-example loss in batch: 0.316031  [   76/   89]
Per-example loss in batch: 0.272424  [   78/   89]
Per-example loss in batch: 0.290340  [   80/   89]
Per-example loss in batch: 0.330913  [   82/   89]
Per-example loss in batch: 0.324319  [   84/   89]
Per-example loss in batch: 0.234299  [   86/   89]
Per-example loss in batch: 0.313985  [   88/   89]
Per-example loss in batch: 0.695817  [   89/   89]
Train Error: Avg loss: 0.29779720
validation Error: 
 Avg loss: 0.51032497 
 F1: 0.417633 
 Precision: 0.503971 
 Recall: 0.356551
 IoU: 0.263929

test Error: 
 Avg loss: 0.49071847 
 F1: 0.450458 
 Precision: 0.542984 
 Recall: 0.384874
 IoU: 0.290704

We have finished training iteration 166
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_164_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.328391  [    2/   89]
Per-example loss in batch: 0.301487  [    4/   89]
Per-example loss in batch: 0.273405  [    6/   89]
Per-example loss in batch: 0.253797  [    8/   89]
Per-example loss in batch: 0.318118  [   10/   89]
Per-example loss in batch: 0.290366  [   12/   89]
Per-example loss in batch: 0.335968  [   14/   89]
Per-example loss in batch: 0.325974  [   16/   89]
Per-example loss in batch: 0.369062  [   18/   89]
Per-example loss in batch: 0.275221  [   20/   89]
Per-example loss in batch: 0.358737  [   22/   89]
Per-example loss in batch: 0.354539  [   24/   89]
Per-example loss in batch: 0.309913  [   26/   89]
Per-example loss in batch: 0.255967  [   28/   89]
Per-example loss in batch: 0.348798  [   30/   89]
Per-example loss in batch: 0.336710  [   32/   89]
Per-example loss in batch: 0.274675  [   34/   89]
Per-example loss in batch: 0.318235  [   36/   89]
Per-example loss in batch: 0.240540  [   38/   89]
Per-example loss in batch: 0.359370  [   40/   89]
Per-example loss in batch: 0.274199  [   42/   89]
Per-example loss in batch: 0.297182  [   44/   89]
Per-example loss in batch: 0.236426  [   46/   89]
Per-example loss in batch: 0.234113  [   48/   89]
Per-example loss in batch: 0.313216  [   50/   89]
Per-example loss in batch: 0.265479  [   52/   89]
Per-example loss in batch: 0.277945  [   54/   89]
Per-example loss in batch: 0.261968  [   56/   89]
Per-example loss in batch: 0.264689  [   58/   89]
Per-example loss in batch: 0.311537  [   60/   89]
Per-example loss in batch: 0.318078  [   62/   89]
Per-example loss in batch: 0.248547  [   64/   89]
Per-example loss in batch: 0.225417  [   66/   89]
Per-example loss in batch: 0.325773  [   68/   89]
Per-example loss in batch: 0.268207  [   70/   89]
Per-example loss in batch: 0.276019  [   72/   89]
Per-example loss in batch: 0.240131  [   74/   89]
Per-example loss in batch: 0.297089  [   76/   89]
Per-example loss in batch: 0.313798  [   78/   89]
Per-example loss in batch: 0.255109  [   80/   89]
Per-example loss in batch: 0.245100  [   82/   89]
Per-example loss in batch: 0.286255  [   84/   89]
Per-example loss in batch: 0.324784  [   86/   89]
Per-example loss in batch: 0.273068  [   88/   89]
Per-example loss in batch: 0.725848  [   89/   89]
Train Error: Avg loss: 0.29722078
validation Error: 
 Avg loss: 0.51041749 
 F1: 0.416748 
 Precision: 0.460870 
 Recall: 0.380335
 IoU: 0.263222

test Error: 
 Avg loss: 0.49113371 
 F1: 0.454371 
 Precision: 0.507959 
 Recall: 0.411011
 IoU: 0.293972

We have finished training iteration 167
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_165_.pth
Per-example loss in batch: 0.261266  [    2/   89]
Per-example loss in batch: 0.312765  [    4/   89]
Per-example loss in batch: 0.238308  [    6/   89]
Per-example loss in batch: 0.248854  [    8/   89]
Per-example loss in batch: 0.326643  [   10/   89]
Per-example loss in batch: 0.336847  [   12/   89]
Per-example loss in batch: 0.328372  [   14/   89]
Per-example loss in batch: 0.345741  [   16/   89]
Per-example loss in batch: 0.303306  [   18/   89]
Per-example loss in batch: 0.350399  [   20/   89]
Per-example loss in batch: 0.257400  [   22/   89]
Per-example loss in batch: 0.241550  [   24/   89]
Per-example loss in batch: 0.284052  [   26/   89]
Per-example loss in batch: 0.228188  [   28/   89]
Per-example loss in batch: 0.294551  [   30/   89]
Per-example loss in batch: 0.310451  [   32/   89]
Per-example loss in batch: 0.374217  [   34/   89]
Per-example loss in batch: 0.262369  [   36/   89]
Per-example loss in batch: 0.312760  [   38/   89]
Per-example loss in batch: 0.233190  [   40/   89]
Per-example loss in batch: 0.308507  [   42/   89]
Per-example loss in batch: 0.319065  [   44/   89]
Per-example loss in batch: 0.331182  [   46/   89]
Per-example loss in batch: 0.281223  [   48/   89]
Per-example loss in batch: 0.324283  [   50/   89]
Per-example loss in batch: 0.267779  [   52/   89]
Per-example loss in batch: 0.287712  [   54/   89]
Per-example loss in batch: 0.234065  [   56/   89]
Per-example loss in batch: 0.268587  [   58/   89]
Per-example loss in batch: 0.309698  [   60/   89]
Per-example loss in batch: 0.288465  [   62/   89]
Per-example loss in batch: 0.246781  [   64/   89]
Per-example loss in batch: 0.289999  [   66/   89]
Per-example loss in batch: 0.329200  [   68/   89]
Per-example loss in batch: 0.252883  [   70/   89]
Per-example loss in batch: 0.362258  [   72/   89]
Per-example loss in batch: 0.317851  [   74/   89]
Per-example loss in batch: 0.284506  [   76/   89]
Per-example loss in batch: 0.332459  [   78/   89]
Per-example loss in batch: 0.284723  [   80/   89]
Per-example loss in batch: 0.311282  [   82/   89]
Per-example loss in batch: 0.302061  [   84/   89]
Per-example loss in batch: 0.251657  [   86/   89]
Per-example loss in batch: 0.303079  [   88/   89]
Per-example loss in batch: 0.541951  [   89/   89]
Train Error: Avg loss: 0.29688777
validation Error: 
 Avg loss: 0.51066937 
 F1: 0.417529 
 Precision: 0.497169 
 Recall: 0.359881
 IoU: 0.263846

test Error: 
 Avg loss: 0.49073638 
 F1: 0.450979 
 Precision: 0.537493 
 Recall: 0.388455
 IoU: 0.291138

We have finished training iteration 168
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_166_.pth
Per-example loss in batch: 0.253194  [    2/   89]
Per-example loss in batch: 0.278625  [    4/   89]
Per-example loss in batch: 0.293894  [    6/   89]
Per-example loss in batch: 0.359113  [    8/   89]
Per-example loss in batch: 0.287174  [   10/   89]
Per-example loss in batch: 0.254513  [   12/   89]
Per-example loss in batch: 0.337077  [   14/   89]
Per-example loss in batch: 0.303312  [   16/   89]
Per-example loss in batch: 0.268890  [   18/   89]
Per-example loss in batch: 0.270230  [   20/   89]
Per-example loss in batch: 0.351731  [   22/   89]
Per-example loss in batch: 0.302941  [   24/   89]
Per-example loss in batch: 0.278827  [   26/   89]
Per-example loss in batch: 0.224004  [   28/   89]
Per-example loss in batch: 0.258772  [   30/   89]
Per-example loss in batch: 0.271952  [   32/   89]
Per-example loss in batch: 0.345712  [   34/   89]
Per-example loss in batch: 0.258046  [   36/   89]
Per-example loss in batch: 0.354832  [   38/   89]
Per-example loss in batch: 0.277070  [   40/   89]
Per-example loss in batch: 0.358497  [   42/   89]
Per-example loss in batch: 0.286505  [   44/   89]
Per-example loss in batch: 0.326208  [   46/   89]
Per-example loss in batch: 0.298771  [   48/   89]
Per-example loss in batch: 0.281928  [   50/   89]
Per-example loss in batch: 0.357909  [   52/   89]
Per-example loss in batch: 0.232647  [   54/   89]
Per-example loss in batch: 0.279416  [   56/   89]
Per-example loss in batch: 0.327299  [   58/   89]
Per-example loss in batch: 0.280129  [   60/   89]
Per-example loss in batch: 0.297721  [   62/   89]
Per-example loss in batch: 0.290989  [   64/   89]
Per-example loss in batch: 0.301712  [   66/   89]
Per-example loss in batch: 0.267202  [   68/   89]
Per-example loss in batch: 0.359368  [   70/   89]
Per-example loss in batch: 0.240044  [   72/   89]
Per-example loss in batch: 0.357554  [   74/   89]
Per-example loss in batch: 0.307929  [   76/   89]
Per-example loss in batch: 0.297275  [   78/   89]
Per-example loss in batch: 0.239466  [   80/   89]
Per-example loss in batch: 0.263237  [   82/   89]
Per-example loss in batch: 0.276377  [   84/   89]
Per-example loss in batch: 0.323573  [   86/   89]
Per-example loss in batch: 0.246118  [   88/   89]
Per-example loss in batch: 0.462882  [   89/   89]
Train Error: Avg loss: 0.29571288
validation Error: 
 Avg loss: 0.51083869 
 F1: 0.416863 
 Precision: 0.474949 
 Recall: 0.371437
 IoU: 0.263315

test Error: 
 Avg loss: 0.49111445 
 F1: 0.454455 
 Precision: 0.519541 
 Recall: 0.403862
 IoU: 0.294042

We have finished training iteration 169
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_167_.pth
Per-example loss in batch: 0.293150  [    2/   89]
Per-example loss in batch: 0.251842  [    4/   89]
Per-example loss in batch: 0.351962  [    6/   89]
Per-example loss in batch: 0.315468  [    8/   89]
Per-example loss in batch: 0.255973  [   10/   89]
Per-example loss in batch: 0.331901  [   12/   89]
Per-example loss in batch: 0.320636  [   14/   89]
Per-example loss in batch: 0.257032  [   16/   89]
Per-example loss in batch: 0.329453  [   18/   89]
Per-example loss in batch: 0.231228  [   20/   89]
Per-example loss in batch: 0.368199  [   22/   89]
Per-example loss in batch: 0.368679  [   24/   89]
Per-example loss in batch: 0.319537  [   26/   89]
Per-example loss in batch: 0.331254  [   28/   89]
Per-example loss in batch: 0.233510  [   30/   89]
Per-example loss in batch: 0.326542  [   32/   89]
Per-example loss in batch: 0.363867  [   34/   89]
Per-example loss in batch: 0.270994  [   36/   89]
Per-example loss in batch: 0.262102  [   38/   89]
Per-example loss in batch: 0.315780  [   40/   89]
Per-example loss in batch: 0.230833  [   42/   89]
Per-example loss in batch: 0.347055  [   44/   89]
Per-example loss in batch: 0.224314  [   46/   89]
Per-example loss in batch: 0.316434  [   48/   89]
Per-example loss in batch: 0.274563  [   50/   89]
Per-example loss in batch: 0.246546  [   52/   89]
Per-example loss in batch: 0.318674  [   54/   89]
Per-example loss in batch: 0.272900  [   56/   89]
Per-example loss in batch: 0.291068  [   58/   89]
Per-example loss in batch: 0.302594  [   60/   89]
Per-example loss in batch: 0.289464  [   62/   89]
Per-example loss in batch: 0.343231  [   64/   89]
Per-example loss in batch: 0.252835  [   66/   89]
Per-example loss in batch: 0.228224  [   68/   89]
Per-example loss in batch: 0.294505  [   70/   89]
Per-example loss in batch: 0.251403  [   72/   89]
Per-example loss in batch: 0.328486  [   74/   89]
Per-example loss in batch: 0.247523  [   76/   89]
Per-example loss in batch: 0.359263  [   78/   89]
Per-example loss in batch: 0.340226  [   80/   89]
Per-example loss in batch: 0.277340  [   82/   89]
Per-example loss in batch: 0.370735  [   84/   89]
Per-example loss in batch: 0.269611  [   86/   89]
Per-example loss in batch: 0.278858  [   88/   89]
Per-example loss in batch: 0.434692  [   89/   89]
Train Error: Avg loss: 0.29827287
validation Error: 
 Avg loss: 0.51061245 
 F1: 0.417577 
 Precision: 0.496013 
 Recall: 0.360561
 IoU: 0.263885

test Error: 
 Avg loss: 0.49090806 
 F1: 0.450806 
 Precision: 0.532845 
 Recall: 0.390658
 IoU: 0.290994

We have finished training iteration 170
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_168_.pth
Per-example loss in batch: 0.310569  [    2/   89]
Per-example loss in batch: 0.304285  [    4/   89]
Per-example loss in batch: 0.239916  [    6/   89]
Per-example loss in batch: 0.292303  [    8/   89]
Per-example loss in batch: 0.282653  [   10/   89]
Per-example loss in batch: 0.224943  [   12/   89]
Per-example loss in batch: 0.348134  [   14/   89]
Per-example loss in batch: 0.301614  [   16/   89]
Per-example loss in batch: 0.322583  [   18/   89]
Per-example loss in batch: 0.289785  [   20/   89]
Per-example loss in batch: 0.323485  [   22/   89]
Per-example loss in batch: 0.308327  [   24/   89]
Per-example loss in batch: 0.395417  [   26/   89]
Per-example loss in batch: 0.366135  [   28/   89]
Per-example loss in batch: 0.271307  [   30/   89]
Per-example loss in batch: 0.330093  [   32/   89]
Per-example loss in batch: 0.274704  [   34/   89]
Per-example loss in batch: 0.308076  [   36/   89]
Per-example loss in batch: 0.301805  [   38/   89]
Per-example loss in batch: 0.245841  [   40/   89]
Per-example loss in batch: 0.326713  [   42/   89]
Per-example loss in batch: 0.357709  [   44/   89]
Per-example loss in batch: 0.229082  [   46/   89]
Per-example loss in batch: 0.232954  [   48/   89]
Per-example loss in batch: 0.279005  [   50/   89]
Per-example loss in batch: 0.298823  [   52/   89]
Per-example loss in batch: 0.286269  [   54/   89]
Per-example loss in batch: 0.251392  [   56/   89]
Per-example loss in batch: 0.277829  [   58/   89]
Per-example loss in batch: 0.240273  [   60/   89]
Per-example loss in batch: 0.314290  [   62/   89]
Per-example loss in batch: 0.283604  [   64/   89]
Per-example loss in batch: 0.284121  [   66/   89]
Per-example loss in batch: 0.329236  [   68/   89]
Per-example loss in batch: 0.278536  [   70/   89]
Per-example loss in batch: 0.285854  [   72/   89]
Per-example loss in batch: 0.251909  [   74/   89]
Per-example loss in batch: 0.316917  [   76/   89]
Per-example loss in batch: 0.304638  [   78/   89]
Per-example loss in batch: 0.214858  [   80/   89]
Per-example loss in batch: 0.281721  [   82/   89]
Per-example loss in batch: 0.354607  [   84/   89]
Per-example loss in batch: 0.249319  [   86/   89]
Per-example loss in batch: 0.368741  [   88/   89]
Per-example loss in batch: 0.752810  [   89/   89]
Train Error: Avg loss: 0.29925348
validation Error: 
 Avg loss: 0.51120848 
 F1: 0.412109 
 Precision: 0.424332 
 Recall: 0.400571
 IoU: 0.259532

test Error: 
 Avg loss: 0.49176802 
 F1: 0.450729 
 Precision: 0.468427 
 Recall: 0.434319
 IoU: 0.290929

We have finished training iteration 171
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_169_.pth
Per-example loss in batch: 0.273635  [    2/   89]
Per-example loss in batch: 0.328973  [    4/   89]
Per-example loss in batch: 0.356221  [    6/   89]
Per-example loss in batch: 0.355865  [    8/   89]
Per-example loss in batch: 0.240504  [   10/   89]
Per-example loss in batch: 0.233944  [   12/   89]
Per-example loss in batch: 0.300080  [   14/   89]
Per-example loss in batch: 0.246723  [   16/   89]
Per-example loss in batch: 0.334908  [   18/   89]
Per-example loss in batch: 0.355819  [   20/   89]
Per-example loss in batch: 0.284567  [   22/   89]
Per-example loss in batch: 0.305051  [   24/   89]
Per-example loss in batch: 0.311250  [   26/   89]
Per-example loss in batch: 0.338303  [   28/   89]
Per-example loss in batch: 0.268891  [   30/   89]
Per-example loss in batch: 0.325728  [   32/   89]
Per-example loss in batch: 0.253656  [   34/   89]
Per-example loss in batch: 0.277968  [   36/   89]
Per-example loss in batch: 0.316896  [   38/   89]
Per-example loss in batch: 0.246755  [   40/   89]
Per-example loss in batch: 0.397152  [   42/   89]
Per-example loss in batch: 0.341142  [   44/   89]
Per-example loss in batch: 0.283521  [   46/   89]
Per-example loss in batch: 0.307281  [   48/   89]
Per-example loss in batch: 0.262013  [   50/   89]
Per-example loss in batch: 0.310582  [   52/   89]
Per-example loss in batch: 0.317582  [   54/   89]
Per-example loss in batch: 0.232064  [   56/   89]
Per-example loss in batch: 0.292855  [   58/   89]
Per-example loss in batch: 0.287847  [   60/   89]
Per-example loss in batch: 0.265627  [   62/   89]
Per-example loss in batch: 0.277573  [   64/   89]
Per-example loss in batch: 0.282812  [   66/   89]
Per-example loss in batch: 0.246951  [   68/   89]
Per-example loss in batch: 0.328948  [   70/   89]
Per-example loss in batch: 0.240126  [   72/   89]
Per-example loss in batch: 0.338652  [   74/   89]
Per-example loss in batch: 0.268256  [   76/   89]
Per-example loss in batch: 0.293458  [   78/   89]
Per-example loss in batch: 0.360017  [   80/   89]
Per-example loss in batch: 0.323773  [   82/   89]
Per-example loss in batch: 0.366540  [   84/   89]
Per-example loss in batch: 0.276236  [   86/   89]
Per-example loss in batch: 0.241128  [   88/   89]
Per-example loss in batch: 0.462834  [   89/   89]
Train Error: Avg loss: 0.29953459
validation Error: 
 Avg loss: 0.51061948 
 F1: 0.417281 
 Precision: 0.485552 
 Recall: 0.365842
 IoU: 0.263649

test Error: 
 Avg loss: 0.49095193 
 F1: 0.451243 
 Precision: 0.524928 
 Recall: 0.395698
 IoU: 0.291358

We have finished training iteration 172
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_170_.pth
Per-example loss in batch: 0.251555  [    2/   89]
Per-example loss in batch: 0.304798  [    4/   89]
Per-example loss in batch: 0.305808  [    6/   89]
Per-example loss in batch: 0.249781  [    8/   89]
Per-example loss in batch: 0.264022  [   10/   89]
Per-example loss in batch: 0.336431  [   12/   89]
Per-example loss in batch: 0.276189  [   14/   89]
Per-example loss in batch: 0.277967  [   16/   89]
Per-example loss in batch: 0.297055  [   18/   89]
Per-example loss in batch: 0.220917  [   20/   89]
Per-example loss in batch: 0.315694  [   22/   89]
Per-example loss in batch: 0.257811  [   24/   89]
Per-example loss in batch: 0.294406  [   26/   89]
Per-example loss in batch: 0.338246  [   28/   89]
Per-example loss in batch: 0.258832  [   30/   89]
Per-example loss in batch: 0.304515  [   32/   89]
Per-example loss in batch: 0.253147  [   34/   89]
Per-example loss in batch: 0.219189  [   36/   89]
Per-example loss in batch: 0.319666  [   38/   89]
Per-example loss in batch: 0.343676  [   40/   89]
Per-example loss in batch: 0.287492  [   42/   89]
Per-example loss in batch: 0.305475  [   44/   89]
Per-example loss in batch: 0.349701  [   46/   89]
Per-example loss in batch: 0.381121  [   48/   89]
Per-example loss in batch: 0.429615  [   50/   89]
Per-example loss in batch: 0.278899  [   52/   89]
Per-example loss in batch: 0.302268  [   54/   89]
Per-example loss in batch: 0.246965  [   56/   89]
Per-example loss in batch: 0.298848  [   58/   89]
Per-example loss in batch: 0.284519  [   60/   89]
Per-example loss in batch: 0.255884  [   62/   89]
Per-example loss in batch: 0.335807  [   64/   89]
Per-example loss in batch: 0.290709  [   66/   89]
Per-example loss in batch: 0.295817  [   68/   89]
Per-example loss in batch: 0.298252  [   70/   89]
Per-example loss in batch: 0.300726  [   72/   89]
Per-example loss in batch: 0.250096  [   74/   89]
Per-example loss in batch: 0.291766  [   76/   89]
Per-example loss in batch: 0.282046  [   78/   89]
Per-example loss in batch: 0.358139  [   80/   89]
Per-example loss in batch: 0.234153  [   82/   89]
Per-example loss in batch: 0.326817  [   84/   89]
Per-example loss in batch: 0.318722  [   86/   89]
Per-example loss in batch: 0.263421  [   88/   89]
Per-example loss in batch: 0.555072  [   89/   89]
Train Error: Avg loss: 0.29740450
validation Error: 
 Avg loss: 0.51088770 
 F1: 0.415868 
 Precision: 0.472339 
 Recall: 0.371458
 IoU: 0.262521

test Error: 
 Avg loss: 0.49120573 
 F1: 0.451026 
 Precision: 0.509625 
 Recall: 0.404514
 IoU: 0.291177

We have finished training iteration 173
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_171_.pth
Per-example loss in batch: 0.336514  [    2/   89]
Per-example loss in batch: 0.310727  [    4/   89]
Per-example loss in batch: 0.395776  [    6/   89]
Per-example loss in batch: 0.281070  [    8/   89]
Per-example loss in batch: 0.264857  [   10/   89]
Per-example loss in batch: 0.297661  [   12/   89]
Per-example loss in batch: 0.228988  [   14/   89]
Per-example loss in batch: 0.270401  [   16/   89]
Per-example loss in batch: 0.250248  [   18/   89]
Per-example loss in batch: 0.356635  [   20/   89]
Per-example loss in batch: 0.235280  [   22/   89]
Per-example loss in batch: 0.303953  [   24/   89]
Per-example loss in batch: 0.317021  [   26/   89]
Per-example loss in batch: 0.288119  [   28/   89]
Per-example loss in batch: 0.226042  [   30/   89]
Per-example loss in batch: 0.340388  [   32/   89]
Per-example loss in batch: 0.274310  [   34/   89]
Per-example loss in batch: 0.305520  [   36/   89]
Per-example loss in batch: 0.247775  [   38/   89]
Per-example loss in batch: 0.306866  [   40/   89]
Per-example loss in batch: 0.377865  [   42/   89]
Per-example loss in batch: 0.365502  [   44/   89]
Per-example loss in batch: 0.223560  [   46/   89]
Per-example loss in batch: 0.299895  [   48/   89]
Per-example loss in batch: 0.231073  [   50/   89]
Per-example loss in batch: 0.272566  [   52/   89]
Per-example loss in batch: 0.289647  [   54/   89]
Per-example loss in batch: 0.230365  [   56/   89]
Per-example loss in batch: 0.251257  [   58/   89]
Per-example loss in batch: 0.341191  [   60/   89]
Per-example loss in batch: 0.280464  [   62/   89]
Per-example loss in batch: 0.328155  [   64/   89]
Per-example loss in batch: 0.300678  [   66/   89]
Per-example loss in batch: 0.353251  [   68/   89]
Per-example loss in batch: 0.243397  [   70/   89]
Per-example loss in batch: 0.343796  [   72/   89]
Per-example loss in batch: 0.345386  [   74/   89]
Per-example loss in batch: 0.285108  [   76/   89]
Per-example loss in batch: 0.286871  [   78/   89]
Per-example loss in batch: 0.319870  [   80/   89]
Per-example loss in batch: 0.324323  [   82/   89]
Per-example loss in batch: 0.327146  [   84/   89]
Per-example loss in batch: 0.256825  [   86/   89]
Per-example loss in batch: 0.252077  [   88/   89]
Per-example loss in batch: 0.735494  [   89/   89]
Train Error: Avg loss: 0.29968914
validation Error: 
 Avg loss: 0.51021806 
 F1: 0.418091 
 Precision: 0.502804 
 Recall: 0.357808
 IoU: 0.264296

test Error: 
 Avg loss: 0.49046235 
 F1: 0.451533 
 Precision: 0.542840 
 Recall: 0.386519
 IoU: 0.291600

We have finished training iteration 174
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_172_.pth
Per-example loss in batch: 0.283716  [    2/   89]
Per-example loss in batch: 0.277072  [    4/   89]
Per-example loss in batch: 0.308992  [    6/   89]
Per-example loss in batch: 0.350058  [    8/   89]
Per-example loss in batch: 0.266484  [   10/   89]
Per-example loss in batch: 0.310109  [   12/   89]
Per-example loss in batch: 0.327891  [   14/   89]
Per-example loss in batch: 0.349099  [   16/   89]
Per-example loss in batch: 0.347476  [   18/   89]
Per-example loss in batch: 0.292397  [   20/   89]
Per-example loss in batch: 0.306255  [   22/   89]
Per-example loss in batch: 0.269048  [   24/   89]
Per-example loss in batch: 0.307148  [   26/   89]
Per-example loss in batch: 0.322679  [   28/   89]
Per-example loss in batch: 0.288753  [   30/   89]
Per-example loss in batch: 0.281377  [   32/   89]
Per-example loss in batch: 0.354582  [   34/   89]
Per-example loss in batch: 0.322617  [   36/   89]
Per-example loss in batch: 0.291437  [   38/   89]
Per-example loss in batch: 0.300195  [   40/   89]
Per-example loss in batch: 0.236589  [   42/   89]
Per-example loss in batch: 0.293912  [   44/   89]
Per-example loss in batch: 0.273437  [   46/   89]
Per-example loss in batch: 0.248888  [   48/   89]
Per-example loss in batch: 0.268755  [   50/   89]
Per-example loss in batch: 0.284969  [   52/   89]
Per-example loss in batch: 0.292335  [   54/   89]
Per-example loss in batch: 0.274921  [   56/   89]
Per-example loss in batch: 0.296079  [   58/   89]
Per-example loss in batch: 0.333338  [   60/   89]
Per-example loss in batch: 0.305703  [   62/   89]
Per-example loss in batch: 0.261816  [   64/   89]
Per-example loss in batch: 0.315231  [   66/   89]
Per-example loss in batch: 0.323665  [   68/   89]
Per-example loss in batch: 0.312179  [   70/   89]
Per-example loss in batch: 0.313397  [   72/   89]
Per-example loss in batch: 0.346572  [   74/   89]
Per-example loss in batch: 0.230290  [   76/   89]
Per-example loss in batch: 0.258470  [   78/   89]
Per-example loss in batch: 0.265349  [   80/   89]
Per-example loss in batch: 0.285518  [   82/   89]
Per-example loss in batch: 0.404561  [   84/   89]
Per-example loss in batch: 0.227695  [   86/   89]
Per-example loss in batch: 0.240417  [   88/   89]
Per-example loss in batch: 0.672662  [   89/   89]
Train Error: Avg loss: 0.30084948
validation Error: 
 Avg loss: 0.51120152 
 F1: 0.412777 
 Precision: 0.451045 
 Recall: 0.380494
 IoU: 0.260062

test Error: 
 Avg loss: 0.49142834 
 F1: 0.447951 
 Precision: 0.491839 
 Recall: 0.411253
 IoU: 0.288619

We have finished training iteration 175
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_173_.pth
Per-example loss in batch: 0.363001  [    2/   89]
Per-example loss in batch: 0.304153  [    4/   89]
Per-example loss in batch: 0.387813  [    6/   89]
Per-example loss in batch: 0.365724  [    8/   89]
Per-example loss in batch: 0.313216  [   10/   89]
Per-example loss in batch: 0.212509  [   12/   89]
Per-example loss in batch: 0.369636  [   14/   89]
Per-example loss in batch: 0.348525  [   16/   89]
Per-example loss in batch: 0.353935  [   18/   89]
Per-example loss in batch: 0.291982  [   20/   89]
Per-example loss in batch: 0.280285  [   22/   89]
Per-example loss in batch: 0.323716  [   24/   89]
Per-example loss in batch: 0.224430  [   26/   89]
Per-example loss in batch: 0.250366  [   28/   89]
Per-example loss in batch: 0.278397  [   30/   89]
Per-example loss in batch: 0.253960  [   32/   89]
Per-example loss in batch: 0.359422  [   34/   89]
Per-example loss in batch: 0.218486  [   36/   89]
Per-example loss in batch: 0.320161  [   38/   89]
Per-example loss in batch: 0.233990  [   40/   89]
Per-example loss in batch: 0.356496  [   42/   89]
Per-example loss in batch: 0.299015  [   44/   89]
Per-example loss in batch: 0.283296  [   46/   89]
Per-example loss in batch: 0.313411  [   48/   89]
Per-example loss in batch: 0.245062  [   50/   89]
Per-example loss in batch: 0.289597  [   52/   89]
Per-example loss in batch: 0.311254  [   54/   89]
Per-example loss in batch: 0.258117  [   56/   89]
Per-example loss in batch: 0.272412  [   58/   89]
Per-example loss in batch: 0.361958  [   60/   89]
Per-example loss in batch: 0.240634  [   62/   89]
Per-example loss in batch: 0.290059  [   64/   89]
Per-example loss in batch: 0.312002  [   66/   89]
Per-example loss in batch: 0.302978  [   68/   89]
Per-example loss in batch: 0.256724  [   70/   89]
Per-example loss in batch: 0.244152  [   72/   89]
Per-example loss in batch: 0.313363  [   74/   89]
Per-example loss in batch: 0.327704  [   76/   89]
Per-example loss in batch: 0.301031  [   78/   89]
Per-example loss in batch: 0.267163  [   80/   89]
Per-example loss in batch: 0.295641  [   82/   89]
Per-example loss in batch: 0.255766  [   84/   89]
Per-example loss in batch: 0.319164  [   86/   89]
Per-example loss in batch: 0.277596  [   88/   89]
Per-example loss in batch: 0.710102  [   89/   89]
Train Error: Avg loss: 0.30119895
validation Error: 
 Avg loss: 0.51104560 
 F1: 0.417025 
 Precision: 0.462096 
 Recall: 0.379964
 IoU: 0.263443

test Error: 
 Avg loss: 0.49131263 
 F1: 0.453904 
 Precision: 0.504143 
 Recall: 0.412770
 IoU: 0.293580

We have finished training iteration 176
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_174_.pth
Per-example loss in batch: 0.304713  [    2/   89]
Per-example loss in batch: 0.269949  [    4/   89]
Per-example loss in batch: 0.338616  [    6/   89]
Per-example loss in batch: 0.304605  [    8/   89]
Per-example loss in batch: 0.270482  [   10/   89]
Per-example loss in batch: 0.291984  [   12/   89]
Per-example loss in batch: 0.352395  [   14/   89]
Per-example loss in batch: 0.227678  [   16/   89]
Per-example loss in batch: 0.293285  [   18/   89]
Per-example loss in batch: 0.266566  [   20/   89]
Per-example loss in batch: 0.312451  [   22/   89]
Per-example loss in batch: 0.314906  [   24/   89]
Per-example loss in batch: 0.298450  [   26/   89]
Per-example loss in batch: 0.354574  [   28/   89]
Per-example loss in batch: 0.304828  [   30/   89]
Per-example loss in batch: 0.345596  [   32/   89]
Per-example loss in batch: 0.298596  [   34/   89]
Per-example loss in batch: 0.327896  [   36/   89]
Per-example loss in batch: 0.258164  [   38/   89]
Per-example loss in batch: 0.351137  [   40/   89]
Per-example loss in batch: 0.316148  [   42/   89]
Per-example loss in batch: 0.374123  [   44/   89]
Per-example loss in batch: 0.281502  [   46/   89]
Per-example loss in batch: 0.330974  [   48/   89]
Per-example loss in batch: 0.263984  [   50/   89]
Per-example loss in batch: 0.282236  [   52/   89]
Per-example loss in batch: 0.306107  [   54/   89]
Per-example loss in batch: 0.265139  [   56/   89]
Per-example loss in batch: 0.281616  [   58/   89]
Per-example loss in batch: 0.313743  [   60/   89]
Per-example loss in batch: 0.276341  [   62/   89]
Per-example loss in batch: 0.309343  [   64/   89]
Per-example loss in batch: 0.320882  [   66/   89]
Per-example loss in batch: 0.262062  [   68/   89]
Per-example loss in batch: 0.252895  [   70/   89]
Per-example loss in batch: 0.231359  [   72/   89]
Per-example loss in batch: 0.311555  [   74/   89]
Per-example loss in batch: 0.377415  [   76/   89]
Per-example loss in batch: 0.293107  [   78/   89]
Per-example loss in batch: 0.274157  [   80/   89]
Per-example loss in batch: 0.285118  [   82/   89]
Per-example loss in batch: 0.229874  [   84/   89]
Per-example loss in batch: 0.309779  [   86/   89]
Per-example loss in batch: 0.296585  [   88/   89]
Per-example loss in batch: 0.472227  [   89/   89]
Train Error: Avg loss: 0.30042754
validation Error: 
 Avg loss: 0.51124341 
 F1: 0.417329 
 Precision: 0.454556 
 Recall: 0.385738
 IoU: 0.263686

test Error: 
 Avg loss: 0.49140511 
 F1: 0.453610 
 Precision: 0.496334 
 Recall: 0.417659
 IoU: 0.293335

We have finished training iteration 177
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_175_.pth
Per-example loss in batch: 0.290191  [    2/   89]
Per-example loss in batch: 0.253256  [    4/   89]
Per-example loss in batch: 0.229122  [    6/   89]
Per-example loss in batch: 0.332457  [    8/   89]
Per-example loss in batch: 0.300158  [   10/   89]
Per-example loss in batch: 0.295341  [   12/   89]
Per-example loss in batch: 0.223167  [   14/   89]
Per-example loss in batch: 0.333040  [   16/   89]
Per-example loss in batch: 0.283849  [   18/   89]
Per-example loss in batch: 0.234425  [   20/   89]
Per-example loss in batch: 0.329377  [   22/   89]
Per-example loss in batch: 0.273347  [   24/   89]
Per-example loss in batch: 0.254534  [   26/   89]
Per-example loss in batch: 0.359814  [   28/   89]
Per-example loss in batch: 0.296463  [   30/   89]
Per-example loss in batch: 0.355435  [   32/   89]
Per-example loss in batch: 0.351096  [   34/   89]
Per-example loss in batch: 0.275853  [   36/   89]
Per-example loss in batch: 0.253927  [   38/   89]
Per-example loss in batch: 0.256061  [   40/   89]
Per-example loss in batch: 0.282728  [   42/   89]
Per-example loss in batch: 0.311646  [   44/   89]
Per-example loss in batch: 0.261370  [   46/   89]
Per-example loss in batch: 0.261649  [   48/   89]
Per-example loss in batch: 0.324610  [   50/   89]
Per-example loss in batch: 0.321195  [   52/   89]
Per-example loss in batch: 0.344406  [   54/   89]
Per-example loss in batch: 0.266335  [   56/   89]
Per-example loss in batch: 0.303423  [   58/   89]
Per-example loss in batch: 0.238757  [   60/   89]
Per-example loss in batch: 0.320547  [   62/   89]
Per-example loss in batch: 0.295575  [   64/   89]
Per-example loss in batch: 0.278759  [   66/   89]
Per-example loss in batch: 0.302497  [   68/   89]
Per-example loss in batch: 0.308389  [   70/   89]
Per-example loss in batch: 0.246430  [   72/   89]
Per-example loss in batch: 0.286042  [   74/   89]
Per-example loss in batch: 0.289272  [   76/   89]
Per-example loss in batch: 0.275693  [   78/   89]
Per-example loss in batch: 0.353602  [   80/   89]
Per-example loss in batch: 0.250263  [   82/   89]
Per-example loss in batch: 0.336556  [   84/   89]
Per-example loss in batch: 0.290876  [   86/   89]
Per-example loss in batch: 0.263272  [   88/   89]
Per-example loss in batch: 0.465834  [   89/   89]
Train Error: Avg loss: 0.29275775
validation Error: 
 Avg loss: 0.51033015 
 F1: 0.418101 
 Precision: 0.495224 
 Recall: 0.361762
 IoU: 0.264303

test Error: 
 Avg loss: 0.49070733 
 F1: 0.452148 
 Precision: 0.537799 
 Recall: 0.390030
 IoU: 0.292113

We have finished training iteration 178
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_176_.pth
Per-example loss in batch: 0.324485  [    2/   89]
Per-example loss in batch: 0.325601  [    4/   89]
Per-example loss in batch: 0.266204  [    6/   89]
Per-example loss in batch: 0.240391  [    8/   89]
Per-example loss in batch: 0.214782  [   10/   89]
Per-example loss in batch: 0.219682  [   12/   89]
Per-example loss in batch: 0.300491  [   14/   89]
Per-example loss in batch: 0.301883  [   16/   89]
Per-example loss in batch: 0.347376  [   18/   89]
Per-example loss in batch: 0.309527  [   20/   89]
Per-example loss in batch: 0.278111  [   22/   89]
Per-example loss in batch: 0.257468  [   24/   89]
Per-example loss in batch: 0.296060  [   26/   89]
Per-example loss in batch: 0.246702  [   28/   89]
Per-example loss in batch: 0.316130  [   30/   89]
Per-example loss in batch: 0.229214  [   32/   89]
Per-example loss in batch: 0.357392  [   34/   89]
Per-example loss in batch: 0.274406  [   36/   89]
Per-example loss in batch: 0.304150  [   38/   89]
Per-example loss in batch: 0.309224  [   40/   89]
Per-example loss in batch: 0.264503  [   42/   89]
Per-example loss in batch: 0.239588  [   44/   89]
Per-example loss in batch: 0.369617  [   46/   89]
Per-example loss in batch: 0.347430  [   48/   89]
Per-example loss in batch: 0.303363  [   50/   89]
Per-example loss in batch: 0.255836  [   52/   89]
Per-example loss in batch: 0.285459  [   54/   89]
Per-example loss in batch: 0.270691  [   56/   89]
Per-example loss in batch: 0.271133  [   58/   89]
Per-example loss in batch: 0.230879  [   60/   89]
Per-example loss in batch: 0.274182  [   62/   89]
Per-example loss in batch: 0.217089  [   64/   89]
Per-example loss in batch: 0.346889  [   66/   89]
Per-example loss in batch: 0.253460  [   68/   89]
Per-example loss in batch: 0.296074  [   70/   89]
Per-example loss in batch: 0.364941  [   72/   89]
Per-example loss in batch: 0.316905  [   74/   89]
Per-example loss in batch: 0.255446  [   76/   89]
Per-example loss in batch: 0.273324  [   78/   89]
Per-example loss in batch: 0.265037  [   80/   89]
Per-example loss in batch: 0.288908  [   82/   89]
Per-example loss in batch: 0.330717  [   84/   89]
Per-example loss in batch: 0.323800  [   86/   89]
Per-example loss in batch: 0.382037  [   88/   89]
Per-example loss in batch: 0.589859  [   89/   89]
Train Error: Avg loss: 0.29306773
validation Error: 
 Avg loss: 0.51089560 
 F1: 0.417811 
 Precision: 0.475177 
 Recall: 0.372805
 IoU: 0.264072

test Error: 
 Avg loss: 0.49121090 
 F1: 0.453479 
 Precision: 0.516787 
 Recall: 0.403989
 IoU: 0.293225

We have finished training iteration 179
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_177_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.292674  [    2/   89]
Per-example loss in batch: 0.351946  [    4/   89]
Per-example loss in batch: 0.310591  [    6/   89]
Per-example loss in batch: 0.242052  [    8/   89]
Per-example loss in batch: 0.285734  [   10/   89]
Per-example loss in batch: 0.320708  [   12/   89]
Per-example loss in batch: 0.322887  [   14/   89]
Per-example loss in batch: 0.271222  [   16/   89]
Per-example loss in batch: 0.254905  [   18/   89]
Per-example loss in batch: 0.296392  [   20/   89]
Per-example loss in batch: 0.278717  [   22/   89]
Per-example loss in batch: 0.250849  [   24/   89]
Per-example loss in batch: 0.350071  [   26/   89]
Per-example loss in batch: 0.236666  [   28/   89]
Per-example loss in batch: 0.287036  [   30/   89]
Per-example loss in batch: 0.336615  [   32/   89]
Per-example loss in batch: 0.323424  [   34/   89]
Per-example loss in batch: 0.315773  [   36/   89]
Per-example loss in batch: 0.251065  [   38/   89]
Per-example loss in batch: 0.269950  [   40/   89]
Per-example loss in batch: 0.261646  [   42/   89]
Per-example loss in batch: 0.292589  [   44/   89]
Per-example loss in batch: 0.277120  [   46/   89]
Per-example loss in batch: 0.286086  [   48/   89]
Per-example loss in batch: 0.266812  [   50/   89]
Per-example loss in batch: 0.327111  [   52/   89]
Per-example loss in batch: 0.253738  [   54/   89]
Per-example loss in batch: 0.353420  [   56/   89]
Per-example loss in batch: 0.275778  [   58/   89]
Per-example loss in batch: 0.354707  [   60/   89]
Per-example loss in batch: 0.343035  [   62/   89]
Per-example loss in batch: 0.313418  [   64/   89]
Per-example loss in batch: 0.256065  [   66/   89]
Per-example loss in batch: 0.358370  [   68/   89]
Per-example loss in batch: 0.336144  [   70/   89]
Per-example loss in batch: 0.290605  [   72/   89]
Per-example loss in batch: 0.365156  [   74/   89]
Per-example loss in batch: 0.234708  [   76/   89]
Per-example loss in batch: 0.234993  [   78/   89]
Per-example loss in batch: 0.241002  [   80/   89]
Per-example loss in batch: 0.359720  [   82/   89]
Per-example loss in batch: 0.320968  [   84/   89]
Per-example loss in batch: 0.275197  [   86/   89]
Per-example loss in batch: 0.356633  [   88/   89]
Per-example loss in batch: 0.496851  [   89/   89]
Train Error: Avg loss: 0.29961178
validation Error: 
 Avg loss: 0.50987260 
 F1: 0.415821 
 Precision: 0.535556 
 Recall: 0.339842
 IoU: 0.262484

test Error: 
 Avg loss: 0.49033376 
 F1: 0.445067 
 Precision: 0.569199 
 Recall: 0.365383
 IoU: 0.286229

We have finished training iteration 180
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_178_.pth
Per-example loss in batch: 0.294335  [    2/   89]
Per-example loss in batch: 0.303400  [    4/   89]
Per-example loss in batch: 0.229194  [    6/   89]
Per-example loss in batch: 0.262093  [    8/   89]
Per-example loss in batch: 0.327349  [   10/   89]
Per-example loss in batch: 0.256534  [   12/   89]
Per-example loss in batch: 0.255779  [   14/   89]
Per-example loss in batch: 0.358211  [   16/   89]
Per-example loss in batch: 0.270453  [   18/   89]
Per-example loss in batch: 0.330363  [   20/   89]
Per-example loss in batch: 0.363740  [   22/   89]
Per-example loss in batch: 0.271300  [   24/   89]
Per-example loss in batch: 0.304633  [   26/   89]
Per-example loss in batch: 0.248197  [   28/   89]
Per-example loss in batch: 0.313027  [   30/   89]
Per-example loss in batch: 0.258910  [   32/   89]
Per-example loss in batch: 0.327584  [   34/   89]
Per-example loss in batch: 0.271473  [   36/   89]
Per-example loss in batch: 0.304311  [   38/   89]
Per-example loss in batch: 0.358223  [   40/   89]
Per-example loss in batch: 0.274198  [   42/   89]
Per-example loss in batch: 0.353385  [   44/   89]
Per-example loss in batch: 0.229908  [   46/   89]
Per-example loss in batch: 0.267704  [   48/   89]
Per-example loss in batch: 0.285208  [   50/   89]
Per-example loss in batch: 0.262015  [   52/   89]
Per-example loss in batch: 0.262598  [   54/   89]
Per-example loss in batch: 0.267881  [   56/   89]
Per-example loss in batch: 0.325417  [   58/   89]
Per-example loss in batch: 0.249628  [   60/   89]
Per-example loss in batch: 0.264158  [   62/   89]
Per-example loss in batch: 0.301018  [   64/   89]
Per-example loss in batch: 0.319844  [   66/   89]
Per-example loss in batch: 0.304693  [   68/   89]
Per-example loss in batch: 0.220922  [   70/   89]
Per-example loss in batch: 0.328105  [   72/   89]
Per-example loss in batch: 0.281627  [   74/   89]
Per-example loss in batch: 0.361268  [   76/   89]
Per-example loss in batch: 0.342099  [   78/   89]
Per-example loss in batch: 0.262591  [   80/   89]
Per-example loss in batch: 0.313391  [   82/   89]
Per-example loss in batch: 0.255027  [   84/   89]
Per-example loss in batch: 0.269168  [   86/   89]
Per-example loss in batch: 0.312377  [   88/   89]
Per-example loss in batch: 0.534589  [   89/   89]
Train Error: Avg loss: 0.29417149
validation Error: 
 Avg loss: 0.51052921 
 F1: 0.417887 
 Precision: 0.496124 
 Recall: 0.360965
 IoU: 0.264133

test Error: 
 Avg loss: 0.49094289 
 F1: 0.450017 
 Precision: 0.534511 
 Recall: 0.388590
 IoU: 0.290337

We have finished training iteration 181
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_179_.pth
Per-example loss in batch: 0.308496  [    2/   89]
Per-example loss in batch: 0.368991  [    4/   89]
Per-example loss in batch: 0.351861  [    6/   89]
Per-example loss in batch: 0.309789  [    8/   89]
Per-example loss in batch: 0.276400  [   10/   89]
Per-example loss in batch: 0.238787  [   12/   89]
Per-example loss in batch: 0.277784  [   14/   89]
Per-example loss in batch: 0.222283  [   16/   89]
Per-example loss in batch: 0.253660  [   18/   89]
Per-example loss in batch: 0.268555  [   20/   89]
Per-example loss in batch: 0.292146  [   22/   89]
Per-example loss in batch: 0.300521  [   24/   89]
Per-example loss in batch: 0.225949  [   26/   89]
Per-example loss in batch: 0.297162  [   28/   89]
Per-example loss in batch: 0.331892  [   30/   89]
Per-example loss in batch: 0.302221  [   32/   89]
Per-example loss in batch: 0.301943  [   34/   89]
Per-example loss in batch: 0.284820  [   36/   89]
Per-example loss in batch: 0.318785  [   38/   89]
Per-example loss in batch: 0.260209  [   40/   89]
Per-example loss in batch: 0.350250  [   42/   89]
Per-example loss in batch: 0.285668  [   44/   89]
Per-example loss in batch: 0.241576  [   46/   89]
Per-example loss in batch: 0.313722  [   48/   89]
Per-example loss in batch: 0.270942  [   50/   89]
Per-example loss in batch: 0.313235  [   52/   89]
Per-example loss in batch: 0.345461  [   54/   89]
Per-example loss in batch: 0.306270  [   56/   89]
Per-example loss in batch: 0.259984  [   58/   89]
Per-example loss in batch: 0.235076  [   60/   89]
Per-example loss in batch: 0.258112  [   62/   89]
Per-example loss in batch: 0.320392  [   64/   89]
Per-example loss in batch: 0.241368  [   66/   89]
Per-example loss in batch: 0.337379  [   68/   89]
Per-example loss in batch: 0.336244  [   70/   89]
Per-example loss in batch: 0.291802  [   72/   89]
Per-example loss in batch: 0.256913  [   74/   89]
Per-example loss in batch: 0.260801  [   76/   89]
Per-example loss in batch: 0.372833  [   78/   89]
Per-example loss in batch: 0.268725  [   80/   89]
Per-example loss in batch: 0.235188  [   82/   89]
Per-example loss in batch: 0.266743  [   84/   89]
Per-example loss in batch: 0.222557  [   86/   89]
Per-example loss in batch: 0.306318  [   88/   89]
Per-example loss in batch: 0.472739  [   89/   89]
Train Error: Avg loss: 0.29047602
validation Error: 
 Avg loss: 0.50997330 
 F1: 0.413578 
 Precision: 0.534271 
 Recall: 0.337367
 IoU: 0.260699

test Error: 
 Avg loss: 0.49043944 
 F1: 0.442720 
 Precision: 0.568329 
 Recall: 0.362584
 IoU: 0.284291

We have finished training iteration 182
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_180_.pth
Per-example loss in batch: 0.339674  [    2/   89]
Per-example loss in batch: 0.328300  [    4/   89]
Per-example loss in batch: 0.229886  [    6/   89]
Per-example loss in batch: 0.303998  [    8/   89]
Per-example loss in batch: 0.307296  [   10/   89]
Per-example loss in batch: 0.366933  [   12/   89]
Per-example loss in batch: 0.234286  [   14/   89]
Per-example loss in batch: 0.312392  [   16/   89]
Per-example loss in batch: 0.314941  [   18/   89]
Per-example loss in batch: 0.340002  [   20/   89]
Per-example loss in batch: 0.228032  [   22/   89]
Per-example loss in batch: 0.297466  [   24/   89]
Per-example loss in batch: 0.250185  [   26/   89]
Per-example loss in batch: 0.276872  [   28/   89]
Per-example loss in batch: 0.317061  [   30/   89]
Per-example loss in batch: 0.294869  [   32/   89]
Per-example loss in batch: 0.230745  [   34/   89]
Per-example loss in batch: 0.367069  [   36/   89]
Per-example loss in batch: 0.286974  [   38/   89]
Per-example loss in batch: 0.238398  [   40/   89]
Per-example loss in batch: 0.293172  [   42/   89]
Per-example loss in batch: 0.254385  [   44/   89]
Per-example loss in batch: 0.303485  [   46/   89]
Per-example loss in batch: 0.299709  [   48/   89]
Per-example loss in batch: 0.357133  [   50/   89]
Per-example loss in batch: 0.241101  [   52/   89]
Per-example loss in batch: 0.325495  [   54/   89]
Per-example loss in batch: 0.322953  [   56/   89]
Per-example loss in batch: 0.244922  [   58/   89]
Per-example loss in batch: 0.334823  [   60/   89]
Per-example loss in batch: 0.324524  [   62/   89]
Per-example loss in batch: 0.272651  [   64/   89]
Per-example loss in batch: 0.287190  [   66/   89]
Per-example loss in batch: 0.327086  [   68/   89]
Per-example loss in batch: 0.254858  [   70/   89]
Per-example loss in batch: 0.266884  [   72/   89]
Per-example loss in batch: 0.377558  [   74/   89]
Per-example loss in batch: 0.276677  [   76/   89]
Per-example loss in batch: 0.274140  [   78/   89]
Per-example loss in batch: 0.229484  [   80/   89]
Per-example loss in batch: 0.338761  [   82/   89]
Per-example loss in batch: 0.301594  [   84/   89]
Per-example loss in batch: 0.351471  [   86/   89]
Per-example loss in batch: 0.264102  [   88/   89]
Per-example loss in batch: 0.482043  [   89/   89]
Train Error: Avg loss: 0.29731586
validation Error: 
 Avg loss: 0.51104151 
 F1: 0.417388 
 Precision: 0.453993 
 Recall: 0.386245
 IoU: 0.263733

test Error: 
 Avg loss: 0.49138201 
 F1: 0.454504 
 Precision: 0.495735 
 Recall: 0.419605
 IoU: 0.294083

We have finished training iteration 183
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_181_.pth
Per-example loss in batch: 0.299665  [    2/   89]
Per-example loss in batch: 0.337905  [    4/   89]
Per-example loss in batch: 0.289693  [    6/   89]
Per-example loss in batch: 0.351013  [    8/   89]
Per-example loss in batch: 0.330210  [   10/   89]
Per-example loss in batch: 0.333031  [   12/   89]
Per-example loss in batch: 0.260565  [   14/   89]
Per-example loss in batch: 0.268658  [   16/   89]
Per-example loss in batch: 0.250170  [   18/   89]
Per-example loss in batch: 0.336762  [   20/   89]
Per-example loss in batch: 0.268074  [   22/   89]
Per-example loss in batch: 0.258999  [   24/   89]
Per-example loss in batch: 0.254546  [   26/   89]
Per-example loss in batch: 0.327580  [   28/   89]
Per-example loss in batch: 0.308955  [   30/   89]
Per-example loss in batch: 0.307389  [   32/   89]
Per-example loss in batch: 0.366957  [   34/   89]
Per-example loss in batch: 0.312920  [   36/   89]
Per-example loss in batch: 0.295568  [   38/   89]
Per-example loss in batch: 0.273596  [   40/   89]
Per-example loss in batch: 0.270813  [   42/   89]
Per-example loss in batch: 0.272008  [   44/   89]
Per-example loss in batch: 0.223689  [   46/   89]
Per-example loss in batch: 0.327591  [   48/   89]
Per-example loss in batch: 0.274098  [   50/   89]
Per-example loss in batch: 0.250139  [   52/   89]
Per-example loss in batch: 0.387151  [   54/   89]
Per-example loss in batch: 0.346232  [   56/   89]
Per-example loss in batch: 0.312997  [   58/   89]
Per-example loss in batch: 0.227540  [   60/   89]
Per-example loss in batch: 0.288670  [   62/   89]
Per-example loss in batch: 0.266201  [   64/   89]
Per-example loss in batch: 0.281547  [   66/   89]
Per-example loss in batch: 0.267499  [   68/   89]
Per-example loss in batch: 0.263343  [   70/   89]
Per-example loss in batch: 0.262824  [   72/   89]
Per-example loss in batch: 0.305653  [   74/   89]
Per-example loss in batch: 0.328043  [   76/   89]
Per-example loss in batch: 0.348914  [   78/   89]
Per-example loss in batch: 0.269326  [   80/   89]
Per-example loss in batch: 0.258320  [   82/   89]
Per-example loss in batch: 0.250933  [   84/   89]
Per-example loss in batch: 0.309862  [   86/   89]
Per-example loss in batch: 0.228509  [   88/   89]
Per-example loss in batch: 0.520179  [   89/   89]
Train Error: Avg loss: 0.29470219
validation Error: 
 Avg loss: 0.51039476 
 F1: 0.418651 
 Precision: 0.478554 
 Recall: 0.372076
 IoU: 0.264743

test Error: 
 Avg loss: 0.49113817 
 F1: 0.452543 
 Precision: 0.516922 
 Recall: 0.402423
 IoU: 0.292443

We have finished training iteration 184
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_182_.pth
Per-example loss in batch: 0.285369  [    2/   89]
Per-example loss in batch: 0.310880  [    4/   89]
Per-example loss in batch: 0.244060  [    6/   89]
Per-example loss in batch: 0.282054  [    8/   89]
Per-example loss in batch: 0.362621  [   10/   89]
Per-example loss in batch: 0.250533  [   12/   89]
Per-example loss in batch: 0.242024  [   14/   89]
Per-example loss in batch: 0.225104  [   16/   89]
Per-example loss in batch: 0.301802  [   18/   89]
Per-example loss in batch: 0.240871  [   20/   89]
Per-example loss in batch: 0.329084  [   22/   89]
Per-example loss in batch: 0.351880  [   24/   89]
Per-example loss in batch: 0.344443  [   26/   89]
Per-example loss in batch: 0.299926  [   28/   89]
Per-example loss in batch: 0.241974  [   30/   89]
Per-example loss in batch: 0.328541  [   32/   89]
Per-example loss in batch: 0.304462  [   34/   89]
Per-example loss in batch: 0.324365  [   36/   89]
Per-example loss in batch: 0.309685  [   38/   89]
Per-example loss in batch: 0.379757  [   40/   89]
Per-example loss in batch: 0.407223  [   42/   89]
Per-example loss in batch: 0.261873  [   44/   89]
Per-example loss in batch: 0.289860  [   46/   89]
Per-example loss in batch: 0.278565  [   48/   89]
Per-example loss in batch: 0.288648  [   50/   89]
Per-example loss in batch: 0.265829  [   52/   89]
Per-example loss in batch: 0.255782  [   54/   89]
Per-example loss in batch: 0.340206  [   56/   89]
Per-example loss in batch: 0.290142  [   58/   89]
Per-example loss in batch: 0.236770  [   60/   89]
Per-example loss in batch: 0.243414  [   62/   89]
Per-example loss in batch: 0.298946  [   64/   89]
Per-example loss in batch: 0.331565  [   66/   89]
Per-example loss in batch: 0.235627  [   68/   89]
Per-example loss in batch: 0.252144  [   70/   89]
Per-example loss in batch: 0.302609  [   72/   89]
Per-example loss in batch: 0.278666  [   74/   89]
Per-example loss in batch: 0.332997  [   76/   89]
Per-example loss in batch: 0.254716  [   78/   89]
Per-example loss in batch: 0.307760  [   80/   89]
Per-example loss in batch: 0.260530  [   82/   89]
Per-example loss in batch: 0.343245  [   84/   89]
Per-example loss in batch: 0.231207  [   86/   89]
Per-example loss in batch: 0.294016  [   88/   89]
Per-example loss in batch: 0.612478  [   89/   89]
Train Error: Avg loss: 0.29546104
validation Error: 
 Avg loss: 0.50931491 
 F1: 0.419893 
 Precision: 0.516629 
 Recall: 0.353670
 IoU: 0.265737

test Error: 
 Avg loss: 0.49017840 
 F1: 0.449951 
 Precision: 0.555126 
 Recall: 0.378281
 IoU: 0.290282

We have finished training iteration 185
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_183_.pth
Per-example loss in batch: 0.305181  [    2/   89]
Per-example loss in batch: 0.236467  [    4/   89]
Per-example loss in batch: 0.277715  [    6/   89]
Per-example loss in batch: 0.273164  [    8/   89]
Per-example loss in batch: 0.360225  [   10/   89]
Per-example loss in batch: 0.289935  [   12/   89]
Per-example loss in batch: 0.218870  [   14/   89]
Per-example loss in batch: 0.246733  [   16/   89]
Per-example loss in batch: 0.364805  [   18/   89]
Per-example loss in batch: 0.322851  [   20/   89]
Per-example loss in batch: 0.331687  [   22/   89]
Per-example loss in batch: 0.352763  [   24/   89]
Per-example loss in batch: 0.228744  [   26/   89]
Per-example loss in batch: 0.274461  [   28/   89]
Per-example loss in batch: 0.297861  [   30/   89]
Per-example loss in batch: 0.344882  [   32/   89]
Per-example loss in batch: 0.226598  [   34/   89]
Per-example loss in batch: 0.291499  [   36/   89]
Per-example loss in batch: 0.254295  [   38/   89]
Per-example loss in batch: 0.233564  [   40/   89]
Per-example loss in batch: 0.345539  [   42/   89]
Per-example loss in batch: 0.252574  [   44/   89]
Per-example loss in batch: 0.350607  [   46/   89]
Per-example loss in batch: 0.295004  [   48/   89]
Per-example loss in batch: 0.265731  [   50/   89]
Per-example loss in batch: 0.348136  [   52/   89]
Per-example loss in batch: 0.318852  [   54/   89]
Per-example loss in batch: 0.365600  [   56/   89]
Per-example loss in batch: 0.286139  [   58/   89]
Per-example loss in batch: 0.287798  [   60/   89]
Per-example loss in batch: 0.294833  [   62/   89]
Per-example loss in batch: 0.286260  [   64/   89]
Per-example loss in batch: 0.325321  [   66/   89]
Per-example loss in batch: 0.232613  [   68/   89]
Per-example loss in batch: 0.254479  [   70/   89]
Per-example loss in batch: 0.253569  [   72/   89]
Per-example loss in batch: 0.289976  [   74/   89]
Per-example loss in batch: 0.265604  [   76/   89]
Per-example loss in batch: 0.299686  [   78/   89]
Per-example loss in batch: 0.309713  [   80/   89]
Per-example loss in batch: 0.302567  [   82/   89]
Per-example loss in batch: 0.297377  [   84/   89]
Per-example loss in batch: 0.260719  [   86/   89]
Per-example loss in batch: 0.321666  [   88/   89]
Per-example loss in batch: 0.686259  [   89/   89]
Train Error: Avg loss: 0.29630984
validation Error: 
 Avg loss: 0.51070732 
 F1: 0.420363 
 Precision: 0.489481 
 Recall: 0.368350
 IoU: 0.266114

test Error: 
 Avg loss: 0.49089439 
 F1: 0.453565 
 Precision: 0.530253 
 Recall: 0.396256
 IoU: 0.293297

We have finished training iteration 186
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_184_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.237097  [    2/   89]
Per-example loss in batch: 0.305432  [    4/   89]
Per-example loss in batch: 0.320625  [    6/   89]
Per-example loss in batch: 0.251824  [    8/   89]
Per-example loss in batch: 0.284788  [   10/   89]
Per-example loss in batch: 0.278780  [   12/   89]
Per-example loss in batch: 0.337301  [   14/   89]
Per-example loss in batch: 0.330889  [   16/   89]
Per-example loss in batch: 0.283014  [   18/   89]
Per-example loss in batch: 0.346877  [   20/   89]
Per-example loss in batch: 0.270549  [   22/   89]
Per-example loss in batch: 0.298861  [   24/   89]
Per-example loss in batch: 0.314167  [   26/   89]
Per-example loss in batch: 0.259667  [   28/   89]
Per-example loss in batch: 0.302903  [   30/   89]
Per-example loss in batch: 0.283195  [   32/   89]
Per-example loss in batch: 0.302153  [   34/   89]
Per-example loss in batch: 0.298840  [   36/   89]
Per-example loss in batch: 0.239361  [   38/   89]
Per-example loss in batch: 0.287948  [   40/   89]
Per-example loss in batch: 0.252986  [   42/   89]
Per-example loss in batch: 0.296732  [   44/   89]
Per-example loss in batch: 0.274613  [   46/   89]
Per-example loss in batch: 0.271850  [   48/   89]
Per-example loss in batch: 0.350451  [   50/   89]
Per-example loss in batch: 0.304498  [   52/   89]
Per-example loss in batch: 0.309967  [   54/   89]
Per-example loss in batch: 0.261197  [   56/   89]
Per-example loss in batch: 0.321642  [   58/   89]
Per-example loss in batch: 0.250002  [   60/   89]
Per-example loss in batch: 0.231271  [   62/   89]
Per-example loss in batch: 0.343278  [   64/   89]
Per-example loss in batch: 0.259255  [   66/   89]
Per-example loss in batch: 0.314261  [   68/   89]
Per-example loss in batch: 0.375253  [   70/   89]
Per-example loss in batch: 0.275091  [   72/   89]
Per-example loss in batch: 0.401414  [   74/   89]
Per-example loss in batch: 0.286518  [   76/   89]
Per-example loss in batch: 0.261523  [   78/   89]
Per-example loss in batch: 0.274675  [   80/   89]
Per-example loss in batch: 0.321694  [   82/   89]
Per-example loss in batch: 0.282076  [   84/   89]
Per-example loss in batch: 0.295650  [   86/   89]
Per-example loss in batch: 0.248602  [   88/   89]
Per-example loss in batch: 0.446666  [   89/   89]
Train Error: Avg loss: 0.29487877
validation Error: 
 Avg loss: 0.51038733 
 F1: 0.420313 
 Precision: 0.494449 
 Recall: 0.365510
 IoU: 0.266074

test Error: 
 Avg loss: 0.49073964 
 F1: 0.453692 
 Precision: 0.536640 
 Recall: 0.392953
 IoU: 0.293403

We have finished training iteration 187
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_185_.pth
Per-example loss in batch: 0.326691  [    2/   89]
Per-example loss in batch: 0.267133  [    4/   89]
Per-example loss in batch: 0.361300  [    6/   89]
Per-example loss in batch: 0.226886  [    8/   89]
Per-example loss in batch: 0.236873  [   10/   89]
Per-example loss in batch: 0.332443  [   12/   89]
Per-example loss in batch: 0.266498  [   14/   89]
Per-example loss in batch: 0.365493  [   16/   89]
Per-example loss in batch: 0.248368  [   18/   89]
Per-example loss in batch: 0.257724  [   20/   89]
Per-example loss in batch: 0.268847  [   22/   89]
Per-example loss in batch: 0.258849  [   24/   89]
Per-example loss in batch: 0.275032  [   26/   89]
Per-example loss in batch: 0.273726  [   28/   89]
Per-example loss in batch: 0.292186  [   30/   89]
Per-example loss in batch: 0.270941  [   32/   89]
Per-example loss in batch: 0.283622  [   34/   89]
Per-example loss in batch: 0.288388  [   36/   89]
Per-example loss in batch: 0.331016  [   38/   89]
Per-example loss in batch: 0.317232  [   40/   89]
Per-example loss in batch: 0.260838  [   42/   89]
Per-example loss in batch: 0.264133  [   44/   89]
Per-example loss in batch: 0.318107  [   46/   89]
Per-example loss in batch: 0.279573  [   48/   89]
Per-example loss in batch: 0.291406  [   50/   89]
Per-example loss in batch: 0.269623  [   52/   89]
Per-example loss in batch: 0.255632  [   54/   89]
Per-example loss in batch: 0.340030  [   56/   89]
Per-example loss in batch: 0.223757  [   58/   89]
Per-example loss in batch: 0.307134  [   60/   89]
Per-example loss in batch: 0.255654  [   62/   89]
Per-example loss in batch: 0.318768  [   64/   89]
Per-example loss in batch: 0.290666  [   66/   89]
Per-example loss in batch: 0.361578  [   68/   89]
Per-example loss in batch: 0.291660  [   70/   89]
Per-example loss in batch: 0.295033  [   72/   89]
Per-example loss in batch: 0.333705  [   74/   89]
Per-example loss in batch: 0.267977  [   76/   89]
Per-example loss in batch: 0.350222  [   78/   89]
Per-example loss in batch: 0.348487  [   80/   89]
Per-example loss in batch: 0.350206  [   82/   89]
Per-example loss in batch: 0.229648  [   84/   89]
Per-example loss in batch: 0.327782  [   86/   89]
Per-example loss in batch: 0.311572  [   88/   89]
Per-example loss in batch: 0.537619  [   89/   89]
Train Error: Avg loss: 0.29575832
validation Error: 
 Avg loss: 0.51038611 
 F1: 0.420674 
 Precision: 0.517207 
 Recall: 0.354508
 IoU: 0.266363

test Error: 
 Avg loss: 0.49052217 
 F1: 0.452254 
 Precision: 0.557605 
 Recall: 0.380386
 IoU: 0.292202

We have finished training iteration 188
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_186_.pth
Per-example loss in batch: 0.228254  [    2/   89]
Per-example loss in batch: 0.316588  [    4/   89]
Per-example loss in batch: 0.339319  [    6/   89]
Per-example loss in batch: 0.370160  [    8/   89]
Per-example loss in batch: 0.293621  [   10/   89]
Per-example loss in batch: 0.277364  [   12/   89]
Per-example loss in batch: 0.228945  [   14/   89]
Per-example loss in batch: 0.350727  [   16/   89]
Per-example loss in batch: 0.285901  [   18/   89]
Per-example loss in batch: 0.313285  [   20/   89]
Per-example loss in batch: 0.266178  [   22/   89]
Per-example loss in batch: 0.292942  [   24/   89]
Per-example loss in batch: 0.228055  [   26/   89]
Per-example loss in batch: 0.255006  [   28/   89]
Per-example loss in batch: 0.292760  [   30/   89]
Per-example loss in batch: 0.294189  [   32/   89]
Per-example loss in batch: 0.300792  [   34/   89]
Per-example loss in batch: 0.319620  [   36/   89]
Per-example loss in batch: 0.273922  [   38/   89]
Per-example loss in batch: 0.286320  [   40/   89]
Per-example loss in batch: 0.290999  [   42/   89]
Per-example loss in batch: 0.336151  [   44/   89]
Per-example loss in batch: 0.253873  [   46/   89]
Per-example loss in batch: 0.307246  [   48/   89]
Per-example loss in batch: 0.236816  [   50/   89]
Per-example loss in batch: 0.236846  [   52/   89]
Per-example loss in batch: 0.271816  [   54/   89]
Per-example loss in batch: 0.267456  [   56/   89]
Per-example loss in batch: 0.288358  [   58/   89]
Per-example loss in batch: 0.375602  [   60/   89]
Per-example loss in batch: 0.261182  [   62/   89]
Per-example loss in batch: 0.350808  [   64/   89]
Per-example loss in batch: 0.250289  [   66/   89]
Per-example loss in batch: 0.318892  [   68/   89]
Per-example loss in batch: 0.387924  [   70/   89]
Per-example loss in batch: 0.358673  [   72/   89]
Per-example loss in batch: 0.316199  [   74/   89]
Per-example loss in batch: 0.351857  [   76/   89]
Per-example loss in batch: 0.274522  [   78/   89]
Per-example loss in batch: 0.234429  [   80/   89]
Per-example loss in batch: 0.228004  [   82/   89]
Per-example loss in batch: 0.227486  [   84/   89]
Per-example loss in batch: 0.337895  [   86/   89]
Per-example loss in batch: 0.295570  [   88/   89]
Per-example loss in batch: 0.727217  [   89/   89]
Train Error: Avg loss: 0.29744830
validation Error: 
 Avg loss: 0.51023191 
 F1: 0.419698 
 Precision: 0.508867 
 Recall: 0.357119
 IoU: 0.265581

test Error: 
 Avg loss: 0.49054040 
 F1: 0.451048 
 Precision: 0.547457 
 Recall: 0.383511
 IoU: 0.291196

We have finished training iteration 189
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_187_.pth
Per-example loss in batch: 0.237236  [    2/   89]
Per-example loss in batch: 0.346461  [    4/   89]
Per-example loss in batch: 0.345342  [    6/   89]
Per-example loss in batch: 0.313554  [    8/   89]
Per-example loss in batch: 0.381297  [   10/   89]
Per-example loss in batch: 0.334408  [   12/   89]
Per-example loss in batch: 0.220653  [   14/   89]
Per-example loss in batch: 0.316178  [   16/   89]
Per-example loss in batch: 0.282894  [   18/   89]
Per-example loss in batch: 0.262706  [   20/   89]
Per-example loss in batch: 0.295942  [   22/   89]
Per-example loss in batch: 0.284827  [   24/   89]
Per-example loss in batch: 0.229340  [   26/   89]
Per-example loss in batch: 0.324767  [   28/   89]
Per-example loss in batch: 0.282429  [   30/   89]
Per-example loss in batch: 0.278014  [   32/   89]
Per-example loss in batch: 0.311158  [   34/   89]
Per-example loss in batch: 0.255208  [   36/   89]
Per-example loss in batch: 0.296072  [   38/   89]
Per-example loss in batch: 0.250210  [   40/   89]
Per-example loss in batch: 0.263663  [   42/   89]
Per-example loss in batch: 0.290025  [   44/   89]
Per-example loss in batch: 0.299579  [   46/   89]
Per-example loss in batch: 0.262611  [   48/   89]
Per-example loss in batch: 0.349370  [   50/   89]
Per-example loss in batch: 0.301442  [   52/   89]
Per-example loss in batch: 0.287219  [   54/   89]
Per-example loss in batch: 0.309362  [   56/   89]
Per-example loss in batch: 0.285375  [   58/   89]
Per-example loss in batch: 0.279189  [   60/   89]
Per-example loss in batch: 0.261622  [   62/   89]
Per-example loss in batch: 0.312908  [   64/   89]
Per-example loss in batch: 0.273906  [   66/   89]
Per-example loss in batch: 0.357316  [   68/   89]
Per-example loss in batch: 0.301363  [   70/   89]
Per-example loss in batch: 0.322852  [   72/   89]
Per-example loss in batch: 0.297460  [   74/   89]
Per-example loss in batch: 0.245400  [   76/   89]
Per-example loss in batch: 0.324382  [   78/   89]
Per-example loss in batch: 0.310943  [   80/   89]
Per-example loss in batch: 0.298137  [   82/   89]
Per-example loss in batch: 0.316291  [   84/   89]
Per-example loss in batch: 0.313714  [   86/   89]
Per-example loss in batch: 0.247725  [   88/   89]
Per-example loss in batch: 0.538232  [   89/   89]
Train Error: Avg loss: 0.29729593
validation Error: 
 Avg loss: 0.51116564 
 F1: 0.417590 
 Precision: 0.452807 
 Recall: 0.387456
 IoU: 0.263895

test Error: 
 Avg loss: 0.49125712 
 F1: 0.455096 
 Precision: 0.497038 
 Recall: 0.419682
 IoU: 0.294579

We have finished training iteration 190
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_188_.pth
Per-example loss in batch: 0.306874  [    2/   89]
Per-example loss in batch: 0.279215  [    4/   89]
Per-example loss in batch: 0.250535  [    6/   89]
Per-example loss in batch: 0.263155  [    8/   89]
Per-example loss in batch: 0.281124  [   10/   89]
Per-example loss in batch: 0.277299  [   12/   89]
Per-example loss in batch: 0.285819  [   14/   89]
Per-example loss in batch: 0.455896  [   16/   89]
Per-example loss in batch: 0.343748  [   18/   89]
Per-example loss in batch: 0.295520  [   20/   89]
Per-example loss in batch: 0.390483  [   22/   89]
Per-example loss in batch: 0.363146  [   24/   89]
Per-example loss in batch: 0.348128  [   26/   89]
Per-example loss in batch: 0.232523  [   28/   89]
Per-example loss in batch: 0.256344  [   30/   89]
Per-example loss in batch: 0.230021  [   32/   89]
Per-example loss in batch: 0.264376  [   34/   89]
Per-example loss in batch: 0.274152  [   36/   89]
Per-example loss in batch: 0.329157  [   38/   89]
Per-example loss in batch: 0.273562  [   40/   89]
Per-example loss in batch: 0.299633  [   42/   89]
Per-example loss in batch: 0.339086  [   44/   89]
Per-example loss in batch: 0.264435  [   46/   89]
Per-example loss in batch: 0.251775  [   48/   89]
Per-example loss in batch: 0.240161  [   50/   89]
Per-example loss in batch: 0.275899  [   52/   89]
Per-example loss in batch: 0.269775  [   54/   89]
Per-example loss in batch: 0.298079  [   56/   89]
Per-example loss in batch: 0.255907  [   58/   89]
Per-example loss in batch: 0.286469  [   60/   89]
Per-example loss in batch: 0.294150  [   62/   89]
Per-example loss in batch: 0.229895  [   64/   89]
Per-example loss in batch: 0.353677  [   66/   89]
Per-example loss in batch: 0.289736  [   68/   89]
Per-example loss in batch: 0.324372  [   70/   89]
Per-example loss in batch: 0.287530  [   72/   89]
Per-example loss in batch: 0.271763  [   74/   89]
Per-example loss in batch: 0.294434  [   76/   89]
Per-example loss in batch: 0.346528  [   78/   89]
Per-example loss in batch: 0.231372  [   80/   89]
Per-example loss in batch: 0.250410  [   82/   89]
Per-example loss in batch: 0.321744  [   84/   89]
Per-example loss in batch: 0.316381  [   86/   89]
Per-example loss in batch: 0.343087  [   88/   89]
Per-example loss in batch: 0.636606  [   89/   89]
Train Error: Avg loss: 0.29788043
validation Error: 
 Avg loss: 0.51057176 
 F1: 0.419301 
 Precision: 0.481208 
 Recall: 0.371506
 IoU: 0.265263

test Error: 
 Avg loss: 0.49083327 
 F1: 0.453547 
 Precision: 0.522829 
 Recall: 0.400478
 IoU: 0.293282

We have finished training iteration 191
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_189_.pth
Per-example loss in batch: 0.320765  [    2/   89]
Per-example loss in batch: 0.247027  [    4/   89]
Per-example loss in batch: 0.229100  [    6/   89]
Per-example loss in batch: 0.330394  [    8/   89]
Per-example loss in batch: 0.298132  [   10/   89]
Per-example loss in batch: 0.221491  [   12/   89]
Per-example loss in batch: 0.282783  [   14/   89]
Per-example loss in batch: 0.282262  [   16/   89]
Per-example loss in batch: 0.297790  [   18/   89]
Per-example loss in batch: 0.366899  [   20/   89]
Per-example loss in batch: 0.359277  [   22/   89]
Per-example loss in batch: 0.315793  [   24/   89]
Per-example loss in batch: 0.251588  [   26/   89]
Per-example loss in batch: 0.238523  [   28/   89]
Per-example loss in batch: 0.289992  [   30/   89]
Per-example loss in batch: 0.319071  [   32/   89]
Per-example loss in batch: 0.254917  [   34/   89]
Per-example loss in batch: 0.246170  [   36/   89]
Per-example loss in batch: 0.251129  [   38/   89]
Per-example loss in batch: 0.271814  [   40/   89]
Per-example loss in batch: 0.245907  [   42/   89]
Per-example loss in batch: 0.348645  [   44/   89]
Per-example loss in batch: 0.261765  [   46/   89]
Per-example loss in batch: 0.251295  [   48/   89]
Per-example loss in batch: 0.266910  [   50/   89]
Per-example loss in batch: 0.325086  [   52/   89]
Per-example loss in batch: 0.281170  [   54/   89]
Per-example loss in batch: 0.296832  [   56/   89]
Per-example loss in batch: 0.352966  [   58/   89]
Per-example loss in batch: 0.345901  [   60/   89]
Per-example loss in batch: 0.257162  [   62/   89]
Per-example loss in batch: 0.270840  [   64/   89]
Per-example loss in batch: 0.303878  [   66/   89]
Per-example loss in batch: 0.262581  [   68/   89]
Per-example loss in batch: 0.312685  [   70/   89]
Per-example loss in batch: 0.297979  [   72/   89]
Per-example loss in batch: 0.283773  [   74/   89]
Per-example loss in batch: 0.245657  [   76/   89]
Per-example loss in batch: 0.305589  [   78/   89]
Per-example loss in batch: 0.242961  [   80/   89]
Per-example loss in batch: 0.285029  [   82/   89]
Per-example loss in batch: 0.270110  [   84/   89]
Per-example loss in batch: 0.238959  [   86/   89]
Per-example loss in batch: 0.311979  [   88/   89]
Per-example loss in batch: 0.749344  [   89/   89]
Train Error: Avg loss: 0.29023028
validation Error: 
 Avg loss: 0.51161567 
 F1: 0.415566 
 Precision: 0.427863 
 Recall: 0.403955
 IoU: 0.262280

test Error: 
 Avg loss: 0.49187240 
 F1: 0.454272 
 Precision: 0.473778 
 Recall: 0.436308
 IoU: 0.293888

We have finished training iteration 192
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_190_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.299583  [    2/   89]
Per-example loss in batch: 0.291209  [    4/   89]
Per-example loss in batch: 0.251898  [    6/   89]
Per-example loss in batch: 0.340346  [    8/   89]
Per-example loss in batch: 0.304461  [   10/   89]
Per-example loss in batch: 0.358600  [   12/   89]
Per-example loss in batch: 0.384423  [   14/   89]
Per-example loss in batch: 0.239026  [   16/   89]
Per-example loss in batch: 0.282646  [   18/   89]
Per-example loss in batch: 0.328001  [   20/   89]
Per-example loss in batch: 0.282988  [   22/   89]
Per-example loss in batch: 0.281442  [   24/   89]
Per-example loss in batch: 0.332679  [   26/   89]
Per-example loss in batch: 0.312210  [   28/   89]
Per-example loss in batch: 0.316563  [   30/   89]
Per-example loss in batch: 0.302874  [   32/   89]
Per-example loss in batch: 0.303875  [   34/   89]
Per-example loss in batch: 0.235259  [   36/   89]
Per-example loss in batch: 0.290543  [   38/   89]
Per-example loss in batch: 0.243255  [   40/   89]
Per-example loss in batch: 0.296289  [   42/   89]
Per-example loss in batch: 0.297790  [   44/   89]
Per-example loss in batch: 0.288153  [   46/   89]
Per-example loss in batch: 0.257411  [   48/   89]
Per-example loss in batch: 0.227555  [   50/   89]
Per-example loss in batch: 0.224855  [   52/   89]
Per-example loss in batch: 0.261871  [   54/   89]
Per-example loss in batch: 0.285295  [   56/   89]
Per-example loss in batch: 0.329236  [   58/   89]
Per-example loss in batch: 0.288264  [   60/   89]
Per-example loss in batch: 0.245924  [   62/   89]
Per-example loss in batch: 0.229381  [   64/   89]
Per-example loss in batch: 0.332414  [   66/   89]
Per-example loss in batch: 0.334880  [   68/   89]
Per-example loss in batch: 0.306509  [   70/   89]
Per-example loss in batch: 0.260566  [   72/   89]
Per-example loss in batch: 0.278855  [   74/   89]
Per-example loss in batch: 0.279247  [   76/   89]
Per-example loss in batch: 0.309846  [   78/   89]
Per-example loss in batch: 0.301128  [   80/   89]
Per-example loss in batch: 0.253322  [   82/   89]
Per-example loss in batch: 0.278772  [   84/   89]
Per-example loss in batch: 0.338893  [   86/   89]
Per-example loss in batch: 0.313681  [   88/   89]
Per-example loss in batch: 0.516542  [   89/   89]
Train Error: Avg loss: 0.29348964
validation Error: 
 Avg loss: 0.51032820 
 F1: 0.420870 
 Precision: 0.492580 
 Recall: 0.367386
 IoU: 0.266520

test Error: 
 Avg loss: 0.49056527 
 F1: 0.454745 
 Precision: 0.536404 
 Recall: 0.394664
 IoU: 0.294285

We have finished training iteration 193
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_191_.pth
Per-example loss in batch: 0.284811  [    2/   89]
Per-example loss in batch: 0.294652  [    4/   89]
Per-example loss in batch: 0.377765  [    6/   89]
Per-example loss in batch: 0.272924  [    8/   89]
Per-example loss in batch: 0.317333  [   10/   89]
Per-example loss in batch: 0.336182  [   12/   89]
Per-example loss in batch: 0.363869  [   14/   89]
Per-example loss in batch: 0.258262  [   16/   89]
Per-example loss in batch: 0.299129  [   18/   89]
Per-example loss in batch: 0.336003  [   20/   89]
Per-example loss in batch: 0.274223  [   22/   89]
Per-example loss in batch: 0.275794  [   24/   89]
Per-example loss in batch: 0.235265  [   26/   89]
Per-example loss in batch: 0.322357  [   28/   89]
Per-example loss in batch: 0.303219  [   30/   89]
Per-example loss in batch: 0.285314  [   32/   89]
Per-example loss in batch: 0.326616  [   34/   89]
Per-example loss in batch: 0.283710  [   36/   89]
Per-example loss in batch: 0.235062  [   38/   89]
Per-example loss in batch: 0.234160  [   40/   89]
Per-example loss in batch: 0.337708  [   42/   89]
Per-example loss in batch: 0.226946  [   44/   89]
Per-example loss in batch: 0.277350  [   46/   89]
Per-example loss in batch: 0.289465  [   48/   89]
Per-example loss in batch: 0.326020  [   50/   89]
Per-example loss in batch: 0.250352  [   52/   89]
Per-example loss in batch: 0.355001  [   54/   89]
Per-example loss in batch: 0.231318  [   56/   89]
Per-example loss in batch: 0.324643  [   58/   89]
Per-example loss in batch: 0.256671  [   60/   89]
Per-example loss in batch: 0.258911  [   62/   89]
Per-example loss in batch: 0.244714  [   64/   89]
Per-example loss in batch: 0.276050  [   66/   89]
Per-example loss in batch: 0.235001  [   68/   89]
Per-example loss in batch: 0.250133  [   70/   89]
Per-example loss in batch: 0.314893  [   72/   89]
Per-example loss in batch: 0.284316  [   74/   89]
Per-example loss in batch: 0.314441  [   76/   89]
Per-example loss in batch: 0.317317  [   78/   89]
Per-example loss in batch: 0.247044  [   80/   89]
Per-example loss in batch: 0.279014  [   82/   89]
Per-example loss in batch: 0.278497  [   84/   89]
Per-example loss in batch: 0.263345  [   86/   89]
Per-example loss in batch: 0.293619  [   88/   89]
Per-example loss in batch: 0.752121  [   89/   89]
Train Error: Avg loss: 0.29270741
validation Error: 
 Avg loss: 0.51068738 
 F1: 0.420123 
 Precision: 0.491409 
 Recall: 0.366899
 IoU: 0.265921

test Error: 
 Avg loss: 0.49081130 
 F1: 0.452517 
 Precision: 0.528062 
 Recall: 0.395882
 IoU: 0.292421

We have finished training iteration 194
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_192_.pth
Per-example loss in batch: 0.226170  [    2/   89]
Per-example loss in batch: 0.258540  [    4/   89]
Per-example loss in batch: 0.248231  [    6/   89]
Per-example loss in batch: 0.266787  [    8/   89]
Per-example loss in batch: 0.321128  [   10/   89]
Per-example loss in batch: 0.358978  [   12/   89]
Per-example loss in batch: 0.334691  [   14/   89]
Per-example loss in batch: 0.266985  [   16/   89]
Per-example loss in batch: 0.243520  [   18/   89]
Per-example loss in batch: 0.253688  [   20/   89]
Per-example loss in batch: 0.311922  [   22/   89]
Per-example loss in batch: 0.258960  [   24/   89]
Per-example loss in batch: 0.266854  [   26/   89]
Per-example loss in batch: 0.233629  [   28/   89]
Per-example loss in batch: 0.359745  [   30/   89]
Per-example loss in batch: 0.247789  [   32/   89]
Per-example loss in batch: 0.255872  [   34/   89]
Per-example loss in batch: 0.244278  [   36/   89]
Per-example loss in batch: 0.336125  [   38/   89]
Per-example loss in batch: 0.314608  [   40/   89]
Per-example loss in batch: 0.330694  [   42/   89]
Per-example loss in batch: 0.333929  [   44/   89]
Per-example loss in batch: 0.360169  [   46/   89]
Per-example loss in batch: 0.307190  [   48/   89]
Per-example loss in batch: 0.314826  [   50/   89]
Per-example loss in batch: 0.276792  [   52/   89]
Per-example loss in batch: 0.295992  [   54/   89]
Per-example loss in batch: 0.248433  [   56/   89]
Per-example loss in batch: 0.233136  [   58/   89]
Per-example loss in batch: 0.317151  [   60/   89]
Per-example loss in batch: 0.361002  [   62/   89]
Per-example loss in batch: 0.275420  [   64/   89]
Per-example loss in batch: 0.308100  [   66/   89]
Per-example loss in batch: 0.292180  [   68/   89]
Per-example loss in batch: 0.248496  [   70/   89]
Per-example loss in batch: 0.381787  [   72/   89]
Per-example loss in batch: 0.250164  [   74/   89]
Per-example loss in batch: 0.327179  [   76/   89]
Per-example loss in batch: 0.323938  [   78/   89]
Per-example loss in batch: 0.243393  [   80/   89]
Per-example loss in batch: 0.340463  [   82/   89]
Per-example loss in batch: 0.304943  [   84/   89]
Per-example loss in batch: 0.333942  [   86/   89]
Per-example loss in batch: 0.242693  [   88/   89]
Per-example loss in batch: 0.617924  [   89/   89]
Train Error: Avg loss: 0.29594326
validation Error: 
 Avg loss: 0.51101635 
 F1: 0.418623 
 Precision: 0.450982 
 Recall: 0.390596
 IoU: 0.264720

test Error: 
 Avg loss: 0.49128138 
 F1: 0.454935 
 Precision: 0.495499 
 Recall: 0.420510
 IoU: 0.294444

We have finished training iteration 195
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_193_.pth
Per-example loss in batch: 0.235787  [    2/   89]
Per-example loss in batch: 0.358743  [    4/   89]
Per-example loss in batch: 0.377566  [    6/   89]
Per-example loss in batch: 0.300573  [    8/   89]
Per-example loss in batch: 0.233501  [   10/   89]
Per-example loss in batch: 0.239576  [   12/   89]
Per-example loss in batch: 0.281428  [   14/   89]
Per-example loss in batch: 0.358763  [   16/   89]
Per-example loss in batch: 0.378306  [   18/   89]
Per-example loss in batch: 0.241076  [   20/   89]
Per-example loss in batch: 0.256686  [   22/   89]
Per-example loss in batch: 0.285269  [   24/   89]
Per-example loss in batch: 0.303989  [   26/   89]
Per-example loss in batch: 0.227300  [   28/   89]
Per-example loss in batch: 0.251915  [   30/   89]
Per-example loss in batch: 0.292903  [   32/   89]
Per-example loss in batch: 0.275973  [   34/   89]
Per-example loss in batch: 0.299275  [   36/   89]
Per-example loss in batch: 0.364069  [   38/   89]
Per-example loss in batch: 0.267370  [   40/   89]
Per-example loss in batch: 0.301515  [   42/   89]
Per-example loss in batch: 0.251085  [   44/   89]
Per-example loss in batch: 0.389554  [   46/   89]
Per-example loss in batch: 0.232071  [   48/   89]
Per-example loss in batch: 0.295666  [   50/   89]
Per-example loss in batch: 0.324376  [   52/   89]
Per-example loss in batch: 0.307162  [   54/   89]
Per-example loss in batch: 0.283738  [   56/   89]
Per-example loss in batch: 0.317194  [   58/   89]
Per-example loss in batch: 0.292746  [   60/   89]
Per-example loss in batch: 0.308936  [   62/   89]
Per-example loss in batch: 0.269198  [   64/   89]
Per-example loss in batch: 0.230088  [   66/   89]
Per-example loss in batch: 0.264213  [   68/   89]
Per-example loss in batch: 0.302202  [   70/   89]
Per-example loss in batch: 0.356730  [   72/   89]
Per-example loss in batch: 0.272219  [   74/   89]
Per-example loss in batch: 0.323955  [   76/   89]
Per-example loss in batch: 0.364002  [   78/   89]
Per-example loss in batch: 0.274334  [   80/   89]
Per-example loss in batch: 0.251774  [   82/   89]
Per-example loss in batch: 0.297144  [   84/   89]
Per-example loss in batch: 0.314752  [   86/   89]
Per-example loss in batch: 0.232481  [   88/   89]
Per-example loss in batch: 0.633706  [   89/   89]
Train Error: Avg loss: 0.29672031
validation Error: 
 Avg loss: 0.51059319 
 F1: 0.421861 
 Precision: 0.482854 
 Recall: 0.374548
 IoU: 0.267315

test Error: 
 Avg loss: 0.49087388 
 F1: 0.455361 
 Precision: 0.524101 
 Recall: 0.402562
 IoU: 0.294801

We have finished training iteration 196
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_194_.pth
Per-example loss in batch: 0.252817  [    2/   89]
Per-example loss in batch: 0.302699  [    4/   89]
Per-example loss in batch: 0.336049  [    6/   89]
Per-example loss in batch: 0.347927  [    8/   89]
Per-example loss in batch: 0.313766  [   10/   89]
Per-example loss in batch: 0.260251  [   12/   89]
Per-example loss in batch: 0.267969  [   14/   89]
Per-example loss in batch: 0.350587  [   16/   89]
Per-example loss in batch: 0.261340  [   18/   89]
Per-example loss in batch: 0.306512  [   20/   89]
Per-example loss in batch: 0.284873  [   22/   89]
Per-example loss in batch: 0.264153  [   24/   89]
Per-example loss in batch: 0.305912  [   26/   89]
Per-example loss in batch: 0.331387  [   28/   89]
Per-example loss in batch: 0.342049  [   30/   89]
Per-example loss in batch: 0.329964  [   32/   89]
Per-example loss in batch: 0.287637  [   34/   89]
Per-example loss in batch: 0.328667  [   36/   89]
Per-example loss in batch: 0.287610  [   38/   89]
Per-example loss in batch: 0.287649  [   40/   89]
Per-example loss in batch: 0.298340  [   42/   89]
Per-example loss in batch: 0.272877  [   44/   89]
Per-example loss in batch: 0.289279  [   46/   89]
Per-example loss in batch: 0.247846  [   48/   89]
Per-example loss in batch: 0.292078  [   50/   89]
Per-example loss in batch: 0.291164  [   52/   89]
Per-example loss in batch: 0.276910  [   54/   89]
Per-example loss in batch: 0.359126  [   56/   89]
Per-example loss in batch: 0.266093  [   58/   89]
Per-example loss in batch: 0.222730  [   60/   89]
Per-example loss in batch: 0.243898  [   62/   89]
Per-example loss in batch: 0.349300  [   64/   89]
Per-example loss in batch: 0.349883  [   66/   89]
Per-example loss in batch: 0.306383  [   68/   89]
Per-example loss in batch: 0.285969  [   70/   89]
Per-example loss in batch: 0.237627  [   72/   89]
Per-example loss in batch: 0.316980  [   74/   89]
Per-example loss in batch: 0.357021  [   76/   89]
Per-example loss in batch: 0.289327  [   78/   89]
Per-example loss in batch: 0.330549  [   80/   89]
Per-example loss in batch: 0.300389  [   82/   89]
Per-example loss in batch: 0.249377  [   84/   89]
Per-example loss in batch: 0.229495  [   86/   89]
Per-example loss in batch: 0.228912  [   88/   89]
Per-example loss in batch: 0.539034  [   89/   89]
Train Error: Avg loss: 0.29687378
validation Error: 
 Avg loss: 0.50981906 
 F1: 0.420652 
 Precision: 0.511665 
 Recall: 0.357128
 IoU: 0.266345

test Error: 
 Avg loss: 0.49044798 
 F1: 0.451743 
 Precision: 0.553333 
 Recall: 0.381669
 IoU: 0.291775

We have finished training iteration 197
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_195_.pth
Per-example loss in batch: 0.297370  [    2/   89]
Per-example loss in batch: 0.372989  [    4/   89]
Per-example loss in batch: 0.367637  [    6/   89]
Per-example loss in batch: 0.282539  [    8/   89]
Per-example loss in batch: 0.368616  [   10/   89]
Per-example loss in batch: 0.354651  [   12/   89]
Per-example loss in batch: 0.305371  [   14/   89]
Per-example loss in batch: 0.246780  [   16/   89]
Per-example loss in batch: 0.264415  [   18/   89]
Per-example loss in batch: 0.306448  [   20/   89]
Per-example loss in batch: 0.358476  [   22/   89]
Per-example loss in batch: 0.262415  [   24/   89]
Per-example loss in batch: 0.238719  [   26/   89]
Per-example loss in batch: 0.357375  [   28/   89]
Per-example loss in batch: 0.284404  [   30/   89]
Per-example loss in batch: 0.290148  [   32/   89]
Per-example loss in batch: 0.360819  [   34/   89]
Per-example loss in batch: 0.268731  [   36/   89]
Per-example loss in batch: 0.281600  [   38/   89]
Per-example loss in batch: 0.282959  [   40/   89]
Per-example loss in batch: 0.329703  [   42/   89]
Per-example loss in batch: 0.317760  [   44/   89]
Per-example loss in batch: 0.257537  [   46/   89]
Per-example loss in batch: 0.292393  [   48/   89]
Per-example loss in batch: 0.258308  [   50/   89]
Per-example loss in batch: 0.331602  [   52/   89]
Per-example loss in batch: 0.252143  [   54/   89]
Per-example loss in batch: 0.352684  [   56/   89]
Per-example loss in batch: 0.281243  [   58/   89]
Per-example loss in batch: 0.230114  [   60/   89]
Per-example loss in batch: 0.324904  [   62/   89]
Per-example loss in batch: 0.249294  [   64/   89]
Per-example loss in batch: 0.331389  [   66/   89]
Per-example loss in batch: 0.266821  [   68/   89]
Per-example loss in batch: 0.291094  [   70/   89]
Per-example loss in batch: 0.250580  [   72/   89]
Per-example loss in batch: 0.281730  [   74/   89]
Per-example loss in batch: 0.322363  [   76/   89]
Per-example loss in batch: 0.314505  [   78/   89]
Per-example loss in batch: 0.299054  [   80/   89]
Per-example loss in batch: 0.351625  [   82/   89]
Per-example loss in batch: 0.276250  [   84/   89]
Per-example loss in batch: 0.246793  [   86/   89]
Per-example loss in batch: 0.281827  [   88/   89]
Per-example loss in batch: 0.741221  [   89/   89]
Train Error: Avg loss: 0.30370311
validation Error: 
 Avg loss: 0.51010394 
 F1: 0.420486 
 Precision: 0.525595 
 Recall: 0.350411
 IoU: 0.266213

test Error: 
 Avg loss: 0.49022219 
 F1: 0.452059 
 Precision: 0.566860 
 Recall: 0.375926
 IoU: 0.292039

We have finished training iteration 198
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_196_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.277388  [    2/   89]
Per-example loss in batch: 0.372119  [    4/   89]
Per-example loss in batch: 0.325160  [    6/   89]
Per-example loss in batch: 0.268193  [    8/   89]
Per-example loss in batch: 0.325266  [   10/   89]
Per-example loss in batch: 0.352813  [   12/   89]
Per-example loss in batch: 0.368995  [   14/   89]
Per-example loss in batch: 0.267339  [   16/   89]
Per-example loss in batch: 0.358303  [   18/   89]
Per-example loss in batch: 0.308908  [   20/   89]
Per-example loss in batch: 0.328009  [   22/   89]
Per-example loss in batch: 0.338606  [   24/   89]
Per-example loss in batch: 0.326985  [   26/   89]
Per-example loss in batch: 0.328243  [   28/   89]
Per-example loss in batch: 0.323542  [   30/   89]
Per-example loss in batch: 0.287340  [   32/   89]
Per-example loss in batch: 0.251489  [   34/   89]
Per-example loss in batch: 0.280389  [   36/   89]
Per-example loss in batch: 0.235055  [   38/   89]
Per-example loss in batch: 0.238676  [   40/   89]
Per-example loss in batch: 0.309494  [   42/   89]
Per-example loss in batch: 0.240752  [   44/   89]
Per-example loss in batch: 0.254355  [   46/   89]
Per-example loss in batch: 0.278660  [   48/   89]
Per-example loss in batch: 0.286225  [   50/   89]
Per-example loss in batch: 0.237494  [   52/   89]
Per-example loss in batch: 0.320602  [   54/   89]
Per-example loss in batch: 0.290633  [   56/   89]
Per-example loss in batch: 0.253802  [   58/   89]
Per-example loss in batch: 0.272908  [   60/   89]
Per-example loss in batch: 0.280413  [   62/   89]
Per-example loss in batch: 0.339764  [   64/   89]
Per-example loss in batch: 0.296196  [   66/   89]
Per-example loss in batch: 0.248428  [   68/   89]
Per-example loss in batch: 0.313455  [   70/   89]
Per-example loss in batch: 0.291478  [   72/   89]
Per-example loss in batch: 0.281177  [   74/   89]
Per-example loss in batch: 0.332789  [   76/   89]
Per-example loss in batch: 0.335194  [   78/   89]
Per-example loss in batch: 0.232911  [   80/   89]
Per-example loss in batch: 0.312948  [   82/   89]
Per-example loss in batch: 0.280776  [   84/   89]
Per-example loss in batch: 0.311204  [   86/   89]
Per-example loss in batch: 0.226078  [   88/   89]
Per-example loss in batch: 0.507806  [   89/   89]
Train Error: Avg loss: 0.29762819
validation Error: 
 Avg loss: 0.51040874 
 F1: 0.420738 
 Precision: 0.516640 
 Recall: 0.354866
 IoU: 0.266415

test Error: 
 Avg loss: 0.49054260 
 F1: 0.452457 
 Precision: 0.554872 
 Recall: 0.381958
 IoU: 0.292371

We have finished training iteration 199
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_197_.pth
Per-example loss in batch: 0.295374  [    2/   89]
Per-example loss in batch: 0.246813  [    4/   89]
Per-example loss in batch: 0.222365  [    6/   89]
Per-example loss in batch: 0.270208  [    8/   89]
Per-example loss in batch: 0.373766  [   10/   89]
Per-example loss in batch: 0.220614  [   12/   89]
Per-example loss in batch: 0.315872  [   14/   89]
Per-example loss in batch: 0.289247  [   16/   89]
Per-example loss in batch: 0.311429  [   18/   89]
Per-example loss in batch: 0.341984  [   20/   89]
Per-example loss in batch: 0.316077  [   22/   89]
Per-example loss in batch: 0.321609  [   24/   89]
Per-example loss in batch: 0.338151  [   26/   89]
Per-example loss in batch: 0.263004  [   28/   89]
Per-example loss in batch: 0.253965  [   30/   89]
Per-example loss in batch: 0.241673  [   32/   89]
Per-example loss in batch: 0.252577  [   34/   89]
Per-example loss in batch: 0.257429  [   36/   89]
Per-example loss in batch: 0.342905  [   38/   89]
Per-example loss in batch: 0.244215  [   40/   89]
Per-example loss in batch: 0.292066  [   42/   89]
Per-example loss in batch: 0.329270  [   44/   89]
Per-example loss in batch: 0.281365  [   46/   89]
Per-example loss in batch: 0.272531  [   48/   89]
Per-example loss in batch: 0.310510  [   50/   89]
Per-example loss in batch: 0.290548  [   52/   89]
Per-example loss in batch: 0.298528  [   54/   89]
Per-example loss in batch: 0.352842  [   56/   89]
Per-example loss in batch: 0.254791  [   58/   89]
Per-example loss in batch: 0.262850  [   60/   89]
Per-example loss in batch: 0.232805  [   62/   89]
Per-example loss in batch: 0.262830  [   64/   89]
Per-example loss in batch: 0.289255  [   66/   89]
Per-example loss in batch: 0.300681  [   68/   89]
Per-example loss in batch: 0.301594  [   70/   89]
Per-example loss in batch: 0.357898  [   72/   89]
Per-example loss in batch: 0.335940  [   74/   89]
Per-example loss in batch: 0.299648  [   76/   89]
Per-example loss in batch: 0.249552  [   78/   89]
Per-example loss in batch: 0.255855  [   80/   89]
Per-example loss in batch: 0.322293  [   82/   89]
Per-example loss in batch: 0.284738  [   84/   89]
Per-example loss in batch: 0.372582  [   86/   89]
Per-example loss in batch: 0.315991  [   88/   89]
Per-example loss in batch: 0.526736  [   89/   89]
Train Error: Avg loss: 0.29459787
validation Error: 
 Avg loss: 0.51030672 
 F1: 0.422591 
 Precision: 0.499584 
 Recall: 0.366160
 IoU: 0.267902

test Error: 
 Avg loss: 0.49063139 
 F1: 0.455959 
 Precision: 0.539631 
 Recall: 0.394751
 IoU: 0.295302

We have finished training iteration 200
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_198_.pth
Per-example loss in batch: 0.304615  [    2/   89]
Per-example loss in batch: 0.349117  [    4/   89]
Per-example loss in batch: 0.253659  [    6/   89]
Per-example loss in batch: 0.341208  [    8/   89]
Per-example loss in batch: 0.222237  [   10/   89]
Per-example loss in batch: 0.226873  [   12/   89]
Per-example loss in batch: 0.284099  [   14/   89]
Per-example loss in batch: 0.273894  [   16/   89]
Per-example loss in batch: 0.326367  [   18/   89]
Per-example loss in batch: 0.260301  [   20/   89]
Per-example loss in batch: 0.346491  [   22/   89]
Per-example loss in batch: 0.265266  [   24/   89]
Per-example loss in batch: 0.258737  [   26/   89]
Per-example loss in batch: 0.344227  [   28/   89]
Per-example loss in batch: 0.351570  [   30/   89]
Per-example loss in batch: 0.255991  [   32/   89]
Per-example loss in batch: 0.336516  [   34/   89]
Per-example loss in batch: 0.301503  [   36/   89]
Per-example loss in batch: 0.339472  [   38/   89]
Per-example loss in batch: 0.246383  [   40/   89]
Per-example loss in batch: 0.291944  [   42/   89]
Per-example loss in batch: 0.228634  [   44/   89]
Per-example loss in batch: 0.231340  [   46/   89]
Per-example loss in batch: 0.262877  [   48/   89]
Per-example loss in batch: 0.336719  [   50/   89]
Per-example loss in batch: 0.317734  [   52/   89]
Per-example loss in batch: 0.283085  [   54/   89]
Per-example loss in batch: 0.392897  [   56/   89]
Per-example loss in batch: 0.297081  [   58/   89]
Per-example loss in batch: 0.292907  [   60/   89]
Per-example loss in batch: 0.249633  [   62/   89]
Per-example loss in batch: 0.262821  [   64/   89]
Per-example loss in batch: 0.232611  [   66/   89]
Per-example loss in batch: 0.269666  [   68/   89]
Per-example loss in batch: 0.316016  [   70/   89]
Per-example loss in batch: 0.315861  [   72/   89]
Per-example loss in batch: 0.273019  [   74/   89]
Per-example loss in batch: 0.283401  [   76/   89]
Per-example loss in batch: 0.329068  [   78/   89]
Per-example loss in batch: 0.267646  [   80/   89]
Per-example loss in batch: 0.289791  [   82/   89]
Per-example loss in batch: 0.361681  [   84/   89]
Per-example loss in batch: 0.302641  [   86/   89]
Per-example loss in batch: 0.340104  [   88/   89]
Per-example loss in batch: 0.729832  [   89/   89]
Train Error: Avg loss: 0.29848579
validation Error: 
 Avg loss: 0.50991744 
 F1: 0.418629 
 Precision: 0.539570 
 Recall: 0.341977
 IoU: 0.264725

test Error: 
 Avg loss: 0.49005689 
 F1: 0.447973 
 Precision: 0.574332 
 Recall: 0.367188
 IoU: 0.288638

We have finished training iteration 201
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_199_.pth
Per-example loss in batch: 0.269267  [    2/   89]
Per-example loss in batch: 0.290665  [    4/   89]
Per-example loss in batch: 0.263445  [    6/   89]
Per-example loss in batch: 0.343777  [    8/   89]
Per-example loss in batch: 0.245969  [   10/   89]
Per-example loss in batch: 0.239705  [   12/   89]
Per-example loss in batch: 0.286709  [   14/   89]
Per-example loss in batch: 0.288991  [   16/   89]
Per-example loss in batch: 0.316519  [   18/   89]
Per-example loss in batch: 0.246905  [   20/   89]
Per-example loss in batch: 0.360668  [   22/   89]
Per-example loss in batch: 0.252817  [   24/   89]
Per-example loss in batch: 0.329562  [   26/   89]
Per-example loss in batch: 0.324458  [   28/   89]
Per-example loss in batch: 0.241239  [   30/   89]
Per-example loss in batch: 0.262224  [   32/   89]
Per-example loss in batch: 0.307756  [   34/   89]
Per-example loss in batch: 0.316789  [   36/   89]
Per-example loss in batch: 0.232127  [   38/   89]
Per-example loss in batch: 0.309277  [   40/   89]
Per-example loss in batch: 0.312825  [   42/   89]
Per-example loss in batch: 0.285500  [   44/   89]
Per-example loss in batch: 0.240848  [   46/   89]
Per-example loss in batch: 0.267737  [   48/   89]
Per-example loss in batch: 0.336867  [   50/   89]
Per-example loss in batch: 0.309494  [   52/   89]
Per-example loss in batch: 0.256366  [   54/   89]
Per-example loss in batch: 0.280078  [   56/   89]
Per-example loss in batch: 0.248101  [   58/   89]
Per-example loss in batch: 0.308794  [   60/   89]
Per-example loss in batch: 0.341666  [   62/   89]
Per-example loss in batch: 0.311636  [   64/   89]
Per-example loss in batch: 0.322853  [   66/   89]
Per-example loss in batch: 0.278802  [   68/   89]
Per-example loss in batch: 0.371877  [   70/   89]
Per-example loss in batch: 0.350415  [   72/   89]
Per-example loss in batch: 0.228871  [   74/   89]
Per-example loss in batch: 0.234620  [   76/   89]
Per-example loss in batch: 0.360903  [   78/   89]
Per-example loss in batch: 0.268114  [   80/   89]
Per-example loss in batch: 0.249805  [   82/   89]
Per-example loss in batch: 0.330948  [   84/   89]
Per-example loss in batch: 0.264115  [   86/   89]
Per-example loss in batch: 0.318891  [   88/   89]
Per-example loss in batch: 0.644566  [   89/   89]
Train Error: Avg loss: 0.29508491
validation Error: 
 Avg loss: 0.51078731 
 F1: 0.420984 
 Precision: 0.471274 
 Recall: 0.380392
 IoU: 0.266612

test Error: 
 Avg loss: 0.49113663 
 F1: 0.455302 
 Precision: 0.511273 
 Recall: 0.410376
 IoU: 0.294751

We have finished training iteration 202
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_200_.pth
Per-example loss in batch: 0.346962  [    2/   89]
Per-example loss in batch: 0.227322  [    4/   89]
Per-example loss in batch: 0.294358  [    6/   89]
Per-example loss in batch: 0.258704  [    8/   89]
Per-example loss in batch: 0.227511  [   10/   89]
Per-example loss in batch: 0.253306  [   12/   89]
Per-example loss in batch: 0.230047  [   14/   89]
Per-example loss in batch: 0.279785  [   16/   89]
Per-example loss in batch: 0.285082  [   18/   89]
Per-example loss in batch: 0.243652  [   20/   89]
Per-example loss in batch: 0.242455  [   22/   89]
Per-example loss in batch: 0.315877  [   24/   89]
Per-example loss in batch: 0.342880  [   26/   89]
Per-example loss in batch: 0.281788  [   28/   89]
Per-example loss in batch: 0.232659  [   30/   89]
Per-example loss in batch: 0.322399  [   32/   89]
Per-example loss in batch: 0.338419  [   34/   89]
Per-example loss in batch: 0.310166  [   36/   89]
Per-example loss in batch: 0.259342  [   38/   89]
Per-example loss in batch: 0.316878  [   40/   89]
Per-example loss in batch: 0.263681  [   42/   89]
Per-example loss in batch: 0.327659  [   44/   89]
Per-example loss in batch: 0.339721  [   46/   89]
Per-example loss in batch: 0.231887  [   48/   89]
Per-example loss in batch: 0.349366  [   50/   89]
Per-example loss in batch: 0.319182  [   52/   89]
Per-example loss in batch: 0.296114  [   54/   89]
Per-example loss in batch: 0.237516  [   56/   89]
Per-example loss in batch: 0.333492  [   58/   89]
Per-example loss in batch: 0.270078  [   60/   89]
Per-example loss in batch: 0.339286  [   62/   89]
Per-example loss in batch: 0.237218  [   64/   89]
Per-example loss in batch: 0.313153  [   66/   89]
Per-example loss in batch: 0.382553  [   68/   89]
Per-example loss in batch: 0.254379  [   70/   89]
Per-example loss in batch: 0.264645  [   72/   89]
Per-example loss in batch: 0.310452  [   74/   89]
Per-example loss in batch: 0.336202  [   76/   89]
Per-example loss in batch: 0.370426  [   78/   89]
Per-example loss in batch: 0.296616  [   80/   89]
Per-example loss in batch: 0.286450  [   82/   89]
Per-example loss in batch: 0.238104  [   84/   89]
Per-example loss in batch: 0.317282  [   86/   89]
Per-example loss in batch: 0.301309  [   88/   89]
Per-example loss in batch: 0.443834  [   89/   89]
Train Error: Avg loss: 0.29321985
validation Error: 
 Avg loss: 0.51073913 
 F1: 0.420641 
 Precision: 0.484485 
 Recall: 0.371664
 IoU: 0.266337

test Error: 
 Avg loss: 0.49104430 
 F1: 0.454329 
 Precision: 0.523734 
 Recall: 0.401167
 IoU: 0.293937

We have finished training iteration 203
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_201_.pth
Per-example loss in batch: 0.236413  [    2/   89]
Per-example loss in batch: 0.256603  [    4/   89]
Per-example loss in batch: 0.283711  [    6/   89]
Per-example loss in batch: 0.321115  [    8/   89]
Per-example loss in batch: 0.271464  [   10/   89]
Per-example loss in batch: 0.344967  [   12/   89]
Per-example loss in batch: 0.299224  [   14/   89]
Per-example loss in batch: 0.316113  [   16/   89]
Per-example loss in batch: 0.280231  [   18/   89]
Per-example loss in batch: 0.276712  [   20/   89]
Per-example loss in batch: 0.296841  [   22/   89]
Per-example loss in batch: 0.322326  [   24/   89]
Per-example loss in batch: 0.215646  [   26/   89]
Per-example loss in batch: 0.307386  [   28/   89]
Per-example loss in batch: 0.284582  [   30/   89]
Per-example loss in batch: 0.290297  [   32/   89]
Per-example loss in batch: 0.314541  [   34/   89]
Per-example loss in batch: 0.286967  [   36/   89]
Per-example loss in batch: 0.363965  [   38/   89]
Per-example loss in batch: 0.277643  [   40/   89]
Per-example loss in batch: 0.228164  [   42/   89]
Per-example loss in batch: 0.343165  [   44/   89]
Per-example loss in batch: 0.252685  [   46/   89]
Per-example loss in batch: 0.352773  [   48/   89]
Per-example loss in batch: 0.250352  [   50/   89]
Per-example loss in batch: 0.321021  [   52/   89]
Per-example loss in batch: 0.242318  [   54/   89]
Per-example loss in batch: 0.312409  [   56/   89]
Per-example loss in batch: 0.357604  [   58/   89]
Per-example loss in batch: 0.254682  [   60/   89]
Per-example loss in batch: 0.340024  [   62/   89]
Per-example loss in batch: 0.328931  [   64/   89]
Per-example loss in batch: 0.239762  [   66/   89]
Per-example loss in batch: 0.344376  [   68/   89]
Per-example loss in batch: 0.240758  [   70/   89]
Per-example loss in batch: 0.274037  [   72/   89]
Per-example loss in batch: 0.353267  [   74/   89]
Per-example loss in batch: 0.252114  [   76/   89]
Per-example loss in batch: 0.322984  [   78/   89]
Per-example loss in batch: 0.297214  [   80/   89]
Per-example loss in batch: 0.304894  [   82/   89]
Per-example loss in batch: 0.308743  [   84/   89]
Per-example loss in batch: 0.329138  [   86/   89]
Per-example loss in batch: 0.292559  [   88/   89]
Per-example loss in batch: 0.574897  [   89/   89]
Train Error: Avg loss: 0.29838588
validation Error: 
 Avg loss: 0.51079234 
 F1: 0.418727 
 Precision: 0.447693 
 Recall: 0.393281
 IoU: 0.264804

test Error: 
 Avg loss: 0.49155391 
 F1: 0.453701 
 Precision: 0.487336 
 Recall: 0.424409
 IoU: 0.293411

We have finished training iteration 204
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_202_.pth
Per-example loss in batch: 0.269855  [    2/   89]
Per-example loss in batch: 0.263326  [    4/   89]
Per-example loss in batch: 0.259763  [    6/   89]
Per-example loss in batch: 0.326576  [    8/   89]
Per-example loss in batch: 0.258591  [   10/   89]
Per-example loss in batch: 0.301129  [   12/   89]
Per-example loss in batch: 0.255119  [   14/   89]
Per-example loss in batch: 0.321373  [   16/   89]
Per-example loss in batch: 0.249385  [   18/   89]
Per-example loss in batch: 0.309081  [   20/   89]
Per-example loss in batch: 0.279756  [   22/   89]
Per-example loss in batch: 0.256499  [   24/   89]
Per-example loss in batch: 0.347874  [   26/   89]
Per-example loss in batch: 0.242159  [   28/   89]
Per-example loss in batch: 0.319759  [   30/   89]
Per-example loss in batch: 0.434624  [   32/   89]
Per-example loss in batch: 0.361441  [   34/   89]
Per-example loss in batch: 0.272439  [   36/   89]
Per-example loss in batch: 0.314722  [   38/   89]
Per-example loss in batch: 0.249481  [   40/   89]
Per-example loss in batch: 0.220175  [   42/   89]
Per-example loss in batch: 0.310882  [   44/   89]
Per-example loss in batch: 0.350495  [   46/   89]
Per-example loss in batch: 0.289267  [   48/   89]
Per-example loss in batch: 0.346910  [   50/   89]
Per-example loss in batch: 0.267753  [   52/   89]
Per-example loss in batch: 0.264943  [   54/   89]
Per-example loss in batch: 0.249534  [   56/   89]
Per-example loss in batch: 0.332655  [   58/   89]
Per-example loss in batch: 0.234139  [   60/   89]
Per-example loss in batch: 0.267417  [   62/   89]
Per-example loss in batch: 0.297574  [   64/   89]
Per-example loss in batch: 0.264293  [   66/   89]
Per-example loss in batch: 0.330565  [   68/   89]
Per-example loss in batch: 0.245197  [   70/   89]
Per-example loss in batch: 0.276049  [   72/   89]
Per-example loss in batch: 0.265586  [   74/   89]
Per-example loss in batch: 0.330235  [   76/   89]
Per-example loss in batch: 0.308291  [   78/   89]
Per-example loss in batch: 0.296936  [   80/   89]
Per-example loss in batch: 0.265079  [   82/   89]
Per-example loss in batch: 0.313469  [   84/   89]
Per-example loss in batch: 0.295693  [   86/   89]
Per-example loss in batch: 0.243907  [   88/   89]
Per-example loss in batch: 0.610242  [   89/   89]
Train Error: Avg loss: 0.29359810
validation Error: 
 Avg loss: 0.51094609 
 F1: 0.421335 
 Precision: 0.474044 
 Recall: 0.379175
 IoU: 0.266893

test Error: 
 Avg loss: 0.49127532 
 F1: 0.456507 
 Precision: 0.516126 
 Recall: 0.409235
 IoU: 0.295762

We have finished training iteration 205
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_203_.pth
Per-example loss in batch: 0.320658  [    2/   89]
Per-example loss in batch: 0.331177  [    4/   89]
Per-example loss in batch: 0.254854  [    6/   89]
Per-example loss in batch: 0.245717  [    8/   89]
Per-example loss in batch: 0.304930  [   10/   89]
Per-example loss in batch: 0.255344  [   12/   89]
Per-example loss in batch: 0.293289  [   14/   89]
Per-example loss in batch: 0.272511  [   16/   89]
Per-example loss in batch: 0.274094  [   18/   89]
Per-example loss in batch: 0.285310  [   20/   89]
Per-example loss in batch: 0.246677  [   22/   89]
Per-example loss in batch: 0.286421  [   24/   89]
Per-example loss in batch: 0.321053  [   26/   89]
Per-example loss in batch: 0.227662  [   28/   89]
Per-example loss in batch: 0.263404  [   30/   89]
Per-example loss in batch: 0.325104  [   32/   89]
Per-example loss in batch: 0.280551  [   34/   89]
Per-example loss in batch: 0.256373  [   36/   89]
Per-example loss in batch: 0.317441  [   38/   89]
Per-example loss in batch: 0.239408  [   40/   89]
Per-example loss in batch: 0.291818  [   42/   89]
Per-example loss in batch: 0.233877  [   44/   89]
Per-example loss in batch: 0.349507  [   46/   89]
Per-example loss in batch: 0.345928  [   48/   89]
Per-example loss in batch: 0.332267  [   50/   89]
Per-example loss in batch: 0.310150  [   52/   89]
Per-example loss in batch: 0.348591  [   54/   89]
Per-example loss in batch: 0.244236  [   56/   89]
Per-example loss in batch: 0.231104  [   58/   89]
Per-example loss in batch: 0.236823  [   60/   89]
Per-example loss in batch: 0.250967  [   62/   89]
Per-example loss in batch: 0.351536  [   64/   89]
Per-example loss in batch: 0.271975  [   66/   89]
Per-example loss in batch: 0.348830  [   68/   89]
Per-example loss in batch: 0.226326  [   70/   89]
Per-example loss in batch: 0.281756  [   72/   89]
Per-example loss in batch: 0.303095  [   74/   89]
Per-example loss in batch: 0.233891  [   76/   89]
Per-example loss in batch: 0.275304  [   78/   89]
Per-example loss in batch: 0.308175  [   80/   89]
Per-example loss in batch: 0.311831  [   82/   89]
Per-example loss in batch: 0.236623  [   84/   89]
Per-example loss in batch: 0.235273  [   86/   89]
Per-example loss in batch: 0.294099  [   88/   89]
Per-example loss in batch: 0.580863  [   89/   89]
Train Error: Avg loss: 0.28643575
validation Error: 
 Avg loss: 0.51070559 
 F1: 0.422785 
 Precision: 0.478044 
 Recall: 0.378978
 IoU: 0.268058

test Error: 
 Avg loss: 0.49105634 
 F1: 0.456527 
 Precision: 0.520144 
 Recall: 0.406776
 IoU: 0.295779

We have finished training iteration 206
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_204_.pth
Per-example loss in batch: 0.285263  [    2/   89]
Per-example loss in batch: 0.338787  [    4/   89]
Per-example loss in batch: 0.301609  [    6/   89]
Per-example loss in batch: 0.317374  [    8/   89]
Per-example loss in batch: 0.294637  [   10/   89]
Per-example loss in batch: 0.259253  [   12/   89]
Per-example loss in batch: 0.235407  [   14/   89]
Per-example loss in batch: 0.288778  [   16/   89]
Per-example loss in batch: 0.251853  [   18/   89]
Per-example loss in batch: 0.240082  [   20/   89]
Per-example loss in batch: 0.332213  [   22/   89]
Per-example loss in batch: 0.302502  [   24/   89]
Per-example loss in batch: 0.253534  [   26/   89]
Per-example loss in batch: 0.262875  [   28/   89]
Per-example loss in batch: 0.299577  [   30/   89]
Per-example loss in batch: 0.222228  [   32/   89]
Per-example loss in batch: 0.337259  [   34/   89]
Per-example loss in batch: 0.260093  [   36/   89]
Per-example loss in batch: 0.317866  [   38/   89]
Per-example loss in batch: 0.253241  [   40/   89]
Per-example loss in batch: 0.238644  [   42/   89]
Per-example loss in batch: 0.222940  [   44/   89]
Per-example loss in batch: 0.254817  [   46/   89]
Per-example loss in batch: 0.324420  [   48/   89]
Per-example loss in batch: 0.360916  [   50/   89]
Per-example loss in batch: 0.319720  [   52/   89]
Per-example loss in batch: 0.339120  [   54/   89]
Per-example loss in batch: 0.342032  [   56/   89]
Per-example loss in batch: 0.351789  [   58/   89]
Per-example loss in batch: 0.281220  [   60/   89]
Per-example loss in batch: 0.331684  [   62/   89]
Per-example loss in batch: 0.326316  [   64/   89]
Per-example loss in batch: 0.284998  [   66/   89]
Per-example loss in batch: 0.297409  [   68/   89]
Per-example loss in batch: 0.234230  [   70/   89]
Per-example loss in batch: 0.260390  [   72/   89]
Per-example loss in batch: 0.317769  [   74/   89]
Per-example loss in batch: 0.294509  [   76/   89]
Per-example loss in batch: 0.328286  [   78/   89]
Per-example loss in batch: 0.308129  [   80/   89]
Per-example loss in batch: 0.226461  [   82/   89]
Per-example loss in batch: 0.262515  [   84/   89]
Per-example loss in batch: 0.283691  [   86/   89]
Per-example loss in batch: 0.297713  [   88/   89]
Per-example loss in batch: 0.457754  [   89/   89]
Train Error: Avg loss: 0.29152874
validation Error: 
 Avg loss: 0.51106145 
 F1: 0.418967 
 Precision: 0.458953 
 Recall: 0.385391
 IoU: 0.264996

test Error: 
 Avg loss: 0.49128926 
 F1: 0.454845 
 Precision: 0.499338 
 Recall: 0.417633
 IoU: 0.294369

We have finished training iteration 207
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_205_.pth
Per-example loss in batch: 0.294713  [    2/   89]
Per-example loss in batch: 0.341035  [    4/   89]
Per-example loss in batch: 0.235215  [    6/   89]
Per-example loss in batch: 0.235564  [    8/   89]
Per-example loss in batch: 0.251391  [   10/   89]
Per-example loss in batch: 0.280733  [   12/   89]
Per-example loss in batch: 0.250910  [   14/   89]
Per-example loss in batch: 0.293558  [   16/   89]
Per-example loss in batch: 0.228523  [   18/   89]
Per-example loss in batch: 0.245706  [   20/   89]
Per-example loss in batch: 0.364711  [   22/   89]
Per-example loss in batch: 0.262866  [   24/   89]
Per-example loss in batch: 0.371060  [   26/   89]
Per-example loss in batch: 0.301861  [   28/   89]
Per-example loss in batch: 0.358395  [   30/   89]
Per-example loss in batch: 0.331508  [   32/   89]
Per-example loss in batch: 0.326670  [   34/   89]
Per-example loss in batch: 0.324473  [   36/   89]
Per-example loss in batch: 0.282493  [   38/   89]
Per-example loss in batch: 0.291068  [   40/   89]
Per-example loss in batch: 0.353390  [   42/   89]
Per-example loss in batch: 0.253973  [   44/   89]
Per-example loss in batch: 0.251324  [   46/   89]
Per-example loss in batch: 0.251807  [   48/   89]
Per-example loss in batch: 0.315386  [   50/   89]
Per-example loss in batch: 0.316299  [   52/   89]
Per-example loss in batch: 0.301745  [   54/   89]
Per-example loss in batch: 0.261976  [   56/   89]
Per-example loss in batch: 0.253018  [   58/   89]
Per-example loss in batch: 0.278850  [   60/   89]
Per-example loss in batch: 0.283371  [   62/   89]
Per-example loss in batch: 0.275988  [   64/   89]
Per-example loss in batch: 0.264420  [   66/   89]
Per-example loss in batch: 0.305957  [   68/   89]
Per-example loss in batch: 0.225425  [   70/   89]
Per-example loss in batch: 0.237996  [   72/   89]
Per-example loss in batch: 0.320306  [   74/   89]
Per-example loss in batch: 0.365811  [   76/   89]
Per-example loss in batch: 0.319703  [   78/   89]
Per-example loss in batch: 0.347288  [   80/   89]
Per-example loss in batch: 0.368730  [   82/   89]
Per-example loss in batch: 0.367840  [   84/   89]
Per-example loss in batch: 0.311443  [   86/   89]
Per-example loss in batch: 0.266623  [   88/   89]
Per-example loss in batch: 0.643567  [   89/   89]
Train Error: Avg loss: 0.29871700
validation Error: 
 Avg loss: 0.51040073 
 F1: 0.422844 
 Precision: 0.488310 
 Recall: 0.372857
 IoU: 0.268106

test Error: 
 Avg loss: 0.49086915 
 F1: 0.454924 
 Precision: 0.527319 
 Recall: 0.400008
 IoU: 0.294435

We have finished training iteration 208
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_206_.pth
Per-example loss in batch: 0.325610  [    2/   89]
Per-example loss in batch: 0.347519  [    4/   89]
Per-example loss in batch: 0.246866  [    6/   89]
Per-example loss in batch: 0.240753  [    8/   89]
Per-example loss in batch: 0.368114  [   10/   89]
Per-example loss in batch: 0.296322  [   12/   89]
Per-example loss in batch: 0.225368  [   14/   89]
Per-example loss in batch: 0.297032  [   16/   89]
Per-example loss in batch: 0.304379  [   18/   89]
Per-example loss in batch: 0.294925  [   20/   89]
Per-example loss in batch: 0.268297  [   22/   89]
Per-example loss in batch: 0.285292  [   24/   89]
Per-example loss in batch: 0.317206  [   26/   89]
Per-example loss in batch: 0.250416  [   28/   89]
Per-example loss in batch: 0.292718  [   30/   89]
Per-example loss in batch: 0.307176  [   32/   89]
Per-example loss in batch: 0.297162  [   34/   89]
Per-example loss in batch: 0.247451  [   36/   89]
Per-example loss in batch: 0.277516  [   38/   89]
Per-example loss in batch: 0.392728  [   40/   89]
Per-example loss in batch: 0.263491  [   42/   89]
Per-example loss in batch: 0.306994  [   44/   89]
Per-example loss in batch: 0.340199  [   46/   89]
Per-example loss in batch: 0.269270  [   48/   89]
Per-example loss in batch: 0.332457  [   50/   89]
Per-example loss in batch: 0.237419  [   52/   89]
Per-example loss in batch: 0.252859  [   54/   89]
Per-example loss in batch: 0.342507  [   56/   89]
Per-example loss in batch: 0.267760  [   58/   89]
Per-example loss in batch: 0.277258  [   60/   89]
Per-example loss in batch: 0.265996  [   62/   89]
Per-example loss in batch: 0.317985  [   64/   89]
Per-example loss in batch: 0.273408  [   66/   89]
Per-example loss in batch: 0.327034  [   68/   89]
Per-example loss in batch: 0.304387  [   70/   89]
Per-example loss in batch: 0.219960  [   72/   89]
Per-example loss in batch: 0.385870  [   74/   89]
Per-example loss in batch: 0.232054  [   76/   89]
Per-example loss in batch: 0.259162  [   78/   89]
Per-example loss in batch: 0.279490  [   80/   89]
Per-example loss in batch: 0.281578  [   82/   89]
Per-example loss in batch: 0.315929  [   84/   89]
Per-example loss in batch: 0.344460  [   86/   89]
Per-example loss in batch: 0.300343  [   88/   89]
Per-example loss in batch: 0.490965  [   89/   89]
Train Error: Avg loss: 0.29497086
validation Error: 
 Avg loss: 0.51050061 
 F1: 0.421068 
 Precision: 0.507388 
 Recall: 0.359848
 IoU: 0.266679

test Error: 
 Avg loss: 0.49065713 
 F1: 0.452502 
 Precision: 0.546709 
 Recall: 0.385989
 IoU: 0.292409

We have finished training iteration 209
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_207_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.321975  [    2/   89]
Per-example loss in batch: 0.254579  [    4/   89]
Per-example loss in batch: 0.304318  [    6/   89]
Per-example loss in batch: 0.281545  [    8/   89]
Per-example loss in batch: 0.365073  [   10/   89]
Per-example loss in batch: 0.237248  [   12/   89]
Per-example loss in batch: 0.236510  [   14/   89]
Per-example loss in batch: 0.308508  [   16/   89]
Per-example loss in batch: 0.325615  [   18/   89]
Per-example loss in batch: 0.284261  [   20/   89]
Per-example loss in batch: 0.249493  [   22/   89]
Per-example loss in batch: 0.364147  [   24/   89]
Per-example loss in batch: 0.280442  [   26/   89]
Per-example loss in batch: 0.278463  [   28/   89]
Per-example loss in batch: 0.233666  [   30/   89]
Per-example loss in batch: 0.346963  [   32/   89]
Per-example loss in batch: 0.234858  [   34/   89]
Per-example loss in batch: 0.322068  [   36/   89]
Per-example loss in batch: 0.268501  [   38/   89]
Per-example loss in batch: 0.261951  [   40/   89]
Per-example loss in batch: 0.339790  [   42/   89]
Per-example loss in batch: 0.313663  [   44/   89]
Per-example loss in batch: 0.241945  [   46/   89]
Per-example loss in batch: 0.262829  [   48/   89]
Per-example loss in batch: 0.303846  [   50/   89]
Per-example loss in batch: 0.291203  [   52/   89]
Per-example loss in batch: 0.321811  [   54/   89]
Per-example loss in batch: 0.228736  [   56/   89]
Per-example loss in batch: 0.245706  [   58/   89]
Per-example loss in batch: 0.247345  [   60/   89]
Per-example loss in batch: 0.308961  [   62/   89]
Per-example loss in batch: 0.272304  [   64/   89]
Per-example loss in batch: 0.343742  [   66/   89]
Per-example loss in batch: 0.323136  [   68/   89]
Per-example loss in batch: 0.246530  [   70/   89]
Per-example loss in batch: 0.317767  [   72/   89]
Per-example loss in batch: 0.289210  [   74/   89]
Per-example loss in batch: 0.336860  [   76/   89]
Per-example loss in batch: 0.297370  [   78/   89]
Per-example loss in batch: 0.275617  [   80/   89]
Per-example loss in batch: 0.274240  [   82/   89]
Per-example loss in batch: 0.361091  [   84/   89]
Per-example loss in batch: 0.260974  [   86/   89]
Per-example loss in batch: 0.356511  [   88/   89]
Per-example loss in batch: 0.448795  [   89/   89]
Train Error: Avg loss: 0.29316336
validation Error: 
 Avg loss: 0.51153307 
 F1: 0.416697 
 Precision: 0.432503 
 Recall: 0.402006
 IoU: 0.263182

test Error: 
 Avg loss: 0.49186106 
 F1: 0.453695 
 Precision: 0.474189 
 Recall: 0.434898
 IoU: 0.293406

We have finished training iteration 210
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_208_.pth
Per-example loss in batch: 0.247271  [    2/   89]
Per-example loss in batch: 0.310731  [    4/   89]
Per-example loss in batch: 0.238937  [    6/   89]
Per-example loss in batch: 0.379053  [    8/   89]
Per-example loss in batch: 0.286333  [   10/   89]
Per-example loss in batch: 0.338941  [   12/   89]
Per-example loss in batch: 0.243232  [   14/   89]
Per-example loss in batch: 0.283002  [   16/   89]
Per-example loss in batch: 0.218714  [   18/   89]
Per-example loss in batch: 0.293193  [   20/   89]
Per-example loss in batch: 0.241598  [   22/   89]
Per-example loss in batch: 0.307374  [   24/   89]
Per-example loss in batch: 0.256230  [   26/   89]
Per-example loss in batch: 0.290330  [   28/   89]
Per-example loss in batch: 0.249539  [   30/   89]
Per-example loss in batch: 0.246567  [   32/   89]
Per-example loss in batch: 0.261005  [   34/   89]
Per-example loss in batch: 0.257985  [   36/   89]
Per-example loss in batch: 0.328731  [   38/   89]
Per-example loss in batch: 0.374317  [   40/   89]
Per-example loss in batch: 0.311812  [   42/   89]
Per-example loss in batch: 0.308037  [   44/   89]
Per-example loss in batch: 0.283017  [   46/   89]
Per-example loss in batch: 0.269530  [   48/   89]
Per-example loss in batch: 0.292205  [   50/   89]
Per-example loss in batch: 0.330821  [   52/   89]
Per-example loss in batch: 0.259410  [   54/   89]
Per-example loss in batch: 0.236168  [   56/   89]
Per-example loss in batch: 0.327472  [   58/   89]
Per-example loss in batch: 0.368011  [   60/   89]
Per-example loss in batch: 0.243176  [   62/   89]
Per-example loss in batch: 0.295095  [   64/   89]
Per-example loss in batch: 0.285647  [   66/   89]
Per-example loss in batch: 0.271081  [   68/   89]
Per-example loss in batch: 0.254852  [   70/   89]
Per-example loss in batch: 0.317402  [   72/   89]
Per-example loss in batch: 0.219679  [   74/   89]
Per-example loss in batch: 0.285972  [   76/   89]
Per-example loss in batch: 0.247272  [   78/   89]
Per-example loss in batch: 0.357003  [   80/   89]
Per-example loss in batch: 0.267003  [   82/   89]
Per-example loss in batch: 0.323925  [   84/   89]
Per-example loss in batch: 0.371927  [   86/   89]
Per-example loss in batch: 0.289361  [   88/   89]
Per-example loss in batch: 0.535941  [   89/   89]
Train Error: Avg loss: 0.29071761
validation Error: 
 Avg loss: 0.51082394 
 F1: 0.422406 
 Precision: 0.486620 
 Recall: 0.373163
 IoU: 0.267753

test Error: 
 Avg loss: 0.49108144 
 F1: 0.455853 
 Precision: 0.527992 
 Recall: 0.401057
 IoU: 0.295214

We have finished training iteration 211
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_209_.pth
Per-example loss in batch: 0.260606  [    2/   89]
Per-example loss in batch: 0.275804  [    4/   89]
Per-example loss in batch: 0.272436  [    6/   89]
Per-example loss in batch: 0.244539  [    8/   89]
Per-example loss in batch: 0.283360  [   10/   89]
Per-example loss in batch: 0.324421  [   12/   89]
Per-example loss in batch: 0.336726  [   14/   89]
Per-example loss in batch: 0.257844  [   16/   89]
Per-example loss in batch: 0.301328  [   18/   89]
Per-example loss in batch: 0.243944  [   20/   89]
Per-example loss in batch: 0.333537  [   22/   89]
Per-example loss in batch: 0.259870  [   24/   89]
Per-example loss in batch: 0.232069  [   26/   89]
Per-example loss in batch: 0.297100  [   28/   89]
Per-example loss in batch: 0.327373  [   30/   89]
Per-example loss in batch: 0.289828  [   32/   89]
Per-example loss in batch: 0.265624  [   34/   89]
Per-example loss in batch: 0.306223  [   36/   89]
Per-example loss in batch: 0.349700  [   38/   89]
Per-example loss in batch: 0.267284  [   40/   89]
Per-example loss in batch: 0.333145  [   42/   89]
Per-example loss in batch: 0.362555  [   44/   89]
Per-example loss in batch: 0.297225  [   46/   89]
Per-example loss in batch: 0.226088  [   48/   89]
Per-example loss in batch: 0.286559  [   50/   89]
Per-example loss in batch: 0.306989  [   52/   89]
Per-example loss in batch: 0.235171  [   54/   89]
Per-example loss in batch: 0.286763  [   56/   89]
Per-example loss in batch: 0.251362  [   58/   89]
Per-example loss in batch: 0.336434  [   60/   89]
Per-example loss in batch: 0.331750  [   62/   89]
Per-example loss in batch: 0.294235  [   64/   89]
Per-example loss in batch: 0.263099  [   66/   89]
Per-example loss in batch: 0.260305  [   68/   89]
Per-example loss in batch: 0.282126  [   70/   89]
Per-example loss in batch: 0.213832  [   72/   89]
Per-example loss in batch: 0.310604  [   74/   89]
Per-example loss in batch: 0.375321  [   76/   89]
Per-example loss in batch: 0.264229  [   78/   89]
Per-example loss in batch: 0.364611  [   80/   89]
Per-example loss in batch: 0.336334  [   82/   89]
Per-example loss in batch: 0.268580  [   84/   89]
Per-example loss in batch: 0.279681  [   86/   89]
Per-example loss in batch: 0.228241  [   88/   89]
Per-example loss in batch: 0.485524  [   89/   89]
Train Error: Avg loss: 0.29140716
validation Error: 
 Avg loss: 0.51020099 
 F1: 0.419002 
 Precision: 0.530939 
 Recall: 0.346045
 IoU: 0.265024

test Error: 
 Avg loss: 0.49045149 
 F1: 0.447873 
 Precision: 0.565291 
 Recall: 0.370844
 IoU: 0.288554

We have finished training iteration 212
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_210_.pth
Per-example loss in batch: 0.303171  [    2/   89]
Per-example loss in batch: 0.225718  [    4/   89]
Per-example loss in batch: 0.287849  [    6/   89]
Per-example loss in batch: 0.227097  [    8/   89]
Per-example loss in batch: 0.249699  [   10/   89]
Per-example loss in batch: 0.321693  [   12/   89]
Per-example loss in batch: 0.243073  [   14/   89]
Per-example loss in batch: 0.309630  [   16/   89]
Per-example loss in batch: 0.231418  [   18/   89]
Per-example loss in batch: 0.312574  [   20/   89]
Per-example loss in batch: 0.309447  [   22/   89]
Per-example loss in batch: 0.233463  [   24/   89]
Per-example loss in batch: 0.245289  [   26/   89]
Per-example loss in batch: 0.346015  [   28/   89]
Per-example loss in batch: 0.298411  [   30/   89]
Per-example loss in batch: 0.306364  [   32/   89]
Per-example loss in batch: 0.252915  [   34/   89]
Per-example loss in batch: 0.327435  [   36/   89]
Per-example loss in batch: 0.314526  [   38/   89]
Per-example loss in batch: 0.299711  [   40/   89]
Per-example loss in batch: 0.251936  [   42/   89]
Per-example loss in batch: 0.266883  [   44/   89]
Per-example loss in batch: 0.286812  [   46/   89]
Per-example loss in batch: 0.273257  [   48/   89]
Per-example loss in batch: 0.319734  [   50/   89]
Per-example loss in batch: 0.336671  [   52/   89]
Per-example loss in batch: 0.328674  [   54/   89]
Per-example loss in batch: 0.276805  [   56/   89]
Per-example loss in batch: 0.261838  [   58/   89]
Per-example loss in batch: 0.249179  [   60/   89]
Per-example loss in batch: 0.291409  [   62/   89]
Per-example loss in batch: 0.318624  [   64/   89]
Per-example loss in batch: 0.259352  [   66/   89]
Per-example loss in batch: 0.357054  [   68/   89]
Per-example loss in batch: 0.321245  [   70/   89]
Per-example loss in batch: 0.249494  [   72/   89]
Per-example loss in batch: 0.351776  [   74/   89]
Per-example loss in batch: 0.219943  [   76/   89]
Per-example loss in batch: 0.315232  [   78/   89]
Per-example loss in batch: 0.245454  [   80/   89]
Per-example loss in batch: 0.269503  [   82/   89]
Per-example loss in batch: 0.238105  [   84/   89]
Per-example loss in batch: 0.292671  [   86/   89]
Per-example loss in batch: 0.347328  [   88/   89]
Per-example loss in batch: 0.660458  [   89/   89]
Train Error: Avg loss: 0.28999343
validation Error: 
 Avg loss: 0.51044939 
 F1: 0.418694 
 Precision: 0.465147 
 Recall: 0.380676
 IoU: 0.264777

test Error: 
 Avg loss: 0.49126559 
 F1: 0.452793 
 Precision: 0.504226 
 Recall: 0.410882
 IoU: 0.292652

We have finished training iteration 213
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_211_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.307838  [    2/   89]
Per-example loss in batch: 0.338540  [    4/   89]
Per-example loss in batch: 0.257727  [    6/   89]
Per-example loss in batch: 0.372977  [    8/   89]
Per-example loss in batch: 0.291285  [   10/   89]
Per-example loss in batch: 0.320955  [   12/   89]
Per-example loss in batch: 0.259597  [   14/   89]
Per-example loss in batch: 0.284636  [   16/   89]
Per-example loss in batch: 0.257968  [   18/   89]
Per-example loss in batch: 0.299151  [   20/   89]
Per-example loss in batch: 0.238211  [   22/   89]
Per-example loss in batch: 0.271307  [   24/   89]
Per-example loss in batch: 0.293634  [   26/   89]
Per-example loss in batch: 0.275142  [   28/   89]
Per-example loss in batch: 0.347535  [   30/   89]
Per-example loss in batch: 0.318250  [   32/   89]
Per-example loss in batch: 0.280860  [   34/   89]
Per-example loss in batch: 0.320622  [   36/   89]
Per-example loss in batch: 0.292325  [   38/   89]
Per-example loss in batch: 0.294933  [   40/   89]
Per-example loss in batch: 0.335639  [   42/   89]
Per-example loss in batch: 0.278967  [   44/   89]
Per-example loss in batch: 0.297818  [   46/   89]
Per-example loss in batch: 0.315826  [   48/   89]
Per-example loss in batch: 0.319488  [   50/   89]
Per-example loss in batch: 0.267271  [   52/   89]
Per-example loss in batch: 0.287122  [   54/   89]
Per-example loss in batch: 0.241035  [   56/   89]
Per-example loss in batch: 0.310217  [   58/   89]
Per-example loss in batch: 0.229273  [   60/   89]
Per-example loss in batch: 0.330151  [   62/   89]
Per-example loss in batch: 0.303595  [   64/   89]
Per-example loss in batch: 0.238504  [   66/   89]
Per-example loss in batch: 0.359638  [   68/   89]
Per-example loss in batch: 0.246977  [   70/   89]
Per-example loss in batch: 0.270274  [   72/   89]
Per-example loss in batch: 0.281030  [   74/   89]
Per-example loss in batch: 0.229706  [   76/   89]
Per-example loss in batch: 0.330642  [   78/   89]
Per-example loss in batch: 0.220880  [   80/   89]
Per-example loss in batch: 0.290453  [   82/   89]
Per-example loss in batch: 0.340816  [   84/   89]
Per-example loss in batch: 0.363379  [   86/   89]
Per-example loss in batch: 0.370518  [   88/   89]
Per-example loss in batch: 0.656031  [   89/   89]
Train Error: Avg loss: 0.29911744
validation Error: 
 Avg loss: 0.51004707 
 F1: 0.421962 
 Precision: 0.523271 
 Recall: 0.353519
 IoU: 0.267397

test Error: 
 Avg loss: 0.49040760 
 F1: 0.453568 
 Precision: 0.562514 
 Recall: 0.379975
 IoU: 0.293300

We have finished training iteration 214
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_212_.pth
Per-example loss in batch: 0.271019  [    2/   89]
Per-example loss in batch: 0.314492  [    4/   89]
Per-example loss in batch: 0.286183  [    6/   89]
Per-example loss in batch: 0.283090  [    8/   89]
Per-example loss in batch: 0.274985  [   10/   89]
Per-example loss in batch: 0.305774  [   12/   89]
Per-example loss in batch: 0.325829  [   14/   89]
Per-example loss in batch: 0.268844  [   16/   89]
Per-example loss in batch: 0.240041  [   18/   89]
Per-example loss in batch: 0.299358  [   20/   89]
Per-example loss in batch: 0.368386  [   22/   89]
Per-example loss in batch: 0.279793  [   24/   89]
Per-example loss in batch: 0.266483  [   26/   89]
Per-example loss in batch: 0.296302  [   28/   89]
Per-example loss in batch: 0.257924  [   30/   89]
Per-example loss in batch: 0.302290  [   32/   89]
Per-example loss in batch: 0.291390  [   34/   89]
Per-example loss in batch: 0.273613  [   36/   89]
Per-example loss in batch: 0.254442  [   38/   89]
Per-example loss in batch: 0.305924  [   40/   89]
Per-example loss in batch: 0.359123  [   42/   89]
Per-example loss in batch: 0.354053  [   44/   89]
Per-example loss in batch: 0.270653  [   46/   89]
Per-example loss in batch: 0.272207  [   48/   89]
Per-example loss in batch: 0.297643  [   50/   89]
Per-example loss in batch: 0.282689  [   52/   89]
Per-example loss in batch: 0.265497  [   54/   89]
Per-example loss in batch: 0.223408  [   56/   89]
Per-example loss in batch: 0.362352  [   58/   89]
Per-example loss in batch: 0.243395  [   60/   89]
Per-example loss in batch: 0.251795  [   62/   89]
Per-example loss in batch: 0.343739  [   64/   89]
Per-example loss in batch: 0.293157  [   66/   89]
Per-example loss in batch: 0.308852  [   68/   89]
Per-example loss in batch: 0.300620  [   70/   89]
Per-example loss in batch: 0.354016  [   72/   89]
Per-example loss in batch: 0.340415  [   74/   89]
Per-example loss in batch: 0.299747  [   76/   89]
Per-example loss in batch: 0.311654  [   78/   89]
Per-example loss in batch: 0.246232  [   80/   89]
Per-example loss in batch: 0.301205  [   82/   89]
Per-example loss in batch: 0.311043  [   84/   89]
Per-example loss in batch: 0.225302  [   86/   89]
Per-example loss in batch: 0.224939  [   88/   89]
Per-example loss in batch: 0.529529  [   89/   89]
Train Error: Avg loss: 0.29381263
validation Error: 
 Avg loss: 0.51065205 
 F1: 0.422954 
 Precision: 0.480622 
 Recall: 0.377642
 IoU: 0.268194

test Error: 
 Avg loss: 0.49092947 
 F1: 0.457801 
 Precision: 0.523660 
 Recall: 0.406658
 IoU: 0.296850

We have finished training iteration 215
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_213_.pth
Per-example loss in batch: 0.294172  [    2/   89]
Per-example loss in batch: 0.245815  [    4/   89]
Per-example loss in batch: 0.310632  [    6/   89]
Per-example loss in batch: 0.252222  [    8/   89]
Per-example loss in batch: 0.235853  [   10/   89]
Per-example loss in batch: 0.282496  [   12/   89]
Per-example loss in batch: 0.337788  [   14/   89]
Per-example loss in batch: 0.220316  [   16/   89]
Per-example loss in batch: 0.332497  [   18/   89]
Per-example loss in batch: 0.311226  [   20/   89]
Per-example loss in batch: 0.287615  [   22/   89]
Per-example loss in batch: 0.307190  [   24/   89]
Per-example loss in batch: 0.349153  [   26/   89]
Per-example loss in batch: 0.229479  [   28/   89]
Per-example loss in batch: 0.250611  [   30/   89]
Per-example loss in batch: 0.275341  [   32/   89]
Per-example loss in batch: 0.369758  [   34/   89]
Per-example loss in batch: 0.283009  [   36/   89]
Per-example loss in batch: 0.288550  [   38/   89]
Per-example loss in batch: 0.288102  [   40/   89]
Per-example loss in batch: 0.279207  [   42/   89]
Per-example loss in batch: 0.373967  [   44/   89]
Per-example loss in batch: 0.312263  [   46/   89]
Per-example loss in batch: 0.253861  [   48/   89]
Per-example loss in batch: 0.355484  [   50/   89]
Per-example loss in batch: 0.266922  [   52/   89]
Per-example loss in batch: 0.309565  [   54/   89]
Per-example loss in batch: 0.255127  [   56/   89]
Per-example loss in batch: 0.331784  [   58/   89]
Per-example loss in batch: 0.318603  [   60/   89]
Per-example loss in batch: 0.260680  [   62/   89]
Per-example loss in batch: 0.323394  [   64/   89]
Per-example loss in batch: 0.352374  [   66/   89]
Per-example loss in batch: 0.260828  [   68/   89]
Per-example loss in batch: 0.220297  [   70/   89]
Per-example loss in batch: 0.373254  [   72/   89]
Per-example loss in batch: 0.330211  [   74/   89]
Per-example loss in batch: 0.317026  [   76/   89]
Per-example loss in batch: 0.250159  [   78/   89]
Per-example loss in batch: 0.302029  [   80/   89]
Per-example loss in batch: 0.305288  [   82/   89]
Per-example loss in batch: 0.349562  [   84/   89]
Per-example loss in batch: 0.294726  [   86/   89]
Per-example loss in batch: 0.242982  [   88/   89]
Per-example loss in batch: 0.574272  [   89/   89]
Train Error: Avg loss: 0.29839443
validation Error: 
 Avg loss: 0.51026126 
 F1: 0.421830 
 Precision: 0.515578 
 Recall: 0.356929
 IoU: 0.267290

test Error: 
 Avg loss: 0.49056320 
 F1: 0.453766 
 Precision: 0.554295 
 Recall: 0.384104
 IoU: 0.293465

We have finished training iteration 216
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_214_.pth
Per-example loss in batch: 0.265482  [    2/   89]
Per-example loss in batch: 0.246011  [    4/   89]
Per-example loss in batch: 0.256753  [    6/   89]
Per-example loss in batch: 0.289489  [    8/   89]
Per-example loss in batch: 0.310434  [   10/   89]
Per-example loss in batch: 0.224808  [   12/   89]
Per-example loss in batch: 0.348230  [   14/   89]
Per-example loss in batch: 0.287640  [   16/   89]
Per-example loss in batch: 0.324543  [   18/   89]
Per-example loss in batch: 0.219976  [   20/   89]
Per-example loss in batch: 0.301512  [   22/   89]
Per-example loss in batch: 0.344837  [   24/   89]
Per-example loss in batch: 0.258526  [   26/   89]
Per-example loss in batch: 0.213768  [   28/   89]
Per-example loss in batch: 0.280793  [   30/   89]
Per-example loss in batch: 0.376482  [   32/   89]
Per-example loss in batch: 0.278837  [   34/   89]
Per-example loss in batch: 0.346383  [   36/   89]
Per-example loss in batch: 0.300947  [   38/   89]
Per-example loss in batch: 0.329374  [   40/   89]
Per-example loss in batch: 0.257015  [   42/   89]
Per-example loss in batch: 0.228164  [   44/   89]
Per-example loss in batch: 0.269966  [   46/   89]
Per-example loss in batch: 0.315906  [   48/   89]
Per-example loss in batch: 0.310826  [   50/   89]
Per-example loss in batch: 0.261800  [   52/   89]
Per-example loss in batch: 0.251713  [   54/   89]
Per-example loss in batch: 0.313506  [   56/   89]
Per-example loss in batch: 0.275030  [   58/   89]
Per-example loss in batch: 0.353484  [   60/   89]
Per-example loss in batch: 0.234402  [   62/   89]
Per-example loss in batch: 0.300540  [   64/   89]
Per-example loss in batch: 0.283136  [   66/   89]
Per-example loss in batch: 0.323868  [   68/   89]
Per-example loss in batch: 0.242726  [   70/   89]
Per-example loss in batch: 0.289067  [   72/   89]
Per-example loss in batch: 0.330150  [   74/   89]
Per-example loss in batch: 0.267422  [   76/   89]
Per-example loss in batch: 0.268124  [   78/   89]
Per-example loss in batch: 0.255702  [   80/   89]
Per-example loss in batch: 0.227442  [   82/   89]
Per-example loss in batch: 0.360631  [   84/   89]
Per-example loss in batch: 0.252220  [   86/   89]
Per-example loss in batch: 0.335251  [   88/   89]
Per-example loss in batch: 0.668904  [   89/   89]
Train Error: Avg loss: 0.29095206
validation Error: 
 Avg loss: 0.50974118 
 F1: 0.420010 
 Precision: 0.544982 
 Recall: 0.341663
 IoU: 0.265831

test Error: 
 Avg loss: 0.48978120 
 F1: 0.451258 
 Precision: 0.584427 
 Recall: 0.367514
 IoU: 0.291370

We have finished training iteration 217
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_215_.pth
Error in cropping the image to the non-zero values in the fourth channel. Returned the original image.
Per-example loss in batch: 0.282644  [    2/   89]
Per-example loss in batch: 0.309725  [    4/   89]
Per-example loss in batch: 0.356151  [    6/   89]
Per-example loss in batch: 0.296187  [    8/   89]
Per-example loss in batch: 0.359910  [   10/   89]
Per-example loss in batch: 0.274404  [   12/   89]
Per-example loss in batch: 0.300621  [   14/   89]
Per-example loss in batch: 0.332184  [   16/   89]
Per-example loss in batch: 0.248435  [   18/   89]
Per-example loss in batch: 0.246247  [   20/   89]
Per-example loss in batch: 0.376943  [   22/   89]
Per-example loss in batch: 0.263317  [   24/   89]
Per-example loss in batch: 0.335944  [   26/   89]
Per-example loss in batch: 0.241068  [   28/   89]
Per-example loss in batch: 0.287548  [   30/   89]
Per-example loss in batch: 0.311905  [   32/   89]
Per-example loss in batch: 0.257556  [   34/   89]
Per-example loss in batch: 0.270835  [   36/   89]
Per-example loss in batch: 0.313051  [   38/   89]
Per-example loss in batch: 0.259719  [   40/   89]
Per-example loss in batch: 0.258354  [   42/   89]
Per-example loss in batch: 0.272335  [   44/   89]
Per-example loss in batch: 0.302743  [   46/   89]
Per-example loss in batch: 0.236036  [   48/   89]
Per-example loss in batch: 0.267323  [   50/   89]
Per-example loss in batch: 0.340239  [   52/   89]
Per-example loss in batch: 0.276767  [   54/   89]
Per-example loss in batch: 0.316469  [   56/   89]
Per-example loss in batch: 0.321824  [   58/   89]
Per-example loss in batch: 0.313045  [   60/   89]
Per-example loss in batch: 0.305784  [   62/   89]
Per-example loss in batch: 0.295574  [   64/   89]
Per-example loss in batch: 0.298398  [   66/   89]
Per-example loss in batch: 0.237652  [   68/   89]
Per-example loss in batch: 0.310946  [   70/   89]
Per-example loss in batch: 0.349385  [   72/   89]
Per-example loss in batch: 0.232155  [   74/   89]
Per-example loss in batch: 0.357340  [   76/   89]
Per-example loss in batch: 0.272985  [   78/   89]
Per-example loss in batch: 0.264698  [   80/   89]
Per-example loss in batch: 0.348064  [   82/   89]
Per-example loss in batch: 0.231676  [   84/   89]
Per-example loss in batch: 0.329237  [   86/   89]
Per-example loss in batch: 0.228595  [   88/   89]
Per-example loss in batch: 0.720624  [   89/   89]
Train Error: Avg loss: 0.29780517
validation Error: 
 Avg loss: 0.51031138 
 F1: 0.421818 
 Precision: 0.469542 
 Recall: 0.382900
 IoU: 0.267281

test Error: 
 Avg loss: 0.49113657 
 F1: 0.456001 
 Precision: 0.510434 
 Recall: 0.412058
 IoU: 0.295337

We have finished training iteration 218
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_216_.pth
Per-example loss in batch: 0.318011  [    2/   89]
Per-example loss in batch: 0.322481  [    4/   89]
Per-example loss in batch: 0.379399  [    6/   89]
Per-example loss in batch: 0.413379  [    8/   89]
Per-example loss in batch: 0.273212  [   10/   89]
Per-example loss in batch: 0.283458  [   12/   89]
Per-example loss in batch: 0.272126  [   14/   89]
Per-example loss in batch: 0.318643  [   16/   89]
Per-example loss in batch: 0.269804  [   18/   89]
Per-example loss in batch: 0.296073  [   20/   89]
Per-example loss in batch: 0.239296  [   22/   89]
Per-example loss in batch: 0.322391  [   24/   89]
Per-example loss in batch: 0.234033  [   26/   89]
Per-example loss in batch: 0.286254  [   28/   89]
Per-example loss in batch: 0.324530  [   30/   89]
Per-example loss in batch: 0.340750  [   32/   89]
Per-example loss in batch: 0.266825  [   34/   89]
Per-example loss in batch: 0.239338  [   36/   89]
Per-example loss in batch: 0.312511  [   38/   89]
Per-example loss in batch: 0.304563  [   40/   89]
Per-example loss in batch: 0.288312  [   42/   89]
Per-example loss in batch: 0.258372  [   44/   89]
Per-example loss in batch: 0.296541  [   46/   89]
Per-example loss in batch: 0.341098  [   48/   89]
Per-example loss in batch: 0.278697  [   50/   89]
Per-example loss in batch: 0.291891  [   52/   89]
Per-example loss in batch: 0.289629  [   54/   89]
Per-example loss in batch: 0.326402  [   56/   89]
Per-example loss in batch: 0.243236  [   58/   89]
Per-example loss in batch: 0.250461  [   60/   89]
Per-example loss in batch: 0.248394  [   62/   89]
Per-example loss in batch: 0.295381  [   64/   89]
Per-example loss in batch: 0.292507  [   66/   89]
Per-example loss in batch: 0.262937  [   68/   89]
Per-example loss in batch: 0.308854  [   70/   89]
Per-example loss in batch: 0.224034  [   72/   89]
Per-example loss in batch: 0.318124  [   74/   89]
Per-example loss in batch: 0.258660  [   76/   89]
Per-example loss in batch: 0.268936  [   78/   89]
Per-example loss in batch: 0.261509  [   80/   89]
Per-example loss in batch: 0.239715  [   82/   89]
Per-example loss in batch: 0.324931  [   84/   89]
Per-example loss in batch: 0.379687  [   86/   89]
Per-example loss in batch: 0.264683  [   88/   89]
Per-example loss in batch: 0.689350  [   89/   89]
Train Error: Avg loss: 0.29606163
validation Error: 
 Avg loss: 0.51046518 
 F1: 0.419675 
 Precision: 0.463138 
 Recall: 0.383669
 IoU: 0.265563

test Error: 
 Avg loss: 0.49132939 
 F1: 0.455172 
 Precision: 0.502214 
 Recall: 0.416188
 IoU: 0.294643

We have finished training iteration 219
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_217_.pth
Per-example loss in batch: 0.308592  [    2/   89]
Per-example loss in batch: 0.262578  [    4/   89]
Per-example loss in batch: 0.237815  [    6/   89]
Per-example loss in batch: 0.275047  [    8/   89]
Per-example loss in batch: 0.299156  [   10/   89]
Per-example loss in batch: 0.351423  [   12/   89]
Per-example loss in batch: 0.307493  [   14/   89]
Per-example loss in batch: 0.281473  [   16/   89]
Per-example loss in batch: 0.329064  [   18/   89]
Per-example loss in batch: 0.296002  [   20/   89]
Per-example loss in batch: 0.231750  [   22/   89]
Per-example loss in batch: 0.276394  [   24/   89]
Per-example loss in batch: 0.343231  [   26/   89]
Per-example loss in batch: 0.336525  [   28/   89]
Per-example loss in batch: 0.283731  [   30/   89]
Per-example loss in batch: 0.290017  [   32/   89]
Per-example loss in batch: 0.241382  [   34/   89]
Per-example loss in batch: 0.366967  [   36/   89]
Per-example loss in batch: 0.284547  [   38/   89]
Per-example loss in batch: 0.244891  [   40/   89]
Per-example loss in batch: 0.347917  [   42/   89]
Per-example loss in batch: 0.274288  [   44/   89]
Per-example loss in batch: 0.284845  [   46/   89]
Per-example loss in batch: 0.220778  [   48/   89]
Per-example loss in batch: 0.280604  [   50/   89]
Per-example loss in batch: 0.337807  [   52/   89]
Per-example loss in batch: 0.267725  [   54/   89]
Per-example loss in batch: 0.291367  [   56/   89]
Per-example loss in batch: 0.284446  [   58/   89]
Per-example loss in batch: 0.270966  [   60/   89]
Per-example loss in batch: 0.348781  [   62/   89]
Per-example loss in batch: 0.272323  [   64/   89]
Per-example loss in batch: 0.228782  [   66/   89]
Per-example loss in batch: 0.231732  [   68/   89]
Per-example loss in batch: 0.326729  [   70/   89]
Per-example loss in batch: 0.285817  [   72/   89]
Per-example loss in batch: 0.264606  [   74/   89]
Per-example loss in batch: 0.284850  [   76/   89]
Per-example loss in batch: 0.269878  [   78/   89]
Per-example loss in batch: 0.328109  [   80/   89]
Per-example loss in batch: 0.271267  [   82/   89]
Per-example loss in batch: 0.345096  [   84/   89]
Per-example loss in batch: 0.343666  [   86/   89]
Per-example loss in batch: 0.262513  [   88/   89]
Per-example loss in batch: 0.583868  [   89/   89]
Train Error: Avg loss: 0.29359344
validation Error: 
 Avg loss: 0.50984473 
 F1: 0.422492 
 Precision: 0.500080 
 Recall: 0.365745
 IoU: 0.267822

test Error: 
 Avg loss: 0.49070776 
 F1: 0.454338 
 Precision: 0.536570 
 Recall: 0.393961
 IoU: 0.293944

We have finished training iteration 220
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_218_.pth
Per-example loss in batch: 0.264517  [    2/   89]
Per-example loss in batch: 0.329008  [    4/   89]
Per-example loss in batch: 0.275303  [    6/   89]
Per-example loss in batch: 0.306950  [    8/   89]
Per-example loss in batch: 0.308445  [   10/   89]
Per-example loss in batch: 0.256249  [   12/   89]
Per-example loss in batch: 0.357364  [   14/   89]
Per-example loss in batch: 0.253591  [   16/   89]
Per-example loss in batch: 0.272389  [   18/   89]
Per-example loss in batch: 0.266204  [   20/   89]
Per-example loss in batch: 0.237332  [   22/   89]
Per-example loss in batch: 0.256032  [   24/   89]
Per-example loss in batch: 0.244152  [   26/   89]
Per-example loss in batch: 0.258299  [   28/   89]
Per-example loss in batch: 0.295738  [   30/   89]
Per-example loss in batch: 0.332244  [   32/   89]
Per-example loss in batch: 0.330592  [   34/   89]
Per-example loss in batch: 0.300002  [   36/   89]
Per-example loss in batch: 0.369223  [   38/   89]
Per-example loss in batch: 0.395531  [   40/   89]
Per-example loss in batch: 0.307838  [   42/   89]
Per-example loss in batch: 0.306316  [   44/   89]
Per-example loss in batch: 0.261733  [   46/   89]
Per-example loss in batch: 0.312404  [   48/   89]
Per-example loss in batch: 0.317470  [   50/   89]
Per-example loss in batch: 0.206005  [   52/   89]
Per-example loss in batch: 0.281687  [   54/   89]
Per-example loss in batch: 0.360368  [   56/   89]
Per-example loss in batch: 0.288699  [   58/   89]
Per-example loss in batch: 0.361935  [   60/   89]
Per-example loss in batch: 0.260325  [   62/   89]
Per-example loss in batch: 0.349483  [   64/   89]
Per-example loss in batch: 0.271348  [   66/   89]
Per-example loss in batch: 0.296605  [   68/   89]
Per-example loss in batch: 0.279345  [   70/   89]
Per-example loss in batch: 0.311177  [   72/   89]
Per-example loss in batch: 0.284553  [   74/   89]
Per-example loss in batch: 0.339269  [   76/   89]
Per-example loss in batch: 0.288475  [   78/   89]
Per-example loss in batch: 0.272674  [   80/   89]
Per-example loss in batch: 0.242217  [   82/   89]
Per-example loss in batch: 0.360129  [   84/   89]
Per-example loss in batch: 0.356409  [   86/   89]
Per-example loss in batch: 0.372236  [   88/   89]
Per-example loss in batch: 0.580078  [   89/   89]
Train Error: Avg loss: 0.30309899
validation Error: 
 Avg loss: 0.51032417 
 F1: 0.419752 
 Precision: 0.510646 
 Recall: 0.356327
 IoU: 0.265624

test Error: 
 Avg loss: 0.49054469 
 F1: 0.450672 
 Precision: 0.548895 
 Recall: 0.382266
 IoU: 0.290882

We have finished training iteration 221
Deleting model ./unet_wscwzo_train/saved_model_wrapper/models/UNet_219_.pth
Per-example loss in batch: 0.241467  [    2/   89]
Per-example loss in batch: 0.276387  [    4/   89]
Per-example loss in batch: 0.353168  [    6/   89]
Per-example loss in batch: 0.359912  [    8/   89]
Per-example loss in batch: 0.253785  [   10/   89]
Per-example loss in batch: 0.284319  [   12/   89]
Per-example loss in batch: 0.253871  [   14/   89]
Per-example loss in batch: 0.313642  [   16/   89]
Per-example loss in batch: 0.213051  [   18/   89]
Per-example loss in batch: 0.332697  [   20/   89]
Per-example loss in batch: 0.239210  [   22/   89]
Per-example loss in batch: 0.284443  [   24/   89]
Per-example loss in batch: 0.302863  [   26/   89]
Per-example loss in batch: 0.274107  [   28/   89]
Per-example loss in batch: 0.333075  [   30/   89]
Per-example loss in batch: 0.337349  [   32/   89]
Per-example loss in batch: 0.232949  [   34/   89]
Per-example loss in batch: 0.335978  [   36/   89]
Per-example loss in batch: 0.226279  [   38/   89]
Per-example loss in batch: 0.308986  [   40/   89]
Per-example loss in batch: 0.356222  [   42/   89]
Per-example loss in batch: 0.261638  [   44/   89]
Per-example loss in batch: 0.290809  [   46/   89]
Per-example loss in batch: 0.281097  [   48/   89]
Per-example loss in batch: 0.244140  [   50/   89]
Per-example loss in batch: 0.257659  [   52/   89]
Per-example loss in batch: 0.298104  [   54/   89]
Per-example loss in batch: 0.324504  [   56/   89]
Per-example loss in batch: 0.331383  [   58/   89]
Per-example loss in batch: 0.307395  [   60/   89]
Per-example loss in batch: 0.339025  [   62/   89]
Per-example loss in batch: 0.257463  [   64/   89]
Per-example loss in batch: 0.345572  [   66/   89]
Per-example loss in batch: 0.261390  [   68/   89]
Per-example loss in batch: 0.310658  [   70/   89]
Per-example loss in batch: 0.304213  [   72/   89]
Per-example loss in batch: 0.276642  [   74/   89]
Per-example loss in batch: 0.333612  [   76/   89]
Per-example loss in batch: 0.351650  [   78/   89]
Per-example loss in batch: 0.332381  [   80/   89]
Per-example loss in batch: 0.241239  [   82/   89]
Per-example loss in batch: 0.254690  [   84/   89]
Per-example loss in batch: 0.265811  [   86/   89]
Per-example loss in batch: 0.309438  [   88/   89]
Per-example loss in batch: 0.574091  [   89/   89]
Train Error: Avg loss: 0.29620943
slurmstepd: error: *** STEP 16441.0 ON aga1 CANCELLED AT 2025-01-07T16:45:17 ***
