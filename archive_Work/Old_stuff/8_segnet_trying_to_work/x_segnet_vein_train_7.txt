/shared/home/matevz.vidovic/Diplomska/Prototip/Delo/model_wrapper.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model = torch.load(self.prev_model_path, map_location=torch.device(device))
segnet_main.py do_log: False
Log file name: log_21_17-18-08_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(sd='segnet_vein_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_segnet/segnet_vein.yaml', yo=None, ntibp=None, ptp=None, map=None)
YAML: {'is_pruning_ready': False, 'path_to_data': './Data/vein_and_sclera_data', 'target': 'veins', 'train_epoch_size': 300, 'val_epoch_size': 100, 'test_epoch_size': 100, 'train_batch_size': 3, 'eval_batch_size': 12, 'learning_rate': 1e-05, 'num_of_dataloader_workers': 32, 'cleanup_k': 3, 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': False, 'loss_fn_name': 'MCDL', 'loss_params': None, 'dataset_type': 'vasd', 'aug_type': 'tf', 'zero_out_non_sclera': True, 'add_sclera_to_img': False, 'add_bcosfire_to_img': True, 'add_coye_to_img': True, 'model': '64_2_6', 'input_width': 2048, 'input_height': 1024, 'input_channels': 5, 'output_channels': 2, 'have_patchification': False, 'patchification_params': 'None', 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq', 'conv2d_prune_limit': 0.2}
Validation phase: False
Namespace(sd='segnet_vein_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_segnet/segnet_vein.yaml', yo=None, ntibp=None, ptp=None, map=None)
Device: cuda
dataset.py do_log: False
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 88
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 88
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 100
val dataloader num of batches: 3
test dataloader num of batches: 1
Loaded model path:  ./segnet_vein_train/saved_model_wrapper/models/SegNet_100_.pth
per-ex loss: 0.526051  [    3/  300]
per-ex loss: 0.466376  [    6/  300]
per-ex loss: 0.400367  [    9/  300]
per-ex loss: 0.576407  [   12/  300]
per-ex loss: 0.417966  [   15/  300]
per-ex loss: 0.463217  [   18/  300]
per-ex loss: 0.563936  [   21/  300]
per-ex loss: 0.608395  [   24/  300]
per-ex loss: 0.538477  [   27/  300]
per-ex loss: 0.486863  [   30/  300]
per-ex loss: 0.544952  [   33/  300]
per-ex loss: 0.419779  [   36/  300]
per-ex loss: 0.510235  [   39/  300]
per-ex loss: 0.490657  [   42/  300]
per-ex loss: 0.553362  [   45/  300]
per-ex loss: 0.493610  [   48/  300]
per-ex loss: 0.605591  [   51/  300]
per-ex loss: 0.497058  [   54/  300]
per-ex loss: 0.404877  [   57/  300]
per-ex loss: 0.476262  [   60/  300]
per-ex loss: 0.504067  [   63/  300]
per-ex loss: 0.531807  [   66/  300]
per-ex loss: 0.552973  [   69/  300]
per-ex loss: 0.492122  [   72/  300]
per-ex loss: 0.526150  [   75/  300]
per-ex loss: 0.533865  [   78/  300]
per-ex loss: 0.444166  [   81/  300]
per-ex loss: 0.515808  [   84/  300]
per-ex loss: 0.500907  [   87/  300]
per-ex loss: 0.426219  [   90/  300]
per-ex loss: 0.584564  [   93/  300]
per-ex loss: 0.396143  [   96/  300]
per-ex loss: 0.625745  [   99/  300]
per-ex loss: 0.586429  [  102/  300]
per-ex loss: 0.449961  [  105/  300]
per-ex loss: 0.448655  [  108/  300]
per-ex loss: 0.440646  [  111/  300]
per-ex loss: 0.403731  [  114/  300]
per-ex loss: 0.442996  [  117/  300]
per-ex loss: 0.658976  [  120/  300]
per-ex loss: 0.545808  [  123/  300]
per-ex loss: 0.524180  [  126/  300]
per-ex loss: 0.547097  [  129/  300]
per-ex loss: 0.598752  [  132/  300]
per-ex loss: 0.548221  [  135/  300]
per-ex loss: 0.493499  [  138/  300]
per-ex loss: 0.524664  [  141/  300]
per-ex loss: 0.480520  [  144/  300]
per-ex loss: 0.441108  [  147/  300]
per-ex loss: 0.601377  [  150/  300]
per-ex loss: 0.465986  [  153/  300]
per-ex loss: 0.553217  [  156/  300]
per-ex loss: 0.532655  [  159/  300]
per-ex loss: 0.403424  [  162/  300]
per-ex loss: 0.444543  [  165/  300]
per-ex loss: 0.439105  [  168/  300]
per-ex loss: 0.561725  [  171/  300]
per-ex loss: 0.594907  [  174/  300]
per-ex loss: 0.456209  [  177/  300]
per-ex loss: 0.422528  [  180/  300]
per-ex loss: 0.608435  [  183/  300]
per-ex loss: 0.500184  [  186/  300]
per-ex loss: 0.469037  [  189/  300]
per-ex loss: 0.489381  [  192/  300]
per-ex loss: 0.485173  [  195/  300]
per-ex loss: 0.523750  [  198/  300]
per-ex loss: 0.380808  [  201/  300]
per-ex loss: 0.533532  [  204/  300]
per-ex loss: 0.624742  [  207/  300]
per-ex loss: 0.492313  [  210/  300]
per-ex loss: 0.486048  [  213/  300]
per-ex loss: 0.602367  [  216/  300]
per-ex loss: 0.517076  [  219/  300]
per-ex loss: 0.432295  [  222/  300]
per-ex loss: 0.433603  [  225/  300]
per-ex loss: 0.628794  [  228/  300]
per-ex loss: 0.475475  [  231/  300]
per-ex loss: 0.499130  [  234/  300]
per-ex loss: 0.515298  [  237/  300]
per-ex loss: 0.622468  [  240/  300]
per-ex loss: 0.543517  [  243/  300]
per-ex loss: 0.529314  [  246/  300]
per-ex loss: 0.502677  [  249/  300]
per-ex loss: 0.486709  [  252/  300]
per-ex loss: 0.431540  [  255/  300]
per-ex loss: 0.494766  [  258/  300]
per-ex loss: 0.601321  [  261/  300]
per-ex loss: 0.484962  [  264/  300]
per-ex loss: 0.493550  [  267/  300]
per-ex loss: 0.527164  [  270/  300]
per-ex loss: 0.497624  [  273/  300]
per-ex loss: 0.696853  [  276/  300]
per-ex loss: 0.454058  [  279/  300]
per-ex loss: 0.438996  [  282/  300]
per-ex loss: 0.433754  [  285/  300]
per-ex loss: 0.534999  [  288/  300]
per-ex loss: 0.545405  [  291/  300]
per-ex loss: 0.419108  [  294/  300]
per-ex loss: 0.528985  [  297/  300]
per-ex loss: 0.400156  [  300/  300]
Train Error: Avg loss: 0.50655257
validation Error: 
 Avg loss: 0.50555986 
 F1: 0.484426 
 Precision: 0.457475 
 Recall: 0.514751
 IoU: 0.319632

test Error: 
 Avg loss: 0.45249110 
 F1: 0.548298 
 Precision: 0.518492 
 Recall: 0.581739
 IoU: 0.377693

We have finished training iteration 101
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_86_.pth
per-ex loss: 0.617967  [    3/  300]
per-ex loss: 0.427923  [    6/  300]
per-ex loss: 0.539695  [    9/  300]
per-ex loss: 0.461702  [   12/  300]
per-ex loss: 0.496281  [   15/  300]
per-ex loss: 0.567706  [   18/  300]
per-ex loss: 0.505714  [   21/  300]
per-ex loss: 0.428842  [   24/  300]
per-ex loss: 0.537223  [   27/  300]
per-ex loss: 0.477534  [   30/  300]
per-ex loss: 0.429516  [   33/  300]
per-ex loss: 0.618954  [   36/  300]
per-ex loss: 0.570772  [   39/  300]
per-ex loss: 0.557891  [   42/  300]
per-ex loss: 0.415903  [   45/  300]
per-ex loss: 0.480152  [   48/  300]
per-ex loss: 0.436082  [   51/  300]
per-ex loss: 0.471233  [   54/  300]
per-ex loss: 0.524700  [   57/  300]
per-ex loss: 0.492517  [   60/  300]
per-ex loss: 0.505895  [   63/  300]
per-ex loss: 0.623931  [   66/  300]
per-ex loss: 0.457037  [   69/  300]
per-ex loss: 0.521077  [   72/  300]
per-ex loss: 0.556908  [   75/  300]
per-ex loss: 0.489452  [   78/  300]
per-ex loss: 0.577211  [   81/  300]
per-ex loss: 0.663511  [   84/  300]
per-ex loss: 0.416997  [   87/  300]
per-ex loss: 0.455463  [   90/  300]
per-ex loss: 0.542794  [   93/  300]
per-ex loss: 0.458960  [   96/  300]
per-ex loss: 0.444053  [   99/  300]
per-ex loss: 0.446720  [  102/  300]
per-ex loss: 0.459543  [  105/  300]
per-ex loss: 0.399555  [  108/  300]
per-ex loss: 0.619273  [  111/  300]
per-ex loss: 0.515693  [  114/  300]
per-ex loss: 0.488923  [  117/  300]
per-ex loss: 0.579424  [  120/  300]
per-ex loss: 0.442485  [  123/  300]
per-ex loss: 0.579040  [  126/  300]
per-ex loss: 0.563802  [  129/  300]
per-ex loss: 0.659952  [  132/  300]
per-ex loss: 0.584813  [  135/  300]
per-ex loss: 0.444785  [  138/  300]
per-ex loss: 0.446523  [  141/  300]
per-ex loss: 0.632404  [  144/  300]
per-ex loss: 0.466051  [  147/  300]
per-ex loss: 0.467708  [  150/  300]
per-ex loss: 0.548215  [  153/  300]
per-ex loss: 0.494470  [  156/  300]
per-ex loss: 0.502915  [  159/  300]
per-ex loss: 0.483871  [  162/  300]
per-ex loss: 0.425782  [  165/  300]
per-ex loss: 0.592849  [  168/  300]
per-ex loss: 0.500394  [  171/  300]
per-ex loss: 0.536573  [  174/  300]
per-ex loss: 0.472865  [  177/  300]
per-ex loss: 0.474660  [  180/  300]
per-ex loss: 0.645822  [  183/  300]
per-ex loss: 0.441991  [  186/  300]
per-ex loss: 0.497829  [  189/  300]
per-ex loss: 0.495389  [  192/  300]
per-ex loss: 0.489584  [  195/  300]
per-ex loss: 0.481303  [  198/  300]
per-ex loss: 0.402440  [  201/  300]
per-ex loss: 0.449242  [  204/  300]
per-ex loss: 0.551859  [  207/  300]
per-ex loss: 0.476190  [  210/  300]
per-ex loss: 0.511372  [  213/  300]
per-ex loss: 0.434554  [  216/  300]
per-ex loss: 0.511840  [  219/  300]
per-ex loss: 0.571432  [  222/  300]
per-ex loss: 0.669048  [  225/  300]
per-ex loss: 0.394225  [  228/  300]
per-ex loss: 0.539880  [  231/  300]
per-ex loss: 0.570743  [  234/  300]
per-ex loss: 0.609486  [  237/  300]
per-ex loss: 0.464631  [  240/  300]
per-ex loss: 0.434800  [  243/  300]
per-ex loss: 0.565721  [  246/  300]
per-ex loss: 0.440586  [  249/  300]
per-ex loss: 0.463225  [  252/  300]
per-ex loss: 0.458638  [  255/  300]
per-ex loss: 0.472464  [  258/  300]
per-ex loss: 0.714601  [  261/  300]
per-ex loss: 0.605854  [  264/  300]
per-ex loss: 0.603546  [  267/  300]
per-ex loss: 0.460305  [  270/  300]
per-ex loss: 0.448103  [  273/  300]
per-ex loss: 0.450951  [  276/  300]
per-ex loss: 0.576588  [  279/  300]
per-ex loss: 0.500562  [  282/  300]
per-ex loss: 0.538338  [  285/  300]
per-ex loss: 0.585953  [  288/  300]
per-ex loss: 0.452343  [  291/  300]
per-ex loss: 0.399346  [  294/  300]
per-ex loss: 0.514956  [  297/  300]
per-ex loss: 0.439431  [  300/  300]
Train Error: Avg loss: 0.50936055
validation Error: 
 Avg loss: 0.49236802 
 F1: 0.488857 
 Precision: 0.425930 
 Recall: 0.573601
 IoU: 0.323501

test Error: 
 Avg loss: 0.45059299 
 F1: 0.550079 
 Precision: 0.489081 
 Recall: 0.628459
 IoU: 0.379385

We have finished training iteration 102
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_99_.pth
per-ex loss: 0.594313  [    3/  300]
per-ex loss: 0.462406  [    6/  300]
per-ex loss: 0.434205  [    9/  300]
per-ex loss: 0.507723  [   12/  300]
per-ex loss: 0.477353  [   15/  300]
per-ex loss: 0.433544  [   18/  300]
per-ex loss: 0.450769  [   21/  300]
per-ex loss: 0.441275  [   24/  300]
per-ex loss: 0.570373  [   27/  300]
per-ex loss: 0.495935  [   30/  300]
per-ex loss: 0.477862  [   33/  300]
per-ex loss: 0.535432  [   36/  300]
per-ex loss: 0.569018  [   39/  300]
per-ex loss: 0.422967  [   42/  300]
per-ex loss: 0.474949  [   45/  300]
per-ex loss: 0.502118  [   48/  300]
per-ex loss: 0.509247  [   51/  300]
per-ex loss: 0.568285  [   54/  300]
per-ex loss: 0.449146  [   57/  300]
per-ex loss: 0.498367  [   60/  300]
per-ex loss: 0.408289  [   63/  300]
per-ex loss: 0.452944  [   66/  300]
per-ex loss: 0.566249  [   69/  300]
per-ex loss: 0.511225  [   72/  300]
per-ex loss: 0.640275  [   75/  300]
per-ex loss: 0.447434  [   78/  300]
per-ex loss: 0.475526  [   81/  300]
per-ex loss: 0.518458  [   84/  300]
per-ex loss: 0.649478  [   87/  300]
per-ex loss: 0.507056  [   90/  300]
per-ex loss: 0.479137  [   93/  300]
per-ex loss: 0.476112  [   96/  300]
per-ex loss: 0.541871  [   99/  300]
per-ex loss: 0.484605  [  102/  300]
per-ex loss: 0.489921  [  105/  300]
per-ex loss: 0.643978  [  108/  300]
per-ex loss: 0.478209  [  111/  300]
per-ex loss: 0.618121  [  114/  300]
per-ex loss: 0.638226  [  117/  300]
per-ex loss: 0.537883  [  120/  300]
per-ex loss: 0.432719  [  123/  300]
per-ex loss: 0.416119  [  126/  300]
per-ex loss: 0.448337  [  129/  300]
per-ex loss: 0.577568  [  132/  300]
per-ex loss: 0.447365  [  135/  300]
per-ex loss: 0.522709  [  138/  300]
per-ex loss: 0.457098  [  141/  300]
per-ex loss: 0.525092  [  144/  300]
per-ex loss: 0.536278  [  147/  300]
per-ex loss: 0.406882  [  150/  300]
per-ex loss: 0.607124  [  153/  300]
per-ex loss: 0.518004  [  156/  300]
per-ex loss: 0.455748  [  159/  300]
per-ex loss: 0.409515  [  162/  300]
per-ex loss: 0.494719  [  165/  300]
per-ex loss: 0.547814  [  168/  300]
per-ex loss: 0.663451  [  171/  300]
per-ex loss: 0.468652  [  174/  300]
per-ex loss: 0.488185  [  177/  300]
per-ex loss: 0.545602  [  180/  300]
per-ex loss: 0.538277  [  183/  300]
per-ex loss: 0.585996  [  186/  300]
per-ex loss: 0.412788  [  189/  300]
per-ex loss: 0.521412  [  192/  300]
per-ex loss: 0.501439  [  195/  300]
per-ex loss: 0.562248  [  198/  300]
per-ex loss: 0.412553  [  201/  300]
per-ex loss: 0.399155  [  204/  300]
per-ex loss: 0.632012  [  207/  300]
per-ex loss: 0.466662  [  210/  300]
per-ex loss: 0.524647  [  213/  300]
per-ex loss: 0.499980  [  216/  300]
per-ex loss: 0.452872  [  219/  300]
per-ex loss: 0.545422  [  222/  300]
per-ex loss: 0.467269  [  225/  300]
per-ex loss: 0.456775  [  228/  300]
per-ex loss: 0.642053  [  231/  300]
per-ex loss: 0.469300  [  234/  300]
per-ex loss: 0.442751  [  237/  300]
per-ex loss: 0.440436  [  240/  300]
per-ex loss: 0.501230  [  243/  300]
per-ex loss: 0.565219  [  246/  300]
per-ex loss: 0.521972  [  249/  300]
per-ex loss: 0.438833  [  252/  300]
per-ex loss: 0.577055  [  255/  300]
per-ex loss: 0.505604  [  258/  300]
per-ex loss: 0.585539  [  261/  300]
per-ex loss: 0.518896  [  264/  300]
per-ex loss: 0.524693  [  267/  300]
per-ex loss: 0.486195  [  270/  300]
per-ex loss: 0.571975  [  273/  300]
per-ex loss: 0.637145  [  276/  300]
per-ex loss: 0.632007  [  279/  300]
per-ex loss: 0.607505  [  282/  300]
per-ex loss: 0.614296  [  285/  300]
per-ex loss: 0.521105  [  288/  300]
per-ex loss: 0.623515  [  291/  300]
per-ex loss: 0.524381  [  294/  300]
per-ex loss: 0.432293  [  297/  300]
per-ex loss: 0.458152  [  300/  300]
Train Error: Avg loss: 0.51230924
validation Error: 
 Avg loss: 0.49347748 
 F1: 0.491684 
 Precision: 0.452039 
 Recall: 0.538952
 IoU: 0.325982

test Error: 
 Avg loss: 0.45401454 
 F1: 0.546745 
 Precision: 0.511732 
 Recall: 0.586902
 IoU: 0.376221

We have finished training iteration 103
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_101_.pth
per-ex loss: 0.433334  [    3/  300]
per-ex loss: 0.481869  [    6/  300]
per-ex loss: 0.445605  [    9/  300]
per-ex loss: 0.577355  [   12/  300]
per-ex loss: 0.555564  [   15/  300]
per-ex loss: 0.526565  [   18/  300]
per-ex loss: 0.464333  [   21/  300]
per-ex loss: 0.449290  [   24/  300]
per-ex loss: 0.647397  [   27/  300]
per-ex loss: 0.498841  [   30/  300]
per-ex loss: 0.582860  [   33/  300]
per-ex loss: 0.443543  [   36/  300]
per-ex loss: 0.538140  [   39/  300]
per-ex loss: 0.535140  [   42/  300]
per-ex loss: 0.476683  [   45/  300]
per-ex loss: 0.554598  [   48/  300]
per-ex loss: 0.480521  [   51/  300]
per-ex loss: 0.486784  [   54/  300]
per-ex loss: 0.456490  [   57/  300]
per-ex loss: 0.444369  [   60/  300]
per-ex loss: 0.518630  [   63/  300]
per-ex loss: 0.425194  [   66/  300]
per-ex loss: 0.502044  [   69/  300]
per-ex loss: 0.581813  [   72/  300]
per-ex loss: 0.499236  [   75/  300]
per-ex loss: 0.420753  [   78/  300]
per-ex loss: 0.527553  [   81/  300]
per-ex loss: 0.633008  [   84/  300]
per-ex loss: 0.570015  [   87/  300]
per-ex loss: 0.538993  [   90/  300]
per-ex loss: 0.470160  [   93/  300]
per-ex loss: 0.571870  [   96/  300]
per-ex loss: 0.497610  [   99/  300]
per-ex loss: 0.484130  [  102/  300]
per-ex loss: 0.601103  [  105/  300]
per-ex loss: 0.533538  [  108/  300]
per-ex loss: 0.542033  [  111/  300]
per-ex loss: 0.456320  [  114/  300]
per-ex loss: 0.462011  [  117/  300]
per-ex loss: 0.603248  [  120/  300]
per-ex loss: 0.628016  [  123/  300]
per-ex loss: 0.574456  [  126/  300]
per-ex loss: 0.600823  [  129/  300]
per-ex loss: 0.484480  [  132/  300]
per-ex loss: 0.447458  [  135/  300]
per-ex loss: 0.543811  [  138/  300]
per-ex loss: 0.496367  [  141/  300]
per-ex loss: 0.497762  [  144/  300]
per-ex loss: 0.552064  [  147/  300]
per-ex loss: 0.409779  [  150/  300]
per-ex loss: 0.469368  [  153/  300]
per-ex loss: 0.513138  [  156/  300]
per-ex loss: 0.420993  [  159/  300]
per-ex loss: 0.498376  [  162/  300]
per-ex loss: 0.449609  [  165/  300]
per-ex loss: 0.533721  [  168/  300]
per-ex loss: 0.550615  [  171/  300]
per-ex loss: 0.475612  [  174/  300]
per-ex loss: 0.424093  [  177/  300]
per-ex loss: 0.435341  [  180/  300]
per-ex loss: 0.466511  [  183/  300]
per-ex loss: 0.617479  [  186/  300]
per-ex loss: 0.651445  [  189/  300]
per-ex loss: 0.493107  [  192/  300]
per-ex loss: 0.508625  [  195/  300]
per-ex loss: 0.396343  [  198/  300]
per-ex loss: 0.436748  [  201/  300]
per-ex loss: 0.470510  [  204/  300]
per-ex loss: 0.526829  [  207/  300]
per-ex loss: 0.526971  [  210/  300]
per-ex loss: 0.513757  [  213/  300]
per-ex loss: 0.565811  [  216/  300]
per-ex loss: 0.461276  [  219/  300]
per-ex loss: 0.421908  [  222/  300]
per-ex loss: 0.569035  [  225/  300]
per-ex loss: 0.516964  [  228/  300]
per-ex loss: 0.493263  [  231/  300]
per-ex loss: 0.525501  [  234/  300]
per-ex loss: 0.496217  [  237/  300]
per-ex loss: 0.528257  [  240/  300]
per-ex loss: 0.467473  [  243/  300]
per-ex loss: 0.486468  [  246/  300]
per-ex loss: 0.459177  [  249/  300]
per-ex loss: 0.612245  [  252/  300]
per-ex loss: 0.497514  [  255/  300]
per-ex loss: 0.491168  [  258/  300]
per-ex loss: 0.457360  [  261/  300]
per-ex loss: 0.483380  [  264/  300]
per-ex loss: 0.533652  [  267/  300]
per-ex loss: 0.521319  [  270/  300]
per-ex loss: 0.480524  [  273/  300]
per-ex loss: 0.424838  [  276/  300]
per-ex loss: 0.535825  [  279/  300]
per-ex loss: 0.449450  [  282/  300]
per-ex loss: 0.455403  [  285/  300]
per-ex loss: 0.546282  [  288/  300]
per-ex loss: 0.567472  [  291/  300]
per-ex loss: 0.421461  [  294/  300]
per-ex loss: 0.497275  [  297/  300]
per-ex loss: 0.518775  [  300/  300]
Train Error: Avg loss: 0.50618041
validation Error: 
 Avg loss: 0.51599413 
 F1: 0.478893 
 Precision: 0.464405 
 Recall: 0.494314
 IoU: 0.314832

test Error: 
 Avg loss: 0.45849413 
 F1: 0.542213 
 Precision: 0.529772 
 Recall: 0.555252
 IoU: 0.371942

We have finished training iteration 104
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_97_.pth
per-ex loss: 0.454644  [    3/  300]
per-ex loss: 0.507097  [    6/  300]
per-ex loss: 0.623875  [    9/  300]
per-ex loss: 0.536005  [   12/  300]
per-ex loss: 0.565977  [   15/  300]
per-ex loss: 0.543107  [   18/  300]
per-ex loss: 0.475144  [   21/  300]
per-ex loss: 0.488896  [   24/  300]
per-ex loss: 0.540097  [   27/  300]
per-ex loss: 0.671610  [   30/  300]
per-ex loss: 0.493343  [   33/  300]
per-ex loss: 0.432000  [   36/  300]
per-ex loss: 0.476565  [   39/  300]
per-ex loss: 0.505296  [   42/  300]
per-ex loss: 0.534213  [   45/  300]
per-ex loss: 0.435273  [   48/  300]
per-ex loss: 0.651682  [   51/  300]
per-ex loss: 0.660106  [   54/  300]
per-ex loss: 0.432770  [   57/  300]
per-ex loss: 0.458062  [   60/  300]
per-ex loss: 0.415145  [   63/  300]
per-ex loss: 0.513368  [   66/  300]
per-ex loss: 0.446735  [   69/  300]
per-ex loss: 0.626643  [   72/  300]
per-ex loss: 0.471226  [   75/  300]
per-ex loss: 0.454186  [   78/  300]
per-ex loss: 0.425769  [   81/  300]
per-ex loss: 0.483470  [   84/  300]
per-ex loss: 0.466334  [   87/  300]
per-ex loss: 0.560988  [   90/  300]
per-ex loss: 0.607141  [   93/  300]
per-ex loss: 0.410944  [   96/  300]
per-ex loss: 0.563947  [   99/  300]
per-ex loss: 0.531077  [  102/  300]
per-ex loss: 0.455150  [  105/  300]
per-ex loss: 0.467021  [  108/  300]
per-ex loss: 0.585104  [  111/  300]
per-ex loss: 0.405903  [  114/  300]
per-ex loss: 0.565752  [  117/  300]
per-ex loss: 0.554962  [  120/  300]
per-ex loss: 0.524791  [  123/  300]
per-ex loss: 0.469653  [  126/  300]
per-ex loss: 0.411933  [  129/  300]
per-ex loss: 0.626098  [  132/  300]
per-ex loss: 0.551531  [  135/  300]
per-ex loss: 0.437319  [  138/  300]
per-ex loss: 0.576709  [  141/  300]
per-ex loss: 0.487902  [  144/  300]
per-ex loss: 0.504848  [  147/  300]
per-ex loss: 0.449370  [  150/  300]
per-ex loss: 0.524124  [  153/  300]
per-ex loss: 0.619659  [  156/  300]
per-ex loss: 0.536186  [  159/  300]
per-ex loss: 0.429090  [  162/  300]
per-ex loss: 0.403347  [  165/  300]
per-ex loss: 0.416385  [  168/  300]
per-ex loss: 0.429958  [  171/  300]
per-ex loss: 0.557648  [  174/  300]
per-ex loss: 0.471018  [  177/  300]
per-ex loss: 0.405440  [  180/  300]
per-ex loss: 0.532535  [  183/  300]
per-ex loss: 0.432966  [  186/  300]
per-ex loss: 0.406562  [  189/  300]
per-ex loss: 0.460453  [  192/  300]
per-ex loss: 0.516936  [  195/  300]
per-ex loss: 0.504382  [  198/  300]
per-ex loss: 0.538759  [  201/  300]
per-ex loss: 0.624063  [  204/  300]
per-ex loss: 0.465631  [  207/  300]
per-ex loss: 0.489280  [  210/  300]
per-ex loss: 0.460560  [  213/  300]
per-ex loss: 0.610622  [  216/  300]
per-ex loss: 0.439219  [  219/  300]
per-ex loss: 0.458417  [  222/  300]
per-ex loss: 0.493257  [  225/  300]
per-ex loss: 0.549979  [  228/  300]
per-ex loss: 0.533590  [  231/  300]
per-ex loss: 0.456711  [  234/  300]
per-ex loss: 0.556642  [  237/  300]
per-ex loss: 0.665688  [  240/  300]
per-ex loss: 0.682939  [  243/  300]
per-ex loss: 0.581852  [  246/  300]
per-ex loss: 0.495497  [  249/  300]
per-ex loss: 0.515084  [  252/  300]
per-ex loss: 0.579714  [  255/  300]
per-ex loss: 0.542373  [  258/  300]
per-ex loss: 0.546403  [  261/  300]
per-ex loss: 0.480259  [  264/  300]
per-ex loss: 0.544521  [  267/  300]
per-ex loss: 0.457503  [  270/  300]
per-ex loss: 0.431479  [  273/  300]
per-ex loss: 0.605502  [  276/  300]
per-ex loss: 0.488897  [  279/  300]
per-ex loss: 0.574819  [  282/  300]
per-ex loss: 0.512014  [  285/  300]
per-ex loss: 0.547195  [  288/  300]
per-ex loss: 0.547351  [  291/  300]
per-ex loss: 0.517673  [  294/  300]
per-ex loss: 0.522277  [  297/  300]
per-ex loss: 0.614768  [  300/  300]
Train Error: Avg loss: 0.51314006
validation Error: 
 Avg loss: 0.49223493 
 F1: 0.482637 
 Precision: 0.409242 
 Recall: 0.588112
 IoU: 0.318076

test Error: 
 Avg loss: 0.45817834 
 F1: 0.542552 
 Precision: 0.455824 
 Recall: 0.670038
 IoU: 0.372262

We have finished training iteration 105
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_100_.pth
per-ex loss: 0.510601  [    3/  300]
per-ex loss: 0.493719  [    6/  300]
per-ex loss: 0.495584  [    9/  300]
per-ex loss: 0.428818  [   12/  300]
per-ex loss: 0.543795  [   15/  300]
per-ex loss: 0.478633  [   18/  300]
per-ex loss: 0.558048  [   21/  300]
per-ex loss: 0.496980  [   24/  300]
per-ex loss: 0.623411  [   27/  300]
per-ex loss: 0.516326  [   30/  300]
per-ex loss: 0.468751  [   33/  300]
per-ex loss: 0.438480  [   36/  300]
per-ex loss: 0.433060  [   39/  300]
per-ex loss: 0.514026  [   42/  300]
per-ex loss: 0.461450  [   45/  300]
per-ex loss: 0.585366  [   48/  300]
per-ex loss: 0.619695  [   51/  300]
per-ex loss: 0.608583  [   54/  300]
per-ex loss: 0.524756  [   57/  300]
per-ex loss: 0.503672  [   60/  300]
per-ex loss: 0.538578  [   63/  300]
per-ex loss: 0.539495  [   66/  300]
per-ex loss: 0.613321  [   69/  300]
per-ex loss: 0.481120  [   72/  300]
per-ex loss: 0.422735  [   75/  300]
per-ex loss: 0.419228  [   78/  300]
per-ex loss: 0.570761  [   81/  300]
per-ex loss: 0.512181  [   84/  300]
per-ex loss: 0.430378  [   87/  300]
per-ex loss: 0.539266  [   90/  300]
per-ex loss: 0.456508  [   93/  300]
per-ex loss: 0.542819  [   96/  300]
per-ex loss: 0.485709  [   99/  300]
per-ex loss: 0.515941  [  102/  300]
per-ex loss: 0.619864  [  105/  300]
per-ex loss: 0.571288  [  108/  300]
per-ex loss: 0.412048  [  111/  300]
per-ex loss: 0.427747  [  114/  300]
per-ex loss: 0.589257  [  117/  300]
per-ex loss: 0.397519  [  120/  300]
per-ex loss: 0.466434  [  123/  300]
per-ex loss: 0.560282  [  126/  300]
per-ex loss: 0.561856  [  129/  300]
per-ex loss: 0.569611  [  132/  300]
per-ex loss: 0.416153  [  135/  300]
per-ex loss: 0.503621  [  138/  300]
per-ex loss: 0.512147  [  141/  300]
per-ex loss: 0.606203  [  144/  300]
per-ex loss: 0.537543  [  147/  300]
per-ex loss: 0.456488  [  150/  300]
per-ex loss: 0.475568  [  153/  300]
per-ex loss: 0.480860  [  156/  300]
per-ex loss: 0.422049  [  159/  300]
per-ex loss: 0.528607  [  162/  300]
per-ex loss: 0.441578  [  165/  300]
per-ex loss: 0.522714  [  168/  300]
per-ex loss: 0.565253  [  171/  300]
per-ex loss: 0.427126  [  174/  300]
per-ex loss: 0.505486  [  177/  300]
per-ex loss: 0.402790  [  180/  300]
per-ex loss: 0.475778  [  183/  300]
per-ex loss: 0.472367  [  186/  300]
per-ex loss: 0.594693  [  189/  300]
per-ex loss: 0.551095  [  192/  300]
per-ex loss: 0.617813  [  195/  300]
per-ex loss: 0.466764  [  198/  300]
per-ex loss: 0.590679  [  201/  300]
per-ex loss: 0.461366  [  204/  300]
per-ex loss: 0.429764  [  207/  300]
per-ex loss: 0.498264  [  210/  300]
per-ex loss: 0.502944  [  213/  300]
per-ex loss: 0.501059  [  216/  300]
per-ex loss: 0.622035  [  219/  300]
per-ex loss: 0.549525  [  222/  300]
per-ex loss: 0.416427  [  225/  300]
per-ex loss: 0.654663  [  228/  300]
per-ex loss: 0.477498  [  231/  300]
per-ex loss: 0.575933  [  234/  300]
per-ex loss: 0.417245  [  237/  300]
per-ex loss: 0.426777  [  240/  300]
per-ex loss: 0.537280  [  243/  300]
per-ex loss: 0.568865  [  246/  300]
per-ex loss: 0.382728  [  249/  300]
per-ex loss: 0.509782  [  252/  300]
per-ex loss: 0.439032  [  255/  300]
per-ex loss: 0.521279  [  258/  300]
per-ex loss: 0.389419  [  261/  300]
per-ex loss: 0.573621  [  264/  300]
per-ex loss: 0.411647  [  267/  300]
per-ex loss: 0.535623  [  270/  300]
per-ex loss: 0.464615  [  273/  300]
per-ex loss: 0.450684  [  276/  300]
per-ex loss: 0.435332  [  279/  300]
per-ex loss: 0.518975  [  282/  300]
per-ex loss: 0.499479  [  285/  300]
per-ex loss: 0.520397  [  288/  300]
per-ex loss: 0.539523  [  291/  300]
per-ex loss: 0.465310  [  294/  300]
per-ex loss: 0.582052  [  297/  300]
per-ex loss: 0.558013  [  300/  300]
Train Error: Avg loss: 0.50558227
validation Error: 
 Avg loss: 0.49052914 
 F1: 0.486393 
 Precision: 0.409905 
 Recall: 0.597975
 IoU: 0.321347

test Error: 
 Avg loss: 0.46468544 
 F1: 0.536038 
 Precision: 0.452762 
 Recall: 0.656851
 IoU: 0.366156

We have finished training iteration 106
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_104_.pth
per-ex loss: 0.463218  [    3/  300]
per-ex loss: 0.510000  [    6/  300]
per-ex loss: 0.480611  [    9/  300]
per-ex loss: 0.566353  [   12/  300]
per-ex loss: 0.540195  [   15/  300]
per-ex loss: 0.440115  [   18/  300]
per-ex loss: 0.677837  [   21/  300]
per-ex loss: 0.588015  [   24/  300]
per-ex loss: 0.518934  [   27/  300]
per-ex loss: 0.427673  [   30/  300]
per-ex loss: 0.468012  [   33/  300]
per-ex loss: 0.414133  [   36/  300]
per-ex loss: 0.516564  [   39/  300]
per-ex loss: 0.499718  [   42/  300]
per-ex loss: 0.410623  [   45/  300]
per-ex loss: 0.571229  [   48/  300]
per-ex loss: 0.621129  [   51/  300]
per-ex loss: 0.621250  [   54/  300]
per-ex loss: 0.481546  [   57/  300]
per-ex loss: 0.611284  [   60/  300]
per-ex loss: 0.502024  [   63/  300]
per-ex loss: 0.461489  [   66/  300]
per-ex loss: 0.448207  [   69/  300]
per-ex loss: 0.428992  [   72/  300]
per-ex loss: 0.461118  [   75/  300]
per-ex loss: 0.565861  [   78/  300]
per-ex loss: 0.573099  [   81/  300]
per-ex loss: 0.546258  [   84/  300]
per-ex loss: 0.474918  [   87/  300]
per-ex loss: 0.546881  [   90/  300]
per-ex loss: 0.479765  [   93/  300]
per-ex loss: 0.521203  [   96/  300]
per-ex loss: 0.640159  [   99/  300]
per-ex loss: 0.597247  [  102/  300]
per-ex loss: 0.503929  [  105/  300]
per-ex loss: 0.478665  [  108/  300]
per-ex loss: 0.554008  [  111/  300]
per-ex loss: 0.594160  [  114/  300]
per-ex loss: 0.396787  [  117/  300]
per-ex loss: 0.625693  [  120/  300]
per-ex loss: 0.489041  [  123/  300]
per-ex loss: 0.421974  [  126/  300]
per-ex loss: 0.486755  [  129/  300]
per-ex loss: 0.450376  [  132/  300]
per-ex loss: 0.555845  [  135/  300]
per-ex loss: 0.475608  [  138/  300]
per-ex loss: 0.442723  [  141/  300]
per-ex loss: 0.438569  [  144/  300]
per-ex loss: 0.520106  [  147/  300]
per-ex loss: 0.654094  [  150/  300]
per-ex loss: 0.555743  [  153/  300]
per-ex loss: 0.526244  [  156/  300]
per-ex loss: 0.549649  [  159/  300]
per-ex loss: 0.481553  [  162/  300]
per-ex loss: 0.425359  [  165/  300]
per-ex loss: 0.497145  [  168/  300]
per-ex loss: 0.563861  [  171/  300]
per-ex loss: 0.464018  [  174/  300]
per-ex loss: 0.469785  [  177/  300]
per-ex loss: 0.568809  [  180/  300]
per-ex loss: 0.610583  [  183/  300]
per-ex loss: 0.455850  [  186/  300]
per-ex loss: 0.434954  [  189/  300]
per-ex loss: 0.482556  [  192/  300]
per-ex loss: 0.427523  [  195/  300]
per-ex loss: 0.547000  [  198/  300]
per-ex loss: 0.454044  [  201/  300]
per-ex loss: 0.487950  [  204/  300]
per-ex loss: 0.461098  [  207/  300]
per-ex loss: 0.427190  [  210/  300]
per-ex loss: 0.687128  [  213/  300]
per-ex loss: 0.594212  [  216/  300]
per-ex loss: 0.564596  [  219/  300]
per-ex loss: 0.563755  [  222/  300]
per-ex loss: 0.475779  [  225/  300]
per-ex loss: 0.573993  [  228/  300]
per-ex loss: 0.529809  [  231/  300]
per-ex loss: 0.499616  [  234/  300]
per-ex loss: 0.515004  [  237/  300]
per-ex loss: 0.466490  [  240/  300]
per-ex loss: 0.541849  [  243/  300]
per-ex loss: 0.503342  [  246/  300]
per-ex loss: 0.469474  [  249/  300]
per-ex loss: 0.424525  [  252/  300]
per-ex loss: 0.384310  [  255/  300]
per-ex loss: 0.561882  [  258/  300]
per-ex loss: 0.498623  [  261/  300]
per-ex loss: 0.434671  [  264/  300]
per-ex loss: 0.498489  [  267/  300]
per-ex loss: 0.452514  [  270/  300]
per-ex loss: 0.509081  [  273/  300]
per-ex loss: 0.438133  [  276/  300]
per-ex loss: 0.569829  [  279/  300]
per-ex loss: 0.454571  [  282/  300]
per-ex loss: 0.475261  [  285/  300]
per-ex loss: 0.490591  [  288/  300]
per-ex loss: 0.414449  [  291/  300]
per-ex loss: 0.473503  [  294/  300]
per-ex loss: 0.614086  [  297/  300]
per-ex loss: 0.533910  [  300/  300]
Train Error: Avg loss: 0.50868382
validation Error: 
 Avg loss: 0.50025050 
 F1: 0.474731 
 Precision: 0.396593 
 Recall: 0.591213
 IoU: 0.311244

test Error: 
 Avg loss: 0.47014147 
 F1: 0.530512 
 Precision: 0.433236 
 Recall: 0.684120
 IoU: 0.361018

We have finished training iteration 107
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_103_.pth
per-ex loss: 0.441825  [    3/  300]
per-ex loss: 0.608838  [    6/  300]
per-ex loss: 0.558707  [    9/  300]
per-ex loss: 0.579930  [   12/  300]
per-ex loss: 0.659542  [   15/  300]
per-ex loss: 0.515925  [   18/  300]
per-ex loss: 0.458844  [   21/  300]
per-ex loss: 0.624451  [   24/  300]
per-ex loss: 0.539900  [   27/  300]
per-ex loss: 0.431503  [   30/  300]
per-ex loss: 0.388860  [   33/  300]
per-ex loss: 0.508874  [   36/  300]
per-ex loss: 0.459917  [   39/  300]
per-ex loss: 0.553405  [   42/  300]
per-ex loss: 0.479147  [   45/  300]
per-ex loss: 0.396283  [   48/  300]
per-ex loss: 0.553030  [   51/  300]
per-ex loss: 0.432451  [   54/  300]
per-ex loss: 0.485969  [   57/  300]
per-ex loss: 0.465275  [   60/  300]
per-ex loss: 0.717444  [   63/  300]
per-ex loss: 0.449668  [   66/  300]
per-ex loss: 0.528202  [   69/  300]
per-ex loss: 0.612006  [   72/  300]
per-ex loss: 0.484469  [   75/  300]
per-ex loss: 0.452066  [   78/  300]
per-ex loss: 0.430432  [   81/  300]
per-ex loss: 0.567304  [   84/  300]
per-ex loss: 0.469731  [   87/  300]
per-ex loss: 0.513290  [   90/  300]
per-ex loss: 0.476562  [   93/  300]
per-ex loss: 0.406365  [   96/  300]
per-ex loss: 0.424755  [   99/  300]
per-ex loss: 0.648593  [  102/  300]
per-ex loss: 0.539554  [  105/  300]
per-ex loss: 0.436194  [  108/  300]
per-ex loss: 0.510398  [  111/  300]
per-ex loss: 0.473830  [  114/  300]
per-ex loss: 0.446212  [  117/  300]
per-ex loss: 0.516137  [  120/  300]
per-ex loss: 0.470763  [  123/  300]
per-ex loss: 0.727448  [  126/  300]
per-ex loss: 0.499928  [  129/  300]
per-ex loss: 0.514768  [  132/  300]
per-ex loss: 0.525929  [  135/  300]
per-ex loss: 0.415232  [  138/  300]
per-ex loss: 0.505806  [  141/  300]
per-ex loss: 0.524742  [  144/  300]
per-ex loss: 0.471178  [  147/  300]
per-ex loss: 0.579031  [  150/  300]
per-ex loss: 0.638409  [  153/  300]
per-ex loss: 0.456284  [  156/  300]
per-ex loss: 0.412831  [  159/  300]
per-ex loss: 0.497539  [  162/  300]
per-ex loss: 0.529720  [  165/  300]
per-ex loss: 0.535144  [  168/  300]
per-ex loss: 0.498355  [  171/  300]
per-ex loss: 0.500561  [  174/  300]
per-ex loss: 0.519839  [  177/  300]
per-ex loss: 0.417197  [  180/  300]
per-ex loss: 0.460215  [  183/  300]
per-ex loss: 0.434531  [  186/  300]
per-ex loss: 0.448570  [  189/  300]
per-ex loss: 0.617926  [  192/  300]
per-ex loss: 0.526448  [  195/  300]
per-ex loss: 0.456188  [  198/  300]
per-ex loss: 0.416625  [  201/  300]
per-ex loss: 0.469641  [  204/  300]
per-ex loss: 0.581840  [  207/  300]
per-ex loss: 0.488408  [  210/  300]
per-ex loss: 0.485728  [  213/  300]
per-ex loss: 0.468413  [  216/  300]
per-ex loss: 0.446187  [  219/  300]
per-ex loss: 0.555388  [  222/  300]
per-ex loss: 0.604894  [  225/  300]
per-ex loss: 0.460461  [  228/  300]
per-ex loss: 0.495364  [  231/  300]
per-ex loss: 0.512550  [  234/  300]
per-ex loss: 0.611059  [  237/  300]
per-ex loss: 0.490826  [  240/  300]
per-ex loss: 0.443887  [  243/  300]
per-ex loss: 0.479061  [  246/  300]
per-ex loss: 0.435821  [  249/  300]
per-ex loss: 0.432064  [  252/  300]
per-ex loss: 0.496234  [  255/  300]
per-ex loss: 0.638167  [  258/  300]
per-ex loss: 0.467131  [  261/  300]
per-ex loss: 0.583529  [  264/  300]
per-ex loss: 0.521991  [  267/  300]
per-ex loss: 0.442886  [  270/  300]
per-ex loss: 0.559389  [  273/  300]
per-ex loss: 0.500641  [  276/  300]
per-ex loss: 0.567152  [  279/  300]
per-ex loss: 0.412743  [  282/  300]
per-ex loss: 0.465131  [  285/  300]
per-ex loss: 0.450441  [  288/  300]
per-ex loss: 0.612022  [  291/  300]
per-ex loss: 0.409846  [  294/  300]
per-ex loss: 0.496805  [  297/  300]
per-ex loss: 0.449938  [  300/  300]
Train Error: Avg loss: 0.50382726
validation Error: 
 Avg loss: 0.49899266 
 F1: 0.483866 
 Precision: 0.432508 
 Recall: 0.549064
 IoU: 0.319145

test Error: 
 Avg loss: 0.45379001 
 F1: 0.546976 
 Precision: 0.485558 
 Recall: 0.626181
 IoU: 0.376439

We have finished training iteration 108
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_102_.pth
per-ex loss: 0.436169  [    3/  300]
per-ex loss: 0.717321  [    6/  300]
per-ex loss: 0.474654  [    9/  300]
per-ex loss: 0.570217  [   12/  300]
per-ex loss: 0.410617  [   15/  300]
per-ex loss: 0.590148  [   18/  300]
per-ex loss: 0.526755  [   21/  300]
per-ex loss: 0.513161  [   24/  300]
per-ex loss: 0.596280  [   27/  300]
per-ex loss: 0.423792  [   30/  300]
per-ex loss: 0.632228  [   33/  300]
per-ex loss: 0.509634  [   36/  300]
per-ex loss: 0.535082  [   39/  300]
per-ex loss: 0.624040  [   42/  300]
per-ex loss: 0.508178  [   45/  300]
per-ex loss: 0.474099  [   48/  300]
per-ex loss: 0.564036  [   51/  300]
per-ex loss: 0.449364  [   54/  300]
per-ex loss: 0.480950  [   57/  300]
per-ex loss: 0.477890  [   60/  300]
per-ex loss: 0.453309  [   63/  300]
per-ex loss: 0.444560  [   66/  300]
per-ex loss: 0.452213  [   69/  300]
per-ex loss: 0.476110  [   72/  300]
per-ex loss: 0.420437  [   75/  300]
per-ex loss: 0.527208  [   78/  300]
per-ex loss: 0.478525  [   81/  300]
per-ex loss: 0.507671  [   84/  300]
per-ex loss: 0.425095  [   87/  300]
per-ex loss: 0.579054  [   90/  300]
per-ex loss: 0.522508  [   93/  300]
per-ex loss: 0.471710  [   96/  300]
per-ex loss: 0.546223  [   99/  300]
per-ex loss: 0.451085  [  102/  300]
per-ex loss: 0.486348  [  105/  300]
per-ex loss: 0.489199  [  108/  300]
per-ex loss: 0.444470  [  111/  300]
per-ex loss: 0.422094  [  114/  300]
per-ex loss: 0.526301  [  117/  300]
per-ex loss: 0.391914  [  120/  300]
per-ex loss: 0.644109  [  123/  300]
per-ex loss: 0.479869  [  126/  300]
per-ex loss: 0.401353  [  129/  300]
per-ex loss: 0.502032  [  132/  300]
per-ex loss: 0.434593  [  135/  300]
per-ex loss: 0.592487  [  138/  300]
per-ex loss: 0.641753  [  141/  300]
per-ex loss: 0.388681  [  144/  300]
per-ex loss: 0.509929  [  147/  300]
per-ex loss: 0.590138  [  150/  300]
per-ex loss: 0.517776  [  153/  300]
per-ex loss: 0.452908  [  156/  300]
per-ex loss: 0.607213  [  159/  300]
per-ex loss: 0.547038  [  162/  300]
per-ex loss: 0.504634  [  165/  300]
per-ex loss: 0.603299  [  168/  300]
per-ex loss: 0.469215  [  171/  300]
per-ex loss: 0.543467  [  174/  300]
per-ex loss: 0.470302  [  177/  300]
per-ex loss: 0.561310  [  180/  300]
per-ex loss: 0.470523  [  183/  300]
per-ex loss: 0.577897  [  186/  300]
per-ex loss: 0.558163  [  189/  300]
per-ex loss: 0.450336  [  192/  300]
per-ex loss: 0.464999  [  195/  300]
per-ex loss: 0.513633  [  198/  300]
per-ex loss: 0.479635  [  201/  300]
per-ex loss: 0.482076  [  204/  300]
per-ex loss: 0.613697  [  207/  300]
per-ex loss: 0.474346  [  210/  300]
per-ex loss: 0.585393  [  213/  300]
per-ex loss: 0.568221  [  216/  300]
per-ex loss: 0.478567  [  219/  300]
per-ex loss: 0.466964  [  222/  300]
per-ex loss: 0.469444  [  225/  300]
per-ex loss: 0.404123  [  228/  300]
per-ex loss: 0.631106  [  231/  300]
per-ex loss: 0.662812  [  234/  300]
per-ex loss: 0.462802  [  237/  300]
per-ex loss: 0.601557  [  240/  300]
per-ex loss: 0.399919  [  243/  300]
per-ex loss: 0.467556  [  246/  300]
per-ex loss: 0.454906  [  249/  300]
per-ex loss: 0.421603  [  252/  300]
per-ex loss: 0.409425  [  255/  300]
per-ex loss: 0.536599  [  258/  300]
per-ex loss: 0.594774  [  261/  300]
per-ex loss: 0.492668  [  264/  300]
per-ex loss: 0.508253  [  267/  300]
per-ex loss: 0.418924  [  270/  300]
per-ex loss: 0.568798  [  273/  300]
per-ex loss: 0.470883  [  276/  300]
per-ex loss: 0.583914  [  279/  300]
per-ex loss: 0.618403  [  282/  300]
per-ex loss: 0.656586  [  285/  300]
per-ex loss: 0.466657  [  288/  300]
per-ex loss: 0.453053  [  291/  300]
per-ex loss: 0.542630  [  294/  300]
per-ex loss: 0.533492  [  297/  300]
per-ex loss: 0.463886  [  300/  300]
Train Error: Avg loss: 0.50971978
validation Error: 
 Avg loss: 0.49330964 
 F1: 0.489345 
 Precision: 0.434942 
 Recall: 0.559303
 IoU: 0.323929

test Error: 
 Avg loss: 0.45630974 
 F1: 0.544442 
 Precision: 0.485196 
 Recall: 0.620169
 IoU: 0.374043

We have finished training iteration 109
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_107_.pth
per-ex loss: 0.492482  [    3/  300]
per-ex loss: 0.391057  [    6/  300]
per-ex loss: 0.402998  [    9/  300]
per-ex loss: 0.416696  [   12/  300]
per-ex loss: 0.494942  [   15/  300]
per-ex loss: 0.556664  [   18/  300]
per-ex loss: 0.470066  [   21/  300]
per-ex loss: 0.487213  [   24/  300]
per-ex loss: 0.440151  [   27/  300]
per-ex loss: 0.417860  [   30/  300]
per-ex loss: 0.545752  [   33/  300]
per-ex loss: 0.576035  [   36/  300]
per-ex loss: 0.468327  [   39/  300]
per-ex loss: 0.440666  [   42/  300]
per-ex loss: 0.634722  [   45/  300]
per-ex loss: 0.623481  [   48/  300]
per-ex loss: 0.598071  [   51/  300]
per-ex loss: 0.464948  [   54/  300]
per-ex loss: 0.599146  [   57/  300]
per-ex loss: 0.441134  [   60/  300]
per-ex loss: 0.560084  [   63/  300]
per-ex loss: 0.424551  [   66/  300]
per-ex loss: 0.629146  [   69/  300]
per-ex loss: 0.627208  [   72/  300]
per-ex loss: 0.552208  [   75/  300]
per-ex loss: 0.471499  [   78/  300]
per-ex loss: 0.440697  [   81/  300]
per-ex loss: 0.537912  [   84/  300]
per-ex loss: 0.409609  [   87/  300]
per-ex loss: 0.533484  [   90/  300]
per-ex loss: 0.428080  [   93/  300]
per-ex loss: 0.420680  [   96/  300]
per-ex loss: 0.404036  [   99/  300]
per-ex loss: 0.444338  [  102/  300]
per-ex loss: 0.464004  [  105/  300]
per-ex loss: 0.480162  [  108/  300]
per-ex loss: 0.479739  [  111/  300]
per-ex loss: 0.604317  [  114/  300]
per-ex loss: 0.588489  [  117/  300]
per-ex loss: 0.577589  [  120/  300]
per-ex loss: 0.618274  [  123/  300]
per-ex loss: 0.587003  [  126/  300]
per-ex loss: 0.612716  [  129/  300]
per-ex loss: 0.437554  [  132/  300]
per-ex loss: 0.473234  [  135/  300]
per-ex loss: 0.493746  [  138/  300]
per-ex loss: 0.484139  [  141/  300]
per-ex loss: 0.499833  [  144/  300]
per-ex loss: 0.414048  [  147/  300]
per-ex loss: 0.390505  [  150/  300]
per-ex loss: 0.480906  [  153/  300]
per-ex loss: 0.452736  [  156/  300]
per-ex loss: 0.501108  [  159/  300]
per-ex loss: 0.534275  [  162/  300]
per-ex loss: 0.531002  [  165/  300]
per-ex loss: 0.677718  [  168/  300]
per-ex loss: 0.499721  [  171/  300]
per-ex loss: 0.516927  [  174/  300]
per-ex loss: 0.449557  [  177/  300]
per-ex loss: 0.438231  [  180/  300]
per-ex loss: 0.450414  [  183/  300]
per-ex loss: 0.435040  [  186/  300]
per-ex loss: 0.501261  [  189/  300]
per-ex loss: 0.427672  [  192/  300]
per-ex loss: 0.582744  [  195/  300]
per-ex loss: 0.577964  [  198/  300]
per-ex loss: 0.425968  [  201/  300]
per-ex loss: 0.534705  [  204/  300]
per-ex loss: 0.475194  [  207/  300]
per-ex loss: 0.517537  [  210/  300]
per-ex loss: 0.520244  [  213/  300]
per-ex loss: 0.564136  [  216/  300]
per-ex loss: 0.447626  [  219/  300]
per-ex loss: 0.579441  [  222/  300]
per-ex loss: 0.526769  [  225/  300]
per-ex loss: 0.500669  [  228/  300]
per-ex loss: 0.538385  [  231/  300]
per-ex loss: 0.476298  [  234/  300]
per-ex loss: 0.570812  [  237/  300]
per-ex loss: 0.453996  [  240/  300]
per-ex loss: 0.538987  [  243/  300]
per-ex loss: 0.465148  [  246/  300]
per-ex loss: 0.409621  [  249/  300]
per-ex loss: 0.613012  [  252/  300]
per-ex loss: 0.481179  [  255/  300]
per-ex loss: 0.468531  [  258/  300]
per-ex loss: 0.521056  [  261/  300]
per-ex loss: 0.535095  [  264/  300]
per-ex loss: 0.446057  [  267/  300]
per-ex loss: 0.604248  [  270/  300]
per-ex loss: 0.436703  [  273/  300]
per-ex loss: 0.580657  [  276/  300]
per-ex loss: 0.418613  [  279/  300]
per-ex loss: 0.442502  [  282/  300]
per-ex loss: 0.485931  [  285/  300]
per-ex loss: 0.419150  [  288/  300]
per-ex loss: 0.521571  [  291/  300]
per-ex loss: 0.537169  [  294/  300]
per-ex loss: 0.491014  [  297/  300]
per-ex loss: 0.437761  [  300/  300]
Train Error: Avg loss: 0.50094356
validation Error: 
 Avg loss: 0.50600334 
 F1: 0.480955 
 Precision: 0.452021 
 Recall: 0.513846
 IoU: 0.316617

test Error: 
 Avg loss: 0.45087081 
 F1: 0.549866 
 Precision: 0.506702 
 Recall: 0.601068
 IoU: 0.379183

We have finished training iteration 110
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_108_.pth
per-ex loss: 0.610363  [    3/  300]
per-ex loss: 0.437484  [    6/  300]
per-ex loss: 0.466064  [    9/  300]
per-ex loss: 0.403955  [   12/  300]
per-ex loss: 0.429273  [   15/  300]
per-ex loss: 0.476656  [   18/  300]
per-ex loss: 0.501799  [   21/  300]
per-ex loss: 0.506222  [   24/  300]
per-ex loss: 0.622469  [   27/  300]
per-ex loss: 0.459300  [   30/  300]
per-ex loss: 0.488265  [   33/  300]
per-ex loss: 0.393879  [   36/  300]
per-ex loss: 0.480721  [   39/  300]
per-ex loss: 0.418741  [   42/  300]
per-ex loss: 0.491889  [   45/  300]
per-ex loss: 0.552282  [   48/  300]
per-ex loss: 0.526472  [   51/  300]
per-ex loss: 0.491645  [   54/  300]
per-ex loss: 0.562377  [   57/  300]
per-ex loss: 0.566446  [   60/  300]
per-ex loss: 0.577041  [   63/  300]
per-ex loss: 0.566070  [   66/  300]
per-ex loss: 0.405020  [   69/  300]
per-ex loss: 0.488974  [   72/  300]
per-ex loss: 0.482739  [   75/  300]
per-ex loss: 0.647475  [   78/  300]
per-ex loss: 0.533481  [   81/  300]
per-ex loss: 0.502979  [   84/  300]
per-ex loss: 0.543091  [   87/  300]
per-ex loss: 0.476159  [   90/  300]
per-ex loss: 0.548689  [   93/  300]
per-ex loss: 0.495227  [   96/  300]
per-ex loss: 0.422146  [   99/  300]
per-ex loss: 0.471197  [  102/  300]
per-ex loss: 0.410311  [  105/  300]
per-ex loss: 0.532901  [  108/  300]
per-ex loss: 0.564105  [  111/  300]
per-ex loss: 0.490176  [  114/  300]
per-ex loss: 0.530279  [  117/  300]
per-ex loss: 0.551187  [  120/  300]
per-ex loss: 0.673357  [  123/  300]
per-ex loss: 0.515568  [  126/  300]
per-ex loss: 0.408518  [  129/  300]
per-ex loss: 0.473540  [  132/  300]
per-ex loss: 0.584881  [  135/  300]
per-ex loss: 0.407329  [  138/  300]
per-ex loss: 0.507288  [  141/  300]
per-ex loss: 0.497860  [  144/  300]
per-ex loss: 0.478685  [  147/  300]
per-ex loss: 0.481900  [  150/  300]
per-ex loss: 0.493547  [  153/  300]
per-ex loss: 0.469692  [  156/  300]
per-ex loss: 0.499950  [  159/  300]
per-ex loss: 0.504943  [  162/  300]
per-ex loss: 0.535158  [  165/  300]
per-ex loss: 0.558932  [  168/  300]
per-ex loss: 0.580906  [  171/  300]
per-ex loss: 0.443854  [  174/  300]
per-ex loss: 0.475632  [  177/  300]
per-ex loss: 0.461073  [  180/  300]
per-ex loss: 0.552698  [  183/  300]
per-ex loss: 0.605219  [  186/  300]
per-ex loss: 0.517715  [  189/  300]
per-ex loss: 0.534487  [  192/  300]
per-ex loss: 0.526192  [  195/  300]
per-ex loss: 0.557002  [  198/  300]
per-ex loss: 0.460537  [  201/  300]
per-ex loss: 0.598032  [  204/  300]
per-ex loss: 0.460595  [  207/  300]
per-ex loss: 0.523548  [  210/  300]
per-ex loss: 0.565550  [  213/  300]
per-ex loss: 0.458801  [  216/  300]
per-ex loss: 0.408909  [  219/  300]
per-ex loss: 0.455351  [  222/  300]
per-ex loss: 0.523439  [  225/  300]
per-ex loss: 0.514499  [  228/  300]
per-ex loss: 0.621913  [  231/  300]
per-ex loss: 0.483471  [  234/  300]
per-ex loss: 0.432509  [  237/  300]
per-ex loss: 0.504870  [  240/  300]
per-ex loss: 0.462266  [  243/  300]
per-ex loss: 0.478202  [  246/  300]
per-ex loss: 0.537514  [  249/  300]
per-ex loss: 0.478834  [  252/  300]
per-ex loss: 0.543767  [  255/  300]
per-ex loss: 0.419600  [  258/  300]
per-ex loss: 0.468822  [  261/  300]
per-ex loss: 0.460577  [  264/  300]
per-ex loss: 0.492497  [  267/  300]
per-ex loss: 0.611056  [  270/  300]
per-ex loss: 0.403236  [  273/  300]
per-ex loss: 0.439227  [  276/  300]
per-ex loss: 0.469148  [  279/  300]
per-ex loss: 0.544676  [  282/  300]
per-ex loss: 0.449881  [  285/  300]
per-ex loss: 0.542643  [  288/  300]
per-ex loss: 0.495510  [  291/  300]
per-ex loss: 0.402497  [  294/  300]
per-ex loss: 0.495077  [  297/  300]
per-ex loss: 0.433686  [  300/  300]
Train Error: Avg loss: 0.50112221
validation Error: 
 Avg loss: 0.49953673 
 F1: 0.485389 
 Precision: 0.439518 
 Recall: 0.541950
 IoU: 0.320471

test Error: 
 Avg loss: 0.45513737 
 F1: 0.545740 
 Precision: 0.494674 
 Recall: 0.608561
 IoU: 0.375270

We have finished training iteration 111
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_109_.pth
per-ex loss: 0.617046  [    3/  300]
per-ex loss: 0.416684  [    6/  300]
per-ex loss: 0.508736  [    9/  300]
per-ex loss: 0.427858  [   12/  300]
per-ex loss: 0.472985  [   15/  300]
per-ex loss: 0.470914  [   18/  300]
per-ex loss: 0.545118  [   21/  300]
per-ex loss: 0.462647  [   24/  300]
per-ex loss: 0.440242  [   27/  300]
per-ex loss: 0.470835  [   30/  300]
per-ex loss: 0.456196  [   33/  300]
per-ex loss: 0.598100  [   36/  300]
per-ex loss: 0.453432  [   39/  300]
per-ex loss: 0.604573  [   42/  300]
per-ex loss: 0.509881  [   45/  300]
per-ex loss: 0.611482  [   48/  300]
per-ex loss: 0.548170  [   51/  300]
per-ex loss: 0.511753  [   54/  300]
per-ex loss: 0.584447  [   57/  300]
per-ex loss: 0.543054  [   60/  300]
per-ex loss: 0.515442  [   63/  300]
per-ex loss: 0.520912  [   66/  300]
per-ex loss: 0.458331  [   69/  300]
per-ex loss: 0.444474  [   72/  300]
per-ex loss: 0.527955  [   75/  300]
per-ex loss: 0.503267  [   78/  300]
per-ex loss: 0.536588  [   81/  300]
per-ex loss: 0.509414  [   84/  300]
per-ex loss: 0.389391  [   87/  300]
per-ex loss: 0.623767  [   90/  300]
per-ex loss: 0.617559  [   93/  300]
per-ex loss: 0.417869  [   96/  300]
per-ex loss: 0.570441  [   99/  300]
per-ex loss: 0.587743  [  102/  300]
per-ex loss: 0.477926  [  105/  300]
per-ex loss: 0.487864  [  108/  300]
per-ex loss: 0.395049  [  111/  300]
per-ex loss: 0.492878  [  114/  300]
per-ex loss: 0.457558  [  117/  300]
per-ex loss: 0.430861  [  120/  300]
per-ex loss: 0.626468  [  123/  300]
per-ex loss: 0.542322  [  126/  300]
per-ex loss: 0.653309  [  129/  300]
per-ex loss: 0.492168  [  132/  300]
per-ex loss: 0.509682  [  135/  300]
per-ex loss: 0.399674  [  138/  300]
per-ex loss: 0.433590  [  141/  300]
per-ex loss: 0.602595  [  144/  300]
per-ex loss: 0.588557  [  147/  300]
per-ex loss: 0.568888  [  150/  300]
per-ex loss: 0.454438  [  153/  300]
per-ex loss: 0.465532  [  156/  300]
per-ex loss: 0.426484  [  159/  300]
per-ex loss: 0.471482  [  162/  300]
per-ex loss: 0.503887  [  165/  300]
per-ex loss: 0.453436  [  168/  300]
per-ex loss: 0.516045  [  171/  300]
per-ex loss: 0.435922  [  174/  300]
per-ex loss: 0.445665  [  177/  300]
per-ex loss: 0.511616  [  180/  300]
per-ex loss: 0.620302  [  183/  300]
per-ex loss: 0.458683  [  186/  300]
per-ex loss: 0.586623  [  189/  300]
per-ex loss: 0.466772  [  192/  300]
per-ex loss: 0.564445  [  195/  300]
per-ex loss: 0.440016  [  198/  300]
per-ex loss: 0.549695  [  201/  300]
per-ex loss: 0.413389  [  204/  300]
per-ex loss: 0.403172  [  207/  300]
per-ex loss: 0.413005  [  210/  300]
per-ex loss: 0.439152  [  213/  300]
per-ex loss: 0.448147  [  216/  300]
per-ex loss: 0.586360  [  219/  300]
per-ex loss: 0.557030  [  222/  300]
per-ex loss: 0.563539  [  225/  300]
per-ex loss: 0.441546  [  228/  300]
per-ex loss: 0.481194  [  231/  300]
per-ex loss: 0.626146  [  234/  300]
per-ex loss: 0.483069  [  237/  300]
per-ex loss: 0.624447  [  240/  300]
per-ex loss: 0.484079  [  243/  300]
per-ex loss: 0.527275  [  246/  300]
per-ex loss: 0.456202  [  249/  300]
per-ex loss: 0.563283  [  252/  300]
per-ex loss: 0.450263  [  255/  300]
per-ex loss: 0.491401  [  258/  300]
per-ex loss: 0.621582  [  261/  300]
per-ex loss: 0.427587  [  264/  300]
per-ex loss: 0.552386  [  267/  300]
per-ex loss: 0.607308  [  270/  300]
per-ex loss: 0.590512  [  273/  300]
per-ex loss: 0.484358  [  276/  300]
per-ex loss: 0.495665  [  279/  300]
per-ex loss: 0.472814  [  282/  300]
per-ex loss: 0.431683  [  285/  300]
per-ex loss: 0.498316  [  288/  300]
per-ex loss: 0.525899  [  291/  300]
per-ex loss: 0.446597  [  294/  300]
per-ex loss: 0.532106  [  297/  300]
per-ex loss: 0.498178  [  300/  300]
Train Error: Avg loss: 0.50645428
validation Error: 
 Avg loss: 0.49859524 
 F1: 0.486706 
 Precision: 0.441147 
 Recall: 0.542760
 IoU: 0.321621

test Error: 
 Avg loss: 0.45457304 
 F1: 0.546080 
 Precision: 0.496495 
 Recall: 0.606668
 IoU: 0.375592

We have finished training iteration 112
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_110_.pth
per-ex loss: 0.524425  [    3/  300]
per-ex loss: 0.478703  [    6/  300]
per-ex loss: 0.438858  [    9/  300]
per-ex loss: 0.389295  [   12/  300]
per-ex loss: 0.588909  [   15/  300]
per-ex loss: 0.472645  [   18/  300]
per-ex loss: 0.600031  [   21/  300]
per-ex loss: 0.490923  [   24/  300]
per-ex loss: 0.522851  [   27/  300]
per-ex loss: 0.519486  [   30/  300]
per-ex loss: 0.440005  [   33/  300]
per-ex loss: 0.468303  [   36/  300]
per-ex loss: 0.461059  [   39/  300]
per-ex loss: 0.442493  [   42/  300]
per-ex loss: 0.492992  [   45/  300]
per-ex loss: 0.479892  [   48/  300]
per-ex loss: 0.448893  [   51/  300]
per-ex loss: 0.552637  [   54/  300]
per-ex loss: 0.554510  [   57/  300]
per-ex loss: 0.590804  [   60/  300]
per-ex loss: 0.502955  [   63/  300]
per-ex loss: 0.412892  [   66/  300]
per-ex loss: 0.538917  [   69/  300]
per-ex loss: 0.491310  [   72/  300]
per-ex loss: 0.605647  [   75/  300]
per-ex loss: 0.609843  [   78/  300]
per-ex loss: 0.571067  [   81/  300]
per-ex loss: 0.522826  [   84/  300]
per-ex loss: 0.488331  [   87/  300]
per-ex loss: 0.407523  [   90/  300]
per-ex loss: 0.422481  [   93/  300]
per-ex loss: 0.454093  [   96/  300]
per-ex loss: 0.497171  [   99/  300]
per-ex loss: 0.465432  [  102/  300]
per-ex loss: 0.455984  [  105/  300]
per-ex loss: 0.491026  [  108/  300]
per-ex loss: 0.468378  [  111/  300]
per-ex loss: 0.645689  [  114/  300]
per-ex loss: 0.427493  [  117/  300]
per-ex loss: 0.555984  [  120/  300]
per-ex loss: 0.505758  [  123/  300]
per-ex loss: 0.524434  [  126/  300]
per-ex loss: 0.507047  [  129/  300]
per-ex loss: 0.517152  [  132/  300]
per-ex loss: 0.440601  [  135/  300]
per-ex loss: 0.589319  [  138/  300]
per-ex loss: 0.578733  [  141/  300]
per-ex loss: 0.394996  [  144/  300]
per-ex loss: 0.422388  [  147/  300]
per-ex loss: 0.447693  [  150/  300]
per-ex loss: 0.459717  [  153/  300]
per-ex loss: 0.548479  [  156/  300]
per-ex loss: 0.573141  [  159/  300]
per-ex loss: 0.653785  [  162/  300]
per-ex loss: 0.535651  [  165/  300]
per-ex loss: 0.521582  [  168/  300]
per-ex loss: 0.539030  [  171/  300]
per-ex loss: 0.605372  [  174/  300]
per-ex loss: 0.587131  [  177/  300]
per-ex loss: 0.430937  [  180/  300]
per-ex loss: 0.442693  [  183/  300]
per-ex loss: 0.487629  [  186/  300]
per-ex loss: 0.474105  [  189/  300]
per-ex loss: 0.540285  [  192/  300]
per-ex loss: 0.478899  [  195/  300]
per-ex loss: 0.535209  [  198/  300]
per-ex loss: 0.623348  [  201/  300]
per-ex loss: 0.520016  [  204/  300]
per-ex loss: 0.461435  [  207/  300]
per-ex loss: 0.420919  [  210/  300]
per-ex loss: 0.442100  [  213/  300]
per-ex loss: 0.422621  [  216/  300]
per-ex loss: 0.515450  [  219/  300]
per-ex loss: 0.448814  [  222/  300]
per-ex loss: 0.481792  [  225/  300]
per-ex loss: 0.528101  [  228/  300]
per-ex loss: 0.535482  [  231/  300]
per-ex loss: 0.648324  [  234/  300]
per-ex loss: 0.560209  [  237/  300]
per-ex loss: 0.493929  [  240/  300]
per-ex loss: 0.533546  [  243/  300]
per-ex loss: 0.482605  [  246/  300]
per-ex loss: 0.451659  [  249/  300]
per-ex loss: 0.569164  [  252/  300]
per-ex loss: 0.558244  [  255/  300]
per-ex loss: 0.417334  [  258/  300]
per-ex loss: 0.481896  [  261/  300]
per-ex loss: 0.594354  [  264/  300]
per-ex loss: 0.461735  [  267/  300]
per-ex loss: 0.721036  [  270/  300]
per-ex loss: 0.578957  [  273/  300]
per-ex loss: 0.413484  [  276/  300]
per-ex loss: 0.451665  [  279/  300]
per-ex loss: 0.457012  [  282/  300]
per-ex loss: 0.514004  [  285/  300]
per-ex loss: 0.489027  [  288/  300]
per-ex loss: 0.474103  [  291/  300]
per-ex loss: 0.506831  [  294/  300]
per-ex loss: 0.655190  [  297/  300]
per-ex loss: 0.475013  [  300/  300]
Train Error: Avg loss: 0.50723928
validation Error: 
 Avg loss: 0.49480772 
 F1: 0.484524 
 Precision: 0.416440 
 Recall: 0.579221
 IoU: 0.319717

test Error: 
 Avg loss: 0.46299732 
 F1: 0.537578 
 Precision: 0.461290 
 Recall: 0.644098
 IoU: 0.367594

We have finished training iteration 113
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_111_.pth
per-ex loss: 0.520295  [    3/  300]
per-ex loss: 0.421081  [    6/  300]
per-ex loss: 0.476943  [    9/  300]
per-ex loss: 0.519295  [   12/  300]
per-ex loss: 0.660610  [   15/  300]
per-ex loss: 0.471307  [   18/  300]
per-ex loss: 0.486038  [   21/  300]
per-ex loss: 0.529145  [   24/  300]
per-ex loss: 0.523887  [   27/  300]
per-ex loss: 0.441295  [   30/  300]
per-ex loss: 0.514182  [   33/  300]
per-ex loss: 0.469603  [   36/  300]
per-ex loss: 0.449061  [   39/  300]
per-ex loss: 0.567589  [   42/  300]
per-ex loss: 0.617842  [   45/  300]
per-ex loss: 0.403565  [   48/  300]
per-ex loss: 0.460743  [   51/  300]
per-ex loss: 0.450257  [   54/  300]
per-ex loss: 0.509904  [   57/  300]
per-ex loss: 0.574761  [   60/  300]
per-ex loss: 0.570026  [   63/  300]
per-ex loss: 0.467427  [   66/  300]
per-ex loss: 0.522604  [   69/  300]
per-ex loss: 0.444976  [   72/  300]
per-ex loss: 0.458880  [   75/  300]
per-ex loss: 0.473949  [   78/  300]
per-ex loss: 0.649584  [   81/  300]
per-ex loss: 0.519381  [   84/  300]
per-ex loss: 0.531951  [   87/  300]
per-ex loss: 0.431516  [   90/  300]
per-ex loss: 0.422582  [   93/  300]
per-ex loss: 0.421048  [   96/  300]
per-ex loss: 0.437929  [   99/  300]
per-ex loss: 0.496273  [  102/  300]
per-ex loss: 0.474701  [  105/  300]
per-ex loss: 0.423998  [  108/  300]
per-ex loss: 0.503468  [  111/  300]
per-ex loss: 0.504434  [  114/  300]
per-ex loss: 0.475751  [  117/  300]
per-ex loss: 0.419591  [  120/  300]
per-ex loss: 0.482399  [  123/  300]
per-ex loss: 0.431427  [  126/  300]
per-ex loss: 0.500405  [  129/  300]
per-ex loss: 0.490592  [  132/  300]
per-ex loss: 0.528370  [  135/  300]
per-ex loss: 0.501176  [  138/  300]
per-ex loss: 0.606664  [  141/  300]
per-ex loss: 0.505481  [  144/  300]
per-ex loss: 0.681718  [  147/  300]
per-ex loss: 0.514960  [  150/  300]
per-ex loss: 0.602863  [  153/  300]
per-ex loss: 0.586730  [  156/  300]
per-ex loss: 0.626972  [  159/  300]
per-ex loss: 0.508983  [  162/  300]
per-ex loss: 0.508384  [  165/  300]
per-ex loss: 0.425443  [  168/  300]
per-ex loss: 0.505574  [  171/  300]
per-ex loss: 0.521616  [  174/  300]
per-ex loss: 0.425785  [  177/  300]
per-ex loss: 0.470240  [  180/  300]
per-ex loss: 0.464235  [  183/  300]
per-ex loss: 0.512574  [  186/  300]
per-ex loss: 0.614502  [  189/  300]
per-ex loss: 0.639337  [  192/  300]
per-ex loss: 0.529069  [  195/  300]
per-ex loss: 0.475419  [  198/  300]
per-ex loss: 0.593406  [  201/  300]
per-ex loss: 0.582569  [  204/  300]
per-ex loss: 0.534140  [  207/  300]
per-ex loss: 0.418259  [  210/  300]
per-ex loss: 0.467863  [  213/  300]
per-ex loss: 0.457492  [  216/  300]
per-ex loss: 0.507113  [  219/  300]
per-ex loss: 0.549742  [  222/  300]
per-ex loss: 0.530628  [  225/  300]
per-ex loss: 0.526829  [  228/  300]
per-ex loss: 0.443437  [  231/  300]
per-ex loss: 0.521452  [  234/  300]
per-ex loss: 0.401887  [  237/  300]
per-ex loss: 0.461230  [  240/  300]
per-ex loss: 0.490873  [  243/  300]
per-ex loss: 0.457120  [  246/  300]
per-ex loss: 0.642945  [  249/  300]
per-ex loss: 0.609356  [  252/  300]
per-ex loss: 0.434617  [  255/  300]
per-ex loss: 0.421941  [  258/  300]
per-ex loss: 0.566557  [  261/  300]
per-ex loss: 0.507182  [  264/  300]
per-ex loss: 0.605246  [  267/  300]
per-ex loss: 0.461346  [  270/  300]
per-ex loss: 0.569315  [  273/  300]
per-ex loss: 0.463543  [  276/  300]
per-ex loss: 0.464599  [  279/  300]
per-ex loss: 0.418091  [  282/  300]
per-ex loss: 0.485147  [  285/  300]
per-ex loss: 0.458756  [  288/  300]
per-ex loss: 0.568307  [  291/  300]
per-ex loss: 0.593057  [  294/  300]
per-ex loss: 0.529679  [  297/  300]
per-ex loss: 0.628396  [  300/  300]
Train Error: Avg loss: 0.50750511
validation Error: 
 Avg loss: 0.49863249 
 F1: 0.478606 
 Precision: 0.414863 
 Recall: 0.565495
 IoU: 0.314584

test Error: 
 Avg loss: 0.46110880 
 F1: 0.539571 
 Precision: 0.456693 
 Recall: 0.659199
 IoU: 0.369461

We have finished training iteration 114
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_112_.pth
per-ex loss: 0.549313  [    3/  300]
per-ex loss: 0.510838  [    6/  300]
per-ex loss: 0.464107  [    9/  300]
per-ex loss: 0.409280  [   12/  300]
per-ex loss: 0.402213  [   15/  300]
per-ex loss: 0.412925  [   18/  300]
per-ex loss: 0.518458  [   21/  300]
per-ex loss: 0.533900  [   24/  300]
per-ex loss: 0.525440  [   27/  300]
per-ex loss: 0.433558  [   30/  300]
per-ex loss: 0.476739  [   33/  300]
per-ex loss: 0.429618  [   36/  300]
per-ex loss: 0.606083  [   39/  300]
per-ex loss: 0.405086  [   42/  300]
per-ex loss: 0.500398  [   45/  300]
per-ex loss: 0.647479  [   48/  300]
per-ex loss: 0.568047  [   51/  300]
per-ex loss: 0.558322  [   54/  300]
per-ex loss: 0.647478  [   57/  300]
per-ex loss: 0.480765  [   60/  300]
per-ex loss: 0.463808  [   63/  300]
per-ex loss: 0.458639  [   66/  300]
per-ex loss: 0.620641  [   69/  300]
per-ex loss: 0.600676  [   72/  300]
per-ex loss: 0.429499  [   75/  300]
per-ex loss: 0.473118  [   78/  300]
per-ex loss: 0.695717  [   81/  300]
per-ex loss: 0.437935  [   84/  300]
per-ex loss: 0.559273  [   87/  300]
per-ex loss: 0.647346  [   90/  300]
per-ex loss: 0.573427  [   93/  300]
per-ex loss: 0.484897  [   96/  300]
per-ex loss: 0.460009  [   99/  300]
per-ex loss: 0.510083  [  102/  300]
per-ex loss: 0.445892  [  105/  300]
per-ex loss: 0.578927  [  108/  300]
per-ex loss: 0.452739  [  111/  300]
per-ex loss: 0.436955  [  114/  300]
per-ex loss: 0.462261  [  117/  300]
per-ex loss: 0.573188  [  120/  300]
per-ex loss: 0.439635  [  123/  300]
per-ex loss: 0.435602  [  126/  300]
per-ex loss: 0.497904  [  129/  300]
per-ex loss: 0.434381  [  132/  300]
per-ex loss: 0.644686  [  135/  300]
per-ex loss: 0.602509  [  138/  300]
per-ex loss: 0.615246  [  141/  300]
per-ex loss: 0.444986  [  144/  300]
per-ex loss: 0.508514  [  147/  300]
per-ex loss: 0.538825  [  150/  300]
per-ex loss: 0.569140  [  153/  300]
per-ex loss: 0.456778  [  156/  300]
per-ex loss: 0.428950  [  159/  300]
per-ex loss: 0.469572  [  162/  300]
per-ex loss: 0.471777  [  165/  300]
per-ex loss: 0.466041  [  168/  300]
per-ex loss: 0.389481  [  171/  300]
per-ex loss: 0.509618  [  174/  300]
per-ex loss: 0.622245  [  177/  300]
per-ex loss: 0.481061  [  180/  300]
per-ex loss: 0.470687  [  183/  300]
per-ex loss: 0.415532  [  186/  300]
per-ex loss: 0.450142  [  189/  300]
per-ex loss: 0.399087  [  192/  300]
per-ex loss: 0.536744  [  195/  300]
per-ex loss: 0.492447  [  198/  300]
per-ex loss: 0.488219  [  201/  300]
per-ex loss: 0.484893  [  204/  300]
per-ex loss: 0.436572  [  207/  300]
per-ex loss: 0.536250  [  210/  300]
per-ex loss: 0.548752  [  213/  300]
per-ex loss: 0.466679  [  216/  300]
per-ex loss: 0.618652  [  219/  300]
per-ex loss: 0.441922  [  222/  300]
per-ex loss: 0.411847  [  225/  300]
per-ex loss: 0.435230  [  228/  300]
per-ex loss: 0.401555  [  231/  300]
per-ex loss: 0.491032  [  234/  300]
per-ex loss: 0.645375  [  237/  300]
per-ex loss: 0.556512  [  240/  300]
per-ex loss: 0.458529  [  243/  300]
per-ex loss: 0.585892  [  246/  300]
per-ex loss: 0.518268  [  249/  300]
per-ex loss: 0.390713  [  252/  300]
per-ex loss: 0.592197  [  255/  300]
per-ex loss: 0.522605  [  258/  300]
per-ex loss: 0.556820  [  261/  300]
per-ex loss: 0.508156  [  264/  300]
per-ex loss: 0.483607  [  267/  300]
per-ex loss: 0.463644  [  270/  300]
per-ex loss: 0.508986  [  273/  300]
per-ex loss: 0.491783  [  276/  300]
per-ex loss: 0.487908  [  279/  300]
per-ex loss: 0.487203  [  282/  300]
per-ex loss: 0.421903  [  285/  300]
per-ex loss: 0.485322  [  288/  300]
per-ex loss: 0.507653  [  291/  300]
per-ex loss: 0.425280  [  294/  300]
per-ex loss: 0.634644  [  297/  300]
per-ex loss: 0.540456  [  300/  300]
Train Error: Avg loss: 0.50271728
validation Error: 
 Avg loss: 0.50247492 
 F1: 0.481435 
 Precision: 0.423982 
 Recall: 0.556900
 IoU: 0.317033

test Error: 
 Avg loss: 0.46208966 
 F1: 0.538590 
 Precision: 0.472285 
 Recall: 0.626553
 IoU: 0.368542

We have finished training iteration 115
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_113_.pth
per-ex loss: 0.650828  [    3/  300]
per-ex loss: 0.601470  [    6/  300]
per-ex loss: 0.586923  [    9/  300]
per-ex loss: 0.650200  [   12/  300]
per-ex loss: 0.554429  [   15/  300]
per-ex loss: 0.503885  [   18/  300]
per-ex loss: 0.418954  [   21/  300]
per-ex loss: 0.392223  [   24/  300]
per-ex loss: 0.446184  [   27/  300]
per-ex loss: 0.586864  [   30/  300]
per-ex loss: 0.414674  [   33/  300]
per-ex loss: 0.515998  [   36/  300]
per-ex loss: 0.506784  [   39/  300]
per-ex loss: 0.598255  [   42/  300]
per-ex loss: 0.575937  [   45/  300]
per-ex loss: 0.431708  [   48/  300]
per-ex loss: 0.542292  [   51/  300]
per-ex loss: 0.518651  [   54/  300]
per-ex loss: 0.406494  [   57/  300]
per-ex loss: 0.561112  [   60/  300]
per-ex loss: 0.488745  [   63/  300]
per-ex loss: 0.446212  [   66/  300]
per-ex loss: 0.398094  [   69/  300]
per-ex loss: 0.510941  [   72/  300]
per-ex loss: 0.639041  [   75/  300]
per-ex loss: 0.431873  [   78/  300]
per-ex loss: 0.395920  [   81/  300]
per-ex loss: 0.479558  [   84/  300]
per-ex loss: 0.526497  [   87/  300]
per-ex loss: 0.447631  [   90/  300]
per-ex loss: 0.452455  [   93/  300]
per-ex loss: 0.542981  [   96/  300]
per-ex loss: 0.577674  [   99/  300]
per-ex loss: 0.442917  [  102/  300]
per-ex loss: 0.488531  [  105/  300]
per-ex loss: 0.441479  [  108/  300]
per-ex loss: 0.411014  [  111/  300]
per-ex loss: 0.536409  [  114/  300]
per-ex loss: 0.386770  [  117/  300]
per-ex loss: 0.496312  [  120/  300]
per-ex loss: 0.598006  [  123/  300]
per-ex loss: 0.431041  [  126/  300]
per-ex loss: 0.604261  [  129/  300]
per-ex loss: 0.442972  [  132/  300]
per-ex loss: 0.564416  [  135/  300]
per-ex loss: 0.448213  [  138/  300]
per-ex loss: 0.555582  [  141/  300]
per-ex loss: 0.522994  [  144/  300]
per-ex loss: 0.538682  [  147/  300]
per-ex loss: 0.535833  [  150/  300]
per-ex loss: 0.535304  [  153/  300]
per-ex loss: 0.548513  [  156/  300]
per-ex loss: 0.679672  [  159/  300]
per-ex loss: 0.407288  [  162/  300]
per-ex loss: 0.410424  [  165/  300]
per-ex loss: 0.456850  [  168/  300]
per-ex loss: 0.554245  [  171/  300]
per-ex loss: 0.415892  [  174/  300]
per-ex loss: 0.463873  [  177/  300]
per-ex loss: 0.465334  [  180/  300]
per-ex loss: 0.610660  [  183/  300]
per-ex loss: 0.525858  [  186/  300]
per-ex loss: 0.526442  [  189/  300]
per-ex loss: 0.552904  [  192/  300]
per-ex loss: 0.598133  [  195/  300]
per-ex loss: 0.444383  [  198/  300]
per-ex loss: 0.445673  [  201/  300]
per-ex loss: 0.380620  [  204/  300]
per-ex loss: 0.482101  [  207/  300]
per-ex loss: 0.402293  [  210/  300]
per-ex loss: 0.522103  [  213/  300]
per-ex loss: 0.489731  [  216/  300]
per-ex loss: 0.475046  [  219/  300]
per-ex loss: 0.490686  [  222/  300]
per-ex loss: 0.624748  [  225/  300]
per-ex loss: 0.621641  [  228/  300]
per-ex loss: 0.430223  [  231/  300]
per-ex loss: 0.539488  [  234/  300]
per-ex loss: 0.408346  [  237/  300]
per-ex loss: 0.407616  [  240/  300]
per-ex loss: 0.514185  [  243/  300]
per-ex loss: 0.415384  [  246/  300]
per-ex loss: 0.596327  [  249/  300]
per-ex loss: 0.452175  [  252/  300]
per-ex loss: 0.432444  [  255/  300]
per-ex loss: 0.518471  [  258/  300]
per-ex loss: 0.611613  [  261/  300]
per-ex loss: 0.623770  [  264/  300]
per-ex loss: 0.470375  [  267/  300]
per-ex loss: 0.425851  [  270/  300]
per-ex loss: 0.506302  [  273/  300]
per-ex loss: 0.528252  [  276/  300]
per-ex loss: 0.472991  [  279/  300]
per-ex loss: 0.584213  [  282/  300]
per-ex loss: 0.488683  [  285/  300]
per-ex loss: 0.462024  [  288/  300]
per-ex loss: 0.499702  [  291/  300]
per-ex loss: 0.398611  [  294/  300]
per-ex loss: 0.634935  [  297/  300]
per-ex loss: 0.486427  [  300/  300]
Train Error: Avg loss: 0.50285744
validation Error: 
 Avg loss: 0.50549587 
 F1: 0.480959 
 Precision: 0.447886 
 Recall: 0.519306
 IoU: 0.316620

test Error: 
 Avg loss: 0.45241183 
 F1: 0.548335 
 Precision: 0.504704 
 Recall: 0.600224
 IoU: 0.377728

We have finished training iteration 116
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_114_.pth
per-ex loss: 0.599715  [    3/  300]
per-ex loss: 0.520631  [    6/  300]
per-ex loss: 0.436426  [    9/  300]
per-ex loss: 0.511049  [   12/  300]
per-ex loss: 0.479512  [   15/  300]
per-ex loss: 0.504697  [   18/  300]
per-ex loss: 0.394396  [   21/  300]
per-ex loss: 0.537086  [   24/  300]
per-ex loss: 0.459248  [   27/  300]
per-ex loss: 0.472681  [   30/  300]
per-ex loss: 0.605977  [   33/  300]
per-ex loss: 0.427530  [   36/  300]
per-ex loss: 0.476935  [   39/  300]
per-ex loss: 0.635670  [   42/  300]
per-ex loss: 0.586771  [   45/  300]
per-ex loss: 0.533648  [   48/  300]
per-ex loss: 0.424469  [   51/  300]
per-ex loss: 0.503849  [   54/  300]
per-ex loss: 0.415349  [   57/  300]
per-ex loss: 0.593369  [   60/  300]
per-ex loss: 0.423989  [   63/  300]
per-ex loss: 0.610776  [   66/  300]
per-ex loss: 0.604597  [   69/  300]
per-ex loss: 0.410457  [   72/  300]
per-ex loss: 0.516286  [   75/  300]
per-ex loss: 0.521046  [   78/  300]
per-ex loss: 0.478994  [   81/  300]
per-ex loss: 0.627274  [   84/  300]
per-ex loss: 0.529306  [   87/  300]
per-ex loss: 0.598754  [   90/  300]
per-ex loss: 0.537646  [   93/  300]
per-ex loss: 0.542577  [   96/  300]
per-ex loss: 0.526459  [   99/  300]
per-ex loss: 0.542930  [  102/  300]
per-ex loss: 0.459435  [  105/  300]
per-ex loss: 0.593442  [  108/  300]
per-ex loss: 0.587629  [  111/  300]
per-ex loss: 0.445367  [  114/  300]
per-ex loss: 0.451931  [  117/  300]
per-ex loss: 0.494902  [  120/  300]
per-ex loss: 0.542587  [  123/  300]
per-ex loss: 0.448466  [  126/  300]
per-ex loss: 0.417851  [  129/  300]
per-ex loss: 0.473746  [  132/  300]
per-ex loss: 0.434103  [  135/  300]
per-ex loss: 0.595585  [  138/  300]
per-ex loss: 0.420017  [  141/  300]
per-ex loss: 0.512846  [  144/  300]
per-ex loss: 0.429162  [  147/  300]
per-ex loss: 0.612526  [  150/  300]
per-ex loss: 0.463636  [  153/  300]
per-ex loss: 0.456391  [  156/  300]
per-ex loss: 0.551994  [  159/  300]
per-ex loss: 0.395534  [  162/  300]
per-ex loss: 0.484911  [  165/  300]
per-ex loss: 0.551060  [  168/  300]
per-ex loss: 0.503913  [  171/  300]
per-ex loss: 0.535042  [  174/  300]
per-ex loss: 0.446336  [  177/  300]
per-ex loss: 0.536249  [  180/  300]
per-ex loss: 0.416247  [  183/  300]
per-ex loss: 0.441378  [  186/  300]
per-ex loss: 0.540693  [  189/  300]
per-ex loss: 0.652301  [  192/  300]
per-ex loss: 0.517088  [  195/  300]
per-ex loss: 0.449135  [  198/  300]
per-ex loss: 0.461984  [  201/  300]
per-ex loss: 0.554096  [  204/  300]
per-ex loss: 0.529782  [  207/  300]
per-ex loss: 0.462490  [  210/  300]
per-ex loss: 0.450727  [  213/  300]
per-ex loss: 0.510098  [  216/  300]
per-ex loss: 0.492907  [  219/  300]
per-ex loss: 0.527269  [  222/  300]
per-ex loss: 0.488656  [  225/  300]
per-ex loss: 0.460038  [  228/  300]
per-ex loss: 0.549118  [  231/  300]
per-ex loss: 0.512201  [  234/  300]
per-ex loss: 0.557983  [  237/  300]
per-ex loss: 0.520932  [  240/  300]
per-ex loss: 0.537587  [  243/  300]
per-ex loss: 0.451554  [  246/  300]
per-ex loss: 0.534170  [  249/  300]
per-ex loss: 0.423328  [  252/  300]
per-ex loss: 0.532654  [  255/  300]
per-ex loss: 0.421948  [  258/  300]
per-ex loss: 0.538032  [  261/  300]
per-ex loss: 0.453089  [  264/  300]
per-ex loss: 0.577899  [  267/  300]
per-ex loss: 0.403878  [  270/  300]
per-ex loss: 0.467538  [  273/  300]
per-ex loss: 0.400971  [  276/  300]
per-ex loss: 0.515464  [  279/  300]
per-ex loss: 0.558005  [  282/  300]
per-ex loss: 0.461656  [  285/  300]
per-ex loss: 0.477812  [  288/  300]
per-ex loss: 0.420931  [  291/  300]
per-ex loss: 0.561690  [  294/  300]
per-ex loss: 0.624108  [  297/  300]
per-ex loss: 0.554094  [  300/  300]
Train Error: Avg loss: 0.50446290
validation Error: 
 Avg loss: 0.49804370 
 F1: 0.485851 
 Precision: 0.436963 
 Recall: 0.547056
 IoU: 0.320874

test Error: 
 Avg loss: 0.45739144 
 F1: 0.543579 
 Precision: 0.487884 
 Recall: 0.613629
 IoU: 0.373229

We have finished training iteration 117
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_115_.pth
per-ex loss: 0.419049  [    3/  300]
per-ex loss: 0.452375  [    6/  300]
per-ex loss: 0.499032  [    9/  300]
per-ex loss: 0.479259  [   12/  300]
per-ex loss: 0.596579  [   15/  300]
per-ex loss: 0.585827  [   18/  300]
per-ex loss: 0.408710  [   21/  300]
per-ex loss: 0.460631  [   24/  300]
per-ex loss: 0.578347  [   27/  300]
per-ex loss: 0.439797  [   30/  300]
per-ex loss: 0.492409  [   33/  300]
per-ex loss: 0.412666  [   36/  300]
per-ex loss: 0.509197  [   39/  300]
per-ex loss: 0.664047  [   42/  300]
per-ex loss: 0.421352  [   45/  300]
per-ex loss: 0.560003  [   48/  300]
per-ex loss: 0.543539  [   51/  300]
per-ex loss: 0.524198  [   54/  300]
per-ex loss: 0.505572  [   57/  300]
per-ex loss: 0.515740  [   60/  300]
per-ex loss: 0.422066  [   63/  300]
per-ex loss: 0.529337  [   66/  300]
per-ex loss: 0.442525  [   69/  300]
per-ex loss: 0.635771  [   72/  300]
per-ex loss: 0.565112  [   75/  300]
per-ex loss: 0.410249  [   78/  300]
per-ex loss: 0.486925  [   81/  300]
per-ex loss: 0.574215  [   84/  300]
per-ex loss: 0.482914  [   87/  300]
per-ex loss: 0.494220  [   90/  300]
per-ex loss: 0.460773  [   93/  300]
per-ex loss: 0.607543  [   96/  300]
per-ex loss: 0.445111  [   99/  300]
per-ex loss: 0.573248  [  102/  300]
per-ex loss: 0.559651  [  105/  300]
per-ex loss: 0.582241  [  108/  300]
per-ex loss: 0.496847  [  111/  300]
per-ex loss: 0.460447  [  114/  300]
per-ex loss: 0.530621  [  117/  300]
per-ex loss: 0.433330  [  120/  300]
per-ex loss: 0.470713  [  123/  300]
per-ex loss: 0.500363  [  126/  300]
per-ex loss: 0.619330  [  129/  300]
per-ex loss: 0.473708  [  132/  300]
per-ex loss: 0.625138  [  135/  300]
per-ex loss: 0.393238  [  138/  300]
per-ex loss: 0.511110  [  141/  300]
per-ex loss: 0.563791  [  144/  300]
per-ex loss: 0.547569  [  147/  300]
per-ex loss: 0.442847  [  150/  300]
per-ex loss: 0.490915  [  153/  300]
per-ex loss: 0.546637  [  156/  300]
per-ex loss: 0.474517  [  159/  300]
per-ex loss: 0.588215  [  162/  300]
per-ex loss: 0.644239  [  165/  300]
per-ex loss: 0.410036  [  168/  300]
per-ex loss: 0.556091  [  171/  300]
per-ex loss: 0.409108  [  174/  300]
per-ex loss: 0.485376  [  177/  300]
per-ex loss: 0.425203  [  180/  300]
per-ex loss: 0.598104  [  183/  300]
per-ex loss: 0.434806  [  186/  300]
per-ex loss: 0.448362  [  189/  300]
per-ex loss: 0.473188  [  192/  300]
per-ex loss: 0.473294  [  195/  300]
per-ex loss: 0.528301  [  198/  300]
per-ex loss: 0.450558  [  201/  300]
per-ex loss: 0.478325  [  204/  300]
per-ex loss: 0.475616  [  207/  300]
per-ex loss: 0.433557  [  210/  300]
per-ex loss: 0.422144  [  213/  300]
per-ex loss: 0.465981  [  216/  300]
per-ex loss: 0.429460  [  219/  300]
per-ex loss: 0.608973  [  222/  300]
per-ex loss: 0.584030  [  225/  300]
per-ex loss: 0.609645  [  228/  300]
per-ex loss: 0.442991  [  231/  300]
per-ex loss: 0.482234  [  234/  300]
per-ex loss: 0.593318  [  237/  300]
per-ex loss: 0.630999  [  240/  300]
per-ex loss: 0.569567  [  243/  300]
per-ex loss: 0.403124  [  246/  300]
per-ex loss: 0.509324  [  249/  300]
per-ex loss: 0.583981  [  252/  300]
per-ex loss: 0.626264  [  255/  300]
per-ex loss: 0.597514  [  258/  300]
per-ex loss: 0.426633  [  261/  300]
per-ex loss: 0.433288  [  264/  300]
per-ex loss: 0.612077  [  267/  300]
per-ex loss: 0.572865  [  270/  300]
per-ex loss: 0.586702  [  273/  300]
per-ex loss: 0.445090  [  276/  300]
per-ex loss: 0.582622  [  279/  300]
per-ex loss: 0.614463  [  282/  300]
per-ex loss: 0.448814  [  285/  300]
per-ex loss: 0.494420  [  288/  300]
per-ex loss: 0.519865  [  291/  300]
per-ex loss: 0.463186  [  294/  300]
per-ex loss: 0.622402  [  297/  300]
per-ex loss: 0.626720  [  300/  300]
Train Error: Avg loss: 0.51242429
validation Error: 
 Avg loss: 0.49558834 
 F1: 0.488023 
 Precision: 0.439280 
 Recall: 0.548931
 IoU: 0.322771

test Error: 
 Avg loss: 0.45067918 
 F1: 0.549871 
 Precision: 0.496856 
 Recall: 0.615552
 IoU: 0.379188

We have finished training iteration 118
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_116_.pth
per-ex loss: 0.472991  [    3/  300]
per-ex loss: 0.416904  [    6/  300]
per-ex loss: 0.505761  [    9/  300]
per-ex loss: 0.485025  [   12/  300]
per-ex loss: 0.570164  [   15/  300]
per-ex loss: 0.422523  [   18/  300]
per-ex loss: 0.443491  [   21/  300]
per-ex loss: 0.487882  [   24/  300]
per-ex loss: 0.518310  [   27/  300]
per-ex loss: 0.415784  [   30/  300]
per-ex loss: 0.445801  [   33/  300]
per-ex loss: 0.504062  [   36/  300]
per-ex loss: 0.580278  [   39/  300]
per-ex loss: 0.460197  [   42/  300]
per-ex loss: 0.478571  [   45/  300]
per-ex loss: 0.407820  [   48/  300]
per-ex loss: 0.486619  [   51/  300]
per-ex loss: 0.603456  [   54/  300]
per-ex loss: 0.454223  [   57/  300]
per-ex loss: 0.535915  [   60/  300]
per-ex loss: 0.476512  [   63/  300]
per-ex loss: 0.488361  [   66/  300]
per-ex loss: 0.631498  [   69/  300]
per-ex loss: 0.456547  [   72/  300]
per-ex loss: 0.705209  [   75/  300]
per-ex loss: 0.504624  [   78/  300]
per-ex loss: 0.493924  [   81/  300]
per-ex loss: 0.613356  [   84/  300]
per-ex loss: 0.468156  [   87/  300]
per-ex loss: 0.604547  [   90/  300]
per-ex loss: 0.558499  [   93/  300]
per-ex loss: 0.593772  [   96/  300]
per-ex loss: 0.562658  [   99/  300]
per-ex loss: 0.412650  [  102/  300]
per-ex loss: 0.452947  [  105/  300]
per-ex loss: 0.565280  [  108/  300]
per-ex loss: 0.557778  [  111/  300]
per-ex loss: 0.486767  [  114/  300]
per-ex loss: 0.443553  [  117/  300]
per-ex loss: 0.466102  [  120/  300]
per-ex loss: 0.512162  [  123/  300]
per-ex loss: 0.419845  [  126/  300]
per-ex loss: 0.464898  [  129/  300]
per-ex loss: 0.502041  [  132/  300]
per-ex loss: 0.545865  [  135/  300]
per-ex loss: 0.406636  [  138/  300]
per-ex loss: 0.489546  [  141/  300]
per-ex loss: 0.526827  [  144/  300]
per-ex loss: 0.455020  [  147/  300]
per-ex loss: 0.464415  [  150/  300]
per-ex loss: 0.607280  [  153/  300]
per-ex loss: 0.444354  [  156/  300]
per-ex loss: 0.457884  [  159/  300]
per-ex loss: 0.626759  [  162/  300]
per-ex loss: 0.500693  [  165/  300]
per-ex loss: 0.581114  [  168/  300]
per-ex loss: 0.438195  [  171/  300]
per-ex loss: 0.474361  [  174/  300]
per-ex loss: 0.515760  [  177/  300]
per-ex loss: 0.584263  [  180/  300]
per-ex loss: 0.487837  [  183/  300]
per-ex loss: 0.482702  [  186/  300]
per-ex loss: 0.514887  [  189/  300]
per-ex loss: 0.534379  [  192/  300]
per-ex loss: 0.463973  [  195/  300]
per-ex loss: 0.427645  [  198/  300]
per-ex loss: 0.527816  [  201/  300]
per-ex loss: 0.438115  [  204/  300]
per-ex loss: 0.497767  [  207/  300]
per-ex loss: 0.442095  [  210/  300]
per-ex loss: 0.478991  [  213/  300]
per-ex loss: 0.658386  [  216/  300]
per-ex loss: 0.500993  [  219/  300]
per-ex loss: 0.459581  [  222/  300]
per-ex loss: 0.401762  [  225/  300]
per-ex loss: 0.614793  [  228/  300]
per-ex loss: 0.465739  [  231/  300]
per-ex loss: 0.461266  [  234/  300]
per-ex loss: 0.497482  [  237/  300]
per-ex loss: 0.535856  [  240/  300]
per-ex loss: 0.446641  [  243/  300]
per-ex loss: 0.425227  [  246/  300]
per-ex loss: 0.453106  [  249/  300]
per-ex loss: 0.437911  [  252/  300]
per-ex loss: 0.594449  [  255/  300]
per-ex loss: 0.469114  [  258/  300]
per-ex loss: 0.589281  [  261/  300]
per-ex loss: 0.493585  [  264/  300]
per-ex loss: 0.453638  [  267/  300]
per-ex loss: 0.548548  [  270/  300]
per-ex loss: 0.546183  [  273/  300]
per-ex loss: 0.485356  [  276/  300]
per-ex loss: 0.532335  [  279/  300]
per-ex loss: 0.511715  [  282/  300]
per-ex loss: 0.487770  [  285/  300]
per-ex loss: 0.445007  [  288/  300]
per-ex loss: 0.435110  [  291/  300]
per-ex loss: 0.473736  [  294/  300]
per-ex loss: 0.620850  [  297/  300]
per-ex loss: 0.454296  [  300/  300]
Train Error: Avg loss: 0.50052361
validation Error: 
 Avg loss: 0.50797866 
 F1: 0.476729 
 Precision: 0.443457 
 Recall: 0.515399
 IoU: 0.312964

test Error: 
 Avg loss: 0.45449460 
 F1: 0.546175 
 Precision: 0.493654 
 Recall: 0.611202
 IoU: 0.375682

We have finished training iteration 119
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_117_.pth
per-ex loss: 0.398566  [    3/  300]
per-ex loss: 0.475633  [    6/  300]
per-ex loss: 0.529252  [    9/  300]
per-ex loss: 0.458348  [   12/  300]
per-ex loss: 0.478084  [   15/  300]
per-ex loss: 0.602573  [   18/  300]
per-ex loss: 0.631389  [   21/  300]
per-ex loss: 0.648927  [   24/  300]
per-ex loss: 0.376000  [   27/  300]
per-ex loss: 0.428080  [   30/  300]
per-ex loss: 0.445823  [   33/  300]
per-ex loss: 0.487878  [   36/  300]
per-ex loss: 0.528083  [   39/  300]
per-ex loss: 0.578138  [   42/  300]
per-ex loss: 0.580028  [   45/  300]
per-ex loss: 0.535021  [   48/  300]
per-ex loss: 0.559425  [   51/  300]
per-ex loss: 0.473766  [   54/  300]
per-ex loss: 0.525368  [   57/  300]
per-ex loss: 0.483229  [   60/  300]
per-ex loss: 0.586175  [   63/  300]
per-ex loss: 0.668108  [   66/  300]
per-ex loss: 0.421390  [   69/  300]
per-ex loss: 0.531529  [   72/  300]
per-ex loss: 0.428402  [   75/  300]
per-ex loss: 0.464583  [   78/  300]
per-ex loss: 0.411902  [   81/  300]
per-ex loss: 0.529204  [   84/  300]
per-ex loss: 0.669424  [   87/  300]
per-ex loss: 0.460884  [   90/  300]
per-ex loss: 0.528658  [   93/  300]
per-ex loss: 0.411952  [   96/  300]
per-ex loss: 0.500851  [   99/  300]
per-ex loss: 0.508626  [  102/  300]
per-ex loss: 0.472756  [  105/  300]
per-ex loss: 0.528422  [  108/  300]
per-ex loss: 0.559283  [  111/  300]
per-ex loss: 0.596202  [  114/  300]
per-ex loss: 0.444347  [  117/  300]
per-ex loss: 0.418449  [  120/  300]
per-ex loss: 0.451813  [  123/  300]
per-ex loss: 0.591327  [  126/  300]
per-ex loss: 0.587887  [  129/  300]
per-ex loss: 0.439235  [  132/  300]
per-ex loss: 0.447012  [  135/  300]
per-ex loss: 0.512223  [  138/  300]
per-ex loss: 0.471325  [  141/  300]
per-ex loss: 0.551836  [  144/  300]
per-ex loss: 0.473311  [  147/  300]
per-ex loss: 0.472425  [  150/  300]
per-ex loss: 0.492854  [  153/  300]
per-ex loss: 0.501796  [  156/  300]
per-ex loss: 0.388460  [  159/  300]
per-ex loss: 0.466892  [  162/  300]
per-ex loss: 0.514870  [  165/  300]
per-ex loss: 0.485744  [  168/  300]
per-ex loss: 0.558703  [  171/  300]
per-ex loss: 0.511404  [  174/  300]
per-ex loss: 0.597212  [  177/  300]
per-ex loss: 0.562498  [  180/  300]
per-ex loss: 0.637390  [  183/  300]
per-ex loss: 0.527014  [  186/  300]
per-ex loss: 0.461081  [  189/  300]
per-ex loss: 0.528962  [  192/  300]
per-ex loss: 0.470339  [  195/  300]
per-ex loss: 0.487018  [  198/  300]
per-ex loss: 0.588828  [  201/  300]
per-ex loss: 0.459457  [  204/  300]
per-ex loss: 0.446854  [  207/  300]
per-ex loss: 0.605064  [  210/  300]
per-ex loss: 0.503777  [  213/  300]
per-ex loss: 0.520246  [  216/  300]
per-ex loss: 0.455085  [  219/  300]
per-ex loss: 0.507648  [  222/  300]
per-ex loss: 0.427811  [  225/  300]
per-ex loss: 0.405812  [  228/  300]
per-ex loss: 0.519621  [  231/  300]
per-ex loss: 0.629190  [  234/  300]
per-ex loss: 0.402554  [  237/  300]
per-ex loss: 0.409302  [  240/  300]
per-ex loss: 0.445614  [  243/  300]
per-ex loss: 0.604233  [  246/  300]
per-ex loss: 0.518669  [  249/  300]
per-ex loss: 0.523802  [  252/  300]
per-ex loss: 0.497466  [  255/  300]
per-ex loss: 0.515827  [  258/  300]
per-ex loss: 0.503697  [  261/  300]
per-ex loss: 0.460008  [  264/  300]
per-ex loss: 0.569525  [  267/  300]
per-ex loss: 0.581107  [  270/  300]
per-ex loss: 0.503636  [  273/  300]
per-ex loss: 0.542792  [  276/  300]
per-ex loss: 0.629924  [  279/  300]
per-ex loss: 0.533629  [  282/  300]
per-ex loss: 0.609765  [  285/  300]
per-ex loss: 0.499112  [  288/  300]
per-ex loss: 0.517081  [  291/  300]
per-ex loss: 0.499844  [  294/  300]
per-ex loss: 0.441588  [  297/  300]
per-ex loss: 0.496472  [  300/  300]
Train Error: Avg loss: 0.50930429
validation Error: 
 Avg loss: 0.49579738 
 F1: 0.484877 
 Precision: 0.423691 
 Recall: 0.566717
 IoU: 0.320024

test Error: 
 Avg loss: 0.45985848 
 F1: 0.540973 
 Precision: 0.474325 
 Recall: 0.629412
 IoU: 0.370776

We have finished training iteration 120
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_118_.pth
per-ex loss: 0.567154  [    3/  300]
per-ex loss: 0.500262  [    6/  300]
per-ex loss: 0.423304  [    9/  300]
per-ex loss: 0.415325  [   12/  300]
per-ex loss: 0.469678  [   15/  300]
per-ex loss: 0.487678  [   18/  300]
per-ex loss: 0.452052  [   21/  300]
per-ex loss: 0.406702  [   24/  300]
per-ex loss: 0.432076  [   27/  300]
per-ex loss: 0.469118  [   30/  300]
per-ex loss: 0.551277  [   33/  300]
per-ex loss: 0.472609  [   36/  300]
per-ex loss: 0.543665  [   39/  300]
per-ex loss: 0.464960  [   42/  300]
per-ex loss: 0.453664  [   45/  300]
per-ex loss: 0.656740  [   48/  300]
per-ex loss: 0.475560  [   51/  300]
per-ex loss: 0.433532  [   54/  300]
per-ex loss: 0.428931  [   57/  300]
per-ex loss: 0.471370  [   60/  300]
per-ex loss: 0.634940  [   63/  300]
per-ex loss: 0.545442  [   66/  300]
per-ex loss: 0.512999  [   69/  300]
per-ex loss: 0.512200  [   72/  300]
per-ex loss: 0.491654  [   75/  300]
per-ex loss: 0.495567  [   78/  300]
per-ex loss: 0.458134  [   81/  300]
per-ex loss: 0.596984  [   84/  300]
per-ex loss: 0.480290  [   87/  300]
per-ex loss: 0.439709  [   90/  300]
per-ex loss: 0.370799  [   93/  300]
per-ex loss: 0.495313  [   96/  300]
per-ex loss: 0.553813  [   99/  300]
per-ex loss: 0.487560  [  102/  300]
per-ex loss: 0.605086  [  105/  300]
per-ex loss: 0.568428  [  108/  300]
per-ex loss: 0.505663  [  111/  300]
per-ex loss: 0.583679  [  114/  300]
per-ex loss: 0.527267  [  117/  300]
per-ex loss: 0.391089  [  120/  300]
per-ex loss: 0.444480  [  123/  300]
per-ex loss: 0.455766  [  126/  300]
per-ex loss: 0.434291  [  129/  300]
per-ex loss: 0.571232  [  132/  300]
per-ex loss: 0.423577  [  135/  300]
per-ex loss: 0.523895  [  138/  300]
per-ex loss: 0.436382  [  141/  300]
per-ex loss: 0.647287  [  144/  300]
per-ex loss: 0.466829  [  147/  300]
per-ex loss: 0.516078  [  150/  300]
per-ex loss: 0.559685  [  153/  300]
per-ex loss: 0.479485  [  156/  300]
per-ex loss: 0.620821  [  159/  300]
per-ex loss: 0.430123  [  162/  300]
per-ex loss: 0.704475  [  165/  300]
per-ex loss: 0.504688  [  168/  300]
per-ex loss: 0.504331  [  171/  300]
per-ex loss: 0.407297  [  174/  300]
per-ex loss: 0.555577  [  177/  300]
per-ex loss: 0.464518  [  180/  300]
per-ex loss: 0.573601  [  183/  300]
per-ex loss: 0.495763  [  186/  300]
per-ex loss: 0.629727  [  189/  300]
per-ex loss: 0.441182  [  192/  300]
per-ex loss: 0.418908  [  195/  300]
per-ex loss: 0.454733  [  198/  300]
per-ex loss: 0.468044  [  201/  300]
per-ex loss: 0.533055  [  204/  300]
per-ex loss: 0.446911  [  207/  300]
per-ex loss: 0.435548  [  210/  300]
per-ex loss: 0.523932  [  213/  300]
per-ex loss: 0.446384  [  216/  300]
per-ex loss: 0.597355  [  219/  300]
per-ex loss: 0.454677  [  222/  300]
per-ex loss: 0.468263  [  225/  300]
per-ex loss: 0.446338  [  228/  300]
per-ex loss: 0.465160  [  231/  300]
per-ex loss: 0.494413  [  234/  300]
per-ex loss: 0.401391  [  237/  300]
per-ex loss: 0.523220  [  240/  300]
per-ex loss: 0.465114  [  243/  300]
per-ex loss: 0.601178  [  246/  300]
per-ex loss: 0.487840  [  249/  300]
per-ex loss: 0.555760  [  252/  300]
per-ex loss: 0.638214  [  255/  300]
per-ex loss: 0.405596  [  258/  300]
per-ex loss: 0.528158  [  261/  300]
per-ex loss: 0.505252  [  264/  300]
per-ex loss: 0.561019  [  267/  300]
per-ex loss: 0.457948  [  270/  300]
per-ex loss: 0.445383  [  273/  300]
per-ex loss: 0.499678  [  276/  300]
per-ex loss: 0.649576  [  279/  300]
per-ex loss: 0.650324  [  282/  300]
per-ex loss: 0.490561  [  285/  300]
per-ex loss: 0.426893  [  288/  300]
per-ex loss: 0.523418  [  291/  300]
per-ex loss: 0.665898  [  294/  300]
per-ex loss: 0.452009  [  297/  300]
per-ex loss: 0.557537  [  300/  300]
Train Error: Avg loss: 0.50269052
validation Error: 
 Avg loss: 0.50081458 
 F1: 0.477461 
 Precision: 0.421811 
 Recall: 0.550026
 IoU: 0.313595

test Error: 
 Avg loss: 0.46317935 
 F1: 0.537194 
 Precision: 0.463805 
 Recall: 0.638174
 IoU: 0.367235

We have finished training iteration 121
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_119_.pth
per-ex loss: 0.511155  [    3/  300]
per-ex loss: 0.589413  [    6/  300]
per-ex loss: 0.428306  [    9/  300]
per-ex loss: 0.474220  [   12/  300]
per-ex loss: 0.413152  [   15/  300]
per-ex loss: 0.550513  [   18/  300]
per-ex loss: 0.533418  [   21/  300]
per-ex loss: 0.511903  [   24/  300]
per-ex loss: 0.382855  [   27/  300]
per-ex loss: 0.493373  [   30/  300]
per-ex loss: 0.410723  [   33/  300]
per-ex loss: 0.664786  [   36/  300]
per-ex loss: 0.513081  [   39/  300]
per-ex loss: 0.496625  [   42/  300]
per-ex loss: 0.478012  [   45/  300]
per-ex loss: 0.432688  [   48/  300]
per-ex loss: 0.643564  [   51/  300]
per-ex loss: 0.492912  [   54/  300]
per-ex loss: 0.443521  [   57/  300]
per-ex loss: 0.431886  [   60/  300]
per-ex loss: 0.452599  [   63/  300]
per-ex loss: 0.540055  [   66/  300]
per-ex loss: 0.472185  [   69/  300]
per-ex loss: 0.497634  [   72/  300]
per-ex loss: 0.531125  [   75/  300]
per-ex loss: 0.520315  [   78/  300]
per-ex loss: 0.585677  [   81/  300]
per-ex loss: 0.446606  [   84/  300]
per-ex loss: 0.444680  [   87/  300]
per-ex loss: 0.628954  [   90/  300]
per-ex loss: 0.489111  [   93/  300]
per-ex loss: 0.543951  [   96/  300]
per-ex loss: 0.447893  [   99/  300]
per-ex loss: 0.529256  [  102/  300]
per-ex loss: 0.601833  [  105/  300]
per-ex loss: 0.453332  [  108/  300]
per-ex loss: 0.530509  [  111/  300]
per-ex loss: 0.474131  [  114/  300]
per-ex loss: 0.615079  [  117/  300]
per-ex loss: 0.459873  [  120/  300]
per-ex loss: 0.465592  [  123/  300]
per-ex loss: 0.625106  [  126/  300]
per-ex loss: 0.533287  [  129/  300]
per-ex loss: 0.428205  [  132/  300]
per-ex loss: 0.512779  [  135/  300]
per-ex loss: 0.397521  [  138/  300]
per-ex loss: 0.528435  [  141/  300]
per-ex loss: 0.603342  [  144/  300]
per-ex loss: 0.453421  [  147/  300]
per-ex loss: 0.452491  [  150/  300]
per-ex loss: 0.534399  [  153/  300]
per-ex loss: 0.463726  [  156/  300]
per-ex loss: 0.592123  [  159/  300]
per-ex loss: 0.447899  [  162/  300]
per-ex loss: 0.563863  [  165/  300]
per-ex loss: 0.454537  [  168/  300]
per-ex loss: 0.439839  [  171/  300]
per-ex loss: 0.435469  [  174/  300]
per-ex loss: 0.499760  [  177/  300]
per-ex loss: 0.535531  [  180/  300]
per-ex loss: 0.509083  [  183/  300]
per-ex loss: 0.582825  [  186/  300]
per-ex loss: 0.517027  [  189/  300]
per-ex loss: 0.528854  [  192/  300]
per-ex loss: 0.642226  [  195/  300]
per-ex loss: 0.478786  [  198/  300]
per-ex loss: 0.438670  [  201/  300]
per-ex loss: 0.554282  [  204/  300]
per-ex loss: 0.400359  [  207/  300]
per-ex loss: 0.402002  [  210/  300]
per-ex loss: 0.505224  [  213/  300]
per-ex loss: 0.457951  [  216/  300]
per-ex loss: 0.388553  [  219/  300]
per-ex loss: 0.489895  [  222/  300]
per-ex loss: 0.432593  [  225/  300]
per-ex loss: 0.687248  [  228/  300]
per-ex loss: 0.483333  [  231/  300]
per-ex loss: 0.517038  [  234/  300]
per-ex loss: 0.532557  [  237/  300]
per-ex loss: 0.501530  [  240/  300]
per-ex loss: 0.530474  [  243/  300]
per-ex loss: 0.456243  [  246/  300]
per-ex loss: 0.598214  [  249/  300]
per-ex loss: 0.545249  [  252/  300]
per-ex loss: 0.393472  [  255/  300]
per-ex loss: 0.498983  [  258/  300]
per-ex loss: 0.546579  [  261/  300]
per-ex loss: 0.474849  [  264/  300]
per-ex loss: 0.597745  [  267/  300]
per-ex loss: 0.593385  [  270/  300]
per-ex loss: 0.511706  [  273/  300]
per-ex loss: 0.477727  [  276/  300]
per-ex loss: 0.494668  [  279/  300]
per-ex loss: 0.435723  [  282/  300]
per-ex loss: 0.427397  [  285/  300]
per-ex loss: 0.425292  [  288/  300]
per-ex loss: 0.413822  [  291/  300]
per-ex loss: 0.631288  [  294/  300]
per-ex loss: 0.583838  [  297/  300]
per-ex loss: 0.568925  [  300/  300]
Train Error: Avg loss: 0.50383845
validation Error: 
 Avg loss: 0.49700816 
 F1: 0.480391 
 Precision: 0.410380 
 Recall: 0.579202
 IoU: 0.316128

test Error: 
 Avg loss: 0.45976752 
 F1: 0.540922 
 Precision: 0.455924 
 Recall: 0.664874
 IoU: 0.370728

We have finished training iteration 122
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_120_.pth
per-ex loss: 0.583015  [    3/  300]
per-ex loss: 0.413055  [    6/  300]
per-ex loss: 0.570544  [    9/  300]
per-ex loss: 0.448771  [   12/  300]
per-ex loss: 0.486108  [   15/  300]
per-ex loss: 0.518149  [   18/  300]
per-ex loss: 0.426038  [   21/  300]
per-ex loss: 0.476276  [   24/  300]
per-ex loss: 0.494674  [   27/  300]
per-ex loss: 0.609244  [   30/  300]
per-ex loss: 0.498039  [   33/  300]
per-ex loss: 0.473712  [   36/  300]
per-ex loss: 0.515946  [   39/  300]
per-ex loss: 0.592769  [   42/  300]
per-ex loss: 0.394936  [   45/  300]
per-ex loss: 0.460365  [   48/  300]
per-ex loss: 0.526990  [   51/  300]
per-ex loss: 0.496480  [   54/  300]
per-ex loss: 0.619738  [   57/  300]
per-ex loss: 0.557236  [   60/  300]
per-ex loss: 0.567289  [   63/  300]
per-ex loss: 0.453278  [   66/  300]
per-ex loss: 0.506135  [   69/  300]
per-ex loss: 0.547288  [   72/  300]
per-ex loss: 0.407788  [   75/  300]
per-ex loss: 0.364118  [   78/  300]
per-ex loss: 0.503631  [   81/  300]
per-ex loss: 0.512399  [   84/  300]
per-ex loss: 0.417636  [   87/  300]
per-ex loss: 0.555910  [   90/  300]
per-ex loss: 0.601614  [   93/  300]
per-ex loss: 0.421487  [   96/  300]
per-ex loss: 0.521929  [   99/  300]
per-ex loss: 0.566202  [  102/  300]
per-ex loss: 0.513202  [  105/  300]
per-ex loss: 0.520412  [  108/  300]
per-ex loss: 0.619289  [  111/  300]
per-ex loss: 0.402681  [  114/  300]
per-ex loss: 0.487363  [  117/  300]
per-ex loss: 0.482130  [  120/  300]
per-ex loss: 0.484994  [  123/  300]
per-ex loss: 0.525858  [  126/  300]
per-ex loss: 0.472527  [  129/  300]
per-ex loss: 0.412178  [  132/  300]
per-ex loss: 0.578146  [  135/  300]
per-ex loss: 0.442236  [  138/  300]
per-ex loss: 0.460781  [  141/  300]
per-ex loss: 0.427748  [  144/  300]
per-ex loss: 0.553680  [  147/  300]
per-ex loss: 0.513333  [  150/  300]
per-ex loss: 0.511901  [  153/  300]
per-ex loss: 0.497513  [  156/  300]
per-ex loss: 0.549091  [  159/  300]
per-ex loss: 0.435116  [  162/  300]
per-ex loss: 0.460194  [  165/  300]
per-ex loss: 0.572752  [  168/  300]
per-ex loss: 0.421175  [  171/  300]
per-ex loss: 0.446452  [  174/  300]
per-ex loss: 0.601368  [  177/  300]
per-ex loss: 0.622594  [  180/  300]
per-ex loss: 0.458863  [  183/  300]
per-ex loss: 0.490008  [  186/  300]
per-ex loss: 0.596117  [  189/  300]
per-ex loss: 0.549813  [  192/  300]
per-ex loss: 0.557696  [  195/  300]
per-ex loss: 0.436037  [  198/  300]
per-ex loss: 0.437234  [  201/  300]
per-ex loss: 0.463163  [  204/  300]
per-ex loss: 0.423200  [  207/  300]
per-ex loss: 0.539057  [  210/  300]
per-ex loss: 0.457108  [  213/  300]
per-ex loss: 0.473447  [  216/  300]
per-ex loss: 0.392026  [  219/  300]
per-ex loss: 0.441578  [  222/  300]
per-ex loss: 0.517813  [  225/  300]
per-ex loss: 0.442565  [  228/  300]
per-ex loss: 0.543648  [  231/  300]
per-ex loss: 0.423320  [  234/  300]
per-ex loss: 0.525854  [  237/  300]
per-ex loss: 0.554682  [  240/  300]
per-ex loss: 0.476648  [  243/  300]
per-ex loss: 0.491154  [  246/  300]
per-ex loss: 0.503030  [  249/  300]
per-ex loss: 0.465844  [  252/  300]
per-ex loss: 0.422766  [  255/  300]
per-ex loss: 0.586616  [  258/  300]
per-ex loss: 0.582950  [  261/  300]
per-ex loss: 0.613971  [  264/  300]
per-ex loss: 0.671052  [  267/  300]
per-ex loss: 0.520778  [  270/  300]
per-ex loss: 0.437004  [  273/  300]
per-ex loss: 0.514463  [  276/  300]
per-ex loss: 0.473167  [  279/  300]
per-ex loss: 0.603050  [  282/  300]
per-ex loss: 0.487294  [  285/  300]
per-ex loss: 0.572453  [  288/  300]
per-ex loss: 0.447832  [  291/  300]
per-ex loss: 0.495198  [  294/  300]
per-ex loss: 0.485960  [  297/  300]
per-ex loss: 0.522437  [  300/  300]
Train Error: Avg loss: 0.50220403
validation Error: 
 Avg loss: 0.51531333 
 F1: 0.473125 
 Precision: 0.454480 
 Recall: 0.493365
 IoU: 0.309865

test Error: 
 Avg loss: 0.45370108 
 F1: 0.547061 
 Precision: 0.507076 
 Recall: 0.593893
 IoU: 0.376521

We have finished training iteration 123
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_121_.pth
per-ex loss: 0.426712  [    3/  300]
per-ex loss: 0.498097  [    6/  300]
per-ex loss: 0.514148  [    9/  300]
per-ex loss: 0.424747  [   12/  300]
per-ex loss: 0.440751  [   15/  300]
per-ex loss: 0.528901  [   18/  300]
per-ex loss: 0.525197  [   21/  300]
per-ex loss: 0.479431  [   24/  300]
per-ex loss: 0.540234  [   27/  300]
per-ex loss: 0.449427  [   30/  300]
per-ex loss: 0.452624  [   33/  300]
per-ex loss: 0.474765  [   36/  300]
per-ex loss: 0.460624  [   39/  300]
per-ex loss: 0.510564  [   42/  300]
per-ex loss: 0.496410  [   45/  300]
per-ex loss: 0.443501  [   48/  300]
per-ex loss: 0.436141  [   51/  300]
per-ex loss: 0.694296  [   54/  300]
per-ex loss: 0.463854  [   57/  300]
per-ex loss: 0.611392  [   60/  300]
per-ex loss: 0.406308  [   63/  300]
per-ex loss: 0.410081  [   66/  300]
per-ex loss: 0.506338  [   69/  300]
per-ex loss: 0.585622  [   72/  300]
per-ex loss: 0.451636  [   75/  300]
per-ex loss: 0.502358  [   78/  300]
per-ex loss: 0.507589  [   81/  300]
per-ex loss: 0.543852  [   84/  300]
per-ex loss: 0.542480  [   87/  300]
per-ex loss: 0.491931  [   90/  300]
per-ex loss: 0.500886  [   93/  300]
per-ex loss: 0.419968  [   96/  300]
per-ex loss: 0.527072  [   99/  300]
per-ex loss: 0.695118  [  102/  300]
per-ex loss: 0.524665  [  105/  300]
per-ex loss: 0.550391  [  108/  300]
per-ex loss: 0.470871  [  111/  300]
per-ex loss: 0.629537  [  114/  300]
per-ex loss: 0.516765  [  117/  300]
per-ex loss: 0.460208  [  120/  300]
per-ex loss: 0.578487  [  123/  300]
per-ex loss: 0.418930  [  126/  300]
per-ex loss: 0.493248  [  129/  300]
per-ex loss: 0.545454  [  132/  300]
per-ex loss: 0.428533  [  135/  300]
per-ex loss: 0.602822  [  138/  300]
per-ex loss: 0.460130  [  141/  300]
per-ex loss: 0.474817  [  144/  300]
per-ex loss: 0.492031  [  147/  300]
per-ex loss: 0.560258  [  150/  300]
per-ex loss: 0.617914  [  153/  300]
per-ex loss: 0.603369  [  156/  300]
per-ex loss: 0.390742  [  159/  300]
per-ex loss: 0.455837  [  162/  300]
per-ex loss: 0.484787  [  165/  300]
per-ex loss: 0.389989  [  168/  300]
per-ex loss: 0.478418  [  171/  300]
per-ex loss: 0.485308  [  174/  300]
per-ex loss: 0.386032  [  177/  300]
per-ex loss: 0.413954  [  180/  300]
per-ex loss: 0.480514  [  183/  300]
per-ex loss: 0.532947  [  186/  300]
per-ex loss: 0.506772  [  189/  300]
per-ex loss: 0.436437  [  192/  300]
per-ex loss: 0.558513  [  195/  300]
per-ex loss: 0.532319  [  198/  300]
per-ex loss: 0.537343  [  201/  300]
per-ex loss: 0.464562  [  204/  300]
per-ex loss: 0.597851  [  207/  300]
per-ex loss: 0.423718  [  210/  300]
per-ex loss: 0.653819  [  213/  300]
per-ex loss: 0.387925  [  216/  300]
per-ex loss: 0.505353  [  219/  300]
per-ex loss: 0.417981  [  222/  300]
per-ex loss: 0.584665  [  225/  300]
per-ex loss: 0.606303  [  228/  300]
per-ex loss: 0.619156  [  231/  300]
per-ex loss: 0.555168  [  234/  300]
per-ex loss: 0.424014  [  237/  300]
per-ex loss: 0.491763  [  240/  300]
per-ex loss: 0.428118  [  243/  300]
per-ex loss: 0.478705  [  246/  300]
per-ex loss: 0.448482  [  249/  300]
per-ex loss: 0.616594  [  252/  300]
per-ex loss: 0.450497  [  255/  300]
per-ex loss: 0.497442  [  258/  300]
per-ex loss: 0.416482  [  261/  300]
per-ex loss: 0.625756  [  264/  300]
per-ex loss: 0.536720  [  267/  300]
per-ex loss: 0.477500  [  270/  300]
per-ex loss: 0.402694  [  273/  300]
per-ex loss: 0.490654  [  276/  300]
per-ex loss: 0.500831  [  279/  300]
per-ex loss: 0.444355  [  282/  300]
per-ex loss: 0.426273  [  285/  300]
per-ex loss: 0.505202  [  288/  300]
per-ex loss: 0.478893  [  291/  300]
per-ex loss: 0.547082  [  294/  300]
per-ex loss: 0.443048  [  297/  300]
per-ex loss: 0.449273  [  300/  300]
Train Error: Avg loss: 0.49858271
validation Error: 
 Avg loss: 0.49307589 
 F1: 0.486067 
 Precision: 0.414472 
 Recall: 0.587561
 IoU: 0.321062

test Error: 
 Avg loss: 0.45557284 
 F1: 0.545028 
 Precision: 0.467313 
 Recall: 0.653747
 IoU: 0.374597

We have finished training iteration 124
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_122_.pth
per-ex loss: 0.527479  [    3/  300]
per-ex loss: 0.480652  [    6/  300]
per-ex loss: 0.472334  [    9/  300]
per-ex loss: 0.599963  [   12/  300]
per-ex loss: 0.398515  [   15/  300]
per-ex loss: 0.386534  [   18/  300]
per-ex loss: 0.575826  [   21/  300]
per-ex loss: 0.400499  [   24/  300]
per-ex loss: 0.504839  [   27/  300]
per-ex loss: 0.521834  [   30/  300]
per-ex loss: 0.446333  [   33/  300]
per-ex loss: 0.578506  [   36/  300]
per-ex loss: 0.544461  [   39/  300]
per-ex loss: 0.393205  [   42/  300]
per-ex loss: 0.505374  [   45/  300]
per-ex loss: 0.631923  [   48/  300]
per-ex loss: 0.553714  [   51/  300]
per-ex loss: 0.501554  [   54/  300]
per-ex loss: 0.410865  [   57/  300]
per-ex loss: 0.431386  [   60/  300]
per-ex loss: 0.489029  [   63/  300]
per-ex loss: 0.433926  [   66/  300]
per-ex loss: 0.492972  [   69/  300]
per-ex loss: 0.501753  [   72/  300]
per-ex loss: 0.534232  [   75/  300]
per-ex loss: 0.595411  [   78/  300]
per-ex loss: 0.625271  [   81/  300]
per-ex loss: 0.512808  [   84/  300]
per-ex loss: 0.621277  [   87/  300]
per-ex loss: 0.457401  [   90/  300]
per-ex loss: 0.591695  [   93/  300]
per-ex loss: 0.574748  [   96/  300]
per-ex loss: 0.483031  [   99/  300]
per-ex loss: 0.507780  [  102/  300]
per-ex loss: 0.404385  [  105/  300]
per-ex loss: 0.459647  [  108/  300]
per-ex loss: 0.460442  [  111/  300]
per-ex loss: 0.594750  [  114/  300]
per-ex loss: 0.439182  [  117/  300]
per-ex loss: 0.446672  [  120/  300]
per-ex loss: 0.687846  [  123/  300]
per-ex loss: 0.414323  [  126/  300]
per-ex loss: 0.473123  [  129/  300]
per-ex loss: 0.610316  [  132/  300]
per-ex loss: 0.423474  [  135/  300]
per-ex loss: 0.418395  [  138/  300]
per-ex loss: 0.498904  [  141/  300]
per-ex loss: 0.539512  [  144/  300]
per-ex loss: 0.679223  [  147/  300]
per-ex loss: 0.522032  [  150/  300]
per-ex loss: 0.438640  [  153/  300]
per-ex loss: 0.598231  [  156/  300]
per-ex loss: 0.531590  [  159/  300]
per-ex loss: 0.455283  [  162/  300]
per-ex loss: 0.510260  [  165/  300]
per-ex loss: 0.592148  [  168/  300]
per-ex loss: 0.501136  [  171/  300]
per-ex loss: 0.449943  [  174/  300]
per-ex loss: 0.528941  [  177/  300]
per-ex loss: 0.566160  [  180/  300]
per-ex loss: 0.548436  [  183/  300]
per-ex loss: 0.493712  [  186/  300]
per-ex loss: 0.570626  [  189/  300]
per-ex loss: 0.401858  [  192/  300]
per-ex loss: 0.454901  [  195/  300]
per-ex loss: 0.531682  [  198/  300]
per-ex loss: 0.440660  [  201/  300]
per-ex loss: 0.458415  [  204/  300]
per-ex loss: 0.456840  [  207/  300]
per-ex loss: 0.473220  [  210/  300]
per-ex loss: 0.449781  [  213/  300]
per-ex loss: 0.520959  [  216/  300]
per-ex loss: 0.604741  [  219/  300]
per-ex loss: 0.465499  [  222/  300]
per-ex loss: 0.463715  [  225/  300]
per-ex loss: 0.497104  [  228/  300]
per-ex loss: 0.524793  [  231/  300]
per-ex loss: 0.500924  [  234/  300]
per-ex loss: 0.458174  [  237/  300]
per-ex loss: 0.555523  [  240/  300]
per-ex loss: 0.442853  [  243/  300]
per-ex loss: 0.451832  [  246/  300]
per-ex loss: 0.545950  [  249/  300]
per-ex loss: 0.495905  [  252/  300]
per-ex loss: 0.437103  [  255/  300]
per-ex loss: 0.422163  [  258/  300]
per-ex loss: 0.476221  [  261/  300]
per-ex loss: 0.414153  [  264/  300]
per-ex loss: 0.469846  [  267/  300]
per-ex loss: 0.506561  [  270/  300]
per-ex loss: 0.572757  [  273/  300]
per-ex loss: 0.410217  [  276/  300]
per-ex loss: 0.394084  [  279/  300]
per-ex loss: 0.504280  [  282/  300]
per-ex loss: 0.528843  [  285/  300]
per-ex loss: 0.440272  [  288/  300]
per-ex loss: 0.567071  [  291/  300]
per-ex loss: 0.637637  [  294/  300]
per-ex loss: 0.537710  [  297/  300]
per-ex loss: 0.544386  [  300/  300]
Train Error: Avg loss: 0.50203088
validation Error: 
 Avg loss: 0.49925506 
 F1: 0.486053 
 Precision: 0.441137 
 Recall: 0.541153
 IoU: 0.321050

test Error: 
 Avg loss: 0.45187008 
 F1: 0.548979 
 Precision: 0.504665 
 Recall: 0.601824
 IoU: 0.378340

We have finished training iteration 125
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_123_.pth
per-ex loss: 0.441097  [    3/  300]
per-ex loss: 0.540116  [    6/  300]
per-ex loss: 0.497244  [    9/  300]
per-ex loss: 0.578191  [   12/  300]
per-ex loss: 0.446919  [   15/  300]
per-ex loss: 0.598591  [   18/  300]
per-ex loss: 0.530198  [   21/  300]
per-ex loss: 0.390832  [   24/  300]
per-ex loss: 0.493304  [   27/  300]
per-ex loss: 0.420053  [   30/  300]
per-ex loss: 0.548605  [   33/  300]
per-ex loss: 0.541764  [   36/  300]
per-ex loss: 0.544943  [   39/  300]
per-ex loss: 0.442955  [   42/  300]
per-ex loss: 0.546836  [   45/  300]
per-ex loss: 0.434044  [   48/  300]
per-ex loss: 0.600612  [   51/  300]
per-ex loss: 0.585369  [   54/  300]
per-ex loss: 0.473011  [   57/  300]
per-ex loss: 0.485311  [   60/  300]
per-ex loss: 0.572328  [   63/  300]
per-ex loss: 0.467114  [   66/  300]
per-ex loss: 0.444426  [   69/  300]
per-ex loss: 0.466213  [   72/  300]
per-ex loss: 0.526868  [   75/  300]
per-ex loss: 0.459657  [   78/  300]
per-ex loss: 0.464502  [   81/  300]
per-ex loss: 0.441042  [   84/  300]
per-ex loss: 0.405469  [   87/  300]
per-ex loss: 0.538372  [   90/  300]
per-ex loss: 0.413451  [   93/  300]
per-ex loss: 0.448901  [   96/  300]
per-ex loss: 0.406420  [   99/  300]
per-ex loss: 0.554920  [  102/  300]
per-ex loss: 0.622282  [  105/  300]
per-ex loss: 0.438613  [  108/  300]
per-ex loss: 0.455552  [  111/  300]
per-ex loss: 0.604878  [  114/  300]
per-ex loss: 0.606435  [  117/  300]
per-ex loss: 0.505515  [  120/  300]
per-ex loss: 0.591015  [  123/  300]
per-ex loss: 0.513136  [  126/  300]
per-ex loss: 0.603064  [  129/  300]
per-ex loss: 0.470500  [  132/  300]
per-ex loss: 0.462689  [  135/  300]
per-ex loss: 0.507923  [  138/  300]
per-ex loss: 0.430657  [  141/  300]
per-ex loss: 0.415016  [  144/  300]
per-ex loss: 0.431930  [  147/  300]
per-ex loss: 0.542604  [  150/  300]
per-ex loss: 0.530145  [  153/  300]
per-ex loss: 0.486267  [  156/  300]
per-ex loss: 0.535203  [  159/  300]
per-ex loss: 0.634230  [  162/  300]
per-ex loss: 0.555854  [  165/  300]
per-ex loss: 0.440300  [  168/  300]
per-ex loss: 0.439050  [  171/  300]
per-ex loss: 0.466919  [  174/  300]
per-ex loss: 0.558686  [  177/  300]
per-ex loss: 0.556500  [  180/  300]
per-ex loss: 0.656309  [  183/  300]
per-ex loss: 0.424601  [  186/  300]
per-ex loss: 0.485042  [  189/  300]
per-ex loss: 0.448515  [  192/  300]
per-ex loss: 0.442934  [  195/  300]
per-ex loss: 0.471101  [  198/  300]
per-ex loss: 0.590235  [  201/  300]
per-ex loss: 0.420695  [  204/  300]
per-ex loss: 0.471472  [  207/  300]
per-ex loss: 0.635056  [  210/  300]
per-ex loss: 0.499422  [  213/  300]
per-ex loss: 0.418123  [  216/  300]
per-ex loss: 0.480313  [  219/  300]
per-ex loss: 0.542751  [  222/  300]
per-ex loss: 0.480250  [  225/  300]
per-ex loss: 0.580874  [  228/  300]
per-ex loss: 0.565588  [  231/  300]
per-ex loss: 0.419678  [  234/  300]
per-ex loss: 0.445720  [  237/  300]
per-ex loss: 0.589105  [  240/  300]
per-ex loss: 0.410869  [  243/  300]
per-ex loss: 0.477081  [  246/  300]
per-ex loss: 0.465659  [  249/  300]
per-ex loss: 0.477579  [  252/  300]
per-ex loss: 0.517193  [  255/  300]
per-ex loss: 0.469676  [  258/  300]
per-ex loss: 0.530802  [  261/  300]
per-ex loss: 0.539714  [  264/  300]
per-ex loss: 0.571239  [  267/  300]
per-ex loss: 0.641902  [  270/  300]
per-ex loss: 0.527270  [  273/  300]
per-ex loss: 0.428138  [  276/  300]
per-ex loss: 0.496720  [  279/  300]
per-ex loss: 0.402724  [  282/  300]
per-ex loss: 0.470274  [  285/  300]
per-ex loss: 0.538062  [  288/  300]
per-ex loss: 0.444404  [  291/  300]
per-ex loss: 0.682784  [  294/  300]
per-ex loss: 0.464944  [  297/  300]
per-ex loss: 0.572083  [  300/  300]
Train Error: Avg loss: 0.50351541
validation Error: 
 Avg loss: 0.49629154 
 F1: 0.480274 
 Precision: 0.405218 
 Recall: 0.589455
 IoU: 0.316027

test Error: 
 Avg loss: 0.45496374 
 F1: 0.545719 
 Precision: 0.467654 
 Recall: 0.655067
 IoU: 0.375250

We have finished training iteration 126
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_124_.pth
per-ex loss: 0.474196  [    3/  300]
per-ex loss: 0.436079  [    6/  300]
per-ex loss: 0.547761  [    9/  300]
per-ex loss: 0.468623  [   12/  300]
per-ex loss: 0.507198  [   15/  300]
per-ex loss: 0.460986  [   18/  300]
per-ex loss: 0.580585  [   21/  300]
per-ex loss: 0.504604  [   24/  300]
per-ex loss: 0.492443  [   27/  300]
per-ex loss: 0.464181  [   30/  300]
per-ex loss: 0.490748  [   33/  300]
per-ex loss: 0.444606  [   36/  300]
per-ex loss: 0.430846  [   39/  300]
per-ex loss: 0.487697  [   42/  300]
per-ex loss: 0.574209  [   45/  300]
per-ex loss: 0.501149  [   48/  300]
per-ex loss: 0.576048  [   51/  300]
per-ex loss: 0.541755  [   54/  300]
per-ex loss: 0.503901  [   57/  300]
per-ex loss: 0.481983  [   60/  300]
per-ex loss: 0.450985  [   63/  300]
per-ex loss: 0.391336  [   66/  300]
per-ex loss: 0.392387  [   69/  300]
per-ex loss: 0.600408  [   72/  300]
per-ex loss: 0.454468  [   75/  300]
per-ex loss: 0.540753  [   78/  300]
per-ex loss: 0.383547  [   81/  300]
per-ex loss: 0.568022  [   84/  300]
per-ex loss: 0.548561  [   87/  300]
per-ex loss: 0.504947  [   90/  300]
per-ex loss: 0.525589  [   93/  300]
per-ex loss: 0.610388  [   96/  300]
per-ex loss: 0.446838  [   99/  300]
per-ex loss: 0.483096  [  102/  300]
per-ex loss: 0.478971  [  105/  300]
per-ex loss: 0.607998  [  108/  300]
per-ex loss: 0.481100  [  111/  300]
per-ex loss: 0.410610  [  114/  300]
per-ex loss: 0.660620  [  117/  300]
per-ex loss: 0.529197  [  120/  300]
per-ex loss: 0.497704  [  123/  300]
per-ex loss: 0.591107  [  126/  300]
per-ex loss: 0.440199  [  129/  300]
per-ex loss: 0.433262  [  132/  300]
per-ex loss: 0.482042  [  135/  300]
per-ex loss: 0.473540  [  138/  300]
per-ex loss: 0.409935  [  141/  300]
per-ex loss: 0.421166  [  144/  300]
per-ex loss: 0.623562  [  147/  300]
per-ex loss: 0.432207  [  150/  300]
per-ex loss: 0.506021  [  153/  300]
per-ex loss: 0.511506  [  156/  300]
per-ex loss: 0.409489  [  159/  300]
per-ex loss: 0.431280  [  162/  300]
per-ex loss: 0.577173  [  165/  300]
per-ex loss: 0.474244  [  168/  300]
per-ex loss: 0.460110  [  171/  300]
per-ex loss: 0.518627  [  174/  300]
per-ex loss: 0.498328  [  177/  300]
per-ex loss: 0.435536  [  180/  300]
per-ex loss: 0.536209  [  183/  300]
per-ex loss: 0.393864  [  186/  300]
per-ex loss: 0.561769  [  189/  300]
per-ex loss: 0.530921  [  192/  300]
per-ex loss: 0.474986  [  195/  300]
per-ex loss: 0.508701  [  198/  300]
per-ex loss: 0.486608  [  201/  300]
per-ex loss: 0.462359  [  204/  300]
per-ex loss: 0.427128  [  207/  300]
per-ex loss: 0.473509  [  210/  300]
per-ex loss: 0.612526  [  213/  300]
per-ex loss: 0.421444  [  216/  300]
per-ex loss: 0.464395  [  219/  300]
per-ex loss: 0.424919  [  222/  300]
per-ex loss: 0.409588  [  225/  300]
per-ex loss: 0.471359  [  228/  300]
per-ex loss: 0.551865  [  231/  300]
per-ex loss: 0.570637  [  234/  300]
per-ex loss: 0.618947  [  237/  300]
per-ex loss: 0.439765  [  240/  300]
per-ex loss: 0.572760  [  243/  300]
per-ex loss: 0.478884  [  246/  300]
per-ex loss: 0.476209  [  249/  300]
per-ex loss: 0.487088  [  252/  300]
per-ex loss: 0.518209  [  255/  300]
per-ex loss: 0.537989  [  258/  300]
per-ex loss: 0.558423  [  261/  300]
per-ex loss: 0.462752  [  264/  300]
per-ex loss: 0.525949  [  267/  300]
per-ex loss: 0.541107  [  270/  300]
per-ex loss: 0.566258  [  273/  300]
per-ex loss: 0.421628  [  276/  300]
per-ex loss: 0.377420  [  279/  300]
per-ex loss: 0.453500  [  282/  300]
per-ex loss: 0.518194  [  285/  300]
per-ex loss: 0.512408  [  288/  300]
per-ex loss: 0.500459  [  291/  300]
per-ex loss: 0.474310  [  294/  300]
per-ex loss: 0.485025  [  297/  300]
per-ex loss: 0.397767  [  300/  300]
Train Error: Avg loss: 0.49374288
validation Error: 
 Avg loss: 0.51741038 
 F1: 0.473434 
 Precision: 0.441983 
 Recall: 0.509705
 IoU: 0.310130

test Error: 
 Avg loss: 0.45378774 
 F1: 0.546806 
 Precision: 0.497256 
 Recall: 0.607324
 IoU: 0.376279

We have finished training iteration 127
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_125_.pth
per-ex loss: 0.483335  [    3/  300]
per-ex loss: 0.491092  [    6/  300]
per-ex loss: 0.450849  [    9/  300]
per-ex loss: 0.443699  [   12/  300]
per-ex loss: 0.426452  [   15/  300]
per-ex loss: 0.414619  [   18/  300]
per-ex loss: 0.447750  [   21/  300]
per-ex loss: 0.505819  [   24/  300]
per-ex loss: 0.598130  [   27/  300]
per-ex loss: 0.551612  [   30/  300]
per-ex loss: 0.565170  [   33/  300]
per-ex loss: 0.511722  [   36/  300]
per-ex loss: 0.516847  [   39/  300]
per-ex loss: 0.525870  [   42/  300]
per-ex loss: 0.504500  [   45/  300]
per-ex loss: 0.512398  [   48/  300]
per-ex loss: 0.504751  [   51/  300]
per-ex loss: 0.508691  [   54/  300]
per-ex loss: 0.388711  [   57/  300]
per-ex loss: 0.463899  [   60/  300]
per-ex loss: 0.460737  [   63/  300]
per-ex loss: 0.529802  [   66/  300]
per-ex loss: 0.500052  [   69/  300]
per-ex loss: 0.479282  [   72/  300]
per-ex loss: 0.424899  [   75/  300]
per-ex loss: 0.411561  [   78/  300]
per-ex loss: 0.503484  [   81/  300]
per-ex loss: 0.578530  [   84/  300]
per-ex loss: 0.474322  [   87/  300]
per-ex loss: 0.605493  [   90/  300]
per-ex loss: 0.466341  [   93/  300]
per-ex loss: 0.458673  [   96/  300]
per-ex loss: 0.473438  [   99/  300]
per-ex loss: 0.454562  [  102/  300]
per-ex loss: 0.421336  [  105/  300]
per-ex loss: 0.555150  [  108/  300]
per-ex loss: 0.496870  [  111/  300]
per-ex loss: 0.617182  [  114/  300]
per-ex loss: 0.523789  [  117/  300]
per-ex loss: 0.509822  [  120/  300]
per-ex loss: 0.671914  [  123/  300]
per-ex loss: 0.496419  [  126/  300]
per-ex loss: 0.506943  [  129/  300]
per-ex loss: 0.675061  [  132/  300]
per-ex loss: 0.368441  [  135/  300]
per-ex loss: 0.464106  [  138/  300]
per-ex loss: 0.446245  [  141/  300]
per-ex loss: 0.466115  [  144/  300]
per-ex loss: 0.506882  [  147/  300]
per-ex loss: 0.651404  [  150/  300]
per-ex loss: 0.554012  [  153/  300]
per-ex loss: 0.497822  [  156/  300]
per-ex loss: 0.463198  [  159/  300]
per-ex loss: 0.422340  [  162/  300]
per-ex loss: 0.414256  [  165/  300]
per-ex loss: 0.431390  [  168/  300]
per-ex loss: 0.485205  [  171/  300]
per-ex loss: 0.579633  [  174/  300]
per-ex loss: 0.532831  [  177/  300]
per-ex loss: 0.576275  [  180/  300]
per-ex loss: 0.557193  [  183/  300]
per-ex loss: 0.396399  [  186/  300]
per-ex loss: 0.423780  [  189/  300]
per-ex loss: 0.446732  [  192/  300]
per-ex loss: 0.516989  [  195/  300]
per-ex loss: 0.467192  [  198/  300]
per-ex loss: 0.446566  [  201/  300]
per-ex loss: 0.438995  [  204/  300]
per-ex loss: 0.552532  [  207/  300]
per-ex loss: 0.575511  [  210/  300]
per-ex loss: 0.418737  [  213/  300]
per-ex loss: 0.519811  [  216/  300]
per-ex loss: 0.669321  [  219/  300]
per-ex loss: 0.444815  [  222/  300]
per-ex loss: 0.428922  [  225/  300]
per-ex loss: 0.540704  [  228/  300]
per-ex loss: 0.621608  [  231/  300]
per-ex loss: 0.602235  [  234/  300]
per-ex loss: 0.584561  [  237/  300]
per-ex loss: 0.439836  [  240/  300]
per-ex loss: 0.475562  [  243/  300]
per-ex loss: 0.476574  [  246/  300]
per-ex loss: 0.486545  [  249/  300]
per-ex loss: 0.436027  [  252/  300]
per-ex loss: 0.468644  [  255/  300]
per-ex loss: 0.434670  [  258/  300]
per-ex loss: 0.516185  [  261/  300]
per-ex loss: 0.478257  [  264/  300]
per-ex loss: 0.421883  [  267/  300]
per-ex loss: 0.452478  [  270/  300]
per-ex loss: 0.440340  [  273/  300]
per-ex loss: 0.430204  [  276/  300]
per-ex loss: 0.561559  [  279/  300]
per-ex loss: 0.562593  [  282/  300]
per-ex loss: 0.446439  [  285/  300]
per-ex loss: 0.605331  [  288/  300]
per-ex loss: 0.440337  [  291/  300]
per-ex loss: 0.602231  [  294/  300]
per-ex loss: 0.409350  [  297/  300]
per-ex loss: 0.424009  [  300/  300]
Train Error: Avg loss: 0.49633390
validation Error: 
 Avg loss: 0.51261626 
 F1: 0.473022 
 Precision: 0.441351 
 Recall: 0.509589
 IoU: 0.309776

test Error: 
 Avg loss: 0.45410734 
 F1: 0.546608 
 Precision: 0.494442 
 Recall: 0.611080
 IoU: 0.376091

We have finished training iteration 128
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_126_.pth
per-ex loss: 0.665179  [    3/  300]
per-ex loss: 0.517151  [    6/  300]
per-ex loss: 0.487043  [    9/  300]
per-ex loss: 0.672191  [   12/  300]
per-ex loss: 0.453120  [   15/  300]
per-ex loss: 0.558175  [   18/  300]
per-ex loss: 0.459627  [   21/  300]
per-ex loss: 0.403007  [   24/  300]
per-ex loss: 0.587602  [   27/  300]
per-ex loss: 0.476230  [   30/  300]
per-ex loss: 0.612450  [   33/  300]
per-ex loss: 0.657609  [   36/  300]
per-ex loss: 0.494026  [   39/  300]
per-ex loss: 0.439061  [   42/  300]
per-ex loss: 0.512782  [   45/  300]
per-ex loss: 0.524212  [   48/  300]
per-ex loss: 0.451361  [   51/  300]
per-ex loss: 0.446288  [   54/  300]
per-ex loss: 0.513255  [   57/  300]
per-ex loss: 0.427135  [   60/  300]
per-ex loss: 0.487177  [   63/  300]
per-ex loss: 0.408286  [   66/  300]
per-ex loss: 0.410636  [   69/  300]
per-ex loss: 0.496808  [   72/  300]
per-ex loss: 0.499912  [   75/  300]
per-ex loss: 0.472273  [   78/  300]
per-ex loss: 0.484678  [   81/  300]
per-ex loss: 0.534951  [   84/  300]
per-ex loss: 0.492479  [   87/  300]
per-ex loss: 0.502068  [   90/  300]
per-ex loss: 0.434761  [   93/  300]
per-ex loss: 0.420950  [   96/  300]
per-ex loss: 0.612147  [   99/  300]
per-ex loss: 0.406865  [  102/  300]
per-ex loss: 0.520180  [  105/  300]
per-ex loss: 0.510229  [  108/  300]
per-ex loss: 0.510902  [  111/  300]
per-ex loss: 0.469649  [  114/  300]
per-ex loss: 0.540820  [  117/  300]
per-ex loss: 0.492223  [  120/  300]
per-ex loss: 0.602079  [  123/  300]
per-ex loss: 0.501974  [  126/  300]
per-ex loss: 0.510133  [  129/  300]
per-ex loss: 0.489290  [  132/  300]
per-ex loss: 0.557105  [  135/  300]
per-ex loss: 0.586731  [  138/  300]
per-ex loss: 0.554304  [  141/  300]
per-ex loss: 0.381185  [  144/  300]
per-ex loss: 0.526703  [  147/  300]
per-ex loss: 0.446486  [  150/  300]
per-ex loss: 0.481816  [  153/  300]
per-ex loss: 0.501599  [  156/  300]
per-ex loss: 0.439875  [  159/  300]
per-ex loss: 0.391336  [  162/  300]
per-ex loss: 0.492103  [  165/  300]
per-ex loss: 0.484708  [  168/  300]
per-ex loss: 0.448759  [  171/  300]
per-ex loss: 0.564448  [  174/  300]
per-ex loss: 0.606453  [  177/  300]
per-ex loss: 0.483435  [  180/  300]
per-ex loss: 0.581449  [  183/  300]
per-ex loss: 0.468119  [  186/  300]
per-ex loss: 0.443311  [  189/  300]
per-ex loss: 0.390330  [  192/  300]
per-ex loss: 0.563283  [  195/  300]
per-ex loss: 0.586752  [  198/  300]
per-ex loss: 0.488965  [  201/  300]
per-ex loss: 0.440761  [  204/  300]
per-ex loss: 0.462229  [  207/  300]
per-ex loss: 0.422897  [  210/  300]
per-ex loss: 0.484927  [  213/  300]
per-ex loss: 0.614512  [  216/  300]
per-ex loss: 0.539979  [  219/  300]
per-ex loss: 0.515003  [  222/  300]
per-ex loss: 0.556510  [  225/  300]
per-ex loss: 0.513062  [  228/  300]
per-ex loss: 0.459571  [  231/  300]
per-ex loss: 0.483867  [  234/  300]
per-ex loss: 0.606946  [  237/  300]
per-ex loss: 0.550693  [  240/  300]
per-ex loss: 0.421957  [  243/  300]
per-ex loss: 0.595759  [  246/  300]
per-ex loss: 0.450924  [  249/  300]
per-ex loss: 0.571548  [  252/  300]
per-ex loss: 0.482085  [  255/  300]
per-ex loss: 0.500920  [  258/  300]
per-ex loss: 0.444965  [  261/  300]
per-ex loss: 0.466058  [  264/  300]
per-ex loss: 0.639023  [  267/  300]
per-ex loss: 0.405110  [  270/  300]
per-ex loss: 0.520113  [  273/  300]
per-ex loss: 0.552946  [  276/  300]
per-ex loss: 0.438358  [  279/  300]
per-ex loss: 0.403954  [  282/  300]
per-ex loss: 0.435406  [  285/  300]
per-ex loss: 0.452258  [  288/  300]
per-ex loss: 0.623869  [  291/  300]
per-ex loss: 0.524370  [  294/  300]
per-ex loss: 0.619834  [  297/  300]
per-ex loss: 0.423523  [  300/  300]
Train Error: Avg loss: 0.50262164
validation Error: 
 Avg loss: 0.49490384 
 F1: 0.486916 
 Precision: 0.430453 
 Recall: 0.560427
 IoU: 0.321803

test Error: 
 Avg loss: 0.45494372 
 F1: 0.545698 
 Precision: 0.484386 
 Recall: 0.624782
 IoU: 0.375230

We have finished training iteration 129
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_127_.pth
per-ex loss: 0.542619  [    3/  300]
per-ex loss: 0.489965  [    6/  300]
per-ex loss: 0.463176  [    9/  300]
per-ex loss: 0.525605  [   12/  300]
per-ex loss: 0.497237  [   15/  300]
per-ex loss: 0.410080  [   18/  300]
per-ex loss: 0.426224  [   21/  300]
per-ex loss: 0.570548  [   24/  300]
per-ex loss: 0.525728  [   27/  300]
per-ex loss: 0.423722  [   30/  300]
per-ex loss: 0.422426  [   33/  300]
per-ex loss: 0.476081  [   36/  300]
per-ex loss: 0.539963  [   39/  300]
per-ex loss: 0.454865  [   42/  300]
per-ex loss: 0.415903  [   45/  300]
per-ex loss: 0.562280  [   48/  300]
per-ex loss: 0.437737  [   51/  300]
per-ex loss: 0.434865  [   54/  300]
per-ex loss: 0.524706  [   57/  300]
per-ex loss: 0.382519  [   60/  300]
per-ex loss: 0.501887  [   63/  300]
per-ex loss: 0.611574  [   66/  300]
per-ex loss: 0.437562  [   69/  300]
per-ex loss: 0.525345  [   72/  300]
per-ex loss: 0.461459  [   75/  300]
per-ex loss: 0.568726  [   78/  300]
per-ex loss: 0.659943  [   81/  300]
per-ex loss: 0.543263  [   84/  300]
per-ex loss: 0.584255  [   87/  300]
per-ex loss: 0.615800  [   90/  300]
per-ex loss: 0.501938  [   93/  300]
per-ex loss: 0.524945  [   96/  300]
per-ex loss: 0.443815  [   99/  300]
per-ex loss: 0.479357  [  102/  300]
per-ex loss: 0.521208  [  105/  300]
per-ex loss: 0.547282  [  108/  300]
per-ex loss: 0.572554  [  111/  300]
per-ex loss: 0.509796  [  114/  300]
per-ex loss: 0.395982  [  117/  300]
per-ex loss: 0.492906  [  120/  300]
per-ex loss: 0.431695  [  123/  300]
per-ex loss: 0.617292  [  126/  300]
per-ex loss: 0.426868  [  129/  300]
per-ex loss: 0.440184  [  132/  300]
per-ex loss: 0.399305  [  135/  300]
per-ex loss: 0.456877  [  138/  300]
per-ex loss: 0.421360  [  141/  300]
per-ex loss: 0.557354  [  144/  300]
per-ex loss: 0.582383  [  147/  300]
per-ex loss: 0.413705  [  150/  300]
per-ex loss: 0.580523  [  153/  300]
per-ex loss: 0.500465  [  156/  300]
per-ex loss: 0.490873  [  159/  300]
per-ex loss: 0.604104  [  162/  300]
per-ex loss: 0.579491  [  165/  300]
per-ex loss: 0.442766  [  168/  300]
per-ex loss: 0.474022  [  171/  300]
per-ex loss: 0.585448  [  174/  300]
per-ex loss: 0.510558  [  177/  300]
per-ex loss: 0.505852  [  180/  300]
per-ex loss: 0.562496  [  183/  300]
per-ex loss: 0.380422  [  186/  300]
per-ex loss: 0.482334  [  189/  300]
per-ex loss: 0.435013  [  192/  300]
per-ex loss: 0.475456  [  195/  300]
per-ex loss: 0.457968  [  198/  300]
per-ex loss: 0.456648  [  201/  300]
per-ex loss: 0.469869  [  204/  300]
per-ex loss: 0.638369  [  207/  300]
per-ex loss: 0.482813  [  210/  300]
per-ex loss: 0.482742  [  213/  300]
per-ex loss: 0.615092  [  216/  300]
per-ex loss: 0.469525  [  219/  300]
per-ex loss: 0.654486  [  222/  300]
per-ex loss: 0.415578  [  225/  300]
per-ex loss: 0.487394  [  228/  300]
per-ex loss: 0.579680  [  231/  300]
per-ex loss: 0.486703  [  234/  300]
per-ex loss: 0.434041  [  237/  300]
per-ex loss: 0.608489  [  240/  300]
per-ex loss: 0.561524  [  243/  300]
per-ex loss: 0.549990  [  246/  300]
per-ex loss: 0.525060  [  249/  300]
per-ex loss: 0.465406  [  252/  300]
per-ex loss: 0.512977  [  255/  300]
per-ex loss: 0.534345  [  258/  300]
per-ex loss: 0.488961  [  261/  300]
per-ex loss: 0.484372  [  264/  300]
per-ex loss: 0.631950  [  267/  300]
per-ex loss: 0.415680  [  270/  300]
per-ex loss: 0.522713  [  273/  300]
per-ex loss: 0.561561  [  276/  300]
per-ex loss: 0.619903  [  279/  300]
per-ex loss: 0.592927  [  282/  300]
per-ex loss: 0.423253  [  285/  300]
per-ex loss: 0.521676  [  288/  300]
per-ex loss: 0.490442  [  291/  300]
per-ex loss: 0.416698  [  294/  300]
per-ex loss: 0.456322  [  297/  300]
per-ex loss: 0.484577  [  300/  300]
Train Error: Avg loss: 0.50382426
validation Error: 
 Avg loss: 0.52955391 
 F1: 0.462011 
 Precision: 0.455287 
 Recall: 0.468936
 IoU: 0.300399

test Error: 
 Avg loss: 0.45872855 
 F1: 0.542066 
 Precision: 0.509144 
 Recall: 0.579539
 IoU: 0.371804

We have finished training iteration 130
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_128_.pth
per-ex loss: 0.478186  [    3/  300]
per-ex loss: 0.411970  [    6/  300]
per-ex loss: 0.552187  [    9/  300]
per-ex loss: 0.572544  [   12/  300]
per-ex loss: 0.405019  [   15/  300]
per-ex loss: 0.571953  [   18/  300]
per-ex loss: 0.441819  [   21/  300]
per-ex loss: 0.439911  [   24/  300]
per-ex loss: 0.496887  [   27/  300]
per-ex loss: 0.544868  [   30/  300]
per-ex loss: 0.555872  [   33/  300]
per-ex loss: 0.587096  [   36/  300]
per-ex loss: 0.489767  [   39/  300]
per-ex loss: 0.434387  [   42/  300]
per-ex loss: 0.507477  [   45/  300]
per-ex loss: 0.446799  [   48/  300]
per-ex loss: 0.445687  [   51/  300]
per-ex loss: 0.495335  [   54/  300]
per-ex loss: 0.442152  [   57/  300]
per-ex loss: 0.566250  [   60/  300]
per-ex loss: 0.547713  [   63/  300]
per-ex loss: 0.487622  [   66/  300]
per-ex loss: 0.610869  [   69/  300]
per-ex loss: 0.501394  [   72/  300]
per-ex loss: 0.566285  [   75/  300]
per-ex loss: 0.438144  [   78/  300]
per-ex loss: 0.501175  [   81/  300]
per-ex loss: 0.466393  [   84/  300]
per-ex loss: 0.574678  [   87/  300]
per-ex loss: 0.647997  [   90/  300]
per-ex loss: 0.486835  [   93/  300]
per-ex loss: 0.576234  [   96/  300]
per-ex loss: 0.563406  [   99/  300]
per-ex loss: 0.427896  [  102/  300]
per-ex loss: 0.395432  [  105/  300]
per-ex loss: 0.425508  [  108/  300]
per-ex loss: 0.597094  [  111/  300]
per-ex loss: 0.450375  [  114/  300]
per-ex loss: 0.424070  [  117/  300]
per-ex loss: 0.452339  [  120/  300]
per-ex loss: 0.556033  [  123/  300]
per-ex loss: 0.475876  [  126/  300]
per-ex loss: 0.516289  [  129/  300]
per-ex loss: 0.452812  [  132/  300]
per-ex loss: 0.405860  [  135/  300]
per-ex loss: 0.525732  [  138/  300]
per-ex loss: 0.534543  [  141/  300]
per-ex loss: 0.518457  [  144/  300]
per-ex loss: 0.503132  [  147/  300]
per-ex loss: 0.471281  [  150/  300]
per-ex loss: 0.506898  [  153/  300]
per-ex loss: 0.450551  [  156/  300]
per-ex loss: 0.515752  [  159/  300]
per-ex loss: 0.596711  [  162/  300]
per-ex loss: 0.670512  [  165/  300]
per-ex loss: 0.545624  [  168/  300]
per-ex loss: 0.496030  [  171/  300]
per-ex loss: 0.404790  [  174/  300]
per-ex loss: 0.460263  [  177/  300]
per-ex loss: 0.546727  [  180/  300]
per-ex loss: 0.518355  [  183/  300]
per-ex loss: 0.523221  [  186/  300]
per-ex loss: 0.591635  [  189/  300]
per-ex loss: 0.514506  [  192/  300]
per-ex loss: 0.411084  [  195/  300]
per-ex loss: 0.514953  [  198/  300]
per-ex loss: 0.600073  [  201/  300]
per-ex loss: 0.510979  [  204/  300]
per-ex loss: 0.402410  [  207/  300]
per-ex loss: 0.394618  [  210/  300]
per-ex loss: 0.437795  [  213/  300]
per-ex loss: 0.582451  [  216/  300]
per-ex loss: 0.468974  [  219/  300]
per-ex loss: 0.567895  [  222/  300]
per-ex loss: 0.397022  [  225/  300]
per-ex loss: 0.680304  [  228/  300]
per-ex loss: 0.529971  [  231/  300]
per-ex loss: 0.537097  [  234/  300]
per-ex loss: 0.407994  [  237/  300]
per-ex loss: 0.488699  [  240/  300]
per-ex loss: 0.478928  [  243/  300]
per-ex loss: 0.547069  [  246/  300]
per-ex loss: 0.392723  [  249/  300]
per-ex loss: 0.480024  [  252/  300]
per-ex loss: 0.584320  [  255/  300]
per-ex loss: 0.425852  [  258/  300]
per-ex loss: 0.397659  [  261/  300]
per-ex loss: 0.559837  [  264/  300]
per-ex loss: 0.537073  [  267/  300]
per-ex loss: 0.548005  [  270/  300]
per-ex loss: 0.611907  [  273/  300]
per-ex loss: 0.439596  [  276/  300]
per-ex loss: 0.551801  [  279/  300]
per-ex loss: 0.501919  [  282/  300]
per-ex loss: 0.463652  [  285/  300]
per-ex loss: 0.443362  [  288/  300]
per-ex loss: 0.447196  [  291/  300]
per-ex loss: 0.619713  [  294/  300]
per-ex loss: 0.516429  [  297/  300]
per-ex loss: 0.475604  [  300/  300]
Train Error: Avg loss: 0.50284201
validation Error: 
 Avg loss: 0.51105968 
 F1: 0.473105 
 Precision: 0.438787 
 Recall: 0.513245
 IoU: 0.309847

test Error: 
 Avg loss: 0.44982338 
 F1: 0.550827 
 Precision: 0.495838 
 Recall: 0.619535
 IoU: 0.380098

We have finished training iteration 131
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_129_.pth
per-ex loss: 0.581453  [    3/  300]
per-ex loss: 0.390648  [    6/  300]
per-ex loss: 0.471981  [    9/  300]
per-ex loss: 0.494567  [   12/  300]
per-ex loss: 0.483122  [   15/  300]
per-ex loss: 0.438021  [   18/  300]
per-ex loss: 0.583179  [   21/  300]
per-ex loss: 0.442462  [   24/  300]
per-ex loss: 0.466231  [   27/  300]
per-ex loss: 0.522779  [   30/  300]
per-ex loss: 0.482704  [   33/  300]
per-ex loss: 0.498368  [   36/  300]
per-ex loss: 0.537948  [   39/  300]
per-ex loss: 0.488222  [   42/  300]
per-ex loss: 0.659626  [   45/  300]
per-ex loss: 0.420133  [   48/  300]
per-ex loss: 0.458004  [   51/  300]
per-ex loss: 0.536901  [   54/  300]
per-ex loss: 0.582315  [   57/  300]
per-ex loss: 0.436875  [   60/  300]
per-ex loss: 0.385556  [   63/  300]
per-ex loss: 0.520325  [   66/  300]
per-ex loss: 0.527773  [   69/  300]
per-ex loss: 0.457831  [   72/  300]
per-ex loss: 0.612018  [   75/  300]
per-ex loss: 0.459978  [   78/  300]
per-ex loss: 0.618445  [   81/  300]
per-ex loss: 0.451536  [   84/  300]
per-ex loss: 0.603043  [   87/  300]
per-ex loss: 0.533015  [   90/  300]
per-ex loss: 0.436232  [   93/  300]
per-ex loss: 0.446467  [   96/  300]
per-ex loss: 0.550086  [   99/  300]
per-ex loss: 0.465100  [  102/  300]
per-ex loss: 0.435927  [  105/  300]
per-ex loss: 0.550986  [  108/  300]
per-ex loss: 0.420073  [  111/  300]
per-ex loss: 0.430837  [  114/  300]
per-ex loss: 0.507479  [  117/  300]
per-ex loss: 0.472749  [  120/  300]
per-ex loss: 0.476884  [  123/  300]
per-ex loss: 0.514778  [  126/  300]
per-ex loss: 0.428350  [  129/  300]
per-ex loss: 0.537718  [  132/  300]
per-ex loss: 0.522900  [  135/  300]
per-ex loss: 0.536507  [  138/  300]
per-ex loss: 0.399474  [  141/  300]
per-ex loss: 0.685939  [  144/  300]
per-ex loss: 0.456522  [  147/  300]
per-ex loss: 0.641671  [  150/  300]
per-ex loss: 0.562798  [  153/  300]
per-ex loss: 0.488950  [  156/  300]
per-ex loss: 0.504742  [  159/  300]
per-ex loss: 0.535339  [  162/  300]
per-ex loss: 0.484899  [  165/  300]
per-ex loss: 0.526172  [  168/  300]
per-ex loss: 0.526704  [  171/  300]
per-ex loss: 0.532978  [  174/  300]
per-ex loss: 0.429134  [  177/  300]
per-ex loss: 0.609526  [  180/  300]
per-ex loss: 0.401920  [  183/  300]
per-ex loss: 0.595177  [  186/  300]
per-ex loss: 0.390842  [  189/  300]
per-ex loss: 0.469194  [  192/  300]
per-ex loss: 0.437439  [  195/  300]
per-ex loss: 0.496708  [  198/  300]
per-ex loss: 0.625220  [  201/  300]
per-ex loss: 0.492451  [  204/  300]
per-ex loss: 0.664550  [  207/  300]
per-ex loss: 0.516856  [  210/  300]
per-ex loss: 0.450701  [  213/  300]
per-ex loss: 0.557103  [  216/  300]
per-ex loss: 0.573945  [  219/  300]
per-ex loss: 0.530310  [  222/  300]
per-ex loss: 0.583112  [  225/  300]
per-ex loss: 0.403592  [  228/  300]
per-ex loss: 0.594050  [  231/  300]
per-ex loss: 0.524445  [  234/  300]
per-ex loss: 0.512924  [  237/  300]
per-ex loss: 0.410426  [  240/  300]
per-ex loss: 0.629485  [  243/  300]
per-ex loss: 0.497367  [  246/  300]
per-ex loss: 0.482349  [  249/  300]
per-ex loss: 0.612981  [  252/  300]
per-ex loss: 0.480611  [  255/  300]
per-ex loss: 0.462586  [  258/  300]
per-ex loss: 0.468964  [  261/  300]
per-ex loss: 0.409945  [  264/  300]
per-ex loss: 0.538190  [  267/  300]
per-ex loss: 0.439557  [  270/  300]
per-ex loss: 0.469774  [  273/  300]
per-ex loss: 0.480088  [  276/  300]
per-ex loss: 0.522925  [  279/  300]
per-ex loss: 0.457450  [  282/  300]
per-ex loss: 0.615589  [  285/  300]
per-ex loss: 0.452090  [  288/  300]
per-ex loss: 0.469444  [  291/  300]
per-ex loss: 0.480310  [  294/  300]
per-ex loss: 0.515974  [  297/  300]
per-ex loss: 0.462294  [  300/  300]
Train Error: Avg loss: 0.50441920
validation Error: 
 Avg loss: 0.59725632 
 F1: 0.418902 
 Precision: 0.466394 
 Recall: 0.380187
 IoU: 0.264943

test Error: 
 Avg loss: 0.46232778 
 F1: 0.538349 
 Precision: 0.537281 
 Recall: 0.539421
 IoU: 0.368316

We have finished training iteration 132
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_130_.pth
per-ex loss: 0.533477  [    3/  300]
per-ex loss: 0.443018  [    6/  300]
per-ex loss: 0.592490  [    9/  300]
per-ex loss: 0.476500  [   12/  300]
per-ex loss: 0.483744  [   15/  300]
per-ex loss: 0.518088  [   18/  300]
per-ex loss: 0.600128  [   21/  300]
per-ex loss: 0.455712  [   24/  300]
per-ex loss: 0.522583  [   27/  300]
per-ex loss: 0.522159  [   30/  300]
per-ex loss: 0.510382  [   33/  300]
per-ex loss: 0.553465  [   36/  300]
per-ex loss: 0.570298  [   39/  300]
per-ex loss: 0.446606  [   42/  300]
per-ex loss: 0.491125  [   45/  300]
per-ex loss: 0.408938  [   48/  300]
per-ex loss: 0.506021  [   51/  300]
per-ex loss: 0.512072  [   54/  300]
per-ex loss: 0.514387  [   57/  300]
per-ex loss: 0.515142  [   60/  300]
per-ex loss: 0.485969  [   63/  300]
per-ex loss: 0.498982  [   66/  300]
per-ex loss: 0.431029  [   69/  300]
per-ex loss: 0.442994  [   72/  300]
per-ex loss: 0.429297  [   75/  300]
per-ex loss: 0.515269  [   78/  300]
per-ex loss: 0.395633  [   81/  300]
per-ex loss: 0.445289  [   84/  300]
per-ex loss: 0.487678  [   87/  300]
per-ex loss: 0.462097  [   90/  300]
per-ex loss: 0.449541  [   93/  300]
per-ex loss: 0.571199  [   96/  300]
per-ex loss: 0.374207  [   99/  300]
per-ex loss: 0.619593  [  102/  300]
per-ex loss: 0.520376  [  105/  300]
per-ex loss: 0.443969  [  108/  300]
per-ex loss: 0.544084  [  111/  300]
per-ex loss: 0.441597  [  114/  300]
per-ex loss: 0.434066  [  117/  300]
per-ex loss: 0.666532  [  120/  300]
per-ex loss: 0.620963  [  123/  300]
per-ex loss: 0.637619  [  126/  300]
per-ex loss: 0.434456  [  129/  300]
per-ex loss: 0.506199  [  132/  300]
per-ex loss: 0.498694  [  135/  300]
per-ex loss: 0.453199  [  138/  300]
per-ex loss: 0.583538  [  141/  300]
per-ex loss: 0.500616  [  144/  300]
per-ex loss: 0.529037  [  147/  300]
per-ex loss: 0.418835  [  150/  300]
per-ex loss: 0.562198  [  153/  300]
per-ex loss: 0.443639  [  156/  300]
per-ex loss: 0.522930  [  159/  300]
per-ex loss: 0.412308  [  162/  300]
per-ex loss: 0.484442  [  165/  300]
per-ex loss: 0.490925  [  168/  300]
per-ex loss: 0.533790  [  171/  300]
per-ex loss: 0.573946  [  174/  300]
per-ex loss: 0.557848  [  177/  300]
per-ex loss: 0.566923  [  180/  300]
per-ex loss: 0.602925  [  183/  300]
per-ex loss: 0.435947  [  186/  300]
per-ex loss: 0.465286  [  189/  300]
per-ex loss: 0.498498  [  192/  300]
per-ex loss: 0.388609  [  195/  300]
per-ex loss: 0.507282  [  198/  300]
per-ex loss: 0.442606  [  201/  300]
per-ex loss: 0.504866  [  204/  300]
per-ex loss: 0.512037  [  207/  300]
per-ex loss: 0.399063  [  210/  300]
per-ex loss: 0.521954  [  213/  300]
per-ex loss: 0.534565  [  216/  300]
per-ex loss: 0.595114  [  219/  300]
per-ex loss: 0.436052  [  222/  300]
per-ex loss: 0.560277  [  225/  300]
per-ex loss: 0.527818  [  228/  300]
per-ex loss: 0.596344  [  231/  300]
per-ex loss: 0.503205  [  234/  300]
per-ex loss: 0.663469  [  237/  300]
per-ex loss: 0.490793  [  240/  300]
per-ex loss: 0.479204  [  243/  300]
per-ex loss: 0.456675  [  246/  300]
per-ex loss: 0.443302  [  249/  300]
per-ex loss: 0.495088  [  252/  300]
per-ex loss: 0.422156  [  255/  300]
per-ex loss: 0.402383  [  258/  300]
per-ex loss: 0.658208  [  261/  300]
per-ex loss: 0.481253  [  264/  300]
per-ex loss: 0.530511  [  267/  300]
per-ex loss: 0.461276  [  270/  300]
per-ex loss: 0.464359  [  273/  300]
per-ex loss: 0.450264  [  276/  300]
per-ex loss: 0.511500  [  279/  300]
per-ex loss: 0.570897  [  282/  300]
per-ex loss: 0.447162  [  285/  300]
per-ex loss: 0.533859  [  288/  300]
per-ex loss: 0.461085  [  291/  300]
per-ex loss: 0.519439  [  294/  300]
per-ex loss: 0.489577  [  297/  300]
per-ex loss: 0.518385  [  300/  300]
Train Error: Avg loss: 0.50151133
validation Error: 
 Avg loss: 0.49324685 
 F1: 0.487008 
 Precision: 0.426540 
 Recall: 0.567451
 IoU: 0.321884

test Error: 
 Avg loss: 0.45274228 
 F1: 0.547961 
 Precision: 0.486775 
 Recall: 0.626741
 IoU: 0.377374

We have finished training iteration 133
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_131_.pth
per-ex loss: 0.486474  [    3/  300]
per-ex loss: 0.503568  [    6/  300]
per-ex loss: 0.560916  [    9/  300]
per-ex loss: 0.464038  [   12/  300]
per-ex loss: 0.618071  [   15/  300]
per-ex loss: 0.486047  [   18/  300]
per-ex loss: 0.620965  [   21/  300]
per-ex loss: 0.564696  [   24/  300]
per-ex loss: 0.456319  [   27/  300]
per-ex loss: 0.448370  [   30/  300]
per-ex loss: 0.462868  [   33/  300]
per-ex loss: 0.493092  [   36/  300]
per-ex loss: 0.582260  [   39/  300]
per-ex loss: 0.493094  [   42/  300]
per-ex loss: 0.634682  [   45/  300]
per-ex loss: 0.540504  [   48/  300]
per-ex loss: 0.448892  [   51/  300]
per-ex loss: 0.411236  [   54/  300]
per-ex loss: 0.479166  [   57/  300]
per-ex loss: 0.441164  [   60/  300]
per-ex loss: 0.633678  [   63/  300]
per-ex loss: 0.402314  [   66/  300]
per-ex loss: 0.396961  [   69/  300]
per-ex loss: 0.544632  [   72/  300]
per-ex loss: 0.437561  [   75/  300]
per-ex loss: 0.519550  [   78/  300]
per-ex loss: 0.498069  [   81/  300]
per-ex loss: 0.482969  [   84/  300]
per-ex loss: 0.448500  [   87/  300]
per-ex loss: 0.528491  [   90/  300]
per-ex loss: 0.466204  [   93/  300]
per-ex loss: 0.434349  [   96/  300]
per-ex loss: 0.594135  [   99/  300]
per-ex loss: 0.519006  [  102/  300]
per-ex loss: 0.477024  [  105/  300]
per-ex loss: 0.625905  [  108/  300]
per-ex loss: 0.581641  [  111/  300]
per-ex loss: 0.487321  [  114/  300]
per-ex loss: 0.582747  [  117/  300]
per-ex loss: 0.445755  [  120/  300]
per-ex loss: 0.484407  [  123/  300]
per-ex loss: 0.592137  [  126/  300]
per-ex loss: 0.586282  [  129/  300]
per-ex loss: 0.479329  [  132/  300]
per-ex loss: 0.643696  [  135/  300]
per-ex loss: 0.584669  [  138/  300]
per-ex loss: 0.435627  [  141/  300]
per-ex loss: 0.636630  [  144/  300]
per-ex loss: 0.435381  [  147/  300]
per-ex loss: 0.557453  [  150/  300]
per-ex loss: 0.414497  [  153/  300]
per-ex loss: 0.382987  [  156/  300]
per-ex loss: 0.441349  [  159/  300]
per-ex loss: 0.452728  [  162/  300]
per-ex loss: 0.371081  [  165/  300]
per-ex loss: 0.415883  [  168/  300]
per-ex loss: 0.522850  [  171/  300]
per-ex loss: 0.452370  [  174/  300]
per-ex loss: 0.442397  [  177/  300]
per-ex loss: 0.440369  [  180/  300]
per-ex loss: 0.612785  [  183/  300]
per-ex loss: 0.460551  [  186/  300]
per-ex loss: 0.503138  [  189/  300]
per-ex loss: 0.520649  [  192/  300]
per-ex loss: 0.447192  [  195/  300]
per-ex loss: 0.538692  [  198/  300]
per-ex loss: 0.602768  [  201/  300]
per-ex loss: 0.440137  [  204/  300]
per-ex loss: 0.643488  [  207/  300]
per-ex loss: 0.455019  [  210/  300]
per-ex loss: 0.506253  [  213/  300]
per-ex loss: 0.547050  [  216/  300]
per-ex loss: 0.414952  [  219/  300]
per-ex loss: 0.412542  [  222/  300]
per-ex loss: 0.524920  [  225/  300]
per-ex loss: 0.511577  [  228/  300]
per-ex loss: 0.523443  [  231/  300]
per-ex loss: 0.425901  [  234/  300]
per-ex loss: 0.541931  [  237/  300]
per-ex loss: 0.460601  [  240/  300]
per-ex loss: 0.608200  [  243/  300]
per-ex loss: 0.572196  [  246/  300]
per-ex loss: 0.363860  [  249/  300]
per-ex loss: 0.668233  [  252/  300]
per-ex loss: 0.556245  [  255/  300]
per-ex loss: 0.409776  [  258/  300]
per-ex loss: 0.527040  [  261/  300]
per-ex loss: 0.418354  [  264/  300]
per-ex loss: 0.462463  [  267/  300]
per-ex loss: 0.471387  [  270/  300]
per-ex loss: 0.480588  [  273/  300]
per-ex loss: 0.533315  [  276/  300]
per-ex loss: 0.516934  [  279/  300]
per-ex loss: 0.423653  [  282/  300]
per-ex loss: 0.512632  [  285/  300]
per-ex loss: 0.436968  [  288/  300]
per-ex loss: 0.560041  [  291/  300]
per-ex loss: 0.478231  [  294/  300]
per-ex loss: 0.511386  [  297/  300]
per-ex loss: 0.469817  [  300/  300]
Train Error: Avg loss: 0.50150271
validation Error: 
 Avg loss: 0.49663907 
 F1: 0.483684 
 Precision: 0.431040 
 Recall: 0.550976
 IoU: 0.318986

test Error: 
 Avg loss: 0.45753211 
 F1: 0.543075 
 Precision: 0.475847 
 Recall: 0.632424
 IoU: 0.372755

We have finished training iteration 134
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_132_.pth
per-ex loss: 0.510494  [    3/  300]
per-ex loss: 0.488999  [    6/  300]
per-ex loss: 0.604720  [    9/  300]
per-ex loss: 0.449329  [   12/  300]
per-ex loss: 0.422942  [   15/  300]
per-ex loss: 0.449922  [   18/  300]
per-ex loss: 0.462116  [   21/  300]
per-ex loss: 0.640284  [   24/  300]
per-ex loss: 0.475312  [   27/  300]
per-ex loss: 0.467914  [   30/  300]
per-ex loss: 0.602394  [   33/  300]
per-ex loss: 0.397349  [   36/  300]
per-ex loss: 0.520501  [   39/  300]
per-ex loss: 0.480218  [   42/  300]
per-ex loss: 0.432933  [   45/  300]
per-ex loss: 0.392027  [   48/  300]
per-ex loss: 0.504309  [   51/  300]
per-ex loss: 0.584680  [   54/  300]
per-ex loss: 0.647477  [   57/  300]
per-ex loss: 0.497291  [   60/  300]
per-ex loss: 0.596372  [   63/  300]
per-ex loss: 0.484721  [   66/  300]
per-ex loss: 0.477462  [   69/  300]
per-ex loss: 0.568942  [   72/  300]
per-ex loss: 0.429364  [   75/  300]
per-ex loss: 0.608346  [   78/  300]
per-ex loss: 0.504949  [   81/  300]
per-ex loss: 0.560085  [   84/  300]
per-ex loss: 0.534013  [   87/  300]
per-ex loss: 0.418980  [   90/  300]
per-ex loss: 0.430215  [   93/  300]
per-ex loss: 0.461432  [   96/  300]
per-ex loss: 0.486400  [   99/  300]
per-ex loss: 0.474288  [  102/  300]
per-ex loss: 0.425031  [  105/  300]
per-ex loss: 0.642595  [  108/  300]
per-ex loss: 0.465065  [  111/  300]
per-ex loss: 0.499194  [  114/  300]
per-ex loss: 0.463430  [  117/  300]
per-ex loss: 0.574255  [  120/  300]
per-ex loss: 0.451138  [  123/  300]
per-ex loss: 0.408630  [  126/  300]
per-ex loss: 0.431067  [  129/  300]
per-ex loss: 0.558225  [  132/  300]
per-ex loss: 0.417227  [  135/  300]
per-ex loss: 0.471607  [  138/  300]
per-ex loss: 0.439723  [  141/  300]
per-ex loss: 0.584210  [  144/  300]
per-ex loss: 0.538499  [  147/  300]
per-ex loss: 0.439384  [  150/  300]
per-ex loss: 0.587481  [  153/  300]
per-ex loss: 0.536702  [  156/  300]
per-ex loss: 0.671897  [  159/  300]
per-ex loss: 0.531318  [  162/  300]
per-ex loss: 0.597225  [  165/  300]
per-ex loss: 0.572037  [  168/  300]
per-ex loss: 0.450626  [  171/  300]
per-ex loss: 0.619846  [  174/  300]
per-ex loss: 0.503439  [  177/  300]
per-ex loss: 0.547769  [  180/  300]
per-ex loss: 0.647213  [  183/  300]
per-ex loss: 0.513031  [  186/  300]
per-ex loss: 0.471931  [  189/  300]
per-ex loss: 0.548722  [  192/  300]
per-ex loss: 0.540056  [  195/  300]
per-ex loss: 0.643399  [  198/  300]
per-ex loss: 0.479325  [  201/  300]
per-ex loss: 0.464966  [  204/  300]
per-ex loss: 0.560271  [  207/  300]
per-ex loss: 0.485287  [  210/  300]
per-ex loss: 0.461977  [  213/  300]
per-ex loss: 0.535656  [  216/  300]
per-ex loss: 0.509200  [  219/  300]
per-ex loss: 0.398881  [  222/  300]
per-ex loss: 0.424230  [  225/  300]
per-ex loss: 0.467475  [  228/  300]
per-ex loss: 0.492208  [  231/  300]
per-ex loss: 0.422439  [  234/  300]
per-ex loss: 0.543587  [  237/  300]
per-ex loss: 0.470597  [  240/  300]
per-ex loss: 0.437417  [  243/  300]
per-ex loss: 0.449863  [  246/  300]
per-ex loss: 0.444463  [  249/  300]
per-ex loss: 0.571352  [  252/  300]
per-ex loss: 0.484710  [  255/  300]
per-ex loss: 0.467087  [  258/  300]
per-ex loss: 0.550392  [  261/  300]
per-ex loss: 0.471813  [  264/  300]
per-ex loss: 0.516639  [  267/  300]
per-ex loss: 0.512251  [  270/  300]
per-ex loss: 0.422250  [  273/  300]
per-ex loss: 0.477470  [  276/  300]
per-ex loss: 0.422525  [  279/  300]
per-ex loss: 0.613953  [  282/  300]
per-ex loss: 0.461160  [  285/  300]
per-ex loss: 0.512786  [  288/  300]
per-ex loss: 0.510199  [  291/  300]
per-ex loss: 0.456691  [  294/  300]
per-ex loss: 0.420747  [  297/  300]
per-ex loss: 0.451460  [  300/  300]
Train Error: Avg loss: 0.50230082
validation Error: 
 Avg loss: 0.50125039 
 F1: 0.481291 
 Precision: 0.441049 
 Recall: 0.529615
 IoU: 0.316908

test Error: 
 Avg loss: 0.45889992 
 F1: 0.541923 
 Precision: 0.485944 
 Recall: 0.612479
 IoU: 0.371670

We have finished training iteration 135
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_133_.pth
per-ex loss: 0.576902  [    3/  300]
per-ex loss: 0.603896  [    6/  300]
per-ex loss: 0.481621  [    9/  300]
per-ex loss: 0.444708  [   12/  300]
per-ex loss: 0.409706  [   15/  300]
per-ex loss: 0.598931  [   18/  300]
per-ex loss: 0.488191  [   21/  300]
per-ex loss: 0.418282  [   24/  300]
per-ex loss: 0.575161  [   27/  300]
per-ex loss: 0.502658  [   30/  300]
per-ex loss: 0.649606  [   33/  300]
per-ex loss: 0.465140  [   36/  300]
per-ex loss: 0.581745  [   39/  300]
per-ex loss: 0.472132  [   42/  300]
per-ex loss: 0.546100  [   45/  300]
per-ex loss: 0.478727  [   48/  300]
per-ex loss: 0.433353  [   51/  300]
per-ex loss: 0.450614  [   54/  300]
per-ex loss: 0.623952  [   57/  300]
per-ex loss: 0.486160  [   60/  300]
per-ex loss: 0.565380  [   63/  300]
per-ex loss: 0.665505  [   66/  300]
per-ex loss: 0.534405  [   69/  300]
per-ex loss: 0.430006  [   72/  300]
per-ex loss: 0.477467  [   75/  300]
per-ex loss: 0.443887  [   78/  300]
per-ex loss: 0.474890  [   81/  300]
per-ex loss: 0.425065  [   84/  300]
per-ex loss: 0.422082  [   87/  300]
per-ex loss: 0.405586  [   90/  300]
per-ex loss: 0.463281  [   93/  300]
per-ex loss: 0.558112  [   96/  300]
per-ex loss: 0.508692  [   99/  300]
per-ex loss: 0.691132  [  102/  300]
per-ex loss: 0.472176  [  105/  300]
per-ex loss: 0.476436  [  108/  300]
per-ex loss: 0.609862  [  111/  300]
per-ex loss: 0.535405  [  114/  300]
per-ex loss: 0.430265  [  117/  300]
per-ex loss: 0.590433  [  120/  300]
per-ex loss: 0.515215  [  123/  300]
per-ex loss: 0.416559  [  126/  300]
per-ex loss: 0.510825  [  129/  300]
per-ex loss: 0.505365  [  132/  300]
per-ex loss: 0.538746  [  135/  300]
per-ex loss: 0.442858  [  138/  300]
per-ex loss: 0.495644  [  141/  300]
per-ex loss: 0.391155  [  144/  300]
per-ex loss: 0.421134  [  147/  300]
per-ex loss: 0.499584  [  150/  300]
per-ex loss: 0.646828  [  153/  300]
per-ex loss: 0.460352  [  156/  300]
per-ex loss: 0.578150  [  159/  300]
per-ex loss: 0.487367  [  162/  300]
per-ex loss: 0.492253  [  165/  300]
per-ex loss: 0.465906  [  168/  300]
per-ex loss: 0.406223  [  171/  300]
per-ex loss: 0.511520  [  174/  300]
per-ex loss: 0.488453  [  177/  300]
per-ex loss: 0.428631  [  180/  300]
per-ex loss: 0.476481  [  183/  300]
per-ex loss: 0.584961  [  186/  300]
per-ex loss: 0.512759  [  189/  300]
per-ex loss: 0.405162  [  192/  300]
per-ex loss: 0.450870  [  195/  300]
per-ex loss: 0.399829  [  198/  300]
per-ex loss: 0.491558  [  201/  300]
per-ex loss: 0.494023  [  204/  300]
per-ex loss: 0.498995  [  207/  300]
per-ex loss: 0.435430  [  210/  300]
per-ex loss: 0.521268  [  213/  300]
per-ex loss: 0.507152  [  216/  300]
per-ex loss: 0.435793  [  219/  300]
per-ex loss: 0.470015  [  222/  300]
per-ex loss: 0.680379  [  225/  300]
per-ex loss: 0.502757  [  228/  300]
per-ex loss: 0.587774  [  231/  300]
per-ex loss: 0.406087  [  234/  300]
per-ex loss: 0.566198  [  237/  300]
per-ex loss: 0.435349  [  240/  300]
per-ex loss: 0.456027  [  243/  300]
per-ex loss: 0.522755  [  246/  300]
per-ex loss: 0.557798  [  249/  300]
per-ex loss: 0.535585  [  252/  300]
per-ex loss: 0.444896  [  255/  300]
per-ex loss: 0.482875  [  258/  300]
per-ex loss: 0.600917  [  261/  300]
per-ex loss: 0.500084  [  264/  300]
per-ex loss: 0.521389  [  267/  300]
per-ex loss: 0.561653  [  270/  300]
per-ex loss: 0.453217  [  273/  300]
per-ex loss: 0.466980  [  276/  300]
per-ex loss: 0.628053  [  279/  300]
per-ex loss: 0.438013  [  282/  300]
per-ex loss: 0.482752  [  285/  300]
per-ex loss: 0.520846  [  288/  300]
per-ex loss: 0.516094  [  291/  300]
per-ex loss: 0.512401  [  294/  300]
per-ex loss: 0.543211  [  297/  300]
per-ex loss: 0.611106  [  300/  300]
Train Error: Avg loss: 0.50389912
validation Error: 
 Avg loss: 0.49947276 
 F1: 0.481790 
 Precision: 0.434219 
 Recall: 0.541066
 IoU: 0.317341

test Error: 
 Avg loss: 0.45720863 
 F1: 0.543465 
 Precision: 0.478526 
 Recall: 0.628796
 IoU: 0.373121

We have finished training iteration 136
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_134_.pth
per-ex loss: 0.405657  [    3/  300]
per-ex loss: 0.420487  [    6/  300]
per-ex loss: 0.525450  [    9/  300]
per-ex loss: 0.492782  [   12/  300]
per-ex loss: 0.585655  [   15/  300]
per-ex loss: 0.368877  [   18/  300]
per-ex loss: 0.602317  [   21/  300]
per-ex loss: 0.531518  [   24/  300]
per-ex loss: 0.554637  [   27/  300]
per-ex loss: 0.481667  [   30/  300]
per-ex loss: 0.472889  [   33/  300]
per-ex loss: 0.524714  [   36/  300]
per-ex loss: 0.477838  [   39/  300]
per-ex loss: 0.444619  [   42/  300]
per-ex loss: 0.449159  [   45/  300]
per-ex loss: 0.596757  [   48/  300]
per-ex loss: 0.487754  [   51/  300]
per-ex loss: 0.527827  [   54/  300]
per-ex loss: 0.533455  [   57/  300]
per-ex loss: 0.447159  [   60/  300]
per-ex loss: 0.565337  [   63/  300]
per-ex loss: 0.440595  [   66/  300]
per-ex loss: 0.435014  [   69/  300]
per-ex loss: 0.640418  [   72/  300]
per-ex loss: 0.518713  [   75/  300]
per-ex loss: 0.605346  [   78/  300]
per-ex loss: 0.598236  [   81/  300]
per-ex loss: 0.415156  [   84/  300]
per-ex loss: 0.439237  [   87/  300]
per-ex loss: 0.611677  [   90/  300]
per-ex loss: 0.455854  [   93/  300]
per-ex loss: 0.442731  [   96/  300]
per-ex loss: 0.480544  [   99/  300]
per-ex loss: 0.418647  [  102/  300]
per-ex loss: 0.467009  [  105/  300]
per-ex loss: 0.412434  [  108/  300]
per-ex loss: 0.436394  [  111/  300]
per-ex loss: 0.410268  [  114/  300]
per-ex loss: 0.461349  [  117/  300]
per-ex loss: 0.542078  [  120/  300]
per-ex loss: 0.655724  [  123/  300]
per-ex loss: 0.420007  [  126/  300]
per-ex loss: 0.517128  [  129/  300]
per-ex loss: 0.581982  [  132/  300]
per-ex loss: 0.503314  [  135/  300]
per-ex loss: 0.671065  [  138/  300]
per-ex loss: 0.497117  [  141/  300]
per-ex loss: 0.455250  [  144/  300]
per-ex loss: 0.458490  [  147/  300]
per-ex loss: 0.422791  [  150/  300]
per-ex loss: 0.507502  [  153/  300]
per-ex loss: 0.625854  [  156/  300]
per-ex loss: 0.476567  [  159/  300]
per-ex loss: 0.508000  [  162/  300]
per-ex loss: 0.495941  [  165/  300]
per-ex loss: 0.524328  [  168/  300]
per-ex loss: 0.646859  [  171/  300]
per-ex loss: 0.470324  [  174/  300]
per-ex loss: 0.536564  [  177/  300]
per-ex loss: 0.547128  [  180/  300]
per-ex loss: 0.497562  [  183/  300]
per-ex loss: 0.467948  [  186/  300]
per-ex loss: 0.426127  [  189/  300]
per-ex loss: 0.478841  [  192/  300]
per-ex loss: 0.472857  [  195/  300]
per-ex loss: 0.513580  [  198/  300]
per-ex loss: 0.532182  [  201/  300]
per-ex loss: 0.631616  [  204/  300]
per-ex loss: 0.507316  [  207/  300]
per-ex loss: 0.516126  [  210/  300]
per-ex loss: 0.573760  [  213/  300]
per-ex loss: 0.508101  [  216/  300]
per-ex loss: 0.381687  [  219/  300]
per-ex loss: 0.637113  [  222/  300]
per-ex loss: 0.479598  [  225/  300]
per-ex loss: 0.547923  [  228/  300]
per-ex loss: 0.398171  [  231/  300]
per-ex loss: 0.508461  [  234/  300]
per-ex loss: 0.484776  [  237/  300]
per-ex loss: 0.480051  [  240/  300]
per-ex loss: 0.491382  [  243/  300]
per-ex loss: 0.514149  [  246/  300]
per-ex loss: 0.489411  [  249/  300]
per-ex loss: 0.437258  [  252/  300]
per-ex loss: 0.550002  [  255/  300]
per-ex loss: 0.474682  [  258/  300]
per-ex loss: 0.602842  [  261/  300]
per-ex loss: 0.513847  [  264/  300]
per-ex loss: 0.530025  [  267/  300]
per-ex loss: 0.480713  [  270/  300]
per-ex loss: 0.430704  [  273/  300]
per-ex loss: 0.497234  [  276/  300]
per-ex loss: 0.430627  [  279/  300]
per-ex loss: 0.502372  [  282/  300]
per-ex loss: 0.598828  [  285/  300]
per-ex loss: 0.581389  [  288/  300]
per-ex loss: 0.513069  [  291/  300]
per-ex loss: 0.544596  [  294/  300]
per-ex loss: 0.606297  [  297/  300]
per-ex loss: 0.461275  [  300/  300]
Train Error: Avg loss: 0.50544689
validation Error: 
 Avg loss: 0.50606650 
 F1: 0.480853 
 Precision: 0.429788 
 Recall: 0.545689
 IoU: 0.316529

test Error: 
 Avg loss: 0.46044952 
 F1: 0.540229 
 Precision: 0.479245 
 Recall: 0.618998
 IoU: 0.370078

We have finished training iteration 137
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_135_.pth
per-ex loss: 0.539657  [    3/  300]
per-ex loss: 0.548353  [    6/  300]
per-ex loss: 0.592205  [    9/  300]
per-ex loss: 0.373807  [   12/  300]
per-ex loss: 0.412761  [   15/  300]
per-ex loss: 0.458123  [   18/  300]
per-ex loss: 0.495295  [   21/  300]
per-ex loss: 0.469772  [   24/  300]
per-ex loss: 0.547533  [   27/  300]
per-ex loss: 0.630897  [   30/  300]
per-ex loss: 0.446587  [   33/  300]
per-ex loss: 0.574255  [   36/  300]
per-ex loss: 0.460885  [   39/  300]
per-ex loss: 0.406857  [   42/  300]
per-ex loss: 0.636941  [   45/  300]
per-ex loss: 0.481453  [   48/  300]
per-ex loss: 0.476993  [   51/  300]
per-ex loss: 0.396063  [   54/  300]
per-ex loss: 0.545086  [   57/  300]
per-ex loss: 0.437034  [   60/  300]
per-ex loss: 0.552929  [   63/  300]
per-ex loss: 0.409976  [   66/  300]
per-ex loss: 0.599253  [   69/  300]
per-ex loss: 0.383738  [   72/  300]
per-ex loss: 0.433380  [   75/  300]
per-ex loss: 0.555266  [   78/  300]
per-ex loss: 0.503721  [   81/  300]
per-ex loss: 0.557851  [   84/  300]
per-ex loss: 0.652916  [   87/  300]
per-ex loss: 0.491364  [   90/  300]
per-ex loss: 0.431795  [   93/  300]
per-ex loss: 0.462986  [   96/  300]
per-ex loss: 0.433371  [   99/  300]
per-ex loss: 0.437043  [  102/  300]
per-ex loss: 0.503276  [  105/  300]
per-ex loss: 0.555796  [  108/  300]
per-ex loss: 0.472405  [  111/  300]
per-ex loss: 0.421815  [  114/  300]
per-ex loss: 0.562210  [  117/  300]
per-ex loss: 0.641264  [  120/  300]
per-ex loss: 0.550313  [  123/  300]
per-ex loss: 0.519324  [  126/  300]
per-ex loss: 0.605641  [  129/  300]
per-ex loss: 0.458203  [  132/  300]
per-ex loss: 0.593123  [  135/  300]
per-ex loss: 0.636054  [  138/  300]
per-ex loss: 0.426276  [  141/  300]
per-ex loss: 0.567001  [  144/  300]
per-ex loss: 0.396056  [  147/  300]
per-ex loss: 0.493659  [  150/  300]
per-ex loss: 0.394384  [  153/  300]
per-ex loss: 0.424721  [  156/  300]
per-ex loss: 0.420611  [  159/  300]
per-ex loss: 0.584713  [  162/  300]
per-ex loss: 0.531152  [  165/  300]
per-ex loss: 0.480041  [  168/  300]
per-ex loss: 0.463119  [  171/  300]
per-ex loss: 0.493101  [  174/  300]
per-ex loss: 0.423859  [  177/  300]
per-ex loss: 0.511562  [  180/  300]
per-ex loss: 0.504528  [  183/  300]
per-ex loss: 0.477020  [  186/  300]
per-ex loss: 0.470662  [  189/  300]
per-ex loss: 0.592215  [  192/  300]
per-ex loss: 0.440168  [  195/  300]
per-ex loss: 0.416203  [  198/  300]
per-ex loss: 0.466571  [  201/  300]
per-ex loss: 0.426514  [  204/  300]
per-ex loss: 0.423580  [  207/  300]
per-ex loss: 0.497289  [  210/  300]
per-ex loss: 0.556179  [  213/  300]
per-ex loss: 0.434847  [  216/  300]
per-ex loss: 0.500617  [  219/  300]
per-ex loss: 0.448359  [  222/  300]
per-ex loss: 0.612777  [  225/  300]
per-ex loss: 0.437967  [  228/  300]
per-ex loss: 0.642004  [  231/  300]
per-ex loss: 0.542434  [  234/  300]
per-ex loss: 0.470366  [  237/  300]
per-ex loss: 0.525461  [  240/  300]
per-ex loss: 0.637506  [  243/  300]
per-ex loss: 0.516166  [  246/  300]
per-ex loss: 0.422948  [  249/  300]
per-ex loss: 0.481093  [  252/  300]
per-ex loss: 0.460563  [  255/  300]
per-ex loss: 0.470030  [  258/  300]
per-ex loss: 0.468801  [  261/  300]
per-ex loss: 0.623756  [  264/  300]
per-ex loss: 0.453353  [  267/  300]
per-ex loss: 0.452991  [  270/  300]
per-ex loss: 0.486496  [  273/  300]
per-ex loss: 0.419883  [  276/  300]
per-ex loss: 0.416967  [  279/  300]
per-ex loss: 0.616769  [  282/  300]
per-ex loss: 0.549769  [  285/  300]
per-ex loss: 0.496393  [  288/  300]
per-ex loss: 0.432997  [  291/  300]
per-ex loss: 0.492441  [  294/  300]
per-ex loss: 0.478928  [  297/  300]
per-ex loss: 0.524492  [  300/  300]
Train Error: Avg loss: 0.49753923
validation Error: 
 Avg loss: 0.50854284 
 F1: 0.478184 
 Precision: 0.436360 
 Recall: 0.528876
 IoU: 0.314219

test Error: 
 Avg loss: 0.45556778 
 F1: 0.545174 
 Precision: 0.486505 
 Recall: 0.619933
 IoU: 0.374735

We have finished training iteration 138
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_136_.pth
per-ex loss: 0.463092  [    3/  300]
per-ex loss: 0.513979  [    6/  300]
per-ex loss: 0.526764  [    9/  300]
per-ex loss: 0.449826  [   12/  300]
per-ex loss: 0.448575  [   15/  300]
per-ex loss: 0.443682  [   18/  300]
per-ex loss: 0.540860  [   21/  300]
per-ex loss: 0.441858  [   24/  300]
per-ex loss: 0.442543  [   27/  300]
per-ex loss: 0.387540  [   30/  300]
per-ex loss: 0.608877  [   33/  300]
per-ex loss: 0.657339  [   36/  300]
per-ex loss: 0.540657  [   39/  300]
per-ex loss: 0.537890  [   42/  300]
per-ex loss: 0.589602  [   45/  300]
per-ex loss: 0.526922  [   48/  300]
per-ex loss: 0.458186  [   51/  300]
per-ex loss: 0.432798  [   54/  300]
per-ex loss: 0.448883  [   57/  300]
per-ex loss: 0.563641  [   60/  300]
per-ex loss: 0.536495  [   63/  300]
per-ex loss: 0.526240  [   66/  300]
per-ex loss: 0.427331  [   69/  300]
per-ex loss: 0.494463  [   72/  300]
per-ex loss: 0.505017  [   75/  300]
per-ex loss: 0.469825  [   78/  300]
per-ex loss: 0.447198  [   81/  300]
per-ex loss: 0.487067  [   84/  300]
per-ex loss: 0.442154  [   87/  300]
per-ex loss: 0.567926  [   90/  300]
per-ex loss: 0.513521  [   93/  300]
per-ex loss: 0.493512  [   96/  300]
per-ex loss: 0.534053  [   99/  300]
per-ex loss: 0.571755  [  102/  300]
per-ex loss: 0.555046  [  105/  300]
per-ex loss: 0.426426  [  108/  300]
per-ex loss: 0.461040  [  111/  300]
per-ex loss: 0.520293  [  114/  300]
per-ex loss: 0.416923  [  117/  300]
per-ex loss: 0.618132  [  120/  300]
per-ex loss: 0.404459  [  123/  300]
per-ex loss: 0.522923  [  126/  300]
per-ex loss: 0.603913  [  129/  300]
per-ex loss: 0.494090  [  132/  300]
per-ex loss: 0.518696  [  135/  300]
per-ex loss: 0.540105  [  138/  300]
per-ex loss: 0.407762  [  141/  300]
per-ex loss: 0.425074  [  144/  300]
per-ex loss: 0.419808  [  147/  300]
per-ex loss: 0.448731  [  150/  300]
per-ex loss: 0.465384  [  153/  300]
per-ex loss: 0.459427  [  156/  300]
per-ex loss: 0.432037  [  159/  300]
per-ex loss: 0.549572  [  162/  300]
per-ex loss: 0.589692  [  165/  300]
per-ex loss: 0.453933  [  168/  300]
per-ex loss: 0.528020  [  171/  300]
per-ex loss: 0.392768  [  174/  300]
per-ex loss: 0.589084  [  177/  300]
per-ex loss: 0.428239  [  180/  300]
per-ex loss: 0.499530  [  183/  300]
per-ex loss: 0.477512  [  186/  300]
per-ex loss: 0.633521  [  189/  300]
per-ex loss: 0.446964  [  192/  300]
per-ex loss: 0.473003  [  195/  300]
per-ex loss: 0.542445  [  198/  300]
per-ex loss: 0.414425  [  201/  300]
per-ex loss: 0.499542  [  204/  300]
per-ex loss: 0.519457  [  207/  300]
per-ex loss: 0.450579  [  210/  300]
per-ex loss: 0.424434  [  213/  300]
per-ex loss: 0.543193  [  216/  300]
per-ex loss: 0.455135  [  219/  300]
per-ex loss: 0.606333  [  222/  300]
per-ex loss: 0.486440  [  225/  300]
per-ex loss: 0.487388  [  228/  300]
per-ex loss: 0.549295  [  231/  300]
per-ex loss: 0.503366  [  234/  300]
per-ex loss: 0.440902  [  237/  300]
per-ex loss: 0.404758  [  240/  300]
per-ex loss: 0.394238  [  243/  300]
per-ex loss: 0.511308  [  246/  300]
per-ex loss: 0.514539  [  249/  300]
per-ex loss: 0.531188  [  252/  300]
per-ex loss: 0.444989  [  255/  300]
per-ex loss: 0.482663  [  258/  300]
per-ex loss: 0.618347  [  261/  300]
per-ex loss: 0.544153  [  264/  300]
per-ex loss: 0.525049  [  267/  300]
per-ex loss: 0.555208  [  270/  300]
per-ex loss: 0.519211  [  273/  300]
per-ex loss: 0.493824  [  276/  300]
per-ex loss: 0.631766  [  279/  300]
per-ex loss: 0.577000  [  282/  300]
per-ex loss: 0.545453  [  285/  300]
per-ex loss: 0.559488  [  288/  300]
per-ex loss: 0.555962  [  291/  300]
per-ex loss: 0.453123  [  294/  300]
per-ex loss: 0.451083  [  297/  300]
per-ex loss: 0.446822  [  300/  300]
Train Error: Avg loss: 0.49925286
validation Error: 
 Avg loss: 0.51310839 
 F1: 0.471208 
 Precision: 0.443933 
 Recall: 0.502055
 IoU: 0.308223

test Error: 
 Avg loss: 0.45213705 
 F1: 0.548593 
 Precision: 0.493589 
 Recall: 0.617393
 IoU: 0.377973

We have finished training iteration 139
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_137_.pth
per-ex loss: 0.705729  [    3/  300]
per-ex loss: 0.553333  [    6/  300]
per-ex loss: 0.644184  [    9/  300]
per-ex loss: 0.538150  [   12/  300]
per-ex loss: 0.483941  [   15/  300]
per-ex loss: 0.433473  [   18/  300]
per-ex loss: 0.428874  [   21/  300]
per-ex loss: 0.514749  [   24/  300]
per-ex loss: 0.525338  [   27/  300]
per-ex loss: 0.512210  [   30/  300]
per-ex loss: 0.433558  [   33/  300]
per-ex loss: 0.547733  [   36/  300]
per-ex loss: 0.498895  [   39/  300]
per-ex loss: 0.427908  [   42/  300]
per-ex loss: 0.515716  [   45/  300]
per-ex loss: 0.499645  [   48/  300]
per-ex loss: 0.576914  [   51/  300]
per-ex loss: 0.530160  [   54/  300]
per-ex loss: 0.550396  [   57/  300]
per-ex loss: 0.496938  [   60/  300]
per-ex loss: 0.416228  [   63/  300]
per-ex loss: 0.500833  [   66/  300]
per-ex loss: 0.512495  [   69/  300]
per-ex loss: 0.370966  [   72/  300]
per-ex loss: 0.497084  [   75/  300]
per-ex loss: 0.445229  [   78/  300]
per-ex loss: 0.395733  [   81/  300]
per-ex loss: 0.473980  [   84/  300]
per-ex loss: 0.407564  [   87/  300]
per-ex loss: 0.580716  [   90/  300]
per-ex loss: 0.626079  [   93/  300]
per-ex loss: 0.601166  [   96/  300]
per-ex loss: 0.500013  [   99/  300]
per-ex loss: 0.475513  [  102/  300]
per-ex loss: 0.462029  [  105/  300]
per-ex loss: 0.649613  [  108/  300]
per-ex loss: 0.437125  [  111/  300]
per-ex loss: 0.380204  [  114/  300]
per-ex loss: 0.544553  [  117/  300]
per-ex loss: 0.409003  [  120/  300]
per-ex loss: 0.537327  [  123/  300]
per-ex loss: 0.463515  [  126/  300]
per-ex loss: 0.485186  [  129/  300]
per-ex loss: 0.425667  [  132/  300]
per-ex loss: 0.464391  [  135/  300]
per-ex loss: 0.469330  [  138/  300]
per-ex loss: 0.461772  [  141/  300]
per-ex loss: 0.523513  [  144/  300]
per-ex loss: 0.609642  [  147/  300]
per-ex loss: 0.443826  [  150/  300]
per-ex loss: 0.487054  [  153/  300]
per-ex loss: 0.423915  [  156/  300]
per-ex loss: 0.558287  [  159/  300]
per-ex loss: 0.449827  [  162/  300]
per-ex loss: 0.440112  [  165/  300]
per-ex loss: 0.436841  [  168/  300]
per-ex loss: 0.535752  [  171/  300]
per-ex loss: 0.516019  [  174/  300]
per-ex loss: 0.641981  [  177/  300]
per-ex loss: 0.449573  [  180/  300]
per-ex loss: 0.552435  [  183/  300]
per-ex loss: 0.637436  [  186/  300]
per-ex loss: 0.406553  [  189/  300]
per-ex loss: 0.462969  [  192/  300]
per-ex loss: 0.539957  [  195/  300]
per-ex loss: 0.448116  [  198/  300]
per-ex loss: 0.519720  [  201/  300]
per-ex loss: 0.478896  [  204/  300]
per-ex loss: 0.601484  [  207/  300]
per-ex loss: 0.596879  [  210/  300]
per-ex loss: 0.459639  [  213/  300]
per-ex loss: 0.450217  [  216/  300]
per-ex loss: 0.429256  [  219/  300]
per-ex loss: 0.574016  [  222/  300]
per-ex loss: 0.453138  [  225/  300]
per-ex loss: 0.460040  [  228/  300]
per-ex loss: 0.556114  [  231/  300]
per-ex loss: 0.473655  [  234/  300]
per-ex loss: 0.589922  [  237/  300]
per-ex loss: 0.484384  [  240/  300]
per-ex loss: 0.477471  [  243/  300]
per-ex loss: 0.405909  [  246/  300]
per-ex loss: 0.641522  [  249/  300]
per-ex loss: 0.504453  [  252/  300]
per-ex loss: 0.433046  [  255/  300]
per-ex loss: 0.574947  [  258/  300]
per-ex loss: 0.447528  [  261/  300]
per-ex loss: 0.415836  [  264/  300]
per-ex loss: 0.446815  [  267/  300]
per-ex loss: 0.510814  [  270/  300]
per-ex loss: 0.494851  [  273/  300]
per-ex loss: 0.505247  [  276/  300]
per-ex loss: 0.624852  [  279/  300]
per-ex loss: 0.451853  [  282/  300]
per-ex loss: 0.576277  [  285/  300]
per-ex loss: 0.505559  [  288/  300]
per-ex loss: 0.432114  [  291/  300]
per-ex loss: 0.552338  [  294/  300]
per-ex loss: 0.496625  [  297/  300]
per-ex loss: 0.456821  [  300/  300]
Train Error: Avg loss: 0.50057204
validation Error: 
 Avg loss: 0.50479422 
 F1: 0.477544 
 Precision: 0.428967 
 Recall: 0.538528
 IoU: 0.313667

test Error: 
 Avg loss: 0.45868260 
 F1: 0.542059 
 Precision: 0.470577 
 Recall: 0.639149
 IoU: 0.371798

We have finished training iteration 140
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_138_.pth
per-ex loss: 0.507976  [    3/  300]
per-ex loss: 0.370418  [    6/  300]
per-ex loss: 0.499041  [    9/  300]
per-ex loss: 0.467424  [   12/  300]
per-ex loss: 0.468537  [   15/  300]
per-ex loss: 0.508057  [   18/  300]
per-ex loss: 0.516580  [   21/  300]
per-ex loss: 0.541339  [   24/  300]
per-ex loss: 0.478310  [   27/  300]
per-ex loss: 0.626882  [   30/  300]
per-ex loss: 0.420430  [   33/  300]
per-ex loss: 0.422100  [   36/  300]
per-ex loss: 0.546336  [   39/  300]
per-ex loss: 0.605112  [   42/  300]
per-ex loss: 0.549339  [   45/  300]
per-ex loss: 0.497038  [   48/  300]
per-ex loss: 0.718134  [   51/  300]
per-ex loss: 0.457435  [   54/  300]
per-ex loss: 0.480247  [   57/  300]
per-ex loss: 0.437232  [   60/  300]
per-ex loss: 0.427250  [   63/  300]
per-ex loss: 0.444449  [   66/  300]
per-ex loss: 0.491026  [   69/  300]
per-ex loss: 0.442025  [   72/  300]
per-ex loss: 0.493519  [   75/  300]
per-ex loss: 0.493082  [   78/  300]
per-ex loss: 0.444077  [   81/  300]
per-ex loss: 0.554321  [   84/  300]
per-ex loss: 0.568525  [   87/  300]
per-ex loss: 0.483746  [   90/  300]
per-ex loss: 0.407791  [   93/  300]
per-ex loss: 0.483021  [   96/  300]
per-ex loss: 0.485856  [   99/  300]
per-ex loss: 0.431182  [  102/  300]
per-ex loss: 0.578850  [  105/  300]
per-ex loss: 0.553135  [  108/  300]
per-ex loss: 0.453862  [  111/  300]
per-ex loss: 0.566343  [  114/  300]
per-ex loss: 0.460384  [  117/  300]
per-ex loss: 0.584599  [  120/  300]
per-ex loss: 0.441250  [  123/  300]
per-ex loss: 0.546361  [  126/  300]
per-ex loss: 0.599224  [  129/  300]
per-ex loss: 0.497738  [  132/  300]
per-ex loss: 0.406483  [  135/  300]
per-ex loss: 0.389954  [  138/  300]
per-ex loss: 0.668313  [  141/  300]
per-ex loss: 0.463870  [  144/  300]
per-ex loss: 0.455104  [  147/  300]
per-ex loss: 0.463633  [  150/  300]
per-ex loss: 0.568401  [  153/  300]
per-ex loss: 0.416629  [  156/  300]
per-ex loss: 0.499506  [  159/  300]
per-ex loss: 0.559292  [  162/  300]
per-ex loss: 0.580928  [  165/  300]
per-ex loss: 0.493533  [  168/  300]
per-ex loss: 0.549247  [  171/  300]
per-ex loss: 0.531026  [  174/  300]
per-ex loss: 0.425279  [  177/  300]
per-ex loss: 0.607677  [  180/  300]
per-ex loss: 0.562684  [  183/  300]
per-ex loss: 0.431769  [  186/  300]
per-ex loss: 0.418089  [  189/  300]
per-ex loss: 0.509451  [  192/  300]
per-ex loss: 0.446682  [  195/  300]
per-ex loss: 0.426373  [  198/  300]
per-ex loss: 0.450660  [  201/  300]
per-ex loss: 0.464323  [  204/  300]
per-ex loss: 0.613020  [  207/  300]
per-ex loss: 0.395272  [  210/  300]
per-ex loss: 0.463891  [  213/  300]
per-ex loss: 0.532351  [  216/  300]
per-ex loss: 0.495843  [  219/  300]
per-ex loss: 0.503564  [  222/  300]
per-ex loss: 0.423245  [  225/  300]
per-ex loss: 0.542749  [  228/  300]
per-ex loss: 0.495746  [  231/  300]
per-ex loss: 0.462845  [  234/  300]
per-ex loss: 0.413056  [  237/  300]
per-ex loss: 0.545833  [  240/  300]
per-ex loss: 0.424719  [  243/  300]
per-ex loss: 0.507800  [  246/  300]
per-ex loss: 0.623742  [  249/  300]
per-ex loss: 0.577851  [  252/  300]
per-ex loss: 0.480564  [  255/  300]
per-ex loss: 0.514392  [  258/  300]
per-ex loss: 0.621926  [  261/  300]
per-ex loss: 0.479404  [  264/  300]
per-ex loss: 0.411708  [  267/  300]
per-ex loss: 0.613114  [  270/  300]
per-ex loss: 0.473212  [  273/  300]
per-ex loss: 0.450731  [  276/  300]
per-ex loss: 0.579136  [  279/  300]
per-ex loss: 0.619679  [  282/  300]
per-ex loss: 0.629362  [  285/  300]
per-ex loss: 0.465791  [  288/  300]
per-ex loss: 0.560198  [  291/  300]
per-ex loss: 0.480593  [  294/  300]
per-ex loss: 0.629287  [  297/  300]
per-ex loss: 0.400620  [  300/  300]
Train Error: Avg loss: 0.50269731
validation Error: 
 Avg loss: 0.51274645 
 F1: 0.474513 
 Precision: 0.442370 
 Recall: 0.511692
 IoU: 0.311057

test Error: 
 Avg loss: 0.45090407 
 F1: 0.549721 
 Precision: 0.499759 
 Recall: 0.610782
 IoU: 0.379045

We have finished training iteration 141
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_139_.pth
per-ex loss: 0.643128  [    3/  300]
per-ex loss: 0.538210  [    6/  300]
per-ex loss: 0.406508  [    9/  300]
per-ex loss: 0.481212  [   12/  300]
per-ex loss: 0.462102  [   15/  300]
per-ex loss: 0.628487  [   18/  300]
per-ex loss: 0.406763  [   21/  300]
per-ex loss: 0.531972  [   24/  300]
per-ex loss: 0.489433  [   27/  300]
per-ex loss: 0.486159  [   30/  300]
per-ex loss: 0.438227  [   33/  300]
per-ex loss: 0.625044  [   36/  300]
per-ex loss: 0.403282  [   39/  300]
per-ex loss: 0.427656  [   42/  300]
per-ex loss: 0.489187  [   45/  300]
per-ex loss: 0.443483  [   48/  300]
per-ex loss: 0.523772  [   51/  300]
per-ex loss: 0.469736  [   54/  300]
per-ex loss: 0.553535  [   57/  300]
per-ex loss: 0.505416  [   60/  300]
per-ex loss: 0.470603  [   63/  300]
per-ex loss: 0.500338  [   66/  300]
per-ex loss: 0.508339  [   69/  300]
per-ex loss: 0.423979  [   72/  300]
per-ex loss: 0.515133  [   75/  300]
per-ex loss: 0.566243  [   78/  300]
per-ex loss: 0.405335  [   81/  300]
per-ex loss: 0.451748  [   84/  300]
per-ex loss: 0.592847  [   87/  300]
per-ex loss: 0.464997  [   90/  300]
per-ex loss: 0.605299  [   93/  300]
per-ex loss: 0.446122  [   96/  300]
per-ex loss: 0.431089  [   99/  300]
per-ex loss: 0.485959  [  102/  300]
per-ex loss: 0.462361  [  105/  300]
per-ex loss: 0.440954  [  108/  300]
per-ex loss: 0.629366  [  111/  300]
per-ex loss: 0.505288  [  114/  300]
per-ex loss: 0.494475  [  117/  300]
per-ex loss: 0.418352  [  120/  300]
per-ex loss: 0.526978  [  123/  300]
per-ex loss: 0.522685  [  126/  300]
per-ex loss: 0.471626  [  129/  300]
per-ex loss: 0.618889  [  132/  300]
per-ex loss: 0.534586  [  135/  300]
per-ex loss: 0.452597  [  138/  300]
per-ex loss: 0.463010  [  141/  300]
per-ex loss: 0.416856  [  144/  300]
per-ex loss: 0.533547  [  147/  300]
per-ex loss: 0.416427  [  150/  300]
per-ex loss: 0.491408  [  153/  300]
per-ex loss: 0.559532  [  156/  300]
per-ex loss: 0.507122  [  159/  300]
per-ex loss: 0.478358  [  162/  300]
per-ex loss: 0.507912  [  165/  300]
per-ex loss: 0.495845  [  168/  300]
per-ex loss: 0.505160  [  171/  300]
per-ex loss: 0.598299  [  174/  300]
per-ex loss: 0.482710  [  177/  300]
per-ex loss: 0.473857  [  180/  300]
per-ex loss: 0.431293  [  183/  300]
per-ex loss: 0.500648  [  186/  300]
per-ex loss: 0.504001  [  189/  300]
per-ex loss: 0.423819  [  192/  300]
per-ex loss: 0.424609  [  195/  300]
per-ex loss: 0.421020  [  198/  300]
per-ex loss: 0.522150  [  201/  300]
per-ex loss: 0.533364  [  204/  300]
per-ex loss: 0.637320  [  207/  300]
per-ex loss: 0.381018  [  210/  300]
per-ex loss: 0.488378  [  213/  300]
per-ex loss: 0.433403  [  216/  300]
per-ex loss: 0.526545  [  219/  300]
per-ex loss: 0.535907  [  222/  300]
per-ex loss: 0.462354  [  225/  300]
per-ex loss: 0.579327  [  228/  300]
per-ex loss: 0.543204  [  231/  300]
per-ex loss: 0.449486  [  234/  300]
per-ex loss: 0.468800  [  237/  300]
per-ex loss: 0.677313  [  240/  300]
per-ex loss: 0.531755  [  243/  300]
per-ex loss: 0.513082  [  246/  300]
per-ex loss: 0.495480  [  249/  300]
per-ex loss: 0.464094  [  252/  300]
per-ex loss: 0.483718  [  255/  300]
per-ex loss: 0.445106  [  258/  300]
per-ex loss: 0.569866  [  261/  300]
per-ex loss: 0.482380  [  264/  300]
per-ex loss: 0.432581  [  267/  300]
per-ex loss: 0.544823  [  270/  300]
per-ex loss: 0.548064  [  273/  300]
per-ex loss: 0.435453  [  276/  300]
per-ex loss: 0.638996  [  279/  300]
per-ex loss: 0.431961  [  282/  300]
per-ex loss: 0.521232  [  285/  300]
per-ex loss: 0.441417  [  288/  300]
per-ex loss: 0.573337  [  291/  300]
per-ex loss: 0.582202  [  294/  300]
per-ex loss: 0.498163  [  297/  300]
per-ex loss: 0.728203  [  300/  300]
Train Error: Avg loss: 0.50135412
validation Error: 
 Avg loss: 0.50773696 
 F1: 0.475784 
 Precision: 0.427316 
 Recall: 0.536653
 IoU: 0.312150

test Error: 
 Avg loss: 0.45644957 
 F1: 0.544269 
 Precision: 0.476550 
 Recall: 0.634422
 IoU: 0.373880

We have finished training iteration 142
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_140_.pth
per-ex loss: 0.488581  [    3/  300]
per-ex loss: 0.568460  [    6/  300]
per-ex loss: 0.436553  [    9/  300]
per-ex loss: 0.471777  [   12/  300]
per-ex loss: 0.530841  [   15/  300]
per-ex loss: 0.528151  [   18/  300]
per-ex loss: 0.411369  [   21/  300]
per-ex loss: 0.529152  [   24/  300]
per-ex loss: 0.509330  [   27/  300]
per-ex loss: 0.439051  [   30/  300]
per-ex loss: 0.633433  [   33/  300]
per-ex loss: 0.511141  [   36/  300]
per-ex loss: 0.473662  [   39/  300]
per-ex loss: 0.475177  [   42/  300]
per-ex loss: 0.514227  [   45/  300]
per-ex loss: 0.507174  [   48/  300]
per-ex loss: 0.453667  [   51/  300]
per-ex loss: 0.484321  [   54/  300]
per-ex loss: 0.530239  [   57/  300]
per-ex loss: 0.435677  [   60/  300]
per-ex loss: 0.486507  [   63/  300]
per-ex loss: 0.577324  [   66/  300]
per-ex loss: 0.563156  [   69/  300]
per-ex loss: 0.527885  [   72/  300]
per-ex loss: 0.528040  [   75/  300]
per-ex loss: 0.427846  [   78/  300]
per-ex loss: 0.431988  [   81/  300]
per-ex loss: 0.378705  [   84/  300]
per-ex loss: 0.614739  [   87/  300]
per-ex loss: 0.409722  [   90/  300]
per-ex loss: 0.524513  [   93/  300]
per-ex loss: 0.524005  [   96/  300]
per-ex loss: 0.562327  [   99/  300]
per-ex loss: 0.388283  [  102/  300]
per-ex loss: 0.444717  [  105/  300]
per-ex loss: 0.544630  [  108/  300]
per-ex loss: 0.516747  [  111/  300]
per-ex loss: 0.413187  [  114/  300]
per-ex loss: 0.449058  [  117/  300]
per-ex loss: 0.577817  [  120/  300]
per-ex loss: 0.517874  [  123/  300]
per-ex loss: 0.594926  [  126/  300]
per-ex loss: 0.419929  [  129/  300]
per-ex loss: 0.533046  [  132/  300]
per-ex loss: 0.522279  [  135/  300]
per-ex loss: 0.468199  [  138/  300]
per-ex loss: 0.426300  [  141/  300]
per-ex loss: 0.576224  [  144/  300]
per-ex loss: 0.417899  [  147/  300]
per-ex loss: 0.500391  [  150/  300]
per-ex loss: 0.472656  [  153/  300]
per-ex loss: 0.460226  [  156/  300]
per-ex loss: 0.521216  [  159/  300]
per-ex loss: 0.507881  [  162/  300]
per-ex loss: 0.537987  [  165/  300]
per-ex loss: 0.527348  [  168/  300]
per-ex loss: 0.455702  [  171/  300]
per-ex loss: 0.440229  [  174/  300]
per-ex loss: 0.529129  [  177/  300]
per-ex loss: 0.561898  [  180/  300]
per-ex loss: 0.425937  [  183/  300]
per-ex loss: 0.466052  [  186/  300]
per-ex loss: 0.481071  [  189/  300]
per-ex loss: 0.491466  [  192/  300]
per-ex loss: 0.446054  [  195/  300]
per-ex loss: 0.543011  [  198/  300]
per-ex loss: 0.512093  [  201/  300]
per-ex loss: 0.455686  [  204/  300]
per-ex loss: 0.597821  [  207/  300]
per-ex loss: 0.508960  [  210/  300]
per-ex loss: 0.481239  [  213/  300]
per-ex loss: 0.479035  [  216/  300]
per-ex loss: 0.516718  [  219/  300]
per-ex loss: 0.650930  [  222/  300]
per-ex loss: 0.521598  [  225/  300]
per-ex loss: 0.444098  [  228/  300]
per-ex loss: 0.545348  [  231/  300]
per-ex loss: 0.470193  [  234/  300]
per-ex loss: 0.490737  [  237/  300]
per-ex loss: 0.396350  [  240/  300]
per-ex loss: 0.473033  [  243/  300]
per-ex loss: 0.706031  [  246/  300]
per-ex loss: 0.576598  [  249/  300]
per-ex loss: 0.517012  [  252/  300]
per-ex loss: 0.414461  [  255/  300]
per-ex loss: 0.435105  [  258/  300]
per-ex loss: 0.449288  [  261/  300]
per-ex loss: 0.422101  [  264/  300]
per-ex loss: 0.469762  [  267/  300]
per-ex loss: 0.467140  [  270/  300]
per-ex loss: 0.406539  [  273/  300]
per-ex loss: 0.504313  [  276/  300]
per-ex loss: 0.584376  [  279/  300]
per-ex loss: 0.579981  [  282/  300]
per-ex loss: 0.505850  [  285/  300]
per-ex loss: 0.402909  [  288/  300]
per-ex loss: 0.519265  [  291/  300]
per-ex loss: 0.378111  [  294/  300]
per-ex loss: 0.451053  [  297/  300]
per-ex loss: 0.469568  [  300/  300]
Train Error: Avg loss: 0.49471410
validation Error: 
 Avg loss: 0.49812591 
 F1: 0.484167 
 Precision: 0.442671 
 Recall: 0.534248
 IoU: 0.319407

test Error: 
 Avg loss: 0.45261627 
 F1: 0.548043 
 Precision: 0.494280 
 Recall: 0.614927
 IoU: 0.377451

We have finished training iteration 143
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_141_.pth
per-ex loss: 0.604554  [    3/  300]
per-ex loss: 0.452001  [    6/  300]
per-ex loss: 0.493863  [    9/  300]
per-ex loss: 0.567958  [   12/  300]
per-ex loss: 0.426754  [   15/  300]
per-ex loss: 0.429916  [   18/  300]
per-ex loss: 0.454471  [   21/  300]
per-ex loss: 0.539141  [   24/  300]
per-ex loss: 0.480908  [   27/  300]
per-ex loss: 0.511774  [   30/  300]
per-ex loss: 0.456451  [   33/  300]
per-ex loss: 0.666493  [   36/  300]
per-ex loss: 0.414707  [   39/  300]
per-ex loss: 0.461500  [   42/  300]
per-ex loss: 0.410102  [   45/  300]
per-ex loss: 0.405474  [   48/  300]
per-ex loss: 0.535379  [   51/  300]
per-ex loss: 0.433883  [   54/  300]
per-ex loss: 0.444828  [   57/  300]
per-ex loss: 0.519606  [   60/  300]
per-ex loss: 0.414093  [   63/  300]
per-ex loss: 0.469471  [   66/  300]
per-ex loss: 0.621744  [   69/  300]
per-ex loss: 0.446209  [   72/  300]
per-ex loss: 0.590159  [   75/  300]
per-ex loss: 0.507088  [   78/  300]
per-ex loss: 0.658426  [   81/  300]
per-ex loss: 0.592040  [   84/  300]
per-ex loss: 0.557931  [   87/  300]
per-ex loss: 0.476629  [   90/  300]
per-ex loss: 0.507718  [   93/  300]
per-ex loss: 0.425498  [   96/  300]
per-ex loss: 0.402529  [   99/  300]
per-ex loss: 0.614929  [  102/  300]
per-ex loss: 0.418427  [  105/  300]
per-ex loss: 0.438283  [  108/  300]
per-ex loss: 0.527642  [  111/  300]
per-ex loss: 0.433591  [  114/  300]
per-ex loss: 0.628598  [  117/  300]
per-ex loss: 0.388736  [  120/  300]
per-ex loss: 0.551983  [  123/  300]
per-ex loss: 0.389442  [  126/  300]
per-ex loss: 0.432149  [  129/  300]
per-ex loss: 0.425583  [  132/  300]
per-ex loss: 0.552612  [  135/  300]
per-ex loss: 0.456913  [  138/  300]
per-ex loss: 0.560931  [  141/  300]
per-ex loss: 0.530342  [  144/  300]
per-ex loss: 0.460705  [  147/  300]
per-ex loss: 0.556045  [  150/  300]
per-ex loss: 0.570019  [  153/  300]
per-ex loss: 0.469422  [  156/  300]
per-ex loss: 0.516985  [  159/  300]
per-ex loss: 0.487053  [  162/  300]
per-ex loss: 0.478121  [  165/  300]
per-ex loss: 0.612206  [  168/  300]
per-ex loss: 0.634229  [  171/  300]
per-ex loss: 0.478466  [  174/  300]
per-ex loss: 0.613292  [  177/  300]
per-ex loss: 0.525587  [  180/  300]
per-ex loss: 0.523451  [  183/  300]
per-ex loss: 0.520504  [  186/  300]
per-ex loss: 0.516573  [  189/  300]
per-ex loss: 0.531906  [  192/  300]
per-ex loss: 0.597392  [  195/  300]
per-ex loss: 0.460977  [  198/  300]
per-ex loss: 0.603793  [  201/  300]
per-ex loss: 0.498740  [  204/  300]
per-ex loss: 0.432250  [  207/  300]
per-ex loss: 0.535531  [  210/  300]
per-ex loss: 0.445010  [  213/  300]
per-ex loss: 0.659739  [  216/  300]
per-ex loss: 0.535935  [  219/  300]
per-ex loss: 0.428180  [  222/  300]
per-ex loss: 0.505471  [  225/  300]
per-ex loss: 0.644281  [  228/  300]
per-ex loss: 0.472971  [  231/  300]
per-ex loss: 0.610469  [  234/  300]
per-ex loss: 0.405469  [  237/  300]
per-ex loss: 0.375515  [  240/  300]
per-ex loss: 0.515845  [  243/  300]
per-ex loss: 0.592558  [  246/  300]
per-ex loss: 0.461558  [  249/  300]
per-ex loss: 0.426716  [  252/  300]
per-ex loss: 0.493626  [  255/  300]
per-ex loss: 0.471101  [  258/  300]
per-ex loss: 0.421016  [  261/  300]
per-ex loss: 0.405154  [  264/  300]
per-ex loss: 0.452392  [  267/  300]
per-ex loss: 0.509168  [  270/  300]
per-ex loss: 0.509008  [  273/  300]
per-ex loss: 0.531134  [  276/  300]
per-ex loss: 0.396534  [  279/  300]
per-ex loss: 0.648611  [  282/  300]
per-ex loss: 0.448429  [  285/  300]
per-ex loss: 0.471209  [  288/  300]
per-ex loss: 0.508156  [  291/  300]
per-ex loss: 0.461352  [  294/  300]
per-ex loss: 0.458494  [  297/  300]
per-ex loss: 0.505049  [  300/  300]
Train Error: Avg loss: 0.50122850
validation Error: 
 Avg loss: 0.50589494 
 F1: 0.483030 
 Precision: 0.456749 
 Recall: 0.512521
 IoU: 0.318418

test Error: 
 Avg loss: 0.45233291 
 F1: 0.548488 
 Precision: 0.518781 
 Recall: 0.581804
 IoU: 0.377874

We have finished training iteration 144
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_142_.pth
per-ex loss: 0.567546  [    3/  300]
per-ex loss: 0.460079  [    6/  300]
per-ex loss: 0.439864  [    9/  300]
per-ex loss: 0.628457  [   12/  300]
per-ex loss: 0.617407  [   15/  300]
per-ex loss: 0.511696  [   18/  300]
per-ex loss: 0.482525  [   21/  300]
per-ex loss: 0.478359  [   24/  300]
per-ex loss: 0.435024  [   27/  300]
per-ex loss: 0.479427  [   30/  300]
per-ex loss: 0.525991  [   33/  300]
per-ex loss: 0.448486  [   36/  300]
per-ex loss: 0.478890  [   39/  300]
per-ex loss: 0.583154  [   42/  300]
per-ex loss: 0.469289  [   45/  300]
per-ex loss: 0.610541  [   48/  300]
per-ex loss: 0.560489  [   51/  300]
per-ex loss: 0.488074  [   54/  300]
per-ex loss: 0.448502  [   57/  300]
per-ex loss: 0.367824  [   60/  300]
per-ex loss: 0.582753  [   63/  300]
per-ex loss: 0.511793  [   66/  300]
per-ex loss: 0.403775  [   69/  300]
per-ex loss: 0.473714  [   72/  300]
per-ex loss: 0.489585  [   75/  300]
per-ex loss: 0.515773  [   78/  300]
per-ex loss: 0.478941  [   81/  300]
per-ex loss: 0.487756  [   84/  300]
per-ex loss: 0.413560  [   87/  300]
per-ex loss: 0.539029  [   90/  300]
per-ex loss: 0.538226  [   93/  300]
per-ex loss: 0.453516  [   96/  300]
per-ex loss: 0.574922  [   99/  300]
per-ex loss: 0.524353  [  102/  300]
per-ex loss: 0.393930  [  105/  300]
per-ex loss: 0.473121  [  108/  300]
per-ex loss: 0.420289  [  111/  300]
per-ex loss: 0.447783  [  114/  300]
per-ex loss: 0.592992  [  117/  300]
per-ex loss: 0.616740  [  120/  300]
per-ex loss: 0.456690  [  123/  300]
per-ex loss: 0.483206  [  126/  300]
per-ex loss: 0.506302  [  129/  300]
per-ex loss: 0.570179  [  132/  300]
per-ex loss: 0.487164  [  135/  300]
per-ex loss: 0.620620  [  138/  300]
per-ex loss: 0.531036  [  141/  300]
per-ex loss: 0.417730  [  144/  300]
per-ex loss: 0.536213  [  147/  300]
per-ex loss: 0.374915  [  150/  300]
per-ex loss: 0.570886  [  153/  300]
per-ex loss: 0.484657  [  156/  300]
per-ex loss: 0.477174  [  159/  300]
per-ex loss: 0.683632  [  162/  300]
per-ex loss: 0.535205  [  165/  300]
per-ex loss: 0.432138  [  168/  300]
per-ex loss: 0.422259  [  171/  300]
per-ex loss: 0.428383  [  174/  300]
per-ex loss: 0.495864  [  177/  300]
per-ex loss: 0.558183  [  180/  300]
per-ex loss: 0.523075  [  183/  300]
per-ex loss: 0.589923  [  186/  300]
per-ex loss: 0.474886  [  189/  300]
per-ex loss: 0.455389  [  192/  300]
per-ex loss: 0.551439  [  195/  300]
per-ex loss: 0.589856  [  198/  300]
per-ex loss: 0.547217  [  201/  300]
per-ex loss: 0.464880  [  204/  300]
per-ex loss: 0.432004  [  207/  300]
per-ex loss: 0.493988  [  210/  300]
per-ex loss: 0.431956  [  213/  300]
per-ex loss: 0.424653  [  216/  300]
per-ex loss: 0.387710  [  219/  300]
per-ex loss: 0.534907  [  222/  300]
per-ex loss: 0.494242  [  225/  300]
per-ex loss: 0.487697  [  228/  300]
per-ex loss: 0.397041  [  231/  300]
per-ex loss: 0.681770  [  234/  300]
per-ex loss: 0.447200  [  237/  300]
per-ex loss: 0.451497  [  240/  300]
per-ex loss: 0.481891  [  243/  300]
per-ex loss: 0.562539  [  246/  300]
per-ex loss: 0.474439  [  249/  300]
per-ex loss: 0.622675  [  252/  300]
per-ex loss: 0.502057  [  255/  300]
per-ex loss: 0.469761  [  258/  300]
per-ex loss: 0.431655  [  261/  300]
per-ex loss: 0.436721  [  264/  300]
per-ex loss: 0.510255  [  267/  300]
per-ex loss: 0.511589  [  270/  300]
per-ex loss: 0.603253  [  273/  300]
per-ex loss: 0.433272  [  276/  300]
per-ex loss: 0.516584  [  279/  300]
per-ex loss: 0.428663  [  282/  300]
per-ex loss: 0.482844  [  285/  300]
per-ex loss: 0.571465  [  288/  300]
per-ex loss: 0.497213  [  291/  300]
per-ex loss: 0.504892  [  294/  300]
per-ex loss: 0.479031  [  297/  300]
per-ex loss: 0.429018  [  300/  300]
Train Error: Avg loss: 0.49897760
validation Error: 
 Avg loss: 0.50205376 
 F1: 0.485451 
 Precision: 0.462694 
 Recall: 0.510561
 IoU: 0.320525

test Error: 
 Avg loss: 0.45129412 
 F1: 0.549434 
 Precision: 0.530577 
 Recall: 0.569680
 IoU: 0.378772

We have finished training iteration 145
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_143_.pth
per-ex loss: 0.525166  [    3/  300]
per-ex loss: 0.516580  [    6/  300]
per-ex loss: 0.595482  [    9/  300]
per-ex loss: 0.554591  [   12/  300]
per-ex loss: 0.618818  [   15/  300]
per-ex loss: 0.476382  [   18/  300]
per-ex loss: 0.429788  [   21/  300]
per-ex loss: 0.445643  [   24/  300]
per-ex loss: 0.553470  [   27/  300]
per-ex loss: 0.442778  [   30/  300]
per-ex loss: 0.463451  [   33/  300]
per-ex loss: 0.545206  [   36/  300]
per-ex loss: 0.453358  [   39/  300]
per-ex loss: 0.438711  [   42/  300]
per-ex loss: 0.485639  [   45/  300]
per-ex loss: 0.472736  [   48/  300]
per-ex loss: 0.601084  [   51/  300]
per-ex loss: 0.480754  [   54/  300]
per-ex loss: 0.479586  [   57/  300]
per-ex loss: 0.451285  [   60/  300]
per-ex loss: 0.467822  [   63/  300]
per-ex loss: 0.411583  [   66/  300]
per-ex loss: 0.580594  [   69/  300]
per-ex loss: 0.603778  [   72/  300]
per-ex loss: 0.449963  [   75/  300]
per-ex loss: 0.509664  [   78/  300]
per-ex loss: 0.546729  [   81/  300]
per-ex loss: 0.409364  [   84/  300]
per-ex loss: 0.445900  [   87/  300]
per-ex loss: 0.541777  [   90/  300]
per-ex loss: 0.422980  [   93/  300]
per-ex loss: 0.418739  [   96/  300]
per-ex loss: 0.515654  [   99/  300]
per-ex loss: 0.442494  [  102/  300]
per-ex loss: 0.399339  [  105/  300]
per-ex loss: 0.557721  [  108/  300]
per-ex loss: 0.416803  [  111/  300]
per-ex loss: 0.500193  [  114/  300]
per-ex loss: 0.673138  [  117/  300]
per-ex loss: 0.591810  [  120/  300]
per-ex loss: 0.512105  [  123/  300]
per-ex loss: 0.600695  [  126/  300]
per-ex loss: 0.418862  [  129/  300]
per-ex loss: 0.462487  [  132/  300]
per-ex loss: 0.417705  [  135/  300]
per-ex loss: 0.569176  [  138/  300]
per-ex loss: 0.442192  [  141/  300]
per-ex loss: 0.472008  [  144/  300]
per-ex loss: 0.478831  [  147/  300]
per-ex loss: 0.555541  [  150/  300]
per-ex loss: 0.648570  [  153/  300]
per-ex loss: 0.549106  [  156/  300]
per-ex loss: 0.448156  [  159/  300]
per-ex loss: 0.505980  [  162/  300]
per-ex loss: 0.510241  [  165/  300]
per-ex loss: 0.438102  [  168/  300]
per-ex loss: 0.498997  [  171/  300]
per-ex loss: 0.588010  [  174/  300]
per-ex loss: 0.524613  [  177/  300]
per-ex loss: 0.504432  [  180/  300]
per-ex loss: 0.429724  [  183/  300]
per-ex loss: 0.534150  [  186/  300]
per-ex loss: 0.398470  [  189/  300]
per-ex loss: 0.558369  [  192/  300]
per-ex loss: 0.593909  [  195/  300]
per-ex loss: 0.444308  [  198/  300]
per-ex loss: 0.451847  [  201/  300]
per-ex loss: 0.468629  [  204/  300]
per-ex loss: 0.520568  [  207/  300]
per-ex loss: 0.437332  [  210/  300]
per-ex loss: 0.394069  [  213/  300]
per-ex loss: 0.603944  [  216/  300]
per-ex loss: 0.515625  [  219/  300]
per-ex loss: 0.511782  [  222/  300]
per-ex loss: 0.578830  [  225/  300]
per-ex loss: 0.572358  [  228/  300]
per-ex loss: 0.383344  [  231/  300]
per-ex loss: 0.630347  [  234/  300]
per-ex loss: 0.490017  [  237/  300]
per-ex loss: 0.572324  [  240/  300]
per-ex loss: 0.506069  [  243/  300]
per-ex loss: 0.413723  [  246/  300]
per-ex loss: 0.484308  [  249/  300]
per-ex loss: 0.467040  [  252/  300]
per-ex loss: 0.481605  [  255/  300]
per-ex loss: 0.574542  [  258/  300]
per-ex loss: 0.458389  [  261/  300]
per-ex loss: 0.481617  [  264/  300]
per-ex loss: 0.616979  [  267/  300]
per-ex loss: 0.457675  [  270/  300]
per-ex loss: 0.507707  [  273/  300]
per-ex loss: 0.492183  [  276/  300]
per-ex loss: 0.546336  [  279/  300]
per-ex loss: 0.545213  [  282/  300]
per-ex loss: 0.432881  [  285/  300]
per-ex loss: 0.519397  [  288/  300]
per-ex loss: 0.396810  [  291/  300]
per-ex loss: 0.592627  [  294/  300]
per-ex loss: 0.634355  [  297/  300]
per-ex loss: 0.530646  [  300/  300]
Train Error: Avg loss: 0.50312409
validation Error: 
 Avg loss: 0.51416536 
 F1: 0.475036 
 Precision: 0.458451 
 Recall: 0.492865
 IoU: 0.311506

test Error: 
 Avg loss: 0.45269239 
 F1: 0.547839 
 Precision: 0.521467 
 Recall: 0.577021
 IoU: 0.377258

We have finished training iteration 146
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_144_.pth
per-ex loss: 0.444866  [    3/  300]
per-ex loss: 0.467524  [    6/  300]
per-ex loss: 0.494566  [    9/  300]
per-ex loss: 0.484233  [   12/  300]
per-ex loss: 0.487171  [   15/  300]
per-ex loss: 0.573976  [   18/  300]
per-ex loss: 0.556489  [   21/  300]
per-ex loss: 0.483420  [   24/  300]
per-ex loss: 0.417912  [   27/  300]
per-ex loss: 0.495967  [   30/  300]
per-ex loss: 0.505873  [   33/  300]
per-ex loss: 0.520552  [   36/  300]
per-ex loss: 0.509429  [   39/  300]
per-ex loss: 0.533237  [   42/  300]
per-ex loss: 0.508912  [   45/  300]
per-ex loss: 0.414949  [   48/  300]
per-ex loss: 0.438504  [   51/  300]
per-ex loss: 0.521599  [   54/  300]
per-ex loss: 0.542754  [   57/  300]
per-ex loss: 0.534069  [   60/  300]
per-ex loss: 0.429395  [   63/  300]
per-ex loss: 0.450528  [   66/  300]
per-ex loss: 0.540446  [   69/  300]
per-ex loss: 0.626595  [   72/  300]
per-ex loss: 0.356097  [   75/  300]
per-ex loss: 0.533173  [   78/  300]
per-ex loss: 0.619966  [   81/  300]
per-ex loss: 0.408998  [   84/  300]
per-ex loss: 0.479102  [   87/  300]
per-ex loss: 0.511490  [   90/  300]
per-ex loss: 0.456214  [   93/  300]
per-ex loss: 0.412935  [   96/  300]
per-ex loss: 0.408863  [   99/  300]
per-ex loss: 0.456941  [  102/  300]
per-ex loss: 0.421988  [  105/  300]
per-ex loss: 0.509297  [  108/  300]
per-ex loss: 0.516576  [  111/  300]
per-ex loss: 0.490206  [  114/  300]
per-ex loss: 0.528445  [  117/  300]
per-ex loss: 0.489438  [  120/  300]
per-ex loss: 0.471059  [  123/  300]
per-ex loss: 0.581290  [  126/  300]
per-ex loss: 0.545157  [  129/  300]
per-ex loss: 0.516853  [  132/  300]
per-ex loss: 0.488654  [  135/  300]
per-ex loss: 0.445783  [  138/  300]
per-ex loss: 0.527529  [  141/  300]
per-ex loss: 0.451587  [  144/  300]
per-ex loss: 0.563697  [  147/  300]
per-ex loss: 0.627525  [  150/  300]
per-ex loss: 0.481717  [  153/  300]
per-ex loss: 0.472169  [  156/  300]
per-ex loss: 0.452072  [  159/  300]
per-ex loss: 0.560098  [  162/  300]
per-ex loss: 0.465465  [  165/  300]
per-ex loss: 0.441978  [  168/  300]
per-ex loss: 0.601342  [  171/  300]
per-ex loss: 0.465445  [  174/  300]
per-ex loss: 0.638636  [  177/  300]
per-ex loss: 0.438354  [  180/  300]
per-ex loss: 0.586800  [  183/  300]
per-ex loss: 0.559158  [  186/  300]
per-ex loss: 0.446387  [  189/  300]
per-ex loss: 0.510732  [  192/  300]
per-ex loss: 0.494735  [  195/  300]
per-ex loss: 0.623030  [  198/  300]
per-ex loss: 0.605587  [  201/  300]
per-ex loss: 0.613933  [  204/  300]
per-ex loss: 0.456146  [  207/  300]
per-ex loss: 0.421708  [  210/  300]
per-ex loss: 0.592095  [  213/  300]
per-ex loss: 0.543004  [  216/  300]
per-ex loss: 0.553256  [  219/  300]
per-ex loss: 0.430935  [  222/  300]
per-ex loss: 0.469396  [  225/  300]
per-ex loss: 0.439402  [  228/  300]
per-ex loss: 0.444209  [  231/  300]
per-ex loss: 0.465381  [  234/  300]
per-ex loss: 0.667428  [  237/  300]
per-ex loss: 0.490392  [  240/  300]
per-ex loss: 0.427387  [  243/  300]
per-ex loss: 0.406640  [  246/  300]
per-ex loss: 0.364716  [  249/  300]
per-ex loss: 0.530133  [  252/  300]
per-ex loss: 0.559913  [  255/  300]
per-ex loss: 0.481734  [  258/  300]
per-ex loss: 0.463109  [  261/  300]
per-ex loss: 0.506020  [  264/  300]
per-ex loss: 0.528781  [  267/  300]
per-ex loss: 0.546721  [  270/  300]
per-ex loss: 0.476141  [  273/  300]
per-ex loss: 0.513401  [  276/  300]
per-ex loss: 0.557468  [  279/  300]
per-ex loss: 0.540077  [  282/  300]
per-ex loss: 0.483397  [  285/  300]
per-ex loss: 0.464165  [  288/  300]
per-ex loss: 0.462397  [  291/  300]
per-ex loss: 0.387344  [  294/  300]
per-ex loss: 0.591641  [  297/  300]
per-ex loss: 0.494082  [  300/  300]
Train Error: Avg loss: 0.50018082
validation Error: 
 Avg loss: 0.49695633 
 F1: 0.485241 
 Precision: 0.444487 
 Recall: 0.534222
 IoU: 0.320342

test Error: 
 Avg loss: 0.45036745 
 F1: 0.550150 
 Precision: 0.504587 
 Recall: 0.604758
 IoU: 0.379453

We have finished training iteration 147
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_145_.pth
per-ex loss: 0.433082  [    3/  300]
per-ex loss: 0.527789  [    6/  300]
per-ex loss: 0.461466  [    9/  300]
per-ex loss: 0.618298  [   12/  300]
per-ex loss: 0.383282  [   15/  300]
per-ex loss: 0.451112  [   18/  300]
per-ex loss: 0.493564  [   21/  300]
per-ex loss: 0.610598  [   24/  300]
per-ex loss: 0.544061  [   27/  300]
per-ex loss: 0.626695  [   30/  300]
per-ex loss: 0.449846  [   33/  300]
per-ex loss: 0.439383  [   36/  300]
per-ex loss: 0.567196  [   39/  300]
per-ex loss: 0.518663  [   42/  300]
per-ex loss: 0.523994  [   45/  300]
per-ex loss: 0.476645  [   48/  300]
per-ex loss: 0.528404  [   51/  300]
per-ex loss: 0.503727  [   54/  300]
per-ex loss: 0.430960  [   57/  300]
per-ex loss: 0.466286  [   60/  300]
per-ex loss: 0.487946  [   63/  300]
per-ex loss: 0.448550  [   66/  300]
per-ex loss: 0.450696  [   69/  300]
per-ex loss: 0.587681  [   72/  300]
per-ex loss: 0.470425  [   75/  300]
per-ex loss: 0.524127  [   78/  300]
per-ex loss: 0.441979  [   81/  300]
per-ex loss: 0.575343  [   84/  300]
per-ex loss: 0.493104  [   87/  300]
per-ex loss: 0.422569  [   90/  300]
per-ex loss: 0.401100  [   93/  300]
per-ex loss: 0.426519  [   96/  300]
per-ex loss: 0.452029  [   99/  300]
per-ex loss: 0.478345  [  102/  300]
per-ex loss: 0.474131  [  105/  300]
per-ex loss: 0.524808  [  108/  300]
per-ex loss: 0.516970  [  111/  300]
per-ex loss: 0.592538  [  114/  300]
per-ex loss: 0.451054  [  117/  300]
per-ex loss: 0.452764  [  120/  300]
per-ex loss: 0.589344  [  123/  300]
per-ex loss: 0.472863  [  126/  300]
per-ex loss: 0.462675  [  129/  300]
per-ex loss: 0.560221  [  132/  300]
per-ex loss: 0.537712  [  135/  300]
per-ex loss: 0.448024  [  138/  300]
per-ex loss: 0.391273  [  141/  300]
per-ex loss: 0.410740  [  144/  300]
per-ex loss: 0.574572  [  147/  300]
per-ex loss: 0.490390  [  150/  300]
per-ex loss: 0.604559  [  153/  300]
per-ex loss: 0.444989  [  156/  300]
per-ex loss: 0.544350  [  159/  300]
per-ex loss: 0.498542  [  162/  300]
per-ex loss: 0.530326  [  165/  300]
per-ex loss: 0.482947  [  168/  300]
per-ex loss: 0.513996  [  171/  300]
per-ex loss: 0.621531  [  174/  300]
per-ex loss: 0.428879  [  177/  300]
per-ex loss: 0.588632  [  180/  300]
per-ex loss: 0.458780  [  183/  300]
per-ex loss: 0.417812  [  186/  300]
per-ex loss: 0.429450  [  189/  300]
per-ex loss: 0.478219  [  192/  300]
per-ex loss: 0.399609  [  195/  300]
per-ex loss: 0.410902  [  198/  300]
per-ex loss: 0.586507  [  201/  300]
per-ex loss: 0.539520  [  204/  300]
per-ex loss: 0.511881  [  207/  300]
per-ex loss: 0.545636  [  210/  300]
per-ex loss: 0.421706  [  213/  300]
per-ex loss: 0.417479  [  216/  300]
per-ex loss: 0.487123  [  219/  300]
per-ex loss: 0.507676  [  222/  300]
per-ex loss: 0.591322  [  225/  300]
per-ex loss: 0.517887  [  228/  300]
per-ex loss: 0.477540  [  231/  300]
per-ex loss: 0.617508  [  234/  300]
per-ex loss: 0.535263  [  237/  300]
per-ex loss: 0.520907  [  240/  300]
per-ex loss: 0.463916  [  243/  300]
per-ex loss: 0.700879  [  246/  300]
per-ex loss: 0.487320  [  249/  300]
per-ex loss: 0.452641  [  252/  300]
per-ex loss: 0.475147  [  255/  300]
per-ex loss: 0.443247  [  258/  300]
per-ex loss: 0.614068  [  261/  300]
per-ex loss: 0.441176  [  264/  300]
per-ex loss: 0.494226  [  267/  300]
per-ex loss: 0.487731  [  270/  300]
per-ex loss: 0.412899  [  273/  300]
per-ex loss: 0.506234  [  276/  300]
per-ex loss: 0.591760  [  279/  300]
per-ex loss: 0.497065  [  282/  300]
per-ex loss: 0.545523  [  285/  300]
per-ex loss: 0.575134  [  288/  300]
per-ex loss: 0.480382  [  291/  300]
per-ex loss: 0.412858  [  294/  300]
per-ex loss: 0.588155  [  297/  300]
per-ex loss: 0.650182  [  300/  300]
Train Error: Avg loss: 0.50117562
validation Error: 
 Avg loss: 0.50987885 
 F1: 0.474817 
 Precision: 0.444697 
 Recall: 0.509315
 IoU: 0.311318

test Error: 
 Avg loss: 0.45395583 
 F1: 0.546694 
 Precision: 0.501787 
 Recall: 0.600429
 IoU: 0.376173

We have finished training iteration 148
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_146_.pth
per-ex loss: 0.408143  [    3/  300]
per-ex loss: 0.543804  [    6/  300]
per-ex loss: 0.599102  [    9/  300]
per-ex loss: 0.572139  [   12/  300]
per-ex loss: 0.525109  [   15/  300]
per-ex loss: 0.595366  [   18/  300]
per-ex loss: 0.537209  [   21/  300]
per-ex loss: 0.559152  [   24/  300]
per-ex loss: 0.416507  [   27/  300]
per-ex loss: 0.413555  [   30/  300]
per-ex loss: 0.444550  [   33/  300]
per-ex loss: 0.446985  [   36/  300]
per-ex loss: 0.608063  [   39/  300]
per-ex loss: 0.480223  [   42/  300]
per-ex loss: 0.566826  [   45/  300]
per-ex loss: 0.479132  [   48/  300]
per-ex loss: 0.489895  [   51/  300]
per-ex loss: 0.435220  [   54/  300]
per-ex loss: 0.478105  [   57/  300]
per-ex loss: 0.505868  [   60/  300]
per-ex loss: 0.415437  [   63/  300]
per-ex loss: 0.488630  [   66/  300]
per-ex loss: 0.498714  [   69/  300]
per-ex loss: 0.503103  [   72/  300]
per-ex loss: 0.488966  [   75/  300]
per-ex loss: 0.502510  [   78/  300]
per-ex loss: 0.424980  [   81/  300]
per-ex loss: 0.458224  [   84/  300]
per-ex loss: 0.435389  [   87/  300]
per-ex loss: 0.555946  [   90/  300]
per-ex loss: 0.392718  [   93/  300]
per-ex loss: 0.448359  [   96/  300]
per-ex loss: 0.467378  [   99/  300]
per-ex loss: 0.521917  [  102/  300]
per-ex loss: 0.548894  [  105/  300]
per-ex loss: 0.420378  [  108/  300]
per-ex loss: 0.459055  [  111/  300]
per-ex loss: 0.417339  [  114/  300]
per-ex loss: 0.473851  [  117/  300]
per-ex loss: 0.578616  [  120/  300]
per-ex loss: 0.432626  [  123/  300]
per-ex loss: 0.515473  [  126/  300]
per-ex loss: 0.421831  [  129/  300]
per-ex loss: 0.446794  [  132/  300]
per-ex loss: 0.520568  [  135/  300]
per-ex loss: 0.507760  [  138/  300]
per-ex loss: 0.431997  [  141/  300]
per-ex loss: 0.449946  [  144/  300]
per-ex loss: 0.414735  [  147/  300]
per-ex loss: 0.657691  [  150/  300]
per-ex loss: 0.631399  [  153/  300]
per-ex loss: 0.523689  [  156/  300]
per-ex loss: 0.544246  [  159/  300]
per-ex loss: 0.462494  [  162/  300]
per-ex loss: 0.494053  [  165/  300]
per-ex loss: 0.511539  [  168/  300]
per-ex loss: 0.521271  [  171/  300]
per-ex loss: 0.654522  [  174/  300]
per-ex loss: 0.566074  [  177/  300]
per-ex loss: 0.433307  [  180/  300]
per-ex loss: 0.507946  [  183/  300]
per-ex loss: 0.544253  [  186/  300]
per-ex loss: 0.416857  [  189/  300]
per-ex loss: 0.464419  [  192/  300]
per-ex loss: 0.429609  [  195/  300]
per-ex loss: 0.406244  [  198/  300]
per-ex loss: 0.573996  [  201/  300]
per-ex loss: 0.629393  [  204/  300]
per-ex loss: 0.453360  [  207/  300]
per-ex loss: 0.459869  [  210/  300]
per-ex loss: 0.531603  [  213/  300]
per-ex loss: 0.575336  [  216/  300]
per-ex loss: 0.584661  [  219/  300]
per-ex loss: 0.411588  [  222/  300]
per-ex loss: 0.473828  [  225/  300]
per-ex loss: 0.632228  [  228/  300]
per-ex loss: 0.433510  [  231/  300]
per-ex loss: 0.557000  [  234/  300]
per-ex loss: 0.418524  [  237/  300]
per-ex loss: 0.511961  [  240/  300]
per-ex loss: 0.535376  [  243/  300]
per-ex loss: 0.597197  [  246/  300]
per-ex loss: 0.612224  [  249/  300]
per-ex loss: 0.474895  [  252/  300]
per-ex loss: 0.454852  [  255/  300]
per-ex loss: 0.592218  [  258/  300]
per-ex loss: 0.558666  [  261/  300]
per-ex loss: 0.410060  [  264/  300]
per-ex loss: 0.459139  [  267/  300]
per-ex loss: 0.462284  [  270/  300]
per-ex loss: 0.481005  [  273/  300]
per-ex loss: 0.422649  [  276/  300]
per-ex loss: 0.525702  [  279/  300]
per-ex loss: 0.426631  [  282/  300]
per-ex loss: 0.452275  [  285/  300]
per-ex loss: 0.389452  [  288/  300]
per-ex loss: 0.646137  [  291/  300]
per-ex loss: 0.514812  [  294/  300]
per-ex loss: 0.440107  [  297/  300]
per-ex loss: 0.573893  [  300/  300]
Train Error: Avg loss: 0.49793133
validation Error: 
 Avg loss: 0.49812448 
 F1: 0.483792 
 Precision: 0.429319 
 Recall: 0.554098
 IoU: 0.319080

test Error: 
 Avg loss: 0.45293146 
 F1: 0.547768 
 Precision: 0.483014 
 Recall: 0.632573
 IoU: 0.377191

We have finished training iteration 149
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_147_.pth
per-ex loss: 0.420945  [    3/  300]
per-ex loss: 0.533102  [    6/  300]
per-ex loss: 0.457574  [    9/  300]
per-ex loss: 0.502917  [   12/  300]
per-ex loss: 0.392224  [   15/  300]
per-ex loss: 0.422182  [   18/  300]
per-ex loss: 0.555973  [   21/  300]
per-ex loss: 0.522198  [   24/  300]
per-ex loss: 0.454419  [   27/  300]
per-ex loss: 0.630455  [   30/  300]
per-ex loss: 0.579093  [   33/  300]
per-ex loss: 0.458631  [   36/  300]
per-ex loss: 0.485323  [   39/  300]
per-ex loss: 0.525091  [   42/  300]
per-ex loss: 0.605893  [   45/  300]
per-ex loss: 0.476607  [   48/  300]
per-ex loss: 0.587769  [   51/  300]
per-ex loss: 0.393752  [   54/  300]
per-ex loss: 0.405524  [   57/  300]
per-ex loss: 0.510381  [   60/  300]
per-ex loss: 0.420777  [   63/  300]
per-ex loss: 0.594274  [   66/  300]
per-ex loss: 0.570470  [   69/  300]
per-ex loss: 0.560504  [   72/  300]
per-ex loss: 0.439211  [   75/  300]
per-ex loss: 0.620882  [   78/  300]
per-ex loss: 0.399488  [   81/  300]
per-ex loss: 0.559522  [   84/  300]
per-ex loss: 0.560631  [   87/  300]
per-ex loss: 0.549361  [   90/  300]
per-ex loss: 0.475313  [   93/  300]
per-ex loss: 0.487293  [   96/  300]
per-ex loss: 0.441779  [   99/  300]
per-ex loss: 0.567195  [  102/  300]
per-ex loss: 0.382396  [  105/  300]
per-ex loss: 0.484560  [  108/  300]
per-ex loss: 0.588938  [  111/  300]
per-ex loss: 0.558587  [  114/  300]
per-ex loss: 0.514378  [  117/  300]
per-ex loss: 0.418573  [  120/  300]
per-ex loss: 0.573857  [  123/  300]
per-ex loss: 0.395211  [  126/  300]
per-ex loss: 0.460079  [  129/  300]
per-ex loss: 0.568266  [  132/  300]
per-ex loss: 0.495200  [  135/  300]
per-ex loss: 0.470260  [  138/  300]
per-ex loss: 0.647310  [  141/  300]
per-ex loss: 0.480684  [  144/  300]
per-ex loss: 0.456409  [  147/  300]
per-ex loss: 0.535574  [  150/  300]
per-ex loss: 0.483889  [  153/  300]
per-ex loss: 0.466037  [  156/  300]
per-ex loss: 0.470296  [  159/  300]
per-ex loss: 0.399005  [  162/  300]
per-ex loss: 0.537616  [  165/  300]
per-ex loss: 0.418343  [  168/  300]
per-ex loss: 0.392900  [  171/  300]
per-ex loss: 0.635970  [  174/  300]
per-ex loss: 0.674617  [  177/  300]
per-ex loss: 0.399050  [  180/  300]
per-ex loss: 0.443555  [  183/  300]
per-ex loss: 0.494360  [  186/  300]
per-ex loss: 0.579642  [  189/  300]
per-ex loss: 0.462774  [  192/  300]
per-ex loss: 0.470257  [  195/  300]
per-ex loss: 0.584175  [  198/  300]
per-ex loss: 0.494526  [  201/  300]
per-ex loss: 0.622600  [  204/  300]
per-ex loss: 0.494148  [  207/  300]
per-ex loss: 0.438848  [  210/  300]
per-ex loss: 0.490903  [  213/  300]
per-ex loss: 0.440347  [  216/  300]
per-ex loss: 0.578732  [  219/  300]
per-ex loss: 0.533123  [  222/  300]
per-ex loss: 0.450046  [  225/  300]
per-ex loss: 0.595641  [  228/  300]
per-ex loss: 0.481537  [  231/  300]
per-ex loss: 0.625490  [  234/  300]
per-ex loss: 0.380393  [  237/  300]
per-ex loss: 0.592003  [  240/  300]
per-ex loss: 0.518116  [  243/  300]
per-ex loss: 0.578231  [  246/  300]
per-ex loss: 0.543707  [  249/  300]
per-ex loss: 0.454601  [  252/  300]
per-ex loss: 0.437936  [  255/  300]
per-ex loss: 0.395311  [  258/  300]
per-ex loss: 0.448811  [  261/  300]
per-ex loss: 0.474695  [  264/  300]
per-ex loss: 0.412664  [  267/  300]
per-ex loss: 0.378340  [  270/  300]
per-ex loss: 0.488312  [  273/  300]
per-ex loss: 0.450549  [  276/  300]
per-ex loss: 0.601812  [  279/  300]
per-ex loss: 0.574376  [  282/  300]
per-ex loss: 0.481936  [  285/  300]
per-ex loss: 0.488198  [  288/  300]
per-ex loss: 0.425084  [  291/  300]
per-ex loss: 0.612310  [  294/  300]
per-ex loss: 0.485011  [  297/  300]
per-ex loss: 0.435980  [  300/  300]
Train Error: Avg loss: 0.50013834
validation Error: 
 Avg loss: 0.49689291 
 F1: 0.483407 
 Precision: 0.421409 
 Recall: 0.566795
 IoU: 0.318746

test Error: 
 Avg loss: 0.45345658 
 F1: 0.547269 
 Precision: 0.475021 
 Recall: 0.645436
 IoU: 0.376717

We have finished training iteration 150
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_148_.pth
per-ex loss: 0.515852  [    3/  300]
per-ex loss: 0.426508  [    6/  300]
per-ex loss: 0.505494  [    9/  300]
per-ex loss: 0.456628  [   12/  300]
per-ex loss: 0.583571  [   15/  300]
per-ex loss: 0.659340  [   18/  300]
per-ex loss: 0.490175  [   21/  300]
per-ex loss: 0.433438  [   24/  300]
per-ex loss: 0.441590  [   27/  300]
per-ex loss: 0.483749  [   30/  300]
per-ex loss: 0.529062  [   33/  300]
per-ex loss: 0.558574  [   36/  300]
per-ex loss: 0.484195  [   39/  300]
per-ex loss: 0.590842  [   42/  300]
per-ex loss: 0.459200  [   45/  300]
per-ex loss: 0.628812  [   48/  300]
per-ex loss: 0.578139  [   51/  300]
per-ex loss: 0.568128  [   54/  300]
per-ex loss: 0.468238  [   57/  300]
per-ex loss: 0.401300  [   60/  300]
per-ex loss: 0.457759  [   63/  300]
per-ex loss: 0.415028  [   66/  300]
per-ex loss: 0.414212  [   69/  300]
per-ex loss: 0.484150  [   72/  300]
per-ex loss: 0.430418  [   75/  300]
per-ex loss: 0.540598  [   78/  300]
per-ex loss: 0.482732  [   81/  300]
per-ex loss: 0.421305  [   84/  300]
per-ex loss: 0.577839  [   87/  300]
per-ex loss: 0.569536  [   90/  300]
per-ex loss: 0.417756  [   93/  300]
per-ex loss: 0.391425  [   96/  300]
per-ex loss: 0.464001  [   99/  300]
per-ex loss: 0.372728  [  102/  300]
per-ex loss: 0.621472  [  105/  300]
per-ex loss: 0.426966  [  108/  300]
per-ex loss: 0.431015  [  111/  300]
per-ex loss: 0.462066  [  114/  300]
per-ex loss: 0.466024  [  117/  300]
per-ex loss: 0.529301  [  120/  300]
per-ex loss: 0.516468  [  123/  300]
per-ex loss: 0.599470  [  126/  300]
per-ex loss: 0.512987  [  129/  300]
per-ex loss: 0.434126  [  132/  300]
per-ex loss: 0.484943  [  135/  300]
per-ex loss: 0.445827  [  138/  300]
per-ex loss: 0.431151  [  141/  300]
per-ex loss: 0.640255  [  144/  300]
per-ex loss: 0.473171  [  147/  300]
per-ex loss: 0.462979  [  150/  300]
per-ex loss: 0.641450  [  153/  300]
per-ex loss: 0.426038  [  156/  300]
per-ex loss: 0.494781  [  159/  300]
per-ex loss: 0.592613  [  162/  300]
per-ex loss: 0.637724  [  165/  300]
per-ex loss: 0.456831  [  168/  300]
per-ex loss: 0.545766  [  171/  300]
per-ex loss: 0.409041  [  174/  300]
per-ex loss: 0.605969  [  177/  300]
per-ex loss: 0.587931  [  180/  300]
per-ex loss: 0.421115  [  183/  300]
per-ex loss: 0.420933  [  186/  300]
per-ex loss: 0.401013  [  189/  300]
per-ex loss: 0.561678  [  192/  300]
per-ex loss: 0.550888  [  195/  300]
per-ex loss: 0.404621  [  198/  300]
per-ex loss: 0.633526  [  201/  300]
per-ex loss: 0.446001  [  204/  300]
per-ex loss: 0.564899  [  207/  300]
per-ex loss: 0.497814  [  210/  300]
per-ex loss: 0.404564  [  213/  300]
per-ex loss: 0.555175  [  216/  300]
per-ex loss: 0.443274  [  219/  300]
per-ex loss: 0.531748  [  222/  300]
per-ex loss: 0.520462  [  225/  300]
per-ex loss: 0.482773  [  228/  300]
per-ex loss: 0.420273  [  231/  300]
per-ex loss: 0.490933  [  234/  300]
per-ex loss: 0.490241  [  237/  300]
per-ex loss: 0.433445  [  240/  300]
per-ex loss: 0.518616  [  243/  300]
per-ex loss: 0.430743  [  246/  300]
per-ex loss: 0.516199  [  249/  300]
per-ex loss: 0.552577  [  252/  300]
per-ex loss: 0.558780  [  255/  300]
per-ex loss: 0.544020  [  258/  300]
per-ex loss: 0.482956  [  261/  300]
per-ex loss: 0.562695  [  264/  300]
per-ex loss: 0.487382  [  267/  300]
per-ex loss: 0.538564  [  270/  300]
per-ex loss: 0.485418  [  273/  300]
per-ex loss: 0.485995  [  276/  300]
per-ex loss: 0.396901  [  279/  300]
per-ex loss: 0.520151  [  282/  300]
per-ex loss: 0.531684  [  285/  300]
per-ex loss: 0.606834  [  288/  300]
per-ex loss: 0.525531  [  291/  300]
per-ex loss: 0.529387  [  294/  300]
per-ex loss: 0.491901  [  297/  300]
per-ex loss: 0.362581  [  300/  300]
Train Error: Avg loss: 0.49842979
validation Error: 
 Avg loss: 0.49635381 
 F1: 0.485137 
 Precision: 0.433085 
 Recall: 0.551410
 IoU: 0.320251

test Error: 
 Avg loss: 0.45275277 
 F1: 0.548098 
 Precision: 0.485854 
 Recall: 0.628634
 IoU: 0.377503

We have finished training iteration 151
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_149_.pth
per-ex loss: 0.538417  [    3/  300]
per-ex loss: 0.515993  [    6/  300]
per-ex loss: 0.406047  [    9/  300]
per-ex loss: 0.558196  [   12/  300]
per-ex loss: 0.578831  [   15/  300]
per-ex loss: 0.456832  [   18/  300]
per-ex loss: 0.439612  [   21/  300]
per-ex loss: 0.525310  [   24/  300]
per-ex loss: 0.506265  [   27/  300]
per-ex loss: 0.423929  [   30/  300]
per-ex loss: 0.602599  [   33/  300]
per-ex loss: 0.554760  [   36/  300]
per-ex loss: 0.527370  [   39/  300]
per-ex loss: 0.358863  [   42/  300]
per-ex loss: 0.476127  [   45/  300]
per-ex loss: 0.516108  [   48/  300]
per-ex loss: 0.590994  [   51/  300]
per-ex loss: 0.569438  [   54/  300]
per-ex loss: 0.450343  [   57/  300]
per-ex loss: 0.487676  [   60/  300]
per-ex loss: 0.516192  [   63/  300]
per-ex loss: 0.413908  [   66/  300]
per-ex loss: 0.649716  [   69/  300]
per-ex loss: 0.500152  [   72/  300]
per-ex loss: 0.488980  [   75/  300]
per-ex loss: 0.521839  [   78/  300]
per-ex loss: 0.489405  [   81/  300]
per-ex loss: 0.488726  [   84/  300]
per-ex loss: 0.439788  [   87/  300]
per-ex loss: 0.541755  [   90/  300]
per-ex loss: 0.417784  [   93/  300]
per-ex loss: 0.515691  [   96/  300]
per-ex loss: 0.431153  [   99/  300]
per-ex loss: 0.555738  [  102/  300]
per-ex loss: 0.462863  [  105/  300]
per-ex loss: 0.609936  [  108/  300]
per-ex loss: 0.513304  [  111/  300]
per-ex loss: 0.481325  [  114/  300]
per-ex loss: 0.551754  [  117/  300]
per-ex loss: 0.409841  [  120/  300]
per-ex loss: 0.420362  [  123/  300]
per-ex loss: 0.437696  [  126/  300]
per-ex loss: 0.457794  [  129/  300]
per-ex loss: 0.555468  [  132/  300]
per-ex loss: 0.623323  [  135/  300]
per-ex loss: 0.442219  [  138/  300]
per-ex loss: 0.535838  [  141/  300]
per-ex loss: 0.634029  [  144/  300]
per-ex loss: 0.532797  [  147/  300]
per-ex loss: 0.618406  [  150/  300]
per-ex loss: 0.480012  [  153/  300]
per-ex loss: 0.468113  [  156/  300]
per-ex loss: 0.442909  [  159/  300]
per-ex loss: 0.450013  [  162/  300]
per-ex loss: 0.478816  [  165/  300]
per-ex loss: 0.522563  [  168/  300]
per-ex loss: 0.511131  [  171/  300]
per-ex loss: 0.466066  [  174/  300]
per-ex loss: 0.477623  [  177/  300]
per-ex loss: 0.528429  [  180/  300]
per-ex loss: 0.568699  [  183/  300]
per-ex loss: 0.538872  [  186/  300]
per-ex loss: 0.444607  [  189/  300]
per-ex loss: 0.411347  [  192/  300]
per-ex loss: 0.484880  [  195/  300]
per-ex loss: 0.454660  [  198/  300]
per-ex loss: 0.484356  [  201/  300]
per-ex loss: 0.515242  [  204/  300]
per-ex loss: 0.528040  [  207/  300]
per-ex loss: 0.462715  [  210/  300]
per-ex loss: 0.412768  [  213/  300]
per-ex loss: 0.574232  [  216/  300]
per-ex loss: 0.477592  [  219/  300]
per-ex loss: 0.471115  [  222/  300]
per-ex loss: 0.516325  [  225/  300]
per-ex loss: 0.573303  [  228/  300]
per-ex loss: 0.400875  [  231/  300]
per-ex loss: 0.576873  [  234/  300]
per-ex loss: 0.408058  [  237/  300]
per-ex loss: 0.616628  [  240/  300]
per-ex loss: 0.454604  [  243/  300]
per-ex loss: 0.545665  [  246/  300]
per-ex loss: 0.419095  [  249/  300]
per-ex loss: 0.440539  [  252/  300]
per-ex loss: 0.434317  [  255/  300]
per-ex loss: 0.461275  [  258/  300]
per-ex loss: 0.656920  [  261/  300]
per-ex loss: 0.423620  [  264/  300]
per-ex loss: 0.565700  [  267/  300]
per-ex loss: 0.538461  [  270/  300]
per-ex loss: 0.472762  [  273/  300]
per-ex loss: 0.463563  [  276/  300]
per-ex loss: 0.413382  [  279/  300]
per-ex loss: 0.416958  [  282/  300]
per-ex loss: 0.412407  [  285/  300]
per-ex loss: 0.522149  [  288/  300]
per-ex loss: 0.420262  [  291/  300]
per-ex loss: 0.687272  [  294/  300]
per-ex loss: 0.574727  [  297/  300]
per-ex loss: 0.499797  [  300/  300]
Train Error: Avg loss: 0.49911825
validation Error: 
 Avg loss: 0.49058636 
 F1: 0.487051 
 Precision: 0.420344 
 Recall: 0.578926
 IoU: 0.321922

test Error: 
 Avg loss: 0.45875084 
 F1: 0.541900 
 Precision: 0.468816 
 Recall: 0.641977
 IoU: 0.371648

We have finished training iteration 152
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_150_.pth
per-ex loss: 0.545025  [    3/  300]
per-ex loss: 0.435346  [    6/  300]
per-ex loss: 0.519374  [    9/  300]
per-ex loss: 0.512338  [   12/  300]
per-ex loss: 0.587316  [   15/  300]
per-ex loss: 0.444171  [   18/  300]
per-ex loss: 0.549637  [   21/  300]
per-ex loss: 0.568807  [   24/  300]
per-ex loss: 0.465108  [   27/  300]
per-ex loss: 0.467529  [   30/  300]
per-ex loss: 0.580885  [   33/  300]
per-ex loss: 0.537777  [   36/  300]
per-ex loss: 0.496487  [   39/  300]
per-ex loss: 0.585580  [   42/  300]
per-ex loss: 0.541203  [   45/  300]
per-ex loss: 0.478900  [   48/  300]
per-ex loss: 0.497904  [   51/  300]
per-ex loss: 0.567815  [   54/  300]
per-ex loss: 0.444450  [   57/  300]
per-ex loss: 0.475556  [   60/  300]
per-ex loss: 0.444030  [   63/  300]
per-ex loss: 0.431071  [   66/  300]
per-ex loss: 0.607264  [   69/  300]
per-ex loss: 0.426982  [   72/  300]
per-ex loss: 0.387268  [   75/  300]
per-ex loss: 0.377314  [   78/  300]
per-ex loss: 0.443520  [   81/  300]
per-ex loss: 0.566778  [   84/  300]
per-ex loss: 0.457372  [   87/  300]
per-ex loss: 0.457463  [   90/  300]
per-ex loss: 0.458709  [   93/  300]
per-ex loss: 0.545478  [   96/  300]
per-ex loss: 0.465795  [   99/  300]
per-ex loss: 0.539655  [  102/  300]
per-ex loss: 0.581311  [  105/  300]
per-ex loss: 0.434795  [  108/  300]
per-ex loss: 0.432549  [  111/  300]
per-ex loss: 0.499797  [  114/  300]
per-ex loss: 0.473578  [  117/  300]
per-ex loss: 0.505239  [  120/  300]
per-ex loss: 0.497256  [  123/  300]
per-ex loss: 0.416621  [  126/  300]
per-ex loss: 0.423009  [  129/  300]
per-ex loss: 0.564882  [  132/  300]
per-ex loss: 0.509770  [  135/  300]
per-ex loss: 0.464105  [  138/  300]
per-ex loss: 0.503726  [  141/  300]
per-ex loss: 0.484492  [  144/  300]
per-ex loss: 0.526527  [  147/  300]
per-ex loss: 0.459747  [  150/  300]
per-ex loss: 0.476145  [  153/  300]
per-ex loss: 0.470855  [  156/  300]
per-ex loss: 0.431285  [  159/  300]
per-ex loss: 0.486704  [  162/  300]
per-ex loss: 0.569047  [  165/  300]
per-ex loss: 0.458562  [  168/  300]
per-ex loss: 0.428474  [  171/  300]
per-ex loss: 0.520454  [  174/  300]
per-ex loss: 0.593554  [  177/  300]
per-ex loss: 0.572177  [  180/  300]
per-ex loss: 0.620399  [  183/  300]
per-ex loss: 0.651214  [  186/  300]
per-ex loss: 0.427450  [  189/  300]
per-ex loss: 0.426279  [  192/  300]
per-ex loss: 0.538562  [  195/  300]
per-ex loss: 0.539866  [  198/  300]
per-ex loss: 0.520640  [  201/  300]
per-ex loss: 0.497536  [  204/  300]
per-ex loss: 0.475215  [  207/  300]
per-ex loss: 0.519473  [  210/  300]
per-ex loss: 0.589759  [  213/  300]
per-ex loss: 0.417670  [  216/  300]
per-ex loss: 0.529198  [  219/  300]
per-ex loss: 0.444660  [  222/  300]
per-ex loss: 0.475468  [  225/  300]
per-ex loss: 0.592936  [  228/  300]
per-ex loss: 0.514248  [  231/  300]
per-ex loss: 0.512221  [  234/  300]
per-ex loss: 0.424700  [  237/  300]
per-ex loss: 0.472609  [  240/  300]
per-ex loss: 0.449604  [  243/  300]
per-ex loss: 0.520822  [  246/  300]
per-ex loss: 0.397036  [  249/  300]
per-ex loss: 0.479528  [  252/  300]
per-ex loss: 0.576361  [  255/  300]
per-ex loss: 0.464781  [  258/  300]
per-ex loss: 0.403603  [  261/  300]
per-ex loss: 0.390732  [  264/  300]
per-ex loss: 0.450129  [  267/  300]
per-ex loss: 0.571694  [  270/  300]
per-ex loss: 0.595110  [  273/  300]
per-ex loss: 0.465081  [  276/  300]
per-ex loss: 0.457502  [  279/  300]
per-ex loss: 0.451691  [  282/  300]
per-ex loss: 0.531891  [  285/  300]
per-ex loss: 0.522015  [  288/  300]
per-ex loss: 0.572788  [  291/  300]
per-ex loss: 0.404164  [  294/  300]
per-ex loss: 0.622236  [  297/  300]
per-ex loss: 0.528011  [  300/  300]
Train Error: Avg loss: 0.49739445
validation Error: 
 Avg loss: 0.50019749 
 F1: 0.481061 
 Precision: 0.434916 
 Recall: 0.538161
 IoU: 0.316709

test Error: 
 Avg loss: 0.45001608 
 F1: 0.550648 
 Precision: 0.495045 
 Recall: 0.620322
 IoU: 0.379927

We have finished training iteration 153
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_151_.pth
per-ex loss: 0.411536  [    3/  300]
per-ex loss: 0.586127  [    6/  300]
per-ex loss: 0.565433  [    9/  300]
per-ex loss: 0.561588  [   12/  300]
per-ex loss: 0.539685  [   15/  300]
per-ex loss: 0.396422  [   18/  300]
per-ex loss: 0.446868  [   21/  300]
per-ex loss: 0.467907  [   24/  300]
per-ex loss: 0.544047  [   27/  300]
per-ex loss: 0.548420  [   30/  300]
per-ex loss: 0.588579  [   33/  300]
per-ex loss: 0.410036  [   36/  300]
per-ex loss: 0.410052  [   39/  300]
per-ex loss: 0.422292  [   42/  300]
per-ex loss: 0.564613  [   45/  300]
per-ex loss: 0.555702  [   48/  300]
per-ex loss: 0.474526  [   51/  300]
per-ex loss: 0.501242  [   54/  300]
per-ex loss: 0.645024  [   57/  300]
per-ex loss: 0.594620  [   60/  300]
per-ex loss: 0.497224  [   63/  300]
per-ex loss: 0.481299  [   66/  300]
per-ex loss: 0.452045  [   69/  300]
per-ex loss: 0.468611  [   72/  300]
per-ex loss: 0.373303  [   75/  300]
per-ex loss: 0.417004  [   78/  300]
per-ex loss: 0.434758  [   81/  300]
per-ex loss: 0.489597  [   84/  300]
per-ex loss: 0.507264  [   87/  300]
per-ex loss: 0.487732  [   90/  300]
per-ex loss: 0.436642  [   93/  300]
per-ex loss: 0.576495  [   96/  300]
per-ex loss: 0.645554  [   99/  300]
per-ex loss: 0.483202  [  102/  300]
per-ex loss: 0.407988  [  105/  300]
per-ex loss: 0.487160  [  108/  300]
per-ex loss: 0.498598  [  111/  300]
per-ex loss: 0.675706  [  114/  300]
per-ex loss: 0.453483  [  117/  300]
per-ex loss: 0.449384  [  120/  300]
per-ex loss: 0.468468  [  123/  300]
per-ex loss: 0.521651  [  126/  300]
per-ex loss: 0.554071  [  129/  300]
per-ex loss: 0.539713  [  132/  300]
per-ex loss: 0.572254  [  135/  300]
per-ex loss: 0.585002  [  138/  300]
per-ex loss: 0.384210  [  141/  300]
per-ex loss: 0.538816  [  144/  300]
per-ex loss: 0.558930  [  147/  300]
per-ex loss: 0.422488  [  150/  300]
per-ex loss: 0.468442  [  153/  300]
per-ex loss: 0.457285  [  156/  300]
per-ex loss: 0.479999  [  159/  300]
per-ex loss: 0.500935  [  162/  300]
per-ex loss: 0.412320  [  165/  300]
per-ex loss: 0.511834  [  168/  300]
per-ex loss: 0.520154  [  171/  300]
per-ex loss: 0.426788  [  174/  300]
per-ex loss: 0.425145  [  177/  300]
per-ex loss: 0.462545  [  180/  300]
per-ex loss: 0.565255  [  183/  300]
per-ex loss: 0.559134  [  186/  300]
per-ex loss: 0.532567  [  189/  300]
per-ex loss: 0.513626  [  192/  300]
per-ex loss: 0.491414  [  195/  300]
per-ex loss: 0.491748  [  198/  300]
per-ex loss: 0.423102  [  201/  300]
per-ex loss: 0.568303  [  204/  300]
per-ex loss: 0.482315  [  207/  300]
per-ex loss: 0.454215  [  210/  300]
per-ex loss: 0.430268  [  213/  300]
per-ex loss: 0.535049  [  216/  300]
per-ex loss: 0.392412  [  219/  300]
per-ex loss: 0.556020  [  222/  300]
per-ex loss: 0.467625  [  225/  300]
per-ex loss: 0.649717  [  228/  300]
per-ex loss: 0.457560  [  231/  300]
per-ex loss: 0.457621  [  234/  300]
per-ex loss: 0.523917  [  237/  300]
per-ex loss: 0.459432  [  240/  300]
per-ex loss: 0.488163  [  243/  300]
per-ex loss: 0.416207  [  246/  300]
per-ex loss: 0.550461  [  249/  300]
per-ex loss: 0.444889  [  252/  300]
per-ex loss: 0.591107  [  255/  300]
per-ex loss: 0.488376  [  258/  300]
per-ex loss: 0.375451  [  261/  300]
per-ex loss: 0.643829  [  264/  300]
per-ex loss: 0.421980  [  267/  300]
per-ex loss: 0.439365  [  270/  300]
per-ex loss: 0.382571  [  273/  300]
per-ex loss: 0.585047  [  276/  300]
per-ex loss: 0.429705  [  279/  300]
per-ex loss: 0.471524  [  282/  300]
per-ex loss: 0.489991  [  285/  300]
per-ex loss: 0.486688  [  288/  300]
per-ex loss: 0.585542  [  291/  300]
per-ex loss: 0.459202  [  294/  300]
per-ex loss: 0.487719  [  297/  300]
per-ex loss: 0.528647  [  300/  300]
Train Error: Avg loss: 0.49546581
validation Error: 
 Avg loss: 0.49808770 
 F1: 0.482127 
 Precision: 0.426882 
 Recall: 0.553796
 IoU: 0.317633

test Error: 
 Avg loss: 0.45964187 
 F1: 0.541159 
 Precision: 0.472476 
 Recall: 0.633207
 IoU: 0.370951

We have finished training iteration 154
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_105_.pth
per-ex loss: 0.409425  [    3/  300]
per-ex loss: 0.409618  [    6/  300]
per-ex loss: 0.651491  [    9/  300]
per-ex loss: 0.529206  [   12/  300]
per-ex loss: 0.396496  [   15/  300]
per-ex loss: 0.474005  [   18/  300]
per-ex loss: 0.590057  [   21/  300]
per-ex loss: 0.647403  [   24/  300]
per-ex loss: 0.559185  [   27/  300]
per-ex loss: 0.490437  [   30/  300]
per-ex loss: 0.462157  [   33/  300]
per-ex loss: 0.396715  [   36/  300]
per-ex loss: 0.472793  [   39/  300]
per-ex loss: 0.543921  [   42/  300]
per-ex loss: 0.460103  [   45/  300]
per-ex loss: 0.482880  [   48/  300]
per-ex loss: 0.530795  [   51/  300]
per-ex loss: 0.499580  [   54/  300]
per-ex loss: 0.610877  [   57/  300]
per-ex loss: 0.540452  [   60/  300]
per-ex loss: 0.569932  [   63/  300]
per-ex loss: 0.455405  [   66/  300]
per-ex loss: 0.554773  [   69/  300]
per-ex loss: 0.495279  [   72/  300]
per-ex loss: 0.419239  [   75/  300]
per-ex loss: 0.432536  [   78/  300]
per-ex loss: 0.494435  [   81/  300]
per-ex loss: 0.607099  [   84/  300]
per-ex loss: 0.469475  [   87/  300]
per-ex loss: 0.384563  [   90/  300]
per-ex loss: 0.512525  [   93/  300]
per-ex loss: 0.411751  [   96/  300]
per-ex loss: 0.432004  [   99/  300]
per-ex loss: 0.587541  [  102/  300]
per-ex loss: 0.428198  [  105/  300]
per-ex loss: 0.546529  [  108/  300]
per-ex loss: 0.488816  [  111/  300]
per-ex loss: 0.583404  [  114/  300]
per-ex loss: 0.618596  [  117/  300]
per-ex loss: 0.432105  [  120/  300]
per-ex loss: 0.408179  [  123/  300]
per-ex loss: 0.499053  [  126/  300]
per-ex loss: 0.435392  [  129/  300]
per-ex loss: 0.448575  [  132/  300]
per-ex loss: 0.473365  [  135/  300]
per-ex loss: 0.388857  [  138/  300]
per-ex loss: 0.503904  [  141/  300]
per-ex loss: 0.703745  [  144/  300]
per-ex loss: 0.414984  [  147/  300]
per-ex loss: 0.474644  [  150/  300]
per-ex loss: 0.473487  [  153/  300]
per-ex loss: 0.597499  [  156/  300]
per-ex loss: 0.533983  [  159/  300]
per-ex loss: 0.514299  [  162/  300]
per-ex loss: 0.474368  [  165/  300]
per-ex loss: 0.470457  [  168/  300]
per-ex loss: 0.527208  [  171/  300]
per-ex loss: 0.506305  [  174/  300]
per-ex loss: 0.469754  [  177/  300]
per-ex loss: 0.466332  [  180/  300]
per-ex loss: 0.467811  [  183/  300]
per-ex loss: 0.551758  [  186/  300]
per-ex loss: 0.502574  [  189/  300]
per-ex loss: 0.413390  [  192/  300]
per-ex loss: 0.478269  [  195/  300]
per-ex loss: 0.433974  [  198/  300]
per-ex loss: 0.418919  [  201/  300]
per-ex loss: 0.422267  [  204/  300]
per-ex loss: 0.519845  [  207/  300]
per-ex loss: 0.373742  [  210/  300]
per-ex loss: 0.463323  [  213/  300]
per-ex loss: 0.538500  [  216/  300]
per-ex loss: 0.557057  [  219/  300]
per-ex loss: 0.640277  [  222/  300]
per-ex loss: 0.627033  [  225/  300]
per-ex loss: 0.445617  [  228/  300]
per-ex loss: 0.431478  [  231/  300]
per-ex loss: 0.456408  [  234/  300]
per-ex loss: 0.516118  [  237/  300]
per-ex loss: 0.517309  [  240/  300]
per-ex loss: 0.601771  [  243/  300]
per-ex loss: 0.389897  [  246/  300]
per-ex loss: 0.545350  [  249/  300]
per-ex loss: 0.624790  [  252/  300]
per-ex loss: 0.628951  [  255/  300]
per-ex loss: 0.477277  [  258/  300]
per-ex loss: 0.563197  [  261/  300]
per-ex loss: 0.465503  [  264/  300]
per-ex loss: 0.431478  [  267/  300]
per-ex loss: 0.576529  [  270/  300]
per-ex loss: 0.553446  [  273/  300]
per-ex loss: 0.522886  [  276/  300]
per-ex loss: 0.429394  [  279/  300]
per-ex loss: 0.673374  [  282/  300]
per-ex loss: 0.428160  [  285/  300]
per-ex loss: 0.578679  [  288/  300]
per-ex loss: 0.399584  [  291/  300]
per-ex loss: 0.537350  [  294/  300]
per-ex loss: 0.515249  [  297/  300]
per-ex loss: 0.555541  [  300/  300]
Train Error: Avg loss: 0.50140260
validation Error: 
 Avg loss: 0.50033126 
 F1: 0.485509 
 Precision: 0.451151 
 Recall: 0.525532
 IoU: 0.320576

test Error: 
 Avg loss: 0.44834805 
 F1: 0.552321 
 Precision: 0.515790 
 Recall: 0.594422
 IoU: 0.381522

We have finished training iteration 155
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_153_.pth
per-ex loss: 0.435891  [    3/  300]
per-ex loss: 0.533028  [    6/  300]
per-ex loss: 0.590954  [    9/  300]
per-ex loss: 0.470448  [   12/  300]
per-ex loss: 0.442203  [   15/  300]
per-ex loss: 0.545802  [   18/  300]
per-ex loss: 0.445789  [   21/  300]
per-ex loss: 0.514390  [   24/  300]
per-ex loss: 0.449831  [   27/  300]
per-ex loss: 0.495848  [   30/  300]
per-ex loss: 0.507577  [   33/  300]
per-ex loss: 0.499113  [   36/  300]
per-ex loss: 0.692000  [   39/  300]
per-ex loss: 0.558811  [   42/  300]
per-ex loss: 0.517498  [   45/  300]
per-ex loss: 0.655818  [   48/  300]
per-ex loss: 0.366906  [   51/  300]
per-ex loss: 0.594309  [   54/  300]
per-ex loss: 0.560521  [   57/  300]
per-ex loss: 0.451121  [   60/  300]
per-ex loss: 0.524960  [   63/  300]
per-ex loss: 0.494307  [   66/  300]
per-ex loss: 0.483130  [   69/  300]
per-ex loss: 0.464480  [   72/  300]
per-ex loss: 0.424588  [   75/  300]
per-ex loss: 0.503739  [   78/  300]
per-ex loss: 0.486686  [   81/  300]
per-ex loss: 0.569596  [   84/  300]
per-ex loss: 0.451401  [   87/  300]
per-ex loss: 0.406774  [   90/  300]
per-ex loss: 0.522918  [   93/  300]
per-ex loss: 0.537297  [   96/  300]
per-ex loss: 0.584999  [   99/  300]
per-ex loss: 0.468596  [  102/  300]
per-ex loss: 0.563206  [  105/  300]
per-ex loss: 0.470637  [  108/  300]
per-ex loss: 0.400564  [  111/  300]
per-ex loss: 0.407954  [  114/  300]
per-ex loss: 0.567183  [  117/  300]
per-ex loss: 0.451452  [  120/  300]
per-ex loss: 0.606695  [  123/  300]
per-ex loss: 0.542562  [  126/  300]
per-ex loss: 0.461389  [  129/  300]
per-ex loss: 0.606768  [  132/  300]
per-ex loss: 0.410796  [  135/  300]
per-ex loss: 0.598103  [  138/  300]
per-ex loss: 0.430271  [  141/  300]
per-ex loss: 0.472614  [  144/  300]
per-ex loss: 0.531647  [  147/  300]
per-ex loss: 0.378775  [  150/  300]
per-ex loss: 0.458571  [  153/  300]
per-ex loss: 0.403550  [  156/  300]
per-ex loss: 0.493349  [  159/  300]
per-ex loss: 0.514190  [  162/  300]
per-ex loss: 0.489081  [  165/  300]
per-ex loss: 0.457303  [  168/  300]
per-ex loss: 0.504235  [  171/  300]
per-ex loss: 0.563550  [  174/  300]
per-ex loss: 0.424700  [  177/  300]
per-ex loss: 0.626192  [  180/  300]
per-ex loss: 0.435913  [  183/  300]
per-ex loss: 0.488799  [  186/  300]
per-ex loss: 0.453074  [  189/  300]
per-ex loss: 0.494747  [  192/  300]
per-ex loss: 0.488828  [  195/  300]
per-ex loss: 0.492205  [  198/  300]
per-ex loss: 0.490603  [  201/  300]
per-ex loss: 0.497406  [  204/  300]
per-ex loss: 0.435061  [  207/  300]
per-ex loss: 0.455927  [  210/  300]
per-ex loss: 0.523041  [  213/  300]
per-ex loss: 0.649097  [  216/  300]
per-ex loss: 0.580696  [  219/  300]
per-ex loss: 0.497215  [  222/  300]
per-ex loss: 0.431221  [  225/  300]
per-ex loss: 0.444080  [  228/  300]
per-ex loss: 0.421209  [  231/  300]
per-ex loss: 0.572153  [  234/  300]
per-ex loss: 0.483136  [  237/  300]
per-ex loss: 0.593787  [  240/  300]
per-ex loss: 0.509684  [  243/  300]
per-ex loss: 0.443135  [  246/  300]
per-ex loss: 0.445711  [  249/  300]
per-ex loss: 0.462999  [  252/  300]
per-ex loss: 0.443840  [  255/  300]
per-ex loss: 0.454141  [  258/  300]
per-ex loss: 0.537590  [  261/  300]
per-ex loss: 0.605262  [  264/  300]
per-ex loss: 0.448236  [  267/  300]
per-ex loss: 0.571540  [  270/  300]
per-ex loss: 0.442251  [  273/  300]
per-ex loss: 0.403739  [  276/  300]
per-ex loss: 0.434916  [  279/  300]
per-ex loss: 0.685442  [  282/  300]
per-ex loss: 0.426855  [  285/  300]
per-ex loss: 0.450365  [  288/  300]
per-ex loss: 0.596546  [  291/  300]
per-ex loss: 0.588337  [  294/  300]
per-ex loss: 0.434605  [  297/  300]
per-ex loss: 0.484579  [  300/  300]
Train Error: Avg loss: 0.49884634
validation Error: 
 Avg loss: 0.49969534 
 F1: 0.484288 
 Precision: 0.446617 
 Recall: 0.528899
 IoU: 0.319512

test Error: 
 Avg loss: 0.44885737 
 F1: 0.552100 
 Precision: 0.507582 
 Recall: 0.605177
 IoU: 0.381311

We have finished training iteration 156
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_154_.pth
per-ex loss: 0.583589  [    3/  300]
per-ex loss: 0.605194  [    6/  300]
per-ex loss: 0.462388  [    9/  300]
per-ex loss: 0.508004  [   12/  300]
per-ex loss: 0.486639  [   15/  300]
per-ex loss: 0.489369  [   18/  300]
per-ex loss: 0.444084  [   21/  300]
per-ex loss: 0.414931  [   24/  300]
per-ex loss: 0.385037  [   27/  300]
per-ex loss: 0.548574  [   30/  300]
per-ex loss: 0.421459  [   33/  300]
per-ex loss: 0.543920  [   36/  300]
per-ex loss: 0.416691  [   39/  300]
per-ex loss: 0.513450  [   42/  300]
per-ex loss: 0.522571  [   45/  300]
per-ex loss: 0.457085  [   48/  300]
per-ex loss: 0.530859  [   51/  300]
per-ex loss: 0.606848  [   54/  300]
per-ex loss: 0.390039  [   57/  300]
per-ex loss: 0.503325  [   60/  300]
per-ex loss: 0.641737  [   63/  300]
per-ex loss: 0.441317  [   66/  300]
per-ex loss: 0.467662  [   69/  300]
per-ex loss: 0.440025  [   72/  300]
per-ex loss: 0.493176  [   75/  300]
per-ex loss: 0.438082  [   78/  300]
per-ex loss: 0.512001  [   81/  300]
per-ex loss: 0.486769  [   84/  300]
per-ex loss: 0.516601  [   87/  300]
per-ex loss: 0.392457  [   90/  300]
per-ex loss: 0.544518  [   93/  300]
per-ex loss: 0.460853  [   96/  300]
per-ex loss: 0.421706  [   99/  300]
per-ex loss: 0.501874  [  102/  300]
per-ex loss: 0.476075  [  105/  300]
per-ex loss: 0.586090  [  108/  300]
per-ex loss: 0.411168  [  111/  300]
per-ex loss: 0.529096  [  114/  300]
per-ex loss: 0.526483  [  117/  300]
per-ex loss: 0.463617  [  120/  300]
per-ex loss: 0.442064  [  123/  300]
per-ex loss: 0.393173  [  126/  300]
per-ex loss: 0.441573  [  129/  300]
per-ex loss: 0.445593  [  132/  300]
per-ex loss: 0.474496  [  135/  300]
per-ex loss: 0.607159  [  138/  300]
per-ex loss: 0.652002  [  141/  300]
per-ex loss: 0.449421  [  144/  300]
per-ex loss: 0.485380  [  147/  300]
per-ex loss: 0.451492  [  150/  300]
per-ex loss: 0.512669  [  153/  300]
per-ex loss: 0.450891  [  156/  300]
per-ex loss: 0.552351  [  159/  300]
per-ex loss: 0.476845  [  162/  300]
per-ex loss: 0.550978  [  165/  300]
per-ex loss: 0.509627  [  168/  300]
per-ex loss: 0.466255  [  171/  300]
per-ex loss: 0.482182  [  174/  300]
per-ex loss: 0.415727  [  177/  300]
per-ex loss: 0.477338  [  180/  300]
per-ex loss: 0.558124  [  183/  300]
per-ex loss: 0.513046  [  186/  300]
per-ex loss: 0.408776  [  189/  300]
per-ex loss: 0.481082  [  192/  300]
per-ex loss: 0.500062  [  195/  300]
per-ex loss: 0.604901  [  198/  300]
per-ex loss: 0.602546  [  201/  300]
per-ex loss: 0.660338  [  204/  300]
per-ex loss: 0.484964  [  207/  300]
per-ex loss: 0.488642  [  210/  300]
per-ex loss: 0.646622  [  213/  300]
per-ex loss: 0.466804  [  216/  300]
per-ex loss: 0.550864  [  219/  300]
per-ex loss: 0.496977  [  222/  300]
per-ex loss: 0.607074  [  225/  300]
per-ex loss: 0.507979  [  228/  300]
per-ex loss: 0.462603  [  231/  300]
per-ex loss: 0.499312  [  234/  300]
per-ex loss: 0.474141  [  237/  300]
per-ex loss: 0.501448  [  240/  300]
per-ex loss: 0.553840  [  243/  300]
per-ex loss: 0.452888  [  246/  300]
per-ex loss: 0.418013  [  249/  300]
per-ex loss: 0.383687  [  252/  300]
per-ex loss: 0.475426  [  255/  300]
per-ex loss: 0.420815  [  258/  300]
per-ex loss: 0.400015  [  261/  300]
per-ex loss: 0.430515  [  264/  300]
per-ex loss: 0.559272  [  267/  300]
per-ex loss: 0.490849  [  270/  300]
per-ex loss: 0.437103  [  273/  300]
per-ex loss: 0.486242  [  276/  300]
per-ex loss: 0.564011  [  279/  300]
per-ex loss: 0.499942  [  282/  300]
per-ex loss: 0.464927  [  285/  300]
per-ex loss: 0.431795  [  288/  300]
per-ex loss: 0.387782  [  291/  300]
per-ex loss: 0.603161  [  294/  300]
per-ex loss: 0.592858  [  297/  300]
per-ex loss: 0.450889  [  300/  300]
Train Error: Avg loss: 0.49342914
validation Error: 
 Avg loss: 0.49637254 
 F1: 0.481956 
 Precision: 0.409998 
 Recall: 0.584548
 IoU: 0.317485

test Error: 
 Avg loss: 0.45735282 
 F1: 0.543394 
 Precision: 0.460821 
 Recall: 0.662019
 IoU: 0.373055

We have finished training iteration 157
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_155_.pth
per-ex loss: 0.437551  [    3/  300]
per-ex loss: 0.501044  [    6/  300]
per-ex loss: 0.528132  [    9/  300]
per-ex loss: 0.550966  [   12/  300]
per-ex loss: 0.478461  [   15/  300]
per-ex loss: 0.452630  [   18/  300]
per-ex loss: 0.408104  [   21/  300]
per-ex loss: 0.437617  [   24/  300]
per-ex loss: 0.591536  [   27/  300]
per-ex loss: 0.483152  [   30/  300]
per-ex loss: 0.580679  [   33/  300]
per-ex loss: 0.599283  [   36/  300]
per-ex loss: 0.460669  [   39/  300]
per-ex loss: 0.525266  [   42/  300]
per-ex loss: 0.436474  [   45/  300]
per-ex loss: 0.511509  [   48/  300]
per-ex loss: 0.395538  [   51/  300]
per-ex loss: 0.485980  [   54/  300]
per-ex loss: 0.417144  [   57/  300]
per-ex loss: 0.416720  [   60/  300]
per-ex loss: 0.485537  [   63/  300]
per-ex loss: 0.482191  [   66/  300]
per-ex loss: 0.561548  [   69/  300]
per-ex loss: 0.687397  [   72/  300]
per-ex loss: 0.423745  [   75/  300]
per-ex loss: 0.432074  [   78/  300]
per-ex loss: 0.528753  [   81/  300]
per-ex loss: 0.513160  [   84/  300]
per-ex loss: 0.517484  [   87/  300]
per-ex loss: 0.605793  [   90/  300]
per-ex loss: 0.419178  [   93/  300]
per-ex loss: 0.422029  [   96/  300]
per-ex loss: 0.374602  [   99/  300]
per-ex loss: 0.476620  [  102/  300]
per-ex loss: 0.525566  [  105/  300]
per-ex loss: 0.496312  [  108/  300]
per-ex loss: 0.445032  [  111/  300]
per-ex loss: 0.568274  [  114/  300]
per-ex loss: 0.411048  [  117/  300]
per-ex loss: 0.460707  [  120/  300]
per-ex loss: 0.592965  [  123/  300]
per-ex loss: 0.511576  [  126/  300]
per-ex loss: 0.440532  [  129/  300]
per-ex loss: 0.627810  [  132/  300]
per-ex loss: 0.483235  [  135/  300]
per-ex loss: 0.565624  [  138/  300]
per-ex loss: 0.516500  [  141/  300]
per-ex loss: 0.501107  [  144/  300]
per-ex loss: 0.584421  [  147/  300]
per-ex loss: 0.432983  [  150/  300]
per-ex loss: 0.473039  [  153/  300]
per-ex loss: 0.498803  [  156/  300]
per-ex loss: 0.490362  [  159/  300]
per-ex loss: 0.597956  [  162/  300]
per-ex loss: 0.444374  [  165/  300]
per-ex loss: 0.492260  [  168/  300]
per-ex loss: 0.480789  [  171/  300]
per-ex loss: 0.565423  [  174/  300]
per-ex loss: 0.500942  [  177/  300]
per-ex loss: 0.423849  [  180/  300]
per-ex loss: 0.442244  [  183/  300]
per-ex loss: 0.450986  [  186/  300]
per-ex loss: 0.508646  [  189/  300]
per-ex loss: 0.423256  [  192/  300]
per-ex loss: 0.558746  [  195/  300]
per-ex loss: 0.410822  [  198/  300]
per-ex loss: 0.475550  [  201/  300]
per-ex loss: 0.461414  [  204/  300]
per-ex loss: 0.445325  [  207/  300]
per-ex loss: 0.552872  [  210/  300]
per-ex loss: 0.431127  [  213/  300]
per-ex loss: 0.501832  [  216/  300]
per-ex loss: 0.461020  [  219/  300]
per-ex loss: 0.500395  [  222/  300]
per-ex loss: 0.597605  [  225/  300]
per-ex loss: 0.576717  [  228/  300]
per-ex loss: 0.542720  [  231/  300]
per-ex loss: 0.397428  [  234/  300]
per-ex loss: 0.558726  [  237/  300]
per-ex loss: 0.479627  [  240/  300]
per-ex loss: 0.532524  [  243/  300]
per-ex loss: 0.419743  [  246/  300]
per-ex loss: 0.499126  [  249/  300]
per-ex loss: 0.670190  [  252/  300]
per-ex loss: 0.488683  [  255/  300]
per-ex loss: 0.572064  [  258/  300]
per-ex loss: 0.530216  [  261/  300]
per-ex loss: 0.656105  [  264/  300]
per-ex loss: 0.582608  [  267/  300]
per-ex loss: 0.543272  [  270/  300]
per-ex loss: 0.520608  [  273/  300]
per-ex loss: 0.556589  [  276/  300]
per-ex loss: 0.517525  [  279/  300]
per-ex loss: 0.431585  [  282/  300]
per-ex loss: 0.559024  [  285/  300]
per-ex loss: 0.517067  [  288/  300]
per-ex loss: 0.429374  [  291/  300]
per-ex loss: 0.603611  [  294/  300]
per-ex loss: 0.452320  [  297/  300]
per-ex loss: 0.459712  [  300/  300]
Train Error: Avg loss: 0.50079055
validation Error: 
 Avg loss: 0.51954822 
 F1: 0.468991 
 Precision: 0.449318 
 Recall: 0.490466
 IoU: 0.306328

test Error: 
 Avg loss: 0.44908208 
 F1: 0.551629 
 Precision: 0.510621 
 Recall: 0.599800
 IoU: 0.380862

We have finished training iteration 158
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_156_.pth
per-ex loss: 0.473299  [    3/  300]
per-ex loss: 0.538486  [    6/  300]
per-ex loss: 0.486780  [    9/  300]
per-ex loss: 0.463638  [   12/  300]
per-ex loss: 0.518005  [   15/  300]
per-ex loss: 0.453068  [   18/  300]
per-ex loss: 0.445928  [   21/  300]
per-ex loss: 0.428592  [   24/  300]
per-ex loss: 0.470482  [   27/  300]
per-ex loss: 0.542432  [   30/  300]
per-ex loss: 0.519798  [   33/  300]
per-ex loss: 0.541718  [   36/  300]
per-ex loss: 0.575388  [   39/  300]
per-ex loss: 0.422171  [   42/  300]
per-ex loss: 0.420881  [   45/  300]
per-ex loss: 0.511125  [   48/  300]
per-ex loss: 0.502967  [   51/  300]
per-ex loss: 0.441531  [   54/  300]
per-ex loss: 0.591998  [   57/  300]
per-ex loss: 0.588661  [   60/  300]
per-ex loss: 0.618483  [   63/  300]
per-ex loss: 0.491506  [   66/  300]
per-ex loss: 0.575259  [   69/  300]
per-ex loss: 0.488340  [   72/  300]
per-ex loss: 0.433129  [   75/  300]
per-ex loss: 0.461119  [   78/  300]
per-ex loss: 0.683620  [   81/  300]
per-ex loss: 0.409516  [   84/  300]
per-ex loss: 0.498435  [   87/  300]
per-ex loss: 0.518409  [   90/  300]
per-ex loss: 0.455740  [   93/  300]
per-ex loss: 0.591306  [   96/  300]
per-ex loss: 0.442551  [   99/  300]
per-ex loss: 0.407023  [  102/  300]
per-ex loss: 0.533624  [  105/  300]
per-ex loss: 0.420000  [  108/  300]
per-ex loss: 0.530256  [  111/  300]
per-ex loss: 0.578907  [  114/  300]
per-ex loss: 0.513982  [  117/  300]
per-ex loss: 0.463220  [  120/  300]
per-ex loss: 0.600328  [  123/  300]
per-ex loss: 0.441936  [  126/  300]
per-ex loss: 0.520902  [  129/  300]
per-ex loss: 0.438549  [  132/  300]
per-ex loss: 0.467508  [  135/  300]
per-ex loss: 0.518799  [  138/  300]
per-ex loss: 0.489046  [  141/  300]
per-ex loss: 0.559097  [  144/  300]
per-ex loss: 0.504081  [  147/  300]
per-ex loss: 0.506280  [  150/  300]
per-ex loss: 0.532000  [  153/  300]
per-ex loss: 0.475652  [  156/  300]
per-ex loss: 0.498928  [  159/  300]
per-ex loss: 0.431784  [  162/  300]
per-ex loss: 0.410694  [  165/  300]
per-ex loss: 0.449925  [  168/  300]
per-ex loss: 0.475052  [  171/  300]
per-ex loss: 0.508704  [  174/  300]
per-ex loss: 0.486592  [  177/  300]
per-ex loss: 0.464049  [  180/  300]
per-ex loss: 0.503698  [  183/  300]
per-ex loss: 0.501987  [  186/  300]
per-ex loss: 0.614906  [  189/  300]
per-ex loss: 0.416183  [  192/  300]
per-ex loss: 0.585425  [  195/  300]
per-ex loss: 0.510530  [  198/  300]
per-ex loss: 0.512371  [  201/  300]
per-ex loss: 0.451513  [  204/  300]
per-ex loss: 0.422686  [  207/  300]
per-ex loss: 0.543585  [  210/  300]
per-ex loss: 0.390982  [  213/  300]
per-ex loss: 0.546970  [  216/  300]
per-ex loss: 0.516948  [  219/  300]
per-ex loss: 0.529264  [  222/  300]
per-ex loss: 0.454984  [  225/  300]
per-ex loss: 0.472251  [  228/  300]
per-ex loss: 0.426482  [  231/  300]
per-ex loss: 0.570420  [  234/  300]
per-ex loss: 0.496845  [  237/  300]
per-ex loss: 0.469870  [  240/  300]
per-ex loss: 0.640145  [  243/  300]
per-ex loss: 0.588173  [  246/  300]
per-ex loss: 0.423141  [  249/  300]
per-ex loss: 0.445246  [  252/  300]
per-ex loss: 0.428714  [  255/  300]
per-ex loss: 0.518408  [  258/  300]
per-ex loss: 0.481457  [  261/  300]
per-ex loss: 0.476410  [  264/  300]
per-ex loss: 0.488905  [  267/  300]
per-ex loss: 0.395706  [  270/  300]
per-ex loss: 0.464475  [  273/  300]
per-ex loss: 0.468314  [  276/  300]
per-ex loss: 0.470250  [  279/  300]
per-ex loss: 0.437905  [  282/  300]
per-ex loss: 0.433844  [  285/  300]
per-ex loss: 0.440202  [  288/  300]
per-ex loss: 0.495907  [  291/  300]
per-ex loss: 0.554019  [  294/  300]
per-ex loss: 0.444794  [  297/  300]
per-ex loss: 0.496233  [  300/  300]
Train Error: Avg loss: 0.49361430
validation Error: 
 Avg loss: 0.49656755 
 F1: 0.484969 
 Precision: 0.421573 
 Recall: 0.570807
 IoU: 0.320105

test Error: 
 Avg loss: 0.46145743 
 F1: 0.539590 
 Precision: 0.471394 
 Recall: 0.630855
 IoU: 0.369479

We have finished training iteration 159
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_157_.pth
per-ex loss: 0.537969  [    3/  300]
per-ex loss: 0.482550  [    6/  300]
per-ex loss: 0.565395  [    9/  300]
per-ex loss: 0.504318  [   12/  300]
per-ex loss: 0.464393  [   15/  300]
per-ex loss: 0.517495  [   18/  300]
per-ex loss: 0.507382  [   21/  300]
per-ex loss: 0.525236  [   24/  300]
per-ex loss: 0.531291  [   27/  300]
per-ex loss: 0.426074  [   30/  300]
per-ex loss: 0.566163  [   33/  300]
per-ex loss: 0.518736  [   36/  300]
per-ex loss: 0.497176  [   39/  300]
per-ex loss: 0.494873  [   42/  300]
per-ex loss: 0.503139  [   45/  300]
per-ex loss: 0.495873  [   48/  300]
per-ex loss: 0.513371  [   51/  300]
per-ex loss: 0.436046  [   54/  300]
per-ex loss: 0.419224  [   57/  300]
per-ex loss: 0.432866  [   60/  300]
per-ex loss: 0.514400  [   63/  300]
per-ex loss: 0.384068  [   66/  300]
per-ex loss: 0.464079  [   69/  300]
per-ex loss: 0.423331  [   72/  300]
per-ex loss: 0.577073  [   75/  300]
per-ex loss: 0.501881  [   78/  300]
per-ex loss: 0.472757  [   81/  300]
per-ex loss: 0.503670  [   84/  300]
per-ex loss: 0.399541  [   87/  300]
per-ex loss: 0.588479  [   90/  300]
per-ex loss: 0.494464  [   93/  300]
per-ex loss: 0.560807  [   96/  300]
per-ex loss: 0.541987  [   99/  300]
per-ex loss: 0.434508  [  102/  300]
per-ex loss: 0.437214  [  105/  300]
per-ex loss: 0.555286  [  108/  300]
per-ex loss: 0.602023  [  111/  300]
per-ex loss: 0.462349  [  114/  300]
per-ex loss: 0.536468  [  117/  300]
per-ex loss: 0.406877  [  120/  300]
per-ex loss: 0.679711  [  123/  300]
per-ex loss: 0.440353  [  126/  300]
per-ex loss: 0.444970  [  129/  300]
per-ex loss: 0.427107  [  132/  300]
per-ex loss: 0.593615  [  135/  300]
per-ex loss: 0.444881  [  138/  300]
per-ex loss: 0.583219  [  141/  300]
per-ex loss: 0.632703  [  144/  300]
per-ex loss: 0.452101  [  147/  300]
per-ex loss: 0.480280  [  150/  300]
per-ex loss: 0.475372  [  153/  300]
per-ex loss: 0.458393  [  156/  300]
per-ex loss: 0.400294  [  159/  300]
per-ex loss: 0.513555  [  162/  300]
per-ex loss: 0.539408  [  165/  300]
per-ex loss: 0.438189  [  168/  300]
per-ex loss: 0.501740  [  171/  300]
per-ex loss: 0.553081  [  174/  300]
per-ex loss: 0.519916  [  177/  300]
per-ex loss: 0.568661  [  180/  300]
per-ex loss: 0.469625  [  183/  300]
per-ex loss: 0.566976  [  186/  300]
per-ex loss: 0.467152  [  189/  300]
per-ex loss: 0.531196  [  192/  300]
per-ex loss: 0.516348  [  195/  300]
per-ex loss: 0.536657  [  198/  300]
per-ex loss: 0.535640  [  201/  300]
per-ex loss: 0.481592  [  204/  300]
per-ex loss: 0.430945  [  207/  300]
per-ex loss: 0.463395  [  210/  300]
per-ex loss: 0.503604  [  213/  300]
per-ex loss: 0.462863  [  216/  300]
per-ex loss: 0.473833  [  219/  300]
per-ex loss: 0.604103  [  222/  300]
per-ex loss: 0.460831  [  225/  300]
per-ex loss: 0.511149  [  228/  300]
per-ex loss: 0.443077  [  231/  300]
per-ex loss: 0.406633  [  234/  300]
per-ex loss: 0.470303  [  237/  300]
per-ex loss: 0.422676  [  240/  300]
per-ex loss: 0.559036  [  243/  300]
per-ex loss: 0.539270  [  246/  300]
per-ex loss: 0.379907  [  249/  300]
per-ex loss: 0.673960  [  252/  300]
per-ex loss: 0.529457  [  255/  300]
per-ex loss: 0.523126  [  258/  300]
per-ex loss: 0.414912  [  261/  300]
per-ex loss: 0.540071  [  264/  300]
per-ex loss: 0.512058  [  267/  300]
per-ex loss: 0.467662  [  270/  300]
per-ex loss: 0.434443  [  273/  300]
per-ex loss: 0.456289  [  276/  300]
per-ex loss: 0.496832  [  279/  300]
per-ex loss: 0.357059  [  282/  300]
per-ex loss: 0.455751  [  285/  300]
per-ex loss: 0.552941  [  288/  300]
per-ex loss: 0.583240  [  291/  300]
per-ex loss: 0.609233  [  294/  300]
per-ex loss: 0.596099  [  297/  300]
per-ex loss: 0.413206  [  300/  300]
Train Error: Avg loss: 0.49801535
validation Error: 
 Avg loss: 0.50713056 
 F1: 0.479404 
 Precision: 0.449096 
 Recall: 0.514097
 IoU: 0.315273

test Error: 
 Avg loss: 0.44436169 
 F1: 0.556228 
 Precision: 0.514994 
 Recall: 0.604640
 IoU: 0.385260

We have finished training iteration 160
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_158_.pth
per-ex loss: 0.554365  [    3/  300]
per-ex loss: 0.461531  [    6/  300]
per-ex loss: 0.443239  [    9/  300]
per-ex loss: 0.514214  [   12/  300]
per-ex loss: 0.606712  [   15/  300]
per-ex loss: 0.434295  [   18/  300]
per-ex loss: 0.499485  [   21/  300]
per-ex loss: 0.457104  [   24/  300]
per-ex loss: 0.499457  [   27/  300]
per-ex loss: 0.479114  [   30/  300]
per-ex loss: 0.496260  [   33/  300]
per-ex loss: 0.426249  [   36/  300]
per-ex loss: 0.489881  [   39/  300]
per-ex loss: 0.407588  [   42/  300]
per-ex loss: 0.455695  [   45/  300]
per-ex loss: 0.460130  [   48/  300]
per-ex loss: 0.508826  [   51/  300]
per-ex loss: 0.430492  [   54/  300]
per-ex loss: 0.505307  [   57/  300]
per-ex loss: 0.578223  [   60/  300]
per-ex loss: 0.410893  [   63/  300]
per-ex loss: 0.485625  [   66/  300]
per-ex loss: 0.537135  [   69/  300]
per-ex loss: 0.576380  [   72/  300]
per-ex loss: 0.558261  [   75/  300]
per-ex loss: 0.526384  [   78/  300]
per-ex loss: 0.562789  [   81/  300]
per-ex loss: 0.406973  [   84/  300]
per-ex loss: 0.490583  [   87/  300]
per-ex loss: 0.379995  [   90/  300]
per-ex loss: 0.450374  [   93/  300]
per-ex loss: 0.498744  [   96/  300]
per-ex loss: 0.510970  [   99/  300]
per-ex loss: 0.648226  [  102/  300]
per-ex loss: 0.428713  [  105/  300]
per-ex loss: 0.530571  [  108/  300]
per-ex loss: 0.512272  [  111/  300]
per-ex loss: 0.447451  [  114/  300]
per-ex loss: 0.555354  [  117/  300]
per-ex loss: 0.528552  [  120/  300]
per-ex loss: 0.399061  [  123/  300]
per-ex loss: 0.444855  [  126/  300]
per-ex loss: 0.453771  [  129/  300]
per-ex loss: 0.469238  [  132/  300]
per-ex loss: 0.401444  [  135/  300]
per-ex loss: 0.458995  [  138/  300]
per-ex loss: 0.576198  [  141/  300]
per-ex loss: 0.532419  [  144/  300]
per-ex loss: 0.537218  [  147/  300]
per-ex loss: 0.486341  [  150/  300]
per-ex loss: 0.403499  [  153/  300]
per-ex loss: 0.511628  [  156/  300]
per-ex loss: 0.531431  [  159/  300]
per-ex loss: 0.479790  [  162/  300]
per-ex loss: 0.586825  [  165/  300]
per-ex loss: 0.464537  [  168/  300]
per-ex loss: 0.476940  [  171/  300]
per-ex loss: 0.502579  [  174/  300]
per-ex loss: 0.512094  [  177/  300]
per-ex loss: 0.502143  [  180/  300]
per-ex loss: 0.398837  [  183/  300]
per-ex loss: 0.429765  [  186/  300]
per-ex loss: 0.517288  [  189/  300]
per-ex loss: 0.459866  [  192/  300]
per-ex loss: 0.534218  [  195/  300]
per-ex loss: 0.383380  [  198/  300]
per-ex loss: 0.428161  [  201/  300]
per-ex loss: 0.433204  [  204/  300]
per-ex loss: 0.457567  [  207/  300]
per-ex loss: 0.580394  [  210/  300]
per-ex loss: 0.413896  [  213/  300]
per-ex loss: 0.564433  [  216/  300]
per-ex loss: 0.413944  [  219/  300]
per-ex loss: 0.425237  [  222/  300]
per-ex loss: 0.512038  [  225/  300]
per-ex loss: 0.521109  [  228/  300]
per-ex loss: 0.427629  [  231/  300]
per-ex loss: 0.614990  [  234/  300]
per-ex loss: 0.519437  [  237/  300]
per-ex loss: 0.423118  [  240/  300]
per-ex loss: 0.491414  [  243/  300]
per-ex loss: 0.456398  [  246/  300]
per-ex loss: 0.436062  [  249/  300]
per-ex loss: 0.444286  [  252/  300]
per-ex loss: 0.493119  [  255/  300]
per-ex loss: 0.556212  [  258/  300]
per-ex loss: 0.565908  [  261/  300]
per-ex loss: 0.609576  [  264/  300]
per-ex loss: 0.430357  [  267/  300]
per-ex loss: 0.455511  [  270/  300]
per-ex loss: 0.479109  [  273/  300]
per-ex loss: 0.466424  [  276/  300]
per-ex loss: 0.543956  [  279/  300]
per-ex loss: 0.453420  [  282/  300]
per-ex loss: 0.525431  [  285/  300]
per-ex loss: 0.428244  [  288/  300]
per-ex loss: 0.382325  [  291/  300]
per-ex loss: 0.550381  [  294/  300]
per-ex loss: 0.487077  [  297/  300]
per-ex loss: 0.464150  [  300/  300]
Train Error: Avg loss: 0.48663286
validation Error: 
 Avg loss: 0.50826263 
 F1: 0.473654 
 Precision: 0.431660 
 Recall: 0.524699
 IoU: 0.310319

test Error: 
 Avg loss: 0.45083928 
 F1: 0.549949 
 Precision: 0.488367 
 Recall: 0.629303
 IoU: 0.379262

We have finished training iteration 161
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_159_.pth
per-ex loss: 0.456481  [    3/  300]
per-ex loss: 0.608944  [    6/  300]
per-ex loss: 0.433793  [    9/  300]
per-ex loss: 0.514899  [   12/  300]
per-ex loss: 0.521496  [   15/  300]
per-ex loss: 0.537245  [   18/  300]
per-ex loss: 0.441584  [   21/  300]
per-ex loss: 0.613948  [   24/  300]
per-ex loss: 0.487476  [   27/  300]
per-ex loss: 0.510179  [   30/  300]
per-ex loss: 0.565162  [   33/  300]
per-ex loss: 0.421739  [   36/  300]
per-ex loss: 0.583788  [   39/  300]
per-ex loss: 0.449648  [   42/  300]
per-ex loss: 0.418491  [   45/  300]
per-ex loss: 0.403910  [   48/  300]
per-ex loss: 0.552155  [   51/  300]
per-ex loss: 0.431360  [   54/  300]
per-ex loss: 0.393738  [   57/  300]
per-ex loss: 0.498464  [   60/  300]
per-ex loss: 0.567794  [   63/  300]
per-ex loss: 0.428995  [   66/  300]
per-ex loss: 0.557263  [   69/  300]
per-ex loss: 0.458399  [   72/  300]
per-ex loss: 0.418244  [   75/  300]
per-ex loss: 0.530547  [   78/  300]
per-ex loss: 0.449586  [   81/  300]
per-ex loss: 0.618674  [   84/  300]
per-ex loss: 0.502527  [   87/  300]
per-ex loss: 0.580747  [   90/  300]
per-ex loss: 0.509287  [   93/  300]
per-ex loss: 0.459212  [   96/  300]
per-ex loss: 0.559718  [   99/  300]
per-ex loss: 0.379614  [  102/  300]
per-ex loss: 0.459145  [  105/  300]
per-ex loss: 0.474049  [  108/  300]
per-ex loss: 0.464728  [  111/  300]
per-ex loss: 0.670131  [  114/  300]
per-ex loss: 0.387106  [  117/  300]
per-ex loss: 0.649335  [  120/  300]
per-ex loss: 0.524762  [  123/  300]
per-ex loss: 0.483513  [  126/  300]
per-ex loss: 0.427790  [  129/  300]
per-ex loss: 0.517278  [  132/  300]
per-ex loss: 0.500753  [  135/  300]
per-ex loss: 0.497400  [  138/  300]
per-ex loss: 0.451783  [  141/  300]
per-ex loss: 0.465427  [  144/  300]
per-ex loss: 0.512628  [  147/  300]
per-ex loss: 0.419340  [  150/  300]
per-ex loss: 0.487929  [  153/  300]
per-ex loss: 0.548696  [  156/  300]
per-ex loss: 0.420818  [  159/  300]
per-ex loss: 0.402811  [  162/  300]
per-ex loss: 0.458447  [  165/  300]
per-ex loss: 0.638364  [  168/  300]
per-ex loss: 0.476346  [  171/  300]
per-ex loss: 0.567323  [  174/  300]
per-ex loss: 0.402221  [  177/  300]
per-ex loss: 0.419805  [  180/  300]
per-ex loss: 0.610745  [  183/  300]
per-ex loss: 0.631114  [  186/  300]
per-ex loss: 0.440511  [  189/  300]
per-ex loss: 0.502775  [  192/  300]
per-ex loss: 0.512647  [  195/  300]
per-ex loss: 0.488377  [  198/  300]
per-ex loss: 0.405397  [  201/  300]
per-ex loss: 0.411936  [  204/  300]
per-ex loss: 0.570437  [  207/  300]
per-ex loss: 0.545943  [  210/  300]
per-ex loss: 0.631464  [  213/  300]
per-ex loss: 0.450735  [  216/  300]
per-ex loss: 0.428974  [  219/  300]
per-ex loss: 0.466293  [  222/  300]
per-ex loss: 0.408733  [  225/  300]
per-ex loss: 0.390850  [  228/  300]
per-ex loss: 0.499190  [  231/  300]
per-ex loss: 0.553305  [  234/  300]
per-ex loss: 0.516648  [  237/  300]
per-ex loss: 0.534855  [  240/  300]
per-ex loss: 0.517773  [  243/  300]
per-ex loss: 0.529910  [  246/  300]
per-ex loss: 0.585901  [  249/  300]
per-ex loss: 0.604394  [  252/  300]
per-ex loss: 0.497417  [  255/  300]
per-ex loss: 0.447832  [  258/  300]
per-ex loss: 0.526047  [  261/  300]
per-ex loss: 0.433196  [  264/  300]
per-ex loss: 0.449513  [  267/  300]
per-ex loss: 0.573205  [  270/  300]
per-ex loss: 0.519481  [  273/  300]
per-ex loss: 0.563368  [  276/  300]
per-ex loss: 0.475816  [  279/  300]
per-ex loss: 0.562528  [  282/  300]
per-ex loss: 0.397612  [  285/  300]
per-ex loss: 0.448423  [  288/  300]
per-ex loss: 0.522425  [  291/  300]
per-ex loss: 0.497367  [  294/  300]
per-ex loss: 0.402969  [  297/  300]
per-ex loss: 0.439447  [  300/  300]
Train Error: Avg loss: 0.49588592
validation Error: 
 Avg loss: 0.50876550 
 F1: 0.476893 
 Precision: 0.439638 
 Recall: 0.521045
 IoU: 0.313105

test Error: 
 Avg loss: 0.45536011 
 F1: 0.545317 
 Precision: 0.490800 
 Recall: 0.613458
 IoU: 0.374870

We have finished training iteration 162
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_160_.pth
per-ex loss: 0.451117  [    3/  300]
per-ex loss: 0.385316  [    6/  300]
per-ex loss: 0.620844  [    9/  300]
per-ex loss: 0.404557  [   12/  300]
per-ex loss: 0.520847  [   15/  300]
per-ex loss: 0.519755  [   18/  300]
per-ex loss: 0.417167  [   21/  300]
per-ex loss: 0.433948  [   24/  300]
per-ex loss: 0.472355  [   27/  300]
per-ex loss: 0.488773  [   30/  300]
per-ex loss: 0.620610  [   33/  300]
per-ex loss: 0.450891  [   36/  300]
per-ex loss: 0.552998  [   39/  300]
per-ex loss: 0.479367  [   42/  300]
per-ex loss: 0.510423  [   45/  300]
per-ex loss: 0.525595  [   48/  300]
per-ex loss: 0.454039  [   51/  300]
per-ex loss: 0.567866  [   54/  300]
per-ex loss: 0.576889  [   57/  300]
per-ex loss: 0.479917  [   60/  300]
per-ex loss: 0.608757  [   63/  300]
per-ex loss: 0.454762  [   66/  300]
per-ex loss: 0.449946  [   69/  300]
per-ex loss: 0.477780  [   72/  300]
per-ex loss: 0.522168  [   75/  300]
per-ex loss: 0.481248  [   78/  300]
per-ex loss: 0.436954  [   81/  300]
per-ex loss: 0.419158  [   84/  300]
per-ex loss: 0.533445  [   87/  300]
per-ex loss: 0.621177  [   90/  300]
per-ex loss: 0.490507  [   93/  300]
per-ex loss: 0.613400  [   96/  300]
per-ex loss: 0.463304  [   99/  300]
per-ex loss: 0.403575  [  102/  300]
per-ex loss: 0.540447  [  105/  300]
per-ex loss: 0.439359  [  108/  300]
per-ex loss: 0.462582  [  111/  300]
per-ex loss: 0.648248  [  114/  300]
per-ex loss: 0.428039  [  117/  300]
per-ex loss: 0.389112  [  120/  300]
per-ex loss: 0.490960  [  123/  300]
per-ex loss: 0.433478  [  126/  300]
per-ex loss: 0.444669  [  129/  300]
per-ex loss: 0.482604  [  132/  300]
per-ex loss: 0.397389  [  135/  300]
per-ex loss: 0.503234  [  138/  300]
per-ex loss: 0.522620  [  141/  300]
per-ex loss: 0.458165  [  144/  300]
per-ex loss: 0.416367  [  147/  300]
per-ex loss: 0.442670  [  150/  300]
per-ex loss: 0.600953  [  153/  300]
per-ex loss: 0.489484  [  156/  300]
per-ex loss: 0.499135  [  159/  300]
per-ex loss: 0.591695  [  162/  300]
per-ex loss: 0.596189  [  165/  300]
per-ex loss: 0.496254  [  168/  300]
per-ex loss: 0.526306  [  171/  300]
per-ex loss: 0.477717  [  174/  300]
per-ex loss: 0.542311  [  177/  300]
per-ex loss: 0.586483  [  180/  300]
per-ex loss: 0.534020  [  183/  300]
per-ex loss: 0.523291  [  186/  300]
per-ex loss: 0.524498  [  189/  300]
per-ex loss: 0.395384  [  192/  300]
per-ex loss: 0.515730  [  195/  300]
per-ex loss: 0.560712  [  198/  300]
per-ex loss: 0.506169  [  201/  300]
per-ex loss: 0.534393  [  204/  300]
per-ex loss: 0.418010  [  207/  300]
per-ex loss: 0.530575  [  210/  300]
per-ex loss: 0.432513  [  213/  300]
per-ex loss: 0.544486  [  216/  300]
per-ex loss: 0.464612  [  219/  300]
per-ex loss: 0.494131  [  222/  300]
per-ex loss: 0.522422  [  225/  300]
per-ex loss: 0.437289  [  228/  300]
per-ex loss: 0.567798  [  231/  300]
per-ex loss: 0.607411  [  234/  300]
per-ex loss: 0.485724  [  237/  300]
per-ex loss: 0.593726  [  240/  300]
per-ex loss: 0.429106  [  243/  300]
per-ex loss: 0.528861  [  246/  300]
per-ex loss: 0.600784  [  249/  300]
per-ex loss: 0.389464  [  252/  300]
per-ex loss: 0.467740  [  255/  300]
per-ex loss: 0.369415  [  258/  300]
per-ex loss: 0.500193  [  261/  300]
per-ex loss: 0.623404  [  264/  300]
per-ex loss: 0.453018  [  267/  300]
per-ex loss: 0.387964  [  270/  300]
per-ex loss: 0.525121  [  273/  300]
per-ex loss: 0.486474  [  276/  300]
per-ex loss: 0.469080  [  279/  300]
per-ex loss: 0.478923  [  282/  300]
per-ex loss: 0.567839  [  285/  300]
per-ex loss: 0.492679  [  288/  300]
per-ex loss: 0.482373  [  291/  300]
per-ex loss: 0.563402  [  294/  300]
per-ex loss: 0.502077  [  297/  300]
per-ex loss: 0.488967  [  300/  300]
Train Error: Avg loss: 0.49815702
validation Error: 
 Avg loss: 0.53961655 
 F1: 0.455738 
 Precision: 0.482028 
 Recall: 0.432167
 IoU: 0.295117

test Error: 
 Avg loss: 0.45168340 
 F1: 0.549002 
 Precision: 0.550423 
 Recall: 0.547588
 IoU: 0.378362

We have finished training iteration 163
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_161_.pth
per-ex loss: 0.477703  [    3/  300]
per-ex loss: 0.510561  [    6/  300]
per-ex loss: 0.438501  [    9/  300]
per-ex loss: 0.519995  [   12/  300]
per-ex loss: 0.408380  [   15/  300]
per-ex loss: 0.553733  [   18/  300]
per-ex loss: 0.609212  [   21/  300]
per-ex loss: 0.387654  [   24/  300]
per-ex loss: 0.553475  [   27/  300]
per-ex loss: 0.396155  [   30/  300]
per-ex loss: 0.468203  [   33/  300]
per-ex loss: 0.438911  [   36/  300]
per-ex loss: 0.645820  [   39/  300]
per-ex loss: 0.445193  [   42/  300]
per-ex loss: 0.508822  [   45/  300]
per-ex loss: 0.503240  [   48/  300]
per-ex loss: 0.538336  [   51/  300]
per-ex loss: 0.400190  [   54/  300]
per-ex loss: 0.617406  [   57/  300]
per-ex loss: 0.471832  [   60/  300]
per-ex loss: 0.561239  [   63/  300]
per-ex loss: 0.424277  [   66/  300]
per-ex loss: 0.564645  [   69/  300]
per-ex loss: 0.462457  [   72/  300]
per-ex loss: 0.596371  [   75/  300]
per-ex loss: 0.494249  [   78/  300]
per-ex loss: 0.465516  [   81/  300]
per-ex loss: 0.481736  [   84/  300]
per-ex loss: 0.525399  [   87/  300]
per-ex loss: 0.446718  [   90/  300]
per-ex loss: 0.513805  [   93/  300]
per-ex loss: 0.439550  [   96/  300]
per-ex loss: 0.435405  [   99/  300]
per-ex loss: 0.500242  [  102/  300]
per-ex loss: 0.493451  [  105/  300]
per-ex loss: 0.502896  [  108/  300]
per-ex loss: 0.554315  [  111/  300]
per-ex loss: 0.449722  [  114/  300]
per-ex loss: 0.406280  [  117/  300]
per-ex loss: 0.491709  [  120/  300]
per-ex loss: 0.467821  [  123/  300]
per-ex loss: 0.477765  [  126/  300]
per-ex loss: 0.458543  [  129/  300]
per-ex loss: 0.407048  [  132/  300]
per-ex loss: 0.451762  [  135/  300]
per-ex loss: 0.449556  [  138/  300]
per-ex loss: 0.499602  [  141/  300]
per-ex loss: 0.424806  [  144/  300]
per-ex loss: 0.574624  [  147/  300]
per-ex loss: 0.605019  [  150/  300]
per-ex loss: 0.615773  [  153/  300]
per-ex loss: 0.477481  [  156/  300]
per-ex loss: 0.537205  [  159/  300]
per-ex loss: 0.506142  [  162/  300]
per-ex loss: 0.553360  [  165/  300]
per-ex loss: 0.564801  [  168/  300]
per-ex loss: 0.381332  [  171/  300]
per-ex loss: 0.511855  [  174/  300]
per-ex loss: 0.482245  [  177/  300]
per-ex loss: 0.570237  [  180/  300]
per-ex loss: 0.539770  [  183/  300]
per-ex loss: 0.432429  [  186/  300]
per-ex loss: 0.431840  [  189/  300]
per-ex loss: 0.457831  [  192/  300]
per-ex loss: 0.426702  [  195/  300]
per-ex loss: 0.530941  [  198/  300]
per-ex loss: 0.443079  [  201/  300]
per-ex loss: 0.544014  [  204/  300]
per-ex loss: 0.547378  [  207/  300]
per-ex loss: 0.444214  [  210/  300]
per-ex loss: 0.461499  [  213/  300]
per-ex loss: 0.477561  [  216/  300]
per-ex loss: 0.597946  [  219/  300]
per-ex loss: 0.464254  [  222/  300]
per-ex loss: 0.586102  [  225/  300]
per-ex loss: 0.545355  [  228/  300]
per-ex loss: 0.423001  [  231/  300]
per-ex loss: 0.539335  [  234/  300]
per-ex loss: 0.420455  [  237/  300]
per-ex loss: 0.415326  [  240/  300]
per-ex loss: 0.445909  [  243/  300]
per-ex loss: 0.594933  [  246/  300]
per-ex loss: 0.610042  [  249/  300]
per-ex loss: 0.514897  [  252/  300]
per-ex loss: 0.634320  [  255/  300]
per-ex loss: 0.411230  [  258/  300]
per-ex loss: 0.517136  [  261/  300]
per-ex loss: 0.422228  [  264/  300]
per-ex loss: 0.534014  [  267/  300]
per-ex loss: 0.509902  [  270/  300]
per-ex loss: 0.646182  [  273/  300]
per-ex loss: 0.537436  [  276/  300]
per-ex loss: 0.375692  [  279/  300]
per-ex loss: 0.523727  [  282/  300]
per-ex loss: 0.492488  [  285/  300]
per-ex loss: 0.530920  [  288/  300]
per-ex loss: 0.454509  [  291/  300]
per-ex loss: 0.502949  [  294/  300]
per-ex loss: 0.570938  [  297/  300]
per-ex loss: 0.461754  [  300/  300]
Train Error: Avg loss: 0.49740522
validation Error: 
 Avg loss: 0.50522765 
 F1: 0.479193 
 Precision: 0.451703 
 Recall: 0.510247
 IoU: 0.315092

test Error: 
 Avg loss: 0.44764131 
 F1: 0.552782 
 Precision: 0.514527 
 Recall: 0.597181
 IoU: 0.381961

We have finished training iteration 164
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_162_.pth
per-ex loss: 0.544931  [    3/  300]
per-ex loss: 0.445488  [    6/  300]
per-ex loss: 0.521181  [    9/  300]
per-ex loss: 0.459289  [   12/  300]
per-ex loss: 0.510951  [   15/  300]
per-ex loss: 0.432261  [   18/  300]
per-ex loss: 0.622407  [   21/  300]
per-ex loss: 0.572417  [   24/  300]
per-ex loss: 0.367910  [   27/  300]
per-ex loss: 0.499876  [   30/  300]
per-ex loss: 0.510400  [   33/  300]
per-ex loss: 0.437142  [   36/  300]
per-ex loss: 0.446494  [   39/  300]
per-ex loss: 0.445204  [   42/  300]
per-ex loss: 0.568961  [   45/  300]
per-ex loss: 0.533509  [   48/  300]
per-ex loss: 0.449929  [   51/  300]
per-ex loss: 0.426851  [   54/  300]
per-ex loss: 0.574115  [   57/  300]
per-ex loss: 0.537696  [   60/  300]
per-ex loss: 0.595344  [   63/  300]
per-ex loss: 0.557702  [   66/  300]
per-ex loss: 0.497207  [   69/  300]
per-ex loss: 0.483813  [   72/  300]
per-ex loss: 0.531376  [   75/  300]
per-ex loss: 0.413158  [   78/  300]
per-ex loss: 0.524089  [   81/  300]
per-ex loss: 0.635950  [   84/  300]
per-ex loss: 0.382623  [   87/  300]
per-ex loss: 0.525872  [   90/  300]
per-ex loss: 0.494181  [   93/  300]
per-ex loss: 0.458321  [   96/  300]
per-ex loss: 0.459108  [   99/  300]
per-ex loss: 0.476659  [  102/  300]
per-ex loss: 0.446225  [  105/  300]
per-ex loss: 0.642611  [  108/  300]
per-ex loss: 0.473211  [  111/  300]
per-ex loss: 0.488665  [  114/  300]
per-ex loss: 0.656523  [  117/  300]
per-ex loss: 0.421050  [  120/  300]
per-ex loss: 0.445965  [  123/  300]
per-ex loss: 0.508720  [  126/  300]
per-ex loss: 0.458622  [  129/  300]
per-ex loss: 0.518192  [  132/  300]
per-ex loss: 0.436541  [  135/  300]
per-ex loss: 0.496357  [  138/  300]
per-ex loss: 0.421318  [  141/  300]
per-ex loss: 0.445439  [  144/  300]
per-ex loss: 0.515863  [  147/  300]
per-ex loss: 0.490206  [  150/  300]
per-ex loss: 0.512285  [  153/  300]
per-ex loss: 0.444525  [  156/  300]
per-ex loss: 0.519433  [  159/  300]
per-ex loss: 0.489566  [  162/  300]
per-ex loss: 0.531628  [  165/  300]
per-ex loss: 0.432729  [  168/  300]
per-ex loss: 0.665487  [  171/  300]
per-ex loss: 0.482487  [  174/  300]
per-ex loss: 0.533933  [  177/  300]
per-ex loss: 0.404925  [  180/  300]
per-ex loss: 0.544273  [  183/  300]
per-ex loss: 0.442534  [  186/  300]
per-ex loss: 0.464741  [  189/  300]
per-ex loss: 0.452594  [  192/  300]
per-ex loss: 0.488817  [  195/  300]
per-ex loss: 0.571887  [  198/  300]
per-ex loss: 0.476381  [  201/  300]
per-ex loss: 0.526102  [  204/  300]
per-ex loss: 0.414456  [  207/  300]
per-ex loss: 0.574304  [  210/  300]
per-ex loss: 0.529216  [  213/  300]
per-ex loss: 0.468000  [  216/  300]
per-ex loss: 0.514287  [  219/  300]
per-ex loss: 0.394772  [  222/  300]
per-ex loss: 0.423734  [  225/  300]
per-ex loss: 0.506741  [  228/  300]
per-ex loss: 0.520513  [  231/  300]
per-ex loss: 0.565370  [  234/  300]
per-ex loss: 0.571118  [  237/  300]
per-ex loss: 0.677773  [  240/  300]
per-ex loss: 0.435619  [  243/  300]
per-ex loss: 0.409302  [  246/  300]
per-ex loss: 0.564880  [  249/  300]
per-ex loss: 0.608930  [  252/  300]
per-ex loss: 0.440426  [  255/  300]
per-ex loss: 0.458895  [  258/  300]
per-ex loss: 0.504641  [  261/  300]
per-ex loss: 0.436396  [  264/  300]
per-ex loss: 0.592776  [  267/  300]
per-ex loss: 0.587973  [  270/  300]
per-ex loss: 0.601823  [  273/  300]
per-ex loss: 0.445131  [  276/  300]
per-ex loss: 0.600922  [  279/  300]
per-ex loss: 0.520111  [  282/  300]
per-ex loss: 0.474505  [  285/  300]
per-ex loss: 0.415186  [  288/  300]
per-ex loss: 0.443403  [  291/  300]
per-ex loss: 0.404093  [  294/  300]
per-ex loss: 0.513170  [  297/  300]
per-ex loss: 0.528417  [  300/  300]
Train Error: Avg loss: 0.49939131
validation Error: 
 Avg loss: 0.49771929 
 F1: 0.487982 
 Precision: 0.456619 
 Recall: 0.523971
 IoU: 0.322736

test Error: 
 Avg loss: 0.45029521 
 F1: 0.550519 
 Precision: 0.513337 
 Recall: 0.593508
 IoU: 0.379805

We have finished training iteration 165
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_163_.pth
per-ex loss: 0.576103  [    3/  300]
per-ex loss: 0.449884  [    6/  300]
per-ex loss: 0.382118  [    9/  300]
per-ex loss: 0.423832  [   12/  300]
per-ex loss: 0.552640  [   15/  300]
per-ex loss: 0.590020  [   18/  300]
per-ex loss: 0.461148  [   21/  300]
per-ex loss: 0.481746  [   24/  300]
per-ex loss: 0.550211  [   27/  300]
per-ex loss: 0.456528  [   30/  300]
per-ex loss: 0.590255  [   33/  300]
per-ex loss: 0.482366  [   36/  300]
per-ex loss: 0.489940  [   39/  300]
per-ex loss: 0.456003  [   42/  300]
per-ex loss: 0.436971  [   45/  300]
per-ex loss: 0.555580  [   48/  300]
per-ex loss: 0.401460  [   51/  300]
per-ex loss: 0.469806  [   54/  300]
per-ex loss: 0.540753  [   57/  300]
per-ex loss: 0.439967  [   60/  300]
per-ex loss: 0.442532  [   63/  300]
per-ex loss: 0.558601  [   66/  300]
per-ex loss: 0.485125  [   69/  300]
per-ex loss: 0.483857  [   72/  300]
per-ex loss: 0.490794  [   75/  300]
per-ex loss: 0.579395  [   78/  300]
per-ex loss: 0.488580  [   81/  300]
per-ex loss: 0.456574  [   84/  300]
per-ex loss: 0.499314  [   87/  300]
per-ex loss: 0.456502  [   90/  300]
per-ex loss: 0.440120  [   93/  300]
per-ex loss: 0.541164  [   96/  300]
per-ex loss: 0.580495  [   99/  300]
per-ex loss: 0.529170  [  102/  300]
per-ex loss: 0.544509  [  105/  300]
per-ex loss: 0.588735  [  108/  300]
per-ex loss: 0.514708  [  111/  300]
per-ex loss: 0.491644  [  114/  300]
per-ex loss: 0.410618  [  117/  300]
per-ex loss: 0.552278  [  120/  300]
per-ex loss: 0.478639  [  123/  300]
per-ex loss: 0.585801  [  126/  300]
per-ex loss: 0.428099  [  129/  300]
per-ex loss: 0.511156  [  132/  300]
per-ex loss: 0.526012  [  135/  300]
per-ex loss: 0.463975  [  138/  300]
per-ex loss: 0.607329  [  141/  300]
per-ex loss: 0.427815  [  144/  300]
per-ex loss: 0.511361  [  147/  300]
per-ex loss: 0.426255  [  150/  300]
per-ex loss: 0.480613  [  153/  300]
per-ex loss: 0.395882  [  156/  300]
per-ex loss: 0.562401  [  159/  300]
per-ex loss: 0.589261  [  162/  300]
per-ex loss: 0.494184  [  165/  300]
per-ex loss: 0.428746  [  168/  300]
per-ex loss: 0.424669  [  171/  300]
per-ex loss: 0.475872  [  174/  300]
per-ex loss: 0.426456  [  177/  300]
per-ex loss: 0.452359  [  180/  300]
per-ex loss: 0.585626  [  183/  300]
per-ex loss: 0.486089  [  186/  300]
per-ex loss: 0.553988  [  189/  300]
per-ex loss: 0.485499  [  192/  300]
per-ex loss: 0.530865  [  195/  300]
per-ex loss: 0.409988  [  198/  300]
per-ex loss: 0.599256  [  201/  300]
per-ex loss: 0.537349  [  204/  300]
per-ex loss: 0.469648  [  207/  300]
per-ex loss: 0.561919  [  210/  300]
per-ex loss: 0.519442  [  213/  300]
per-ex loss: 0.573788  [  216/  300]
per-ex loss: 0.550299  [  219/  300]
per-ex loss: 0.511947  [  222/  300]
per-ex loss: 0.549706  [  225/  300]
per-ex loss: 0.427864  [  228/  300]
per-ex loss: 0.487426  [  231/  300]
per-ex loss: 0.400328  [  234/  300]
per-ex loss: 0.555364  [  237/  300]
per-ex loss: 0.525524  [  240/  300]
per-ex loss: 0.476804  [  243/  300]
per-ex loss: 0.421182  [  246/  300]
per-ex loss: 0.398574  [  249/  300]
per-ex loss: 0.431107  [  252/  300]
per-ex loss: 0.445400  [  255/  300]
per-ex loss: 0.495629  [  258/  300]
per-ex loss: 0.500138  [  261/  300]
per-ex loss: 0.595029  [  264/  300]
per-ex loss: 0.429215  [  267/  300]
per-ex loss: 0.460943  [  270/  300]
per-ex loss: 0.403498  [  273/  300]
per-ex loss: 0.431538  [  276/  300]
per-ex loss: 0.430950  [  279/  300]
per-ex loss: 0.427794  [  282/  300]
per-ex loss: 0.479445  [  285/  300]
per-ex loss: 0.394704  [  288/  300]
per-ex loss: 0.588157  [  291/  300]
per-ex loss: 0.425777  [  294/  300]
per-ex loss: 0.549637  [  297/  300]
per-ex loss: 0.596090  [  300/  300]
Train Error: Avg loss: 0.49322462
validation Error: 
 Avg loss: 0.48905847 
 F1: 0.489698 
 Precision: 0.416196 
 Recall: 0.594730
 IoU: 0.324238

test Error: 
 Avg loss: 0.45555264 
 F1: 0.545320 
 Precision: 0.471050 
 Recall: 0.647394
 IoU: 0.374873

We have finished training iteration 166
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_164_.pth
per-ex loss: 0.470566  [    3/  300]
per-ex loss: 0.440152  [    6/  300]
per-ex loss: 0.572423  [    9/  300]
per-ex loss: 0.502407  [   12/  300]
per-ex loss: 0.624264  [   15/  300]
per-ex loss: 0.572770  [   18/  300]
per-ex loss: 0.518162  [   21/  300]
per-ex loss: 0.454663  [   24/  300]
per-ex loss: 0.492819  [   27/  300]
per-ex loss: 0.462096  [   30/  300]
per-ex loss: 0.471893  [   33/  300]
per-ex loss: 0.412549  [   36/  300]
per-ex loss: 0.496194  [   39/  300]
per-ex loss: 0.623827  [   42/  300]
per-ex loss: 0.417882  [   45/  300]
per-ex loss: 0.410524  [   48/  300]
per-ex loss: 0.466588  [   51/  300]
per-ex loss: 0.507957  [   54/  300]
per-ex loss: 0.528077  [   57/  300]
per-ex loss: 0.387340  [   60/  300]
per-ex loss: 0.465102  [   63/  300]
per-ex loss: 0.542719  [   66/  300]
per-ex loss: 0.513691  [   69/  300]
per-ex loss: 0.464025  [   72/  300]
per-ex loss: 0.488881  [   75/  300]
per-ex loss: 0.434830  [   78/  300]
per-ex loss: 0.549088  [   81/  300]
per-ex loss: 0.623740  [   84/  300]
per-ex loss: 0.543346  [   87/  300]
per-ex loss: 0.486472  [   90/  300]
per-ex loss: 0.573482  [   93/  300]
per-ex loss: 0.493229  [   96/  300]
per-ex loss: 0.394038  [   99/  300]
per-ex loss: 0.500925  [  102/  300]
per-ex loss: 0.531104  [  105/  300]
per-ex loss: 0.430065  [  108/  300]
per-ex loss: 0.456357  [  111/  300]
per-ex loss: 0.556040  [  114/  300]
per-ex loss: 0.399491  [  117/  300]
per-ex loss: 0.444457  [  120/  300]
per-ex loss: 0.624719  [  123/  300]
per-ex loss: 0.575326  [  126/  300]
per-ex loss: 0.544288  [  129/  300]
per-ex loss: 0.521680  [  132/  300]
per-ex loss: 0.537474  [  135/  300]
per-ex loss: 0.569117  [  138/  300]
per-ex loss: 0.415265  [  141/  300]
per-ex loss: 0.446819  [  144/  300]
per-ex loss: 0.479273  [  147/  300]
per-ex loss: 0.448396  [  150/  300]
per-ex loss: 0.440637  [  153/  300]
per-ex loss: 0.457421  [  156/  300]
per-ex loss: 0.599539  [  159/  300]
per-ex loss: 0.444420  [  162/  300]
per-ex loss: 0.519945  [  165/  300]
per-ex loss: 0.417091  [  168/  300]
per-ex loss: 0.517915  [  171/  300]
per-ex loss: 0.489028  [  174/  300]
per-ex loss: 0.443211  [  177/  300]
per-ex loss: 0.477925  [  180/  300]
per-ex loss: 0.444880  [  183/  300]
per-ex loss: 0.467044  [  186/  300]
per-ex loss: 0.517208  [  189/  300]
per-ex loss: 0.543217  [  192/  300]
per-ex loss: 0.521778  [  195/  300]
per-ex loss: 0.439712  [  198/  300]
per-ex loss: 0.401762  [  201/  300]
per-ex loss: 0.413730  [  204/  300]
per-ex loss: 0.446463  [  207/  300]
per-ex loss: 0.432929  [  210/  300]
per-ex loss: 0.588684  [  213/  300]
per-ex loss: 0.429922  [  216/  300]
per-ex loss: 0.529151  [  219/  300]
per-ex loss: 0.480009  [  222/  300]
per-ex loss: 0.542963  [  225/  300]
per-ex loss: 0.470409  [  228/  300]
per-ex loss: 0.443700  [  231/  300]
per-ex loss: 0.490960  [  234/  300]
per-ex loss: 0.534338  [  237/  300]
per-ex loss: 0.601054  [  240/  300]
per-ex loss: 0.446693  [  243/  300]
per-ex loss: 0.440661  [  246/  300]
per-ex loss: 0.597423  [  249/  300]
per-ex loss: 0.423639  [  252/  300]
per-ex loss: 0.557958  [  255/  300]
per-ex loss: 0.521608  [  258/  300]
per-ex loss: 0.608383  [  261/  300]
per-ex loss: 0.529370  [  264/  300]
per-ex loss: 0.497701  [  267/  300]
per-ex loss: 0.438137  [  270/  300]
per-ex loss: 0.553266  [  273/  300]
per-ex loss: 0.540738  [  276/  300]
per-ex loss: 0.400103  [  279/  300]
per-ex loss: 0.648482  [  282/  300]
per-ex loss: 0.420106  [  285/  300]
per-ex loss: 0.497446  [  288/  300]
per-ex loss: 0.573687  [  291/  300]
per-ex loss: 0.508327  [  294/  300]
per-ex loss: 0.512672  [  297/  300]
per-ex loss: 0.452873  [  300/  300]
Train Error: Avg loss: 0.49602908
validation Error: 
 Avg loss: 0.49094540 
 F1: 0.490493 
 Precision: 0.449367 
 Recall: 0.539906
 IoU: 0.324936

test Error: 
 Avg loss: 0.44810981 
 F1: 0.552524 
 Precision: 0.506574 
 Recall: 0.607643
 IoU: 0.381716

We have finished training iteration 167
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_165_.pth
per-ex loss: 0.545794  [    3/  300]
per-ex loss: 0.441154  [    6/  300]
per-ex loss: 0.481040  [    9/  300]
per-ex loss: 0.461294  [   12/  300]
per-ex loss: 0.553616  [   15/  300]
per-ex loss: 0.553047  [   18/  300]
per-ex loss: 0.440935  [   21/  300]
per-ex loss: 0.544776  [   24/  300]
per-ex loss: 0.453860  [   27/  300]
per-ex loss: 0.458154  [   30/  300]
per-ex loss: 0.568762  [   33/  300]
per-ex loss: 0.386163  [   36/  300]
per-ex loss: 0.636941  [   39/  300]
per-ex loss: 0.456288  [   42/  300]
per-ex loss: 0.414392  [   45/  300]
per-ex loss: 0.566115  [   48/  300]
per-ex loss: 0.504534  [   51/  300]
per-ex loss: 0.598177  [   54/  300]
per-ex loss: 0.458147  [   57/  300]
per-ex loss: 0.468673  [   60/  300]
per-ex loss: 0.456283  [   63/  300]
per-ex loss: 0.555911  [   66/  300]
per-ex loss: 0.626964  [   69/  300]
per-ex loss: 0.415014  [   72/  300]
per-ex loss: 0.475107  [   75/  300]
per-ex loss: 0.476996  [   78/  300]
per-ex loss: 0.448048  [   81/  300]
per-ex loss: 0.439394  [   84/  300]
per-ex loss: 0.430276  [   87/  300]
per-ex loss: 0.434984  [   90/  300]
per-ex loss: 0.593569  [   93/  300]
per-ex loss: 0.488814  [   96/  300]
per-ex loss: 0.486246  [   99/  300]
per-ex loss: 0.587623  [  102/  300]
per-ex loss: 0.568401  [  105/  300]
per-ex loss: 0.485239  [  108/  300]
per-ex loss: 0.540085  [  111/  300]
per-ex loss: 0.488636  [  114/  300]
per-ex loss: 0.463825  [  117/  300]
per-ex loss: 0.562612  [  120/  300]
per-ex loss: 0.536160  [  123/  300]
per-ex loss: 0.461424  [  126/  300]
per-ex loss: 0.421296  [  129/  300]
per-ex loss: 0.445406  [  132/  300]
per-ex loss: 0.528895  [  135/  300]
per-ex loss: 0.415542  [  138/  300]
per-ex loss: 0.434096  [  141/  300]
per-ex loss: 0.503244  [  144/  300]
per-ex loss: 0.476246  [  147/  300]
per-ex loss: 0.455960  [  150/  300]
per-ex loss: 0.424632  [  153/  300]
per-ex loss: 0.407788  [  156/  300]
per-ex loss: 0.523245  [  159/  300]
per-ex loss: 0.593557  [  162/  300]
per-ex loss: 0.407489  [  165/  300]
per-ex loss: 0.584496  [  168/  300]
per-ex loss: 0.559468  [  171/  300]
per-ex loss: 0.471583  [  174/  300]
per-ex loss: 0.510988  [  177/  300]
per-ex loss: 0.493927  [  180/  300]
per-ex loss: 0.614299  [  183/  300]
per-ex loss: 0.511880  [  186/  300]
per-ex loss: 0.664978  [  189/  300]
per-ex loss: 0.375897  [  192/  300]
per-ex loss: 0.461691  [  195/  300]
per-ex loss: 0.413162  [  198/  300]
per-ex loss: 0.442633  [  201/  300]
per-ex loss: 0.549073  [  204/  300]
per-ex loss: 0.455947  [  207/  300]
per-ex loss: 0.464423  [  210/  300]
per-ex loss: 0.468608  [  213/  300]
per-ex loss: 0.414813  [  216/  300]
per-ex loss: 0.636111  [  219/  300]
per-ex loss: 0.544688  [  222/  300]
per-ex loss: 0.467035  [  225/  300]
per-ex loss: 0.458798  [  228/  300]
per-ex loss: 0.444918  [  231/  300]
per-ex loss: 0.544175  [  234/  300]
per-ex loss: 0.518110  [  237/  300]
per-ex loss: 0.589617  [  240/  300]
per-ex loss: 0.525276  [  243/  300]
per-ex loss: 0.449976  [  246/  300]
per-ex loss: 0.520536  [  249/  300]
per-ex loss: 0.430223  [  252/  300]
per-ex loss: 0.647978  [  255/  300]
per-ex loss: 0.373777  [  258/  300]
per-ex loss: 0.567055  [  261/  300]
per-ex loss: 0.405727  [  264/  300]
per-ex loss: 0.577280  [  267/  300]
per-ex loss: 0.568182  [  270/  300]
per-ex loss: 0.485681  [  273/  300]
per-ex loss: 0.440806  [  276/  300]
per-ex loss: 0.452682  [  279/  300]
per-ex loss: 0.476387  [  282/  300]
per-ex loss: 0.385844  [  285/  300]
per-ex loss: 0.445823  [  288/  300]
per-ex loss: 0.411551  [  291/  300]
per-ex loss: 0.530664  [  294/  300]
per-ex loss: 0.590396  [  297/  300]
per-ex loss: 0.543736  [  300/  300]
Train Error: Avg loss: 0.49541767
validation Error: 
 Avg loss: 0.49701850 
 F1: 0.483856 
 Precision: 0.444812 
 Recall: 0.530414
 IoU: 0.319136

test Error: 
 Avg loss: 0.44699043 
 F1: 0.553639 
 Precision: 0.502234 
 Recall: 0.616768
 IoU: 0.382781

We have finished training iteration 168
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_152_.pth
per-ex loss: 0.460382  [    3/  300]
per-ex loss: 0.469128  [    6/  300]
per-ex loss: 0.643308  [    9/  300]
per-ex loss: 0.448854  [   12/  300]
per-ex loss: 0.551793  [   15/  300]
per-ex loss: 0.497073  [   18/  300]
per-ex loss: 0.478599  [   21/  300]
per-ex loss: 0.401629  [   24/  300]
per-ex loss: 0.404944  [   27/  300]
per-ex loss: 0.469434  [   30/  300]
per-ex loss: 0.467320  [   33/  300]
per-ex loss: 0.456862  [   36/  300]
per-ex loss: 0.506318  [   39/  300]
per-ex loss: 0.550288  [   42/  300]
per-ex loss: 0.391347  [   45/  300]
per-ex loss: 0.596035  [   48/  300]
per-ex loss: 0.410685  [   51/  300]
per-ex loss: 0.556378  [   54/  300]
per-ex loss: 0.495381  [   57/  300]
per-ex loss: 0.438234  [   60/  300]
per-ex loss: 0.455794  [   63/  300]
per-ex loss: 0.535048  [   66/  300]
per-ex loss: 0.419844  [   69/  300]
per-ex loss: 0.515983  [   72/  300]
per-ex loss: 0.638526  [   75/  300]
per-ex loss: 0.423171  [   78/  300]
per-ex loss: 0.466833  [   81/  300]
per-ex loss: 0.458465  [   84/  300]
per-ex loss: 0.490388  [   87/  300]
per-ex loss: 0.460577  [   90/  300]
per-ex loss: 0.480415  [   93/  300]
per-ex loss: 0.429805  [   96/  300]
per-ex loss: 0.400382  [   99/  300]
per-ex loss: 0.461284  [  102/  300]
per-ex loss: 0.484981  [  105/  300]
per-ex loss: 0.466289  [  108/  300]
per-ex loss: 0.622393  [  111/  300]
per-ex loss: 0.431759  [  114/  300]
per-ex loss: 0.506475  [  117/  300]
per-ex loss: 0.435559  [  120/  300]
per-ex loss: 0.533288  [  123/  300]
per-ex loss: 0.430884  [  126/  300]
per-ex loss: 0.452866  [  129/  300]
per-ex loss: 0.493878  [  132/  300]
per-ex loss: 0.534701  [  135/  300]
per-ex loss: 0.453187  [  138/  300]
per-ex loss: 0.463440  [  141/  300]
per-ex loss: 0.525806  [  144/  300]
per-ex loss: 0.426342  [  147/  300]
per-ex loss: 0.630647  [  150/  300]
per-ex loss: 0.604066  [  153/  300]
per-ex loss: 0.609545  [  156/  300]
per-ex loss: 0.470367  [  159/  300]
per-ex loss: 0.511591  [  162/  300]
per-ex loss: 0.564873  [  165/  300]
per-ex loss: 0.496230  [  168/  300]
per-ex loss: 0.437921  [  171/  300]
per-ex loss: 0.602457  [  174/  300]
per-ex loss: 0.490755  [  177/  300]
per-ex loss: 0.556913  [  180/  300]
per-ex loss: 0.544877  [  183/  300]
per-ex loss: 0.531600  [  186/  300]
per-ex loss: 0.404010  [  189/  300]
per-ex loss: 0.399423  [  192/  300]
per-ex loss: 0.386879  [  195/  300]
per-ex loss: 0.479311  [  198/  300]
per-ex loss: 0.453277  [  201/  300]
per-ex loss: 0.537554  [  204/  300]
per-ex loss: 0.483007  [  207/  300]
per-ex loss: 0.423354  [  210/  300]
per-ex loss: 0.397374  [  213/  300]
per-ex loss: 0.522615  [  216/  300]
per-ex loss: 0.423162  [  219/  300]
per-ex loss: 0.540612  [  222/  300]
per-ex loss: 0.555935  [  225/  300]
per-ex loss: 0.419845  [  228/  300]
per-ex loss: 0.639320  [  231/  300]
per-ex loss: 0.428561  [  234/  300]
per-ex loss: 0.479523  [  237/  300]
per-ex loss: 0.435697  [  240/  300]
per-ex loss: 0.470184  [  243/  300]
per-ex loss: 0.448621  [  246/  300]
per-ex loss: 0.633140  [  249/  300]
per-ex loss: 0.590993  [  252/  300]
per-ex loss: 0.580269  [  255/  300]
per-ex loss: 0.441015  [  258/  300]
per-ex loss: 0.516174  [  261/  300]
per-ex loss: 0.414363  [  264/  300]
per-ex loss: 0.473558  [  267/  300]
per-ex loss: 0.439632  [  270/  300]
per-ex loss: 0.425372  [  273/  300]
per-ex loss: 0.413480  [  276/  300]
per-ex loss: 0.546664  [  279/  300]
per-ex loss: 0.567777  [  282/  300]
per-ex loss: 0.403557  [  285/  300]
per-ex loss: 0.692926  [  288/  300]
per-ex loss: 0.427807  [  291/  300]
per-ex loss: 0.432439  [  294/  300]
per-ex loss: 0.572931  [  297/  300]
per-ex loss: 0.535927  [  300/  300]
Train Error: Avg loss: 0.49080483
validation Error: 
 Avg loss: 0.51168688 
 F1: 0.472740 
 Precision: 0.434976 
 Recall: 0.517684
 IoU: 0.309535

test Error: 
 Avg loss: 0.45436549 
 F1: 0.546454 
 Precision: 0.486376 
 Recall: 0.623466
 IoU: 0.375945

We have finished training iteration 169
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_167_.pth
per-ex loss: 0.516111  [    3/  300]
per-ex loss: 0.536078  [    6/  300]
per-ex loss: 0.453282  [    9/  300]
per-ex loss: 0.484489  [   12/  300]
per-ex loss: 0.588723  [   15/  300]
per-ex loss: 0.453276  [   18/  300]
per-ex loss: 0.588779  [   21/  300]
per-ex loss: 0.517689  [   24/  300]
per-ex loss: 0.391284  [   27/  300]
per-ex loss: 0.620847  [   30/  300]
per-ex loss: 0.409771  [   33/  300]
per-ex loss: 0.467658  [   36/  300]
per-ex loss: 0.621318  [   39/  300]
per-ex loss: 0.409175  [   42/  300]
per-ex loss: 0.569120  [   45/  300]
per-ex loss: 0.401997  [   48/  300]
per-ex loss: 0.483147  [   51/  300]
per-ex loss: 0.532037  [   54/  300]
per-ex loss: 0.381801  [   57/  300]
per-ex loss: 0.480543  [   60/  300]
per-ex loss: 0.503706  [   63/  300]
per-ex loss: 0.462743  [   66/  300]
per-ex loss: 0.436383  [   69/  300]
per-ex loss: 0.514054  [   72/  300]
per-ex loss: 0.457610  [   75/  300]
per-ex loss: 0.603936  [   78/  300]
per-ex loss: 0.525747  [   81/  300]
per-ex loss: 0.477295  [   84/  300]
per-ex loss: 0.494179  [   87/  300]
per-ex loss: 0.495938  [   90/  300]
per-ex loss: 0.506950  [   93/  300]
per-ex loss: 0.482601  [   96/  300]
per-ex loss: 0.490142  [   99/  300]
per-ex loss: 0.423364  [  102/  300]
per-ex loss: 0.395415  [  105/  300]
per-ex loss: 0.472871  [  108/  300]
per-ex loss: 0.614904  [  111/  300]
per-ex loss: 0.384960  [  114/  300]
per-ex loss: 0.563527  [  117/  300]
per-ex loss: 0.537625  [  120/  300]
per-ex loss: 0.459836  [  123/  300]
per-ex loss: 0.540538  [  126/  300]
per-ex loss: 0.488960  [  129/  300]
per-ex loss: 0.458324  [  132/  300]
per-ex loss: 0.414207  [  135/  300]
per-ex loss: 0.459481  [  138/  300]
per-ex loss: 0.624247  [  141/  300]
per-ex loss: 0.423887  [  144/  300]
per-ex loss: 0.589082  [  147/  300]
per-ex loss: 0.504782  [  150/  300]
per-ex loss: 0.597635  [  153/  300]
per-ex loss: 0.478746  [  156/  300]
per-ex loss: 0.474770  [  159/  300]
per-ex loss: 0.482579  [  162/  300]
per-ex loss: 0.439348  [  165/  300]
per-ex loss: 0.434022  [  168/  300]
per-ex loss: 0.474539  [  171/  300]
per-ex loss: 0.476428  [  174/  300]
per-ex loss: 0.505750  [  177/  300]
per-ex loss: 0.460034  [  180/  300]
per-ex loss: 0.543600  [  183/  300]
per-ex loss: 0.442905  [  186/  300]
per-ex loss: 0.441384  [  189/  300]
per-ex loss: 0.542026  [  192/  300]
per-ex loss: 0.434232  [  195/  300]
per-ex loss: 0.479607  [  198/  300]
per-ex loss: 0.432765  [  201/  300]
per-ex loss: 0.436300  [  204/  300]
per-ex loss: 0.549092  [  207/  300]
per-ex loss: 0.491522  [  210/  300]
per-ex loss: 0.446821  [  213/  300]
per-ex loss: 0.543027  [  216/  300]
per-ex loss: 0.518257  [  219/  300]
per-ex loss: 0.434018  [  222/  300]
per-ex loss: 0.441991  [  225/  300]
per-ex loss: 0.567226  [  228/  300]
per-ex loss: 0.448701  [  231/  300]
per-ex loss: 0.421744  [  234/  300]
per-ex loss: 0.451819  [  237/  300]
per-ex loss: 0.531132  [  240/  300]
per-ex loss: 0.540851  [  243/  300]
per-ex loss: 0.522350  [  246/  300]
per-ex loss: 0.416469  [  249/  300]
per-ex loss: 0.435078  [  252/  300]
per-ex loss: 0.571051  [  255/  300]
per-ex loss: 0.588877  [  258/  300]
per-ex loss: 0.580867  [  261/  300]
per-ex loss: 0.645536  [  264/  300]
per-ex loss: 0.540846  [  267/  300]
per-ex loss: 0.589913  [  270/  300]
per-ex loss: 0.527350  [  273/  300]
per-ex loss: 0.613333  [  276/  300]
per-ex loss: 0.523744  [  279/  300]
per-ex loss: 0.496703  [  282/  300]
per-ex loss: 0.513302  [  285/  300]
per-ex loss: 0.457181  [  288/  300]
per-ex loss: 0.435964  [  291/  300]
per-ex loss: 0.438627  [  294/  300]
per-ex loss: 0.464193  [  297/  300]
per-ex loss: 0.420987  [  300/  300]
Train Error: Avg loss: 0.49459662
validation Error: 
 Avg loss: 0.57897758 
 F1: 0.424031 
 Precision: 0.478889 
 Recall: 0.380449
 IoU: 0.269060

test Error: 
 Avg loss: 0.45138639 
 F1: 0.549587 
 Precision: 0.551361 
 Recall: 0.547824
 IoU: 0.378917

We have finished training iteration 170
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_168_.pth
per-ex loss: 0.445624  [    3/  300]
per-ex loss: 0.630712  [    6/  300]
per-ex loss: 0.436831  [    9/  300]
per-ex loss: 0.496187  [   12/  300]
per-ex loss: 0.478268  [   15/  300]
per-ex loss: 0.630915  [   18/  300]
per-ex loss: 0.567221  [   21/  300]
per-ex loss: 0.423846  [   24/  300]
per-ex loss: 0.478038  [   27/  300]
per-ex loss: 0.474125  [   30/  300]
per-ex loss: 0.490727  [   33/  300]
per-ex loss: 0.541526  [   36/  300]
per-ex loss: 0.525324  [   39/  300]
per-ex loss: 0.476795  [   42/  300]
per-ex loss: 0.484411  [   45/  300]
per-ex loss: 0.470625  [   48/  300]
per-ex loss: 0.494732  [   51/  300]
per-ex loss: 0.474513  [   54/  300]
per-ex loss: 0.578335  [   57/  300]
per-ex loss: 0.413682  [   60/  300]
per-ex loss: 0.492120  [   63/  300]
per-ex loss: 0.443574  [   66/  300]
per-ex loss: 0.435830  [   69/  300]
per-ex loss: 0.534651  [   72/  300]
per-ex loss: 0.550906  [   75/  300]
per-ex loss: 0.456638  [   78/  300]
per-ex loss: 0.367384  [   81/  300]
per-ex loss: 0.448961  [   84/  300]
per-ex loss: 0.456134  [   87/  300]
per-ex loss: 0.502707  [   90/  300]
per-ex loss: 0.517565  [   93/  300]
per-ex loss: 0.415155  [   96/  300]
per-ex loss: 0.640335  [   99/  300]
per-ex loss: 0.514598  [  102/  300]
per-ex loss: 0.599945  [  105/  300]
per-ex loss: 0.591526  [  108/  300]
per-ex loss: 0.456025  [  111/  300]
per-ex loss: 0.428641  [  114/  300]
per-ex loss: 0.474471  [  117/  300]
per-ex loss: 0.472306  [  120/  300]
per-ex loss: 0.602242  [  123/  300]
per-ex loss: 0.405046  [  126/  300]
per-ex loss: 0.537269  [  129/  300]
per-ex loss: 0.425237  [  132/  300]
per-ex loss: 0.528921  [  135/  300]
per-ex loss: 0.552261  [  138/  300]
per-ex loss: 0.603579  [  141/  300]
per-ex loss: 0.583771  [  144/  300]
per-ex loss: 0.520166  [  147/  300]
per-ex loss: 0.499015  [  150/  300]
per-ex loss: 0.443198  [  153/  300]
per-ex loss: 0.415061  [  156/  300]
per-ex loss: 0.481381  [  159/  300]
per-ex loss: 0.405113  [  162/  300]
per-ex loss: 0.515490  [  165/  300]
per-ex loss: 0.544362  [  168/  300]
per-ex loss: 0.464907  [  171/  300]
per-ex loss: 0.453604  [  174/  300]
per-ex loss: 0.445192  [  177/  300]
per-ex loss: 0.486980  [  180/  300]
per-ex loss: 0.536776  [  183/  300]
per-ex loss: 0.420553  [  186/  300]
per-ex loss: 0.537613  [  189/  300]
per-ex loss: 0.460011  [  192/  300]
per-ex loss: 0.590580  [  195/  300]
per-ex loss: 0.491730  [  198/  300]
per-ex loss: 0.430516  [  201/  300]
per-ex loss: 0.442642  [  204/  300]
per-ex loss: 0.427144  [  207/  300]
per-ex loss: 0.537544  [  210/  300]
per-ex loss: 0.474481  [  213/  300]
per-ex loss: 0.451308  [  216/  300]
per-ex loss: 0.431522  [  219/  300]
per-ex loss: 0.463321  [  222/  300]
per-ex loss: 0.432010  [  225/  300]
per-ex loss: 0.423167  [  228/  300]
per-ex loss: 0.468997  [  231/  300]
per-ex loss: 0.553798  [  234/  300]
per-ex loss: 0.498391  [  237/  300]
per-ex loss: 0.430463  [  240/  300]
per-ex loss: 0.586095  [  243/  300]
per-ex loss: 0.575168  [  246/  300]
per-ex loss: 0.507464  [  249/  300]
per-ex loss: 0.611645  [  252/  300]
per-ex loss: 0.692087  [  255/  300]
per-ex loss: 0.448641  [  258/  300]
per-ex loss: 0.486207  [  261/  300]
per-ex loss: 0.462532  [  264/  300]
per-ex loss: 0.650697  [  267/  300]
per-ex loss: 0.492077  [  270/  300]
per-ex loss: 0.415254  [  273/  300]
per-ex loss: 0.426952  [  276/  300]
per-ex loss: 0.619892  [  279/  300]
per-ex loss: 0.566209  [  282/  300]
per-ex loss: 0.607235  [  285/  300]
per-ex loss: 0.402099  [  288/  300]
per-ex loss: 0.462504  [  291/  300]
per-ex loss: 0.469407  [  294/  300]
per-ex loss: 0.521791  [  297/  300]
per-ex loss: 0.497608  [  300/  300]
Train Error: Avg loss: 0.49728837
validation Error: 
 Avg loss: 0.49150922 
 F1: 0.489259 
 Precision: 0.432545 
 Recall: 0.563090
 IoU: 0.323854

test Error: 
 Avg loss: 0.45346224 
 F1: 0.547209 
 Precision: 0.489922 
 Recall: 0.619667
 IoU: 0.376660

We have finished training iteration 171
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_169_.pth
per-ex loss: 0.489242  [    3/  300]
per-ex loss: 0.394505  [    6/  300]
per-ex loss: 0.406042  [    9/  300]
per-ex loss: 0.436763  [   12/  300]
per-ex loss: 0.515620  [   15/  300]
per-ex loss: 0.504457  [   18/  300]
per-ex loss: 0.504229  [   21/  300]
per-ex loss: 0.566215  [   24/  300]
per-ex loss: 0.461240  [   27/  300]
per-ex loss: 0.437257  [   30/  300]
per-ex loss: 0.387067  [   33/  300]
per-ex loss: 0.577310  [   36/  300]
per-ex loss: 0.576143  [   39/  300]
per-ex loss: 0.483889  [   42/  300]
per-ex loss: 0.473701  [   45/  300]
per-ex loss: 0.488696  [   48/  300]
per-ex loss: 0.564102  [   51/  300]
per-ex loss: 0.494422  [   54/  300]
per-ex loss: 0.458530  [   57/  300]
per-ex loss: 0.470466  [   60/  300]
per-ex loss: 0.512315  [   63/  300]
per-ex loss: 0.586644  [   66/  300]
per-ex loss: 0.383738  [   69/  300]
per-ex loss: 0.535206  [   72/  300]
per-ex loss: 0.419240  [   75/  300]
per-ex loss: 0.588780  [   78/  300]
per-ex loss: 0.564498  [   81/  300]
per-ex loss: 0.677580  [   84/  300]
per-ex loss: 0.477091  [   87/  300]
per-ex loss: 0.495657  [   90/  300]
per-ex loss: 0.659185  [   93/  300]
per-ex loss: 0.556046  [   96/  300]
per-ex loss: 0.395697  [   99/  300]
per-ex loss: 0.407661  [  102/  300]
per-ex loss: 0.518088  [  105/  300]
per-ex loss: 0.563008  [  108/  300]
per-ex loss: 0.536922  [  111/  300]
per-ex loss: 0.484632  [  114/  300]
per-ex loss: 0.428726  [  117/  300]
per-ex loss: 0.537609  [  120/  300]
per-ex loss: 0.417239  [  123/  300]
per-ex loss: 0.418841  [  126/  300]
per-ex loss: 0.507925  [  129/  300]
per-ex loss: 0.487217  [  132/  300]
per-ex loss: 0.480634  [  135/  300]
per-ex loss: 0.515882  [  138/  300]
per-ex loss: 0.558747  [  141/  300]
per-ex loss: 0.389125  [  144/  300]
per-ex loss: 0.508488  [  147/  300]
per-ex loss: 0.581550  [  150/  300]
per-ex loss: 0.493850  [  153/  300]
per-ex loss: 0.504772  [  156/  300]
per-ex loss: 0.595589  [  159/  300]
per-ex loss: 0.438425  [  162/  300]
per-ex loss: 0.515311  [  165/  300]
per-ex loss: 0.467769  [  168/  300]
per-ex loss: 0.557614  [  171/  300]
per-ex loss: 0.462402  [  174/  300]
per-ex loss: 0.422313  [  177/  300]
per-ex loss: 0.458287  [  180/  300]
per-ex loss: 0.475095  [  183/  300]
per-ex loss: 0.490974  [  186/  300]
per-ex loss: 0.526093  [  189/  300]
per-ex loss: 0.483283  [  192/  300]
per-ex loss: 0.481456  [  195/  300]
per-ex loss: 0.483618  [  198/  300]
per-ex loss: 0.435926  [  201/  300]
per-ex loss: 0.593353  [  204/  300]
per-ex loss: 0.485923  [  207/  300]
per-ex loss: 0.463857  [  210/  300]
per-ex loss: 0.534341  [  213/  300]
per-ex loss: 0.597834  [  216/  300]
per-ex loss: 0.425924  [  219/  300]
per-ex loss: 0.379493  [  222/  300]
per-ex loss: 0.464244  [  225/  300]
per-ex loss: 0.450352  [  228/  300]
per-ex loss: 0.581380  [  231/  300]
per-ex loss: 0.427878  [  234/  300]
per-ex loss: 0.506513  [  237/  300]
per-ex loss: 0.469706  [  240/  300]
per-ex loss: 0.402604  [  243/  300]
per-ex loss: 0.541919  [  246/  300]
per-ex loss: 0.569726  [  249/  300]
per-ex loss: 0.468065  [  252/  300]
per-ex loss: 0.475802  [  255/  300]
per-ex loss: 0.519888  [  258/  300]
per-ex loss: 0.608516  [  261/  300]
per-ex loss: 0.441931  [  264/  300]
per-ex loss: 0.486771  [  267/  300]
per-ex loss: 0.560062  [  270/  300]
per-ex loss: 0.633892  [  273/  300]
per-ex loss: 0.425800  [  276/  300]
per-ex loss: 0.662824  [  279/  300]
per-ex loss: 0.452477  [  282/  300]
per-ex loss: 0.480397  [  285/  300]
per-ex loss: 0.417450  [  288/  300]
per-ex loss: 0.511429  [  291/  300]
per-ex loss: 0.469481  [  294/  300]
per-ex loss: 0.503212  [  297/  300]
per-ex loss: 0.506061  [  300/  300]
Train Error: Avg loss: 0.49695744
validation Error: 
 Avg loss: 0.49811105 
 F1: 0.484391 
 Precision: 0.440883 
 Recall: 0.537425
 IoU: 0.319601

test Error: 
 Avg loss: 0.44910908 
 F1: 0.551606 
 Precision: 0.494861 
 Recall: 0.623051
 IoU: 0.380840

We have finished training iteration 172
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_170_.pth
per-ex loss: 0.545148  [    3/  300]
per-ex loss: 0.480300  [    6/  300]
per-ex loss: 0.566063  [    9/  300]
per-ex loss: 0.464444  [   12/  300]
per-ex loss: 0.374228  [   15/  300]
per-ex loss: 0.511273  [   18/  300]
per-ex loss: 0.490530  [   21/  300]
per-ex loss: 0.628131  [   24/  300]
per-ex loss: 0.471705  [   27/  300]
per-ex loss: 0.401759  [   30/  300]
per-ex loss: 0.648878  [   33/  300]
per-ex loss: 0.532223  [   36/  300]
per-ex loss: 0.442712  [   39/  300]
per-ex loss: 0.650738  [   42/  300]
per-ex loss: 0.447168  [   45/  300]
per-ex loss: 0.469678  [   48/  300]
per-ex loss: 0.426616  [   51/  300]
per-ex loss: 0.557095  [   54/  300]
per-ex loss: 0.421511  [   57/  300]
per-ex loss: 0.503006  [   60/  300]
per-ex loss: 0.589839  [   63/  300]
per-ex loss: 0.426503  [   66/  300]
per-ex loss: 0.520562  [   69/  300]
per-ex loss: 0.385689  [   72/  300]
per-ex loss: 0.426273  [   75/  300]
per-ex loss: 0.488748  [   78/  300]
per-ex loss: 0.614961  [   81/  300]
per-ex loss: 0.547916  [   84/  300]
per-ex loss: 0.412511  [   87/  300]
per-ex loss: 0.497183  [   90/  300]
per-ex loss: 0.412454  [   93/  300]
per-ex loss: 0.412115  [   96/  300]
per-ex loss: 0.404870  [   99/  300]
per-ex loss: 0.488108  [  102/  300]
per-ex loss: 0.509044  [  105/  300]
per-ex loss: 0.559790  [  108/  300]
per-ex loss: 0.415031  [  111/  300]
per-ex loss: 0.455332  [  114/  300]
per-ex loss: 0.627126  [  117/  300]
per-ex loss: 0.577141  [  120/  300]
per-ex loss: 0.593848  [  123/  300]
per-ex loss: 0.472258  [  126/  300]
per-ex loss: 0.538616  [  129/  300]
per-ex loss: 0.388449  [  132/  300]
per-ex loss: 0.533871  [  135/  300]
per-ex loss: 0.387356  [  138/  300]
per-ex loss: 0.615083  [  141/  300]
per-ex loss: 0.529005  [  144/  300]
per-ex loss: 0.600320  [  147/  300]
per-ex loss: 0.500018  [  150/  300]
per-ex loss: 0.401840  [  153/  300]
per-ex loss: 0.441630  [  156/  300]
per-ex loss: 0.520053  [  159/  300]
per-ex loss: 0.431526  [  162/  300]
per-ex loss: 0.438780  [  165/  300]
per-ex loss: 0.523090  [  168/  300]
per-ex loss: 0.486122  [  171/  300]
per-ex loss: 0.530903  [  174/  300]
per-ex loss: 0.570445  [  177/  300]
per-ex loss: 0.390476  [  180/  300]
per-ex loss: 0.539495  [  183/  300]
per-ex loss: 0.560704  [  186/  300]
per-ex loss: 0.411520  [  189/  300]
per-ex loss: 0.468283  [  192/  300]
per-ex loss: 0.548284  [  195/  300]
per-ex loss: 0.458324  [  198/  300]
per-ex loss: 0.589305  [  201/  300]
per-ex loss: 0.513017  [  204/  300]
per-ex loss: 0.454089  [  207/  300]
per-ex loss: 0.511687  [  210/  300]
per-ex loss: 0.522873  [  213/  300]
per-ex loss: 0.463690  [  216/  300]
per-ex loss: 0.448780  [  219/  300]
per-ex loss: 0.474411  [  222/  300]
per-ex loss: 0.522148  [  225/  300]
per-ex loss: 0.389406  [  228/  300]
per-ex loss: 0.493151  [  231/  300]
per-ex loss: 0.534368  [  234/  300]
per-ex loss: 0.465010  [  237/  300]
per-ex loss: 0.568179  [  240/  300]
per-ex loss: 0.413217  [  243/  300]
per-ex loss: 0.490045  [  246/  300]
per-ex loss: 0.520541  [  249/  300]
per-ex loss: 0.517868  [  252/  300]
per-ex loss: 0.657060  [  255/  300]
per-ex loss: 0.548031  [  258/  300]
per-ex loss: 0.397914  [  261/  300]
per-ex loss: 0.516624  [  264/  300]
per-ex loss: 0.426135  [  267/  300]
per-ex loss: 0.632247  [  270/  300]
per-ex loss: 0.500886  [  273/  300]
per-ex loss: 0.408343  [  276/  300]
per-ex loss: 0.556962  [  279/  300]
per-ex loss: 0.428634  [  282/  300]
per-ex loss: 0.488723  [  285/  300]
per-ex loss: 0.630445  [  288/  300]
per-ex loss: 0.445564  [  291/  300]
per-ex loss: 0.493441  [  294/  300]
per-ex loss: 0.463439  [  297/  300]
per-ex loss: 0.542915  [  300/  300]
Train Error: Avg loss: 0.49713850
validation Error: 
 Avg loss: 0.51185445 
 F1: 0.473069 
 Precision: 0.438648 
 Recall: 0.513353
 IoU: 0.309817

test Error: 
 Avg loss: 0.45856178 
 F1: 0.542178 
 Precision: 0.493488 
 Recall: 0.601527
 IoU: 0.371909

We have finished training iteration 173
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_171_.pth
per-ex loss: 0.474418  [    3/  300]
per-ex loss: 0.600019  [    6/  300]
per-ex loss: 0.449513  [    9/  300]
per-ex loss: 0.534362  [   12/  300]
per-ex loss: 0.443720  [   15/  300]
per-ex loss: 0.553850  [   18/  300]
per-ex loss: 0.525658  [   21/  300]
per-ex loss: 0.576482  [   24/  300]
per-ex loss: 0.565316  [   27/  300]
per-ex loss: 0.470302  [   30/  300]
per-ex loss: 0.521752  [   33/  300]
per-ex loss: 0.411178  [   36/  300]
per-ex loss: 0.512036  [   39/  300]
per-ex loss: 0.407149  [   42/  300]
per-ex loss: 0.430581  [   45/  300]
per-ex loss: 0.465748  [   48/  300]
per-ex loss: 0.487961  [   51/  300]
per-ex loss: 0.425144  [   54/  300]
per-ex loss: 0.576744  [   57/  300]
per-ex loss: 0.614784  [   60/  300]
per-ex loss: 0.444939  [   63/  300]
per-ex loss: 0.427361  [   66/  300]
per-ex loss: 0.558214  [   69/  300]
per-ex loss: 0.433322  [   72/  300]
per-ex loss: 0.397911  [   75/  300]
per-ex loss: 0.522561  [   78/  300]
per-ex loss: 0.577153  [   81/  300]
per-ex loss: 0.525686  [   84/  300]
per-ex loss: 0.569748  [   87/  300]
per-ex loss: 0.614858  [   90/  300]
per-ex loss: 0.393861  [   93/  300]
per-ex loss: 0.513678  [   96/  300]
per-ex loss: 0.564568  [   99/  300]
per-ex loss: 0.493967  [  102/  300]
per-ex loss: 0.552281  [  105/  300]
per-ex loss: 0.498295  [  108/  300]
per-ex loss: 0.476222  [  111/  300]
per-ex loss: 0.556936  [  114/  300]
per-ex loss: 0.514304  [  117/  300]
per-ex loss: 0.544062  [  120/  300]
per-ex loss: 0.483263  [  123/  300]
per-ex loss: 0.497403  [  126/  300]
per-ex loss: 0.434440  [  129/  300]
per-ex loss: 0.447999  [  132/  300]
per-ex loss: 0.458967  [  135/  300]
per-ex loss: 0.435984  [  138/  300]
per-ex loss: 0.620716  [  141/  300]
per-ex loss: 0.549012  [  144/  300]
per-ex loss: 0.518580  [  147/  300]
per-ex loss: 0.455959  [  150/  300]
per-ex loss: 0.444132  [  153/  300]
per-ex loss: 0.590669  [  156/  300]
per-ex loss: 0.394131  [  159/  300]
per-ex loss: 0.482572  [  162/  300]
per-ex loss: 0.460143  [  165/  300]
per-ex loss: 0.467600  [  168/  300]
per-ex loss: 0.678044  [  171/  300]
per-ex loss: 0.471674  [  174/  300]
per-ex loss: 0.424792  [  177/  300]
per-ex loss: 0.531776  [  180/  300]
per-ex loss: 0.436598  [  183/  300]
per-ex loss: 0.430284  [  186/  300]
per-ex loss: 0.436545  [  189/  300]
per-ex loss: 0.469565  [  192/  300]
per-ex loss: 0.506733  [  195/  300]
per-ex loss: 0.441182  [  198/  300]
per-ex loss: 0.593674  [  201/  300]
per-ex loss: 0.399593  [  204/  300]
per-ex loss: 0.533725  [  207/  300]
per-ex loss: 0.421200  [  210/  300]
per-ex loss: 0.574694  [  213/  300]
per-ex loss: 0.591787  [  216/  300]
per-ex loss: 0.467913  [  219/  300]
per-ex loss: 0.496099  [  222/  300]
per-ex loss: 0.530464  [  225/  300]
per-ex loss: 0.405899  [  228/  300]
per-ex loss: 0.502027  [  231/  300]
per-ex loss: 0.414554  [  234/  300]
per-ex loss: 0.468231  [  237/  300]
per-ex loss: 0.506454  [  240/  300]
per-ex loss: 0.467403  [  243/  300]
per-ex loss: 0.434994  [  246/  300]
per-ex loss: 0.577377  [  249/  300]
per-ex loss: 0.474831  [  252/  300]
per-ex loss: 0.472696  [  255/  300]
per-ex loss: 0.531027  [  258/  300]
per-ex loss: 0.406580  [  261/  300]
per-ex loss: 0.476637  [  264/  300]
per-ex loss: 0.507497  [  267/  300]
per-ex loss: 0.497199  [  270/  300]
per-ex loss: 0.417795  [  273/  300]
per-ex loss: 0.408243  [  276/  300]
per-ex loss: 0.431118  [  279/  300]
per-ex loss: 0.489642  [  282/  300]
per-ex loss: 0.425427  [  285/  300]
per-ex loss: 0.581475  [  288/  300]
per-ex loss: 0.605452  [  291/  300]
per-ex loss: 0.630677  [  294/  300]
per-ex loss: 0.439247  [  297/  300]
per-ex loss: 0.397867  [  300/  300]
Train Error: Avg loss: 0.49374904
validation Error: 
 Avg loss: 0.50481186 
 F1: 0.475884 
 Precision: 0.436887 
 Recall: 0.522524
 IoU: 0.312236

test Error: 
 Avg loss: 0.44414705 
 F1: 0.556643 
 Precision: 0.501285 
 Recall: 0.625744
 IoU: 0.385658

We have finished training iteration 174
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_172_.pth
per-ex loss: 0.493041  [    3/  300]
per-ex loss: 0.478767  [    6/  300]
per-ex loss: 0.435054  [    9/  300]
per-ex loss: 0.551505  [   12/  300]
per-ex loss: 0.675254  [   15/  300]
per-ex loss: 0.515139  [   18/  300]
per-ex loss: 0.561945  [   21/  300]
per-ex loss: 0.401499  [   24/  300]
per-ex loss: 0.489614  [   27/  300]
per-ex loss: 0.538081  [   30/  300]
per-ex loss: 0.500090  [   33/  300]
per-ex loss: 0.448097  [   36/  300]
per-ex loss: 0.482163  [   39/  300]
per-ex loss: 0.463824  [   42/  300]
per-ex loss: 0.437720  [   45/  300]
per-ex loss: 0.494372  [   48/  300]
per-ex loss: 0.532543  [   51/  300]
per-ex loss: 0.488391  [   54/  300]
per-ex loss: 0.423130  [   57/  300]
per-ex loss: 0.452668  [   60/  300]
per-ex loss: 0.488313  [   63/  300]
per-ex loss: 0.553383  [   66/  300]
per-ex loss: 0.435531  [   69/  300]
per-ex loss: 0.519543  [   72/  300]
per-ex loss: 0.505388  [   75/  300]
per-ex loss: 0.484047  [   78/  300]
per-ex loss: 0.510617  [   81/  300]
per-ex loss: 0.531911  [   84/  300]
per-ex loss: 0.508528  [   87/  300]
per-ex loss: 0.434310  [   90/  300]
per-ex loss: 0.451549  [   93/  300]
per-ex loss: 0.521590  [   96/  300]
per-ex loss: 0.572518  [   99/  300]
per-ex loss: 0.502900  [  102/  300]
per-ex loss: 0.452323  [  105/  300]
per-ex loss: 0.430841  [  108/  300]
per-ex loss: 0.383614  [  111/  300]
per-ex loss: 0.373114  [  114/  300]
per-ex loss: 0.610366  [  117/  300]
per-ex loss: 0.440884  [  120/  300]
per-ex loss: 0.407525  [  123/  300]
per-ex loss: 0.515657  [  126/  300]
per-ex loss: 0.581505  [  129/  300]
per-ex loss: 0.501917  [  132/  300]
per-ex loss: 0.547193  [  135/  300]
per-ex loss: 0.410690  [  138/  300]
per-ex loss: 0.524788  [  141/  300]
per-ex loss: 0.482930  [  144/  300]
per-ex loss: 0.503671  [  147/  300]
per-ex loss: 0.443153  [  150/  300]
per-ex loss: 0.465218  [  153/  300]
per-ex loss: 0.570814  [  156/  300]
per-ex loss: 0.632023  [  159/  300]
per-ex loss: 0.475538  [  162/  300]
per-ex loss: 0.522909  [  165/  300]
per-ex loss: 0.456590  [  168/  300]
per-ex loss: 0.552744  [  171/  300]
per-ex loss: 0.557140  [  174/  300]
per-ex loss: 0.482055  [  177/  300]
per-ex loss: 0.516189  [  180/  300]
per-ex loss: 0.528928  [  183/  300]
per-ex loss: 0.477200  [  186/  300]
per-ex loss: 0.427248  [  189/  300]
per-ex loss: 0.495851  [  192/  300]
per-ex loss: 0.516083  [  195/  300]
per-ex loss: 0.425462  [  198/  300]
per-ex loss: 0.397642  [  201/  300]
per-ex loss: 0.492678  [  204/  300]
per-ex loss: 0.441969  [  207/  300]
per-ex loss: 0.542431  [  210/  300]
per-ex loss: 0.578001  [  213/  300]
per-ex loss: 0.514534  [  216/  300]
per-ex loss: 0.425626  [  219/  300]
per-ex loss: 0.537575  [  222/  300]
per-ex loss: 0.530190  [  225/  300]
per-ex loss: 0.575088  [  228/  300]
per-ex loss: 0.474638  [  231/  300]
per-ex loss: 0.512974  [  234/  300]
per-ex loss: 0.522954  [  237/  300]
per-ex loss: 0.516157  [  240/  300]
per-ex loss: 0.520217  [  243/  300]
per-ex loss: 0.453619  [  246/  300]
per-ex loss: 0.435835  [  249/  300]
per-ex loss: 0.464877  [  252/  300]
per-ex loss: 0.535907  [  255/  300]
per-ex loss: 0.661071  [  258/  300]
per-ex loss: 0.421621  [  261/  300]
per-ex loss: 0.399062  [  264/  300]
per-ex loss: 0.498543  [  267/  300]
per-ex loss: 0.399596  [  270/  300]
per-ex loss: 0.544379  [  273/  300]
per-ex loss: 0.431742  [  276/  300]
per-ex loss: 0.471329  [  279/  300]
per-ex loss: 0.469354  [  282/  300]
per-ex loss: 0.507952  [  285/  300]
per-ex loss: 0.474690  [  288/  300]
per-ex loss: 0.512221  [  291/  300]
per-ex loss: 0.507444  [  294/  300]
per-ex loss: 0.480005  [  297/  300]
per-ex loss: 0.561379  [  300/  300]
Train Error: Avg loss: 0.49408490
validation Error: 
 Avg loss: 0.51847831 
 F1: 0.466592 
 Precision: 0.431562 
 Recall: 0.507812
 IoU: 0.304285

test Error: 
 Avg loss: 0.46266574 
 F1: 0.538166 
 Precision: 0.478117 
 Recall: 0.615465
 IoU: 0.368144

We have finished training iteration 175
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_173_.pth
slurmstepd: error: *** STEP 17821.0 ON aga1 CANCELLED AT 2025-01-21T20:38:35 ***
