/shared/home/matevz.vidovic/Diplomska/Prototip/Delo/model_wrapper.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model = torch.load(self.prev_model_path, map_location=torch.device(device))
segnet_main.py do_log: False
Log file name: log_21_12-50-13_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(sd='segnet_vein_train', mti=1, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_segnet/segnet_vein.yaml', yo=None, ntibp=None, ptp=None, map=None)
YAML: {'is_pruning_ready': False, 'path_to_data': './Data/vein_and_sclera_data', 'target': 'veins', 'train_epoch_size': 300, 'val_epoch_size': 100, 'test_epoch_size': 100, 'train_batch_size': 3, 'eval_batch_size': 12, 'learning_rate': 0.0001, 'num_of_dataloader_workers': 32, 'cleanup_k': 3, 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': False, 'loss_fn_name': 'MCDL', 'loss_params': None, 'dataset_type': 'vasd', 'aug_type': 'tf', 'zero_out_non_sclera': True, 'add_sclera_to_img': False, 'add_bcosfire_to_img': True, 'add_coye_to_img': True, 'model': '64_2_6', 'input_width': 2048, 'input_height': 1024, 'input_channels': 5, 'output_channels': 2, 'have_patchification': False, 'patchification_params': 'None', 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq', 'conv2d_prune_limit': 0.2}
Validation phase: False
Namespace(sd='segnet_vein_train', mti=1, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_segnet/segnet_vein.yaml', yo=None, ntibp=None, ptp=None, map=None)
Device: cuda
dataset.py do_log: False
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 88
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 88
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 100
val dataloader num of batches: 3
test dataloader num of batches: 1
Loaded model path:  ./segnet_vein_train/saved_model_wrapper/models/SegNet_2_.pth
per-ex loss: 0.955181  [    3/  300]
per-ex loss: 0.938205  [    6/  300]
per-ex loss: 0.908066  [    9/  300]
per-ex loss: 0.955958  [   12/  300]
per-ex loss: 0.898927  [   15/  300]
per-ex loss: 0.958081  [   18/  300]
per-ex loss: 0.934173  [   21/  300]
per-ex loss: 0.980253  [   24/  300]
per-ex loss: 0.974396  [   27/  300]
per-ex loss: 0.927910  [   30/  300]
per-ex loss: 0.931205  [   33/  300]
per-ex loss: 0.942682  [   36/  300]
per-ex loss: 0.984385  [   39/  300]
per-ex loss: 0.924979  [   42/  300]
per-ex loss: 0.950023  [   45/  300]
per-ex loss: 0.974543  [   48/  300]
per-ex loss: 0.940655  [   51/  300]
per-ex loss: 0.961969  [   54/  300]
per-ex loss: 0.933998  [   57/  300]
per-ex loss: 0.951266  [   60/  300]
per-ex loss: 0.949433  [   63/  300]
per-ex loss: 0.961120  [   66/  300]
per-ex loss: 0.953548  [   69/  300]
per-ex loss: 0.915795  [   72/  300]
per-ex loss: 0.942916  [   75/  300]
per-ex loss: 0.952339  [   78/  300]
per-ex loss: 0.925896  [   81/  300]
per-ex loss: 0.982315  [   84/  300]
per-ex loss: 0.953257  [   87/  300]
per-ex loss: 0.899564  [   90/  300]
per-ex loss: 0.952377  [   93/  300]
per-ex loss: 0.943069  [   96/  300]
per-ex loss: 0.951227  [   99/  300]
per-ex loss: 0.901479  [  102/  300]
per-ex loss: 0.929052  [  105/  300]
per-ex loss: 0.976322  [  108/  300]
per-ex loss: 0.915712  [  111/  300]
per-ex loss: 0.926800  [  114/  300]
per-ex loss: 0.942934  [  117/  300]
per-ex loss: 0.893823  [  120/  300]
per-ex loss: 0.921781  [  123/  300]
per-ex loss: 0.882013  [  126/  300]
per-ex loss: 0.968535  [  129/  300]
per-ex loss: 0.919492  [  132/  300]
per-ex loss: 0.962387  [  135/  300]
per-ex loss: 0.939294  [  138/  300]
per-ex loss: 0.867777  [  141/  300]
per-ex loss: 0.931141  [  144/  300]
per-ex loss: 0.914308  [  147/  300]
per-ex loss: 0.906388  [  150/  300]
per-ex loss: 0.920970  [  153/  300]
per-ex loss: 0.912266  [  156/  300]
per-ex loss: 0.897192  [  159/  300]
per-ex loss: 0.923019  [  162/  300]
per-ex loss: 0.890378  [  165/  300]
per-ex loss: 0.844451  [  168/  300]
per-ex loss: 0.899179  [  171/  300]
per-ex loss: 0.952812  [  174/  300]
per-ex loss: 0.831395  [  177/  300]
per-ex loss: 0.930098  [  180/  300]
per-ex loss: 0.889563  [  183/  300]
per-ex loss: 0.835998  [  186/  300]
per-ex loss: 0.929916  [  189/  300]
per-ex loss: 0.904291  [  192/  300]
per-ex loss: 0.874501  [  195/  300]
per-ex loss: 0.799349  [  198/  300]
per-ex loss: 0.830945  [  201/  300]
per-ex loss: 0.906612  [  204/  300]
per-ex loss: 0.870475  [  207/  300]
per-ex loss: 0.846114  [  210/  300]
per-ex loss: 0.897709  [  213/  300]
per-ex loss: 0.856251  [  216/  300]
per-ex loss: 0.892547  [  219/  300]
per-ex loss: 0.906300  [  222/  300]
per-ex loss: 0.939784  [  225/  300]
per-ex loss: 0.851991  [  228/  300]
per-ex loss: 0.847789  [  231/  300]
per-ex loss: 0.803502  [  234/  300]
per-ex loss: 0.871088  [  237/  300]
per-ex loss: 0.778271  [  240/  300]
per-ex loss: 0.813092  [  243/  300]
per-ex loss: 0.876967  [  246/  300]
per-ex loss: 0.910320  [  249/  300]
per-ex loss: 0.888196  [  252/  300]
per-ex loss: 0.846309  [  255/  300]
per-ex loss: 0.915383  [  258/  300]
per-ex loss: 0.817002  [  261/  300]
per-ex loss: 0.842735  [  264/  300]
per-ex loss: 0.860349  [  267/  300]
per-ex loss: 0.738999  [  270/  300]
per-ex loss: 0.834616  [  273/  300]
per-ex loss: 0.831476  [  276/  300]
per-ex loss: 0.813503  [  279/  300]
per-ex loss: 0.880650  [  282/  300]
per-ex loss: 0.801698  [  285/  300]
per-ex loss: 0.934155  [  288/  300]
per-ex loss: 0.824476  [  291/  300]
per-ex loss: 0.858381  [  294/  300]
per-ex loss: 0.815782  [  297/  300]
per-ex loss: 0.824294  [  300/  300]
Train Error: Avg loss: 0.90172087
validation Error: 
 Avg loss: 0.80901704 
 F1: 0.290515 
 Precision: 0.310870 
 Recall: 0.272662
 IoU: 0.169943

test Error: 
 Avg loss: 0.79382777 
 F1: 0.312722 
 Precision: 0.394821 
 Recall: 0.258890
 IoU: 0.185341

We have finished training iteration 3
Max training iterations reached: 1. Train_iter: 3, Initial_train_iter: 2
