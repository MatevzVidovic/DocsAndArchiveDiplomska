/shared/home/matevz.vidovic/Diplomska/Prototip/Delo/model_wrapper.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model = torch.load(self.prev_model_path, map_location=torch.device(device))
segnet_main.py do_log: False
Log file name: log_21_17-02-36_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(sd='segnet_vein_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_segnet/segnet_vein.yaml', yo=None, ntibp=None, ptp=None, map=None)
YAML: {'is_pruning_ready': False, 'path_to_data': './Data/vein_and_sclera_data', 'target': 'veins', 'train_epoch_size': 300, 'val_epoch_size': 100, 'test_epoch_size': 100, 'train_batch_size': 3, 'eval_batch_size': 12, 'learning_rate': 1e-05, 'num_of_dataloader_workers': 32, 'cleanup_k': 3, 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': False, 'loss_fn_name': 'MCDL', 'loss_params': None, 'dataset_type': 'vasd', 'aug_type': 'tf', 'zero_out_non_sclera': True, 'add_sclera_to_img': False, 'add_bcosfire_to_img': True, 'add_coye_to_img': True, 'model': '64_2_6', 'input_width': 2048, 'input_height': 1024, 'input_channels': 5, 'output_channels': 2, 'have_patchification': False, 'patchification_params': 'None', 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq', 'conv2d_prune_limit': 0.2}
Validation phase: False
Namespace(sd='segnet_vein_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_segnet/segnet_vein.yaml', yo=None, ntibp=None, ptp=None, map=None)
Device: cuda
dataset.py do_log: False
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 88
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 88
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 100
val dataloader num of batches: 3
test dataloader num of batches: 1
Loaded model path:  ./segnet_vein_train/saved_model_wrapper/models/SegNet_95_.pth
per-ex loss: 0.582501  [    3/  300]
per-ex loss: 0.614281  [    6/  300]
per-ex loss: 0.414360  [    9/  300]
per-ex loss: 0.498067  [   12/  300]
per-ex loss: 0.515766  [   15/  300]
per-ex loss: 0.518776  [   18/  300]
per-ex loss: 0.523794  [   21/  300]
per-ex loss: 0.505219  [   24/  300]
per-ex loss: 0.451071  [   27/  300]
per-ex loss: 0.521491  [   30/  300]
per-ex loss: 0.409843  [   33/  300]
per-ex loss: 0.605042  [   36/  300]
per-ex loss: 0.532130  [   39/  300]
per-ex loss: 0.591966  [   42/  300]
per-ex loss: 0.531334  [   45/  300]
per-ex loss: 0.497846  [   48/  300]
per-ex loss: 0.517066  [   51/  300]
per-ex loss: 0.479981  [   54/  300]
per-ex loss: 0.594716  [   57/  300]
per-ex loss: 0.620060  [   60/  300]
per-ex loss: 0.556608  [   63/  300]
per-ex loss: 0.455717  [   66/  300]
per-ex loss: 0.443934  [   69/  300]
per-ex loss: 0.478661  [   72/  300]
per-ex loss: 0.652600  [   75/  300]
per-ex loss: 0.472431  [   78/  300]
per-ex loss: 0.450608  [   81/  300]
per-ex loss: 0.507347  [   84/  300]
per-ex loss: 0.488194  [   87/  300]
per-ex loss: 0.538202  [   90/  300]
per-ex loss: 0.625033  [   93/  300]
per-ex loss: 0.465252  [   96/  300]
per-ex loss: 0.571516  [   99/  300]
per-ex loss: 0.541994  [  102/  300]
per-ex loss: 0.501677  [  105/  300]
per-ex loss: 0.505391  [  108/  300]
per-ex loss: 0.396655  [  111/  300]
per-ex loss: 0.510921  [  114/  300]
per-ex loss: 0.471196  [  117/  300]
per-ex loss: 0.551605  [  120/  300]
per-ex loss: 0.403927  [  123/  300]
per-ex loss: 0.445006  [  126/  300]
per-ex loss: 0.521687  [  129/  300]
per-ex loss: 0.423806  [  132/  300]
per-ex loss: 0.622150  [  135/  300]
per-ex loss: 0.448026  [  138/  300]
per-ex loss: 0.724440  [  141/  300]
per-ex loss: 0.485621  [  144/  300]
per-ex loss: 0.529062  [  147/  300]
per-ex loss: 0.478370  [  150/  300]
per-ex loss: 0.615587  [  153/  300]
per-ex loss: 0.476879  [  156/  300]
per-ex loss: 0.435383  [  159/  300]
per-ex loss: 0.577423  [  162/  300]
per-ex loss: 0.436606  [  165/  300]
per-ex loss: 0.640035  [  168/  300]
per-ex loss: 0.600193  [  171/  300]
per-ex loss: 0.529222  [  174/  300]
per-ex loss: 0.541681  [  177/  300]
per-ex loss: 0.597770  [  180/  300]
per-ex loss: 0.528432  [  183/  300]
per-ex loss: 0.490996  [  186/  300]
per-ex loss: 0.524114  [  189/  300]
per-ex loss: 0.515815  [  192/  300]
per-ex loss: 0.435072  [  195/  300]
per-ex loss: 0.510713  [  198/  300]
per-ex loss: 0.603693  [  201/  300]
per-ex loss: 0.557433  [  204/  300]
per-ex loss: 0.584557  [  207/  300]
per-ex loss: 0.463622  [  210/  300]
per-ex loss: 0.485460  [  213/  300]
per-ex loss: 0.598656  [  216/  300]
per-ex loss: 0.497250  [  219/  300]
per-ex loss: 0.512848  [  222/  300]
per-ex loss: 0.492513  [  225/  300]
per-ex loss: 0.434047  [  228/  300]
per-ex loss: 0.476326  [  231/  300]
per-ex loss: 0.422051  [  234/  300]
per-ex loss: 0.421896  [  237/  300]
per-ex loss: 0.451439  [  240/  300]
per-ex loss: 0.581207  [  243/  300]
per-ex loss: 0.458962  [  246/  300]
per-ex loss: 0.512741  [  249/  300]
per-ex loss: 0.442879  [  252/  300]
per-ex loss: 0.684602  [  255/  300]
per-ex loss: 0.607669  [  258/  300]
per-ex loss: 0.428243  [  261/  300]
per-ex loss: 0.482105  [  264/  300]
per-ex loss: 0.589527  [  267/  300]
per-ex loss: 0.538357  [  270/  300]
per-ex loss: 0.515448  [  273/  300]
per-ex loss: 0.661921  [  276/  300]
per-ex loss: 0.577912  [  279/  300]
per-ex loss: 0.558733  [  282/  300]
per-ex loss: 0.540144  [  285/  300]
per-ex loss: 0.419937  [  288/  300]
per-ex loss: 0.447360  [  291/  300]
per-ex loss: 0.548912  [  294/  300]
per-ex loss: 0.515004  [  297/  300]
per-ex loss: 0.438620  [  300/  300]
Train Error: Avg loss: 0.51800934
validation Error: 
 Avg loss: 0.51484215 
 F1: 0.480222 
 Precision: 0.448347 
 Recall: 0.516977
 IoU: 0.315982

test Error: 
 Avg loss: 0.45504856 
 F1: 0.545628 
 Precision: 0.527580 
 Recall: 0.564954
 IoU: 0.375164

We have finished training iteration 96
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_94_.pth
per-ex loss: 0.486017  [    3/  300]
per-ex loss: 0.529153  [    6/  300]
per-ex loss: 0.515161  [    9/  300]
per-ex loss: 0.667022  [   12/  300]
per-ex loss: 0.414187  [   15/  300]
per-ex loss: 0.515334  [   18/  300]
per-ex loss: 0.476604  [   21/  300]
per-ex loss: 0.456607  [   24/  300]
per-ex loss: 0.393444  [   27/  300]
per-ex loss: 0.540542  [   30/  300]
per-ex loss: 0.526660  [   33/  300]
per-ex loss: 0.430055  [   36/  300]
per-ex loss: 0.507433  [   39/  300]
per-ex loss: 0.491275  [   42/  300]
per-ex loss: 0.523904  [   45/  300]
per-ex loss: 0.551984  [   48/  300]
per-ex loss: 0.414083  [   51/  300]
per-ex loss: 0.529998  [   54/  300]
per-ex loss: 0.468001  [   57/  300]
per-ex loss: 0.505805  [   60/  300]
per-ex loss: 0.546294  [   63/  300]
per-ex loss: 0.553850  [   66/  300]
per-ex loss: 0.507545  [   69/  300]
per-ex loss: 0.457165  [   72/  300]
per-ex loss: 0.459185  [   75/  300]
per-ex loss: 0.520084  [   78/  300]
per-ex loss: 0.495455  [   81/  300]
per-ex loss: 0.505917  [   84/  300]
per-ex loss: 0.577058  [   87/  300]
per-ex loss: 0.489987  [   90/  300]
per-ex loss: 0.413289  [   93/  300]
per-ex loss: 0.537715  [   96/  300]
per-ex loss: 0.443081  [   99/  300]
per-ex loss: 0.500779  [  102/  300]
per-ex loss: 0.496801  [  105/  300]
per-ex loss: 0.391364  [  108/  300]
per-ex loss: 0.409339  [  111/  300]
per-ex loss: 0.627207  [  114/  300]
per-ex loss: 0.418327  [  117/  300]
per-ex loss: 0.499549  [  120/  300]
per-ex loss: 0.571225  [  123/  300]
per-ex loss: 0.411672  [  126/  300]
per-ex loss: 0.446292  [  129/  300]
per-ex loss: 0.470372  [  132/  300]
per-ex loss: 0.558771  [  135/  300]
per-ex loss: 0.595662  [  138/  300]
per-ex loss: 0.626546  [  141/  300]
per-ex loss: 0.511043  [  144/  300]
per-ex loss: 0.527337  [  147/  300]
per-ex loss: 0.470717  [  150/  300]
per-ex loss: 0.482426  [  153/  300]
per-ex loss: 0.578740  [  156/  300]
per-ex loss: 0.500636  [  159/  300]
per-ex loss: 0.662710  [  162/  300]
per-ex loss: 0.471320  [  165/  300]
per-ex loss: 0.646617  [  168/  300]
per-ex loss: 0.579905  [  171/  300]
per-ex loss: 0.575994  [  174/  300]
per-ex loss: 0.539304  [  177/  300]
per-ex loss: 0.627745  [  180/  300]
per-ex loss: 0.640212  [  183/  300]
per-ex loss: 0.442885  [  186/  300]
per-ex loss: 0.540132  [  189/  300]
per-ex loss: 0.443659  [  192/  300]
per-ex loss: 0.452155  [  195/  300]
per-ex loss: 0.428552  [  198/  300]
per-ex loss: 0.432868  [  201/  300]
per-ex loss: 0.476735  [  204/  300]
per-ex loss: 0.547469  [  207/  300]
per-ex loss: 0.441436  [  210/  300]
per-ex loss: 0.454394  [  213/  300]
per-ex loss: 0.449170  [  216/  300]
per-ex loss: 0.528194  [  219/  300]
per-ex loss: 0.504425  [  222/  300]
per-ex loss: 0.473276  [  225/  300]
per-ex loss: 0.540083  [  228/  300]
per-ex loss: 0.591523  [  231/  300]
per-ex loss: 0.457646  [  234/  300]
per-ex loss: 0.611412  [  237/  300]
per-ex loss: 0.581093  [  240/  300]
per-ex loss: 0.551440  [  243/  300]
per-ex loss: 0.493943  [  246/  300]
per-ex loss: 0.456356  [  249/  300]
per-ex loss: 0.420713  [  252/  300]
per-ex loss: 0.556817  [  255/  300]
per-ex loss: 0.512552  [  258/  300]
per-ex loss: 0.503124  [  261/  300]
per-ex loss: 0.438448  [  264/  300]
per-ex loss: 0.474993  [  267/  300]
per-ex loss: 0.473801  [  270/  300]
per-ex loss: 0.525251  [  273/  300]
per-ex loss: 0.638068  [  276/  300]
per-ex loss: 0.440632  [  279/  300]
per-ex loss: 0.482017  [  282/  300]
per-ex loss: 0.601115  [  285/  300]
per-ex loss: 0.504776  [  288/  300]
per-ex loss: 0.515465  [  291/  300]
per-ex loss: 0.448146  [  294/  300]
per-ex loss: 0.434808  [  297/  300]
per-ex loss: 0.462546  [  300/  300]
Train Error: Avg loss: 0.50620624
validation Error: 
 Avg loss: 0.49788000 
 F1: 0.482832 
 Precision: 0.415659 
 Recall: 0.575902
 IoU: 0.318246

test Error: 
 Avg loss: 0.45630348 
 F1: 0.544264 
 Precision: 0.471729 
 Recall: 0.643158
 IoU: 0.373875

We have finished training iteration 97
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_95_.pth
per-ex loss: 0.436140  [    3/  300]
per-ex loss: 0.577470  [    6/  300]
per-ex loss: 0.522951  [    9/  300]
per-ex loss: 0.413636  [   12/  300]
per-ex loss: 0.448326  [   15/  300]
per-ex loss: 0.508482  [   18/  300]
per-ex loss: 0.560008  [   21/  300]
per-ex loss: 0.523167  [   24/  300]
per-ex loss: 0.444225  [   27/  300]
per-ex loss: 0.601190  [   30/  300]
per-ex loss: 0.574952  [   33/  300]
per-ex loss: 0.428571  [   36/  300]
per-ex loss: 0.525256  [   39/  300]
per-ex loss: 0.463722  [   42/  300]
per-ex loss: 0.488222  [   45/  300]
per-ex loss: 0.480443  [   48/  300]
per-ex loss: 0.404879  [   51/  300]
per-ex loss: 0.573919  [   54/  300]
per-ex loss: 0.626718  [   57/  300]
per-ex loss: 0.487324  [   60/  300]
per-ex loss: 0.439242  [   63/  300]
per-ex loss: 0.670874  [   66/  300]
per-ex loss: 0.597870  [   69/  300]
per-ex loss: 0.528207  [   72/  300]
per-ex loss: 0.481302  [   75/  300]
per-ex loss: 0.452244  [   78/  300]
per-ex loss: 0.498210  [   81/  300]
per-ex loss: 0.517116  [   84/  300]
per-ex loss: 0.589002  [   87/  300]
per-ex loss: 0.521696  [   90/  300]
per-ex loss: 0.419935  [   93/  300]
per-ex loss: 0.450329  [   96/  300]
per-ex loss: 0.375688  [   99/  300]
per-ex loss: 0.576377  [  102/  300]
per-ex loss: 0.479209  [  105/  300]
per-ex loss: 0.427130  [  108/  300]
per-ex loss: 0.618534  [  111/  300]
per-ex loss: 0.582150  [  114/  300]
per-ex loss: 0.535402  [  117/  300]
per-ex loss: 0.422716  [  120/  300]
per-ex loss: 0.534894  [  123/  300]
per-ex loss: 0.609652  [  126/  300]
per-ex loss: 0.440684  [  129/  300]
per-ex loss: 0.455786  [  132/  300]
per-ex loss: 0.406448  [  135/  300]
per-ex loss: 0.495941  [  138/  300]
per-ex loss: 0.507321  [  141/  300]
per-ex loss: 0.582607  [  144/  300]
per-ex loss: 0.508663  [  147/  300]
per-ex loss: 0.472538  [  150/  300]
per-ex loss: 0.637539  [  153/  300]
per-ex loss: 0.471761  [  156/  300]
per-ex loss: 0.421740  [  159/  300]
per-ex loss: 0.595969  [  162/  300]
per-ex loss: 0.541851  [  165/  300]
per-ex loss: 0.524203  [  168/  300]
per-ex loss: 0.488819  [  171/  300]
per-ex loss: 0.481872  [  174/  300]
per-ex loss: 0.584910  [  177/  300]
per-ex loss: 0.495367  [  180/  300]
per-ex loss: 0.507445  [  183/  300]
per-ex loss: 0.420464  [  186/  300]
per-ex loss: 0.578532  [  189/  300]
per-ex loss: 0.473270  [  192/  300]
per-ex loss: 0.476779  [  195/  300]
per-ex loss: 0.488959  [  198/  300]
per-ex loss: 0.528061  [  201/  300]
per-ex loss: 0.468304  [  204/  300]
per-ex loss: 0.480112  [  207/  300]
per-ex loss: 0.640286  [  210/  300]
per-ex loss: 0.466466  [  213/  300]
per-ex loss: 0.624929  [  216/  300]
per-ex loss: 0.385296  [  219/  300]
per-ex loss: 0.606699  [  222/  300]
per-ex loss: 0.657979  [  225/  300]
per-ex loss: 0.439739  [  228/  300]
per-ex loss: 0.616945  [  231/  300]
per-ex loss: 0.493329  [  234/  300]
per-ex loss: 0.426824  [  237/  300]
per-ex loss: 0.502210  [  240/  300]
per-ex loss: 0.439612  [  243/  300]
per-ex loss: 0.471457  [  246/  300]
per-ex loss: 0.582176  [  249/  300]
per-ex loss: 0.582089  [  252/  300]
per-ex loss: 0.606720  [  255/  300]
per-ex loss: 0.439524  [  258/  300]
per-ex loss: 0.463626  [  261/  300]
per-ex loss: 0.416353  [  264/  300]
per-ex loss: 0.504927  [  267/  300]
per-ex loss: 0.527653  [  270/  300]
per-ex loss: 0.405913  [  273/  300]
per-ex loss: 0.395540  [  276/  300]
per-ex loss: 0.580474  [  279/  300]
per-ex loss: 0.471396  [  282/  300]
per-ex loss: 0.470436  [  285/  300]
per-ex loss: 0.450343  [  288/  300]
per-ex loss: 0.505698  [  291/  300]
per-ex loss: 0.538650  [  294/  300]
per-ex loss: 0.502665  [  297/  300]
per-ex loss: 0.460687  [  300/  300]
Train Error: Avg loss: 0.50629965
validation Error: 
 Avg loss: 0.51338573 
 F1: 0.481216 
 Precision: 0.465318 
 Recall: 0.498238
 IoU: 0.316843

test Error: 
 Avg loss: 0.45950329 
 F1: 0.541050 
 Precision: 0.528900 
 Recall: 0.553770
 IoU: 0.370849

We have finished training iteration 98
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_96_.pth
per-ex loss: 0.407558  [    3/  300]
per-ex loss: 0.475195  [    6/  300]
per-ex loss: 0.581045  [    9/  300]
per-ex loss: 0.396041  [   12/  300]
per-ex loss: 0.495404  [   15/  300]
per-ex loss: 0.528344  [   18/  300]
per-ex loss: 0.562862  [   21/  300]
per-ex loss: 0.485873  [   24/  300]
per-ex loss: 0.575634  [   27/  300]
per-ex loss: 0.536540  [   30/  300]
per-ex loss: 0.443488  [   33/  300]
per-ex loss: 0.574485  [   36/  300]
per-ex loss: 0.411815  [   39/  300]
per-ex loss: 0.517975  [   42/  300]
per-ex loss: 0.487707  [   45/  300]
per-ex loss: 0.660044  [   48/  300]
per-ex loss: 0.452000  [   51/  300]
per-ex loss: 0.451626  [   54/  300]
per-ex loss: 0.490453  [   57/  300]
per-ex loss: 0.524438  [   60/  300]
per-ex loss: 0.589884  [   63/  300]
per-ex loss: 0.413119  [   66/  300]
per-ex loss: 0.531715  [   69/  300]
per-ex loss: 0.488998  [   72/  300]
per-ex loss: 0.516455  [   75/  300]
per-ex loss: 0.522869  [   78/  300]
per-ex loss: 0.622720  [   81/  300]
per-ex loss: 0.522748  [   84/  300]
per-ex loss: 0.664321  [   87/  300]
per-ex loss: 0.414205  [   90/  300]
per-ex loss: 0.398191  [   93/  300]
per-ex loss: 0.548879  [   96/  300]
per-ex loss: 0.437535  [   99/  300]
per-ex loss: 0.570468  [  102/  300]
per-ex loss: 0.557210  [  105/  300]
per-ex loss: 0.456135  [  108/  300]
per-ex loss: 0.404304  [  111/  300]
per-ex loss: 0.481216  [  114/  300]
per-ex loss: 0.635724  [  117/  300]
per-ex loss: 0.524572  [  120/  300]
per-ex loss: 0.435022  [  123/  300]
per-ex loss: 0.411989  [  126/  300]
per-ex loss: 0.568446  [  129/  300]
per-ex loss: 0.435517  [  132/  300]
per-ex loss: 0.567302  [  135/  300]
per-ex loss: 0.491095  [  138/  300]
per-ex loss: 0.455861  [  141/  300]
per-ex loss: 0.500891  [  144/  300]
per-ex loss: 0.433888  [  147/  300]
per-ex loss: 0.493943  [  150/  300]
per-ex loss: 0.579237  [  153/  300]
per-ex loss: 0.460988  [  156/  300]
per-ex loss: 0.556797  [  159/  300]
per-ex loss: 0.644031  [  162/  300]
per-ex loss: 0.691087  [  165/  300]
per-ex loss: 0.485199  [  168/  300]
per-ex loss: 0.435525  [  171/  300]
per-ex loss: 0.675929  [  174/  300]
per-ex loss: 0.489239  [  177/  300]
per-ex loss: 0.509551  [  180/  300]
per-ex loss: 0.448568  [  183/  300]
per-ex loss: 0.470204  [  186/  300]
per-ex loss: 0.473973  [  189/  300]
per-ex loss: 0.569824  [  192/  300]
per-ex loss: 0.409860  [  195/  300]
per-ex loss: 0.544812  [  198/  300]
per-ex loss: 0.401652  [  201/  300]
per-ex loss: 0.646542  [  204/  300]
per-ex loss: 0.565596  [  207/  300]
per-ex loss: 0.556353  [  210/  300]
per-ex loss: 0.452433  [  213/  300]
per-ex loss: 0.548086  [  216/  300]
per-ex loss: 0.462819  [  219/  300]
per-ex loss: 0.470020  [  222/  300]
per-ex loss: 0.516429  [  225/  300]
per-ex loss: 0.442423  [  228/  300]
per-ex loss: 0.382450  [  231/  300]
per-ex loss: 0.547076  [  234/  300]
per-ex loss: 0.528507  [  237/  300]
per-ex loss: 0.604188  [  240/  300]
per-ex loss: 0.412357  [  243/  300]
per-ex loss: 0.603463  [  246/  300]
per-ex loss: 0.653240  [  249/  300]
per-ex loss: 0.431844  [  252/  300]
per-ex loss: 0.522205  [  255/  300]
per-ex loss: 0.555724  [  258/  300]
per-ex loss: 0.469215  [  261/  300]
per-ex loss: 0.459655  [  264/  300]
per-ex loss: 0.482106  [  267/  300]
per-ex loss: 0.565204  [  270/  300]
per-ex loss: 0.459232  [  273/  300]
per-ex loss: 0.471698  [  276/  300]
per-ex loss: 0.572103  [  279/  300]
per-ex loss: 0.528341  [  282/  300]
per-ex loss: 0.513164  [  285/  300]
per-ex loss: 0.442843  [  288/  300]
per-ex loss: 0.400008  [  291/  300]
per-ex loss: 0.392333  [  294/  300]
per-ex loss: 0.546586  [  297/  300]
per-ex loss: 0.447708  [  300/  300]
Train Error: Avg loss: 0.50652173
validation Error: 
 Avg loss: 0.49968195 
 F1: 0.486155 
 Precision: 0.442537 
 Recall: 0.539311
 IoU: 0.321139

test Error: 
 Avg loss: 0.45355844 
 F1: 0.547185 
 Precision: 0.500740 
 Recall: 0.603127
 IoU: 0.376638

We have finished training iteration 99
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_88_.pth
per-ex loss: 0.563279  [    3/  300]
per-ex loss: 0.461309  [    6/  300]
per-ex loss: 0.535162  [    9/  300]
per-ex loss: 0.451048  [   12/  300]
per-ex loss: 0.511348  [   15/  300]
per-ex loss: 0.535482  [   18/  300]
per-ex loss: 0.619817  [   21/  300]
per-ex loss: 0.623425  [   24/  300]
per-ex loss: 0.508100  [   27/  300]
per-ex loss: 0.612908  [   30/  300]
per-ex loss: 0.537746  [   33/  300]
per-ex loss: 0.619902  [   36/  300]
per-ex loss: 0.436866  [   39/  300]
per-ex loss: 0.419264  [   42/  300]
per-ex loss: 0.440548  [   45/  300]
per-ex loss: 0.451872  [   48/  300]
per-ex loss: 0.534224  [   51/  300]
per-ex loss: 0.441396  [   54/  300]
per-ex loss: 0.433966  [   57/  300]
per-ex loss: 0.500894  [   60/  300]
per-ex loss: 0.503315  [   63/  300]
per-ex loss: 0.556636  [   66/  300]
per-ex loss: 0.567636  [   69/  300]
per-ex loss: 0.395546  [   72/  300]
per-ex loss: 0.430876  [   75/  300]
per-ex loss: 0.520109  [   78/  300]
per-ex loss: 0.484140  [   81/  300]
per-ex loss: 0.480252  [   84/  300]
per-ex loss: 0.662032  [   87/  300]
per-ex loss: 0.573395  [   90/  300]
per-ex loss: 0.442224  [   93/  300]
per-ex loss: 0.473004  [   96/  300]
per-ex loss: 0.582735  [   99/  300]
per-ex loss: 0.427458  [  102/  300]
per-ex loss: 0.413283  [  105/  300]
per-ex loss: 0.598816  [  108/  300]
per-ex loss: 0.633723  [  111/  300]
per-ex loss: 0.485908  [  114/  300]
per-ex loss: 0.518053  [  117/  300]
per-ex loss: 0.576768  [  120/  300]
per-ex loss: 0.543980  [  123/  300]
per-ex loss: 0.579377  [  126/  300]
per-ex loss: 0.575301  [  129/  300]
per-ex loss: 0.548441  [  132/  300]
per-ex loss: 0.522637  [  135/  300]
per-ex loss: 0.441890  [  138/  300]
per-ex loss: 0.448196  [  141/  300]
per-ex loss: 0.417029  [  144/  300]
per-ex loss: 0.529876  [  147/  300]
per-ex loss: 0.533520  [  150/  300]
per-ex loss: 0.498397  [  153/  300]
per-ex loss: 0.500037  [  156/  300]
per-ex loss: 0.448336  [  159/  300]
per-ex loss: 0.516289  [  162/  300]
per-ex loss: 0.386341  [  165/  300]
per-ex loss: 0.420477  [  168/  300]
per-ex loss: 0.543528  [  171/  300]
per-ex loss: 0.547054  [  174/  300]
per-ex loss: 0.552305  [  177/  300]
per-ex loss: 0.534993  [  180/  300]
per-ex loss: 0.644071  [  183/  300]
per-ex loss: 0.454249  [  186/  300]
per-ex loss: 0.489076  [  189/  300]
per-ex loss: 0.463360  [  192/  300]
per-ex loss: 0.612155  [  195/  300]
per-ex loss: 0.545987  [  198/  300]
per-ex loss: 0.554520  [  201/  300]
per-ex loss: 0.643156  [  204/  300]
per-ex loss: 0.504241  [  207/  300]
per-ex loss: 0.607188  [  210/  300]
per-ex loss: 0.670893  [  213/  300]
per-ex loss: 0.525607  [  216/  300]
per-ex loss: 0.414354  [  219/  300]
per-ex loss: 0.503526  [  222/  300]
per-ex loss: 0.534316  [  225/  300]
per-ex loss: 0.494463  [  228/  300]
per-ex loss: 0.561623  [  231/  300]
per-ex loss: 0.436441  [  234/  300]
per-ex loss: 0.413387  [  237/  300]
per-ex loss: 0.490264  [  240/  300]
per-ex loss: 0.487259  [  243/  300]
per-ex loss: 0.491605  [  246/  300]
per-ex loss: 0.428650  [  249/  300]
per-ex loss: 0.432338  [  252/  300]
per-ex loss: 0.412600  [  255/  300]
per-ex loss: 0.449822  [  258/  300]
per-ex loss: 0.473682  [  261/  300]
per-ex loss: 0.470939  [  264/  300]
per-ex loss: 0.723102  [  267/  300]
per-ex loss: 0.447860  [  270/  300]
per-ex loss: 0.440451  [  273/  300]
per-ex loss: 0.558548  [  276/  300]
per-ex loss: 0.636876  [  279/  300]
per-ex loss: 0.609264  [  282/  300]
per-ex loss: 0.458993  [  285/  300]
per-ex loss: 0.446124  [  288/  300]
per-ex loss: 0.402858  [  291/  300]
per-ex loss: 0.569727  [  294/  300]
per-ex loss: 0.474560  [  297/  300]
per-ex loss: 0.418012  [  300/  300]
Train Error: Avg loss: 0.51048613
validation Error: 
 Avg loss: 0.49364154 
 F1: 0.489091 
 Precision: 0.430987 
 Recall: 0.565303
 IoU: 0.323706

test Error: 
 Avg loss: 0.44959563 
 F1: 0.550952 
 Precision: 0.496769 
 Recall: 0.618403
 IoU: 0.380217

We have finished training iteration 100
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_98_.pth
slurmstepd: error: *** STEP 17820.0 ON aga1 CANCELLED AT 2025-01-21T17:17:30 ***
