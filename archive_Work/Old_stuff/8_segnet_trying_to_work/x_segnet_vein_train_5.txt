/shared/home/matevz.vidovic/Diplomska/Prototip/Delo/model_wrapper.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.model = torch.load(self.prev_model_path, map_location=torch.device(device))
segnet_main.py do_log: False
Log file name: log_21_12-58-46_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(sd='segnet_vein_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_segnet/segnet_vein.yaml', yo=None, ntibp=None, ptp=None, map=None)
YAML: {'is_pruning_ready': False, 'path_to_data': './Data/vein_and_sclera_data', 'target': 'veins', 'train_epoch_size': 300, 'val_epoch_size': 100, 'test_epoch_size': 100, 'train_batch_size': 3, 'eval_batch_size': 12, 'learning_rate': 0.0001, 'num_of_dataloader_workers': 32, 'cleanup_k': 3, 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': False, 'loss_fn_name': 'MCDL', 'loss_params': None, 'dataset_type': 'vasd', 'aug_type': 'tf', 'zero_out_non_sclera': True, 'add_sclera_to_img': False, 'add_bcosfire_to_img': True, 'add_coye_to_img': True, 'model': '64_2_6', 'input_width': 2048, 'input_height': 1024, 'input_channels': 5, 'output_channels': 2, 'have_patchification': False, 'patchification_params': 'None', 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq', 'conv2d_prune_limit': 0.2}
Validation phase: False
Namespace(sd='segnet_vein_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_segnet/segnet_vein.yaml', yo=None, ntibp=None, ptp=None, map=None)
Device: cuda
dataset.py do_log: False
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 88
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 88
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 100
val dataloader num of batches: 3
test dataloader num of batches: 1
Loaded model path:  ./segnet_vein_train/saved_model_wrapper/models/SegNet_3_.pth
per-ex loss: 0.808000  [    3/  300]
per-ex loss: 0.810496  [    6/  300]
per-ex loss: 0.732588  [    9/  300]
per-ex loss: 0.727601  [   12/  300]
per-ex loss: 0.665154  [   15/  300]
per-ex loss: 0.809327  [   18/  300]
per-ex loss: 0.803372  [   21/  300]
per-ex loss: 0.731991  [   24/  300]
per-ex loss: 0.786639  [   27/  300]
per-ex loss: 0.681800  [   30/  300]
per-ex loss: 0.867768  [   33/  300]
per-ex loss: 0.694274  [   36/  300]
per-ex loss: 0.703831  [   39/  300]
per-ex loss: 0.705518  [   42/  300]
per-ex loss: 0.613405  [   45/  300]
per-ex loss: 0.769994  [   48/  300]
per-ex loss: 0.727385  [   51/  300]
per-ex loss: 0.750775  [   54/  300]
per-ex loss: 0.788232  [   57/  300]
per-ex loss: 0.736802  [   60/  300]
per-ex loss: 0.764327  [   63/  300]
per-ex loss: 0.842762  [   66/  300]
per-ex loss: 0.664702  [   69/  300]
per-ex loss: 0.701862  [   72/  300]
per-ex loss: 0.747601  [   75/  300]
per-ex loss: 0.746541  [   78/  300]
per-ex loss: 0.780635  [   81/  300]
per-ex loss: 0.702975  [   84/  300]
per-ex loss: 0.624587  [   87/  300]
per-ex loss: 0.710997  [   90/  300]
per-ex loss: 0.690766  [   93/  300]
per-ex loss: 0.654041  [   96/  300]
per-ex loss: 0.746303  [   99/  300]
per-ex loss: 0.628622  [  102/  300]
per-ex loss: 0.806394  [  105/  300]
per-ex loss: 0.698747  [  108/  300]
per-ex loss: 0.812184  [  111/  300]
per-ex loss: 0.779606  [  114/  300]
per-ex loss: 0.752076  [  117/  300]
per-ex loss: 0.593660  [  120/  300]
per-ex loss: 0.688946  [  123/  300]
per-ex loss: 0.751324  [  126/  300]
per-ex loss: 0.762817  [  129/  300]
per-ex loss: 0.635460  [  132/  300]
per-ex loss: 0.677700  [  135/  300]
per-ex loss: 0.635450  [  138/  300]
per-ex loss: 0.692318  [  141/  300]
per-ex loss: 0.784781  [  144/  300]
per-ex loss: 0.563769  [  147/  300]
per-ex loss: 0.797308  [  150/  300]
per-ex loss: 0.602169  [  153/  300]
per-ex loss: 0.616157  [  156/  300]
per-ex loss: 0.681443  [  159/  300]
per-ex loss: 0.752749  [  162/  300]
per-ex loss: 0.589126  [  165/  300]
per-ex loss: 0.558260  [  168/  300]
per-ex loss: 0.826818  [  171/  300]
per-ex loss: 0.603339  [  174/  300]
per-ex loss: 0.842272  [  177/  300]
per-ex loss: 0.769870  [  180/  300]
per-ex loss: 0.689602  [  183/  300]
per-ex loss: 0.583519  [  186/  300]
per-ex loss: 0.707363  [  189/  300]
per-ex loss: 0.736578  [  192/  300]
per-ex loss: 0.603232  [  195/  300]
per-ex loss: 0.636534  [  198/  300]
per-ex loss: 0.606014  [  201/  300]
per-ex loss: 0.716602  [  204/  300]
per-ex loss: 0.745711  [  207/  300]
per-ex loss: 0.671287  [  210/  300]
per-ex loss: 0.749937  [  213/  300]
per-ex loss: 0.519993  [  216/  300]
per-ex loss: 0.615821  [  219/  300]
per-ex loss: 0.678315  [  222/  300]
per-ex loss: 0.529180  [  225/  300]
per-ex loss: 0.597593  [  228/  300]
per-ex loss: 0.691051  [  231/  300]
per-ex loss: 0.737588  [  234/  300]
per-ex loss: 0.764465  [  237/  300]
per-ex loss: 0.564644  [  240/  300]
per-ex loss: 0.726098  [  243/  300]
per-ex loss: 0.597488  [  246/  300]
per-ex loss: 0.748334  [  249/  300]
per-ex loss: 0.577674  [  252/  300]
per-ex loss: 0.619068  [  255/  300]
per-ex loss: 0.556680  [  258/  300]
per-ex loss: 0.527683  [  261/  300]
per-ex loss: 0.643037  [  264/  300]
per-ex loss: 0.697822  [  267/  300]
per-ex loss: 0.552888  [  270/  300]
per-ex loss: 0.594099  [  273/  300]
per-ex loss: 0.626119  [  276/  300]
per-ex loss: 0.659666  [  279/  300]
per-ex loss: 0.720633  [  282/  300]
per-ex loss: 0.742219  [  285/  300]
per-ex loss: 0.562084  [  288/  300]
per-ex loss: 0.617063  [  291/  300]
per-ex loss: 0.547970  [  294/  300]
per-ex loss: 0.578257  [  297/  300]
per-ex loss: 0.803473  [  300/  300]
Train Error: Avg loss: 0.69039797
validation Error: 
 Avg loss: 0.66015339 
 F1: 0.308628 
 Precision: 0.196094 
 Recall: 0.724273
 IoU: 0.182472

test Error: 
 Avg loss: 0.64839303 
 F1: 0.365206 
 Precision: 0.242207 
 Recall: 0.742033
 IoU: 0.223396

We have finished training iteration 4
per-ex loss: 0.718090  [    3/  300]
per-ex loss: 0.748469  [    6/  300]
per-ex loss: 0.645682  [    9/  300]
per-ex loss: 0.735247  [   12/  300]
per-ex loss: 0.761990  [   15/  300]
per-ex loss: 0.499646  [   18/  300]
per-ex loss: 0.653734  [   21/  300]
per-ex loss: 0.642856  [   24/  300]
per-ex loss: 0.682478  [   27/  300]
per-ex loss: 0.624781  [   30/  300]
per-ex loss: 0.615170  [   33/  300]
per-ex loss: 0.710033  [   36/  300]
per-ex loss: 0.782051  [   39/  300]
per-ex loss: 0.723901  [   42/  300]
per-ex loss: 0.692400  [   45/  300]
per-ex loss: 0.584269  [   48/  300]
per-ex loss: 0.690979  [   51/  300]
per-ex loss: 0.620649  [   54/  300]
per-ex loss: 0.731691  [   57/  300]
per-ex loss: 0.613893  [   60/  300]
per-ex loss: 0.670086  [   63/  300]
per-ex loss: 0.588109  [   66/  300]
per-ex loss: 0.538304  [   69/  300]
per-ex loss: 0.664365  [   72/  300]
per-ex loss: 0.584808  [   75/  300]
per-ex loss: 0.606419  [   78/  300]
per-ex loss: 0.544316  [   81/  300]
per-ex loss: 0.624198  [   84/  300]
per-ex loss: 0.784944  [   87/  300]
per-ex loss: 0.572821  [   90/  300]
per-ex loss: 0.616464  [   93/  300]
per-ex loss: 0.631051  [   96/  300]
per-ex loss: 0.533695  [   99/  300]
per-ex loss: 0.550087  [  102/  300]
per-ex loss: 0.516096  [  105/  300]
per-ex loss: 0.740072  [  108/  300]
per-ex loss: 0.657129  [  111/  300]
per-ex loss: 0.640055  [  114/  300]
per-ex loss: 0.641285  [  117/  300]
per-ex loss: 0.620399  [  120/  300]
per-ex loss: 0.646672  [  123/  300]
per-ex loss: 0.600950  [  126/  300]
per-ex loss: 0.526546  [  129/  300]
per-ex loss: 0.613846  [  132/  300]
per-ex loss: 0.692089  [  135/  300]
per-ex loss: 0.598456  [  138/  300]
per-ex loss: 0.673205  [  141/  300]
per-ex loss: 0.666782  [  144/  300]
per-ex loss: 0.722705  [  147/  300]
per-ex loss: 0.791855  [  150/  300]
per-ex loss: 0.762903  [  153/  300]
per-ex loss: 0.486172  [  156/  300]
per-ex loss: 0.591451  [  159/  300]
per-ex loss: 0.657022  [  162/  300]
per-ex loss: 0.625132  [  165/  300]
per-ex loss: 0.552509  [  168/  300]
per-ex loss: 0.659877  [  171/  300]
per-ex loss: 0.673941  [  174/  300]
per-ex loss: 0.648449  [  177/  300]
per-ex loss: 0.678911  [  180/  300]
per-ex loss: 0.634989  [  183/  300]
per-ex loss: 0.604997  [  186/  300]
per-ex loss: 0.707419  [  189/  300]
per-ex loss: 0.639055  [  192/  300]
per-ex loss: 0.679706  [  195/  300]
per-ex loss: 0.621942  [  198/  300]
per-ex loss: 0.715118  [  201/  300]
per-ex loss: 0.666107  [  204/  300]
per-ex loss: 0.610270  [  207/  300]
per-ex loss: 0.627228  [  210/  300]
per-ex loss: 0.796471  [  213/  300]
per-ex loss: 0.606704  [  216/  300]
per-ex loss: 0.666508  [  219/  300]
per-ex loss: 0.674898  [  222/  300]
per-ex loss: 0.506339  [  225/  300]
per-ex loss: 0.509228  [  228/  300]
per-ex loss: 0.485101  [  231/  300]
per-ex loss: 0.694517  [  234/  300]
per-ex loss: 0.612885  [  237/  300]
per-ex loss: 0.658625  [  240/  300]
per-ex loss: 0.702248  [  243/  300]
per-ex loss: 0.572501  [  246/  300]
per-ex loss: 0.587053  [  249/  300]
per-ex loss: 0.627686  [  252/  300]
per-ex loss: 0.570450  [  255/  300]
per-ex loss: 0.572312  [  258/  300]
per-ex loss: 0.606904  [  261/  300]
per-ex loss: 0.598318  [  264/  300]
per-ex loss: 0.630759  [  267/  300]
per-ex loss: 0.615704  [  270/  300]
per-ex loss: 0.660379  [  273/  300]
per-ex loss: 0.674615  [  276/  300]
per-ex loss: 0.664291  [  279/  300]
per-ex loss: 0.665351  [  282/  300]
per-ex loss: 0.650053  [  285/  300]
per-ex loss: 0.627605  [  288/  300]
per-ex loss: 0.640121  [  291/  300]
per-ex loss: 0.648264  [  294/  300]
per-ex loss: 0.689949  [  297/  300]
per-ex loss: 0.628918  [  300/  300]
Train Error: Avg loss: 0.64023776
validation Error: 
 Avg loss: 0.67234401 
 F1: 0.353455 
 Precision: 0.423777 
 Recall: 0.303150
 IoU: 0.214665

test Error: 
 Avg loss: 0.61432469 
 F1: 0.393327 
 Precision: 0.554384 
 Recall: 0.304784
 IoU: 0.244809

We have finished training iteration 5
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_1_.pth
per-ex loss: 0.663287  [    3/  300]
per-ex loss: 0.617533  [    6/  300]
per-ex loss: 0.671070  [    9/  300]
per-ex loss: 0.609719  [   12/  300]
per-ex loss: 0.645334  [   15/  300]
per-ex loss: 0.528087  [   18/  300]
per-ex loss: 0.582616  [   21/  300]
per-ex loss: 0.583641  [   24/  300]
per-ex loss: 0.718679  [   27/  300]
per-ex loss: 0.597049  [   30/  300]
per-ex loss: 0.586540  [   33/  300]
per-ex loss: 0.617236  [   36/  300]
per-ex loss: 0.656866  [   39/  300]
per-ex loss: 0.552307  [   42/  300]
per-ex loss: 0.614068  [   45/  300]
per-ex loss: 0.712743  [   48/  300]
per-ex loss: 0.592008  [   51/  300]
per-ex loss: 0.665851  [   54/  300]
per-ex loss: 0.572364  [   57/  300]
per-ex loss: 0.759740  [   60/  300]
per-ex loss: 0.725859  [   63/  300]
per-ex loss: 0.506773  [   66/  300]
per-ex loss: 0.744861  [   69/  300]
per-ex loss: 0.721057  [   72/  300]
per-ex loss: 0.539846  [   75/  300]
per-ex loss: 0.596127  [   78/  300]
per-ex loss: 0.544466  [   81/  300]
per-ex loss: 0.491895  [   84/  300]
per-ex loss: 0.636248  [   87/  300]
per-ex loss: 0.616490  [   90/  300]
per-ex loss: 0.665598  [   93/  300]
per-ex loss: 0.574029  [   96/  300]
per-ex loss: 0.654705  [   99/  300]
per-ex loss: 0.521754  [  102/  300]
per-ex loss: 0.524728  [  105/  300]
per-ex loss: 0.499460  [  108/  300]
per-ex loss: 0.525914  [  111/  300]
per-ex loss: 0.553117  [  114/  300]
per-ex loss: 0.516128  [  117/  300]
per-ex loss: 0.680885  [  120/  300]
per-ex loss: 0.557621  [  123/  300]
per-ex loss: 0.607535  [  126/  300]
per-ex loss: 0.601285  [  129/  300]
per-ex loss: 0.631864  [  132/  300]
per-ex loss: 0.652207  [  135/  300]
per-ex loss: 0.658301  [  138/  300]
per-ex loss: 0.651874  [  141/  300]
per-ex loss: 0.558558  [  144/  300]
per-ex loss: 0.651634  [  147/  300]
per-ex loss: 0.592521  [  150/  300]
per-ex loss: 0.654985  [  153/  300]
per-ex loss: 0.573920  [  156/  300]
per-ex loss: 0.609437  [  159/  300]
per-ex loss: 0.707698  [  162/  300]
per-ex loss: 0.805731  [  165/  300]
per-ex loss: 0.660739  [  168/  300]
per-ex loss: 0.659041  [  171/  300]
per-ex loss: 0.662408  [  174/  300]
per-ex loss: 0.593057  [  177/  300]
per-ex loss: 0.707413  [  180/  300]
per-ex loss: 0.641808  [  183/  300]
per-ex loss: 0.557929  [  186/  300]
per-ex loss: 0.644490  [  189/  300]
per-ex loss: 0.664447  [  192/  300]
per-ex loss: 0.515328  [  195/  300]
per-ex loss: 0.519904  [  198/  300]
per-ex loss: 0.622211  [  201/  300]
per-ex loss: 0.556672  [  204/  300]
per-ex loss: 0.548284  [  207/  300]
per-ex loss: 0.576496  [  210/  300]
per-ex loss: 0.662584  [  213/  300]
per-ex loss: 0.602851  [  216/  300]
per-ex loss: 0.512968  [  219/  300]
per-ex loss: 0.637152  [  222/  300]
per-ex loss: 0.558024  [  225/  300]
per-ex loss: 0.723757  [  228/  300]
per-ex loss: 0.517929  [  231/  300]
per-ex loss: 0.775656  [  234/  300]
per-ex loss: 0.682983  [  237/  300]
per-ex loss: 0.660793  [  240/  300]
per-ex loss: 0.535851  [  243/  300]
per-ex loss: 0.688863  [  246/  300]
per-ex loss: 0.720353  [  249/  300]
per-ex loss: 0.547676  [  252/  300]
per-ex loss: 0.489304  [  255/  300]
per-ex loss: 0.695658  [  258/  300]
per-ex loss: 0.790514  [  261/  300]
per-ex loss: 0.644987  [  264/  300]
per-ex loss: 0.665162  [  267/  300]
per-ex loss: 0.569999  [  270/  300]
per-ex loss: 0.547297  [  273/  300]
per-ex loss: 0.510739  [  276/  300]
per-ex loss: 0.749701  [  279/  300]
per-ex loss: 0.665377  [  282/  300]
per-ex loss: 0.518306  [  285/  300]
per-ex loss: 0.611980  [  288/  300]
per-ex loss: 0.695086  [  291/  300]
per-ex loss: 0.598217  [  294/  300]
per-ex loss: 0.568842  [  297/  300]
per-ex loss: 0.700480  [  300/  300]
Train Error: Avg loss: 0.61881161
validation Error: 
 Avg loss: 0.60244022 
 F1: 0.407418 
 Precision: 0.373188 
 Recall: 0.448562
 IoU: 0.255822

test Error: 
 Avg loss: 0.54026699 
 F1: 0.466436 
 Precision: 0.470183 
 Recall: 0.462748
 IoU: 0.304151

We have finished training iteration 6
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_2_.pth
per-ex loss: 0.541421  [    3/  300]
per-ex loss: 0.581647  [    6/  300]
per-ex loss: 0.609326  [    9/  300]
per-ex loss: 0.658760  [   12/  300]
per-ex loss: 0.576293  [   15/  300]
per-ex loss: 0.516497  [   18/  300]
per-ex loss: 0.624982  [   21/  300]
per-ex loss: 0.546427  [   24/  300]
per-ex loss: 0.618753  [   27/  300]
per-ex loss: 0.579820  [   30/  300]
per-ex loss: 0.491766  [   33/  300]
per-ex loss: 0.615736  [   36/  300]
per-ex loss: 0.588299  [   39/  300]
per-ex loss: 0.569133  [   42/  300]
per-ex loss: 0.622843  [   45/  300]
per-ex loss: 0.497007  [   48/  300]
per-ex loss: 0.740590  [   51/  300]
per-ex loss: 0.611677  [   54/  300]
per-ex loss: 0.603176  [   57/  300]
per-ex loss: 0.659335  [   60/  300]
per-ex loss: 0.531586  [   63/  300]
per-ex loss: 0.649281  [   66/  300]
per-ex loss: 0.507049  [   69/  300]
per-ex loss: 0.699457  [   72/  300]
per-ex loss: 0.615046  [   75/  300]
per-ex loss: 0.712735  [   78/  300]
per-ex loss: 0.527790  [   81/  300]
per-ex loss: 0.663159  [   84/  300]
per-ex loss: 0.599047  [   87/  300]
per-ex loss: 0.534668  [   90/  300]
per-ex loss: 0.614710  [   93/  300]
per-ex loss: 0.522264  [   96/  300]
per-ex loss: 0.615833  [   99/  300]
per-ex loss: 0.616560  [  102/  300]
per-ex loss: 0.567792  [  105/  300]
per-ex loss: 0.638193  [  108/  300]
per-ex loss: 0.665823  [  111/  300]
per-ex loss: 0.542935  [  114/  300]
per-ex loss: 0.649663  [  117/  300]
per-ex loss: 0.639826  [  120/  300]
per-ex loss: 0.634864  [  123/  300]
per-ex loss: 0.592633  [  126/  300]
per-ex loss: 0.608195  [  129/  300]
per-ex loss: 0.732670  [  132/  300]
per-ex loss: 0.535216  [  135/  300]
per-ex loss: 0.623884  [  138/  300]
per-ex loss: 0.684634  [  141/  300]
per-ex loss: 0.534174  [  144/  300]
per-ex loss: 0.735081  [  147/  300]
per-ex loss: 0.626273  [  150/  300]
per-ex loss: 0.584487  [  153/  300]
per-ex loss: 0.510950  [  156/  300]
per-ex loss: 0.608572  [  159/  300]
per-ex loss: 0.645236  [  162/  300]
per-ex loss: 0.533389  [  165/  300]
per-ex loss: 0.619090  [  168/  300]
per-ex loss: 0.549731  [  171/  300]
per-ex loss: 0.556642  [  174/  300]
per-ex loss: 0.661116  [  177/  300]
per-ex loss: 0.569680  [  180/  300]
per-ex loss: 0.576808  [  183/  300]
per-ex loss: 0.642381  [  186/  300]
per-ex loss: 0.667679  [  189/  300]
per-ex loss: 0.543598  [  192/  300]
per-ex loss: 0.707345  [  195/  300]
per-ex loss: 0.562960  [  198/  300]
per-ex loss: 0.676142  [  201/  300]
per-ex loss: 0.534143  [  204/  300]
per-ex loss: 0.673957  [  207/  300]
per-ex loss: 0.600390  [  210/  300]
per-ex loss: 0.487191  [  213/  300]
per-ex loss: 0.504793  [  216/  300]
per-ex loss: 0.564418  [  219/  300]
per-ex loss: 0.536905  [  222/  300]
per-ex loss: 0.612769  [  225/  300]
per-ex loss: 0.532897  [  228/  300]
per-ex loss: 0.616327  [  231/  300]
per-ex loss: 0.577921  [  234/  300]
per-ex loss: 0.676311  [  237/  300]
per-ex loss: 0.775143  [  240/  300]
per-ex loss: 0.660843  [  243/  300]
per-ex loss: 0.552045  [  246/  300]
per-ex loss: 0.733666  [  249/  300]
per-ex loss: 0.611798  [  252/  300]
per-ex loss: 0.682481  [  255/  300]
per-ex loss: 0.513500  [  258/  300]
per-ex loss: 0.679046  [  261/  300]
per-ex loss: 0.614297  [  264/  300]
per-ex loss: 0.588189  [  267/  300]
per-ex loss: 0.536631  [  270/  300]
per-ex loss: 0.677128  [  273/  300]
per-ex loss: 0.520945  [  276/  300]
per-ex loss: 0.606480  [  279/  300]
per-ex loss: 0.760430  [  282/  300]
per-ex loss: 0.543295  [  285/  300]
per-ex loss: 0.543456  [  288/  300]
per-ex loss: 0.496239  [  291/  300]
per-ex loss: 0.495662  [  294/  300]
per-ex loss: 0.750718  [  297/  300]
per-ex loss: 0.712318  [  300/  300]
Train Error: Avg loss: 0.60462659
validation Error: 
 Avg loss: 0.58498367 
 F1: 0.396976 
 Precision: 0.303206 
 Recall: 0.574713
 IoU: 0.247642

test Error: 
 Avg loss: 0.53302151 
 F1: 0.474465 
 Precision: 0.390858 
 Recall: 0.603573
 IoU: 0.311015

We have finished training iteration 7
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_3_.pth
per-ex loss: 0.767959  [    3/  300]
per-ex loss: 0.673995  [    6/  300]
per-ex loss: 0.601608  [    9/  300]
per-ex loss: 0.540497  [   12/  300]
per-ex loss: 0.628967  [   15/  300]
per-ex loss: 0.524110  [   18/  300]
per-ex loss: 0.629068  [   21/  300]
per-ex loss: 0.709410  [   24/  300]
per-ex loss: 0.757893  [   27/  300]
per-ex loss: 0.533136  [   30/  300]
per-ex loss: 0.574126  [   33/  300]
per-ex loss: 0.552781  [   36/  300]
per-ex loss: 0.490602  [   39/  300]
per-ex loss: 0.747608  [   42/  300]
per-ex loss: 0.558738  [   45/  300]
per-ex loss: 0.540701  [   48/  300]
per-ex loss: 0.655186  [   51/  300]
per-ex loss: 0.525242  [   54/  300]
per-ex loss: 0.605613  [   57/  300]
per-ex loss: 0.561851  [   60/  300]
per-ex loss: 0.638204  [   63/  300]
per-ex loss: 0.579034  [   66/  300]
per-ex loss: 0.628069  [   69/  300]
per-ex loss: 0.777341  [   72/  300]
per-ex loss: 0.527544  [   75/  300]
per-ex loss: 0.472889  [   78/  300]
per-ex loss: 0.561784  [   81/  300]
per-ex loss: 0.490752  [   84/  300]
per-ex loss: 0.691189  [   87/  300]
per-ex loss: 0.677849  [   90/  300]
per-ex loss: 0.601767  [   93/  300]
per-ex loss: 0.584405  [   96/  300]
per-ex loss: 0.798528  [   99/  300]
per-ex loss: 0.490578  [  102/  300]
per-ex loss: 0.495339  [  105/  300]
per-ex loss: 0.581593  [  108/  300]
per-ex loss: 0.488576  [  111/  300]
per-ex loss: 0.569888  [  114/  300]
per-ex loss: 0.546219  [  117/  300]
per-ex loss: 0.601520  [  120/  300]
per-ex loss: 0.524179  [  123/  300]
per-ex loss: 0.560900  [  126/  300]
per-ex loss: 0.539659  [  129/  300]
per-ex loss: 0.589967  [  132/  300]
per-ex loss: 0.595973  [  135/  300]
per-ex loss: 0.649987  [  138/  300]
per-ex loss: 0.566512  [  141/  300]
per-ex loss: 0.600168  [  144/  300]
per-ex loss: 0.668584  [  147/  300]
per-ex loss: 0.593043  [  150/  300]
per-ex loss: 0.680611  [  153/  300]
per-ex loss: 0.504874  [  156/  300]
per-ex loss: 0.651388  [  159/  300]
per-ex loss: 0.571519  [  162/  300]
per-ex loss: 0.566914  [  165/  300]
per-ex loss: 0.534971  [  168/  300]
per-ex loss: 0.480125  [  171/  300]
per-ex loss: 0.574770  [  174/  300]
per-ex loss: 0.728461  [  177/  300]
per-ex loss: 0.568268  [  180/  300]
per-ex loss: 0.651240  [  183/  300]
per-ex loss: 0.649217  [  186/  300]
per-ex loss: 0.560858  [  189/  300]
per-ex loss: 0.551674  [  192/  300]
per-ex loss: 0.557042  [  195/  300]
per-ex loss: 0.637855  [  198/  300]
per-ex loss: 0.494884  [  201/  300]
per-ex loss: 0.749224  [  204/  300]
per-ex loss: 0.498256  [  207/  300]
per-ex loss: 0.664494  [  210/  300]
per-ex loss: 0.619840  [  213/  300]
per-ex loss: 0.649274  [  216/  300]
per-ex loss: 0.499545  [  219/  300]
per-ex loss: 0.665293  [  222/  300]
per-ex loss: 0.660631  [  225/  300]
per-ex loss: 0.498479  [  228/  300]
per-ex loss: 0.586485  [  231/  300]
per-ex loss: 0.513165  [  234/  300]
per-ex loss: 0.647942  [  237/  300]
per-ex loss: 0.485188  [  240/  300]
per-ex loss: 0.679592  [  243/  300]
per-ex loss: 0.497339  [  246/  300]
per-ex loss: 0.548729  [  249/  300]
per-ex loss: 0.555986  [  252/  300]
per-ex loss: 0.685509  [  255/  300]
per-ex loss: 0.642134  [  258/  300]
per-ex loss: 0.645371  [  261/  300]
per-ex loss: 0.735991  [  264/  300]
per-ex loss: 0.547233  [  267/  300]
per-ex loss: 0.639946  [  270/  300]
per-ex loss: 0.594545  [  273/  300]
per-ex loss: 0.583649  [  276/  300]
per-ex loss: 0.566324  [  279/  300]
per-ex loss: 0.585382  [  282/  300]
per-ex loss: 0.646011  [  285/  300]
per-ex loss: 0.634567  [  288/  300]
per-ex loss: 0.550320  [  291/  300]
per-ex loss: 0.635108  [  294/  300]
per-ex loss: 0.573276  [  297/  300]
per-ex loss: 0.553988  [  300/  300]
Train Error: Avg loss: 0.59674616
validation Error: 
 Avg loss: 0.58100166 
 F1: 0.421701 
 Precision: 0.386552 
 Recall: 0.463881
 IoU: 0.267187

test Error: 
 Avg loss: 0.50932074 
 F1: 0.495410 
 Precision: 0.494320 
 Recall: 0.496505
 IoU: 0.329265

We have finished training iteration 8
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_5_.pth
per-ex loss: 0.646487  [    3/  300]
per-ex loss: 0.657411  [    6/  300]
per-ex loss: 0.482619  [    9/  300]
per-ex loss: 0.501321  [   12/  300]
per-ex loss: 0.535819  [   15/  300]
per-ex loss: 0.621876  [   18/  300]
per-ex loss: 0.582836  [   21/  300]
per-ex loss: 0.750029  [   24/  300]
per-ex loss: 0.576315  [   27/  300]
per-ex loss: 0.622041  [   30/  300]
per-ex loss: 0.501902  [   33/  300]
per-ex loss: 0.614414  [   36/  300]
per-ex loss: 0.534267  [   39/  300]
per-ex loss: 0.600859  [   42/  300]
per-ex loss: 0.703939  [   45/  300]
per-ex loss: 0.549766  [   48/  300]
per-ex loss: 0.614380  [   51/  300]
per-ex loss: 0.634400  [   54/  300]
per-ex loss: 0.524633  [   57/  300]
per-ex loss: 0.817567  [   60/  300]
per-ex loss: 0.555399  [   63/  300]
per-ex loss: 0.593145  [   66/  300]
per-ex loss: 0.542060  [   69/  300]
per-ex loss: 0.673802  [   72/  300]
per-ex loss: 0.528888  [   75/  300]
per-ex loss: 0.546730  [   78/  300]
per-ex loss: 0.644644  [   81/  300]
per-ex loss: 0.581481  [   84/  300]
per-ex loss: 0.505561  [   87/  300]
per-ex loss: 0.708679  [   90/  300]
per-ex loss: 0.593425  [   93/  300]
per-ex loss: 0.687067  [   96/  300]
per-ex loss: 0.572251  [   99/  300]
per-ex loss: 0.489775  [  102/  300]
per-ex loss: 0.593441  [  105/  300]
per-ex loss: 0.557071  [  108/  300]
per-ex loss: 0.493504  [  111/  300]
per-ex loss: 0.671978  [  114/  300]
per-ex loss: 0.517629  [  117/  300]
per-ex loss: 0.556998  [  120/  300]
per-ex loss: 0.619505  [  123/  300]
per-ex loss: 0.556678  [  126/  300]
per-ex loss: 0.717170  [  129/  300]
per-ex loss: 0.654989  [  132/  300]
per-ex loss: 0.653540  [  135/  300]
per-ex loss: 0.514616  [  138/  300]
per-ex loss: 0.627073  [  141/  300]
per-ex loss: 0.455826  [  144/  300]
per-ex loss: 0.688383  [  147/  300]
per-ex loss: 0.569618  [  150/  300]
per-ex loss: 0.535189  [  153/  300]
per-ex loss: 0.575521  [  156/  300]
per-ex loss: 0.574177  [  159/  300]
per-ex loss: 0.685417  [  162/  300]
per-ex loss: 0.658358  [  165/  300]
per-ex loss: 0.633632  [  168/  300]
per-ex loss: 0.639756  [  171/  300]
per-ex loss: 0.519525  [  174/  300]
per-ex loss: 0.668335  [  177/  300]
per-ex loss: 0.522820  [  180/  300]
per-ex loss: 0.582945  [  183/  300]
per-ex loss: 0.596694  [  186/  300]
per-ex loss: 0.515881  [  189/  300]
per-ex loss: 0.556557  [  192/  300]
per-ex loss: 0.648242  [  195/  300]
per-ex loss: 0.567289  [  198/  300]
per-ex loss: 0.657114  [  201/  300]
per-ex loss: 0.556387  [  204/  300]
per-ex loss: 0.609320  [  207/  300]
per-ex loss: 0.761810  [  210/  300]
per-ex loss: 0.533209  [  213/  300]
per-ex loss: 0.588492  [  216/  300]
per-ex loss: 0.529339  [  219/  300]
per-ex loss: 0.552416  [  222/  300]
per-ex loss: 0.488225  [  225/  300]
per-ex loss: 0.536807  [  228/  300]
per-ex loss: 0.515493  [  231/  300]
per-ex loss: 0.642233  [  234/  300]
per-ex loss: 0.660028  [  237/  300]
per-ex loss: 0.760783  [  240/  300]
per-ex loss: 0.534115  [  243/  300]
per-ex loss: 0.669613  [  246/  300]
per-ex loss: 0.531451  [  249/  300]
per-ex loss: 0.561708  [  252/  300]
per-ex loss: 0.606270  [  255/  300]
per-ex loss: 0.699513  [  258/  300]
per-ex loss: 0.507771  [  261/  300]
per-ex loss: 0.537890  [  264/  300]
per-ex loss: 0.555271  [  267/  300]
per-ex loss: 0.565663  [  270/  300]
per-ex loss: 0.696437  [  273/  300]
per-ex loss: 0.509336  [  276/  300]
per-ex loss: 0.526129  [  279/  300]
per-ex loss: 0.537810  [  282/  300]
per-ex loss: 0.471096  [  285/  300]
per-ex loss: 0.535504  [  288/  300]
per-ex loss: 0.600769  [  291/  300]
per-ex loss: 0.610505  [  294/  300]
per-ex loss: 0.562744  [  297/  300]
per-ex loss: 0.540293  [  300/  300]
Train Error: Avg loss: 0.58975688
validation Error: 
 Avg loss: 0.55766479 
 F1: 0.427563 
 Precision: 0.350374 
 Recall: 0.548370
 IoU: 0.271911

test Error: 
 Avg loss: 0.50232613 
 F1: 0.501937 
 Precision: 0.439360 
 Recall: 0.585302
 IoU: 0.335058

We have finished training iteration 9
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_4_.pth
per-ex loss: 0.525144  [    3/  300]
per-ex loss: 0.514741  [    6/  300]
per-ex loss: 0.623155  [    9/  300]
per-ex loss: 0.604466  [   12/  300]
per-ex loss: 0.571342  [   15/  300]
per-ex loss: 0.674353  [   18/  300]
per-ex loss: 0.638912  [   21/  300]
per-ex loss: 0.569695  [   24/  300]
per-ex loss: 0.581390  [   27/  300]
per-ex loss: 0.567904  [   30/  300]
per-ex loss: 0.475877  [   33/  300]
per-ex loss: 0.684675  [   36/  300]
per-ex loss: 0.585524  [   39/  300]
per-ex loss: 0.571933  [   42/  300]
per-ex loss: 0.526963  [   45/  300]
per-ex loss: 0.482859  [   48/  300]
per-ex loss: 0.542499  [   51/  300]
per-ex loss: 0.563584  [   54/  300]
per-ex loss: 0.558683  [   57/  300]
per-ex loss: 0.583234  [   60/  300]
per-ex loss: 0.605720  [   63/  300]
per-ex loss: 0.530493  [   66/  300]
per-ex loss: 0.637932  [   69/  300]
per-ex loss: 0.649820  [   72/  300]
per-ex loss: 0.473959  [   75/  300]
per-ex loss: 0.513807  [   78/  300]
per-ex loss: 0.581698  [   81/  300]
per-ex loss: 0.733877  [   84/  300]
per-ex loss: 0.662382  [   87/  300]
per-ex loss: 0.597699  [   90/  300]
per-ex loss: 0.718473  [   93/  300]
per-ex loss: 0.579239  [   96/  300]
per-ex loss: 0.625251  [   99/  300]
per-ex loss: 0.747618  [  102/  300]
per-ex loss: 0.514073  [  105/  300]
per-ex loss: 0.501355  [  108/  300]
per-ex loss: 0.594812  [  111/  300]
per-ex loss: 0.591112  [  114/  300]
per-ex loss: 0.609901  [  117/  300]
per-ex loss: 0.565222  [  120/  300]
per-ex loss: 0.680618  [  123/  300]
per-ex loss: 0.470941  [  126/  300]
per-ex loss: 0.617137  [  129/  300]
per-ex loss: 0.668336  [  132/  300]
per-ex loss: 0.676826  [  135/  300]
per-ex loss: 0.606505  [  138/  300]
per-ex loss: 0.585608  [  141/  300]
per-ex loss: 0.607732  [  144/  300]
per-ex loss: 0.537633  [  147/  300]
per-ex loss: 0.702361  [  150/  300]
per-ex loss: 0.514057  [  153/  300]
per-ex loss: 0.521833  [  156/  300]
per-ex loss: 0.586927  [  159/  300]
per-ex loss: 0.567876  [  162/  300]
per-ex loss: 0.551686  [  165/  300]
per-ex loss: 0.612025  [  168/  300]
per-ex loss: 0.583859  [  171/  300]
per-ex loss: 0.569946  [  174/  300]
per-ex loss: 0.728440  [  177/  300]
per-ex loss: 0.536990  [  180/  300]
per-ex loss: 0.557796  [  183/  300]
per-ex loss: 0.604224  [  186/  300]
per-ex loss: 0.563455  [  189/  300]
per-ex loss: 0.608669  [  192/  300]
per-ex loss: 0.558778  [  195/  300]
per-ex loss: 0.627835  [  198/  300]
per-ex loss: 0.555315  [  201/  300]
per-ex loss: 0.649181  [  204/  300]
per-ex loss: 0.628092  [  207/  300]
per-ex loss: 0.595240  [  210/  300]
per-ex loss: 0.575641  [  213/  300]
per-ex loss: 0.472771  [  216/  300]
per-ex loss: 0.599216  [  219/  300]
per-ex loss: 0.571696  [  222/  300]
per-ex loss: 0.572135  [  225/  300]
per-ex loss: 0.625632  [  228/  300]
per-ex loss: 0.489865  [  231/  300]
per-ex loss: 0.555676  [  234/  300]
per-ex loss: 0.651077  [  237/  300]
per-ex loss: 0.598016  [  240/  300]
per-ex loss: 0.635673  [  243/  300]
per-ex loss: 0.563804  [  246/  300]
per-ex loss: 0.547939  [  249/  300]
per-ex loss: 0.599888  [  252/  300]
per-ex loss: 0.496989  [  255/  300]
per-ex loss: 0.534365  [  258/  300]
per-ex loss: 0.655430  [  261/  300]
per-ex loss: 0.743051  [  264/  300]
per-ex loss: 0.586264  [  267/  300]
per-ex loss: 0.686769  [  270/  300]
per-ex loss: 0.486898  [  273/  300]
per-ex loss: 0.731292  [  276/  300]
per-ex loss: 0.607228  [  279/  300]
per-ex loss: 0.572440  [  282/  300]
per-ex loss: 0.578017  [  285/  300]
per-ex loss: 0.669742  [  288/  300]
per-ex loss: 0.511003  [  291/  300]
per-ex loss: 0.522366  [  294/  300]
per-ex loss: 0.587969  [  297/  300]
per-ex loss: 0.568383  [  300/  300]
Train Error: Avg loss: 0.58954535
validation Error: 
 Avg loss: 0.56122390 
 F1: 0.425989 
 Precision: 0.339465 
 Recall: 0.571706
 IoU: 0.270639

test Error: 
 Avg loss: 0.51630723 
 F1: 0.488653 
 Precision: 0.424989 
 Recall: 0.574752
 IoU: 0.323323

We have finished training iteration 10
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_6_.pth
per-ex loss: 0.701058  [    3/  300]
per-ex loss: 0.454365  [    6/  300]
per-ex loss: 0.557865  [    9/  300]
per-ex loss: 0.470099  [   12/  300]
per-ex loss: 0.647849  [   15/  300]
per-ex loss: 0.467698  [   18/  300]
per-ex loss: 0.525267  [   21/  300]
per-ex loss: 0.708930  [   24/  300]
per-ex loss: 0.609535  [   27/  300]
per-ex loss: 0.582068  [   30/  300]
per-ex loss: 0.609168  [   33/  300]
per-ex loss: 0.693679  [   36/  300]
per-ex loss: 0.513245  [   39/  300]
per-ex loss: 0.648806  [   42/  300]
per-ex loss: 0.504731  [   45/  300]
per-ex loss: 0.522423  [   48/  300]
per-ex loss: 0.736219  [   51/  300]
per-ex loss: 0.606395  [   54/  300]
per-ex loss: 0.648157  [   57/  300]
per-ex loss: 0.665488  [   60/  300]
per-ex loss: 0.572213  [   63/  300]
per-ex loss: 0.509336  [   66/  300]
per-ex loss: 0.512865  [   69/  300]
per-ex loss: 0.659349  [   72/  300]
per-ex loss: 0.627973  [   75/  300]
per-ex loss: 0.599107  [   78/  300]
per-ex loss: 0.503330  [   81/  300]
per-ex loss: 0.634219  [   84/  300]
per-ex loss: 0.577833  [   87/  300]
per-ex loss: 0.655058  [   90/  300]
per-ex loss: 0.646759  [   93/  300]
per-ex loss: 0.594611  [   96/  300]
per-ex loss: 0.576821  [   99/  300]
per-ex loss: 0.549553  [  102/  300]
per-ex loss: 0.579312  [  105/  300]
per-ex loss: 0.514578  [  108/  300]
per-ex loss: 0.702917  [  111/  300]
per-ex loss: 0.538013  [  114/  300]
per-ex loss: 0.735608  [  117/  300]
per-ex loss: 0.479940  [  120/  300]
per-ex loss: 0.659650  [  123/  300]
per-ex loss: 0.685538  [  126/  300]
per-ex loss: 0.710671  [  129/  300]
per-ex loss: 0.595042  [  132/  300]
per-ex loss: 0.640962  [  135/  300]
per-ex loss: 0.497478  [  138/  300]
per-ex loss: 0.726432  [  141/  300]
per-ex loss: 0.687510  [  144/  300]
per-ex loss: 0.586765  [  147/  300]
per-ex loss: 0.508066  [  150/  300]
per-ex loss: 0.587296  [  153/  300]
per-ex loss: 0.544727  [  156/  300]
per-ex loss: 0.631359  [  159/  300]
per-ex loss: 0.475456  [  162/  300]
per-ex loss: 0.708014  [  165/  300]
per-ex loss: 0.449142  [  168/  300]
per-ex loss: 0.546708  [  171/  300]
per-ex loss: 0.526617  [  174/  300]
per-ex loss: 0.542739  [  177/  300]
per-ex loss: 0.675762  [  180/  300]
per-ex loss: 0.546049  [  183/  300]
per-ex loss: 0.480989  [  186/  300]
per-ex loss: 0.577019  [  189/  300]
per-ex loss: 0.492137  [  192/  300]
per-ex loss: 0.634862  [  195/  300]
per-ex loss: 0.496427  [  198/  300]
per-ex loss: 0.716011  [  201/  300]
per-ex loss: 0.552039  [  204/  300]
per-ex loss: 0.645033  [  207/  300]
per-ex loss: 0.711466  [  210/  300]
per-ex loss: 0.556635  [  213/  300]
per-ex loss: 0.590201  [  216/  300]
per-ex loss: 0.594986  [  219/  300]
per-ex loss: 0.694479  [  222/  300]
per-ex loss: 0.537977  [  225/  300]
per-ex loss: 0.585690  [  228/  300]
per-ex loss: 0.501139  [  231/  300]
per-ex loss: 0.593563  [  234/  300]
per-ex loss: 0.633956  [  237/  300]
per-ex loss: 0.764879  [  240/  300]
per-ex loss: 0.496008  [  243/  300]
per-ex loss: 0.465306  [  246/  300]
per-ex loss: 0.599896  [  249/  300]
per-ex loss: 0.545005  [  252/  300]
per-ex loss: 0.555606  [  255/  300]
per-ex loss: 0.543275  [  258/  300]
per-ex loss: 0.644023  [  261/  300]
per-ex loss: 0.518416  [  264/  300]
per-ex loss: 0.608317  [  267/  300]
per-ex loss: 0.502071  [  270/  300]
per-ex loss: 0.613557  [  273/  300]
per-ex loss: 0.514396  [  276/  300]
per-ex loss: 0.567101  [  279/  300]
per-ex loss: 0.530280  [  282/  300]
per-ex loss: 0.575183  [  285/  300]
per-ex loss: 0.626139  [  288/  300]
per-ex loss: 0.468129  [  291/  300]
per-ex loss: 0.562261  [  294/  300]
per-ex loss: 0.518144  [  297/  300]
per-ex loss: 0.692670  [  300/  300]
Train Error: Avg loss: 0.58679695
validation Error: 
 Avg loss: 0.54976916 
 F1: 0.423228 
 Precision: 0.328166 
 Recall: 0.595823
 IoU: 0.268414

test Error: 
 Avg loss: 0.50015473 
 F1: 0.504093 
 Precision: 0.424817 
 Recall: 0.619745
 IoU: 0.336982

We have finished training iteration 11
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_7_.pth
per-ex loss: 0.471301  [    3/  300]
per-ex loss: 0.469718  [    6/  300]
per-ex loss: 0.548098  [    9/  300]
per-ex loss: 0.507788  [   12/  300]
per-ex loss: 0.580636  [   15/  300]
per-ex loss: 0.565397  [   18/  300]
per-ex loss: 0.596378  [   21/  300]
per-ex loss: 0.690490  [   24/  300]
per-ex loss: 0.692358  [   27/  300]
per-ex loss: 0.573220  [   30/  300]
per-ex loss: 0.595585  [   33/  300]
per-ex loss: 0.636984  [   36/  300]
per-ex loss: 0.558878  [   39/  300]
per-ex loss: 0.523062  [   42/  300]
per-ex loss: 0.578450  [   45/  300]
per-ex loss: 0.479427  [   48/  300]
per-ex loss: 0.628994  [   51/  300]
per-ex loss: 0.698130  [   54/  300]
per-ex loss: 0.644826  [   57/  300]
per-ex loss: 0.654030  [   60/  300]
per-ex loss: 0.534693  [   63/  300]
per-ex loss: 0.540786  [   66/  300]
per-ex loss: 0.646065  [   69/  300]
per-ex loss: 0.662107  [   72/  300]
per-ex loss: 0.509567  [   75/  300]
per-ex loss: 0.621587  [   78/  300]
per-ex loss: 0.592621  [   81/  300]
per-ex loss: 0.584808  [   84/  300]
per-ex loss: 0.515268  [   87/  300]
per-ex loss: 0.503031  [   90/  300]
per-ex loss: 0.691218  [   93/  300]
per-ex loss: 0.442115  [   96/  300]
per-ex loss: 0.583771  [   99/  300]
per-ex loss: 0.556034  [  102/  300]
per-ex loss: 0.474708  [  105/  300]
per-ex loss: 0.603575  [  108/  300]
per-ex loss: 0.663021  [  111/  300]
per-ex loss: 0.506401  [  114/  300]
per-ex loss: 0.650316  [  117/  300]
per-ex loss: 0.638327  [  120/  300]
per-ex loss: 0.496496  [  123/  300]
per-ex loss: 0.513090  [  126/  300]
per-ex loss: 0.617080  [  129/  300]
per-ex loss: 0.563003  [  132/  300]
per-ex loss: 0.509363  [  135/  300]
per-ex loss: 0.737343  [  138/  300]
per-ex loss: 0.691550  [  141/  300]
per-ex loss: 0.604549  [  144/  300]
per-ex loss: 0.546530  [  147/  300]
per-ex loss: 0.531218  [  150/  300]
per-ex loss: 0.625119  [  153/  300]
per-ex loss: 0.622620  [  156/  300]
per-ex loss: 0.644909  [  159/  300]
per-ex loss: 0.579828  [  162/  300]
per-ex loss: 0.714201  [  165/  300]
per-ex loss: 0.622801  [  168/  300]
per-ex loss: 0.572480  [  171/  300]
per-ex loss: 0.618120  [  174/  300]
per-ex loss: 0.537955  [  177/  300]
per-ex loss: 0.593996  [  180/  300]
per-ex loss: 0.518208  [  183/  300]
per-ex loss: 0.543829  [  186/  300]
per-ex loss: 0.515317  [  189/  300]
per-ex loss: 0.628111  [  192/  300]
per-ex loss: 0.601589  [  195/  300]
per-ex loss: 0.520667  [  198/  300]
per-ex loss: 0.705961  [  201/  300]
per-ex loss: 0.597491  [  204/  300]
per-ex loss: 0.559975  [  207/  300]
per-ex loss: 0.605702  [  210/  300]
per-ex loss: 0.699702  [  213/  300]
per-ex loss: 0.683257  [  216/  300]
per-ex loss: 0.521798  [  219/  300]
per-ex loss: 0.699219  [  222/  300]
per-ex loss: 0.520854  [  225/  300]
per-ex loss: 0.510840  [  228/  300]
per-ex loss: 0.579499  [  231/  300]
per-ex loss: 0.619876  [  234/  300]
per-ex loss: 0.507858  [  237/  300]
per-ex loss: 0.661266  [  240/  300]
per-ex loss: 0.563269  [  243/  300]
per-ex loss: 0.590486  [  246/  300]
per-ex loss: 0.623088  [  249/  300]
per-ex loss: 0.513884  [  252/  300]
per-ex loss: 0.497816  [  255/  300]
per-ex loss: 0.610056  [  258/  300]
per-ex loss: 0.552885  [  261/  300]
per-ex loss: 0.653913  [  264/  300]
per-ex loss: 0.660488  [  267/  300]
per-ex loss: 0.536349  [  270/  300]
per-ex loss: 0.485092  [  273/  300]
per-ex loss: 0.563669  [  276/  300]
per-ex loss: 0.519317  [  279/  300]
per-ex loss: 0.593369  [  282/  300]
per-ex loss: 0.506363  [  285/  300]
per-ex loss: 0.454461  [  288/  300]
per-ex loss: 0.636937  [  291/  300]
per-ex loss: 0.604261  [  294/  300]
per-ex loss: 0.628242  [  297/  300]
per-ex loss: 0.505091  [  300/  300]
Train Error: Avg loss: 0.58256076
validation Error: 
 Avg loss: 0.56248758 
 F1: 0.416036 
 Precision: 0.315778 
 Recall: 0.609572
 IoU: 0.262655

test Error: 
 Avg loss: 0.53020787 
 F1: 0.474650 
 Precision: 0.378411 
 Recall: 0.636534
 IoU: 0.311174

We have finished training iteration 12
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_8_.pth
per-ex loss: 0.535031  [    3/  300]
per-ex loss: 0.636958  [    6/  300]
per-ex loss: 0.654182  [    9/  300]
per-ex loss: 0.542741  [   12/  300]
per-ex loss: 0.701180  [   15/  300]
per-ex loss: 0.597263  [   18/  300]
per-ex loss: 0.556216  [   21/  300]
per-ex loss: 0.630297  [   24/  300]
per-ex loss: 0.533735  [   27/  300]
per-ex loss: 0.556913  [   30/  300]
per-ex loss: 0.631705  [   33/  300]
per-ex loss: 0.521079  [   36/  300]
per-ex loss: 0.549235  [   39/  300]
per-ex loss: 0.474820  [   42/  300]
per-ex loss: 0.585836  [   45/  300]
per-ex loss: 0.593020  [   48/  300]
per-ex loss: 0.564753  [   51/  300]
per-ex loss: 0.450160  [   54/  300]
per-ex loss: 0.599161  [   57/  300]
per-ex loss: 0.608107  [   60/  300]
per-ex loss: 0.605551  [   63/  300]
per-ex loss: 0.663506  [   66/  300]
per-ex loss: 0.523689  [   69/  300]
per-ex loss: 0.494304  [   72/  300]
per-ex loss: 0.609883  [   75/  300]
per-ex loss: 0.662228  [   78/  300]
per-ex loss: 0.658105  [   81/  300]
per-ex loss: 0.523772  [   84/  300]
per-ex loss: 0.705759  [   87/  300]
per-ex loss: 0.606542  [   90/  300]
per-ex loss: 0.476970  [   93/  300]
per-ex loss: 0.554817  [   96/  300]
per-ex loss: 0.620608  [   99/  300]
per-ex loss: 0.575551  [  102/  300]
per-ex loss: 0.708519  [  105/  300]
per-ex loss: 0.559105  [  108/  300]
per-ex loss: 0.585684  [  111/  300]
per-ex loss: 0.698546  [  114/  300]
per-ex loss: 0.643739  [  117/  300]
per-ex loss: 0.714842  [  120/  300]
per-ex loss: 0.498165  [  123/  300]
per-ex loss: 0.604501  [  126/  300]
per-ex loss: 0.590277  [  129/  300]
per-ex loss: 0.563928  [  132/  300]
per-ex loss: 0.553437  [  135/  300]
per-ex loss: 0.563167  [  138/  300]
per-ex loss: 0.478992  [  141/  300]
per-ex loss: 0.531143  [  144/  300]
per-ex loss: 0.522146  [  147/  300]
per-ex loss: 0.640630  [  150/  300]
per-ex loss: 0.643033  [  153/  300]
per-ex loss: 0.621483  [  156/  300]
per-ex loss: 0.666256  [  159/  300]
per-ex loss: 0.512301  [  162/  300]
per-ex loss: 0.752455  [  165/  300]
per-ex loss: 0.731710  [  168/  300]
per-ex loss: 0.643654  [  171/  300]
per-ex loss: 0.508569  [  174/  300]
per-ex loss: 0.610847  [  177/  300]
per-ex loss: 0.509510  [  180/  300]
per-ex loss: 0.678436  [  183/  300]
per-ex loss: 0.603755  [  186/  300]
per-ex loss: 0.500664  [  189/  300]
per-ex loss: 0.591449  [  192/  300]
per-ex loss: 0.702622  [  195/  300]
per-ex loss: 0.544591  [  198/  300]
per-ex loss: 0.642981  [  201/  300]
per-ex loss: 0.690231  [  204/  300]
per-ex loss: 0.546709  [  207/  300]
per-ex loss: 0.654975  [  210/  300]
per-ex loss: 0.682900  [  213/  300]
per-ex loss: 0.556782  [  216/  300]
per-ex loss: 0.594506  [  219/  300]
per-ex loss: 0.628826  [  222/  300]
per-ex loss: 0.578313  [  225/  300]
per-ex loss: 0.509116  [  228/  300]
per-ex loss: 0.640038  [  231/  300]
per-ex loss: 0.690629  [  234/  300]
per-ex loss: 0.501387  [  237/  300]
per-ex loss: 0.572699  [  240/  300]
per-ex loss: 0.532476  [  243/  300]
per-ex loss: 0.469028  [  246/  300]
per-ex loss: 0.582961  [  249/  300]
per-ex loss: 0.604017  [  252/  300]
per-ex loss: 0.545280  [  255/  300]
per-ex loss: 0.660103  [  258/  300]
per-ex loss: 0.516993  [  261/  300]
per-ex loss: 0.778409  [  264/  300]
per-ex loss: 0.470654  [  267/  300]
per-ex loss: 0.608889  [  270/  300]
per-ex loss: 0.590756  [  273/  300]
per-ex loss: 0.661769  [  276/  300]
per-ex loss: 0.479679  [  279/  300]
per-ex loss: 0.465097  [  282/  300]
per-ex loss: 0.513993  [  285/  300]
per-ex loss: 0.698140  [  288/  300]
per-ex loss: 0.694426  [  291/  300]
per-ex loss: 0.633706  [  294/  300]
per-ex loss: 0.559071  [  297/  300]
per-ex loss: 0.737449  [  300/  300]
Train Error: Avg loss: 0.59374824
validation Error: 
 Avg loss: 0.59380569 
 F1: 0.417149 
 Precision: 0.431147 
 Recall: 0.404032
 IoU: 0.263543

test Error: 
 Avg loss: 0.51786101 
 F1: 0.485481 
 Precision: 0.548025 
 Recall: 0.435750
 IoU: 0.320551

We have finished training iteration 13
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_10_.pth
per-ex loss: 0.641513  [    3/  300]
per-ex loss: 0.594952  [    6/  300]
per-ex loss: 0.629266  [    9/  300]
per-ex loss: 0.553117  [   12/  300]
per-ex loss: 0.566878  [   15/  300]
per-ex loss: 0.647767  [   18/  300]
per-ex loss: 0.619120  [   21/  300]
per-ex loss: 0.595522  [   24/  300]
per-ex loss: 0.678431  [   27/  300]
per-ex loss: 0.651069  [   30/  300]
per-ex loss: 0.501637  [   33/  300]
per-ex loss: 0.497407  [   36/  300]
per-ex loss: 0.580150  [   39/  300]
per-ex loss: 0.714854  [   42/  300]
per-ex loss: 0.477904  [   45/  300]
per-ex loss: 0.548452  [   48/  300]
per-ex loss: 0.620594  [   51/  300]
per-ex loss: 0.608563  [   54/  300]
per-ex loss: 0.715755  [   57/  300]
per-ex loss: 0.680529  [   60/  300]
per-ex loss: 0.616194  [   63/  300]
per-ex loss: 0.695990  [   66/  300]
per-ex loss: 0.452863  [   69/  300]
per-ex loss: 0.489231  [   72/  300]
per-ex loss: 0.481030  [   75/  300]
per-ex loss: 0.580632  [   78/  300]
per-ex loss: 0.517453  [   81/  300]
per-ex loss: 0.653326  [   84/  300]
per-ex loss: 0.719406  [   87/  300]
per-ex loss: 0.508562  [   90/  300]
per-ex loss: 0.525281  [   93/  300]
per-ex loss: 0.686263  [   96/  300]
per-ex loss: 0.503685  [   99/  300]
per-ex loss: 0.673698  [  102/  300]
per-ex loss: 0.705104  [  105/  300]
per-ex loss: 0.505724  [  108/  300]
per-ex loss: 0.519575  [  111/  300]
per-ex loss: 0.714409  [  114/  300]
per-ex loss: 0.626537  [  117/  300]
per-ex loss: 0.718584  [  120/  300]
per-ex loss: 0.548141  [  123/  300]
per-ex loss: 0.484120  [  126/  300]
per-ex loss: 0.594914  [  129/  300]
per-ex loss: 0.589393  [  132/  300]
per-ex loss: 0.576896  [  135/  300]
per-ex loss: 0.649197  [  138/  300]
per-ex loss: 0.612152  [  141/  300]
per-ex loss: 0.620982  [  144/  300]
per-ex loss: 0.499779  [  147/  300]
per-ex loss: 0.624997  [  150/  300]
per-ex loss: 0.549108  [  153/  300]
per-ex loss: 0.669027  [  156/  300]
per-ex loss: 0.533439  [  159/  300]
per-ex loss: 0.764615  [  162/  300]
per-ex loss: 0.588787  [  165/  300]
per-ex loss: 0.643317  [  168/  300]
per-ex loss: 0.589123  [  171/  300]
per-ex loss: 0.534688  [  174/  300]
per-ex loss: 0.555459  [  177/  300]
per-ex loss: 0.688136  [  180/  300]
per-ex loss: 0.476175  [  183/  300]
per-ex loss: 0.516851  [  186/  300]
per-ex loss: 0.590747  [  189/  300]
per-ex loss: 0.536301  [  192/  300]
per-ex loss: 0.695888  [  195/  300]
per-ex loss: 0.617742  [  198/  300]
per-ex loss: 0.541102  [  201/  300]
per-ex loss: 0.659092  [  204/  300]
per-ex loss: 0.467049  [  207/  300]
per-ex loss: 0.559003  [  210/  300]
per-ex loss: 0.657908  [  213/  300]
per-ex loss: 0.624471  [  216/  300]
per-ex loss: 0.660002  [  219/  300]
per-ex loss: 0.615658  [  222/  300]
per-ex loss: 0.665864  [  225/  300]
per-ex loss: 0.631299  [  228/  300]
per-ex loss: 0.703390  [  231/  300]
per-ex loss: 0.535091  [  234/  300]
per-ex loss: 0.509655  [  237/  300]
per-ex loss: 0.605448  [  240/  300]
per-ex loss: 0.572234  [  243/  300]
per-ex loss: 0.650218  [  246/  300]
per-ex loss: 0.527239  [  249/  300]
per-ex loss: 0.669214  [  252/  300]
per-ex loss: 0.588490  [  255/  300]
per-ex loss: 0.689675  [  258/  300]
per-ex loss: 0.642486  [  261/  300]
per-ex loss: 0.503536  [  264/  300]
per-ex loss: 0.481251  [  267/  300]
per-ex loss: 0.613196  [  270/  300]
per-ex loss: 0.536495  [  273/  300]
per-ex loss: 0.718975  [  276/  300]
per-ex loss: 0.558366  [  279/  300]
per-ex loss: 0.459540  [  282/  300]
per-ex loss: 0.486826  [  285/  300]
per-ex loss: 0.629442  [  288/  300]
per-ex loss: 0.511963  [  291/  300]
per-ex loss: 0.521336  [  294/  300]
per-ex loss: 0.628316  [  297/  300]
per-ex loss: 0.636652  [  300/  300]
Train Error: Avg loss: 0.59427482
validation Error: 
 Avg loss: 0.57740414 
 F1: 0.413776 
 Precision: 0.354022 
 Recall: 0.497797
 IoU: 0.260856

test Error: 
 Avg loss: 0.50831771 
 F1: 0.495431 
 Precision: 0.464529 
 Recall: 0.530738
 IoU: 0.329284

We have finished training iteration 14
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_12_.pth
per-ex loss: 0.603002  [    3/  300]
per-ex loss: 0.521825  [    6/  300]
per-ex loss: 0.650494  [    9/  300]
per-ex loss: 0.611932  [   12/  300]
per-ex loss: 0.597461  [   15/  300]
per-ex loss: 0.677007  [   18/  300]
per-ex loss: 0.742300  [   21/  300]
per-ex loss: 0.560130  [   24/  300]
per-ex loss: 0.566930  [   27/  300]
per-ex loss: 0.671010  [   30/  300]
per-ex loss: 0.580034  [   33/  300]
per-ex loss: 0.720380  [   36/  300]
per-ex loss: 0.504570  [   39/  300]
per-ex loss: 0.603008  [   42/  300]
per-ex loss: 0.618667  [   45/  300]
per-ex loss: 0.517745  [   48/  300]
per-ex loss: 0.453875  [   51/  300]
per-ex loss: 0.681852  [   54/  300]
per-ex loss: 0.531945  [   57/  300]
per-ex loss: 0.596579  [   60/  300]
per-ex loss: 0.600695  [   63/  300]
per-ex loss: 0.512558  [   66/  300]
per-ex loss: 0.537011  [   69/  300]
per-ex loss: 0.567262  [   72/  300]
per-ex loss: 0.552021  [   75/  300]
per-ex loss: 0.674110  [   78/  300]
per-ex loss: 0.512724  [   81/  300]
per-ex loss: 0.658364  [   84/  300]
per-ex loss: 0.678530  [   87/  300]
per-ex loss: 0.621751  [   90/  300]
per-ex loss: 0.507638  [   93/  300]
per-ex loss: 0.545044  [   96/  300]
per-ex loss: 0.484169  [   99/  300]
per-ex loss: 0.622612  [  102/  300]
per-ex loss: 0.672709  [  105/  300]
per-ex loss: 0.504910  [  108/  300]
per-ex loss: 0.534780  [  111/  300]
per-ex loss: 0.581834  [  114/  300]
per-ex loss: 0.615460  [  117/  300]
per-ex loss: 0.453322  [  120/  300]
per-ex loss: 0.597585  [  123/  300]
per-ex loss: 0.730243  [  126/  300]
per-ex loss: 0.650891  [  129/  300]
per-ex loss: 0.525280  [  132/  300]
per-ex loss: 0.621106  [  135/  300]
per-ex loss: 0.660593  [  138/  300]
per-ex loss: 0.677374  [  141/  300]
per-ex loss: 0.528297  [  144/  300]
per-ex loss: 0.537569  [  147/  300]
per-ex loss: 0.507023  [  150/  300]
per-ex loss: 0.617946  [  153/  300]
per-ex loss: 0.660148  [  156/  300]
per-ex loss: 0.563696  [  159/  300]
per-ex loss: 0.756141  [  162/  300]
per-ex loss: 0.584901  [  165/  300]
per-ex loss: 0.513735  [  168/  300]
per-ex loss: 0.556248  [  171/  300]
per-ex loss: 0.480806  [  174/  300]
per-ex loss: 0.705684  [  177/  300]
per-ex loss: 0.626170  [  180/  300]
per-ex loss: 0.584945  [  183/  300]
per-ex loss: 0.559812  [  186/  300]
per-ex loss: 0.627109  [  189/  300]
per-ex loss: 0.505151  [  192/  300]
per-ex loss: 0.567573  [  195/  300]
per-ex loss: 0.458769  [  198/  300]
per-ex loss: 0.714864  [  201/  300]
per-ex loss: 0.643941  [  204/  300]
per-ex loss: 0.720130  [  207/  300]
per-ex loss: 0.512937  [  210/  300]
per-ex loss: 0.624778  [  213/  300]
per-ex loss: 0.660776  [  216/  300]
per-ex loss: 0.712050  [  219/  300]
per-ex loss: 0.545855  [  222/  300]
per-ex loss: 0.572314  [  225/  300]
per-ex loss: 0.544695  [  228/  300]
per-ex loss: 0.479911  [  231/  300]
per-ex loss: 0.548155  [  234/  300]
per-ex loss: 0.629203  [  237/  300]
per-ex loss: 0.543345  [  240/  300]
per-ex loss: 0.660369  [  243/  300]
per-ex loss: 0.668764  [  246/  300]
per-ex loss: 0.540721  [  249/  300]
per-ex loss: 0.512029  [  252/  300]
per-ex loss: 0.538698  [  255/  300]
per-ex loss: 0.464406  [  258/  300]
per-ex loss: 0.584307  [  261/  300]
per-ex loss: 0.484512  [  264/  300]
per-ex loss: 0.715826  [  267/  300]
per-ex loss: 0.566316  [  270/  300]
per-ex loss: 0.483504  [  273/  300]
per-ex loss: 0.694697  [  276/  300]
per-ex loss: 0.488311  [  279/  300]
per-ex loss: 0.466159  [  282/  300]
per-ex loss: 0.502042  [  285/  300]
per-ex loss: 0.460407  [  288/  300]
per-ex loss: 0.743119  [  291/  300]
per-ex loss: 0.631652  [  294/  300]
per-ex loss: 0.523226  [  297/  300]
per-ex loss: 0.673039  [  300/  300]
Train Error: Avg loss: 0.58708106
validation Error: 
 Avg loss: 0.56267476 
 F1: 0.429829 
 Precision: 0.375385 
 Recall: 0.502745
 IoU: 0.273746

test Error: 
 Avg loss: 0.48736751 
 F1: 0.515115 
 Precision: 0.497089 
 Recall: 0.534498
 IoU: 0.346906

We have finished training iteration 15
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_13_.pth
per-ex loss: 0.749628  [    3/  300]
per-ex loss: 0.605309  [    6/  300]
per-ex loss: 0.541168  [    9/  300]
per-ex loss: 0.594783  [   12/  300]
per-ex loss: 0.554625  [   15/  300]
per-ex loss: 0.577905  [   18/  300]
per-ex loss: 0.490282  [   21/  300]
per-ex loss: 0.570335  [   24/  300]
per-ex loss: 0.677337  [   27/  300]
per-ex loss: 0.578591  [   30/  300]
per-ex loss: 0.607356  [   33/  300]
per-ex loss: 0.561754  [   36/  300]
per-ex loss: 0.474018  [   39/  300]
per-ex loss: 0.819708  [   42/  300]
per-ex loss: 0.600378  [   45/  300]
per-ex loss: 0.600839  [   48/  300]
per-ex loss: 0.559187  [   51/  300]
per-ex loss: 0.609997  [   54/  300]
per-ex loss: 0.527006  [   57/  300]
per-ex loss: 0.666033  [   60/  300]
per-ex loss: 0.599430  [   63/  300]
per-ex loss: 0.514353  [   66/  300]
per-ex loss: 0.703174  [   69/  300]
per-ex loss: 0.514656  [   72/  300]
per-ex loss: 0.553555  [   75/  300]
per-ex loss: 0.499587  [   78/  300]
per-ex loss: 0.497468  [   81/  300]
per-ex loss: 0.536984  [   84/  300]
per-ex loss: 0.702918  [   87/  300]
per-ex loss: 0.703670  [   90/  300]
per-ex loss: 0.650386  [   93/  300]
per-ex loss: 0.572159  [   96/  300]
per-ex loss: 0.520266  [   99/  300]
per-ex loss: 0.513918  [  102/  300]
per-ex loss: 0.676599  [  105/  300]
per-ex loss: 0.546162  [  108/  300]
per-ex loss: 0.592599  [  111/  300]
per-ex loss: 0.481391  [  114/  300]
per-ex loss: 0.534369  [  117/  300]
per-ex loss: 0.556043  [  120/  300]
per-ex loss: 0.597852  [  123/  300]
per-ex loss: 0.627255  [  126/  300]
per-ex loss: 0.507642  [  129/  300]
per-ex loss: 0.519485  [  132/  300]
per-ex loss: 0.496899  [  135/  300]
per-ex loss: 0.701711  [  138/  300]
per-ex loss: 0.501557  [  141/  300]
per-ex loss: 0.495455  [  144/  300]
per-ex loss: 0.638887  [  147/  300]
per-ex loss: 0.574925  [  150/  300]
per-ex loss: 0.606609  [  153/  300]
per-ex loss: 0.473209  [  156/  300]
per-ex loss: 0.518818  [  159/  300]
per-ex loss: 0.558212  [  162/  300]
per-ex loss: 0.597145  [  165/  300]
per-ex loss: 0.603008  [  168/  300]
per-ex loss: 0.537349  [  171/  300]
per-ex loss: 0.602217  [  174/  300]
per-ex loss: 0.574024  [  177/  300]
per-ex loss: 0.539582  [  180/  300]
per-ex loss: 0.541337  [  183/  300]
per-ex loss: 0.715170  [  186/  300]
per-ex loss: 0.575817  [  189/  300]
per-ex loss: 0.533109  [  192/  300]
per-ex loss: 0.571203  [  195/  300]
per-ex loss: 0.694736  [  198/  300]
per-ex loss: 0.625470  [  201/  300]
per-ex loss: 0.481998  [  204/  300]
per-ex loss: 0.505948  [  207/  300]
per-ex loss: 0.673744  [  210/  300]
per-ex loss: 0.629676  [  213/  300]
per-ex loss: 0.598870  [  216/  300]
per-ex loss: 0.588740  [  219/  300]
per-ex loss: 0.561837  [  222/  300]
per-ex loss: 0.605549  [  225/  300]
per-ex loss: 0.644975  [  228/  300]
per-ex loss: 0.489211  [  231/  300]
per-ex loss: 0.444312  [  234/  300]
per-ex loss: 0.530653  [  237/  300]
per-ex loss: 0.621157  [  240/  300]
per-ex loss: 0.488090  [  243/  300]
per-ex loss: 0.493781  [  246/  300]
per-ex loss: 0.529565  [  249/  300]
per-ex loss: 0.554105  [  252/  300]
per-ex loss: 0.594019  [  255/  300]
per-ex loss: 0.626410  [  258/  300]
per-ex loss: 0.665930  [  261/  300]
per-ex loss: 0.590739  [  264/  300]
per-ex loss: 0.729873  [  267/  300]
per-ex loss: 0.627522  [  270/  300]
per-ex loss: 0.576222  [  273/  300]
per-ex loss: 0.484787  [  276/  300]
per-ex loss: 0.546284  [  279/  300]
per-ex loss: 0.684383  [  282/  300]
per-ex loss: 0.517922  [  285/  300]
per-ex loss: 0.595299  [  288/  300]
per-ex loss: 0.564932  [  291/  300]
per-ex loss: 0.754735  [  294/  300]
per-ex loss: 0.577405  [  297/  300]
per-ex loss: 0.504077  [  300/  300]
Train Error: Avg loss: 0.58047356
validation Error: 
 Avg loss: 0.55561352 
 F1: 0.440817 
 Precision: 0.392651 
 Recall: 0.502454
 IoU: 0.282723

test Error: 
 Avg loss: 0.48981690 
 F1: 0.512519 
 Precision: 0.506303 
 Recall: 0.518890
 IoU: 0.344555

We have finished training iteration 16
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_14_.pth
per-ex loss: 0.594394  [    3/  300]
per-ex loss: 0.674887  [    6/  300]
per-ex loss: 0.563352  [    9/  300]
per-ex loss: 0.675267  [   12/  300]
per-ex loss: 0.696480  [   15/  300]
per-ex loss: 0.478662  [   18/  300]
per-ex loss: 0.704859  [   21/  300]
per-ex loss: 0.433709  [   24/  300]
per-ex loss: 0.655942  [   27/  300]
per-ex loss: 0.461503  [   30/  300]
per-ex loss: 0.539931  [   33/  300]
per-ex loss: 0.567682  [   36/  300]
per-ex loss: 0.528762  [   39/  300]
per-ex loss: 0.517547  [   42/  300]
per-ex loss: 0.713845  [   45/  300]
per-ex loss: 0.591838  [   48/  300]
per-ex loss: 0.520788  [   51/  300]
per-ex loss: 0.475536  [   54/  300]
per-ex loss: 0.562441  [   57/  300]
per-ex loss: 0.497780  [   60/  300]
per-ex loss: 0.556711  [   63/  300]
per-ex loss: 0.467322  [   66/  300]
per-ex loss: 0.705055  [   69/  300]
per-ex loss: 0.681267  [   72/  300]
per-ex loss: 0.559891  [   75/  300]
per-ex loss: 0.516949  [   78/  300]
per-ex loss: 0.546622  [   81/  300]
per-ex loss: 0.587916  [   84/  300]
per-ex loss: 0.616910  [   87/  300]
per-ex loss: 0.637237  [   90/  300]
per-ex loss: 0.611007  [   93/  300]
per-ex loss: 0.656993  [   96/  300]
per-ex loss: 0.546973  [   99/  300]
per-ex loss: 0.484308  [  102/  300]
per-ex loss: 0.519157  [  105/  300]
per-ex loss: 0.525880  [  108/  300]
per-ex loss: 0.558794  [  111/  300]
per-ex loss: 0.450519  [  114/  300]
per-ex loss: 0.507570  [  117/  300]
per-ex loss: 0.559865  [  120/  300]
per-ex loss: 0.558205  [  123/  300]
per-ex loss: 0.668597  [  126/  300]
per-ex loss: 0.557418  [  129/  300]
per-ex loss: 0.624596  [  132/  300]
per-ex loss: 0.584535  [  135/  300]
per-ex loss: 0.803087  [  138/  300]
per-ex loss: 0.504245  [  141/  300]
per-ex loss: 0.697565  [  144/  300]
per-ex loss: 0.676435  [  147/  300]
per-ex loss: 0.704584  [  150/  300]
per-ex loss: 0.568628  [  153/  300]
per-ex loss: 0.677309  [  156/  300]
per-ex loss: 0.600452  [  159/  300]
per-ex loss: 0.469298  [  162/  300]
per-ex loss: 0.521576  [  165/  300]
per-ex loss: 0.585171  [  168/  300]
per-ex loss: 0.560326  [  171/  300]
per-ex loss: 0.536389  [  174/  300]
per-ex loss: 0.690556  [  177/  300]
per-ex loss: 0.644092  [  180/  300]
per-ex loss: 0.546512  [  183/  300]
per-ex loss: 0.511649  [  186/  300]
per-ex loss: 0.641666  [  189/  300]
per-ex loss: 0.657799  [  192/  300]
per-ex loss: 0.480250  [  195/  300]
per-ex loss: 0.595917  [  198/  300]
per-ex loss: 0.536570  [  201/  300]
per-ex loss: 0.558815  [  204/  300]
per-ex loss: 0.624317  [  207/  300]
per-ex loss: 0.535382  [  210/  300]
per-ex loss: 0.579919  [  213/  300]
per-ex loss: 0.590320  [  216/  300]
per-ex loss: 0.489208  [  219/  300]
per-ex loss: 0.729973  [  222/  300]
per-ex loss: 0.558477  [  225/  300]
per-ex loss: 0.592519  [  228/  300]
per-ex loss: 0.552898  [  231/  300]
per-ex loss: 0.530229  [  234/  300]
per-ex loss: 0.612911  [  237/  300]
per-ex loss: 0.530569  [  240/  300]
per-ex loss: 0.510451  [  243/  300]
per-ex loss: 0.499667  [  246/  300]
per-ex loss: 0.451229  [  249/  300]
per-ex loss: 0.505009  [  252/  300]
per-ex loss: 0.575136  [  255/  300]
per-ex loss: 0.485132  [  258/  300]
per-ex loss: 0.706967  [  261/  300]
per-ex loss: 0.662789  [  264/  300]
per-ex loss: 0.536846  [  267/  300]
per-ex loss: 0.574578  [  270/  300]
per-ex loss: 0.490521  [  273/  300]
per-ex loss: 0.600766  [  276/  300]
per-ex loss: 0.734435  [  279/  300]
per-ex loss: 0.554560  [  282/  300]
per-ex loss: 0.571156  [  285/  300]
per-ex loss: 0.632744  [  288/  300]
per-ex loss: 0.707131  [  291/  300]
per-ex loss: 0.526675  [  294/  300]
per-ex loss: 0.559707  [  297/  300]
per-ex loss: 0.643148  [  300/  300]
Train Error: Avg loss: 0.57969759
validation Error: 
 Avg loss: 0.58722607 
 F1: 0.421749 
 Precision: 0.420713 
 Recall: 0.422790
 IoU: 0.267225

test Error: 
 Avg loss: 0.50830084 
 F1: 0.493789 
 Precision: 0.541398 
 Recall: 0.453877
 IoU: 0.327835

We have finished training iteration 17
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_15_.pth
per-ex loss: 0.698040  [    3/  300]
per-ex loss: 0.588582  [    6/  300]
per-ex loss: 0.600560  [    9/  300]
per-ex loss: 0.647228  [   12/  300]
per-ex loss: 0.567542  [   15/  300]
per-ex loss: 0.645431  [   18/  300]
per-ex loss: 0.550967  [   21/  300]
per-ex loss: 0.613073  [   24/  300]
per-ex loss: 0.478004  [   27/  300]
per-ex loss: 0.540389  [   30/  300]
per-ex loss: 0.568491  [   33/  300]
per-ex loss: 0.509723  [   36/  300]
per-ex loss: 0.584452  [   39/  300]
per-ex loss: 0.625690  [   42/  300]
per-ex loss: 0.624246  [   45/  300]
per-ex loss: 0.563984  [   48/  300]
per-ex loss: 0.522088  [   51/  300]
per-ex loss: 0.601741  [   54/  300]
per-ex loss: 0.504257  [   57/  300]
per-ex loss: 0.612071  [   60/  300]
per-ex loss: 0.486097  [   63/  300]
per-ex loss: 0.714699  [   66/  300]
per-ex loss: 0.520310  [   69/  300]
per-ex loss: 0.529589  [   72/  300]
per-ex loss: 0.428359  [   75/  300]
per-ex loss: 0.686669  [   78/  300]
per-ex loss: 0.540947  [   81/  300]
per-ex loss: 0.530823  [   84/  300]
per-ex loss: 0.641785  [   87/  300]
per-ex loss: 0.652609  [   90/  300]
per-ex loss: 0.604709  [   93/  300]
per-ex loss: 0.571208  [   96/  300]
per-ex loss: 0.547445  [   99/  300]
per-ex loss: 0.516753  [  102/  300]
per-ex loss: 0.659372  [  105/  300]
per-ex loss: 0.630366  [  108/  300]
per-ex loss: 0.608115  [  111/  300]
per-ex loss: 0.551327  [  114/  300]
per-ex loss: 0.559270  [  117/  300]
per-ex loss: 0.565920  [  120/  300]
per-ex loss: 0.557960  [  123/  300]
per-ex loss: 0.546139  [  126/  300]
per-ex loss: 0.509432  [  129/  300]
per-ex loss: 0.548078  [  132/  300]
per-ex loss: 0.589483  [  135/  300]
per-ex loss: 0.592004  [  138/  300]
per-ex loss: 0.470590  [  141/  300]
per-ex loss: 0.490132  [  144/  300]
per-ex loss: 0.722047  [  147/  300]
per-ex loss: 0.558068  [  150/  300]
per-ex loss: 0.692982  [  153/  300]
per-ex loss: 0.599346  [  156/  300]
per-ex loss: 0.467291  [  159/  300]
per-ex loss: 0.714049  [  162/  300]
per-ex loss: 0.499609  [  165/  300]
per-ex loss: 0.563923  [  168/  300]
per-ex loss: 0.606047  [  171/  300]
per-ex loss: 0.761164  [  174/  300]
per-ex loss: 0.516611  [  177/  300]
per-ex loss: 0.576877  [  180/  300]
per-ex loss: 0.524812  [  183/  300]
per-ex loss: 0.649799  [  186/  300]
per-ex loss: 0.572449  [  189/  300]
per-ex loss: 0.556483  [  192/  300]
per-ex loss: 0.550493  [  195/  300]
per-ex loss: 0.540509  [  198/  300]
per-ex loss: 0.595538  [  201/  300]
per-ex loss: 0.477040  [  204/  300]
per-ex loss: 0.549297  [  207/  300]
per-ex loss: 0.576201  [  210/  300]
per-ex loss: 0.553125  [  213/  300]
per-ex loss: 0.683387  [  216/  300]
per-ex loss: 0.652882  [  219/  300]
per-ex loss: 0.647964  [  222/  300]
per-ex loss: 0.555714  [  225/  300]
per-ex loss: 0.668514  [  228/  300]
per-ex loss: 0.589647  [  231/  300]
per-ex loss: 0.506703  [  234/  300]
per-ex loss: 0.686464  [  237/  300]
per-ex loss: 0.658480  [  240/  300]
per-ex loss: 0.611617  [  243/  300]
per-ex loss: 0.598419  [  246/  300]
per-ex loss: 0.510128  [  249/  300]
per-ex loss: 0.516809  [  252/  300]
per-ex loss: 0.507823  [  255/  300]
per-ex loss: 0.565374  [  258/  300]
per-ex loss: 0.488986  [  261/  300]
per-ex loss: 0.712272  [  264/  300]
per-ex loss: 0.530119  [  267/  300]
per-ex loss: 0.547039  [  270/  300]
per-ex loss: 0.785971  [  273/  300]
per-ex loss: 0.524797  [  276/  300]
per-ex loss: 0.557700  [  279/  300]
per-ex loss: 0.653288  [  282/  300]
per-ex loss: 0.610894  [  285/  300]
per-ex loss: 0.557608  [  288/  300]
per-ex loss: 0.474414  [  291/  300]
per-ex loss: 0.449513  [  294/  300]
per-ex loss: 0.585439  [  297/  300]
per-ex loss: 0.734881  [  300/  300]
Train Error: Avg loss: 0.58093358
validation Error: 
 Avg loss: 0.56695114 
 F1: 0.432906 
 Precision: 0.385758 
 Recall: 0.493183
 IoU: 0.276248

test Error: 
 Avg loss: 0.50371921 
 F1: 0.498858 
 Precision: 0.481138 
 Recall: 0.517932
 IoU: 0.332319

We have finished training iteration 18
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_9_.pth
per-ex loss: 0.561672  [    3/  300]
per-ex loss: 0.717675  [    6/  300]
per-ex loss: 0.523358  [    9/  300]
per-ex loss: 0.606714  [   12/  300]
per-ex loss: 0.550922  [   15/  300]
per-ex loss: 0.631532  [   18/  300]
per-ex loss: 0.617459  [   21/  300]
per-ex loss: 0.528386  [   24/  300]
per-ex loss: 0.486253  [   27/  300]
per-ex loss: 0.592409  [   30/  300]
per-ex loss: 0.498096  [   33/  300]
per-ex loss: 0.577962  [   36/  300]
per-ex loss: 0.690336  [   39/  300]
per-ex loss: 0.478997  [   42/  300]
per-ex loss: 0.669626  [   45/  300]
per-ex loss: 0.543649  [   48/  300]
per-ex loss: 0.594444  [   51/  300]
per-ex loss: 0.563001  [   54/  300]
per-ex loss: 0.630050  [   57/  300]
per-ex loss: 0.437448  [   60/  300]
per-ex loss: 0.587727  [   63/  300]
per-ex loss: 0.580649  [   66/  300]
per-ex loss: 0.599817  [   69/  300]
per-ex loss: 0.628595  [   72/  300]
per-ex loss: 0.465666  [   75/  300]
per-ex loss: 0.556572  [   78/  300]
per-ex loss: 0.561682  [   81/  300]
per-ex loss: 0.751566  [   84/  300]
per-ex loss: 0.700951  [   87/  300]
per-ex loss: 0.656647  [   90/  300]
per-ex loss: 0.645087  [   93/  300]
per-ex loss: 0.496237  [   96/  300]
per-ex loss: 0.541936  [   99/  300]
per-ex loss: 0.693848  [  102/  300]
per-ex loss: 0.550710  [  105/  300]
per-ex loss: 0.695296  [  108/  300]
per-ex loss: 0.612641  [  111/  300]
per-ex loss: 0.566379  [  114/  300]
per-ex loss: 0.490100  [  117/  300]
per-ex loss: 0.529487  [  120/  300]
per-ex loss: 0.603196  [  123/  300]
per-ex loss: 0.454397  [  126/  300]
per-ex loss: 0.687158  [  129/  300]
per-ex loss: 0.550995  [  132/  300]
per-ex loss: 0.743349  [  135/  300]
per-ex loss: 0.734071  [  138/  300]
per-ex loss: 0.721855  [  141/  300]
per-ex loss: 0.619089  [  144/  300]
per-ex loss: 0.451942  [  147/  300]
per-ex loss: 0.542943  [  150/  300]
per-ex loss: 0.540059  [  153/  300]
per-ex loss: 0.533388  [  156/  300]
per-ex loss: 0.471631  [  159/  300]
per-ex loss: 0.557647  [  162/  300]
per-ex loss: 0.540137  [  165/  300]
per-ex loss: 0.505252  [  168/  300]
per-ex loss: 0.517479  [  171/  300]
per-ex loss: 0.530163  [  174/  300]
per-ex loss: 0.592784  [  177/  300]
per-ex loss: 0.540339  [  180/  300]
per-ex loss: 0.441233  [  183/  300]
per-ex loss: 0.511394  [  186/  300]
per-ex loss: 0.645791  [  189/  300]
per-ex loss: 0.574007  [  192/  300]
per-ex loss: 0.613560  [  195/  300]
per-ex loss: 0.474315  [  198/  300]
per-ex loss: 0.514144  [  201/  300]
per-ex loss: 0.486743  [  204/  300]
per-ex loss: 0.610340  [  207/  300]
per-ex loss: 0.592565  [  210/  300]
per-ex loss: 0.588059  [  213/  300]
per-ex loss: 0.545633  [  216/  300]
per-ex loss: 0.704970  [  219/  300]
per-ex loss: 0.516186  [  222/  300]
per-ex loss: 0.624399  [  225/  300]
per-ex loss: 0.631197  [  228/  300]
per-ex loss: 0.495314  [  231/  300]
per-ex loss: 0.642313  [  234/  300]
per-ex loss: 0.527571  [  237/  300]
per-ex loss: 0.510660  [  240/  300]
per-ex loss: 0.546965  [  243/  300]
per-ex loss: 0.592018  [  246/  300]
per-ex loss: 0.594730  [  249/  300]
per-ex loss: 0.509002  [  252/  300]
per-ex loss: 0.625175  [  255/  300]
per-ex loss: 0.656911  [  258/  300]
per-ex loss: 0.632688  [  261/  300]
per-ex loss: 0.665825  [  264/  300]
per-ex loss: 0.604609  [  267/  300]
per-ex loss: 0.491822  [  270/  300]
per-ex loss: 0.588485  [  273/  300]
per-ex loss: 0.583391  [  276/  300]
per-ex loss: 0.644264  [  279/  300]
per-ex loss: 0.495022  [  282/  300]
per-ex loss: 0.477903  [  285/  300]
per-ex loss: 0.640030  [  288/  300]
per-ex loss: 0.628400  [  291/  300]
per-ex loss: 0.500499  [  294/  300]
per-ex loss: 0.508575  [  297/  300]
per-ex loss: 0.519199  [  300/  300]
Train Error: Avg loss: 0.57605358
validation Error: 
 Avg loss: 0.59175917 
 F1: 0.419991 
 Precision: 0.439850 
 Recall: 0.401848
 IoU: 0.265816

test Error: 
 Avg loss: 0.53359842 
 F1: 0.468637 
 Precision: 0.556667 
 Recall: 0.404647
 IoU: 0.306026

We have finished training iteration 19
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_17_.pth
per-ex loss: 0.600617  [    3/  300]
per-ex loss: 0.446421  [    6/  300]
per-ex loss: 0.650990  [    9/  300]
per-ex loss: 0.564153  [   12/  300]
per-ex loss: 0.586849  [   15/  300]
per-ex loss: 0.604514  [   18/  300]
per-ex loss: 0.549931  [   21/  300]
per-ex loss: 0.546792  [   24/  300]
per-ex loss: 0.499523  [   27/  300]
per-ex loss: 0.444720  [   30/  300]
per-ex loss: 0.581112  [   33/  300]
per-ex loss: 0.533630  [   36/  300]
per-ex loss: 0.500290  [   39/  300]
per-ex loss: 0.710724  [   42/  300]
per-ex loss: 0.482123  [   45/  300]
per-ex loss: 0.638528  [   48/  300]
per-ex loss: 0.676333  [   51/  300]
per-ex loss: 0.586776  [   54/  300]
per-ex loss: 0.569533  [   57/  300]
per-ex loss: 0.746624  [   60/  300]
per-ex loss: 0.562169  [   63/  300]
per-ex loss: 0.474249  [   66/  300]
per-ex loss: 0.566169  [   69/  300]
per-ex loss: 0.665254  [   72/  300]
per-ex loss: 0.687173  [   75/  300]
per-ex loss: 0.511766  [   78/  300]
per-ex loss: 0.551496  [   81/  300]
per-ex loss: 0.641647  [   84/  300]
per-ex loss: 0.710251  [   87/  300]
per-ex loss: 0.520904  [   90/  300]
per-ex loss: 0.611381  [   93/  300]
per-ex loss: 0.501335  [   96/  300]
per-ex loss: 0.576166  [   99/  300]
per-ex loss: 0.643070  [  102/  300]
per-ex loss: 0.523855  [  105/  300]
per-ex loss: 0.564091  [  108/  300]
per-ex loss: 0.703191  [  111/  300]
per-ex loss: 0.523452  [  114/  300]
per-ex loss: 0.561916  [  117/  300]
per-ex loss: 0.630033  [  120/  300]
per-ex loss: 0.687044  [  123/  300]
per-ex loss: 0.512863  [  126/  300]
per-ex loss: 0.728001  [  129/  300]
per-ex loss: 0.496625  [  132/  300]
per-ex loss: 0.477175  [  135/  300]
per-ex loss: 0.628909  [  138/  300]
per-ex loss: 0.556499  [  141/  300]
per-ex loss: 0.659059  [  144/  300]
per-ex loss: 0.445557  [  147/  300]
per-ex loss: 0.659607  [  150/  300]
per-ex loss: 0.554849  [  153/  300]
per-ex loss: 0.594200  [  156/  300]
per-ex loss: 0.733570  [  159/  300]
per-ex loss: 0.656151  [  162/  300]
per-ex loss: 0.528171  [  165/  300]
per-ex loss: 0.642805  [  168/  300]
per-ex loss: 0.504023  [  171/  300]
per-ex loss: 0.466821  [  174/  300]
per-ex loss: 0.464705  [  177/  300]
per-ex loss: 0.590775  [  180/  300]
per-ex loss: 0.647493  [  183/  300]
per-ex loss: 0.595946  [  186/  300]
per-ex loss: 0.679056  [  189/  300]
per-ex loss: 0.446098  [  192/  300]
per-ex loss: 0.672303  [  195/  300]
per-ex loss: 0.455272  [  198/  300]
per-ex loss: 0.523060  [  201/  300]
per-ex loss: 0.492896  [  204/  300]
per-ex loss: 0.758175  [  207/  300]
per-ex loss: 0.477613  [  210/  300]
per-ex loss: 0.536635  [  213/  300]
per-ex loss: 0.547184  [  216/  300]
per-ex loss: 0.545270  [  219/  300]
per-ex loss: 0.694723  [  222/  300]
per-ex loss: 0.593460  [  225/  300]
per-ex loss: 0.507193  [  228/  300]
per-ex loss: 0.469824  [  231/  300]
per-ex loss: 0.640981  [  234/  300]
per-ex loss: 0.602324  [  237/  300]
per-ex loss: 0.461753  [  240/  300]
per-ex loss: 0.488015  [  243/  300]
per-ex loss: 0.530922  [  246/  300]
per-ex loss: 0.566055  [  249/  300]
per-ex loss: 0.662762  [  252/  300]
per-ex loss: 0.707936  [  255/  300]
per-ex loss: 0.680788  [  258/  300]
per-ex loss: 0.656418  [  261/  300]
per-ex loss: 0.610914  [  264/  300]
per-ex loss: 0.534540  [  267/  300]
per-ex loss: 0.589154  [  270/  300]
per-ex loss: 0.538480  [  273/  300]
per-ex loss: 0.538390  [  276/  300]
per-ex loss: 0.641631  [  279/  300]
per-ex loss: 0.471840  [  282/  300]
per-ex loss: 0.521856  [  285/  300]
per-ex loss: 0.671109  [  288/  300]
per-ex loss: 0.585083  [  291/  300]
per-ex loss: 0.537981  [  294/  300]
per-ex loss: 0.660979  [  297/  300]
per-ex loss: 0.532097  [  300/  300]
Train Error: Avg loss: 0.57911372
validation Error: 
 Avg loss: 0.58124202 
 F1: 0.398832 
 Precision: 0.299550 
 Recall: 0.596555
 IoU: 0.249088

test Error: 
 Avg loss: 0.52865183 
 F1: 0.474476 
 Precision: 0.382413 
 Recall: 0.624922
 IoU: 0.311025

We have finished training iteration 20
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_18_.pth
per-ex loss: 0.611828  [    3/  300]
per-ex loss: 0.563939  [    6/  300]
per-ex loss: 0.445756  [    9/  300]
per-ex loss: 0.565771  [   12/  300]
per-ex loss: 0.580718  [   15/  300]
per-ex loss: 0.558437  [   18/  300]
per-ex loss: 0.560815  [   21/  300]
per-ex loss: 0.613132  [   24/  300]
per-ex loss: 0.544600  [   27/  300]
per-ex loss: 0.548144  [   30/  300]
per-ex loss: 0.561855  [   33/  300]
per-ex loss: 0.565721  [   36/  300]
per-ex loss: 0.633545  [   39/  300]
per-ex loss: 0.617465  [   42/  300]
per-ex loss: 0.579902  [   45/  300]
per-ex loss: 0.611103  [   48/  300]
per-ex loss: 0.784690  [   51/  300]
per-ex loss: 0.493628  [   54/  300]
per-ex loss: 0.636532  [   57/  300]
per-ex loss: 0.605562  [   60/  300]
per-ex loss: 0.562803  [   63/  300]
per-ex loss: 0.660470  [   66/  300]
per-ex loss: 0.519073  [   69/  300]
per-ex loss: 0.510861  [   72/  300]
per-ex loss: 0.479704  [   75/  300]
per-ex loss: 0.599523  [   78/  300]
per-ex loss: 0.483564  [   81/  300]
per-ex loss: 0.458034  [   84/  300]
per-ex loss: 0.532300  [   87/  300]
per-ex loss: 0.629386  [   90/  300]
per-ex loss: 0.596616  [   93/  300]
per-ex loss: 0.562862  [   96/  300]
per-ex loss: 0.528202  [   99/  300]
per-ex loss: 0.553350  [  102/  300]
per-ex loss: 0.471822  [  105/  300]
per-ex loss: 0.663456  [  108/  300]
per-ex loss: 0.526568  [  111/  300]
per-ex loss: 0.535379  [  114/  300]
per-ex loss: 0.563989  [  117/  300]
per-ex loss: 0.481693  [  120/  300]
per-ex loss: 0.497193  [  123/  300]
per-ex loss: 0.568447  [  126/  300]
per-ex loss: 0.720674  [  129/  300]
per-ex loss: 0.632822  [  132/  300]
per-ex loss: 0.463441  [  135/  300]
per-ex loss: 0.528187  [  138/  300]
per-ex loss: 0.537175  [  141/  300]
per-ex loss: 0.529038  [  144/  300]
per-ex loss: 0.629559  [  147/  300]
per-ex loss: 0.639326  [  150/  300]
per-ex loss: 0.489062  [  153/  300]
per-ex loss: 0.566967  [  156/  300]
per-ex loss: 0.685730  [  159/  300]
per-ex loss: 0.708449  [  162/  300]
per-ex loss: 0.696822  [  165/  300]
per-ex loss: 0.471964  [  168/  300]
per-ex loss: 0.572875  [  171/  300]
per-ex loss: 0.649574  [  174/  300]
per-ex loss: 0.660200  [  177/  300]
per-ex loss: 0.621122  [  180/  300]
per-ex loss: 0.596718  [  183/  300]
per-ex loss: 0.575112  [  186/  300]
per-ex loss: 0.567428  [  189/  300]
per-ex loss: 0.592498  [  192/  300]
per-ex loss: 0.565968  [  195/  300]
per-ex loss: 0.565957  [  198/  300]
per-ex loss: 0.548646  [  201/  300]
per-ex loss: 0.527477  [  204/  300]
per-ex loss: 0.554928  [  207/  300]
per-ex loss: 0.566939  [  210/  300]
per-ex loss: 0.558230  [  213/  300]
per-ex loss: 0.553495  [  216/  300]
per-ex loss: 0.538767  [  219/  300]
per-ex loss: 0.540998  [  222/  300]
per-ex loss: 0.511715  [  225/  300]
per-ex loss: 0.541628  [  228/  300]
per-ex loss: 0.550932  [  231/  300]
per-ex loss: 0.725899  [  234/  300]
per-ex loss: 0.480506  [  237/  300]
per-ex loss: 0.483116  [  240/  300]
per-ex loss: 0.585137  [  243/  300]
per-ex loss: 0.698035  [  246/  300]
per-ex loss: 0.545293  [  249/  300]
per-ex loss: 0.676324  [  252/  300]
per-ex loss: 0.525224  [  255/  300]
per-ex loss: 0.440761  [  258/  300]
per-ex loss: 0.636618  [  261/  300]
per-ex loss: 0.522858  [  264/  300]
per-ex loss: 0.677520  [  267/  300]
per-ex loss: 0.577095  [  270/  300]
per-ex loss: 0.646052  [  273/  300]
per-ex loss: 0.572616  [  276/  300]
per-ex loss: 0.441211  [  279/  300]
per-ex loss: 0.707908  [  282/  300]
per-ex loss: 0.566897  [  285/  300]
per-ex loss: 0.520552  [  288/  300]
per-ex loss: 0.434637  [  291/  300]
per-ex loss: 0.585213  [  294/  300]
per-ex loss: 0.523580  [  297/  300]
per-ex loss: 0.474065  [  300/  300]
Train Error: Avg loss: 0.56975980
validation Error: 
 Avg loss: 0.58860966 
 F1: 0.422283 
 Precision: 0.426255 
 Recall: 0.418384
 IoU: 0.267654

test Error: 
 Avg loss: 0.50259954 
 F1: 0.499173 
 Precision: 0.583373 
 Recall: 0.436213
 IoU: 0.332599

We have finished training iteration 21
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_19_.pth
per-ex loss: 0.644304  [    3/  300]
per-ex loss: 0.524454  [    6/  300]
per-ex loss: 0.511209  [    9/  300]
per-ex loss: 0.493695  [   12/  300]
per-ex loss: 0.559181  [   15/  300]
per-ex loss: 0.634981  [   18/  300]
per-ex loss: 0.466467  [   21/  300]
per-ex loss: 0.493777  [   24/  300]
per-ex loss: 0.534303  [   27/  300]
per-ex loss: 0.522574  [   30/  300]
per-ex loss: 0.649598  [   33/  300]
per-ex loss: 0.507723  [   36/  300]
per-ex loss: 0.726536  [   39/  300]
per-ex loss: 0.509480  [   42/  300]
per-ex loss: 0.562595  [   45/  300]
per-ex loss: 0.545796  [   48/  300]
per-ex loss: 0.551722  [   51/  300]
per-ex loss: 0.665240  [   54/  300]
per-ex loss: 0.669158  [   57/  300]
per-ex loss: 0.563905  [   60/  300]
per-ex loss: 0.696135  [   63/  300]
per-ex loss: 0.606573  [   66/  300]
per-ex loss: 0.627175  [   69/  300]
per-ex loss: 0.587453  [   72/  300]
per-ex loss: 0.591703  [   75/  300]
per-ex loss: 0.520428  [   78/  300]
per-ex loss: 0.575310  [   81/  300]
per-ex loss: 0.569252  [   84/  300]
per-ex loss: 0.446556  [   87/  300]
per-ex loss: 0.671372  [   90/  300]
per-ex loss: 0.720995  [   93/  300]
per-ex loss: 0.594905  [   96/  300]
per-ex loss: 0.652213  [   99/  300]
per-ex loss: 0.472412  [  102/  300]
per-ex loss: 0.605644  [  105/  300]
per-ex loss: 0.546628  [  108/  300]
per-ex loss: 0.639146  [  111/  300]
per-ex loss: 0.650678  [  114/  300]
per-ex loss: 0.526385  [  117/  300]
per-ex loss: 0.644042  [  120/  300]
per-ex loss: 0.607124  [  123/  300]
per-ex loss: 0.504978  [  126/  300]
per-ex loss: 0.518167  [  129/  300]
per-ex loss: 0.463436  [  132/  300]
per-ex loss: 0.511790  [  135/  300]
per-ex loss: 0.506782  [  138/  300]
per-ex loss: 0.635453  [  141/  300]
per-ex loss: 0.678372  [  144/  300]
per-ex loss: 0.717871  [  147/  300]
per-ex loss: 0.503547  [  150/  300]
per-ex loss: 0.535227  [  153/  300]
per-ex loss: 0.590136  [  156/  300]
per-ex loss: 0.551911  [  159/  300]
per-ex loss: 0.732896  [  162/  300]
per-ex loss: 0.468759  [  165/  300]
per-ex loss: 0.510284  [  168/  300]
per-ex loss: 0.508595  [  171/  300]
per-ex loss: 0.574755  [  174/  300]
per-ex loss: 0.570926  [  177/  300]
per-ex loss: 0.653879  [  180/  300]
per-ex loss: 0.685955  [  183/  300]
per-ex loss: 0.567887  [  186/  300]
per-ex loss: 0.554596  [  189/  300]
per-ex loss: 0.504250  [  192/  300]
per-ex loss: 0.642838  [  195/  300]
per-ex loss: 0.511111  [  198/  300]
per-ex loss: 0.639230  [  201/  300]
per-ex loss: 0.477086  [  204/  300]
per-ex loss: 0.630438  [  207/  300]
per-ex loss: 0.631991  [  210/  300]
per-ex loss: 0.527027  [  213/  300]
per-ex loss: 0.492515  [  216/  300]
per-ex loss: 0.650241  [  219/  300]
per-ex loss: 0.506672  [  222/  300]
per-ex loss: 0.573199  [  225/  300]
per-ex loss: 0.502362  [  228/  300]
per-ex loss: 0.513605  [  231/  300]
per-ex loss: 0.531710  [  234/  300]
per-ex loss: 0.611995  [  237/  300]
per-ex loss: 0.609482  [  240/  300]
per-ex loss: 0.467444  [  243/  300]
per-ex loss: 0.527137  [  246/  300]
per-ex loss: 0.765550  [  249/  300]
per-ex loss: 0.497076  [  252/  300]
per-ex loss: 0.630217  [  255/  300]
per-ex loss: 0.596920  [  258/  300]
per-ex loss: 0.427157  [  261/  300]
per-ex loss: 0.697782  [  264/  300]
per-ex loss: 0.477383  [  267/  300]
per-ex loss: 0.520572  [  270/  300]
per-ex loss: 0.573663  [  273/  300]
per-ex loss: 0.600345  [  276/  300]
per-ex loss: 0.520574  [  279/  300]
per-ex loss: 0.554391  [  282/  300]
per-ex loss: 0.699966  [  285/  300]
per-ex loss: 0.671766  [  288/  300]
per-ex loss: 0.523420  [  291/  300]
per-ex loss: 0.548105  [  294/  300]
per-ex loss: 0.541207  [  297/  300]
per-ex loss: 0.600146  [  300/  300]
Train Error: Avg loss: 0.57431601
validation Error: 
 Avg loss: 0.57125239 
 F1: 0.437490 
 Precision: 0.449233 
 Recall: 0.426344
 IoU: 0.279991

test Error: 
 Avg loss: 0.50592744 
 F1: 0.495621 
 Precision: 0.555827 
 Recall: 0.447183
 IoU: 0.329452

We have finished training iteration 22
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_20_.pth
per-ex loss: 0.461766  [    3/  300]
per-ex loss: 0.476164  [    6/  300]
per-ex loss: 0.552639  [    9/  300]
per-ex loss: 0.653730  [   12/  300]
per-ex loss: 0.718531  [   15/  300]
per-ex loss: 0.675921  [   18/  300]
per-ex loss: 0.598427  [   21/  300]
per-ex loss: 0.512754  [   24/  300]
per-ex loss: 0.496365  [   27/  300]
per-ex loss: 0.571470  [   30/  300]
per-ex loss: 0.610389  [   33/  300]
per-ex loss: 0.534099  [   36/  300]
per-ex loss: 0.497122  [   39/  300]
per-ex loss: 0.532008  [   42/  300]
per-ex loss: 0.451180  [   45/  300]
per-ex loss: 0.576046  [   48/  300]
per-ex loss: 0.540417  [   51/  300]
per-ex loss: 0.687311  [   54/  300]
per-ex loss: 0.429911  [   57/  300]
per-ex loss: 0.647113  [   60/  300]
per-ex loss: 0.456098  [   63/  300]
per-ex loss: 0.649274  [   66/  300]
per-ex loss: 0.606612  [   69/  300]
per-ex loss: 0.549277  [   72/  300]
per-ex loss: 0.595690  [   75/  300]
per-ex loss: 0.435917  [   78/  300]
per-ex loss: 0.685369  [   81/  300]
per-ex loss: 0.529188  [   84/  300]
per-ex loss: 0.568088  [   87/  300]
per-ex loss: 0.447568  [   90/  300]
per-ex loss: 0.586220  [   93/  300]
per-ex loss: 0.577620  [   96/  300]
per-ex loss: 0.528592  [   99/  300]
per-ex loss: 0.486937  [  102/  300]
per-ex loss: 0.487483  [  105/  300]
per-ex loss: 0.581479  [  108/  300]
per-ex loss: 0.511753  [  111/  300]
per-ex loss: 0.512018  [  114/  300]
per-ex loss: 0.707625  [  117/  300]
per-ex loss: 0.501422  [  120/  300]
per-ex loss: 0.592729  [  123/  300]
per-ex loss: 0.631312  [  126/  300]
per-ex loss: 0.710283  [  129/  300]
per-ex loss: 0.697408  [  132/  300]
per-ex loss: 0.646027  [  135/  300]
per-ex loss: 0.579642  [  138/  300]
per-ex loss: 0.636651  [  141/  300]
per-ex loss: 0.517258  [  144/  300]
per-ex loss: 0.620111  [  147/  300]
per-ex loss: 0.496576  [  150/  300]
per-ex loss: 0.581478  [  153/  300]
per-ex loss: 0.593974  [  156/  300]
per-ex loss: 0.677020  [  159/  300]
per-ex loss: 0.512782  [  162/  300]
per-ex loss: 0.556431  [  165/  300]
per-ex loss: 0.554455  [  168/  300]
per-ex loss: 0.577114  [  171/  300]
per-ex loss: 0.532325  [  174/  300]
per-ex loss: 0.469197  [  177/  300]
per-ex loss: 0.545996  [  180/  300]
per-ex loss: 0.669101  [  183/  300]
per-ex loss: 0.615362  [  186/  300]
per-ex loss: 0.613677  [  189/  300]
per-ex loss: 0.540781  [  192/  300]
per-ex loss: 0.557189  [  195/  300]
per-ex loss: 0.616694  [  198/  300]
per-ex loss: 0.540607  [  201/  300]
per-ex loss: 0.640480  [  204/  300]
per-ex loss: 0.523641  [  207/  300]
per-ex loss: 0.750940  [  210/  300]
per-ex loss: 0.558916  [  213/  300]
per-ex loss: 0.535458  [  216/  300]
per-ex loss: 0.577570  [  219/  300]
per-ex loss: 0.617675  [  222/  300]
per-ex loss: 0.481465  [  225/  300]
per-ex loss: 0.570011  [  228/  300]
per-ex loss: 0.621618  [  231/  300]
per-ex loss: 0.526654  [  234/  300]
per-ex loss: 0.655715  [  237/  300]
per-ex loss: 0.505694  [  240/  300]
per-ex loss: 0.423408  [  243/  300]
per-ex loss: 0.756467  [  246/  300]
per-ex loss: 0.513330  [  249/  300]
per-ex loss: 0.512149  [  252/  300]
per-ex loss: 0.558240  [  255/  300]
per-ex loss: 0.567978  [  258/  300]
per-ex loss: 0.541681  [  261/  300]
per-ex loss: 0.489785  [  264/  300]
per-ex loss: 0.541407  [  267/  300]
per-ex loss: 0.563223  [  270/  300]
per-ex loss: 0.557627  [  273/  300]
per-ex loss: 0.700228  [  276/  300]
per-ex loss: 0.490883  [  279/  300]
per-ex loss: 0.596999  [  282/  300]
per-ex loss: 0.543920  [  285/  300]
per-ex loss: 0.541616  [  288/  300]
per-ex loss: 0.584060  [  291/  300]
per-ex loss: 0.545165  [  294/  300]
per-ex loss: 0.714233  [  297/  300]
per-ex loss: 0.600761  [  300/  300]
Train Error: Avg loss: 0.57018768
validation Error: 
 Avg loss: 0.54481222 
 F1: 0.440413 
 Precision: 0.361447 
 Recall: 0.563527
 IoU: 0.282391

test Error: 
 Avg loss: 0.49270076 
 F1: 0.510400 
 Precision: 0.450441 
 Recall: 0.588773
 IoU: 0.342642

We have finished training iteration 23
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_21_.pth
per-ex loss: 0.638233  [    3/  300]
per-ex loss: 0.711592  [    6/  300]
per-ex loss: 0.520137  [    9/  300]
per-ex loss: 0.620448  [   12/  300]
per-ex loss: 0.510283  [   15/  300]
per-ex loss: 0.637979  [   18/  300]
per-ex loss: 0.527918  [   21/  300]
per-ex loss: 0.511389  [   24/  300]
per-ex loss: 0.694348  [   27/  300]
per-ex loss: 0.542769  [   30/  300]
per-ex loss: 0.586459  [   33/  300]
per-ex loss: 0.587315  [   36/  300]
per-ex loss: 0.553966  [   39/  300]
per-ex loss: 0.512832  [   42/  300]
per-ex loss: 0.612310  [   45/  300]
per-ex loss: 0.672743  [   48/  300]
per-ex loss: 0.448748  [   51/  300]
per-ex loss: 0.653198  [   54/  300]
per-ex loss: 0.543333  [   57/  300]
per-ex loss: 0.447086  [   60/  300]
per-ex loss: 0.452091  [   63/  300]
per-ex loss: 0.530958  [   66/  300]
per-ex loss: 0.664107  [   69/  300]
per-ex loss: 0.513435  [   72/  300]
per-ex loss: 0.503487  [   75/  300]
per-ex loss: 0.515536  [   78/  300]
per-ex loss: 0.697743  [   81/  300]
per-ex loss: 0.575069  [   84/  300]
per-ex loss: 0.679544  [   87/  300]
per-ex loss: 0.455643  [   90/  300]
per-ex loss: 0.675902  [   93/  300]
per-ex loss: 0.524180  [   96/  300]
per-ex loss: 0.511058  [   99/  300]
per-ex loss: 0.488512  [  102/  300]
per-ex loss: 0.419530  [  105/  300]
per-ex loss: 0.610371  [  108/  300]
per-ex loss: 0.561663  [  111/  300]
per-ex loss: 0.506601  [  114/  300]
per-ex loss: 0.515094  [  117/  300]
per-ex loss: 0.631199  [  120/  300]
per-ex loss: 0.599277  [  123/  300]
per-ex loss: 0.702444  [  126/  300]
per-ex loss: 0.530667  [  129/  300]
per-ex loss: 0.549785  [  132/  300]
per-ex loss: 0.521975  [  135/  300]
per-ex loss: 0.579384  [  138/  300]
per-ex loss: 0.581483  [  141/  300]
per-ex loss: 0.691363  [  144/  300]
per-ex loss: 0.716200  [  147/  300]
per-ex loss: 0.648893  [  150/  300]
per-ex loss: 0.530212  [  153/  300]
per-ex loss: 0.534799  [  156/  300]
per-ex loss: 0.561027  [  159/  300]
per-ex loss: 0.495301  [  162/  300]
per-ex loss: 0.560452  [  165/  300]
per-ex loss: 0.556978  [  168/  300]
per-ex loss: 0.624985  [  171/  300]
per-ex loss: 0.582090  [  174/  300]
per-ex loss: 0.612454  [  177/  300]
per-ex loss: 0.516377  [  180/  300]
per-ex loss: 0.512011  [  183/  300]
per-ex loss: 0.532340  [  186/  300]
per-ex loss: 0.608336  [  189/  300]
per-ex loss: 0.609224  [  192/  300]
per-ex loss: 0.461694  [  195/  300]
per-ex loss: 0.531045  [  198/  300]
per-ex loss: 0.630389  [  201/  300]
per-ex loss: 0.517418  [  204/  300]
per-ex loss: 0.605600  [  207/  300]
per-ex loss: 0.641289  [  210/  300]
per-ex loss: 0.603985  [  213/  300]
per-ex loss: 0.461502  [  216/  300]
per-ex loss: 0.646022  [  219/  300]
per-ex loss: 0.559667  [  222/  300]
per-ex loss: 0.511465  [  225/  300]
per-ex loss: 0.511569  [  228/  300]
per-ex loss: 0.488719  [  231/  300]
per-ex loss: 0.566536  [  234/  300]
per-ex loss: 0.548492  [  237/  300]
per-ex loss: 0.534216  [  240/  300]
per-ex loss: 0.557363  [  243/  300]
per-ex loss: 0.517121  [  246/  300]
per-ex loss: 0.521898  [  249/  300]
per-ex loss: 0.472639  [  252/  300]
per-ex loss: 0.550006  [  255/  300]
per-ex loss: 0.543675  [  258/  300]
per-ex loss: 0.644498  [  261/  300]
per-ex loss: 0.566211  [  264/  300]
per-ex loss: 0.622216  [  267/  300]
per-ex loss: 0.514309  [  270/  300]
per-ex loss: 0.632186  [  273/  300]
per-ex loss: 0.561496  [  276/  300]
per-ex loss: 0.556686  [  279/  300]
per-ex loss: 0.557065  [  282/  300]
per-ex loss: 0.532294  [  285/  300]
per-ex loss: 0.547637  [  288/  300]
per-ex loss: 0.570326  [  291/  300]
per-ex loss: 0.507506  [  294/  300]
per-ex loss: 0.510448  [  297/  300]
per-ex loss: 0.526223  [  300/  300]
Train Error: Avg loss: 0.56354280
validation Error: 
 Avg loss: 0.55676760 
 F1: 0.434695 
 Precision: 0.370353 
 Recall: 0.526093
 IoU: 0.277706

test Error: 
 Avg loss: 0.49542725 
 F1: 0.506832 
 Precision: 0.471389 
 Recall: 0.548038
 IoU: 0.339434

We have finished training iteration 24
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_22_.pth
per-ex loss: 0.528301  [    3/  300]
per-ex loss: 0.483177  [    6/  300]
per-ex loss: 0.448755  [    9/  300]
per-ex loss: 0.492195  [   12/  300]
per-ex loss: 0.562102  [   15/  300]
per-ex loss: 0.676376  [   18/  300]
per-ex loss: 0.639829  [   21/  300]
per-ex loss: 0.412237  [   24/  300]
per-ex loss: 0.605538  [   27/  300]
per-ex loss: 0.627905  [   30/  300]
per-ex loss: 0.666001  [   33/  300]
per-ex loss: 0.454588  [   36/  300]
per-ex loss: 0.562007  [   39/  300]
per-ex loss: 0.495838  [   42/  300]
per-ex loss: 0.627846  [   45/  300]
per-ex loss: 0.583461  [   48/  300]
per-ex loss: 0.667212  [   51/  300]
per-ex loss: 0.479074  [   54/  300]
per-ex loss: 0.566584  [   57/  300]
per-ex loss: 0.602416  [   60/  300]
per-ex loss: 0.509870  [   63/  300]
per-ex loss: 0.738013  [   66/  300]
per-ex loss: 0.688426  [   69/  300]
per-ex loss: 0.590702  [   72/  300]
per-ex loss: 0.538017  [   75/  300]
per-ex loss: 0.653463  [   78/  300]
per-ex loss: 0.544855  [   81/  300]
per-ex loss: 0.476432  [   84/  300]
per-ex loss: 0.683143  [   87/  300]
per-ex loss: 0.554993  [   90/  300]
per-ex loss: 0.556464  [   93/  300]
per-ex loss: 0.450447  [   96/  300]
per-ex loss: 0.594451  [   99/  300]
per-ex loss: 0.706182  [  102/  300]
per-ex loss: 0.499829  [  105/  300]
per-ex loss: 0.567986  [  108/  300]
per-ex loss: 0.612220  [  111/  300]
per-ex loss: 0.497330  [  114/  300]
per-ex loss: 0.710869  [  117/  300]
per-ex loss: 0.535506  [  120/  300]
per-ex loss: 0.477618  [  123/  300]
per-ex loss: 0.721588  [  126/  300]
per-ex loss: 0.558326  [  129/  300]
per-ex loss: 0.493854  [  132/  300]
per-ex loss: 0.503380  [  135/  300]
per-ex loss: 0.520934  [  138/  300]
per-ex loss: 0.620803  [  141/  300]
per-ex loss: 0.591217  [  144/  300]
per-ex loss: 0.498784  [  147/  300]
per-ex loss: 0.582845  [  150/  300]
per-ex loss: 0.619385  [  153/  300]
per-ex loss: 0.587418  [  156/  300]
per-ex loss: 0.552943  [  159/  300]
per-ex loss: 0.439254  [  162/  300]
per-ex loss: 0.592885  [  165/  300]
per-ex loss: 0.578171  [  168/  300]
per-ex loss: 0.561422  [  171/  300]
per-ex loss: 0.639559  [  174/  300]
per-ex loss: 0.426405  [  177/  300]
per-ex loss: 0.480711  [  180/  300]
per-ex loss: 0.596179  [  183/  300]
per-ex loss: 0.674181  [  186/  300]
per-ex loss: 0.617830  [  189/  300]
per-ex loss: 0.466884  [  192/  300]
per-ex loss: 0.680494  [  195/  300]
per-ex loss: 0.618437  [  198/  300]
per-ex loss: 0.480854  [  201/  300]
per-ex loss: 0.632843  [  204/  300]
per-ex loss: 0.680314  [  207/  300]
per-ex loss: 0.564591  [  210/  300]
per-ex loss: 0.477745  [  213/  300]
per-ex loss: 0.489103  [  216/  300]
per-ex loss: 0.549583  [  219/  300]
per-ex loss: 0.563788  [  222/  300]
per-ex loss: 0.522596  [  225/  300]
per-ex loss: 0.540495  [  228/  300]
per-ex loss: 0.673611  [  231/  300]
per-ex loss: 0.526195  [  234/  300]
per-ex loss: 0.557485  [  237/  300]
per-ex loss: 0.577810  [  240/  300]
per-ex loss: 0.543152  [  243/  300]
per-ex loss: 0.495746  [  246/  300]
per-ex loss: 0.559443  [  249/  300]
per-ex loss: 0.458037  [  252/  300]
per-ex loss: 0.637127  [  255/  300]
per-ex loss: 0.455579  [  258/  300]
per-ex loss: 0.599398  [  261/  300]
per-ex loss: 0.663209  [  264/  300]
per-ex loss: 0.601135  [  267/  300]
per-ex loss: 0.627195  [  270/  300]
per-ex loss: 0.690317  [  273/  300]
per-ex loss: 0.732425  [  276/  300]
per-ex loss: 0.531175  [  279/  300]
per-ex loss: 0.450089  [  282/  300]
per-ex loss: 0.519191  [  285/  300]
per-ex loss: 0.484558  [  288/  300]
per-ex loss: 0.470215  [  291/  300]
per-ex loss: 0.552954  [  294/  300]
per-ex loss: 0.507217  [  297/  300]
per-ex loss: 0.581143  [  300/  300]
Train Error: Avg loss: 0.56588468
validation Error: 
 Avg loss: 0.54750093 
 F1: 0.451063 
 Precision: 0.429871 
 Recall: 0.474453
 IoU: 0.291208

test Error: 
 Avg loss: 0.48560280 
 F1: 0.515929 
 Precision: 0.536476 
 Recall: 0.496898
 IoU: 0.347645

We have finished training iteration 25
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_16_.pth
per-ex loss: 0.618973  [    3/  300]
per-ex loss: 0.533749  [    6/  300]
per-ex loss: 0.568871  [    9/  300]
per-ex loss: 0.464240  [   12/  300]
per-ex loss: 0.460040  [   15/  300]
per-ex loss: 0.463127  [   18/  300]
per-ex loss: 0.568681  [   21/  300]
per-ex loss: 0.564284  [   24/  300]
per-ex loss: 0.530125  [   27/  300]
per-ex loss: 0.573995  [   30/  300]
per-ex loss: 0.681747  [   33/  300]
per-ex loss: 0.478883  [   36/  300]
per-ex loss: 0.618099  [   39/  300]
per-ex loss: 0.603813  [   42/  300]
per-ex loss: 0.586980  [   45/  300]
per-ex loss: 0.722950  [   48/  300]
per-ex loss: 0.622661  [   51/  300]
per-ex loss: 0.680609  [   54/  300]
per-ex loss: 0.594594  [   57/  300]
per-ex loss: 0.489482  [   60/  300]
per-ex loss: 0.485135  [   63/  300]
per-ex loss: 0.604297  [   66/  300]
per-ex loss: 0.520727  [   69/  300]
per-ex loss: 0.624712  [   72/  300]
per-ex loss: 0.478191  [   75/  300]
per-ex loss: 0.612274  [   78/  300]
per-ex loss: 0.437467  [   81/  300]
per-ex loss: 0.446137  [   84/  300]
per-ex loss: 0.691913  [   87/  300]
per-ex loss: 0.545123  [   90/  300]
per-ex loss: 0.521152  [   93/  300]
per-ex loss: 0.553981  [   96/  300]
per-ex loss: 0.575725  [   99/  300]
per-ex loss: 0.510468  [  102/  300]
per-ex loss: 0.538570  [  105/  300]
per-ex loss: 0.666230  [  108/  300]
per-ex loss: 0.665353  [  111/  300]
per-ex loss: 0.580487  [  114/  300]
per-ex loss: 0.537458  [  117/  300]
per-ex loss: 0.579199  [  120/  300]
per-ex loss: 0.558477  [  123/  300]
per-ex loss: 0.584186  [  126/  300]
per-ex loss: 0.656844  [  129/  300]
per-ex loss: 0.574153  [  132/  300]
per-ex loss: 0.553607  [  135/  300]
per-ex loss: 0.546565  [  138/  300]
per-ex loss: 0.515417  [  141/  300]
per-ex loss: 0.548657  [  144/  300]
per-ex loss: 0.638307  [  147/  300]
per-ex loss: 0.614658  [  150/  300]
per-ex loss: 0.504800  [  153/  300]
per-ex loss: 0.465122  [  156/  300]
per-ex loss: 0.699341  [  159/  300]
per-ex loss: 0.424201  [  162/  300]
per-ex loss: 0.488449  [  165/  300]
per-ex loss: 0.517771  [  168/  300]
per-ex loss: 0.593665  [  171/  300]
per-ex loss: 0.445289  [  174/  300]
per-ex loss: 0.582940  [  177/  300]
per-ex loss: 0.551693  [  180/  300]
per-ex loss: 0.599869  [  183/  300]
per-ex loss: 0.632262  [  186/  300]
per-ex loss: 0.524265  [  189/  300]
per-ex loss: 0.576760  [  192/  300]
per-ex loss: 0.549519  [  195/  300]
per-ex loss: 0.462188  [  198/  300]
per-ex loss: 0.724009  [  201/  300]
per-ex loss: 0.579337  [  204/  300]
per-ex loss: 0.601456  [  207/  300]
per-ex loss: 0.543386  [  210/  300]
per-ex loss: 0.533111  [  213/  300]
per-ex loss: 0.545936  [  216/  300]
per-ex loss: 0.514705  [  219/  300]
per-ex loss: 0.610883  [  222/  300]
per-ex loss: 0.568365  [  225/  300]
per-ex loss: 0.544936  [  228/  300]
per-ex loss: 0.645642  [  231/  300]
per-ex loss: 0.685002  [  234/  300]
per-ex loss: 0.530650  [  237/  300]
per-ex loss: 0.516131  [  240/  300]
per-ex loss: 0.480122  [  243/  300]
per-ex loss: 0.574334  [  246/  300]
per-ex loss: 0.494392  [  249/  300]
per-ex loss: 0.665204  [  252/  300]
per-ex loss: 0.494556  [  255/  300]
per-ex loss: 0.549271  [  258/  300]
per-ex loss: 0.522768  [  261/  300]
per-ex loss: 0.592131  [  264/  300]
per-ex loss: 0.460829  [  267/  300]
per-ex loss: 0.590298  [  270/  300]
per-ex loss: 0.564249  [  273/  300]
per-ex loss: 0.486218  [  276/  300]
per-ex loss: 0.746314  [  279/  300]
per-ex loss: 0.528587  [  282/  300]
per-ex loss: 0.493789  [  285/  300]
per-ex loss: 0.602620  [  288/  300]
per-ex loss: 0.459539  [  291/  300]
per-ex loss: 0.591201  [  294/  300]
per-ex loss: 0.607571  [  297/  300]
per-ex loss: 0.546648  [  300/  300]
Train Error: Avg loss: 0.56203692
validation Error: 
 Avg loss: 0.55994296 
 F1: 0.436419 
 Precision: 0.396254 
 Recall: 0.485645
 IoU: 0.279115

test Error: 
 Avg loss: 0.48906744 
 F1: 0.512804 
 Precision: 0.503628 
 Recall: 0.522322
 IoU: 0.344813

We have finished training iteration 26
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_24_.pth
per-ex loss: 0.467389  [    3/  300]
per-ex loss: 0.583472  [    6/  300]
per-ex loss: 0.542105  [    9/  300]
per-ex loss: 0.568370  [   12/  300]
per-ex loss: 0.507434  [   15/  300]
per-ex loss: 0.502551  [   18/  300]
per-ex loss: 0.451815  [   21/  300]
per-ex loss: 0.588871  [   24/  300]
per-ex loss: 0.582202  [   27/  300]
per-ex loss: 0.635396  [   30/  300]
per-ex loss: 0.477919  [   33/  300]
per-ex loss: 0.544399  [   36/  300]
per-ex loss: 0.559679  [   39/  300]
per-ex loss: 0.642310  [   42/  300]
per-ex loss: 0.571366  [   45/  300]
per-ex loss: 0.678312  [   48/  300]
per-ex loss: 0.635417  [   51/  300]
per-ex loss: 0.671086  [   54/  300]
per-ex loss: 0.521565  [   57/  300]
per-ex loss: 0.538598  [   60/  300]
per-ex loss: 0.502504  [   63/  300]
per-ex loss: 0.606846  [   66/  300]
per-ex loss: 0.527437  [   69/  300]
per-ex loss: 0.659602  [   72/  300]
per-ex loss: 0.597856  [   75/  300]
per-ex loss: 0.460616  [   78/  300]
per-ex loss: 0.673716  [   81/  300]
per-ex loss: 0.600896  [   84/  300]
per-ex loss: 0.650325  [   87/  300]
per-ex loss: 0.468261  [   90/  300]
per-ex loss: 0.692514  [   93/  300]
per-ex loss: 0.679061  [   96/  300]
per-ex loss: 0.549733  [   99/  300]
per-ex loss: 0.541731  [  102/  300]
per-ex loss: 0.542170  [  105/  300]
per-ex loss: 0.646620  [  108/  300]
per-ex loss: 0.592788  [  111/  300]
per-ex loss: 0.560695  [  114/  300]
per-ex loss: 0.613664  [  117/  300]
per-ex loss: 0.456910  [  120/  300]
per-ex loss: 0.534389  [  123/  300]
per-ex loss: 0.533021  [  126/  300]
per-ex loss: 0.523646  [  129/  300]
per-ex loss: 0.587723  [  132/  300]
per-ex loss: 0.724487  [  135/  300]
per-ex loss: 0.569017  [  138/  300]
per-ex loss: 0.517271  [  141/  300]
per-ex loss: 0.626179  [  144/  300]
per-ex loss: 0.460958  [  147/  300]
per-ex loss: 0.517868  [  150/  300]
per-ex loss: 0.676045  [  153/  300]
per-ex loss: 0.501033  [  156/  300]
per-ex loss: 0.524529  [  159/  300]
per-ex loss: 0.613579  [  162/  300]
per-ex loss: 0.676823  [  165/  300]
per-ex loss: 0.492495  [  168/  300]
per-ex loss: 0.461118  [  171/  300]
per-ex loss: 0.552217  [  174/  300]
per-ex loss: 0.661955  [  177/  300]
per-ex loss: 0.729320  [  180/  300]
per-ex loss: 0.503464  [  183/  300]
per-ex loss: 0.594861  [  186/  300]
per-ex loss: 0.736665  [  189/  300]
per-ex loss: 0.649615  [  192/  300]
per-ex loss: 0.536561  [  195/  300]
per-ex loss: 0.605029  [  198/  300]
per-ex loss: 0.559145  [  201/  300]
per-ex loss: 0.631007  [  204/  300]
per-ex loss: 0.598827  [  207/  300]
per-ex loss: 0.560935  [  210/  300]
per-ex loss: 0.577586  [  213/  300]
per-ex loss: 0.515293  [  216/  300]
per-ex loss: 0.633357  [  219/  300]
per-ex loss: 0.549868  [  222/  300]
per-ex loss: 0.616973  [  225/  300]
per-ex loss: 0.531831  [  228/  300]
per-ex loss: 0.614579  [  231/  300]
per-ex loss: 0.549601  [  234/  300]
per-ex loss: 0.450692  [  237/  300]
per-ex loss: 0.558712  [  240/  300]
per-ex loss: 0.752294  [  243/  300]
per-ex loss: 0.476410  [  246/  300]
per-ex loss: 0.453002  [  249/  300]
per-ex loss: 0.634246  [  252/  300]
per-ex loss: 0.728685  [  255/  300]
per-ex loss: 0.548886  [  258/  300]
per-ex loss: 0.454277  [  261/  300]
per-ex loss: 0.590798  [  264/  300]
per-ex loss: 0.533913  [  267/  300]
per-ex loss: 0.652002  [  270/  300]
per-ex loss: 0.549742  [  273/  300]
per-ex loss: 0.608728  [  276/  300]
per-ex loss: 0.595635  [  279/  300]
per-ex loss: 0.569753  [  282/  300]
per-ex loss: 0.535936  [  285/  300]
per-ex loss: 0.584686  [  288/  300]
per-ex loss: 0.542913  [  291/  300]
per-ex loss: 0.645448  [  294/  300]
per-ex loss: 0.492721  [  297/  300]
per-ex loss: 0.474983  [  300/  300]
Train Error: Avg loss: 0.57377532
validation Error: 
 Avg loss: 0.54161974 
 F1: 0.439262 
 Precision: 0.359483 
 Recall: 0.564552
 IoU: 0.281445

test Error: 
 Avg loss: 0.49700022 
 F1: 0.505314 
 Precision: 0.434403 
 Recall: 0.603892
 IoU: 0.338074

We have finished training iteration 27
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_11_.pth
per-ex loss: 0.444943  [    3/  300]
per-ex loss: 0.745732  [    6/  300]
per-ex loss: 0.508464  [    9/  300]
per-ex loss: 0.734093  [   12/  300]
per-ex loss: 0.513618  [   15/  300]
per-ex loss: 0.539564  [   18/  300]
per-ex loss: 0.705071  [   21/  300]
per-ex loss: 0.488315  [   24/  300]
per-ex loss: 0.469309  [   27/  300]
per-ex loss: 0.503429  [   30/  300]
per-ex loss: 0.498995  [   33/  300]
per-ex loss: 0.575970  [   36/  300]
per-ex loss: 0.495543  [   39/  300]
per-ex loss: 0.506989  [   42/  300]
per-ex loss: 0.522293  [   45/  300]
per-ex loss: 0.507837  [   48/  300]
per-ex loss: 0.530210  [   51/  300]
per-ex loss: 0.586690  [   54/  300]
per-ex loss: 0.454649  [   57/  300]
per-ex loss: 0.514025  [   60/  300]
per-ex loss: 0.461639  [   63/  300]
per-ex loss: 0.521567  [   66/  300]
per-ex loss: 0.678022  [   69/  300]
per-ex loss: 0.703316  [   72/  300]
per-ex loss: 0.756139  [   75/  300]
per-ex loss: 0.621806  [   78/  300]
per-ex loss: 0.627135  [   81/  300]
per-ex loss: 0.585099  [   84/  300]
per-ex loss: 0.624257  [   87/  300]
per-ex loss: 0.537205  [   90/  300]
per-ex loss: 0.580364  [   93/  300]
per-ex loss: 0.551507  [   96/  300]
per-ex loss: 0.759440  [   99/  300]
per-ex loss: 0.627188  [  102/  300]
per-ex loss: 0.640112  [  105/  300]
per-ex loss: 0.734176  [  108/  300]
per-ex loss: 0.559871  [  111/  300]
per-ex loss: 0.621327  [  114/  300]
per-ex loss: 0.615958  [  117/  300]
per-ex loss: 0.513905  [  120/  300]
per-ex loss: 0.546144  [  123/  300]
per-ex loss: 0.503519  [  126/  300]
per-ex loss: 0.520526  [  129/  300]
per-ex loss: 0.625948  [  132/  300]
per-ex loss: 0.647872  [  135/  300]
per-ex loss: 0.516003  [  138/  300]
per-ex loss: 0.530036  [  141/  300]
per-ex loss: 0.493202  [  144/  300]
per-ex loss: 0.508509  [  147/  300]
per-ex loss: 0.721772  [  150/  300]
per-ex loss: 0.622914  [  153/  300]
per-ex loss: 0.463241  [  156/  300]
per-ex loss: 0.704589  [  159/  300]
per-ex loss: 0.488386  [  162/  300]
per-ex loss: 0.566021  [  165/  300]
per-ex loss: 0.653978  [  168/  300]
per-ex loss: 0.510131  [  171/  300]
per-ex loss: 0.537100  [  174/  300]
per-ex loss: 0.524517  [  177/  300]
per-ex loss: 0.607113  [  180/  300]
per-ex loss: 0.648586  [  183/  300]
per-ex loss: 0.561050  [  186/  300]
per-ex loss: 0.640670  [  189/  300]
per-ex loss: 0.482637  [  192/  300]
per-ex loss: 0.508247  [  195/  300]
per-ex loss: 0.578251  [  198/  300]
per-ex loss: 0.530554  [  201/  300]
per-ex loss: 0.524089  [  204/  300]
per-ex loss: 0.585205  [  207/  300]
per-ex loss: 0.573410  [  210/  300]
per-ex loss: 0.472512  [  213/  300]
per-ex loss: 0.513367  [  216/  300]
per-ex loss: 0.553766  [  219/  300]
per-ex loss: 0.549624  [  222/  300]
per-ex loss: 0.613196  [  225/  300]
per-ex loss: 0.536515  [  228/  300]
per-ex loss: 0.539567  [  231/  300]
per-ex loss: 0.525513  [  234/  300]
per-ex loss: 0.695205  [  237/  300]
per-ex loss: 0.691719  [  240/  300]
per-ex loss: 0.634670  [  243/  300]
per-ex loss: 0.476369  [  246/  300]
per-ex loss: 0.538314  [  249/  300]
per-ex loss: 0.619410  [  252/  300]
per-ex loss: 0.486004  [  255/  300]
per-ex loss: 0.580964  [  258/  300]
per-ex loss: 0.460085  [  261/  300]
per-ex loss: 0.466220  [  264/  300]
per-ex loss: 0.511872  [  267/  300]
per-ex loss: 0.523451  [  270/  300]
per-ex loss: 0.604836  [  273/  300]
per-ex loss: 0.545328  [  276/  300]
per-ex loss: 0.670922  [  279/  300]
per-ex loss: 0.499736  [  282/  300]
per-ex loss: 0.629099  [  285/  300]
per-ex loss: 0.670693  [  288/  300]
per-ex loss: 0.537413  [  291/  300]
per-ex loss: 0.726007  [  294/  300]
per-ex loss: 0.530293  [  297/  300]
per-ex loss: 0.495819  [  300/  300]
Train Error: Avg loss: 0.56988481
validation Error: 
 Avg loss: 0.55001177 
 F1: 0.440894 
 Precision: 0.377189 
 Recall: 0.530492
 IoU: 0.282787

test Error: 
 Avg loss: 0.48478353 
 F1: 0.517100 
 Precision: 0.484014 
 Recall: 0.555043
 IoU: 0.348709

We have finished training iteration 28
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_26_.pth
per-ex loss: 0.590118  [    3/  300]
per-ex loss: 0.665156  [    6/  300]
per-ex loss: 0.694792  [    9/  300]
per-ex loss: 0.456106  [   12/  300]
per-ex loss: 0.513805  [   15/  300]
per-ex loss: 0.643534  [   18/  300]
per-ex loss: 0.509507  [   21/  300]
per-ex loss: 0.536311  [   24/  300]
per-ex loss: 0.456467  [   27/  300]
per-ex loss: 0.633370  [   30/  300]
per-ex loss: 0.589069  [   33/  300]
per-ex loss: 0.570795  [   36/  300]
per-ex loss: 0.506246  [   39/  300]
per-ex loss: 0.518402  [   42/  300]
per-ex loss: 0.502645  [   45/  300]
per-ex loss: 0.583401  [   48/  300]
per-ex loss: 0.490035  [   51/  300]
per-ex loss: 0.555929  [   54/  300]
per-ex loss: 0.540144  [   57/  300]
per-ex loss: 0.720680  [   60/  300]
per-ex loss: 0.676792  [   63/  300]
per-ex loss: 0.630618  [   66/  300]
per-ex loss: 0.469854  [   69/  300]
per-ex loss: 0.698423  [   72/  300]
per-ex loss: 0.554044  [   75/  300]
per-ex loss: 0.498921  [   78/  300]
per-ex loss: 0.582937  [   81/  300]
per-ex loss: 0.434823  [   84/  300]
per-ex loss: 0.585177  [   87/  300]
per-ex loss: 0.651242  [   90/  300]
per-ex loss: 0.439881  [   93/  300]
per-ex loss: 0.545751  [   96/  300]
per-ex loss: 0.474344  [   99/  300]
per-ex loss: 0.566399  [  102/  300]
per-ex loss: 0.453332  [  105/  300]
per-ex loss: 0.629012  [  108/  300]
per-ex loss: 0.561096  [  111/  300]
per-ex loss: 0.580662  [  114/  300]
per-ex loss: 0.746979  [  117/  300]
per-ex loss: 0.523111  [  120/  300]
per-ex loss: 0.531109  [  123/  300]
per-ex loss: 0.636062  [  126/  300]
per-ex loss: 0.585952  [  129/  300]
per-ex loss: 0.478120  [  132/  300]
per-ex loss: 0.485737  [  135/  300]
per-ex loss: 0.647413  [  138/  300]
per-ex loss: 0.502855  [  141/  300]
per-ex loss: 0.556809  [  144/  300]
per-ex loss: 0.649245  [  147/  300]
per-ex loss: 0.631975  [  150/  300]
per-ex loss: 0.592912  [  153/  300]
per-ex loss: 0.672315  [  156/  300]
per-ex loss: 0.497307  [  159/  300]
per-ex loss: 0.491179  [  162/  300]
per-ex loss: 0.611694  [  165/  300]
per-ex loss: 0.648883  [  168/  300]
per-ex loss: 0.503705  [  171/  300]
per-ex loss: 0.628988  [  174/  300]
per-ex loss: 0.523345  [  177/  300]
per-ex loss: 0.712614  [  180/  300]
per-ex loss: 0.645184  [  183/  300]
per-ex loss: 0.492961  [  186/  300]
per-ex loss: 0.558697  [  189/  300]
per-ex loss: 0.524354  [  192/  300]
per-ex loss: 0.683876  [  195/  300]
per-ex loss: 0.438959  [  198/  300]
per-ex loss: 0.518082  [  201/  300]
per-ex loss: 0.458645  [  204/  300]
per-ex loss: 0.634966  [  207/  300]
per-ex loss: 0.729475  [  210/  300]
per-ex loss: 0.645316  [  213/  300]
per-ex loss: 0.605199  [  216/  300]
per-ex loss: 0.605375  [  219/  300]
per-ex loss: 0.628451  [  222/  300]
per-ex loss: 0.551207  [  225/  300]
per-ex loss: 0.660498  [  228/  300]
per-ex loss: 0.473766  [  231/  300]
per-ex loss: 0.549385  [  234/  300]
per-ex loss: 0.524300  [  237/  300]
per-ex loss: 0.484225  [  240/  300]
per-ex loss: 0.492055  [  243/  300]
per-ex loss: 0.593739  [  246/  300]
per-ex loss: 0.485385  [  249/  300]
per-ex loss: 0.597772  [  252/  300]
per-ex loss: 0.459540  [  255/  300]
per-ex loss: 0.648105  [  258/  300]
per-ex loss: 0.528205  [  261/  300]
per-ex loss: 0.582828  [  264/  300]
per-ex loss: 0.527177  [  267/  300]
per-ex loss: 0.501046  [  270/  300]
per-ex loss: 0.561800  [  273/  300]
per-ex loss: 0.581489  [  276/  300]
per-ex loss: 0.480898  [  279/  300]
per-ex loss: 0.590546  [  282/  300]
per-ex loss: 0.578100  [  285/  300]
per-ex loss: 0.593225  [  288/  300]
per-ex loss: 0.512561  [  291/  300]
per-ex loss: 0.469000  [  294/  300]
per-ex loss: 0.617497  [  297/  300]
per-ex loss: 0.584582  [  300/  300]
Train Error: Avg loss: 0.56566600
validation Error: 
 Avg loss: 0.55017763 
 F1: 0.445214 
 Precision: 0.402148 
 Recall: 0.498610
 IoU: 0.286351

test Error: 
 Avg loss: 0.48039210 
 F1: 0.521216 
 Precision: 0.507158 
 Recall: 0.536076
 IoU: 0.352463

We have finished training iteration 29
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_25_.pth
per-ex loss: 0.555600  [    3/  300]
per-ex loss: 0.466211  [    6/  300]
per-ex loss: 0.557035  [    9/  300]
per-ex loss: 0.642389  [   12/  300]
per-ex loss: 0.587613  [   15/  300]
per-ex loss: 0.519334  [   18/  300]
per-ex loss: 0.638711  [   21/  300]
per-ex loss: 0.647322  [   24/  300]
per-ex loss: 0.466606  [   27/  300]
per-ex loss: 0.561935  [   30/  300]
per-ex loss: 0.515865  [   33/  300]
per-ex loss: 0.584499  [   36/  300]
per-ex loss: 0.590039  [   39/  300]
per-ex loss: 0.478264  [   42/  300]
per-ex loss: 0.731864  [   45/  300]
per-ex loss: 0.577354  [   48/  300]
per-ex loss: 0.506337  [   51/  300]
per-ex loss: 0.548002  [   54/  300]
per-ex loss: 0.556377  [   57/  300]
per-ex loss: 0.482463  [   60/  300]
per-ex loss: 0.500681  [   63/  300]
per-ex loss: 0.688240  [   66/  300]
per-ex loss: 0.631670  [   69/  300]
per-ex loss: 0.645117  [   72/  300]
per-ex loss: 0.597593  [   75/  300]
per-ex loss: 0.601189  [   78/  300]
per-ex loss: 0.548715  [   81/  300]
per-ex loss: 0.717320  [   84/  300]
per-ex loss: 0.529753  [   87/  300]
per-ex loss: 0.589206  [   90/  300]
per-ex loss: 0.684092  [   93/  300]
per-ex loss: 0.528906  [   96/  300]
per-ex loss: 0.544230  [   99/  300]
per-ex loss: 0.596672  [  102/  300]
per-ex loss: 0.610575  [  105/  300]
per-ex loss: 0.485071  [  108/  300]
per-ex loss: 0.567842  [  111/  300]
per-ex loss: 0.541883  [  114/  300]
per-ex loss: 0.546515  [  117/  300]
per-ex loss: 0.538417  [  120/  300]
per-ex loss: 0.571786  [  123/  300]
per-ex loss: 0.504879  [  126/  300]
per-ex loss: 0.517923  [  129/  300]
per-ex loss: 0.614563  [  132/  300]
per-ex loss: 0.477383  [  135/  300]
per-ex loss: 0.600924  [  138/  300]
per-ex loss: 0.663759  [  141/  300]
per-ex loss: 0.594860  [  144/  300]
per-ex loss: 0.641843  [  147/  300]
per-ex loss: 0.483666  [  150/  300]
per-ex loss: 0.505337  [  153/  300]
per-ex loss: 0.669376  [  156/  300]
per-ex loss: 0.530888  [  159/  300]
per-ex loss: 0.646504  [  162/  300]
per-ex loss: 0.591902  [  165/  300]
per-ex loss: 0.478650  [  168/  300]
per-ex loss: 0.612586  [  171/  300]
per-ex loss: 0.510072  [  174/  300]
per-ex loss: 0.618266  [  177/  300]
per-ex loss: 0.579451  [  180/  300]
per-ex loss: 0.541867  [  183/  300]
per-ex loss: 0.656413  [  186/  300]
per-ex loss: 0.739239  [  189/  300]
per-ex loss: 0.520398  [  192/  300]
per-ex loss: 0.542164  [  195/  300]
per-ex loss: 0.472838  [  198/  300]
per-ex loss: 0.470586  [  201/  300]
per-ex loss: 0.537747  [  204/  300]
per-ex loss: 0.537526  [  207/  300]
per-ex loss: 0.634134  [  210/  300]
per-ex loss: 0.715335  [  213/  300]
per-ex loss: 0.477406  [  216/  300]
per-ex loss: 0.580555  [  219/  300]
per-ex loss: 0.544592  [  222/  300]
per-ex loss: 0.533592  [  225/  300]
per-ex loss: 0.525910  [  228/  300]
per-ex loss: 0.630754  [  231/  300]
per-ex loss: 0.487723  [  234/  300]
per-ex loss: 0.559393  [  237/  300]
per-ex loss: 0.526792  [  240/  300]
per-ex loss: 0.575156  [  243/  300]
per-ex loss: 0.527860  [  246/  300]
per-ex loss: 0.672500  [  249/  300]
per-ex loss: 0.664001  [  252/  300]
per-ex loss: 0.611930  [  255/  300]
per-ex loss: 0.592926  [  258/  300]
per-ex loss: 0.479656  [  261/  300]
per-ex loss: 0.675573  [  264/  300]
per-ex loss: 0.568904  [  267/  300]
per-ex loss: 0.622457  [  270/  300]
per-ex loss: 0.447831  [  273/  300]
per-ex loss: 0.521679  [  276/  300]
per-ex loss: 0.541526  [  279/  300]
per-ex loss: 0.499230  [  282/  300]
per-ex loss: 0.630125  [  285/  300]
per-ex loss: 0.698283  [  288/  300]
per-ex loss: 0.437443  [  291/  300]
per-ex loss: 0.504288  [  294/  300]
per-ex loss: 0.559614  [  297/  300]
per-ex loss: 0.627438  [  300/  300]
Train Error: Avg loss: 0.57015507
validation Error: 
 Avg loss: 0.54744486 
 F1: 0.447024 
 Precision: 0.382464 
 Recall: 0.537805
 IoU: 0.287850

test Error: 
 Avg loss: 0.49596310 
 F1: 0.505708 
 Precision: 0.456057 
 Recall: 0.567490
 IoU: 0.338426

We have finished training iteration 30
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_28_.pth
per-ex loss: 0.527962  [    3/  300]
per-ex loss: 0.571974  [    6/  300]
per-ex loss: 0.507244  [    9/  300]
per-ex loss: 0.472297  [   12/  300]
per-ex loss: 0.566155  [   15/  300]
per-ex loss: 0.641832  [   18/  300]
per-ex loss: 0.544432  [   21/  300]
per-ex loss: 0.649886  [   24/  300]
per-ex loss: 0.516590  [   27/  300]
per-ex loss: 0.596308  [   30/  300]
per-ex loss: 0.567277  [   33/  300]
per-ex loss: 0.641666  [   36/  300]
per-ex loss: 0.539774  [   39/  300]
per-ex loss: 0.714143  [   42/  300]
per-ex loss: 0.526119  [   45/  300]
per-ex loss: 0.475645  [   48/  300]
per-ex loss: 0.491111  [   51/  300]
per-ex loss: 0.460716  [   54/  300]
per-ex loss: 0.554830  [   57/  300]
per-ex loss: 0.570881  [   60/  300]
per-ex loss: 0.571823  [   63/  300]
per-ex loss: 0.516244  [   66/  300]
per-ex loss: 0.497195  [   69/  300]
per-ex loss: 0.686116  [   72/  300]
per-ex loss: 0.646472  [   75/  300]
per-ex loss: 0.522323  [   78/  300]
per-ex loss: 0.532830  [   81/  300]
per-ex loss: 0.487439  [   84/  300]
per-ex loss: 0.507412  [   87/  300]
per-ex loss: 0.507506  [   90/  300]
per-ex loss: 0.510380  [   93/  300]
per-ex loss: 0.666310  [   96/  300]
per-ex loss: 0.498886  [   99/  300]
per-ex loss: 0.499990  [  102/  300]
per-ex loss: 0.595026  [  105/  300]
per-ex loss: 0.476260  [  108/  300]
per-ex loss: 0.450709  [  111/  300]
per-ex loss: 0.618085  [  114/  300]
per-ex loss: 0.481187  [  117/  300]
per-ex loss: 0.600628  [  120/  300]
per-ex loss: 0.488079  [  123/  300]
per-ex loss: 0.478073  [  126/  300]
per-ex loss: 0.759804  [  129/  300]
per-ex loss: 0.502472  [  132/  300]
per-ex loss: 0.570477  [  135/  300]
per-ex loss: 0.686303  [  138/  300]
per-ex loss: 0.583369  [  141/  300]
per-ex loss: 0.623479  [  144/  300]
per-ex loss: 0.545377  [  147/  300]
per-ex loss: 0.519217  [  150/  300]
per-ex loss: 0.454985  [  153/  300]
per-ex loss: 0.581458  [  156/  300]
per-ex loss: 0.664581  [  159/  300]
per-ex loss: 0.572516  [  162/  300]
per-ex loss: 0.512780  [  165/  300]
per-ex loss: 0.655043  [  168/  300]
per-ex loss: 0.552350  [  171/  300]
per-ex loss: 0.665720  [  174/  300]
per-ex loss: 0.619752  [  177/  300]
per-ex loss: 0.502332  [  180/  300]
per-ex loss: 0.491280  [  183/  300]
per-ex loss: 0.486972  [  186/  300]
per-ex loss: 0.537635  [  189/  300]
per-ex loss: 0.538006  [  192/  300]
per-ex loss: 0.727669  [  195/  300]
per-ex loss: 0.429925  [  198/  300]
per-ex loss: 0.681192  [  201/  300]
per-ex loss: 0.648700  [  204/  300]
per-ex loss: 0.542455  [  207/  300]
per-ex loss: 0.625112  [  210/  300]
per-ex loss: 0.686118  [  213/  300]
per-ex loss: 0.503804  [  216/  300]
per-ex loss: 0.593402  [  219/  300]
per-ex loss: 0.444729  [  222/  300]
per-ex loss: 0.526907  [  225/  300]
per-ex loss: 0.508082  [  228/  300]
per-ex loss: 0.601273  [  231/  300]
per-ex loss: 0.555369  [  234/  300]
per-ex loss: 0.467350  [  237/  300]
per-ex loss: 0.532895  [  240/  300]
per-ex loss: 0.615731  [  243/  300]
per-ex loss: 0.498313  [  246/  300]
per-ex loss: 0.525574  [  249/  300]
per-ex loss: 0.508032  [  252/  300]
per-ex loss: 0.652097  [  255/  300]
per-ex loss: 0.606157  [  258/  300]
per-ex loss: 0.477474  [  261/  300]
per-ex loss: 0.680730  [  264/  300]
per-ex loss: 0.465962  [  267/  300]
per-ex loss: 0.508944  [  270/  300]
per-ex loss: 0.642558  [  273/  300]
per-ex loss: 0.565465  [  276/  300]
per-ex loss: 0.559378  [  279/  300]
per-ex loss: 0.476129  [  282/  300]
per-ex loss: 0.703143  [  285/  300]
per-ex loss: 0.709093  [  288/  300]
per-ex loss: 0.491969  [  291/  300]
per-ex loss: 0.700033  [  294/  300]
per-ex loss: 0.521792  [  297/  300]
per-ex loss: 0.516986  [  300/  300]
Train Error: Avg loss: 0.56102264
validation Error: 
 Avg loss: 0.56232784 
 F1: 0.444928 
 Precision: 0.443182 
 Recall: 0.446689
 IoU: 0.286114

test Error: 
 Avg loss: 0.49032944 
 F1: 0.510771 
 Precision: 0.545199 
 Recall: 0.480433
 IoU: 0.342977

We have finished training iteration 31
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_29_.pth
per-ex loss: 0.650676  [    3/  300]
per-ex loss: 0.517439  [    6/  300]
per-ex loss: 0.549894  [    9/  300]
per-ex loss: 0.475851  [   12/  300]
per-ex loss: 0.597239  [   15/  300]
per-ex loss: 0.586560  [   18/  300]
per-ex loss: 0.632826  [   21/  300]
per-ex loss: 0.490794  [   24/  300]
per-ex loss: 0.507196  [   27/  300]
per-ex loss: 0.518338  [   30/  300]
per-ex loss: 0.576134  [   33/  300]
per-ex loss: 0.566303  [   36/  300]
per-ex loss: 0.626294  [   39/  300]
per-ex loss: 0.595193  [   42/  300]
per-ex loss: 0.493755  [   45/  300]
per-ex loss: 0.660870  [   48/  300]
per-ex loss: 0.673455  [   51/  300]
per-ex loss: 0.554118  [   54/  300]
per-ex loss: 0.551636  [   57/  300]
per-ex loss: 0.664165  [   60/  300]
per-ex loss: 0.640786  [   63/  300]
per-ex loss: 0.456901  [   66/  300]
per-ex loss: 0.514693  [   69/  300]
per-ex loss: 0.461225  [   72/  300]
per-ex loss: 0.636333  [   75/  300]
per-ex loss: 0.511132  [   78/  300]
per-ex loss: 0.468822  [   81/  300]
per-ex loss: 0.592373  [   84/  300]
per-ex loss: 0.599202  [   87/  300]
per-ex loss: 0.590708  [   90/  300]
per-ex loss: 0.545265  [   93/  300]
per-ex loss: 0.461402  [   96/  300]
per-ex loss: 0.594361  [   99/  300]
per-ex loss: 0.582573  [  102/  300]
per-ex loss: 0.567345  [  105/  300]
per-ex loss: 0.512494  [  108/  300]
per-ex loss: 0.571457  [  111/  300]
per-ex loss: 0.540076  [  114/  300]
per-ex loss: 0.525726  [  117/  300]
per-ex loss: 0.666866  [  120/  300]
per-ex loss: 0.648359  [  123/  300]
per-ex loss: 0.494733  [  126/  300]
per-ex loss: 0.513138  [  129/  300]
per-ex loss: 0.617042  [  132/  300]
per-ex loss: 0.629582  [  135/  300]
per-ex loss: 0.465299  [  138/  300]
per-ex loss: 0.510651  [  141/  300]
per-ex loss: 0.490321  [  144/  300]
per-ex loss: 0.511821  [  147/  300]
per-ex loss: 0.487106  [  150/  300]
per-ex loss: 0.502508  [  153/  300]
per-ex loss: 0.717311  [  156/  300]
per-ex loss: 0.704647  [  159/  300]
per-ex loss: 0.462895  [  162/  300]
per-ex loss: 0.591560  [  165/  300]
per-ex loss: 0.490423  [  168/  300]
per-ex loss: 0.611506  [  171/  300]
per-ex loss: 0.479852  [  174/  300]
per-ex loss: 0.637772  [  177/  300]
per-ex loss: 0.495850  [  180/  300]
per-ex loss: 0.498648  [  183/  300]
per-ex loss: 0.435041  [  186/  300]
per-ex loss: 0.660859  [  189/  300]
per-ex loss: 0.479707  [  192/  300]
per-ex loss: 0.568432  [  195/  300]
per-ex loss: 0.528755  [  198/  300]
per-ex loss: 0.462325  [  201/  300]
per-ex loss: 0.483131  [  204/  300]
per-ex loss: 0.621100  [  207/  300]
per-ex loss: 0.694279  [  210/  300]
per-ex loss: 0.535604  [  213/  300]
per-ex loss: 0.694081  [  216/  300]
per-ex loss: 0.560683  [  219/  300]
per-ex loss: 0.595040  [  222/  300]
per-ex loss: 0.498897  [  225/  300]
per-ex loss: 0.512064  [  228/  300]
per-ex loss: 0.553168  [  231/  300]
per-ex loss: 0.522678  [  234/  300]
per-ex loss: 0.541289  [  237/  300]
per-ex loss: 0.466306  [  240/  300]
per-ex loss: 0.533375  [  243/  300]
per-ex loss: 0.548505  [  246/  300]
per-ex loss: 0.629245  [  249/  300]
per-ex loss: 0.650007  [  252/  300]
per-ex loss: 0.476454  [  255/  300]
per-ex loss: 0.495200  [  258/  300]
per-ex loss: 0.633693  [  261/  300]
per-ex loss: 0.577172  [  264/  300]
per-ex loss: 0.602942  [  267/  300]
per-ex loss: 0.600635  [  270/  300]
per-ex loss: 0.612793  [  273/  300]
per-ex loss: 0.628439  [  276/  300]
per-ex loss: 0.680556  [  279/  300]
per-ex loss: 0.538367  [  282/  300]
per-ex loss: 0.577780  [  285/  300]
per-ex loss: 0.507558  [  288/  300]
per-ex loss: 0.579360  [  291/  300]
per-ex loss: 0.577760  [  294/  300]
per-ex loss: 0.572819  [  297/  300]
per-ex loss: 0.555756  [  300/  300]
Train Error: Avg loss: 0.56081355
validation Error: 
 Avg loss: 0.53863504 
 F1: 0.449367 
 Precision: 0.379987 
 Recall: 0.549742
 IoU: 0.289796

test Error: 
 Avg loss: 0.48341691 
 F1: 0.518124 
 Precision: 0.458799 
 Recall: 0.595069
 IoU: 0.349641

We have finished training iteration 32
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_30_.pth
per-ex loss: 0.702192  [    3/  300]
per-ex loss: 0.530679  [    6/  300]
per-ex loss: 0.539992  [    9/  300]
per-ex loss: 0.472345  [   12/  300]
per-ex loss: 0.606159  [   15/  300]
per-ex loss: 0.551228  [   18/  300]
per-ex loss: 0.651098  [   21/  300]
per-ex loss: 0.565423  [   24/  300]
per-ex loss: 0.449254  [   27/  300]
per-ex loss: 0.524846  [   30/  300]
per-ex loss: 0.442221  [   33/  300]
per-ex loss: 0.444568  [   36/  300]
per-ex loss: 0.516131  [   39/  300]
per-ex loss: 0.634231  [   42/  300]
per-ex loss: 0.516240  [   45/  300]
per-ex loss: 0.487476  [   48/  300]
per-ex loss: 0.551802  [   51/  300]
per-ex loss: 0.694375  [   54/  300]
per-ex loss: 0.699799  [   57/  300]
per-ex loss: 0.494781  [   60/  300]
per-ex loss: 0.457976  [   63/  300]
per-ex loss: 0.670312  [   66/  300]
per-ex loss: 0.639761  [   69/  300]
per-ex loss: 0.593375  [   72/  300]
per-ex loss: 0.493490  [   75/  300]
per-ex loss: 0.675180  [   78/  300]
per-ex loss: 0.499252  [   81/  300]
per-ex loss: 0.543873  [   84/  300]
per-ex loss: 0.623952  [   87/  300]
per-ex loss: 0.584582  [   90/  300]
per-ex loss: 0.545868  [   93/  300]
per-ex loss: 0.621079  [   96/  300]
per-ex loss: 0.686309  [   99/  300]
per-ex loss: 0.473336  [  102/  300]
per-ex loss: 0.517114  [  105/  300]
per-ex loss: 0.425694  [  108/  300]
per-ex loss: 0.681497  [  111/  300]
per-ex loss: 0.552523  [  114/  300]
per-ex loss: 0.634552  [  117/  300]
per-ex loss: 0.574090  [  120/  300]
per-ex loss: 0.532712  [  123/  300]
per-ex loss: 0.509180  [  126/  300]
per-ex loss: 0.545052  [  129/  300]
per-ex loss: 0.488667  [  132/  300]
per-ex loss: 0.568081  [  135/  300]
per-ex loss: 0.486633  [  138/  300]
per-ex loss: 0.645476  [  141/  300]
per-ex loss: 0.527632  [  144/  300]
per-ex loss: 0.470313  [  147/  300]
per-ex loss: 0.665542  [  150/  300]
per-ex loss: 0.546001  [  153/  300]
per-ex loss: 0.645865  [  156/  300]
per-ex loss: 0.472436  [  159/  300]
per-ex loss: 0.461929  [  162/  300]
per-ex loss: 0.589338  [  165/  300]
per-ex loss: 0.687268  [  168/  300]
per-ex loss: 0.527641  [  171/  300]
per-ex loss: 0.488903  [  174/  300]
per-ex loss: 0.619860  [  177/  300]
per-ex loss: 0.603523  [  180/  300]
per-ex loss: 0.490061  [  183/  300]
per-ex loss: 0.599045  [  186/  300]
per-ex loss: 0.599861  [  189/  300]
per-ex loss: 0.492959  [  192/  300]
per-ex loss: 0.523756  [  195/  300]
per-ex loss: 0.602173  [  198/  300]
per-ex loss: 0.663554  [  201/  300]
per-ex loss: 0.472706  [  204/  300]
per-ex loss: 0.524806  [  207/  300]
per-ex loss: 0.518370  [  210/  300]
per-ex loss: 0.642882  [  213/  300]
per-ex loss: 0.515147  [  216/  300]
per-ex loss: 0.643018  [  219/  300]
per-ex loss: 0.614133  [  222/  300]
per-ex loss: 0.505847  [  225/  300]
per-ex loss: 0.455004  [  228/  300]
per-ex loss: 0.442961  [  231/  300]
per-ex loss: 0.560641  [  234/  300]
per-ex loss: 0.595228  [  237/  300]
per-ex loss: 0.457325  [  240/  300]
per-ex loss: 0.639986  [  243/  300]
per-ex loss: 0.680674  [  246/  300]
per-ex loss: 0.511820  [  249/  300]
per-ex loss: 0.483082  [  252/  300]
per-ex loss: 0.568537  [  255/  300]
per-ex loss: 0.686569  [  258/  300]
per-ex loss: 0.609834  [  261/  300]
per-ex loss: 0.420524  [  264/  300]
per-ex loss: 0.558743  [  267/  300]
per-ex loss: 0.695010  [  270/  300]
per-ex loss: 0.471413  [  273/  300]
per-ex loss: 0.554345  [  276/  300]
per-ex loss: 0.570583  [  279/  300]
per-ex loss: 0.481117  [  282/  300]
per-ex loss: 0.556240  [  285/  300]
per-ex loss: 0.445817  [  288/  300]
per-ex loss: 0.491983  [  291/  300]
per-ex loss: 0.571314  [  294/  300]
per-ex loss: 0.562795  [  297/  300]
per-ex loss: 0.502586  [  300/  300]
Train Error: Avg loss: 0.55633156
validation Error: 
 Avg loss: 0.54204867 
 F1: 0.449985 
 Precision: 0.390696 
 Recall: 0.530488
 IoU: 0.290310

test Error: 
 Avg loss: 0.47304052 
 F1: 0.528566 
 Precision: 0.491186 
 Recall: 0.572102
 IoU: 0.359218

We have finished training iteration 33
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_31_.pth
per-ex loss: 0.484897  [    3/  300]
per-ex loss: 0.607006  [    6/  300]
per-ex loss: 0.528308  [    9/  300]
per-ex loss: 0.519810  [   12/  300]
per-ex loss: 0.451028  [   15/  300]
per-ex loss: 0.452159  [   18/  300]
per-ex loss: 0.557693  [   21/  300]
per-ex loss: 0.642696  [   24/  300]
per-ex loss: 0.616527  [   27/  300]
per-ex loss: 0.472748  [   30/  300]
per-ex loss: 0.637151  [   33/  300]
per-ex loss: 0.557675  [   36/  300]
per-ex loss: 0.473235  [   39/  300]
per-ex loss: 0.463477  [   42/  300]
per-ex loss: 0.504442  [   45/  300]
per-ex loss: 0.566216  [   48/  300]
per-ex loss: 0.536916  [   51/  300]
per-ex loss: 0.563504  [   54/  300]
per-ex loss: 0.562861  [   57/  300]
per-ex loss: 0.498574  [   60/  300]
per-ex loss: 0.585024  [   63/  300]
per-ex loss: 0.534686  [   66/  300]
per-ex loss: 0.500687  [   69/  300]
per-ex loss: 0.545902  [   72/  300]
per-ex loss: 0.708270  [   75/  300]
per-ex loss: 0.660075  [   78/  300]
per-ex loss: 0.514036  [   81/  300]
per-ex loss: 0.550166  [   84/  300]
per-ex loss: 0.558220  [   87/  300]
per-ex loss: 0.601205  [   90/  300]
per-ex loss: 0.551399  [   93/  300]
per-ex loss: 0.627220  [   96/  300]
per-ex loss: 0.485938  [   99/  300]
per-ex loss: 0.518570  [  102/  300]
per-ex loss: 0.690936  [  105/  300]
per-ex loss: 0.637403  [  108/  300]
per-ex loss: 0.568737  [  111/  300]
per-ex loss: 0.523202  [  114/  300]
per-ex loss: 0.575726  [  117/  300]
per-ex loss: 0.485844  [  120/  300]
per-ex loss: 0.508625  [  123/  300]
per-ex loss: 0.602058  [  126/  300]
per-ex loss: 0.477274  [  129/  300]
per-ex loss: 0.420644  [  132/  300]
per-ex loss: 0.608769  [  135/  300]
per-ex loss: 0.510510  [  138/  300]
per-ex loss: 0.553691  [  141/  300]
per-ex loss: 0.725367  [  144/  300]
per-ex loss: 0.500207  [  147/  300]
per-ex loss: 0.664743  [  150/  300]
per-ex loss: 0.555706  [  153/  300]
per-ex loss: 0.506074  [  156/  300]
per-ex loss: 0.465902  [  159/  300]
per-ex loss: 0.640263  [  162/  300]
per-ex loss: 0.571398  [  165/  300]
per-ex loss: 0.564397  [  168/  300]
per-ex loss: 0.601320  [  171/  300]
per-ex loss: 0.478370  [  174/  300]
per-ex loss: 0.565811  [  177/  300]
per-ex loss: 0.460438  [  180/  300]
per-ex loss: 0.441560  [  183/  300]
per-ex loss: 0.696573  [  186/  300]
per-ex loss: 0.607442  [  189/  300]
per-ex loss: 0.587783  [  192/  300]
per-ex loss: 0.524124  [  195/  300]
per-ex loss: 0.469773  [  198/  300]
per-ex loss: 0.397178  [  201/  300]
per-ex loss: 0.509214  [  204/  300]
per-ex loss: 0.567663  [  207/  300]
per-ex loss: 0.491773  [  210/  300]
per-ex loss: 0.602205  [  213/  300]
per-ex loss: 0.468421  [  216/  300]
per-ex loss: 0.515029  [  219/  300]
per-ex loss: 0.514996  [  222/  300]
per-ex loss: 0.744053  [  225/  300]
per-ex loss: 0.640450  [  228/  300]
per-ex loss: 0.540634  [  231/  300]
per-ex loss: 0.609971  [  234/  300]
per-ex loss: 0.619960  [  237/  300]
per-ex loss: 0.543241  [  240/  300]
per-ex loss: 0.496091  [  243/  300]
per-ex loss: 0.575281  [  246/  300]
per-ex loss: 0.489434  [  249/  300]
per-ex loss: 0.725517  [  252/  300]
per-ex loss: 0.660202  [  255/  300]
per-ex loss: 0.496863  [  258/  300]
per-ex loss: 0.488920  [  261/  300]
per-ex loss: 0.647324  [  264/  300]
per-ex loss: 0.610159  [  267/  300]
per-ex loss: 0.600120  [  270/  300]
per-ex loss: 0.615713  [  273/  300]
per-ex loss: 0.564952  [  276/  300]
per-ex loss: 0.621203  [  279/  300]
per-ex loss: 0.441009  [  282/  300]
per-ex loss: 0.435379  [  285/  300]
per-ex loss: 0.644371  [  288/  300]
per-ex loss: 0.496947  [  291/  300]
per-ex loss: 0.533644  [  294/  300]
per-ex loss: 0.693192  [  297/  300]
per-ex loss: 0.563985  [  300/  300]
Train Error: Avg loss: 0.55594083
validation Error: 
 Avg loss: 0.52926042 
 F1: 0.452425 
 Precision: 0.368396 
 Recall: 0.586114
 IoU: 0.292344

test Error: 
 Avg loss: 0.48554623 
 F1: 0.516089 
 Precision: 0.438840 
 Recall: 0.626343
 IoU: 0.347789

We have finished training iteration 34
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_23_.pth
per-ex loss: 0.554729  [    3/  300]
per-ex loss: 0.537185  [    6/  300]
per-ex loss: 0.662400  [    9/  300]
per-ex loss: 0.450421  [   12/  300]
per-ex loss: 0.637877  [   15/  300]
per-ex loss: 0.464202  [   18/  300]
per-ex loss: 0.491283  [   21/  300]
per-ex loss: 0.602194  [   24/  300]
per-ex loss: 0.657773  [   27/  300]
per-ex loss: 0.629018  [   30/  300]
per-ex loss: 0.511292  [   33/  300]
per-ex loss: 0.645214  [   36/  300]
per-ex loss: 0.515486  [   39/  300]
per-ex loss: 0.621304  [   42/  300]
per-ex loss: 0.566660  [   45/  300]
per-ex loss: 0.557604  [   48/  300]
per-ex loss: 0.534626  [   51/  300]
per-ex loss: 0.532124  [   54/  300]
per-ex loss: 0.589154  [   57/  300]
per-ex loss: 0.538247  [   60/  300]
per-ex loss: 0.496440  [   63/  300]
per-ex loss: 0.518629  [   66/  300]
per-ex loss: 0.438808  [   69/  300]
per-ex loss: 0.580029  [   72/  300]
per-ex loss: 0.471025  [   75/  300]
per-ex loss: 0.555940  [   78/  300]
per-ex loss: 0.649572  [   81/  300]
per-ex loss: 0.616335  [   84/  300]
per-ex loss: 0.426526  [   87/  300]
per-ex loss: 0.631812  [   90/  300]
per-ex loss: 0.535761  [   93/  300]
per-ex loss: 0.539214  [   96/  300]
per-ex loss: 0.557488  [   99/  300]
per-ex loss: 0.610791  [  102/  300]
per-ex loss: 0.603252  [  105/  300]
per-ex loss: 0.470780  [  108/  300]
per-ex loss: 0.672426  [  111/  300]
per-ex loss: 0.580563  [  114/  300]
per-ex loss: 0.504093  [  117/  300]
per-ex loss: 0.500611  [  120/  300]
per-ex loss: 0.453809  [  123/  300]
per-ex loss: 0.622270  [  126/  300]
per-ex loss: 0.714303  [  129/  300]
per-ex loss: 0.575423  [  132/  300]
per-ex loss: 0.591083  [  135/  300]
per-ex loss: 0.603860  [  138/  300]
per-ex loss: 0.511668  [  141/  300]
per-ex loss: 0.489674  [  144/  300]
per-ex loss: 0.528117  [  147/  300]
per-ex loss: 0.479448  [  150/  300]
per-ex loss: 0.607076  [  153/  300]
per-ex loss: 0.537851  [  156/  300]
per-ex loss: 0.451859  [  159/  300]
per-ex loss: 0.478547  [  162/  300]
per-ex loss: 0.440796  [  165/  300]
per-ex loss: 0.506809  [  168/  300]
per-ex loss: 0.645611  [  171/  300]
per-ex loss: 0.471904  [  174/  300]
per-ex loss: 0.549934  [  177/  300]
per-ex loss: 0.540968  [  180/  300]
per-ex loss: 0.599736  [  183/  300]
per-ex loss: 0.616720  [  186/  300]
per-ex loss: 0.481765  [  189/  300]
per-ex loss: 0.555544  [  192/  300]
per-ex loss: 0.484801  [  195/  300]
per-ex loss: 0.454132  [  198/  300]
per-ex loss: 0.675362  [  201/  300]
per-ex loss: 0.689173  [  204/  300]
per-ex loss: 0.480703  [  207/  300]
per-ex loss: 0.659281  [  210/  300]
per-ex loss: 0.671993  [  213/  300]
per-ex loss: 0.445295  [  216/  300]
per-ex loss: 0.499394  [  219/  300]
per-ex loss: 0.524909  [  222/  300]
per-ex loss: 0.641380  [  225/  300]
per-ex loss: 0.477891  [  228/  300]
per-ex loss: 0.493303  [  231/  300]
per-ex loss: 0.473424  [  234/  300]
per-ex loss: 0.670246  [  237/  300]
per-ex loss: 0.509381  [  240/  300]
per-ex loss: 0.441786  [  243/  300]
per-ex loss: 0.471911  [  246/  300]
per-ex loss: 0.521481  [  249/  300]
per-ex loss: 0.703632  [  252/  300]
per-ex loss: 0.511102  [  255/  300]
per-ex loss: 0.509132  [  258/  300]
per-ex loss: 0.631101  [  261/  300]
per-ex loss: 0.544207  [  264/  300]
per-ex loss: 0.512490  [  267/  300]
per-ex loss: 0.506613  [  270/  300]
per-ex loss: 0.689247  [  273/  300]
per-ex loss: 0.563164  [  276/  300]
per-ex loss: 0.527966  [  279/  300]
per-ex loss: 0.497425  [  282/  300]
per-ex loss: 0.539054  [  285/  300]
per-ex loss: 0.591945  [  288/  300]
per-ex loss: 0.585994  [  291/  300]
per-ex loss: 0.526479  [  294/  300]
per-ex loss: 0.532883  [  297/  300]
per-ex loss: 0.614919  [  300/  300]
Train Error: Avg loss: 0.55186859
validation Error: 
 Avg loss: 0.54044239 
 F1: 0.450577 
 Precision: 0.377902 
 Recall: 0.557860
 IoU: 0.290803

test Error: 
 Avg loss: 0.48298633 
 F1: 0.518276 
 Precision: 0.452280 
 Recall: 0.606821
 IoU: 0.349779

We have finished training iteration 35
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_33_.pth
per-ex loss: 0.507661  [    3/  300]
per-ex loss: 0.562915  [    6/  300]
per-ex loss: 0.503244  [    9/  300]
per-ex loss: 0.523361  [   12/  300]
per-ex loss: 0.519024  [   15/  300]
per-ex loss: 0.667032  [   18/  300]
per-ex loss: 0.581425  [   21/  300]
per-ex loss: 0.581833  [   24/  300]
per-ex loss: 0.509874  [   27/  300]
per-ex loss: 0.495999  [   30/  300]
per-ex loss: 0.528588  [   33/  300]
per-ex loss: 0.589389  [   36/  300]
per-ex loss: 0.539126  [   39/  300]
per-ex loss: 0.452306  [   42/  300]
per-ex loss: 0.711885  [   45/  300]
per-ex loss: 0.559534  [   48/  300]
per-ex loss: 0.565361  [   51/  300]
per-ex loss: 0.567186  [   54/  300]
per-ex loss: 0.536223  [   57/  300]
per-ex loss: 0.525073  [   60/  300]
per-ex loss: 0.480377  [   63/  300]
per-ex loss: 0.551781  [   66/  300]
per-ex loss: 0.732997  [   69/  300]
per-ex loss: 0.568386  [   72/  300]
per-ex loss: 0.592920  [   75/  300]
per-ex loss: 0.721217  [   78/  300]
per-ex loss: 0.469147  [   81/  300]
per-ex loss: 0.557803  [   84/  300]
per-ex loss: 0.601899  [   87/  300]
per-ex loss: 0.624438  [   90/  300]
per-ex loss: 0.626016  [   93/  300]
per-ex loss: 0.743304  [   96/  300]
per-ex loss: 0.532450  [   99/  300]
per-ex loss: 0.542222  [  102/  300]
per-ex loss: 0.511631  [  105/  300]
per-ex loss: 0.486782  [  108/  300]
per-ex loss: 0.456474  [  111/  300]
per-ex loss: 0.542103  [  114/  300]
per-ex loss: 0.638359  [  117/  300]
per-ex loss: 0.461906  [  120/  300]
per-ex loss: 0.606926  [  123/  300]
per-ex loss: 0.692471  [  126/  300]
per-ex loss: 0.550885  [  129/  300]
per-ex loss: 0.605498  [  132/  300]
per-ex loss: 0.553367  [  135/  300]
per-ex loss: 0.562819  [  138/  300]
per-ex loss: 0.653519  [  141/  300]
per-ex loss: 0.446611  [  144/  300]
per-ex loss: 0.537042  [  147/  300]
per-ex loss: 0.580781  [  150/  300]
per-ex loss: 0.592972  [  153/  300]
per-ex loss: 0.551463  [  156/  300]
per-ex loss: 0.565964  [  159/  300]
per-ex loss: 0.469203  [  162/  300]
per-ex loss: 0.628233  [  165/  300]
per-ex loss: 0.430178  [  168/  300]
per-ex loss: 0.491020  [  171/  300]
per-ex loss: 0.634020  [  174/  300]
per-ex loss: 0.502044  [  177/  300]
per-ex loss: 0.613036  [  180/  300]
per-ex loss: 0.476517  [  183/  300]
per-ex loss: 0.524280  [  186/  300]
per-ex loss: 0.553054  [  189/  300]
per-ex loss: 0.587512  [  192/  300]
per-ex loss: 0.504823  [  195/  300]
per-ex loss: 0.516235  [  198/  300]
per-ex loss: 0.585147  [  201/  300]
per-ex loss: 0.599642  [  204/  300]
per-ex loss: 0.500103  [  207/  300]
per-ex loss: 0.515883  [  210/  300]
per-ex loss: 0.422430  [  213/  300]
per-ex loss: 0.550114  [  216/  300]
per-ex loss: 0.593642  [  219/  300]
per-ex loss: 0.685530  [  222/  300]
per-ex loss: 0.564714  [  225/  300]
per-ex loss: 0.473104  [  228/  300]
per-ex loss: 0.518391  [  231/  300]
per-ex loss: 0.621711  [  234/  300]
per-ex loss: 0.573692  [  237/  300]
per-ex loss: 0.501568  [  240/  300]
per-ex loss: 0.514026  [  243/  300]
per-ex loss: 0.580116  [  246/  300]
per-ex loss: 0.605634  [  249/  300]
per-ex loss: 0.612568  [  252/  300]
per-ex loss: 0.594059  [  255/  300]
per-ex loss: 0.612693  [  258/  300]
per-ex loss: 0.468252  [  261/  300]
per-ex loss: 0.449717  [  264/  300]
per-ex loss: 0.485928  [  267/  300]
per-ex loss: 0.592924  [  270/  300]
per-ex loss: 0.421619  [  273/  300]
per-ex loss: 0.663231  [  276/  300]
per-ex loss: 0.426032  [  279/  300]
per-ex loss: 0.649490  [  282/  300]
per-ex loss: 0.514399  [  285/  300]
per-ex loss: 0.481734  [  288/  300]
per-ex loss: 0.601077  [  291/  300]
per-ex loss: 0.568315  [  294/  300]
per-ex loss: 0.554466  [  297/  300]
per-ex loss: 0.693507  [  300/  300]
Train Error: Avg loss: 0.55695181
validation Error: 
 Avg loss: 0.54289514 
 F1: 0.446199 
 Precision: 0.380919 
 Recall: 0.538482
 IoU: 0.287166

test Error: 
 Avg loss: 0.50075829 
 F1: 0.500522 
 Precision: 0.454347 
 Recall: 0.557146
 IoU: 0.333798

We have finished training iteration 36
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_27_.pth
per-ex loss: 0.660932  [    3/  300]
per-ex loss: 0.501534  [    6/  300]
per-ex loss: 0.453574  [    9/  300]
per-ex loss: 0.520913  [   12/  300]
per-ex loss: 0.587222  [   15/  300]
per-ex loss: 0.543851  [   18/  300]
per-ex loss: 0.561268  [   21/  300]
per-ex loss: 0.513663  [   24/  300]
per-ex loss: 0.705140  [   27/  300]
per-ex loss: 0.650405  [   30/  300]
per-ex loss: 0.507879  [   33/  300]
per-ex loss: 0.440765  [   36/  300]
per-ex loss: 0.562805  [   39/  300]
per-ex loss: 0.530589  [   42/  300]
per-ex loss: 0.608144  [   45/  300]
per-ex loss: 0.565526  [   48/  300]
per-ex loss: 0.531506  [   51/  300]
per-ex loss: 0.479597  [   54/  300]
per-ex loss: 0.565766  [   57/  300]
per-ex loss: 0.424808  [   60/  300]
per-ex loss: 0.533882  [   63/  300]
per-ex loss: 0.482031  [   66/  300]
per-ex loss: 0.550954  [   69/  300]
per-ex loss: 0.501564  [   72/  300]
per-ex loss: 0.485682  [   75/  300]
per-ex loss: 0.650912  [   78/  300]
per-ex loss: 0.539447  [   81/  300]
per-ex loss: 0.651896  [   84/  300]
per-ex loss: 0.643596  [   87/  300]
per-ex loss: 0.615965  [   90/  300]
per-ex loss: 0.596280  [   93/  300]
per-ex loss: 0.634668  [   96/  300]
per-ex loss: 0.429889  [   99/  300]
per-ex loss: 0.488319  [  102/  300]
per-ex loss: 0.569523  [  105/  300]
per-ex loss: 0.567649  [  108/  300]
per-ex loss: 0.498495  [  111/  300]
per-ex loss: 0.569404  [  114/  300]
per-ex loss: 0.573236  [  117/  300]
per-ex loss: 0.467687  [  120/  300]
per-ex loss: 0.531937  [  123/  300]
per-ex loss: 0.523547  [  126/  300]
per-ex loss: 0.451096  [  129/  300]
per-ex loss: 0.502875  [  132/  300]
per-ex loss: 0.522030  [  135/  300]
per-ex loss: 0.566200  [  138/  300]
per-ex loss: 0.659291  [  141/  300]
per-ex loss: 0.641613  [  144/  300]
per-ex loss: 0.539736  [  147/  300]
per-ex loss: 0.664522  [  150/  300]
per-ex loss: 0.572866  [  153/  300]
per-ex loss: 0.523335  [  156/  300]
per-ex loss: 0.580966  [  159/  300]
per-ex loss: 0.577212  [  162/  300]
per-ex loss: 0.554668  [  165/  300]
per-ex loss: 0.616050  [  168/  300]
per-ex loss: 0.601289  [  171/  300]
per-ex loss: 0.597906  [  174/  300]
per-ex loss: 0.575406  [  177/  300]
per-ex loss: 0.519184  [  180/  300]
per-ex loss: 0.499319  [  183/  300]
per-ex loss: 0.606128  [  186/  300]
per-ex loss: 0.597157  [  189/  300]
per-ex loss: 0.443588  [  192/  300]
per-ex loss: 0.678123  [  195/  300]
per-ex loss: 0.548297  [  198/  300]
per-ex loss: 0.546606  [  201/  300]
per-ex loss: 0.519813  [  204/  300]
per-ex loss: 0.635436  [  207/  300]
per-ex loss: 0.493933  [  210/  300]
per-ex loss: 0.522969  [  213/  300]
per-ex loss: 0.696042  [  216/  300]
per-ex loss: 0.473874  [  219/  300]
per-ex loss: 0.673276  [  222/  300]
per-ex loss: 0.564127  [  225/  300]
per-ex loss: 0.480533  [  228/  300]
per-ex loss: 0.618643  [  231/  300]
per-ex loss: 0.592398  [  234/  300]
per-ex loss: 0.603189  [  237/  300]
per-ex loss: 0.657183  [  240/  300]
per-ex loss: 0.490283  [  243/  300]
per-ex loss: 0.468127  [  246/  300]
per-ex loss: 0.534938  [  249/  300]
per-ex loss: 0.567069  [  252/  300]
per-ex loss: 0.521080  [  255/  300]
per-ex loss: 0.625498  [  258/  300]
per-ex loss: 0.604548  [  261/  300]
per-ex loss: 0.522560  [  264/  300]
per-ex loss: 0.574390  [  267/  300]
per-ex loss: 0.691277  [  270/  300]
per-ex loss: 0.534899  [  273/  300]
per-ex loss: 0.586394  [  276/  300]
per-ex loss: 0.490438  [  279/  300]
per-ex loss: 0.614601  [  282/  300]
per-ex loss: 0.649989  [  285/  300]
per-ex loss: 0.641869  [  288/  300]
per-ex loss: 0.546692  [  291/  300]
per-ex loss: 0.446154  [  294/  300]
per-ex loss: 0.502061  [  297/  300]
per-ex loss: 0.560560  [  300/  300]
Train Error: Avg loss: 0.55912753
validation Error: 
 Avg loss: 0.57160227 
 F1: 0.435945 
 Precision: 0.414477 
 Recall: 0.459759
 IoU: 0.278727

test Error: 
 Avg loss: 0.49695557 
 F1: 0.504040 
 Precision: 0.528763 
 Recall: 0.481526
 IoU: 0.336934

We have finished training iteration 37
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_35_.pth
per-ex loss: 0.601464  [    3/  300]
per-ex loss: 0.505345  [    6/  300]
per-ex loss: 0.507437  [    9/  300]
per-ex loss: 0.545058  [   12/  300]
per-ex loss: 0.640366  [   15/  300]
per-ex loss: 0.616267  [   18/  300]
per-ex loss: 0.610764  [   21/  300]
per-ex loss: 0.675005  [   24/  300]
per-ex loss: 0.654980  [   27/  300]
per-ex loss: 0.496340  [   30/  300]
per-ex loss: 0.645057  [   33/  300]
per-ex loss: 0.526739  [   36/  300]
per-ex loss: 0.614703  [   39/  300]
per-ex loss: 0.491364  [   42/  300]
per-ex loss: 0.603926  [   45/  300]
per-ex loss: 0.467217  [   48/  300]
per-ex loss: 0.705875  [   51/  300]
per-ex loss: 0.547779  [   54/  300]
per-ex loss: 0.633958  [   57/  300]
per-ex loss: 0.721583  [   60/  300]
per-ex loss: 0.477338  [   63/  300]
per-ex loss: 0.405496  [   66/  300]
per-ex loss: 0.508743  [   69/  300]
per-ex loss: 0.440263  [   72/  300]
per-ex loss: 0.485687  [   75/  300]
per-ex loss: 0.474482  [   78/  300]
per-ex loss: 0.550763  [   81/  300]
per-ex loss: 0.517502  [   84/  300]
per-ex loss: 0.588298  [   87/  300]
per-ex loss: 0.525286  [   90/  300]
per-ex loss: 0.488980  [   93/  300]
per-ex loss: 0.601556  [   96/  300]
per-ex loss: 0.526452  [   99/  300]
per-ex loss: 0.637792  [  102/  300]
per-ex loss: 0.663709  [  105/  300]
per-ex loss: 0.690499  [  108/  300]
per-ex loss: 0.601138  [  111/  300]
per-ex loss: 0.541026  [  114/  300]
per-ex loss: 0.479523  [  117/  300]
per-ex loss: 0.513024  [  120/  300]
per-ex loss: 0.571515  [  123/  300]
per-ex loss: 0.443113  [  126/  300]
per-ex loss: 0.704700  [  129/  300]
per-ex loss: 0.514253  [  132/  300]
per-ex loss: 0.448980  [  135/  300]
per-ex loss: 0.622564  [  138/  300]
per-ex loss: 0.707795  [  141/  300]
per-ex loss: 0.568700  [  144/  300]
per-ex loss: 0.652373  [  147/  300]
per-ex loss: 0.496209  [  150/  300]
per-ex loss: 0.666194  [  153/  300]
per-ex loss: 0.611074  [  156/  300]
per-ex loss: 0.562960  [  159/  300]
per-ex loss: 0.454297  [  162/  300]
per-ex loss: 0.487360  [  165/  300]
per-ex loss: 0.571691  [  168/  300]
per-ex loss: 0.565568  [  171/  300]
per-ex loss: 0.548743  [  174/  300]
per-ex loss: 0.476254  [  177/  300]
per-ex loss: 0.661975  [  180/  300]
per-ex loss: 0.544791  [  183/  300]
per-ex loss: 0.582281  [  186/  300]
per-ex loss: 0.591395  [  189/  300]
per-ex loss: 0.540546  [  192/  300]
per-ex loss: 0.548975  [  195/  300]
per-ex loss: 0.467266  [  198/  300]
per-ex loss: 0.538700  [  201/  300]
per-ex loss: 0.574839  [  204/  300]
per-ex loss: 0.405505  [  207/  300]
per-ex loss: 0.538347  [  210/  300]
per-ex loss: 0.595562  [  213/  300]
per-ex loss: 0.529948  [  216/  300]
per-ex loss: 0.499486  [  219/  300]
per-ex loss: 0.467819  [  222/  300]
per-ex loss: 0.667178  [  225/  300]
per-ex loss: 0.592406  [  228/  300]
per-ex loss: 0.539629  [  231/  300]
per-ex loss: 0.642120  [  234/  300]
per-ex loss: 0.538155  [  237/  300]
per-ex loss: 0.628785  [  240/  300]
per-ex loss: 0.470972  [  243/  300]
per-ex loss: 0.622052  [  246/  300]
per-ex loss: 0.484336  [  249/  300]
per-ex loss: 0.509848  [  252/  300]
per-ex loss: 0.590452  [  255/  300]
per-ex loss: 0.481466  [  258/  300]
per-ex loss: 0.528680  [  261/  300]
per-ex loss: 0.569957  [  264/  300]
per-ex loss: 0.481060  [  267/  300]
per-ex loss: 0.558613  [  270/  300]
per-ex loss: 0.715973  [  273/  300]
per-ex loss: 0.542690  [  276/  300]
per-ex loss: 0.624318  [  279/  300]
per-ex loss: 0.499051  [  282/  300]
per-ex loss: 0.693809  [  285/  300]
per-ex loss: 0.537451  [  288/  300]
per-ex loss: 0.572903  [  291/  300]
per-ex loss: 0.445160  [  294/  300]
per-ex loss: 0.616330  [  297/  300]
per-ex loss: 0.523743  [  300/  300]
Train Error: Avg loss: 0.55967703
validation Error: 
 Avg loss: 0.56239297 
 F1: 0.444808 
 Precision: 0.483044 
 Recall: 0.412182
 IoU: 0.286015

test Error: 
 Avg loss: 0.49436873 
 F1: 0.506620 
 Precision: 0.587140 
 Recall: 0.445521
 IoU: 0.339244

We have finished training iteration 38
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_36_.pth
per-ex loss: 0.616767  [    3/  300]
per-ex loss: 0.581027  [    6/  300]
per-ex loss: 0.667022  [    9/  300]
per-ex loss: 0.508245  [   12/  300]
per-ex loss: 0.592410  [   15/  300]
per-ex loss: 0.463458  [   18/  300]
per-ex loss: 0.518802  [   21/  300]
per-ex loss: 0.531888  [   24/  300]
per-ex loss: 0.491572  [   27/  300]
per-ex loss: 0.499336  [   30/  300]
per-ex loss: 0.642183  [   33/  300]
per-ex loss: 0.502668  [   36/  300]
per-ex loss: 0.533436  [   39/  300]
per-ex loss: 0.465986  [   42/  300]
per-ex loss: 0.552225  [   45/  300]
per-ex loss: 0.603353  [   48/  300]
per-ex loss: 0.463461  [   51/  300]
per-ex loss: 0.662167  [   54/  300]
per-ex loss: 0.553713  [   57/  300]
per-ex loss: 0.567119  [   60/  300]
per-ex loss: 0.507616  [   63/  300]
per-ex loss: 0.596571  [   66/  300]
per-ex loss: 0.668755  [   69/  300]
per-ex loss: 0.498711  [   72/  300]
per-ex loss: 0.619990  [   75/  300]
per-ex loss: 0.606607  [   78/  300]
per-ex loss: 0.509154  [   81/  300]
per-ex loss: 0.443096  [   84/  300]
per-ex loss: 0.516681  [   87/  300]
per-ex loss: 0.459273  [   90/  300]
per-ex loss: 0.575959  [   93/  300]
per-ex loss: 0.564574  [   96/  300]
per-ex loss: 0.541956  [   99/  300]
per-ex loss: 0.535262  [  102/  300]
per-ex loss: 0.546272  [  105/  300]
per-ex loss: 0.567636  [  108/  300]
per-ex loss: 0.537525  [  111/  300]
per-ex loss: 0.540258  [  114/  300]
per-ex loss: 0.734978  [  117/  300]
per-ex loss: 0.541141  [  120/  300]
per-ex loss: 0.561783  [  123/  300]
per-ex loss: 0.521450  [  126/  300]
per-ex loss: 0.623195  [  129/  300]
per-ex loss: 0.633088  [  132/  300]
per-ex loss: 0.529498  [  135/  300]
per-ex loss: 0.512121  [  138/  300]
per-ex loss: 0.482478  [  141/  300]
per-ex loss: 0.575193  [  144/  300]
per-ex loss: 0.587459  [  147/  300]
per-ex loss: 0.448965  [  150/  300]
per-ex loss: 0.416272  [  153/  300]
per-ex loss: 0.449698  [  156/  300]
per-ex loss: 0.593089  [  159/  300]
per-ex loss: 0.549432  [  162/  300]
per-ex loss: 0.693903  [  165/  300]
per-ex loss: 0.581383  [  168/  300]
per-ex loss: 0.599797  [  171/  300]
per-ex loss: 0.514701  [  174/  300]
per-ex loss: 0.475657  [  177/  300]
per-ex loss: 0.564159  [  180/  300]
per-ex loss: 0.597439  [  183/  300]
per-ex loss: 0.598873  [  186/  300]
per-ex loss: 0.515576  [  189/  300]
per-ex loss: 0.513076  [  192/  300]
per-ex loss: 0.600125  [  195/  300]
per-ex loss: 0.521434  [  198/  300]
per-ex loss: 0.583503  [  201/  300]
per-ex loss: 0.601711  [  204/  300]
per-ex loss: 0.532922  [  207/  300]
per-ex loss: 0.448359  [  210/  300]
per-ex loss: 0.583992  [  213/  300]
per-ex loss: 0.431350  [  216/  300]
per-ex loss: 0.570119  [  219/  300]
per-ex loss: 0.672379  [  222/  300]
per-ex loss: 0.559327  [  225/  300]
per-ex loss: 0.585207  [  228/  300]
per-ex loss: 0.534473  [  231/  300]
per-ex loss: 0.615803  [  234/  300]
per-ex loss: 0.529114  [  237/  300]
per-ex loss: 0.548071  [  240/  300]
per-ex loss: 0.512743  [  243/  300]
per-ex loss: 0.459590  [  246/  300]
per-ex loss: 0.472586  [  249/  300]
per-ex loss: 0.499627  [  252/  300]
per-ex loss: 0.714764  [  255/  300]
per-ex loss: 0.544281  [  258/  300]
per-ex loss: 0.555308  [  261/  300]
per-ex loss: 0.500640  [  264/  300]
per-ex loss: 0.547575  [  267/  300]
per-ex loss: 0.641514  [  270/  300]
per-ex loss: 0.657023  [  273/  300]
per-ex loss: 0.590164  [  276/  300]
per-ex loss: 0.498823  [  279/  300]
per-ex loss: 0.474234  [  282/  300]
per-ex loss: 0.475747  [  285/  300]
per-ex loss: 0.705777  [  288/  300]
per-ex loss: 0.515150  [  291/  300]
per-ex loss: 0.660522  [  294/  300]
per-ex loss: 0.533894  [  297/  300]
per-ex loss: 0.490191  [  300/  300]
Train Error: Avg loss: 0.55235184
validation Error: 
 Avg loss: 0.54003388 
 F1: 0.449368 
 Precision: 0.378940 
 Recall: 0.551950
 IoU: 0.289797

test Error: 
 Avg loss: 0.48283631 
 F1: 0.518204 
 Precision: 0.475132 
 Recall: 0.569864
 IoU: 0.349713

We have finished training iteration 39
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_37_.pth
per-ex loss: 0.569811  [    3/  300]
per-ex loss: 0.487331  [    6/  300]
per-ex loss: 0.458879  [    9/  300]
per-ex loss: 0.439354  [   12/  300]
per-ex loss: 0.700541  [   15/  300]
per-ex loss: 0.523564  [   18/  300]
per-ex loss: 0.503527  [   21/  300]
per-ex loss: 0.677992  [   24/  300]
per-ex loss: 0.578748  [   27/  300]
per-ex loss: 0.498844  [   30/  300]
per-ex loss: 0.593309  [   33/  300]
per-ex loss: 0.538310  [   36/  300]
per-ex loss: 0.502646  [   39/  300]
per-ex loss: 0.521543  [   42/  300]
per-ex loss: 0.522414  [   45/  300]
per-ex loss: 0.645436  [   48/  300]
per-ex loss: 0.535322  [   51/  300]
per-ex loss: 0.512194  [   54/  300]
per-ex loss: 0.508256  [   57/  300]
per-ex loss: 0.622521  [   60/  300]
per-ex loss: 0.627134  [   63/  300]
per-ex loss: 0.489271  [   66/  300]
per-ex loss: 0.566093  [   69/  300]
per-ex loss: 0.525649  [   72/  300]
per-ex loss: 0.471150  [   75/  300]
per-ex loss: 0.512786  [   78/  300]
per-ex loss: 0.616443  [   81/  300]
per-ex loss: 0.641122  [   84/  300]
per-ex loss: 0.659977  [   87/  300]
per-ex loss: 0.477976  [   90/  300]
per-ex loss: 0.488625  [   93/  300]
per-ex loss: 0.528362  [   96/  300]
per-ex loss: 0.614727  [   99/  300]
per-ex loss: 0.523706  [  102/  300]
per-ex loss: 0.502542  [  105/  300]
per-ex loss: 0.438119  [  108/  300]
per-ex loss: 0.486776  [  111/  300]
per-ex loss: 0.553855  [  114/  300]
per-ex loss: 0.473950  [  117/  300]
per-ex loss: 0.466624  [  120/  300]
per-ex loss: 0.609621  [  123/  300]
per-ex loss: 0.627240  [  126/  300]
per-ex loss: 0.637947  [  129/  300]
per-ex loss: 0.624977  [  132/  300]
per-ex loss: 0.626779  [  135/  300]
per-ex loss: 0.473030  [  138/  300]
per-ex loss: 0.539067  [  141/  300]
per-ex loss: 0.666288  [  144/  300]
per-ex loss: 0.608167  [  147/  300]
per-ex loss: 0.549792  [  150/  300]
per-ex loss: 0.550853  [  153/  300]
per-ex loss: 0.542804  [  156/  300]
per-ex loss: 0.531777  [  159/  300]
per-ex loss: 0.509278  [  162/  300]
per-ex loss: 0.566179  [  165/  300]
per-ex loss: 0.634277  [  168/  300]
per-ex loss: 0.637602  [  171/  300]
per-ex loss: 0.437119  [  174/  300]
per-ex loss: 0.681673  [  177/  300]
per-ex loss: 0.505808  [  180/  300]
per-ex loss: 0.686309  [  183/  300]
per-ex loss: 0.584166  [  186/  300]
per-ex loss: 0.473638  [  189/  300]
per-ex loss: 0.515619  [  192/  300]
per-ex loss: 0.573754  [  195/  300]
per-ex loss: 0.580814  [  198/  300]
per-ex loss: 0.518305  [  201/  300]
per-ex loss: 0.469675  [  204/  300]
per-ex loss: 0.528748  [  207/  300]
per-ex loss: 0.533795  [  210/  300]
per-ex loss: 0.590586  [  213/  300]
per-ex loss: 0.555627  [  216/  300]
per-ex loss: 0.447709  [  219/  300]
per-ex loss: 0.558897  [  222/  300]
per-ex loss: 0.529283  [  225/  300]
per-ex loss: 0.454581  [  228/  300]
per-ex loss: 0.625541  [  231/  300]
per-ex loss: 0.630634  [  234/  300]
per-ex loss: 0.487352  [  237/  300]
per-ex loss: 0.640549  [  240/  300]
per-ex loss: 0.544370  [  243/  300]
per-ex loss: 0.556266  [  246/  300]
per-ex loss: 0.471125  [  249/  300]
per-ex loss: 0.577429  [  252/  300]
per-ex loss: 0.527351  [  255/  300]
per-ex loss: 0.525611  [  258/  300]
per-ex loss: 0.530821  [  261/  300]
per-ex loss: 0.584407  [  264/  300]
per-ex loss: 0.504039  [  267/  300]
per-ex loss: 0.677279  [  270/  300]
per-ex loss: 0.544593  [  273/  300]
per-ex loss: 0.472106  [  276/  300]
per-ex loss: 0.607052  [  279/  300]
per-ex loss: 0.500303  [  282/  300]
per-ex loss: 0.563765  [  285/  300]
per-ex loss: 0.593236  [  288/  300]
per-ex loss: 0.578404  [  291/  300]
per-ex loss: 0.618070  [  294/  300]
per-ex loss: 0.559753  [  297/  300]
per-ex loss: 0.538865  [  300/  300]
Train Error: Avg loss: 0.55226135
validation Error: 
 Avg loss: 0.53762805 
 F1: 0.456178 
 Precision: 0.416346 
 Recall: 0.504437
 IoU: 0.295486

test Error: 
 Avg loss: 0.48103005 
 F1: 0.520488 
 Precision: 0.516403 
 Recall: 0.524639
 IoU: 0.351797

We have finished training iteration 40
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_38_.pth
per-ex loss: 0.520216  [    3/  300]
per-ex loss: 0.561867  [    6/  300]
per-ex loss: 0.695853  [    9/  300]
per-ex loss: 0.499332  [   12/  300]
per-ex loss: 0.590309  [   15/  300]
per-ex loss: 0.656402  [   18/  300]
per-ex loss: 0.528464  [   21/  300]
per-ex loss: 0.529872  [   24/  300]
per-ex loss: 0.549563  [   27/  300]
per-ex loss: 0.585411  [   30/  300]
per-ex loss: 0.536092  [   33/  300]
per-ex loss: 0.541685  [   36/  300]
per-ex loss: 0.483937  [   39/  300]
per-ex loss: 0.536384  [   42/  300]
per-ex loss: 0.669574  [   45/  300]
per-ex loss: 0.597292  [   48/  300]
per-ex loss: 0.580243  [   51/  300]
per-ex loss: 0.455252  [   54/  300]
per-ex loss: 0.538906  [   57/  300]
per-ex loss: 0.471033  [   60/  300]
per-ex loss: 0.472692  [   63/  300]
per-ex loss: 0.538440  [   66/  300]
per-ex loss: 0.518736  [   69/  300]
per-ex loss: 0.668678  [   72/  300]
per-ex loss: 0.617839  [   75/  300]
per-ex loss: 0.614386  [   78/  300]
per-ex loss: 0.550738  [   81/  300]
per-ex loss: 0.580076  [   84/  300]
per-ex loss: 0.453754  [   87/  300]
per-ex loss: 0.590804  [   90/  300]
per-ex loss: 0.580237  [   93/  300]
per-ex loss: 0.676570  [   96/  300]
per-ex loss: 0.535549  [   99/  300]
per-ex loss: 0.530035  [  102/  300]
per-ex loss: 0.537606  [  105/  300]
per-ex loss: 0.588217  [  108/  300]
per-ex loss: 0.607357  [  111/  300]
per-ex loss: 0.560762  [  114/  300]
per-ex loss: 0.554765  [  117/  300]
per-ex loss: 0.536427  [  120/  300]
per-ex loss: 0.679503  [  123/  300]
per-ex loss: 0.536417  [  126/  300]
per-ex loss: 0.605081  [  129/  300]
per-ex loss: 0.519994  [  132/  300]
per-ex loss: 0.643326  [  135/  300]
per-ex loss: 0.605038  [  138/  300]
per-ex loss: 0.464961  [  141/  300]
per-ex loss: 0.481755  [  144/  300]
per-ex loss: 0.455062  [  147/  300]
per-ex loss: 0.538278  [  150/  300]
per-ex loss: 0.488291  [  153/  300]
per-ex loss: 0.458298  [  156/  300]
per-ex loss: 0.457693  [  159/  300]
per-ex loss: 0.516001  [  162/  300]
per-ex loss: 0.680280  [  165/  300]
per-ex loss: 0.529229  [  168/  300]
per-ex loss: 0.697068  [  171/  300]
per-ex loss: 0.709548  [  174/  300]
per-ex loss: 0.443682  [  177/  300]
per-ex loss: 0.701323  [  180/  300]
per-ex loss: 0.488688  [  183/  300]
per-ex loss: 0.473139  [  186/  300]
per-ex loss: 0.572393  [  189/  300]
per-ex loss: 0.436494  [  192/  300]
per-ex loss: 0.596041  [  195/  300]
per-ex loss: 0.564638  [  198/  300]
per-ex loss: 0.659931  [  201/  300]
per-ex loss: 0.533827  [  204/  300]
per-ex loss: 0.571445  [  207/  300]
per-ex loss: 0.493590  [  210/  300]
per-ex loss: 0.468643  [  213/  300]
per-ex loss: 0.542348  [  216/  300]
per-ex loss: 0.563612  [  219/  300]
per-ex loss: 0.625833  [  222/  300]
per-ex loss: 0.595179  [  225/  300]
per-ex loss: 0.556394  [  228/  300]
per-ex loss: 0.455288  [  231/  300]
per-ex loss: 0.483836  [  234/  300]
per-ex loss: 0.628638  [  237/  300]
per-ex loss: 0.574688  [  240/  300]
per-ex loss: 0.597144  [  243/  300]
per-ex loss: 0.485089  [  246/  300]
per-ex loss: 0.653912  [  249/  300]
per-ex loss: 0.483246  [  252/  300]
per-ex loss: 0.426780  [  255/  300]
per-ex loss: 0.582490  [  258/  300]
per-ex loss: 0.650950  [  261/  300]
per-ex loss: 0.474848  [  264/  300]
per-ex loss: 0.447091  [  267/  300]
per-ex loss: 0.619389  [  270/  300]
per-ex loss: 0.600584  [  273/  300]
per-ex loss: 0.525903  [  276/  300]
per-ex loss: 0.401203  [  279/  300]
per-ex loss: 0.601849  [  282/  300]
per-ex loss: 0.675381  [  285/  300]
per-ex loss: 0.567506  [  288/  300]
per-ex loss: 0.544373  [  291/  300]
per-ex loss: 0.539007  [  294/  300]
per-ex loss: 0.515454  [  297/  300]
per-ex loss: 0.470059  [  300/  300]
Train Error: Avg loss: 0.55399086
validation Error: 
 Avg loss: 0.53008934 
 F1: 0.457095 
 Precision: 0.389657 
 Recall: 0.552763
 IoU: 0.296256

test Error: 
 Avg loss: 0.46964788 
 F1: 0.531837 
 Precision: 0.500511 
 Recall: 0.567346
 IoU: 0.362246

We have finished training iteration 41
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_39_.pth
per-ex loss: 0.476723  [    3/  300]
per-ex loss: 0.555646  [    6/  300]
per-ex loss: 0.556997  [    9/  300]
per-ex loss: 0.571521  [   12/  300]
per-ex loss: 0.519401  [   15/  300]
per-ex loss: 0.522448  [   18/  300]
per-ex loss: 0.679331  [   21/  300]
per-ex loss: 0.631408  [   24/  300]
per-ex loss: 0.470004  [   27/  300]
per-ex loss: 0.561048  [   30/  300]
per-ex loss: 0.617771  [   33/  300]
per-ex loss: 0.469835  [   36/  300]
per-ex loss: 0.455443  [   39/  300]
per-ex loss: 0.599306  [   42/  300]
per-ex loss: 0.651428  [   45/  300]
per-ex loss: 0.667479  [   48/  300]
per-ex loss: 0.435503  [   51/  300]
per-ex loss: 0.590316  [   54/  300]
per-ex loss: 0.473479  [   57/  300]
per-ex loss: 0.676175  [   60/  300]
per-ex loss: 0.656961  [   63/  300]
per-ex loss: 0.610331  [   66/  300]
per-ex loss: 0.469724  [   69/  300]
per-ex loss: 0.489560  [   72/  300]
per-ex loss: 0.514397  [   75/  300]
per-ex loss: 0.483504  [   78/  300]
per-ex loss: 0.642590  [   81/  300]
per-ex loss: 0.420419  [   84/  300]
per-ex loss: 0.459891  [   87/  300]
per-ex loss: 0.517012  [   90/  300]
per-ex loss: 0.579291  [   93/  300]
per-ex loss: 0.493387  [   96/  300]
per-ex loss: 0.585298  [   99/  300]
per-ex loss: 0.559562  [  102/  300]
per-ex loss: 0.536470  [  105/  300]
per-ex loss: 0.555378  [  108/  300]
per-ex loss: 0.543763  [  111/  300]
per-ex loss: 0.490814  [  114/  300]
per-ex loss: 0.517558  [  117/  300]
per-ex loss: 0.486514  [  120/  300]
per-ex loss: 0.543356  [  123/  300]
per-ex loss: 0.535544  [  126/  300]
per-ex loss: 0.608526  [  129/  300]
per-ex loss: 0.491422  [  132/  300]
per-ex loss: 0.464220  [  135/  300]
per-ex loss: 0.510221  [  138/  300]
per-ex loss: 0.532429  [  141/  300]
per-ex loss: 0.556287  [  144/  300]
per-ex loss: 0.536792  [  147/  300]
per-ex loss: 0.606440  [  150/  300]
per-ex loss: 0.518031  [  153/  300]
per-ex loss: 0.494667  [  156/  300]
per-ex loss: 0.427442  [  159/  300]
per-ex loss: 0.592640  [  162/  300]
per-ex loss: 0.656891  [  165/  300]
per-ex loss: 0.542059  [  168/  300]
per-ex loss: 0.526907  [  171/  300]
per-ex loss: 0.689188  [  174/  300]
per-ex loss: 0.489603  [  177/  300]
per-ex loss: 0.426124  [  180/  300]
per-ex loss: 0.696071  [  183/  300]
per-ex loss: 0.520370  [  186/  300]
per-ex loss: 0.540230  [  189/  300]
per-ex loss: 0.565860  [  192/  300]
per-ex loss: 0.461021  [  195/  300]
per-ex loss: 0.706505  [  198/  300]
per-ex loss: 0.651437  [  201/  300]
per-ex loss: 0.443875  [  204/  300]
per-ex loss: 0.525217  [  207/  300]
per-ex loss: 0.484855  [  210/  300]
per-ex loss: 0.624875  [  213/  300]
per-ex loss: 0.545301  [  216/  300]
per-ex loss: 0.726771  [  219/  300]
per-ex loss: 0.541223  [  222/  300]
per-ex loss: 0.643572  [  225/  300]
per-ex loss: 0.476548  [  228/  300]
per-ex loss: 0.551764  [  231/  300]
per-ex loss: 0.677637  [  234/  300]
per-ex loss: 0.560334  [  237/  300]
per-ex loss: 0.650420  [  240/  300]
per-ex loss: 0.552692  [  243/  300]
per-ex loss: 0.469689  [  246/  300]
per-ex loss: 0.476810  [  249/  300]
per-ex loss: 0.560675  [  252/  300]
per-ex loss: 0.503593  [  255/  300]
per-ex loss: 0.443746  [  258/  300]
per-ex loss: 0.525314  [  261/  300]
per-ex loss: 0.477325  [  264/  300]
per-ex loss: 0.554879  [  267/  300]
per-ex loss: 0.501740  [  270/  300]
per-ex loss: 0.555433  [  273/  300]
per-ex loss: 0.515506  [  276/  300]
per-ex loss: 0.466329  [  279/  300]
per-ex loss: 0.502568  [  282/  300]
per-ex loss: 0.472762  [  285/  300]
per-ex loss: 0.525189  [  288/  300]
per-ex loss: 0.533374  [  291/  300]
per-ex loss: 0.430060  [  294/  300]
per-ex loss: 0.570602  [  297/  300]
per-ex loss: 0.508405  [  300/  300]
Train Error: Avg loss: 0.54283047
validation Error: 
 Avg loss: 0.52025247 
 F1: 0.461090 
 Precision: 0.394403 
 Recall: 0.554919
 IoU: 0.299621

test Error: 
 Avg loss: 0.46778494 
 F1: 0.533488 
 Precision: 0.484798 
 Recall: 0.593049
 IoU: 0.363780

We have finished training iteration 42
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_32_.pth
per-ex loss: 0.490469  [    3/  300]
per-ex loss: 0.530176  [    6/  300]
per-ex loss: 0.555740  [    9/  300]
per-ex loss: 0.587481  [   12/  300]
per-ex loss: 0.570990  [   15/  300]
per-ex loss: 0.480654  [   18/  300]
per-ex loss: 0.469617  [   21/  300]
per-ex loss: 0.458838  [   24/  300]
per-ex loss: 0.461604  [   27/  300]
per-ex loss: 0.529596  [   30/  300]
per-ex loss: 0.490134  [   33/  300]
per-ex loss: 0.599178  [   36/  300]
per-ex loss: 0.515853  [   39/  300]
per-ex loss: 0.469022  [   42/  300]
per-ex loss: 0.620409  [   45/  300]
per-ex loss: 0.614524  [   48/  300]
per-ex loss: 0.635381  [   51/  300]
per-ex loss: 0.542999  [   54/  300]
per-ex loss: 0.590609  [   57/  300]
per-ex loss: 0.437085  [   60/  300]
per-ex loss: 0.648820  [   63/  300]
per-ex loss: 0.535480  [   66/  300]
per-ex loss: 0.618818  [   69/  300]
per-ex loss: 0.506686  [   72/  300]
per-ex loss: 0.494156  [   75/  300]
per-ex loss: 0.533733  [   78/  300]
per-ex loss: 0.482105  [   81/  300]
per-ex loss: 0.609755  [   84/  300]
per-ex loss: 0.678469  [   87/  300]
per-ex loss: 0.415966  [   90/  300]
per-ex loss: 0.502449  [   93/  300]
per-ex loss: 0.646312  [   96/  300]
per-ex loss: 0.516870  [   99/  300]
per-ex loss: 0.587540  [  102/  300]
per-ex loss: 0.442723  [  105/  300]
per-ex loss: 0.520244  [  108/  300]
per-ex loss: 0.456364  [  111/  300]
per-ex loss: 0.473701  [  114/  300]
per-ex loss: 0.480140  [  117/  300]
per-ex loss: 0.496114  [  120/  300]
per-ex loss: 0.501249  [  123/  300]
per-ex loss: 0.567707  [  126/  300]
per-ex loss: 0.624208  [  129/  300]
per-ex loss: 0.431736  [  132/  300]
per-ex loss: 0.527873  [  135/  300]
per-ex loss: 0.645908  [  138/  300]
per-ex loss: 0.680548  [  141/  300]
per-ex loss: 0.581719  [  144/  300]
per-ex loss: 0.524183  [  147/  300]
per-ex loss: 0.734394  [  150/  300]
per-ex loss: 0.647289  [  153/  300]
per-ex loss: 0.500521  [  156/  300]
per-ex loss: 0.503425  [  159/  300]
per-ex loss: 0.478467  [  162/  300]
per-ex loss: 0.535292  [  165/  300]
per-ex loss: 0.533085  [  168/  300]
per-ex loss: 0.657303  [  171/  300]
per-ex loss: 0.466341  [  174/  300]
per-ex loss: 0.628916  [  177/  300]
per-ex loss: 0.453022  [  180/  300]
per-ex loss: 0.523275  [  183/  300]
per-ex loss: 0.500875  [  186/  300]
per-ex loss: 0.574146  [  189/  300]
per-ex loss: 0.607582  [  192/  300]
per-ex loss: 0.455826  [  195/  300]
per-ex loss: 0.488891  [  198/  300]
per-ex loss: 0.575483  [  201/  300]
per-ex loss: 0.453284  [  204/  300]
per-ex loss: 0.573287  [  207/  300]
per-ex loss: 0.645602  [  210/  300]
per-ex loss: 0.516872  [  213/  300]
per-ex loss: 0.566967  [  216/  300]
per-ex loss: 0.472290  [  219/  300]
per-ex loss: 0.529290  [  222/  300]
per-ex loss: 0.483665  [  225/  300]
per-ex loss: 0.652744  [  228/  300]
per-ex loss: 0.544028  [  231/  300]
per-ex loss: 0.504644  [  234/  300]
per-ex loss: 0.587873  [  237/  300]
per-ex loss: 0.437344  [  240/  300]
per-ex loss: 0.511141  [  243/  300]
per-ex loss: 0.562588  [  246/  300]
per-ex loss: 0.614277  [  249/  300]
per-ex loss: 0.621483  [  252/  300]
per-ex loss: 0.582683  [  255/  300]
per-ex loss: 0.724934  [  258/  300]
per-ex loss: 0.525834  [  261/  300]
per-ex loss: 0.722287  [  264/  300]
per-ex loss: 0.459103  [  267/  300]
per-ex loss: 0.707340  [  270/  300]
per-ex loss: 0.455249  [  273/  300]
per-ex loss: 0.546835  [  276/  300]
per-ex loss: 0.613323  [  279/  300]
per-ex loss: 0.542306  [  282/  300]
per-ex loss: 0.616152  [  285/  300]
per-ex loss: 0.519113  [  288/  300]
per-ex loss: 0.435595  [  291/  300]
per-ex loss: 0.477448  [  294/  300]
per-ex loss: 0.673854  [  297/  300]
per-ex loss: 0.611101  [  300/  300]
Train Error: Avg loss: 0.54736599
validation Error: 
 Avg loss: 0.52974709 
 F1: 0.448430 
 Precision: 0.367287 
 Recall: 0.575594
 IoU: 0.289017

test Error: 
 Avg loss: 0.47599423 
 F1: 0.524878 
 Precision: 0.455354 
 Recall: 0.619457
 IoU: 0.355820

We have finished training iteration 43
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_40_.pth
per-ex loss: 0.506764  [    3/  300]
per-ex loss: 0.566542  [    6/  300]
per-ex loss: 0.593684  [    9/  300]
per-ex loss: 0.490739  [   12/  300]
per-ex loss: 0.511717  [   15/  300]
per-ex loss: 0.431603  [   18/  300]
per-ex loss: 0.676295  [   21/  300]
per-ex loss: 0.641799  [   24/  300]
per-ex loss: 0.667965  [   27/  300]
per-ex loss: 0.531799  [   30/  300]
per-ex loss: 0.537151  [   33/  300]
per-ex loss: 0.493308  [   36/  300]
per-ex loss: 0.605799  [   39/  300]
per-ex loss: 0.470425  [   42/  300]
per-ex loss: 0.536442  [   45/  300]
per-ex loss: 0.570483  [   48/  300]
per-ex loss: 0.602337  [   51/  300]
per-ex loss: 0.554713  [   54/  300]
per-ex loss: 0.678905  [   57/  300]
per-ex loss: 0.499624  [   60/  300]
per-ex loss: 0.444481  [   63/  300]
per-ex loss: 0.542869  [   66/  300]
per-ex loss: 0.596355  [   69/  300]
per-ex loss: 0.430477  [   72/  300]
per-ex loss: 0.474190  [   75/  300]
per-ex loss: 0.500921  [   78/  300]
per-ex loss: 0.628503  [   81/  300]
per-ex loss: 0.575063  [   84/  300]
per-ex loss: 0.651836  [   87/  300]
per-ex loss: 0.499158  [   90/  300]
per-ex loss: 0.526327  [   93/  300]
per-ex loss: 0.620007  [   96/  300]
per-ex loss: 0.545513  [   99/  300]
per-ex loss: 0.565121  [  102/  300]
per-ex loss: 0.466887  [  105/  300]
per-ex loss: 0.707259  [  108/  300]
per-ex loss: 0.474693  [  111/  300]
per-ex loss: 0.467084  [  114/  300]
per-ex loss: 0.530851  [  117/  300]
per-ex loss: 0.621599  [  120/  300]
per-ex loss: 0.588431  [  123/  300]
per-ex loss: 0.543751  [  126/  300]
per-ex loss: 0.509079  [  129/  300]
per-ex loss: 0.625459  [  132/  300]
per-ex loss: 0.445089  [  135/  300]
per-ex loss: 0.425401  [  138/  300]
per-ex loss: 0.531413  [  141/  300]
per-ex loss: 0.420081  [  144/  300]
per-ex loss: 0.676367  [  147/  300]
per-ex loss: 0.643215  [  150/  300]
per-ex loss: 0.769586  [  153/  300]
per-ex loss: 0.532283  [  156/  300]
per-ex loss: 0.528585  [  159/  300]
per-ex loss: 0.587628  [  162/  300]
per-ex loss: 0.440294  [  165/  300]
per-ex loss: 0.559236  [  168/  300]
per-ex loss: 0.615389  [  171/  300]
per-ex loss: 0.589550  [  174/  300]
per-ex loss: 0.532053  [  177/  300]
per-ex loss: 0.543517  [  180/  300]
per-ex loss: 0.569937  [  183/  300]
per-ex loss: 0.515076  [  186/  300]
per-ex loss: 0.511261  [  189/  300]
per-ex loss: 0.461094  [  192/  300]
per-ex loss: 0.450868  [  195/  300]
per-ex loss: 0.464573  [  198/  300]
per-ex loss: 0.442943  [  201/  300]
per-ex loss: 0.604650  [  204/  300]
per-ex loss: 0.609508  [  207/  300]
per-ex loss: 0.627355  [  210/  300]
per-ex loss: 0.697098  [  213/  300]
per-ex loss: 0.674949  [  216/  300]
per-ex loss: 0.504127  [  219/  300]
per-ex loss: 0.612731  [  222/  300]
per-ex loss: 0.457915  [  225/  300]
per-ex loss: 0.528123  [  228/  300]
per-ex loss: 0.465758  [  231/  300]
per-ex loss: 0.619752  [  234/  300]
per-ex loss: 0.593296  [  237/  300]
per-ex loss: 0.500365  [  240/  300]
per-ex loss: 0.663999  [  243/  300]
per-ex loss: 0.578444  [  246/  300]
per-ex loss: 0.589786  [  249/  300]
per-ex loss: 0.743603  [  252/  300]
per-ex loss: 0.445466  [  255/  300]
per-ex loss: 0.487933  [  258/  300]
per-ex loss: 0.648305  [  261/  300]
per-ex loss: 0.477957  [  264/  300]
per-ex loss: 0.517717  [  267/  300]
per-ex loss: 0.556013  [  270/  300]
per-ex loss: 0.479504  [  273/  300]
per-ex loss: 0.559359  [  276/  300]
per-ex loss: 0.624069  [  279/  300]
per-ex loss: 0.519467  [  282/  300]
per-ex loss: 0.441521  [  285/  300]
per-ex loss: 0.547554  [  288/  300]
per-ex loss: 0.637667  [  291/  300]
per-ex loss: 0.628494  [  294/  300]
per-ex loss: 0.436388  [  297/  300]
per-ex loss: 0.508539  [  300/  300]
Train Error: Avg loss: 0.55146859
validation Error: 
 Avg loss: 0.52766804 
 F1: 0.466117 
 Precision: 0.423512 
 Recall: 0.518254
 IoU: 0.303881

test Error: 
 Avg loss: 0.47407770 
 F1: 0.527066 
 Precision: 0.516242 
 Recall: 0.538354
 IoU: 0.357834

We have finished training iteration 44
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_41_.pth
per-ex loss: 0.503050  [    3/  300]
per-ex loss: 0.657714  [    6/  300]
per-ex loss: 0.480552  [    9/  300]
per-ex loss: 0.510064  [   12/  300]
per-ex loss: 0.591025  [   15/  300]
per-ex loss: 0.522962  [   18/  300]
per-ex loss: 0.685817  [   21/  300]
per-ex loss: 0.668456  [   24/  300]
per-ex loss: 0.538239  [   27/  300]
per-ex loss: 0.614736  [   30/  300]
per-ex loss: 0.687875  [   33/  300]
per-ex loss: 0.431425  [   36/  300]
per-ex loss: 0.549249  [   39/  300]
per-ex loss: 0.507884  [   42/  300]
per-ex loss: 0.466939  [   45/  300]
per-ex loss: 0.479840  [   48/  300]
per-ex loss: 0.636806  [   51/  300]
per-ex loss: 0.458053  [   54/  300]
per-ex loss: 0.628781  [   57/  300]
per-ex loss: 0.652921  [   60/  300]
per-ex loss: 0.543115  [   63/  300]
per-ex loss: 0.512969  [   66/  300]
per-ex loss: 0.558845  [   69/  300]
per-ex loss: 0.436130  [   72/  300]
per-ex loss: 0.519875  [   75/  300]
per-ex loss: 0.504776  [   78/  300]
per-ex loss: 0.614148  [   81/  300]
per-ex loss: 0.465025  [   84/  300]
per-ex loss: 0.533633  [   87/  300]
per-ex loss: 0.604669  [   90/  300]
per-ex loss: 0.559449  [   93/  300]
per-ex loss: 0.537241  [   96/  300]
per-ex loss: 0.603323  [   99/  300]
per-ex loss: 0.404846  [  102/  300]
per-ex loss: 0.501309  [  105/  300]
per-ex loss: 0.708399  [  108/  300]
per-ex loss: 0.598717  [  111/  300]
per-ex loss: 0.621798  [  114/  300]
per-ex loss: 0.540926  [  117/  300]
per-ex loss: 0.470119  [  120/  300]
per-ex loss: 0.709950  [  123/  300]
per-ex loss: 0.474485  [  126/  300]
per-ex loss: 0.426400  [  129/  300]
per-ex loss: 0.634609  [  132/  300]
per-ex loss: 0.496479  [  135/  300]
per-ex loss: 0.564191  [  138/  300]
per-ex loss: 0.547701  [  141/  300]
per-ex loss: 0.675387  [  144/  300]
per-ex loss: 0.584071  [  147/  300]
per-ex loss: 0.541250  [  150/  300]
per-ex loss: 0.642253  [  153/  300]
per-ex loss: 0.456420  [  156/  300]
per-ex loss: 0.463102  [  159/  300]
per-ex loss: 0.499713  [  162/  300]
per-ex loss: 0.586548  [  165/  300]
per-ex loss: 0.468772  [  168/  300]
per-ex loss: 0.561855  [  171/  300]
per-ex loss: 0.564794  [  174/  300]
per-ex loss: 0.556281  [  177/  300]
per-ex loss: 0.491128  [  180/  300]
per-ex loss: 0.515503  [  183/  300]
per-ex loss: 0.538578  [  186/  300]
per-ex loss: 0.496191  [  189/  300]
per-ex loss: 0.474607  [  192/  300]
per-ex loss: 0.651883  [  195/  300]
per-ex loss: 0.563159  [  198/  300]
per-ex loss: 0.578360  [  201/  300]
per-ex loss: 0.636053  [  204/  300]
per-ex loss: 0.595280  [  207/  300]
per-ex loss: 0.512338  [  210/  300]
per-ex loss: 0.638172  [  213/  300]
per-ex loss: 0.520960  [  216/  300]
per-ex loss: 0.510080  [  219/  300]
per-ex loss: 0.468803  [  222/  300]
per-ex loss: 0.594579  [  225/  300]
per-ex loss: 0.463878  [  228/  300]
per-ex loss: 0.616321  [  231/  300]
per-ex loss: 0.603675  [  234/  300]
per-ex loss: 0.542641  [  237/  300]
per-ex loss: 0.539321  [  240/  300]
per-ex loss: 0.449913  [  243/  300]
per-ex loss: 0.735986  [  246/  300]
per-ex loss: 0.548135  [  249/  300]
per-ex loss: 0.443182  [  252/  300]
per-ex loss: 0.534074  [  255/  300]
per-ex loss: 0.576633  [  258/  300]
per-ex loss: 0.506251  [  261/  300]
per-ex loss: 0.571415  [  264/  300]
per-ex loss: 0.513314  [  267/  300]
per-ex loss: 0.487954  [  270/  300]
per-ex loss: 0.614433  [  273/  300]
per-ex loss: 0.418897  [  276/  300]
per-ex loss: 0.558744  [  279/  300]
per-ex loss: 0.538794  [  282/  300]
per-ex loss: 0.498876  [  285/  300]
per-ex loss: 0.562267  [  288/  300]
per-ex loss: 0.435318  [  291/  300]
per-ex loss: 0.643625  [  294/  300]
per-ex loss: 0.587098  [  297/  300]
per-ex loss: 0.659100  [  300/  300]
Train Error: Avg loss: 0.55001477
validation Error: 
 Avg loss: 0.52894598 
 F1: 0.466028 
 Precision: 0.440805 
 Recall: 0.494314
 IoU: 0.303805

test Error: 
 Avg loss: 0.46288288 
 F1: 0.538317 
 Precision: 0.537864 
 Recall: 0.538770
 IoU: 0.368285

We have finished training iteration 45
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_43_.pth
per-ex loss: 0.627864  [    3/  300]
per-ex loss: 0.552303  [    6/  300]
per-ex loss: 0.541922  [    9/  300]
per-ex loss: 0.615982  [   12/  300]
per-ex loss: 0.484962  [   15/  300]
per-ex loss: 0.501527  [   18/  300]
per-ex loss: 0.578400  [   21/  300]
per-ex loss: 0.517746  [   24/  300]
per-ex loss: 0.529261  [   27/  300]
per-ex loss: 0.639796  [   30/  300]
per-ex loss: 0.477948  [   33/  300]
per-ex loss: 0.602559  [   36/  300]
per-ex loss: 0.540295  [   39/  300]
per-ex loss: 0.439347  [   42/  300]
per-ex loss: 0.483128  [   45/  300]
per-ex loss: 0.568282  [   48/  300]
per-ex loss: 0.513777  [   51/  300]
per-ex loss: 0.509730  [   54/  300]
per-ex loss: 0.622362  [   57/  300]
per-ex loss: 0.514591  [   60/  300]
per-ex loss: 0.526617  [   63/  300]
per-ex loss: 0.552941  [   66/  300]
per-ex loss: 0.520836  [   69/  300]
per-ex loss: 0.504621  [   72/  300]
per-ex loss: 0.682946  [   75/  300]
per-ex loss: 0.524357  [   78/  300]
per-ex loss: 0.469894  [   81/  300]
per-ex loss: 0.584072  [   84/  300]
per-ex loss: 0.505429  [   87/  300]
per-ex loss: 0.583989  [   90/  300]
per-ex loss: 0.619990  [   93/  300]
per-ex loss: 0.479368  [   96/  300]
per-ex loss: 0.505641  [   99/  300]
per-ex loss: 0.610554  [  102/  300]
per-ex loss: 0.455027  [  105/  300]
per-ex loss: 0.653814  [  108/  300]
per-ex loss: 0.589896  [  111/  300]
per-ex loss: 0.552404  [  114/  300]
per-ex loss: 0.460798  [  117/  300]
per-ex loss: 0.600981  [  120/  300]
per-ex loss: 0.509948  [  123/  300]
per-ex loss: 0.573283  [  126/  300]
per-ex loss: 0.429706  [  129/  300]
per-ex loss: 0.630835  [  132/  300]
per-ex loss: 0.492198  [  135/  300]
per-ex loss: 0.497836  [  138/  300]
per-ex loss: 0.469710  [  141/  300]
per-ex loss: 0.682194  [  144/  300]
per-ex loss: 0.673191  [  147/  300]
per-ex loss: 0.533548  [  150/  300]
per-ex loss: 0.474118  [  153/  300]
per-ex loss: 0.515629  [  156/  300]
per-ex loss: 0.564580  [  159/  300]
per-ex loss: 0.542497  [  162/  300]
per-ex loss: 0.560941  [  165/  300]
per-ex loss: 0.454656  [  168/  300]
per-ex loss: 0.463068  [  171/  300]
per-ex loss: 0.543373  [  174/  300]
per-ex loss: 0.633685  [  177/  300]
per-ex loss: 0.513633  [  180/  300]
per-ex loss: 0.570539  [  183/  300]
per-ex loss: 0.642072  [  186/  300]
per-ex loss: 0.649222  [  189/  300]
per-ex loss: 0.550774  [  192/  300]
per-ex loss: 0.552193  [  195/  300]
per-ex loss: 0.546771  [  198/  300]
per-ex loss: 0.468513  [  201/  300]
per-ex loss: 0.531964  [  204/  300]
per-ex loss: 0.482675  [  207/  300]
per-ex loss: 0.514636  [  210/  300]
per-ex loss: 0.580159  [  213/  300]
per-ex loss: 0.483458  [  216/  300]
per-ex loss: 0.486453  [  219/  300]
per-ex loss: 0.453435  [  222/  300]
per-ex loss: 0.489535  [  225/  300]
per-ex loss: 0.452827  [  228/  300]
per-ex loss: 0.603285  [  231/  300]
per-ex loss: 0.628069  [  234/  300]
per-ex loss: 0.553839  [  237/  300]
per-ex loss: 0.658644  [  240/  300]
per-ex loss: 0.555272  [  243/  300]
per-ex loss: 0.652728  [  246/  300]
per-ex loss: 0.564641  [  249/  300]
per-ex loss: 0.526420  [  252/  300]
per-ex loss: 0.662967  [  255/  300]
per-ex loss: 0.562193  [  258/  300]
per-ex loss: 0.470770  [  261/  300]
per-ex loss: 0.569788  [  264/  300]
per-ex loss: 0.560076  [  267/  300]
per-ex loss: 0.548983  [  270/  300]
per-ex loss: 0.493156  [  273/  300]
per-ex loss: 0.508640  [  276/  300]
per-ex loss: 0.536859  [  279/  300]
per-ex loss: 0.575050  [  282/  300]
per-ex loss: 0.569368  [  285/  300]
per-ex loss: 0.661296  [  288/  300]
per-ex loss: 0.544439  [  291/  300]
per-ex loss: 0.532779  [  294/  300]
per-ex loss: 0.708407  [  297/  300]
per-ex loss: 0.554165  [  300/  300]
Train Error: Avg loss: 0.54861643
validation Error: 
 Avg loss: 0.52519208 
 F1: 0.445354 
 Precision: 0.340147 
 Recall: 0.644784
 IoU: 0.286466

test Error: 
 Avg loss: 0.49268466 
 F1: 0.509023 
 Precision: 0.407439 
 Recall: 0.678087
 IoU: 0.341402

We have finished training iteration 46
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_34_.pth
per-ex loss: 0.496520  [    3/  300]
per-ex loss: 0.522541  [    6/  300]
per-ex loss: 0.563694  [    9/  300]
per-ex loss: 0.619690  [   12/  300]
per-ex loss: 0.513672  [   15/  300]
per-ex loss: 0.496272  [   18/  300]
per-ex loss: 0.659641  [   21/  300]
per-ex loss: 0.550935  [   24/  300]
per-ex loss: 0.565334  [   27/  300]
per-ex loss: 0.469065  [   30/  300]
per-ex loss: 0.528174  [   33/  300]
per-ex loss: 0.439275  [   36/  300]
per-ex loss: 0.564959  [   39/  300]
per-ex loss: 0.417916  [   42/  300]
per-ex loss: 0.510787  [   45/  300]
per-ex loss: 0.527205  [   48/  300]
per-ex loss: 0.480959  [   51/  300]
per-ex loss: 0.573812  [   54/  300]
per-ex loss: 0.694299  [   57/  300]
per-ex loss: 0.458009  [   60/  300]
per-ex loss: 0.671566  [   63/  300]
per-ex loss: 0.547714  [   66/  300]
per-ex loss: 0.462560  [   69/  300]
per-ex loss: 0.492839  [   72/  300]
per-ex loss: 0.617479  [   75/  300]
per-ex loss: 0.573945  [   78/  300]
per-ex loss: 0.530122  [   81/  300]
per-ex loss: 0.584134  [   84/  300]
per-ex loss: 0.525406  [   87/  300]
per-ex loss: 0.579280  [   90/  300]
per-ex loss: 0.567726  [   93/  300]
per-ex loss: 0.542620  [   96/  300]
per-ex loss: 0.603064  [   99/  300]
per-ex loss: 0.568167  [  102/  300]
per-ex loss: 0.480267  [  105/  300]
per-ex loss: 0.513847  [  108/  300]
per-ex loss: 0.518851  [  111/  300]
per-ex loss: 0.532755  [  114/  300]
per-ex loss: 0.560661  [  117/  300]
per-ex loss: 0.530789  [  120/  300]
per-ex loss: 0.497290  [  123/  300]
per-ex loss: 0.443122  [  126/  300]
per-ex loss: 0.588673  [  129/  300]
per-ex loss: 0.476678  [  132/  300]
per-ex loss: 0.471169  [  135/  300]
per-ex loss: 0.576744  [  138/  300]
per-ex loss: 0.581348  [  141/  300]
per-ex loss: 0.550706  [  144/  300]
per-ex loss: 0.570629  [  147/  300]
per-ex loss: 0.564323  [  150/  300]
per-ex loss: 0.442447  [  153/  300]
per-ex loss: 0.633982  [  156/  300]
per-ex loss: 0.636586  [  159/  300]
per-ex loss: 0.480730  [  162/  300]
per-ex loss: 0.497087  [  165/  300]
per-ex loss: 0.639594  [  168/  300]
per-ex loss: 0.540089  [  171/  300]
per-ex loss: 0.503697  [  174/  300]
per-ex loss: 0.586591  [  177/  300]
per-ex loss: 0.525323  [  180/  300]
per-ex loss: 0.571600  [  183/  300]
per-ex loss: 0.625639  [  186/  300]
per-ex loss: 0.619931  [  189/  300]
per-ex loss: 0.459653  [  192/  300]
per-ex loss: 0.519442  [  195/  300]
per-ex loss: 0.490401  [  198/  300]
per-ex loss: 0.492557  [  201/  300]
per-ex loss: 0.406473  [  204/  300]
per-ex loss: 0.591061  [  207/  300]
per-ex loss: 0.496496  [  210/  300]
per-ex loss: 0.609957  [  213/  300]
per-ex loss: 0.528570  [  216/  300]
per-ex loss: 0.503601  [  219/  300]
per-ex loss: 0.628409  [  222/  300]
per-ex loss: 0.427273  [  225/  300]
per-ex loss: 0.513197  [  228/  300]
per-ex loss: 0.472029  [  231/  300]
per-ex loss: 0.524929  [  234/  300]
per-ex loss: 0.560270  [  237/  300]
per-ex loss: 0.711767  [  240/  300]
per-ex loss: 0.542200  [  243/  300]
per-ex loss: 0.504693  [  246/  300]
per-ex loss: 0.474599  [  249/  300]
per-ex loss: 0.448005  [  252/  300]
per-ex loss: 0.515579  [  255/  300]
per-ex loss: 0.527127  [  258/  300]
per-ex loss: 0.496912  [  261/  300]
per-ex loss: 0.663759  [  264/  300]
per-ex loss: 0.507136  [  267/  300]
per-ex loss: 0.530086  [  270/  300]
per-ex loss: 0.454852  [  273/  300]
per-ex loss: 0.649150  [  276/  300]
per-ex loss: 0.477561  [  279/  300]
per-ex loss: 0.613475  [  282/  300]
per-ex loss: 0.474944  [  285/  300]
per-ex loss: 0.464663  [  288/  300]
per-ex loss: 0.652481  [  291/  300]
per-ex loss: 0.456720  [  294/  300]
per-ex loss: 0.410184  [  297/  300]
per-ex loss: 0.580592  [  300/  300]
Train Error: Avg loss: 0.53661333
validation Error: 
 Avg loss: 0.52411497 
 F1: 0.470307 
 Precision: 0.427436 
 Recall: 0.522737
 IoU: 0.307452

test Error: 
 Avg loss: 0.46267974 
 F1: 0.538199 
 Precision: 0.520130 
 Recall: 0.557570
 IoU: 0.368176

We have finished training iteration 47
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_45_.pth
per-ex loss: 0.468715  [    3/  300]
per-ex loss: 0.564808  [    6/  300]
per-ex loss: 0.603213  [    9/  300]
per-ex loss: 0.471808  [   12/  300]
per-ex loss: 0.579418  [   15/  300]
per-ex loss: 0.514916  [   18/  300]
per-ex loss: 0.596371  [   21/  300]
per-ex loss: 0.556416  [   24/  300]
per-ex loss: 0.614646  [   27/  300]
per-ex loss: 0.591405  [   30/  300]
per-ex loss: 0.565939  [   33/  300]
per-ex loss: 0.530948  [   36/  300]
per-ex loss: 0.636076  [   39/  300]
per-ex loss: 0.639852  [   42/  300]
per-ex loss: 0.472079  [   45/  300]
per-ex loss: 0.506429  [   48/  300]
per-ex loss: 0.488449  [   51/  300]
per-ex loss: 0.562006  [   54/  300]
per-ex loss: 0.424508  [   57/  300]
per-ex loss: 0.550950  [   60/  300]
per-ex loss: 0.618321  [   63/  300]
per-ex loss: 0.605806  [   66/  300]
per-ex loss: 0.452510  [   69/  300]
per-ex loss: 0.550205  [   72/  300]
per-ex loss: 0.507279  [   75/  300]
per-ex loss: 0.593648  [   78/  300]
per-ex loss: 0.655782  [   81/  300]
per-ex loss: 0.505895  [   84/  300]
per-ex loss: 0.451528  [   87/  300]
per-ex loss: 0.494408  [   90/  300]
per-ex loss: 0.649803  [   93/  300]
per-ex loss: 0.508547  [   96/  300]
per-ex loss: 0.634081  [   99/  300]
per-ex loss: 0.587913  [  102/  300]
per-ex loss: 0.432266  [  105/  300]
per-ex loss: 0.631144  [  108/  300]
per-ex loss: 0.516226  [  111/  300]
per-ex loss: 0.402561  [  114/  300]
per-ex loss: 0.504045  [  117/  300]
per-ex loss: 0.476188  [  120/  300]
per-ex loss: 0.442763  [  123/  300]
per-ex loss: 0.562883  [  126/  300]
per-ex loss: 0.611768  [  129/  300]
per-ex loss: 0.459522  [  132/  300]
per-ex loss: 0.522935  [  135/  300]
per-ex loss: 0.658991  [  138/  300]
per-ex loss: 0.469092  [  141/  300]
per-ex loss: 0.541226  [  144/  300]
per-ex loss: 0.555374  [  147/  300]
per-ex loss: 0.733729  [  150/  300]
per-ex loss: 0.515074  [  153/  300]
per-ex loss: 0.527857  [  156/  300]
per-ex loss: 0.512312  [  159/  300]
per-ex loss: 0.517198  [  162/  300]
per-ex loss: 0.661553  [  165/  300]
per-ex loss: 0.662022  [  168/  300]
per-ex loss: 0.437010  [  171/  300]
per-ex loss: 0.518885  [  174/  300]
per-ex loss: 0.545002  [  177/  300]
per-ex loss: 0.620028  [  180/  300]
per-ex loss: 0.501850  [  183/  300]
per-ex loss: 0.559478  [  186/  300]
per-ex loss: 0.624712  [  189/  300]
per-ex loss: 0.555387  [  192/  300]
per-ex loss: 0.602157  [  195/  300]
per-ex loss: 0.625484  [  198/  300]
per-ex loss: 0.543402  [  201/  300]
per-ex loss: 0.582634  [  204/  300]
per-ex loss: 0.705664  [  207/  300]
per-ex loss: 0.566446  [  210/  300]
per-ex loss: 0.628958  [  213/  300]
per-ex loss: 0.500310  [  216/  300]
per-ex loss: 0.474865  [  219/  300]
per-ex loss: 0.529091  [  222/  300]
per-ex loss: 0.463643  [  225/  300]
per-ex loss: 0.442921  [  228/  300]
per-ex loss: 0.478650  [  231/  300]
per-ex loss: 0.570548  [  234/  300]
per-ex loss: 0.564493  [  237/  300]
per-ex loss: 0.479779  [  240/  300]
per-ex loss: 0.497736  [  243/  300]
per-ex loss: 0.514396  [  246/  300]
per-ex loss: 0.473265  [  249/  300]
per-ex loss: 0.590267  [  252/  300]
per-ex loss: 0.494726  [  255/  300]
per-ex loss: 0.537440  [  258/  300]
per-ex loss: 0.444058  [  261/  300]
per-ex loss: 0.476977  [  264/  300]
per-ex loss: 0.506500  [  267/  300]
per-ex loss: 0.497447  [  270/  300]
per-ex loss: 0.556309  [  273/  300]
per-ex loss: 0.510360  [  276/  300]
per-ex loss: 0.561384  [  279/  300]
per-ex loss: 0.492540  [  282/  300]
per-ex loss: 0.611172  [  285/  300]
per-ex loss: 0.455488  [  288/  300]
per-ex loss: 0.497604  [  291/  300]
per-ex loss: 0.519749  [  294/  300]
per-ex loss: 0.553352  [  297/  300]
per-ex loss: 0.644265  [  300/  300]
Train Error: Avg loss: 0.54227841
validation Error: 
 Avg loss: 0.50832838 
 F1: 0.470010 
 Precision: 0.384692 
 Recall: 0.603958
 IoU: 0.307198

test Error: 
 Avg loss: 0.46821070 
 F1: 0.533167 
 Precision: 0.458532 
 Recall: 0.636823
 IoU: 0.363482

We have finished training iteration 48
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_44_.pth
per-ex loss: 0.544818  [    3/  300]
per-ex loss: 0.470176  [    6/  300]
per-ex loss: 0.519966  [    9/  300]
per-ex loss: 0.521759  [   12/  300]
per-ex loss: 0.626207  [   15/  300]
per-ex loss: 0.423506  [   18/  300]
per-ex loss: 0.686507  [   21/  300]
per-ex loss: 0.549667  [   24/  300]
per-ex loss: 0.444688  [   27/  300]
per-ex loss: 0.698159  [   30/  300]
per-ex loss: 0.586601  [   33/  300]
per-ex loss: 0.610295  [   36/  300]
per-ex loss: 0.687515  [   39/  300]
per-ex loss: 0.478919  [   42/  300]
per-ex loss: 0.489830  [   45/  300]
per-ex loss: 0.436511  [   48/  300]
per-ex loss: 0.538772  [   51/  300]
per-ex loss: 0.467702  [   54/  300]
per-ex loss: 0.507356  [   57/  300]
per-ex loss: 0.547635  [   60/  300]
per-ex loss: 0.478300  [   63/  300]
per-ex loss: 0.488193  [   66/  300]
per-ex loss: 0.458486  [   69/  300]
per-ex loss: 0.555796  [   72/  300]
per-ex loss: 0.674901  [   75/  300]
per-ex loss: 0.565246  [   78/  300]
per-ex loss: 0.439602  [   81/  300]
per-ex loss: 0.482877  [   84/  300]
per-ex loss: 0.634370  [   87/  300]
per-ex loss: 0.458993  [   90/  300]
per-ex loss: 0.423834  [   93/  300]
per-ex loss: 0.619268  [   96/  300]
per-ex loss: 0.550919  [   99/  300]
per-ex loss: 0.583631  [  102/  300]
per-ex loss: 0.662225  [  105/  300]
per-ex loss: 0.604331  [  108/  300]
per-ex loss: 0.477538  [  111/  300]
per-ex loss: 0.548960  [  114/  300]
per-ex loss: 0.642041  [  117/  300]
per-ex loss: 0.492589  [  120/  300]
per-ex loss: 0.491904  [  123/  300]
per-ex loss: 0.524842  [  126/  300]
per-ex loss: 0.493678  [  129/  300]
per-ex loss: 0.591181  [  132/  300]
per-ex loss: 0.742305  [  135/  300]
per-ex loss: 0.543193  [  138/  300]
per-ex loss: 0.507254  [  141/  300]
per-ex loss: 0.628932  [  144/  300]
per-ex loss: 0.610620  [  147/  300]
per-ex loss: 0.574322  [  150/  300]
per-ex loss: 0.634709  [  153/  300]
per-ex loss: 0.518000  [  156/  300]
per-ex loss: 0.628575  [  159/  300]
per-ex loss: 0.499771  [  162/  300]
per-ex loss: 0.600039  [  165/  300]
per-ex loss: 0.552837  [  168/  300]
per-ex loss: 0.573486  [  171/  300]
per-ex loss: 0.509427  [  174/  300]
per-ex loss: 0.538784  [  177/  300]
per-ex loss: 0.553777  [  180/  300]
per-ex loss: 0.466709  [  183/  300]
per-ex loss: 0.596220  [  186/  300]
per-ex loss: 0.499709  [  189/  300]
per-ex loss: 0.542835  [  192/  300]
per-ex loss: 0.541762  [  195/  300]
per-ex loss: 0.462348  [  198/  300]
per-ex loss: 0.558565  [  201/  300]
per-ex loss: 0.466168  [  204/  300]
per-ex loss: 0.538527  [  207/  300]
per-ex loss: 0.566590  [  210/  300]
per-ex loss: 0.692815  [  213/  300]
per-ex loss: 0.520307  [  216/  300]
per-ex loss: 0.562679  [  219/  300]
per-ex loss: 0.480975  [  222/  300]
per-ex loss: 0.484032  [  225/  300]
per-ex loss: 0.537504  [  228/  300]
per-ex loss: 0.489377  [  231/  300]
per-ex loss: 0.560167  [  234/  300]
per-ex loss: 0.507559  [  237/  300]
per-ex loss: 0.477603  [  240/  300]
per-ex loss: 0.669121  [  243/  300]
per-ex loss: 0.616130  [  246/  300]
per-ex loss: 0.472583  [  249/  300]
per-ex loss: 0.680171  [  252/  300]
per-ex loss: 0.520355  [  255/  300]
per-ex loss: 0.433418  [  258/  300]
per-ex loss: 0.615021  [  261/  300]
per-ex loss: 0.472006  [  264/  300]
per-ex loss: 0.491424  [  267/  300]
per-ex loss: 0.542107  [  270/  300]
per-ex loss: 0.546473  [  273/  300]
per-ex loss: 0.597029  [  276/  300]
per-ex loss: 0.510194  [  279/  300]
per-ex loss: 0.529905  [  282/  300]
per-ex loss: 0.459514  [  285/  300]
per-ex loss: 0.512866  [  288/  300]
per-ex loss: 0.503717  [  291/  300]
per-ex loss: 0.476829  [  294/  300]
per-ex loss: 0.534248  [  297/  300]
per-ex loss: 0.660032  [  300/  300]
Train Error: Avg loss: 0.54391918
validation Error: 
 Avg loss: 0.51237822 
 F1: 0.467949 
 Precision: 0.388320 
 Recall: 0.588660
 IoU: 0.305440

test Error: 
 Avg loss: 0.45992714 
 F1: 0.540946 
 Precision: 0.480611 
 Recall: 0.618604
 IoU: 0.370751

We have finished training iteration 49
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_46_.pth
per-ex loss: 0.541749  [    3/  300]
per-ex loss: 0.467760  [    6/  300]
per-ex loss: 0.552380  [    9/  300]
per-ex loss: 0.428243  [   12/  300]
per-ex loss: 0.508514  [   15/  300]
per-ex loss: 0.510500  [   18/  300]
per-ex loss: 0.539151  [   21/  300]
per-ex loss: 0.406889  [   24/  300]
per-ex loss: 0.568306  [   27/  300]
per-ex loss: 0.553726  [   30/  300]
per-ex loss: 0.486531  [   33/  300]
per-ex loss: 0.536777  [   36/  300]
per-ex loss: 0.502931  [   39/  300]
per-ex loss: 0.499172  [   42/  300]
per-ex loss: 0.575505  [   45/  300]
per-ex loss: 0.522376  [   48/  300]
per-ex loss: 0.700818  [   51/  300]
per-ex loss: 0.622851  [   54/  300]
per-ex loss: 0.533278  [   57/  300]
per-ex loss: 0.521624  [   60/  300]
per-ex loss: 0.602550  [   63/  300]
per-ex loss: 0.540814  [   66/  300]
per-ex loss: 0.547698  [   69/  300]
per-ex loss: 0.546767  [   72/  300]
per-ex loss: 0.544394  [   75/  300]
per-ex loss: 0.486470  [   78/  300]
per-ex loss: 0.662587  [   81/  300]
per-ex loss: 0.532174  [   84/  300]
per-ex loss: 0.479129  [   87/  300]
per-ex loss: 0.526353  [   90/  300]
per-ex loss: 0.532212  [   93/  300]
per-ex loss: 0.498661  [   96/  300]
per-ex loss: 0.456423  [   99/  300]
per-ex loss: 0.525629  [  102/  300]
per-ex loss: 0.458512  [  105/  300]
per-ex loss: 0.626884  [  108/  300]
per-ex loss: 0.681772  [  111/  300]
per-ex loss: 0.565393  [  114/  300]
per-ex loss: 0.519759  [  117/  300]
per-ex loss: 0.579615  [  120/  300]
per-ex loss: 0.501630  [  123/  300]
per-ex loss: 0.517389  [  126/  300]
per-ex loss: 0.607961  [  129/  300]
per-ex loss: 0.556172  [  132/  300]
per-ex loss: 0.420738  [  135/  300]
per-ex loss: 0.540690  [  138/  300]
per-ex loss: 0.661528  [  141/  300]
per-ex loss: 0.575412  [  144/  300]
per-ex loss: 0.457120  [  147/  300]
per-ex loss: 0.528628  [  150/  300]
per-ex loss: 0.498615  [  153/  300]
per-ex loss: 0.483804  [  156/  300]
per-ex loss: 0.429900  [  159/  300]
per-ex loss: 0.526238  [  162/  300]
per-ex loss: 0.506579  [  165/  300]
per-ex loss: 0.517892  [  168/  300]
per-ex loss: 0.440217  [  171/  300]
per-ex loss: 0.530824  [  174/  300]
per-ex loss: 0.642786  [  177/  300]
per-ex loss: 0.457958  [  180/  300]
per-ex loss: 0.571664  [  183/  300]
per-ex loss: 0.641065  [  186/  300]
per-ex loss: 0.600058  [  189/  300]
per-ex loss: 0.462746  [  192/  300]
per-ex loss: 0.691373  [  195/  300]
per-ex loss: 0.462716  [  198/  300]
per-ex loss: 0.430004  [  201/  300]
per-ex loss: 0.707655  [  204/  300]
per-ex loss: 0.613302  [  207/  300]
per-ex loss: 0.543650  [  210/  300]
per-ex loss: 0.560231  [  213/  300]
per-ex loss: 0.494093  [  216/  300]
per-ex loss: 0.528618  [  219/  300]
per-ex loss: 0.477314  [  222/  300]
per-ex loss: 0.573310  [  225/  300]
per-ex loss: 0.537171  [  228/  300]
per-ex loss: 0.482523  [  231/  300]
per-ex loss: 0.553937  [  234/  300]
per-ex loss: 0.571346  [  237/  300]
per-ex loss: 0.523688  [  240/  300]
per-ex loss: 0.428414  [  243/  300]
per-ex loss: 0.655825  [  246/  300]
per-ex loss: 0.412395  [  249/  300]
per-ex loss: 0.571047  [  252/  300]
per-ex loss: 0.464652  [  255/  300]
per-ex loss: 0.632367  [  258/  300]
per-ex loss: 0.511193  [  261/  300]
per-ex loss: 0.532221  [  264/  300]
per-ex loss: 0.477727  [  267/  300]
per-ex loss: 0.591989  [  270/  300]
per-ex loss: 0.581850  [  273/  300]
per-ex loss: 0.703125  [  276/  300]
per-ex loss: 0.676015  [  279/  300]
per-ex loss: 0.512462  [  282/  300]
per-ex loss: 0.673904  [  285/  300]
per-ex loss: 0.503566  [  288/  300]
per-ex loss: 0.668302  [  291/  300]
per-ex loss: 0.620226  [  294/  300]
per-ex loss: 0.571219  [  297/  300]
per-ex loss: 0.530736  [  300/  300]
Train Error: Avg loss: 0.54240661
validation Error: 
 Avg loss: 0.53390219 
 F1: 0.462732 
 Precision: 0.426952 
 Recall: 0.505059
 IoU: 0.301010

test Error: 
 Avg loss: 0.47224754 
 F1: 0.528922 
 Precision: 0.525290 
 Recall: 0.532605
 IoU: 0.359547

We have finished training iteration 50
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_47_.pth
per-ex loss: 0.582562  [    3/  300]
per-ex loss: 0.526961  [    6/  300]
per-ex loss: 0.477398  [    9/  300]
per-ex loss: 0.572606  [   12/  300]
per-ex loss: 0.632231  [   15/  300]
per-ex loss: 0.438649  [   18/  300]
per-ex loss: 0.594568  [   21/  300]
per-ex loss: 0.462811  [   24/  300]
per-ex loss: 0.567366  [   27/  300]
per-ex loss: 0.484643  [   30/  300]
per-ex loss: 0.598263  [   33/  300]
per-ex loss: 0.523997  [   36/  300]
per-ex loss: 0.669878  [   39/  300]
per-ex loss: 0.527808  [   42/  300]
per-ex loss: 0.467577  [   45/  300]
per-ex loss: 0.433054  [   48/  300]
per-ex loss: 0.556582  [   51/  300]
per-ex loss: 0.562246  [   54/  300]
per-ex loss: 0.645133  [   57/  300]
per-ex loss: 0.644975  [   60/  300]
per-ex loss: 0.694165  [   63/  300]
per-ex loss: 0.590803  [   66/  300]
per-ex loss: 0.674064  [   69/  300]
per-ex loss: 0.470131  [   72/  300]
per-ex loss: 0.546586  [   75/  300]
per-ex loss: 0.472159  [   78/  300]
per-ex loss: 0.591251  [   81/  300]
per-ex loss: 0.520627  [   84/  300]
per-ex loss: 0.496657  [   87/  300]
per-ex loss: 0.500061  [   90/  300]
per-ex loss: 0.603502  [   93/  300]
per-ex loss: 0.551119  [   96/  300]
per-ex loss: 0.545541  [   99/  300]
per-ex loss: 0.482002  [  102/  300]
per-ex loss: 0.512084  [  105/  300]
per-ex loss: 0.519035  [  108/  300]
per-ex loss: 0.599427  [  111/  300]
per-ex loss: 0.484957  [  114/  300]
per-ex loss: 0.574967  [  117/  300]
per-ex loss: 0.413905  [  120/  300]
per-ex loss: 0.563229  [  123/  300]
per-ex loss: 0.480192  [  126/  300]
per-ex loss: 0.603807  [  129/  300]
per-ex loss: 0.444991  [  132/  300]
per-ex loss: 0.511134  [  135/  300]
per-ex loss: 0.476972  [  138/  300]
per-ex loss: 0.551792  [  141/  300]
per-ex loss: 0.521197  [  144/  300]
per-ex loss: 0.533979  [  147/  300]
per-ex loss: 0.645412  [  150/  300]
per-ex loss: 0.509624  [  153/  300]
per-ex loss: 0.570202  [  156/  300]
per-ex loss: 0.519583  [  159/  300]
per-ex loss: 0.600765  [  162/  300]
per-ex loss: 0.645468  [  165/  300]
per-ex loss: 0.567362  [  168/  300]
per-ex loss: 0.526122  [  171/  300]
per-ex loss: 0.475878  [  174/  300]
per-ex loss: 0.614699  [  177/  300]
per-ex loss: 0.559515  [  180/  300]
per-ex loss: 0.557777  [  183/  300]
per-ex loss: 0.548510  [  186/  300]
per-ex loss: 0.506328  [  189/  300]
per-ex loss: 0.523545  [  192/  300]
per-ex loss: 0.492461  [  195/  300]
per-ex loss: 0.551385  [  198/  300]
per-ex loss: 0.493719  [  201/  300]
per-ex loss: 0.580797  [  204/  300]
per-ex loss: 0.435361  [  207/  300]
per-ex loss: 0.605460  [  210/  300]
per-ex loss: 0.554424  [  213/  300]
per-ex loss: 0.504972  [  216/  300]
per-ex loss: 0.483371  [  219/  300]
per-ex loss: 0.497783  [  222/  300]
per-ex loss: 0.494074  [  225/  300]
per-ex loss: 0.575817  [  228/  300]
per-ex loss: 0.551507  [  231/  300]
per-ex loss: 0.511729  [  234/  300]
per-ex loss: 0.510271  [  237/  300]
per-ex loss: 0.576805  [  240/  300]
per-ex loss: 0.525404  [  243/  300]
per-ex loss: 0.435086  [  246/  300]
per-ex loss: 0.458200  [  249/  300]
per-ex loss: 0.452803  [  252/  300]
per-ex loss: 0.709608  [  255/  300]
per-ex loss: 0.667118  [  258/  300]
per-ex loss: 0.614938  [  261/  300]
per-ex loss: 0.540853  [  264/  300]
per-ex loss: 0.444392  [  267/  300]
per-ex loss: 0.487619  [  270/  300]
per-ex loss: 0.603563  [  273/  300]
per-ex loss: 0.641081  [  276/  300]
per-ex loss: 0.454482  [  279/  300]
per-ex loss: 0.503257  [  282/  300]
per-ex loss: 0.438469  [  285/  300]
per-ex loss: 0.599469  [  288/  300]
per-ex loss: 0.639197  [  291/  300]
per-ex loss: 0.489523  [  294/  300]
per-ex loss: 0.483621  [  297/  300]
per-ex loss: 0.573988  [  300/  300]
Train Error: Avg loss: 0.54053038
validation Error: 
 Avg loss: 0.53590441 
 F1: 0.456705 
 Precision: 0.412340 
 Recall: 0.511766
 IoU: 0.295928

test Error: 
 Avg loss: 0.48675179 
 F1: 0.514627 
 Precision: 0.497941 
 Recall: 0.532469
 IoU: 0.346463

We have finished training iteration 51
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_42_.pth
per-ex loss: 0.646351  [    3/  300]
per-ex loss: 0.417436  [    6/  300]
per-ex loss: 0.498894  [    9/  300]
per-ex loss: 0.456573  [   12/  300]
per-ex loss: 0.440375  [   15/  300]
per-ex loss: 0.519151  [   18/  300]
per-ex loss: 0.528595  [   21/  300]
per-ex loss: 0.680415  [   24/  300]
per-ex loss: 0.589833  [   27/  300]
per-ex loss: 0.620821  [   30/  300]
per-ex loss: 0.506283  [   33/  300]
per-ex loss: 0.564503  [   36/  300]
per-ex loss: 0.557406  [   39/  300]
per-ex loss: 0.522722  [   42/  300]
per-ex loss: 0.488705  [   45/  300]
per-ex loss: 0.573221  [   48/  300]
per-ex loss: 0.618413  [   51/  300]
per-ex loss: 0.458501  [   54/  300]
per-ex loss: 0.506044  [   57/  300]
per-ex loss: 0.612205  [   60/  300]
per-ex loss: 0.530067  [   63/  300]
per-ex loss: 0.574583  [   66/  300]
per-ex loss: 0.623101  [   69/  300]
per-ex loss: 0.520240  [   72/  300]
per-ex loss: 0.550491  [   75/  300]
per-ex loss: 0.582680  [   78/  300]
per-ex loss: 0.585706  [   81/  300]
per-ex loss: 0.511713  [   84/  300]
per-ex loss: 0.451416  [   87/  300]
per-ex loss: 0.575172  [   90/  300]
per-ex loss: 0.621426  [   93/  300]
per-ex loss: 0.652188  [   96/  300]
per-ex loss: 0.589564  [   99/  300]
per-ex loss: 0.574196  [  102/  300]
per-ex loss: 0.501675  [  105/  300]
per-ex loss: 0.465526  [  108/  300]
per-ex loss: 0.494301  [  111/  300]
per-ex loss: 0.446127  [  114/  300]
per-ex loss: 0.490987  [  117/  300]
per-ex loss: 0.632733  [  120/  300]
per-ex loss: 0.521436  [  123/  300]
per-ex loss: 0.511379  [  126/  300]
per-ex loss: 0.594260  [  129/  300]
per-ex loss: 0.477160  [  132/  300]
per-ex loss: 0.473605  [  135/  300]
per-ex loss: 0.587391  [  138/  300]
per-ex loss: 0.509377  [  141/  300]
per-ex loss: 0.485928  [  144/  300]
per-ex loss: 0.553065  [  147/  300]
per-ex loss: 0.460459  [  150/  300]
per-ex loss: 0.583992  [  153/  300]
per-ex loss: 0.600744  [  156/  300]
per-ex loss: 0.526943  [  159/  300]
per-ex loss: 0.712504  [  162/  300]
per-ex loss: 0.579861  [  165/  300]
per-ex loss: 0.492242  [  168/  300]
per-ex loss: 0.522811  [  171/  300]
per-ex loss: 0.478350  [  174/  300]
per-ex loss: 0.402158  [  177/  300]
per-ex loss: 0.487895  [  180/  300]
per-ex loss: 0.431277  [  183/  300]
per-ex loss: 0.584363  [  186/  300]
per-ex loss: 0.515477  [  189/  300]
per-ex loss: 0.523834  [  192/  300]
per-ex loss: 0.591269  [  195/  300]
per-ex loss: 0.543549  [  198/  300]
per-ex loss: 0.494179  [  201/  300]
per-ex loss: 0.426966  [  204/  300]
per-ex loss: 0.451647  [  207/  300]
per-ex loss: 0.703580  [  210/  300]
per-ex loss: 0.464799  [  213/  300]
per-ex loss: 0.481854  [  216/  300]
per-ex loss: 0.443684  [  219/  300]
per-ex loss: 0.448966  [  222/  300]
per-ex loss: 0.599767  [  225/  300]
per-ex loss: 0.537257  [  228/  300]
per-ex loss: 0.603590  [  231/  300]
per-ex loss: 0.495718  [  234/  300]
per-ex loss: 0.566961  [  237/  300]
per-ex loss: 0.651320  [  240/  300]
per-ex loss: 0.579469  [  243/  300]
per-ex loss: 0.670808  [  246/  300]
per-ex loss: 0.506776  [  249/  300]
per-ex loss: 0.555416  [  252/  300]
per-ex loss: 0.558504  [  255/  300]
per-ex loss: 0.612366  [  258/  300]
per-ex loss: 0.469563  [  261/  300]
per-ex loss: 0.619501  [  264/  300]
per-ex loss: 0.447385  [  267/  300]
per-ex loss: 0.459355  [  270/  300]
per-ex loss: 0.640361  [  273/  300]
per-ex loss: 0.597622  [  276/  300]
per-ex loss: 0.587706  [  279/  300]
per-ex loss: 0.478159  [  282/  300]
per-ex loss: 0.506908  [  285/  300]
per-ex loss: 0.541205  [  288/  300]
per-ex loss: 0.522452  [  291/  300]
per-ex loss: 0.503592  [  294/  300]
per-ex loss: 0.476796  [  297/  300]
per-ex loss: 0.456872  [  300/  300]
Train Error: Avg loss: 0.53692771
validation Error: 
 Avg loss: 0.53171039 
 F1: 0.461735 
 Precision: 0.450788 
 Recall: 0.473227
 IoU: 0.300166

test Error: 
 Avg loss: 0.47294223 
 F1: 0.527953 
 Precision: 0.510380 
 Recall: 0.546779
 IoU: 0.358652

We have finished training iteration 52
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_50_.pth
per-ex loss: 0.677503  [    3/  300]
per-ex loss: 0.523840  [    6/  300]
per-ex loss: 0.515100  [    9/  300]
per-ex loss: 0.443595  [   12/  300]
per-ex loss: 0.503245  [   15/  300]
per-ex loss: 0.500574  [   18/  300]
per-ex loss: 0.486807  [   21/  300]
per-ex loss: 0.534108  [   24/  300]
per-ex loss: 0.477075  [   27/  300]
per-ex loss: 0.614054  [   30/  300]
per-ex loss: 0.562646  [   33/  300]
per-ex loss: 0.463280  [   36/  300]
per-ex loss: 0.581498  [   39/  300]
per-ex loss: 0.629377  [   42/  300]
per-ex loss: 0.605300  [   45/  300]
per-ex loss: 0.409483  [   48/  300]
per-ex loss: 0.581941  [   51/  300]
per-ex loss: 0.472120  [   54/  300]
per-ex loss: 0.476475  [   57/  300]
per-ex loss: 0.665824  [   60/  300]
per-ex loss: 0.472324  [   63/  300]
per-ex loss: 0.515065  [   66/  300]
per-ex loss: 0.544710  [   69/  300]
per-ex loss: 0.528823  [   72/  300]
per-ex loss: 0.474668  [   75/  300]
per-ex loss: 0.660690  [   78/  300]
per-ex loss: 0.564252  [   81/  300]
per-ex loss: 0.438964  [   84/  300]
per-ex loss: 0.496213  [   87/  300]
per-ex loss: 0.432266  [   90/  300]
per-ex loss: 0.511641  [   93/  300]
per-ex loss: 0.655525  [   96/  300]
per-ex loss: 0.465208  [   99/  300]
per-ex loss: 0.472685  [  102/  300]
per-ex loss: 0.568034  [  105/  300]
per-ex loss: 0.548112  [  108/  300]
per-ex loss: 0.479111  [  111/  300]
per-ex loss: 0.519298  [  114/  300]
per-ex loss: 0.437413  [  117/  300]
per-ex loss: 0.470609  [  120/  300]
per-ex loss: 0.476732  [  123/  300]
per-ex loss: 0.517908  [  126/  300]
per-ex loss: 0.487118  [  129/  300]
per-ex loss: 0.475573  [  132/  300]
per-ex loss: 0.656559  [  135/  300]
per-ex loss: 0.621977  [  138/  300]
per-ex loss: 0.537880  [  141/  300]
per-ex loss: 0.553075  [  144/  300]
per-ex loss: 0.518129  [  147/  300]
per-ex loss: 0.545539  [  150/  300]
per-ex loss: 0.665520  [  153/  300]
per-ex loss: 0.530410  [  156/  300]
per-ex loss: 0.447037  [  159/  300]
per-ex loss: 0.693396  [  162/  300]
per-ex loss: 0.615436  [  165/  300]
per-ex loss: 0.395320  [  168/  300]
per-ex loss: 0.555825  [  171/  300]
per-ex loss: 0.568846  [  174/  300]
per-ex loss: 0.487437  [  177/  300]
per-ex loss: 0.449738  [  180/  300]
per-ex loss: 0.423310  [  183/  300]
per-ex loss: 0.457507  [  186/  300]
per-ex loss: 0.451844  [  189/  300]
per-ex loss: 0.565036  [  192/  300]
per-ex loss: 0.706522  [  195/  300]
per-ex loss: 0.626320  [  198/  300]
per-ex loss: 0.424781  [  201/  300]
per-ex loss: 0.567451  [  204/  300]
per-ex loss: 0.498399  [  207/  300]
per-ex loss: 0.489759  [  210/  300]
per-ex loss: 0.573700  [  213/  300]
per-ex loss: 0.491085  [  216/  300]
per-ex loss: 0.634235  [  219/  300]
per-ex loss: 0.573933  [  222/  300]
per-ex loss: 0.469322  [  225/  300]
per-ex loss: 0.560434  [  228/  300]
per-ex loss: 0.653215  [  231/  300]
per-ex loss: 0.480579  [  234/  300]
per-ex loss: 0.532668  [  237/  300]
per-ex loss: 0.557218  [  240/  300]
per-ex loss: 0.605761  [  243/  300]
per-ex loss: 0.490917  [  246/  300]
per-ex loss: 0.624937  [  249/  300]
per-ex loss: 0.469303  [  252/  300]
per-ex loss: 0.399735  [  255/  300]
per-ex loss: 0.586345  [  258/  300]
per-ex loss: 0.627446  [  261/  300]
per-ex loss: 0.543604  [  264/  300]
per-ex loss: 0.474840  [  267/  300]
per-ex loss: 0.417415  [  270/  300]
per-ex loss: 0.565291  [  273/  300]
per-ex loss: 0.608194  [  276/  300]
per-ex loss: 0.474891  [  279/  300]
per-ex loss: 0.632696  [  282/  300]
per-ex loss: 0.554727  [  285/  300]
per-ex loss: 0.548084  [  288/  300]
per-ex loss: 0.522381  [  291/  300]
per-ex loss: 0.426772  [  294/  300]
per-ex loss: 0.486920  [  297/  300]
per-ex loss: 0.482908  [  300/  300]
Train Error: Avg loss: 0.53059393
validation Error: 
 Avg loss: 0.51531971 
 F1: 0.459461 
 Precision: 0.376221 
 Recall: 0.590002
 IoU: 0.298247

test Error: 
 Avg loss: 0.47825855 
 F1: 0.522945 
 Precision: 0.448184 
 Recall: 0.627641
 IoU: 0.354046

We have finished training iteration 53
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_51_.pth
per-ex loss: 0.412719  [    3/  300]
per-ex loss: 0.497362  [    6/  300]
per-ex loss: 0.577481  [    9/  300]
per-ex loss: 0.578945  [   12/  300]
per-ex loss: 0.563809  [   15/  300]
per-ex loss: 0.566612  [   18/  300]
per-ex loss: 0.519390  [   21/  300]
per-ex loss: 0.532811  [   24/  300]
per-ex loss: 0.533480  [   27/  300]
per-ex loss: 0.512925  [   30/  300]
per-ex loss: 0.530720  [   33/  300]
per-ex loss: 0.517467  [   36/  300]
per-ex loss: 0.495712  [   39/  300]
per-ex loss: 0.601679  [   42/  300]
per-ex loss: 0.537766  [   45/  300]
per-ex loss: 0.560650  [   48/  300]
per-ex loss: 0.580500  [   51/  300]
per-ex loss: 0.429633  [   54/  300]
per-ex loss: 0.529709  [   57/  300]
per-ex loss: 0.485321  [   60/  300]
per-ex loss: 0.632221  [   63/  300]
per-ex loss: 0.475098  [   66/  300]
per-ex loss: 0.539202  [   69/  300]
per-ex loss: 0.617637  [   72/  300]
per-ex loss: 0.454128  [   75/  300]
per-ex loss: 0.561208  [   78/  300]
per-ex loss: 0.472277  [   81/  300]
per-ex loss: 0.538835  [   84/  300]
per-ex loss: 0.683204  [   87/  300]
per-ex loss: 0.659056  [   90/  300]
per-ex loss: 0.432735  [   93/  300]
per-ex loss: 0.551621  [   96/  300]
per-ex loss: 0.646734  [   99/  300]
per-ex loss: 0.448836  [  102/  300]
per-ex loss: 0.478688  [  105/  300]
per-ex loss: 0.505466  [  108/  300]
per-ex loss: 0.490587  [  111/  300]
per-ex loss: 0.599112  [  114/  300]
per-ex loss: 0.690375  [  117/  300]
per-ex loss: 0.659687  [  120/  300]
per-ex loss: 0.526456  [  123/  300]
per-ex loss: 0.581660  [  126/  300]
per-ex loss: 0.443194  [  129/  300]
per-ex loss: 0.525489  [  132/  300]
per-ex loss: 0.478976  [  135/  300]
per-ex loss: 0.449439  [  138/  300]
per-ex loss: 0.534639  [  141/  300]
per-ex loss: 0.516769  [  144/  300]
per-ex loss: 0.540517  [  147/  300]
per-ex loss: 0.444870  [  150/  300]
per-ex loss: 0.610242  [  153/  300]
per-ex loss: 0.695152  [  156/  300]
per-ex loss: 0.616098  [  159/  300]
per-ex loss: 0.548380  [  162/  300]
per-ex loss: 0.570254  [  165/  300]
per-ex loss: 0.585969  [  168/  300]
per-ex loss: 0.510696  [  171/  300]
per-ex loss: 0.667393  [  174/  300]
per-ex loss: 0.585187  [  177/  300]
per-ex loss: 0.569537  [  180/  300]
per-ex loss: 0.502662  [  183/  300]
per-ex loss: 0.515241  [  186/  300]
per-ex loss: 0.527053  [  189/  300]
per-ex loss: 0.640638  [  192/  300]
per-ex loss: 0.532557  [  195/  300]
per-ex loss: 0.518269  [  198/  300]
per-ex loss: 0.448466  [  201/  300]
per-ex loss: 0.557974  [  204/  300]
per-ex loss: 0.545368  [  207/  300]
per-ex loss: 0.570813  [  210/  300]
per-ex loss: 0.430884  [  213/  300]
per-ex loss: 0.552235  [  216/  300]
per-ex loss: 0.483929  [  219/  300]
per-ex loss: 0.517277  [  222/  300]
per-ex loss: 0.660199  [  225/  300]
per-ex loss: 0.690275  [  228/  300]
per-ex loss: 0.508818  [  231/  300]
per-ex loss: 0.685286  [  234/  300]
per-ex loss: 0.480228  [  237/  300]
per-ex loss: 0.491844  [  240/  300]
per-ex loss: 0.638428  [  243/  300]
per-ex loss: 0.605299  [  246/  300]
per-ex loss: 0.557209  [  249/  300]
per-ex loss: 0.605565  [  252/  300]
per-ex loss: 0.543159  [  255/  300]
per-ex loss: 0.559098  [  258/  300]
per-ex loss: 0.616672  [  261/  300]
per-ex loss: 0.597887  [  264/  300]
per-ex loss: 0.622852  [  267/  300]
per-ex loss: 0.505628  [  270/  300]
per-ex loss: 0.466367  [  273/  300]
per-ex loss: 0.489378  [  276/  300]
per-ex loss: 0.614824  [  279/  300]
per-ex loss: 0.488706  [  282/  300]
per-ex loss: 0.679952  [  285/  300]
per-ex loss: 0.703138  [  288/  300]
per-ex loss: 0.547531  [  291/  300]
per-ex loss: 0.517413  [  294/  300]
per-ex loss: 0.585489  [  297/  300]
per-ex loss: 0.485527  [  300/  300]
Train Error: Avg loss: 0.54992456
validation Error: 
 Avg loss: 0.52083357 
 F1: 0.451859 
 Precision: 0.351460 
 Recall: 0.632558
 IoU: 0.291872

test Error: 
 Avg loss: 0.50208664 
 F1: 0.499983 
 Precision: 0.398648 
 Recall: 0.670396
 IoU: 0.333318

We have finished training iteration 54
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_52_.pth
per-ex loss: 0.499000  [    3/  300]
per-ex loss: 0.608544  [    6/  300]
per-ex loss: 0.432777  [    9/  300]
per-ex loss: 0.577062  [   12/  300]
per-ex loss: 0.595208  [   15/  300]
per-ex loss: 0.490349  [   18/  300]
per-ex loss: 0.554105  [   21/  300]
per-ex loss: 0.485963  [   24/  300]
per-ex loss: 0.653028  [   27/  300]
per-ex loss: 0.539314  [   30/  300]
per-ex loss: 0.622955  [   33/  300]
per-ex loss: 0.486637  [   36/  300]
per-ex loss: 0.647801  [   39/  300]
per-ex loss: 0.643102  [   42/  300]
per-ex loss: 0.511969  [   45/  300]
per-ex loss: 0.553205  [   48/  300]
per-ex loss: 0.660513  [   51/  300]
per-ex loss: 0.464955  [   54/  300]
per-ex loss: 0.570021  [   57/  300]
per-ex loss: 0.516847  [   60/  300]
per-ex loss: 0.454673  [   63/  300]
per-ex loss: 0.706000  [   66/  300]
per-ex loss: 0.560582  [   69/  300]
per-ex loss: 0.437385  [   72/  300]
per-ex loss: 0.540169  [   75/  300]
per-ex loss: 0.554886  [   78/  300]
per-ex loss: 0.478759  [   81/  300]
per-ex loss: 0.590621  [   84/  300]
per-ex loss: 0.588494  [   87/  300]
per-ex loss: 0.517465  [   90/  300]
per-ex loss: 0.640080  [   93/  300]
per-ex loss: 0.470248  [   96/  300]
per-ex loss: 0.492826  [   99/  300]
per-ex loss: 0.573138  [  102/  300]
per-ex loss: 0.486069  [  105/  300]
per-ex loss: 0.640914  [  108/  300]
per-ex loss: 0.611018  [  111/  300]
per-ex loss: 0.619872  [  114/  300]
per-ex loss: 0.549433  [  117/  300]
per-ex loss: 0.560856  [  120/  300]
per-ex loss: 0.608091  [  123/  300]
per-ex loss: 0.446572  [  126/  300]
per-ex loss: 0.515630  [  129/  300]
per-ex loss: 0.569803  [  132/  300]
per-ex loss: 0.550442  [  135/  300]
per-ex loss: 0.513356  [  138/  300]
per-ex loss: 0.458900  [  141/  300]
per-ex loss: 0.578276  [  144/  300]
per-ex loss: 0.477638  [  147/  300]
per-ex loss: 0.685000  [  150/  300]
per-ex loss: 0.506233  [  153/  300]
per-ex loss: 0.693609  [  156/  300]
per-ex loss: 0.590193  [  159/  300]
per-ex loss: 0.550895  [  162/  300]
per-ex loss: 0.489069  [  165/  300]
per-ex loss: 0.458782  [  168/  300]
per-ex loss: 0.482702  [  171/  300]
per-ex loss: 0.470797  [  174/  300]
per-ex loss: 0.479253  [  177/  300]
per-ex loss: 0.477423  [  180/  300]
per-ex loss: 0.584355  [  183/  300]
per-ex loss: 0.593485  [  186/  300]
per-ex loss: 0.589413  [  189/  300]
per-ex loss: 0.525706  [  192/  300]
per-ex loss: 0.515660  [  195/  300]
per-ex loss: 0.432519  [  198/  300]
per-ex loss: 0.632693  [  201/  300]
per-ex loss: 0.518971  [  204/  300]
per-ex loss: 0.501955  [  207/  300]
per-ex loss: 0.484205  [  210/  300]
per-ex loss: 0.562016  [  213/  300]
per-ex loss: 0.507932  [  216/  300]
per-ex loss: 0.661442  [  219/  300]
per-ex loss: 0.429500  [  222/  300]
per-ex loss: 0.605648  [  225/  300]
per-ex loss: 0.545666  [  228/  300]
per-ex loss: 0.585043  [  231/  300]
per-ex loss: 0.490635  [  234/  300]
per-ex loss: 0.695473  [  237/  300]
per-ex loss: 0.448019  [  240/  300]
per-ex loss: 0.510034  [  243/  300]
per-ex loss: 0.443801  [  246/  300]
per-ex loss: 0.457694  [  249/  300]
per-ex loss: 0.491693  [  252/  300]
per-ex loss: 0.599459  [  255/  300]
per-ex loss: 0.539041  [  258/  300]
per-ex loss: 0.649388  [  261/  300]
per-ex loss: 0.512723  [  264/  300]
per-ex loss: 0.442050  [  267/  300]
per-ex loss: 0.467152  [  270/  300]
per-ex loss: 0.486001  [  273/  300]
per-ex loss: 0.563965  [  276/  300]
per-ex loss: 0.430673  [  279/  300]
per-ex loss: 0.511503  [  282/  300]
per-ex loss: 0.555660  [  285/  300]
per-ex loss: 0.494563  [  288/  300]
per-ex loss: 0.488958  [  291/  300]
per-ex loss: 0.487216  [  294/  300]
per-ex loss: 0.439567  [  297/  300]
per-ex loss: 0.509492  [  300/  300]
Train Error: Avg loss: 0.53780477
validation Error: 
 Avg loss: 0.52391283 
 F1: 0.460901 
 Precision: 0.391442 
 Recall: 0.560326
 IoU: 0.299461

test Error: 
 Avg loss: 0.46723711 
 F1: 0.533791 
 Precision: 0.491931 
 Recall: 0.583439
 IoU: 0.364062

We have finished training iteration 55
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_53_.pth
per-ex loss: 0.537594  [    3/  300]
per-ex loss: 0.482016  [    6/  300]
per-ex loss: 0.546888  [    9/  300]
per-ex loss: 0.449457  [   12/  300]
per-ex loss: 0.539785  [   15/  300]
per-ex loss: 0.546702  [   18/  300]
per-ex loss: 0.457066  [   21/  300]
per-ex loss: 0.530949  [   24/  300]
per-ex loss: 0.636501  [   27/  300]
per-ex loss: 0.544681  [   30/  300]
per-ex loss: 0.645277  [   33/  300]
per-ex loss: 0.478052  [   36/  300]
per-ex loss: 0.527199  [   39/  300]
per-ex loss: 0.461105  [   42/  300]
per-ex loss: 0.497906  [   45/  300]
per-ex loss: 0.618748  [   48/  300]
per-ex loss: 0.490659  [   51/  300]
per-ex loss: 0.579046  [   54/  300]
per-ex loss: 0.503673  [   57/  300]
per-ex loss: 0.456006  [   60/  300]
per-ex loss: 0.568613  [   63/  300]
per-ex loss: 0.602708  [   66/  300]
per-ex loss: 0.493994  [   69/  300]
per-ex loss: 0.499579  [   72/  300]
per-ex loss: 0.604409  [   75/  300]
per-ex loss: 0.605842  [   78/  300]
per-ex loss: 0.493157  [   81/  300]
per-ex loss: 0.474813  [   84/  300]
per-ex loss: 0.521796  [   87/  300]
per-ex loss: 0.691158  [   90/  300]
per-ex loss: 0.537141  [   93/  300]
per-ex loss: 0.679290  [   96/  300]
per-ex loss: 0.499975  [   99/  300]
per-ex loss: 0.478903  [  102/  300]
per-ex loss: 0.562478  [  105/  300]
per-ex loss: 0.430600  [  108/  300]
per-ex loss: 0.743107  [  111/  300]
per-ex loss: 0.700095  [  114/  300]
per-ex loss: 0.450047  [  117/  300]
per-ex loss: 0.561003  [  120/  300]
per-ex loss: 0.540165  [  123/  300]
per-ex loss: 0.446312  [  126/  300]
per-ex loss: 0.439833  [  129/  300]
per-ex loss: 0.548677  [  132/  300]
per-ex loss: 0.539659  [  135/  300]
per-ex loss: 0.556290  [  138/  300]
per-ex loss: 0.503645  [  141/  300]
per-ex loss: 0.564226  [  144/  300]
per-ex loss: 0.584596  [  147/  300]
per-ex loss: 0.598654  [  150/  300]
per-ex loss: 0.456126  [  153/  300]
per-ex loss: 0.576427  [  156/  300]
per-ex loss: 0.555353  [  159/  300]
per-ex loss: 0.475042  [  162/  300]
per-ex loss: 0.468573  [  165/  300]
per-ex loss: 0.513384  [  168/  300]
per-ex loss: 0.535399  [  171/  300]
per-ex loss: 0.522426  [  174/  300]
per-ex loss: 0.568127  [  177/  300]
per-ex loss: 0.500704  [  180/  300]
per-ex loss: 0.558725  [  183/  300]
per-ex loss: 0.595284  [  186/  300]
per-ex loss: 0.486831  [  189/  300]
per-ex loss: 0.460475  [  192/  300]
per-ex loss: 0.583999  [  195/  300]
per-ex loss: 0.487844  [  198/  300]
per-ex loss: 0.498337  [  201/  300]
per-ex loss: 0.470386  [  204/  300]
per-ex loss: 0.610171  [  207/  300]
per-ex loss: 0.520260  [  210/  300]
per-ex loss: 0.449245  [  213/  300]
per-ex loss: 0.446727  [  216/  300]
per-ex loss: 0.508666  [  219/  300]
per-ex loss: 0.510345  [  222/  300]
per-ex loss: 0.621344  [  225/  300]
per-ex loss: 0.452482  [  228/  300]
per-ex loss: 0.510692  [  231/  300]
per-ex loss: 0.589051  [  234/  300]
per-ex loss: 0.652348  [  237/  300]
per-ex loss: 0.537480  [  240/  300]
per-ex loss: 0.632805  [  243/  300]
per-ex loss: 0.453442  [  246/  300]
per-ex loss: 0.495639  [  249/  300]
per-ex loss: 0.467051  [  252/  300]
per-ex loss: 0.635680  [  255/  300]
per-ex loss: 0.500830  [  258/  300]
per-ex loss: 0.510975  [  261/  300]
per-ex loss: 0.515531  [  264/  300]
per-ex loss: 0.471046  [  267/  300]
per-ex loss: 0.614621  [  270/  300]
per-ex loss: 0.469005  [  273/  300]
per-ex loss: 0.597172  [  276/  300]
per-ex loss: 0.582285  [  279/  300]
per-ex loss: 0.533291  [  282/  300]
per-ex loss: 0.522267  [  285/  300]
per-ex loss: 0.507468  [  288/  300]
per-ex loss: 0.647733  [  291/  300]
per-ex loss: 0.590673  [  294/  300]
per-ex loss: 0.484960  [  297/  300]
per-ex loss: 0.535217  [  300/  300]
Train Error: Avg loss: 0.53540018
validation Error: 
 Avg loss: 0.51693032 
 F1: 0.472562 
 Precision: 0.443874 
 Recall: 0.505215
 IoU: 0.309382

test Error: 
 Avg loss: 0.46664631 
 F1: 0.534099 
 Precision: 0.533688 
 Recall: 0.534511
 IoU: 0.364349

We have finished training iteration 56
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_54_.pth
per-ex loss: 0.582800  [    3/  300]
per-ex loss: 0.578750  [    6/  300]
per-ex loss: 0.476788  [    9/  300]
per-ex loss: 0.667106  [   12/  300]
per-ex loss: 0.520284  [   15/  300]
per-ex loss: 0.496812  [   18/  300]
per-ex loss: 0.549213  [   21/  300]
per-ex loss: 0.635565  [   24/  300]
per-ex loss: 0.482320  [   27/  300]
per-ex loss: 0.616518  [   30/  300]
per-ex loss: 0.615170  [   33/  300]
per-ex loss: 0.511472  [   36/  300]
per-ex loss: 0.478772  [   39/  300]
per-ex loss: 0.485325  [   42/  300]
per-ex loss: 0.459963  [   45/  300]
per-ex loss: 0.445042  [   48/  300]
per-ex loss: 0.426224  [   51/  300]
per-ex loss: 0.565560  [   54/  300]
per-ex loss: 0.479287  [   57/  300]
per-ex loss: 0.665741  [   60/  300]
per-ex loss: 0.468221  [   63/  300]
per-ex loss: 0.565470  [   66/  300]
per-ex loss: 0.419766  [   69/  300]
per-ex loss: 0.392812  [   72/  300]
per-ex loss: 0.622210  [   75/  300]
per-ex loss: 0.638485  [   78/  300]
per-ex loss: 0.544948  [   81/  300]
per-ex loss: 0.525378  [   84/  300]
per-ex loss: 0.626818  [   87/  300]
per-ex loss: 0.615091  [   90/  300]
per-ex loss: 0.585755  [   93/  300]
per-ex loss: 0.527217  [   96/  300]
per-ex loss: 0.568660  [   99/  300]
per-ex loss: 0.621836  [  102/  300]
per-ex loss: 0.618324  [  105/  300]
per-ex loss: 0.423791  [  108/  300]
per-ex loss: 0.544611  [  111/  300]
per-ex loss: 0.459789  [  114/  300]
per-ex loss: 0.501299  [  117/  300]
per-ex loss: 0.593079  [  120/  300]
per-ex loss: 0.524798  [  123/  300]
per-ex loss: 0.498583  [  126/  300]
per-ex loss: 0.727447  [  129/  300]
per-ex loss: 0.675401  [  132/  300]
per-ex loss: 0.510544  [  135/  300]
per-ex loss: 0.425170  [  138/  300]
per-ex loss: 0.662642  [  141/  300]
per-ex loss: 0.587885  [  144/  300]
per-ex loss: 0.539127  [  147/  300]
per-ex loss: 0.469875  [  150/  300]
per-ex loss: 0.460202  [  153/  300]
per-ex loss: 0.475462  [  156/  300]
per-ex loss: 0.442149  [  159/  300]
per-ex loss: 0.547664  [  162/  300]
per-ex loss: 0.633278  [  165/  300]
per-ex loss: 0.510517  [  168/  300]
per-ex loss: 0.462436  [  171/  300]
per-ex loss: 0.461752  [  174/  300]
per-ex loss: 0.554840  [  177/  300]
per-ex loss: 0.452986  [  180/  300]
per-ex loss: 0.680979  [  183/  300]
per-ex loss: 0.545088  [  186/  300]
per-ex loss: 0.496189  [  189/  300]
per-ex loss: 0.598311  [  192/  300]
per-ex loss: 0.478338  [  195/  300]
per-ex loss: 0.467215  [  198/  300]
per-ex loss: 0.592925  [  201/  300]
per-ex loss: 0.572228  [  204/  300]
per-ex loss: 0.565591  [  207/  300]
per-ex loss: 0.498037  [  210/  300]
per-ex loss: 0.512204  [  213/  300]
per-ex loss: 0.557598  [  216/  300]
per-ex loss: 0.537148  [  219/  300]
per-ex loss: 0.574225  [  222/  300]
per-ex loss: 0.452205  [  225/  300]
per-ex loss: 0.549688  [  228/  300]
per-ex loss: 0.452594  [  231/  300]
per-ex loss: 0.513273  [  234/  300]
per-ex loss: 0.587225  [  237/  300]
per-ex loss: 0.483059  [  240/  300]
per-ex loss: 0.491116  [  243/  300]
per-ex loss: 0.477406  [  246/  300]
per-ex loss: 0.566915  [  249/  300]
per-ex loss: 0.429589  [  252/  300]
per-ex loss: 0.509316  [  255/  300]
per-ex loss: 0.610727  [  258/  300]
per-ex loss: 0.476864  [  261/  300]
per-ex loss: 0.526777  [  264/  300]
per-ex loss: 0.645472  [  267/  300]
per-ex loss: 0.554223  [  270/  300]
per-ex loss: 0.544252  [  273/  300]
per-ex loss: 0.536323  [  276/  300]
per-ex loss: 0.723013  [  279/  300]
per-ex loss: 0.665274  [  282/  300]
per-ex loss: 0.438726  [  285/  300]
per-ex loss: 0.635457  [  288/  300]
per-ex loss: 0.467121  [  291/  300]
per-ex loss: 0.668504  [  294/  300]
per-ex loss: 0.481770  [  297/  300]
per-ex loss: 0.552007  [  300/  300]
Train Error: Avg loss: 0.53940007
validation Error: 
 Avg loss: 0.52383216 
 F1: 0.464487 
 Precision: 0.448736 
 Recall: 0.481384
 IoU: 0.302496

test Error: 
 Avg loss: 0.47363204 
 F1: 0.527166 
 Precision: 0.533383 
 Recall: 0.521093
 IoU: 0.357927

We have finished training iteration 57
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_55_.pth
per-ex loss: 0.465321  [    3/  300]
per-ex loss: 0.504827  [    6/  300]
per-ex loss: 0.498307  [    9/  300]
per-ex loss: 0.522209  [   12/  300]
per-ex loss: 0.522219  [   15/  300]
per-ex loss: 0.538927  [   18/  300]
per-ex loss: 0.458256  [   21/  300]
per-ex loss: 0.599458  [   24/  300]
per-ex loss: 0.462800  [   27/  300]
per-ex loss: 0.634628  [   30/  300]
per-ex loss: 0.483193  [   33/  300]
per-ex loss: 0.619398  [   36/  300]
per-ex loss: 0.531915  [   39/  300]
per-ex loss: 0.502342  [   42/  300]
per-ex loss: 0.503485  [   45/  300]
per-ex loss: 0.440233  [   48/  300]
per-ex loss: 0.519685  [   51/  300]
per-ex loss: 0.668972  [   54/  300]
per-ex loss: 0.575430  [   57/  300]
per-ex loss: 0.487046  [   60/  300]
per-ex loss: 0.514725  [   63/  300]
per-ex loss: 0.407786  [   66/  300]
per-ex loss: 0.541379  [   69/  300]
per-ex loss: 0.629417  [   72/  300]
per-ex loss: 0.481758  [   75/  300]
per-ex loss: 0.645006  [   78/  300]
per-ex loss: 0.648291  [   81/  300]
per-ex loss: 0.537900  [   84/  300]
per-ex loss: 0.508358  [   87/  300]
per-ex loss: 0.556914  [   90/  300]
per-ex loss: 0.598457  [   93/  300]
per-ex loss: 0.472062  [   96/  300]
per-ex loss: 0.562603  [   99/  300]
per-ex loss: 0.586244  [  102/  300]
per-ex loss: 0.472070  [  105/  300]
per-ex loss: 0.456809  [  108/  300]
per-ex loss: 0.470807  [  111/  300]
per-ex loss: 0.474361  [  114/  300]
per-ex loss: 0.453352  [  117/  300]
per-ex loss: 0.693648  [  120/  300]
per-ex loss: 0.510166  [  123/  300]
per-ex loss: 0.691435  [  126/  300]
per-ex loss: 0.537866  [  129/  300]
per-ex loss: 0.476199  [  132/  300]
per-ex loss: 0.524471  [  135/  300]
per-ex loss: 0.609223  [  138/  300]
per-ex loss: 0.565320  [  141/  300]
per-ex loss: 0.424591  [  144/  300]
per-ex loss: 0.536432  [  147/  300]
per-ex loss: 0.481366  [  150/  300]
per-ex loss: 0.531535  [  153/  300]
per-ex loss: 0.450089  [  156/  300]
per-ex loss: 0.492066  [  159/  300]
per-ex loss: 0.703050  [  162/  300]
per-ex loss: 0.539956  [  165/  300]
per-ex loss: 0.561723  [  168/  300]
per-ex loss: 0.555787  [  171/  300]
per-ex loss: 0.660469  [  174/  300]
per-ex loss: 0.554381  [  177/  300]
per-ex loss: 0.512245  [  180/  300]
per-ex loss: 0.510833  [  183/  300]
per-ex loss: 0.507239  [  186/  300]
per-ex loss: 0.662818  [  189/  300]
per-ex loss: 0.592916  [  192/  300]
per-ex loss: 0.437608  [  195/  300]
per-ex loss: 0.571484  [  198/  300]
per-ex loss: 0.502049  [  201/  300]
per-ex loss: 0.522667  [  204/  300]
per-ex loss: 0.518083  [  207/  300]
per-ex loss: 0.453548  [  210/  300]
per-ex loss: 0.499805  [  213/  300]
per-ex loss: 0.501186  [  216/  300]
per-ex loss: 0.619974  [  219/  300]
per-ex loss: 0.593115  [  222/  300]
per-ex loss: 0.456983  [  225/  300]
per-ex loss: 0.566605  [  228/  300]
per-ex loss: 0.452223  [  231/  300]
per-ex loss: 0.479653  [  234/  300]
per-ex loss: 0.640728  [  237/  300]
per-ex loss: 0.435974  [  240/  300]
per-ex loss: 0.512329  [  243/  300]
per-ex loss: 0.559244  [  246/  300]
per-ex loss: 0.615102  [  249/  300]
per-ex loss: 0.446943  [  252/  300]
per-ex loss: 0.690831  [  255/  300]
per-ex loss: 0.607358  [  258/  300]
per-ex loss: 0.599324  [  261/  300]
per-ex loss: 0.540116  [  264/  300]
per-ex loss: 0.404694  [  267/  300]
per-ex loss: 0.527049  [  270/  300]
per-ex loss: 0.620667  [  273/  300]
per-ex loss: 0.557589  [  276/  300]
per-ex loss: 0.517923  [  279/  300]
per-ex loss: 0.408618  [  282/  300]
per-ex loss: 0.494036  [  285/  300]
per-ex loss: 0.488959  [  288/  300]
per-ex loss: 0.629122  [  291/  300]
per-ex loss: 0.478697  [  294/  300]
per-ex loss: 0.507506  [  297/  300]
per-ex loss: 0.554470  [  300/  300]
Train Error: Avg loss: 0.53455037
validation Error: 
 Avg loss: 0.51293260 
 F1: 0.472618 
 Precision: 0.420681 
 Recall: 0.539184
 IoU: 0.309430

test Error: 
 Avg loss: 0.46761447 
 F1: 0.533234 
 Precision: 0.490737 
 Recall: 0.583789
 IoU: 0.363544

We have finished training iteration 58
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_56_.pth
per-ex loss: 0.481259  [    3/  300]
per-ex loss: 0.486447  [    6/  300]
per-ex loss: 0.579030  [    9/  300]
per-ex loss: 0.600346  [   12/  300]
per-ex loss: 0.700993  [   15/  300]
per-ex loss: 0.555688  [   18/  300]
per-ex loss: 0.472410  [   21/  300]
per-ex loss: 0.435259  [   24/  300]
per-ex loss: 0.506269  [   27/  300]
per-ex loss: 0.563199  [   30/  300]
per-ex loss: 0.725474  [   33/  300]
per-ex loss: 0.667580  [   36/  300]
per-ex loss: 0.532933  [   39/  300]
per-ex loss: 0.510593  [   42/  300]
per-ex loss: 0.677754  [   45/  300]
per-ex loss: 0.614526  [   48/  300]
per-ex loss: 0.616283  [   51/  300]
per-ex loss: 0.520225  [   54/  300]
per-ex loss: 0.467942  [   57/  300]
per-ex loss: 0.478787  [   60/  300]
per-ex loss: 0.460702  [   63/  300]
per-ex loss: 0.458257  [   66/  300]
per-ex loss: 0.697725  [   69/  300]
per-ex loss: 0.448658  [   72/  300]
per-ex loss: 0.593982  [   75/  300]
per-ex loss: 0.475745  [   78/  300]
per-ex loss: 0.580216  [   81/  300]
per-ex loss: 0.460947  [   84/  300]
per-ex loss: 0.509017  [   87/  300]
per-ex loss: 0.644142  [   90/  300]
per-ex loss: 0.501599  [   93/  300]
per-ex loss: 0.522603  [   96/  300]
per-ex loss: 0.459672  [   99/  300]
per-ex loss: 0.458639  [  102/  300]
per-ex loss: 0.430798  [  105/  300]
per-ex loss: 0.505764  [  108/  300]
per-ex loss: 0.549531  [  111/  300]
per-ex loss: 0.600666  [  114/  300]
per-ex loss: 0.492706  [  117/  300]
per-ex loss: 0.631106  [  120/  300]
per-ex loss: 0.463653  [  123/  300]
per-ex loss: 0.485387  [  126/  300]
per-ex loss: 0.605143  [  129/  300]
per-ex loss: 0.525263  [  132/  300]
per-ex loss: 0.562711  [  135/  300]
per-ex loss: 0.719021  [  138/  300]
per-ex loss: 0.455056  [  141/  300]
per-ex loss: 0.551412  [  144/  300]
per-ex loss: 0.609657  [  147/  300]
per-ex loss: 0.625696  [  150/  300]
per-ex loss: 0.594926  [  153/  300]
per-ex loss: 0.668482  [  156/  300]
per-ex loss: 0.458996  [  159/  300]
per-ex loss: 0.595081  [  162/  300]
per-ex loss: 0.621988  [  165/  300]
per-ex loss: 0.558207  [  168/  300]
per-ex loss: 0.499490  [  171/  300]
per-ex loss: 0.477450  [  174/  300]
per-ex loss: 0.537797  [  177/  300]
per-ex loss: 0.625939  [  180/  300]
per-ex loss: 0.610955  [  183/  300]
per-ex loss: 0.531129  [  186/  300]
per-ex loss: 0.498802  [  189/  300]
per-ex loss: 0.487613  [  192/  300]
per-ex loss: 0.462080  [  195/  300]
per-ex loss: 0.613907  [  198/  300]
per-ex loss: 0.534098  [  201/  300]
per-ex loss: 0.479782  [  204/  300]
per-ex loss: 0.464413  [  207/  300]
per-ex loss: 0.502047  [  210/  300]
per-ex loss: 0.593111  [  213/  300]
per-ex loss: 0.457245  [  216/  300]
per-ex loss: 0.551715  [  219/  300]
per-ex loss: 0.533463  [  222/  300]
per-ex loss: 0.575835  [  225/  300]
per-ex loss: 0.636525  [  228/  300]
per-ex loss: 0.478086  [  231/  300]
per-ex loss: 0.568050  [  234/  300]
per-ex loss: 0.603759  [  237/  300]
per-ex loss: 0.470112  [  240/  300]
per-ex loss: 0.542107  [  243/  300]
per-ex loss: 0.559500  [  246/  300]
per-ex loss: 0.508212  [  249/  300]
per-ex loss: 0.653297  [  252/  300]
per-ex loss: 0.614510  [  255/  300]
per-ex loss: 0.543119  [  258/  300]
per-ex loss: 0.483732  [  261/  300]
per-ex loss: 0.472917  [  264/  300]
per-ex loss: 0.503443  [  267/  300]
per-ex loss: 0.594309  [  270/  300]
per-ex loss: 0.499747  [  273/  300]
per-ex loss: 0.578911  [  276/  300]
per-ex loss: 0.438566  [  279/  300]
per-ex loss: 0.565109  [  282/  300]
per-ex loss: 0.508652  [  285/  300]
per-ex loss: 0.453425  [  288/  300]
per-ex loss: 0.584549  [  291/  300]
per-ex loss: 0.681456  [  294/  300]
per-ex loss: 0.452933  [  297/  300]
per-ex loss: 0.532212  [  300/  300]
Train Error: Avg loss: 0.54340260
validation Error: 
 Avg loss: 0.51322118 
 F1: 0.472680 
 Precision: 0.406630 
 Recall: 0.564349
 IoU: 0.309484

test Error: 
 Avg loss: 0.46704650 
 F1: 0.533890 
 Precision: 0.486531 
 Recall: 0.591462
 IoU: 0.364154

We have finished training iteration 59
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_57_.pth
per-ex loss: 0.517041  [    3/  300]
per-ex loss: 0.431626  [    6/  300]
per-ex loss: 0.527158  [    9/  300]
per-ex loss: 0.464865  [   12/  300]
per-ex loss: 0.481103  [   15/  300]
per-ex loss: 0.652587  [   18/  300]
per-ex loss: 0.690758  [   21/  300]
per-ex loss: 0.490961  [   24/  300]
per-ex loss: 0.495838  [   27/  300]
per-ex loss: 0.551020  [   30/  300]
per-ex loss: 0.598983  [   33/  300]
per-ex loss: 0.608678  [   36/  300]
per-ex loss: 0.640360  [   39/  300]
per-ex loss: 0.484683  [   42/  300]
per-ex loss: 0.583830  [   45/  300]
per-ex loss: 0.480863  [   48/  300]
per-ex loss: 0.658768  [   51/  300]
per-ex loss: 0.455860  [   54/  300]
per-ex loss: 0.494783  [   57/  300]
per-ex loss: 0.652079  [   60/  300]
per-ex loss: 0.650147  [   63/  300]
per-ex loss: 0.545345  [   66/  300]
per-ex loss: 0.617631  [   69/  300]
per-ex loss: 0.477340  [   72/  300]
per-ex loss: 0.514893  [   75/  300]
per-ex loss: 0.560370  [   78/  300]
per-ex loss: 0.488849  [   81/  300]
per-ex loss: 0.472197  [   84/  300]
per-ex loss: 0.439822  [   87/  300]
per-ex loss: 0.515022  [   90/  300]
per-ex loss: 0.458458  [   93/  300]
per-ex loss: 0.566394  [   96/  300]
per-ex loss: 0.544098  [   99/  300]
per-ex loss: 0.601704  [  102/  300]
per-ex loss: 0.533613  [  105/  300]
per-ex loss: 0.495104  [  108/  300]
per-ex loss: 0.486085  [  111/  300]
per-ex loss: 0.453100  [  114/  300]
per-ex loss: 0.503046  [  117/  300]
per-ex loss: 0.517624  [  120/  300]
per-ex loss: 0.611330  [  123/  300]
per-ex loss: 0.486524  [  126/  300]
per-ex loss: 0.408315  [  129/  300]
per-ex loss: 0.531953  [  132/  300]
per-ex loss: 0.648649  [  135/  300]
per-ex loss: 0.509823  [  138/  300]
per-ex loss: 0.499536  [  141/  300]
per-ex loss: 0.435099  [  144/  300]
per-ex loss: 0.517742  [  147/  300]
per-ex loss: 0.623393  [  150/  300]
per-ex loss: 0.545183  [  153/  300]
per-ex loss: 0.585774  [  156/  300]
per-ex loss: 0.472638  [  159/  300]
per-ex loss: 0.479765  [  162/  300]
per-ex loss: 0.527136  [  165/  300]
per-ex loss: 0.514599  [  168/  300]
per-ex loss: 0.692551  [  171/  300]
per-ex loss: 0.566012  [  174/  300]
per-ex loss: 0.557502  [  177/  300]
per-ex loss: 0.561285  [  180/  300]
per-ex loss: 0.485563  [  183/  300]
per-ex loss: 0.486150  [  186/  300]
per-ex loss: 0.720143  [  189/  300]
per-ex loss: 0.570545  [  192/  300]
per-ex loss: 0.463379  [  195/  300]
per-ex loss: 0.550653  [  198/  300]
per-ex loss: 0.459596  [  201/  300]
per-ex loss: 0.666053  [  204/  300]
per-ex loss: 0.480273  [  207/  300]
per-ex loss: 0.662273  [  210/  300]
per-ex loss: 0.471100  [  213/  300]
per-ex loss: 0.434502  [  216/  300]
per-ex loss: 0.571177  [  219/  300]
per-ex loss: 0.575642  [  222/  300]
per-ex loss: 0.503256  [  225/  300]
per-ex loss: 0.574375  [  228/  300]
per-ex loss: 0.596451  [  231/  300]
per-ex loss: 0.568523  [  234/  300]
per-ex loss: 0.529974  [  237/  300]
per-ex loss: 0.466566  [  240/  300]
per-ex loss: 0.552148  [  243/  300]
per-ex loss: 0.457988  [  246/  300]
per-ex loss: 0.496618  [  249/  300]
per-ex loss: 0.495566  [  252/  300]
per-ex loss: 0.647125  [  255/  300]
per-ex loss: 0.573074  [  258/  300]
per-ex loss: 0.652275  [  261/  300]
per-ex loss: 0.649205  [  264/  300]
per-ex loss: 0.589096  [  267/  300]
per-ex loss: 0.511780  [  270/  300]
per-ex loss: 0.561178  [  273/  300]
per-ex loss: 0.410469  [  276/  300]
per-ex loss: 0.498256  [  279/  300]
per-ex loss: 0.643590  [  282/  300]
per-ex loss: 0.513004  [  285/  300]
per-ex loss: 0.472971  [  288/  300]
per-ex loss: 0.481819  [  291/  300]
per-ex loss: 0.560875  [  294/  300]
per-ex loss: 0.627891  [  297/  300]
per-ex loss: 0.688172  [  300/  300]
Train Error: Avg loss: 0.54096790
validation Error: 
 Avg loss: 0.52248106 
 F1: 0.463604 
 Precision: 0.420630 
 Recall: 0.516357
 IoU: 0.301747

test Error: 
 Avg loss: 0.46877980 
 F1: 0.532481 
 Precision: 0.505503 
 Recall: 0.562501
 IoU: 0.362844

We have finished training iteration 60
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_58_.pth
per-ex loss: 0.522335  [    3/  300]
per-ex loss: 0.493838  [    6/  300]
per-ex loss: 0.528866  [    9/  300]
per-ex loss: 0.477138  [   12/  300]
per-ex loss: 0.597316  [   15/  300]
per-ex loss: 0.545370  [   18/  300]
per-ex loss: 0.616327  [   21/  300]
per-ex loss: 0.599573  [   24/  300]
per-ex loss: 0.490789  [   27/  300]
per-ex loss: 0.487237  [   30/  300]
per-ex loss: 0.611977  [   33/  300]
per-ex loss: 0.494157  [   36/  300]
per-ex loss: 0.512006  [   39/  300]
per-ex loss: 0.499220  [   42/  300]
per-ex loss: 0.586718  [   45/  300]
per-ex loss: 0.527871  [   48/  300]
per-ex loss: 0.436679  [   51/  300]
per-ex loss: 0.514569  [   54/  300]
per-ex loss: 0.495317  [   57/  300]
per-ex loss: 0.656707  [   60/  300]
per-ex loss: 0.495667  [   63/  300]
per-ex loss: 0.644974  [   66/  300]
per-ex loss: 0.511091  [   69/  300]
per-ex loss: 0.501211  [   72/  300]
per-ex loss: 0.526776  [   75/  300]
per-ex loss: 0.560160  [   78/  300]
per-ex loss: 0.506540  [   81/  300]
per-ex loss: 0.589078  [   84/  300]
per-ex loss: 0.501417  [   87/  300]
per-ex loss: 0.528930  [   90/  300]
per-ex loss: 0.512097  [   93/  300]
per-ex loss: 0.480689  [   96/  300]
per-ex loss: 0.560301  [   99/  300]
per-ex loss: 0.610676  [  102/  300]
per-ex loss: 0.622951  [  105/  300]
per-ex loss: 0.522006  [  108/  300]
per-ex loss: 0.562852  [  111/  300]
per-ex loss: 0.538904  [  114/  300]
per-ex loss: 0.456788  [  117/  300]
per-ex loss: 0.581280  [  120/  300]
per-ex loss: 0.512734  [  123/  300]
per-ex loss: 0.627295  [  126/  300]
per-ex loss: 0.484819  [  129/  300]
per-ex loss: 0.438828  [  132/  300]
per-ex loss: 0.436433  [  135/  300]
per-ex loss: 0.553188  [  138/  300]
per-ex loss: 0.531800  [  141/  300]
per-ex loss: 0.610444  [  144/  300]
per-ex loss: 0.491883  [  147/  300]
per-ex loss: 0.459185  [  150/  300]
per-ex loss: 0.508084  [  153/  300]
per-ex loss: 0.656016  [  156/  300]
per-ex loss: 0.544148  [  159/  300]
per-ex loss: 0.512100  [  162/  300]
per-ex loss: 0.521954  [  165/  300]
per-ex loss: 0.563086  [  168/  300]
per-ex loss: 0.441499  [  171/  300]
per-ex loss: 0.536235  [  174/  300]
per-ex loss: 0.481298  [  177/  300]
per-ex loss: 0.510560  [  180/  300]
per-ex loss: 0.498785  [  183/  300]
per-ex loss: 0.505490  [  186/  300]
per-ex loss: 0.449339  [  189/  300]
per-ex loss: 0.447307  [  192/  300]
per-ex loss: 0.553108  [  195/  300]
per-ex loss: 0.681868  [  198/  300]
per-ex loss: 0.423630  [  201/  300]
per-ex loss: 0.487746  [  204/  300]
per-ex loss: 0.515121  [  207/  300]
per-ex loss: 0.479007  [  210/  300]
per-ex loss: 0.564358  [  213/  300]
per-ex loss: 0.542502  [  216/  300]
per-ex loss: 0.487885  [  219/  300]
per-ex loss: 0.591361  [  222/  300]
per-ex loss: 0.629348  [  225/  300]
per-ex loss: 0.459653  [  228/  300]
per-ex loss: 0.409329  [  231/  300]
per-ex loss: 0.631157  [  234/  300]
per-ex loss: 0.645916  [  237/  300]
per-ex loss: 0.480386  [  240/  300]
per-ex loss: 0.644418  [  243/  300]
per-ex loss: 0.708563  [  246/  300]
per-ex loss: 0.475860  [  249/  300]
per-ex loss: 0.664595  [  252/  300]
per-ex loss: 0.535573  [  255/  300]
per-ex loss: 0.474163  [  258/  300]
per-ex loss: 0.465397  [  261/  300]
per-ex loss: 0.601142  [  264/  300]
per-ex loss: 0.568932  [  267/  300]
per-ex loss: 0.475934  [  270/  300]
per-ex loss: 0.531132  [  273/  300]
per-ex loss: 0.511939  [  276/  300]
per-ex loss: 0.597059  [  279/  300]
per-ex loss: 0.502902  [  282/  300]
per-ex loss: 0.560006  [  285/  300]
per-ex loss: 0.536215  [  288/  300]
per-ex loss: 0.554165  [  291/  300]
per-ex loss: 0.526839  [  294/  300]
per-ex loss: 0.489651  [  297/  300]
per-ex loss: 0.434417  [  300/  300]
Train Error: Avg loss: 0.53272189
validation Error: 
 Avg loss: 0.51423065 
 F1: 0.475081 
 Precision: 0.437440 
 Recall: 0.519811
 IoU: 0.311545

test Error: 
 Avg loss: 0.46286577 
 F1: 0.538103 
 Precision: 0.521357 
 Recall: 0.555961
 IoU: 0.368085

We have finished training iteration 61
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_59_.pth
per-ex loss: 0.625490  [    3/  300]
per-ex loss: 0.471981  [    6/  300]
per-ex loss: 0.485871  [    9/  300]
per-ex loss: 0.449015  [   12/  300]
per-ex loss: 0.565859  [   15/  300]
per-ex loss: 0.459458  [   18/  300]
per-ex loss: 0.596700  [   21/  300]
per-ex loss: 0.588114  [   24/  300]
per-ex loss: 0.596199  [   27/  300]
per-ex loss: 0.546721  [   30/  300]
per-ex loss: 0.458091  [   33/  300]
per-ex loss: 0.649792  [   36/  300]
per-ex loss: 0.468482  [   39/  300]
per-ex loss: 0.496599  [   42/  300]
per-ex loss: 0.601505  [   45/  300]
per-ex loss: 0.493927  [   48/  300]
per-ex loss: 0.578472  [   51/  300]
per-ex loss: 0.479448  [   54/  300]
per-ex loss: 0.491214  [   57/  300]
per-ex loss: 0.557030  [   60/  300]
per-ex loss: 0.648712  [   63/  300]
per-ex loss: 0.482256  [   66/  300]
per-ex loss: 0.438820  [   69/  300]
per-ex loss: 0.673645  [   72/  300]
per-ex loss: 0.567042  [   75/  300]
per-ex loss: 0.542572  [   78/  300]
per-ex loss: 0.485529  [   81/  300]
per-ex loss: 0.694506  [   84/  300]
per-ex loss: 0.540591  [   87/  300]
per-ex loss: 0.540048  [   90/  300]
per-ex loss: 0.411799  [   93/  300]
per-ex loss: 0.656099  [   96/  300]
per-ex loss: 0.465416  [   99/  300]
per-ex loss: 0.675978  [  102/  300]
per-ex loss: 0.524525  [  105/  300]
per-ex loss: 0.482757  [  108/  300]
per-ex loss: 0.423903  [  111/  300]
per-ex loss: 0.498247  [  114/  300]
per-ex loss: 0.482527  [  117/  300]
per-ex loss: 0.478805  [  120/  300]
per-ex loss: 0.581115  [  123/  300]
per-ex loss: 0.627761  [  126/  300]
per-ex loss: 0.544685  [  129/  300]
per-ex loss: 0.477173  [  132/  300]
per-ex loss: 0.424524  [  135/  300]
per-ex loss: 0.582699  [  138/  300]
per-ex loss: 0.480313  [  141/  300]
per-ex loss: 0.579329  [  144/  300]
per-ex loss: 0.671955  [  147/  300]
per-ex loss: 0.481827  [  150/  300]
per-ex loss: 0.521256  [  153/  300]
per-ex loss: 0.440122  [  156/  300]
per-ex loss: 0.543582  [  159/  300]
per-ex loss: 0.678124  [  162/  300]
per-ex loss: 0.582030  [  165/  300]
per-ex loss: 0.635182  [  168/  300]
per-ex loss: 0.535069  [  171/  300]
per-ex loss: 0.581048  [  174/  300]
per-ex loss: 0.563845  [  177/  300]
per-ex loss: 0.577917  [  180/  300]
per-ex loss: 0.544176  [  183/  300]
per-ex loss: 0.494398  [  186/  300]
per-ex loss: 0.488254  [  189/  300]
per-ex loss: 0.694093  [  192/  300]
per-ex loss: 0.575744  [  195/  300]
per-ex loss: 0.463472  [  198/  300]
per-ex loss: 0.441241  [  201/  300]
per-ex loss: 0.429403  [  204/  300]
per-ex loss: 0.464655  [  207/  300]
per-ex loss: 0.535270  [  210/  300]
per-ex loss: 0.559762  [  213/  300]
per-ex loss: 0.580593  [  216/  300]
per-ex loss: 0.619282  [  219/  300]
per-ex loss: 0.593493  [  222/  300]
per-ex loss: 0.465541  [  225/  300]
per-ex loss: 0.482056  [  228/  300]
per-ex loss: 0.594332  [  231/  300]
per-ex loss: 0.652126  [  234/  300]
per-ex loss: 0.503546  [  237/  300]
per-ex loss: 0.442028  [  240/  300]
per-ex loss: 0.564238  [  243/  300]
per-ex loss: 0.504428  [  246/  300]
per-ex loss: 0.640259  [  249/  300]
per-ex loss: 0.525771  [  252/  300]
per-ex loss: 0.520951  [  255/  300]
per-ex loss: 0.576069  [  258/  300]
per-ex loss: 0.543138  [  261/  300]
per-ex loss: 0.472684  [  264/  300]
per-ex loss: 0.492323  [  267/  300]
per-ex loss: 0.682799  [  270/  300]
per-ex loss: 0.530296  [  273/  300]
per-ex loss: 0.431702  [  276/  300]
per-ex loss: 0.537742  [  279/  300]
per-ex loss: 0.501807  [  282/  300]
per-ex loss: 0.564055  [  285/  300]
per-ex loss: 0.472235  [  288/  300]
per-ex loss: 0.576048  [  291/  300]
per-ex loss: 0.565027  [  294/  300]
per-ex loss: 0.525342  [  297/  300]
per-ex loss: 0.640020  [  300/  300]
Train Error: Avg loss: 0.53973700
validation Error: 
 Avg loss: 0.52703716 
 F1: 0.462883 
 Precision: 0.399711 
 Recall: 0.549771
 IoU: 0.301137

test Error: 
 Avg loss: 0.47243649 
 F1: 0.528299 
 Precision: 0.483423 
 Recall: 0.582359
 IoU: 0.358972

We have finished training iteration 62
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_60_.pth
per-ex loss: 0.441377  [    3/  300]
per-ex loss: 0.442204  [    6/  300]
per-ex loss: 0.497865  [    9/  300]
per-ex loss: 0.481926  [   12/  300]
per-ex loss: 0.432530  [   15/  300]
per-ex loss: 0.445224  [   18/  300]
per-ex loss: 0.660945  [   21/  300]
per-ex loss: 0.519180  [   24/  300]
per-ex loss: 0.533854  [   27/  300]
per-ex loss: 0.672578  [   30/  300]
per-ex loss: 0.476175  [   33/  300]
per-ex loss: 0.597532  [   36/  300]
per-ex loss: 0.624473  [   39/  300]
per-ex loss: 0.569020  [   42/  300]
per-ex loss: 0.465215  [   45/  300]
per-ex loss: 0.721741  [   48/  300]
per-ex loss: 0.578623  [   51/  300]
per-ex loss: 0.464344  [   54/  300]
per-ex loss: 0.661888  [   57/  300]
per-ex loss: 0.618536  [   60/  300]
per-ex loss: 0.453730  [   63/  300]
per-ex loss: 0.498873  [   66/  300]
per-ex loss: 0.573785  [   69/  300]
per-ex loss: 0.598895  [   72/  300]
per-ex loss: 0.512268  [   75/  300]
per-ex loss: 0.753924  [   78/  300]
per-ex loss: 0.451990  [   81/  300]
per-ex loss: 0.476519  [   84/  300]
per-ex loss: 0.582919  [   87/  300]
per-ex loss: 0.512338  [   90/  300]
per-ex loss: 0.518005  [   93/  300]
per-ex loss: 0.473390  [   96/  300]
per-ex loss: 0.452218  [   99/  300]
per-ex loss: 0.518568  [  102/  300]
per-ex loss: 0.584563  [  105/  300]
per-ex loss: 0.439529  [  108/  300]
per-ex loss: 0.649552  [  111/  300]
per-ex loss: 0.573113  [  114/  300]
per-ex loss: 0.539050  [  117/  300]
per-ex loss: 0.448586  [  120/  300]
per-ex loss: 0.432914  [  123/  300]
per-ex loss: 0.614805  [  126/  300]
per-ex loss: 0.507350  [  129/  300]
per-ex loss: 0.666919  [  132/  300]
per-ex loss: 0.629638  [  135/  300]
per-ex loss: 0.539273  [  138/  300]
per-ex loss: 0.477956  [  141/  300]
per-ex loss: 0.536213  [  144/  300]
per-ex loss: 0.592388  [  147/  300]
per-ex loss: 0.473593  [  150/  300]
per-ex loss: 0.561044  [  153/  300]
per-ex loss: 0.455986  [  156/  300]
per-ex loss: 0.612423  [  159/  300]
per-ex loss: 0.524826  [  162/  300]
per-ex loss: 0.573359  [  165/  300]
per-ex loss: 0.492632  [  168/  300]
per-ex loss: 0.448478  [  171/  300]
per-ex loss: 0.513239  [  174/  300]
per-ex loss: 0.457530  [  177/  300]
per-ex loss: 0.587691  [  180/  300]
per-ex loss: 0.427458  [  183/  300]
per-ex loss: 0.730858  [  186/  300]
per-ex loss: 0.652840  [  189/  300]
per-ex loss: 0.590791  [  192/  300]
per-ex loss: 0.587211  [  195/  300]
per-ex loss: 0.567101  [  198/  300]
per-ex loss: 0.577931  [  201/  300]
per-ex loss: 0.490667  [  204/  300]
per-ex loss: 0.540858  [  207/  300]
per-ex loss: 0.514815  [  210/  300]
per-ex loss: 0.471016  [  213/  300]
per-ex loss: 0.646430  [  216/  300]
per-ex loss: 0.503212  [  219/  300]
per-ex loss: 0.436747  [  222/  300]
per-ex loss: 0.597909  [  225/  300]
per-ex loss: 0.593723  [  228/  300]
per-ex loss: 0.509920  [  231/  300]
per-ex loss: 0.445315  [  234/  300]
per-ex loss: 0.508838  [  237/  300]
per-ex loss: 0.484771  [  240/  300]
per-ex loss: 0.480589  [  243/  300]
per-ex loss: 0.520309  [  246/  300]
per-ex loss: 0.502246  [  249/  300]
per-ex loss: 0.520564  [  252/  300]
per-ex loss: 0.397124  [  255/  300]
per-ex loss: 0.633683  [  258/  300]
per-ex loss: 0.443398  [  261/  300]
per-ex loss: 0.673300  [  264/  300]
per-ex loss: 0.640137  [  267/  300]
per-ex loss: 0.487295  [  270/  300]
per-ex loss: 0.613142  [  273/  300]
per-ex loss: 0.458780  [  276/  300]
per-ex loss: 0.511088  [  279/  300]
per-ex loss: 0.623803  [  282/  300]
per-ex loss: 0.498246  [  285/  300]
per-ex loss: 0.491054  [  288/  300]
per-ex loss: 0.416415  [  291/  300]
per-ex loss: 0.512431  [  294/  300]
per-ex loss: 0.407344  [  297/  300]
per-ex loss: 0.616919  [  300/  300]
Train Error: Avg loss: 0.53541581
validation Error: 
 Avg loss: 0.51987004 
 F1: 0.470127 
 Precision: 0.435787 
 Recall: 0.510342
 IoU: 0.307298

test Error: 
 Avg loss: 0.47101945 
 F1: 0.529799 
 Precision: 0.531197 
 Recall: 0.528408
 IoU: 0.360358

We have finished training iteration 63
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_61_.pth
per-ex loss: 0.571285  [    3/  300]
per-ex loss: 0.583344  [    6/  300]
per-ex loss: 0.677843  [    9/  300]
per-ex loss: 0.509632  [   12/  300]
per-ex loss: 0.564781  [   15/  300]
per-ex loss: 0.685081  [   18/  300]
per-ex loss: 0.422044  [   21/  300]
per-ex loss: 0.480710  [   24/  300]
per-ex loss: 0.514889  [   27/  300]
per-ex loss: 0.581199  [   30/  300]
per-ex loss: 0.598473  [   33/  300]
per-ex loss: 0.525356  [   36/  300]
per-ex loss: 0.526069  [   39/  300]
per-ex loss: 0.449301  [   42/  300]
per-ex loss: 0.612676  [   45/  300]
per-ex loss: 0.658520  [   48/  300]
per-ex loss: 0.492264  [   51/  300]
per-ex loss: 0.480516  [   54/  300]
per-ex loss: 0.553816  [   57/  300]
per-ex loss: 0.545471  [   60/  300]
per-ex loss: 0.633251  [   63/  300]
per-ex loss: 0.492977  [   66/  300]
per-ex loss: 0.527917  [   69/  300]
per-ex loss: 0.468253  [   72/  300]
per-ex loss: 0.531871  [   75/  300]
per-ex loss: 0.482561  [   78/  300]
per-ex loss: 0.370209  [   81/  300]
per-ex loss: 0.529196  [   84/  300]
per-ex loss: 0.483388  [   87/  300]
per-ex loss: 0.545063  [   90/  300]
per-ex loss: 0.649433  [   93/  300]
per-ex loss: 0.508052  [   96/  300]
per-ex loss: 0.582196  [   99/  300]
per-ex loss: 0.518040  [  102/  300]
per-ex loss: 0.413603  [  105/  300]
per-ex loss: 0.681135  [  108/  300]
per-ex loss: 0.493816  [  111/  300]
per-ex loss: 0.525544  [  114/  300]
per-ex loss: 0.625924  [  117/  300]
per-ex loss: 0.514034  [  120/  300]
per-ex loss: 0.533139  [  123/  300]
per-ex loss: 0.530964  [  126/  300]
per-ex loss: 0.475551  [  129/  300]
per-ex loss: 0.421847  [  132/  300]
per-ex loss: 0.463685  [  135/  300]
per-ex loss: 0.531872  [  138/  300]
per-ex loss: 0.476229  [  141/  300]
per-ex loss: 0.614316  [  144/  300]
per-ex loss: 0.470520  [  147/  300]
per-ex loss: 0.634407  [  150/  300]
per-ex loss: 0.501099  [  153/  300]
per-ex loss: 0.506774  [  156/  300]
per-ex loss: 0.593229  [  159/  300]
per-ex loss: 0.543673  [  162/  300]
per-ex loss: 0.542576  [  165/  300]
per-ex loss: 0.523938  [  168/  300]
per-ex loss: 0.398962  [  171/  300]
per-ex loss: 0.687561  [  174/  300]
per-ex loss: 0.502348  [  177/  300]
per-ex loss: 0.515819  [  180/  300]
per-ex loss: 0.504053  [  183/  300]
per-ex loss: 0.589573  [  186/  300]
per-ex loss: 0.466072  [  189/  300]
per-ex loss: 0.481453  [  192/  300]
per-ex loss: 0.531233  [  195/  300]
per-ex loss: 0.467124  [  198/  300]
per-ex loss: 0.492318  [  201/  300]
per-ex loss: 0.405694  [  204/  300]
per-ex loss: 0.537033  [  207/  300]
per-ex loss: 0.479155  [  210/  300]
per-ex loss: 0.648266  [  213/  300]
per-ex loss: 0.546551  [  216/  300]
per-ex loss: 0.611734  [  219/  300]
per-ex loss: 0.535058  [  222/  300]
per-ex loss: 0.554129  [  225/  300]
per-ex loss: 0.453844  [  228/  300]
per-ex loss: 0.442723  [  231/  300]
per-ex loss: 0.426658  [  234/  300]
per-ex loss: 0.691350  [  237/  300]
per-ex loss: 0.665918  [  240/  300]
per-ex loss: 0.514037  [  243/  300]
per-ex loss: 0.611193  [  246/  300]
per-ex loss: 0.618251  [  249/  300]
per-ex loss: 0.447390  [  252/  300]
per-ex loss: 0.564237  [  255/  300]
per-ex loss: 0.435293  [  258/  300]
per-ex loss: 0.634904  [  261/  300]
per-ex loss: 0.537649  [  264/  300]
per-ex loss: 0.701159  [  267/  300]
per-ex loss: 0.452925  [  270/  300]
per-ex loss: 0.509310  [  273/  300]
per-ex loss: 0.584907  [  276/  300]
per-ex loss: 0.437239  [  279/  300]
per-ex loss: 0.479848  [  282/  300]
per-ex loss: 0.613760  [  285/  300]
per-ex loss: 0.578343  [  288/  300]
per-ex loss: 0.455163  [  291/  300]
per-ex loss: 0.479129  [  294/  300]
per-ex loss: 0.497714  [  297/  300]
per-ex loss: 0.433257  [  300/  300]
Train Error: Avg loss: 0.53193893
validation Error: 
 Avg loss: 0.52061683 
 F1: 0.470921 
 Precision: 0.420120 
 Recall: 0.535697
 IoU: 0.307977

test Error: 
 Avg loss: 0.46387202 
 F1: 0.536928 
 Precision: 0.504323 
 Recall: 0.574039
 IoU: 0.366986

We have finished training iteration 64
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_62_.pth
per-ex loss: 0.491481  [    3/  300]
per-ex loss: 0.414202  [    6/  300]
per-ex loss: 0.582207  [    9/  300]
per-ex loss: 0.581769  [   12/  300]
per-ex loss: 0.664776  [   15/  300]
per-ex loss: 0.460923  [   18/  300]
per-ex loss: 0.536014  [   21/  300]
per-ex loss: 0.490595  [   24/  300]
per-ex loss: 0.512675  [   27/  300]
per-ex loss: 0.645805  [   30/  300]
per-ex loss: 0.454653  [   33/  300]
per-ex loss: 0.636055  [   36/  300]
per-ex loss: 0.514381  [   39/  300]
per-ex loss: 0.442041  [   42/  300]
per-ex loss: 0.478947  [   45/  300]
per-ex loss: 0.483094  [   48/  300]
per-ex loss: 0.557361  [   51/  300]
per-ex loss: 0.557051  [   54/  300]
per-ex loss: 0.636393  [   57/  300]
per-ex loss: 0.536097  [   60/  300]
per-ex loss: 0.395304  [   63/  300]
per-ex loss: 0.510041  [   66/  300]
per-ex loss: 0.534649  [   69/  300]
per-ex loss: 0.553747  [   72/  300]
per-ex loss: 0.590413  [   75/  300]
per-ex loss: 0.544340  [   78/  300]
per-ex loss: 0.682553  [   81/  300]
per-ex loss: 0.434463  [   84/  300]
per-ex loss: 0.606259  [   87/  300]
per-ex loss: 0.539778  [   90/  300]
per-ex loss: 0.446633  [   93/  300]
per-ex loss: 0.583787  [   96/  300]
per-ex loss: 0.631186  [   99/  300]
per-ex loss: 0.712956  [  102/  300]
per-ex loss: 0.484265  [  105/  300]
per-ex loss: 0.550140  [  108/  300]
per-ex loss: 0.437741  [  111/  300]
per-ex loss: 0.557973  [  114/  300]
per-ex loss: 0.611988  [  117/  300]
per-ex loss: 0.603630  [  120/  300]
per-ex loss: 0.632124  [  123/  300]
per-ex loss: 0.494853  [  126/  300]
per-ex loss: 0.519615  [  129/  300]
per-ex loss: 0.453041  [  132/  300]
per-ex loss: 0.595700  [  135/  300]
per-ex loss: 0.675477  [  138/  300]
per-ex loss: 0.580795  [  141/  300]
per-ex loss: 0.404321  [  144/  300]
per-ex loss: 0.401552  [  147/  300]
per-ex loss: 0.554748  [  150/  300]
per-ex loss: 0.500879  [  153/  300]
per-ex loss: 0.538508  [  156/  300]
per-ex loss: 0.472566  [  159/  300]
per-ex loss: 0.649237  [  162/  300]
per-ex loss: 0.500064  [  165/  300]
per-ex loss: 0.445767  [  168/  300]
per-ex loss: 0.455303  [  171/  300]
per-ex loss: 0.505404  [  174/  300]
per-ex loss: 0.549241  [  177/  300]
per-ex loss: 0.475076  [  180/  300]
per-ex loss: 0.565781  [  183/  300]
per-ex loss: 0.653574  [  186/  300]
per-ex loss: 0.506305  [  189/  300]
per-ex loss: 0.459101  [  192/  300]
per-ex loss: 0.526466  [  195/  300]
per-ex loss: 0.614410  [  198/  300]
per-ex loss: 0.488220  [  201/  300]
per-ex loss: 0.529518  [  204/  300]
per-ex loss: 0.591500  [  207/  300]
per-ex loss: 0.481406  [  210/  300]
per-ex loss: 0.630964  [  213/  300]
per-ex loss: 0.482599  [  216/  300]
per-ex loss: 0.545447  [  219/  300]
per-ex loss: 0.409420  [  222/  300]
per-ex loss: 0.553718  [  225/  300]
per-ex loss: 0.602758  [  228/  300]
per-ex loss: 0.520136  [  231/  300]
per-ex loss: 0.545550  [  234/  300]
per-ex loss: 0.550753  [  237/  300]
per-ex loss: 0.533310  [  240/  300]
per-ex loss: 0.541523  [  243/  300]
per-ex loss: 0.465387  [  246/  300]
per-ex loss: 0.639974  [  249/  300]
per-ex loss: 0.625821  [  252/  300]
per-ex loss: 0.431056  [  255/  300]
per-ex loss: 0.692344  [  258/  300]
per-ex loss: 0.564636  [  261/  300]
per-ex loss: 0.521360  [  264/  300]
per-ex loss: 0.563704  [  267/  300]
per-ex loss: 0.552852  [  270/  300]
per-ex loss: 0.444881  [  273/  300]
per-ex loss: 0.451760  [  276/  300]
per-ex loss: 0.556613  [  279/  300]
per-ex loss: 0.627044  [  282/  300]
per-ex loss: 0.612282  [  285/  300]
per-ex loss: 0.656822  [  288/  300]
per-ex loss: 0.530912  [  291/  300]
per-ex loss: 0.443129  [  294/  300]
per-ex loss: 0.563893  [  297/  300]
per-ex loss: 0.651413  [  300/  300]
Train Error: Avg loss: 0.54020979
validation Error: 
 Avg loss: 0.51969322 
 F1: 0.471120 
 Precision: 0.414424 
 Recall: 0.545788
 IoU: 0.308147

test Error: 
 Avg loss: 0.46050447 
 F1: 0.540436 
 Precision: 0.510792 
 Recall: 0.573733
 IoU: 0.370272

We have finished training iteration 65
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_63_.pth
per-ex loss: 0.605640  [    3/  300]
per-ex loss: 0.569065  [    6/  300]
per-ex loss: 0.429001  [    9/  300]
per-ex loss: 0.673013  [   12/  300]
per-ex loss: 0.537536  [   15/  300]
per-ex loss: 0.610264  [   18/  300]
per-ex loss: 0.427078  [   21/  300]
per-ex loss: 0.439507  [   24/  300]
per-ex loss: 0.518673  [   27/  300]
per-ex loss: 0.591944  [   30/  300]
per-ex loss: 0.569928  [   33/  300]
per-ex loss: 0.544414  [   36/  300]
per-ex loss: 0.429538  [   39/  300]
per-ex loss: 0.448079  [   42/  300]
per-ex loss: 0.591098  [   45/  300]
per-ex loss: 0.447274  [   48/  300]
per-ex loss: 0.520572  [   51/  300]
per-ex loss: 0.609545  [   54/  300]
per-ex loss: 0.649751  [   57/  300]
per-ex loss: 0.533847  [   60/  300]
per-ex loss: 0.515261  [   63/  300]
per-ex loss: 0.646168  [   66/  300]
per-ex loss: 0.424639  [   69/  300]
per-ex loss: 0.634049  [   72/  300]
per-ex loss: 0.464213  [   75/  300]
per-ex loss: 0.512748  [   78/  300]
per-ex loss: 0.524372  [   81/  300]
per-ex loss: 0.522618  [   84/  300]
per-ex loss: 0.488674  [   87/  300]
per-ex loss: 0.456876  [   90/  300]
per-ex loss: 0.467580  [   93/  300]
per-ex loss: 0.476402  [   96/  300]
per-ex loss: 0.546805  [   99/  300]
per-ex loss: 0.617933  [  102/  300]
per-ex loss: 0.514608  [  105/  300]
per-ex loss: 0.472303  [  108/  300]
per-ex loss: 0.589341  [  111/  300]
per-ex loss: 0.650069  [  114/  300]
per-ex loss: 0.536511  [  117/  300]
per-ex loss: 0.581807  [  120/  300]
per-ex loss: 0.484773  [  123/  300]
per-ex loss: 0.511528  [  126/  300]
per-ex loss: 0.482113  [  129/  300]
per-ex loss: 0.539337  [  132/  300]
per-ex loss: 0.610115  [  135/  300]
per-ex loss: 0.461490  [  138/  300]
per-ex loss: 0.564718  [  141/  300]
per-ex loss: 0.552967  [  144/  300]
per-ex loss: 0.528773  [  147/  300]
per-ex loss: 0.471548  [  150/  300]
per-ex loss: 0.539466  [  153/  300]
per-ex loss: 0.677770  [  156/  300]
per-ex loss: 0.626131  [  159/  300]
per-ex loss: 0.554852  [  162/  300]
per-ex loss: 0.533486  [  165/  300]
per-ex loss: 0.412904  [  168/  300]
per-ex loss: 0.553751  [  171/  300]
per-ex loss: 0.503089  [  174/  300]
per-ex loss: 0.462438  [  177/  300]
per-ex loss: 0.570574  [  180/  300]
per-ex loss: 0.590816  [  183/  300]
per-ex loss: 0.482572  [  186/  300]
per-ex loss: 0.502217  [  189/  300]
per-ex loss: 0.516821  [  192/  300]
per-ex loss: 0.466402  [  195/  300]
per-ex loss: 0.511794  [  198/  300]
per-ex loss: 0.487159  [  201/  300]
per-ex loss: 0.432566  [  204/  300]
per-ex loss: 0.552264  [  207/  300]
per-ex loss: 0.455772  [  210/  300]
per-ex loss: 0.616442  [  213/  300]
per-ex loss: 0.501712  [  216/  300]
per-ex loss: 0.495032  [  219/  300]
per-ex loss: 0.423095  [  222/  300]
per-ex loss: 0.549321  [  225/  300]
per-ex loss: 0.434448  [  228/  300]
per-ex loss: 0.516502  [  231/  300]
per-ex loss: 0.465225  [  234/  300]
per-ex loss: 0.523283  [  237/  300]
per-ex loss: 0.638445  [  240/  300]
per-ex loss: 0.639314  [  243/  300]
per-ex loss: 0.478133  [  246/  300]
per-ex loss: 0.534375  [  249/  300]
per-ex loss: 0.493182  [  252/  300]
per-ex loss: 0.485968  [  255/  300]
per-ex loss: 0.630878  [  258/  300]
per-ex loss: 0.560302  [  261/  300]
per-ex loss: 0.591000  [  264/  300]
per-ex loss: 0.512759  [  267/  300]
per-ex loss: 0.661752  [  270/  300]
per-ex loss: 0.522078  [  273/  300]
per-ex loss: 0.524707  [  276/  300]
per-ex loss: 0.547812  [  279/  300]
per-ex loss: 0.636097  [  282/  300]
per-ex loss: 0.426721  [  285/  300]
per-ex loss: 0.521704  [  288/  300]
per-ex loss: 0.613155  [  291/  300]
per-ex loss: 0.537924  [  294/  300]
per-ex loss: 0.421146  [  297/  300]
per-ex loss: 0.465006  [  300/  300]
Train Error: Avg loss: 0.52996470
validation Error: 
 Avg loss: 0.52963765 
 F1: 0.470270 
 Precision: 0.465115 
 Recall: 0.475542
 IoU: 0.307421

test Error: 
 Avg loss: 0.47119462 
 F1: 0.529586 
 Precision: 0.562514 
 Recall: 0.500299
 IoU: 0.360161

We have finished training iteration 66
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_64_.pth
per-ex loss: 0.685036  [    3/  300]
per-ex loss: 0.593006  [    6/  300]
per-ex loss: 0.495349  [    9/  300]
per-ex loss: 0.626711  [   12/  300]
per-ex loss: 0.597661  [   15/  300]
per-ex loss: 0.486219  [   18/  300]
per-ex loss: 0.459547  [   21/  300]
per-ex loss: 0.536095  [   24/  300]
per-ex loss: 0.474362  [   27/  300]
per-ex loss: 0.482605  [   30/  300]
per-ex loss: 0.481106  [   33/  300]
per-ex loss: 0.571425  [   36/  300]
per-ex loss: 0.496840  [   39/  300]
per-ex loss: 0.580508  [   42/  300]
per-ex loss: 0.605139  [   45/  300]
per-ex loss: 0.622018  [   48/  300]
per-ex loss: 0.535415  [   51/  300]
per-ex loss: 0.531684  [   54/  300]
per-ex loss: 0.418860  [   57/  300]
per-ex loss: 0.452611  [   60/  300]
per-ex loss: 0.579470  [   63/  300]
per-ex loss: 0.471197  [   66/  300]
per-ex loss: 0.522710  [   69/  300]
per-ex loss: 0.509312  [   72/  300]
per-ex loss: 0.531225  [   75/  300]
per-ex loss: 0.652701  [   78/  300]
per-ex loss: 0.582266  [   81/  300]
per-ex loss: 0.508774  [   84/  300]
per-ex loss: 0.449465  [   87/  300]
per-ex loss: 0.545975  [   90/  300]
per-ex loss: 0.422717  [   93/  300]
per-ex loss: 0.539494  [   96/  300]
per-ex loss: 0.666104  [   99/  300]
per-ex loss: 0.598430  [  102/  300]
per-ex loss: 0.597969  [  105/  300]
per-ex loss: 0.515629  [  108/  300]
per-ex loss: 0.582312  [  111/  300]
per-ex loss: 0.502675  [  114/  300]
per-ex loss: 0.601963  [  117/  300]
per-ex loss: 0.500238  [  120/  300]
per-ex loss: 0.530404  [  123/  300]
per-ex loss: 0.492246  [  126/  300]
per-ex loss: 0.523141  [  129/  300]
per-ex loss: 0.431100  [  132/  300]
per-ex loss: 0.522888  [  135/  300]
per-ex loss: 0.502281  [  138/  300]
per-ex loss: 0.502409  [  141/  300]
per-ex loss: 0.482249  [  144/  300]
per-ex loss: 0.641590  [  147/  300]
per-ex loss: 0.605689  [  150/  300]
per-ex loss: 0.410872  [  153/  300]
per-ex loss: 0.515123  [  156/  300]
per-ex loss: 0.452439  [  159/  300]
per-ex loss: 0.562226  [  162/  300]
per-ex loss: 0.496333  [  165/  300]
per-ex loss: 0.523575  [  168/  300]
per-ex loss: 0.428989  [  171/  300]
per-ex loss: 0.558170  [  174/  300]
per-ex loss: 0.530069  [  177/  300]
per-ex loss: 0.552780  [  180/  300]
per-ex loss: 0.463057  [  183/  300]
per-ex loss: 0.548340  [  186/  300]
per-ex loss: 0.469170  [  189/  300]
per-ex loss: 0.681746  [  192/  300]
per-ex loss: 0.640630  [  195/  300]
per-ex loss: 0.493849  [  198/  300]
per-ex loss: 0.475925  [  201/  300]
per-ex loss: 0.452495  [  204/  300]
per-ex loss: 0.431741  [  207/  300]
per-ex loss: 0.503436  [  210/  300]
per-ex loss: 0.530893  [  213/  300]
per-ex loss: 0.415958  [  216/  300]
per-ex loss: 0.485611  [  219/  300]
per-ex loss: 0.661299  [  222/  300]
per-ex loss: 0.606341  [  225/  300]
per-ex loss: 0.608152  [  228/  300]
per-ex loss: 0.488127  [  231/  300]
per-ex loss: 0.548677  [  234/  300]
per-ex loss: 0.470229  [  237/  300]
per-ex loss: 0.659402  [  240/  300]
per-ex loss: 0.525130  [  243/  300]
per-ex loss: 0.551784  [  246/  300]
per-ex loss: 0.537609  [  249/  300]
per-ex loss: 0.602482  [  252/  300]
per-ex loss: 0.528141  [  255/  300]
per-ex loss: 0.508177  [  258/  300]
per-ex loss: 0.481365  [  261/  300]
per-ex loss: 0.545678  [  264/  300]
per-ex loss: 0.519678  [  267/  300]
per-ex loss: 0.631442  [  270/  300]
per-ex loss: 0.510999  [  273/  300]
per-ex loss: 0.475485  [  276/  300]
per-ex loss: 0.639937  [  279/  300]
per-ex loss: 0.469120  [  282/  300]
per-ex loss: 0.650732  [  285/  300]
per-ex loss: 0.599191  [  288/  300]
per-ex loss: 0.497431  [  291/  300]
per-ex loss: 0.553838  [  294/  300]
per-ex loss: 0.412542  [  297/  300]
per-ex loss: 0.512648  [  300/  300]
Train Error: Avg loss: 0.53265885
validation Error: 
 Avg loss: 0.51815635 
 F1: 0.448993 
 Precision: 0.339067 
 Recall: 0.664390
 IoU: 0.289485

test Error: 
 Avg loss: 0.49083078 
 F1: 0.510281 
 Precision: 0.399575 
 Recall: 0.705841
 IoU: 0.342535

We have finished training iteration 67
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_65_.pth
per-ex loss: 0.428621  [    3/  300]
per-ex loss: 0.499912  [    6/  300]
per-ex loss: 0.503409  [    9/  300]
per-ex loss: 0.547329  [   12/  300]
per-ex loss: 0.567580  [   15/  300]
per-ex loss: 0.528042  [   18/  300]
per-ex loss: 0.625616  [   21/  300]
per-ex loss: 0.559491  [   24/  300]
per-ex loss: 0.496351  [   27/  300]
per-ex loss: 0.685745  [   30/  300]
per-ex loss: 0.618948  [   33/  300]
per-ex loss: 0.585355  [   36/  300]
per-ex loss: 0.469368  [   39/  300]
per-ex loss: 0.550998  [   42/  300]
per-ex loss: 0.639288  [   45/  300]
per-ex loss: 0.659840  [   48/  300]
per-ex loss: 0.475460  [   51/  300]
per-ex loss: 0.442814  [   54/  300]
per-ex loss: 0.556613  [   57/  300]
per-ex loss: 0.469061  [   60/  300]
per-ex loss: 0.560848  [   63/  300]
per-ex loss: 0.469096  [   66/  300]
per-ex loss: 0.503282  [   69/  300]
per-ex loss: 0.534565  [   72/  300]
per-ex loss: 0.551464  [   75/  300]
per-ex loss: 0.537382  [   78/  300]
per-ex loss: 0.405353  [   81/  300]
per-ex loss: 0.427900  [   84/  300]
per-ex loss: 0.439609  [   87/  300]
per-ex loss: 0.482715  [   90/  300]
per-ex loss: 0.496790  [   93/  300]
per-ex loss: 0.639222  [   96/  300]
per-ex loss: 0.419850  [   99/  300]
per-ex loss: 0.594396  [  102/  300]
per-ex loss: 0.675081  [  105/  300]
per-ex loss: 0.472732  [  108/  300]
per-ex loss: 0.497015  [  111/  300]
per-ex loss: 0.511583  [  114/  300]
per-ex loss: 0.619112  [  117/  300]
per-ex loss: 0.500891  [  120/  300]
per-ex loss: 0.445084  [  123/  300]
per-ex loss: 0.518277  [  126/  300]
per-ex loss: 0.556849  [  129/  300]
per-ex loss: 0.499325  [  132/  300]
per-ex loss: 0.486852  [  135/  300]
per-ex loss: 0.604471  [  138/  300]
per-ex loss: 0.511278  [  141/  300]
per-ex loss: 0.538827  [  144/  300]
per-ex loss: 0.466891  [  147/  300]
per-ex loss: 0.488299  [  150/  300]
per-ex loss: 0.518496  [  153/  300]
per-ex loss: 0.481443  [  156/  300]
per-ex loss: 0.457575  [  159/  300]
per-ex loss: 0.636383  [  162/  300]
per-ex loss: 0.471655  [  165/  300]
per-ex loss: 0.524275  [  168/  300]
per-ex loss: 0.457771  [  171/  300]
per-ex loss: 0.554930  [  174/  300]
per-ex loss: 0.541834  [  177/  300]
per-ex loss: 0.553269  [  180/  300]
per-ex loss: 0.563083  [  183/  300]
per-ex loss: 0.620731  [  186/  300]
per-ex loss: 0.477788  [  189/  300]
per-ex loss: 0.492819  [  192/  300]
per-ex loss: 0.440583  [  195/  300]
per-ex loss: 0.519902  [  198/  300]
per-ex loss: 0.564053  [  201/  300]
per-ex loss: 0.588987  [  204/  300]
per-ex loss: 0.523090  [  207/  300]
per-ex loss: 0.518427  [  210/  300]
per-ex loss: 0.483329  [  213/  300]
per-ex loss: 0.409409  [  216/  300]
per-ex loss: 0.652083  [  219/  300]
per-ex loss: 0.564464  [  222/  300]
per-ex loss: 0.442247  [  225/  300]
per-ex loss: 0.545642  [  228/  300]
per-ex loss: 0.468323  [  231/  300]
per-ex loss: 0.582792  [  234/  300]
per-ex loss: 0.514152  [  237/  300]
per-ex loss: 0.501953  [  240/  300]
per-ex loss: 0.474219  [  243/  300]
per-ex loss: 0.611534  [  246/  300]
per-ex loss: 0.632346  [  249/  300]
per-ex loss: 0.584874  [  252/  300]
per-ex loss: 0.498268  [  255/  300]
per-ex loss: 0.511471  [  258/  300]
per-ex loss: 0.572208  [  261/  300]
per-ex loss: 0.469520  [  264/  300]
per-ex loss: 0.501548  [  267/  300]
per-ex loss: 0.515871  [  270/  300]
per-ex loss: 0.443792  [  273/  300]
per-ex loss: 0.447973  [  276/  300]
per-ex loss: 0.538416  [  279/  300]
per-ex loss: 0.407732  [  282/  300]
per-ex loss: 0.621833  [  285/  300]
per-ex loss: 0.534097  [  288/  300]
per-ex loss: 0.510669  [  291/  300]
per-ex loss: 0.523577  [  294/  300]
per-ex loss: 0.527491  [  297/  300]
per-ex loss: 0.578046  [  300/  300]
Train Error: Avg loss: 0.52541853
validation Error: 
 Avg loss: 0.50778403 
 F1: 0.458321 
 Precision: 0.353604 
 Recall: 0.651153
 IoU: 0.297287

test Error: 
 Avg loss: 0.47492462 
 F1: 0.526377 
 Precision: 0.425565 
 Recall: 0.689778
 IoU: 0.357199

We have finished training iteration 68
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_66_.pth
per-ex loss: 0.515896  [    3/  300]
per-ex loss: 0.560989  [    6/  300]
per-ex loss: 0.511492  [    9/  300]
per-ex loss: 0.514245  [   12/  300]
per-ex loss: 0.686259  [   15/  300]
per-ex loss: 0.546390  [   18/  300]
per-ex loss: 0.508944  [   21/  300]
per-ex loss: 0.475003  [   24/  300]
per-ex loss: 0.629163  [   27/  300]
per-ex loss: 0.497168  [   30/  300]
per-ex loss: 0.575286  [   33/  300]
per-ex loss: 0.536423  [   36/  300]
per-ex loss: 0.545138  [   39/  300]
per-ex loss: 0.513023  [   42/  300]
per-ex loss: 0.455782  [   45/  300]
per-ex loss: 0.551319  [   48/  300]
per-ex loss: 0.533038  [   51/  300]
per-ex loss: 0.464297  [   54/  300]
per-ex loss: 0.503988  [   57/  300]
per-ex loss: 0.482787  [   60/  300]
per-ex loss: 0.472399  [   63/  300]
per-ex loss: 0.544915  [   66/  300]
per-ex loss: 0.516002  [   69/  300]
per-ex loss: 0.638227  [   72/  300]
per-ex loss: 0.523272  [   75/  300]
per-ex loss: 0.600852  [   78/  300]
per-ex loss: 0.543808  [   81/  300]
per-ex loss: 0.519141  [   84/  300]
per-ex loss: 0.572812  [   87/  300]
per-ex loss: 0.392641  [   90/  300]
per-ex loss: 0.485559  [   93/  300]
per-ex loss: 0.572433  [   96/  300]
per-ex loss: 0.518090  [   99/  300]
per-ex loss: 0.590953  [  102/  300]
per-ex loss: 0.436298  [  105/  300]
per-ex loss: 0.580337  [  108/  300]
per-ex loss: 0.482575  [  111/  300]
per-ex loss: 0.511919  [  114/  300]
per-ex loss: 0.518066  [  117/  300]
per-ex loss: 0.496695  [  120/  300]
per-ex loss: 0.462481  [  123/  300]
per-ex loss: 0.533462  [  126/  300]
per-ex loss: 0.539426  [  129/  300]
per-ex loss: 0.584737  [  132/  300]
per-ex loss: 0.472467  [  135/  300]
per-ex loss: 0.590636  [  138/  300]
per-ex loss: 0.454694  [  141/  300]
per-ex loss: 0.457722  [  144/  300]
per-ex loss: 0.523470  [  147/  300]
per-ex loss: 0.660540  [  150/  300]
per-ex loss: 0.662655  [  153/  300]
per-ex loss: 0.589640  [  156/  300]
per-ex loss: 0.632790  [  159/  300]
per-ex loss: 0.581663  [  162/  300]
per-ex loss: 0.516745  [  165/  300]
per-ex loss: 0.443209  [  168/  300]
per-ex loss: 0.463977  [  171/  300]
per-ex loss: 0.533521  [  174/  300]
per-ex loss: 0.550082  [  177/  300]
per-ex loss: 0.623414  [  180/  300]
per-ex loss: 0.569498  [  183/  300]
per-ex loss: 0.578741  [  186/  300]
per-ex loss: 0.476239  [  189/  300]
per-ex loss: 0.704330  [  192/  300]
per-ex loss: 0.433110  [  195/  300]
per-ex loss: 0.496766  [  198/  300]
per-ex loss: 0.621797  [  201/  300]
per-ex loss: 0.625407  [  204/  300]
per-ex loss: 0.557742  [  207/  300]
per-ex loss: 0.523696  [  210/  300]
per-ex loss: 0.647342  [  213/  300]
per-ex loss: 0.525695  [  216/  300]
per-ex loss: 0.455491  [  219/  300]
per-ex loss: 0.597417  [  222/  300]
per-ex loss: 0.705206  [  225/  300]
per-ex loss: 0.552590  [  228/  300]
per-ex loss: 0.504681  [  231/  300]
per-ex loss: 0.488597  [  234/  300]
per-ex loss: 0.555982  [  237/  300]
per-ex loss: 0.532410  [  240/  300]
per-ex loss: 0.633391  [  243/  300]
per-ex loss: 0.528881  [  246/  300]
per-ex loss: 0.526130  [  249/  300]
per-ex loss: 0.522696  [  252/  300]
per-ex loss: 0.440028  [  255/  300]
per-ex loss: 0.513482  [  258/  300]
per-ex loss: 0.460191  [  261/  300]
per-ex loss: 0.464048  [  264/  300]
per-ex loss: 0.460574  [  267/  300]
per-ex loss: 0.522023  [  270/  300]
per-ex loss: 0.522935  [  273/  300]
per-ex loss: 0.508979  [  276/  300]
per-ex loss: 0.448358  [  279/  300]
per-ex loss: 0.496667  [  282/  300]
per-ex loss: 0.507982  [  285/  300]
per-ex loss: 0.448050  [  288/  300]
per-ex loss: 0.595163  [  291/  300]
per-ex loss: 0.473202  [  294/  300]
per-ex loss: 0.576881  [  297/  300]
per-ex loss: 0.477072  [  300/  300]
Train Error: Avg loss: 0.53280430
validation Error: 
 Avg loss: 0.51358382 
 F1: 0.466734 
 Precision: 0.390617 
 Recall: 0.579698
 IoU: 0.304405

test Error: 
 Avg loss: 0.46200633 
 F1: 0.539068 
 Precision: 0.478389 
 Recall: 0.617376
 IoU: 0.368989

We have finished training iteration 69
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_67_.pth
per-ex loss: 0.438095  [    3/  300]
per-ex loss: 0.431027  [    6/  300]
per-ex loss: 0.529054  [    9/  300]
per-ex loss: 0.412196  [   12/  300]
per-ex loss: 0.598508  [   15/  300]
per-ex loss: 0.568033  [   18/  300]
per-ex loss: 0.449034  [   21/  300]
per-ex loss: 0.619327  [   24/  300]
per-ex loss: 0.591200  [   27/  300]
per-ex loss: 0.591619  [   30/  300]
per-ex loss: 0.571601  [   33/  300]
per-ex loss: 0.521558  [   36/  300]
per-ex loss: 0.592310  [   39/  300]
per-ex loss: 0.498036  [   42/  300]
per-ex loss: 0.593253  [   45/  300]
per-ex loss: 0.647751  [   48/  300]
per-ex loss: 0.688179  [   51/  300]
per-ex loss: 0.495270  [   54/  300]
per-ex loss: 0.501818  [   57/  300]
per-ex loss: 0.540081  [   60/  300]
per-ex loss: 0.483172  [   63/  300]
per-ex loss: 0.538015  [   66/  300]
per-ex loss: 0.574383  [   69/  300]
per-ex loss: 0.488298  [   72/  300]
per-ex loss: 0.475031  [   75/  300]
per-ex loss: 0.471551  [   78/  300]
per-ex loss: 0.436475  [   81/  300]
per-ex loss: 0.490505  [   84/  300]
per-ex loss: 0.463259  [   87/  300]
per-ex loss: 0.461549  [   90/  300]
per-ex loss: 0.448261  [   93/  300]
per-ex loss: 0.557828  [   96/  300]
per-ex loss: 0.579554  [   99/  300]
per-ex loss: 0.432433  [  102/  300]
per-ex loss: 0.481217  [  105/  300]
per-ex loss: 0.503084  [  108/  300]
per-ex loss: 0.647336  [  111/  300]
per-ex loss: 0.563949  [  114/  300]
per-ex loss: 0.471149  [  117/  300]
per-ex loss: 0.532406  [  120/  300]
per-ex loss: 0.618815  [  123/  300]
per-ex loss: 0.472113  [  126/  300]
per-ex loss: 0.559596  [  129/  300]
per-ex loss: 0.559886  [  132/  300]
per-ex loss: 0.509154  [  135/  300]
per-ex loss: 0.674839  [  138/  300]
per-ex loss: 0.491436  [  141/  300]
per-ex loss: 0.518799  [  144/  300]
per-ex loss: 0.459867  [  147/  300]
per-ex loss: 0.556337  [  150/  300]
per-ex loss: 0.449656  [  153/  300]
per-ex loss: 0.554211  [  156/  300]
per-ex loss: 0.596122  [  159/  300]
per-ex loss: 0.589507  [  162/  300]
per-ex loss: 0.584073  [  165/  300]
per-ex loss: 0.469345  [  168/  300]
per-ex loss: 0.531046  [  171/  300]
per-ex loss: 0.600421  [  174/  300]
per-ex loss: 0.508134  [  177/  300]
per-ex loss: 0.499303  [  180/  300]
per-ex loss: 0.509387  [  183/  300]
per-ex loss: 0.602944  [  186/  300]
per-ex loss: 0.581452  [  189/  300]
per-ex loss: 0.501969  [  192/  300]
per-ex loss: 0.621831  [  195/  300]
per-ex loss: 0.490914  [  198/  300]
per-ex loss: 0.543593  [  201/  300]
per-ex loss: 0.454840  [  204/  300]
per-ex loss: 0.509896  [  207/  300]
per-ex loss: 0.524630  [  210/  300]
per-ex loss: 0.466217  [  213/  300]
per-ex loss: 0.504429  [  216/  300]
per-ex loss: 0.582927  [  219/  300]
per-ex loss: 0.462785  [  222/  300]
per-ex loss: 0.505962  [  225/  300]
per-ex loss: 0.638091  [  228/  300]
per-ex loss: 0.438302  [  231/  300]
per-ex loss: 0.474913  [  234/  300]
per-ex loss: 0.668154  [  237/  300]
per-ex loss: 0.465896  [  240/  300]
per-ex loss: 0.488626  [  243/  300]
per-ex loss: 0.507226  [  246/  300]
per-ex loss: 0.577185  [  249/  300]
per-ex loss: 0.481233  [  252/  300]
per-ex loss: 0.692889  [  255/  300]
per-ex loss: 0.479136  [  258/  300]
per-ex loss: 0.675052  [  261/  300]
per-ex loss: 0.527078  [  264/  300]
per-ex loss: 0.525685  [  267/  300]
per-ex loss: 0.413304  [  270/  300]
per-ex loss: 0.601797  [  273/  300]
per-ex loss: 0.498006  [  276/  300]
per-ex loss: 0.540734  [  279/  300]
per-ex loss: 0.484443  [  282/  300]
per-ex loss: 0.485446  [  285/  300]
per-ex loss: 0.472610  [  288/  300]
per-ex loss: 0.487093  [  291/  300]
per-ex loss: 0.421565  [  294/  300]
per-ex loss: 0.620311  [  297/  300]
per-ex loss: 0.418466  [  300/  300]
Train Error: Avg loss: 0.52727081
validation Error: 
 Avg loss: 0.51406068 
 F1: 0.474338 
 Precision: 0.414188 
 Recall: 0.554927
 IoU: 0.310906

test Error: 
 Avg loss: 0.45810461 
 F1: 0.542883 
 Precision: 0.504432 
 Recall: 0.587680
 IoU: 0.372574

We have finished training iteration 70
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_49_.pth
per-ex loss: 0.572975  [    3/  300]
per-ex loss: 0.372012  [    6/  300]
per-ex loss: 0.518609  [    9/  300]
per-ex loss: 0.528062  [   12/  300]
per-ex loss: 0.499666  [   15/  300]
per-ex loss: 0.498669  [   18/  300]
per-ex loss: 0.574382  [   21/  300]
per-ex loss: 0.621873  [   24/  300]
per-ex loss: 0.590163  [   27/  300]
per-ex loss: 0.550191  [   30/  300]
per-ex loss: 0.487221  [   33/  300]
per-ex loss: 0.480178  [   36/  300]
per-ex loss: 0.596522  [   39/  300]
per-ex loss: 0.584710  [   42/  300]
per-ex loss: 0.709298  [   45/  300]
per-ex loss: 0.507634  [   48/  300]
per-ex loss: 0.551906  [   51/  300]
per-ex loss: 0.548272  [   54/  300]
per-ex loss: 0.549435  [   57/  300]
per-ex loss: 0.602706  [   60/  300]
per-ex loss: 0.437380  [   63/  300]
per-ex loss: 0.522019  [   66/  300]
per-ex loss: 0.454956  [   69/  300]
per-ex loss: 0.576228  [   72/  300]
per-ex loss: 0.513354  [   75/  300]
per-ex loss: 0.437834  [   78/  300]
per-ex loss: 0.652860  [   81/  300]
per-ex loss: 0.468981  [   84/  300]
per-ex loss: 0.579802  [   87/  300]
per-ex loss: 0.440351  [   90/  300]
per-ex loss: 0.506044  [   93/  300]
per-ex loss: 0.580798  [   96/  300]
per-ex loss: 0.417381  [   99/  300]
per-ex loss: 0.686002  [  102/  300]
per-ex loss: 0.625596  [  105/  300]
per-ex loss: 0.605083  [  108/  300]
per-ex loss: 0.560996  [  111/  300]
per-ex loss: 0.693202  [  114/  300]
per-ex loss: 0.443826  [  117/  300]
per-ex loss: 0.585559  [  120/  300]
per-ex loss: 0.510643  [  123/  300]
per-ex loss: 0.471625  [  126/  300]
per-ex loss: 0.543457  [  129/  300]
per-ex loss: 0.537643  [  132/  300]
per-ex loss: 0.559399  [  135/  300]
per-ex loss: 0.580833  [  138/  300]
per-ex loss: 0.527576  [  141/  300]
per-ex loss: 0.527422  [  144/  300]
per-ex loss: 0.511043  [  147/  300]
per-ex loss: 0.523216  [  150/  300]
per-ex loss: 0.455977  [  153/  300]
per-ex loss: 0.579452  [  156/  300]
per-ex loss: 0.565200  [  159/  300]
per-ex loss: 0.461295  [  162/  300]
per-ex loss: 0.558861  [  165/  300]
per-ex loss: 0.574100  [  168/  300]
per-ex loss: 0.465230  [  171/  300]
per-ex loss: 0.482890  [  174/  300]
per-ex loss: 0.466141  [  177/  300]
per-ex loss: 0.645678  [  180/  300]
per-ex loss: 0.659266  [  183/  300]
per-ex loss: 0.623350  [  186/  300]
per-ex loss: 0.520408  [  189/  300]
per-ex loss: 0.411301  [  192/  300]
per-ex loss: 0.643250  [  195/  300]
per-ex loss: 0.496152  [  198/  300]
per-ex loss: 0.392225  [  201/  300]
per-ex loss: 0.460919  [  204/  300]
per-ex loss: 0.447287  [  207/  300]
per-ex loss: 0.451168  [  210/  300]
per-ex loss: 0.491195  [  213/  300]
per-ex loss: 0.597034  [  216/  300]
per-ex loss: 0.598839  [  219/  300]
per-ex loss: 0.460823  [  222/  300]
per-ex loss: 0.455290  [  225/  300]
per-ex loss: 0.503939  [  228/  300]
per-ex loss: 0.436946  [  231/  300]
per-ex loss: 0.556397  [  234/  300]
per-ex loss: 0.668432  [  237/  300]
per-ex loss: 0.512336  [  240/  300]
per-ex loss: 0.499107  [  243/  300]
per-ex loss: 0.599629  [  246/  300]
per-ex loss: 0.577444  [  249/  300]
per-ex loss: 0.595623  [  252/  300]
per-ex loss: 0.611493  [  255/  300]
per-ex loss: 0.560064  [  258/  300]
per-ex loss: 0.454327  [  261/  300]
per-ex loss: 0.601852  [  264/  300]
per-ex loss: 0.540779  [  267/  300]
per-ex loss: 0.563287  [  270/  300]
per-ex loss: 0.465690  [  273/  300]
per-ex loss: 0.508503  [  276/  300]
per-ex loss: 0.654137  [  279/  300]
per-ex loss: 0.434257  [  282/  300]
per-ex loss: 0.551314  [  285/  300]
per-ex loss: 0.537560  [  288/  300]
per-ex loss: 0.586473  [  291/  300]
per-ex loss: 0.422617  [  294/  300]
per-ex loss: 0.536578  [  297/  300]
per-ex loss: 0.531960  [  300/  300]
Train Error: Avg loss: 0.53499668
validation Error: 
 Avg loss: 0.50841635 
 F1: 0.469815 
 Precision: 0.391355 
 Recall: 0.587622
 IoU: 0.307031

test Error: 
 Avg loss: 0.47337955 
 F1: 0.527593 
 Precision: 0.467351 
 Recall: 0.605663
 IoU: 0.358320

We have finished training iteration 71
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_69_.pth
per-ex loss: 0.664417  [    3/  300]
per-ex loss: 0.672550  [    6/  300]
per-ex loss: 0.538875  [    9/  300]
per-ex loss: 0.564301  [   12/  300]
per-ex loss: 0.484993  [   15/  300]
per-ex loss: 0.583396  [   18/  300]
per-ex loss: 0.521958  [   21/  300]
per-ex loss: 0.539274  [   24/  300]
per-ex loss: 0.519983  [   27/  300]
per-ex loss: 0.491887  [   30/  300]
per-ex loss: 0.540548  [   33/  300]
per-ex loss: 0.428605  [   36/  300]
per-ex loss: 0.432560  [   39/  300]
per-ex loss: 0.580184  [   42/  300]
per-ex loss: 0.559058  [   45/  300]
per-ex loss: 0.506681  [   48/  300]
per-ex loss: 0.671492  [   51/  300]
per-ex loss: 0.547591  [   54/  300]
per-ex loss: 0.562579  [   57/  300]
per-ex loss: 0.480312  [   60/  300]
per-ex loss: 0.418615  [   63/  300]
per-ex loss: 0.450018  [   66/  300]
per-ex loss: 0.530905  [   69/  300]
per-ex loss: 0.721560  [   72/  300]
per-ex loss: 0.602963  [   75/  300]
per-ex loss: 0.495439  [   78/  300]
per-ex loss: 0.509072  [   81/  300]
per-ex loss: 0.469942  [   84/  300]
per-ex loss: 0.450328  [   87/  300]
per-ex loss: 0.614666  [   90/  300]
per-ex loss: 0.464130  [   93/  300]
per-ex loss: 0.615919  [   96/  300]
per-ex loss: 0.457672  [   99/  300]
per-ex loss: 0.605615  [  102/  300]
per-ex loss: 0.593549  [  105/  300]
per-ex loss: 0.750078  [  108/  300]
per-ex loss: 0.436713  [  111/  300]
per-ex loss: 0.599332  [  114/  300]
per-ex loss: 0.589206  [  117/  300]
per-ex loss: 0.530978  [  120/  300]
per-ex loss: 0.533435  [  123/  300]
per-ex loss: 0.455120  [  126/  300]
per-ex loss: 0.592996  [  129/  300]
per-ex loss: 0.589333  [  132/  300]
per-ex loss: 0.502807  [  135/  300]
per-ex loss: 0.532508  [  138/  300]
per-ex loss: 0.525567  [  141/  300]
per-ex loss: 0.451930  [  144/  300]
per-ex loss: 0.595602  [  147/  300]
per-ex loss: 0.457425  [  150/  300]
per-ex loss: 0.504005  [  153/  300]
per-ex loss: 0.502337  [  156/  300]
per-ex loss: 0.634711  [  159/  300]
per-ex loss: 0.472626  [  162/  300]
per-ex loss: 0.544667  [  165/  300]
per-ex loss: 0.436684  [  168/  300]
per-ex loss: 0.658548  [  171/  300]
per-ex loss: 0.526720  [  174/  300]
per-ex loss: 0.553224  [  177/  300]
per-ex loss: 0.530929  [  180/  300]
per-ex loss: 0.589044  [  183/  300]
per-ex loss: 0.508827  [  186/  300]
per-ex loss: 0.481269  [  189/  300]
per-ex loss: 0.626659  [  192/  300]
per-ex loss: 0.629968  [  195/  300]
per-ex loss: 0.519155  [  198/  300]
per-ex loss: 0.569112  [  201/  300]
per-ex loss: 0.478491  [  204/  300]
per-ex loss: 0.508003  [  207/  300]
per-ex loss: 0.672587  [  210/  300]
per-ex loss: 0.566478  [  213/  300]
per-ex loss: 0.443179  [  216/  300]
per-ex loss: 0.486818  [  219/  300]
per-ex loss: 0.527608  [  222/  300]
per-ex loss: 0.437230  [  225/  300]
per-ex loss: 0.576412  [  228/  300]
per-ex loss: 0.557310  [  231/  300]
per-ex loss: 0.525385  [  234/  300]
per-ex loss: 0.449954  [  237/  300]
per-ex loss: 0.524283  [  240/  300]
per-ex loss: 0.621341  [  243/  300]
per-ex loss: 0.455712  [  246/  300]
per-ex loss: 0.678992  [  249/  300]
per-ex loss: 0.650099  [  252/  300]
per-ex loss: 0.512675  [  255/  300]
per-ex loss: 0.613399  [  258/  300]
per-ex loss: 0.492629  [  261/  300]
per-ex loss: 0.484467  [  264/  300]
per-ex loss: 0.471979  [  267/  300]
per-ex loss: 0.487168  [  270/  300]
per-ex loss: 0.530895  [  273/  300]
per-ex loss: 0.478104  [  276/  300]
per-ex loss: 0.582662  [  279/  300]
per-ex loss: 0.641483  [  282/  300]
per-ex loss: 0.430037  [  285/  300]
per-ex loss: 0.428343  [  288/  300]
per-ex loss: 0.546726  [  291/  300]
per-ex loss: 0.608775  [  294/  300]
per-ex loss: 0.482143  [  297/  300]
per-ex loss: 0.703882  [  300/  300]
Train Error: Avg loss: 0.53978401
validation Error: 
 Avg loss: 0.52338463 
 F1: 0.470103 
 Precision: 0.418117 
 Recall: 0.536851
 IoU: 0.307277

test Error: 
 Avg loss: 0.46271080 
 F1: 0.537971 
 Precision: 0.520420 
 Recall: 0.556748
 IoU: 0.367962

We have finished training iteration 72
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_70_.pth
per-ex loss: 0.404768  [    3/  300]
per-ex loss: 0.666954  [    6/  300]
per-ex loss: 0.589693  [    9/  300]
per-ex loss: 0.573676  [   12/  300]
per-ex loss: 0.586790  [   15/  300]
per-ex loss: 0.414718  [   18/  300]
per-ex loss: 0.685772  [   21/  300]
per-ex loss: 0.495637  [   24/  300]
per-ex loss: 0.532868  [   27/  300]
per-ex loss: 0.564822  [   30/  300]
per-ex loss: 0.474038  [   33/  300]
per-ex loss: 0.553970  [   36/  300]
per-ex loss: 0.513249  [   39/  300]
per-ex loss: 0.461265  [   42/  300]
per-ex loss: 0.533430  [   45/  300]
per-ex loss: 0.529621  [   48/  300]
per-ex loss: 0.505921  [   51/  300]
per-ex loss: 0.484324  [   54/  300]
per-ex loss: 0.564191  [   57/  300]
per-ex loss: 0.565639  [   60/  300]
per-ex loss: 0.445482  [   63/  300]
per-ex loss: 0.609522  [   66/  300]
per-ex loss: 0.479809  [   69/  300]
per-ex loss: 0.579429  [   72/  300]
per-ex loss: 0.433196  [   75/  300]
per-ex loss: 0.507757  [   78/  300]
per-ex loss: 0.507152  [   81/  300]
per-ex loss: 0.582104  [   84/  300]
per-ex loss: 0.651492  [   87/  300]
per-ex loss: 0.476576  [   90/  300]
per-ex loss: 0.417527  [   93/  300]
per-ex loss: 0.615483  [   96/  300]
per-ex loss: 0.444383  [   99/  300]
per-ex loss: 0.454321  [  102/  300]
per-ex loss: 0.421727  [  105/  300]
per-ex loss: 0.674157  [  108/  300]
per-ex loss: 0.676097  [  111/  300]
per-ex loss: 0.637848  [  114/  300]
per-ex loss: 0.520370  [  117/  300]
per-ex loss: 0.448932  [  120/  300]
per-ex loss: 0.503255  [  123/  300]
per-ex loss: 0.521491  [  126/  300]
per-ex loss: 0.622865  [  129/  300]
per-ex loss: 0.542711  [  132/  300]
per-ex loss: 0.632378  [  135/  300]
per-ex loss: 0.575917  [  138/  300]
per-ex loss: 0.451691  [  141/  300]
per-ex loss: 0.614279  [  144/  300]
per-ex loss: 0.520744  [  147/  300]
per-ex loss: 0.474244  [  150/  300]
per-ex loss: 0.490790  [  153/  300]
per-ex loss: 0.451215  [  156/  300]
per-ex loss: 0.605844  [  159/  300]
per-ex loss: 0.555590  [  162/  300]
per-ex loss: 0.476856  [  165/  300]
per-ex loss: 0.539455  [  168/  300]
per-ex loss: 0.461653  [  171/  300]
per-ex loss: 0.473052  [  174/  300]
per-ex loss: 0.510150  [  177/  300]
per-ex loss: 0.563169  [  180/  300]
per-ex loss: 0.538700  [  183/  300]
per-ex loss: 0.498019  [  186/  300]
per-ex loss: 0.539855  [  189/  300]
per-ex loss: 0.428776  [  192/  300]
per-ex loss: 0.467357  [  195/  300]
per-ex loss: 0.517176  [  198/  300]
per-ex loss: 0.446543  [  201/  300]
per-ex loss: 0.478662  [  204/  300]
per-ex loss: 0.543592  [  207/  300]
per-ex loss: 0.617371  [  210/  300]
per-ex loss: 0.560028  [  213/  300]
per-ex loss: 0.425853  [  216/  300]
per-ex loss: 0.485365  [  219/  300]
per-ex loss: 0.468586  [  222/  300]
per-ex loss: 0.495922  [  225/  300]
per-ex loss: 0.492899  [  228/  300]
per-ex loss: 0.498137  [  231/  300]
per-ex loss: 0.534278  [  234/  300]
per-ex loss: 0.509169  [  237/  300]
per-ex loss: 0.529961  [  240/  300]
per-ex loss: 0.501894  [  243/  300]
per-ex loss: 0.533477  [  246/  300]
per-ex loss: 0.495962  [  249/  300]
per-ex loss: 0.488979  [  252/  300]
per-ex loss: 0.661746  [  255/  300]
per-ex loss: 0.628446  [  258/  300]
per-ex loss: 0.625451  [  261/  300]
per-ex loss: 0.678801  [  264/  300]
per-ex loss: 0.468353  [  267/  300]
per-ex loss: 0.569494  [  270/  300]
per-ex loss: 0.603000  [  273/  300]
per-ex loss: 0.472687  [  276/  300]
per-ex loss: 0.497352  [  279/  300]
per-ex loss: 0.465835  [  282/  300]
per-ex loss: 0.632487  [  285/  300]
per-ex loss: 0.624148  [  288/  300]
per-ex loss: 0.469962  [  291/  300]
per-ex loss: 0.450069  [  294/  300]
per-ex loss: 0.504583  [  297/  300]
per-ex loss: 0.457916  [  300/  300]
Train Error: Avg loss: 0.52778948
validation Error: 
 Avg loss: 0.53282650 
 F1: 0.461216 
 Precision: 0.401040 
 Recall: 0.542640
 IoU: 0.299728

test Error: 
 Avg loss: 0.47932488 
 F1: 0.521400 
 Precision: 0.481367 
 Recall: 0.568697
 IoU: 0.352631

We have finished training iteration 73
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_71_.pth
per-ex loss: 0.420184  [    3/  300]
per-ex loss: 0.649476  [    6/  300]
per-ex loss: 0.510208  [    9/  300]
per-ex loss: 0.640362  [   12/  300]
per-ex loss: 0.419321  [   15/  300]
per-ex loss: 0.516566  [   18/  300]
per-ex loss: 0.582527  [   21/  300]
per-ex loss: 0.639275  [   24/  300]
per-ex loss: 0.556189  [   27/  300]
per-ex loss: 0.510053  [   30/  300]
per-ex loss: 0.465867  [   33/  300]
per-ex loss: 0.556426  [   36/  300]
per-ex loss: 0.620174  [   39/  300]
per-ex loss: 0.611223  [   42/  300]
per-ex loss: 0.500816  [   45/  300]
per-ex loss: 0.494402  [   48/  300]
per-ex loss: 0.558476  [   51/  300]
per-ex loss: 0.545496  [   54/  300]
per-ex loss: 0.612938  [   57/  300]
per-ex loss: 0.413851  [   60/  300]
per-ex loss: 0.534473  [   63/  300]
per-ex loss: 0.448779  [   66/  300]
per-ex loss: 0.477551  [   69/  300]
per-ex loss: 0.524431  [   72/  300]
per-ex loss: 0.564309  [   75/  300]
per-ex loss: 0.485416  [   78/  300]
per-ex loss: 0.431455  [   81/  300]
per-ex loss: 0.645959  [   84/  300]
per-ex loss: 0.564197  [   87/  300]
per-ex loss: 0.494812  [   90/  300]
per-ex loss: 0.501221  [   93/  300]
per-ex loss: 0.539512  [   96/  300]
per-ex loss: 0.652284  [   99/  300]
per-ex loss: 0.390073  [  102/  300]
per-ex loss: 0.506840  [  105/  300]
per-ex loss: 0.496922  [  108/  300]
per-ex loss: 0.411741  [  111/  300]
per-ex loss: 0.469991  [  114/  300]
per-ex loss: 0.501845  [  117/  300]
per-ex loss: 0.498154  [  120/  300]
per-ex loss: 0.672655  [  123/  300]
per-ex loss: 0.560503  [  126/  300]
per-ex loss: 0.445948  [  129/  300]
per-ex loss: 0.468210  [  132/  300]
per-ex loss: 0.518478  [  135/  300]
per-ex loss: 0.682785  [  138/  300]
per-ex loss: 0.701878  [  141/  300]
per-ex loss: 0.718465  [  144/  300]
per-ex loss: 0.472362  [  147/  300]
per-ex loss: 0.518568  [  150/  300]
per-ex loss: 0.533240  [  153/  300]
per-ex loss: 0.424563  [  156/  300]
per-ex loss: 0.519252  [  159/  300]
per-ex loss: 0.512975  [  162/  300]
per-ex loss: 0.474930  [  165/  300]
per-ex loss: 0.667717  [  168/  300]
per-ex loss: 0.540776  [  171/  300]
per-ex loss: 0.624783  [  174/  300]
per-ex loss: 0.445295  [  177/  300]
per-ex loss: 0.580051  [  180/  300]
per-ex loss: 0.569738  [  183/  300]
per-ex loss: 0.585205  [  186/  300]
per-ex loss: 0.639977  [  189/  300]
per-ex loss: 0.470073  [  192/  300]
per-ex loss: 0.521390  [  195/  300]
per-ex loss: 0.474201  [  198/  300]
per-ex loss: 0.554511  [  201/  300]
per-ex loss: 0.474773  [  204/  300]
per-ex loss: 0.480751  [  207/  300]
per-ex loss: 0.440564  [  210/  300]
per-ex loss: 0.680621  [  213/  300]
per-ex loss: 0.518504  [  216/  300]
per-ex loss: 0.616057  [  219/  300]
per-ex loss: 0.470255  [  222/  300]
per-ex loss: 0.548074  [  225/  300]
per-ex loss: 0.498019  [  228/  300]
per-ex loss: 0.527933  [  231/  300]
per-ex loss: 0.540068  [  234/  300]
per-ex loss: 0.542075  [  237/  300]
per-ex loss: 0.597007  [  240/  300]
per-ex loss: 0.472436  [  243/  300]
per-ex loss: 0.586467  [  246/  300]
per-ex loss: 0.441158  [  249/  300]
per-ex loss: 0.471007  [  252/  300]
per-ex loss: 0.572129  [  255/  300]
per-ex loss: 0.432207  [  258/  300]
per-ex loss: 0.550840  [  261/  300]
per-ex loss: 0.528057  [  264/  300]
per-ex loss: 0.450337  [  267/  300]
per-ex loss: 0.570270  [  270/  300]
per-ex loss: 0.521935  [  273/  300]
per-ex loss: 0.467267  [  276/  300]
per-ex loss: 0.564636  [  279/  300]
per-ex loss: 0.630052  [  282/  300]
per-ex loss: 0.419370  [  285/  300]
per-ex loss: 0.471632  [  288/  300]
per-ex loss: 0.469526  [  291/  300]
per-ex loss: 0.555887  [  294/  300]
per-ex loss: 0.613579  [  297/  300]
per-ex loss: 0.476209  [  300/  300]
Train Error: Avg loss: 0.53062025
validation Error: 
 Avg loss: 0.51258947 
 F1: 0.479619 
 Precision: 0.447870 
 Recall: 0.516213
 IoU: 0.315460

test Error: 
 Avg loss: 0.46277112 
 F1: 0.538063 
 Precision: 0.523559 
 Recall: 0.553394
 IoU: 0.368048

We have finished training iteration 74
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_72_.pth
per-ex loss: 0.514796  [    3/  300]
per-ex loss: 0.561579  [    6/  300]
per-ex loss: 0.560978  [    9/  300]
per-ex loss: 0.566154  [   12/  300]
per-ex loss: 0.561140  [   15/  300]
per-ex loss: 0.551815  [   18/  300]
per-ex loss: 0.490605  [   21/  300]
per-ex loss: 0.498632  [   24/  300]
per-ex loss: 0.521410  [   27/  300]
per-ex loss: 0.626299  [   30/  300]
per-ex loss: 0.491805  [   33/  300]
per-ex loss: 0.470146  [   36/  300]
per-ex loss: 0.608942  [   39/  300]
per-ex loss: 0.548155  [   42/  300]
per-ex loss: 0.565913  [   45/  300]
per-ex loss: 0.575071  [   48/  300]
per-ex loss: 0.422406  [   51/  300]
per-ex loss: 0.454261  [   54/  300]
per-ex loss: 0.469583  [   57/  300]
per-ex loss: 0.571957  [   60/  300]
per-ex loss: 0.464874  [   63/  300]
per-ex loss: 0.496449  [   66/  300]
per-ex loss: 0.563708  [   69/  300]
per-ex loss: 0.474958  [   72/  300]
per-ex loss: 0.527692  [   75/  300]
per-ex loss: 0.607152  [   78/  300]
per-ex loss: 0.679847  [   81/  300]
per-ex loss: 0.451572  [   84/  300]
per-ex loss: 0.584115  [   87/  300]
per-ex loss: 0.401240  [   90/  300]
per-ex loss: 0.616977  [   93/  300]
per-ex loss: 0.486290  [   96/  300]
per-ex loss: 0.582796  [   99/  300]
per-ex loss: 0.450409  [  102/  300]
per-ex loss: 0.459973  [  105/  300]
per-ex loss: 0.483525  [  108/  300]
per-ex loss: 0.624627  [  111/  300]
per-ex loss: 0.598962  [  114/  300]
per-ex loss: 0.559574  [  117/  300]
per-ex loss: 0.658301  [  120/  300]
per-ex loss: 0.641401  [  123/  300]
per-ex loss: 0.557883  [  126/  300]
per-ex loss: 0.456005  [  129/  300]
per-ex loss: 0.646386  [  132/  300]
per-ex loss: 0.517086  [  135/  300]
per-ex loss: 0.564298  [  138/  300]
per-ex loss: 0.524912  [  141/  300]
per-ex loss: 0.435658  [  144/  300]
per-ex loss: 0.586839  [  147/  300]
per-ex loss: 0.443794  [  150/  300]
per-ex loss: 0.497414  [  153/  300]
per-ex loss: 0.459032  [  156/  300]
per-ex loss: 0.532306  [  159/  300]
per-ex loss: 0.473402  [  162/  300]
per-ex loss: 0.508741  [  165/  300]
per-ex loss: 0.572903  [  168/  300]
per-ex loss: 0.494255  [  171/  300]
per-ex loss: 0.463870  [  174/  300]
per-ex loss: 0.527410  [  177/  300]
per-ex loss: 0.594573  [  180/  300]
per-ex loss: 0.585778  [  183/  300]
per-ex loss: 0.724812  [  186/  300]
per-ex loss: 0.499475  [  189/  300]
per-ex loss: 0.587947  [  192/  300]
per-ex loss: 0.440300  [  195/  300]
per-ex loss: 0.485817  [  198/  300]
per-ex loss: 0.528890  [  201/  300]
per-ex loss: 0.543886  [  204/  300]
per-ex loss: 0.473566  [  207/  300]
per-ex loss: 0.407654  [  210/  300]
per-ex loss: 0.615811  [  213/  300]
per-ex loss: 0.510345  [  216/  300]
per-ex loss: 0.558687  [  219/  300]
per-ex loss: 0.568738  [  222/  300]
per-ex loss: 0.420986  [  225/  300]
per-ex loss: 0.501005  [  228/  300]
per-ex loss: 0.569266  [  231/  300]
per-ex loss: 0.516689  [  234/  300]
per-ex loss: 0.557755  [  237/  300]
per-ex loss: 0.430299  [  240/  300]
per-ex loss: 0.496949  [  243/  300]
per-ex loss: 0.477077  [  246/  300]
per-ex loss: 0.554724  [  249/  300]
per-ex loss: 0.538516  [  252/  300]
per-ex loss: 0.645399  [  255/  300]
per-ex loss: 0.522142  [  258/  300]
per-ex loss: 0.487761  [  261/  300]
per-ex loss: 0.529137  [  264/  300]
per-ex loss: 0.472126  [  267/  300]
per-ex loss: 0.518652  [  270/  300]
per-ex loss: 0.438671  [  273/  300]
per-ex loss: 0.465161  [  276/  300]
per-ex loss: 0.594501  [  279/  300]
per-ex loss: 0.474123  [  282/  300]
per-ex loss: 0.607436  [  285/  300]
per-ex loss: 0.570999  [  288/  300]
per-ex loss: 0.458272  [  291/  300]
per-ex loss: 0.585901  [  294/  300]
per-ex loss: 0.548072  [  297/  300]
per-ex loss: 0.565349  [  300/  300]
Train Error: Avg loss: 0.52981530
validation Error: 
 Avg loss: 0.50906479 
 F1: 0.474076 
 Precision: 0.433910 
 Recall: 0.522437
 IoU: 0.310682

test Error: 
 Avg loss: 0.45723248 
 F1: 0.543583 
 Precision: 0.497434 
 Recall: 0.599170
 IoU: 0.373233

We have finished training iteration 75
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_73_.pth
per-ex loss: 0.577028  [    3/  300]
per-ex loss: 0.459677  [    6/  300]
per-ex loss: 0.420070  [    9/  300]
per-ex loss: 0.442396  [   12/  300]
per-ex loss: 0.481702  [   15/  300]
per-ex loss: 0.656262  [   18/  300]
per-ex loss: 0.488923  [   21/  300]
per-ex loss: 0.512109  [   24/  300]
per-ex loss: 0.497005  [   27/  300]
per-ex loss: 0.554550  [   30/  300]
per-ex loss: 0.432998  [   33/  300]
per-ex loss: 0.555869  [   36/  300]
per-ex loss: 0.572149  [   39/  300]
per-ex loss: 0.445212  [   42/  300]
per-ex loss: 0.478373  [   45/  300]
per-ex loss: 0.637073  [   48/  300]
per-ex loss: 0.628405  [   51/  300]
per-ex loss: 0.441870  [   54/  300]
per-ex loss: 0.509272  [   57/  300]
per-ex loss: 0.580971  [   60/  300]
per-ex loss: 0.629488  [   63/  300]
per-ex loss: 0.536937  [   66/  300]
per-ex loss: 0.438129  [   69/  300]
per-ex loss: 0.553245  [   72/  300]
per-ex loss: 0.612624  [   75/  300]
per-ex loss: 0.593929  [   78/  300]
per-ex loss: 0.536458  [   81/  300]
per-ex loss: 0.497599  [   84/  300]
per-ex loss: 0.427699  [   87/  300]
per-ex loss: 0.476206  [   90/  300]
per-ex loss: 0.509024  [   93/  300]
per-ex loss: 0.469325  [   96/  300]
per-ex loss: 0.467885  [   99/  300]
per-ex loss: 0.657820  [  102/  300]
per-ex loss: 0.444417  [  105/  300]
per-ex loss: 0.523507  [  108/  300]
per-ex loss: 0.446578  [  111/  300]
per-ex loss: 0.521859  [  114/  300]
per-ex loss: 0.536175  [  117/  300]
per-ex loss: 0.461972  [  120/  300]
per-ex loss: 0.516535  [  123/  300]
per-ex loss: 0.441773  [  126/  300]
per-ex loss: 0.429109  [  129/  300]
per-ex loss: 0.522169  [  132/  300]
per-ex loss: 0.583384  [  135/  300]
per-ex loss: 0.588803  [  138/  300]
per-ex loss: 0.493118  [  141/  300]
per-ex loss: 0.419140  [  144/  300]
per-ex loss: 0.487115  [  147/  300]
per-ex loss: 0.464271  [  150/  300]
per-ex loss: 0.488433  [  153/  300]
per-ex loss: 0.572183  [  156/  300]
per-ex loss: 0.452173  [  159/  300]
per-ex loss: 0.639102  [  162/  300]
per-ex loss: 0.484173  [  165/  300]
per-ex loss: 0.659278  [  168/  300]
per-ex loss: 0.624263  [  171/  300]
per-ex loss: 0.567799  [  174/  300]
per-ex loss: 0.638944  [  177/  300]
per-ex loss: 0.601111  [  180/  300]
per-ex loss: 0.535476  [  183/  300]
per-ex loss: 0.493931  [  186/  300]
per-ex loss: 0.471723  [  189/  300]
per-ex loss: 0.468659  [  192/  300]
per-ex loss: 0.462148  [  195/  300]
per-ex loss: 0.522792  [  198/  300]
per-ex loss: 0.416340  [  201/  300]
per-ex loss: 0.492713  [  204/  300]
per-ex loss: 0.456811  [  207/  300]
per-ex loss: 0.501271  [  210/  300]
per-ex loss: 0.634526  [  213/  300]
per-ex loss: 0.473056  [  216/  300]
per-ex loss: 0.560842  [  219/  300]
per-ex loss: 0.424701  [  222/  300]
per-ex loss: 0.498486  [  225/  300]
per-ex loss: 0.708742  [  228/  300]
per-ex loss: 0.564921  [  231/  300]
per-ex loss: 0.602546  [  234/  300]
per-ex loss: 0.732030  [  237/  300]
per-ex loss: 0.527678  [  240/  300]
per-ex loss: 0.404037  [  243/  300]
per-ex loss: 0.450584  [  246/  300]
per-ex loss: 0.624957  [  249/  300]
per-ex loss: 0.687065  [  252/  300]
per-ex loss: 0.571916  [  255/  300]
per-ex loss: 0.501228  [  258/  300]
per-ex loss: 0.638412  [  261/  300]
per-ex loss: 0.501794  [  264/  300]
per-ex loss: 0.536618  [  267/  300]
per-ex loss: 0.495206  [  270/  300]
per-ex loss: 0.463221  [  273/  300]
per-ex loss: 0.665258  [  276/  300]
per-ex loss: 0.486675  [  279/  300]
per-ex loss: 0.488914  [  282/  300]
per-ex loss: 0.515149  [  285/  300]
per-ex loss: 0.440172  [  288/  300]
per-ex loss: 0.512592  [  291/  300]
per-ex loss: 0.504820  [  294/  300]
per-ex loss: 0.537897  [  297/  300]
per-ex loss: 0.620314  [  300/  300]
Train Error: Avg loss: 0.52581886
validation Error: 
 Avg loss: 0.51474351 
 F1: 0.475420 
 Precision: 0.434505 
 Recall: 0.524840
 IoU: 0.311836

test Error: 
 Avg loss: 0.46107787 
 F1: 0.539640 
 Precision: 0.519632 
 Recall: 0.561251
 IoU: 0.369526

We have finished training iteration 76
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_74_.pth
per-ex loss: 0.565909  [    3/  300]
per-ex loss: 0.490974  [    6/  300]
per-ex loss: 0.456119  [    9/  300]
per-ex loss: 0.600811  [   12/  300]
per-ex loss: 0.442748  [   15/  300]
per-ex loss: 0.451232  [   18/  300]
per-ex loss: 0.518739  [   21/  300]
per-ex loss: 0.621848  [   24/  300]
per-ex loss: 0.440997  [   27/  300]
per-ex loss: 0.445000  [   30/  300]
per-ex loss: 0.499638  [   33/  300]
per-ex loss: 0.462839  [   36/  300]
per-ex loss: 0.647687  [   39/  300]
per-ex loss: 0.453510  [   42/  300]
per-ex loss: 0.700219  [   45/  300]
per-ex loss: 0.486910  [   48/  300]
per-ex loss: 0.657309  [   51/  300]
per-ex loss: 0.577414  [   54/  300]
per-ex loss: 0.513500  [   57/  300]
per-ex loss: 0.444851  [   60/  300]
per-ex loss: 0.508230  [   63/  300]
per-ex loss: 0.577131  [   66/  300]
per-ex loss: 0.497435  [   69/  300]
per-ex loss: 0.441679  [   72/  300]
per-ex loss: 0.611195  [   75/  300]
per-ex loss: 0.559950  [   78/  300]
per-ex loss: 0.575929  [   81/  300]
per-ex loss: 0.451694  [   84/  300]
per-ex loss: 0.632856  [   87/  300]
per-ex loss: 0.650396  [   90/  300]
per-ex loss: 0.446931  [   93/  300]
per-ex loss: 0.608711  [   96/  300]
per-ex loss: 0.477100  [   99/  300]
per-ex loss: 0.441312  [  102/  300]
per-ex loss: 0.549577  [  105/  300]
per-ex loss: 0.444671  [  108/  300]
per-ex loss: 0.566588  [  111/  300]
per-ex loss: 0.646907  [  114/  300]
per-ex loss: 0.497799  [  117/  300]
per-ex loss: 0.603035  [  120/  300]
per-ex loss: 0.487578  [  123/  300]
per-ex loss: 0.490462  [  126/  300]
per-ex loss: 0.483281  [  129/  300]
per-ex loss: 0.588844  [  132/  300]
per-ex loss: 0.462993  [  135/  300]
per-ex loss: 0.446262  [  138/  300]
per-ex loss: 0.656326  [  141/  300]
per-ex loss: 0.435075  [  144/  300]
per-ex loss: 0.675816  [  147/  300]
per-ex loss: 0.468274  [  150/  300]
per-ex loss: 0.519809  [  153/  300]
per-ex loss: 0.506948  [  156/  300]
per-ex loss: 0.481088  [  159/  300]
per-ex loss: 0.644353  [  162/  300]
per-ex loss: 0.510304  [  165/  300]
per-ex loss: 0.473190  [  168/  300]
per-ex loss: 0.550947  [  171/  300]
per-ex loss: 0.458716  [  174/  300]
per-ex loss: 0.514395  [  177/  300]
per-ex loss: 0.639758  [  180/  300]
per-ex loss: 0.462439  [  183/  300]
per-ex loss: 0.472051  [  186/  300]
per-ex loss: 0.458537  [  189/  300]
per-ex loss: 0.597341  [  192/  300]
per-ex loss: 0.540005  [  195/  300]
per-ex loss: 0.458752  [  198/  300]
per-ex loss: 0.565005  [  201/  300]
per-ex loss: 0.436074  [  204/  300]
per-ex loss: 0.424186  [  207/  300]
per-ex loss: 0.542102  [  210/  300]
per-ex loss: 0.440299  [  213/  300]
per-ex loss: 0.459155  [  216/  300]
per-ex loss: 0.609321  [  219/  300]
per-ex loss: 0.471385  [  222/  300]
per-ex loss: 0.609103  [  225/  300]
per-ex loss: 0.550996  [  228/  300]
per-ex loss: 0.544167  [  231/  300]
per-ex loss: 0.528817  [  234/  300]
per-ex loss: 0.644637  [  237/  300]
per-ex loss: 0.638197  [  240/  300]
per-ex loss: 0.592240  [  243/  300]
per-ex loss: 0.474626  [  246/  300]
per-ex loss: 0.407881  [  249/  300]
per-ex loss: 0.590453  [  252/  300]
per-ex loss: 0.642442  [  255/  300]
per-ex loss: 0.598701  [  258/  300]
per-ex loss: 0.493795  [  261/  300]
per-ex loss: 0.426732  [  264/  300]
per-ex loss: 0.533243  [  267/  300]
per-ex loss: 0.528169  [  270/  300]
per-ex loss: 0.461897  [  273/  300]
per-ex loss: 0.519975  [  276/  300]
per-ex loss: 0.476918  [  279/  300]
per-ex loss: 0.617015  [  282/  300]
per-ex loss: 0.469056  [  285/  300]
per-ex loss: 0.535110  [  288/  300]
per-ex loss: 0.550692  [  291/  300]
per-ex loss: 0.550496  [  294/  300]
per-ex loss: 0.502962  [  297/  300]
per-ex loss: 0.544850  [  300/  300]
Train Error: Avg loss: 0.52731619
validation Error: 
 Avg loss: 0.50884189 
 F1: 0.471087 
 Precision: 0.409746 
 Recall: 0.554029
 IoU: 0.308119

test Error: 
 Avg loss: 0.46243173 
 F1: 0.538830 
 Precision: 0.461279 
 Recall: 0.647727
 IoU: 0.368766

We have finished training iteration 77
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_75_.pth
per-ex loss: 0.559536  [    3/  300]
per-ex loss: 0.447061  [    6/  300]
per-ex loss: 0.570922  [    9/  300]
per-ex loss: 0.581073  [   12/  300]
per-ex loss: 0.495974  [   15/  300]
per-ex loss: 0.451056  [   18/  300]
per-ex loss: 0.472713  [   21/  300]
per-ex loss: 0.529999  [   24/  300]
per-ex loss: 0.528440  [   27/  300]
per-ex loss: 0.504917  [   30/  300]
per-ex loss: 0.524133  [   33/  300]
per-ex loss: 0.610557  [   36/  300]
per-ex loss: 0.560025  [   39/  300]
per-ex loss: 0.478811  [   42/  300]
per-ex loss: 0.425788  [   45/  300]
per-ex loss: 0.480027  [   48/  300]
per-ex loss: 0.551782  [   51/  300]
per-ex loss: 0.554207  [   54/  300]
per-ex loss: 0.593142  [   57/  300]
per-ex loss: 0.459106  [   60/  300]
per-ex loss: 0.664647  [   63/  300]
per-ex loss: 0.525291  [   66/  300]
per-ex loss: 0.418864  [   69/  300]
per-ex loss: 0.501520  [   72/  300]
per-ex loss: 0.541707  [   75/  300]
per-ex loss: 0.648308  [   78/  300]
per-ex loss: 0.536767  [   81/  300]
per-ex loss: 0.669648  [   84/  300]
per-ex loss: 0.467154  [   87/  300]
per-ex loss: 0.511132  [   90/  300]
per-ex loss: 0.527122  [   93/  300]
per-ex loss: 0.665449  [   96/  300]
per-ex loss: 0.473084  [   99/  300]
per-ex loss: 0.458609  [  102/  300]
per-ex loss: 0.590069  [  105/  300]
per-ex loss: 0.512292  [  108/  300]
per-ex loss: 0.591799  [  111/  300]
per-ex loss: 0.503436  [  114/  300]
per-ex loss: 0.617337  [  117/  300]
per-ex loss: 0.653888  [  120/  300]
per-ex loss: 0.505850  [  123/  300]
per-ex loss: 0.496243  [  126/  300]
per-ex loss: 0.440941  [  129/  300]
per-ex loss: 0.462045  [  132/  300]
per-ex loss: 0.589352  [  135/  300]
per-ex loss: 0.522247  [  138/  300]
per-ex loss: 0.481698  [  141/  300]
per-ex loss: 0.454430  [  144/  300]
per-ex loss: 0.520623  [  147/  300]
per-ex loss: 0.487221  [  150/  300]
per-ex loss: 0.585468  [  153/  300]
per-ex loss: 0.438816  [  156/  300]
per-ex loss: 0.541242  [  159/  300]
per-ex loss: 0.511064  [  162/  300]
per-ex loss: 0.590839  [  165/  300]
per-ex loss: 0.607238  [  168/  300]
per-ex loss: 0.507186  [  171/  300]
per-ex loss: 0.619815  [  174/  300]
per-ex loss: 0.479895  [  177/  300]
per-ex loss: 0.534853  [  180/  300]
per-ex loss: 0.516812  [  183/  300]
per-ex loss: 0.489782  [  186/  300]
per-ex loss: 0.505574  [  189/  300]
per-ex loss: 0.498306  [  192/  300]
per-ex loss: 0.624191  [  195/  300]
per-ex loss: 0.660739  [  198/  300]
per-ex loss: 0.462044  [  201/  300]
per-ex loss: 0.557748  [  204/  300]
per-ex loss: 0.516154  [  207/  300]
per-ex loss: 0.513281  [  210/  300]
per-ex loss: 0.570145  [  213/  300]
per-ex loss: 0.424085  [  216/  300]
per-ex loss: 0.442259  [  219/  300]
per-ex loss: 0.450192  [  222/  300]
per-ex loss: 0.699915  [  225/  300]
per-ex loss: 0.451501  [  228/  300]
per-ex loss: 0.594397  [  231/  300]
per-ex loss: 0.497782  [  234/  300]
per-ex loss: 0.527836  [  237/  300]
per-ex loss: 0.631343  [  240/  300]
per-ex loss: 0.492118  [  243/  300]
per-ex loss: 0.444531  [  246/  300]
per-ex loss: 0.582431  [  249/  300]
per-ex loss: 0.547982  [  252/  300]
per-ex loss: 0.462060  [  255/  300]
per-ex loss: 0.520384  [  258/  300]
per-ex loss: 0.508154  [  261/  300]
per-ex loss: 0.531806  [  264/  300]
per-ex loss: 0.623900  [  267/  300]
per-ex loss: 0.425968  [  270/  300]
per-ex loss: 0.499770  [  273/  300]
per-ex loss: 0.465346  [  276/  300]
per-ex loss: 0.620727  [  279/  300]
per-ex loss: 0.640976  [  282/  300]
per-ex loss: 0.518598  [  285/  300]
per-ex loss: 0.578506  [  288/  300]
per-ex loss: 0.547960  [  291/  300]
per-ex loss: 0.477150  [  294/  300]
per-ex loss: 0.486170  [  297/  300]
per-ex loss: 0.609364  [  300/  300]
Train Error: Avg loss: 0.53052449
validation Error: 
 Avg loss: 0.51116453 
 F1: 0.476054 
 Precision: 0.430806 
 Recall: 0.531923
 IoU: 0.312383

test Error: 
 Avg loss: 0.45723879 
 F1: 0.543217 
 Precision: 0.509232 
 Recall: 0.582062
 IoU: 0.372888

We have finished training iteration 78
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_76_.pth
per-ex loss: 0.531997  [    3/  300]
per-ex loss: 0.646669  [    6/  300]
per-ex loss: 0.475948  [    9/  300]
per-ex loss: 0.464360  [   12/  300]
per-ex loss: 0.671273  [   15/  300]
per-ex loss: 0.561241  [   18/  300]
per-ex loss: 0.604379  [   21/  300]
per-ex loss: 0.463287  [   24/  300]
per-ex loss: 0.513149  [   27/  300]
per-ex loss: 0.537801  [   30/  300]
per-ex loss: 0.458561  [   33/  300]
per-ex loss: 0.501967  [   36/  300]
per-ex loss: 0.464972  [   39/  300]
per-ex loss: 0.531579  [   42/  300]
per-ex loss: 0.454447  [   45/  300]
per-ex loss: 0.405227  [   48/  300]
per-ex loss: 0.680510  [   51/  300]
per-ex loss: 0.444897  [   54/  300]
per-ex loss: 0.520239  [   57/  300]
per-ex loss: 0.490689  [   60/  300]
per-ex loss: 0.496454  [   63/  300]
per-ex loss: 0.554453  [   66/  300]
per-ex loss: 0.499490  [   69/  300]
per-ex loss: 0.446870  [   72/  300]
per-ex loss: 0.428923  [   75/  300]
per-ex loss: 0.619917  [   78/  300]
per-ex loss: 0.596129  [   81/  300]
per-ex loss: 0.553135  [   84/  300]
per-ex loss: 0.569938  [   87/  300]
per-ex loss: 0.577518  [   90/  300]
per-ex loss: 0.479375  [   93/  300]
per-ex loss: 0.485248  [   96/  300]
per-ex loss: 0.479116  [   99/  300]
per-ex loss: 0.618525  [  102/  300]
per-ex loss: 0.595001  [  105/  300]
per-ex loss: 0.521531  [  108/  300]
per-ex loss: 0.443747  [  111/  300]
per-ex loss: 0.541142  [  114/  300]
per-ex loss: 0.537847  [  117/  300]
per-ex loss: 0.549513  [  120/  300]
per-ex loss: 0.700426  [  123/  300]
per-ex loss: 0.479313  [  126/  300]
per-ex loss: 0.555046  [  129/  300]
per-ex loss: 0.479078  [  132/  300]
per-ex loss: 0.510010  [  135/  300]
per-ex loss: 0.613711  [  138/  300]
per-ex loss: 0.446365  [  141/  300]
per-ex loss: 0.529282  [  144/  300]
per-ex loss: 0.480211  [  147/  300]
per-ex loss: 0.596002  [  150/  300]
per-ex loss: 0.453963  [  153/  300]
per-ex loss: 0.399644  [  156/  300]
per-ex loss: 0.529410  [  159/  300]
per-ex loss: 0.407961  [  162/  300]
per-ex loss: 0.558034  [  165/  300]
per-ex loss: 0.512313  [  168/  300]
per-ex loss: 0.571526  [  171/  300]
per-ex loss: 0.611024  [  174/  300]
per-ex loss: 0.440639  [  177/  300]
per-ex loss: 0.702698  [  180/  300]
per-ex loss: 0.524790  [  183/  300]
per-ex loss: 0.509503  [  186/  300]
per-ex loss: 0.489302  [  189/  300]
per-ex loss: 0.581645  [  192/  300]
per-ex loss: 0.471179  [  195/  300]
per-ex loss: 0.536658  [  198/  300]
per-ex loss: 0.527406  [  201/  300]
per-ex loss: 0.510246  [  204/  300]
per-ex loss: 0.529184  [  207/  300]
per-ex loss: 0.649361  [  210/  300]
per-ex loss: 0.492433  [  213/  300]
per-ex loss: 0.451563  [  216/  300]
per-ex loss: 0.520166  [  219/  300]
per-ex loss: 0.540133  [  222/  300]
per-ex loss: 0.654700  [  225/  300]
per-ex loss: 0.599521  [  228/  300]
per-ex loss: 0.649286  [  231/  300]
per-ex loss: 0.462941  [  234/  300]
per-ex loss: 0.440220  [  237/  300]
per-ex loss: 0.416222  [  240/  300]
per-ex loss: 0.526099  [  243/  300]
per-ex loss: 0.467413  [  246/  300]
per-ex loss: 0.509529  [  249/  300]
per-ex loss: 0.500840  [  252/  300]
per-ex loss: 0.441836  [  255/  300]
per-ex loss: 0.684792  [  258/  300]
per-ex loss: 0.431957  [  261/  300]
per-ex loss: 0.609069  [  264/  300]
per-ex loss: 0.549556  [  267/  300]
per-ex loss: 0.380650  [  270/  300]
per-ex loss: 0.500726  [  273/  300]
per-ex loss: 0.561739  [  276/  300]
per-ex loss: 0.490316  [  279/  300]
per-ex loss: 0.636271  [  282/  300]
per-ex loss: 0.638382  [  285/  300]
per-ex loss: 0.517489  [  288/  300]
per-ex loss: 0.568031  [  291/  300]
per-ex loss: 0.515983  [  294/  300]
per-ex loss: 0.464370  [  297/  300]
per-ex loss: 0.490565  [  300/  300]
Train Error: Avg loss: 0.52635789
validation Error: 
 Avg loss: 0.52329002 
 F1: 0.470353 
 Precision: 0.418970 
 Recall: 0.536102
 IoU: 0.307492

test Error: 
 Avg loss: 0.45760804 
 F1: 0.543079 
 Precision: 0.523759 
 Recall: 0.563879
 IoU: 0.372758

We have finished training iteration 79
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_77_.pth
per-ex loss: 0.512299  [    3/  300]
per-ex loss: 0.487114  [    6/  300]
per-ex loss: 0.410108  [    9/  300]
per-ex loss: 0.513736  [   12/  300]
per-ex loss: 0.557286  [   15/  300]
per-ex loss: 0.665329  [   18/  300]
per-ex loss: 0.539215  [   21/  300]
per-ex loss: 0.581270  [   24/  300]
per-ex loss: 0.477319  [   27/  300]
per-ex loss: 0.619494  [   30/  300]
per-ex loss: 0.404731  [   33/  300]
per-ex loss: 0.450118  [   36/  300]
per-ex loss: 0.637240  [   39/  300]
per-ex loss: 0.636564  [   42/  300]
per-ex loss: 0.574482  [   45/  300]
per-ex loss: 0.491144  [   48/  300]
per-ex loss: 0.537506  [   51/  300]
per-ex loss: 0.535495  [   54/  300]
per-ex loss: 0.469958  [   57/  300]
per-ex loss: 0.444692  [   60/  300]
per-ex loss: 0.434017  [   63/  300]
per-ex loss: 0.464250  [   66/  300]
per-ex loss: 0.453899  [   69/  300]
per-ex loss: 0.503402  [   72/  300]
per-ex loss: 0.628414  [   75/  300]
per-ex loss: 0.426061  [   78/  300]
per-ex loss: 0.651677  [   81/  300]
per-ex loss: 0.534784  [   84/  300]
per-ex loss: 0.556691  [   87/  300]
per-ex loss: 0.563475  [   90/  300]
per-ex loss: 0.407924  [   93/  300]
per-ex loss: 0.486111  [   96/  300]
per-ex loss: 0.629615  [   99/  300]
per-ex loss: 0.598814  [  102/  300]
per-ex loss: 0.495558  [  105/  300]
per-ex loss: 0.591744  [  108/  300]
per-ex loss: 0.512005  [  111/  300]
per-ex loss: 0.463902  [  114/  300]
per-ex loss: 0.511248  [  117/  300]
per-ex loss: 0.584054  [  120/  300]
per-ex loss: 0.479385  [  123/  300]
per-ex loss: 0.522127  [  126/  300]
per-ex loss: 0.629942  [  129/  300]
per-ex loss: 0.540392  [  132/  300]
per-ex loss: 0.564737  [  135/  300]
per-ex loss: 0.430138  [  138/  300]
per-ex loss: 0.502982  [  141/  300]
per-ex loss: 0.629075  [  144/  300]
per-ex loss: 0.426609  [  147/  300]
per-ex loss: 0.531868  [  150/  300]
per-ex loss: 0.503249  [  153/  300]
per-ex loss: 0.480307  [  156/  300]
per-ex loss: 0.515907  [  159/  300]
per-ex loss: 0.567006  [  162/  300]
per-ex loss: 0.470295  [  165/  300]
per-ex loss: 0.417439  [  168/  300]
per-ex loss: 0.669186  [  171/  300]
per-ex loss: 0.563068  [  174/  300]
per-ex loss: 0.533377  [  177/  300]
per-ex loss: 0.467693  [  180/  300]
per-ex loss: 0.538344  [  183/  300]
per-ex loss: 0.468357  [  186/  300]
per-ex loss: 0.521691  [  189/  300]
per-ex loss: 0.530996  [  192/  300]
per-ex loss: 0.440421  [  195/  300]
per-ex loss: 0.520183  [  198/  300]
per-ex loss: 0.538408  [  201/  300]
per-ex loss: 0.622728  [  204/  300]
per-ex loss: 0.495961  [  207/  300]
per-ex loss: 0.549579  [  210/  300]
per-ex loss: 0.550573  [  213/  300]
per-ex loss: 0.553939  [  216/  300]
per-ex loss: 0.464218  [  219/  300]
per-ex loss: 0.552257  [  222/  300]
per-ex loss: 0.498360  [  225/  300]
per-ex loss: 0.498620  [  228/  300]
per-ex loss: 0.630607  [  231/  300]
per-ex loss: 0.596767  [  234/  300]
per-ex loss: 0.464883  [  237/  300]
per-ex loss: 0.515376  [  240/  300]
per-ex loss: 0.384442  [  243/  300]
per-ex loss: 0.509152  [  246/  300]
per-ex loss: 0.502732  [  249/  300]
per-ex loss: 0.560946  [  252/  300]
per-ex loss: 0.544824  [  255/  300]
per-ex loss: 0.520983  [  258/  300]
per-ex loss: 0.552692  [  261/  300]
per-ex loss: 0.463305  [  264/  300]
per-ex loss: 0.628475  [  267/  300]
per-ex loss: 0.540874  [  270/  300]
per-ex loss: 0.480910  [  273/  300]
per-ex loss: 0.648032  [  276/  300]
per-ex loss: 0.435184  [  279/  300]
per-ex loss: 0.523833  [  282/  300]
per-ex loss: 0.440657  [  285/  300]
per-ex loss: 0.617962  [  288/  300]
per-ex loss: 0.536340  [  291/  300]
per-ex loss: 0.498235  [  294/  300]
per-ex loss: 0.453125  [  297/  300]
per-ex loss: 0.475274  [  300/  300]
Train Error: Avg loss: 0.52357773
validation Error: 
 Avg loss: 0.50847512 
 F1: 0.478853 
 Precision: 0.429213 
 Recall: 0.541476
 IoU: 0.314797

test Error: 
 Avg loss: 0.45170450 
 F1: 0.548972 
 Precision: 0.505871 
 Recall: 0.600101
 IoU: 0.378333

We have finished training iteration 80
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_78_.pth
per-ex loss: 0.610311  [    3/  300]
per-ex loss: 0.564958  [    6/  300]
per-ex loss: 0.503132  [    9/  300]
per-ex loss: 0.449462  [   12/  300]
per-ex loss: 0.530532  [   15/  300]
per-ex loss: 0.475132  [   18/  300]
per-ex loss: 0.653545  [   21/  300]
per-ex loss: 0.503892  [   24/  300]
per-ex loss: 0.459440  [   27/  300]
per-ex loss: 0.493152  [   30/  300]
per-ex loss: 0.456250  [   33/  300]
per-ex loss: 0.464597  [   36/  300]
per-ex loss: 0.471415  [   39/  300]
per-ex loss: 0.432479  [   42/  300]
per-ex loss: 0.539609  [   45/  300]
per-ex loss: 0.549107  [   48/  300]
per-ex loss: 0.603924  [   51/  300]
per-ex loss: 0.451180  [   54/  300]
per-ex loss: 0.502407  [   57/  300]
per-ex loss: 0.488687  [   60/  300]
per-ex loss: 0.521440  [   63/  300]
per-ex loss: 0.414602  [   66/  300]
per-ex loss: 0.557567  [   69/  300]
per-ex loss: 0.398178  [   72/  300]
per-ex loss: 0.468267  [   75/  300]
per-ex loss: 0.600863  [   78/  300]
per-ex loss: 0.615378  [   81/  300]
per-ex loss: 0.561637  [   84/  300]
per-ex loss: 0.526124  [   87/  300]
per-ex loss: 0.631307  [   90/  300]
per-ex loss: 0.546873  [   93/  300]
per-ex loss: 0.451065  [   96/  300]
per-ex loss: 0.566357  [   99/  300]
per-ex loss: 0.431111  [  102/  300]
per-ex loss: 0.574795  [  105/  300]
per-ex loss: 0.449362  [  108/  300]
per-ex loss: 0.503448  [  111/  300]
per-ex loss: 0.593418  [  114/  300]
per-ex loss: 0.483530  [  117/  300]
per-ex loss: 0.458998  [  120/  300]
per-ex loss: 0.466416  [  123/  300]
per-ex loss: 0.490789  [  126/  300]
per-ex loss: 0.718609  [  129/  300]
per-ex loss: 0.664247  [  132/  300]
per-ex loss: 0.469871  [  135/  300]
per-ex loss: 0.492451  [  138/  300]
per-ex loss: 0.602950  [  141/  300]
per-ex loss: 0.549079  [  144/  300]
per-ex loss: 0.520259  [  147/  300]
per-ex loss: 0.538370  [  150/  300]
per-ex loss: 0.481045  [  153/  300]
per-ex loss: 0.521213  [  156/  300]
per-ex loss: 0.492499  [  159/  300]
per-ex loss: 0.539898  [  162/  300]
per-ex loss: 0.463008  [  165/  300]
per-ex loss: 0.497198  [  168/  300]
per-ex loss: 0.451828  [  171/  300]
per-ex loss: 0.484564  [  174/  300]
per-ex loss: 0.426675  [  177/  300]
per-ex loss: 0.565524  [  180/  300]
per-ex loss: 0.505804  [  183/  300]
per-ex loss: 0.485661  [  186/  300]
per-ex loss: 0.642284  [  189/  300]
per-ex loss: 0.592479  [  192/  300]
per-ex loss: 0.600388  [  195/  300]
per-ex loss: 0.516815  [  198/  300]
per-ex loss: 0.568905  [  201/  300]
per-ex loss: 0.462887  [  204/  300]
per-ex loss: 0.542749  [  207/  300]
per-ex loss: 0.461353  [  210/  300]
per-ex loss: 0.437593  [  213/  300]
per-ex loss: 0.679353  [  216/  300]
per-ex loss: 0.668268  [  219/  300]
per-ex loss: 0.474488  [  222/  300]
per-ex loss: 0.494132  [  225/  300]
per-ex loss: 0.547030  [  228/  300]
per-ex loss: 0.506808  [  231/  300]
per-ex loss: 0.483391  [  234/  300]
per-ex loss: 0.479569  [  237/  300]
per-ex loss: 0.486055  [  240/  300]
per-ex loss: 0.467401  [  243/  300]
per-ex loss: 0.501608  [  246/  300]
per-ex loss: 0.596291  [  249/  300]
per-ex loss: 0.541717  [  252/  300]
per-ex loss: 0.500579  [  255/  300]
per-ex loss: 0.451884  [  258/  300]
per-ex loss: 0.589219  [  261/  300]
per-ex loss: 0.493763  [  264/  300]
per-ex loss: 0.542335  [  267/  300]
per-ex loss: 0.450566  [  270/  300]
per-ex loss: 0.507836  [  273/  300]
per-ex loss: 0.426151  [  276/  300]
per-ex loss: 0.434910  [  279/  300]
per-ex loss: 0.510007  [  282/  300]
per-ex loss: 0.506998  [  285/  300]
per-ex loss: 0.688604  [  288/  300]
per-ex loss: 0.677949  [  291/  300]
per-ex loss: 0.553879  [  294/  300]
per-ex loss: 0.456554  [  297/  300]
per-ex loss: 0.526465  [  300/  300]
Train Error: Avg loss: 0.52054748
validation Error: 
 Avg loss: 0.53406650 
 F1: 0.456623 
 Precision: 0.463043 
 Recall: 0.450379
 IoU: 0.295860

test Error: 
 Avg loss: 0.46154195 
 F1: 0.539327 
 Precision: 0.521426 
 Recall: 0.558501
 IoU: 0.369232

We have finished training iteration 81
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_79_.pth
per-ex loss: 0.546960  [    3/  300]
per-ex loss: 0.492867  [    6/  300]
per-ex loss: 0.455664  [    9/  300]
per-ex loss: 0.521308  [   12/  300]
per-ex loss: 0.615288  [   15/  300]
per-ex loss: 0.471335  [   18/  300]
per-ex loss: 0.501600  [   21/  300]
per-ex loss: 0.422164  [   24/  300]
per-ex loss: 0.537925  [   27/  300]
per-ex loss: 0.410228  [   30/  300]
per-ex loss: 0.528284  [   33/  300]
per-ex loss: 0.466065  [   36/  300]
per-ex loss: 0.578472  [   39/  300]
per-ex loss: 0.518018  [   42/  300]
per-ex loss: 0.528280  [   45/  300]
per-ex loss: 0.466095  [   48/  300]
per-ex loss: 0.645228  [   51/  300]
per-ex loss: 0.499609  [   54/  300]
per-ex loss: 0.560089  [   57/  300]
per-ex loss: 0.610327  [   60/  300]
per-ex loss: 0.604679  [   63/  300]
per-ex loss: 0.437799  [   66/  300]
per-ex loss: 0.484689  [   69/  300]
per-ex loss: 0.662855  [   72/  300]
per-ex loss: 0.463489  [   75/  300]
per-ex loss: 0.655307  [   78/  300]
per-ex loss: 0.488253  [   81/  300]
per-ex loss: 0.517817  [   84/  300]
per-ex loss: 0.461316  [   87/  300]
per-ex loss: 0.564740  [   90/  300]
per-ex loss: 0.498579  [   93/  300]
per-ex loss: 0.441140  [   96/  300]
per-ex loss: 0.499588  [   99/  300]
per-ex loss: 0.570687  [  102/  300]
per-ex loss: 0.626033  [  105/  300]
per-ex loss: 0.515348  [  108/  300]
per-ex loss: 0.444255  [  111/  300]
per-ex loss: 0.522190  [  114/  300]
per-ex loss: 0.624935  [  117/  300]
per-ex loss: 0.564869  [  120/  300]
per-ex loss: 0.481463  [  123/  300]
per-ex loss: 0.489658  [  126/  300]
per-ex loss: 0.543126  [  129/  300]
per-ex loss: 0.616894  [  132/  300]
per-ex loss: 0.547249  [  135/  300]
per-ex loss: 0.467891  [  138/  300]
per-ex loss: 0.538075  [  141/  300]
per-ex loss: 0.533985  [  144/  300]
per-ex loss: 0.447547  [  147/  300]
per-ex loss: 0.560503  [  150/  300]
per-ex loss: 0.523288  [  153/  300]
per-ex loss: 0.403880  [  156/  300]
per-ex loss: 0.477107  [  159/  300]
per-ex loss: 0.649841  [  162/  300]
per-ex loss: 0.454908  [  165/  300]
per-ex loss: 0.639219  [  168/  300]
per-ex loss: 0.447071  [  171/  300]
per-ex loss: 0.580285  [  174/  300]
per-ex loss: 0.574246  [  177/  300]
per-ex loss: 0.587163  [  180/  300]
per-ex loss: 0.434169  [  183/  300]
per-ex loss: 0.417041  [  186/  300]
per-ex loss: 0.531175  [  189/  300]
per-ex loss: 0.508931  [  192/  300]
per-ex loss: 0.570545  [  195/  300]
per-ex loss: 0.510711  [  198/  300]
per-ex loss: 0.556862  [  201/  300]
per-ex loss: 0.594215  [  204/  300]
per-ex loss: 0.512383  [  207/  300]
per-ex loss: 0.476189  [  210/  300]
per-ex loss: 0.481120  [  213/  300]
per-ex loss: 0.605924  [  216/  300]
per-ex loss: 0.528721  [  219/  300]
per-ex loss: 0.704928  [  222/  300]
per-ex loss: 0.515581  [  225/  300]
per-ex loss: 0.444246  [  228/  300]
per-ex loss: 0.534222  [  231/  300]
per-ex loss: 0.555102  [  234/  300]
per-ex loss: 0.526927  [  237/  300]
per-ex loss: 0.669522  [  240/  300]
per-ex loss: 0.558527  [  243/  300]
per-ex loss: 0.496400  [  246/  300]
per-ex loss: 0.497119  [  249/  300]
per-ex loss: 0.461115  [  252/  300]
per-ex loss: 0.529588  [  255/  300]
per-ex loss: 0.553398  [  258/  300]
per-ex loss: 0.452982  [  261/  300]
per-ex loss: 0.423563  [  264/  300]
per-ex loss: 0.648016  [  267/  300]
per-ex loss: 0.452012  [  270/  300]
per-ex loss: 0.527191  [  273/  300]
per-ex loss: 0.502133  [  276/  300]
per-ex loss: 0.499454  [  279/  300]
per-ex loss: 0.534401  [  282/  300]
per-ex loss: 0.525235  [  285/  300]
per-ex loss: 0.655913  [  288/  300]
per-ex loss: 0.553627  [  291/  300]
per-ex loss: 0.605294  [  294/  300]
per-ex loss: 0.625095  [  297/  300]
per-ex loss: 0.628596  [  300/  300]
Train Error: Avg loss: 0.52995946
validation Error: 
 Avg loss: 0.50818694 
 F1: 0.475979 
 Precision: 0.407535 
 Recall: 0.572054
 IoU: 0.312318

test Error: 
 Avg loss: 0.47168899 
 F1: 0.528813 
 Precision: 0.459311 
 Recall: 0.623099
 IoU: 0.359447

We have finished training iteration 82
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_80_.pth
per-ex loss: 0.434673  [    3/  300]
per-ex loss: 0.495778  [    6/  300]
per-ex loss: 0.526030  [    9/  300]
per-ex loss: 0.502020  [   12/  300]
per-ex loss: 0.567557  [   15/  300]
per-ex loss: 0.403250  [   18/  300]
per-ex loss: 0.713136  [   21/  300]
per-ex loss: 0.469543  [   24/  300]
per-ex loss: 0.493779  [   27/  300]
per-ex loss: 0.462916  [   30/  300]
per-ex loss: 0.561284  [   33/  300]
per-ex loss: 0.530470  [   36/  300]
per-ex loss: 0.578451  [   39/  300]
per-ex loss: 0.618791  [   42/  300]
per-ex loss: 0.449179  [   45/  300]
per-ex loss: 0.440838  [   48/  300]
per-ex loss: 0.640524  [   51/  300]
per-ex loss: 0.535221  [   54/  300]
per-ex loss: 0.641965  [   57/  300]
per-ex loss: 0.509522  [   60/  300]
per-ex loss: 0.726688  [   63/  300]
per-ex loss: 0.549305  [   66/  300]
per-ex loss: 0.472721  [   69/  300]
per-ex loss: 0.541644  [   72/  300]
per-ex loss: 0.447339  [   75/  300]
per-ex loss: 0.473544  [   78/  300]
per-ex loss: 0.603138  [   81/  300]
per-ex loss: 0.507636  [   84/  300]
per-ex loss: 0.432073  [   87/  300]
per-ex loss: 0.514493  [   90/  300]
per-ex loss: 0.440354  [   93/  300]
per-ex loss: 0.471677  [   96/  300]
per-ex loss: 0.515640  [   99/  300]
per-ex loss: 0.573261  [  102/  300]
per-ex loss: 0.425450  [  105/  300]
per-ex loss: 0.461563  [  108/  300]
per-ex loss: 0.485226  [  111/  300]
per-ex loss: 0.466763  [  114/  300]
per-ex loss: 0.598846  [  117/  300]
per-ex loss: 0.654645  [  120/  300]
per-ex loss: 0.574009  [  123/  300]
per-ex loss: 0.442650  [  126/  300]
per-ex loss: 0.441721  [  129/  300]
per-ex loss: 0.471444  [  132/  300]
per-ex loss: 0.531875  [  135/  300]
per-ex loss: 0.569257  [  138/  300]
per-ex loss: 0.450702  [  141/  300]
per-ex loss: 0.541067  [  144/  300]
per-ex loss: 0.665345  [  147/  300]
per-ex loss: 0.437672  [  150/  300]
per-ex loss: 0.654452  [  153/  300]
per-ex loss: 0.560265  [  156/  300]
per-ex loss: 0.607349  [  159/  300]
per-ex loss: 0.618483  [  162/  300]
per-ex loss: 0.469304  [  165/  300]
per-ex loss: 0.476086  [  168/  300]
per-ex loss: 0.582391  [  171/  300]
per-ex loss: 0.409433  [  174/  300]
per-ex loss: 0.540900  [  177/  300]
per-ex loss: 0.474120  [  180/  300]
per-ex loss: 0.493608  [  183/  300]
per-ex loss: 0.427277  [  186/  300]
per-ex loss: 0.525064  [  189/  300]
per-ex loss: 0.423210  [  192/  300]
per-ex loss: 0.648932  [  195/  300]
per-ex loss: 0.542837  [  198/  300]
per-ex loss: 0.465403  [  201/  300]
per-ex loss: 0.634732  [  204/  300]
per-ex loss: 0.567416  [  207/  300]
per-ex loss: 0.496964  [  210/  300]
per-ex loss: 0.481416  [  213/  300]
per-ex loss: 0.439105  [  216/  300]
per-ex loss: 0.572458  [  219/  300]
per-ex loss: 0.515765  [  222/  300]
per-ex loss: 0.550936  [  225/  300]
per-ex loss: 0.625899  [  228/  300]
per-ex loss: 0.509706  [  231/  300]
per-ex loss: 0.550159  [  234/  300]
per-ex loss: 0.488460  [  237/  300]
per-ex loss: 0.569222  [  240/  300]
per-ex loss: 0.450464  [  243/  300]
per-ex loss: 0.468937  [  246/  300]
per-ex loss: 0.539867  [  249/  300]
per-ex loss: 0.520027  [  252/  300]
per-ex loss: 0.475432  [  255/  300]
per-ex loss: 0.516745  [  258/  300]
per-ex loss: 0.594486  [  261/  300]
per-ex loss: 0.549665  [  264/  300]
per-ex loss: 0.579244  [  267/  300]
per-ex loss: 0.414664  [  270/  300]
per-ex loss: 0.564728  [  273/  300]
per-ex loss: 0.564029  [  276/  300]
per-ex loss: 0.472865  [  279/  300]
per-ex loss: 0.513778  [  282/  300]
per-ex loss: 0.558315  [  285/  300]
per-ex loss: 0.493721  [  288/  300]
per-ex loss: 0.675422  [  291/  300]
per-ex loss: 0.429155  [  294/  300]
per-ex loss: 0.484859  [  297/  300]
per-ex loss: 0.430828  [  300/  300]
Train Error: Avg loss: 0.52281260
validation Error: 
 Avg loss: 0.50556495 
 F1: 0.477276 
 Precision: 0.419200 
 Recall: 0.554031
 IoU: 0.313435

test Error: 
 Avg loss: 0.47482324 
 F1: 0.526241 
 Precision: 0.465541 
 Recall: 0.605142
 IoU: 0.357074

We have finished training iteration 83
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_81_.pth
per-ex loss: 0.456297  [    3/  300]
per-ex loss: 0.598667  [    6/  300]
per-ex loss: 0.615912  [    9/  300]
per-ex loss: 0.513260  [   12/  300]
per-ex loss: 0.496064  [   15/  300]
per-ex loss: 0.471201  [   18/  300]
per-ex loss: 0.658245  [   21/  300]
per-ex loss: 0.629694  [   24/  300]
per-ex loss: 0.574906  [   27/  300]
per-ex loss: 0.464320  [   30/  300]
per-ex loss: 0.421246  [   33/  300]
per-ex loss: 0.464083  [   36/  300]
per-ex loss: 0.525668  [   39/  300]
per-ex loss: 0.469171  [   42/  300]
per-ex loss: 0.475735  [   45/  300]
per-ex loss: 0.534969  [   48/  300]
per-ex loss: 0.446848  [   51/  300]
per-ex loss: 0.526960  [   54/  300]
per-ex loss: 0.461214  [   57/  300]
per-ex loss: 0.529211  [   60/  300]
per-ex loss: 0.500601  [   63/  300]
per-ex loss: 0.573137  [   66/  300]
per-ex loss: 0.603469  [   69/  300]
per-ex loss: 0.570572  [   72/  300]
per-ex loss: 0.563239  [   75/  300]
per-ex loss: 0.418883  [   78/  300]
per-ex loss: 0.459033  [   81/  300]
per-ex loss: 0.610345  [   84/  300]
per-ex loss: 0.666123  [   87/  300]
per-ex loss: 0.511762  [   90/  300]
per-ex loss: 0.507341  [   93/  300]
per-ex loss: 0.514734  [   96/  300]
per-ex loss: 0.505353  [   99/  300]
per-ex loss: 0.441798  [  102/  300]
per-ex loss: 0.469301  [  105/  300]
per-ex loss: 0.461350  [  108/  300]
per-ex loss: 0.566545  [  111/  300]
per-ex loss: 0.581227  [  114/  300]
per-ex loss: 0.527046  [  117/  300]
per-ex loss: 0.485442  [  120/  300]
per-ex loss: 0.604846  [  123/  300]
per-ex loss: 0.574261  [  126/  300]
per-ex loss: 0.394111  [  129/  300]
per-ex loss: 0.519562  [  132/  300]
per-ex loss: 0.412269  [  135/  300]
per-ex loss: 0.595476  [  138/  300]
per-ex loss: 0.598152  [  141/  300]
per-ex loss: 0.651929  [  144/  300]
per-ex loss: 0.501509  [  147/  300]
per-ex loss: 0.505875  [  150/  300]
per-ex loss: 0.499727  [  153/  300]
per-ex loss: 0.615265  [  156/  300]
per-ex loss: 0.454700  [  159/  300]
per-ex loss: 0.610673  [  162/  300]
per-ex loss: 0.570655  [  165/  300]
per-ex loss: 0.532378  [  168/  300]
per-ex loss: 0.446617  [  171/  300]
per-ex loss: 0.489196  [  174/  300]
per-ex loss: 0.475296  [  177/  300]
per-ex loss: 0.463853  [  180/  300]
per-ex loss: 0.459975  [  183/  300]
per-ex loss: 0.552697  [  186/  300]
per-ex loss: 0.526723  [  189/  300]
per-ex loss: 0.461794  [  192/  300]
per-ex loss: 0.649634  [  195/  300]
per-ex loss: 0.514742  [  198/  300]
per-ex loss: 0.433012  [  201/  300]
per-ex loss: 0.530775  [  204/  300]
per-ex loss: 0.574574  [  207/  300]
per-ex loss: 0.536367  [  210/  300]
per-ex loss: 0.535139  [  213/  300]
per-ex loss: 0.473321  [  216/  300]
per-ex loss: 0.455172  [  219/  300]
per-ex loss: 0.560398  [  222/  300]
per-ex loss: 0.538400  [  225/  300]
per-ex loss: 0.561778  [  228/  300]
per-ex loss: 0.558867  [  231/  300]
per-ex loss: 0.550247  [  234/  300]
per-ex loss: 0.484016  [  237/  300]
per-ex loss: 0.475482  [  240/  300]
per-ex loss: 0.515375  [  243/  300]
per-ex loss: 0.557375  [  246/  300]
per-ex loss: 0.678351  [  249/  300]
per-ex loss: 0.559618  [  252/  300]
per-ex loss: 0.490790  [  255/  300]
per-ex loss: 0.498134  [  258/  300]
per-ex loss: 0.449895  [  261/  300]
per-ex loss: 0.550486  [  264/  300]
per-ex loss: 0.526258  [  267/  300]
per-ex loss: 0.432069  [  270/  300]
per-ex loss: 0.445770  [  273/  300]
per-ex loss: 0.553468  [  276/  300]
per-ex loss: 0.537319  [  279/  300]
per-ex loss: 0.651387  [  282/  300]
per-ex loss: 0.503905  [  285/  300]
per-ex loss: 0.453432  [  288/  300]
per-ex loss: 0.535002  [  291/  300]
per-ex loss: 0.592675  [  294/  300]
per-ex loss: 0.551958  [  297/  300]
per-ex loss: 0.560616  [  300/  300]
Train Error: Avg loss: 0.52464314
validation Error: 
 Avg loss: 0.50387524 
 F1: 0.468317 
 Precision: 0.375997 
 Recall: 0.620728
 IoU: 0.305754

test Error: 
 Avg loss: 0.47455615 
 F1: 0.526589 
 Precision: 0.441770 
 Recall: 0.651718
 IoU: 0.357395

We have finished training iteration 84
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_48_.pth
per-ex loss: 0.466645  [    3/  300]
per-ex loss: 0.417157  [    6/  300]
per-ex loss: 0.474845  [    9/  300]
per-ex loss: 0.553385  [   12/  300]
per-ex loss: 0.499753  [   15/  300]
per-ex loss: 0.495382  [   18/  300]
per-ex loss: 0.552288  [   21/  300]
per-ex loss: 0.519087  [   24/  300]
per-ex loss: 0.548992  [   27/  300]
per-ex loss: 0.658623  [   30/  300]
per-ex loss: 0.569160  [   33/  300]
per-ex loss: 0.520710  [   36/  300]
per-ex loss: 0.478085  [   39/  300]
per-ex loss: 0.532292  [   42/  300]
per-ex loss: 0.561830  [   45/  300]
per-ex loss: 0.529829  [   48/  300]
per-ex loss: 0.512605  [   51/  300]
per-ex loss: 0.643263  [   54/  300]
per-ex loss: 0.591346  [   57/  300]
per-ex loss: 0.475091  [   60/  300]
per-ex loss: 0.475417  [   63/  300]
per-ex loss: 0.449235  [   66/  300]
per-ex loss: 0.622326  [   69/  300]
per-ex loss: 0.563318  [   72/  300]
per-ex loss: 0.409703  [   75/  300]
per-ex loss: 0.528565  [   78/  300]
per-ex loss: 0.420735  [   81/  300]
per-ex loss: 0.605524  [   84/  300]
per-ex loss: 0.460903  [   87/  300]
per-ex loss: 0.655054  [   90/  300]
per-ex loss: 0.458953  [   93/  300]
per-ex loss: 0.495866  [   96/  300]
per-ex loss: 0.412165  [   99/  300]
per-ex loss: 0.619333  [  102/  300]
per-ex loss: 0.427607  [  105/  300]
per-ex loss: 0.519296  [  108/  300]
per-ex loss: 0.381435  [  111/  300]
per-ex loss: 0.567441  [  114/  300]
per-ex loss: 0.492839  [  117/  300]
per-ex loss: 0.551731  [  120/  300]
per-ex loss: 0.431720  [  123/  300]
per-ex loss: 0.439764  [  126/  300]
per-ex loss: 0.662598  [  129/  300]
per-ex loss: 0.525086  [  132/  300]
per-ex loss: 0.540846  [  135/  300]
per-ex loss: 0.523235  [  138/  300]
per-ex loss: 0.555429  [  141/  300]
per-ex loss: 0.489173  [  144/  300]
per-ex loss: 0.490731  [  147/  300]
per-ex loss: 0.456569  [  150/  300]
per-ex loss: 0.582184  [  153/  300]
per-ex loss: 0.630557  [  156/  300]
per-ex loss: 0.608227  [  159/  300]
per-ex loss: 0.584923  [  162/  300]
per-ex loss: 0.456030  [  165/  300]
per-ex loss: 0.499526  [  168/  300]
per-ex loss: 0.614025  [  171/  300]
per-ex loss: 0.628821  [  174/  300]
per-ex loss: 0.445547  [  177/  300]
per-ex loss: 0.468704  [  180/  300]
per-ex loss: 0.453130  [  183/  300]
per-ex loss: 0.469537  [  186/  300]
per-ex loss: 0.589041  [  189/  300]
per-ex loss: 0.628551  [  192/  300]
per-ex loss: 0.453544  [  195/  300]
per-ex loss: 0.545345  [  198/  300]
per-ex loss: 0.653213  [  201/  300]
per-ex loss: 0.558034  [  204/  300]
per-ex loss: 0.538826  [  207/  300]
per-ex loss: 0.493852  [  210/  300]
per-ex loss: 0.653079  [  213/  300]
per-ex loss: 0.513751  [  216/  300]
per-ex loss: 0.501637  [  219/  300]
per-ex loss: 0.424874  [  222/  300]
per-ex loss: 0.535799  [  225/  300]
per-ex loss: 0.548803  [  228/  300]
per-ex loss: 0.479688  [  231/  300]
per-ex loss: 0.689837  [  234/  300]
per-ex loss: 0.474050  [  237/  300]
per-ex loss: 0.422617  [  240/  300]
per-ex loss: 0.465725  [  243/  300]
per-ex loss: 0.485893  [  246/  300]
per-ex loss: 0.524058  [  249/  300]
per-ex loss: 0.466321  [  252/  300]
per-ex loss: 0.476650  [  255/  300]
per-ex loss: 0.498002  [  258/  300]
per-ex loss: 0.484131  [  261/  300]
per-ex loss: 0.550456  [  264/  300]
per-ex loss: 0.528297  [  267/  300]
per-ex loss: 0.581777  [  270/  300]
per-ex loss: 0.492778  [  273/  300]
per-ex loss: 0.449484  [  276/  300]
per-ex loss: 0.537632  [  279/  300]
per-ex loss: 0.438530  [  282/  300]
per-ex loss: 0.435400  [  285/  300]
per-ex loss: 0.500787  [  288/  300]
per-ex loss: 0.660453  [  291/  300]
per-ex loss: 0.611881  [  294/  300]
per-ex loss: 0.601914  [  297/  300]
per-ex loss: 0.466423  [  300/  300]
Train Error: Avg loss: 0.52235307
validation Error: 
 Avg loss: 0.51057261 
 F1: 0.480219 
 Precision: 0.433166 
 Recall: 0.538739
 IoU: 0.315979

test Error: 
 Avg loss: 0.45740545 
 F1: 0.543531 
 Precision: 0.518970 
 Recall: 0.570533
 IoU: 0.373184

We have finished training iteration 85
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_82_.pth
per-ex loss: 0.461191  [    3/  300]
per-ex loss: 0.567701  [    6/  300]
per-ex loss: 0.487507  [    9/  300]
per-ex loss: 0.507406  [   12/  300]
per-ex loss: 0.540076  [   15/  300]
per-ex loss: 0.553611  [   18/  300]
per-ex loss: 0.429822  [   21/  300]
per-ex loss: 0.440843  [   24/  300]
per-ex loss: 0.605618  [   27/  300]
per-ex loss: 0.529556  [   30/  300]
per-ex loss: 0.549103  [   33/  300]
per-ex loss: 0.449261  [   36/  300]
per-ex loss: 0.503661  [   39/  300]
per-ex loss: 0.586895  [   42/  300]
per-ex loss: 0.545268  [   45/  300]
per-ex loss: 0.669423  [   48/  300]
per-ex loss: 0.455129  [   51/  300]
per-ex loss: 0.380127  [   54/  300]
per-ex loss: 0.597669  [   57/  300]
per-ex loss: 0.501510  [   60/  300]
per-ex loss: 0.492511  [   63/  300]
per-ex loss: 0.480347  [   66/  300]
per-ex loss: 0.603220  [   69/  300]
per-ex loss: 0.417703  [   72/  300]
per-ex loss: 0.626898  [   75/  300]
per-ex loss: 0.559673  [   78/  300]
per-ex loss: 0.588081  [   81/  300]
per-ex loss: 0.468385  [   84/  300]
per-ex loss: 0.519984  [   87/  300]
per-ex loss: 0.496886  [   90/  300]
per-ex loss: 0.564872  [   93/  300]
per-ex loss: 0.472719  [   96/  300]
per-ex loss: 0.632855  [   99/  300]
per-ex loss: 0.630088  [  102/  300]
per-ex loss: 0.421741  [  105/  300]
per-ex loss: 0.490515  [  108/  300]
per-ex loss: 0.648360  [  111/  300]
per-ex loss: 0.556519  [  114/  300]
per-ex loss: 0.441162  [  117/  300]
per-ex loss: 0.553827  [  120/  300]
per-ex loss: 0.509761  [  123/  300]
per-ex loss: 0.451245  [  126/  300]
per-ex loss: 0.474704  [  129/  300]
per-ex loss: 0.436245  [  132/  300]
per-ex loss: 0.455215  [  135/  300]
per-ex loss: 0.709034  [  138/  300]
per-ex loss: 0.527660  [  141/  300]
per-ex loss: 0.505422  [  144/  300]
per-ex loss: 0.487171  [  147/  300]
per-ex loss: 0.516696  [  150/  300]
per-ex loss: 0.414416  [  153/  300]
per-ex loss: 0.695523  [  156/  300]
per-ex loss: 0.409785  [  159/  300]
per-ex loss: 0.636171  [  162/  300]
per-ex loss: 0.545894  [  165/  300]
per-ex loss: 0.592519  [  168/  300]
per-ex loss: 0.661958  [  171/  300]
per-ex loss: 0.595591  [  174/  300]
per-ex loss: 0.545404  [  177/  300]
per-ex loss: 0.542333  [  180/  300]
per-ex loss: 0.511851  [  183/  300]
per-ex loss: 0.458989  [  186/  300]
per-ex loss: 0.465655  [  189/  300]
per-ex loss: 0.525173  [  192/  300]
per-ex loss: 0.513331  [  195/  300]
per-ex loss: 0.559806  [  198/  300]
per-ex loss: 0.475879  [  201/  300]
per-ex loss: 0.400516  [  204/  300]
per-ex loss: 0.605309  [  207/  300]
per-ex loss: 0.578321  [  210/  300]
per-ex loss: 0.439320  [  213/  300]
per-ex loss: 0.450508  [  216/  300]
per-ex loss: 0.620065  [  219/  300]
per-ex loss: 0.586730  [  222/  300]
per-ex loss: 0.567932  [  225/  300]
per-ex loss: 0.554725  [  228/  300]
per-ex loss: 0.692831  [  231/  300]
per-ex loss: 0.569256  [  234/  300]
per-ex loss: 0.445174  [  237/  300]
per-ex loss: 0.591836  [  240/  300]
per-ex loss: 0.692160  [  243/  300]
per-ex loss: 0.467650  [  246/  300]
per-ex loss: 0.668580  [  249/  300]
per-ex loss: 0.563354  [  252/  300]
per-ex loss: 0.611217  [  255/  300]
per-ex loss: 0.456975  [  258/  300]
per-ex loss: 0.526661  [  261/  300]
per-ex loss: 0.464744  [  264/  300]
per-ex loss: 0.424348  [  267/  300]
per-ex loss: 0.483457  [  270/  300]
per-ex loss: 0.530938  [  273/  300]
per-ex loss: 0.494264  [  276/  300]
per-ex loss: 0.618124  [  279/  300]
per-ex loss: 0.568646  [  282/  300]
per-ex loss: 0.571464  [  285/  300]
per-ex loss: 0.432333  [  288/  300]
per-ex loss: 0.592079  [  291/  300]
per-ex loss: 0.498302  [  294/  300]
per-ex loss: 0.473636  [  297/  300]
per-ex loss: 0.519616  [  300/  300]
Train Error: Avg loss: 0.53012221
validation Error: 
 Avg loss: 0.50067747 
 F1: 0.481922 
 Precision: 0.439648 
 Recall: 0.533191
 IoU: 0.317455

test Error: 
 Avg loss: 0.45948136 
 F1: 0.541486 
 Precision: 0.493586 
 Recall: 0.599682
 IoU: 0.371258

We have finished training iteration 86
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_68_.pth
per-ex loss: 0.461163  [    3/  300]
per-ex loss: 0.455332  [    6/  300]
per-ex loss: 0.463350  [    9/  300]
per-ex loss: 0.592028  [   12/  300]
per-ex loss: 0.428531  [   15/  300]
per-ex loss: 0.584435  [   18/  300]
per-ex loss: 0.563617  [   21/  300]
per-ex loss: 0.624900  [   24/  300]
per-ex loss: 0.677689  [   27/  300]
per-ex loss: 0.609893  [   30/  300]
per-ex loss: 0.492816  [   33/  300]
per-ex loss: 0.534708  [   36/  300]
per-ex loss: 0.434482  [   39/  300]
per-ex loss: 0.448294  [   42/  300]
per-ex loss: 0.454400  [   45/  300]
per-ex loss: 0.430408  [   48/  300]
per-ex loss: 0.622448  [   51/  300]
per-ex loss: 0.498751  [   54/  300]
per-ex loss: 0.590884  [   57/  300]
per-ex loss: 0.712594  [   60/  300]
per-ex loss: 0.422299  [   63/  300]
per-ex loss: 0.482952  [   66/  300]
per-ex loss: 0.548024  [   69/  300]
per-ex loss: 0.551096  [   72/  300]
per-ex loss: 0.502715  [   75/  300]
per-ex loss: 0.491600  [   78/  300]
per-ex loss: 0.457714  [   81/  300]
per-ex loss: 0.544151  [   84/  300]
per-ex loss: 0.558176  [   87/  300]
per-ex loss: 0.461034  [   90/  300]
per-ex loss: 0.504594  [   93/  300]
per-ex loss: 0.468747  [   96/  300]
per-ex loss: 0.506136  [   99/  300]
per-ex loss: 0.475213  [  102/  300]
per-ex loss: 0.604856  [  105/  300]
per-ex loss: 0.520371  [  108/  300]
per-ex loss: 0.537764  [  111/  300]
per-ex loss: 0.499121  [  114/  300]
per-ex loss: 0.437789  [  117/  300]
per-ex loss: 0.608053  [  120/  300]
per-ex loss: 0.468966  [  123/  300]
per-ex loss: 0.438522  [  126/  300]
per-ex loss: 0.516694  [  129/  300]
per-ex loss: 0.486832  [  132/  300]
per-ex loss: 0.568930  [  135/  300]
per-ex loss: 0.526111  [  138/  300]
per-ex loss: 0.643283  [  141/  300]
per-ex loss: 0.655393  [  144/  300]
per-ex loss: 0.616687  [  147/  300]
per-ex loss: 0.438285  [  150/  300]
per-ex loss: 0.447812  [  153/  300]
per-ex loss: 0.563221  [  156/  300]
per-ex loss: 0.480853  [  159/  300]
per-ex loss: 0.491868  [  162/  300]
per-ex loss: 0.446830  [  165/  300]
per-ex loss: 0.499891  [  168/  300]
per-ex loss: 0.510087  [  171/  300]
per-ex loss: 0.577761  [  174/  300]
per-ex loss: 0.443181  [  177/  300]
per-ex loss: 0.480191  [  180/  300]
per-ex loss: 0.503515  [  183/  300]
per-ex loss: 0.632839  [  186/  300]
per-ex loss: 0.484333  [  189/  300]
per-ex loss: 0.489418  [  192/  300]
per-ex loss: 0.548977  [  195/  300]
per-ex loss: 0.448590  [  198/  300]
per-ex loss: 0.421038  [  201/  300]
per-ex loss: 0.535888  [  204/  300]
per-ex loss: 0.543489  [  207/  300]
per-ex loss: 0.578261  [  210/  300]
per-ex loss: 0.481970  [  213/  300]
per-ex loss: 0.674999  [  216/  300]
per-ex loss: 0.592045  [  219/  300]
per-ex loss: 0.490908  [  222/  300]
per-ex loss: 0.479980  [  225/  300]
per-ex loss: 0.429296  [  228/  300]
per-ex loss: 0.485229  [  231/  300]
per-ex loss: 0.524315  [  234/  300]
per-ex loss: 0.472869  [  237/  300]
per-ex loss: 0.494708  [  240/  300]
per-ex loss: 0.540671  [  243/  300]
per-ex loss: 0.569703  [  246/  300]
per-ex loss: 0.418914  [  249/  300]
per-ex loss: 0.671849  [  252/  300]
per-ex loss: 0.592613  [  255/  300]
per-ex loss: 0.546182  [  258/  300]
per-ex loss: 0.618316  [  261/  300]
per-ex loss: 0.428978  [  264/  300]
per-ex loss: 0.573533  [  267/  300]
per-ex loss: 0.468660  [  270/  300]
per-ex loss: 0.469545  [  273/  300]
per-ex loss: 0.532244  [  276/  300]
per-ex loss: 0.591493  [  279/  300]
per-ex loss: 0.450669  [  282/  300]
per-ex loss: 0.478818  [  285/  300]
per-ex loss: 0.557341  [  288/  300]
per-ex loss: 0.507464  [  291/  300]
per-ex loss: 0.626758  [  294/  300]
per-ex loss: 0.464257  [  297/  300]
per-ex loss: 0.575978  [  300/  300]
Train Error: Avg loss: 0.52161178
validation Error: 
 Avg loss: 0.51510479 
 F1: 0.475826 
 Precision: 0.425746 
 Recall: 0.539258
 IoU: 0.312186

test Error: 
 Avg loss: 0.45818663 
 F1: 0.542487 
 Precision: 0.504550 
 Recall: 0.586592
 IoU: 0.372200

We have finished training iteration 87
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_85_.pth
per-ex loss: 0.393953  [    3/  300]
per-ex loss: 0.583168  [    6/  300]
per-ex loss: 0.512133  [    9/  300]
per-ex loss: 0.459731  [   12/  300]
per-ex loss: 0.393521  [   15/  300]
per-ex loss: 0.628021  [   18/  300]
per-ex loss: 0.579330  [   21/  300]
per-ex loss: 0.568205  [   24/  300]
per-ex loss: 0.610662  [   27/  300]
per-ex loss: 0.483659  [   30/  300]
per-ex loss: 0.601793  [   33/  300]
per-ex loss: 0.488097  [   36/  300]
per-ex loss: 0.518541  [   39/  300]
per-ex loss: 0.556612  [   42/  300]
per-ex loss: 0.500764  [   45/  300]
per-ex loss: 0.526576  [   48/  300]
per-ex loss: 0.512884  [   51/  300]
per-ex loss: 0.529198  [   54/  300]
per-ex loss: 0.513014  [   57/  300]
per-ex loss: 0.454660  [   60/  300]
per-ex loss: 0.494787  [   63/  300]
per-ex loss: 0.562013  [   66/  300]
per-ex loss: 0.523935  [   69/  300]
per-ex loss: 0.525042  [   72/  300]
per-ex loss: 0.409541  [   75/  300]
per-ex loss: 0.450842  [   78/  300]
per-ex loss: 0.434084  [   81/  300]
per-ex loss: 0.618207  [   84/  300]
per-ex loss: 0.627133  [   87/  300]
per-ex loss: 0.505731  [   90/  300]
per-ex loss: 0.559877  [   93/  300]
per-ex loss: 0.479762  [   96/  300]
per-ex loss: 0.509186  [   99/  300]
per-ex loss: 0.526312  [  102/  300]
per-ex loss: 0.466047  [  105/  300]
per-ex loss: 0.539533  [  108/  300]
per-ex loss: 0.508790  [  111/  300]
per-ex loss: 0.624319  [  114/  300]
per-ex loss: 0.668621  [  117/  300]
per-ex loss: 0.537090  [  120/  300]
per-ex loss: 0.564855  [  123/  300]
per-ex loss: 0.492959  [  126/  300]
per-ex loss: 0.483463  [  129/  300]
per-ex loss: 0.476338  [  132/  300]
per-ex loss: 0.415276  [  135/  300]
per-ex loss: 0.618148  [  138/  300]
per-ex loss: 0.497057  [  141/  300]
per-ex loss: 0.502945  [  144/  300]
per-ex loss: 0.496135  [  147/  300]
per-ex loss: 0.520384  [  150/  300]
per-ex loss: 0.547250  [  153/  300]
per-ex loss: 0.551849  [  156/  300]
per-ex loss: 0.483578  [  159/  300]
per-ex loss: 0.654677  [  162/  300]
per-ex loss: 0.507143  [  165/  300]
per-ex loss: 0.525293  [  168/  300]
per-ex loss: 0.564422  [  171/  300]
per-ex loss: 0.377136  [  174/  300]
per-ex loss: 0.551216  [  177/  300]
per-ex loss: 0.504423  [  180/  300]
per-ex loss: 0.538077  [  183/  300]
per-ex loss: 0.606815  [  186/  300]
per-ex loss: 0.505992  [  189/  300]
per-ex loss: 0.601815  [  192/  300]
per-ex loss: 0.534450  [  195/  300]
per-ex loss: 0.505566  [  198/  300]
per-ex loss: 0.505125  [  201/  300]
per-ex loss: 0.455822  [  204/  300]
per-ex loss: 0.630234  [  207/  300]
per-ex loss: 0.497975  [  210/  300]
per-ex loss: 0.486356  [  213/  300]
per-ex loss: 0.550283  [  216/  300]
per-ex loss: 0.479425  [  219/  300]
per-ex loss: 0.640989  [  222/  300]
per-ex loss: 0.467998  [  225/  300]
per-ex loss: 0.556007  [  228/  300]
per-ex loss: 0.485509  [  231/  300]
per-ex loss: 0.551602  [  234/  300]
per-ex loss: 0.548449  [  237/  300]
per-ex loss: 0.471599  [  240/  300]
per-ex loss: 0.553101  [  243/  300]
per-ex loss: 0.501873  [  246/  300]
per-ex loss: 0.480965  [  249/  300]
per-ex loss: 0.432892  [  252/  300]
per-ex loss: 0.500262  [  255/  300]
per-ex loss: 0.424502  [  258/  300]
per-ex loss: 0.575242  [  261/  300]
per-ex loss: 0.425637  [  264/  300]
per-ex loss: 0.664888  [  267/  300]
per-ex loss: 0.597842  [  270/  300]
per-ex loss: 0.434969  [  273/  300]
per-ex loss: 0.544400  [  276/  300]
per-ex loss: 0.440394  [  279/  300]
per-ex loss: 0.595301  [  282/  300]
per-ex loss: 0.670075  [  285/  300]
per-ex loss: 0.462453  [  288/  300]
per-ex loss: 0.500275  [  291/  300]
per-ex loss: 0.546891  [  294/  300]
per-ex loss: 0.512306  [  297/  300]
per-ex loss: 0.425015  [  300/  300]
Train Error: Avg loss: 0.52229291
validation Error: 
 Avg loss: 0.50365909 
 F1: 0.482547 
 Precision: 0.430829 
 Recall: 0.548374
 IoU: 0.317998

test Error: 
 Avg loss: 0.45878470 
 F1: 0.542090 
 Precision: 0.495296 
 Recall: 0.598650
 IoU: 0.371827

We have finished training iteration 88
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_83_.pth
per-ex loss: 0.480492  [    3/  300]
per-ex loss: 0.513573  [    6/  300]
per-ex loss: 0.554592  [    9/  300]
per-ex loss: 0.503555  [   12/  300]
per-ex loss: 0.619320  [   15/  300]
per-ex loss: 0.584429  [   18/  300]
per-ex loss: 0.497561  [   21/  300]
per-ex loss: 0.476075  [   24/  300]
per-ex loss: 0.415322  [   27/  300]
per-ex loss: 0.468437  [   30/  300]
per-ex loss: 0.550136  [   33/  300]
per-ex loss: 0.532605  [   36/  300]
per-ex loss: 0.477808  [   39/  300]
per-ex loss: 0.452403  [   42/  300]
per-ex loss: 0.578284  [   45/  300]
per-ex loss: 0.578384  [   48/  300]
per-ex loss: 0.521954  [   51/  300]
per-ex loss: 0.395473  [   54/  300]
per-ex loss: 0.517814  [   57/  300]
per-ex loss: 0.572676  [   60/  300]
per-ex loss: 0.611542  [   63/  300]
per-ex loss: 0.568738  [   66/  300]
per-ex loss: 0.485048  [   69/  300]
per-ex loss: 0.536157  [   72/  300]
per-ex loss: 0.588062  [   75/  300]
per-ex loss: 0.435004  [   78/  300]
per-ex loss: 0.512647  [   81/  300]
per-ex loss: 0.431011  [   84/  300]
per-ex loss: 0.472314  [   87/  300]
per-ex loss: 0.590767  [   90/  300]
per-ex loss: 0.431138  [   93/  300]
per-ex loss: 0.711740  [   96/  300]
per-ex loss: 0.462464  [   99/  300]
per-ex loss: 0.414637  [  102/  300]
per-ex loss: 0.491181  [  105/  300]
per-ex loss: 0.517752  [  108/  300]
per-ex loss: 0.486265  [  111/  300]
per-ex loss: 0.593835  [  114/  300]
per-ex loss: 0.522976  [  117/  300]
per-ex loss: 0.612651  [  120/  300]
per-ex loss: 0.571568  [  123/  300]
per-ex loss: 0.567200  [  126/  300]
per-ex loss: 0.588901  [  129/  300]
per-ex loss: 0.508053  [  132/  300]
per-ex loss: 0.584202  [  135/  300]
per-ex loss: 0.420953  [  138/  300]
per-ex loss: 0.513042  [  141/  300]
per-ex loss: 0.509707  [  144/  300]
per-ex loss: 0.442125  [  147/  300]
per-ex loss: 0.622183  [  150/  300]
per-ex loss: 0.454582  [  153/  300]
per-ex loss: 0.554881  [  156/  300]
per-ex loss: 0.473037  [  159/  300]
per-ex loss: 0.410868  [  162/  300]
per-ex loss: 0.722825  [  165/  300]
per-ex loss: 0.477866  [  168/  300]
per-ex loss: 0.456543  [  171/  300]
per-ex loss: 0.474252  [  174/  300]
per-ex loss: 0.670215  [  177/  300]
per-ex loss: 0.518201  [  180/  300]
per-ex loss: 0.452159  [  183/  300]
per-ex loss: 0.657680  [  186/  300]
per-ex loss: 0.534400  [  189/  300]
per-ex loss: 0.504233  [  192/  300]
per-ex loss: 0.617247  [  195/  300]
per-ex loss: 0.609074  [  198/  300]
per-ex loss: 0.410690  [  201/  300]
per-ex loss: 0.418507  [  204/  300]
per-ex loss: 0.541912  [  207/  300]
per-ex loss: 0.438084  [  210/  300]
per-ex loss: 0.433845  [  213/  300]
per-ex loss: 0.601102  [  216/  300]
per-ex loss: 0.509694  [  219/  300]
per-ex loss: 0.499870  [  222/  300]
per-ex loss: 0.594366  [  225/  300]
per-ex loss: 0.539069  [  228/  300]
per-ex loss: 0.383010  [  231/  300]
per-ex loss: 0.484042  [  234/  300]
per-ex loss: 0.486466  [  237/  300]
per-ex loss: 0.500435  [  240/  300]
per-ex loss: 0.484562  [  243/  300]
per-ex loss: 0.501335  [  246/  300]
per-ex loss: 0.468112  [  249/  300]
per-ex loss: 0.441946  [  252/  300]
per-ex loss: 0.580467  [  255/  300]
per-ex loss: 0.658095  [  258/  300]
per-ex loss: 0.633008  [  261/  300]
per-ex loss: 0.541676  [  264/  300]
per-ex loss: 0.564474  [  267/  300]
per-ex loss: 0.593667  [  270/  300]
per-ex loss: 0.412522  [  273/  300]
per-ex loss: 0.640614  [  276/  300]
per-ex loss: 0.531359  [  279/  300]
per-ex loss: 0.583747  [  282/  300]
per-ex loss: 0.491616  [  285/  300]
per-ex loss: 0.484161  [  288/  300]
per-ex loss: 0.486976  [  291/  300]
per-ex loss: 0.457114  [  294/  300]
per-ex loss: 0.620481  [  297/  300]
per-ex loss: 0.438892  [  300/  300]
Train Error: Avg loss: 0.52138736
validation Error: 
 Avg loss: 0.50555778 
 F1: 0.481238 
 Precision: 0.435326 
 Recall: 0.537975
 IoU: 0.316862

test Error: 
 Avg loss: 0.46441078 
 F1: 0.536162 
 Precision: 0.489174 
 Recall: 0.593137
 IoU: 0.366271

We have finished training iteration 89
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_87_.pth
per-ex loss: 0.506750  [    3/  300]
per-ex loss: 0.409591  [    6/  300]
per-ex loss: 0.500648  [    9/  300]
per-ex loss: 0.518893  [   12/  300]
per-ex loss: 0.513175  [   15/  300]
per-ex loss: 0.598428  [   18/  300]
per-ex loss: 0.491472  [   21/  300]
per-ex loss: 0.495722  [   24/  300]
per-ex loss: 0.417387  [   27/  300]
per-ex loss: 0.474344  [   30/  300]
per-ex loss: 0.469386  [   33/  300]
per-ex loss: 0.555659  [   36/  300]
per-ex loss: 0.548733  [   39/  300]
per-ex loss: 0.406641  [   42/  300]
per-ex loss: 0.477088  [   45/  300]
per-ex loss: 0.422284  [   48/  300]
per-ex loss: 0.576597  [   51/  300]
per-ex loss: 0.638349  [   54/  300]
per-ex loss: 0.495526  [   57/  300]
per-ex loss: 0.513966  [   60/  300]
per-ex loss: 0.466744  [   63/  300]
per-ex loss: 0.630156  [   66/  300]
per-ex loss: 0.552113  [   69/  300]
per-ex loss: 0.609609  [   72/  300]
per-ex loss: 0.457567  [   75/  300]
per-ex loss: 0.483531  [   78/  300]
per-ex loss: 0.680787  [   81/  300]
per-ex loss: 0.520676  [   84/  300]
per-ex loss: 0.565703  [   87/  300]
per-ex loss: 0.422245  [   90/  300]
per-ex loss: 0.461983  [   93/  300]
per-ex loss: 0.496604  [   96/  300]
per-ex loss: 0.546107  [   99/  300]
per-ex loss: 0.593606  [  102/  300]
per-ex loss: 0.637524  [  105/  300]
per-ex loss: 0.498791  [  108/  300]
per-ex loss: 0.509585  [  111/  300]
per-ex loss: 0.612514  [  114/  300]
per-ex loss: 0.429058  [  117/  300]
per-ex loss: 0.611580  [  120/  300]
per-ex loss: 0.708977  [  123/  300]
per-ex loss: 0.497576  [  126/  300]
per-ex loss: 0.557177  [  129/  300]
per-ex loss: 0.560392  [  132/  300]
per-ex loss: 0.441804  [  135/  300]
per-ex loss: 0.641429  [  138/  300]
per-ex loss: 0.625399  [  141/  300]
per-ex loss: 0.565615  [  144/  300]
per-ex loss: 0.481559  [  147/  300]
per-ex loss: 0.456620  [  150/  300]
per-ex loss: 0.543268  [  153/  300]
per-ex loss: 0.459591  [  156/  300]
per-ex loss: 0.513566  [  159/  300]
per-ex loss: 0.474689  [  162/  300]
per-ex loss: 0.572344  [  165/  300]
per-ex loss: 0.420647  [  168/  300]
per-ex loss: 0.642818  [  171/  300]
per-ex loss: 0.514883  [  174/  300]
per-ex loss: 0.602517  [  177/  300]
per-ex loss: 0.531556  [  180/  300]
per-ex loss: 0.629764  [  183/  300]
per-ex loss: 0.570723  [  186/  300]
per-ex loss: 0.523905  [  189/  300]
per-ex loss: 0.564952  [  192/  300]
per-ex loss: 0.435593  [  195/  300]
per-ex loss: 0.564906  [  198/  300]
per-ex loss: 0.565141  [  201/  300]
per-ex loss: 0.610792  [  204/  300]
per-ex loss: 0.552753  [  207/  300]
per-ex loss: 0.554219  [  210/  300]
per-ex loss: 0.442070  [  213/  300]
per-ex loss: 0.436089  [  216/  300]
per-ex loss: 0.463152  [  219/  300]
per-ex loss: 0.492773  [  222/  300]
per-ex loss: 0.537697  [  225/  300]
per-ex loss: 0.542998  [  228/  300]
per-ex loss: 0.574525  [  231/  300]
per-ex loss: 0.464355  [  234/  300]
per-ex loss: 0.433199  [  237/  300]
per-ex loss: 0.431028  [  240/  300]
per-ex loss: 0.494541  [  243/  300]
per-ex loss: 0.535883  [  246/  300]
per-ex loss: 0.514058  [  249/  300]
per-ex loss: 0.655773  [  252/  300]
per-ex loss: 0.501098  [  255/  300]
per-ex loss: 0.481591  [  258/  300]
per-ex loss: 0.555547  [  261/  300]
per-ex loss: 0.443466  [  264/  300]
per-ex loss: 0.534924  [  267/  300]
per-ex loss: 0.559308  [  270/  300]
per-ex loss: 0.405869  [  273/  300]
per-ex loss: 0.424221  [  276/  300]
per-ex loss: 0.488018  [  279/  300]
per-ex loss: 0.486462  [  282/  300]
per-ex loss: 0.496313  [  285/  300]
per-ex loss: 0.403307  [  288/  300]
per-ex loss: 0.609089  [  291/  300]
per-ex loss: 0.415949  [  294/  300]
per-ex loss: 0.505778  [  297/  300]
per-ex loss: 0.615356  [  300/  300]
Train Error: Avg loss: 0.52146736
validation Error: 
 Avg loss: 0.50962871 
 F1: 0.467235 
 Precision: 0.376162 
 Recall: 0.616494
 IoU: 0.304831

test Error: 
 Avg loss: 0.47129512 
 F1: 0.529735 
 Precision: 0.440908 
 Recall: 0.663383
 IoU: 0.360299

We have finished training iteration 90
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_84_.pth
per-ex loss: 0.430623  [    3/  300]
per-ex loss: 0.518397  [    6/  300]
per-ex loss: 0.462087  [    9/  300]
per-ex loss: 0.626034  [   12/  300]
per-ex loss: 0.664784  [   15/  300]
per-ex loss: 0.563462  [   18/  300]
per-ex loss: 0.548982  [   21/  300]
per-ex loss: 0.497266  [   24/  300]
per-ex loss: 0.585262  [   27/  300]
per-ex loss: 0.566007  [   30/  300]
per-ex loss: 0.460009  [   33/  300]
per-ex loss: 0.424977  [   36/  300]
per-ex loss: 0.473357  [   39/  300]
per-ex loss: 0.503093  [   42/  300]
per-ex loss: 0.493511  [   45/  300]
per-ex loss: 0.473583  [   48/  300]
per-ex loss: 0.485546  [   51/  300]
per-ex loss: 0.522060  [   54/  300]
per-ex loss: 0.461306  [   57/  300]
per-ex loss: 0.439813  [   60/  300]
per-ex loss: 0.478009  [   63/  300]
per-ex loss: 0.484670  [   66/  300]
per-ex loss: 0.463381  [   69/  300]
per-ex loss: 0.605462  [   72/  300]
per-ex loss: 0.543617  [   75/  300]
per-ex loss: 0.506172  [   78/  300]
per-ex loss: 0.558261  [   81/  300]
per-ex loss: 0.518710  [   84/  300]
per-ex loss: 0.584602  [   87/  300]
per-ex loss: 0.585386  [   90/  300]
per-ex loss: 0.426727  [   93/  300]
per-ex loss: 0.575009  [   96/  300]
per-ex loss: 0.495625  [   99/  300]
per-ex loss: 0.427237  [  102/  300]
per-ex loss: 0.621753  [  105/  300]
per-ex loss: 0.559553  [  108/  300]
per-ex loss: 0.472177  [  111/  300]
per-ex loss: 0.512599  [  114/  300]
per-ex loss: 0.427123  [  117/  300]
per-ex loss: 0.545459  [  120/  300]
per-ex loss: 0.413678  [  123/  300]
per-ex loss: 0.652526  [  126/  300]
per-ex loss: 0.456130  [  129/  300]
per-ex loss: 0.488340  [  132/  300]
per-ex loss: 0.438881  [  135/  300]
per-ex loss: 0.447549  [  138/  300]
per-ex loss: 0.483374  [  141/  300]
per-ex loss: 0.596989  [  144/  300]
per-ex loss: 0.620695  [  147/  300]
per-ex loss: 0.533677  [  150/  300]
per-ex loss: 0.546124  [  153/  300]
per-ex loss: 0.515711  [  156/  300]
per-ex loss: 0.639639  [  159/  300]
per-ex loss: 0.476092  [  162/  300]
per-ex loss: 0.569569  [  165/  300]
per-ex loss: 0.648441  [  168/  300]
per-ex loss: 0.494348  [  171/  300]
per-ex loss: 0.534433  [  174/  300]
per-ex loss: 0.493995  [  177/  300]
per-ex loss: 0.517968  [  180/  300]
per-ex loss: 0.515765  [  183/  300]
per-ex loss: 0.654199  [  186/  300]
per-ex loss: 0.514583  [  189/  300]
per-ex loss: 0.406823  [  192/  300]
per-ex loss: 0.512114  [  195/  300]
per-ex loss: 0.583873  [  198/  300]
per-ex loss: 0.536611  [  201/  300]
per-ex loss: 0.607041  [  204/  300]
per-ex loss: 0.495967  [  207/  300]
per-ex loss: 0.594683  [  210/  300]
per-ex loss: 0.412893  [  213/  300]
per-ex loss: 0.493397  [  216/  300]
per-ex loss: 0.457253  [  219/  300]
per-ex loss: 0.669421  [  222/  300]
per-ex loss: 0.453599  [  225/  300]
per-ex loss: 0.474692  [  228/  300]
per-ex loss: 0.511178  [  231/  300]
per-ex loss: 0.434291  [  234/  300]
per-ex loss: 0.589884  [  237/  300]
per-ex loss: 0.464413  [  240/  300]
per-ex loss: 0.426818  [  243/  300]
per-ex loss: 0.592036  [  246/  300]
per-ex loss: 0.521719  [  249/  300]
per-ex loss: 0.495113  [  252/  300]
per-ex loss: 0.505918  [  255/  300]
per-ex loss: 0.429489  [  258/  300]
per-ex loss: 0.547142  [  261/  300]
per-ex loss: 0.655129  [  264/  300]
per-ex loss: 0.419176  [  267/  300]
per-ex loss: 0.523921  [  270/  300]
per-ex loss: 0.433860  [  273/  300]
per-ex loss: 0.665825  [  276/  300]
per-ex loss: 0.419204  [  279/  300]
per-ex loss: 0.502954  [  282/  300]
per-ex loss: 0.569734  [  285/  300]
per-ex loss: 0.552587  [  288/  300]
per-ex loss: 0.593681  [  291/  300]
per-ex loss: 0.583428  [  294/  300]
per-ex loss: 0.667715  [  297/  300]
per-ex loss: 0.429863  [  300/  300]
Train Error: Avg loss: 0.52077837
validation Error: 
 Avg loss: 0.50416219 
 F1: 0.475824 
 Precision: 0.400392 
 Recall: 0.586276
 IoU: 0.312185

test Error: 
 Avg loss: 0.46287423 
 F1: 0.537802 
 Precision: 0.462486 
 Recall: 0.642419
 IoU: 0.367803

We have finished training iteration 91
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_89_.pth
per-ex loss: 0.447216  [    3/  300]
per-ex loss: 0.433125  [    6/  300]
per-ex loss: 0.599834  [    9/  300]
per-ex loss: 0.540312  [   12/  300]
per-ex loss: 0.429803  [   15/  300]
per-ex loss: 0.462457  [   18/  300]
per-ex loss: 0.570925  [   21/  300]
per-ex loss: 0.491667  [   24/  300]
per-ex loss: 0.547092  [   27/  300]
per-ex loss: 0.548886  [   30/  300]
per-ex loss: 0.462962  [   33/  300]
per-ex loss: 0.567122  [   36/  300]
per-ex loss: 0.556361  [   39/  300]
per-ex loss: 0.492882  [   42/  300]
per-ex loss: 0.483827  [   45/  300]
per-ex loss: 0.470659  [   48/  300]
per-ex loss: 0.535623  [   51/  300]
per-ex loss: 0.509922  [   54/  300]
per-ex loss: 0.562653  [   57/  300]
per-ex loss: 0.465135  [   60/  300]
per-ex loss: 0.620780  [   63/  300]
per-ex loss: 0.620029  [   66/  300]
per-ex loss: 0.480264  [   69/  300]
per-ex loss: 0.424854  [   72/  300]
per-ex loss: 0.617210  [   75/  300]
per-ex loss: 0.472906  [   78/  300]
per-ex loss: 0.526698  [   81/  300]
per-ex loss: 0.537161  [   84/  300]
per-ex loss: 0.393168  [   87/  300]
per-ex loss: 0.533403  [   90/  300]
per-ex loss: 0.400438  [   93/  300]
per-ex loss: 0.626696  [   96/  300]
per-ex loss: 0.499081  [   99/  300]
per-ex loss: 0.547010  [  102/  300]
per-ex loss: 0.650479  [  105/  300]
per-ex loss: 0.621690  [  108/  300]
per-ex loss: 0.393321  [  111/  300]
per-ex loss: 0.583142  [  114/  300]
per-ex loss: 0.482975  [  117/  300]
per-ex loss: 0.523899  [  120/  300]
per-ex loss: 0.598269  [  123/  300]
per-ex loss: 0.595634  [  126/  300]
per-ex loss: 0.482396  [  129/  300]
per-ex loss: 0.554467  [  132/  300]
per-ex loss: 0.575567  [  135/  300]
per-ex loss: 0.504368  [  138/  300]
per-ex loss: 0.401981  [  141/  300]
per-ex loss: 0.457140  [  144/  300]
per-ex loss: 0.476565  [  147/  300]
per-ex loss: 0.660006  [  150/  300]
per-ex loss: 0.492447  [  153/  300]
per-ex loss: 0.459160  [  156/  300]
per-ex loss: 0.475567  [  159/  300]
per-ex loss: 0.410920  [  162/  300]
per-ex loss: 0.500855  [  165/  300]
per-ex loss: 0.464188  [  168/  300]
per-ex loss: 0.543422  [  171/  300]
per-ex loss: 0.486295  [  174/  300]
per-ex loss: 0.483671  [  177/  300]
per-ex loss: 0.547372  [  180/  300]
per-ex loss: 0.466165  [  183/  300]
per-ex loss: 0.502624  [  186/  300]
per-ex loss: 0.560954  [  189/  300]
per-ex loss: 0.463659  [  192/  300]
per-ex loss: 0.469954  [  195/  300]
per-ex loss: 0.538548  [  198/  300]
per-ex loss: 0.606601  [  201/  300]
per-ex loss: 0.544475  [  204/  300]
per-ex loss: 0.545949  [  207/  300]
per-ex loss: 0.478376  [  210/  300]
per-ex loss: 0.458329  [  213/  300]
per-ex loss: 0.619282  [  216/  300]
per-ex loss: 0.437226  [  219/  300]
per-ex loss: 0.470559  [  222/  300]
per-ex loss: 0.651617  [  225/  300]
per-ex loss: 0.466164  [  228/  300]
per-ex loss: 0.530354  [  231/  300]
per-ex loss: 0.471220  [  234/  300]
per-ex loss: 0.471163  [  237/  300]
per-ex loss: 0.479981  [  240/  300]
per-ex loss: 0.523678  [  243/  300]
per-ex loss: 0.548143  [  246/  300]
per-ex loss: 0.617095  [  249/  300]
per-ex loss: 0.652359  [  252/  300]
per-ex loss: 0.460522  [  255/  300]
per-ex loss: 0.434015  [  258/  300]
per-ex loss: 0.502383  [  261/  300]
per-ex loss: 0.515032  [  264/  300]
per-ex loss: 0.647921  [  267/  300]
per-ex loss: 0.474980  [  270/  300]
per-ex loss: 0.479126  [  273/  300]
per-ex loss: 0.542125  [  276/  300]
per-ex loss: 0.456475  [  279/  300]
per-ex loss: 0.476310  [  282/  300]
per-ex loss: 0.664026  [  285/  300]
per-ex loss: 0.574189  [  288/  300]
per-ex loss: 0.478905  [  291/  300]
per-ex loss: 0.593313  [  294/  300]
per-ex loss: 0.574109  [  297/  300]
per-ex loss: 0.537689  [  300/  300]
Train Error: Avg loss: 0.51889552
validation Error: 
 Avg loss: 0.50671691 
 F1: 0.478752 
 Precision: 0.435390 
 Recall: 0.531705
 IoU: 0.314710

test Error: 
 Avg loss: 0.45938003 
 F1: 0.541386 
 Precision: 0.494479 
 Recall: 0.598125
 IoU: 0.371165

We have finished training iteration 92
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_90_.pth
per-ex loss: 0.537627  [    3/  300]
per-ex loss: 0.606749  [    6/  300]
per-ex loss: 0.558167  [    9/  300]
per-ex loss: 0.508091  [   12/  300]
per-ex loss: 0.456235  [   15/  300]
per-ex loss: 0.484141  [   18/  300]
per-ex loss: 0.433000  [   21/  300]
per-ex loss: 0.584998  [   24/  300]
per-ex loss: 0.635589  [   27/  300]
per-ex loss: 0.591747  [   30/  300]
per-ex loss: 0.582158  [   33/  300]
per-ex loss: 0.490360  [   36/  300]
per-ex loss: 0.409370  [   39/  300]
per-ex loss: 0.555870  [   42/  300]
per-ex loss: 0.511079  [   45/  300]
per-ex loss: 0.578529  [   48/  300]
per-ex loss: 0.491597  [   51/  300]
per-ex loss: 0.434750  [   54/  300]
per-ex loss: 0.426855  [   57/  300]
per-ex loss: 0.508430  [   60/  300]
per-ex loss: 0.472492  [   63/  300]
per-ex loss: 0.606509  [   66/  300]
per-ex loss: 0.468539  [   69/  300]
per-ex loss: 0.639695  [   72/  300]
per-ex loss: 0.431305  [   75/  300]
per-ex loss: 0.459726  [   78/  300]
per-ex loss: 0.475941  [   81/  300]
per-ex loss: 0.458362  [   84/  300]
per-ex loss: 0.511726  [   87/  300]
per-ex loss: 0.628881  [   90/  300]
per-ex loss: 0.378805  [   93/  300]
per-ex loss: 0.486226  [   96/  300]
per-ex loss: 0.509536  [   99/  300]
per-ex loss: 0.432519  [  102/  300]
per-ex loss: 0.529924  [  105/  300]
per-ex loss: 0.525356  [  108/  300]
per-ex loss: 0.604748  [  111/  300]
per-ex loss: 0.492850  [  114/  300]
per-ex loss: 0.569878  [  117/  300]
per-ex loss: 0.452277  [  120/  300]
per-ex loss: 0.575462  [  123/  300]
per-ex loss: 0.489060  [  126/  300]
per-ex loss: 0.423567  [  129/  300]
per-ex loss: 0.539069  [  132/  300]
per-ex loss: 0.455415  [  135/  300]
per-ex loss: 0.416386  [  138/  300]
per-ex loss: 0.534737  [  141/  300]
per-ex loss: 0.638038  [  144/  300]
per-ex loss: 0.451461  [  147/  300]
per-ex loss: 0.566936  [  150/  300]
per-ex loss: 0.562290  [  153/  300]
per-ex loss: 0.569438  [  156/  300]
per-ex loss: 0.605295  [  159/  300]
per-ex loss: 0.621934  [  162/  300]
per-ex loss: 0.527715  [  165/  300]
per-ex loss: 0.632596  [  168/  300]
per-ex loss: 0.541928  [  171/  300]
per-ex loss: 0.540755  [  174/  300]
per-ex loss: 0.624267  [  177/  300]
per-ex loss: 0.606810  [  180/  300]
per-ex loss: 0.580017  [  183/  300]
per-ex loss: 0.526907  [  186/  300]
per-ex loss: 0.584539  [  189/  300]
per-ex loss: 0.514824  [  192/  300]
per-ex loss: 0.533451  [  195/  300]
per-ex loss: 0.470622  [  198/  300]
per-ex loss: 0.628233  [  201/  300]
per-ex loss: 0.505198  [  204/  300]
per-ex loss: 0.548353  [  207/  300]
per-ex loss: 0.427320  [  210/  300]
per-ex loss: 0.425401  [  213/  300]
per-ex loss: 0.448904  [  216/  300]
per-ex loss: 0.591581  [  219/  300]
per-ex loss: 0.545569  [  222/  300]
per-ex loss: 0.483576  [  225/  300]
per-ex loss: 0.525205  [  228/  300]
per-ex loss: 0.475655  [  231/  300]
per-ex loss: 0.433196  [  234/  300]
per-ex loss: 0.457786  [  237/  300]
per-ex loss: 0.572469  [  240/  300]
per-ex loss: 0.510658  [  243/  300]
per-ex loss: 0.516111  [  246/  300]
per-ex loss: 0.644673  [  249/  300]
per-ex loss: 0.546784  [  252/  300]
per-ex loss: 0.534429  [  255/  300]
per-ex loss: 0.595383  [  258/  300]
per-ex loss: 0.481284  [  261/  300]
per-ex loss: 0.566823  [  264/  300]
per-ex loss: 0.461591  [  267/  300]
per-ex loss: 0.460245  [  270/  300]
per-ex loss: 0.550750  [  273/  300]
per-ex loss: 0.606270  [  276/  300]
per-ex loss: 0.489005  [  279/  300]
per-ex loss: 0.467284  [  282/  300]
per-ex loss: 0.471377  [  285/  300]
per-ex loss: 0.483604  [  288/  300]
per-ex loss: 0.451206  [  291/  300]
per-ex loss: 0.566375  [  294/  300]
per-ex loss: 0.598262  [  297/  300]
per-ex loss: 0.534305  [  300/  300]
Train Error: Avg loss: 0.52259024
validation Error: 
 Avg loss: 0.51157494 
 F1: 0.474019 
 Precision: 0.432354 
 Recall: 0.524570
 IoU: 0.310632

test Error: 
 Avg loss: 0.46634954 
 F1: 0.534271 
 Precision: 0.486391 
 Recall: 0.592608
 IoU: 0.364509

We have finished training iteration 93
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_91_.pth
per-ex loss: 0.610519  [    3/  300]
per-ex loss: 0.477193  [    6/  300]
per-ex loss: 0.502414  [    9/  300]
per-ex loss: 0.433269  [   12/  300]
per-ex loss: 0.507286  [   15/  300]
per-ex loss: 0.526243  [   18/  300]
per-ex loss: 0.594464  [   21/  300]
per-ex loss: 0.669877  [   24/  300]
per-ex loss: 0.447167  [   27/  300]
per-ex loss: 0.635777  [   30/  300]
per-ex loss: 0.483513  [   33/  300]
per-ex loss: 0.626795  [   36/  300]
per-ex loss: 0.549318  [   39/  300]
per-ex loss: 0.462928  [   42/  300]
per-ex loss: 0.543513  [   45/  300]
per-ex loss: 0.528130  [   48/  300]
per-ex loss: 0.517643  [   51/  300]
per-ex loss: 0.540191  [   54/  300]
per-ex loss: 0.497899  [   57/  300]
per-ex loss: 0.517257  [   60/  300]
per-ex loss: 0.433852  [   63/  300]
per-ex loss: 0.497947  [   66/  300]
per-ex loss: 0.578007  [   69/  300]
per-ex loss: 0.497626  [   72/  300]
per-ex loss: 0.448775  [   75/  300]
per-ex loss: 0.615711  [   78/  300]
per-ex loss: 0.557480  [   81/  300]
per-ex loss: 0.428339  [   84/  300]
per-ex loss: 0.504587  [   87/  300]
per-ex loss: 0.530390  [   90/  300]
per-ex loss: 0.454403  [   93/  300]
per-ex loss: 0.610483  [   96/  300]
per-ex loss: 0.598958  [   99/  300]
per-ex loss: 0.486394  [  102/  300]
per-ex loss: 0.596647  [  105/  300]
per-ex loss: 0.496867  [  108/  300]
per-ex loss: 0.440366  [  111/  300]
per-ex loss: 0.523065  [  114/  300]
per-ex loss: 0.508166  [  117/  300]
per-ex loss: 0.505393  [  120/  300]
per-ex loss: 0.655865  [  123/  300]
per-ex loss: 0.595740  [  126/  300]
per-ex loss: 0.465945  [  129/  300]
per-ex loss: 0.510246  [  132/  300]
per-ex loss: 0.509336  [  135/  300]
per-ex loss: 0.536454  [  138/  300]
per-ex loss: 0.470466  [  141/  300]
per-ex loss: 0.419440  [  144/  300]
per-ex loss: 0.554801  [  147/  300]
per-ex loss: 0.432432  [  150/  300]
per-ex loss: 0.527105  [  153/  300]
per-ex loss: 0.431124  [  156/  300]
per-ex loss: 0.577425  [  159/  300]
per-ex loss: 0.637064  [  162/  300]
per-ex loss: 0.559162  [  165/  300]
per-ex loss: 0.415669  [  168/  300]
per-ex loss: 0.593600  [  171/  300]
per-ex loss: 0.500035  [  174/  300]
per-ex loss: 0.608459  [  177/  300]
per-ex loss: 0.401813  [  180/  300]
per-ex loss: 0.480005  [  183/  300]
per-ex loss: 0.457603  [  186/  300]
per-ex loss: 0.471730  [  189/  300]
per-ex loss: 0.396666  [  192/  300]
per-ex loss: 0.618485  [  195/  300]
per-ex loss: 0.468748  [  198/  300]
per-ex loss: 0.438729  [  201/  300]
per-ex loss: 0.450085  [  204/  300]
per-ex loss: 0.466291  [  207/  300]
per-ex loss: 0.566291  [  210/  300]
per-ex loss: 0.666882  [  213/  300]
per-ex loss: 0.579358  [  216/  300]
per-ex loss: 0.568356  [  219/  300]
per-ex loss: 0.481174  [  222/  300]
per-ex loss: 0.465711  [  225/  300]
per-ex loss: 0.436750  [  228/  300]
per-ex loss: 0.462347  [  231/  300]
per-ex loss: 0.431262  [  234/  300]
per-ex loss: 0.560842  [  237/  300]
per-ex loss: 0.516436  [  240/  300]
per-ex loss: 0.657467  [  243/  300]
per-ex loss: 0.473967  [  246/  300]
per-ex loss: 0.535945  [  249/  300]
per-ex loss: 0.578878  [  252/  300]
per-ex loss: 0.645090  [  255/  300]
per-ex loss: 0.577770  [  258/  300]
per-ex loss: 0.540725  [  261/  300]
per-ex loss: 0.544119  [  264/  300]
per-ex loss: 0.471792  [  267/  300]
per-ex loss: 0.622663  [  270/  300]
per-ex loss: 0.607240  [  273/  300]
per-ex loss: 0.599821  [  276/  300]
per-ex loss: 0.519550  [  279/  300]
per-ex loss: 0.560889  [  282/  300]
per-ex loss: 0.525024  [  285/  300]
per-ex loss: 0.575721  [  288/  300]
per-ex loss: 0.689257  [  291/  300]
per-ex loss: 0.437160  [  294/  300]
per-ex loss: 0.501056  [  297/  300]
per-ex loss: 0.613807  [  300/  300]
Train Error: Avg loss: 0.52648722
validation Error: 
 Avg loss: 0.50838671 
 F1: 0.479525 
 Precision: 0.437329 
 Recall: 0.530733
 IoU: 0.315378

test Error: 
 Avg loss: 0.46058643 
 F1: 0.540101 
 Precision: 0.500071 
 Recall: 0.587099
 IoU: 0.369958

We have finished training iteration 94
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_92_.pth
per-ex loss: 0.473307  [    3/  300]
per-ex loss: 0.448324  [    6/  300]
per-ex loss: 0.516207  [    9/  300]
per-ex loss: 0.633809  [   12/  300]
per-ex loss: 0.510314  [   15/  300]
per-ex loss: 0.447970  [   18/  300]
per-ex loss: 0.523291  [   21/  300]
per-ex loss: 0.537180  [   24/  300]
per-ex loss: 0.692831  [   27/  300]
per-ex loss: 0.542384  [   30/  300]
per-ex loss: 0.454516  [   33/  300]
per-ex loss: 0.484418  [   36/  300]
per-ex loss: 0.478228  [   39/  300]
per-ex loss: 0.521240  [   42/  300]
per-ex loss: 0.543862  [   45/  300]
per-ex loss: 0.414443  [   48/  300]
per-ex loss: 0.470563  [   51/  300]
per-ex loss: 0.524463  [   54/  300]
per-ex loss: 0.643840  [   57/  300]
per-ex loss: 0.451841  [   60/  300]
per-ex loss: 0.482572  [   63/  300]
per-ex loss: 0.545938  [   66/  300]
per-ex loss: 0.568113  [   69/  300]
per-ex loss: 0.482879  [   72/  300]
per-ex loss: 0.455866  [   75/  300]
per-ex loss: 0.556855  [   78/  300]
per-ex loss: 0.720753  [   81/  300]
per-ex loss: 0.694027  [   84/  300]
per-ex loss: 0.439929  [   87/  300]
per-ex loss: 0.583915  [   90/  300]
per-ex loss: 0.675273  [   93/  300]
per-ex loss: 0.504032  [   96/  300]
per-ex loss: 0.530588  [   99/  300]
per-ex loss: 0.490840  [  102/  300]
per-ex loss: 0.456515  [  105/  300]
per-ex loss: 0.470087  [  108/  300]
per-ex loss: 0.603557  [  111/  300]
per-ex loss: 0.555060  [  114/  300]
per-ex loss: 0.593469  [  117/  300]
per-ex loss: 0.645100  [  120/  300]
per-ex loss: 0.474834  [  123/  300]
per-ex loss: 0.503489  [  126/  300]
per-ex loss: 0.502941  [  129/  300]
per-ex loss: 0.643538  [  132/  300]
per-ex loss: 0.513707  [  135/  300]
per-ex loss: 0.491100  [  138/  300]
per-ex loss: 0.430400  [  141/  300]
per-ex loss: 0.673353  [  144/  300]
per-ex loss: 0.583422  [  147/  300]
per-ex loss: 0.457594  [  150/  300]
per-ex loss: 0.470016  [  153/  300]
per-ex loss: 0.401329  [  156/  300]
per-ex loss: 0.542736  [  159/  300]
per-ex loss: 0.491211  [  162/  300]
per-ex loss: 0.490623  [  165/  300]
per-ex loss: 0.508887  [  168/  300]
per-ex loss: 0.458497  [  171/  300]
per-ex loss: 0.468192  [  174/  300]
per-ex loss: 0.444574  [  177/  300]
per-ex loss: 0.503051  [  180/  300]
per-ex loss: 0.449019  [  183/  300]
per-ex loss: 0.421077  [  186/  300]
per-ex loss: 0.505297  [  189/  300]
per-ex loss: 0.499969  [  192/  300]
per-ex loss: 0.517263  [  195/  300]
per-ex loss: 0.480407  [  198/  300]
per-ex loss: 0.510671  [  201/  300]
per-ex loss: 0.570676  [  204/  300]
per-ex loss: 0.493967  [  207/  300]
per-ex loss: 0.450304  [  210/  300]
per-ex loss: 0.389852  [  213/  300]
per-ex loss: 0.548805  [  216/  300]
per-ex loss: 0.415598  [  219/  300]
per-ex loss: 0.616838  [  222/  300]
per-ex loss: 0.570917  [  225/  300]
per-ex loss: 0.719657  [  228/  300]
per-ex loss: 0.553080  [  231/  300]
per-ex loss: 0.661445  [  234/  300]
per-ex loss: 0.701423  [  237/  300]
per-ex loss: 0.609101  [  240/  300]
per-ex loss: 0.495629  [  243/  300]
per-ex loss: 0.491976  [  246/  300]
per-ex loss: 0.533121  [  249/  300]
per-ex loss: 0.464989  [  252/  300]
per-ex loss: 0.486426  [  255/  300]
per-ex loss: 0.523044  [  258/  300]
per-ex loss: 0.467289  [  261/  300]
per-ex loss: 0.582069  [  264/  300]
per-ex loss: 0.467406  [  267/  300]
per-ex loss: 0.405515  [  270/  300]
per-ex loss: 0.507373  [  273/  300]
per-ex loss: 0.639276  [  276/  300]
per-ex loss: 0.635549  [  279/  300]
per-ex loss: 0.404740  [  282/  300]
per-ex loss: 0.525625  [  285/  300]
per-ex loss: 0.642618  [  288/  300]
per-ex loss: 0.519127  [  291/  300]
per-ex loss: 0.462093  [  294/  300]
per-ex loss: 0.661326  [  297/  300]
per-ex loss: 0.581531  [  300/  300]
Train Error: Avg loss: 0.52603981
validation Error: 
 Avg loss: 0.50825312 
 F1: 0.478809 
 Precision: 0.426004 
 Recall: 0.546556
 IoU: 0.314759

test Error: 
 Avg loss: 0.46489668 
 F1: 0.535858 
 Precision: 0.484764 
 Recall: 0.598991
 IoU: 0.365987

We have finished training iteration 95
Deleting model ./segnet_vein_train/saved_model_wrapper/models/SegNet_93_.pth
slurmstepd: error: *** STEP 17778.0 ON aga2 CANCELLED AT 2025-01-21T17:01:43 ***
