segnet_main.py do_log: False
Log file name: log_21_13-05-34_01-2025.log.
            Add print_log_file_name=False to file_handler_setup() to disable this printout.
min_resource_percentage.py do_log: False
model_wrapper.py do_log: True
training_wrapper.py do_log: True
helper_img_and_fig_tools.py do_log: False
conv_resource_calc.py do_log: False
pruner.py do_log: False
helper_model_vizualization.py do_log: False
training_support.py do_log: True
helper_model_eval_graphs.py do_log: False
losses.py do_log: False
Args: Namespace(sd='segnet_vein_patch_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_segnet/segnet_vein_patch.yaml', yo=None, ntibp=None, ptp=None, map=None)
YAML: {'is_pruning_ready': False, 'path_to_data': './Data/vein_and_sclera_data', 'target': 'veins', 'train_epoch_size': 300, 'val_epoch_size': 100, 'test_epoch_size': 100, 'train_batch_size': 3, 'eval_batch_size': 12, 'learning_rate': 0.0001, 'num_of_dataloader_workers': 32, 'cleanup_k': 3, 'optimizer_used': 'Adam', 'zero_out_non_sclera_on_predictions': False, 'loss_fn_name': 'MCDL', 'loss_params': None, 'dataset_type': 'vasd', 'aug_type': 'tf', 'zero_out_non_sclera': True, 'add_sclera_to_img': False, 'add_bcosfire_to_img': True, 'add_coye_to_img': True, 'model': '64_2_6', 'input_width': 2048, 'input_height': 1024, 'input_channels': 5, 'output_channels': 2, 'have_patchification': True, 'patchification_params': {'patch_x': 256, 'patch_y': 128, 'stride_percent_of_patch_x': 0.5, 'stride_percent_of_patch_y': 0.5, 'input_size_limit': 41943040, 'num_of_patches_from_img': 64, 'prob_zero_patch_resample': 0.95}, 'num_train_iters_between_prunings': 10, 'max_auto_prunings': 70, 'proportion_to_prune': 0.01, 'prune_by_original_percent': True, 'num_filters_to_prune': -1, 'prune_n_kernels_at_once': 100, 'resource_name_to_prune_by': 'flops_num', 'importance_func': 'IPAD_eq', 'conv2d_prune_limit': 0.2}
Validation phase: False
Namespace(sd='segnet_vein_patch_train', mti=150, mtti=1000000000.0, pruning_phase=False, ifn='IPAD_eq', ips=1000000000.0, tras=-1, tp=False, yaml='z_pipeline_segnet/segnet_vein_patch.yaml', yo=None, ntibp=None, ptp=None, map=None)
Device: cuda
dataset.py do_log: False
img_augments.py do_log: False
path to file: ./Data/vein_and_sclera_data
summary for train
valid images: 88
summary for val
valid images: 27
summary for test
valid images: 12
train dataset len: 88
val dataset len: 27
test dataset len: 12
train dataloader num of batches: 100
val dataloader num of batches: 3
test dataloader num of batches: 1
Created new model instance.
per-ex loss: 0.943174  [  192/19200]
per-ex loss: 0.968665  [  384/19200]
per-ex loss: 0.906887  [  576/19200]
per-ex loss: 0.954069  [  768/19200]
per-ex loss: 0.879155  [  960/19200]
per-ex loss: 0.954467  [ 1152/19200]
per-ex loss: 0.906321  [ 1344/19200]
per-ex loss: 0.908818  [ 1536/19200]
per-ex loss: 0.915409  [ 1728/19200]
per-ex loss: 0.928822  [ 1920/19200]
per-ex loss: 0.937313  [ 2112/19200]
per-ex loss: 0.823189  [ 2304/19200]
per-ex loss: 0.966420  [ 2496/19200]
per-ex loss: 0.896292  [ 2688/19200]
per-ex loss: 0.911513  [ 2880/19200]
per-ex loss: 0.932660  [ 3072/19200]
per-ex loss: 0.850414  [ 3264/19200]
per-ex loss: 0.870653  [ 3456/19200]
per-ex loss: 0.915326  [ 3648/19200]
per-ex loss: 0.831643  [ 3840/19200]
per-ex loss: 0.890212  [ 4032/19200]
per-ex loss: 0.957626  [ 4224/19200]
per-ex loss: 0.884583  [ 4416/19200]
per-ex loss: 0.896946  [ 4608/19200]
per-ex loss: 0.866341  [ 4800/19200]
per-ex loss: 0.890808  [ 4992/19200]
per-ex loss: 0.924619  [ 5184/19200]
per-ex loss: 0.918198  [ 5376/19200]
per-ex loss: 0.856399  [ 5568/19200]
per-ex loss: 0.884313  [ 5760/19200]
per-ex loss: 0.883770  [ 5952/19200]
per-ex loss: 0.948411  [ 6144/19200]
per-ex loss: 0.848437  [ 6336/19200]
per-ex loss: 0.772818  [ 6528/19200]
per-ex loss: 0.820127  [ 6720/19200]
per-ex loss: 0.878044  [ 6912/19200]
per-ex loss: 0.886240  [ 7104/19200]
per-ex loss: 0.763249  [ 7296/19200]
per-ex loss: 0.833624  [ 7488/19200]
per-ex loss: 0.914518  [ 7680/19200]
per-ex loss: 0.841214  [ 7872/19200]
per-ex loss: 0.881022  [ 8064/19200]
per-ex loss: 0.843356  [ 8256/19200]
per-ex loss: 0.875495  [ 8448/19200]
per-ex loss: 0.909570  [ 8640/19200]
per-ex loss: 0.940998  [ 8832/19200]
per-ex loss: 0.871864  [ 9024/19200]
per-ex loss: 0.920769  [ 9216/19200]
per-ex loss: 0.852837  [ 9408/19200]
per-ex loss: 0.777050  [ 9600/19200]
per-ex loss: 0.905529  [ 9792/19200]
per-ex loss: 0.783668  [ 9984/19200]
per-ex loss: 0.870138  [10176/19200]
per-ex loss: 0.852446  [10368/19200]
per-ex loss: 0.799392  [10560/19200]
per-ex loss: 0.728074  [10752/19200]
per-ex loss: 0.896525  [10944/19200]
per-ex loss: 0.854294  [11136/19200]
per-ex loss: 0.897663  [11328/19200]
per-ex loss: 0.873952  [11520/19200]
per-ex loss: 0.870165  [11712/19200]
per-ex loss: 0.816348  [11904/19200]
per-ex loss: 0.760517  [12096/19200]
per-ex loss: 0.887974  [12288/19200]
per-ex loss: 0.837132  [12480/19200]
per-ex loss: 0.868905  [12672/19200]
per-ex loss: 0.617458  [12864/19200]
per-ex loss: 0.826954  [13056/19200]
per-ex loss: 0.898216  [13248/19200]
per-ex loss: 0.693775  [13440/19200]
per-ex loss: 0.649113  [13632/19200]
per-ex loss: 0.772592  [13824/19200]
per-ex loss: 0.818309  [14016/19200]
per-ex loss: 0.803368  [14208/19200]
per-ex loss: 0.750852  [14400/19200]
per-ex loss: 0.877628  [14592/19200]
per-ex loss: 0.905742  [14784/19200]
per-ex loss: 0.780360  [14976/19200]
per-ex loss: 0.780801  [15168/19200]
per-ex loss: 0.787462  [15360/19200]
per-ex loss: 0.758263  [15552/19200]
per-ex loss: 0.691600  [15744/19200]
per-ex loss: 0.851061  [15936/19200]
per-ex loss: 0.737814  [16128/19200]
per-ex loss: 0.674178  [16320/19200]
per-ex loss: 0.867160  [16512/19200]
per-ex loss: 0.784604  [16704/19200]
per-ex loss: 0.710299  [16896/19200]
per-ex loss: 0.682973  [17088/19200]
per-ex loss: 0.803467  [17280/19200]
per-ex loss: 0.808892  [17472/19200]
per-ex loss: 0.765297  [17664/19200]
per-ex loss: 0.718116  [17856/19200]
per-ex loss: 0.811072  [18048/19200]
per-ex loss: 0.737803  [18240/19200]
per-ex loss: 0.624002  [18432/19200]
per-ex loss: 0.604735  [18624/19200]
per-ex loss: 0.763243  [18816/19200]
per-ex loss: 0.717567  [19008/19200]
per-ex loss: 0.783336  [19200/19200]
Train Error: Avg loss: 0.83795499
validation Error: 
 Avg loss: 0.74413387 
 F1: 0.360020 
 Precision: 0.273729 
 Recall: 0.525760
 IoU: 0.219527

test Error: 
 Avg loss: 0.71323800 
 F1: 0.441968 
 Precision: 0.359732 
 Recall: 0.572946
 IoU: 0.283671

We have finished training iteration 1
per-ex loss: 0.812582  [  192/19200]
per-ex loss: 0.782055  [  384/19200]
per-ex loss: 0.615413  [  576/19200]
per-ex loss: 0.693155  [  768/19200]
per-ex loss: 0.604024  [  960/19200]
per-ex loss: 0.882049  [ 1152/19200]
per-ex loss: 0.671757  [ 1344/19200]
per-ex loss: 0.611299  [ 1536/19200]
per-ex loss: 0.846375  [ 1728/19200]
per-ex loss: 0.747711  [ 1920/19200]
per-ex loss: 0.864189  [ 2112/19200]
per-ex loss: 0.583552  [ 2304/19200]
per-ex loss: 0.826653  [ 2496/19200]
per-ex loss: 0.658011  [ 2688/19200]
per-ex loss: 0.603570  [ 2880/19200]
per-ex loss: 0.691109  [ 3072/19200]
per-ex loss: 0.610175  [ 3264/19200]
per-ex loss: 0.801970  [ 3456/19200]
per-ex loss: 0.659548  [ 3648/19200]
per-ex loss: 0.670894  [ 3840/19200]
per-ex loss: 0.667408  [ 4032/19200]
per-ex loss: 0.818681  [ 4224/19200]
per-ex loss: 0.580701  [ 4416/19200]
per-ex loss: 0.706491  [ 4608/19200]
per-ex loss: 0.641769  [ 4800/19200]
per-ex loss: 0.639498  [ 4992/19200]
per-ex loss: 0.628734  [ 5184/19200]
per-ex loss: 0.768839  [ 5376/19200]
per-ex loss: 0.605839  [ 5568/19200]
per-ex loss: 0.473210  [ 5760/19200]
per-ex loss: 0.663250  [ 5952/19200]
per-ex loss: 0.593904  [ 6144/19200]
per-ex loss: 0.830433  [ 6336/19200]
per-ex loss: 0.727867  [ 6528/19200]
per-ex loss: 0.784811  [ 6720/19200]
per-ex loss: 0.660702  [ 6912/19200]
per-ex loss: 0.691487  [ 7104/19200]
per-ex loss: 0.626273  [ 7296/19200]
per-ex loss: 0.522686  [ 7488/19200]
per-ex loss: 0.650843  [ 7680/19200]
per-ex loss: 0.562538  [ 7872/19200]
per-ex loss: 0.652654  [ 8064/19200]
per-ex loss: 0.669626  [ 8256/19200]
per-ex loss: 0.684322  [ 8448/19200]
per-ex loss: 0.551933  [ 8640/19200]
per-ex loss: 0.535963  [ 8832/19200]
per-ex loss: 0.735902  [ 9024/19200]
per-ex loss: 0.747746  [ 9216/19200]
per-ex loss: 0.830901  [ 9408/19200]
per-ex loss: 0.660869  [ 9600/19200]
per-ex loss: 0.636733  [ 9792/19200]
per-ex loss: 0.693676  [ 9984/19200]
per-ex loss: 0.619087  [10176/19200]
per-ex loss: 0.555186  [10368/19200]
per-ex loss: 0.582990  [10560/19200]
per-ex loss: 0.496762  [10752/19200]
per-ex loss: 0.532833  [10944/19200]
per-ex loss: 0.617880  [11136/19200]
per-ex loss: 0.685090  [11328/19200]
per-ex loss: 0.645564  [11520/19200]
per-ex loss: 0.533626  [11712/19200]
per-ex loss: 0.730554  [11904/19200]
per-ex loss: 0.581093  [12096/19200]
per-ex loss: 0.523783  [12288/19200]
per-ex loss: 0.590237  [12480/19200]
per-ex loss: 0.680807  [12672/19200]
per-ex loss: 0.619109  [12864/19200]
per-ex loss: 0.564986  [13056/19200]
per-ex loss: 0.612404  [13248/19200]
per-ex loss: 0.575263  [13440/19200]
per-ex loss: 0.582820  [13632/19200]
per-ex loss: 0.537412  [13824/19200]
per-ex loss: 0.533269  [14016/19200]
per-ex loss: 0.619038  [14208/19200]
per-ex loss: 0.516229  [14400/19200]
per-ex loss: 0.690664  [14592/19200]
per-ex loss: 0.595484  [14784/19200]
per-ex loss: 0.715620  [14976/19200]
per-ex loss: 0.727634  [15168/19200]
per-ex loss: 0.731598  [15360/19200]
per-ex loss: 0.671519  [15552/19200]
per-ex loss: 0.520898  [15744/19200]
per-ex loss: 0.628142  [15936/19200]
per-ex loss: 0.669215  [16128/19200]
per-ex loss: 0.647602  [16320/19200]
per-ex loss: 0.650458  [16512/19200]
per-ex loss: 0.576248  [16704/19200]
per-ex loss: 0.759658  [16896/19200]
per-ex loss: 0.631818  [17088/19200]
per-ex loss: 0.484374  [17280/19200]
per-ex loss: 0.524756  [17472/19200]
per-ex loss: 0.504547  [17664/19200]
per-ex loss: 0.583439  [17856/19200]
per-ex loss: 0.617600  [18048/19200]
per-ex loss: 0.751298  [18240/19200]
per-ex loss: 0.611498  [18432/19200]
per-ex loss: 0.522520  [18624/19200]
per-ex loss: 0.510405  [18816/19200]
per-ex loss: 0.669399  [19008/19200]
per-ex loss: 0.585712  [19200/19200]
Train Error: Avg loss: 0.64600503
validation Error: 
 Avg loss: 0.61822212 
 F1: 0.407598 
 Precision: 0.341681 
 Recall: 0.505030
 IoU: 0.255964

test Error: 
 Avg loss: 0.55212080 
 F1: 0.497123 
 Precision: 0.464099 
 Recall: 0.535206
 IoU: 0.330781

We have finished training iteration 2
per-ex loss: 0.527650  [  192/19200]
per-ex loss: 0.694435  [  384/19200]
per-ex loss: 0.731423  [  576/19200]
per-ex loss: 0.771518  [  768/19200]
per-ex loss: 0.583604  [  960/19200]
per-ex loss: 0.768050  [ 1152/19200]
per-ex loss: 0.820039  [ 1344/19200]
per-ex loss: 0.636122  [ 1536/19200]
per-ex loss: 0.671805  [ 1728/19200]
per-ex loss: 0.579723  [ 1920/19200]
per-ex loss: 0.599297  [ 2112/19200]
per-ex loss: 0.573309  [ 2304/19200]
per-ex loss: 0.763631  [ 2496/19200]
per-ex loss: 0.688150  [ 2688/19200]
per-ex loss: 0.638255  [ 2880/19200]
per-ex loss: 0.577849  [ 3072/19200]
per-ex loss: 0.592558  [ 3264/19200]
per-ex loss: 0.623789  [ 3456/19200]
per-ex loss: 0.681171  [ 3648/19200]
per-ex loss: 0.822778  [ 3840/19200]
per-ex loss: 0.651089  [ 4032/19200]
per-ex loss: 0.476611  [ 4224/19200]
per-ex loss: 0.522067  [ 4416/19200]
per-ex loss: 0.697953  [ 4608/19200]
per-ex loss: 0.513232  [ 4800/19200]
per-ex loss: 0.529000  [ 4992/19200]
per-ex loss: 0.509259  [ 5184/19200]
per-ex loss: 0.662656  [ 5376/19200]
per-ex loss: 0.704729  [ 5568/19200]
per-ex loss: 0.650437  [ 5760/19200]
per-ex loss: 0.702306  [ 5952/19200]
per-ex loss: 0.556077  [ 6144/19200]
per-ex loss: 0.600120  [ 6336/19200]
per-ex loss: 0.535866  [ 6528/19200]
per-ex loss: 0.729757  [ 6720/19200]
per-ex loss: 0.697212  [ 6912/19200]
per-ex loss: 0.635171  [ 7104/19200]
per-ex loss: 0.659025  [ 7296/19200]
per-ex loss: 0.566482  [ 7488/19200]
per-ex loss: 0.598107  [ 7680/19200]
per-ex loss: 0.650233  [ 7872/19200]
per-ex loss: 0.640675  [ 8064/19200]
per-ex loss: 0.805304  [ 8256/19200]
per-ex loss: 0.510620  [ 8448/19200]
per-ex loss: 0.699327  [ 8640/19200]
per-ex loss: 0.752916  [ 8832/19200]
per-ex loss: 0.548020  [ 9024/19200]
per-ex loss: 0.568509  [ 9216/19200]
per-ex loss: 0.629324  [ 9408/19200]
per-ex loss: 0.595564  [ 9600/19200]
per-ex loss: 0.546878  [ 9792/19200]
per-ex loss: 0.550338  [ 9984/19200]
per-ex loss: 0.576065  [10176/19200]
per-ex loss: 0.646221  [10368/19200]
per-ex loss: 0.657540  [10560/19200]
per-ex loss: 0.654027  [10752/19200]
per-ex loss: 0.608414  [10944/19200]
per-ex loss: 0.594734  [11136/19200]
per-ex loss: 0.644690  [11328/19200]
per-ex loss: 0.702201  [11520/19200]
per-ex loss: 0.627676  [11712/19200]
per-ex loss: 0.700703  [11904/19200]
per-ex loss: 0.696824  [12096/19200]
per-ex loss: 0.737589  [12288/19200]
per-ex loss: 0.733479  [12480/19200]
per-ex loss: 0.643574  [12672/19200]
per-ex loss: 0.791990  [12864/19200]
per-ex loss: 0.640149  [13056/19200]
per-ex loss: 0.673679  [13248/19200]
per-ex loss: 0.687094  [13440/19200]
per-ex loss: 0.583022  [13632/19200]
per-ex loss: 0.641627  [13824/19200]
per-ex loss: 0.551997  [14016/19200]
per-ex loss: 0.584581  [14208/19200]
per-ex loss: 0.545747  [14400/19200]
per-ex loss: 0.566499  [14592/19200]
per-ex loss: 0.610620  [14784/19200]
per-ex loss: 0.663330  [14976/19200]
per-ex loss: 0.839835  [15168/19200]
per-ex loss: 0.599389  [15360/19200]
per-ex loss: 0.540227  [15552/19200]
per-ex loss: 0.574202  [15744/19200]
per-ex loss: 0.685564  [15936/19200]
per-ex loss: 0.629921  [16128/19200]
per-ex loss: 0.624767  [16320/19200]
per-ex loss: 0.475621  [16512/19200]
per-ex loss: 0.623007  [16704/19200]
per-ex loss: 0.505497  [16896/19200]
per-ex loss: 0.564896  [17088/19200]
per-ex loss: 0.560694  [17280/19200]
per-ex loss: 0.654146  [17472/19200]
per-ex loss: 0.559885  [17664/19200]
per-ex loss: 0.468883  [17856/19200]
per-ex loss: 0.639558  [18048/19200]
per-ex loss: 0.799263  [18240/19200]
per-ex loss: 0.661411  [18432/19200]
per-ex loss: 0.693874  [18624/19200]
per-ex loss: 0.594440  [18816/19200]
per-ex loss: 0.656379  [19008/19200]
per-ex loss: 0.795715  [19200/19200]
Train Error: Avg loss: 0.63551262
validation Error: 
 Avg loss: 0.67444364 
 F1: 0.352694 
 Precision: 0.399679 
 Recall: 0.315593
 IoU: 0.214103

test Error: 
 Avg loss: 0.58505625 
 F1: 0.439137 
 Precision: 0.538338 
 Recall: 0.370807
 IoU: 0.281342

We have finished training iteration 3
per-ex loss: 0.622507  [  192/19200]
per-ex loss: 0.589806  [  384/19200]
per-ex loss: 0.715910  [  576/19200]
per-ex loss: 0.684618  [  768/19200]
per-ex loss: 0.604427  [  960/19200]
per-ex loss: 0.609681  [ 1152/19200]
per-ex loss: 0.638082  [ 1344/19200]
per-ex loss: 0.567193  [ 1536/19200]
per-ex loss: 0.534432  [ 1728/19200]
per-ex loss: 0.560882  [ 1920/19200]
per-ex loss: 0.673737  [ 2112/19200]
per-ex loss: 0.672789  [ 2304/19200]
per-ex loss: 0.558527  [ 2496/19200]
per-ex loss: 0.512222  [ 2688/19200]
per-ex loss: 0.619135  [ 2880/19200]
per-ex loss: 0.564128  [ 3072/19200]
per-ex loss: 0.582033  [ 3264/19200]
per-ex loss: 0.450650  [ 3456/19200]
per-ex loss: 0.554373  [ 3648/19200]
per-ex loss: 0.779721  [ 3840/19200]
per-ex loss: 0.655953  [ 4032/19200]
per-ex loss: 0.565660  [ 4224/19200]
per-ex loss: 0.763867  [ 4416/19200]
per-ex loss: 0.656854  [ 4608/19200]
per-ex loss: 0.612897  [ 4800/19200]
per-ex loss: 0.798859  [ 4992/19200]
per-ex loss: 0.659414  [ 5184/19200]
per-ex loss: 0.600680  [ 5376/19200]
per-ex loss: 0.600270  [ 5568/19200]
per-ex loss: 0.668138  [ 5760/19200]
per-ex loss: 0.651875  [ 5952/19200]
per-ex loss: 0.603224  [ 6144/19200]
per-ex loss: 0.601412  [ 6336/19200]
per-ex loss: 0.641446  [ 6528/19200]
per-ex loss: 0.752916  [ 6720/19200]
per-ex loss: 0.631408  [ 6912/19200]
per-ex loss: 0.559479  [ 7104/19200]
per-ex loss: 0.549892  [ 7296/19200]
per-ex loss: 0.505844  [ 7488/19200]
per-ex loss: 0.463033  [ 7680/19200]
per-ex loss: 0.577298  [ 7872/19200]
per-ex loss: 0.657040  [ 8064/19200]
per-ex loss: 0.573977  [ 8256/19200]
per-ex loss: 0.651436  [ 8448/19200]
per-ex loss: 0.521101  [ 8640/19200]
per-ex loss: 0.734722  [ 8832/19200]
per-ex loss: 0.620001  [ 9024/19200]
per-ex loss: 0.612001  [ 9216/19200]
per-ex loss: 0.594226  [ 9408/19200]
per-ex loss: 0.664042  [ 9600/19200]
per-ex loss: 0.598278  [ 9792/19200]
per-ex loss: 0.780522  [ 9984/19200]
per-ex loss: 0.769719  [10176/19200]
per-ex loss: 0.643971  [10368/19200]
per-ex loss: 0.538360  [10560/19200]
per-ex loss: 0.663470  [10752/19200]
per-ex loss: 0.560682  [10944/19200]
per-ex loss: 0.623476  [11136/19200]
per-ex loss: 0.535700  [11328/19200]
per-ex loss: 0.458935  [11520/19200]
per-ex loss: 0.490602  [11712/19200]
per-ex loss: 0.624896  [11904/19200]
per-ex loss: 0.523792  [12096/19200]
per-ex loss: 0.753257  [12288/19200]
per-ex loss: 0.599215  [12480/19200]
per-ex loss: 0.631411  [12672/19200]
per-ex loss: 0.582471  [12864/19200]
per-ex loss: 0.598330  [13056/19200]
per-ex loss: 0.706631  [13248/19200]
per-ex loss: 0.698987  [13440/19200]
per-ex loss: 0.653907  [13632/19200]
per-ex loss: 0.646290  [13824/19200]
per-ex loss: 0.586257  [14016/19200]
per-ex loss: 0.622811  [14208/19200]
per-ex loss: 0.590192  [14400/19200]
per-ex loss: 0.625019  [14592/19200]
per-ex loss: 0.738327  [14784/19200]
per-ex loss: 0.648935  [14976/19200]
per-ex loss: 0.537624  [15168/19200]
per-ex loss: 0.664596  [15360/19200]
per-ex loss: 0.643081  [15552/19200]
per-ex loss: 0.574422  [15744/19200]
per-ex loss: 0.611387  [15936/19200]
per-ex loss: 0.597572  [16128/19200]
per-ex loss: 0.535630  [16320/19200]
per-ex loss: 0.593101  [16512/19200]
per-ex loss: 0.524597  [16704/19200]
per-ex loss: 0.496052  [16896/19200]
per-ex loss: 0.535674  [17088/19200]
per-ex loss: 0.581937  [17280/19200]
per-ex loss: 0.796732  [17472/19200]
per-ex loss: 0.600140  [17664/19200]
per-ex loss: 0.434107  [17856/19200]
per-ex loss: 0.597269  [18048/19200]
per-ex loss: 0.783889  [18240/19200]
per-ex loss: 0.663280  [18432/19200]
per-ex loss: 0.582482  [18624/19200]
per-ex loss: 0.463276  [18816/19200]
per-ex loss: 0.609910  [19008/19200]
per-ex loss: 0.490876  [19200/19200]
Train Error: Avg loss: 0.61255890
validation Error: 
 Avg loss: 0.58178176 
 F1: 0.425670 
 Precision: 0.385800 
 Recall: 0.474729
 IoU: 0.270381

test Error: 
 Avg loss: 0.50473821 
 F1: 0.509729 
 Precision: 0.505359 
 Recall: 0.514177
 IoU: 0.342038

We have finished training iteration 4
per-ex loss: 0.711965  [  192/19200]
per-ex loss: 0.712365  [  384/19200]
per-ex loss: 0.597587  [  576/19200]
per-ex loss: 0.618095  [  768/19200]
per-ex loss: 0.740765  [  960/19200]
per-ex loss: 0.509024  [ 1152/19200]
per-ex loss: 0.560258  [ 1344/19200]
per-ex loss: 0.597398  [ 1536/19200]
per-ex loss: 0.540660  [ 1728/19200]
per-ex loss: 0.696473  [ 1920/19200]
per-ex loss: 0.516969  [ 2112/19200]
per-ex loss: 0.579401  [ 2304/19200]
per-ex loss: 0.591496  [ 2496/19200]
per-ex loss: 0.708464  [ 2688/19200]
per-ex loss: 0.626183  [ 2880/19200]
per-ex loss: 0.573107  [ 3072/19200]
per-ex loss: 0.513170  [ 3264/19200]
per-ex loss: 0.465613  [ 3456/19200]
per-ex loss: 0.553221  [ 3648/19200]
per-ex loss: 0.520019  [ 3840/19200]
per-ex loss: 0.458855  [ 4032/19200]
per-ex loss: 0.518473  [ 4224/19200]
per-ex loss: 0.593481  [ 4416/19200]
per-ex loss: 0.713219  [ 4608/19200]
per-ex loss: 0.607866  [ 4800/19200]
per-ex loss: 0.635804  [ 4992/19200]
per-ex loss: 0.594201  [ 5184/19200]
per-ex loss: 0.620708  [ 5376/19200]
per-ex loss: 0.681800  [ 5568/19200]
per-ex loss: 0.586410  [ 5760/19200]
per-ex loss: 0.562634  [ 5952/19200]
per-ex loss: 0.738586  [ 6144/19200]
per-ex loss: 0.563641  [ 6336/19200]
per-ex loss: 0.534550  [ 6528/19200]
per-ex loss: 0.529888  [ 6720/19200]
per-ex loss: 0.617277  [ 6912/19200]
per-ex loss: 0.541714  [ 7104/19200]
per-ex loss: 0.664921  [ 7296/19200]
per-ex loss: 0.615811  [ 7488/19200]
per-ex loss: 0.687044  [ 7680/19200]
per-ex loss: 0.593299  [ 7872/19200]
per-ex loss: 0.570799  [ 8064/19200]
per-ex loss: 0.518153  [ 8256/19200]
per-ex loss: 0.605501  [ 8448/19200]
per-ex loss: 0.784558  [ 8640/19200]
per-ex loss: 0.573997  [ 8832/19200]
per-ex loss: 0.463070  [ 9024/19200]
per-ex loss: 0.489866  [ 9216/19200]
per-ex loss: 0.614381  [ 9408/19200]
per-ex loss: 0.655265  [ 9600/19200]
per-ex loss: 0.707842  [ 9792/19200]
per-ex loss: 0.595673  [ 9984/19200]
per-ex loss: 0.515652  [10176/19200]
per-ex loss: 0.659271  [10368/19200]
per-ex loss: 0.560464  [10560/19200]
per-ex loss: 0.546227  [10752/19200]
per-ex loss: 0.626158  [10944/19200]
per-ex loss: 0.573559  [11136/19200]
per-ex loss: 0.538547  [11328/19200]
per-ex loss: 0.592473  [11520/19200]
per-ex loss: 0.678605  [11712/19200]
per-ex loss: 0.538329  [11904/19200]
per-ex loss: 0.731645  [12096/19200]
per-ex loss: 0.485482  [12288/19200]
per-ex loss: 0.527782  [12480/19200]
per-ex loss: 0.502872  [12672/19200]
per-ex loss: 0.534410  [12864/19200]
per-ex loss: 0.572752  [13056/19200]
per-ex loss: 0.777560  [13248/19200]
per-ex loss: 0.525258  [13440/19200]
per-ex loss: 0.595800  [13632/19200]
per-ex loss: 0.646746  [13824/19200]
per-ex loss: 0.541148  [14016/19200]
per-ex loss: 0.525141  [14208/19200]
per-ex loss: 0.591686  [14400/19200]
per-ex loss: 0.508751  [14592/19200]
per-ex loss: 0.500875  [14784/19200]
per-ex loss: 0.629870  [14976/19200]
per-ex loss: 0.724683  [15168/19200]
per-ex loss: 0.743668  [15360/19200]
per-ex loss: 0.598559  [15552/19200]
per-ex loss: 0.671534  [15744/19200]
per-ex loss: 0.544801  [15936/19200]
per-ex loss: 0.675410  [16128/19200]
per-ex loss: 0.569401  [16320/19200]
per-ex loss: 0.558167  [16512/19200]
per-ex loss: 0.444511  [16704/19200]
per-ex loss: 0.543771  [16896/19200]
per-ex loss: 0.453271  [17088/19200]
per-ex loss: 0.734220  [17280/19200]
per-ex loss: 0.740301  [17472/19200]
per-ex loss: 0.606805  [17664/19200]
per-ex loss: 0.633461  [17856/19200]
per-ex loss: 0.547836  [18048/19200]
per-ex loss: 0.739680  [18240/19200]
per-ex loss: 0.643292  [18432/19200]
per-ex loss: 0.572731  [18624/19200]
per-ex loss: 0.646524  [18816/19200]
per-ex loss: 0.688728  [19008/19200]
per-ex loss: 0.597723  [19200/19200]
Train Error: Avg loss: 0.59877664
validation Error: 
 Avg loss: 0.57496675 
 F1: 0.426977 
 Precision: 0.382901 
 Recall: 0.482521
 IoU: 0.271438

test Error: 
 Avg loss: 0.50131494 
 F1: 0.508561 
 Precision: 0.516211 
 Recall: 0.501135
 IoU: 0.340987

We have finished training iteration 5
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_1_.pth
per-ex loss: 0.497809  [  192/19200]
per-ex loss: 0.587562  [  384/19200]
per-ex loss: 0.651900  [  576/19200]
per-ex loss: 0.476548  [  768/19200]
per-ex loss: 0.516933  [  960/19200]
per-ex loss: 0.599165  [ 1152/19200]
per-ex loss: 0.696893  [ 1344/19200]
per-ex loss: 0.620336  [ 1536/19200]
per-ex loss: 0.565915  [ 1728/19200]
per-ex loss: 0.658324  [ 1920/19200]
per-ex loss: 0.613589  [ 2112/19200]
per-ex loss: 0.517150  [ 2304/19200]
per-ex loss: 0.641007  [ 2496/19200]
per-ex loss: 0.607772  [ 2688/19200]
per-ex loss: 0.637212  [ 2880/19200]
per-ex loss: 0.630870  [ 3072/19200]
per-ex loss: 0.510660  [ 3264/19200]
per-ex loss: 0.740994  [ 3456/19200]
per-ex loss: 0.503083  [ 3648/19200]
per-ex loss: 0.516609  [ 3840/19200]
per-ex loss: 0.668266  [ 4032/19200]
per-ex loss: 0.723585  [ 4224/19200]
per-ex loss: 0.686328  [ 4416/19200]
per-ex loss: 0.532735  [ 4608/19200]
per-ex loss: 0.729518  [ 4800/19200]
per-ex loss: 0.606167  [ 4992/19200]
per-ex loss: 0.434520  [ 5184/19200]
per-ex loss: 0.605862  [ 5376/19200]
per-ex loss: 0.529607  [ 5568/19200]
per-ex loss: 0.714293  [ 5760/19200]
per-ex loss: 0.635887  [ 5952/19200]
per-ex loss: 0.487257  [ 6144/19200]
per-ex loss: 0.520549  [ 6336/19200]
per-ex loss: 0.542633  [ 6528/19200]
per-ex loss: 0.625122  [ 6720/19200]
per-ex loss: 0.769367  [ 6912/19200]
per-ex loss: 0.522858  [ 7104/19200]
per-ex loss: 0.550597  [ 7296/19200]
per-ex loss: 0.553432  [ 7488/19200]
per-ex loss: 0.446122  [ 7680/19200]
per-ex loss: 0.774708  [ 7872/19200]
per-ex loss: 0.641955  [ 8064/19200]
per-ex loss: 0.571216  [ 8256/19200]
per-ex loss: 0.478000  [ 8448/19200]
per-ex loss: 0.590467  [ 8640/19200]
per-ex loss: 0.491220  [ 8832/19200]
per-ex loss: 0.695511  [ 9024/19200]
per-ex loss: 0.641253  [ 9216/19200]
per-ex loss: 0.531656  [ 9408/19200]
per-ex loss: 0.655699  [ 9600/19200]
per-ex loss: 0.584586  [ 9792/19200]
per-ex loss: 0.481214  [ 9984/19200]
per-ex loss: 0.592861  [10176/19200]
per-ex loss: 0.568136  [10368/19200]
per-ex loss: 0.711405  [10560/19200]
per-ex loss: 0.613018  [10752/19200]
per-ex loss: 0.665205  [10944/19200]
per-ex loss: 0.661179  [11136/19200]
per-ex loss: 0.515506  [11328/19200]
per-ex loss: 0.617193  [11520/19200]
per-ex loss: 0.538692  [11712/19200]
per-ex loss: 0.526398  [11904/19200]
per-ex loss: 0.537087  [12096/19200]
per-ex loss: 0.625457  [12288/19200]
per-ex loss: 0.724081  [12480/19200]
per-ex loss: 0.603629  [12672/19200]
per-ex loss: 0.465792  [12864/19200]
per-ex loss: 0.493757  [13056/19200]
per-ex loss: 0.444552  [13248/19200]
per-ex loss: 0.564001  [13440/19200]
per-ex loss: 0.631818  [13632/19200]
per-ex loss: 0.550852  [13824/19200]
per-ex loss: 0.511261  [14016/19200]
per-ex loss: 0.623598  [14208/19200]
per-ex loss: 0.643977  [14400/19200]
per-ex loss: 0.476553  [14592/19200]
per-ex loss: 0.749185  [14784/19200]
per-ex loss: 0.512023  [14976/19200]
per-ex loss: 0.713421  [15168/19200]
per-ex loss: 0.577282  [15360/19200]
per-ex loss: 0.648362  [15552/19200]
per-ex loss: 0.641052  [15744/19200]
per-ex loss: 0.551758  [15936/19200]
per-ex loss: 0.670530  [16128/19200]
per-ex loss: 0.648120  [16320/19200]
per-ex loss: 0.588847  [16512/19200]
per-ex loss: 0.612157  [16704/19200]
per-ex loss: 0.503549  [16896/19200]
per-ex loss: 0.481750  [17088/19200]
per-ex loss: 0.536106  [17280/19200]
per-ex loss: 0.597579  [17472/19200]
per-ex loss: 0.571199  [17664/19200]
per-ex loss: 0.636541  [17856/19200]
per-ex loss: 0.483467  [18048/19200]
per-ex loss: 0.520589  [18240/19200]
per-ex loss: 0.587536  [18432/19200]
per-ex loss: 0.606041  [18624/19200]
per-ex loss: 0.643979  [18816/19200]
per-ex loss: 0.479891  [19008/19200]
per-ex loss: 0.436966  [19200/19200]
Train Error: Avg loss: 0.58716460
validation Error: 
 Avg loss: 0.55498870 
 F1: 0.440357 
 Precision: 0.380092 
 Recall: 0.523334
 IoU: 0.282345

test Error: 
 Avg loss: 0.48487610 
 F1: 0.524149 
 Precision: 0.499485 
 Recall: 0.551374
 IoU: 0.355150

We have finished training iteration 6
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_3_.pth
per-ex loss: 0.558464  [  192/19200]
per-ex loss: 0.517176  [  384/19200]
per-ex loss: 0.581457  [  576/19200]
per-ex loss: 0.451719  [  768/19200]
per-ex loss: 0.667901  [  960/19200]
per-ex loss: 0.678607  [ 1152/19200]
per-ex loss: 0.605791  [ 1344/19200]
per-ex loss: 0.691547  [ 1536/19200]
per-ex loss: 0.641189  [ 1728/19200]
per-ex loss: 0.451405  [ 1920/19200]
per-ex loss: 0.762316  [ 2112/19200]
per-ex loss: 0.558644  [ 2304/19200]
per-ex loss: 0.561792  [ 2496/19200]
per-ex loss: 0.510007  [ 2688/19200]
per-ex loss: 0.570461  [ 2880/19200]
per-ex loss: 0.591974  [ 3072/19200]
per-ex loss: 0.549126  [ 3264/19200]
per-ex loss: 0.449890  [ 3456/19200]
per-ex loss: 0.622943  [ 3648/19200]
per-ex loss: 0.575066  [ 3840/19200]
per-ex loss: 0.578711  [ 4032/19200]
per-ex loss: 0.545299  [ 4224/19200]
per-ex loss: 0.724513  [ 4416/19200]
per-ex loss: 0.497757  [ 4608/19200]
per-ex loss: 0.531784  [ 4800/19200]
per-ex loss: 0.550688  [ 4992/19200]
per-ex loss: 0.461969  [ 5184/19200]
per-ex loss: 0.690517  [ 5376/19200]
per-ex loss: 0.623895  [ 5568/19200]
per-ex loss: 0.622195  [ 5760/19200]
per-ex loss: 0.557698  [ 5952/19200]
per-ex loss: 0.489546  [ 6144/19200]
per-ex loss: 0.563829  [ 6336/19200]
per-ex loss: 0.754974  [ 6528/19200]
per-ex loss: 0.544307  [ 6720/19200]
per-ex loss: 0.707566  [ 6912/19200]
per-ex loss: 0.531997  [ 7104/19200]
per-ex loss: 0.603168  [ 7296/19200]
per-ex loss: 0.553381  [ 7488/19200]
per-ex loss: 0.730087  [ 7680/19200]
per-ex loss: 0.468264  [ 7872/19200]
per-ex loss: 0.687715  [ 8064/19200]
per-ex loss: 0.619449  [ 8256/19200]
per-ex loss: 0.576127  [ 8448/19200]
per-ex loss: 0.555841  [ 8640/19200]
per-ex loss: 0.499182  [ 8832/19200]
per-ex loss: 0.564953  [ 9024/19200]
per-ex loss: 0.571856  [ 9216/19200]
per-ex loss: 0.521383  [ 9408/19200]
per-ex loss: 0.608868  [ 9600/19200]
per-ex loss: 0.657680  [ 9792/19200]
per-ex loss: 0.462027  [ 9984/19200]
per-ex loss: 0.459505  [10176/19200]
per-ex loss: 0.519529  [10368/19200]
per-ex loss: 0.516826  [10560/19200]
per-ex loss: 0.694949  [10752/19200]
per-ex loss: 0.560104  [10944/19200]
per-ex loss: 0.641381  [11136/19200]
per-ex loss: 0.581836  [11328/19200]
per-ex loss: 0.535997  [11520/19200]
per-ex loss: 0.456275  [11712/19200]
per-ex loss: 0.502519  [11904/19200]
per-ex loss: 0.592936  [12096/19200]
per-ex loss: 0.462924  [12288/19200]
per-ex loss: 0.576789  [12480/19200]
per-ex loss: 0.661252  [12672/19200]
per-ex loss: 0.648567  [12864/19200]
per-ex loss: 0.535752  [13056/19200]
per-ex loss: 0.610305  [13248/19200]
per-ex loss: 0.504826  [13440/19200]
per-ex loss: 0.520923  [13632/19200]
per-ex loss: 0.585208  [13824/19200]
per-ex loss: 0.541227  [14016/19200]
per-ex loss: 0.666945  [14208/19200]
per-ex loss: 0.560863  [14400/19200]
per-ex loss: 0.552848  [14592/19200]
per-ex loss: 0.546492  [14784/19200]
per-ex loss: 0.649820  [14976/19200]
per-ex loss: 0.488505  [15168/19200]
per-ex loss: 0.589715  [15360/19200]
per-ex loss: 0.623391  [15552/19200]
per-ex loss: 0.596868  [15744/19200]
per-ex loss: 0.680997  [15936/19200]
per-ex loss: 0.519482  [16128/19200]
per-ex loss: 0.516993  [16320/19200]
per-ex loss: 0.539606  [16512/19200]
per-ex loss: 0.692625  [16704/19200]
per-ex loss: 0.658457  [16896/19200]
per-ex loss: 0.540786  [17088/19200]
per-ex loss: 0.556718  [17280/19200]
per-ex loss: 0.539184  [17472/19200]
per-ex loss: 0.575475  [17664/19200]
per-ex loss: 0.669256  [17856/19200]
per-ex loss: 0.633053  [18048/19200]
per-ex loss: 0.491632  [18240/19200]
per-ex loss: 0.523986  [18432/19200]
per-ex loss: 0.541453  [18624/19200]
per-ex loss: 0.586501  [18816/19200]
per-ex loss: 0.649249  [19008/19200]
per-ex loss: 0.551116  [19200/19200]
Train Error: Avg loss: 0.57706345
validation Error: 
 Avg loss: 0.55549790 
 F1: 0.440264 
 Precision: 0.381507 
 Recall: 0.520416
 IoU: 0.282269

test Error: 
 Avg loss: 0.47942007 
 F1: 0.527624 
 Precision: 0.502999 
 Recall: 0.554785
 IoU: 0.358349

We have finished training iteration 7
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_2_.pth
per-ex loss: 0.641052  [  192/19200]
per-ex loss: 0.646303  [  384/19200]
per-ex loss: 0.498707  [  576/19200]
per-ex loss: 0.642003  [  768/19200]
per-ex loss: 0.477196  [  960/19200]
per-ex loss: 0.498293  [ 1152/19200]
per-ex loss: 0.595450  [ 1344/19200]
per-ex loss: 0.480742  [ 1536/19200]
per-ex loss: 0.459578  [ 1728/19200]
per-ex loss: 0.590402  [ 1920/19200]
per-ex loss: 0.485802  [ 2112/19200]
per-ex loss: 0.611234  [ 2304/19200]
per-ex loss: 0.596969  [ 2496/19200]
per-ex loss: 0.478356  [ 2688/19200]
per-ex loss: 0.635809  [ 2880/19200]
per-ex loss: 0.680454  [ 3072/19200]
per-ex loss: 0.502442  [ 3264/19200]
per-ex loss: 0.645300  [ 3456/19200]
per-ex loss: 0.624089  [ 3648/19200]
per-ex loss: 0.449942  [ 3840/19200]
per-ex loss: 0.628250  [ 4032/19200]
per-ex loss: 0.604381  [ 4224/19200]
per-ex loss: 0.717597  [ 4416/19200]
per-ex loss: 0.484294  [ 4608/19200]
per-ex loss: 0.686563  [ 4800/19200]
per-ex loss: 0.659929  [ 4992/19200]
per-ex loss: 0.577065  [ 5184/19200]
per-ex loss: 0.575677  [ 5376/19200]
per-ex loss: 0.581888  [ 5568/19200]
per-ex loss: 0.495713  [ 5760/19200]
per-ex loss: 0.666692  [ 5952/19200]
per-ex loss: 0.655042  [ 6144/19200]
per-ex loss: 0.616562  [ 6336/19200]
per-ex loss: 0.516452  [ 6528/19200]
per-ex loss: 0.623324  [ 6720/19200]
per-ex loss: 0.758263  [ 6912/19200]
per-ex loss: 0.528793  [ 7104/19200]
per-ex loss: 0.710573  [ 7296/19200]
per-ex loss: 0.484884  [ 7488/19200]
per-ex loss: 0.595925  [ 7680/19200]
per-ex loss: 0.701146  [ 7872/19200]
per-ex loss: 0.812568  [ 8064/19200]
per-ex loss: 0.621923  [ 8256/19200]
per-ex loss: 0.577425  [ 8448/19200]
per-ex loss: 0.547317  [ 8640/19200]
per-ex loss: 0.709840  [ 8832/19200]
per-ex loss: 0.568226  [ 9024/19200]
per-ex loss: 0.640080  [ 9216/19200]
per-ex loss: 0.625515  [ 9408/19200]
per-ex loss: 0.591093  [ 9600/19200]
per-ex loss: 0.658632  [ 9792/19200]
per-ex loss: 0.769211  [ 9984/19200]
per-ex loss: 0.603676  [10176/19200]
per-ex loss: 0.578446  [10368/19200]
per-ex loss: 0.510675  [10560/19200]
per-ex loss: 0.702251  [10752/19200]
per-ex loss: 0.501374  [10944/19200]
per-ex loss: 0.630590  [11136/19200]
per-ex loss: 0.577076  [11328/19200]
per-ex loss: 0.613077  [11520/19200]
per-ex loss: 0.507518  [11712/19200]
per-ex loss: 0.672055  [11904/19200]
per-ex loss: 0.698561  [12096/19200]
per-ex loss: 0.667374  [12288/19200]
per-ex loss: 0.475649  [12480/19200]
per-ex loss: 0.552164  [12672/19200]
per-ex loss: 0.595988  [12864/19200]
per-ex loss: 0.532218  [13056/19200]
per-ex loss: 0.612229  [13248/19200]
per-ex loss: 0.525539  [13440/19200]
per-ex loss: 0.569296  [13632/19200]
per-ex loss: 0.463273  [13824/19200]
per-ex loss: 0.546485  [14016/19200]
per-ex loss: 0.662306  [14208/19200]
per-ex loss: 0.663487  [14400/19200]
per-ex loss: 0.643276  [14592/19200]
per-ex loss: 0.612356  [14784/19200]
per-ex loss: 0.604677  [14976/19200]
per-ex loss: 0.569941  [15168/19200]
per-ex loss: 0.570502  [15360/19200]
per-ex loss: 0.491870  [15552/19200]
per-ex loss: 0.771602  [15744/19200]
per-ex loss: 0.548096  [15936/19200]
per-ex loss: 0.586432  [16128/19200]
per-ex loss: 0.692859  [16320/19200]
per-ex loss: 0.696346  [16512/19200]
per-ex loss: 0.478224  [16704/19200]
per-ex loss: 0.618710  [16896/19200]
per-ex loss: 0.580848  [17088/19200]
per-ex loss: 0.585942  [17280/19200]
per-ex loss: 0.630386  [17472/19200]
per-ex loss: 0.622327  [17664/19200]
per-ex loss: 0.619748  [17856/19200]
per-ex loss: 0.650576  [18048/19200]
per-ex loss: 0.586646  [18240/19200]
per-ex loss: 0.621337  [18432/19200]
per-ex loss: 0.558071  [18624/19200]
per-ex loss: 0.771597  [18816/19200]
per-ex loss: 0.551350  [19008/19200]
per-ex loss: 0.493423  [19200/19200]
Train Error: Avg loss: 0.59849416
validation Error: 
 Avg loss: 0.58012205 
 F1: 0.418700 
 Precision: 0.363366 
 Recall: 0.493913
 IoU: 0.264782

test Error: 
 Avg loss: 0.50224209 
 F1: 0.503223 
 Precision: 0.476253 
 Recall: 0.533431
 IoU: 0.336205

We have finished training iteration 8
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_4_.pth
per-ex loss: 0.585346  [  192/19200]
per-ex loss: 0.762694  [  384/19200]
per-ex loss: 0.687463  [  576/19200]
per-ex loss: 0.533867  [  768/19200]
per-ex loss: 0.471488  [  960/19200]
per-ex loss: 0.551952  [ 1152/19200]
per-ex loss: 0.557741  [ 1344/19200]
per-ex loss: 0.673037  [ 1536/19200]
per-ex loss: 0.487315  [ 1728/19200]
per-ex loss: 0.591067  [ 1920/19200]
per-ex loss: 0.587426  [ 2112/19200]
per-ex loss: 0.602269  [ 2304/19200]
per-ex loss: 0.512742  [ 2496/19200]
per-ex loss: 0.633745  [ 2688/19200]
per-ex loss: 0.658046  [ 2880/19200]
per-ex loss: 0.721316  [ 3072/19200]
per-ex loss: 0.532732  [ 3264/19200]
per-ex loss: 0.749156  [ 3456/19200]
per-ex loss: 0.613571  [ 3648/19200]
per-ex loss: 0.462883  [ 3840/19200]
per-ex loss: 0.457376  [ 4032/19200]
per-ex loss: 0.630126  [ 4224/19200]
per-ex loss: 0.519493  [ 4416/19200]
per-ex loss: 0.604204  [ 4608/19200]
per-ex loss: 0.716099  [ 4800/19200]
per-ex loss: 0.681237  [ 4992/19200]
per-ex loss: 0.515010  [ 5184/19200]
per-ex loss: 0.618712  [ 5376/19200]
per-ex loss: 0.548607  [ 5568/19200]
per-ex loss: 0.619607  [ 5760/19200]
per-ex loss: 0.505656  [ 5952/19200]
per-ex loss: 0.598097  [ 6144/19200]
per-ex loss: 0.559409  [ 6336/19200]
per-ex loss: 0.540374  [ 6528/19200]
per-ex loss: 0.561211  [ 6720/19200]
per-ex loss: 0.550445  [ 6912/19200]
per-ex loss: 0.605134  [ 7104/19200]
per-ex loss: 0.663732  [ 7296/19200]
per-ex loss: 0.598506  [ 7488/19200]
per-ex loss: 0.805821  [ 7680/19200]
per-ex loss: 0.530412  [ 7872/19200]
per-ex loss: 0.472018  [ 8064/19200]
per-ex loss: 0.542614  [ 8256/19200]
per-ex loss: 0.593523  [ 8448/19200]
per-ex loss: 0.549375  [ 8640/19200]
per-ex loss: 0.548994  [ 8832/19200]
per-ex loss: 0.597491  [ 9024/19200]
per-ex loss: 0.721253  [ 9216/19200]
per-ex loss: 0.589229  [ 9408/19200]
per-ex loss: 0.565309  [ 9600/19200]
per-ex loss: 0.445430  [ 9792/19200]
per-ex loss: 0.717467  [ 9984/19200]
per-ex loss: 0.483289  [10176/19200]
per-ex loss: 0.609538  [10368/19200]
per-ex loss: 0.658819  [10560/19200]
per-ex loss: 0.745937  [10752/19200]
per-ex loss: 0.518585  [10944/19200]
per-ex loss: 0.694737  [11136/19200]
per-ex loss: 0.669469  [11328/19200]
per-ex loss: 0.584534  [11520/19200]
per-ex loss: 0.511895  [11712/19200]
per-ex loss: 0.508954  [11904/19200]
per-ex loss: 0.654895  [12096/19200]
per-ex loss: 0.570937  [12288/19200]
per-ex loss: 0.547638  [12480/19200]
per-ex loss: 0.493431  [12672/19200]
per-ex loss: 0.513759  [12864/19200]
per-ex loss: 0.745379  [13056/19200]
per-ex loss: 0.577974  [13248/19200]
per-ex loss: 0.622732  [13440/19200]
per-ex loss: 0.538050  [13632/19200]
per-ex loss: 0.478129  [13824/19200]
per-ex loss: 0.510052  [14016/19200]
per-ex loss: 0.507039  [14208/19200]
per-ex loss: 0.521747  [14400/19200]
per-ex loss: 0.561608  [14592/19200]
per-ex loss: 0.655445  [14784/19200]
per-ex loss: 0.646418  [14976/19200]
per-ex loss: 0.542528  [15168/19200]
per-ex loss: 0.507228  [15360/19200]
per-ex loss: 0.743721  [15552/19200]
per-ex loss: 0.643345  [15744/19200]
per-ex loss: 0.502968  [15936/19200]
per-ex loss: 0.641272  [16128/19200]
per-ex loss: 0.688613  [16320/19200]
per-ex loss: 0.546359  [16512/19200]
per-ex loss: 0.620895  [16704/19200]
per-ex loss: 0.629282  [16896/19200]
per-ex loss: 0.563615  [17088/19200]
per-ex loss: 0.805583  [17280/19200]
per-ex loss: 0.591104  [17472/19200]
per-ex loss: 0.598617  [17664/19200]
per-ex loss: 0.645990  [17856/19200]
per-ex loss: 0.601202  [18048/19200]
per-ex loss: 0.590283  [18240/19200]
per-ex loss: 0.540816  [18432/19200]
per-ex loss: 0.642300  [18624/19200]
per-ex loss: 0.715036  [18816/19200]
per-ex loss: 0.625196  [19008/19200]
per-ex loss: 0.669903  [19200/19200]
Train Error: Avg loss: 0.59534672
validation Error: 
 Avg loss: 0.57649281 
 F1: 0.418022 
 Precision: 0.353292 
 Recall: 0.511791
 IoU: 0.264240

test Error: 
 Avg loss: 0.50188524 
 F1: 0.504024 
 Precision: 0.465472 
 Recall: 0.549538
 IoU: 0.336920

We have finished training iteration 9
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_5_.pth
per-ex loss: 0.510565  [  192/19200]
per-ex loss: 0.571047  [  384/19200]
per-ex loss: 0.682033  [  576/19200]
per-ex loss: 0.487120  [  768/19200]
per-ex loss: 0.500199  [  960/19200]
per-ex loss: 0.606861  [ 1152/19200]
per-ex loss: 0.503052  [ 1344/19200]
per-ex loss: 0.500871  [ 1536/19200]
per-ex loss: 0.515462  [ 1728/19200]
per-ex loss: 0.593903  [ 1920/19200]
per-ex loss: 0.636132  [ 2112/19200]
per-ex loss: 0.710883  [ 2304/19200]
per-ex loss: 0.533457  [ 2496/19200]
per-ex loss: 0.667696  [ 2688/19200]
per-ex loss: 0.741805  [ 2880/19200]
per-ex loss: 0.500610  [ 3072/19200]
per-ex loss: 0.592258  [ 3264/19200]
per-ex loss: 0.574669  [ 3456/19200]
per-ex loss: 0.736359  [ 3648/19200]
per-ex loss: 0.573800  [ 3840/19200]
per-ex loss: 0.653238  [ 4032/19200]
per-ex loss: 0.613778  [ 4224/19200]
per-ex loss: 0.619713  [ 4416/19200]
per-ex loss: 0.502815  [ 4608/19200]
per-ex loss: 0.530525  [ 4800/19200]
per-ex loss: 0.482752  [ 4992/19200]
per-ex loss: 0.599556  [ 5184/19200]
per-ex loss: 0.638120  [ 5376/19200]
per-ex loss: 0.630806  [ 5568/19200]
per-ex loss: 0.679577  [ 5760/19200]
per-ex loss: 0.729382  [ 5952/19200]
per-ex loss: 0.584957  [ 6144/19200]
per-ex loss: 0.667814  [ 6336/19200]
per-ex loss: 0.594310  [ 6528/19200]
per-ex loss: 0.429737  [ 6720/19200]
per-ex loss: 0.451701  [ 6912/19200]
per-ex loss: 0.609668  [ 7104/19200]
per-ex loss: 0.813926  [ 7296/19200]
per-ex loss: 0.535242  [ 7488/19200]
per-ex loss: 0.629605  [ 7680/19200]
per-ex loss: 0.580590  [ 7872/19200]
per-ex loss: 0.485476  [ 8064/19200]
per-ex loss: 0.503039  [ 8256/19200]
per-ex loss: 0.632426  [ 8448/19200]
per-ex loss: 0.601140  [ 8640/19200]
per-ex loss: 0.464226  [ 8832/19200]
per-ex loss: 0.563197  [ 9024/19200]
per-ex loss: 0.681613  [ 9216/19200]
per-ex loss: 0.612759  [ 9408/19200]
per-ex loss: 0.499524  [ 9600/19200]
per-ex loss: 0.692160  [ 9792/19200]
per-ex loss: 0.505606  [ 9984/19200]
per-ex loss: 0.645539  [10176/19200]
per-ex loss: 0.545547  [10368/19200]
per-ex loss: 0.633985  [10560/19200]
per-ex loss: 0.673962  [10752/19200]
per-ex loss: 0.668863  [10944/19200]
per-ex loss: 0.738622  [11136/19200]
per-ex loss: 0.552043  [11328/19200]
per-ex loss: 0.511403  [11520/19200]
per-ex loss: 0.591651  [11712/19200]
per-ex loss: 0.482446  [11904/19200]
per-ex loss: 0.571605  [12096/19200]
per-ex loss: 0.551401  [12288/19200]
per-ex loss: 0.616684  [12480/19200]
per-ex loss: 0.559269  [12672/19200]
per-ex loss: 0.466457  [12864/19200]
per-ex loss: 0.715461  [13056/19200]
per-ex loss: 0.667364  [13248/19200]
per-ex loss: 0.601697  [13440/19200]
per-ex loss: 0.619552  [13632/19200]
per-ex loss: 0.577913  [13824/19200]
per-ex loss: 0.545950  [14016/19200]
per-ex loss: 0.689689  [14208/19200]
per-ex loss: 0.702648  [14400/19200]
per-ex loss: 0.585481  [14592/19200]
per-ex loss: 0.625187  [14784/19200]
per-ex loss: 0.596428  [14976/19200]
per-ex loss: 0.648482  [15168/19200]
per-ex loss: 0.646256  [15360/19200]
per-ex loss: 0.472332  [15552/19200]
per-ex loss: 0.527510  [15744/19200]
per-ex loss: 0.587293  [15936/19200]
per-ex loss: 0.619271  [16128/19200]
per-ex loss: 0.649338  [16320/19200]
per-ex loss: 0.578834  [16512/19200]
per-ex loss: 0.662103  [16704/19200]
per-ex loss: 0.613415  [16896/19200]
per-ex loss: 0.445830  [17088/19200]
per-ex loss: 0.648995  [17280/19200]
per-ex loss: 0.628042  [17472/19200]
per-ex loss: 0.556041  [17664/19200]
per-ex loss: 0.626403  [17856/19200]
per-ex loss: 0.466992  [18048/19200]
per-ex loss: 0.485991  [18240/19200]
per-ex loss: 0.728640  [18432/19200]
per-ex loss: 0.622507  [18624/19200]
per-ex loss: 0.599125  [18816/19200]
per-ex loss: 0.619288  [19008/19200]
per-ex loss: 0.597291  [19200/19200]
Train Error: Avg loss: 0.59324586
validation Error: 
 Avg loss: 0.55909854 
 F1: 0.427415 
 Precision: 0.344547 
 Recall: 0.562767
 IoU: 0.271791

test Error: 
 Avg loss: 0.50004840 
 F1: 0.506711 
 Precision: 0.436382 
 Recall: 0.604063
 IoU: 0.339325

We have finished training iteration 10
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_8_.pth
per-ex loss: 0.549326  [  192/19200]
per-ex loss: 0.609130  [  384/19200]
per-ex loss: 0.630638  [  576/19200]
per-ex loss: 0.547670  [  768/19200]
per-ex loss: 0.529355  [  960/19200]
per-ex loss: 0.610288  [ 1152/19200]
per-ex loss: 0.523786  [ 1344/19200]
per-ex loss: 0.653207  [ 1536/19200]
per-ex loss: 0.584893  [ 1728/19200]
per-ex loss: 0.462426  [ 1920/19200]
per-ex loss: 0.460965  [ 2112/19200]
per-ex loss: 0.590553  [ 2304/19200]
per-ex loss: 0.567839  [ 2496/19200]
per-ex loss: 0.502282  [ 2688/19200]
per-ex loss: 0.626751  [ 2880/19200]
per-ex loss: 0.593037  [ 3072/19200]
per-ex loss: 0.600673  [ 3264/19200]
per-ex loss: 0.585236  [ 3456/19200]
per-ex loss: 0.739074  [ 3648/19200]
per-ex loss: 0.503434  [ 3840/19200]
per-ex loss: 0.656720  [ 4032/19200]
per-ex loss: 0.499764  [ 4224/19200]
per-ex loss: 0.476382  [ 4416/19200]
per-ex loss: 0.699443  [ 4608/19200]
per-ex loss: 0.545639  [ 4800/19200]
per-ex loss: 0.510324  [ 4992/19200]
per-ex loss: 0.645849  [ 5184/19200]
per-ex loss: 0.592243  [ 5376/19200]
per-ex loss: 0.718758  [ 5568/19200]
per-ex loss: 0.563862  [ 5760/19200]
per-ex loss: 0.611847  [ 5952/19200]
per-ex loss: 0.625779  [ 6144/19200]
per-ex loss: 0.520574  [ 6336/19200]
per-ex loss: 0.512539  [ 6528/19200]
per-ex loss: 0.416013  [ 6720/19200]
per-ex loss: 0.547076  [ 6912/19200]
per-ex loss: 0.647594  [ 7104/19200]
per-ex loss: 0.544558  [ 7296/19200]
per-ex loss: 0.627782  [ 7488/19200]
per-ex loss: 0.761665  [ 7680/19200]
per-ex loss: 0.621753  [ 7872/19200]
per-ex loss: 0.439391  [ 8064/19200]
per-ex loss: 0.518759  [ 8256/19200]
per-ex loss: 0.558762  [ 8448/19200]
per-ex loss: 0.565282  [ 8640/19200]
per-ex loss: 0.481230  [ 8832/19200]
per-ex loss: 0.583658  [ 9024/19200]
per-ex loss: 0.485142  [ 9216/19200]
per-ex loss: 0.653655  [ 9408/19200]
per-ex loss: 0.628954  [ 9600/19200]
per-ex loss: 0.580243  [ 9792/19200]
per-ex loss: 0.542186  [ 9984/19200]
per-ex loss: 0.739555  [10176/19200]
per-ex loss: 0.556489  [10368/19200]
per-ex loss: 0.643099  [10560/19200]
per-ex loss: 0.534405  [10752/19200]
per-ex loss: 0.680530  [10944/19200]
per-ex loss: 0.505683  [11136/19200]
per-ex loss: 0.492974  [11328/19200]
per-ex loss: 0.632974  [11520/19200]
per-ex loss: 0.528965  [11712/19200]
per-ex loss: 0.603200  [11904/19200]
per-ex loss: 0.732203  [12096/19200]
per-ex loss: 0.601274  [12288/19200]
per-ex loss: 0.564374  [12480/19200]
per-ex loss: 0.643755  [12672/19200]
per-ex loss: 0.522412  [12864/19200]
per-ex loss: 0.662107  [13056/19200]
per-ex loss: 0.609999  [13248/19200]
per-ex loss: 0.651496  [13440/19200]
per-ex loss: 0.614547  [13632/19200]
per-ex loss: 0.557191  [13824/19200]
per-ex loss: 0.523825  [14016/19200]
per-ex loss: 0.583439  [14208/19200]
per-ex loss: 0.491338  [14400/19200]
per-ex loss: 0.515698  [14592/19200]
per-ex loss: 0.464532  [14784/19200]
per-ex loss: 0.580999  [14976/19200]
per-ex loss: 0.679358  [15168/19200]
per-ex loss: 0.536924  [15360/19200]
per-ex loss: 0.668654  [15552/19200]
per-ex loss: 0.522522  [15744/19200]
per-ex loss: 0.609927  [15936/19200]
per-ex loss: 0.458022  [16128/19200]
per-ex loss: 0.553047  [16320/19200]
per-ex loss: 0.598021  [16512/19200]
per-ex loss: 0.534310  [16704/19200]
per-ex loss: 0.458758  [16896/19200]
per-ex loss: 0.463307  [17088/19200]
per-ex loss: 0.592579  [17280/19200]
per-ex loss: 0.483825  [17472/19200]
per-ex loss: 0.681356  [17664/19200]
per-ex loss: 0.666366  [17856/19200]
per-ex loss: 0.509163  [18048/19200]
per-ex loss: 0.567381  [18240/19200]
per-ex loss: 0.579204  [18432/19200]
per-ex loss: 0.533880  [18624/19200]
per-ex loss: 0.654368  [18816/19200]
per-ex loss: 0.576709  [19008/19200]
per-ex loss: 0.521527  [19200/19200]
Train Error: Avg loss: 0.57540258
validation Error: 
 Avg loss: 0.55132159 
 F1: 0.430801 
 Precision: 0.341530 
 Recall: 0.583257
 IoU: 0.274536

test Error: 
 Avg loss: 0.49829310 
 F1: 0.507572 
 Precision: 0.429357 
 Recall: 0.620633
 IoU: 0.340098

We have finished training iteration 11
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_9_.pth
per-ex loss: 0.419096  [  192/19200]
per-ex loss: 0.469905  [  384/19200]
per-ex loss: 0.636529  [  576/19200]
per-ex loss: 0.464466  [  768/19200]
per-ex loss: 0.520685  [  960/19200]
per-ex loss: 0.579354  [ 1152/19200]
per-ex loss: 0.504271  [ 1344/19200]
per-ex loss: 0.623170  [ 1536/19200]
per-ex loss: 0.639391  [ 1728/19200]
per-ex loss: 0.494233  [ 1920/19200]
per-ex loss: 0.488230  [ 2112/19200]
per-ex loss: 0.625622  [ 2304/19200]
per-ex loss: 0.836078  [ 2496/19200]
per-ex loss: 0.723786  [ 2688/19200]
per-ex loss: 0.579269  [ 2880/19200]
per-ex loss: 0.543189  [ 3072/19200]
per-ex loss: 0.513951  [ 3264/19200]
per-ex loss: 0.596556  [ 3456/19200]
per-ex loss: 0.472157  [ 3648/19200]
per-ex loss: 0.505927  [ 3840/19200]
per-ex loss: 0.692081  [ 4032/19200]
per-ex loss: 0.465497  [ 4224/19200]
per-ex loss: 0.553170  [ 4416/19200]
per-ex loss: 0.718938  [ 4608/19200]
per-ex loss: 0.507622  [ 4800/19200]
per-ex loss: 0.609290  [ 4992/19200]
per-ex loss: 0.568000  [ 5184/19200]
per-ex loss: 0.617294  [ 5376/19200]
per-ex loss: 0.572440  [ 5568/19200]
per-ex loss: 0.671512  [ 5760/19200]
per-ex loss: 0.708414  [ 5952/19200]
per-ex loss: 0.679993  [ 6144/19200]
per-ex loss: 0.473202  [ 6336/19200]
per-ex loss: 0.477174  [ 6528/19200]
per-ex loss: 0.574506  [ 6720/19200]
per-ex loss: 0.458580  [ 6912/19200]
per-ex loss: 0.497314  [ 7104/19200]
per-ex loss: 0.479588  [ 7296/19200]
per-ex loss: 0.511541  [ 7488/19200]
per-ex loss: 0.486923  [ 7680/19200]
per-ex loss: 0.587024  [ 7872/19200]
per-ex loss: 0.600469  [ 8064/19200]
per-ex loss: 0.572127  [ 8256/19200]
per-ex loss: 0.617648  [ 8448/19200]
per-ex loss: 0.557642  [ 8640/19200]
per-ex loss: 0.704015  [ 8832/19200]
per-ex loss: 0.512098  [ 9024/19200]
per-ex loss: 0.563694  [ 9216/19200]
per-ex loss: 0.574688  [ 9408/19200]
per-ex loss: 0.492625  [ 9600/19200]
per-ex loss: 0.744950  [ 9792/19200]
per-ex loss: 0.764984  [ 9984/19200]
per-ex loss: 0.591872  [10176/19200]
per-ex loss: 0.590441  [10368/19200]
per-ex loss: 0.529864  [10560/19200]
per-ex loss: 0.635162  [10752/19200]
per-ex loss: 0.694054  [10944/19200]
per-ex loss: 0.634875  [11136/19200]
per-ex loss: 0.476592  [11328/19200]
per-ex loss: 0.617805  [11520/19200]
per-ex loss: 0.622796  [11712/19200]
per-ex loss: 0.618895  [11904/19200]
per-ex loss: 0.674027  [12096/19200]
per-ex loss: 0.482108  [12288/19200]
per-ex loss: 0.458835  [12480/19200]
per-ex loss: 0.590876  [12672/19200]
per-ex loss: 0.534147  [12864/19200]
per-ex loss: 0.528612  [13056/19200]
per-ex loss: 0.522550  [13248/19200]
per-ex loss: 0.677758  [13440/19200]
per-ex loss: 0.587496  [13632/19200]
per-ex loss: 0.643900  [13824/19200]
per-ex loss: 0.536540  [14016/19200]
per-ex loss: 0.555413  [14208/19200]
per-ex loss: 0.463383  [14400/19200]
per-ex loss: 0.573860  [14592/19200]
per-ex loss: 0.509965  [14784/19200]
per-ex loss: 0.579815  [14976/19200]
per-ex loss: 0.696290  [15168/19200]
per-ex loss: 0.577234  [15360/19200]
per-ex loss: 0.657450  [15552/19200]
per-ex loss: 0.529257  [15744/19200]
per-ex loss: 0.468431  [15936/19200]
per-ex loss: 0.537884  [16128/19200]
per-ex loss: 0.692205  [16320/19200]
per-ex loss: 0.669472  [16512/19200]
per-ex loss: 0.559199  [16704/19200]
per-ex loss: 0.621528  [16896/19200]
per-ex loss: 0.562863  [17088/19200]
per-ex loss: 0.518455  [17280/19200]
per-ex loss: 0.577360  [17472/19200]
per-ex loss: 0.587472  [17664/19200]
per-ex loss: 0.501597  [17856/19200]
per-ex loss: 0.577859  [18048/19200]
per-ex loss: 0.587810  [18240/19200]
per-ex loss: 0.538898  [18432/19200]
per-ex loss: 0.491691  [18624/19200]
per-ex loss: 0.542296  [18816/19200]
per-ex loss: 0.606924  [19008/19200]
per-ex loss: 0.583718  [19200/19200]
Train Error: Avg loss: 0.57468443
validation Error: 
 Avg loss: 0.55914426 
 F1: 0.445347 
 Precision: 0.423004 
 Recall: 0.470183
 IoU: 0.286461

test Error: 
 Avg loss: 0.48750418 
 F1: 0.516405 
 Precision: 0.528682 
 Recall: 0.504685
 IoU: 0.348077

We have finished training iteration 12
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_10_.pth
per-ex loss: 0.648666  [  192/19200]
per-ex loss: 0.557777  [  384/19200]
per-ex loss: 0.536149  [  576/19200]
per-ex loss: 0.573453  [  768/19200]
per-ex loss: 0.438552  [  960/19200]
per-ex loss: 0.619287  [ 1152/19200]
per-ex loss: 0.558410  [ 1344/19200]
per-ex loss: 0.596119  [ 1536/19200]
per-ex loss: 0.432673  [ 1728/19200]
per-ex loss: 0.575808  [ 1920/19200]
per-ex loss: 0.473834  [ 2112/19200]
per-ex loss: 0.471812  [ 2304/19200]
per-ex loss: 0.743421  [ 2496/19200]
per-ex loss: 0.533800  [ 2688/19200]
per-ex loss: 0.640838  [ 2880/19200]
per-ex loss: 0.573162  [ 3072/19200]
per-ex loss: 0.721905  [ 3264/19200]
per-ex loss: 0.658176  [ 3456/19200]
per-ex loss: 0.489757  [ 3648/19200]
per-ex loss: 0.659558  [ 3840/19200]
per-ex loss: 0.557673  [ 4032/19200]
per-ex loss: 0.573797  [ 4224/19200]
per-ex loss: 0.443151  [ 4416/19200]
per-ex loss: 0.597012  [ 4608/19200]
per-ex loss: 0.480700  [ 4800/19200]
per-ex loss: 0.629365  [ 4992/19200]
per-ex loss: 0.555536  [ 5184/19200]
per-ex loss: 0.559586  [ 5376/19200]
per-ex loss: 0.548515  [ 5568/19200]
per-ex loss: 0.639315  [ 5760/19200]
per-ex loss: 0.768339  [ 5952/19200]
per-ex loss: 0.502015  [ 6144/19200]
per-ex loss: 0.611140  [ 6336/19200]
per-ex loss: 0.646697  [ 6528/19200]
per-ex loss: 0.503322  [ 6720/19200]
per-ex loss: 0.687500  [ 6912/19200]
per-ex loss: 0.481506  [ 7104/19200]
per-ex loss: 0.573690  [ 7296/19200]
per-ex loss: 0.546985  [ 7488/19200]
per-ex loss: 0.666105  [ 7680/19200]
per-ex loss: 0.590996  [ 7872/19200]
per-ex loss: 0.489133  [ 8064/19200]
per-ex loss: 0.598499  [ 8256/19200]
per-ex loss: 0.718339  [ 8448/19200]
per-ex loss: 0.471790  [ 8640/19200]
per-ex loss: 0.557791  [ 8832/19200]
per-ex loss: 0.709905  [ 9024/19200]
per-ex loss: 0.529037  [ 9216/19200]
per-ex loss: 0.554674  [ 9408/19200]
per-ex loss: 0.546119  [ 9600/19200]
per-ex loss: 0.487122  [ 9792/19200]
per-ex loss: 0.594382  [ 9984/19200]
per-ex loss: 0.508763  [10176/19200]
per-ex loss: 0.496499  [10368/19200]
per-ex loss: 0.575542  [10560/19200]
per-ex loss: 0.619903  [10752/19200]
per-ex loss: 0.566029  [10944/19200]
per-ex loss: 0.631727  [11136/19200]
per-ex loss: 0.520762  [11328/19200]
per-ex loss: 0.535342  [11520/19200]
per-ex loss: 0.463791  [11712/19200]
per-ex loss: 0.658781  [11904/19200]
per-ex loss: 0.566350  [12096/19200]
per-ex loss: 0.579966  [12288/19200]
per-ex loss: 0.541927  [12480/19200]
per-ex loss: 0.609410  [12672/19200]
per-ex loss: 0.708876  [12864/19200]
per-ex loss: 0.528944  [13056/19200]
per-ex loss: 0.558737  [13248/19200]
per-ex loss: 0.718548  [13440/19200]
per-ex loss: 0.582012  [13632/19200]
per-ex loss: 0.466211  [13824/19200]
per-ex loss: 0.614585  [14016/19200]
per-ex loss: 0.692052  [14208/19200]
per-ex loss: 0.574628  [14400/19200]
per-ex loss: 0.681662  [14592/19200]
per-ex loss: 0.470000  [14784/19200]
per-ex loss: 0.536502  [14976/19200]
per-ex loss: 0.547032  [15168/19200]
per-ex loss: 0.490856  [15360/19200]
per-ex loss: 0.451155  [15552/19200]
per-ex loss: 0.698912  [15744/19200]
per-ex loss: 0.598072  [15936/19200]
per-ex loss: 0.614702  [16128/19200]
per-ex loss: 0.516313  [16320/19200]
per-ex loss: 0.530333  [16512/19200]
per-ex loss: 0.563508  [16704/19200]
per-ex loss: 0.525054  [16896/19200]
per-ex loss: 0.535138  [17088/19200]
per-ex loss: 0.549904  [17280/19200]
per-ex loss: 0.467594  [17472/19200]
per-ex loss: 0.549919  [17664/19200]
per-ex loss: 0.538450  [17856/19200]
per-ex loss: 0.508059  [18048/19200]
per-ex loss: 0.650866  [18240/19200]
per-ex loss: 0.534921  [18432/19200]
per-ex loss: 0.735536  [18624/19200]
per-ex loss: 0.438998  [18816/19200]
per-ex loss: 0.679072  [19008/19200]
per-ex loss: 0.686453  [19200/19200]
Train Error: Avg loss: 0.57341293
validation Error: 
 Avg loss: 0.56837112 
 F1: 0.423381 
 Precision: 0.359379 
 Recall: 0.515118
 IoU: 0.268537

test Error: 
 Avg loss: 0.50158429 
 F1: 0.503301 
 Precision: 0.445835 
 Recall: 0.577773
 IoU: 0.336274

We have finished training iteration 13
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_7_.pth
per-ex loss: 0.595994  [  192/19200]
per-ex loss: 0.707871  [  384/19200]
per-ex loss: 0.583299  [  576/19200]
per-ex loss: 0.604204  [  768/19200]
per-ex loss: 0.568316  [  960/19200]
per-ex loss: 0.550490  [ 1152/19200]
per-ex loss: 0.584646  [ 1344/19200]
per-ex loss: 0.578250  [ 1536/19200]
per-ex loss: 0.741470  [ 1728/19200]
per-ex loss: 0.500195  [ 1920/19200]
per-ex loss: 0.599896  [ 2112/19200]
per-ex loss: 0.492025  [ 2304/19200]
per-ex loss: 0.658467  [ 2496/19200]
per-ex loss: 0.543822  [ 2688/19200]
per-ex loss: 0.670755  [ 2880/19200]
per-ex loss: 0.463227  [ 3072/19200]
per-ex loss: 0.600446  [ 3264/19200]
per-ex loss: 0.547893  [ 3456/19200]
per-ex loss: 0.566208  [ 3648/19200]
per-ex loss: 0.540622  [ 3840/19200]
per-ex loss: 0.541877  [ 4032/19200]
per-ex loss: 0.543907  [ 4224/19200]
per-ex loss: 0.664206  [ 4416/19200]
per-ex loss: 0.733999  [ 4608/19200]
per-ex loss: 0.557175  [ 4800/19200]
per-ex loss: 0.448220  [ 4992/19200]
per-ex loss: 0.661441  [ 5184/19200]
per-ex loss: 0.642965  [ 5376/19200]
per-ex loss: 0.725413  [ 5568/19200]
per-ex loss: 0.540044  [ 5760/19200]
per-ex loss: 0.563992  [ 5952/19200]
per-ex loss: 0.452996  [ 6144/19200]
per-ex loss: 0.590538  [ 6336/19200]
per-ex loss: 0.771747  [ 6528/19200]
per-ex loss: 0.741187  [ 6720/19200]
per-ex loss: 0.485881  [ 6912/19200]
per-ex loss: 0.637510  [ 7104/19200]
per-ex loss: 0.568411  [ 7296/19200]
per-ex loss: 0.590922  [ 7488/19200]
per-ex loss: 0.443159  [ 7680/19200]
per-ex loss: 0.529999  [ 7872/19200]
per-ex loss: 0.611363  [ 8064/19200]
per-ex loss: 0.481439  [ 8256/19200]
per-ex loss: 0.574114  [ 8448/19200]
per-ex loss: 0.524602  [ 8640/19200]
per-ex loss: 0.615015  [ 8832/19200]
per-ex loss: 0.542083  [ 9024/19200]
per-ex loss: 0.617184  [ 9216/19200]
per-ex loss: 0.629402  [ 9408/19200]
per-ex loss: 0.699876  [ 9600/19200]
per-ex loss: 0.605272  [ 9792/19200]
per-ex loss: 0.578013  [ 9984/19200]
per-ex loss: 0.611105  [10176/19200]
per-ex loss: 0.699234  [10368/19200]
per-ex loss: 0.569719  [10560/19200]
per-ex loss: 0.654370  [10752/19200]
per-ex loss: 0.547174  [10944/19200]
per-ex loss: 0.602180  [11136/19200]
per-ex loss: 0.632926  [11328/19200]
per-ex loss: 0.574305  [11520/19200]
per-ex loss: 0.635041  [11712/19200]
per-ex loss: 0.567298  [11904/19200]
per-ex loss: 0.603210  [12096/19200]
per-ex loss: 0.517109  [12288/19200]
per-ex loss: 0.735457  [12480/19200]
per-ex loss: 0.528290  [12672/19200]
per-ex loss: 0.633269  [12864/19200]
per-ex loss: 0.650660  [13056/19200]
per-ex loss: 0.536417  [13248/19200]
per-ex loss: 0.522867  [13440/19200]
per-ex loss: 0.436696  [13632/19200]
per-ex loss: 0.555886  [13824/19200]
per-ex loss: 0.624033  [14016/19200]
per-ex loss: 0.492935  [14208/19200]
per-ex loss: 0.450944  [14400/19200]
per-ex loss: 0.477670  [14592/19200]
per-ex loss: 0.660082  [14784/19200]
per-ex loss: 0.596153  [14976/19200]
per-ex loss: 0.552215  [15168/19200]
per-ex loss: 0.613372  [15360/19200]
per-ex loss: 0.646014  [15552/19200]
per-ex loss: 0.552829  [15744/19200]
per-ex loss: 0.580881  [15936/19200]
per-ex loss: 0.621855  [16128/19200]
per-ex loss: 0.708244  [16320/19200]
per-ex loss: 0.595036  [16512/19200]
per-ex loss: 0.650210  [16704/19200]
per-ex loss: 0.543061  [16896/19200]
per-ex loss: 0.518665  [17088/19200]
per-ex loss: 0.561315  [17280/19200]
per-ex loss: 0.532850  [17472/19200]
per-ex loss: 0.585103  [17664/19200]
per-ex loss: 0.501724  [17856/19200]
per-ex loss: 0.575441  [18048/19200]
per-ex loss: 0.610077  [18240/19200]
per-ex loss: 0.683864  [18432/19200]
per-ex loss: 0.532495  [18624/19200]
per-ex loss: 0.618933  [18816/19200]
per-ex loss: 0.593192  [19008/19200]
per-ex loss: 0.718385  [19200/19200]
Train Error: Avg loss: 0.58822841
validation Error: 
 Avg loss: 0.54351775 
 F1: 0.424599 
 Precision: 0.317027 
 Recall: 0.642665
 IoU: 0.269518

test Error: 
 Avg loss: 0.50621384 
 F1: 0.498849 
 Precision: 0.391846 
 Recall: 0.686245
 IoU: 0.332311

We have finished training iteration 14
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_12_.pth
per-ex loss: 0.521626  [  192/19200]
per-ex loss: 0.730906  [  384/19200]
per-ex loss: 0.529857  [  576/19200]
per-ex loss: 0.524499  [  768/19200]
per-ex loss: 0.474280  [  960/19200]
per-ex loss: 0.550967  [ 1152/19200]
per-ex loss: 0.716173  [ 1344/19200]
per-ex loss: 0.545260  [ 1536/19200]
per-ex loss: 0.587904  [ 1728/19200]
per-ex loss: 0.505073  [ 1920/19200]
per-ex loss: 0.622936  [ 2112/19200]
per-ex loss: 0.506622  [ 2304/19200]
per-ex loss: 0.749069  [ 2496/19200]
per-ex loss: 0.627459  [ 2688/19200]
per-ex loss: 0.767889  [ 2880/19200]
per-ex loss: 0.562721  [ 3072/19200]
per-ex loss: 0.602794  [ 3264/19200]
per-ex loss: 0.540209  [ 3456/19200]
per-ex loss: 0.648685  [ 3648/19200]
per-ex loss: 0.600196  [ 3840/19200]
per-ex loss: 0.687539  [ 4032/19200]
per-ex loss: 0.539744  [ 4224/19200]
per-ex loss: 0.619457  [ 4416/19200]
per-ex loss: 0.589483  [ 4608/19200]
per-ex loss: 0.560271  [ 4800/19200]
per-ex loss: 0.576361  [ 4992/19200]
per-ex loss: 0.531809  [ 5184/19200]
per-ex loss: 0.537980  [ 5376/19200]
per-ex loss: 0.583014  [ 5568/19200]
per-ex loss: 0.568871  [ 5760/19200]
per-ex loss: 0.634709  [ 5952/19200]
per-ex loss: 0.634729  [ 6144/19200]
per-ex loss: 0.590042  [ 6336/19200]
per-ex loss: 0.491629  [ 6528/19200]
per-ex loss: 0.575398  [ 6720/19200]
per-ex loss: 0.619407  [ 6912/19200]
per-ex loss: 0.510920  [ 7104/19200]
per-ex loss: 0.607369  [ 7296/19200]
per-ex loss: 0.653341  [ 7488/19200]
per-ex loss: 0.586217  [ 7680/19200]
per-ex loss: 0.581162  [ 7872/19200]
per-ex loss: 0.583992  [ 8064/19200]
per-ex loss: 0.601262  [ 8256/19200]
per-ex loss: 0.504724  [ 8448/19200]
per-ex loss: 0.460416  [ 8640/19200]
per-ex loss: 0.469184  [ 8832/19200]
per-ex loss: 0.579674  [ 9024/19200]
per-ex loss: 0.441438  [ 9216/19200]
per-ex loss: 0.580411  [ 9408/19200]
per-ex loss: 0.634176  [ 9600/19200]
per-ex loss: 0.629013  [ 9792/19200]
per-ex loss: 0.563197  [ 9984/19200]
per-ex loss: 0.547149  [10176/19200]
per-ex loss: 0.481582  [10368/19200]
per-ex loss: 0.477397  [10560/19200]
per-ex loss: 0.586394  [10752/19200]
per-ex loss: 0.519280  [10944/19200]
per-ex loss: 0.634362  [11136/19200]
per-ex loss: 0.600482  [11328/19200]
per-ex loss: 0.569549  [11520/19200]
per-ex loss: 0.585559  [11712/19200]
per-ex loss: 0.538628  [11904/19200]
per-ex loss: 0.487129  [12096/19200]
per-ex loss: 0.545948  [12288/19200]
per-ex loss: 0.542708  [12480/19200]
per-ex loss: 0.739624  [12672/19200]
per-ex loss: 0.510247  [12864/19200]
per-ex loss: 0.552109  [13056/19200]
per-ex loss: 0.556936  [13248/19200]
per-ex loss: 0.710175  [13440/19200]
per-ex loss: 0.611490  [13632/19200]
per-ex loss: 0.689189  [13824/19200]
per-ex loss: 0.501940  [14016/19200]
per-ex loss: 0.492932  [14208/19200]
per-ex loss: 0.583643  [14400/19200]
per-ex loss: 0.463474  [14592/19200]
per-ex loss: 0.667654  [14784/19200]
per-ex loss: 0.728865  [14976/19200]
per-ex loss: 0.463847  [15168/19200]
per-ex loss: 0.577044  [15360/19200]
per-ex loss: 0.463947  [15552/19200]
per-ex loss: 0.679954  [15744/19200]
per-ex loss: 0.602964  [15936/19200]
per-ex loss: 0.552698  [16128/19200]
per-ex loss: 0.580975  [16320/19200]
per-ex loss: 0.636267  [16512/19200]
per-ex loss: 0.466195  [16704/19200]
per-ex loss: 0.446063  [16896/19200]
per-ex loss: 0.583348  [17088/19200]
per-ex loss: 0.485325  [17280/19200]
per-ex loss: 0.630232  [17472/19200]
per-ex loss: 0.670823  [17664/19200]
per-ex loss: 0.645074  [17856/19200]
per-ex loss: 0.611059  [18048/19200]
per-ex loss: 0.689881  [18240/19200]
per-ex loss: 0.501002  [18432/19200]
per-ex loss: 0.493304  [18624/19200]
per-ex loss: 0.634273  [18816/19200]
per-ex loss: 0.540120  [19008/19200]
per-ex loss: 0.545687  [19200/19200]
Train Error: Avg loss: 0.57666592
validation Error: 
 Avg loss: 0.55017291 
 F1: 0.437883 
 Precision: 0.380722 
 Recall: 0.515241
 IoU: 0.280314

test Error: 
 Avg loss: 0.47958440 
 F1: 0.524746 
 Precision: 0.492005 
 Recall: 0.562156
 IoU: 0.355699

We have finished training iteration 15
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_13_.pth
per-ex loss: 0.470705  [  192/19200]
per-ex loss: 0.497127  [  384/19200]
per-ex loss: 0.573277  [  576/19200]
per-ex loss: 0.469836  [  768/19200]
per-ex loss: 0.469773  [  960/19200]
per-ex loss: 0.564203  [ 1152/19200]
per-ex loss: 0.636429  [ 1344/19200]
per-ex loss: 0.669395  [ 1536/19200]
per-ex loss: 0.556731  [ 1728/19200]
per-ex loss: 0.676857  [ 1920/19200]
per-ex loss: 0.476823  [ 2112/19200]
per-ex loss: 0.647888  [ 2304/19200]
per-ex loss: 0.456187  [ 2496/19200]
per-ex loss: 0.484639  [ 2688/19200]
per-ex loss: 0.620182  [ 2880/19200]
per-ex loss: 0.673130  [ 3072/19200]
per-ex loss: 0.673874  [ 3264/19200]
per-ex loss: 0.684909  [ 3456/19200]
per-ex loss: 0.486738  [ 3648/19200]
per-ex loss: 0.677325  [ 3840/19200]
per-ex loss: 0.706560  [ 4032/19200]
per-ex loss: 0.527862  [ 4224/19200]
per-ex loss: 0.555920  [ 4416/19200]
per-ex loss: 0.495459  [ 4608/19200]
per-ex loss: 0.487300  [ 4800/19200]
per-ex loss: 0.474524  [ 4992/19200]
per-ex loss: 0.627320  [ 5184/19200]
per-ex loss: 0.710824  [ 5376/19200]
per-ex loss: 0.578106  [ 5568/19200]
per-ex loss: 0.540502  [ 5760/19200]
per-ex loss: 0.589564  [ 5952/19200]
per-ex loss: 0.642696  [ 6144/19200]
per-ex loss: 0.451231  [ 6336/19200]
per-ex loss: 0.643694  [ 6528/19200]
per-ex loss: 0.527062  [ 6720/19200]
per-ex loss: 0.571748  [ 6912/19200]
per-ex loss: 0.692388  [ 7104/19200]
per-ex loss: 0.678851  [ 7296/19200]
per-ex loss: 0.514162  [ 7488/19200]
per-ex loss: 0.519922  [ 7680/19200]
per-ex loss: 0.459636  [ 7872/19200]
per-ex loss: 0.648939  [ 8064/19200]
per-ex loss: 0.571564  [ 8256/19200]
per-ex loss: 0.705246  [ 8448/19200]
per-ex loss: 0.487965  [ 8640/19200]
per-ex loss: 0.536155  [ 8832/19200]
per-ex loss: 0.614633  [ 9024/19200]
per-ex loss: 0.497083  [ 9216/19200]
per-ex loss: 0.680361  [ 9408/19200]
per-ex loss: 0.590626  [ 9600/19200]
per-ex loss: 0.543566  [ 9792/19200]
per-ex loss: 0.718534  [ 9984/19200]
per-ex loss: 0.629855  [10176/19200]
per-ex loss: 0.530084  [10368/19200]
per-ex loss: 0.659144  [10560/19200]
per-ex loss: 0.625123  [10752/19200]
per-ex loss: 0.536584  [10944/19200]
per-ex loss: 0.577570  [11136/19200]
per-ex loss: 0.436920  [11328/19200]
per-ex loss: 0.541552  [11520/19200]
per-ex loss: 0.531302  [11712/19200]
per-ex loss: 0.506626  [11904/19200]
per-ex loss: 0.478337  [12096/19200]
per-ex loss: 0.611920  [12288/19200]
per-ex loss: 0.684116  [12480/19200]
per-ex loss: 0.806691  [12672/19200]
per-ex loss: 0.574129  [12864/19200]
per-ex loss: 0.642257  [13056/19200]
per-ex loss: 0.437729  [13248/19200]
per-ex loss: 0.525804  [13440/19200]
per-ex loss: 0.533358  [13632/19200]
per-ex loss: 0.515871  [13824/19200]
per-ex loss: 0.632088  [14016/19200]
per-ex loss: 0.447807  [14208/19200]
per-ex loss: 0.532006  [14400/19200]
per-ex loss: 0.621130  [14592/19200]
per-ex loss: 0.545643  [14784/19200]
per-ex loss: 0.658485  [14976/19200]
per-ex loss: 0.504174  [15168/19200]
per-ex loss: 0.622927  [15360/19200]
per-ex loss: 0.552374  [15552/19200]
per-ex loss: 0.766421  [15744/19200]
per-ex loss: 0.470758  [15936/19200]
per-ex loss: 0.687881  [16128/19200]
per-ex loss: 0.568683  [16320/19200]
per-ex loss: 0.550587  [16512/19200]
per-ex loss: 0.634910  [16704/19200]
per-ex loss: 0.713065  [16896/19200]
per-ex loss: 0.610383  [17088/19200]
per-ex loss: 0.623200  [17280/19200]
per-ex loss: 0.571862  [17472/19200]
per-ex loss: 0.495005  [17664/19200]
per-ex loss: 0.450890  [17856/19200]
per-ex loss: 0.560831  [18048/19200]
per-ex loss: 0.732739  [18240/19200]
per-ex loss: 0.567838  [18432/19200]
per-ex loss: 0.569382  [18624/19200]
per-ex loss: 0.505591  [18816/19200]
per-ex loss: 0.515456  [19008/19200]
per-ex loss: 0.683682  [19200/19200]
Train Error: Avg loss: 0.57836802
validation Error: 
 Avg loss: 0.60382261 
 F1: 0.405564 
 Precision: 0.431246 
 Recall: 0.382770
 IoU: 0.254362

test Error: 
 Avg loss: 0.52265561 
 F1: 0.480292 
 Precision: 0.577793 
 Recall: 0.410947
 IoU: 0.316043

We have finished training iteration 16
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_6_.pth
per-ex loss: 0.582039  [  192/19200]
per-ex loss: 0.645146  [  384/19200]
per-ex loss: 0.653037  [  576/19200]
per-ex loss: 0.506635  [  768/19200]
per-ex loss: 0.727105  [  960/19200]
per-ex loss: 0.613208  [ 1152/19200]
per-ex loss: 0.557773  [ 1344/19200]
per-ex loss: 0.756409  [ 1536/19200]
per-ex loss: 0.643361  [ 1728/19200]
per-ex loss: 0.704423  [ 1920/19200]
per-ex loss: 0.610565  [ 2112/19200]
per-ex loss: 0.585479  [ 2304/19200]
per-ex loss: 0.434898  [ 2496/19200]
per-ex loss: 0.649823  [ 2688/19200]
per-ex loss: 0.518168  [ 2880/19200]
per-ex loss: 0.516873  [ 3072/19200]
per-ex loss: 0.507517  [ 3264/19200]
per-ex loss: 0.567122  [ 3456/19200]
per-ex loss: 0.623693  [ 3648/19200]
per-ex loss: 0.506103  [ 3840/19200]
per-ex loss: 0.526393  [ 4032/19200]
per-ex loss: 0.590763  [ 4224/19200]
per-ex loss: 0.594890  [ 4416/19200]
per-ex loss: 0.631992  [ 4608/19200]
per-ex loss: 0.585709  [ 4800/19200]
per-ex loss: 0.477676  [ 4992/19200]
per-ex loss: 0.477149  [ 5184/19200]
per-ex loss: 0.524490  [ 5376/19200]
per-ex loss: 0.520230  [ 5568/19200]
per-ex loss: 0.695881  [ 5760/19200]
per-ex loss: 0.651677  [ 5952/19200]
per-ex loss: 0.572556  [ 6144/19200]
per-ex loss: 0.505581  [ 6336/19200]
per-ex loss: 0.627129  [ 6528/19200]
per-ex loss: 0.488915  [ 6720/19200]
per-ex loss: 0.627243  [ 6912/19200]
per-ex loss: 0.709884  [ 7104/19200]
per-ex loss: 0.589473  [ 7296/19200]
per-ex loss: 0.490089  [ 7488/19200]
per-ex loss: 0.606883  [ 7680/19200]
per-ex loss: 0.553996  [ 7872/19200]
per-ex loss: 0.529155  [ 8064/19200]
per-ex loss: 0.540578  [ 8256/19200]
per-ex loss: 0.554844  [ 8448/19200]
per-ex loss: 0.611985  [ 8640/19200]
per-ex loss: 0.552509  [ 8832/19200]
per-ex loss: 0.599060  [ 9024/19200]
per-ex loss: 0.564460  [ 9216/19200]
per-ex loss: 0.555747  [ 9408/19200]
per-ex loss: 0.561818  [ 9600/19200]
per-ex loss: 0.497327  [ 9792/19200]
per-ex loss: 0.545839  [ 9984/19200]
per-ex loss: 0.669202  [10176/19200]
per-ex loss: 0.652646  [10368/19200]
per-ex loss: 0.765865  [10560/19200]
per-ex loss: 0.591925  [10752/19200]
per-ex loss: 0.540631  [10944/19200]
per-ex loss: 0.498803  [11136/19200]
per-ex loss: 0.690920  [11328/19200]
per-ex loss: 0.571017  [11520/19200]
per-ex loss: 0.549719  [11712/19200]
per-ex loss: 0.568147  [11904/19200]
per-ex loss: 0.526695  [12096/19200]
per-ex loss: 0.666237  [12288/19200]
per-ex loss: 0.618678  [12480/19200]
per-ex loss: 0.579646  [12672/19200]
per-ex loss: 0.540807  [12864/19200]
per-ex loss: 0.761497  [13056/19200]
per-ex loss: 0.523584  [13248/19200]
per-ex loss: 0.603160  [13440/19200]
per-ex loss: 0.615291  [13632/19200]
per-ex loss: 0.489611  [13824/19200]
per-ex loss: 0.623967  [14016/19200]
per-ex loss: 0.655599  [14208/19200]
per-ex loss: 0.524271  [14400/19200]
per-ex loss: 0.555395  [14592/19200]
per-ex loss: 0.537173  [14784/19200]
per-ex loss: 0.705194  [14976/19200]
per-ex loss: 0.609556  [15168/19200]
per-ex loss: 0.655151  [15360/19200]
per-ex loss: 0.599318  [15552/19200]
per-ex loss: 0.554694  [15744/19200]
per-ex loss: 0.683962  [15936/19200]
per-ex loss: 0.529449  [16128/19200]
per-ex loss: 0.580666  [16320/19200]
per-ex loss: 0.493283  [16512/19200]
per-ex loss: 0.502053  [16704/19200]
per-ex loss: 0.516368  [16896/19200]
per-ex loss: 0.462525  [17088/19200]
per-ex loss: 0.483504  [17280/19200]
per-ex loss: 0.586053  [17472/19200]
per-ex loss: 0.601851  [17664/19200]
per-ex loss: 0.460998  [17856/19200]
per-ex loss: 0.550922  [18048/19200]
per-ex loss: 0.610157  [18240/19200]
per-ex loss: 0.753404  [18432/19200]
per-ex loss: 0.532760  [18624/19200]
per-ex loss: 0.626620  [18816/19200]
per-ex loss: 0.664315  [19008/19200]
per-ex loss: 0.514789  [19200/19200]
Train Error: Avg loss: 0.58269378
validation Error: 
 Avg loss: 0.54911115 
 F1: 0.442306 
 Precision: 0.388755 
 Recall: 0.512967
 IoU: 0.283949

test Error: 
 Avg loss: 0.48904115 
 F1: 0.514285 
 Precision: 0.489666 
 Recall: 0.541511
 IoU: 0.346153

We have finished training iteration 17
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_11_.pth
per-ex loss: 0.813100  [  192/19200]
per-ex loss: 0.497131  [  384/19200]
per-ex loss: 0.427637  [  576/19200]
per-ex loss: 0.443487  [  768/19200]
per-ex loss: 0.611188  [  960/19200]
per-ex loss: 0.743327  [ 1152/19200]
per-ex loss: 0.581576  [ 1344/19200]
per-ex loss: 0.532728  [ 1536/19200]
per-ex loss: 0.462220  [ 1728/19200]
per-ex loss: 0.542730  [ 1920/19200]
per-ex loss: 0.642547  [ 2112/19200]
per-ex loss: 0.423175  [ 2304/19200]
per-ex loss: 0.473417  [ 2496/19200]
per-ex loss: 0.554026  [ 2688/19200]
per-ex loss: 0.520622  [ 2880/19200]
per-ex loss: 0.658349  [ 3072/19200]
per-ex loss: 0.611646  [ 3264/19200]
per-ex loss: 0.474102  [ 3456/19200]
per-ex loss: 0.605893  [ 3648/19200]
per-ex loss: 0.609159  [ 3840/19200]
per-ex loss: 0.557699  [ 4032/19200]
per-ex loss: 0.602663  [ 4224/19200]
per-ex loss: 0.485774  [ 4416/19200]
per-ex loss: 0.634523  [ 4608/19200]
per-ex loss: 0.569055  [ 4800/19200]
per-ex loss: 0.533483  [ 4992/19200]
per-ex loss: 0.516348  [ 5184/19200]
per-ex loss: 0.489324  [ 5376/19200]
per-ex loss: 0.725756  [ 5568/19200]
per-ex loss: 0.659104  [ 5760/19200]
per-ex loss: 0.684708  [ 5952/19200]
per-ex loss: 0.488825  [ 6144/19200]
per-ex loss: 0.711141  [ 6336/19200]
per-ex loss: 0.614620  [ 6528/19200]
per-ex loss: 0.543963  [ 6720/19200]
per-ex loss: 0.535344  [ 6912/19200]
per-ex loss: 0.441260  [ 7104/19200]
per-ex loss: 0.465012  [ 7296/19200]
per-ex loss: 0.635830  [ 7488/19200]
per-ex loss: 0.539857  [ 7680/19200]
per-ex loss: 0.474959  [ 7872/19200]
per-ex loss: 0.520270  [ 8064/19200]
per-ex loss: 0.517418  [ 8256/19200]
per-ex loss: 0.527000  [ 8448/19200]
per-ex loss: 0.609240  [ 8640/19200]
per-ex loss: 0.633418  [ 8832/19200]
per-ex loss: 0.456956  [ 9024/19200]
per-ex loss: 0.665322  [ 9216/19200]
per-ex loss: 0.491885  [ 9408/19200]
per-ex loss: 0.590223  [ 9600/19200]
per-ex loss: 0.560170  [ 9792/19200]
per-ex loss: 0.583038  [ 9984/19200]
per-ex loss: 0.570293  [10176/19200]
per-ex loss: 0.611800  [10368/19200]
per-ex loss: 0.509838  [10560/19200]
per-ex loss: 0.518889  [10752/19200]
per-ex loss: 0.542791  [10944/19200]
per-ex loss: 0.540010  [11136/19200]
per-ex loss: 0.501289  [11328/19200]
per-ex loss: 0.643385  [11520/19200]
per-ex loss: 0.656616  [11712/19200]
per-ex loss: 0.573551  [11904/19200]
per-ex loss: 0.573661  [12096/19200]
per-ex loss: 0.546180  [12288/19200]
per-ex loss: 0.621292  [12480/19200]
per-ex loss: 0.474303  [12672/19200]
per-ex loss: 0.657916  [12864/19200]
per-ex loss: 0.511336  [13056/19200]
per-ex loss: 0.640556  [13248/19200]
per-ex loss: 0.537932  [13440/19200]
per-ex loss: 0.494511  [13632/19200]
per-ex loss: 0.562061  [13824/19200]
per-ex loss: 0.463660  [14016/19200]
per-ex loss: 0.480158  [14208/19200]
per-ex loss: 0.666282  [14400/19200]
per-ex loss: 0.689469  [14592/19200]
per-ex loss: 0.647638  [14784/19200]
per-ex loss: 0.557847  [14976/19200]
per-ex loss: 0.547996  [15168/19200]
per-ex loss: 0.548375  [15360/19200]
per-ex loss: 0.468495  [15552/19200]
per-ex loss: 0.698559  [15744/19200]
per-ex loss: 0.503149  [15936/19200]
per-ex loss: 0.501566  [16128/19200]
per-ex loss: 0.553364  [16320/19200]
per-ex loss: 0.538412  [16512/19200]
per-ex loss: 0.507773  [16704/19200]
per-ex loss: 0.535020  [16896/19200]
per-ex loss: 0.436798  [17088/19200]
per-ex loss: 0.623068  [17280/19200]
per-ex loss: 0.500905  [17472/19200]
per-ex loss: 0.564579  [17664/19200]
per-ex loss: 0.541554  [17856/19200]
per-ex loss: 0.530671  [18048/19200]
per-ex loss: 0.761902  [18240/19200]
per-ex loss: 0.485121  [18432/19200]
per-ex loss: 0.519726  [18624/19200]
per-ex loss: 0.443982  [18816/19200]
per-ex loss: 0.606669  [19008/19200]
per-ex loss: 0.519012  [19200/19200]
Train Error: Avg loss: 0.56022211
validation Error: 
 Avg loss: 0.53903149 
 F1: 0.445814 
 Precision: 0.372033 
 Recall: 0.556100
 IoU: 0.286848

test Error: 
 Avg loss: 0.48271078 
 F1: 0.520530 
 Precision: 0.460575 
 Recall: 0.598431
 IoU: 0.351836

We have finished training iteration 18
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_16_.pth
per-ex loss: 0.505014  [  192/19200]
per-ex loss: 0.713077  [  384/19200]
per-ex loss: 0.546611  [  576/19200]
per-ex loss: 0.479116  [  768/19200]
per-ex loss: 0.506889  [  960/19200]
per-ex loss: 0.464141  [ 1152/19200]
per-ex loss: 0.498211  [ 1344/19200]
per-ex loss: 0.518863  [ 1536/19200]
per-ex loss: 0.709563  [ 1728/19200]
per-ex loss: 0.530808  [ 1920/19200]
per-ex loss: 0.537400  [ 2112/19200]
per-ex loss: 0.502460  [ 2304/19200]
per-ex loss: 0.473335  [ 2496/19200]
per-ex loss: 0.541354  [ 2688/19200]
per-ex loss: 0.524117  [ 2880/19200]
per-ex loss: 0.434780  [ 3072/19200]
per-ex loss: 0.622901  [ 3264/19200]
per-ex loss: 0.700594  [ 3456/19200]
per-ex loss: 0.535005  [ 3648/19200]
per-ex loss: 0.502978  [ 3840/19200]
per-ex loss: 0.565129  [ 4032/19200]
per-ex loss: 0.550177  [ 4224/19200]
per-ex loss: 0.450077  [ 4416/19200]
per-ex loss: 0.692559  [ 4608/19200]
per-ex loss: 0.503436  [ 4800/19200]
per-ex loss: 0.529955  [ 4992/19200]
per-ex loss: 0.632140  [ 5184/19200]
per-ex loss: 0.577515  [ 5376/19200]
per-ex loss: 0.590070  [ 5568/19200]
per-ex loss: 0.441391  [ 5760/19200]
per-ex loss: 0.532339  [ 5952/19200]
per-ex loss: 0.471233  [ 6144/19200]
per-ex loss: 0.584250  [ 6336/19200]
per-ex loss: 0.634430  [ 6528/19200]
per-ex loss: 0.568443  [ 6720/19200]
per-ex loss: 0.537078  [ 6912/19200]
per-ex loss: 0.484901  [ 7104/19200]
per-ex loss: 0.558525  [ 7296/19200]
per-ex loss: 0.699398  [ 7488/19200]
per-ex loss: 0.459342  [ 7680/19200]
per-ex loss: 0.506510  [ 7872/19200]
per-ex loss: 0.614475  [ 8064/19200]
per-ex loss: 0.576764  [ 8256/19200]
per-ex loss: 0.506671  [ 8448/19200]
per-ex loss: 0.484983  [ 8640/19200]
per-ex loss: 0.671328  [ 8832/19200]
per-ex loss: 0.549580  [ 9024/19200]
per-ex loss: 0.621676  [ 9216/19200]
per-ex loss: 0.442097  [ 9408/19200]
per-ex loss: 0.441834  [ 9600/19200]
per-ex loss: 0.551657  [ 9792/19200]
per-ex loss: 0.609626  [ 9984/19200]
per-ex loss: 0.576043  [10176/19200]
per-ex loss: 0.579818  [10368/19200]
per-ex loss: 0.616038  [10560/19200]
per-ex loss: 0.645566  [10752/19200]
per-ex loss: 0.576071  [10944/19200]
per-ex loss: 0.465776  [11136/19200]
per-ex loss: 0.619296  [11328/19200]
per-ex loss: 0.522351  [11520/19200]
per-ex loss: 0.673235  [11712/19200]
per-ex loss: 0.595302  [11904/19200]
per-ex loss: 0.485705  [12096/19200]
per-ex loss: 0.505348  [12288/19200]
per-ex loss: 0.465104  [12480/19200]
per-ex loss: 0.590192  [12672/19200]
per-ex loss: 0.501039  [12864/19200]
per-ex loss: 0.599280  [13056/19200]
per-ex loss: 0.587137  [13248/19200]
per-ex loss: 0.571477  [13440/19200]
per-ex loss: 0.555227  [13632/19200]
per-ex loss: 0.731205  [13824/19200]
per-ex loss: 0.538720  [14016/19200]
per-ex loss: 0.571841  [14208/19200]
per-ex loss: 0.601891  [14400/19200]
per-ex loss: 0.542656  [14592/19200]
per-ex loss: 0.742854  [14784/19200]
per-ex loss: 0.522581  [14976/19200]
per-ex loss: 0.546701  [15168/19200]
per-ex loss: 0.479500  [15360/19200]
per-ex loss: 0.603799  [15552/19200]
per-ex loss: 0.613349  [15744/19200]
per-ex loss: 0.735757  [15936/19200]
per-ex loss: 0.605319  [16128/19200]
per-ex loss: 0.659359  [16320/19200]
per-ex loss: 0.606760  [16512/19200]
per-ex loss: 0.549170  [16704/19200]
per-ex loss: 0.659535  [16896/19200]
per-ex loss: 0.616226  [17088/19200]
per-ex loss: 0.479539  [17280/19200]
per-ex loss: 0.646556  [17472/19200]
per-ex loss: 0.642760  [17664/19200]
per-ex loss: 0.641820  [17856/19200]
per-ex loss: 0.546458  [18048/19200]
per-ex loss: 0.584270  [18240/19200]
per-ex loss: 0.537489  [18432/19200]
per-ex loss: 0.582629  [18624/19200]
per-ex loss: 0.530182  [18816/19200]
per-ex loss: 0.606007  [19008/19200]
per-ex loss: 0.735805  [19200/19200]
Train Error: Avg loss: 0.56677553
validation Error: 
 Avg loss: 0.57433923 
 F1: 0.414988 
 Precision: 0.345289 
 Recall: 0.519941
 IoU: 0.261820

test Error: 
 Avg loss: 0.50813448 
 F1: 0.495857 
 Precision: 0.436820 
 Recall: 0.573344
 IoU: 0.329661

We have finished training iteration 19
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_15_.pth
per-ex loss: 0.684621  [  192/19200]
per-ex loss: 0.648205  [  384/19200]
per-ex loss: 0.609080  [  576/19200]
per-ex loss: 0.575684  [  768/19200]
per-ex loss: 0.434816  [  960/19200]
per-ex loss: 0.712034  [ 1152/19200]
per-ex loss: 0.571447  [ 1344/19200]
per-ex loss: 0.600301  [ 1536/19200]
per-ex loss: 0.515860  [ 1728/19200]
per-ex loss: 0.676548  [ 1920/19200]
per-ex loss: 0.547727  [ 2112/19200]
per-ex loss: 0.541213  [ 2304/19200]
per-ex loss: 0.545492  [ 2496/19200]
per-ex loss: 0.554307  [ 2688/19200]
per-ex loss: 0.641111  [ 2880/19200]
per-ex loss: 0.558153  [ 3072/19200]
per-ex loss: 0.543907  [ 3264/19200]
per-ex loss: 0.481611  [ 3456/19200]
per-ex loss: 0.531314  [ 3648/19200]
per-ex loss: 0.544328  [ 3840/19200]
per-ex loss: 0.475269  [ 4032/19200]
per-ex loss: 0.634983  [ 4224/19200]
per-ex loss: 0.624258  [ 4416/19200]
per-ex loss: 0.575383  [ 4608/19200]
per-ex loss: 0.439031  [ 4800/19200]
per-ex loss: 0.532586  [ 4992/19200]
per-ex loss: 0.463869  [ 5184/19200]
per-ex loss: 0.620008  [ 5376/19200]
per-ex loss: 0.669515  [ 5568/19200]
per-ex loss: 0.531755  [ 5760/19200]
per-ex loss: 0.480250  [ 5952/19200]
per-ex loss: 0.599068  [ 6144/19200]
per-ex loss: 0.502057  [ 6336/19200]
per-ex loss: 0.601621  [ 6528/19200]
per-ex loss: 0.525919  [ 6720/19200]
per-ex loss: 0.519847  [ 6912/19200]
per-ex loss: 0.581120  [ 7104/19200]
per-ex loss: 0.660764  [ 7296/19200]
per-ex loss: 0.738007  [ 7488/19200]
per-ex loss: 0.540335  [ 7680/19200]
per-ex loss: 0.654814  [ 7872/19200]
per-ex loss: 0.607815  [ 8064/19200]
per-ex loss: 0.678335  [ 8256/19200]
per-ex loss: 0.535374  [ 8448/19200]
per-ex loss: 0.681391  [ 8640/19200]
per-ex loss: 0.577701  [ 8832/19200]
per-ex loss: 0.489384  [ 9024/19200]
per-ex loss: 0.601794  [ 9216/19200]
per-ex loss: 0.495765  [ 9408/19200]
per-ex loss: 0.522814  [ 9600/19200]
per-ex loss: 0.494165  [ 9792/19200]
per-ex loss: 0.667729  [ 9984/19200]
per-ex loss: 0.638494  [10176/19200]
per-ex loss: 0.475896  [10368/19200]
per-ex loss: 0.557918  [10560/19200]
per-ex loss: 0.558470  [10752/19200]
per-ex loss: 0.486135  [10944/19200]
per-ex loss: 0.651407  [11136/19200]
per-ex loss: 0.486319  [11328/19200]
per-ex loss: 0.512402  [11520/19200]
per-ex loss: 0.696449  [11712/19200]
per-ex loss: 0.604825  [11904/19200]
per-ex loss: 0.751751  [12096/19200]
per-ex loss: 0.476783  [12288/19200]
per-ex loss: 0.539194  [12480/19200]
per-ex loss: 0.492699  [12672/19200]
per-ex loss: 0.590308  [12864/19200]
per-ex loss: 0.531031  [13056/19200]
per-ex loss: 0.427596  [13248/19200]
per-ex loss: 0.490681  [13440/19200]
per-ex loss: 0.489760  [13632/19200]
per-ex loss: 0.588934  [13824/19200]
per-ex loss: 0.641260  [14016/19200]
per-ex loss: 0.481542  [14208/19200]
per-ex loss: 0.551732  [14400/19200]
per-ex loss: 0.567874  [14592/19200]
per-ex loss: 0.523486  [14784/19200]
per-ex loss: 0.529225  [14976/19200]
per-ex loss: 0.585428  [15168/19200]
per-ex loss: 0.457208  [15360/19200]
per-ex loss: 0.704521  [15552/19200]
per-ex loss: 0.627729  [15744/19200]
per-ex loss: 0.472245  [15936/19200]
per-ex loss: 0.560233  [16128/19200]
per-ex loss: 0.462107  [16320/19200]
per-ex loss: 0.556843  [16512/19200]
per-ex loss: 0.572622  [16704/19200]
per-ex loss: 0.605596  [16896/19200]
per-ex loss: 0.569857  [17088/19200]
per-ex loss: 0.552815  [17280/19200]
per-ex loss: 0.706365  [17472/19200]
per-ex loss: 0.551957  [17664/19200]
per-ex loss: 0.585004  [17856/19200]
per-ex loss: 0.430371  [18048/19200]
per-ex loss: 0.531381  [18240/19200]
per-ex loss: 0.458579  [18432/19200]
per-ex loss: 0.515870  [18624/19200]
per-ex loss: 0.575867  [18816/19200]
per-ex loss: 0.575621  [19008/19200]
per-ex loss: 0.673441  [19200/19200]
Train Error: Avg loss: 0.56514318
validation Error: 
 Avg loss: 0.54640249 
 F1: 0.442176 
 Precision: 0.365415 
 Recall: 0.559763
 IoU: 0.283842

test Error: 
 Avg loss: 0.48504972 
 F1: 0.518702 
 Precision: 0.453257 
 Recall: 0.606235
 IoU: 0.350167

We have finished training iteration 20
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_17_.pth
per-ex loss: 0.462381  [  192/19200]
per-ex loss: 0.494840  [  384/19200]
per-ex loss: 0.447751  [  576/19200]
per-ex loss: 0.533575  [  768/19200]
per-ex loss: 0.594592  [  960/19200]
per-ex loss: 0.604016  [ 1152/19200]
per-ex loss: 0.520332  [ 1344/19200]
per-ex loss: 0.723708  [ 1536/19200]
per-ex loss: 0.437907  [ 1728/19200]
per-ex loss: 0.524034  [ 1920/19200]
per-ex loss: 0.576484  [ 2112/19200]
per-ex loss: 0.501655  [ 2304/19200]
per-ex loss: 0.468635  [ 2496/19200]
per-ex loss: 0.449543  [ 2688/19200]
per-ex loss: 0.526720  [ 2880/19200]
per-ex loss: 0.715650  [ 3072/19200]
per-ex loss: 0.542294  [ 3264/19200]
per-ex loss: 0.722680  [ 3456/19200]
per-ex loss: 0.605542  [ 3648/19200]
per-ex loss: 0.528150  [ 3840/19200]
per-ex loss: 0.459495  [ 4032/19200]
per-ex loss: 0.560680  [ 4224/19200]
per-ex loss: 0.575117  [ 4416/19200]
per-ex loss: 0.522710  [ 4608/19200]
per-ex loss: 0.626976  [ 4800/19200]
per-ex loss: 0.607287  [ 4992/19200]
per-ex loss: 0.614055  [ 5184/19200]
per-ex loss: 0.673669  [ 5376/19200]
per-ex loss: 0.671908  [ 5568/19200]
per-ex loss: 0.625189  [ 5760/19200]
per-ex loss: 0.534442  [ 5952/19200]
per-ex loss: 0.483957  [ 6144/19200]
per-ex loss: 0.593548  [ 6336/19200]
per-ex loss: 0.701083  [ 6528/19200]
per-ex loss: 0.480454  [ 6720/19200]
per-ex loss: 0.594064  [ 6912/19200]
per-ex loss: 0.680783  [ 7104/19200]
per-ex loss: 0.484202  [ 7296/19200]
per-ex loss: 0.494586  [ 7488/19200]
per-ex loss: 0.588158  [ 7680/19200]
per-ex loss: 0.639531  [ 7872/19200]
per-ex loss: 0.572831  [ 8064/19200]
per-ex loss: 0.516646  [ 8256/19200]
per-ex loss: 0.558671  [ 8448/19200]
per-ex loss: 0.552751  [ 8640/19200]
per-ex loss: 0.522348  [ 8832/19200]
per-ex loss: 0.589012  [ 9024/19200]
per-ex loss: 0.653594  [ 9216/19200]
per-ex loss: 0.510684  [ 9408/19200]
per-ex loss: 0.819337  [ 9600/19200]
per-ex loss: 0.618314  [ 9792/19200]
per-ex loss: 0.697545  [ 9984/19200]
per-ex loss: 0.488112  [10176/19200]
per-ex loss: 0.637893  [10368/19200]
per-ex loss: 0.655624  [10560/19200]
per-ex loss: 0.504550  [10752/19200]
per-ex loss: 0.570939  [10944/19200]
per-ex loss: 0.609587  [11136/19200]
per-ex loss: 0.563236  [11328/19200]
per-ex loss: 0.637993  [11520/19200]
per-ex loss: 0.612167  [11712/19200]
per-ex loss: 0.543949  [11904/19200]
per-ex loss: 0.761738  [12096/19200]
per-ex loss: 0.507313  [12288/19200]
per-ex loss: 0.581564  [12480/19200]
per-ex loss: 0.504352  [12672/19200]
per-ex loss: 0.561465  [12864/19200]
per-ex loss: 0.603394  [13056/19200]
per-ex loss: 0.473084  [13248/19200]
per-ex loss: 0.617686  [13440/19200]
per-ex loss: 0.539843  [13632/19200]
per-ex loss: 0.646839  [13824/19200]
per-ex loss: 0.538070  [14016/19200]
per-ex loss: 0.558570  [14208/19200]
per-ex loss: 0.472111  [14400/19200]
per-ex loss: 0.507809  [14592/19200]
per-ex loss: 0.583940  [14784/19200]
per-ex loss: 0.531980  [14976/19200]
per-ex loss: 0.589977  [15168/19200]
per-ex loss: 0.615936  [15360/19200]
per-ex loss: 0.542042  [15552/19200]
per-ex loss: 0.467210  [15744/19200]
per-ex loss: 0.520709  [15936/19200]
per-ex loss: 0.612825  [16128/19200]
per-ex loss: 0.696844  [16320/19200]
per-ex loss: 0.736669  [16512/19200]
per-ex loss: 0.482523  [16704/19200]
per-ex loss: 0.622263  [16896/19200]
per-ex loss: 0.596044  [17088/19200]
per-ex loss: 0.721369  [17280/19200]
per-ex loss: 0.631821  [17472/19200]
per-ex loss: 0.469246  [17664/19200]
per-ex loss: 0.534365  [17856/19200]
per-ex loss: 0.543353  [18048/19200]
per-ex loss: 0.688121  [18240/19200]
per-ex loss: 0.510283  [18432/19200]
per-ex loss: 0.602077  [18624/19200]
per-ex loss: 0.610600  [18816/19200]
per-ex loss: 0.636748  [19008/19200]
per-ex loss: 0.489826  [19200/19200]
Train Error: Avg loss: 0.57538767
validation Error: 
 Avg loss: 0.57358982 
 F1: 0.434512 
 Precision: 0.427271 
 Recall: 0.442003
 IoU: 0.277557

test Error: 
 Avg loss: 0.48968989 
 F1: 0.512625 
 Precision: 0.541643 
 Recall: 0.486558
 IoU: 0.344651

We have finished training iteration 21
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_19_.pth
per-ex loss: 0.397213  [  192/19200]
per-ex loss: 0.643503  [  384/19200]
per-ex loss: 0.595153  [  576/19200]
per-ex loss: 0.546631  [  768/19200]
per-ex loss: 0.550614  [  960/19200]
per-ex loss: 0.455022  [ 1152/19200]
per-ex loss: 0.600992  [ 1344/19200]
per-ex loss: 0.454269  [ 1536/19200]
per-ex loss: 0.828513  [ 1728/19200]
per-ex loss: 0.545145  [ 1920/19200]
per-ex loss: 0.643617  [ 2112/19200]
per-ex loss: 0.570466  [ 2304/19200]
per-ex loss: 0.546455  [ 2496/19200]
per-ex loss: 0.540731  [ 2688/19200]
per-ex loss: 0.757113  [ 2880/19200]
per-ex loss: 0.672479  [ 3072/19200]
per-ex loss: 0.652273  [ 3264/19200]
per-ex loss: 0.490476  [ 3456/19200]
per-ex loss: 0.549925  [ 3648/19200]
per-ex loss: 0.592262  [ 3840/19200]
per-ex loss: 0.513459  [ 4032/19200]
per-ex loss: 0.474326  [ 4224/19200]
per-ex loss: 0.524011  [ 4416/19200]
per-ex loss: 0.676416  [ 4608/19200]
per-ex loss: 0.467723  [ 4800/19200]
per-ex loss: 0.625069  [ 4992/19200]
per-ex loss: 0.529550  [ 5184/19200]
per-ex loss: 0.567567  [ 5376/19200]
per-ex loss: 0.523514  [ 5568/19200]
per-ex loss: 0.596493  [ 5760/19200]
per-ex loss: 0.624605  [ 5952/19200]
per-ex loss: 0.524261  [ 6144/19200]
per-ex loss: 0.508206  [ 6336/19200]
per-ex loss: 0.767541  [ 6528/19200]
per-ex loss: 0.628239  [ 6720/19200]
per-ex loss: 0.670720  [ 6912/19200]
per-ex loss: 0.466170  [ 7104/19200]
per-ex loss: 0.584512  [ 7296/19200]
per-ex loss: 0.672569  [ 7488/19200]
per-ex loss: 0.584780  [ 7680/19200]
per-ex loss: 0.495529  [ 7872/19200]
per-ex loss: 0.506589  [ 8064/19200]
per-ex loss: 0.634703  [ 8256/19200]
per-ex loss: 0.662035  [ 8448/19200]
per-ex loss: 0.508041  [ 8640/19200]
per-ex loss: 0.522177  [ 8832/19200]
per-ex loss: 0.689924  [ 9024/19200]
per-ex loss: 0.516698  [ 9216/19200]
per-ex loss: 0.737804  [ 9408/19200]
per-ex loss: 0.613133  [ 9600/19200]
per-ex loss: 0.555707  [ 9792/19200]
per-ex loss: 0.620496  [ 9984/19200]
per-ex loss: 0.571942  [10176/19200]
per-ex loss: 0.665097  [10368/19200]
per-ex loss: 0.671198  [10560/19200]
per-ex loss: 0.670325  [10752/19200]
per-ex loss: 0.587136  [10944/19200]
per-ex loss: 0.636901  [11136/19200]
per-ex loss: 0.460077  [11328/19200]
per-ex loss: 0.455211  [11520/19200]
per-ex loss: 0.493034  [11712/19200]
per-ex loss: 0.522876  [11904/19200]
per-ex loss: 0.464206  [12096/19200]
per-ex loss: 0.583959  [12288/19200]
per-ex loss: 0.672042  [12480/19200]
per-ex loss: 0.638856  [12672/19200]
per-ex loss: 0.572786  [12864/19200]
per-ex loss: 0.595657  [13056/19200]
per-ex loss: 0.547862  [13248/19200]
per-ex loss: 0.613127  [13440/19200]
per-ex loss: 0.662534  [13632/19200]
per-ex loss: 0.600026  [13824/19200]
per-ex loss: 0.777811  [14016/19200]
per-ex loss: 0.497871  [14208/19200]
per-ex loss: 0.618178  [14400/19200]
per-ex loss: 0.716049  [14592/19200]
per-ex loss: 0.664440  [14784/19200]
per-ex loss: 0.493727  [14976/19200]
per-ex loss: 0.521818  [15168/19200]
per-ex loss: 0.543505  [15360/19200]
per-ex loss: 0.554675  [15552/19200]
per-ex loss: 0.529415  [15744/19200]
per-ex loss: 0.540098  [15936/19200]
per-ex loss: 0.468403  [16128/19200]
per-ex loss: 0.588833  [16320/19200]
per-ex loss: 0.629553  [16512/19200]
per-ex loss: 0.424477  [16704/19200]
per-ex loss: 0.698082  [16896/19200]
per-ex loss: 0.430313  [17088/19200]
per-ex loss: 0.498742  [17280/19200]
per-ex loss: 0.683959  [17472/19200]
per-ex loss: 0.560370  [17664/19200]
per-ex loss: 0.662328  [17856/19200]
per-ex loss: 0.560375  [18048/19200]
per-ex loss: 0.574963  [18240/19200]
per-ex loss: 0.600029  [18432/19200]
per-ex loss: 0.623477  [18624/19200]
per-ex loss: 0.743262  [18816/19200]
per-ex loss: 0.687973  [19008/19200]
per-ex loss: 0.723575  [19200/19200]
Train Error: Avg loss: 0.58526570
validation Error: 
 Avg loss: 0.55854289 
 F1: 0.440050 
 Precision: 0.436821 
 Recall: 0.443326
 IoU: 0.282092

test Error: 
 Avg loss: 0.49056453 
 F1: 0.511847 
 Precision: 0.532691 
 Recall: 0.492574
 IoU: 0.343948

We have finished training iteration 22
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_20_.pth
per-ex loss: 0.585113  [  192/19200]
per-ex loss: 0.590601  [  384/19200]
per-ex loss: 0.801672  [  576/19200]
per-ex loss: 0.688432  [  768/19200]
per-ex loss: 0.588729  [  960/19200]
per-ex loss: 0.521889  [ 1152/19200]
per-ex loss: 0.618951  [ 1344/19200]
per-ex loss: 0.507033  [ 1536/19200]
per-ex loss: 0.594318  [ 1728/19200]
per-ex loss: 0.579405  [ 1920/19200]
per-ex loss: 0.717853  [ 2112/19200]
per-ex loss: 0.538003  [ 2304/19200]
per-ex loss: 0.471509  [ 2496/19200]
per-ex loss: 0.525535  [ 2688/19200]
per-ex loss: 0.515526  [ 2880/19200]
per-ex loss: 0.493582  [ 3072/19200]
per-ex loss: 0.433962  [ 3264/19200]
per-ex loss: 0.520147  [ 3456/19200]
per-ex loss: 0.577600  [ 3648/19200]
per-ex loss: 0.539586  [ 3840/19200]
per-ex loss: 0.601980  [ 4032/19200]
per-ex loss: 0.533909  [ 4224/19200]
per-ex loss: 0.510180  [ 4416/19200]
per-ex loss: 0.583265  [ 4608/19200]
per-ex loss: 0.474744  [ 4800/19200]
per-ex loss: 0.633070  [ 4992/19200]
per-ex loss: 0.529605  [ 5184/19200]
per-ex loss: 0.610604  [ 5376/19200]
per-ex loss: 0.647710  [ 5568/19200]
per-ex loss: 0.508121  [ 5760/19200]
per-ex loss: 0.504966  [ 5952/19200]
per-ex loss: 0.424905  [ 6144/19200]
per-ex loss: 0.611136  [ 6336/19200]
per-ex loss: 0.607937  [ 6528/19200]
per-ex loss: 0.540077  [ 6720/19200]
per-ex loss: 0.538102  [ 6912/19200]
per-ex loss: 0.521755  [ 7104/19200]
per-ex loss: 0.510855  [ 7296/19200]
per-ex loss: 0.455530  [ 7488/19200]
per-ex loss: 0.504668  [ 7680/19200]
per-ex loss: 0.535612  [ 7872/19200]
per-ex loss: 0.500521  [ 8064/19200]
per-ex loss: 0.509220  [ 8256/19200]
per-ex loss: 0.571121  [ 8448/19200]
per-ex loss: 0.522976  [ 8640/19200]
per-ex loss: 0.558159  [ 8832/19200]
per-ex loss: 0.542392  [ 9024/19200]
per-ex loss: 0.662175  [ 9216/19200]
per-ex loss: 0.528684  [ 9408/19200]
per-ex loss: 0.574316  [ 9600/19200]
per-ex loss: 0.738812  [ 9792/19200]
per-ex loss: 0.669764  [ 9984/19200]
per-ex loss: 0.495040  [10176/19200]
per-ex loss: 0.551382  [10368/19200]
per-ex loss: 0.530167  [10560/19200]
per-ex loss: 0.634046  [10752/19200]
per-ex loss: 0.507600  [10944/19200]
per-ex loss: 0.700515  [11136/19200]
per-ex loss: 0.555158  [11328/19200]
per-ex loss: 0.676931  [11520/19200]
per-ex loss: 0.677971  [11712/19200]
per-ex loss: 0.459416  [11904/19200]
per-ex loss: 0.542826  [12096/19200]
per-ex loss: 0.517945  [12288/19200]
per-ex loss: 0.542250  [12480/19200]
per-ex loss: 0.623466  [12672/19200]
per-ex loss: 0.563584  [12864/19200]
per-ex loss: 0.679049  [13056/19200]
per-ex loss: 0.502499  [13248/19200]
per-ex loss: 0.524789  [13440/19200]
per-ex loss: 0.517933  [13632/19200]
per-ex loss: 0.458036  [13824/19200]
per-ex loss: 0.492361  [14016/19200]
per-ex loss: 0.533709  [14208/19200]
per-ex loss: 0.542768  [14400/19200]
per-ex loss: 0.514318  [14592/19200]
per-ex loss: 0.658244  [14784/19200]
per-ex loss: 0.460108  [14976/19200]
per-ex loss: 0.561909  [15168/19200]
per-ex loss: 0.632476  [15360/19200]
per-ex loss: 0.568655  [15552/19200]
per-ex loss: 0.579097  [15744/19200]
per-ex loss: 0.664537  [15936/19200]
per-ex loss: 0.456702  [16128/19200]
per-ex loss: 0.506490  [16320/19200]
per-ex loss: 0.530662  [16512/19200]
per-ex loss: 0.627591  [16704/19200]
per-ex loss: 0.634934  [16896/19200]
per-ex loss: 0.502042  [17088/19200]
per-ex loss: 0.483402  [17280/19200]
per-ex loss: 0.673721  [17472/19200]
per-ex loss: 0.450604  [17664/19200]
per-ex loss: 0.636242  [17856/19200]
per-ex loss: 0.443194  [18048/19200]
per-ex loss: 0.552774  [18240/19200]
per-ex loss: 0.693265  [18432/19200]
per-ex loss: 0.534490  [18624/19200]
per-ex loss: 0.516287  [18816/19200]
per-ex loss: 0.720936  [19008/19200]
per-ex loss: 0.450914  [19200/19200]
Train Error: Avg loss: 0.56059350
validation Error: 
 Avg loss: 0.55679607 
 F1: 0.444224 
 Precision: 0.412445 
 Recall: 0.481310
 IoU: 0.285532

test Error: 
 Avg loss: 0.48733419 
 F1: 0.515185 
 Precision: 0.497582 
 Recall: 0.534078
 IoU: 0.346969

We have finished training iteration 23
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_21_.pth
per-ex loss: 0.607896  [  192/19200]
per-ex loss: 0.552072  [  384/19200]
per-ex loss: 0.559866  [  576/19200]
per-ex loss: 0.670522  [  768/19200]
per-ex loss: 0.575358  [  960/19200]
per-ex loss: 0.642823  [ 1152/19200]
per-ex loss: 0.550285  [ 1344/19200]
per-ex loss: 0.560532  [ 1536/19200]
per-ex loss: 0.616814  [ 1728/19200]
per-ex loss: 0.442107  [ 1920/19200]
per-ex loss: 0.537213  [ 2112/19200]
per-ex loss: 0.534867  [ 2304/19200]
per-ex loss: 0.470510  [ 2496/19200]
per-ex loss: 0.500731  [ 2688/19200]
per-ex loss: 0.769383  [ 2880/19200]
per-ex loss: 0.494920  [ 3072/19200]
per-ex loss: 0.446748  [ 3264/19200]
per-ex loss: 0.634149  [ 3456/19200]
per-ex loss: 0.485595  [ 3648/19200]
per-ex loss: 0.670476  [ 3840/19200]
per-ex loss: 0.644004  [ 4032/19200]
per-ex loss: 0.501195  [ 4224/19200]
per-ex loss: 0.706555  [ 4416/19200]
per-ex loss: 0.440901  [ 4608/19200]
per-ex loss: 0.605225  [ 4800/19200]
per-ex loss: 0.572335  [ 4992/19200]
per-ex loss: 0.731680  [ 5184/19200]
per-ex loss: 0.576033  [ 5376/19200]
per-ex loss: 0.484851  [ 5568/19200]
per-ex loss: 0.650958  [ 5760/19200]
per-ex loss: 0.593799  [ 5952/19200]
per-ex loss: 0.534293  [ 6144/19200]
per-ex loss: 0.504246  [ 6336/19200]
per-ex loss: 0.539360  [ 6528/19200]
per-ex loss: 0.439006  [ 6720/19200]
per-ex loss: 0.578208  [ 6912/19200]
per-ex loss: 0.524414  [ 7104/19200]
per-ex loss: 0.640807  [ 7296/19200]
per-ex loss: 0.552240  [ 7488/19200]
per-ex loss: 0.742095  [ 7680/19200]
per-ex loss: 0.478583  [ 7872/19200]
per-ex loss: 0.499829  [ 8064/19200]
per-ex loss: 0.647660  [ 8256/19200]
per-ex loss: 0.630040  [ 8448/19200]
per-ex loss: 0.564669  [ 8640/19200]
per-ex loss: 0.670424  [ 8832/19200]
per-ex loss: 0.539754  [ 9024/19200]
per-ex loss: 0.529108  [ 9216/19200]
per-ex loss: 0.672971  [ 9408/19200]
per-ex loss: 0.587418  [ 9600/19200]
per-ex loss: 0.555405  [ 9792/19200]
per-ex loss: 0.470864  [ 9984/19200]
per-ex loss: 0.581384  [10176/19200]
per-ex loss: 0.630022  [10368/19200]
per-ex loss: 0.595014  [10560/19200]
per-ex loss: 0.441595  [10752/19200]
per-ex loss: 0.504222  [10944/19200]
per-ex loss: 0.595520  [11136/19200]
per-ex loss: 0.510902  [11328/19200]
per-ex loss: 0.532734  [11520/19200]
per-ex loss: 0.613171  [11712/19200]
per-ex loss: 0.598771  [11904/19200]
per-ex loss: 0.502828  [12096/19200]
per-ex loss: 0.582055  [12288/19200]
per-ex loss: 0.603970  [12480/19200]
per-ex loss: 0.515785  [12672/19200]
per-ex loss: 0.574713  [12864/19200]
per-ex loss: 0.611016  [13056/19200]
per-ex loss: 0.465792  [13248/19200]
per-ex loss: 0.485668  [13440/19200]
per-ex loss: 0.504773  [13632/19200]
per-ex loss: 0.492209  [13824/19200]
per-ex loss: 0.538593  [14016/19200]
per-ex loss: 0.601047  [14208/19200]
per-ex loss: 0.639467  [14400/19200]
per-ex loss: 0.545326  [14592/19200]
per-ex loss: 0.551439  [14784/19200]
per-ex loss: 0.610844  [14976/19200]
per-ex loss: 0.649710  [15168/19200]
per-ex loss: 0.618890  [15360/19200]
per-ex loss: 0.536732  [15552/19200]
per-ex loss: 0.686603  [15744/19200]
per-ex loss: 0.596232  [15936/19200]
per-ex loss: 0.523500  [16128/19200]
per-ex loss: 0.720812  [16320/19200]
per-ex loss: 0.708083  [16512/19200]
per-ex loss: 0.530095  [16704/19200]
per-ex loss: 0.710306  [16896/19200]
per-ex loss: 0.574950  [17088/19200]
per-ex loss: 0.595286  [17280/19200]
per-ex loss: 0.473899  [17472/19200]
per-ex loss: 0.527672  [17664/19200]
per-ex loss: 0.690374  [17856/19200]
per-ex loss: 0.484782  [18048/19200]
per-ex loss: 0.546733  [18240/19200]
per-ex loss: 0.537706  [18432/19200]
per-ex loss: 0.664546  [18624/19200]
per-ex loss: 0.653282  [18816/19200]
per-ex loss: 0.544513  [19008/19200]
per-ex loss: 0.670918  [19200/19200]
Train Error: Avg loss: 0.57406285
validation Error: 
 Avg loss: 0.55471802 
 F1: 0.443980 
 Precision: 0.393033 
 Recall: 0.510104
 IoU: 0.285331

test Error: 
 Avg loss: 0.48977625 
 F1: 0.513161 
 Precision: 0.477435 
 Recall: 0.554667
 IoU: 0.345136

We have finished training iteration 24
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_22_.pth
per-ex loss: 0.665859  [  192/19200]
per-ex loss: 0.610925  [  384/19200]
per-ex loss: 0.543972  [  576/19200]
per-ex loss: 0.553272  [  768/19200]
per-ex loss: 0.497592  [  960/19200]
per-ex loss: 0.573304  [ 1152/19200]
per-ex loss: 0.499749  [ 1344/19200]
per-ex loss: 0.614517  [ 1536/19200]
per-ex loss: 0.500797  [ 1728/19200]
per-ex loss: 0.665425  [ 1920/19200]
per-ex loss: 0.646806  [ 2112/19200]
per-ex loss: 0.482518  [ 2304/19200]
per-ex loss: 0.480490  [ 2496/19200]
per-ex loss: 0.537467  [ 2688/19200]
per-ex loss: 0.443530  [ 2880/19200]
per-ex loss: 0.715256  [ 3072/19200]
per-ex loss: 0.501419  [ 3264/19200]
per-ex loss: 0.668153  [ 3456/19200]
per-ex loss: 0.677886  [ 3648/19200]
per-ex loss: 0.505888  [ 3840/19200]
per-ex loss: 0.508759  [ 4032/19200]
per-ex loss: 0.590111  [ 4224/19200]
per-ex loss: 0.508917  [ 4416/19200]
per-ex loss: 0.554763  [ 4608/19200]
per-ex loss: 0.478224  [ 4800/19200]
per-ex loss: 0.622626  [ 4992/19200]
per-ex loss: 0.594884  [ 5184/19200]
per-ex loss: 0.441867  [ 5376/19200]
per-ex loss: 0.608031  [ 5568/19200]
per-ex loss: 0.467322  [ 5760/19200]
per-ex loss: 0.616859  [ 5952/19200]
per-ex loss: 0.542171  [ 6144/19200]
per-ex loss: 0.477640  [ 6336/19200]
per-ex loss: 0.552833  [ 6528/19200]
per-ex loss: 0.527704  [ 6720/19200]
per-ex loss: 0.641608  [ 6912/19200]
per-ex loss: 0.550394  [ 7104/19200]
per-ex loss: 0.533036  [ 7296/19200]
per-ex loss: 0.429447  [ 7488/19200]
per-ex loss: 0.665721  [ 7680/19200]
per-ex loss: 0.584597  [ 7872/19200]
per-ex loss: 0.577562  [ 8064/19200]
per-ex loss: 0.617986  [ 8256/19200]
per-ex loss: 0.512522  [ 8448/19200]
per-ex loss: 0.583068  [ 8640/19200]
per-ex loss: 0.508573  [ 8832/19200]
per-ex loss: 0.471353  [ 9024/19200]
per-ex loss: 0.519613  [ 9216/19200]
per-ex loss: 0.562508  [ 9408/19200]
per-ex loss: 0.560453  [ 9600/19200]
per-ex loss: 0.488167  [ 9792/19200]
per-ex loss: 0.568417  [ 9984/19200]
per-ex loss: 0.480531  [10176/19200]
per-ex loss: 0.710726  [10368/19200]
per-ex loss: 0.652945  [10560/19200]
per-ex loss: 0.656234  [10752/19200]
per-ex loss: 0.702964  [10944/19200]
per-ex loss: 0.668212  [11136/19200]
per-ex loss: 0.454360  [11328/19200]
per-ex loss: 0.607259  [11520/19200]
per-ex loss: 0.492237  [11712/19200]
per-ex loss: 0.513109  [11904/19200]
per-ex loss: 0.596298  [12096/19200]
per-ex loss: 0.657064  [12288/19200]
per-ex loss: 0.572193  [12480/19200]
per-ex loss: 0.558072  [12672/19200]
per-ex loss: 0.519570  [12864/19200]
per-ex loss: 0.663461  [13056/19200]
per-ex loss: 0.544802  [13248/19200]
per-ex loss: 0.574571  [13440/19200]
per-ex loss: 0.547757  [13632/19200]
per-ex loss: 0.508817  [13824/19200]
per-ex loss: 0.598774  [14016/19200]
per-ex loss: 0.546599  [14208/19200]
per-ex loss: 0.610325  [14400/19200]
per-ex loss: 0.657297  [14592/19200]
per-ex loss: 0.631188  [14784/19200]
per-ex loss: 0.583595  [14976/19200]
per-ex loss: 0.507578  [15168/19200]
per-ex loss: 0.592368  [15360/19200]
per-ex loss: 0.503383  [15552/19200]
per-ex loss: 0.477330  [15744/19200]
per-ex loss: 0.646724  [15936/19200]
per-ex loss: 0.733921  [16128/19200]
per-ex loss: 0.649926  [16320/19200]
per-ex loss: 0.484315  [16512/19200]
per-ex loss: 0.518377  [16704/19200]
per-ex loss: 0.496006  [16896/19200]
per-ex loss: 0.491006  [17088/19200]
per-ex loss: 0.479974  [17280/19200]
per-ex loss: 0.512787  [17472/19200]
per-ex loss: 0.544758  [17664/19200]
per-ex loss: 0.615849  [17856/19200]
per-ex loss: 0.676282  [18048/19200]
per-ex loss: 0.510589  [18240/19200]
per-ex loss: 0.549226  [18432/19200]
per-ex loss: 0.708763  [18624/19200]
per-ex loss: 0.607014  [18816/19200]
per-ex loss: 0.707279  [19008/19200]
per-ex loss: 0.591376  [19200/19200]
Train Error: Avg loss: 0.56766354
validation Error: 
 Avg loss: 0.55001024 
 F1: 0.417341 
 Precision: 0.309036 
 Recall: 0.642518
 IoU: 0.263696

test Error: 
 Avg loss: 0.50776279 
 F1: 0.495993 
 Precision: 0.385455 
 Recall: 0.695422
 IoU: 0.329781

We have finished training iteration 25
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_23_.pth
per-ex loss: 0.696312  [  192/19200]
per-ex loss: 0.669188  [  384/19200]
per-ex loss: 0.570013  [  576/19200]
per-ex loss: 0.440719  [  768/19200]
per-ex loss: 0.529374  [  960/19200]
per-ex loss: 0.586753  [ 1152/19200]
per-ex loss: 0.555292  [ 1344/19200]
per-ex loss: 0.635380  [ 1536/19200]
per-ex loss: 0.658973  [ 1728/19200]
per-ex loss: 0.659652  [ 1920/19200]
per-ex loss: 0.577312  [ 2112/19200]
per-ex loss: 0.534021  [ 2304/19200]
per-ex loss: 0.480858  [ 2496/19200]
per-ex loss: 0.630648  [ 2688/19200]
per-ex loss: 0.545606  [ 2880/19200]
per-ex loss: 0.478555  [ 3072/19200]
per-ex loss: 0.508260  [ 3264/19200]
per-ex loss: 0.602102  [ 3456/19200]
per-ex loss: 0.513354  [ 3648/19200]
per-ex loss: 0.507513  [ 3840/19200]
per-ex loss: 0.501572  [ 4032/19200]
per-ex loss: 0.571734  [ 4224/19200]
per-ex loss: 0.450741  [ 4416/19200]
per-ex loss: 0.629948  [ 4608/19200]
per-ex loss: 0.583183  [ 4800/19200]
per-ex loss: 0.588085  [ 4992/19200]
per-ex loss: 0.648707  [ 5184/19200]
per-ex loss: 0.599950  [ 5376/19200]
per-ex loss: 0.658366  [ 5568/19200]
per-ex loss: 0.555110  [ 5760/19200]
per-ex loss: 0.512393  [ 5952/19200]
per-ex loss: 0.442250  [ 6144/19200]
per-ex loss: 0.491256  [ 6336/19200]
per-ex loss: 0.530197  [ 6528/19200]
per-ex loss: 0.583078  [ 6720/19200]
per-ex loss: 0.792907  [ 6912/19200]
per-ex loss: 0.580027  [ 7104/19200]
per-ex loss: 0.555953  [ 7296/19200]
per-ex loss: 0.517964  [ 7488/19200]
per-ex loss: 0.670722  [ 7680/19200]
per-ex loss: 0.477077  [ 7872/19200]
per-ex loss: 0.492669  [ 8064/19200]
per-ex loss: 0.643693  [ 8256/19200]
per-ex loss: 0.512434  [ 8448/19200]
per-ex loss: 0.640571  [ 8640/19200]
per-ex loss: 0.477867  [ 8832/19200]
per-ex loss: 0.578255  [ 9024/19200]
per-ex loss: 0.463041  [ 9216/19200]
per-ex loss: 0.689040  [ 9408/19200]
per-ex loss: 0.622555  [ 9600/19200]
per-ex loss: 0.611315  [ 9792/19200]
per-ex loss: 0.501362  [ 9984/19200]
per-ex loss: 0.435598  [10176/19200]
per-ex loss: 0.646581  [10368/19200]
per-ex loss: 0.552991  [10560/19200]
per-ex loss: 0.581537  [10752/19200]
per-ex loss: 0.569196  [10944/19200]
per-ex loss: 0.659064  [11136/19200]
per-ex loss: 0.492931  [11328/19200]
per-ex loss: 0.527851  [11520/19200]
per-ex loss: 0.613710  [11712/19200]
per-ex loss: 0.531339  [11904/19200]
per-ex loss: 0.588014  [12096/19200]
per-ex loss: 0.501972  [12288/19200]
per-ex loss: 0.622775  [12480/19200]
per-ex loss: 0.480514  [12672/19200]
per-ex loss: 0.589660  [12864/19200]
per-ex loss: 0.515296  [13056/19200]
per-ex loss: 0.517741  [13248/19200]
per-ex loss: 0.526144  [13440/19200]
per-ex loss: 0.429246  [13632/19200]
per-ex loss: 0.497389  [13824/19200]
per-ex loss: 0.562257  [14016/19200]
per-ex loss: 0.501390  [14208/19200]
per-ex loss: 0.682009  [14400/19200]
per-ex loss: 0.746666  [14592/19200]
per-ex loss: 0.658554  [14784/19200]
per-ex loss: 0.490726  [14976/19200]
per-ex loss: 0.461982  [15168/19200]
per-ex loss: 0.562747  [15360/19200]
per-ex loss: 0.517549  [15552/19200]
per-ex loss: 0.629800  [15744/19200]
per-ex loss: 0.637769  [15936/19200]
per-ex loss: 0.608017  [16128/19200]
per-ex loss: 0.531420  [16320/19200]
per-ex loss: 0.493724  [16512/19200]
per-ex loss: 0.537858  [16704/19200]
per-ex loss: 0.603637  [16896/19200]
per-ex loss: 0.705880  [17088/19200]
per-ex loss: 0.728594  [17280/19200]
per-ex loss: 0.440029  [17472/19200]
per-ex loss: 0.505494  [17664/19200]
per-ex loss: 0.480112  [17856/19200]
per-ex loss: 0.508276  [18048/19200]
per-ex loss: 0.540551  [18240/19200]
per-ex loss: 0.469967  [18432/19200]
per-ex loss: 0.631033  [18624/19200]
per-ex loss: 0.550729  [18816/19200]
per-ex loss: 0.527386  [19008/19200]
per-ex loss: 0.586998  [19200/19200]
Train Error: Avg loss: 0.56334610
validation Error: 
 Avg loss: 0.53825456 
 F1: 0.434898 
 Precision: 0.334135 
 Recall: 0.622673
 IoU: 0.277872

test Error: 
 Avg loss: 0.49318391 
 F1: 0.510111 
 Precision: 0.412595 
 Recall: 0.667991
 IoU: 0.342382

We have finished training iteration 26
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_24_.pth
per-ex loss: 0.617872  [  192/19200]
per-ex loss: 0.530393  [  384/19200]
per-ex loss: 0.578251  [  576/19200]
per-ex loss: 0.470075  [  768/19200]
per-ex loss: 0.455948  [  960/19200]
per-ex loss: 0.418366  [ 1152/19200]
per-ex loss: 0.661528  [ 1344/19200]
per-ex loss: 0.522526  [ 1536/19200]
per-ex loss: 0.474381  [ 1728/19200]
per-ex loss: 0.599676  [ 1920/19200]
per-ex loss: 0.662981  [ 2112/19200]
per-ex loss: 0.525335  [ 2304/19200]
per-ex loss: 0.650650  [ 2496/19200]
per-ex loss: 0.787917  [ 2688/19200]
per-ex loss: 0.600248  [ 2880/19200]
per-ex loss: 0.540081  [ 3072/19200]
per-ex loss: 0.545860  [ 3264/19200]
per-ex loss: 0.507345  [ 3456/19200]
per-ex loss: 0.416035  [ 3648/19200]
per-ex loss: 0.500292  [ 3840/19200]
per-ex loss: 0.534827  [ 4032/19200]
per-ex loss: 0.605830  [ 4224/19200]
per-ex loss: 0.512697  [ 4416/19200]
per-ex loss: 0.586563  [ 4608/19200]
per-ex loss: 0.714768  [ 4800/19200]
per-ex loss: 0.548493  [ 4992/19200]
per-ex loss: 0.617101  [ 5184/19200]
per-ex loss: 0.579670  [ 5376/19200]
per-ex loss: 0.480404  [ 5568/19200]
per-ex loss: 0.674189  [ 5760/19200]
per-ex loss: 0.578684  [ 5952/19200]
per-ex loss: 0.495584  [ 6144/19200]
per-ex loss: 0.462600  [ 6336/19200]
per-ex loss: 0.590939  [ 6528/19200]
per-ex loss: 0.519857  [ 6720/19200]
per-ex loss: 0.604942  [ 6912/19200]
per-ex loss: 0.562065  [ 7104/19200]
per-ex loss: 0.495420  [ 7296/19200]
per-ex loss: 0.654711  [ 7488/19200]
per-ex loss: 0.573535  [ 7680/19200]
per-ex loss: 0.652926  [ 7872/19200]
per-ex loss: 0.434419  [ 8064/19200]
per-ex loss: 0.494425  [ 8256/19200]
per-ex loss: 0.494275  [ 8448/19200]
per-ex loss: 0.535446  [ 8640/19200]
per-ex loss: 0.464056  [ 8832/19200]
per-ex loss: 0.604908  [ 9024/19200]
per-ex loss: 0.509962  [ 9216/19200]
per-ex loss: 0.488374  [ 9408/19200]
per-ex loss: 0.565215  [ 9600/19200]
per-ex loss: 0.587401  [ 9792/19200]
per-ex loss: 0.583561  [ 9984/19200]
per-ex loss: 0.517760  [10176/19200]
per-ex loss: 0.821474  [10368/19200]
per-ex loss: 0.601449  [10560/19200]
per-ex loss: 0.438407  [10752/19200]
per-ex loss: 0.624618  [10944/19200]
per-ex loss: 0.677535  [11136/19200]
per-ex loss: 0.696881  [11328/19200]
per-ex loss: 0.550471  [11520/19200]
per-ex loss: 0.509637  [11712/19200]
per-ex loss: 0.585884  [11904/19200]
per-ex loss: 0.737225  [12096/19200]
per-ex loss: 0.646019  [12288/19200]
per-ex loss: 0.541804  [12480/19200]
per-ex loss: 0.630392  [12672/19200]
per-ex loss: 0.603622  [12864/19200]
per-ex loss: 0.447952  [13056/19200]
per-ex loss: 0.522211  [13248/19200]
per-ex loss: 0.459984  [13440/19200]
per-ex loss: 0.700812  [13632/19200]
per-ex loss: 0.455148  [13824/19200]
per-ex loss: 0.551323  [14016/19200]
per-ex loss: 0.614864  [14208/19200]
per-ex loss: 0.438496  [14400/19200]
per-ex loss: 0.580923  [14592/19200]
per-ex loss: 0.490446  [14784/19200]
per-ex loss: 0.502593  [14976/19200]
per-ex loss: 0.570680  [15168/19200]
per-ex loss: 0.631813  [15360/19200]
per-ex loss: 0.649215  [15552/19200]
per-ex loss: 0.675196  [15744/19200]
per-ex loss: 0.488187  [15936/19200]
per-ex loss: 0.491743  [16128/19200]
per-ex loss: 0.681385  [16320/19200]
per-ex loss: 0.524696  [16512/19200]
per-ex loss: 0.579525  [16704/19200]
per-ex loss: 0.564008  [16896/19200]
per-ex loss: 0.512291  [17088/19200]
per-ex loss: 0.540250  [17280/19200]
per-ex loss: 0.720681  [17472/19200]
per-ex loss: 0.480580  [17664/19200]
per-ex loss: 0.542006  [17856/19200]
per-ex loss: 0.483006  [18048/19200]
per-ex loss: 0.518263  [18240/19200]
per-ex loss: 0.568186  [18432/19200]
per-ex loss: 0.472037  [18624/19200]
per-ex loss: 0.638481  [18816/19200]
per-ex loss: 0.503111  [19008/19200]
per-ex loss: 0.564488  [19200/19200]
Train Error: Avg loss: 0.56221367
validation Error: 
 Avg loss: 0.53580499 
 F1: 0.439844 
 Precision: 0.344088 
 Recall: 0.609447
 IoU: 0.281923

test Error: 
 Avg loss: 0.48421186 
 F1: 0.518378 
 Precision: 0.427288 
 Recall: 0.658827
 IoU: 0.349872

We have finished training iteration 27
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_25_.pth
per-ex loss: 0.647802  [  192/19200]
per-ex loss: 0.546049  [  384/19200]
per-ex loss: 0.489478  [  576/19200]
per-ex loss: 0.582539  [  768/19200]
per-ex loss: 0.575137  [  960/19200]
per-ex loss: 0.726384  [ 1152/19200]
per-ex loss: 0.606551  [ 1344/19200]
per-ex loss: 0.451946  [ 1536/19200]
per-ex loss: 0.515619  [ 1728/19200]
per-ex loss: 0.598311  [ 1920/19200]
per-ex loss: 0.613446  [ 2112/19200]
per-ex loss: 0.566959  [ 2304/19200]
per-ex loss: 0.624353  [ 2496/19200]
per-ex loss: 0.594764  [ 2688/19200]
per-ex loss: 0.453612  [ 2880/19200]
per-ex loss: 0.633371  [ 3072/19200]
per-ex loss: 0.639252  [ 3264/19200]
per-ex loss: 0.655943  [ 3456/19200]
per-ex loss: 0.419618  [ 3648/19200]
per-ex loss: 0.610731  [ 3840/19200]
per-ex loss: 0.424358  [ 4032/19200]
per-ex loss: 0.549992  [ 4224/19200]
per-ex loss: 0.571650  [ 4416/19200]
per-ex loss: 0.538709  [ 4608/19200]
per-ex loss: 0.542085  [ 4800/19200]
per-ex loss: 0.598217  [ 4992/19200]
per-ex loss: 0.584307  [ 5184/19200]
per-ex loss: 0.608068  [ 5376/19200]
per-ex loss: 0.708347  [ 5568/19200]
per-ex loss: 0.584641  [ 5760/19200]
per-ex loss: 0.512022  [ 5952/19200]
per-ex loss: 0.487540  [ 6144/19200]
per-ex loss: 0.561479  [ 6336/19200]
per-ex loss: 0.635858  [ 6528/19200]
per-ex loss: 0.516445  [ 6720/19200]
per-ex loss: 0.624194  [ 6912/19200]
per-ex loss: 0.625216  [ 7104/19200]
per-ex loss: 0.530455  [ 7296/19200]
per-ex loss: 0.677246  [ 7488/19200]
per-ex loss: 0.653352  [ 7680/19200]
per-ex loss: 0.697581  [ 7872/19200]
per-ex loss: 0.629132  [ 8064/19200]
per-ex loss: 0.592948  [ 8256/19200]
per-ex loss: 0.588605  [ 8448/19200]
per-ex loss: 0.569211  [ 8640/19200]
per-ex loss: 0.593898  [ 8832/19200]
per-ex loss: 0.525973  [ 9024/19200]
per-ex loss: 0.676749  [ 9216/19200]
per-ex loss: 0.471830  [ 9408/19200]
per-ex loss: 0.581495  [ 9600/19200]
per-ex loss: 0.567734  [ 9792/19200]
per-ex loss: 0.605044  [ 9984/19200]
per-ex loss: 0.561492  [10176/19200]
per-ex loss: 0.717686  [10368/19200]
per-ex loss: 0.584177  [10560/19200]
per-ex loss: 0.490039  [10752/19200]
per-ex loss: 0.491985  [10944/19200]
per-ex loss: 0.561175  [11136/19200]
per-ex loss: 0.511611  [11328/19200]
per-ex loss: 0.617834  [11520/19200]
per-ex loss: 0.563550  [11712/19200]
per-ex loss: 0.509491  [11904/19200]
per-ex loss: 0.619555  [12096/19200]
per-ex loss: 0.616155  [12288/19200]
per-ex loss: 0.505255  [12480/19200]
per-ex loss: 0.530980  [12672/19200]
per-ex loss: 0.601408  [12864/19200]
per-ex loss: 0.637029  [13056/19200]
per-ex loss: 0.602639  [13248/19200]
per-ex loss: 0.599826  [13440/19200]
per-ex loss: 0.719469  [13632/19200]
per-ex loss: 0.495502  [13824/19200]
per-ex loss: 0.594979  [14016/19200]
per-ex loss: 0.590037  [14208/19200]
per-ex loss: 0.520577  [14400/19200]
per-ex loss: 0.591504  [14592/19200]
per-ex loss: 0.698288  [14784/19200]
per-ex loss: 0.596100  [14976/19200]
per-ex loss: 0.585145  [15168/19200]
per-ex loss: 0.558115  [15360/19200]
per-ex loss: 0.678986  [15552/19200]
per-ex loss: 0.545006  [15744/19200]
per-ex loss: 0.735786  [15936/19200]
per-ex loss: 0.494041  [16128/19200]
per-ex loss: 0.719538  [16320/19200]
per-ex loss: 0.523226  [16512/19200]
per-ex loss: 0.692055  [16704/19200]
per-ex loss: 0.607409  [16896/19200]
per-ex loss: 0.717929  [17088/19200]
per-ex loss: 0.596216  [17280/19200]
per-ex loss: 0.638574  [17472/19200]
per-ex loss: 0.568451  [17664/19200]
per-ex loss: 0.552413  [17856/19200]
per-ex loss: 0.611099  [18048/19200]
per-ex loss: 0.580827  [18240/19200]
per-ex loss: 0.651670  [18432/19200]
per-ex loss: 0.542725  [18624/19200]
per-ex loss: 0.660525  [18816/19200]
per-ex loss: 0.638623  [19008/19200]
per-ex loss: 0.498905  [19200/19200]
Train Error: Avg loss: 0.58711848
validation Error: 
 Avg loss: 0.59360846 
 F1: 0.416746 
 Precision: 0.379633 
 Recall: 0.461900
 IoU: 0.263221

test Error: 
 Avg loss: 0.52147073 
 F1: 0.481117 
 Precision: 0.468615 
 Recall: 0.494305
 IoU: 0.316758

We have finished training iteration 28
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_14_.pth
per-ex loss: 0.510984  [  192/19200]
per-ex loss: 0.523371  [  384/19200]
per-ex loss: 0.496870  [  576/19200]
per-ex loss: 0.529477  [  768/19200]
per-ex loss: 0.644266  [  960/19200]
per-ex loss: 0.589455  [ 1152/19200]
per-ex loss: 0.564912  [ 1344/19200]
per-ex loss: 0.512305  [ 1536/19200]
per-ex loss: 0.642848  [ 1728/19200]
per-ex loss: 0.650399  [ 1920/19200]
per-ex loss: 0.656839  [ 2112/19200]
per-ex loss: 0.472077  [ 2304/19200]
per-ex loss: 0.648394  [ 2496/19200]
per-ex loss: 0.677029  [ 2688/19200]
per-ex loss: 0.493283  [ 2880/19200]
per-ex loss: 0.717505  [ 3072/19200]
per-ex loss: 0.464460  [ 3264/19200]
per-ex loss: 0.737801  [ 3456/19200]
per-ex loss: 0.524872  [ 3648/19200]
per-ex loss: 0.629771  [ 3840/19200]
per-ex loss: 0.612886  [ 4032/19200]
per-ex loss: 0.683813  [ 4224/19200]
per-ex loss: 0.635531  [ 4416/19200]
per-ex loss: 0.608237  [ 4608/19200]
per-ex loss: 0.576209  [ 4800/19200]
per-ex loss: 0.636551  [ 4992/19200]
per-ex loss: 0.608852  [ 5184/19200]
per-ex loss: 0.557155  [ 5376/19200]
per-ex loss: 0.681578  [ 5568/19200]
per-ex loss: 0.584576  [ 5760/19200]
per-ex loss: 0.540295  [ 5952/19200]
per-ex loss: 0.508054  [ 6144/19200]
per-ex loss: 0.595521  [ 6336/19200]
per-ex loss: 0.455262  [ 6528/19200]
per-ex loss: 0.535141  [ 6720/19200]
per-ex loss: 0.677014  [ 6912/19200]
per-ex loss: 0.442808  [ 7104/19200]
per-ex loss: 0.646296  [ 7296/19200]
per-ex loss: 0.524922  [ 7488/19200]
per-ex loss: 0.574701  [ 7680/19200]
per-ex loss: 0.646162  [ 7872/19200]
per-ex loss: 0.708895  [ 8064/19200]
per-ex loss: 0.491492  [ 8256/19200]
per-ex loss: 0.490318  [ 8448/19200]
per-ex loss: 0.565046  [ 8640/19200]
per-ex loss: 0.550377  [ 8832/19200]
per-ex loss: 0.496819  [ 9024/19200]
per-ex loss: 0.499905  [ 9216/19200]
per-ex loss: 0.454582  [ 9408/19200]
per-ex loss: 0.546836  [ 9600/19200]
per-ex loss: 0.675319  [ 9792/19200]
per-ex loss: 0.553289  [ 9984/19200]
per-ex loss: 0.573869  [10176/19200]
per-ex loss: 0.536920  [10368/19200]
per-ex loss: 0.499875  [10560/19200]
per-ex loss: 0.541208  [10752/19200]
per-ex loss: 0.589576  [10944/19200]
per-ex loss: 0.681585  [11136/19200]
per-ex loss: 0.589397  [11328/19200]
per-ex loss: 0.684519  [11520/19200]
per-ex loss: 0.493118  [11712/19200]
per-ex loss: 0.606992  [11904/19200]
per-ex loss: 0.574149  [12096/19200]
per-ex loss: 0.529824  [12288/19200]
per-ex loss: 0.658305  [12480/19200]
per-ex loss: 0.537441  [12672/19200]
per-ex loss: 0.462263  [12864/19200]
per-ex loss: 0.687863  [13056/19200]
per-ex loss: 0.632202  [13248/19200]
per-ex loss: 0.544567  [13440/19200]
per-ex loss: 0.543627  [13632/19200]
per-ex loss: 0.622193  [13824/19200]
per-ex loss: 0.640898  [14016/19200]
per-ex loss: 0.574343  [14208/19200]
per-ex loss: 0.628544  [14400/19200]
per-ex loss: 0.734093  [14592/19200]
per-ex loss: 0.812116  [14784/19200]
per-ex loss: 0.601584  [14976/19200]
per-ex loss: 0.483805  [15168/19200]
per-ex loss: 0.700534  [15360/19200]
per-ex loss: 0.551930  [15552/19200]
per-ex loss: 0.590301  [15744/19200]
per-ex loss: 0.602548  [15936/19200]
per-ex loss: 0.489665  [16128/19200]
per-ex loss: 0.601412  [16320/19200]
per-ex loss: 0.536705  [16512/19200]
per-ex loss: 0.534890  [16704/19200]
per-ex loss: 0.645930  [16896/19200]
per-ex loss: 0.639040  [17088/19200]
per-ex loss: 0.502090  [17280/19200]
per-ex loss: 0.643929  [17472/19200]
per-ex loss: 0.538217  [17664/19200]
per-ex loss: 0.619391  [17856/19200]
per-ex loss: 0.642816  [18048/19200]
per-ex loss: 0.528891  [18240/19200]
per-ex loss: 0.604694  [18432/19200]
per-ex loss: 0.568982  [18624/19200]
per-ex loss: 0.576962  [18816/19200]
per-ex loss: 0.563084  [19008/19200]
per-ex loss: 0.522696  [19200/19200]
Train Error: Avg loss: 0.58320944
validation Error: 
 Avg loss: 0.57281750 
 F1: 0.414897 
 Precision: 0.329776 
 Recall: 0.559250
 IoU: 0.261748

test Error: 
 Avg loss: 0.51488721 
 F1: 0.488281 
 Precision: 0.407648 
 Recall: 0.608679
 IoU: 0.322997

We have finished training iteration 29
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_18_.pth
per-ex loss: 0.575406  [  192/19200]
per-ex loss: 0.649554  [  384/19200]
per-ex loss: 0.601586  [  576/19200]
per-ex loss: 0.726748  [  768/19200]
per-ex loss: 0.682952  [  960/19200]
per-ex loss: 0.495128  [ 1152/19200]
per-ex loss: 0.505146  [ 1344/19200]
per-ex loss: 0.549058  [ 1536/19200]
per-ex loss: 0.510122  [ 1728/19200]
per-ex loss: 0.661785  [ 1920/19200]
per-ex loss: 0.596143  [ 2112/19200]
per-ex loss: 0.694037  [ 2304/19200]
per-ex loss: 0.505578  [ 2496/19200]
per-ex loss: 0.705983  [ 2688/19200]
per-ex loss: 0.864960  [ 2880/19200]
per-ex loss: 0.501657  [ 3072/19200]
per-ex loss: 0.587360  [ 3264/19200]
per-ex loss: 0.479533  [ 3456/19200]
per-ex loss: 0.623250  [ 3648/19200]
per-ex loss: 0.582969  [ 3840/19200]
per-ex loss: 0.625817  [ 4032/19200]
per-ex loss: 0.579800  [ 4224/19200]
per-ex loss: 0.473920  [ 4416/19200]
per-ex loss: 0.630824  [ 4608/19200]
per-ex loss: 0.569914  [ 4800/19200]
per-ex loss: 0.558906  [ 4992/19200]
per-ex loss: 0.556299  [ 5184/19200]
per-ex loss: 0.468464  [ 5376/19200]
per-ex loss: 0.589804  [ 5568/19200]
per-ex loss: 0.468165  [ 5760/19200]
per-ex loss: 0.641344  [ 5952/19200]
per-ex loss: 0.553362  [ 6144/19200]
per-ex loss: 0.604049  [ 6336/19200]
per-ex loss: 0.462966  [ 6528/19200]
per-ex loss: 0.726218  [ 6720/19200]
per-ex loss: 0.543689  [ 6912/19200]
per-ex loss: 0.535570  [ 7104/19200]
per-ex loss: 0.609258  [ 7296/19200]
per-ex loss: 0.715493  [ 7488/19200]
per-ex loss: 0.438983  [ 7680/19200]
per-ex loss: 0.630315  [ 7872/19200]
per-ex loss: 0.503935  [ 8064/19200]
per-ex loss: 0.608987  [ 8256/19200]
per-ex loss: 0.702022  [ 8448/19200]
per-ex loss: 0.538691  [ 8640/19200]
per-ex loss: 0.660970  [ 8832/19200]
per-ex loss: 0.618184  [ 9024/19200]
per-ex loss: 0.747488  [ 9216/19200]
per-ex loss: 0.538502  [ 9408/19200]
per-ex loss: 0.494591  [ 9600/19200]
per-ex loss: 0.579029  [ 9792/19200]
per-ex loss: 0.544913  [ 9984/19200]
per-ex loss: 0.524003  [10176/19200]
per-ex loss: 0.526359  [10368/19200]
per-ex loss: 0.565526  [10560/19200]
per-ex loss: 0.724444  [10752/19200]
per-ex loss: 0.610443  [10944/19200]
per-ex loss: 0.588467  [11136/19200]
per-ex loss: 0.501883  [11328/19200]
per-ex loss: 0.437860  [11520/19200]
per-ex loss: 0.610834  [11712/19200]
per-ex loss: 0.510956  [11904/19200]
per-ex loss: 0.663542  [12096/19200]
per-ex loss: 0.541679  [12288/19200]
per-ex loss: 0.488989  [12480/19200]
per-ex loss: 0.555112  [12672/19200]
per-ex loss: 0.537532  [12864/19200]
per-ex loss: 0.494366  [13056/19200]
per-ex loss: 0.606434  [13248/19200]
per-ex loss: 0.542036  [13440/19200]
per-ex loss: 0.732316  [13632/19200]
per-ex loss: 0.603853  [13824/19200]
per-ex loss: 0.656827  [14016/19200]
per-ex loss: 0.558317  [14208/19200]
per-ex loss: 0.461086  [14400/19200]
per-ex loss: 0.623363  [14592/19200]
per-ex loss: 0.518008  [14784/19200]
per-ex loss: 0.495257  [14976/19200]
per-ex loss: 0.705671  [15168/19200]
per-ex loss: 0.616651  [15360/19200]
per-ex loss: 0.651583  [15552/19200]
per-ex loss: 0.528781  [15744/19200]
per-ex loss: 0.574489  [15936/19200]
per-ex loss: 0.515297  [16128/19200]
per-ex loss: 0.594897  [16320/19200]
per-ex loss: 0.615161  [16512/19200]
per-ex loss: 0.621621  [16704/19200]
per-ex loss: 0.749832  [16896/19200]
per-ex loss: 0.525662  [17088/19200]
per-ex loss: 0.612729  [17280/19200]
per-ex loss: 0.537971  [17472/19200]
per-ex loss: 0.526622  [17664/19200]
per-ex loss: 0.593010  [17856/19200]
per-ex loss: 0.574151  [18048/19200]
per-ex loss: 0.704852  [18240/19200]
per-ex loss: 0.473595  [18432/19200]
per-ex loss: 0.555674  [18624/19200]
per-ex loss: 0.627622  [18816/19200]
per-ex loss: 0.543663  [19008/19200]
per-ex loss: 0.552473  [19200/19200]
Train Error: Avg loss: 0.58300929
validation Error: 
 Avg loss: 0.57320074 
 F1: 0.425827 
 Precision: 0.394636 
 Recall: 0.462373
 IoU: 0.270509

test Error: 
 Avg loss: 0.50737727 
 F1: 0.495493 
 Precision: 0.498232 
 Recall: 0.492784
 IoU: 0.329339

We have finished training iteration 30
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_28_.pth
per-ex loss: 0.596531  [  192/19200]
per-ex loss: 0.571878  [  384/19200]
per-ex loss: 0.665489  [  576/19200]
per-ex loss: 0.469838  [  768/19200]
per-ex loss: 0.573888  [  960/19200]
per-ex loss: 0.763461  [ 1152/19200]
per-ex loss: 0.597744  [ 1344/19200]
per-ex loss: 0.570224  [ 1536/19200]
per-ex loss: 0.672759  [ 1728/19200]
per-ex loss: 0.719414  [ 1920/19200]
per-ex loss: 0.484556  [ 2112/19200]
per-ex loss: 0.531833  [ 2304/19200]
per-ex loss: 0.679522  [ 2496/19200]
per-ex loss: 0.607118  [ 2688/19200]
per-ex loss: 0.462967  [ 2880/19200]
per-ex loss: 0.552531  [ 3072/19200]
per-ex loss: 0.497546  [ 3264/19200]
per-ex loss: 0.715222  [ 3456/19200]
per-ex loss: 0.575769  [ 3648/19200]
per-ex loss: 0.651023  [ 3840/19200]
per-ex loss: 0.587510  [ 4032/19200]
per-ex loss: 0.646454  [ 4224/19200]
per-ex loss: 0.727058  [ 4416/19200]
per-ex loss: 0.636492  [ 4608/19200]
per-ex loss: 0.621715  [ 4800/19200]
per-ex loss: 0.594615  [ 4992/19200]
per-ex loss: 0.650003  [ 5184/19200]
per-ex loss: 0.517199  [ 5376/19200]
per-ex loss: 0.555125  [ 5568/19200]
per-ex loss: 0.509732  [ 5760/19200]
per-ex loss: 0.518252  [ 5952/19200]
per-ex loss: 0.599980  [ 6144/19200]
per-ex loss: 0.574038  [ 6336/19200]
per-ex loss: 0.646999  [ 6528/19200]
per-ex loss: 0.646199  [ 6720/19200]
per-ex loss: 0.684728  [ 6912/19200]
per-ex loss: 0.523205  [ 7104/19200]
per-ex loss: 0.585811  [ 7296/19200]
per-ex loss: 0.592573  [ 7488/19200]
per-ex loss: 0.681814  [ 7680/19200]
per-ex loss: 0.560600  [ 7872/19200]
per-ex loss: 0.618429  [ 8064/19200]
per-ex loss: 0.699764  [ 8256/19200]
per-ex loss: 0.825159  [ 8448/19200]
per-ex loss: 0.560136  [ 8640/19200]
per-ex loss: 0.672866  [ 8832/19200]
per-ex loss: 0.517514  [ 9024/19200]
per-ex loss: 0.727990  [ 9216/19200]
per-ex loss: 0.488733  [ 9408/19200]
per-ex loss: 0.548176  [ 9600/19200]
per-ex loss: 0.741879  [ 9792/19200]
per-ex loss: 0.606340  [ 9984/19200]
per-ex loss: 0.450354  [10176/19200]
per-ex loss: 0.478564  [10368/19200]
per-ex loss: 0.477154  [10560/19200]
per-ex loss: 0.582273  [10752/19200]
per-ex loss: 0.574107  [10944/19200]
per-ex loss: 0.626182  [11136/19200]
per-ex loss: 0.688011  [11328/19200]
per-ex loss: 0.539245  [11520/19200]
per-ex loss: 0.536331  [11712/19200]
per-ex loss: 0.526583  [11904/19200]
per-ex loss: 0.512210  [12096/19200]
per-ex loss: 0.605947  [12288/19200]
per-ex loss: 0.571506  [12480/19200]
per-ex loss: 0.588761  [12672/19200]
per-ex loss: 0.493364  [12864/19200]
per-ex loss: 0.657522  [13056/19200]
per-ex loss: 0.580371  [13248/19200]
per-ex loss: 0.519595  [13440/19200]
per-ex loss: 0.522747  [13632/19200]
per-ex loss: 0.652976  [13824/19200]
per-ex loss: 0.633680  [14016/19200]
per-ex loss: 0.648683  [14208/19200]
per-ex loss: 0.610800  [14400/19200]
per-ex loss: 0.652849  [14592/19200]
per-ex loss: 0.496053  [14784/19200]
per-ex loss: 0.678885  [14976/19200]
per-ex loss: 0.707018  [15168/19200]
per-ex loss: 0.520402  [15360/19200]
per-ex loss: 0.538246  [15552/19200]
per-ex loss: 0.685093  [15744/19200]
per-ex loss: 0.578224  [15936/19200]
per-ex loss: 0.588670  [16128/19200]
per-ex loss: 0.532727  [16320/19200]
per-ex loss: 0.625684  [16512/19200]
per-ex loss: 0.740083  [16704/19200]
per-ex loss: 0.507332  [16896/19200]
per-ex loss: 0.586841  [17088/19200]
per-ex loss: 0.596512  [17280/19200]
per-ex loss: 0.457994  [17472/19200]
per-ex loss: 0.519380  [17664/19200]
per-ex loss: 0.467821  [17856/19200]
per-ex loss: 0.763461  [18048/19200]
per-ex loss: 0.579348  [18240/19200]
per-ex loss: 0.639215  [18432/19200]
per-ex loss: 0.469916  [18624/19200]
per-ex loss: 0.521348  [18816/19200]
per-ex loss: 0.569598  [19008/19200]
per-ex loss: 0.511173  [19200/19200]
Train Error: Avg loss: 0.59237243
validation Error: 
 Avg loss: 0.57605392 
 F1: 0.422340 
 Precision: 0.356529 
 Recall: 0.517948
 IoU: 0.267700

test Error: 
 Avg loss: 0.50537801 
 F1: 0.498005 
 Precision: 0.449175 
 Recall: 0.558746
 IoU: 0.331562

We have finished training iteration 31
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_29_.pth
per-ex loss: 0.490233  [  192/19200]
per-ex loss: 0.555470  [  384/19200]
per-ex loss: 0.781868  [  576/19200]
per-ex loss: 0.486147  [  768/19200]
per-ex loss: 0.558196  [  960/19200]
per-ex loss: 0.521885  [ 1152/19200]
per-ex loss: 0.556440  [ 1344/19200]
per-ex loss: 0.516688  [ 1536/19200]
per-ex loss: 0.510950  [ 1728/19200]
per-ex loss: 0.459160  [ 1920/19200]
per-ex loss: 0.693914  [ 2112/19200]
per-ex loss: 0.583565  [ 2304/19200]
per-ex loss: 0.590913  [ 2496/19200]
per-ex loss: 0.663581  [ 2688/19200]
per-ex loss: 0.692171  [ 2880/19200]
per-ex loss: 0.636795  [ 3072/19200]
per-ex loss: 0.811977  [ 3264/19200]
per-ex loss: 0.556251  [ 3456/19200]
per-ex loss: 0.607439  [ 3648/19200]
per-ex loss: 0.474011  [ 3840/19200]
per-ex loss: 0.633864  [ 4032/19200]
per-ex loss: 0.552738  [ 4224/19200]
per-ex loss: 0.614498  [ 4416/19200]
per-ex loss: 0.532812  [ 4608/19200]
per-ex loss: 0.572215  [ 4800/19200]
per-ex loss: 0.476276  [ 4992/19200]
per-ex loss: 0.472599  [ 5184/19200]
per-ex loss: 0.716151  [ 5376/19200]
per-ex loss: 0.667542  [ 5568/19200]
per-ex loss: 0.710653  [ 5760/19200]
per-ex loss: 0.589772  [ 5952/19200]
per-ex loss: 0.668631  [ 6144/19200]
per-ex loss: 0.628990  [ 6336/19200]
per-ex loss: 0.559956  [ 6528/19200]
per-ex loss: 0.609298  [ 6720/19200]
per-ex loss: 0.439048  [ 6912/19200]
per-ex loss: 0.689939  [ 7104/19200]
per-ex loss: 0.651687  [ 7296/19200]
per-ex loss: 0.683591  [ 7488/19200]
per-ex loss: 0.570353  [ 7680/19200]
per-ex loss: 0.453389  [ 7872/19200]
per-ex loss: 0.474428  [ 8064/19200]
per-ex loss: 0.430424  [ 8256/19200]
per-ex loss: 0.634433  [ 8448/19200]
per-ex loss: 0.480168  [ 8640/19200]
per-ex loss: 0.548800  [ 8832/19200]
per-ex loss: 0.657832  [ 9024/19200]
per-ex loss: 0.530452  [ 9216/19200]
per-ex loss: 0.642461  [ 9408/19200]
per-ex loss: 0.506597  [ 9600/19200]
per-ex loss: 0.586343  [ 9792/19200]
per-ex loss: 0.442329  [ 9984/19200]
per-ex loss: 0.565807  [10176/19200]
per-ex loss: 0.610023  [10368/19200]
per-ex loss: 0.622440  [10560/19200]
per-ex loss: 0.666754  [10752/19200]
per-ex loss: 0.608003  [10944/19200]
per-ex loss: 0.596872  [11136/19200]
per-ex loss: 0.572364  [11328/19200]
per-ex loss: 0.648603  [11520/19200]
per-ex loss: 0.599865  [11712/19200]
per-ex loss: 0.480115  [11904/19200]
per-ex loss: 0.701472  [12096/19200]
per-ex loss: 0.575340  [12288/19200]
per-ex loss: 0.572966  [12480/19200]
per-ex loss: 0.481499  [12672/19200]
per-ex loss: 0.639781  [12864/19200]
per-ex loss: 0.513923  [13056/19200]
per-ex loss: 0.523877  [13248/19200]
per-ex loss: 0.544351  [13440/19200]
per-ex loss: 0.645220  [13632/19200]
per-ex loss: 0.462410  [13824/19200]
per-ex loss: 0.451263  [14016/19200]
per-ex loss: 0.553554  [14208/19200]
per-ex loss: 0.559130  [14400/19200]
per-ex loss: 0.679986  [14592/19200]
per-ex loss: 0.572796  [14784/19200]
per-ex loss: 0.592820  [14976/19200]
per-ex loss: 0.576920  [15168/19200]
per-ex loss: 0.529397  [15360/19200]
per-ex loss: 0.659403  [15552/19200]
per-ex loss: 0.655642  [15744/19200]
per-ex loss: 0.544643  [15936/19200]
per-ex loss: 0.567178  [16128/19200]
per-ex loss: 0.644890  [16320/19200]
per-ex loss: 0.483776  [16512/19200]
per-ex loss: 0.653252  [16704/19200]
per-ex loss: 0.583361  [16896/19200]
per-ex loss: 0.549159  [17088/19200]
per-ex loss: 0.516422  [17280/19200]
per-ex loss: 0.680241  [17472/19200]
per-ex loss: 0.546952  [17664/19200]
per-ex loss: 0.731012  [17856/19200]
per-ex loss: 0.513218  [18048/19200]
per-ex loss: 0.560296  [18240/19200]
per-ex loss: 0.608100  [18432/19200]
per-ex loss: 0.590733  [18624/19200]
per-ex loss: 0.529279  [18816/19200]
per-ex loss: 0.531836  [19008/19200]
per-ex loss: 0.698817  [19200/19200]
Train Error: Avg loss: 0.58171679
validation Error: 
 Avg loss: 0.57167834 
 F1: 0.412909 
 Precision: 0.327287 
 Recall: 0.559204
 IoU: 0.260167

test Error: 
 Avg loss: 0.51517761 
 F1: 0.487520 
 Precision: 0.403920 
 Recall: 0.614757
 IoU: 0.322331

We have finished training iteration 32
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_30_.pth
per-ex loss: 0.682628  [  192/19200]
per-ex loss: 0.529951  [  384/19200]
per-ex loss: 0.610872  [  576/19200]
per-ex loss: 0.606964  [  768/19200]
per-ex loss: 0.641930  [  960/19200]
per-ex loss: 0.513009  [ 1152/19200]
per-ex loss: 0.600726  [ 1344/19200]
per-ex loss: 0.618413  [ 1536/19200]
per-ex loss: 0.739505  [ 1728/19200]
per-ex loss: 0.726251  [ 1920/19200]
per-ex loss: 0.586880  [ 2112/19200]
per-ex loss: 0.686737  [ 2304/19200]
per-ex loss: 0.485355  [ 2496/19200]
per-ex loss: 0.591677  [ 2688/19200]
per-ex loss: 0.572264  [ 2880/19200]
per-ex loss: 0.649552  [ 3072/19200]
per-ex loss: 0.652079  [ 3264/19200]
per-ex loss: 0.625494  [ 3456/19200]
per-ex loss: 0.638674  [ 3648/19200]
per-ex loss: 0.578423  [ 3840/19200]
per-ex loss: 0.454428  [ 4032/19200]
per-ex loss: 0.521936  [ 4224/19200]
per-ex loss: 0.538695  [ 4416/19200]
per-ex loss: 0.477150  [ 4608/19200]
per-ex loss: 0.583383  [ 4800/19200]
per-ex loss: 0.571738  [ 4992/19200]
per-ex loss: 0.692322  [ 5184/19200]
per-ex loss: 0.606979  [ 5376/19200]
per-ex loss: 0.640649  [ 5568/19200]
per-ex loss: 0.549354  [ 5760/19200]
per-ex loss: 0.551854  [ 5952/19200]
per-ex loss: 0.610827  [ 6144/19200]
per-ex loss: 0.681399  [ 6336/19200]
per-ex loss: 0.620361  [ 6528/19200]
per-ex loss: 0.465443  [ 6720/19200]
per-ex loss: 0.704238  [ 6912/19200]
per-ex loss: 0.796289  [ 7104/19200]
per-ex loss: 0.523639  [ 7296/19200]
per-ex loss: 0.544295  [ 7488/19200]
per-ex loss: 0.427736  [ 7680/19200]
per-ex loss: 0.574796  [ 7872/19200]
per-ex loss: 0.505299  [ 8064/19200]
per-ex loss: 0.573353  [ 8256/19200]
per-ex loss: 0.708601  [ 8448/19200]
per-ex loss: 0.530446  [ 8640/19200]
per-ex loss: 0.554603  [ 8832/19200]
per-ex loss: 0.774118  [ 9024/19200]
per-ex loss: 0.641217  [ 9216/19200]
per-ex loss: 0.556127  [ 9408/19200]
per-ex loss: 0.574451  [ 9600/19200]
per-ex loss: 0.478239  [ 9792/19200]
per-ex loss: 0.531756  [ 9984/19200]
per-ex loss: 0.683626  [10176/19200]
per-ex loss: 0.517059  [10368/19200]
per-ex loss: 0.435936  [10560/19200]
per-ex loss: 0.498138  [10752/19200]
per-ex loss: 0.553453  [10944/19200]
per-ex loss: 0.705658  [11136/19200]
per-ex loss: 0.718848  [11328/19200]
per-ex loss: 0.590436  [11520/19200]
per-ex loss: 0.590938  [11712/19200]
per-ex loss: 0.765352  [11904/19200]
per-ex loss: 0.564225  [12096/19200]
per-ex loss: 0.485941  [12288/19200]
per-ex loss: 0.562928  [12480/19200]
per-ex loss: 0.625076  [12672/19200]
per-ex loss: 0.611445  [12864/19200]
per-ex loss: 0.561324  [13056/19200]
per-ex loss: 0.597507  [13248/19200]
per-ex loss: 0.511510  [13440/19200]
per-ex loss: 0.557233  [13632/19200]
per-ex loss: 0.712171  [13824/19200]
per-ex loss: 0.645727  [14016/19200]
per-ex loss: 0.544551  [14208/19200]
per-ex loss: 0.668817  [14400/19200]
per-ex loss: 0.563555  [14592/19200]
per-ex loss: 0.562590  [14784/19200]
per-ex loss: 0.558421  [14976/19200]
per-ex loss: 0.484135  [15168/19200]
per-ex loss: 0.698186  [15360/19200]
per-ex loss: 0.634370  [15552/19200]
per-ex loss: 0.492392  [15744/19200]
per-ex loss: 0.455163  [15936/19200]
per-ex loss: 0.520860  [16128/19200]
per-ex loss: 0.567318  [16320/19200]
per-ex loss: 0.634013  [16512/19200]
per-ex loss: 0.504968  [16704/19200]
per-ex loss: 0.600862  [16896/19200]
per-ex loss: 0.551962  [17088/19200]
per-ex loss: 0.600293  [17280/19200]
per-ex loss: 0.445679  [17472/19200]
per-ex loss: 0.524222  [17664/19200]
per-ex loss: 0.549579  [17856/19200]
per-ex loss: 0.583386  [18048/19200]
per-ex loss: 0.497075  [18240/19200]
per-ex loss: 0.742025  [18432/19200]
per-ex loss: 0.552478  [18624/19200]
per-ex loss: 0.482899  [18816/19200]
per-ex loss: 0.458212  [19008/19200]
per-ex loss: 0.636084  [19200/19200]
Train Error: Avg loss: 0.58511730
validation Error: 
 Avg loss: 0.56513874 
 F1: 0.434063 
 Precision: 0.376844 
 Recall: 0.511768
 IoU: 0.277190

test Error: 
 Avg loss: 0.49828237 
 F1: 0.504379 
 Precision: 0.464890 
 Recall: 0.551199
 IoU: 0.337237

We have finished training iteration 33
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_31_.pth
per-ex loss: 0.719796  [  192/19200]
per-ex loss: 0.663765  [  384/19200]
per-ex loss: 0.679975  [  576/19200]
per-ex loss: 0.708300  [  768/19200]
per-ex loss: 0.547810  [  960/19200]
per-ex loss: 0.583053  [ 1152/19200]
per-ex loss: 0.617935  [ 1344/19200]
per-ex loss: 0.734808  [ 1536/19200]
per-ex loss: 0.512350  [ 1728/19200]
per-ex loss: 0.611246  [ 1920/19200]
per-ex loss: 0.492777  [ 2112/19200]
per-ex loss: 0.699365  [ 2304/19200]
per-ex loss: 0.496965  [ 2496/19200]
per-ex loss: 0.595206  [ 2688/19200]
per-ex loss: 0.521445  [ 2880/19200]
per-ex loss: 0.560212  [ 3072/19200]
per-ex loss: 0.554887  [ 3264/19200]
per-ex loss: 0.465394  [ 3456/19200]
per-ex loss: 0.564013  [ 3648/19200]
per-ex loss: 0.527787  [ 3840/19200]
per-ex loss: 0.589711  [ 4032/19200]
per-ex loss: 0.433158  [ 4224/19200]
per-ex loss: 0.716728  [ 4416/19200]
per-ex loss: 0.563480  [ 4608/19200]
per-ex loss: 0.550091  [ 4800/19200]
per-ex loss: 0.561704  [ 4992/19200]
per-ex loss: 0.504938  [ 5184/19200]
per-ex loss: 0.513689  [ 5376/19200]
per-ex loss: 0.500871  [ 5568/19200]
per-ex loss: 0.504309  [ 5760/19200]
per-ex loss: 0.700115  [ 5952/19200]
per-ex loss: 0.587873  [ 6144/19200]
per-ex loss: 0.657604  [ 6336/19200]
per-ex loss: 0.678248  [ 6528/19200]
per-ex loss: 0.577446  [ 6720/19200]
per-ex loss: 0.583883  [ 6912/19200]
per-ex loss: 0.429970  [ 7104/19200]
per-ex loss: 0.719079  [ 7296/19200]
per-ex loss: 0.738714  [ 7488/19200]
per-ex loss: 0.510546  [ 7680/19200]
per-ex loss: 0.535804  [ 7872/19200]
per-ex loss: 0.560442  [ 8064/19200]
per-ex loss: 0.527451  [ 8256/19200]
per-ex loss: 0.780083  [ 8448/19200]
per-ex loss: 0.492063  [ 8640/19200]
per-ex loss: 0.486610  [ 8832/19200]
per-ex loss: 0.475970  [ 9024/19200]
per-ex loss: 0.658348  [ 9216/19200]
per-ex loss: 0.500049  [ 9408/19200]
per-ex loss: 0.645420  [ 9600/19200]
per-ex loss: 0.709900  [ 9792/19200]
per-ex loss: 0.421552  [ 9984/19200]
per-ex loss: 0.546862  [10176/19200]
per-ex loss: 0.664294  [10368/19200]
per-ex loss: 0.597036  [10560/19200]
per-ex loss: 0.664586  [10752/19200]
per-ex loss: 0.636295  [10944/19200]
per-ex loss: 0.563656  [11136/19200]
per-ex loss: 0.571051  [11328/19200]
per-ex loss: 0.659130  [11520/19200]
per-ex loss: 0.630507  [11712/19200]
per-ex loss: 0.580553  [11904/19200]
per-ex loss: 0.626745  [12096/19200]
per-ex loss: 0.489497  [12288/19200]
per-ex loss: 0.545992  [12480/19200]
per-ex loss: 0.638132  [12672/19200]
per-ex loss: 0.535955  [12864/19200]
per-ex loss: 0.506010  [13056/19200]
per-ex loss: 0.562172  [13248/19200]
per-ex loss: 0.520873  [13440/19200]
per-ex loss: 0.555202  [13632/19200]
per-ex loss: 0.636023  [13824/19200]
per-ex loss: 0.575551  [14016/19200]
per-ex loss: 0.668674  [14208/19200]
per-ex loss: 0.626539  [14400/19200]
per-ex loss: 0.513672  [14592/19200]
per-ex loss: 0.538699  [14784/19200]
per-ex loss: 0.492475  [14976/19200]
per-ex loss: 0.545011  [15168/19200]
per-ex loss: 0.644887  [15360/19200]
per-ex loss: 0.517205  [15552/19200]
per-ex loss: 0.601129  [15744/19200]
per-ex loss: 0.611162  [15936/19200]
per-ex loss: 0.671010  [16128/19200]
per-ex loss: 0.442120  [16320/19200]
per-ex loss: 0.543220  [16512/19200]
per-ex loss: 0.580525  [16704/19200]
per-ex loss: 0.699536  [16896/19200]
per-ex loss: 0.533508  [17088/19200]
per-ex loss: 0.541532  [17280/19200]
per-ex loss: 0.571222  [17472/19200]
per-ex loss: 0.535608  [17664/19200]
per-ex loss: 0.476711  [17856/19200]
per-ex loss: 0.512161  [18048/19200]
per-ex loss: 0.619972  [18240/19200]
per-ex loss: 0.681364  [18432/19200]
per-ex loss: 0.516031  [18624/19200]
per-ex loss: 0.546991  [18816/19200]
per-ex loss: 0.615685  [19008/19200]
per-ex loss: 0.563727  [19200/19200]
Train Error: Avg loss: 0.57989431
validation Error: 
 Avg loss: 0.56572749 
 F1: 0.421627 
 Precision: 0.343116 
 Recall: 0.546729
 IoU: 0.267128

test Error: 
 Avg loss: 0.49677175 
 F1: 0.505762 
 Precision: 0.442216 
 Recall: 0.590636
 IoU: 0.338475

We have finished training iteration 34
Deleting model ./segnet_vein_patch_train/saved_model_wrapper/models/SegNet_32_.pth
slurmstepd: error: *** STEP 17779.0 ON aga2 CANCELLED AT 2025-01-21T17:01:43 ***
